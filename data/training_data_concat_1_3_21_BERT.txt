The	O
GANfather	O
:	O
The	O
man	O
whos	O
given	O
machines	O
the	O
gift	O
of	O
imagination	O
By	O
pitting	O
neural	B
networks	I
against	O
one	O
another	O
,	O
Ian	O
Goodfellow	O
has	O
created	O
a	O
powerful	O
AI	B
tool	O
.	O

Now	O
he	O
,	O
and	O
the	O
rest	O
of	O
us	O
,	O
must	O
face	O
the	O
consequences	O
.	O

By	O
Martin	O
Giles	O
February	O
21	O
,	O
2018	O
One	O
night	O
in	O
2014	O
,	O
Ian	O
Goodfellow	O
went	O
drinking	O
to	O
celebrate	O
with	O
a	O
fellow	O
doctoral	O
student	O
who	O
had	O
just	O
graduated	O
.	O

At	O
Les	O
3	O
Brasseurs	O
(	O
The	O
Three	O
Brewers	O
)	O
,	O
a	O
favorite	O
Montreal	O
watering	O
hole	O
,	O
some	O
friends	O
asked	O
for	O
his	O
help	O
with	O
a	O
thorny	O
project	O
they	O
were	O
working	O
on	O
:	O
a	O
computer	B
that	O
could	O
create	O
photos	O
by	O
itself	O
.	O

Researchers	O
were	O
already	O
using	O
neural	B
networks	I
,	O
algorithms	B
loosely	O
modeled	O
on	O
the	O
web	O
of	O
neurons	O
in	O
the	O
human	O
brain	O
,	O
as	O
generative	B
models	I
to	O
create	O
plausible	O
new	O
data	O
of	O
their	O
own	O
.	O

But	O
the	O
results	O
were	O
often	O
not	O
very	O
good	O
:	O
images	O
of	O
a	O
computer-generated	O
face	O
tended	O
to	O
be	O
blurry	O
or	O
have	O
errors	O
like	O
missing	O
ears	O
.	O

The	O
plan	O
Goodfellows	O
friends	O
were	O
proposing	O
was	O
to	O
use	O
a	O
complex	O
statistical	O
analysis	O
of	O
the	O
elements	O
that	O
make	O
up	O
a	O
photograph	O
to	O
help	O
machines	O
come	O
up	O
with	O
images	O
by	O
themselves	O
.	O

This	O
would	O
have	O
required	O
a	O
massive	O
amount	O
of	O
number-crunching	O
,	O
and	O
Goodfellow	O
told	O
them	O
it	O
simply	O
wasnt	O
going	O
to	O
work	O
.	O

But	O
as	O
he	O
pondered	O
the	O
problem	O
over	O
his	O
beer	O
,	O
he	O
hit	O
on	O
an	O
idea	O
.	O

What	O
if	O
you	O
pitted	O
two	O
neural	B
networks	I
against	O
each	O
other	O
?	O

His	O
friends	O
were	O
skeptical	O
,	O
so	O
once	O
he	O
got	O
home	O
,	O
where	O
his	O
girlfriend	O
was	O
already	O
fast	O
asleep	O
,	O
he	O
decided	O
to	O
give	O
it	O
a	O
try	O
.	O

Goodfellow	O
coded	O
into	O
the	O
early	O
hours	O
and	O
then	O
tested	O
his	O
software	O
.	O

It	O
worked	O
the	O
first	O
time	O
.	O

What	O
he	O
invented	O
that	O
night	O
is	O
now	O
called	O
a	O
GAN	B
,	O
or	O
generative	B
adversarial	I
network	I
.	O

The	O
technique	O
has	O
sparked	O
huge	O
excitement	O
in	O
the	O
field	O
of	O
machine	B
learning	I
and	O
turned	O
its	O
creator	O
into	O
an	O
AI	B
celebrity	O
.	O

In	O
the	O
last	O
few	O
years	O
,	O
AI	B
researchers	O
have	O
made	O
impressive	O
progress	O
using	O
a	O
technique	O
called	O
deep	B
learning	I
.	O

Supply	O
a	O
deep-learning	B
system	O
with	O
enough	O
images	O
and	O
it	O
learns	O
to	O
,	O
say	O
,	O
recognize	O
a	O
pedestrian	O
whos	O
about	O
to	O
cross	O
a	O
road	O
.	O

This	O
approach	O
has	O
made	O
possible	O
things	O
like	O
self-driving	B
cars	I
and	O
the	O
conversational	B
technology	I
that	O
powers	O
Alexa	O
,	O
Siri	O
,	O
and	O
other	O
virtual	B
assistants	I
.	O

But	O
while	O
deep-learning	B
AIs	I
can	O
learn	O
to	O
recognize	O
things	O
,	O
they	O
have	O
not	O
been	O
good	O
at	O
creating	O
them	O
.	O

The	O
goal	O
of	O
GANs	B
is	O
to	O
give	O
machines	O
something	O
akin	O
to	O
an	O
imagination	O
.	O

In	O
the	O
future	O
,	O
computers	B
will	O
get	O
much	O
better	O
at	O
feasting	O
on	O
raw	O
data	O
and	O
working	O
out	O
what	O
they	O
need	O
to	O
learn	O
from	O
it	O
.	O

Doing	O
so	O
wouldnt	O
merely	O
enable	O
them	O
to	O
draw	O
pretty	O
pictures	O
or	O
compose	O
music	O
;	O
it	O
would	O
make	O
them	O
less	O
reliant	O
on	O
humans	O
to	O
instruct	O
them	O
about	O
the	O
world	O
and	O
the	O
way	O
it	O
works	O
.	O

Today	O
,	O
AI	B
programmers	O
often	O
need	O
to	O
tell	O
a	O
machine	O
exactly	O
whats	O
in	O
the	O
training	O
data	O
its	O
being	O
fedwhich	O
of	O
a	O
million	O
pictures	O
contain	O
a	O
pedestrian	O
crossing	O
a	O
road	O
,	O
and	O
which	O
dont	O
.	O

This	O
is	O
not	O
only	O
costly	O
and	O
labor-intensive	O
;	O
it	O
limits	O
how	O
well	O
the	O
system	O
deals	O
with	O
even	O
slight	O
departures	O
from	O
what	O
it	O
was	O
trained	O
on	O
.	O

In	O
the	O
future	O
,	O
computers	O
will	O
get	O
much	O
better	O
at	O
feasting	O
on	O
raw	O
data	O
and	O
working	O
out	O
what	O
they	O
need	O
to	O
learn	O
from	O
it	O
without	O
being	O
told	O
.	O

That	O
will	O
mark	O
a	O
big	O
leap	O
forward	O
in	O
whats	O
known	O
in	O
AI	B
as	O
unsupervised	B
learning	I
.	O

A	O
self-driving	B
car	I
could	O
teach	O
itself	O
about	O
many	O
different	O
road	O
conditions	O
without	O
leaving	O
the	O
garage	O
.	O

A	O
robot	B
could	O
anticipate	O
the	O
obstacles	O
it	O
might	O
encounter	O
in	O
a	O
busy	O
warehouse	O
without	O
needing	O
to	O
be	O
taken	O
around	O
it	O
.	O

That	O
will	O
mark	O
a	O
big	O
leap	O
forward	O
in	O
what	O
is	O
known	O
in	O
AI	B
as	O
unsupervised	B
learning	I
.	O

Our	O
ability	O
to	O
imagine	O
and	O
reflect	O
on	O
many	O
different	O
scenarios	O
is	O
part	O
of	O
what	O
makes	O
us	O
human	O
.	O

And	O
when	O
future	O
historians	O
of	O
technology	O
look	O
back	O
,	O
theyre	O
likely	O
to	O
see	O
GANs	B
as	O
a	O
big	O
step	O
toward	O
creating	O
machines	B
with	I
a	I
human-like	I
consciousness	I
.	O

Yann	O
LeCun	O
,	O
Facebooks	O
chief	O
AI	B
scientist	O
,	O
has	O
called	O
GANs	B
the	O
coolest	O
idea	O
in	O
deep	O
learning	O
in	O
the	O
last	O
20	O
years	O
.	O

Another	O
AI	O
luminary	O
,	O
Andrew	O
Ng	O
,	O
the	O
former	O
chief	O
scientist	O
of	O
Chinas	O
Baidu	O
,	O
says	O
GANs	B
represent	O
a	O
significant	O
and	O
fundamental	O
advance	O
thats	O
inspired	O
a	O
growing	O
global	O
community	O
of	O
researchers	O
.	O

The	O
GANfather	O
,	O
Part	O
II	O
:	O
AI	O
fight	O
club	O
Goodfellow	O
is	O
now	O
a	O
research	O
scientist	O
on	O
the	O
Google	O
Brain	O
team	O
,	O
at	O
the	O
companys	O
headquarters	O
in	O
Mountain	O
View	O
,	O
California	O
.	O

When	O
I	O
met	O
him	O
there	O
recently	O
,	O
he	O
still	O
seemed	O
surprised	O
by	O
his	O
superstar	O
status	O
,	O
calling	O
it	O
a	O
little	O
surreal	O
.	O

Perhaps	O
no	O
less	O
surprising	O
is	O
that	O
,	O
having	O
made	O
his	O
discovery	O
,	O
he	O
now	O
spends	O
much	O
of	O
his	O
time	O
working	O
against	O
those	O
who	O
wish	O
to	O
use	O
it	O
for	O
evil	O
ends	O
.	O

The	O
magic	O
of	O
GANs	B
lies	O
in	O
the	O
rivalry	O
between	O
the	O
two	O
neural	B
nets	I
.	O

It	O
mimics	O
the	O
back-and-forth	O
between	O
a	O
picture	O
forger	O
and	O
an	O
art	O
detective	O
who	O
repeatedly	O
try	O
to	O
outwit	O
one	O
another	O
.	O

Both	O
networks	B
are	O
trained	O
on	O
the	O
same	O
data	O
set	O
.	O

The	O
first	O
one	O
,	O
known	O
as	O
the	O
generator	B
,	O
is	O
charged	O
with	O
producing	O
artificial	O
outputs	O
,	O
such	O
as	O
photos	O
or	O
handwriting	O
,	O
that	O
are	O
as	O
realistic	O
as	O
possible	O
.	O

The	O
second	O
,	O
known	O
as	O
the	O
discriminator	B
,	O
compares	O
these	O
with	O
genuine	O
images	O
from	O
the	O
original	O
data	O
set	O
and	O
tries	O
to	O
determine	O
which	O
are	O
real	O
and	O
which	O
are	O
fake	O
.	O

On	O
the	O
basis	O
of	O
those	O
results	O
,	O
the	O
generator	B
adjusts	O
its	O
parameters	O
for	O
creating	O
new	O
images	O
.	O

And	O
so	O
it	O
goes	O
,	O
until	O
the	O
discriminator	B
can	O
no	O
longer	O
tell	O
whats	O
genuine	O
and	O
whats	O
bogus	O
.	O

A	O
GAN	B
trained	O
on	O
photos	O
of	O
real	O
celebrities	O
came	O
up	O
with	O
its	O
own	O
set	O
of	O
imaginary	O
stars	O
.	O

In	O
most	O
cases	O
,	O
the	O
fakes	O
looked	O
pretty	O
realistic	O
.	O

In	O
one	O
widely	O
publicized	O
example	O
last	O
year	O
,	O
researchers	O
at	O
Nvidia	O
,	O
a	O
chip	B
company	O
heavily	O
invested	O
in	O
AI	B
,	O
trained	O
a	O
GAN	B
to	O
generate	O
pictures	O
of	O
imaginary	O
celebrities	O
by	O
studying	O
real	O
ones	O
.	O

Not	O
all	O
the	O
fake	O
stars	O
it	O
produced	O
were	O
perfect	O
,	O
but	O
some	O
were	O
impressively	O
realistic	O
.	O

Unlike	O
other	O
machine-learning	B
approaches	O
that	O
require	O
tens	O
of	O
thousands	O
of	O
training	O
images	O
,	O
GANs	B
can	O
become	O
proficient	O
with	O
a	O
few	O
hundred	O
.	O

This	O
power	O
of	O
imagination	O
is	O
still	O
limited	O
.	O

Once	O
its	O
been	O
trained	O
on	O
a	O
lot	O
of	O
dog	O
photos	O
,	O
a	O
GAN	B
can	O
generate	O
a	O
convincing	O
fake	B
image	I
of	O
a	O
dog	O
that	O
has	O
,	O
say	O
,	O
a	O
different	O
pattern	O
of	O
spots	O
;	O
but	O
it	O
cant	O
conceive	O
of	O
an	O
entirely	O
new	O
animal	O
.	O

The	O
quality	O
of	O
the	O
original	O
training	O
data	O
also	O
has	O
a	O
big	O
influence	O
on	O
the	O
results	O
.	O

In	O
one	O
telling	O
example	O
,	O
a	O
GAN	B
began	O
producing	O
pictures	O
of	O
cats	O
with	O
random	O
letters	O
integrated	O
into	O
the	O
images	O
.	O

Because	O
the	O
training	O
data	O
contained	O
cat	O
memes	O
from	O
the	O
internet	B
,	O
the	O
machine	O
had	O
taught	O
itself	O
that	O
words	O
were	O
part	O
of	O
what	O
it	O
meant	O
to	O
be	O
a	O
cat	O
.	O

GANs	B
are	O
also	O
temperamental	O
,	O
says	O
Pedro	O
Domingos	O
,	O
a	O
machine-learning	B
researcher	O
at	O
the	O
University	O
of	O
Washington	O
.	O

If	O
the	O
discriminator	B
is	O
too	O
easy	O
to	O
fool	O
,	O
the	O
generators	B
output	O
wont	O
look	O
realistic	O
.	O

And	O
calibrating	O
the	O
two	O
dueling	O
neural	B
nets	I
can	O
be	O
difficult	O
,	O
which	O
explains	O
why	O
GANs	B
sometimes	O
spit	O
out	O
bizarre	O
stuff	O
such	O
as	O
animals	O
with	O
two	O
heads	O
.	O

Still	O
,	O
the	O
challenges	O
havent	O
deterred	O
researchers	O
.	O

Since	O
Goodfellow	O
and	O
a	O
few	O
others	O
published	O
the	O
first	O
study	O
on	O
his	O
discovery	O
,	O
in	O
2014	O
,	O
hundreds	O
of	O
GAN-related	B
papers	O
have	O
been	O
written	O
.	O

One	O
fan	O
of	O
the	O
technology	B
has	O
even	O
created	O
a	O
web	B
page	I
called	O
the	O
GAN	B
zoo	O
,	O
dedicated	O
to	O
keeping	O
track	O
of	O
the	O
various	O
versions	O
of	O
the	O
technique	O
that	O
have	O
been	O
developed	O
.	O

The	O
most	O
obvious	O
immediate	O
applications	O
are	O
in	O
areas	O
that	O
involve	O
a	O
lot	O
of	O
imagery	O
,	O
such	O
as	O
video	B
games	I
and	O
fashion	O
:	O
what	O
,	O
for	O
instance	O
,	O
might	O
a	O
game	O
character	O
look	O
like	O
running	O
through	O
the	O
rain	O
?	O

But	O
looking	O
ahead	O
,	O
Goodfellow	O
thinks	O
GANs	B
will	O
drive	O
more	O
significant	O
advances	O
.	O

There	O
are	O
a	O
lot	O
of	O
areas	O
of	O
science	O
and	O
engineering	O
where	O
we	O
need	O
to	O
optimize	O
something	O
,	O
he	O
says	O
,	O
citing	O
examples	O
such	O
as	O
medicines	B
that	O
need	O
to	O
be	O
more	O
effective	O
or	O
batteries	B
that	O
must	O
get	O
more	O
efficient	O
.	O

Thats	O
going	O
to	O
be	O
the	O
next	O
big	O
wave	O
.	O

In	O
high-energy	O
physics	O
,	O
scientists	O
use	O
powerful	O
computers	B
to	O
simulate	O
the	O
likely	O
interactions	O
of	O
hundreds	O
of	O
subatomic	O
particles	O
in	O
machines	B
like	O
the	O
Large	B
Hadron	I
Collider	I
at	O
CERN	O
in	O
Switzerland	O
.	O

These	O
simulations	B
are	O
slow	O
and	O
require	O
massive	O
computing	O
power	O
.	O

Researchers	O
at	O
Yale	O
University	O
and	O
Lawrence	O
Berkeley	O
National	O
Laboratory	O
have	O
developed	O
a	O
GAN	B
that	O
,	O
after	O
training	O
on	O
existing	O
simulation	O
data	O
,	O
learns	O
to	O
generate	O
pretty	O
accurate	O
predictions	O
of	O
how	O
a	O
particular	O
particle	O
will	O
behave	O
,	O
and	O
does	O
it	O
much	O
faster	O
.	O

Goodfellow's	O
creation	O
can	O
be	O
used	O
to	O
imagine	O
all	O
sorts	O
of	O
things	O
,	O
including	O
new	O
interior	O
designs	O
.	O

Medical	O
research	O
is	O
another	O
promising	O
field	O
.	O

Privacy	O
concerns	O
mean	O
researchers	O
sometimes	O
cant	O
get	O
enough	O
real	O
patient	O
data	O
to	O
,	O
say	O
,	O
analyze	O
why	O
a	O
drug	O
didnt	O
work	O
.	O

GANs	B
can	O
help	O
solve	O
this	O
problem	O
by	O
generating	O
fake	O
records	O
that	O
are	O
almost	O
as	O
good	O
as	O
the	O
real	O
thing	O
,	O
says	O
Casey	O
Greene	O
of	O
the	O
University	O
of	O
Pennsylvania	O
.	O

This	O
data	O
could	O
be	O
shared	O
more	O
widely	O
,	O
helping	O
to	O
advance	O
research	O
,	O
while	O
the	O
real	O
records	O
are	O
tightly	O
protected	O
.	O

The	O
GANfather	O
,	O
Part	O
III	O
:	O
Bad	O
fellows	O
There	O
is	O
a	O
darker	O
side	O
,	O
however	O
.	O

A	O
machine	B
designed	O
to	O
create	O
realistic	B
fakes	I
is	O
a	O
perfect	O
weapon	O
for	O
purveyors	O
of	O
fake	B
news	I
who	O
want	O
to	O
influence	O
everything	O
from	O
stock	O
prices	O
to	O
elections	O
.	O

AI	B
tools	O
are	O
already	O
being	O
used	O
to	O
put	O
pictures	O
of	O
other	O
peoples	O
faces	O
on	O
the	O
bodies	O
of	O
porn	O
stars	O
and	O
put	O
words	O
in	O
the	O
mouths	O
of	O
politicians	O
.	O

GANs	B
didnt	O
create	O
this	O
problem	O
,	O
but	O
theyll	O
make	O
it	O
worse	O
.	O

Hany	O
Farid	O
,	O
who	O
studies	O
digital	O
forensics	O
at	O
Dartmouth	O
College	O
,	O
is	O
working	O
on	O
better	O
ways	O
to	O
spot	O
fake	B
videos	I
,	O
such	O
as	O
detecting	O
slight	O
changes	O
in	O
the	O
color	O
of	O
faces	O
caused	O
by	O
inhaling	O
and	O
exhaling	O
that	O
GANs	B
find	O
hard	O
to	O
mimic	O
precisely	O
.	O

But	O
he	O
warns	O
that	O
GANs	B
will	O
adapt	O
in	O
turn	O
.	O

Were	O
fundamentally	O
in	O
a	O
weak	O
position	O
,	O
says	O
Farid	O
.	O

This	O
cat-and-mouse	O
game	O
will	O
play	O
out	O
in	O
cybersecurity	O
,	O
too	O
.	O

Researchers	O
are	O
already	O
highlighting	O
the	O
risk	O
of	O
black	O
box	O
attacks	O
,	O
in	O
which	O
GANs	B
are	O
used	O
to	O
figure	O
out	O
the	O
machine-learning	B
models	I
with	O
which	O
plenty	O
of	O
security	O
programs	O
spot	O
malware	O
.	O

Having	O
divined	O
how	O
a	O
defenders	O
algorithm	B
works	O
,	O
an	O
attacker	O
can	O
evade	O
it	O
and	O
insert	O
rogue	O
code	B
.	O

The	O
same	O
approach	O
could	O
also	O
be	O
used	O
to	O
dodge	O
spam	B
filters	I
and	O
other	O
defenses	O
.	O

There	O
are	O
a	O
lot	O
of	O
areas	O
of	O
science	O
and	O
engineering	O
where	O
we	O
need	O
to	O
optimize	O
something	O
.	O

Thats	O
going	O
to	O
be	O
the	O
next	O
big	O
wave	O
.	O

Goodfellow	O
is	O
well	O
aware	O
of	O
the	O
dangers	O
.	O

Now	O
heading	O
a	O
team	O
at	O
Google	O
thats	O
focused	O
on	O
making	O
machine	B
learning	I
more	O
secure	O
,	O
he	O
warns	O
that	O
the	O
AI	O
community	O
must	O
learn	O
the	O
lesson	O
of	O
previous	O
waves	O
of	O
innovation	O
,	O
in	O
which	O
technologists	O
treated	O
security	O
and	O
privacy	O
as	O
an	O
afterthought	O
.	O

By	O
the	O
time	O
they	O
woke	O
up	O
to	O
the	O
risks	O
,	O
the	O
bad	O
guys	O
had	O
a	O
significant	O
lead	O
.	O

Clearly	O
,	O
were	O
already	O
beyond	O
the	O
start	O
,	O
he	O
says	O
,	O
but	O
hopefully	O
we	O
can	O
make	O
significant	O
advances	O
in	O
security	O
before	O
were	O
too	O
far	O
in	O
.	O

Nonetheless	O
,	O
he	O
doesnt	O
think	O
there	O
will	O
be	O
a	O
purely	O
technological	O
solution	O
to	O
fakery	O
.	O

Instead	O
,	O
he	O
believes	O
,	O
well	O
have	O
to	O
rely	O
on	O
societal	O
ones	O
,	O
such	O
as	O
teaching	O
kids	O
critical	O
thinking	O
by	O
getting	O
them	O
to	O
take	O
things	O
like	O
speech	O
and	O
debating	O
classes	O
.	O

In	O
speech	O
and	O
debate	O
youre	O
competing	O
against	O
another	O
student	O
,	O
he	O
says	O
,	O
and	O
youre	O
thinking	O
about	O
how	O
to	O
craft	O
misleading	O
claims	O
,	O
or	O
how	O
to	O
craft	O
correct	O
claims	O
that	O
are	O
very	O
persuasive	O
.	O

He	O
may	O
well	O
be	O
right	O
,	O
but	O
his	O
conclusion	O
that	O
technology	O
cant	O
cure	O
the	O
fake-news	O
problem	O
is	O
not	O
one	O
many	O
will	O
want	O
to	O
hear	O
.	O

Todays	O
large	B
language	I
models	I
have	O
greatly	O
improved	O
their	O
task-agnostic	O
,	O
few-shot	O
performance	O
,	O
with	O
top	O
models	B
like	O
GPT	B
-	I
3	I
competitive	O
with	O
state-of-the-art	O
finetuning	O
approaches	O
when	O
provided	O
only	O
a	O
few	O
examples	O
in	O
a	O
natural	B
language	I
prompt	I
.	O

This	O
few-shot	O
,	O
in-context	O
learning	O
approach	O
is	O
gaining	O
traction	O
in	O
large	O
part	O
due	O
to	O
its	O
ability	O
to	O
learn	O
without	O
parameter	O
updates	O
.	O

Compared	O
to	O
traditional	O
finetuning	O
methods	O
,	O
few-shot	B
learning	I
enables	O
practitioners	O
to	O
more	O
quickly	O
prototype	O
NLP	B
models	I
,	O
allows	O
non-technical	O
users	O
to	O
create	O
NLP	B
systems	I
,	O
and	O
efficiently	O
reuses	O
models	B
to	O
reduce	O
system	O
memory	O
and	O
complexity	O
.	O

GPT	B
-	I
3s	I
accuracy	O
however	O
can	O
be	O
highly	O
unstable	O
across	O
different	O
prompts	O
(	O
training	O
examples	O
,	O
permutation	O
,	O
format	O
)	O
.	O

To	O
address	O
this	O
,	O
a	O
new	O
UC	O
Berkeley	O
,	O
University	O
of	O
Maryland	O
and	O
UC	O
Irvine	O
study	O
sets	O
out	O
to	O
identify	O
the	O
pitfalls	O
that	O
can	O
cause	O
instability	O
in	O
the	O
GPT	B
-	I
3	I
language	I
model	I
and	O
proposes	O
a	O
contextual	B
calibration	I
procedure	I
that	O
consistently	O
improves	O
GPT	B
-	I
3	I
(	O
and	O
GPT	B
-	I
2	I
)	O
accuracy	O
across	O
different	O
prompt	O
format	O
choices	O
and	O
examples	O
.	O

Typically	O
,	O
a	O
natural	B
language	I
prompt	I
is	O
fed	O
to	O
neural	B
autoregressive	I
language	I
models	I
to	O
ensure	O
they	O
perform	O
few-shot	B
learning	I
using	O
in-context	B
learning	I
.	O

The	O
prompt	O
consists	O
of	O
three	O
components	O
:	O
a	O
format	O
,	O
a	O
set	O
of	O
training	O
examples	O
,	O
and	O
a	O
permutation	O
of	O
the	O
training	O
examples	O
.	O

The	O
researchers	O
first	O
studied	O
how	O
GPT	B
-	I
3s	I
accuracy	O
changes	O
across	O
different	O
prompts	O
.	O

They	O
conducted	O
sentiment	B
analysis	I
task	O
experiments	O
on	O
three	O
GPT	B
-	I
3	I
model	O
sizes	O
(	O
2.7B	O
,	O
13B	O
,	O
and	O
175B	O
parameters	O
)	O
trained	O
on	O
SST	O
-	O
2	O
datasets	O
,	O
and	O
observed	O
high	O
variance	O
in	O
GPT	B
-	I
3s	I
accuracy	O
across	O
the	O
prompts	O
training	O
examples	O
,	O
permutation	O
of	O
examples	O
,	O
as	O
well	O
as	O
format	O
.	O

Surprising	O
,	O
varying	O
the	O
permutation	O
of	O
the	O
training	O
examples	O
could	O
cause	O
accuracy	O
to	O
range	O
from	O
54.3	O
percent	O
to	O
near	O
state-of-the-art	O
(	O
93.4	O
percent	O
)	O
.	O

Accuracy	O
across	O
training	O
sets	O
,	O
permutations	O
and	O
formats	O
The	O
researchers	O
next	O
analyzed	O
factors	O
that	O
contribute	O
to	O
GPT	B
-	I
3	I
instability	O
,	O
identifying	O
three	O
biases	O
behind	O
the	O
accuracy	O
variance	O
:	O
Majority	O
Label	O
Bias	O
GPT	B
-	I
3	I
is	O
biased	O
towards	O
answers	O
that	O
are	O
frequent	O
in	O
the	O
prompt	O
.	O

The	O
majority	O
label	O
bias	O
helps	O
explain	O
why	O
different	O
choices	O
for	O
the	O
training	O
examples	O
heavily	O
influence	O
GPT	B
-	I
3s	I
accuracy	O
as	O
this	O
shifts	O
the	O
distribution	O
of	O
model	O
predictions	O
.	O

Recency	O
Bias	O
The	O
models	B
majority	O
label	O
bias	O
is	O
aggravated	O
by	O
its	O
recency	O
bias	O
:	O
the	O
tendency	O
to	O
repeat	O
answers	O
that	O
appear	O
towards	O
the	O
end	O
of	O
the	O
prompt	O
.	O

Overall	O
,	O
recency	O
bias	O
helps	O
to	O
explain	O
why	O
the	O
permutation	O
of	O
the	O
training	O
examples	O
is	O
important	O
.	O

Common	O
Token	O
Bias	O
GPT	O
-	O
3	O
is	O
biased	O
towards	O
outputting	O
tokens	O
that	O
are	O
common	O
in	O
its	O
pretraining	O
distribution	O
.	O

The	O
common	O
token	O
bias	O
helps	O
explain	O
why	O
the	O
choice	O
of	O
label	O
names	O
is	O
important	O
,	O
and	O
why	O
the	O
model	B
struggles	O
with	O
rare	O
answers	O
.	O

The	O
team	O
says	O
these	O
three	O
biases	O
together	O
tend	O
to	O
contribute	O
to	O
a	O
simple	O
shift	O
in	O
a	O
models	O
output	O
distribution	O
.	O

Inspired	O
by	O
the	O
idea	O
that	O
model	B
biases	O
towards	O
certain	O
answers	O
can	O
be	O
estimated	O
by	O
feeding	O
content-free	O
inputs	O
,	O
the	O
researchers	O
proposed	O
a	O
novel	O
data-free	O
contextual	B
calibration	I
procedure	I
to	O
infer	O
parameters	O
.	O

To	O
evaluate	O
the	O
contextual	B
calibrations	I
effectiveness	O
,	O
they	O
conducted	O
experiments	O
on	O
text	B
classification	I
,	O
fact	B
retrieval	I
and	O
information	B
extraction	I
tasks	O
across	O
different	O
datasets	O
(	O
AGNews	O
,	O
MIT	O
Director	O
,	O
DBPedia	O
,	O
TREC	O
etc	O
.	O
)	O
.	O

Mean	O
accuracy	O
comparison	O
across	O
different	O
training	O
example	O
choices	O
for	O
different	O
datasets	O
and	O
model	B
sizes	O
.	O

The	O
steep	O
red	O
lines	O
on	O
all	O
three	O
model	O
sizes	O
indicate	O
that	O
few-shot	B
learning	I
can	O
be	O
highly	O
unstable	O
across	O
different	O
numbers	O
of	O
training	O
examples	O
.	O

The	O
more	O
stable	O
blue	O
lines	O
show	O
that	O
the	O
calibration	O
method	O
improves	O
the	O
accuracy	O
and	O
robustness	O
of	O
GPT	B
-	I
3	I
models	I
.	O

Contextual	B
calibration	I
improves	O
accuracy	O
across	O
a	O
range	O
of	O
tasks	O
The	O
proposed	O
contextual	B
calibration	I
method	I
improves	O
the	O
accuracy	O
and	O
reduces	O
the	O
variance	O
of	O
GPT	O
-	O
3	O
models	O
,	O
boosting	O
average	O
and	O
worst-case	O
absolute	O
accuracy	O
by	O
up	O
to	O
30	O
percent	O
.	O

The	O
study	O
highlights	O
the	O
need	O
for	O
better	O
understanding	O
and	O
analysis	O
of	O
the	O
dynamics	O
of	O
in-context	B
learning	I
.	O

Multiplexing	O
Could	O
Give	O
Neural	B
Networks	I
a	O
Big	O
Boost	O
Combining	O
multiple	O
data	O
streams	O
into	O
one	O
feed	O
could	O
speed	O
up	O
networks	B
and	O
let	O
them	O
tackle	O
more	O
than	O
one	O
task	O
at	O
a	O
time	O
CHARLES	O
Q	O
.	O

CHOI	O
Just	O
as	O
multiplexing	O
can	O
help	O
a	O
single	O
communication	B
channel	I
carry	O
many	O
signals	O
at	O
the	O
same	O
time	O
,	O
a	O
new	O
study	O
reveals	O
that	O
multiplexing	O
can	O
help	O
neural	O
networksthe	O
AI	B
systems	I
that	O
now	O
often	O
power	O
speech	B
recognition	I
,	O
computer	B
vision	I
,	O
and	O
morescan	O
dozens	O
of	O
streams	O
of	O
data	O
simultaneously	O
,	O
letting	O
them	O
greatly	O
boost	O
the	O
rate	O
at	O
which	O
they	O
analyze	O
information	O
.	O

In	O
artificial	B
neural	I
networks	I
,	O
components	O
dubbed	O
neurons	O
are	O
fed	O
data	O
and	O
cooperate	O
to	O
solve	O
a	O
problem	O
,	O
such	O
as	O
recognizing	O
images	O
.	O

The	O
neural	B
net	I
repeatedly	O
adjusts	O
the	O
links	O
between	O
its	O
neurons	O
and	O
sees	O
if	O
the	O
resulting	O
patterns	O
of	O
behavior	O
are	O
better	O
at	O
finding	O
a	O
solution	O
.	O

Over	O
time	O
,	O
the	O
network	O
discovers	O
which	O
patterns	O
are	O
best	O
at	O
computing	O
results	O
.	O

It	O
then	O
adopts	O
these	O
as	O
defaults	O
,	O
mimicking	O
the	O
process	O
of	O
learning	O
in	O
the	O
human	O
brain	O
.	O

The	O
features	O
of	O
a	O
neural	B
net	I
that	O
change	O
with	O
learning	O
,	O
such	O
as	O
the	O
nature	O
of	O
the	O
connections	O
between	O
neurons	O
,	O
are	O
known	O
as	O
its	O
parameters	O
.	O

Recent	O
research	O
suggests	O
that	O
modern	O
neural	B
networks	I
often	O
have	O
vastly	O
more	O
parameters	O
than	O
they	O
needpotentially	O
,	O
they	O
could	O
prune	O
the	O
numbers	O
of	O
their	O
parameters	O
by	O
more	O
than	O
90	O
percent	O
to	O
reduce	O
their	O
sizes	O
without	O
harming	O
their	O
accuracy	O
.	O

This	O
raised	O
a	O
question	O
that	O
researchers	O
at	O
Princeton	O
University	O
aimed	O
to	O
addressif	O
neural	B
networks	I
possessed	O
more	O
computing	O
power	O
than	O
they	O
needed	O
,	O
could	O
they	O
each	O
analyze	O
multiple	O
streams	O
of	O
information	O
simultaneously	O
to	O
help	O
learn	O
a	O
task	O
,	O
just	O
as	O
a	O
radio	B
channel	O
can	O
share	O
its	O
bandwidth	O
to	O
carry	O
multiple	O
signals	O
at	O
the	O
same	O
time	O
?	O

The	O
researchers	O
developed	O
a	O
technique	O
they	O
named	O
DataMUX	B
wherein	O
a	O
neural	B
network	I
can	O
analyze	O
multiple	O
data	O
feeds	O
simultaneously	O
as	O
one	O
mixed	O
clump	O
of	O
information	O
.	O

This	O
can	O
significantly	O
boost	O
its	O
efficiency	O
,	O
letting	O
it	O
analyze	O
substantially	O
more	O
quickly	O
while	O
demanding	O
little	O
in	O
the	O
way	O
of	O
extra	O
time	O
,	O
computation	O
,	O
or	O
memory	O
requirements	O
.	O

They	O
suggest	O
their	O
new	O
method	O
,	O
which	O
they	O
detailed	O
online	O
18	O
February	O
on	O
the	O
ArXiv	O
preprint	O
server	B
,	O
may	O
be	O
the	O
first	O
instance	O
of	O
data	B
multiplexing	I
in	O
neural	B
networks	I
.	O

We	O
hope	O
this	O
can	O
have	O
a	O
substantial	O
impact	O
on	O
energy	O
consumption	O
and	O
the	O
environmental	O
footprint	O
of	O
machine-learning	B
models	I
,	O
especially	O
for	O
computing	B
services	I
that	O
process	O
a	O
large	O
number	O
of	O
requests	O
at	O
a	O
time	O
,	O
says	O
study	O
coauthor	O
Vishvak	O
Murahari	O
,	O
a	O
machine-learning	O
researcher	O
at	O
Princeton	O
.	O

DataMUX	B
works	O
by	O
adding	O
a	O
multiplexing	B
layer	I
and	O
a	O
demultiplexing	B
layer	I
to	O
both	O
ends	O
of	O
a	O
neural	B
network	I
.	O

The	O
signals	O
entering	O
the	O
neural	B
network	I
are	O
each	O
given	O
a	O
specific	O
unique	O
key	O
to	O
help	O
distinguish	O
them	O
all	O
,	O
and	O
the	O
multiplexing	B
layer	I
then	O
merges	O
these	O
multiple	O
inputs	O
together	O
into	O
a	O
single	O
compressed	O
feed	O
.	O

After	O
the	O
neural	B
network	I
processes	O
this	O
input	O
,	O
the	O
demultiplexing	O
layer	O
converts	O
the	O
combined	O
output	O
back	O
into	O
multiple	O
separate	O
results	O
.	O

The	O
scientists	O
conducted	O
experiments	O
with	O
DataMUX	B
using	O
three	O
different	O
kinds	O
of	O
neural	O
networkstransformers	O
,	O
multilayer	B
perceptrons	I
,	O
and	O
convolutional	B
neural	I
networks	I
.	O

The	O
experiments	O
involved	O
several	O
tasksimage	B
recognition	I
;	O
sentence	B
classification	I
,	O
in	O
which	O
a	O
machine	O
aims	O
to	O
identify	O
whether	O
text	O
is	O
spam	O
,	O
a	O
business	O
article	O
,	O
and	O
so	O
on	O
;	O
named	B
entity	I
recognition	I
,	O
which	O
involves	O
locating	O
and	O
classifying	O
named	O
entities	O
such	O
as	O
people	O
,	O
groups	O
,	O
and	O
places	O
.	O

Experiments	O
with	O
transformers	B
on	O
text-classification	B
tasks	O
revealed	O
they	O
could	O
multiplex	O
up	O
to	O
40	O
inputs	O
,	O
achieving	O
up	O
to	O
an	O
18	O
-	O
fold	O
speedup	O
in	O
the	O
rate	O
at	O
which	O
they	O
could	O
process	O
these	O
inputs	O
with	O
as	O
little	O
as	O
a	O
2	O
percent	O
drop	O
in	O
accuracy	O
.	O

We	O
could	O
aim	O
to	O
increase	O
throughput	O
of	O
machine-learning	B
models	I
manyfold	O
,	O
Murahari	O
says	O
.	O

This	O
is	O
profound	O
since	O
it	O
opens	O
up	O
applications	O
where	O
one	O
might	O
need	O
to	O
invoke	O
the	O
model	O
dynamically	O
with	O
high	O
frequency	O
.	O

For	O
instance	O
,	O
imagine	O
an	O
assistive	B
writing	I
application	I
where	O
one	O
could	O
invoke	O
state-of-the-art	O
language	B
models	I
very	O
frequently	O
,	O
leading	O
to	O
a	O
more	O
intuitive	O
and	O
a	O
smooth	O
running	O
application	O
.	O

In	O
addition	O
,	O
products	O
using	O
large	B
machine-learning	I
models	I
could	O
decrease	O
their	O
compute	O
costs	O
dramatically	O
with	O
DataMUX	B
,	O
Murahari	O
says	O
.	O

We	O
could	O
also	O
imagine	O
models	B
running	O
on	O
less	O
specialized	O
hardware	B
,	O
such	O
as	O
CPUs	B
instead	O
of	O
GPUs	B
,	O
since	O
models	O
running	O
DataMUX	B
on	O
CPUs	B
could	O
close	O
the	O
throughput	O
gap	O
with	O
nonmultiplexed	O
models	B
running	O
on	O
GPU	B
.	O

This	O
would	O
enable	O
a	O
large	O
number	O
of	O
machine-learning	B
applications	I
to	O
run	O
on	O
low-resource	B
edge	I
devices	I
.	O

This	O
40	O
-	O
fold	O
increase	O
in	O
the	O
inputs	O
received	O
led	O
to	O
only	O
an	O
18	O
-	O
fold	O
boost	O
in	O
throughput	O
,	O
instead	O
of	O
the	O
expected	O
40	O
-	O
fold	O
enhancement	O
,	O
likely	O
because	O
of	O
the	O
way	O
in	O
which	O
the	O
keys	O
associated	O
with	O
each	O
input	O
grew	O
in	O
length	O
the	O
more	O
inputs	O
there	O
were	O
.	O

Future	O
work	O
may	O
potentially	O
improve	O
this	O
speedup	O
through	O
better	O
multiplexing	B
and	O
demultiplexing	B
strategies	O
,	O
the	O
researchers	O
say	O
.	O

As	O
to	O
how	O
neural	B
networks	I
using	O
DataMUX	B
do	O
not	O
get	O
confused	O
by	O
this	O
mixed	O
feed	O
,	O
we	O
don't	O
really	O
know	O
,	O
says	O
study	O
coauthor	O
Carlos	O
Jimenez	O
,	O
a	O
machine	O
learning	O
researcher	O
at	O
Princeton	O
.	O

He	O
notes	O
there	O
is	O
nothing	O
that	O
necessarily	O
causes	O
multiple	O
inputs	O
streaming	O
into	O
a	O
neural	B
network	I
to	O
interfere	O
with	O
each	O
other	O
,	O
but	O
more	O
work	O
needs	O
to	O
be	O
done	O
to	O
really	O
get	O
to	O
the	O
bottom	O
of	O
this	O
.	O

A	O
neural	B
network	I
using	O
DataMUX	B
is	O
not	O
limited	O
to	O
performing	O
just	O
one	O
task	O
on	O
this	O
multiplexed	O
data	O
,	O
such	O
as	O
just	O
recognizing	O
names	O
.	O

It	O
could	O
use	O
this	O
combined	O
input	O
to	O
carry	O
out	O
multiple	O
tasks	O
that	O
it	O
is	O
trained	O
on	O
at	O
the	O
same	O
time	O
,	O
such	O
as	O
both	O
recognizing	B
names	I
and	O
classifying	B
sentences	I
,	O
notes	O
study	O
senior	O
author	O
Karthik	O
Narasimhan	O
,	O
a	O
machine-learning	B
researcher	O
at	O
Princeton	O
.	O

In	O
the	O
future	O
,	O
the	O
researchers	O
aim	O
to	O
experiment	O
with	O
multiplexing	B
state-of-the-art	O
neural	B
networks	I
such	O
as	O
BERT	B
and	O
GPT	B
-	I
3	I
.	O

They	O
would	O
also	O
like	O
to	O
investigate	O
other	O
multiplexing	B
schemes	O
with	O
which	O
they	O
could	O
scale	O
up	O
to	O
hundreds	O
or	O
even	O
thousands	O
of	O
inputs	O
at	O
once	O
,	O
leading	O
to	O
even	O
larger	O
improvements	O
in	O
throughput	O
,	O
Murahari	O
says	O
.	O

We	O
could	O
really	O
just	O
be	O
at	O
the	O
tip	O
of	O
the	O
iceberg	O
.	O

