KHARI	O
JOHNSONBUSINESSMAY	O
5	O
,	O
2022	O
8	O
:	O
00	O
AM	O
DALL-E	B
2	I
Creates	O
Incredible	O
Imagesand	O
Biased	O
Ones	O
You	O
Dont	O
See	O
OpenAIs	O
new	O
system	O
is	O
adept	O
at	O
turning	O
text	O
into	O
images	O
.	O

But	O
researchers	O
say	O
it	O
also	O
reinforces	O
stereotypes	O
against	O
women	O
and	O
people	O
of	O
color	O
.	O

MARCELO	O
RINESI	O
REMEMBERS	O
what	O
it	O
was	O
like	O
to	O
watch	O
Jurassic	O
Park	O
for	O
the	O
first	O
time	O
in	O
a	O
theater	O
.	O

The	O
dinosaurs	O
looked	O
so	O
convincing	O
that	O
they	O
felt	O
like	O
the	O
real	O
thing	O
,	O
a	O
special	B
effects	I
breakthrough	O
that	O
permanently	O
shifted	O
peoples	O
perception	O
of	O
whats	O
possible	O
.	O

After	O
two	O
weeks	O
of	O
testing	O
DALL-E	B
2	I
,	O
the	O
CTO	O
of	O
the	O
Institute	O
for	O
Ethics	O
and	O
Emerging	O
Technologies	O
thinks	O
AI	B
might	O
be	O
on	O
the	O
verge	O
of	O
its	O
own	O
Jurassic	O
Park	O
moment	O
.	O

Last	O
month	O
,	O
OpenAI	O
introduced	O
the	O
second-generation	O
version	O
of	O
DALL-E	B
,	O
an	O
AI	B
model	I
trained	O
on	O
650	O
million	O
images	O
and	O
text	O
captions	O
.	O

It	O
can	O
take	O
in	O
text	O
and	O
spit	O
out	O
images	O
,	O
whether	O
thats	O
a	O
Dystopian	O
Great	O
Wave	O
off	O
Kanagawa	O
as	O
Godzilla	O
eating	O
Tokyo	O
or	O
Teddy	O
bears	O
working	O
on	O
new	O
AI	B
research	O
on	O
the	O
moon	O
in	O
the	O
1980s	O
.	O

It	O
can	O
create	O
variations	O
based	O
on	O
the	O
style	O
of	O
a	O
particular	O
artist	O
,	O
like	O
Salvador	O
Dali	O
,	O
or	O
popular	O
software	B
like	O
Unreal	B
Engine	I
.	O

Photorealistic	O
depictions	O
that	O
look	O
like	O
the	O
real	O
world	O
,	O
shared	O
widely	O
on	O
social	B
media	I
by	O
a	O
select	O
number	O
of	O
early	O
testers	O
,	O
have	O
given	O
the	O
impression	O
that	O
the	O
model	B
can	O
create	O
images	O
of	O
almost	O
anything	O
.	O

What	O
people	O
thought	O
might	O
take	O
five	O
to	O
10	O
years	O
,	O
were	O
already	O
in	O
it	O
.	O

We	O
are	O
in	O
the	O
future	O
,	O
says	O
Vipul	O
Gupta	O
,	O
a	O
PhD	O
candidate	O
at	O
Penn	O
State	O
who	O
has	O
used	O
DALL-E	B
2	I
.	O

But	O
amid	O
promotional	O
depictions	O
of	O
koalas	O
and	O
pandas	O
spreading	O
on	O
social	O
media	O
is	O
a	O
notable	O
absence	O
:	O
peoples	O
faces	O
.	O

As	O
part	O
of	O
OpenAIs	O
red	O
team	O
processin	O
which	O
external	O
experts	O
look	O
for	O
ways	O
things	O
can	O
go	O
wrong	O
before	O
the	O
products	O
broader	O
distributionAI	O
researchers	O
found	O
that	O
DALL-E	B
2s	I
depictions	O
of	O
people	O
can	O
be	O
too	O
biased	O
for	O
public	O
consumption	O
.	O

Early	O
tests	O
by	O
red	O
team	O
members	O
and	O
OpenAI	O
have	O
shown	O
that	O
DALL-E	B
2	I
leans	O
toward	O
generating	O
images	O
of	O
white	O
men	O
by	O
default	O
,	O
overly	O
sexualizes	O
images	O
of	O
women	O
,	O
and	O
reinforces	O
racial	O
stereotypes	O
.	O

From	O
conversations	O
with	O
roughly	O
half	O
of	O
the	O
23	O
-	O
member	O
red	O
team	O
,	O
we	O
found	O
that	O
a	O
number	O
of	O
them	O
recommended	O
OpenAI	O
release	O
DALL-E	B
2	I
without	O
the	O
ability	O
to	O
generate	O
faces	O
at	O
all	O
.	O

One	O
red	O
team	O
member	O
told	O
WIRED	O
that	O
eight	O
out	O
of	O
eight	O
attempts	O
to	O
generate	O
images	O
with	O
words	O
like	O
a	O
man	O
sitting	O
in	O
a	O
prison	O
cell	O
or	O
a	O
photo	O
of	O
an	O
angry	O
man	O
returned	O
images	O
of	O
men	O
of	O
color	O
.	O

There	O
were	O
a	O
lot	O
of	O
non-white	O
people	O
whenever	O
there	O
was	O
a	O
negative	O
adjective	O
associated	O
with	O
the	O
person	O
,	O
says	O
Maarten	O
Sap	O
,	O
an	O
external	O
red	O
team	O
member	O
who	O
researches	O
stereotypes	O
and	O
reasoning	O
in	O
AI	B
models	I
.	O

Enough	O
risks	O
were	O
found	O
that	O
maybe	O
it	O
shouldnt	O
generate	O
people	O
or	O
anything	O
photorealistic	O
.	O

Another	O
red	O
team	O
member	O
,	O
who	O
asked	O
WIRED	O
not	O
to	O
use	O
their	O
name	O
due	O
to	O
concerns	O
about	O
possible	O
retribution	O
,	O
said	O
that	O
while	O
they	O
found	O
the	O
OpenAI	O
ethics	O
team	O
to	O
be	O
responsive	O
to	O
concerns	O
,	O
they	O
were	O
against	O
releasing	O
DALL-E	B
2	I
with	O
the	O
ability	O
to	O
generate	O
faces	O
.	O

They	O
question	O
the	O
rush	O
to	O
release	O
technology	B
that	O
can	O
automate	O
discrimination	O
.	O

I	O
wonder	O
why	O
theyre	O
releasing	O
this	O
model	B
now	O
,	O
besides	O
to	O
show	O
off	O
their	O
impressive	O
technology	B
to	O
people	O
,	O
the	O
person	O
said	O
.	O

It	O
just	O
seems	O
like	O
there's	O
so	O
much	O
room	O
for	O
harm	O
right	O
now	O
,	O
and	O
Im	O
not	O
seeing	O
enough	O
room	O
for	O
good	O
to	O
justify	O
it	O
being	O
in	O
the	O
world	O
yet	O
.	O

DALL-Es	B
creators	O
call	O
the	O
model	B
experimental	O
and	O
not	O
yet	O
fit	O
for	O
commercial	O
use	O
but	O
say	O
it	O
could	O
influence	O
industries	O
like	O
art	O
,	O
education	O
,	O
and	O
marketing	O
and	O
help	O
advance	O
OpenAIs	O
stated	O
goal	O
of	O
creating	O
artificial	B
general	I
intelligence	I
.	O

But	O
by	O
OpenAIs	O
own	O
admission	O
,	O
DALL-E	B
2	I
is	O
more	O
racist	O
and	O
sexist	O
than	O
a	O
similar	O
,	O
smaller	O
model	B
.	O

The	O
companys	O
own	O
risks	O
and	O
limitations	O
document	O
gives	O
examples	O
of	O
words	O
like	O
assistant	O
and	O
flight	O
attendant	O
generating	O
images	O
of	O
women	O
and	O
words	O
like	O
CEO	O
and	O
builder	O
almost	O
exclusively	O
generating	O
images	O
of	O
white	O
men	O
.	O

Left	O
out	O
of	O
that	O
analysis	O
are	O
images	O
of	O
people	O
created	O
by	O
words	O
like	O
racist	O
,	O
savage	O
,	O
or	O
terrorist	O
.	O

Those	O
text	O
prompts	O
and	O
dozens	O
of	O
others	O
were	O
recommended	O
to	O
OpenAI	O
by	O
the	O
creators	O
of	O
DALL-Eval	O
,	O
a	O
team	O
of	O
researchers	O
from	O
the	O
MURGe	O
Lab	O
at	O
the	O
University	O
of	O
North	O
Carolina	O
.	O

They	O
claim	O
to	O
have	O
made	O
the	O
first	O
method	O
for	O
evaluating	O
multimodal	B
AI	I
models	I
for	O
reasoning	O
and	O
societal	O
bias	O
.	O

The	O
DALL-Eval	O
team	O
found	O
that	O
bigger	O
multimodal	B
models	I
generally	O
have	O
more	O
impressive	O
performancebut	O
also	O
more	O
biased	O
outputs	O
.	O

OpenAI	O
VP	O
of	O
communications	O
Steve	O
Dowling	O
declined	O
to	O
share	O
images	O
generated	O
from	O
text	O
prompts	O
recommended	O
by	O
DALL-Eval	O
creators	O
when	O
WIRED	O
requested	O
them	O
.	O

Dowling	O
said	O
early	O
testers	O
werent	O
told	O
to	O
avoid	O
posting	O
negative	O
or	O
racist	O
content	O
generated	O
by	O
the	O
system	O
.	O

But	O
as	O
OpenAI	O
CEO	O
Sam	O
Altman	O
said	O
in	O
a	O
late	O
April	O
interview	O
,	O
text	O
prompts	O
involving	O
people	O
,	O
and	O
in	O
particular	O
photorealistic	O
faces	O
,	O
generate	O
the	O
most	O
problematic	O
content	O
.	O

The	O
400	O
people	O
with	O
early	O
access	O
to	O
DALL-E	O
2predominantly	O
OpenAI	O
employees	O
,	O
board	O
members	O
,	O
and	O
Microsoft	O
employeeswere	O
told	O
not	O
to	O
share	O
photorealistic	O
images	O
in	O
public	O
,	O
in	O
large	O
part	O
due	O
to	O
these	O
issues	O
.	O

The	O
purpose	O
of	O
this	O
is	O
to	O
learn	O
how	O
to	O
eventually	O
do	O
faces	O
safely	O
if	O
we	O
can	O
,	O
which	O
is	O
a	O
goal	O
wed	O
like	O
to	O
get	O
to	O
,	O
says	O
Altman	O
.	O

Computer	B
vision	I
has	O
a	O
history	O
of	O
deploying	O
AI	B
first	O
,	O
then	O
apologizing	O
years	O
later	O
when	O
audits	O
reveal	O
a	O
history	O
of	O
harm	O
.	O

The	O
ImageNet	O
competition	O
and	O
resulting	O
data	B
set	I
laid	O
the	O
foundation	O
for	O
the	O
field	O
in	O
2009	O
and	O
led	O
to	O
the	O
launch	O
of	O
a	O
number	O
of	O
companies	O
,	O
but	O
sources	O
of	O
bias	O
in	O
its	O
training	O
data	O
led	O
its	O
creators	O
to	O
cut	O
labels	O
related	O
to	O
people	O
in	O
2019	O
.	O

A	O
year	O
later	O
,	O
the	O
creators	O
of	O
a	O
data	B
set	I
called	O
80	B
Million	I
Tiny	I
Images	I
took	O
it	O
offline	O
after	O
a	O
decade	O
of	O
circulation	O
,	O
citing	O
racial	O
slurs	O
and	O
other	O
harmful	O
labels	O
within	O
the	O
training	O
data	O
.	O

Last	O
year	O
,	O
MIT	O
researchers	O
concluded	O
that	O
the	O
measurement	O
and	O
mitigation	O
of	O
bias	O
in	O
vision	B
data	I
sets	I
is	O
critical	O
to	O
building	O
a	O
fair	O
society	O
.	O

DALL-E	B
2	I
was	O
trained	O
using	O
a	O
combination	O
of	O
photos	O
scraped	O
from	O
the	O
internet	B
and	O
acquired	O
from	O
licensed	O
sources	O
,	O
according	O
to	O
the	O
document	O
authored	O
by	O
OpenAI	O
ethics	O
and	O
policy	O
researchers	O
.	O

OpenAI	O
did	O
make	O
efforts	O
to	O
mitigate	O
toxicity	O
or	O
the	O
spread	O
of	O
disinformation	O
,	O
applying	O
text	B
filters	I
to	O
the	O
image	B
generator	I
and	O
removing	O
some	O
images	O
that	O
were	O
sexually	O
explicit	O
or	O
gory	O
.	O

Only	O
noncommercial	O
use	O
is	O
allowed	O
today	O
,	O
and	O
early	O
users	O
are	O
required	O
to	O
label	O
images	O
with	O
a	O
signature	O
bar	O
of	O
color	O
in	O
the	O
bottom-right	O
corner	O
generated	O
by	O
DALL-E	O
2	O
.	O

But	O
the	O
red	O
team	O
was	O
not	O
given	O
access	O
to	O
the	O
DALL-E	B
2	I
training	O
data	B
set	I
.	O

OpenAI	O
knows	O
better	O
than	O
anyone	O
the	O
harm	O
that	O
can	O
come	O
from	O
deploying	O
AI	O
built	O
with	O
massive	O
,	O
poorly	O
curated	O
data	B
sets	I
.	O

Documentation	O
by	O
OpenAI	O
found	O
that	O
its	O
multimodal	B
model	I
CLIP	B
,	O
which	O
plays	O
a	O
role	O
in	O
the	O
DALL-E	B
2	I
training	O
process	O
,	O
exhibits	O
racist	O
and	O
sexist	O
behavior	O
.	O

Using	O
a	O
data	B
set	I
of	O
10,000	O
images	O
of	O
faces	O
divided	O
into	O
seven	O
racial	O
categories	O
,	O
OpenAI	O
found	O
that	O
CLIP	B
is	O
more	O
likely	O
to	O
misclassify	O
Black	O
people	O
as	O
less	O
than	O
human	O
than	O
any	O
other	O
racial	O
group	O
,	O
and	O
in	O
some	O
cases	O
more	O
likely	O
to	O
label	O
the	O
faces	O
of	O
men	O
as	O
executive	O
or	O
doctor	O
than	O
those	O
of	O
women	O
.	O

Upon	O
release	O
of	O
GPT	B
-	I
2	I
in	O
February	O
2019	O
,	O
OpenAI	O
adopted	O
a	O
staggered	O
approach	O
to	O
the	O
release	O
of	O
the	O
largest	O
form	O
of	O
the	O
model	B
on	O
the	O
claim	O
that	O
text	O
it	O
generated	O
was	O
too	O
realistic	O
and	O
dangerous	O
to	O
release	O
.	O

That	O
approach	O
sparked	O
debate	O
about	O
how	O
to	O
responsibly	O
release	O
large	B
language	I
models	I
,	O
as	O
well	O
as	O
criticism	O
that	O
the	O
elaborate	O
release	O
method	O
was	O
designed	O
to	O
drum	O
up	O
publicity	O
.	O

Despite	O
GPT	B
-	I
3	I
being	O
more	O
than	O
100	O
times	O
larger	O
than	O
GPT	O
-	O
2with	O
a	O
well-documented	O
bias	O
toward	O
Black	O
people	O
,	O
Muslims	O
,	O
and	O
other	O
groups	O
of	O
peopleefforts	O
to	O
commercialize	O
GPT	B
-	I
3	I
with	O
exclusive	O
partner	O
Microsoft	O
went	O
forward	O
in	O
2020	O
with	O
no	O
specific	O
data-driven	O
or	O
quantitative	O
method	O
to	O
determine	O
whether	O
the	O
model	B
was	O
fit	O
for	O
release	O
.	O

Altman	O
suggested	O
that	O
DALL-E	B
2	I
may	O
follow	O
the	O
same	O
approach	O
to	O
GPT	B
-	I
3	I
.	O

There	O
arent	O
obvious	O
metrics	O
that	O
we've	O
all	O
agreed	O
on	O
that	O
we	O
can	O
point	O
to	O
that	O
society	O
can	O
say	O
this	O
is	O
the	O
right	O
way	O
to	O
handle	O
this	O
,	O
he	O
says	O
,	O
but	O
OpenAI	O
does	O
want	O
to	O
follow	O
metrics	O
like	O
the	O
number	O
of	O
DALL-E	B
2	I
images	O
that	O
depict	O
,	O
say	O
,	O
a	O
person	O
of	O
color	O
in	O
a	O
jail	O
cell	O
.	O

