Todays _ O
large _ B-TECH
language _ I-TECH
models _ I-TECH
have _ O
greatly _ O
improved _ O
their _ O
task-agnostic _ O
, _ O
few-shot _ O
performance _ O
, _ O
with _ O
top _ O
models _ B-TECH
like _ O
GPT _ B-TECH
- _ I-TECH
3 _ I-TECH
competitive _ O
with _ O
state-of-the-art _ O
finetuning _ O
approaches _ O
when _ O
provided _ O
only _ O
a _ O
few _ O
examples _ O
in _ O
a _ O
natural _ B-TECH
language _ I-TECH
prompt _ I-TECH
. _ O

This _ O
few-shot _ O
, _ O
in-context _ O
learning _ O
approach _ O
is _ O
gaining _ O
traction _ O
in _ O
large _ O
part _ O
due _ O
to _ O
its _ O
ability _ O
to _ O
learn _ O
without _ O
parameter _ O
updates _ O
. _ O

Compared _ O
to _ O
traditional _ O
finetuning _ O
methods _ O
, _ O
few-shot _ B-TECH
learning _ I-TECH
enables _ O
practitioners _ O
to _ O
more _ O
quickly _ O
prototype _ O
NLP _ B-TECH
models _ I-TECH
, _ O
allows _ O
non-technical _ O
users _ O
to _ O
create _ O
NLP _ B-TECH
systems _ I-TECH
, _ O
and _ O
efficiently _ O
reuses _ O
models _ B-TECH
to _ O
reduce _ O
system _ O
memory _ O
and _ O
complexity _ O
. _ O

GPT _ B-TECH
- _ I-TECH
3s _ I-TECH
accuracy _ O
however _ O
can _ O
be _ O
highly _ O
unstable _ O
across _ O
different _ O
prompts _ O
( _ O
training _ O
examples _ O
, _ O
permutation _ O
, _ O
format _ O
) _ O
. _ O

To _ O
address _ O
this _ O
, _ O
a _ O
new _ O
UC _ O
Berkeley _ O
, _ O
University _ O
of _ O
Maryland _ O
and _ O
UC _ O
Irvine _ O
study _ O
sets _ O
out _ O
to _ O
identify _ O
the _ O
pitfalls _ O
that _ O
can _ O
cause _ O
instability _ O
in _ O
the _ O
GPT _ B-TECH
- _ I-TECH
3 _ I-TECH
language _ I-TECH
model _ I-TECH
and _ O
proposes _ O
a _ O
contextual _ B-TECH
calibration _ I-TECH
procedure _ I-TECH
that _ O
consistently _ O
improves _ O
GPT _ B-TECH
- _ I-TECH
3 _ I-TECH
( _ O
and _ O
GPT _ B-TECH
- _ I-TECH
2 _ I-TECH
) _ O
accuracy _ O
across _ O
different _ O
prompt _ O
format _ O
choices _ O
and _ O
examples _ O
. _ O

Typically _ O
, _ O
a _ O
natural _ B-TECH
language _ I-TECH
prompt _ I-TECH
is _ O
fed _ O
to _ O
neural _ B-TECH
autoregressive _ I-TECH
language _ I-TECH
models _ I-TECH
to _ O
ensure _ O
they _ O
perform _ O
few-shot _ B-TECH
learning _ I-TECH
using _ O
in-context _ B-TECH
learning _ I-TECH
. _ O

The _ O
prompt _ O
consists _ O
of _ O
three _ O
components _ O
: _ O
a _ O
format _ O
, _ O
a _ O
set _ O
of _ O
training _ O
examples _ O
, _ O
and _ O
a _ O
permutation _ O
of _ O
the _ O
training _ O
examples _ O
. _ O

The _ O
researchers _ O
first _ O
studied _ O
how _ O
GPT _ B-TECH
- _ I-TECH
3s _ I-TECH
accuracy _ O
changes _ O
across _ O
different _ O
prompts _ O
. _ O

They _ O
conducted _ O
sentiment _ B-TECH
analysis _ I-TECH
task _ O
experiments _ O
on _ O
three _ O
GPT _ B-TECH
- _ I-TECH
3 _ I-TECH
model _ O
sizes _ O
( _ O
2.7B _ O
, _ O
13B _ O
, _ O
and _ O
175B _ O
parameters _ O
) _ O
trained _ O
on _ O
SST _ O
- _ O
2 _ O
datasets _ O
, _ O
and _ O
observed _ O
high _ O
variance _ O
in _ O
GPT _ B-TECH
- _ I-TECH
3s _ I-TECH
accuracy _ O
across _ O
the _ O
prompts _ O
training _ O
examples _ O
, _ O
permutation _ O
of _ O
examples _ O
, _ O
as _ O
well _ O
as _ O
format _ O
. _ O

Surprising _ O
, _ O
varying _ O
the _ O
permutation _ O
of _ O
the _ O
training _ O
examples _ O
could _ O
cause _ O
accuracy _ O
to _ O
range _ O
from _ O
54.3 _ O
percent _ O
to _ O
near _ O
state-of-the-art _ O
( _ O
93.4 _ O
percent _ O
) _ O
. _ O

Accuracy _ O
across _ O
training _ O
sets _ O
, _ O
permutations _ O
and _ O
formats _ O
The _ O
researchers _ O
next _ O
analyzed _ O
factors _ O
that _ O
contribute _ O
to _ O
GPT _ B-TECH
- _ I-TECH
3 _ I-TECH
instability _ O
, _ O
identifying _ O
three _ O
biases _ O
behind _ O
the _ O
accuracy _ O
variance _ O
: _ O
Majority _ O
Label _ O
Bias _ O
GPT _ B-TECH
- _ I-TECH
3 _ I-TECH
is _ O
biased _ O
towards _ O
answers _ O
that _ O
are _ O
frequent _ O
in _ O
the _ O
prompt _ O
. _ O

The _ O
majority _ O
label _ O
bias _ O
helps _ O
explain _ O
why _ O
different _ O
choices _ O
for _ O
the _ O
training _ O
examples _ O
heavily _ O
influence _ O
GPT _ B-TECH
- _ I-TECH
3s _ I-TECH
accuracy _ O
as _ O
this _ O
shifts _ O
the _ O
distribution _ O
of _ O
model _ O
predictions _ O
. _ O

Recency _ O
Bias _ O
The _ O
models _ B-TECH
majority _ O
label _ O
bias _ O
is _ O
aggravated _ O
by _ O
its _ O
recency _ O
bias _ O
: _ O
the _ O
tendency _ O
to _ O
repeat _ O
answers _ O
that _ O
appear _ O
towards _ O
the _ O
end _ O
of _ O
the _ O
prompt _ O
. _ O

Overall _ O
, _ O
recency _ O
bias _ O
helps _ O
to _ O
explain _ O
why _ O
the _ O
permutation _ O
of _ O
the _ O
training _ O
examples _ O
is _ O
important _ O
. _ O

Common _ O
Token _ O
Bias _ O
GPT _ O
- _ O
3 _ O
is _ O
biased _ O
towards _ O
outputting _ O
tokens _ O
that _ O
are _ O
common _ O
in _ O
its _ O
pretraining _ O
distribution _ O
. _ O

The _ O
common _ O
token _ O
bias _ O
helps _ O
explain _ O
why _ O
the _ O
choice _ O
of _ O
label _ O
names _ O
is _ O
important _ O
, _ O
and _ O
why _ O
the _ O
model _ B-TECH
struggles _ O
with _ O
rare _ O
answers _ O
. _ O

The _ O
team _ O
says _ O
these _ O
three _ O
biases _ O
together _ O
tend _ O
to _ O
contribute _ O
to _ O
a _ O
simple _ O
shift _ O
in _ O
a _ O
models _ O
output _ O
distribution _ O
. _ O

Inspired _ O
by _ O
the _ O
idea _ O
that _ O
model _ B-TECH
biases _ O
towards _ O
certain _ O
answers _ O
can _ O
be _ O
estimated _ O
by _ O
feeding _ O
content-free _ O
inputs _ O
, _ O
the _ O
researchers _ O
proposed _ O
a _ O
novel _ O
data-free _ O
contextual _ B-TECH
calibration _ I-TECH
procedure _ I-TECH
to _ O
infer _ O
parameters _ O
. _ O

To _ O
evaluate _ O
the _ O
contextual _ B-TECH
calibrations _ I-TECH
effectiveness _ O
, _ O
they _ O
conducted _ O
experiments _ O
on _ O
text _ B-TECH
classification _ I-TECH
, _ O
fact _ B-TECH
retrieval _ I-TECH
and _ O
information _ B-TECH
extraction _ I-TECH
tasks _ O
across _ O
different _ O
datasets _ O
( _ O
AGNews _ O
, _ O
MIT _ O
Director _ O
, _ O
DBPedia _ O
, _ O
TREC _ O
etc _ O
. _ O
) _ O
. _ O

Mean _ O
accuracy _ O
comparison _ O
across _ O
different _ O
training _ O
example _ O
choices _ O
for _ O
different _ O
datasets _ O
and _ O
model _ B-TECH
sizes _ O
. _ O

The _ O
steep _ O
red _ O
lines _ O
on _ O
all _ O
three _ O
model _ O
sizes _ O
indicate _ O
that _ O
few-shot _ B-TECH
learning _ I-TECH
can _ O
be _ O
highly _ O
unstable _ O
across _ O
different _ O
numbers _ O
of _ O
training _ O
examples _ O
. _ O

The _ O
more _ O
stable _ O
blue _ O
lines _ O
show _ O
that _ O
the _ O
calibration _ O
method _ O
improves _ O
the _ O
accuracy _ O
and _ O
robustness _ O
of _ O
GPT _ B-TECH
- _ I-TECH
3 _ I-TECH
models _ I-TECH
. _ O

Contextual _ B-TECH
calibration _ I-TECH
improves _ O
accuracy _ O
across _ O
a _ O
range _ O
of _ O
tasks _ O
The _ O
proposed _ O
contextual _ B-TECH
calibration _ I-TECH
method _ I-TECH
improves _ O
the _ O
accuracy _ O
and _ O
reduces _ O
the _ O
variance _ O
of _ O
GPT _ O
- _ O
3 _ O
models _ O
, _ O
boosting _ O
average _ O
and _ O
worst-case _ O
absolute _ O
accuracy _ O
by _ O
up _ O
to _ O
30 _ O
percent _ O
. _ O

The _ O
study _ O
highlights _ O
the _ O
need _ O
for _ O
better _ O
understanding _ O
and _ O
analysis _ O
of _ O
the _ O
dynamics _ O
of _ O
in-context _ B-TECH
learning _ I-TECH
. _ O

