D2.14TechnologyDeepDive–SpeechTechnologiesAuthorsGerhardBackfried,MarcinSkowron,EvaNavas,AivarsBērziņš,JoachimVandenBogaert,FranciskadeJong,AndreaDeMarco,InmaHernaez,MarekKováč,PeterPolák,JohanRohdin,MichaelRosner,JonSanchez,IbonSaratxaga,PetrSchwarzDisseminationlevelPublicDate28-02-2022D2.14:TechnologyDeepDive–SpeechTechnologiesAboutthisdocumentProjectEuropeanLanguageEquality(ELE)Grantagreementno.LC-01641480–101018166ELECoordinatorProf.Dr.AndyWay(DCU)Co-coordinatorProf.Dr.GeorgRehm(DFKI)Startdate,duration01-01-2021,18monthsDeliverablenumberD2.14DeliverabletitleTechnologyDeepDive–SpeechTechnologiesTypeReportNumberofpages72StatusandversionFinalDisseminationlevelPublicDateofdeliveryContractual:28-02-2022–Actual:28-02-2022WorkpackageWP2:EuropeanLanguageEquality–TheFutureSituationin2030TaskTask2.3Science–Technology–Society:LanguageTechnologyin2030AuthorsGerhardBackfried,MarcinSkowron,EvaNavas,AivarsBērziņš,JoachimVandenBogaert,FranciskadeJong,AndreaDeMarco,InmaHernaez,MarekKováč,PeterPolák,JohanRohdin,MichaelRosner,JonSanchez,IbonSaratxaga,PetrSchwarzReviewersItziarAldabe,PeterPolákECprojectofficersSusanFraser,MiklosDruskocziContactEuropeanLanguageEquality(ELE)ADAPTCentre,DublinCityUniversityGlasnevin,Dublin9,IrelandProf.Dr.AndyWay–andy.way@adaptcentre.ieEuropeanLanguageEquality(ELE)DFKIGmbHAlt-Moabit91c,10559Berlin,GermanyProf.Dr.GeorgRehm–georg.rehm@dfki.dehttp://www.european-language-equality.eu©2022ELEConsortiumWP2:EuropeanLanguageEquality–TheFutureSituationin2030iiD2.14:TechnologyDeepDive–SpeechTechnologiesConsortium1DublinCityUniversity(Coordinator)DCUIE2DeutschesForschungszentrumfürKünstlicheIntelligenzGmbH(Co-coordinator)DFKIDE3UniverzitaKarlova(CharlesUniversity)CUNICZ4Athina-ErevnitikoKentroKainotomiasStisTechnologiesTisPliroforias,TonEpikoinonionKaiTisGnosisILSPGR5UniversidadDelPaisVasco/EuskalHerrikoUnibertsitatea(UniversityoftheBasqueCountry)UPV/EHUES6CROSSLANGNVCRSLNGBE7EuropeanFederationofNationalInstitutesforLanguageEFNILLU8Réseaueuropéenpourl’égalitédeslangues(EuropeanLanguageEqualityNetwork)ELENFR9EuropeanCivilSocietyPlatformforMultilingualismECSPMDK10CLARINERIC–CommonLanguageResourcesandTechnologyInfrastructureasaEuropeanResearchInfrastructureConsortiumCLARINNL11UniversiteitLeiden(UniversityofLeiden)ULEINL12Eurescom(EuropeanInstituteforResearchandStrategicStudiesinTelecommunicationsGmbH)ERSCMDE13StichtingLIBER(AssociationofEuropeanResearchLibraries)LIBERNL14WikimediaDeutschland(GesellschaftzurFörderungfreienWissense.V.)WMDDE15TildeSIATILDELV16EvaluationsandLanguageResourcesDistributionAgencyELDAFR17ExpertSystemIberiaSLEXPSYSES18HENSOLDTAnalyticsGmbHHENSAT19XceleratorMachineTranslationsLtd.(KantanMT)KNTNIE20PANGEANIC-B.I.EuropaSLUPANES21SemanticWebCompanyGmbHSWCAT22SIRMAAIEAD(Ontotext)ONTOBG23SAPSESAPDE24UniversitätWien(UniversityofVienna)UVIEAT25UniversiteitAntwerpen(UniversityofAntwerp)UANTWBE26InstituteforBulgarianLanguage“Prof.LyubomirAndreychin”IBLBG27SveučilišteuZagrebuFilozofskifakultet(Univ.ofZagreb,FacultyofHum.andSocialSciences)FFZGHR28KøbenhavnsUniversitet(UniversityofCopenhagen)UCPHDK29TartuUlikool(UniversityofTartu)UTARTEE30HelsinginYliopisto(UniversityofHelsinki)UHELFI31CentreNationaldelaRechercheScientifiqueCNRSFR32NyelvtudományiKutatóközpont(ResearchInstituteforLinguistics)NYTKHU33StofnunÁrnaMagnússonarííslenskumfræðumSAM(ÁrniMagnússonInst.forIcelandicStudies)SAMIS34FondazioneBrunoKesslerFBKIT35LatvijasUniversitātesMatemātikasunInformātikasinstitūts(InstituteofMathematicsandComputerScience,UniversityofLatvia)IMCSLV36LietuviųKalbosInstitutas(InstituteoftheLithuanianLanguage)LKILT37LuxembourgInstituteofScienceandTechnologyLISTLU38UniversitàtaMalta(UniversityofMalta)UMMT39StichtingInstituutvoordeNederlandseTaal(DutchLanguageInstitute)INTNL40Språkrådet(LanguageCouncilofNorway)LCNORNO41InstytutPodstawInformatykiPolskiejAkademiiNauk(PolishAcademyofSciences)IPIPANPL42UniversidadedeLisboa,FaculdadedeCiências(UniversityofLisbon,FacultyofScience)FCULisbonPT43InstitutuldeCercetăriPentruInteligențăArtificială(RomanianAcademy)ICIARO44UniversityofCyprus,FrenchandEuropeanStudiesUCYCY45JazykovednýústavĽudovítaŠtúraSlovenskejakadémievied(SlovakAcademyofSciences)JULSSK46InstitutJožefStefan(JozefStefanInstitute)JSISI47CentroNacionaldeSupercomputación(BarcelonaSupercomputingCenter)BSCES48KungligaTekniskahögskolan(RoyalInstituteofTechnology)KTHSE49UniversitätZürich(UniversityofZurich)UZHCH50UniversityofSheffieldUSFDUK51UniversidaddeVigo(UniversityofVigo)UVIGOES52BangorUniversityBNGRUKWP2:EuropeanLanguageEquality–TheFutureSituationin2030iiiD2.14:TechnologyDeepDive–SpeechTechnologiesContents1Introduction12ScopeofthisDeepDive33SpeechTechnologies:MainComponents33.1AutomaticSpeechRecognition..............................33.2SpeakerRecognition....................................63.3LanguageIdentification..................................73.4Assessmentofemotions,cognitiveconditionsandpersonalitytraits.......83.5TexttoSpeech........................................94SpeechTechnologies:CurrentStateoftheArt104.1AutomaticSpeechRecognition..............................104.2SpeakerRecognition....................................144.3LanguageIdentification..................................154.4Assessmentofemotions,cognitiveconditionsandpersonalitytraits.......154.5TexttoSpeech........................................175SpeechTechnologies:MainGaps195.1Backgroundandoverviewofthemainchallenges..................195.2Data:alignment,labelling,anonymisation,diversity................225.3Accuracy:reachingusablethresholdsforapplications...............245.4Dialectalspeechandmultilingualtraining.......................255.5Explainabilityandtransparencyforcriticalmethodsandtechnologies.....256SpeechTechnologies:ContributiontoDigitalLanguageEqualityandImpactonSociety256.1Digitallanguageinequalities...............................266.2Biases,fairnessandethicalissues............................276.3Userswithspecialneeds..................................286.4Businessesandeffectsofscale..............................306.5Energyconsumptionandsustainability.........................306.6Privacy,surveillanceandtrust..............................317SpeechTechnologies:MainBreakthroughsNeeded327.1Accesstoanddiscoverabilityoftrainingdata.....................327.2Newtrainingparadigms..................................337.3Confluenceandcontextinformationintegration...................347.4Explainability,transparencyandprivacyconcerns.................357.5Supportforless-resourcedlanguages..........................367.6Performance,robustnessandevaluationparadigms.................367.7Outreach–communities,non-experts..........................377.8AlignmentswithEUpoliciesandbreakthroughsneededonthepolicylevel..378SpeechTechnologies:MainTechnologyVisionsandDevelopmentGoals388.1Speechtechnologies–theinterfaceofthefuture...................388.2Capabilitiesandtechnologyshifts............................398.3Privacy,accountabilityandregulations.........................418.4Futureapplications.....................................438.4.1Customercontactcentres/callcentres.....................438.4.2Mediaandentertainment.............................43WP2:EuropeanLanguageEquality–TheFutureSituationin2030ivD2.14:TechnologyDeepDive–SpeechTechnologies8.4.3MarketingandPR..................................438.4.4Healthcare......................................448.4.5Frauddetectionandsecurity...........................448.4.6PersonalisedST...................................448.5Possiblefuturedirectionsandvisions..........................448.5.1Actorsandmarkets.................................448.5.2Customisation....................................448.5.3Privacyandethics..................................458.5.4AmbientIntelligence................................458.5.5AugmentedIntelligence..............................458.5.6Ontheroadtowinteragain?...........................458.5.7Supermodels.....................................468.6ExamplesforIntelligentPersonalAssistants......................469SpeechTechnologies:TowardsDeepNaturalLanguageUnderstanding4810SummaryandConclusions49WP2:EuropeanLanguageEquality–TheFutureSituationin2030vD2.14:TechnologyDeepDive–SpeechTechnologiesListofAcronymsAAEAdversarialAutoencoderADAlzheimer’sDiseaseAIArtificialIntelligenceAMAcousticModelAPACAsiaPacificRegionASRAutomaticSpeechRecognitionASVAutomaticSpeakerVerificationBERTBidirectionalEncoderRepresentationsfromTransformersBPEBytePairEncodingCAConversationalAgentCERCharacterErrorRateCLIPContrastiveLanguage-ImagePre-trainingCNNConvolutionalNeuralNetworkCTCConnectionistTemporalClassificationCVCurriculumVitaeDARPADefenseAdvancedResearchProjectsAgencyDBNDeepBeliefNetworkDCFDetectionCostFunctionDERDiarisationErrorRateDLEDigitalLanguageEqualityDNNDeepNeuralNetworkDSDataScienceDS-LTSMDual-SequenceLongShort-TermMemoryE2EEnd-to-EndEEREqualErrorRateFAFalseAcceptFRFalseRejectG2PGrapheme-to-PhonemeGAFAGoogle,Apple,FacebookandAmazonGDPRGeneralDataProtectionRegulationGMMGaussianMixtureModelGPT-3GenerativePre-trainedTransformer3GPUGraphicsProcessingUnitGRUGatedRecurrentUnitHCIHuman-ComputerInteractionHMMHiddenMarkovModelIoTInternetofThingsIPAIntelligentPersonalAssistantJERJaccardErrorRateJFAJointFactorAnalysisLIDLanguageIdentificationLLMLargeLanguageModelLLRLog-LikelihoodRatioLMLanguageModelLSTMLongShort-TermMemoryLTLanguageTechnologyMFCCMelFrequencyCepstralCoefficientsMLMachineLearningWP2:EuropeanLanguageEquality–TheFutureSituationin2030viD2.14:TechnologyDeepDive–SpeechTechnologiesMMLMMultilingualMaskedLanguageModellingMOSMeanOpinionScoreMTMachineTranslationNERNamedEntityRecognitionNISTNationalInstituteofStandardsNLPNaturalLanguageProcessingNLUNaturalLanguageUnderstandingNNNeuralNetworkOOVOut-of-VocabularyRatePERPhonemeErrorRatePLDAProbabilisticLinearDiscriminantAnalysisPOSPart-of-SpeechPRPublicRelationsRNNRecurrentNeuralNetworkRWTHRheinisch-WestfälischeTechnischeHochschuleAachen(AachenUniver-sity)S2SSequence-to-sequenceSA-ASRSpeaker-AttributedAutomaticSpeechRecognitionSdSVShort-durationSpeakerVerificationSERSpeechEmotionRecognitionSIDSpeakerIdentificationSOTAStateoftheArtSRSpeakerRecognitionSSHSocialSciencesandHumanitiesSTSpeechTechnologiesSVSpeakerVerificationSVMSupportVectorMachinesSWSoftwareTDNNTimedelayneuralnetworkTTSTexttoSpeechVAVoiceAssistantWERWord-Error-RateWFSTWeightedFinite-StateTransducersWP2:EuropeanLanguageEquality–TheFutureSituationin2030viiD2.14:TechnologyDeepDive–SpeechTechnologiesAbstractD2.14providesanoverviewanddescribesthestateoftheartanddevelopmentswithinthefieldofSpeechTechnologies(ST).Thisfieldisinterpretedtocomprisetechnologiesaimedattheprocessingandproductionofthehumanvoice,both,fromalinguisticaswellasparalin-guisticangle.Itprovidesanin-depthaccountofcurrentresearchtrendsandapplicationsinvariousSTsub-fields,detailstechnical,scientific,commercialandsocietalaspects,relatesSTtothewiderfieldsofNLPandAIandprovidesanoutlookofSTtowards2030.Chapters3and4,presentingthemainSTcomponentsandthestate-of-the-arearedividedaccordingtothedifferentsub-fieldscovered:AutomaticSpeechRecognition(ASR),SpeakerIdentifica-tion(SID),LanguageIdentification(LID),technologiestargetingparalinguisticphenomenaandTexttoSpeech(TTS).Chapter5discussesthemaingapsinspeechtechnologiesrelatedtoissuessuchasdatarequirements,STperformance,explainabilityofthecriticalmethods,regulationsinfluencingthepaceofdevelopmentinthefieldorspecificrequirementsforless-resourcedlanguages.ThefollowingchapterpresentsaspectsofthewiderimpactofSTonsocietyanddescribesthecontributionsofspeechtechnologiestoDigitalLanguageEquality.Chapters7-9outlinesomebreakthroughsneeded,themaintechnologyvisionsandpresenthowSTmayfitintoandcontributetoawidervisionofwhatmaybetermedDeepNaturalLanguageUnderstanding.Thedeliverableintegratestheviewsofcompaniesandinstitutionsinvolvedinresearch,commercialexploitationandapplicationofspeechtechnologies.1IntroductionSpeech–asthemostnaturalmannerforhumanstointeractwithcomputers–hasalwaysattractedenormousinterestinacademiaandtheindustry.SpeechTechnologies(ST)haveconsequentlybeenthefocusofamultitudeofresearchandcommercialactivitiesoverthepastdecades.Fromhumblebeginningsinthe1950’ies,theyhavecomealongwaytothecurrentstate-of-the-art,deep-neural-network(DNN)basedapproaches.Stimulatedbyashifttowardsstatisticalmethods,the1980’ieswitnessedaneraofHidden-Markov-Models(HMM),Gaussian-Mixture-Models(GMM)andword-basedn-grammodelscombinedintospeechre-cognitionenginesemployingevermorerefineddata-structuresandsearchalgorithmssuchasprefix-treesorViterbibeam-search(Jelinek,1998).Theavailabilityofdataresourcestotrainthesesystemswaslimitedtoonlyafewlanguages,oftendrivenbysecurity(andcom-mercial)interest.Eventhen,workonNeuralNetworks(NN)wasalreadybeingcarriedoutandviewedbymanyasthemostpromisingapproach.However,itwasn’tuntillater(2000’s),whentheavailabilityoftrainingdatapairedwithadvancesinalgorithmsandcomputingpowerfinallybegantocometogethertounleashthefullpotentialofNN-basedST.AsArtificialIntelligence(AI)wasenteringspringtimeagain–followingtheso-calledAI-Winter(Hendler,2008;Floridi,2020)–generalinterest,researchactivities,fundingoppor-tunitiesandinvestmentswitnesseddramaticgrowth.Thishasledtosignificantprogressinmanyrelatedfields,includingthoseofNaturalLanguageProcessing(NLP),MachineLearn-ing(ML)andDataScience(DS).SpeechTechnologiesprofitedgreatlyfromtheseadvancesandhavebecomemainstreamtechnologies,deployedinnumerousdomainsandviewedascommoditiesinmanyinstances.Especiallyoverthepastcoupleofdecades,SThaveevolveddramaticallyandbecomeom-nipresentinmanyareasofhuman-machineinteraction.EmbeddedintothewiderfieldsofArtificialIntelligence(AI)andNaturalLanguageProcessing,theexpansionandscopeofSTandtheirapplicationshaveacceleratedfurtherandgainedconsiderablemomentum.Intherecentyears,thesetrendswerepairedwiththeundergoing,profoundparadigmshiftrelatedtotheriseofthefoundationmodels(Bommasanietal.,2021),suchasBERT(Devlinetal.,WP2:EuropeanLanguageEquality–TheFutureSituationin20301D2.14:TechnologyDeepDive–SpeechTechnologies2018),GPT-3(Brownetal.,2020),CLIP(Radfordetal.,2021)andDALL-E(Rameshetal.,2021)–aclassofmodelstrainedonbroaddataatscale,adaptablevianaturallanguagepromptsandabletoperformreasonablywellonawiderangeoftasksdespitenotbeingtrainedex-plicitlyonthosedownstreamtasks.Changes,newrequirementsandrestrictionsintroducedbytheCOVID-19pandemicpairedwithsubstantialadvancesinalgorithmsandspecialised,high-performancehardwareaswellasthewideavailabilityofmobiledeviceshaveledtomassivelyincreasedadoptionandfur-thertechnologicalimprovements.Withspeechandnaturallanguageformingfundamentalpillarsof(human)communication,STmaynowevenbeperceivedas“speech-centricAI”.Duringtheseexceptionaltimesofaglobalpandemic,businessesandadministrationsalikehavebeenencouragedandurgedtoimprovetheirvirtualtieswithcustomersandcitizens.Thishasledtoincreasedadoptionandextensionofthescopeandapplicationofvirtualassis-tants,chatbots,andothervoice-enabledtechnologies.Withtheemergenceofintelligentvir-tualassistantsSThavebecomeubiquitous,yetmanySTsystemscanonlycopewithrestrictedsettingsanddomainsandcanbeusedonlywiththemostwidelyspokenoftheworld’smanythousandsoflanguages.Forlanguageswithfewerspeakersandthusoflessercommercialinterest,STsystemsarestillallbutabsentand/orseverelylimitedintheirscope.Asaconse-quence,millionsofindividualswhospeaktheselanguagesarevirtuallycutofffromawiderangeofspeech-basedservicesandapplicationsorforcedtocommunicateinlanguagesotherthantheirnativeone(s).Currenttechnologiesoftenrequirethepresenceoflargeamountsofdatatotrainsystemsandcreatecorrespondingmodels.Despitethelackofmassivevolumesoftrainingmate-rial(e.g.,transcribedspeechincaseofASRorannotatedaudioforTTS),recentadvancesinMLandSThavebeguntoenablethecreationofmodelsalsoforlesscommonlanguages.Theseapproacheshoweveraregenerallymorecomplex,expensiveandlesssuitableforwideadoption.Whilerecentlypresentedresultsindicatethatnovelapproachescouldindeedbeappliedtoaddresssomeofthechallengesrelatedtothecreationofmodelsforlowresourcedlanguages,thescopeoftheirapplicationandinherentlimitationsarestillthesubjectofon-goingresearch(Laietal.,2021).ThedemocratisationofSTmaythusbeviewedaspartofthedemocratisationofAIingen-eral,notonlyinthesenseofallowingthegeneralpublictoparticipateinthegenerationofmodelsandsolutions,butalsotoequallyparticipateintheiruse.Thisreportdescribesthestateoftheartinthefieldofspeechtechnologies,shortcomingsandchallengesandoutlinestechnical,scientificaswellascommercialalleystowardsthefuture.Itprovidesanin-depthaccountofthecurrentresearchactivitiesandapplicationsinvarioussub-fieldsandputstheseactivitiesintoperspectiveandrelationwitheachother.Chapter2presentsthescopeofthisdeepdiveandintroducesthemainfieldscovered.Inchapter3,themaincomponentsofthefieldofSpeechTechnologiesarepresentedfollowedbyadescriptionofdifferentsub-fieldsofST:AutomaticSpeechRecognition(ASR),SpeakerIdentification(SID),LanguageIdentification(LID),technologiesaddressingparalinguisticas-pectsofspeechandTexttoSpeech(TTS).Chapter4providesanin-depthdescriptionofthecurrentstate-of-the-artmethodsofeachoftheST-sub-fieldscovered.Chapter5discussesthemaingapsinspeechtechnologiessuchastheonesrelatedtodataneeds,STperformance,ex-plainabilityofthecriticalmethods,regulationsinfluencingthepaceofdevelopmentinthefield,andspecificchallengesconcerninglow-resourcedlanguages.Inchapter6,STcontri-butionstoDigitalLanguageEqualityandthewiderimpactofthetechnologiesonsocietyarepresented.Theseincludethediscussionaboutdigitallanguageinequalities,biases,fairnessandethicalissuesrelatedtotheuseofST.WealsooutlinetheimpactofSTonuserswithspecialneedsandpresenttherelationsbetweenthedevelopmentandapplicationofSTinawiderlandscapeofbusinessenvironments,itsfootprintonenergyconsumptionandram-ificationsinthecontextofprivacyandtrustintechnology.Chapter7outlineschallengesandindicatesseveralbreakthroughsneededtoovercomethem.TheseincludetheaccessWP2:EuropeanLanguageEquality–TheFutureSituationin20302D2.14:TechnologyDeepDive–SpeechTechnologiesanddiscoverabilityoftrainingdataandchangesrequiredintrainingparadigms.Further-more,arangeofchallengesrelatedtotheperformance,robustness,evaluation,integrationofSTcomponentsbeyondthefieldandrequirementsforreachingouttoothercommunitiesandnon-expertusersisdiscussed.Thechapterconcludeswithanoverviewoftherequire-mentsforthealignmentswiththeexistingEUpoliciesandchangesneededonthepolicylevel.Chapter8presentsthemaintechnologyvisionsanddevelopmentgoalsuntil2030.Fi-nally,chapter9,presentshowSTfitintoandcontributetoawidervisionofDeepNaturalLanguageUnderstanding.2ScopeofthisDeepDiveThescopeofthisdeepdiveencapsulatesawiderangeofspeechtechnologiesincludinglan-guageidentificationandspeakerrecognition,automaticspeechrecognition,technologiestoaddressparalinguisticphenomenaaswellastexttospeech.ItgathersandsynthesisestheperspectivesoftheEuropeanresearchandindustrystakeholdersonthecurrentstateofaf-fairs,identifiesseveralmaingapsaffectingthefield,outlinesanumberofbreakthroughsrequiredandpresentsthetechnologicalvisionanddevelopmentgoalsinthenexttenyears.Theviewsexpressedstemfromadiversesetofgroupsandcompriseelementsofresearchaswellastheindustry.InlinewithotherdeepdivesofWP2,weadoptamultidimensionalapproachwherebothmarket/commercialaswellresearchperspectivesareconsideredandconcentrateontheseimportantaspects–technologies,models,data,applicationsandtheimpactofspeechtech-nologiesonsociety.Thetendencyforthecombinationoftechnologiesintomorepowerfulsystems,encom-passingseveralindividualtechnologiesandmodelshasbecomeapparentandisreflectedinnumerousoccasionswithinthisdocument.Weexpectthistrendtocontinueandevengetstrongerovertime.STcanbeinvestigatedandresearchedintheirownrightandmuchefforthasbeeninvestedinthisdirection(andcontinuestobeinvested).However,theirfullpotentialoftenonlybe-comesevidentwhentheyarecombinedwithfurthertechnologiesformingintelligentsys-temscapableofcomplexinteractionanddialogues.Thiskindofinteractionmayencompassadiversesetofcontexts,historyandspanmultiplemodalities.Tothecasualuser,individualcomponentsthenbecomeblurredandalmostinvisiblewithoneoverallapplicationactingasthepartnerwithinanactivitywhichmayotherwisebecarriedouttogetherwithafellowhumanbeing.Inthissetting,theconglomerateandaggregationoftechnologiesformastepawayfromnarrowandhighlyspecialisedsystemstowardscombinedandcomplexsystems,providinganotionofamoregeneralandbroaderkindofintelligence.Speechandlanguage,asthemostnaturalandappropriatevehicleforhumanstocommunicatewithmachinesinmanyinstances,thusbecomesthegatekeepertoandcoreofabroaderkindofAI.3SpeechTechnologies:MainComponents3.1AutomaticSpeechRecognitionGeneralintroductionThegoalofanautomaticspeechrecognitionsystemistoconvertspeechintosequencesofunitssuchaswordsorcharacters.Intheprocess,severalstepsareperformed,involvingaWP2:EuropeanLanguageEquality–TheFutureSituationin20303D2.14:TechnologyDeepDive–SpeechTechnologiesvarietyofmodelsandalgorithms.Theseconverttherawaudioinputintoincreasinglymoreabstractunits(suchasfeatures,phonemes,phones,morphemes,words)andeventuallytothedesiredunitoftranscription.Eachofthesestepsinvolvesdifferentknowledgesourcesandcanbemodelledbyindividualcomponentsorbycombinedmodelsspanningseveralsteps.Traditionally,theseknowledgesourceswereeachmodelledandoptimisedindividu-ally.Morerecently,modelshavebeencombinedandeventuallyresultedinso-calledend-to-end(E2E)models1aimingtorepresent(andoptimise)thecompletetranscriptionprocesswithinasinglemodel.Conceptually,ASRsystemsconsistofanacousticmodel(AM),alexiconandalanguagemodel(LM).TheAM’sgoalistomodelspeechsoundsandtheirvariations.Thelexiconde-finestheinventoryofunits(words)toberecognised.EachoftheseunitshasoneormorepronunciationslinkingittotheAMandthelower-levelaudio-units(e.g.,phonemes,viaapronunciation-lexicon).Specialcomponentsmappingspellingstosounds(grapheme-to-phoneme,G2P)areemployedtocreatethesepronunciationsinaconsistentmanner.Finally,theLM’staskistomodelhowtheunitsofthelexiconinteract,typicallybyassigningproba-bilitiestosequencesoftheseunits.ProcessingASRsystemstypicallyperformsomeinitialpre-processingtoproduceasequenceoffeaturesinregulartime-intervals(e.g.,everyfewmilliseconds).Traditionallythesefeaturesweremodelledafterthehuman-ear(e.g.,mel-cepstrumcoefficients)butpotentiallytheymayalsobecreatedbytheinitialstagesofNNs.AMandLMarethenemployedwithinasearchalgo-rithmtoproduceatranscriptorsetofalternativetranscripts.Thebasicprocesswasintro-ducedalreadyinthe1980’iesbyBahlandJellinek(Jelinek,1998).Somepost-processing,e.g.,forhandlingnumbersoracronyms,issometimesappliedtothisinitialtranscripttoproduceafinaltranscript.TheASR-processcanbecarriedoutinacausalfashion(sometimesalsore-ferredtoasonline)andin(near-)real-timeorrunningfrompre-existingaudio(sometimesreferredtoasoffline),inwhichcasemorecontextisavailablefortheprocessandfasterthanreal-timeprocessingcanbeachieved.Duringthepastdecade,hybridmodels(BourlardandMorgan,1993;Seideetal.,2011),combiningtraditionalelementssuchasHMMwithNNsaswellasE2Emodels,combiningvariouslevelsofprocessing,havewitnessedconsider-ableprogress.Thistendencycontinues,alsoallowingfornovelmethodsinvolvingdifferentmodalitiesandtheintegrationofresearchresultsfromotherfieldssuchasmachinetrans-lation(MT,e.g.,transformers(Vaswanietal.,2017))orvisualprocessing(e.g.,theuseofconvolutionalNN’s(Guetal.,2018)).PerformancemeasuresTheperformanceofASRsystemsistypicallymeasuredintermsofWord-Error-Rate(WER)usingdynamicalignmentbetweenreferenceandtranscript.TheWERdependsonavarietyoffactorsconcerningtheacousticconditionsunderwhichthespeechwasproduced,speaker-specifictraitssuchasnativeness,emotionalstate,etc.andaseriesoflinguisticfactorssuchasdialects,socialregister,specificdomainsorthespontaneityofspeech.Asmodelsaretypicallyofastatisticalnature,theavailabilityofadequatetrainingcorporaisakeyfactorindeterminingtheperformanceofASRsystemsasarethealgorithmsappliedtocombinethevariousmodelsduringthesearch.1TheexactdefinitionofE2Evariesaccordingtocontextandauthors,butforpracticalpurposes,itcanbeassumedtomeanadirectconversionfrominput(audio)tooutput(transcript).WP2:EuropeanLanguageEquality–TheFutureSituationin20304D2.14:TechnologyDeepDive–SpeechTechnologiesCurrenttrendsRecentyearshavewitnessedsubstantialprogressrequiringlessandlesslabelledspeechdatafortrainingandtheadoptionofsemi-supervisedorunsupervisedstrategiesformodelcre-ation.However,thereisstillabigdisparitybetweenASRperformanceinlanguagesanddomainsforwhichtherearecopiousamountsoflabelleddataandthosewithoutsuchrichresources.ThemethodsprevalentinASRsystemstodayarebasedonstatisticalmethodsandML,particularlyonE2EDNNmodels.Theseareactiveareasofresearchandvarywidelyincomplexityandscope.Pre-trainedmodelsareoftenfine-tunedspecificallyfortheintendeduseandalsoserveasthestartingpointofASRforspecificlanguages.MethodssuchasHMM,hybridHMM-NNmodelsorweightedfinite-state-transducers(WFST)arelikewisestillinuse.PriorapproachesareslowlybeingphasedoutasprogressandtheapplicabilityofNN-basedmodelstakesprecedenceinmanycases.AsetofframeworksandtoolkitssuchasKaldi(Poveyetal.,2011)haveallowedabroaderaudiencetodevelopandusemodelsforASR.Theutilisationoflanguagemodels–alsoincombinationwithNN-basedmethods–isstillfavouredforASRsystemperformance.Inthisarea,severalstandardsexist,e.g.,ARPALMorKenLMstylelanguagemodelscanbesuppliedandpluggedintothemajorASRframeworksavailable.WhereasASRsystemstypicallyoutputasingletranscript,theoutputofricherstructuressuchasn-bestorlatticescanbebeneficialforinformationretrievalcontexts.DataneedsTherehasbeenastrongdependenceontheavailabilityoflargecorporatotrainthecurrentstateoftheartsystems.Thishasleftmanyinstitutionsunabletocompeteorinnovatetoameaningfulextent.Asaconsequence,theriseofASRsystemsbysuper-institutions,mostlyindustrial,hasdominatedASRdevelopment.Datarequirementshaveexploded,within-creasinglytighterandmorerestrictedreturnsoninvestment.Forinstance,itisnotunheardoftosee1000hoursormoreoflanguage-specifictranscribeddatabeingusedforthecreationofanASRmodel.Thepositiveflip-sidetothisisthatthearchitecturesofthelatestmodelshavealreadybeenbuiltwithtransferlearningandfine-tuninginmind,requiringfarlessalignedtrainingdata.Thedegreetowhichthesepre-trainedmodelsperformwhenportedtootherlanguagesalsodependsonthelinguisticproximity(phonetic,phonological,etc.)ofthelanguagetothebasemodel.Thesmallerthisproximityis,themorefine-tuningdataistypicallyrequired(e.g.,10hoursvs.100hours).Datarequirementsforlanguagemodelgen-erationarefareasiertocomebyformostlanguages,aswrittentextisgenerallymuchmorereadilyavailable.Specificinstances,suchaslanguageswithstrongdialectalinfluencesandnon-standardspelling(e.g.,thevariousdialectsofspokenArabic)formnotableexceptionshere.Domain-specificityoftextsaswellasregionalparticularitiesandlexicalshiftovertimeneedtobetakenintoaccount.TargetusesASRservesasakeytechnologyforturningunstructureddataintostructuredcontent,thusmakingitaccessiblefordownstreamprocessing(enrichment,informationretrieval,dia-loguesystems,etc.).Itisanessentialingredientinsupportingspeechinputasthemostnaturalwayforhumanstointeractwithcomputersystems.Speechtechnologieshavecometoberegardedmoreandmoreasacommodity,withbil-lionsofmobiledevicesofferinginteractivitythroughvoiceandvoice-appliancesenteringthehomesofusers(smarthomes,IoT).Spurredbyanincreasedneedforsanitation,touch-lessinteractionisbecomingthepreferredmannerofinteractionintimesofapandemiclikethecurrentCOVID-19one.Fromtheperspectiveofbusinesses,speechtechnologiespromiseWP2:EuropeanLanguageEquality–TheFutureSituationin20305D2.14:TechnologyDeepDive–SpeechTechnologiesthepossibilitytoextendserviceswithnewcapabilities,makinginteractionmoreintuitiveandnaturalandallowingatthesametimetoscaleservicestolevelsthatwerepreviouslyimpossible(duetolackofresources)ornotcost-effective.Assuch,speechtechnologiesareviewedtoplayamajorpartintheprocessofdigitaltransformation.Inmostcontexts–otherthanpersonaldictationsystems–ASRsystemsaremeanttoworkinaspeaker-independentmanner.Duetothestatisticalnatureofmodelsandthedatasetsemployedtotrainthesemodels,certainkindsofbiashavebeenobservedinthepast,e.g.,earlyASRsystemsusedtoworkbetterformalespeakersthanforfemalespeakers.AsinallotherML-basedsystems,thespecificitiesofthetrainingdataarereflectedwithintheresult-ingmodelsandspecialcareneedstobetakeninordertomitigateandminimisethiseffect.Morerecently,theuseoflanguageconcerninggenderisreceivinghigherattention.Morpho-logicalparticularities,pronounsetc.needtobereflectedproperlyinASRmodelsinordertoadequatelytranscribeutterancescontainingsuchelements.3.2SpeakerRecognitionGeneralintroductionSpeakerrecognition(SR)referstotheprocesswhereamachineinferstheidentityofaspeakerbyanalysinghis/hervoice/speech.ThebasisofSRisthetaskofspeakerverification(SV).Inthistask,oneormoreenrolment(registration)utterancesfromaspeakerarecomparedwithatestutterance.Thesystemthenprovidesascorethatindicateshowlikelyitisthatthetestutteranceisspokenbythesamepersonastheenrolmentutterances.Speakerverificationisoftenreferredtoasanopensetproblemsinceneithertheenrolledspeakersnorthetestspeakerisusedforbuildingthesystem.Asystemthatcanperformthespeakerverificationtaskcanalsobeusedforspeakeridentification,wheremanyspeakersareenrolled(regis-tered).Intesting,anutteranceisassignedtooneoftheenrolledspeakersorpossiblyalsotononeofthem.Speakerclusteringisbasedonasetofunlabelledrecordings,withtheaimtoinferthenumberofspeakersandattributethemtotherespectiverecordings.Manyapplica-tionsalsorequirecombinationsoftheabovetasks.Forexample,asetofutterancescouldbesubjectedtoidentificationafterwhichallutteranceswhichdidnotmatchanyoftheenrolledspeakersareclustered.ScopeAgeandgenderrecognitionaretwotaskscloselyrelatedtoSRsinceintypicaldatabasesthepropertyof“age”usuallydoesnotchangesignificantlybetweendifferentrecordingsofthesamespeakerand“gender”doesnotchangeatall.Utterancerepresentations(embeddings)producedbySRcanthereforebeusedasaninputfeaturetoageandgenderrecognitionsystems.Agedetectionisfrequentlydividedintoagerangesratherthanbeingtargetedattheexactageofaspeaker.Emotionrecognitionfromthespeechisanotherhighlyrelevanttopic.Itisusedinawidevarietyofapplicationsfrombusinessestogovernmentalbodies.Forexample,incallcentres,itsupportsmonitoringofclientsupportqualityandisemployedtostudyclients’reactionstocertainemotionaltriggers.Multiplestudieshavebeenconductedonemotionrecognitionfromthespeechsignal.DataneedsState-of-the-artspeakerrecognitionsystemsaretrainedondatafromseveralthousandsofspeakers,eachprovidingmanyutterances.Inaddition,thistrainingdataisoftenaugmentedwithversionsoftheutteranceswithadditivenoiseorreverberation(dataaugmentation).WP2:EuropeanLanguageEquality–TheFutureSituationin20306D2.14:TechnologyDeepDive–SpeechTechnologiesMostoftheresearchpapersinvestigatemachinelearningmodelperformanceonpublicartificiallycreateddatasetssuchasEMODB(Burkhardtetal.,2005),IEMOCAP(Bussoetal.,2008),TESS(Pichora-FullerandDupuis,2020),andRAVDESS(LivingstoneandRusso,2018).Althoughthisapproachensuresacommonbenchmark,itignoresthefactthatintherealworldspeechdataisnotasclearorwell-defined.Afewpaperssuchas(Kostoulasetal.,2008),(Dhalletal.,2013)and(TawariandTrivedi,2010)aimtoaddressthisissue.Interpretability,explainability,transparencySpeakerrecognitionsystemsaretypicallydesignedtooutputthelog-likelihoodratio(LLR)forthehypothesisthatthespeakersarethesamevs.the(alternative)hypothesisthatthespeakersaredifferent.Inthemathematicalsense,anLLRiscompletelyinterpretableforanyonewithasufficientlevelofexpertiseandtraining.Theneedforexplainability,i.e.,forthesystemtobeabletoexplainhowitreacheditsconclusion,naturallydependsontheap-plication.Iftheresultofspeakerrecognitionistobeusedasforensicevidenceatcourt,thismayindeedbeconsideredimportant.Sofar,methodsforexplainabilityofspeakerrecog-nitionsystemshavereceivedrelativelylittleattentionfromresearchers.Someworkwithgradient-basedmethodshasbeenpresentedinMuckenhirnetal.(2019).3.3LanguageIdentificationGeneralintroductionLanguageidentificationaimstorecognisewhichlanguageisspokeninanutterance.Typi-cally,itistreatedasaclosedsetclassificationproblem,i.e.,thelanguagestoberecognisedarefixed.Accordingly,languagerecognitionsystemscanbebuiltusingsupervisedmachinelearningtechniques.Inthissense,itisasimplerproblemthane.g.,ASRwheretheoutputspaceisastructuredcombinationoffixedsymbols(wordsorgraphemes),orSRwherethetypicalapplicationsrequirethesystemtocomparevoicesfromspeakersnotseenintraining.Partlyforthisreason,languagerecognitiongainslessattentionintheresearchcommunitythanASRorSR.Forexample,themostrecentlargeevaluationLIDtookplacein2017(organ-isedbytheNationalInstituteofStandardsandStatistics,NIST)whereasthereareseveralrecurringevaluationsperyearinASRandSR.ItshouldbenotedthatLIDisgenerallyper-formedbeforeASR.AnalternativewouldbetoprocesstheaudiowithASRsystemsformanydifferentlanguagesandthenanalysetheirtextoutputandconfidencescorestodeterminethelanguages.However,thisapproachisnotpracticalandgenerallynoteffective(eventhoughitwasallegedlyappliedbyAmazonAlexaintheearlystages).DataneedsBuildingstate-of-the-artLIDsystemstypicallyrequire15ormorehoursofspeechperlan-guage.Comparedtootherspeechprocessingtasks,acquiringlabelleddata(i.e.,theaudioandthelanguagelabel)forlanguageidentificationtrainingisfairlyeasy.Forexample,thespokenlanguageinvideorecordingsontheinternetcanoftenbeinferredfrommetadata.Therefore,companiesandresearchlaboratoriestypicallyhaveLIDdataformorethan50differentlanguages.Dependenciesondomainandaudioconditions,however,stillapplytosomeextent.Voxlingua107(ValkandAlumäe,2021)isadatasetforspokenlanguagerecog-nitionof6628hours(62hoursperlanguageontheaverage)anditisaccompaniedbyanevaluationsetof1609verifiedutterances.WP2:EuropeanLanguageEquality–TheFutureSituationin20307D2.14:TechnologyDeepDive–SpeechTechnologies3.4Assessmentofemotions,cognitiveconditionsandpersonalitytraitsGeneralintroductionRecognitionofemotions,moodandpersonalitytraitsfromthespeechisanactiveresearchareawithawiderangeofpotentialapplicationssuchas,e.g.,intelligenthuman-computerinteraction,callcentres,onboardvehicledrivingsystems,financialsecurityandsmarten-vironments.Asemotionsplayacrucialroleininterpersonalcommunication,perceivingtheemotionalstatesofinterlocutorsisalsoanessentialcomponentforholisticmodellingofusers,enablinganinstantaneousreceptionoffeedbackrelatedtothesystem’sactionsandbetter-informedactionselection.Theemotionalcuesaswellastheabilitytodetectuserspersonalitytraitsandcognitiveconditionsarealsousefulforasystemadaptationtoauserinthelongerterm.Automaticvoiceanalysistechniquesarealsousedforthedetectionofcognitiveconditions,monitoringofpatientssufferingfromaneurodegenerativedisorder,suchasAlzheimer’sdisease(AD).ScopeSpeechandthehumanvoicehavebeeninvestigatedforaconsiderableamountoftimewiththegoaltoinferinformationaboutthespeaker’smoodandemotions(Schulleretal.,2011;Batlineretal.,2011;KoolagudiandRao,2012;Dasgupta,2017;Albanieetal.,2018;Rouastetal.,2019;AkçayandOğuz,2020),mooddisorders(Huangetal.,2019)andsignsofdepres-sion(Cumminsetal.,2015;Totoetal.,2021).Asimilarstrandofresearchaimsatthedetectionofspecificcognitivestatesandconditions(Tóthetal.,2018;Konigetal.,2018;Pulidoetal.,2020),certaindiseases,suchasCOVID-19(Dashetal.,2021;Schulleretal.,2021)andotherhealthstates(Sertollietal.,2021),aswellaspersonalitytraits(Mairesseetal.,2007;Polzehletal.,2010;MohammadiandVinciarelli,2012;Guidietal.,2019;Leeetal.,2021).InSpeechEmotionRecognition(SER)andinspeechsynthesis,boththediscreteanddimensionalemo-tionalmodelsareused(Kwonetal.,2003;Giannakopoulosetal.,2009;Schröder,2004).Inthedimensionalmodels,twoorthreecontinuousspacesforarousal,valence,andpotencyarefrequentlyconsidered.Thediscretemodels’taxonomiesaremorediverse,withdifferencesinthenumberofemotionsincluded.Workintheareasthattargettheparalinguisticaspectsofspeechishighlyinterdisciplinaryinnature.Itbringstogetherfieldssuchaspsychology,medicineorcomputationallinguistics.Duetoitsinter-disciplinaryheritage,ithasbroughtwithitawidediversityinterminologyandapproacheswhichcanonlybedescribedatasuperficiallevelwithinthisdocument.Dataneeds,interpretability,explainability,transparencyTheamount,qualityanddiversityofdataemployedareasdiverseastheeffortsandteamsac-tiveinthisdomain.Inmanycases,experimentsareconductedinaqualitativemanner.Eth-icalandlegalissuesareofhighimportance,especiallyinclinicalsettings.Regardingtheau-tomaticprocessingofspeechwiththefocusonparalinguisticaspects,theInterspeechCom-putationalParalinguisticsChallenge(ComParE2)hasbeentakingplaceforthepast12years,introducingnewtaskseveryyearandaddressingimportantbutasofyetunder-exploredparalinguisticphenomena(Schulleretal.,2020).InSERavarietyofdatasetsisbeingused,includingtheacted(simulated),naturalandelicited(induced)speech.TheexamplesofprominentdatasetsincludeBerlinEmotionalDatabase(German)(BorchertandDusterhoft,2005),eNTERFACE’05Audio-VisualEmotion2http://www.compare.openaudio.euWP2:EuropeanLanguageEquality–TheFutureSituationin20308D2.14:TechnologyDeepDive–SpeechTechnologiesDatabase(English)(Martinetal.,2006),SEMAINEDatabase(English,Greek,Hebrew)(McK-eownetal.,2011),RECOLASpeechDatabase(French)(Ringevaletal.,2013),VeraAmMit-tagDatabase(German)(Grimmetal.,2008),AFEWDatabase(English)(Kossaifietal.,2017),TurkishEmotionalSpeechDatabase(Turkish)(OflazogluandYildirim,2013).Thesizeofthedatasetsusedinthistaskvariessignificantlybothinthenumberofspeakers(from2toafewhundred),theirprofile,andthenumberofutterancesincludedinaset.Similarly,differentemotiontaxonomiesarebeingused.See(Swainetal.,2018;Khaliletal.,2019;AkçayandOğuz,2020)foranextensivereviewofdatabases,models,resourcesaswellasapproachesappliedintheSERfield.Recently,dedicateddatasetsandchallengesaimedatthedetectionofcognitivedisorders,especiallyAlzheimer’sdementiawerecreated.Forexample,theADReSSChallengeatIN-TERSPEECH20203introducedataskforevaluatingdifferentapproachestotheautomatedrecognitionofAlzheimer’sdementiafromspontaneousspeech.Thechallengeprovidedtheresearchercommunitywithabenchmarkspeechdatasetthathasbeenacousticallypre-processedandbalancedintermsofageandgender,definingtwocognitiveassessmenttasks:detectionoftheAlzheimer’sspeech,andtheneuropsychologicalscoreregressiontask(Luzetal.,2020).3.5TexttoSpeechGeneralintroductionThetaskofgeneratingspeechfromsomeothermodalityliketext,musclemovementinfor-mation,lip-reading,etc.iscalledspeechsynthesis.Inmostapplications,thetextisusedasthepreferredformfortheinputandthisparticularcaseiscalledtexttospeech(TTS)con-version.ThegoalofaTTSsystemistogeneratespeechfromwrittennaturallanguage.Intheidealcase,TTSsystemsshouldproducenaturalvoicesthatcancommunicateinacertainstyle,areabletoreflecttheaccent,moodandothercharacteristicsofthespeakerandareindistinguishablefromthoseofhumans.Manyfactorsaffectthequalityofthesyntheticvoices,suchasthesynthesistechniqueappliedandthesizeandqualityoftheavailablespeechdatabasesusedformodelcreation/adaptation.Machinelearningvs.symbolicmethodsCurrentTTSsystemsarebasedondeeplearning.NeuralnetworkshaveallbutreplacedtraditionalsynthesistechnologiessuchasHMM-basedstatisticalparametricsynthesisandconcatenativesynthesisachievingabettervoicequalitywhiledemandinglesspreparationoftrainingdata.Speechandthecorrespondingalignedtext(orphonetictranscription)isusuallytherequirementtoproperlytraintheseDNNbasedTTSsystems.DNNbasedsystemsaretypicallysplitintotwoparts–aneuralacousticmodelwhichgeneratesacousticfeaturesofthespeechgivenlinguisticfeatures,suchasgraphemesorphonemes,andaneuralaudiogenerationmodel(alsoknownasavocoder)whichgeneratesanaudiowaveformgiventheseacousticfeatures,e.g.,Mel-spectrogramframes.However,thetextisonlyabletodescribecertainaspectsofthecontentthatistobeproducedasspeech.Thussuchmodelsgenerallyproduceonlyneutralspeech.Recently,effortshavebeenmadetowardssynthesisingexpressivespeech.Thiscanbeachievedeitherbydirectlycontrollingtherhythm,pitch,andenergyofthespeech(Valleetal.,2020a;Renetal.,2020),orindirectlybypassinganembeddingcorrespondingtoacertainemotionalstyle(Wangetal.,2018).3http://www.interspeech2020.org/index.php?m=content&c=index&a=show&catid=315&id=755,acessed17.1.2022WP2:EuropeanLanguageEquality–TheFutureSituationin20309D2.14:TechnologyDeepDive–SpeechTechnologiesDataneedsVerylargecorporaareneededtoobtaingoodsyntheticvoiceswiththistechnology,oftenasbigashundredsofhoursofveryhigh-qualityrecordings.Bigcompanieshavetakentheleadinthedevelopmentofthiskindofsystem.Usingaround20hoursofcarefullyselectedspeechfromaprofessionalspeakerandrecordedinaprofessionalstudioisacommonrequirementfulfilledbymostofthecurrentcommercialsystems.Smallerresearchgroupsandcompanieshavedifficultiestocompetetakingintoaccountthequantityofgoodqualityspeechrecord-ingsrequired.Also,smalllanguages(inthesenseofcommercialinterestand/oranumberofspeakers)sufferfromthisissueasthemaincompaniesdeveloptheirsystemsalmostex-clusivelyformajorlanguagesandtherearenoavailablespeechcorporaoftherequiredsizetotraintheproposedarchitecturesfortheselanguages.Insteadofrecordingdedicateddatasets,audiobooksprovideaviablealternativeandcanbeusedasapotentialsourceofspeechdata.Audiobooksusuallycontainseveralhoursofhigh-qualityaudiofromasinglespeaker,thusadheringtotherequirementsofDNNbasedTTSsystemtraining.Couplingtheaudiowiththetextversionofthebook,e.g,withthehelpofforcedalignmentbyASR,cansignificantlydecreasetheeffortrequiredtoobtainaudiotranscriptions.However,suchanapproachrequiressomeadditionalprocessingsteps,suchasfilteringtheaudiofromanybackgroundnoises,textnormalisation,checkingtheaudioandtextforanydiscrepancies,etc.4SpeechTechnologies:CurrentStateoftheArtDeliverableD1.2ReportonthestateoftheartinLTandlanguage-centricAIprovidesanoverviewofpreviousandpresentapproachesforthetechnologiescoveredbythisdocu-ment.Inthefollowing,furtheraspectsanddevelopmentssincethecreationofthebefore-mentioneddocumentarediscussed.4.1AutomaticSpeechRecognitionSOTAofthecurrentmethodsandalgorithmsThetraditional(andbynowclassical)pipelineofASRconsistsofcomponentsforaudiopre-processing,anacousticmodel,apronunciationmodelaswellasalanguagemodeldefinedoverunitsofalexicon.Withinthescopeofasearchalgorithm,theseelementsarecombinedtoproducethemostlikelytranscriptgiventheinputaudio.Inthisscheme,modelsgenerallyareofagenerativekind(suchasGMMs,HMMsandn-grammodelsfortheLM)andoptimisedindividually.Thissetupwasconsideredstandardinthefirstdecadeofthiscentury.However,alreadystartingintheearly2000s,moreandmoreofthesecomponentswerebeingreplacedwithDNNs,hybridDNN-HMMs,LSTM-HMMsorRNNs.Thischangewasmadepossiblebyadvancesinalgorithmsandmodelsaswellasthemassiveincreaseinavailabletrainingdataandcomputingpower(inparticularofGPUs).Asaresult,WERscouldbere-ducedbymorethan50%inmanydomainsandlanguages(Schlüter,2019).However,theper-formanceofASRsystemsstillvariesdramaticallydependingonthedomainandlanguage,withlow-resourcelanguagesstillexhibitingWERsresemblingthoseofEnglishmanyyearsago.Forapplicationsinpractice(ASRintheWild),hybridsystemscombiningtraditionalele-mentssuchasHMMsandDNNsstilldominatethestateofplay.Assuch,theycanbere-gardedasstate-of-the-artoutsideofresearchlabs.ToolkitslikeKaldi(Poveyetal.,2011)WP2:EuropeanLanguageEquality–TheFutureSituationin203010D2.14:TechnologyDeepDive–SpeechTechnologiesprovideasoundbasisforthedevelopmentofsystemsforresearchaswellascommercialenvironments.KaldiiscurrentlyundergoingaredesignprocessandwillbenamedK24.TheinitialphasesoftheintroductionofNNsconcernedthepre-processing,e.g.,tandemfeaturesandbottleneckfeatures(Hermanskyetal.,2000;Grézletal.,2007)andAM,withmodelstakingintoaccountincreasinglylargercontext(recurrentmodelslikeRNNs,LSTMs,GRUs).ApproachessuchasLSTM(Sundermeyeretal.,2015)augmentedthisbyallowingnovelmannerstorepresenttheLM.Theintroductionofsequence-to-sequence(S2S)approachessuchasConnectionistTempo-ralClassification,CTC(Gravesetal.,2006),or“Listen,AttendandSpell”(Chanetal.,2015)tookthisprocesstotheextreme.Theyintroducedoneglobalmodelthatmapsacousticfea-turesdirectlytothetext.Thismodelisoptimisedwithonlyoneobjective–asopposedtobefore,wheredifferentsub-modelswereoptimisedindependentlyandusingdifferentob-jectives.Theseend-to-endmodelstypicallyconsistofanencoder(DNN)generatingadeepandrichrepresentationoftheinput(audio)followedbyadecoder(DNN)payingattention(Bahdanauetal.,2014)tothe(encoded)inputaswellasitsinternalstatesandthelastemittedoutputs.State-of-the-artapproachesusuallyutiliseRNNsandTransformers(Vaswanietal.,2017),thoughrecentresearchsuggestthatthelatterisbetter(Karitaetal.,2019;Zeyeretal.,2019).NovelresearchtriestoovercomesomeshortcomingsoftheTransformersforspeechbycom-biningthemwith,e.g.,convolutionalNNs(CNNs)(Dongetal.,2018;Gulatietal.,2020).Novelapproaches,suchasWav2Vec2.0byFacebook(Baevskietal.,2020)focusonleverag-ingvastamountsofunlabelledspeechdata.Inthisapproach,latentrepresentationsofaudioareproducedwhichrepresentspeechsoundssimilarto(sub-)phonemeswhicharethenfedintoaTransformernetwork.Theapproachhasbeenshowntooutperformothertypicalpathsofsemi-supervisedmethods,whilealsobeingconceptuallysimplertoimplementandexecute.Thepossibilitytoemploysmalleramountsoflabelleddataaswellasbeingabletotrainmultilingualmodelsprovidestrongargumentsforthisapproach.CurrenttrendsregardingtheSOTASeveraltrendsconcerningtheSOTAcanbediscernedandcanbeexpectedtoalsocontinueintheforeseeablefuture:•Manualconfigurationorcustomisationwillbeminimisedoreliminatedaltogether.•Severalstandard-evaluationsofASRexist,leadingtoasystematicpushinfrontiersandperformanceonacontinuousbasis.Existingevaluationsarelikelytobecomplementedbyfurther,morecomplexsetups(alsonon-Englishand/ormultilingual!).•AstextisabundantandLMscanbetrainedfromtext-only,theincorporationofstrongLMs(tobiasNNs)willremainanactivetopicofresearch.Shallowanddeepfusion(Leetal.,2021)toblenddifferentmodels,suchasspecialisedLMsandgenericLMs,providecurrentapproachesaddressingthisproblem•TheintegrationoffurtherknowledgesourcesintoE2Esystems.•Reinforcementlearninghasgainedpopularityinanumberofareas.TheadoptionalsoforcertaintaskswithinASRispending.•Alotofattentionhasbeenpaidtosinglemicrophonesettings(see,e.g.,(Kandaetal.,2021a,b)forexamplesofrecentworksonE2Emulti-speakerASRformeetingtranscrip-tion).Multi-speaker,multi-channel,multi-microphonesetupsmayprovidefurtheran-glesandleadtoimprovements.4https://www.kaldi.dev/industry_overview.htmlWP2:EuropeanLanguageEquality–TheFutureSituationin203011D2.14:TechnologyDeepDive–SpeechTechnologies•ThecombinationofASRwithfurtherNLPtechnologies(suchasMT)inasinglemodelmayproduceevenmorepowerfulcombined,E2Emodels.•Asmodelcomplexityhasbecomeprohibitiveinmanycasesforallbutthemostpotentparticipants,thetrade-offbetweensystem-complexityandperformanceisapromisingtargetareaforfuturework.•Amulti-passapproachtorecognition,asitwaspopularinthepre-DNNdaysmayseearevival,duetoworkonfusionandcombinationwithfurtherknowledgesources.•Language-agnosticormultilingualmodels,cross-lingualandmulti-lingualtrainingandmodels(alsointheguiseof“co-training”ofmodels)arereceivingincreasedattention.•FurtherinfluencefromotherfieldssuchasMT,visionandML,ingeneral,maycarryovertothefieldofASR,asallofthesefieldstendtosharemethodsandmodelsinanincreasingmanner(e.g.,theadoptionofCNNsfromvisiontoASR).•Novelmannerstodefinetheunitsoftextualelementsforvocabularydesignemerge,tomitigateout-of-vocabulary(OOV)5effectse.g.,viabyte-pair-encoding(BPE)andtheinclusionofsinglecharacters(Sennrichetal.,2016).•Hyper-parametertuningmayreceiveanincreaseininterest,whichmaycurrentlybelowduetoprohibitivecostsformanyparticipantsintheASRmarket.•Furtheradvancesinsearchalgorithmsmayemergeasmethodslikebeam-searchingdonotguaranteeoptimalresults.Datausevs.otherresourcesThescarcityoftrainingdata(aligneddataofaudioandtext)isawell-knownproblemformostlanguages.WhereasforcommerciallyimportantlanguagessuchasEnglishorMan-darinChineseanabundanceofdataisavailable,thisisnotthecaseformanyotherlan-guages.WhilecompanieslikeGoogletrainmodelson125.000hofspeech,witharesultantmodelsizeofupto87GB,thisisunthinkableduetolackofdataaswellasresourcesformostotheractors.Severaltrendscanbeobserved:•Theincreaseduseofpre-trainedmodelsandfine-tuning/adaptation.Severalplatforms(likeHuggingface6)provideagrowingsetofpre-trainedmodelsforavarietyoflan-guagesanddomains.•Workondataaugmentationandpoolingofresourcesisreceivingmoreattention.Forexample,thereissomeongoingworkinevaluatingthebestdataaugmentationandpoolingmethods,andtheireffectonASRperformance.ThishasbeendoneextensivelyforMaltesespeechdata,whereonlyaround7hoursofhigh-qualitytranscribedspeechdataisavailable(whichisarguablylowevenforfine-tuningasystemsuchasWav2Vec).Thishasbeendocumentedextensivelyandcouldserveasaguidetoothersimilaref-fortsforotherlanguages(Menaetal.,2021).Infact,anabsoluteworderrorratereduc-tionof15%isreported,justthroughcarefulaugmentationalone–andwithoutthehelpofalanguagemodel.•Co-trainingofmodels:thecombinationoftrainingdataforseveral,relatedlanguagesanddomainstocreatemulti-language,multi-domainmodels(orbase-modelsforfine-tuning).5OOVsarewordsthatoccurintheaudiobutwhichdonotformpartofthevocabulary6https://huggingface.coWP2:EuropeanLanguageEquality–TheFutureSituationin203012D2.14:TechnologyDeepDive–SpeechTechnologies•Multilingualmodels:thecreationoftrulymulti-lingualorlanguage-agnosticmodels.•Variousmethodstoaddresstheout-of-vocabularyproblem.Wordsmaythusbedecom-posedintosmallerunits(e.g.,morphs),theymaybereconstructedfromintermediatesearchresults(byextendinglattices)orre-trainingofmodelsmaybecarriedouttoincludecurrentvocabulary.•Ageingofmodels:modelsarefrequentlyoutdatedoncetheyaredeployedandneedtobere-trainedcontinuously.Throughthis,theshiftinlanguagemayalsobeaddressed(inadditiontoshiftsintopics).•Weakly-andsemi-supervisedtraining:Thereisalsoastronginterestinweakly-orsemi-supervisedtrainingmethods,thatenabletheapplicationofandun-transcribedandun-annotateddataforASRtraining.Insemi-supervisedtraining,aseriesofmod-elsaretrainedwhereagivenmodelintheseriesservesasateachertothesucceed-ingmodelbygeneratinglabelsontheunlabelleddataset.Thestudenttothisteachermodelistrainedonthedatasetobtainedbycombiningthesupervisedsetwiththeteacher-labelleddataset.Thisideahasbeenshowntoworkandprovidegoodimprove-mentsinrecognitionqualityinmultipleresearchpapers,bothinlow-resourceandhigh-resourcescenarios(Wallingtonetal.,2021;Zhangetal.,2020;Synnaeveetal.,2019).Accuracies,measuresused,humanvs.autoevaluationThemainmeasuresofdeterminingtheperformanceofASRsystemsareWordErrorRate(WER),CharacterErrorRate(CER)andPhonemeErrorRate(PER).WERisderivedfromtheLevenshteindistance,performingadynamicalignmentofreferenceandoutputatthewordlevel.WER,thoughmostlystandard,doesnotalwayscorrelatewiththequalityofspeechrecognitionsystems,assomeword-levelerrorscanbequalitativelymoreacceptablethanothers.WERdoesnottakeintoaccountthisqualitativedifferenceandtreatsallwordsequal–whichformanypurposes,suchasinformationretrieval,isclearlynotoptimal.CERissimilartoWERbuttypicallyappliestolanguagesusingcharacter-basedscripts.PERlooksatanerrorratewhichconsistsofthenumberofallphonemeerrors.GiventhenatureofsomeoftheASRarchitectures,frequentlyutilisingaflavourofConnectionistTemporalClassification(CTC),thisisanincreasinglyimportantperformancemetric.Intermsofperformanceovern-bestresultsorlattices,measuressuchasprecision,recallandtheirharmonicmean,theF1-measurearecommonplace.Downstreamtaskaccuracy,efficiency,thresholdsTypically,ASRoutputsunstructuredandnormalisedtextwithoutanypunctuationmarks.Thisisnotanissueinuse-cases,wheretheuserinputisshortandconcise,e.g.,whenaskingaquestiontoavirtualassistant.However,whengeneratingtranscriptsforlongerspeech,itiscrucialtorestoringpunctuationmarkstoimprovereadabilityandprovidestructuretothetranscript.Moreover,punctuationmarksareoftenusedinfurtherdownstreamtaskssuchasNER,POStaggingandMT.ASRsystemscanintroduceerrorsthatastandardMTsystemhasnotseenduringtrainingandthuscannothandle.Insuchinstances,thetranslationqualitymaysuffer,eventoanextentwherethetranslationsareeffectivelyincomprehensible(Ruizetal.,2019).WP2:EuropeanLanguageEquality–TheFutureSituationin203013D2.14:TechnologyDeepDive–SpeechTechnologies4.2SpeakerRecognitionSOTAofcurrentmethodsandalgorithmsAsforlanguagerecognition,state-of-the-artSRsystemsuseneuralnetworkstoextractarep-resentation(usuallyreferredtoasanembedding)forthespeakerinanutterance.Theinputtothenetworkisusuallygivenbyfeaturesextractedfromframesof20-30ms,althoughtherearealsoongoingeffortstotaketherawwaveformasinputtothenetwork.Embeddingsarethencomparedwithabackendinordertodecidewhethertheyarefromthesamepersonornot.TypicalneuralnetworkarchitecturesforembeddingextractionareTDNN,ResNet,LSTMandversionsthereof.ThestandardchoiceofbackendisProbabilisticLinearDiscriminantAnalysis(PLDA)whichisagenerativemodel.Intherecentfewyears,usingcosinesimilarityplusanaffinetransformationhaveproventogiveacompetitiveperformance,especiallyforaudiowitha16kHzsamplingrate.Anadvantageofgenerativebackendshoweveris,thatscoringwithdifferentnumbersofenrolmentutterancesbecomestrivial.Inadditiontovari-ationsoftheembeddingextractorarchitecture,manyrecentresearcheffortshavefocusedonthetrainingobjective.Ifthetaskathandisverification,themostintuitivemannerwouldbetotraintheextractorforthistask.However,inpractice,itoftenworksbettertotraintheextractorforclassification.Thatis,foratrainingutterancethenetworkshouldclassifywhoamongthespeakersinthetrainingsetspeaksintheutterance.Accuracies,measuresused,humanvs.autoevaluationTheevaluationmetricinSRdependsonthetaskstudied.ThemostcommonSRtaskinaca-demicresearchisspeakerverification.Duetothemanyevaluationsinthistask,thereisalargeconsensusonwhichmetricstouse.Twotypesoferrorscanoccur:falseaccept(FA)torecognisethespeakerintheenrolment-andtestutteranceasbeingthesamewhentheyaredifferent,andfalsereject(FR)torecognisethespeakerintheenrolmentandtestutter-ancesasbeingdifferentalthoughtheyareidentical.Itisimportanttodistinguishwhetheranevaluationmetriciscalibrationsensitiveorcalibrationinsensitive.Putsimply,calibrationsensitivemetricscareaboutwhetherthedecisionthresholdiscorrectlyspecifiedwhereascalibrationinsensitivemetricsdonot.ThemostcommonmetricsareEqualerrorrate(EER),detectioncostfunction(DCF)andlog-likelihoodratiocost(CLLR).TheEERisdefinedastheerrorratewhenthethresholdisadjustedsothatFAandFRareequal.Thusthismetricdisregardsforthethresholdusedbythesystemandaccordinglyisacalibrationinsensitivemetric.Thedetectioncostfunctionisbasedonuser-specifiedcostsofFAandFRaswellasthepriorprobabilityforthespeakersintheenrolandtestutterancesbeingidentical.Forthedetectioncostfunction,thereisbothacalibrationsensitive(actualDCF)forwhichthesystem’sthresholdisusedandacalibrationinsensitivevariant(minimumDCF)forwhichthethresholdthatminimisedtheDCFonthetestsetisused.Finally,CLLR,isdesignedtobeanapplication-independentevaluationmetric.ItcanbeviewedasanaverageofDCFs.Theperformanceofspeakerverificationsystemsvariesgreatlyinparticulardependingonthedurationoftheutterancebutalsoontheacousticconditionssuchasthenoiselevelandsamplingrate.Mismatchinthelanguagesspokeninthetrainingdataandtestdatamayalsodegradetheperformance.For16kHzdatawithlownoiseconditions,afewsecondsofspeechisusuallysufficienttoproduceanequalerrorrateofaround1%.For8kHztelephonedatainnoisyenvironmentsandwithamismatchintrainingandtestinglanguages,EERcanbe5-10%orevenworse.WP2:EuropeanLanguageEquality–TheFutureSituationin203014D2.14:TechnologyDeepDive–SpeechTechnologies4.3LanguageIdentificationState-of-the-artLIDsystemsare,similarlytoSRsystems,basedonDNNs(e.g.,TDNNorResNet)thatingestsequencesofframe-levelspeechfeaturesasinput,aftersomeprocessingapplyapoolingmechanismtothesefeaturestoobtainanutterancelevelrepresentation,andthenfinallytrytoclassifytheutterancelevelrepresentations.Intraining,thiswholechainistrainedinanE2Efashion.Intesting,eitherthetrainedDNNisuseddirectlyforclassifica-tionor,theutterancelevelrepresentationscanbeextractedandusedinasimplebackendforclassification,e.g.,aGaussianlinearclassifier.Severaltrendscanbeidentifiedandbeexpectedtocontinue:•Theuseoffine-tuningonASRmodelssuchasWav2VectoextractembeddingsforLID(thisalsoappliestoSID/Emotion-ID).•Theperformanceofthesesystemsisrapidlyoutperformingmorespecificmethodse.g.,thei-VectorapproachestoLID/SIDhavenowbeenmostlysuperseded.Morerecently,reportsshowthesametendencyforemotiondetection.•Parallelattemptsarestillverymuchbeinggivenimportance–e.g.,i-VectormethodswithSVMs,LSTM-DNN,attention-basedandResNet-basedclassifiers.•Severalsimilarcategoriese.g.,SID,LID,Accent-ID,emotiondetectionandotherspeaker-profiletypeclassificationsfallunderthesamefamilyoftechniques.•Accesstocorporaisgenerallynotproblematic,giventhelanguage-independentnatureofthedevelopedalgorithms.•Theperformance(standardmetricssuchasF1scores)correlateswiththelengthofutteranceundertest.Theshortertheutterance,themoredifficultthetask.•Asinothersub-fieldsofspeechprocessingchallengesliketheVoxCelebSpeakerRecog-nitionChallenge20217andtheShort-durationSpeakerVerification2021(SdSV)8havebeenpropagatedtoboostthedevelopmentoftechnologies.•ThetaskofSpeakerDiarisation–segmentingaudiocontainingmultiplespeakers(and/orspeakingconditions)iscloselyrelatedtothisfield.Performancemeasuresfordiarisa-tionincludetheDiarisationErrorRate(DER)andtheJaccardErrorRate(JER).4.4Assessmentofemotions,cognitiveconditionsandpersonalitytraitsDuetothewidescopeanddifferentdisciplinescontributingtothefield,nosinglestate-of-the-artcaneasilybedescribedwhichwouldaddressthecompletefield.Wecanthereforeprovideanoverviewofselectedaspectsonly.Evaluationtypicallytakesplaceinaqualitativemanner(i.e.,byhuman-ratersandinter-rater-agreement)andwithdatasets,whicharespecifictotheparticulartask.EffortssuchastheComputationalParalinguisticsChallenge9aimtointroducefurthertasksonayearlybasis.7https://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2021.html8https://sdsvc.github.io9http://www.compare.openaudio.euWP2:EuropeanLanguageEquality–TheFutureSituationin203015D2.14:TechnologyDeepDive–SpeechTechnologiesSpeechEmotionRecognitionInSpeechEmotionRecognition(SER),awiderangeofmethodshavebeenusedtoextractemotionsfromsignals.SimilartootherSTfields,DeepLearningisrapidlybecomingamethodofchoiceandseveralE2Emodelshaverecentlybeenproposed(Tangetal.,2018;Kumaretal.,2021).However,intheSERfield,unlikeinASR,despitethefactthatthereareSERsystemsandrealisationsofreal-timeemotionrecognition,thesehavenotyetbecomepartofoureverydaylives.Toachievethisgoal,SERsystemsrequiremoreaccuratelylabelleddatatoimprovetrainingaccuracy,morepowerfulhardwaretospeedupprocessing,andmorepow-erfulalgorithmstoimprovetherecognitionrates.Inaddition,furtherinsightsfromfieldssuchaspsychologyorneurologymayberequired.ExamplesoftheSOTAmethodsappliedforthedetectionoftheemotionfeaturestorecog-niseemotionspeechincludetheapplicationoftwoCNNandLSTMnetworkstolearnlo-calandglobalemotion-relatedfeaturesfromspeechandlog-melspectrogramrespectively(Zhaoetal.,2019).Theresultsdemonstratedthatthecombinationofnetworksachieveex-cellentperformanceonthetaskofrecognisingspeechemotion,outperformingtraditionalapproaches,suchasDBNandCNN.InanotherSOTAapproach,adual-levelmodelthatpredictsemotionsbasedonbothMFCCfeaturesandmel-spectrogramsproducedfromrawaudiosignalswasexplored.Inthisap-proach,eachutterancewaspreprocessedintoMFCCfeaturesandtwomel-spectrogramsatdifferenttime-frequencyresolutions.AstandardLSTMwasappliedtoprocesstheMFCCfeatures,whileanovelLSTMarchitecture,denotedasDual-SequenceLSTM(DS-LSTM),pro-cessedthetwomel-spectrogramssimultaneously.Theproposedmodelsurpassedthestate-of-the-art(2019)unimodalmodels(Wangetal.,2020).Adifferentlineofwork,motivatedbythechallengesinthedevelopmentofrobustSERsys-temsrelatedtothescarcityofemotiondatasets,amulti-tasklearningframeworkthatusesauxiliarytasksforwhichdataisabundantlyavailablewasproposedby(Latifetal.,2020).TheapproachexploredthebenefitsoftheuseofadditionaldatatoimprovetheprimarytaskofSERforwhichonlylimitedlabelleddatawasavailable.Specifically,genderidentificationsandspeakerrecognitionweretargetedasauxiliarytasks,whichallowedtheuseofverylargedatasets.Tomaximisethebenefitofmulti-tasklearning,AdversarialAutoencoders(AAE)wereusedwithintheframeworkalongwiththeunsupervisedAAEincombinationwiththesupervisedclassificationnetworks.Theproposedsemi-supervisedlearninghelpedtoim-provethegeneralisationoftheframeworkandledtoimprovementsinSERperformance,demonstratedforcategoricalanddimensionalemotionrecognitionaswellascross-corpusscenarios.Withthegrowingpopularityofambientintelligencetechnologythatusesavarietyoflow-power,resource-constraineddevices,thedevelopmentofmethodsthateffectivelyusecom-putationalresourceshasgainedtheincreasinginterestoftheresearchcommunity.Amongothers,theseincludeapplicationsinhealthandelderlycaretechnologies,whereinterven-tionscanbetriggeredbythedetectionofemotionalstates.Examplesofrecent,SOTAap-proachestoSERinsuchsettingsincludeHaideretal.(2021).Thestudydemonstratedthatsimilarorbetteraccuracycouldbeachievedwithsubsetsoffeaturessubstantiallysmallerthantheentirefeatureset.AnotherrecentworkinthislineofresearchdescribesalightweightSERmodelusingaCNNapproachtolearnthedeepfrequencyfeaturesbyusingaplainrectangularfilterwithamodifiedpoolingstrategy(Anvarjonetal.,2020).Theproposedmodeloutperformedthestate-of-the-artwhileloweringthecomputationalcosts.DevelopmentsinIoTandedgecomputinghavealsomotivatedresearchinwhichthecom-pactspeechrecognitionnetworkwithspatio-temporalfeaturesforedgecomputing,EdgeRNNwasdescribed(Yangetal.,2020).ItusesCNNtoprocesstheoverallspatialinformation,RNNtoprocessthetemporalinformationandasimplifiedattentionmechanismtoenhanceWP2:EuropeanLanguageEquality–TheFutureSituationin203016D2.14:TechnologyDeepDive–SpeechTechnologiestheportionofthenetworkthatcontributestothefinalidentification.OnRaspberryPi3B+(notably,asmalllowresourcecomputer),themethodimprovedboth,speechemotionandkeywordsrecognition.Cognitivedisorders,healthconditionsandpersonalitytraitsDetectingthecognitivestatesandreactionsofauserisasteptowardsdesigningproactivesystemscapableofadaptingtotheuser’sneeds,preferencesandabilities.AsotherrelatedST-fields,thedetectionofpersonalitytraits,mooddisorders,signsofdepressionandothermedicalconditionshavefoundtheirapplicationinrecentyears.Oneofthefirstsignsofneurodegenerativedisordersisdeteriorationinlanguageandspeechproduction.Inrecentyears,techniquesbasedonautomaticprocessingofthevoicesignalhavebeenusedforlanguageandcognitiveassessments.Theseapproachesprovidethemeansforquantifyingsignalpropertiesrelevantforthedetectionofspecificpatholo-gies.DuetothedevelopmentofautomaticmethodsfacilitatingtheevolvingcontrolofawidepopulationsufferingfromAD,anumberofindustryapplicationsaimedatthedetec-tionofneurodegenerativedisorders,developedbycompaniessuchasIBMWatson10,Cantab–CambridgeCognition11orWinterlightsLab12wereintroduced.TheSOTAapproachesappliedintheADdetectionfromspeechincludemethodsthatcom-binetheautomaticallyextractedacousticmarkersfromspontaneousspeechwithsemanticlinguisticfeatures.Inthetaskfocusedonthedetectionofsubjectsfrompatientsandthehealthycontrolgroup,andindistinguishingADpatientsfromthosewithmildcognitiveim-pairment,theaccuracyofthepresentedapproachwasinarangeof80-86%,andcorrespond-ingF1valuesbetween78-86%(Gosztolyaetal.,2019).ThedetailedpresentationoftheSOTAintheseandrelevantsubfieldsextendsbeyondthescopeofthisreport(forthereviewoftherecentworksfocusedonthedetectionofADfromspeech,see(Pulidoetal.,2020;delaFuenteGarciaetal.,2020)).FurtherSOTAworksintherelevantsubfieldsincludethedetectionofmild-cognitiveim-pairments(Tóthetal.,2018),assessmentofcognitiveimpairmentinelderlypeople(Konigetal.,2018;Schulleretal.,2021),theapplicationofrepresentationtransferlearningfromdeepE2Espeechrecognitionnetworksforthedetectionofspeakerintoxication(Sertollietal.,2021),detectionofCOVID-19fromspeechsignalusingbio-inspiredbasedcepstralfeatures(Dashetal.,2021),mooddisorders(Huangetal.,2019),signsofdepression(Totoetal.,2021)andthedetectionofpersonalitytraitsfromspeech(Guidietal.,2019;Leeetal.,2021).4.5TexttoSpeechGeneralintroductionNeuralnetworkshavegreatlyimpactedthespeechsynthesisfieldbyimprovingthequalityandnaturalnessofsyntheticvoiceswithrespecttothetraditionalsystems.Anothercontri-butionmadebyneuralnetworksisthepossibilityoftraininganddesigningthesystemsinanE2Efashion.Whiletraditionalmulti-stagepipelinesarecomplexandrequireextensivedomainexpertise,E2Esystemsreducethecomplexitybyextractingtheaudiodirectlyfromtheinputtextwithoutrequiringseparatedmodels.AlthoughE2ETTSsystemshaveshownexcellentresultsintermsofaudioqualityandnaturalness,therearestillsomeissuestobefaced.Ontheonehand,thesesystemsusuallysufferfromlowtrainingefficiency,requiringalargesetofaudiorecordingstogetherwiththecorrespondingtexttotrainproperly.On10https://www.ibm.com/blogs/research/2020/10/ai-predict-alzheimers/,accessed17.1.202211https://www.cambridgecognition.com/news/entry/speech-recognition-to-improve-clinical-trial-efficiency,accessed17.1.202212https://www.veritone.com/press-releases/voice-analysis-detects-alzheimers-disease/,accessed17.1.2022WP2:EuropeanLanguageEquality–TheFutureSituationin203017D2.14:TechnologyDeepDive–SpeechTechnologiestheotherhand,synthesisedspeechisusuallynotrobust,duetoalignmentfailuresbetweeninputtextandspeechduringthegeneration.SOTAofthecurrentmethodsandalgorithmsLately,themostfavouredapproachtospeechsynthesissystemsistosubstitutethewholechainintheTTSsystemwithDNNs(Ningetal.,2019).DeepVoice(Arıketal.,2017)wasthefirstsystemwhereallthestepsintheTTSprocesswereimplementedbymeansofDNNs.ThequalityofthegeneratedvoiceswasinferiortothatobtainedwithWaveNet(vandenOordetal.,2016),soseveralimprovementswereproposed,suchasDeepVoice2(Gibianskyetal.,2017)and3(Pingetal.,2018b),whereWaveNetcouldbeusedasaneuralvocodertoanalyseandsynthesisetheacousticsignal.AnotherapproachthatcanbeconsideredmoreE2EisChar2Wav(Soteloetal.,2017),althoughitstillconcatenatestwomodules:thefirstpredictacousticparametersfromtextandthesecond,aneuralvocoder,generatesawaveformfromtheseparameters.FullE2Earchitectureshavealsobeenproposed,includingTacotron(Wangetal.,2017),Tacotron2(Shenetal.,2018),FastSpeech(Renetal.,2019a),FastSpeech2(Renetal.,2020)andClariNet(Pingetal.,2018a).Thesesystemsareabletoproducespectrogramsfromtext,applyinganencoder-decoderarchitecturethatproducesalatentrepresentationoftheinputtext(orphonetictranscription)thatissubsequentlytransformedviaconvolu-tionalneuralnetworksassociatedwithattentionmechanismsintospectrograms,whicharethenconvertedintospeechusingtheGriffin-Limalgorithm(GriffinandLim,1984),WaveNetorotherneuralvocoderssuchasWaveGlow(Prengeretal.,2019),WaveRNN(Kalchbren-neretal.,2018)andMelGAN(Kumaretal.,2019).Thesystemsprovideoutstandingresultsintermsofthequalityofthegeneratedvoicesbutrequirelargeamountsofhigh-qualityrecordingstobetrainedproperly.Currently,effortsarebeingmadetodeploythesesystemsforlow-resourcelanguagesbyimprovingdataefficiency(Chungetal.,2019),applyingtrans-ferlearning(Chenetal.,2019)ortrainingmultilingualmodels(Zhangetal.,2019c).Otherareasofintenseresearchactivityarestyletransfer(Zhangetal.,2019a;Lietal.,2021),newefficientneuralvocoders(chunHsuandyiLee,2020;Pauletal.,2020;Jangetal.,2021)andspeakeradaptationwithareducedamountofdata(Xinetal.,2021;Maniatietal.,2021).Regardingexpressivespeechsynthesis,GlobalStyleTokens(Wangetal.,2018)canbenamedasoneofthemostcommonapproaches.Itconsistsofareferenceencoder,whichencodesthereferencespeechMel-spectrogram,andastyletokenlayer,whichlearnsdiffer-entprosodicaspectsinasetoftrainableembeddings.Thereferenceembeddingiscomparedwitheachstyletokenwiththehelpofasequence-to-sequencemulti-headattentionmodule,formingaweightedsumofthestyletokenscalledstyleembedding.Thestyleembeddingisthenconcatenatedtothetextencoderoutput,thusconditioningtheMel-spectrogramsynthe-sisonbothtextandencodedprosodyofthespeech.OthermethodsincludeFlowtron(Valleetal.,2020b),whichusesagenerativeflow-basedmodelforlearninginvertibletransforma-tionsfromdatatoacontrolledlatentspacethatcanbesampledduringinferencetoachievethedesiredprosody.Mellotron(Valleetal.,2020a),Fastspeech2(Renetal.,2020),andCtrl-P(Mohanetal.,2021)controlprosodybyconcatenatingthetextencoderoutputwithmoretraditionalacousticfeatures,suchasF0contourorenergy.Datausevs.otherresourcesDevelopinghigh-qualitysyntheticvoiceswithDNNbasedtechniquesrequireslargeamountsofgoodqualityrecordingsfromonesinglespeaker.Thisrequirementisoftendifficulttoful-fil,especiallyforminoritylanguagesanddialectalspeech.Thegenerationofnewsyntheticvoicesisalsohinderedbythisextensivedatarequirement.Effortsarebeingmadetosharedataamonglanguagesandspeakersinordertotrainthecommonaspectsmorerobustly.WP2:EuropeanLanguageEquality–TheFutureSituationin203018D2.14:TechnologyDeepDive–SpeechTechnologiesMulti-speakerandmulti-languagemodellingisausualstrategyinDNNbasedTTSsynthesistoachieveimprovedvoicequalitywithareducedamountofdatafromasinglespeaker(Yangetal.,2021;Casanovaetal.,2021;Shangetal.,2021).Thequalityofthesevoiceshoweverisnotyetcomparabletotheoneobtainedwithlargedatabases.Accuracies,measuresused,humanvs.autoevaluationThemostpopularmeasureofqualityinTTSistheMeanOpinionScore(MOS)wherepeopleexpresstheiropinionaboutseveralaspectsofthesyntheticutterancesona1to5scale(Gold-stein,1995;Rec,2006).Considerabletimeandeffortmustbedevotedtothedevelopmentofthissubjectiveevaluation,asalargenumberofindividualsisneededtoreliablyratetheTTSsystems.AlthoughMOStestsarestillthemostfrequentlyusedoptiontoassessTTS,theyhavebeencriticisedastheyofferonlyageneralmeasureoftheoverallqualityandmaynotbesuitableforevaluatinglongsyntheticspeechpassages(Clarketal.,2019b;Wagneretal.,2019).Moreover,theyareoftenproducedusingtoofewevaluatorstobereliable(Westeretal.,2015).OtherTTSperformancemeasuresfocusonintelligibility.ThemainstrategyforevaluatingthisaspectistoaskpeopletotranscribesemanticallyunpredictablesentencesandmeasuretheWERofthetranscriptions(Benoîtetal.,1996).Asthisevaluationalsocallsfortheparticipationofhumanevaluatorsandthisisatime-consumingprocess,ASRisincreas-inglybeingusedtoevaluateintelligibilityTaylorandRichmond(2021).Newdimensionstobeevaluatedarealsoarisingsuchasmeasuringlisteningeffortwhilelisteningtoasyntheticspeechbymeansofpupilometry(Simantirakietal.,2018)andelectroencephalographyandmeasuringcognitiveloadrelatedtotheprocessoflisteningtothiskindofspeech(GovenderandKing,2018).(BotinhaoandKing,2021)proposeamethodforautomaticerrordetectionandanalysisbasedontheattentionalignmentbetweentheencoderanddecoder.Theattentionalignmentforacorrectlysynthesisedspeechshouldbeuninterruptedandmonotonic.Anydeviationsorartefactsinthealignmentcanindicatethatthemodelfailedtocorrectlysynthesisetheaudio.5SpeechTechnologies:MainGaps5.1BackgroundandoverviewofthemainchallengesWhilespeechtechnologieshavefoundtheirwayintoaseriesofapplicationfields,severalimportantissueshavenotbeenaddressedthoroughlyandremainactiveareasofresearch.Inthefollowing,weoverviewthemaingapsinSTandpresenttheminawidercontextoftheglobalandregionalbusinessactivities,requirementsrelatedtotheavailabilityofqualifiedpersonnel,privacyandtrustconcerns,aswellastechnicalandend-userperspectives.EffectsofscaleBeyondtheprogressmadewithinacademicinstitutions,suchastheUniversityofToronto,theUniversityofCambridge,JohnsHopkins,RWTHandmanymore,muchoftheadvancesmadeduringthepastdecadehasbeendrivenbytheresearchlabsofcompaniessuchasGoogle,Facebook,Apple,Amazon,andtheirChinesecounterparts.Understandably,thedrivingfactorsbehindtheactivitiesofthesecompaniesistogeneratebusiness–andnottoperformfundamentalresearch.Hence,advancesaremotivatedbyacommercialper-spectiveandthussomeofthemarenotsharedastheyprovidemarketadvantagesoverthecompetition.WP2:EuropeanLanguageEquality–TheFutureSituationin203019D2.14:TechnologyDeepDive–SpeechTechnologiesTheshiftofactorscoincideswiththeriseofmassiveprogressinML,basedonaccesstohuge,andpreviouslyunthinkable,amountsofdataandprocessingpower.Itisnosurprisethatthecompanieswiththelargestpoolsofdataandthemostextensiveinfrastructurearenowtheleadingactorsintheirrespectivefields,leavingonlynichemarketsanddomainstosmaller,buthighlyspecialisedplayers.Theseniches,ofcourse,mayalsoprovideampleopportunitiesforsuccessiftargetedproperly.Asoutlinedabove,atrendtowardsincreasinglycomplexE2EsystemscanbeobservedforallsectorsofST.Duetotheextremedemandonresourcessuchasdata,computingpower,energy,infrastructure,thegenericconstructionofsuchmodelsisinmanycaseslimitedtoahandfulofactors.Theactivitiestomakepre-trainedmodelsavailablefortransferlearningandfine-tuningsettingsandthustoallowotherstoalsoparticipatefrommajoradvancesarecertainlybeneficial.However,theextentofthistransferandthelevelofcontrolinthehandsofafewinstitutionsposesaseriousrisktootheractors,tothemarketandpotentiallyeventoinnovationintheASRsectorasawhole.Moreover,commercialinterestmaypromptinstitutionstonotmaketheirbest-performingmodelsavailablebutratheronlymorelimited,smallerversionsofthesemodels.FurtherissuesrelatetotheinterestandcapabilitiesofEuropeanentitiesvis-a-vistoded-icatetheresourcesrequiredfordevelopingstate-of-the-artSTsystems.WhencomparedtotheresourcesallocatedbytheGAFAandtheirChinesecounterparts,(forexampleGoogleallegedlyhasmorethan250peopleworkingonASRalone,trainingmodelsonmorethan125.000hofspeech,withresultantmodel-sizesofupto87GB)theresourcesavailabletoEu-ropeancompaniesandinstitutionsareverylimited.Asimilarsituationexistsonthehard-wareside:companieslikeNVIDIAdominatetheGPUmarket,andteamupwithMicrosofttotrainthelargestlanguagemodels,whilethereisnorealisticEuropeancounterpartinsight.Lackingthenecessaryfundingenvironment(venturecapitalaswellasmindset)aEuro-peanstrategycannotcompeteonthesametermsbutratherhastoinvestigateandfollowinnovativepathsthatrequirefewerresources.Thismayalsobepromisingwithregardtosustainabilitygoals.TrainedpersonnelandexpertiseAfurthergap,concerningallareasofspeechprocessing,canbeidentifiedinthescarcityoftrainedpersonnelandexpertiseaswellastheriskoflosingemergingtalenttoinnova-tivepower-playersoutsideofEurope(withpossibilitiesandsalarieswhichcangenerallynotbematchedbyEuropeanplayers).Eveninlightofthedemocratisationoftechnologyandauto-ML,allowingamuchbroaderaudiencetocreatemodelsanddeploytheseforuse,re-spectiveeducationalprogramsinspeech(andNLP)technologiesformthefoundationforfutureEuropeansuccessintheseareasandmayhinderitifnotappropriatelyestablishedandstrengthened.PrivacyandtrustDataleaksandscandalsinrecentyearshavespurredtheinterestonpartofindividualsaswellasofpolicy-makers.Concernshavearisenregardingtrust,privacy,intrusion,eaves-dropping,orthehiddencollectionanduseofdata.Theseconcernshavebeenrecognisedbymanyactorsbutareonlyaddressedtoalimitedamount(clearlyso,aslongastheycounter-actcommercialinterests).Furtherworkandinvestigationintothesetopicsmaybebeneficialcommercially,academicallyaswellasforpolicy-making.Inaddition,processingforSTincommercialcontextsoftenreliesoncloud-basedinfrastructureswithfew(ifany)guaran-teesregardinghowdatastoredinthecloudiseventuallyusedorwillbeusedinthefuturebyserviceproviders.WP2:EuropeanLanguageEquality–TheFutureSituationin203020D2.14:TechnologyDeepDive–SpeechTechnologiesTechnicalperspectivesOnatechnicallevel,thefocusintheASRsubfieldonratherconstrainedconditionshasleftgapsinmorediversesettingssuchas:distantspeechrecognitioninsteadofsinglemicro-phones;noisyenvironments;accentedspeech,non-nativespeech,dialectalspeechandsoci-olinguisticfactorsaffectingspeech;spontaneous,unplannedspeech;emotionalspeech(in-cludingspeechduringstressfulordangeroussituations)andconnectedaspectsconcerningsentimentsexpressed(empathy);theintegrationofspeechtechnologiesintocollaborativeenvironments,multiple,simultaneousspeakersengagedindiscussions;aswellastheinte-grationofparalinguisticaspectsandtechnologiesaddressingthem.Alloftheseissueswar-rantfutureattentionandresearch.ModernTTSsystemscanproducehigh-qualityspeechprovidedtheyaretrainedwithasufficientamountofgoodqualitydata.However,theyareusuallypreparedtosynthesiseisolatedsentencesastheyarebuiltusingonlythiskindofrecordings.Therefore,whentryingtosynthesiseparagraphs,speechfordialogueoraudiobooks,thegeneratedspeechismuchlessexpressiveandnatural(Cambreetal.,2020).ThisissuelimitsthepracticalapplicationofE2ETTSsystemsandtheiradoptionbythepublic.Whilemostresearchfocusesonasingleuser’sinteractions,speechtechnologiesembod-iedinvirtualassistantsarebecomingincreasinglypopularinsocialspaces.Thishighlightsagapinourunderstandingoftheopportunitiesandconstraintsuniquetomultipleusersce-narios.Theseincludedetectingifusersaddressthesystemorotherparticipants,speakerdiarization(seeParketal.(2022)forareviewofrecentadvancesinspeakerdiarizationwithdeeplearningmethods),understandingaspectsofsocialdynamics,andfindinginteractionbarriersaresomeofthefactorsthatrestricttheusefulnessofvoiceinterfacesingroupset-tings.Theconnectiontothefieldofdigitalhumanitiesandcomputationalsocialsciencesisnotyetfirmlyestablishedbutitcouldbebeneficialtosetupcollaborativelinkswitharangeofdisciplinesanddomainsworkingwithspokendatainthedomainofsocialsciencesandhumanities(SSH).Inparticular,theinsightsandrequirementsstemmingfromtheneedsfortranscriptionworkflowsandaudiominingtoolsofcommunitiesproducingand(re)usingoralhistorydataandinterviewrecordingsmayhelpidentifygapsinlanguageresourcesformodeltraininganddomainadaptation(Draxleretal.,2020).Integrationwithmethodologyfortheautomatedannotationofspokeninterviewdatawithparalinguisticfeaturesisgain-ingattention(seeAkbarietal.(2021)fortheroleofsilence),andcanwidenthebasisforusecasesinmultidisciplinarysetting,includingthestudyofmentalhealthconditionsandtherapeuticinterventions(Catalaetal.,2020).Itcouldbebeneficialtoidentifyanyunbal-anceinlanguage-specificsupportfortherecognition,annotationandretrievalofthetypesofstructuredconversationalspeechthatareusedininterviewsettings,bothinSSHandbe-yond.Expertisefromthehumanitiescanalsoproviderelevantinsightsforaddressingthechallengesinthedigitalarchivingofinterviewdata(FPessanhaandSalah,2022).Theincreaseinmodellingpowerandperformanceachievedoverthelastyearsalsocomeswithsomedrawbacksandchallenges.Theseincludeaneedforevenmoredataofalignedtext/audiopairs,respectivelyalackofinterestandworkonthecreationofnewparadigmsusingfewerdata.Currentapproachesincludeshallowanddeepfusion,butthequestionofhowtooptimallycombineLMsandDNNstructureshasstillnotbeenaddressedcomprehen-sively.Modelsrequiringthecompleteinputsequenceforprocessingdonotmatchwellwithrequirementstoperformcausalprocessing.Severalattemptstoenablecausalprocessingarebeingexplored,amongthemtheuseofneuraltransducersrunningprocessingatregu-larintervals.Theextentofcontextmayalsoincuradditionalprocessingcostswhichneedtobebalancedandmitigated.Modelsarenottransparentandthushardtointerpret.Thisispartlyduetothefactthatpreviouslyindividualcomponentshavebeencombinedintosinglemodels.Thecomplexprocessofhyper-parametertuningisoftentooresource-intensiveandthushasnotbeenWP2:EuropeanLanguageEquality–TheFutureSituationin203021D2.14:TechnologyDeepDive–SpeechTechnologiesaddressedinmanyinstances.Elementsofinput/outputlikebyte-pair-encodings(BPE)havebeensuggestedbutthesecontradicttheideaofgenuineE2Eprocessingasthisdecisionistakenbeforehandandoutsideofthemodelitself.Integrationofseveralcomponentsintoonemodelpromptsthequestionofwhetherfur-therdownstreamtechnologies,relyingonASRoutputtoperformvariousNLPtaskswillalsobecomepartofsuchintegratedmodels.Thecombinationinturnraisesquestionsabouttheinterpretabilityandtransparencyofsuchblackboxsystemsaswellasconcerningthemodal-itiesfortheintegrationoffurtherknowledgesources.End-usersperspectiveOverall,speechtechnologieshavemadealeapingettingadoptedinmanycommercialset-tings,witheasyaccessibilityoftechnologiesandpowerfulmodelsforcommerciallyattrac-tivelanguages.EspeciallytheproliferationofintelligentVoiceAssistants(VAs)hasmadespeechacommonmodeofinteractionforawiderangeofusers.Whileprovidingseveralusefulfeatures,issueslimitingthefurtheradoptionandwidespreaduseofspeechtechnolo-gieshavebeenidentified.Concerningtheusers’perspective,amongothers,theseincludeproblemsinaccuratelyrecognisingaccentedspeech(Cowanetal.,2017),alackoftrustinVAstoexecutemorecomplexorsociallysensitivetasks(Porcheronetal.,2018),andconcernsrelatedtoprivacyaswellas(clandestine)datacollectionanditsuse(Clarketal.,2019a;Am-marietal.,2019).Thisissueisfurtherexacerbatedbythefactthatsystemsoftenoperate“inthecloud”ratherthanon-premise.ManyVAsmaynowbeutilisedinlanguagesotherthanEnglish,butcoverageandsup-portedfunctionalityvarygreatly.Thegapsinthesupportofdifferentlanguagescreatebar-riersforuserswhoseprimarylanguageisnotfullysupported,orsupportedonlytoalimitedextent,forcingthemtocommunicateinanon-nativelanguageorriskbeingexcludedfromusingtheevermorepopularsystemsandservicesbasedonspeechtechnologies.Thereby,non-nativeusersarepushedtodevelopdifferentstrategiesandmodesofinteraction,includ-ingareducedleveloflanguageproductionininteractionandmorefrequentuseofvisualfeedback(Wuetal.,2020).5.2Data:alignment,labelling,anonymisation,diversityAsoutlinedabove,themainchallenge/gaprelatedtodataconcernsitsavailability–ofade-quatedatasetsforlow-resourcelanguages,ofanappropriateamountandquality.Variouseffortsaimtomitigatethisfactbyfocusingontransferlearningandfine-tuningofmodels.However,whereasthisapproachiscertainlybeneficial,itgenerallydoesnotyieldmodelsofequalperformance(asforlanguagesexhibitinglargeamountsoftrainingdata).Forafew,commerciallyhighlyinterestinglanguages,anabundanceoftrainingdata(corporawithalignedaudioandtranscripts)isavailable.However,formany(themajority)oflanguages,thisisnotthecaseandonlycorporawhichareminusculeincomparisontoEnglishareavail-able.NotonlydoesthislackofaligneddatameanthattheresultingperformanceofSTwillbesubstantiallyworsethanforEnglish,iteffectivelyexcludescertainapproachesfrombeingapplied–asthesedependexactlyontheavailabilityoflargeamountsoftrainingdata.Withregardtothetextualcontentsrequired(e.g.,forLMtraining),thesituationismorebalanced.However,certainlanguagesanddialectsdonothaveonedefinedwayofspellingnoradequateamountsoftextualdataduetolowlevelsofgeneraldigitalisation.Inaddi-tion,certainmarketsaredominatedbyindividualplayerswithcontrolovertheresourcesrequiredbypotentialcompetitorstobuildmodels.Thisstrategytoprotectone’sownmarketfurtherhindersprogressanddevelopmentforspecificregionsandlanguages.Asaconse-quenceandduetothehighcostofvoicedatacollectionandlabelling,currentvoiceinterac-WP2:EuropeanLanguageEquality–TheFutureSituationin203022D2.14:TechnologyDeepDive–SpeechTechnologiestiontechnologieshaveastrongbiasinfavouroflanguageswithawideruserbase(suchasEnglish),thuspotentiallyexcludingmanyusers.ComparedtoASR,obtainingtrainingdataforspeakerrecognitionandlanguageidentifica-tiondisplaydifferentchallenges.InthecaseofSIDandLID,thesituationismorefavourablesincetheonlyannotationneededistheidentityofthespeakerorlanguage.Ontheotherhand,inthecaseofSIDthisannotationcannotbecreatedbysimplylisteningtotheutter-ancebecausehumansarenotgoodenoughtorecognisespeakersbytheirvoice.Further,itiscrucialthatthetrainingdataforSIDandLIDcontainmanyrecordingsofthesamespeakerorofthesamelanguage,whereasforASRtrainingdataitispreferabletohaveasmany(differ-ent)speakersaspossible.Whiletherehasbeenmuchprogressincollectingdatafromvideosontheinternet,progressontelephonydataisstilllimitedbylackofdata,inparticularforlesscommonlanguages.Activitiesandliteratureregardingthedetectionofemotionsfromaudioinless-resourcedlanguageareverylimited.Forexample,neitherdatasetsnorwell-recognisedresearchonthetopicexistsfortheLatvianlanguage.AlthoughtherearesomepublicdatabasesavailabletotrainDNNbasedTTSsystems,theseareingeneralonlyusefulforbuildingmonolingualneutralvoicesinareducednumberofmajorlanguages(ParkandMulc,2019;Zenetal.,2019).TheavailabilityofopendatafreeofrestrictionssuchascopyrightandlimitationsduetoGDPRregulationsintheremainingmajorlanguagesandallminoritylanguageswouldallowthedevelopmentofTTSsystemsfortheselanguagestoo.Inaddition,databaseswithmoreexpressiveandspontaneousrecord-ingsareneededtobeabletobuildTTSsystemssuitableformoreemotion-demandingappli-cationslikeaudiobookreading,moviedubbingandhuman-computerinteractionthataimstobesimilartointeractionsbetweenhumans.Moreover,thevastmajorityofdatasetscorre-spondtoadultvoicesandthereisalackofdatatogeneratechildandelderlyvoices.Takingintoaccountthatthevoiceisanimportantcomponentofouridentity,morediversedatasetsareneededinordertogeneratepersonalisedvoicesthatcansuitanyuser.ThediversityofcontextsandspeakersrepresentedbypopularASRbenchmarkslikeLib-rispeech(Panayotovetal.,2015;Garnerinetal.,2021)(readspeech),andSwitchboard(God-freyetal.,1992)(spontaneousspeech)islimited.Recentworksattempttoaddressthisprob-lembyintroducingbenchmarksthatmimicreal-worldsettings,withthegoalofdetectingmodelbiasesandflaws(Riviereetal.,2021).Theresultsobtainedonthissetshowthatwhilecontemporarymodelsdonotappeartohaveagenderbias,theyoftenrevealsignif-icantperformancedifferencesbyaccent,andmuchgreaterdifferencesdependingonthesocio-economicbackgroundofthespeakers.Whentestedonconversationalspeech,allmod-elsexhibitasignificantperformancedrop,andevenalanguagemodeltrainedonadatasetaslargeasCommonCrawldoesnotappeartohaveasignificantpositiveeffect,highlightingtheimportanceofdevelopingconversationallanguagemodels.OtherrecentworksinthisareadiscussthenextgenerationofASRbenchmarksandframeworksdesignedtodescribeinteractionsbetweenlinguisticvariationandASRperformancemetrics(Aksënovaetal.,2021).Amongothers,AppleandGoogleutilisedistributedandanonymisedlearninge.g.,privacy-orientedfederatedlearning.Forexample,inthemethodologyappliedbyGoogle,voicequeriesarekeptforalimitedperiodoftimeforcontinuoussemi-supervisedlearning.Anassistantquerylike’Whatisthetallestbuildingintheworld?’returnsareplyandlinkstoaWikiarticle.Ifauserclicksonthearticle,itisanindicationthatthequestionwasun-derstoodcorrectly.Are-querymeansthattheASRsystemwaswrong.Thesesoftlabelsareusedforfurthertraining,spanningmorevoices/accentsandawiderarrayofcontexts.WP2:EuropeanLanguageEquality–TheFutureSituationin203023D2.14:TechnologyDeepDive–SpeechTechnologies5.3Accuracy:reachingusablethresholdsforapplicationsThesinglemostfrequentlymentionedhinderingfactorforthebroadadoptionofspeechtechnologyisonethathasbeenmentionedforthepast40years:accuracy.Theperceivedaccuracyanditsexactmeaninghavechangeddramatically–fromindividualwordsbeingmis-recognisedtointentionsthatarenotcorrectlyinterpretedincomplexsituations,withaccuracyreachingwellbeyondtheactualaccuracyofASRonly,regardingitinamorecom-prehensiveandembeddedmanner.WhereasWERasanevaluationmeasurehashaditsmeritstomeasureprogressinASR(andstilldoesso),morecomprehensiveapproachestomeasuringtheimpactofASRperformanceondownstreamtasksandactualdeploymentsmayrequirenovelapproaches.WERaloneclearlydoesnotprovidethefullpicturewhenitcomestotheperceivedperformanceandusabilityofcompletesystemscomprisingseveralkindsofspeechandlanguagetechnologies.WERstillprovidesthestandardmeasurefortheevaluationofASRsystems.However,asnotedabove,itfallsshortofcapturingcertainqualitativeaspectsoflanguage.Dependingonthetaskanduseofdownstreamtechnology,WERsmaynothavetobeextremelylowandstillallowtheapplicationofASRwithinaparticularfield(itdoesnothavetobeperfecttomakeperfectsense).PerformingevaluationsalsobeyondpureWERsmaythenbehelpfulinsuchinstances.Currentapplicationsofspeechprocessing,especiallyincludingsmart-homesystemsthatmakeuseofspeechinterfaces,areheavilybiasedtowardsmajorspeech-technologyenabledlanguagese.g.,English,Mandarin.InferiorperformancemayrenderthemlessusableandlesspopularinEurope.OneissuethataffectscurrentTTSsystemsisthelackofrobustnessinthesynthesisedspeech:someinputsentencesmayleadtoskippingorrepeatingwordsortobabbling,es-peciallywhenthekindofsentencesseeninthetrainingisverydifferentfromtheonessyn-thesised(Heetal.,2019).Thisproblemmostlyoccursinattention-basedsystems,wheretheoutputframesarerelatedtospecificpartsoftheinputsentencebymeansofanattentionmechanismrelatingandaligningtextandvoice(Zhuetal.,2019;Renetal.,2019b).Inordertoaddressthisproblem,differentapproacheshavebeentaken,someofthemfocusedonde-signingmorerobustattentionmechanisms(Heetal.,2019;Battenbergetal.,2020),othersin-cludingalignmentinformationattheinputofthesystem Zhuetal.(2019).Someresearchershaveproposedtosubstitutetheattentionmechanismwithnetworksthatcanpredictthees-timateddurationoftheinputphonemes(Shenetal.,2020;Yuetal.,2020).However,theproblemhasnotbeensolvedcompletelyyetandkeepshinderingthepracticalapplicationofTTSsystemsinmanyinstances.Speakerrecognitiontechnologieshavealreadyreachedacceptableperformanceformanyapplications.Inmanysituations,itmaybeacceptableifthesystemdoesnottakeanyde-cisionimmediatelywhenitisnotconfidentenough.Suchsituationscanthenbetreatedaccordinglyinanapplication.Forexample,indialoguesystems,onecouldwaitforthede-cisionuntilmorespeechisavailable.However,thisdoesnotmeanthatthereisnoneedoropportunityforfurtherresearch.Allapplicationsofspeakerrecognitionwouldbenefitfrombetterperformanceofthecoresystemaswellasbetterrobustnesstoacousticconditions,ut-terancedurationandothervariablesthatoccurinspeechdata.Likewise,regardingtheexpressivenessofTTSsystems,ampleroomforimprovementre-mains.Modellingprosodywiththehelpoflearnedlatentembeddings,suchasGlobalStyleTokens,allowssynthesisingspeechinaparticularemotionalstyle,whichcanbedifficulttodefinebyexplicitacousticfeatures,suchasF0,duration,andenergy.However,theseembed-dingsareoftenineffective,entangled,anddifficulttointerpret.Effortsaremadetoimproveembeddingrobustnessandefficiency,forexample,(Daietal.,2021)proposeaddingastyleembeddingdown-samplingandup-samplinglayer,inordertoreduceoverfittingtowardstrainingdataandforcethemodeltofocusonmoregeneralprosodyfeatures.WP2:EuropeanLanguageEquality–TheFutureSituationin203024D2.14:TechnologyDeepDive–SpeechTechnologies5.4DialectalspeechandmultilingualtrainingMostTTSsystemsproducespeechinthemainvarietyoflanguages.Todate,littleattentionhasbeendevotedtosynthesisingdialectalspeechwiththelatesttechnology.AttemptstomultilingualTTShavebeenmade,usingmultilingualspeakersifavailable(Maitietal.,2020)andmorecommonlyusingmonolingualdatasetsrecordedbydifferentspeakersandthenapplyingvoiceconversiontogeneratesyntheticsignalsinseverallanguageswiththesamevoice(Zhangetal.,2019b;NachmaniandWolf,2019).Thequalityofthevoicesgeneratedwiththesetechniquesisstillworsethantheoneobtainedusingmonolingualdatabases.TobetrulymultilingualtheTTSsystemshouldalsobeabletocopewithcodeswitchedtextandalthoughsomeeffortshavebeenmadeinthisregard(Caoetal.,2019;Zhouetal.,2020),thereisstillroomforimprovement.ContrarytootherSTsuchasASR,aspeakerrecognitionsystemcanbeusedinlanguagesdifferentfromtheonethatitwasoriginallytrainedfor.Theperformanceofthesystemmayhoweverdeteriorateinthiscase.Someprogresshasbeenmadetomakesystemsmorelanguage-independentforexamplebymultilingualtrainingorbyadversarialadaptation.However,theeffectivenessofthisisnotwellunderstoodforlanguagesthataretoodifferentfromthelanguagesusedintraining.5.5ExplainabilityandtransparencyforcriticalmethodsandtechnologiesWhileinthelastdecadeSTresearchhasmademuchprogressintermsofperformanceofthesystemsaswellasinapplicationsofthetechnologies,progressintermsofunderstandingoftheusedarchitectures(whysomearchitecturesworkbetterthanothersetc.)aswellasthenatureofthedataandtask(forexampletounderstandtowhatextentitispossibletoobtaindomaininvariantrepresentations)hasbeenmuchmorelimited.Thisispartlyduetothefactthattheneuralnetworksusedinmodernsystemsarehardertounderstandthanthegener-ativemodels(GMMs,i-vectors,etc.)ofthepreviousgenerationspeakerrecognitionsystems.Butpartlyitisalsoduetoalackofinterestfromtheindustryandfundingagenciestosupportthattypeofresearch.Studentsarealsogenerallyinclinedtoworkontopicsthatmainlyaimatimprovingperformancesincethisincreasestheirchancesofobtainingawell-paidjobintheindustryaftergraduation.Historically,agoodunderstandingofthemethodshasbeencrucialfortechnologicalbreakthroughsthough,e.g.,forthetransitionintosubspace-basedmethodsforSRsuchasJFA(JointFactorAnalysis)andi-vectors.Itispossiblethatamoretrialanderrorbasedresearchmethodologywhichiscurrentlypopularisindeedthemosteffectivefortheverycomplexmodelsthatarecurrentlystateoftheart.Technologyadoptersandend-userspromptingformoreinsightintothecapabilitiesofsystemsandthegenerationofresults–potentiallywantingtointerveneinthisprocessorinfluenceit–maywarrantfurtherresearch.e.g.,forASR,thismayconcerntheinventoryofrecognisableunits,forLIDtheinventoryoflanguagesandlanguagevarietieswhichcanbeprocessed.Inallcases,insightsintohowaparticularresultwasreachedmaybebeneficialforexplanatorypurposes.6SpeechTechnologies:ContributiontoDigitalLanguageEqualityandImpactonSocietyPurelytechnologicalsystemsalonedonotexist–theyarealwaysembeddedinasocialcon-textandshouldthusalwaysratherbeviewedassocio-technicalsystems.TheapplicationsofSThavediverseandmultifacetedimpactsonseveralkeyaspectsforsocieties.ImprovedWP2:EuropeanLanguageEquality–TheFutureSituationin203025D2.14:TechnologyDeepDive–SpeechTechnologiestechnologiesreachingperformancelevelsresemblingthoseofhumansmayinmanyaspectsleadtoahumanisationoftechnology,ascribinghumanattributestosystembehaviour.Pat-ternsofhuman-to-humaninteractionmaybeappliedtohuman-machineinteractionleadingtoheightenedexpectationsandsubsequentdisillusion.6.1DigitallanguageinequalitiesTheunbalancedavailabilityandqualityofSTresources,(e.g.,data-sets,annotations,models)stronglyimpacttheperformanceofSTfordifferentgroupsoflanguages.ThislackofparityinSTresourcesfordifferentlanguagestranslatesdirectlytodigitallanguageinequalities.Forlanguagessupportedtoa(much)lesserextent,performanceandaccuracyaretypicallysignificantlylowercomparedtoresource-richlanguages.Inextremecases,selectedfunc-tionalitiesand/orsupportforminorlanguagesmaynotbeavailableatall.Inadditiontothesupportofalanguageperse,languagevarieties,dialectsoraccentsmaynotbesupportedoronlysupportedonverylimitedlevels.STarethusnotaccessiblenoravailabletoeveryoneonanequallevel,i.e.,functions,performance,robustnessmaybedramaticallydifferentfromcasetocase.WhilenewadvancesinSTcontributetothereductionofthisdivisionbetweenresource-richandresource-poorlanguages,thelackofcommercialinterestinthelongtailofsmalllanguages13translatestoasignificantlyslowerpaceofSTimprovementsandcommercialadoptionforthelattergroupoflanguages.Fornativespeakersoftheselanguages,theseimbalancesleadtowiderusageofthebetter-supported,majorlanguages,suchasEnglish,French,GermanorSpanish.Motivatingspeakerstousethesemajorlanguagesmorefrequentlycreatesanewsetofchallengesrelatedtohandlingaccentedandnon-nativespeech.Comparedtothelevelofserviceandthesupportprovidedfornativespeakers,thisresultsinlowerperformance,weakenedexperienceandreducedusabilityforthisgroupofspeakers,renderingSTlessusefulorevenuselessintheextremecase.ForTTS,thelimitedperformancemaytranslateintosynthesisedspeechwhichisnotperceivedasnaturalnorpleasantandconsequentlyleadstoloweracceptanceandadoption.Inthelongerterm,childrenwhoaremoreexposedandflexibleintermsofadoptionofnewtechnologiesmayendupspeakingmoreofaforeignlanguageoramixoftheirnativelanguage(s)andamajorlanguage,causingissuesofsocialfrictionwithparentsandrelativeswhomaynotpossessthesamecommandofthatlanguageorwhomaynotbeabletounder-standitatall.Generationalissuesmayfurtherarisebythefactthatadultsmaybesomewhatlimitedintheirwillingness,opennessorcapabilitiestoadoptnewtechnologies,includingST,whereasyoungergenerationsmaybemuchmoreflexibletotheadoptionandexplorationofnewtypesofvoice-basedinteraction.Whenitcomestopeers,thewidespreadadoptionofST,includingvoiceassistantsmayalsoinfluencehowusersmightcommunicatewithandaddresseachother–potentiallythismayresultincommunicationstylesmoresimilartoissuingcommandstodevicesandsmart-speakersathome.Further,(children’s)personificationofvoiceassistantsandsmartdevicesmaybecomeadouble-edgedsword:itassistsparentsinencouragingyoungchildrentousethedevicesforknowledgeseekingandlearning,butitcanalsofrustratethemaschildrencandevelopanat-tachmenttothedevicesandcometorelyonthetechnologytoanextentthatcommunicationwithparentsandpeersisreduceddramatically(GargandSengupta,2020).Thisraisesthefollowingquestions:•Willthecommerciallyimportantlanguagesstayaheadofthemajorityoflanguagesalsointhelongrun?13LanguagesspokenbyarelativelysmallnumberofspeakersWP2:EuropeanLanguageEquality–TheFutureSituationin203026D2.14:TechnologyDeepDive–SpeechTechnologies•Whateffectwillthishaveonspeakersofsuchsmaller(lessfrequentlyspoken)lan-guages?•Willalackofcommercialinterestinsuch“smalllanguages”,alsotranslatetoalackofimprovementsandinnovationintheselanguages?•WilltheimbalancebetweenlanguagesupportmotivatespeakerstouseEnglish(oran-otherlargelanguage)moreoftenasthismightprovideabetterexperienceinstead(butatthecostofnon-nativelanguageuse)?•Willthedigitalfootprintofminorlanguagesbereducedtoaminimumandeventuallybemarginalised?6.2Biases,fairnessandethicalissuesThedevelopment,applicationandadoptionofSTarealsoconnectedtoarangeofissuesrelatingtofairness,biases,ethicalandlegalaspectsthathavetobeaccountedforandtobeproperlyaddressedtosupportadoption.Astechnologiesareenteringthehomesandofficesofusersonabroadscale,anenhancedlevelofattentiontoprivacyconcerns,ethicsandpolicyisessential.Policymakers,policywatchdogs,themediaandconsumersalikeneedtoassumetheroleofgatekeeperstotheintroductionofSTintomanycornersofsociety.Trustisviewedasthemaincurrencyandkeytotheadoptionandacceptanceoftechnologiesaswellastotheperceptionofmarketparticipantsandtheirroleinthisprocess.ScandalsandopaquebehaviouronpartofSTprovidersmayhavedetrimentaleffects.Voiceassistantsfrequentlyutilisefemalevoices.Someofthemofferthepossibilityofusingmalevoices,butthedefaultvoiceisusuallyfemale.Thisfacthasbeenextensivelycriticisedasitcancontributetotheoutdatedviewofwomenasthegenderthatmusthelpandtakecareofothers.Moreover,nowadaysthegenerationofgender-neutralvoicesisgainingim-portance,asmanypeopledonotidentifythemselveswiththeclassicbinarygenders.MoreeffortisneededinthedevelopmentofmodernTTSsystemstoincludegender-sensitiveprac-ticesandoptionsforadaptation.Similartogender-relatedbiases,race-relatedbiasesmaybepresentinmanykindsofSTmodels.Duetothefactthatmodelsdependontheamountandcompositionoftrainingdata,ethicalaspectsoflanguageandlanguageusepresentinthesedatamayalsobepresentintheresultingmodels.Systemscapableofself-learningmayadaptintodirectionscompletelyunplannedandundesiredbythedevelopersorbegamed(attacked)byusersintodoingso14.Duetotheseinherentconditions,systemsmaysubsequentlyperformatdifferentlevelsofaccuracyforparticularsectionsofthepopulation.Furthermore,disabilitiesrelatedtolan-guageproductionmaynotbeaccountedforandexcludesectionsofthepopulationfromusingSTsystemsatall.Speakerrecognitionsystemsareusuallylessaccurateforfemalevoicesthanformales.Thisisnotbecausewomenareunderrepresentedinthetrainingdatabutmorelikelyduetothepropertiesoffemaleandmalevoices.Variousethnicgroupsmayhoweverbeunder-representedinthetrainingdataandthuslessaccuratelyrecognised.Itshouldalsobenotedthatbeinginthegroupforwhichthesystemperformsworsecanbeeitheranadvantageoradisadvantagedependingontheapplicationandthetypeoferrorthesystemtendstocommit14AfterMicrosoft’sreleaseofitschatbotTayin2016,thechatbotbegantopostracist,sexually-charged,inflam-matoryandoffensivetweetspromptingMicrosofttoshutdowntheserviceagainwithin16hoursofitslaunch(https://en.wikipedia.org/wiki/Tay_(bot)).Allegedlythisbehaviourwasprovokedbyusersgamingtheservice.Thisepisodealsopromptquestionsaboutproperevaluationandtestingofsuch(self-learning)servicesbeforereleasingthemonalargescale.WP2:EuropeanLanguageEquality–TheFutureSituationin203027D2.14:TechnologyDeepDive–SpeechTechnologiesmoreoften(falsepositivesorfalsenegatives).AnotherethicalconcernpertainingtoSRarepossibleprivacybreachesthroughmasssurveillanceofphonecalls.CurrentDNNbasedTTSsystemshavereachedaqualitylevelandadegreeofsimilaritywiththevoiceofrealpeoplethatcouldbeusedtogeneratedeepfakevoices.Manyofthepossibleapplicationsofhigh-qualityvoicesindistinguishablefromthoseofhumansareposi-tiveandpeoplewithspeechdisorders,visualimpairmentandotherdisabilitiescouldgreatlybenefitfromthem.However,speechdeepfakescouldalsobeusedasatoolforillegalactivi-tiessuchascommittingfraudordiscreditingpeople.Newregulationsandthedevelopmentofad-hoclegislationiscriticaltomitigatingthisperniciouseffectoftheTTStechnology.Somenewtoolsabletodetectspeechdeepfakesmustbeproduced,andanti-spoofingtechniquesthatdiscriminatesynthesisedfromnaturalspeechmustbedevelopedinclosecollaborationwithteamsworkinginTTS.6.3UserswithspecialneedsWhilestate-of-the-artASRsystemsachievegreataccuracyontypicalspeech,theyperformpoorlyondisorderedspeechandotheratypicalspeechpatterns.PersonalisationofASRmod-els,acommonlyappliedsolutiontothisproblem,isusuallyperformedonservers.Thatinturnposesproblemsrelatedtodataprivacy,delayedmodel-updatecycles,andcommuni-cationcostforcopyingdataandmodelsbetweenserversandmobiledevices.Whileon-devicepersonalisationofASRrecentlyshowedpromising,preliminaryresultsinahomeautomationdomainforuserswithdisorderedspeech(Tomaneketal.,2021),moreresearchisrequiredtofurtherincreasetheASRperformanceforthesegroupsofusersandprovidesupportforopenconversationswithlongerphrases.TTStechnologieshaveawiderangeofapplications,someofthemofgreatsocialimpact.TTSisconsideredassistivetechnologyandassuch,itmaycontributetotheintegrationofpeoplewithvisualimpairmentsandlearningdisabilitieslikedyslexia.Bydevelopingrobustsystemscapableofreadinganytextfromanysourceincludingbooks,websitesandsocialmedia,thesepeoplewouldbeabletoenjoythesameadvantagesasanypersonwithoutadisability.Italsofacilitatesequalaccesstoeducationforpeoplewithvisualandlearningdisabilitiesaswellasforforeignerswhomaystrugglewiththelanguage.Thistechnologymayhelpthesestudentsusecomputersintheclassroomastherestoftheirfellowstudents.Inaddition,itcancontributetotheintegrationofimmigrantsbymakingiteasierthelearningofthelocallanguageasTTSallowslisteningtowordsandsentenceswhenreadingthem.Inthissamelineofapplications,TTScanhelppeoplewithliteracyissuesandpre-literatechildrenlearntospeakforthefirsttimeaccessinganycontentpresentedinwrittenform.Finally,TTSmayprovehelpfulintimesofageingpopulationswithdegradingeyesight.AnothercontributionofTTStosocietyrelatestoorallyimpairedpeople.Voiceisanessen-tialcomponentofouridentitythatweusuallytakeforgranted.However,losingitcanaffecthowothersperceiveusaswellasourownsenseofwhoweare.Wecommunicatewithotherpeoplemainlythroughourvoicesthathelpusmakesocialconnections.TTStechnologyisabletoprovideavoiceforthosewhohavelosttheirown.Syntheticvoicescanbeperson-alisedsotheysuitthecharacteristicsdesiredbyeachuser,byapplyingspeakeradaptationtechniques.Evengeneratingsyntheticvoicesthatcanreproducethesoundofthevoicethepersonhadbeforetheylostitispossible,providedrecordingsareavailable.Thiswayindi-vidualscanspeakwithsyntheticvoicesthatmatchtheirpersonalityandcharacterinsteadofusingthestandardvoicesprovidedbydefaultbycompanies.Inanothervein,STcanmakeourliveseasierasitallowsustoreceiveinformationwhileoureyesareengagedinotheractivitiesthanreading.Thankstothistechnology,wecanaccessinformationonthegobymeansofsoundorwhenweareinvolvedinphysicalinter-action,e.g.,work,sports.Integratedwithvirtualassistants,TTSsystemsareabletoprovideWP2:EuropeanLanguageEquality–TheFutureSituationin203028D2.14:TechnologyDeepDive–SpeechTechnologiessupporttoelderlypeople,assistingthemwithremindersofappointmentsandmedicationneeds,providingthemaccesstoonlineinformationandimprovingboththeirabilitytolivebythemselvesandstrengthentheirautonomy.Eventually,thistechnologycanalsobenefitanyindividuallivingalonebyallowingthemtohaveconversationsandbeingakindofsocialcompanion,helpingtoreduceloneliness.InclusionVoicetechnologiesandsubsequentautomationandmultiplicationofservicescouldbebene-ficialforunderrepresentedminoritiesfromaninclusionperspective.Partsofthepopulationmaynothaveaccesstosmartdevicesornotbemedia-literate(seebelow).Technologiesmaynotexistforparticularlanguagesordialectsornotfunctionatthesamelevelofperformance.Languageconveyedbymeansotherthanaudio–signlanguages–maybeatadisadvantageandtechnicallyrequiredifferentprocessingchannels(visualprocessing).Forspeechoutput,powerfulTTStechnologyreadytobeusedinmanylanguages(anylanguage)andequippedwithefficientinterfacesisimperativetoachieveaninclusivesocietywhereeverybodyhasequalaccesstoinformation,educationandcommunication.Inalloftheabovecases,situationsmaybenefitfromadvancesinSTandNLPtechnologies(suchasmechanismsnotrequiringhugeamountsofannotatedtrainingdata)butequally,needtobeconsideredonthepolicyandsocietallevel.Media-literacyAshuman-computerinteractionisbeingfacilitatedbytheuseofvoice,avastportionofonlinesearchesisalreadybeingperformedviavoice.Theomnipresenceofhand-heldde-vicesandsmartphonespairedwiththepresenceoftheInternetaspartofdailyroutineshascreatedan“informationatyourfingertips”world,whereinformationisameresearch(type/click)orvoice-commandaway.Willtheincreaseduseofvoicetechnologies,inpartic-ularforsearch,acceleratethis“don’tneedtoknowbecauseIcanquery(typicallygoogle)it”attitude?Istheinformationsoobtainedreliableandcanitbetrusted?(becauseapresum-ablynear-perfecttechnologyproducesit).Andwhateffectwillthishaveonmedialiteracyinthemid-andlong-term?Aclosewatchneedstobekeptonsucheffectsonmedialiteracyandmoreresearchdirectedtowardsthesephenomena.PoliticsandDemocracyIthasbeenpointedoutthatlanguagestronglyinfluencesthemannerwethinkandargueaboutpoliticalissuesandtopics.Languagecausesmentalframestobeactivatedandformourportfolioofideas(Wehling,2018).Politiciansandinfluencershavelongdiscoveredthesemechanismsandareapplyingthemactivelyonadailybasistopushtheirrespectiveagen-das.Havingthiscentralandimmediateeffectoncognitivemechanisms,linguisticpluralityalsoformsthebasisofcognitivepluralityandassuchplaysafundamentalroleinsecur-ingdiverseanddemocraticvalues.Limitationtoafewindividuallanguages–suchasmayhappenduetolimiteddigitalsupportforcertainlanguages–impoverishesandreducesthisvariety,theflexibilityandspectrumforexpressionofthoughtsand(political)ideas.RegionaldifferencesSpeechtechnologieswillprobablyexhibitthehighestimpactintheAPACregion.Thisislargelyduetopopulation-andeconomicgrowthaswellasthefactthatcharacter-basedWP2:EuropeanLanguageEquality–TheFutureSituationin203029D2.14:TechnologyDeepDive–SpeechTechnologiesscriptsandkeyboardsarenotanoptimalcombinationforinteraction.Inaddition,thepen-etrationofsmartdevicesisexpectedtoincreaseevenfurtherinthisregion.Ontheotherendofthegrowthspectrum,Africacanbeidentified,astheregionwiththelowestimpactofST.Willsuchdevelopmentsbroadenthedigitaldivideevenmore?ThedesignofSTtypicallyreflectsthesocialandeconomicbackgroundoftheexpertsbehindtheircreationanddesign.Potentially,thismayresultinalargegapbetweenintendedusabilityandactualadoptioninthefieldconcerningcertainregions(e.g.,partsofAfricaandAsia).6.4BusinessesandeffectsofscaleSpeechtechnologieshaveasignificantimpactinthewidercontextofeconomicalandbusi-nessactivities.Amongothers,theseincludethedifferencesinthelegalandfinancialframe-worksinwhichtheglobaltechnologycompaniesoperate,andtowhichtheycanflexiblyad-justdependingonthemostsuitablesetofconditionsofferedinthedifferentcountries.Thesefavoursorevenenablespecifictypesofactivitiesrelatedtodatacollection,itsprocessinganduse,thatcaneitherbemoredifficult,cost-intenseornotpossibleatallintheotherregions.Inthisscope,thefar-reachingconsequencesoftheregionaldifferenceinthedevelopmentofcompetitivebusinessenvironmentscannotbeoverlookedastheysignificantlyimpactsoci-etyatlarge.Byinfluencingthepaceofeconomical,technologicalandsocietaldevelopmenttheycreateopportunities,effectsofscaleandinfluencethedecisionsoftheindividualsandenterprisesabouttheregionsinwhichtheyengage,investandoperate.BearinginmindtheMatthewEffect(Rigney,2010)thequestionremainsifthecurrentdominanceofahandfulofsuper-actorswillincreaseevenfurtherinthefuture.Andifso,ifacertainkindofmonopolyofspeechtechnologieswillensue.AfurthereconomicalaspectconcernstheimpactofSTonautomationandasaconse-quenceonthejobmarketasawhole.Astechnologiessuchaschatbotsarebeingadoptedinpursuitofefficiency,theyalsoperformanincreasingnumberoftaskspreviouslyreservedforhumans.STandAIthusblurtheboundarybetweenhumansandtechnologyleadingtoshiftsinjobsandentireindustries.Clearly,amessageofcooperationandsupportratherthanofrivalryandreplacementneedstobecommunicatedandactedupon.6.5EnergyconsumptionandsustainabilityThegrowingenergyconsumptionrequiredfortheever-expandingamountofdatabeingprocessedandthetendencytowardscontinuouslymorecomplexSTmodelshasbecomeev-identsincetheraceforthelargestmodelshasbeengoingon.AtrendtowardsincreasinglycomplexE2EsystemscanbeobservedinmanyareasofAI,NLPandST.Duetotheextremede-mandonresources(data,computingpower,energy,infrastructure)thegenericconstructionofsuchmodels,inmanycasesisnowlimitedtoafewactors.Themotiontomakepre-trainedmodelsavailablefortransferlearningandfine-tuningthusallowingotherstoalsopartici-patefrommajoradvancesiscertainlybeneficial.However,theextentofthistransferandthelevelofcontrolinthehandsofafewinstitutionsposesaseriousrisktootheractors,tothemarketandpotentiallytoinnovationintheSTsectorasawhole.Surginginterestinsustain-abilityandethicsmaycauseactorstoreconsiderthemassiveincreaseinenergyconsump-tionthatcurrentlyaccompaniesprogressinST.Anopportunity(andmarketingadvantage)mayarisefromdirectingeffortsspecificallytowardsthecreationofhigh-performance/lowenergy-consumptionST–greenST.WP2:EuropeanLanguageEquality–TheFutureSituationin203030D2.14:TechnologyDeepDive–SpeechTechnologies6.6Privacy,surveillanceandtrustWheneverSTislinkedtoaperson’sidentityandthislinkisusedforaccesscontrolorautho-risation,theissueoftrustbecomesespeciallyimportant.ThemainapplicationsofAutomaticSpeakerVerification(ASV)areexactlytheareasofaccesscontrol,surveillance,forensicsorvoiceassistants.ASVisusedtoauthoriseaccesstoresourcessuchasabankaccountorbuild-ing.Insurveillanceapplications,itisusedfordetectingandidentifyingawantedcriminalinacollectionoftelephonerecordings.Inforensics,ASVisusedforcomparingavoicerecord-ingfromacrimescenewiththevoiceofasuspectoravictim.Forvoiceassistants,speakerrecognitioncanbeessentialtomakesurethatcertainrequestsarefulfilledonlyifmadebytheowneroftherespectivedeviceorcommodity(e.g.,computer,phone,houseorvehi-cle).Alloftheaboveapplicationsrelyonhigh-performanceandtrustedSTandcanbenefittremendouslyincommercialtermsifappliedwithinthesecontexts.AnotherSTwhichiseffectiveinintelligenceandsurveillancetasksistheidentificationofthelanguage(s)spokeninanaudiofileorstream.LanguageIDisaprerequisitestepinsettingswhendownstreamprocessing(e.g.,ASR)istobeappliedandmodelsareavailableforparticularlanguagesonly.Aspointedout,manySTsrequirehugeamountsofspeechdatatoreachstate-of-the-artperformance.Thestandardtodayistostoreaudio(thevoicesofpersons)inthecloudandlabelthemmanually.Therearenoguaranteesregardinghowdatastoredinthecloudisusedorwillbeusedinthefuturebycloudserviceproviders(orwhetheritmayleak).Thisgeneralapproachraisescriticalprivacyconcernsandithasledtomarketanddataconcentrationinthehandsofafew,bigcorporations.Dramaticimprovementsinspeechsynthesis(Székelyetal.,2019),voicecloning(Vestmanetal.,2020)andspeakerrecognition(Snyderetal.,2018)posesevereprivacyandsecuritythreatstotheusers.Thisresultedinagrowthofinterestinnewvoiceprivacy-preservingtransformationsandvoiceprivacyeval-uations(Srivastavaetal.,2019,2020;Ribaricetal.,2016;Qianetal.,2018).RecentlytheVoicePrivacyinitiativewasstartedtospearheadtheefforttodevelopprivacypreservationsolutionsforspeechtechnologyandcreateanewcommunity(Tomashenkoetal.,2020).Inthelongrun,thequestionwillbewhetheranypossiblebreaches,leaksorscandalsinvolvingSTwillerodetrusttoalevelthatuserswillnolongervolunteertoprovidetheirdatafortrainingpurposes(deepfakesmayposeaparticularrisk).Ofcourse,thedistrustwillbeweighedagainstthecommodityofusingcertaindevicesandplatformswhosetermsofusemaysimplyrequiretheusertodoso.Afurtherareaofconcernistheextentofunlawfulsurveillancebygovernments,stateagenciesor(large)corporations,infringingcitizens’rights,liberties,adverselyaffectingpub-licdiscourse,democraticvaluesandinfluencingthepoliticalpowers(Stahl,2016).TheSnow-denrevelationssparkedaglobaldiscussionaboutthegeneralnatureofmasssurveillanceanditsconsequencesforstateandcorporateintelligenceservices.Theconcernsabouttheextentofprivacyinvasion,accountabilityofintelligenceandsecurityservices,the(non-)conformityofmasssurveillanceactivitieswithfundamentalrights(Garrido,2021),theireffectsonthesocialfabricofnationscanonlybeconsideredandanalysedjointlywiththerapidlyextendingtechnologicalcapacities,includingST,andthepervasivenessofdevicesabletocapture,processandtransmitrelevantdata.Regardlessoftheformofcurrentgov-ernment,thegrowingextentofmasssurveillanceandespeciallyitsunlawfulapplicationmayleadtoerosionofpublictrustingovernmentsandstateagencies(seeLoraAnneandLaidler,2021(Westerlundetal.,2021)forarecent,in-depthpresentationoftheoreticalandempiricalrelationshipsbetweentransparencyandtrustinthecontextofsurveillance).Inaddition,dataleakscausedbysuchagenciesmayinadvertentlyleadtofurtherandcascadedinfringementsandillegaluseofdata.AverydifferentkindofriskisposedbyoverlyeagersalespersonsoversellingSTdramat-icallyandthefollowing–inevitable–chasmintowhichuserswillfallindisappointment.WP2:EuropeanLanguageEquality–TheFutureSituationin203031D2.14:TechnologyDeepDive–SpeechTechnologiesProperresponsibilityandmanagementofexpectationsneedtobecarriedoutinordertoavoidthisdetrimentalsituationandasituationsimilartotheWinterwhichAIwentthrough.7SpeechTechnologies:MainBreakthroughsNeededThelistofthemainbreakthroughsneededstemsfromthelimitationsidentifiedinchapter5,therecognitionofawider-reachingimpactofSpeechTechnologiesonsocietyatlarge,andtheircontributionstoDigitalLanguageEquality.Atthetechnologicallevel,theserelatetoaccuracy,reachingacceptablethresholdsforap-plicationsandinthecreationofthedatasetsrequiredforthecontinuousimprovementofthecoreSTcomponents.InthecontextoftheDLE,animportantchallengeandbreakthroughrequiredrelatetotheresourcesavailableforthedevelopmentoflesscommonlanguages;improvingtheperformanceandextendingthecapacitiesoftheSTcomponentsfortheselanguagesinparallelwiththeSOTAsystems.TheextendedproliferationofST,includingtotheareaswithahighpotentialimpactonindividualsandlargegroupsofusers,alsohastobeconsideredinawidercontextofpoliciesgoverningSTandrelevantfieldsandcallsformajorbreakthroughsintermsofexplainabilityforthecriticalmethodsandtechnologies.PoliciesandgovernanceconcerningtheuseofSTanddata–inparticularpersonaldata–needtobekeptuptodateandonparwithrapidlydevelopingtechnologiesandapplications.InordertodemocratisevoicetechnologiesandtostrengthentheirpositionwithinLTandtheevenwiderfieldofAI,thebaseofusers–onalllevelsofexpertise–shouldbewidened.Anincreaseineducationalprograms,includingingeneralAI,ML,NLP,andinter-disciplinaryactivities,projectsandprogramsaredeemedbeneficialforthegenerationofexpertsinthesefieldsabletodrawuponexpertiseinvoicetechnologiesbutatthesametimealsoindomain-specificfieldsthusformingthelinksbetweenthem.7.1AccesstoanddiscoverabilityoftrainingdataTobuildaDNNbasedTTSsystemnowadaystensofhoursofhigh-qualityspeechrecordingsmustbeathandandconsiderablecomputingcapacityisrequired.Thisseverelylimitsthepossibilitiesforsmallcompaniestocompeteandbeabletodeveloptheirowncustomvoices.OptimisingthearchitecturestomakethemlessintensivefromthecomputationalpointofviewwouldallowforcompanieswithlimitedresourcestocreatetheirownTTSservicesandvoicesandboostthecompetenceandcompetitioninafieldthatisbeingmoreandmoredominatedbyafewverybigcompanieslikeGoogle,AmazonorBaidu.ForASR,thesamelimitationregardingtheavailabilityoflargeamountsofannotateddataapplies.Onlythatinthiscase,theorderofmagnitudeoftrainingmaterialistypicallyevenhigher.Whereasintheearly2000s,severaldozensofhoursofaudiowereregardedasasufficientbasefortrainingAMforlanguages,thisamounthasrapidlyincreasedtohundredsor(tensof)thousandsofhoursofannotatedspeech.Theproblemisaggravatedbythefactthattrainingdataneedstobeavailableinaparticularlanguageordialect,assharingofacousticdatabetweenlanguagesisoftennotdeemedpossible.Semi-supervisedmethodshaveallowedextendingdatasets,however,theamountofdataavailabletoindustrygiantsexceedsthatofcommonmarketplayersbyordersofmagnitude.Datasetsforlanguagesoflessercommercialinterestarescarceandinsomecases,individualplayershaveachievedaquasi-monopolyondatasetsforparticularlanguagesanddomains.ForSID,thesituationisslightlydifferentinthattheamountoftrainingmaterialmaynotbeasmuchofafactorasforotherspeechtechnologies.Here,theavailabilityoftherightkindofdatapairedwithmechanismsforeffectiveandrapidmodeladaptationmaybekey.Privacyplaysaparticularlyimportantroleforthistypeofdata.WP2:EuropeanLanguageEquality–TheFutureSituationin203032D2.14:TechnologyDeepDive–SpeechTechnologiesAplethoraofdifferentlicensingagreementsandmechanismsposefurtherobstaclestoaccesstodatasetsandresources.Simplificationandharmonisationofthesemechanismswouldbehighlybeneficial.Whilesomeoftheseissuesfitintoalargerthemeofopendatasharingandbringingdig-italtechnologytobusinesses,citizensandpublicadministrationswhichareinthefocusof,forexample,theDigitalEuropeProgramme–DIGITAL15,itisimportanttoconsiderinsuchframeworksthespecificrequirementsandchallengesrelatedtotheacquisitionanduseofthedatasetstypicalfortrainingandevaluationofSTandLTmodels.7.2NewtrainingparadigmsForapproachesrequiringlargeamountsofproperlyannotateddata,strategiesandframe-worksforjoint(potentiallydistributed)datacollection,improveddataannotation(poten-tiallyautomated),aswellasjointprovision,maybeneeded.Thisnotonlyconcernsthecollectionitselfbutequallythestorageandprovisionofsuchresources.Alackofdataforparticulardomainsandlanguagesduetoalackofcommercialinterestneedstobecounteredbypubliceffortstojump-startandboosteffortsintheselanguagesandnottoriskcertainlan-guagesbecomingeffectivelyextinctinthedigitalrealm.Fromtheperspectiveofdataaugmentation,thegenerationanduseofsyntheticdatamayprovideacomplementaryalleyinthecreationorextensionofdatasets.Likewise,theappli-cationofmethodsmodifyingtheaudiosignalsthemselvesmayprovideaviablemannertoextenddatasetsandmakeresultingmodelsmorerobust.Workonadvancingalgorithmsandmethodstorequirelessdataortoyieldmorerobustmodelsusingsmalleramountsofdata,moreeffectiveuseoftransferlearningandfine-tuninglikewiseprovidepromisingapproachestoalleviatethelack-of-datadilemma.Forspecificfieldsofspeechtechnologies,improveduseofunlabelleddatainanunsupervisedorsemi-supervisedmanner(pre-training,self-supervisedtraining)mayprovidefurtherpos-sibilities(Laietal.,2021).NovelstrategieslikeMMLM(MultilingualMaskedLanguageModelling)whichhavesuc-cessfullybeenappliedtolearnmulti-lingual(orcross-lingual)representationsoflanguagemayprovidefurtherangles(Goyaletal.,2021).Whilesomepreliminaryworksexist,e.g.,(vanderGootetal.,2021),extensivestudiesarerequiredtoassessandevaluatetheextenttowhichsuchprogresscanbetransferredandappliedtovoicetechnologies.Inaddition,experimentsindicatethatMMLMmayalsoaidinthecross-lingualtransferofdeeprepresentationsduetothelearnedsharedlatentpropertiesoflanguage,linkingthisadvancetothetendencyofincludingbroaderanddeepercontextwithspeechtechnologiestoarriveatapplicationscombiningtechnologiesandallowingformanycomprehensiveap-plicationsanduserexperiences.IntheareaofSID,thetransferofknowledgelearnedfromlanguageswithalotoftrainingdatatomodelspeakersoflanguagesforwhichonlylittletrainingdataisavailablehasnotbeenexaminedthoroughly.Theinteractionbetweenthe(front-end)extractorandtheback-endlikewiseneedfurtherresearch(E2Etraining,noveltrainingobjectivesthatencouragetheembeddingstomatchtheassumptionsmadeintheback-endetc.).Forseveraltechnologies,makingbetteruseofthehierarchicalstructureandrelatednessoflanguagesmaybebeneficial.Asystemthathasnotseendataofaparticularlanguage(ordialect)intrainingshouldstillbeabletobenefitfromdatafromsimilar(close16)languageswhichmayprovidemoredata.Eventually,evenwithverylimitedtrainingdataforaparticu-larlanguage,itshouldthenbepossibletotrainamodelusingdatafromthespecificlanguage15https://digital-strategy.ec.europa.eu/en/activities/digital-programme16theappropriatedefinitionofclosenessmaydependonthespecifictechnologyandapplicationWP2:EuropeanLanguageEquality–TheFutureSituationin203033D2.14:TechnologyDeepDive–SpeechTechnologiesaswellasfromthesecloselyrelatedlanguages.Methodslikeone-shotlearningorfew-shotlearninglikewiseprovidepromisingapproaches.Tocomplementtechnological/algorithmicadvancesitmaybebeneficialtodevelopnewschemesinvolvingusersmoreactivelyinthegenerationofdatasetsfortrainingandevalua-tionpurposes.Suchapproachesneedtohavesafeguardsimplementedtopreventpollutionand/orbiasing(intentionalorunintentional)andbetransparentinthepreparationandcura-tionofthedata.Furthermore,incentivesforparticipationmustbepresentandthescopeofuseoftheresultingdatasetsbemanagedsuchthattheirusealsobenefitsthegeneralpublic(theEuropeancitizen)andnotonlyaselectedcrowdofcommercialactors.7.3ConfluenceandcontextinformationintegrationWhereaspreviouslythefocusofactivitieswasoftenplacedontheadvancementofindividualtechnologiesandspecificcapabilities,atendencytowardsconfluence–ofthecombinationoftechnologiesandinclusionofalargercontextaswellasthehistoryofeventsandinteractions–canbeobservedalreadytosomeextentandcanalsobeassumedtoplayamorepronouncedroleinthefuture.Forexample,therecentlypresentedE2Emodelforspeaker-attributedautomaticspeechrecognition(SA-ASR)wasproposedasajointmodelofspeakercounting,speechrecogni-tionandspeakeridentificationformonauraloverlappedspeech(Kandaetal.,2020).Itpro-ducedencouragingresultsforsimulatedspeechmixturesconsistingofvariousnumbersofspeakers.However,inordertoconductspeakeridentification,themodelrequiredpriorknowledgeofspeakerprofiles,whichseverelylimitedthemodel’sapplicability.Thefollow-upworkaddressedtheissuewherenospeakerprofileisavailablebyperformingspeakercountingandclusteringwiththeinternalspeakerrepresentationsoftheE2ESA-ASRmodeltodiarisetheutterancesofthespeakerswhoseprofilesweremissingfromthespeakerin-ventory(Kandaetal.,2021a).Theincreasedpresenceofconversationalinterfaces,aproliferationofchatbotscombiningASR,NLPandTTSwithanever-increasingpresenceofAI,ingeneral,hasmodifiednotonlythetechnicalandcommerciallandscapebutalsotheexpectationsofuserswheninteractingwithsuchsystemshavegrowndramatically.Userstendtoviewsuchsystemsasakindofdigitalassistant,apersonalconciergemorethanamereblockofinterconnectedcomponents(andreally,ascasualusers,theyshouldalsonotbeconcernedthatinreality,thismightbethecase).Thisrisingexpectationandperceiveduserneedhavebeenacceleratedbyincreasedperiodsofhome-officesetupsandvirtualmeetingswhicharelikelytoalsocontinueinthefuture.Morepowerfultoolsandgreatercapabilitiesalsoprompttheinclusionofupstreamtech-nologiessuchassummarisationorsentimentanalysistobeintegratedwithvoicetechnolo-gies.Speechsynthesisisboundtobecomeasemotionalandpersuasiveasthehumanvoiceitself.Theautomatictranslationmaybeusedwithinthelooptobridgelanguagebound-aries.Furthermore,technologieswillneedtobeintegratedinamannerallowingforfeed-backloopsandadaptationinaseamlessway.Modelsneedtobedynamicandmethodsallowingfordynamicadaptation–learningandunlearningcertainfeatures–willneedtobedevelopedtoaccountforflexibleandcontinuouslychangingconditions.Theintegrationoftechnologiesandtheinclusionofamuchbroadertypeofcontextmayallowcapturingintentionsandrealuserneeds,creatinganoverallexperienceofrealcon-versationalAI-poweredbyspeechandlanguagetechnologies,fullyinterconnectedwithbusi-nessapplicationsandprivatedatasources.Thusarequiredsteprelatestothetransferofvoicetechnologyperformanceimprove-mentsintodownstreamtechnologiesandthentoimprovedoveralluserexperience.Subsequently,suchsetupsmayneedtobeinterconnectedbetweengroupsofpersons,e.g.,WP2:EuropeanLanguageEquality–TheFutureSituationin203034D2.14:TechnologyDeepDive–SpeechTechnologiesfamily,friends,andteam-memberstoalsoincludefurthercontextnotlimitedtoasingleindividual.Examplesarebusinesssettingsinvolvingmultiplespeakersspeakingdifferentlanguagesduringmeetingsorgroupactivitieswhereculturalfactorsandbackgroundinfor-mationofseveralpersonsinvolvedwillneedtobetakenintoaccount.Areasoflinguisticssuchaspragmaticsaswellasparalinguisticswillneedtobeconsid-eredandintegratedtoamuchhigherextentthancurrentlytoallowformorenaturalandhuman-likeinteraction.Addingemotionsandaffectionsintotherecipesforhuman-machineinteraction,recognisingintentandtakingintoaccountabroadvarietyofcontextsholdsthepotentialtoturntheseinteractionsintotrulyhuman-likeexperiences.Thecomponentsre-latedtoemotionalunderstandingandempathy,whilerelevanttoallIntelligentPersonalAssistants(IPAs)andConversationalAgents(CAs),areespeciallyrelevantforsystemsfunc-tioninginsocialdomains,suchashealthcare,education,andcustomerservice.Combin-ingemotionalawarenesswithCAtechnologiesandapproachesalsonecessitatesincorpo-ratinginsightsfrommultipledomains,includingpsychology,artificialintelligence,human-computerinteraction,sociology,educationalresearch(Andreetal.,2004;Vinciarellietal.,2011;Skowronetal.,2013;Zhouetal.,2018;Belainineetal.,2020;D’MelloandGraesser,2012).7.4Explainability,transparencyandprivacyconcernsTheabove-outlinedincreaseinthecomplexityandcombinationoftechnologiesandmodelsrequiresacarefulbalancewithregardtoprivacyandethics.Scandalsanddata-leaksliketheonecausedbyCambridgeAnalytica(Hu,2020)orFacebookFiles17,theoftenintranspar-entmannerofhowpersonaldataarehandledbymanycompaniesandgrowingconscienceofthevalueofpersonaldatahasleadtoincreasedinterestandlevelofanxietyacrosssoci-eties.ActivitiesliketheEU’sGDPRregulation(Regulation,2016)areaimingtopavethewaytowardsahigherlevelofdatasovereignty.Attitudestowardssuchmotionsarecertainlydifferentdependingonregion,culture,politicalsystemetc.butmaybeseentoplayamoreimportantroleonaglobalscaleinthefuture.Trustinspeechtechnologiesandintheuseofdataobtainedbyinteractionwiththesetechnologiesmaybecomeadecisivefactorintheadoptionoftechnologiesandofthesuccessofindividualmarketplayers.Anincreasedinterestin“whathappensunderthehood”andinprovidingmoretransparencyofdatauseandsystemfunctionalitycanbeobservedacrosstheboardinmanyareasofMLandAI.ThisiscertainlyalsotrueforSTandwillbecomemorepronouncedifthesetechnologiesaretobecoupledwithothersourcesofdata(asdescribedabove).Afundamentalquestiontobeansweredtransparentlybyproviderswillbewhereexactlyprocessingisperformedandtowhatextentandpurposedataisusedtomodify(retrain,adapt)models.Oneendofthespectrumofprocessingislarge,anonymousdata-centresspreadaroundtheglobe.Theotherendofthespectrumisformedbystrictlylocalprocessingonpersonaldevices(ontheedge).Private,on-premisesolutionsprovidedbycompaniesorinstitutionsformanintermediatesetting.Inallofthesesetups,thebalancebetweencapabilitiesandtherequirementstoachievethesecapabilitieswillneedtobedeterminedandbalancedagainstethicalconcernsandpersonalandprivacy-preservingarguments.Theextentandamountofend-usercontrolandtransparencyonpartoftheapplicationproviderswillbeacrucialfactorinthisequation.Methodstoallowforflexibleandtransparentwaystoallowforsuchcontrolmaybepromisingareasnotonlyforvoicetechnologiesandmodels.Approacheslikeprivacy-by-designaccompaniedbyhighethicalandlegalstandardsmaybedeterminingfactorsinenablingtrust,fosteringadoptionandleadingtoeconomicsuccess.17https://www.washingtonpost.com/technology/2021/10/25/what-are-the-facebook-papers/,accessed17.1.2022WP2:EuropeanLanguageEquality–TheFutureSituationin203035D2.14:TechnologyDeepDive–SpeechTechnologies7.5Supportforless-resourcedlanguagesTobeabletoprovidefirst-rateSTinanylanguage,additionalhigh-qualitydatasetsareessen-tial.Ideally,theyshouldbeopenandavailablewithoutusagerightslimitationsforallthelanguagesandincluderecordingswithavarietyofconditionsandrepresentativesettings.Theseincludeavarietyofspeakers,languagevarieties,dialects,sociolects,dataincludingspontaneousspeech,variedprosodicpatterns,diversesentencelengthsandawiderangeofemotions.Creatingthiswidesetmaynotbefeasibleingeneral,butcouldbeachievedatleastforseveralmajorEuropeanlanguages.Newtechniquesfortransferlearningandmodeladaptationfromsystemstrainedforoneresource-richlanguagetosystemsabletofunctioninlanguageswithmorereducedquantitiesofavailabledatashouldbedeveloped.Thesetechniqueswouldallowthedevelopmentofcutting-edgeSTsystemsalsoforless-resourcedlanguages.Also,newarchitecturesallowusingresourcesfromseverallanguagesinsuchawaythatcommonalitiesamonglanguagesarelearnedinamorerobustwaybycross-lingualknowledge-sharingormethodsforthecreationofmultilingualorlanguage-agnosticmodelswhichcanbeappliedtoanumberofdifferentlanguagesareofutmostimportance.7.6Performance,robustnessandevaluationparadigmsDrivenbyvariousnationalandinternational(e.g.,DARPA-sponsored)evaluationsstandardperformancemeasureshavebeendefinedandmeasuredonstandardtestsetsduringcon-certedevaluations.CurrentmeasureslikethestandardWERonlytakecertainperformanceaspectsintoaccountandmayneedtobereconsidered,resp.beextendedorcomplemented.RobustnessandgeneralizabilityofSTcomponentsandmodelsaswellasstandard-evaluationsetsformultiplelanguagesandevaluationsetsallowingthe“parallelevaluation”ofseveraltechnologies(e.g.,LID,followedbySIDandASRallonthesamedataset)shouldbedevised.Thetopicofageingandrecencyofdataforevaluationsets(e.g.,ASRtalkingaboutGeorgeBushastheUSpresidentinadataset)needtobetakenintoconsideration.Likewise,chang-ingtechnologystandardsregardingaudioqualityshouldberevisited(e.g.,forSIDwherethetargetspeakersfurthermoreposeasimilarproblemasageingvocabularydoesforASR).Ingeneral,evaluation(aswellastraining)datasetsshouldbeviewedmoreasworkinprogressthanstaticartefacts.Incertaininstances,thecurrentstateoftheartTTSsystemssufferfromalackofrobust-nessinthegeneratedspeech,mainlywhenthekindofsentencesseenduringtrainingisdifferentfromtheoneusedduringinference.Differencesinlengthandsyntacticstructuremaketheunderlyingattentionmechanismlosetrackandword-skipping,longsilences,wordrepetitionorevenbabblingmayariseinthegeneratedsignal.However,thesemalfunctionsaremostlyscarce.Therefore,evenifthesystemsuffersfromthisproblemitisdifficulttoobserveitinalimitedsetofsentencesliketheoneusuallyincludedinsubjectiveevaluations.Thislackofrobustness,evenifrare,limitstheapplicationofTTStechnologyanddegradestheuserexperience.GuaranteeingrobustnessinmodernTTSsystemsisparamounttoen-suretheirubiquitouspresenceandadoptioninreal-lifeproducts.Beingabletomeasureperformanceonseveraldimensionssimultaneously,e.g.,bymea-suringWERforASRbutunderspecificruntimeandmemory,constraintsmightbebeneficialwheninvestigatingdifferentsetupsandbalancesbetweenperformance,model-scaleandre-sourceconsumption.EvenregardingestablishedtechnologiessuchasLID,evaluationsshouldbeupdatedinordertoallowforsuchmultidimensionalevaluations.Extensiontofurtherlanguagesandlanguagevarieties,dialectsandspeakingconditionslikewiseshouldreceivefurtheratten-tiontoensurebroadavailabilityandadoption.AnotherveryneededinnovationisamethodtoobjectivelymeasureTTSresults.TTSsys-temsareevaluatedbymeansofsubjectiveevaluationscampaignswhichmakesthemeasure-WP2:EuropeanLanguageEquality–TheFutureSituationin203036D2.14:TechnologyDeepDive–SpeechTechnologiesmentofanyadvancetimeconsumingandlaborious.Severalattemptstodeveloparobustobjectivemeasurethatcorrelateswellwithpeople’sjudgementshavebeenmade,butnore-liablealgorithmhasbeenfoundyet.SuchanalgorithmwouldmakeiteasiertoevaluatethedevelopmentofnewTTStechniquesandwouldboosttheadvancementsinthefield.StandardevaluationsregardingprivacyissuesandbiasofmodelsarelargelymissingformanyareasofST.Evaluationsintherealmofparalinguisticsarestillonlyscarce(anotableeffortinthisdirectionareyearlyParalinguisticChallengesatInterspeechconferences).7.7Outreach–communities,non-expertsRecentyearshavewitnessedanincreaseininterestinthedemocratisationofAI.Thiscon-cernsmanyfieldsofAIandML;amongthemalsothefieldsofvoiceandtexttechnologiesaswellasthelargerareasofNLPandNLU.ThewidespreadapplicationofMLandthewell-knownfactthatexpertsinMLandAIhavebecomescarceresourceshasledtothedesiretoempowerawidersetofindividualstoparticipateinthecreationanduseofthesetech-nologies.Toolkitsanddo-it-yourselfmodellingformpartofthetrendtodemocratisevoicetechnologies.ApproacheslikeAuto-MLaimtoprovideaccesstoMLalsofornon-expertsandassuchalignwithstrategiestoallowawideraudiencetoparticipateintheprocess.Aslan-guagetechnologiesareaggregatedandappliedtomorecomplexsettings,inter-disciplinaryresearchandactivitiese.g.,fromfieldsinthesocialsciencesarebecomingmorerelevantandsynergiesbecomeapparent.Programsandfundingschemestoactivelyengagethesecommunitiesandfosterinter-disciplinaryresearchwouldfurtherboostdevelopments.7.8AlignmentswithEUpoliciesandbreakthroughsneededonthepolicylevelIntermsofcopyright,rulesinEuropearemorerestrictivethaninothereconomicregionsandcountriessuchastheUnitedStates.Forexample,utilisingclosedcaptionsfromTVbroadcastsorsubtitlesfromacopyrightedfilmtotrainandevaluateSTmodelscoulden-ableaccesstohigh-qualitylanguagedataiflawmakerscouldagreethattrainingofmodelsoncopyrighteddataconstitutesfairuse,aslongasitdoesnotdiminishthevalueoftheassetsorreducetheprofitsreasonablyexpectedbytheowner.SimilarlylikeinotherLT,thepaceofthedevelopmentofSpeechTechnologiesinEuropecouldbefurtherincreasedbyintroducingchangesthatenablethere-useofexistingdata,whileatthesametimeensuringthatthevalueofthecopyrightownersisnotimpaired.TheGDPRintroducedanewglobalstandardthatplacesanemphasisonindividualrightsandreflectsEuropeanvalues,andassuchcontributestobuildingtrustinAItechnologies.Regrettably,theGDPRhashadanegativeimpactonthemajorityofEurope’sLTbusinessandresearchactivities(Smaletal.,2020).Forexample,manystakeholdersindatamanagement,publication,andcollectionhavecometowronglybelievethatalldataispersonalbydefault.Asaresult,costlylegalcounselandanonymisationmethodsareusedincircumstanceswhentheymaybeavoidedorarenotrequiredatall.Furthermore,non-EuropeanAIfirmshavebeenabletooperatefreeofGDPRconstraints(incaseswhereneitherdatastoragenortheidentityofcitizensconcernsEurope)sincethen,givingthemaneconomicadvantageoverEUfirms.Oneoftherequiredbreakthroughsrelatesthustoensurethatwhiletheindividualrightsareprotected,theextentofthese,inparticular,inthepracticalsettingsandday-to-dayoperations,doesnotextendbeyondtheintendedscope.Automatic,efficientandfreeanonymisationtoolsliketheonesofferedbytheMapaproject18arerequiredforallEuropeanlanguages.18https://mapa-project.euWP2:EuropeanLanguageEquality–TheFutureSituationin203037D2.14:TechnologyDeepDive–SpeechTechnologies8SpeechTechnologies:MainTechnologyVisionsandDevelopmentGoals8.1Speechtechnologies–theinterfaceofthefutureInmanysettings,voiceprovidesthemostnaturalwaytointeractwithdevicesandappli-ances.TheInternetofThings(IoT)andthetendencyforcomputationtotakeplace“attheedge”isturningintoakeyenablerofvoiceandspeechtechnologiesinmanyfieldsandap-plicationareas.Thecomingyearswillwitnessanincreasedadvanceinvoicetechnologiestothepointthatinteractingwithautomatedsystemswillbevirtuallyindistinguishablefromcommunicationwithhumanbeingsinmanycases(ideallysuchsystemsshouldmakeitclearfromthestartthattheyareindeednothuman).Interfacespredominatelyrelyingontyping,clickingandswipingwillgraduallytransformintomultimodal(orevenfullyvirtual)interfacesincludingvoice,shiftingthetaskofadaptationfromhumanuserstocomputersystems.Atthesametime,comparedtotheothermodalitiescurrentlydominatingtheHuman-ComputerInteraction(HCI)landscape,communicationwillencompassricherkindsof(lin-guisticandparalinguistic)information,includinggender,age,emotionalorcognitivestate,healthconditionsorspeakerspecifictraitsallowingforamoresophisticatedandaccuratespeakeridentification,modelling,adaptationandpersonalisation.ThesefactorsandtheirintegrationintoHCI–asbeneficialandpowerfulastheymaybe–alsogiverisetopri-vacyandethicalconcerns.Theypromptquestionsofcontrol,userunderstandingandintentwhenitcomestosharinginformationandtheextenttowhichdifferentkindsofinformationaretransmittedandusedinthefuture.Ensuingrisksandthepotentialimpactneedtobecarefullymetandbalancedwithmeasurestoincreasesecurityandtrust,both,bytechnicalmeansaswellaspolicy-legislativemeasures.Thisformationofthisbalancewillaffecttheadoptionofawiderangeofdevicesandservices:fromintelligentvoiceassistantsinhomesandonsmartphones,navigationandcontrolsystemsincarstocooperativeofficeandworkenvironmentsandsystemssupportingawiderangeofbusinessandleisureactivities.TheheavyincreaseintheuseofvirtualcommunicationtechnologyexperiencedduringtheCOVIDpandemicisviewedtobeatrendthatisalsolikelytocontinueintothefuture.Avarietyofplatformswereabletoincreasetheirpresenceandextendtheirfunctionalityduringthatperiod.Furtherextensionandtheinclusionofspeechandlanguagetechnolo-giesintoexistingandemergingoffersarelikelytoseamlesslylinkprojectmanagementandcommunicationtoolswithnaturallanguageprocessing.TTStechnologieswilladvanceuntiltheyareabletogeneratenaturalspeechwithanyde-siredvoice,speakingstyleoremotion.ASRtechnologiescanbeexpectedtoadvancetoper-formancelevelsonparwithhumanoperators(orexceedingit).Ingeneral,ourinteractionwithmachineswillincreasinglybecarriedoutthroughspeechandnaturallanguage.Ourhomeappliances,cars,electronicgadgetsanddigitalassistantswillcommunicatewithustoinformusaboutmalfunctions,tohelpusprogramandusethem,toadviseusabouttheirneeds,toentertainusandkeepuscompanyandactasvirtualcolleaguesatwork.Tech-nologiesforinput(likeASR)canbeexpectedtoincreasetheircapacitytohandledifferentexpressionsoflanguageasmuchasoutputtechnologies(likeTTS)canbeexpectedtogainexpressivenessandbeabletogeneratevoiceswithdiversespeakingstylesandpersonalities.ProgressinASRwillallowtappingintothelargeseaofmultimediadata(includinglargeex-istingarchives),SIDwillactforauthenticationandpersonalisation,TTSsystemsintegratedintoourdeviceswillallowconvertinganydigitalcontentintoamultimediaexperience.WP2:EuropeanLanguageEquality–TheFutureSituationin203038D2.14:TechnologyDeepDive–SpeechTechnologies8.2CapabilitiesandtechnologyshiftsUserandapplicationcontextsAtrendtowardstheintegrationofrichercontextistobeexpected,regardlessofthesub-fieldofvoiceprocessing.Thisconcernstheindividualtechnologiesaswellastheircombination.ForTTS,tohaveatrulyinteractiveexperiencewhendealingwithourdevices,theinte-grationofcontextwillplayamajorrole.E.g.,thecorrectwaytopronounceamessageshouldbeinferredfromthetextcontextorthepreviousstepsofadialogue.Inthisway,TTSsystemswouldbeabletogeneratetheresponsewiththecorrectinflexionsoparalinguisticfactorsarecorrectlyconveyedinadditiontothepurelylinguisticinfor-mation.Technologieswillneedtobesensitivetotheuser’scharacter,state,moodandneedsandadaptthemselvesaccordingly.Potentially,theywillalsoneedtotakeintoaccountothermembers’statesincaseofgroupactivitiessuchasbusinessmeetings.Topicsofpragmaticswillbereflectedbyalltechnologies.Ratherthanindividualcommunicationturns,completeconversationswithhistoryandcontextwillbethenorm.AddressingtheexistingtechnologicalgapsIntheareaofASR,continueseffortstowardsbetterunderstandingandmodellinghumanspeechperceptionmightresultinsophisti-catedspeechrecognitionaddressingseveralofthetechnicallimitationsandgapsidenti-fiedincurrentapproaches.Improvedhandlingofaudioconditionscurrentlyperceivedasdifficult(e.g.,multiplesimultaneousspeakersinnoisyenvironmentsspeakingspon-taneouslyandhighlyemotionallyinamixoflanguages)willbepossiblebysuchad-vances.Atthesametime,awiderdeploymentandfurtherpopularisationofSTwillalsorequiresolutionsthatofferhighrobustness,lowlatency,efficientcustomisationandtheabilitytoprovidepossibleequalsupportforadiversesetofspeakers.SpeechtechnologiesintegrationAnintimaterelationofASR,SIDandTTSwithdownstreamNLPandNLUtechnologiesisneededtoallowthecorrectinterpretationoftheinputsothatrecognition,meaningandoutputcanbeproducedinanaturalandcorrectman-ner.Acombinationoftechnologiestointeractinmultimodalways(includingvisuals)andtheefficientcombinationofinter-linkedmodelswillbeabletoguaranteethebestexperiencepossible.Inturn,thesuccessfulcombinationwillresultinanenhancedeas-inessandnaturalnessofuse,hidingindividualcomponentsandallowingtoperceivesystemsasassistantsusingnaturallanguagemuchinthewaythathumanassistantswould.MultimodalmodelsRecentlyintroducedneuralnetarchitectures,e.g.,PerceiverIO(Jaegleetal.,2021),supportencodinganddecodingschemesofvariousmodalities.TheycandirectlyworkwithBERT-stylemaskedlanguagemodellingusingbytesinsteadofto-kenisedinputs.Anotheradvantageofthistypeofarchitectureisthatthecomputationandmemoryrequirementsoftheself-attentionmechanismdon’tdependonthesizeoftheinputsandoutputs,asthebulkofcomputinghappensinacommonTransformer-amenablelatentspace.Althoughbeingatask-agnosticarchitecture,themodelprovidescompetitiveresultsonmodalitiessuchaslanguage,vision,multimodaldata,andpointclouds.Inthenearfuture,thistypeofarchitecturewillbecommonlyusedinarangeofapplicationswheremultimodalcontentneedstobejointlyanalysed.Further,thefu-turelineofworkrelatestothetrainingofasingle,sharedneuralnetencoderonseveralmodalitiesatthesametime,andonlyusingmodality-specificpre-andpost-processors.ThecomputationalcomplexityofPerceiverIO19islinearandthebulkoftheprocessing19https://huggingface.co/docs/transformers/model_doc/perceiverWP2:EuropeanLanguageEquality–TheFutureSituationin203039D2.14:TechnologyDeepDive–SpeechTechnologiesoccursinthelatentspace,allowingtoprocesslargerinputsandoutputswhencom-paredtothestandardTransformers.Thisenableslargeandheterogeneousmodelstobecomemorescalable,andavailabletowidergroupsofusersandapplicationscenar-ios.Moreover,connectionstoareassuchasknowledge-representationandknowledge-graphsmayprovidefurtheralleysforresearch.Inthelonger-termperspective,suchmultimodal,plugandplayarchitecturesandmodels,willprovidestrongbaselinesinmanyareas,potentiallyalsosupportinglesstechnicaluserswithvisualdesigntools,tractablehyper-parametersearch,automatedarchitecture,popularisingtheaccesstohighperformance,multimodalanalysisandinferences.DevelopmentpaceThepaceofdevelopmentinvoice-basedtechnologiesisdrivenbygen-eraladvancesinMLandassociatedhardwareaswellasdomain-specificadvancesinthemodellingofspeechperceptionandproduction.TheformercanbeexpectedtoaccelerateevenmoreduetogeneralinterestinMLandAIfromawideportfolioofdo-mains.Advancesintransferlearning,reinforcementlearning,fine-tuning,theuseofpre-trainedmodelsandcomponentsaswellasthearrivalofplatformssuchaHuggingFacehavecreatedadditionalmomentum.GPUsupportandextensionofGPUcapabili-tiescanlikewisebeexpectedtocontinueatafastpace,whichmightalsohaveeffectsontheavailabilityofhardwareresources.Thelattertopicshavebeenreceivingincreasedattentionasvoiceandlanguagetechnologiesenteredthemainstream.Voice,beingthemostnaturalwaytointeractwithsystemscansurelybeassumedtoattractevenmorecommercialandacademicinterestinthefuture.TrainingandevaluationSimultaneously,therewillbefurtherimprovementsintroducedintheprocessofcreationanddistributionofever-growing,evermorecoherent(labellingquality),anddiversedatasets.Thesewillalsoincludethecreationofandincreaseinanumberoflarge,multilingual,multi-domainandmultimodaldatasets,thatwillbe-comedefactostandardsetsforthetrainingandevaluationoftheSTmethodsandsys-temsthatincludeSTcomponents.Inthenextyears,wewillalsowitnessanincreaseinlabellingefficiency,awideradaptationofcontinuouslearning,self-adaptationandself-modificationparadigms.Whilethenumberoflanguagesavailableinthedatasetswillcontinuetogrow,thequalityandamountofdataavailableforthemostcommon,currentlyrich-resourcedandthelesscommon,currentlylow-resourcedlanguagesareunlikelytoconvergeinashorterterm.Thisdevelopmentinthecreationofmorecomplexandmultifaceteddatasetscallsforamorecomprehensiveevaluationandqualitycriteria;ashiftthatwouldchangeafocusfromanindividualspeechtechnologytoanend-userassessmentofacompleteexperi-encewhenconductingaspecifictaskinagiven,non-laboratoryenvironmentandinagivenoperationalandpersonalisedcontexts.Whereascurrentlearningparadigmsfocuspredominatelyontrainingmodelsonmas-siveamountsofdatainonego(eventhoughthisitselfmaycomprisemanyiterations),humanlearningtakesplaceincomplexstepsovertime,refiningitselfconstantlyalongtheway.Newparadigmsincorporatingcomplexsequencelearningmaynotonlypro-videfurtherinsightintohumanlanguageacquisitionbutlikewiseleadtoevenmorepowerfulST(NLP,NLU)models.Infrastructure,hardwareExtrapolatingfromthecurrenttrendsafurtherrapidincreaseinthecapacitiesoftheSTrelatedhardwareandinfrastructurecanbeforeseen.Thesein-clude,e.g.,fastercommunicationnetworksandhigherbandwidths,developmentandwiderdeploymentofthespecifichardwaresolutionsdedicatedtoefficientlysupportspecificSTcomponents,e.g.,audiolevelforASR.Also,furtherpopularisationoftheSTsolutionsinparticularinaformofIoT,andanewsetofvoice-enableddevicesthatwillWP2:EuropeanLanguageEquality–TheFutureSituationin203040D2.14:TechnologyDeepDive–SpeechTechnologiesbeavailabletousersinwork,leisureandcommercesettingscanbeforeseen.Thesedevelopmentscreateinturnadditionalchallengesrelatedtoloadandscalabilityoftheunderlyinginfrastructure,hardwareandnetworksused.Movingcomputationtoedgedeviceswillcertainlyalsocontinuetobeatrendinthenearfuture.8.3Privacy,accountabilityandregulationsThefutureofSTandawiderLTfielddevelopmentwillbestronglyinfluencedbythereg-ulationsgoverningthecollection,storage,transmission,anduseofpersonaldata.Theserelatetotheusers’concernsandexpectations,theinfluenceofthegroupsofinterest,bothatthenationalandtrans-nationallevels,andthefuturedevelopmentsoftheST,theirgrowingscopeofapplication,functionalitiesandperformanceimprovements.InthecontextofEuro-peanAIcompaniesandresearchinstitutes,thedevelopmentpaceappearstobeparticularlystronglyinfluencedbythecurrentandfutureregulationschemes.Lawmakers’decisionswillthushavetoconsiderthewideandprofoundimpactoftheirregulations–onthepro-tectionofcitizenspersonaldataandprivacyontheonehand,andonthewiderfieldofAItechnologies(research,developmentandapplication)andthecomparativeadvantagesanddisadvantagesvis-a-visothergeopoliticalregionsontheotherhand.Extrapolatingfromthecurrentregulationsconcerninguserprivacy,thedifferencesindatacollectionanduse,thedividebetweentheEUandnon-EUcountrieswillcontinuetogrow.AsAItechnologiesinthefuturewillplayacrucialroleindefiningcompetitiveadvantagesacrossthedifferentfieldsofhumanactivities,includingthecommercial,social,militaryandintelligence,itisunlikelythatawiderandfar-reachingconsensusbetweenthecompetingcountriesandregionswillbefound,whichwouldleadtoastandardisingsetofregulationsacrosstheregions.Withthegrowingpresenceofspeechtechnologies,MLandAIingeneral,growingcon-cernsaboutthehiddenflaws,shortcomingsandbaked-inbiasesofsuchsystemsisgainingmomentum.Thisiscertainlytruefromcitizensperspectives,butalsofromacademiaaswellasindustryperspectives.Whereascitizensandacademiamayworktowardsenhanc-ingtransparencyandcreatingmechanismsthatmaybeabletoavoidcertainphenomena,theindustrymayworktowardsobfuscationandhindranceoftheverysamemechanisms.IntheUS,lawsrequiringauditsofalgorithmsusedbyemployersforhiringandpromotionarebeinginstalledandbillsaredraftedbyCongressabouttheevaluationofdecision-makingsystemsusedinareassuchashealthcare,housing,employmentoreducation20.Atthesametime,EUpolicy-makersareconsideringlegislationrequiringinspectionofhigh-riskAIandapublicregistryofsuchsystems.HiringsoftwareisknowntoalreadyassigncertainpersonalityscoresbasedontheSWusedtocreateCVsorwhetherabookshelfisvisibleinthebackgroundduringaninterview.Theuseofvoiceandspeechtechnologiescaneasilybeenvisionedtoextendsuchscenarios,e.g.,bymeasuringanxietyinanapplicant’svoiceusingemotiondetectiontechnology.UserswillneitherbeablenorwanttodistinguishbetweenAI,NLPorNLU,betweenaplatformandaparticularapplicationorpartthereof.Tothem,theoverallsystemwillbewhattheyinteractwithandpotentiallywhattheywillperceiveasbeingbiased,unfairorharmingtheminanyway.DisclosureoftheuseofAI/speechtechnologiesDuetotheevermorehuman-likenatureofspeechtechnologies,theuseofAItechnologiesshouldbedisclosedattheearlieststagepossibleforalltransactionsandapplications.Makingusersawareofwhattheyinteractwithisregardedasafundamentalstepinthecreationofmoretransparency.20https://www.wired.com/story/movement-hold-ai-accountable-gains-steam/,AlgorithmicAccountabilityAct,ac-cessedon12/13/2021WP2:EuropeanLanguageEquality–TheFutureSituationin203041D2.14:TechnologyDeepDive–SpeechTechnologiesThiswillnotpreventhumanstoattributepersonhoodtomachines(thinkoftoysandpetanimals)orhinderhuman-likecommunication,butpresentanethicalandtransparentframearoundsuchsettings.MandatoryauditsofalgorithmsandmodelsAuditorswillhavetobeindependentforthistomakesenseandnotopenthedoortoevenmoresecretiveandevasivebehaviourbycompanies.Federalagenciesorboardsmayberequiredtopresideoversuchactivities.Standardtestsetsandtestsmayhavetobecreatedandapplied.MandatoryimpactassessmentsoftheintroductionofsuchtechnologiesTheconceptofmeasuringimpactandpotentialharmisfirmlyestablishedinfieldssuchasenviron-mentalimpact.Similarly,algorithmicimpactassessmentswouldneedtocoverabroadrangeoffactors,withspeechtechnologiesandNLPfocussingonlanguageandlanguage-userelatedaspects.PublicrepositoriesofincidentswhereAI/NLPcausedharmPublicrepositoriesandwaystoreportproblematicusesofAIwouldallowtoidentifyofrepeatoffendersandfinethemincaseofrecurringproblemsortheunwillingnesstoact.Furthermore,makingsuchcasesknownpubliclymayserveasanincentivetocorrectorpreventsuchcases.PolicyandlawmakingMethodsforassessmentsandmeasuringimpactsneedtotransformintolawsothattheycanproperlybeaddressedandemployedbycourtsandjudges.However,asthescopeandimpactofharmproducedbyAI/NLPareonlyknowntoasmallextentatthisstage,furtherresearchintoalloftheaboveareasisneededtocreateasoundfoundationforthepropermanagementofsuchrisks.Effectsonsociety,workplaceThecurrentdiscussionaboutwhichjobsorareaswithindo-mainsarelikelycandidatestobereplacedbyAIalsocarriesoverdirectlytothedomainofspeechprocessing–aswellastoNLPmoregenerally–astheycanbeseentoformacoreelementofAIinthiscontext.Issuesconcerningautomationandjobreplace-mentandtheensuingpolicy-makingandsocialramificationsthusalsodirectlyconcernspeechtechnologiesandtheirperception.PervasivenessAfurtherspreadandubiquitouspresenceofvoice-basedtechnologies,andwiderdeploymentofspeechtechnologiesacrossamultitudeofservicesanddevicesduetoreductioninsizeandintegrationintowearableandvirtualenvironmentscanbeexpected.Thismayalsoconcernfurtherpersonsbeinginthevicinityofsuchde-ploymentswhomaybeinvolvedindirectlybysomeoneelse’suseofST.SectorsThemostlikelycommercialareaswheretheSTwillseefurtherdynamicgrowthincludebanking,financeandinsurance,consumerandelectronics,education,health-careaswellastheautomotivesector.Thesesectorswilltakeadvantageofthehard-waredirectlysupportingspeechandvoicetechnologieswhilereducingtheoperatingcostsrelatedtoprocessingthecustomers’requestsandneeds.Thegrowthinthesesec-torswillinturnrequirelargelanguagemodels(LLMs)trainedonmassiveamountsofdomain-specificdatafordifferentindustriesandverticals,aswellastheall-terrainNLP.Theprocessingwillbehandledbylargedatacentres,proprietarysystemsonthepremises,aswellasonend-userdevicesattheedge.Thedistributionofthisloadacrossdifferentprocessingfacilitiesanddeviceswillbedrivenbythepoliciesandregulationsgoverningthedatacollection,storageanduse,thecapacitiesoftheportabledevicesandnetworksused,effectsofscaleandnewtrainingparadigmsdevelopedaswellasbyrequirementsandpreferencesofusersandbusinessesusingthetechnology.Arangeofnewapplicationswillemerge,e.g.,in-carassistance,combinedwiththeself-drivingcars,convertingcommutingintomovingoffices;self-servicerestaurants,whichWP2:EuropeanLanguageEquality–TheFutureSituationin203042D2.14:TechnologyDeepDive–SpeechTechnologiescombineST,NLPandrecommendertechnology,supportingfullyautomatedordertak-ing;orintelligentbusinessmeetingsortravelassistance(seesection8.6outliningfur-therexamplesofIntelligentPersonalAssistants).Theusualdriversofinnovationarealsopornandcrime.AwideradaptationofSTbythesesectorsgivesaraisetoaques-tiononthe(negative)effects,beyondthepreviouslydiscussedchallengesrelatedtothedataprotection,hacking,fakes,massandunlawfulsurveillancebyagrowingnumberofactors,andtheever-presentprivacyconcerns.Intheenvironmentwhere“yourvoicebecomesyouridentity”ensuringthesecurityofthissensitivedataisofprimeimportance,e.g.,zerotrustapproachesforvoicerequir-ingstringentauthenticationandmonitoring,combinedwithadvancedencryption.8.4FutureapplicationsSTandinparticulartheircombinationwithotherNLPandAItechnologiestoformintelligentapplicationswithhuman-likecapabilitieshavethepotentialfordisruptiveinnovationinavarietyofsectors.Intelligentassistantsandchat-botscurrentlyprovidetheleadingpathstowardsgeneralandbroadadoption.Futureapplicationswillbeexpectedtounderstandusersintentsoversequencesofinteractions,blurringorcompletelyeliminatingperceivedboundariesbetweenindividualtechnologies.8.4.1Customercontactcentres/callcentresSTisalreadybeingusedbymultipleindustriesforcustomercontactcentrestoincreaseself-servicefunctionalities,reduceaveragehandlingtime,increaseavailabilityandreduceem-ployeecosts.STwithinthisscope–inparticularwhenabletointeractinavarietyoflan-guagesandtakingintoaccountcontext–hasthepotentialtoincreasecustomeracceptanceandsatisfaction.8.4.2MediaandentertainmentThegamingandentertainmentindustrieshavetraditionallybeenattheforefrontoftheadoptionofnewandemergingtechnologies.Whereasforgaming,theimmersiveexperi-encesincludinginteractionbyvoicehavebecomearealityalready,thefurtheradoptionofSTinthemediaindustrymayprovidedecisivemechanismstoreachglobalaudiences(orlocalaudiencesspeakingdifferentlanguages).AutomaticASR,MTandTTStoproducesub-titlesorclosed-captionsonthefly,aswellasthesamearrayoftechnologiesforinteractionwithsmarthomedevices,providealleystodeliverproductstomultinationalaudienceswithminimaladditionalcosts.Aboomingpodcastindustry,producinghighvolumesofmultime-diacontentmaylikewisebenefitdramaticallyfromSTbyallowingittoexpandtopreviouslyinaccessibleregionsandaudiences.8.4.3MarketingandPRTheintegrationofSTandinparticularofparalinguisticfactorsintothefeedbackcycleforcustomerreviewsandcommentsmayprovideapromisingfieldforextensionofapplicationsbeyondcurrentcapabilities.Thesemechanismsmayalsobecombinedwithothersettingssuchasincustomercontactcentres(e.g.,toindicatefluctuationsofsentimentorphasesofanxietyorangerduringconversations)ormediaandentertainmentapplications(e.g.,toallowforpersonalisationofrecommendationsbasedoncustomermood).Furthermore,STmaybeadoptedintheareaofreputationmanagementandnewsscreeningbyallowingtoWP2:EuropeanLanguageEquality–TheFutureSituationin203043D2.14:TechnologyDeepDive–SpeechTechnologiesgaininsightsnotonlyontextualinformationbutalsoonmultimediainformationconcerninginstitutionsandindividuals.8.4.4HealthcareSThavebeenactivelyusedinthehealthcaresectorforaconsiderableperiodoftimealready.WhereascompaniessuchasPhilipsandNuancehavebeensuccessfulinthetranscriptionofradiologyservices,MicrosoftisaimingtoaccelerateitspresenceinthemedicalsectorbyacquiringNuanceasof2021.21ThecombinationoftechnologiestoadvanceconversationalagentsandNLUisexpectedtoincreaseMicrosoft’sfootprintintheareaofambientclinicalintelligencesolutions.STmaybehelpfulinsituationswherepeoplecannotuselanguage,donotknowhowtoreadorwrite(e.gbetooyoung)ormaysufferfromcognitivedisabilities.8.4.5FrauddetectionandsecurityTheuseofSTonpubliclyavailablecontentaswellasonphoneconversationsmayaidindetectingfinancialcrimesandsupportcomplianceandrisk-managementefforts.8.4.6PersonalisedSTVoicesforTTSwillbegeneratedforanylanguageandbefullycustomisable.Inthesameway,aswecannowpersonalisetheavatarsinvideogames,wewillbeabletoseteveryaspectofthesyntheticvoiceaswepleasetosuitthecharacteristicswepreferforeachsituation.Capabilitiesofthevoiceswillbeincreasedandthesystemswillbeabletosinginanymusicalstyle.Theywillalsohavetheabilitytoadapttotheacousticenvironmentandproducespeechthatiseasytolistentoeveninunfavourableconditions.Moreover,TTStechnologywillextendandspeechwillbegeneratednotonlyfromthetextbutfromotherinputinformationthatcouldbemoreconvenientforsomeuserswhodonothaveeasyaccesstotextorforsomesituationsliketheonesrequiringprivacy.Multi-modalsystemswillallowgeneratingspeechfromlip-reading,articulatorydataacquiredbydiversetechnologiessuchaselectromyography,permanentmagnetarticulographyandothersilentspeechinterfaces,andevencerebralactivitywithbrain-computerinterfaces.ASRtechnologieswillbecustomisabledependingonspeakerpreferencesandtraits,e.g.,adaptingthemselvestospeakingstyle,jargon,preferencesinformulationsetc.Furthermore,theymaychangedynamicallyonsocialandprofessionalcontext.8.5Possiblefuturedirectionsandvisions8.5.1ActorsandmarketsInordertocounteracttheMatthewEffect(wherebytheGAFAswouldbegettingintoanevenstrongerpositionovertime),itisimperativetoboosteffortsleadingtomoredataeffectiveuseofresources.Trendssupportingthehomemarket,leadingtoincreasedinnovationfocussingonlocalconsumersandvalues,shouldbeassistedandfacilitated.8.5.2CustomisationTechnologiesmayhavereachedanadvancedlevelofmaturityformanylanguagesanddo-mains.However,numerousfurthernichesremainwhichrequireexpertiseandadaptationofbasemodelstocoverthelastmiletothecustomer.InallareasofST,theopportunityto21https://techcrunch.com/2021/04/12/microsoft-is-acquiring-nuance-communications-for-19-7b/WP2:EuropeanLanguageEquality–TheFutureSituationin203044D2.14:TechnologyDeepDive–SpeechTechnologiescapitaliseoneffortsandtaskswhichfallintothiscategoryexistsandcanbetakenupbylocalchampions.8.5.3PrivacyandethicsAsequenceofscandalsandgrowinginterestinissuesofethicsandprivacyhaveledtoanincreasedawarenessinsociety.Trustintechnologyisakeyingredientfortheadoptionoftechnologiesbyalargeportionofthepopulation.Transparencyinhowprivacyisinte-gratedintotechnologies–algorithmsandmodels–isexpectedtobeacrucialingredienttoearncustomers’trust.Despitethedisparityoflegislationacrossnationalandeconomiccon-glomerates,privacy-by-designbeyondmerestatementsmaybecomeadecisivefactorfortechnologyuptakeandmarketsuccess.Itisworthpointingoutthatprivacydoesnotendwithin/aroundone’sowndeviceorsphere,butmayalsoincludeneighboursandbystandersaswasmadeexplicitbyarecentcourtrulingintheUK.228.5.4AmbientIntelligenceTheconfluenceofindividualtechnologiestoformanentitythatislargerthanthesumoftheindividualtechnologiesisarecurrentthemewithinthisdocument.Thisisespeciallyim-portantwhencombininghuman-likemodalitiesforinputandoutputwithknowledgerep-resentationandreasoning,potentiallyinanaugmentedorvirtualenvironment.ViewingSTasameansforintelligentinteraction,integratingnuancedandfine-grainedcontextandinputfrommultiplemodalitiescanbeexpectedtoleadtomorehuman-likesystemswheretheperceptionofindividualcomponentswillblurintoanoverallexperienceforend-users.SuchcombinationsmaybeasteptowardsabroaderkindofAIasopposedtothenarrow,highly-specialisedversionsinusetoday.8.5.5AugmentedIntelligenceThecurrentwaveofAIholdsthepotentialtoimpactandchangeawidefieldofbusinessesandworkplaces.WhereasexpertspredictthatputtingAI(includingNLPandST)toworkatalargerscalewilladdmorethan15trillionUSDtotheglobaleconomyby203023,thisoutlookalsocreatesfearsandanxietiesaboutthereplacementofalargeportionoftheworkforcebymachines.EffortstopositionAIasintelligentpartnersincollaborativeenvironmentsofhumansandmachines–augmentinghumancapabilitieswithAI–witheachsidecapitalisingontheirrespectivestrengthsmaynotonlyleadtogreaterproductivitybutalsoleadtohighersocietalacceptanceofthe(disruptive)introductionofnoveltechnologyintoareaspreviouslyperceivedtobethedomainofhumansonly.8.5.6Ontheroadtowinteragain?ThehypeaboutAIandtheaccompanyingover-sellingbysalesorganisationshascausedwavesofdeceptionandeventuallyendedintheso-calledAIWinterbefore(Hendler,2008;Floridi,2020;Muehlhauser,2016).Currently,AIisexperiencinganotherwaveofhype.In-vestments,inparticularintheUSAandChina,areexploding24,withstart-upsandcompaniesreceivinghundredsofmillionsofdollars25.ST(intheguiseofLanguageAI,VoiceTechnolo-giesorConversationalAI)isatthecentreofmanysuchinvestmentsandactivities.Basedon22https://www.theguardian.com/uk-news/2021/oct/14/amazon-asks-ring-owners-to-respect-privacy-after-court-rules-usage-broke-law23https://hbr.org/2021/03/ai-should-augment-human-intelligence-not-replace-it24https://www.forbes.com/sites/robtoews/2021/12/22/10-ai-predictions-for-2022/amp/25https://towardsdatascience.com/nlp-how-to-spend-a-billion-dollars-e0dcdf82ea9fWP2:EuropeanLanguageEquality–TheFutureSituationin203045D2.14:TechnologyDeepDive–SpeechTechnologiestheassumptionthattechnologieshavematuredandarenowreadyfordeploymentonthemarket,largesumsarebeinginvestedandhighexpectationsraised.Itremainstobeseenwhetherandtowhatextenttheseexpectationscanbefulfilledorwhetherover-sellingwillsendAI–andwithitNLPandST–intoanotherphaseofhibernation.8.5.7SupermodelsRecentyearshavewitnessedafierceracebetweenrenownedinstitutionsandresearchlabsonwhocanbuildthelargestmodelforNLP.Ithasbecomecustomarythatonlyactorswithenormousresourcesattheirdisposalcanparticipateinthisrace:Facebook,Google,Mi-crosoftandtheirChinesecounterparts.Recently,MicrosoftteamedupwithNvidiatocreatealanguagemodelwith530billionparameters(MT-NLG26)andDeepMindcreatedRetro27.Whereasthesehugefoundationmodelssufferfromthesameshortcomingsastheirprede-cessorsintermsofbias,theintegrationoftoxiclanguage,thelackofexplainability,etc.,performanceonmanytasksisstillimprovingwiththenumberofparametersandnoendofthisraceiscurrentlyinsight.Asisthecaseforsearchtechnologies,theUSandChinesegiantsareleadingtheseactivities.EuropeaneffortsliketheGermanOpenGPT-Xproject28aimtomitigatethisimbalance.Text-creationinahuman-likemanner,foramultitudeofdo-mains,differentlanguagesandincludingavarietyofstylisticelementsarealreadypossibletodayandcanonlybeexpectedtoimprovefurther.WithregardtoSTandE2Emodelsoraspartofaggregatedmodelscomprisingseveraltechnologies,similaractivitiescanbeex-pected.Whereasinthepastaccesstosufficientamountsofdatahasbeenthedeterminingfactor,thistendencyturnsaccesstocomputingresourcesintothenextcrucialbottleneck.Asforthelargelanguagemodels,againGAFAswithaccesstodata(andusersfeedingmoredatatothemeveryday)areinthepolepositionformarketdominance.Intherecentlypublishedwork,Bommasanietal.2021(Bommasanietal.,2021),providesathoroughaccountoftheopportunitiesandrisksofsuchfoundationmodels,rangingfromtheircapabilities,technicalprinciples,applicationsandsocietalimpacts.8.6ExamplesforIntelligentPersonalAssistantsPublictransportWhilewaitingforatrainatthetrainstation,acommuternoticesthatthetrainseemstobelate.Asisoftenthecase,noannouncementaboutthisstatewasmadeandthedisplaystillshowstheoriginaltimeofdeparture.Ontheplatformteemingwithpeople,theuser,upsetaboutthisrecurrentsituationaskstheirdevice“what’sthemat-terwiththestupidtrain?”Thisquestionisutteredinanoisyenvironment(abusytrainstation)andinahighlyemotionalmanner.Potentially,dialectorslangisusedwhenexpressingdiscontent.Inspiteofthechallengingconditions,thesystemrecognisesthecommutersquestion.Bysearchingthetraincompany’sdatabase,itretrievesthereasonforthedelayandtheexpectedtimeofdeparture.Takingintoaccountthetimeofdayandweek-day(andhencethecommutingconditions),itsuggestsanalternativewaytotravel.Outputisprovidedinawaytakingintoaccountthenoisyconditions(volume,speed)aswellastheannoyingstateofthecommuter(appropriatephrasing,toneandvoice).BusinessmeetingDuringabusinessmeetingwithseveralmembers,severalissuesremaintoberesolvedandfollow-updataandtimeisbeingsearched.Whileacoupleofcol-26https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/27https://deepmind.com/research/publications/2021/improving-language-models-by-retrieving-from-trillions-of-tokens28https://www.iais.fraunhofer.de/de/presse/news/news-210701.htmlWP2:EuropeanLanguageEquality–TheFutureSituationin203046D2.14:TechnologyDeepDive–SpeechTechnologiesleaguesdiscussapossibleslotforameetingonthemorningofthefollowingThursday,oneofthemaddressestheuserwiththequestion“anychanceyoucanmakeit?”ThesystemrecognisesthatthecolleaguewhoaskedisJohnandthatitsowneristheonebeingaddressedbyJohn,translatestimeanddayintoactualcalendardays,searchesJohn’sandtheowner’scalendarandfinds,thataslotafter10AMwouldbebestastheownerhasaprivate(doctor’sappointmentat8AM)andthatJohnusuallydoesnotcometoworkbefore10AM.Asothersarelisteningandthedoctor’sappointmentisdeemedtobeapersonalmatter,thesystemdecidesnottousevoicebutratherdisplaytheinfor-mationonthesmartdevice’sscreen,signallingtheownerofthepotentialavailabilityattheintendedtimeandday.BusinessAssistantWhilepreparingapresentationforanupcomingmeeting,auserre-alisesthattheyhavecreatedaslideonasimilartopicnottoolongago.ByaskingtheIPA“Canyoufindthatslidewiththefigureonthedifferentkindsofmediaandtheroletheyplaywithregardtohate-speechforme?”asearchistriggered.TheIPArespondswith“YoumeantheonewhichyouusedatthespeechyougaveattheUniversityofViennainDecember?”turningspeechintoaninteractivetoolforsearch.Uponconfirmation,thecorrectinformationisprovided.TravelTryingtobookaflight,theusertellstheirIPA“I’dliketoflytoBostonnextTuesdayorWednesday”.Recognisingtheintenttofindasuitableconnection,theIPAtakesintoaccountthattheuserprefersdirectflights,isanaficionadoofnewaircraft(sowouldprefere.g.,anAirbusA350oraBoeing787),prefersairlineswithintheMilesandMoregroup,hatesParis’Charles-de-Gaulleairportandpreferswindowseatupfront.Further-more,theIPAknowsthattheuser’shomebaseinMunich,sothedeparturewouldlikelybefromthere.Searchingtherespectivedatabases,thesystemfindsasmallnumberofflightsfromMUCtoBOSandpresentsthetop-rankingone(LufthansawhichhappenstobeusingthenewA350-900onthisroute)inaverbosemannerwhereasonlypointingoutsomekeyfactsabouttheremainingflights.Receivingconfirmationthedialoguethenproceedstotheactualbooking.MultimodalsearchWhilesearchingonlineforanewcomputer,usingsmartglassesandvoice,theuserispresentedwithvisualfeedbackaboutseveralmodelsfittingthebill.Theuserscrollsthroughthelistandattheendofitfindsthatthe3rdmodelonthefirstlistofresultssuitedthembest.Theuserentersintoadialoguelikethefollowing“showmethedetailsagainofthatonewiththenewIntelquantumXYZCPUandwiththe100TBdrive,yesthatoneandtheonebelowaswellforcomparison”.TheIPAtranslatesfromdescriptivelanguageintotheactualitems,takingintoaccountthehistoryandvisualpositioningofpreviouslyshownresults.PersonalCoachDuringalongdayoftenseandstressfulbusinessmeetings,theIPAnoticesanincreasedleveloftensionintheuser’svoice.Combininginformationfromwear-ables,itdeterminesthattheusershoulddefinitelytakeadeepbreathandmaybetakeashortbreakbeforecontinuing.Takingintoaccountthemeeting’sstateitsendsabuzztoalerttheuserandthenusesasmoothandreconfirmingvoicetoprompthertoopenthewindowandhaveacupofcoffee–takingintoaccountthatshehashadonecupsofarandusuallyconsumesuptofivecupsofcoffeeaday.WP2:EuropeanLanguageEquality–TheFutureSituationin203047D2.14:TechnologyDeepDive–SpeechTechnologies9SpeechTechnologies:TowardsDeepNaturalLanguageUnderstandingInthisdocument,NaturalLanguageUnderstandingisviewedandtreatedasasubsetofthefieldofNaturalLanguageProcessingwhichitselfisasubfieldofArtificialIntelligence.Fur-thermore,asoutlinedinpreviouschapters,SpeechTechnologiesformasubfieldofNLP.ThetermDeepinthiscontextisinterpretedasameanstodistinguishthiskindofNLUfrompre-viousapproachesintwofundamentalways:theinclusionofavarietyofknowledgesourcesallowingforaricherandmorecomplexmannerofprocessingandinteractionaswellastheuseofDNNs(withmultipleheterogeneouslayers)toencompassseveralmodelswhichtra-ditionallyformedseparateunitsintooneoverallmodel(E2E).Furthermore,thesesystemsandmodelsmayinvolvedifferentmodalities,receivingtheirinputasamixofaudio,videoandtextsuchasdescribedby(Akbarietal.,2021).Asstatedinthechaptersabove,inmanyinstancesthemostnaturalmannerforhumanstointeractwithmachinesisthroughvoice.Thisentailsusingvoicetoissuecommandsorqueriesaswellastheuseofvoiceforthegenerationofanswersandstatements29.Certaintypesofscenarios(e.g.,oneslimitingtheinteractiontosmall,hand-helddevices)maycallforvoice-onlyinteractionwhereasotherscenarios(e.g.,allowingforfeedbackvialargescreens,augmented-orvirtualrealityenvironments)mayfavourmultimediasettings,permittingtheflowofinformationacrossdifferentmodalitiesinparallel.Yetotherscenariosmayaskforcommunicationcompletelywithouttheuseofaudio,inparticularwhenconsid-eringspecialneedsandinclusivecommunication.Speechtechnologiesplayaroleintheingestionofinformation–byactingasakindofsensorconveyinglinguisticaswellasparalinguisticinputsandconvertingthemintostruc-turedinformation.Equally,theiruseconcernstheoutputofinformationinauditive(speechbutalsonon-speech,suchasconfirming“uh-huh”)formtocommunicatewithhumanusers.Bothdirectionsoftheflowofinformationapplytohuman-computerinteractionaswellashuman-to-humaninteractioninthecaseofgroupsofhumanusersinteractingwitheachotherorwithcomputers,e.g.,duringmeetingswithintelligentassistantsfortranscription,translationandsummarization.Speechtechnologiesthusformanintermediateinterfacelayerbetweenhumansandma-chines.Inbound(auditive)informationiscapturedandenrichedbySTbeforebeingpassedontodownstreamNaturalLanguageUnderstanding(NLU)processing.Outboundinforma-tionisenriched,transformedandeventuallyrealisedasaudiobasedoncontent,structureandmeta-informationprovidedbysemanticcomponents.ThesemanticsandinterpretationofutterancesaswellasthegenerationofappropriateresponsesbasedonalogicalrepresentationandstateofaconversationfullyresideswithinthescopeandcomponentsofNLUandtechnologiessuchasdialogue-managers(tocarryonconversations)orknowledgegraphs(networksforsemanticrepresentation).Assuch,STprovideessentialcontributionstothefunctioningofNLUboth,intheinputaswellastheoutputdirections.However,theydonotperformanysemanticprocessing(understanding)themselves.Asindicatedabove,visualcuessuchasgesturesormanualarticulation(sign-language)mayreplacetheaudio-elementofSTwhenoperatinginnoisyenvironmentsorinvolvinghardhearingimpairedordeafpeople.Technologiesfromthefieldofvisualprocessingas-sumetherolesofSTinthesecases.Thecombinationofmodalitiesisalsopossibleandmaybeappropriate/imperativedependingontheactualenvironment,suchasworkingenviron-29Insayingso,wesilentlyassumethat“weexpecttocommunicatewiththecomputerinthesamewaywewouldwithanotherhuman”(Winograd,2006).However,weacknowledgethattheremightbepracticalandevenphilo-sophicalobjectionstoencouragingpeopletoattributehumanqualitiesandabilitiestotheircomputers(idem).Whetherandtowhatextentthisispossibleisamatterofdiscussion.WP2:EuropeanLanguageEquality–TheFutureSituationin203048D2.14:TechnologyDeepDive–SpeechTechnologiesmentsrequiringahands-freeoperation.ThecontributionofSTtowardsachievingdeepNLUmaythuslieintheimprovementandextensionoftheindividualtechnologies(bothfromaccuracyaswellasalanguage-/domain-coverageperspective),fromtheirintegrationintoE2Esystemsallowingforjointoperationandoptimisation,includingdifferentkindsofknowledgesourcesandfromtheirflexibleanddynamicconfigurationdependingonthestateandcontextofanapplicationoruser.Approachesincludingthecombinationofseveralmodalitiesboth,forinputandforoutputmaylikewiseprovidebeneficialinthecontextofachievingdeepNLU.Inmanycases,therealpowerofNLUwillbecomeperceptiblewhenitfeaturesaspartofacomplexsystemfunctioningasahuman-likecounterpartincommunication–exhibitingcontext,historyandelementsofgeneralintelligence.However,itmayalsobethen,thatNLUisovershadowedbythecognitivedownstreamprocessingandeventuallyperceivedasamerecommodity.Theelementofadmirationandaweonpartoftheuserwillthenconcernthecompletesystemperformance,withNLUitselfdisappearinginimportanceasasmallpartofamuchlargerandcomplexintelligentsystem.10SummaryandConclusionsThesubstantialadvancesmadeinthefieldofSToverthepastdecadesholdthepotentialfordisruptiveinnovationinmanyareasandapplicationdomains.CombinedwiththeprogressofrelatedfieldssuchasAI,NLU,NLPandML,theyprovidethebasisforbroadadoptionofspeechandvoiceastheprimarymodalitytointeractwithcomputersystemsaspartoflargerandmorecomplexsystemsmodellinghuman-likecommunicationandinteraction.Thisreporthasidentifiedseveralfieldsandbusinessdomainsthatprovidepromisingar-easfortheuseofSTandtheirinclusionintolargersolutionsprovidingamorenaturalmeansofcommunication.However,atthesametime,severalissuesandchallengeshavealsobeenidentifiedwhichneedtobeaddressedandresolvedinordertomakethispromisemate-rialise.Thefollowinglistsummarisesthekeyelementsidentifiedwithinthisreportandprovidesalistofdirectionsandrecommendationsforpossiblefutureactions.Ingeneral,STareexpectedtobecomepartoflargersystems,interactingwithusersinahuman-likemannerandthusallowingawideradoptionofST,NLPandNLU.Inparallel,theindividualtechnologiesandcomponentswillcontinuetobeimproved,bothintermsofaccuracyaswellasofcoverage(oflanguage).Allthesestrandsofadvancementscanaidinsupportingtheoverarchinggoalofachievingdigitallanguageequalitybyprovidingservicesmadepossiblebythesetechnologiestolargeraudiencesorequal(similar)levelsofscopeandperformance.PandemicchangesAspeoplearenowmoreusedtoonline,collaborativeenvironmentsduetolockdownsandlimitedaccesstooffices,theywilllikelywanttokeepusingthem.ThiscreatesfurtherdemandforbetterandmoretightlyintegratedSTanddownstreamprocessing.TheimportanceofdataTheavailability/scarcityoftrainingdataisstillakeyfactorinthecreationofSTaslongassupervisedparadigmsprevail.Accessibilityisoftenlimited,orevenlocked,withindividualactorsamassingmassiveamountsofdata,effectivelycreatingmonopoliesforcertainmarkets.Licensesanddatarelatedregulationsaswellasoperabilityandcompatibilityofdifferentdataresourcesandprovidersremainsanobstaclethatneedstobeaddressed.Methodsnotrelyingonvastamountsofdata(e.g.,fine-tuning)formanactiveareaofresearch.Furthermore,language-agnostic(ormul-tilingual)modelsmayprovideanswersandworkarounds.ApproacheslikePARP(Laietal.,2021)alreadyprovidepromisingresultsbutneedtobeextendedfurther.EvenifWP2:EuropeanLanguageEquality–TheFutureSituationin203049D2.14:TechnologyDeepDive–SpeechTechnologiestrainingparadigmsmightchangetoresemblehumanlearning(focussingoncomplexsequencesratherthanonsingleavalanchesofdata),thischallengewillremain.Multi-lingual,language-agnosticmodels“Takeanylanguage,forexample,English...”–thishasbeenarunninggaginmanyinstitutionsdedicatedtoST,butitequallyappliestoNLPandNLU.EventhoughthescopeoflanguagessupportedbySThasincreaseddra-maticallyoverthepastdecades,Englishstillholdsauniquepositionwhenitcomestoresources.Ontheonehand,thecreationofresourcesforfurtherlanguagesanddialects(somemayonlybespoken)isanongoingactivity.Ontheotherhand,theinvestigationofphenomenathatareonlypresentinotherlanguagefamiliesformsanactiveareaofresearcharoundtheglobe.Thecreationofmulti-lingualorlanguage-agnosticmodelsprovidesfurtheralleysforimprovement.ComplexE2Esystems,combinedknowledgesourcesSubstantialgrowthintheavailabil-ityofdataforsomelanguagespairedwithaboostofprocessingcapabilitiescreatedatrendofintegratingmodels,whichpreviouslyexistedinisolation,intoacombinedoverall,model.Trainingandoptimisationtakeplaceinasingleframeworkratherthanindividually,bettercapitalisingonjointfactors.Considerableprogressinperformancehasbeenmadethroughthisapproachwhichcanbeexpectedtocontinuealsointhefu-ture.TheintegrationofsemanticcomponentssuchasNLUorknowledge-graphs,intotheseframeworks,mayprovideadditionalelementsrequiredfortrulyintelligentinter-action.However,anincreasedlackofexplainabilitymayensuefromsuchintegrationandpromptadditionalactivitiesandparalleleffortstoaddressthissignificantlimita-tion.Progressandcollaborationwithfieldssuchasneuroscienceandpsychologymayleadtodeeperandmorehuman-likeapproachestolearningandmodelingofcognitivecapabilities.DiversityofcontextIncurrentsetupsandapplications,differentcomponents,includingST,largelyoperateinanindependentandisolatedmanner.Forexample,ASRrecog-nisesspeechwithoutanyspecificcontextconcerningdialogue-stateoruser-preferencesorintentions.Thedynamicinclusionandintegrationofsuchfurthercontextwouldpo-tentiallyallowforSTtooperateonasignificantlyhigherlevelofaccuracy,eliminatingerrorsandnarrowingdownalternativesbasedontheincreasedcontextand/orboost-ingmoresensiblealternatives.Variouswaysforthefusionofinformationhavebeeninvestigatedsuchasearlyandlatefusion,buthavenoteffectivelycometofruitioninmanycircumstances.Novelwaysemployingsystems,parallelsystemsformultipartyconversationsettingsandmultimodalapproachesmayprovideawayforward.MultimodalitySTpredominantlyaddressthemodalityofusingvoiceforinteractionwithcomputers.Thisencompasseslinguisticaswellasparalinguisticelementsandmayex-tendtosign-languagetosomeextent.ThecombinationofSTwithmultimodalinputsandoutputsmayprovideabasisfornext-generationHCIs.Inclusionofgestures,facialexpression,emotionsorhapticsaswellasthegenerationofmultimodaloutputsreflect-ingtheseelementsmayresultinamuchricherandmorenaturaluserexperienceandleadtowideradoptionandacceptanceofST.Measuringperformance,benchmarksandrobustnessWERhasbeenthestandardmea-sureofperformanceforASRfordecades.AlthoughthisestablishedmeasureallowsquantifyingprogressinASR,itonlytellspartofthetruthwhenitcomestotherealapplicationofASRanditscombinationwithdownstreamprocessing.TheevaluationofTTSisstilllargelysubjectiveandreliesonhumansubjectsandlacksatrulyobjec-tiveapproach.InmanyfieldsofST,performancehasreached(near-)humanlevelsundercontrolledconditionswithacademicprogressbeingsignificantintheorybutof-tenonlymarginalwhentranslatedintoreality.AshifttowardsincreasingrobustnessWP2:EuropeanLanguageEquality–TheFutureSituationin203050D2.14:TechnologyDeepDive–SpeechTechnologiesandgeneralityofresultsmayprovebeneficialatthisstage.SeveralstandarddatasetsforevaluationexistforavarietyofSTandlanguages.However,forseveralareas,e.g.,onesconcerningparalinguisticphenomenaorcertainlanguagesanddialects,nosuchstandarddatasetsexistandremaintobeestablished.STascommoditiesRecentprogressandanabundanceofSTinchatbotslikeAmazon’sAlexaorApple’sSirimayevokeexpectationsofSTbeingamerecommodityandraiseunreal-isticexpectationsonthepartofcasualusers.Ontheonehand,likeothertechnologiesandmodels,STperformconsiderablyworsewhenappliedtoconditions,unliketheonesforwhichtheywereoriginallycreated.Adaptationandcustomisationtospecialdomainsthusprovideanopportunityandmarketnicheforspecialists.Inaddition,themanagementofexpectationsandopencommunicationaboutthepossibilitiesbutalsolimitationsonpartoftheSTcommunitymayhelpsetexpectationstorealisticandpracticallevels.DigitallanguageequalityIfanequallevelofsupportacrosslanguagesisthelong-termgoal,itcanbefullyaddressedneitherinthecurrentnon-freemarket,digitaloligopoliesdominatedenvironment,norbypurelyfree-marketmechanisms.DevelopmentandprogressinSThasbeendrivenbycommercialentitiesoverthepastyears.These,un-derstandablyoperateintermsofmarketsandshareholdervalueandthuswillonlybewillingtoinvesteffortsoneconomicallylessappealinglanguagesundercertainlimitedconditions,e.g.,PR,influentialindividuals,policy-demandsorwhentheprofitsfore-seenforprovidingsupportforalesscommonlanguageoutweighsthemarginalprofitsthatcanberealisedbyprovidinganincrementalimprovementtooneofthealreadysupportedlanguagesordialects.BiasesandethicalissuesTheinterestandconcernaboutfairnessandbiasesbakedintomodelsandethicalissuesrelatingtomodelsandtheirusehavebeenreceivingin-creasedattention.Methodsfordetectingbiasesandde-biasingneedtobeimprovedandareexpectedtobecomeamoreactiveareaofdevelopment.Furthermore,accesstoSTforpeoplewithdisabilitiesandimpairments,e.g.,bytheinclusionofvisualpro-cessing,needstobeextended.Culturalfactorsoflanguageanditsuse(e.g.,levelsofpolitenessetc.)shouldbeconfigurableandadaptivetothesituationathand.Anotherrelatedethicalissueariseswhenconsideringinfluentialagentsandbots(Al-louchetal.,2021).Withthecurrentandnear-futurestateofthetechnology,manybusi-nesses,politicalpartiesandideologicalmovementsmaydevelopconversationalagentsasarepresentativetoconveytheiragendaandswaypublicopiniontogetsupportfortheircause.Situations,wheretheagents’identityisknownorhidden,shouldbeclearlydistinguished.Caseswhereacompanyorpartyisrepresentedbyasingleconversa-tionalagentorbyseveral,hundreds,oreventhousandstocreatearepresentationofmasssupport,shouldbeclarifiedandmarked.WhilemanyapplicationsthatintegrateSTandLTareusefulandevennecessary,theseapplicationscenariosshouldbecloselymonitoredforethicalandprivacyaspects.TransparencyandinterpretabilityTriggeredbyanincreasedinterestinthefairnessoftheapplicationofAIsystems,e.g.,filteringandpreliminaryassessmentsofjobapplica-tions,prison-parole,credits,sectorslikeNLPandSTareandjustifiablywillinfuturecontinuetobeconfrontedwithsimilarquestioningandscrutiny.UsersarelikelytodemandexplanationsonthecapabilitiesandfunctioningofST.Resultsarelikelytobequestionedwithsomeapplicationareasdemandingauditsofmodelsandalgorithms.Technicalissueswillneedtobeaddressedandaccompaniedonapolicy-makingandlegislativelevel.Standardisationofevaluationsandpublicationofresultsmayfunc-tionasmotivatingfactorsforproviderstoaddresstheseissuesmorethoroughly.WP2:EuropeanLanguageEquality–TheFutureSituationin203051D2.14:TechnologyDeepDive–SpeechTechnologiesBalancebetweenconvenienceandprivacyScandals,dataleaksandanincreaseincyber-crimehavebroughtissuesofsecurityandprivacyontothetable.Ontheonehand,devicesareevermorepervasive,takingSTintopeople’sofficesandhomes.IoTandwearableswillcontinueandfurtheracceleratethistrend.Ontheotherhand,usersarebecomingincreasinglywaryoftherisksandundesiredeffectsrelatedtotheintroduc-tionofST.Clandestinemannersofdatacollectionandeavesdroppinginfringingprivacyarepublishedandcastigatedbythemedia.Actorsrisksufferingdireconsequencesiftheydonotrespondandputcorrectivemeasuresintoposition.Thebalancebetweenconvenienceandprivacywillremainafluidonetobenegotiatedrepeatedlyandonmultiplelevels.PoliciesandregulationsThelegislationgoverningtheacquisition,storage,transmission,anduseofpersonaldatawillhaveasignificantimpactonthefutureofSTandthewiderLTarea.Theseinturnstemfromuserconcernsandexpectations,theinfluenceofinterestgroupsbothatthenationalandtransnationallevelsaswellastherapiddevelopmentsintherelevantfields.Extrapolatingfromthecurrenttrends,thegapbetweentheregulationsusedindifferentregionswillcontinuetowiden.AsAItech-nologiesplayacriticalroleincreatingcompetitiveadvantagesacrossawiderangeofhumanactivities,includingcommercial,social,military,andintelligence,itisunlikelythatcompetingcountriesandregionswillbeabletoreachabroad,far-reachingagree-ment,resultinginonestandardisedsetofregulations,respectedandfollowedinprac-ticalsettingsindifferentsectors.Thelawmakers’decisionswillthushavetoconsiderawideandprofoundimpactoftheirregulations,ontheprotectionofcitizenspersonaldataandprivacyontheonehand,andonthepaceofdevelopmentinawiderfieldofAItechnologies:research,developmentandapplicationandthecomparativeadvantagesanddisadvantagesvis-a-visotherregionsandtheglobalcentresoftheAItechnologiesdevelopment.STimpactonsocietyAstechnologiesareneversociallyneutralandneedtobeacceptedbysocietyinordertobeadopted,technologicaladvancementsasdescribedinthisdocu-mentarenotexclusivelytechnicalones,butneedtobeaccompaniedbyprogressfromthehumanities.Multi-disciplinaryapproaches,asdemonstratedbytheriseofthedig-italhumanitiesmayproveadvantageousalsointhesescenarios.Assystemsbecomenaturalcompanions,thefieldsofpsychology,neuroscienceandphilosophywillbringnewaspectsandvisionstotheagendaandinspirenovelapproaches.Fearandanxietiesgeneratedbyoverlyaggressivemarketing,science-fictionanddisinformationneedtobemetwithprudenttransparency,adequatemanagementofexpectationsandaccom-panyingpolicymeasures.AninclusiveapproachinthesenseofmakingST(andAI)visible,transparentandunderstandabletoalargerpublic–akindofAI-literacyinthesenseofmedia-literacy–maybeastrongsupportingtopicforallabove-mentioneddomains.AnincreaseintransparencymaybeexpectedtoleadtochangesinwhatisperceivedasNLU(orAI):adeeperknowledgeofalgorithmsandmodelsmadechangethenotionofwhatintelligencepersemeans–muchaswhenviewingamosaicupcloseandstatingthatitis“amerecollectionofsmalltilesandsomemudinbetween”,ratherthanmarvellingatthebyzantinemosaicsoftheHagiaSophiafrombelow.Peoplehavealwaystendedtohumanisemachines.Morepowerfulsystemsformedbythecombinationandintegrationoftechnologiesandcomponentsdescribedabovemayeffectivelybeattributedhuman-likequalitiesandpersonhoodbytheirusers.Itisim-perativethatethicalaspectsofsuchinteractionalsobeaddressedinparallelwithtech-nologicalprogress.Transparency,e.g.,bychatbotsintroducingthemselvesandstatingclearlythattheyareamachine,andopennessisamongthekeyfactorstobeconsideredwhenleavingusersafreedomofchoiceratherthanimposingtechnologyonthem.ThisWP2:EuropeanLanguageEquality–TheFutureSituationin203052D2.14:TechnologyDeepDive–SpeechTechnologiescertainlyreachesfarbeyondSTbutratherconcernsAIingeneral.Lastly,intryingtoaddressthegoalsofestablishingDLEandaimingtomeasureitsimportance,weshouldmaybealsoaskourselveswhattheconsequencesandeffectswouldbeofNOTdoingso.ReferencesHassanAkbari,LinagzheYuan,RuiQuian,Wei-HongChuang,Shih-FuChang,YinCui,andGongBo-quinq.Vatt:Transformersformultimodalself-supervisedlearningfromrawvideo,audioandtext.arXivpreprintarXiv:2104:11178,2021.MehmetBerkehanAkçayandKayaOğuz.Speechemotionrecognition:Emotionalmodels,databases,features,preprocessingmethods,supportingmodalities,andclassifiers.SpeechCommunication,116:56–76,2020.AlënaAksënova,DaanvanEsch,JamesFlynn,andPavelGolik.Howmightwecreatebetterbenchmarksforspeechrecognition?InProceedingsofthe1stWorkshoponBenchmarking:Past,PresentandFuture,pages22–34,2021.SamuelAlbanie,ArshaNagrani,AndreaVedaldi,andAndrewZisserman.Emotionrecognitioninspeechusingcross-modaltransferinthewild.InProceedingsofthe26thACMinternationalcon-ferenceonMultimedia,pages292–301,2018.MeravAllouch,AmosAzaria,andRinaAzoulay.Conversationalagents:Goals,technologies,visionandchallenges.Sensors,21(24):8448,2021.TawfiqAmmari,JofishKaye,JaniceYTsai,andFrankBentley.Music,search,andiot:Howpeople(really)usevoiceassistants.ACMTransactionsonComputer-HumanInteraction,2019.ElisabethAndre,MatthiasRehm,WolfgangMinker,andDirkBühler.Endowingspokenlanguagedia-loguesystemswithemotionalintelligence.InTutorialandResearchWorkshoponAffectiveDialogueSystems,pages178–187.Springer,2004.TursunovAnvarjon,SoonilKwon,etal.Deep-net:Alightweightcnn-basedspeechemotionrecognitionsystemusingdeepfrequencyfeatures.Sensors,20(18):5212,2020.SercanÖArık,MikeChrzanowski,AdamCoates,GregoryDiamos,AndrewGibiansky,YongguoKang,XianLi,JohnMiller,AndrewNg,JonathanRaiman,etal.Deepvoice:Real-timeneuraltext-to-speech.InInternationalConferenceonMachineLearning,pages195–204.PMLR,2017.AlexeiBaevski,YuhaoZhou,AbdelrahmanMohamed,andMichaelAuli.wav2vec2.0:Aframeworkforself-supervisedlearningofspeechrepresentations.AdvancesinNeuralInformationProcessingSystems,33,2020.DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.Neuralmachinetranslationbyjointlylearn-ingtoalignandtranslate.arXivpreprintarXiv:1409.0473,2014.AntonBatliner,BjörnSchuller,DinoSeppi,StefanSteidl,LaurenceDevillers,LaurenceVidrascu,ThuridVogt,VeredAharonson,andNoamAmir.Theautomaticrecognitionofemotionsinspeech.InEmotion-OrientedSystems,pages71–99.Springer,2011.EricBattenberg,RJSkerry-Ryan,SorooshMariooryad,DaisyStanton,DavidKao,MattShannon,andTomBagby.Location-relativeattentionmechanismsforrobustlong-formspeechsynthesis.InICASSP2020-2020IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages6194–6198.IEEE,2020.BillalBelainine,FatihaSadat,andHakimLounis.Modellingaconversationalagentwithcomplexemo-tionalintelligence.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages13710–13711,2020.WP2:EuropeanLanguageEquality–TheFutureSituationin203053D2.14:TechnologyDeepDive–SpeechTechnologiesChristianBenoît,MartineGrice,andValerieHazan.Thesustest:Amethodfortheassessmentoftext-to-speechsynthesisintelligibilityusingsemanticallyunpredictablesentences.Speechcommunication,18(4):381–392,1996.RishiBommasani,DrewAHudson,EhsanAdeli,RussAltman,SimranArora,SydneyvonArx,MichaelSBernstein,JeannetteBohg,AntoineBosselut,EmmaBrunskill,etal.Ontheopportunitiesandrisksoffoundationmodels.arXivpreprintarXiv:2108.07258,2021.MartinBorchertandAntjeDusterhoft.Emotionsinspeech-experimentswithprosodyandqualityfea-turesinspeechforuseincategoricalanddimensionalemotionrecognitionenvironments.In2005InternationalConferenceonNaturalLanguageProcessingandKnowledgeEngineering,pages147–151.IEEE,2005.CassiaValentiniBotinhaoandSimonKing.Detectionandanalysisofattentionerrorsinsequence-to-sequencetext-to-speech.InInterspeech2021:The22ndAnnualConferenceoftheInternationalSpeechCommunicationAssociation,2021.HerveBourlardandNelsonMorgan.Continuousspeechrecognitionbyconnectioniststatisticalmeth-ods.IEEETransactionsonNeuralNetworks,4(6):893–909,1993.TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal.Languagemodelsarefew-shotlearners.Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.FelixBurkhardt,AstridPaeschke,MiriamRolfes,WalterFSendlmeier,andBenjaminWeiss.Adatabaseofgermanemotionalspeech.InNinthEuropeanConferenceonSpeechCommunicationandTechnol-ogy,2005.CarlosBusso,MurtazaBulut,Chi-ChunLee,AbeKazemzadeh,EmilyMower,SamuelKim,JeannetteNChang,SungbokLee,andShrikanthSNarayanan.Iemocap:Interactiveemotionaldyadicmotioncapturedatabase.Languageresourcesandevaluation,42(4):335–359,2008.JuliaCambre,JessicaColnago,JimMaddock,JaniceTsai,andJofishKaye.Choiceofvoices:Alarge-scaleevaluationoftext-to-speechvoicequalityforlong-formcontent.InProceedingsofthe2020CHIConferenceonHumanFactorsinComputingSystems,pages1–13,2020.YuewenCao,XixinWu,SongxiangLiu,JianweiYu,XuLi,ZhiyongWu,XunyingLiu,andHelenMeng.End-to-endcode-switchedttswithmixofmonolingualrecordings.InICASSP2019-2019IEEEInter-nationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages6935–6939,2019.EdressonCasanova,ChristopherShulby,ErenGölge,NicolasMichaelMüller,FredericoSantosdeOliveira,ArnaldoCandidoJr.,AndersondaSilvaSoares,SandraMariaAluisio,andMoacirAn-tonelliPonti.SC-GlowTTS:AnEfficientZero-ShotMulti-SpeakerText-To-SpeechModel.InProc.In-terspeech2021,pages3645–3649,2021.AlejandroCatala,DenieceS.Nazareth,PauloFélix,KhietP.Truong,andGerbenJ.Westerhof.Emobook:Amultimedialifestorybookappforreminiscenceintervention.In22ndInternationalConferenceonHuman-ComputerInteractionwithMobileDevicesandServices,MobileHCI’20.AssociationforComputingMachinery,2020.ISBN9781450380522.doi:10.1145/3406324.3410717.WilliamChan,NavdeepJaitly,QuocVLe,andOriolVinyals.Listen,attendandspell.arXivpreprintarXiv:1508.01211,2015.Yuan-JuiChen,TaoTu,ChengchiehYeh,andHung-YiLee.End-to-EndText-to-SpeechforLow-ResourceLanguagesbyCross-LingualTransferLearning.InProc.Interspeech2019,pages2075–2079,2019.PochunHsuandHungyiLee.WG-WaveNet:Real-TimeHigh-FidelitySpeechSynthesisWithoutGPU.InProc.Interspeech2020,pages210–214,2020.Yu-AnChung,YuxuanWang,Wei-NingHsu,YuZhang,andRJSkerry-Ryan.Semi-supervisedtrainingforimprovingdataefficiencyinend-to-endspeechsynthesis.InICASSP2019-2019IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages6940–6944.IEEE,2019.WP2:EuropeanLanguageEquality–TheFutureSituationin203054D2.14:TechnologyDeepDive–SpeechTechnologiesLeighClark,NadiaPantidi,OrlaCooney,PhilipDoyle,DiegoGaraialde,JustinEdwards,BrendanSpillane,EmerGilmartin,ChristineMurad,CosminMunteanu,etal.Whatmakesagoodconversa-tion?challengesindesigningtrulyconversationalagents.InProceedingsofthe2019CHIConferenceonHumanFactorsinComputingSystems,pages1–12,2019a.RobClark,HannaSilen,TomKenter,andRalphLeith.EvaluatingLong-formText-to-Speech:Compar-ingtheRatingsofSentencesandParagraphs.InProc.10thISCAWorkshoponSpeechSynthesis(SSW10),pages99–104,2019b.BenjaminRCowan,NadiaPantidi,DavidCoyle,KellieMorrissey,PeterClarke,SaraAl-Shehri,DavidEarley,andNatashaBandeira.”whatcanihelpyouwith?”infrequentusers’experiencesofintel-ligentpersonalassistants.InProceedingsofthe19thInternationalConferenceonHuman-ComputerInteractionwithMobileDevicesandServices,pages1–12,2017.NicholasCummins,StefanScherer,JarekKrajewski,SebastianSchnieder,JulienEpps,andThomasFQuatieri.Areviewofdepressionandsuicideriskassessmentusingspeechanalysis.SpeechCommu-nication,71:10–49,2015.XudongDai,ChengGong,LongbiaoWang,andKailiZhang.Informationsieve:Contentleakagereduc-tioninend-to-endprosodyforexpressivespeechsynthesis.arXivpreprintarXiv:2108.01831,2021.PoornaBanerjeeDasgupta.Detectionandanalysisofhumanemotionsthroughvoiceandspeechpat-ternprocessing.arXivpreprintarXiv:1710.10198,2017.TusarKantiDash,SoumyaMishra,GanapatiPanda,andSureshChandraSatapathy.Detectionofcovid-19fromspeechsignalusingbio-inspiredbasedcepstralfeatures.PatternRecognition,117,2021.doi:https://doi.org/10.1016/j.patcog.2021.107999.SofiadelaFuenteGarcia,CraigRitchie,andSaturninoLuz.Artificialintelligence,speech,andlan-guageprocessingapproachestomonitoringalzheimer’sdisease:asystematicreview.JournalofAlzheimer’sDisease,pages1–27,2020.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.Bert:Pre-trainingofdeepbidirec-tionaltransformersforlanguageunderstanding.AssociationforComputationalLinguistics,2018.AbhinavDhall,RolandGoecke,JyotiJoshi,MichaelWagner,andTomGedeon.Emotionrecognitioninthewildchallenge2013.InProceedingsofthe15thACMonInternationalconferenceonmultimodalinteraction,pages509–516,2013.LinhaoDong,ShuangXu,andBoXu.Speech-transformer:ano-recurrencesequence-to-sequencemodelforspeechrecognition.In2018IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages5884–5888.IEEE,2018.ChristophDraxler,HenkvandenHeuvel,ArjanvanHessen,SilviaCalamai,andLouiseCorti.ACLARINtranscriptionportalforinterviewdata.InProceedingsofthe12thLanguageResourcesandEvaluationConference,pages3353–3359,Marseille,France,May2020.EuropeanLanguageResourcesAssocia-tion.ISBN979-10-95546-34-4.URLhttps://aclanthology.org/2020.lrec-1.411.SidneyD’MelloandArtGraesser.Dynamicsofaffectivestatesduringcomplexlearning.LearningandInstruction,22(2):145–157,2012.FranciscaFPessanhaandAlmilaAkdagSalah.Acomputationallookatoralhistoryarchives.JournalonComputingandCulturalHeritage,15(1),2022.doi:https://doi.org/10.1145/3477605.LucianoFloridi.Aianditsnewwinter:frommythstorealities.Philosophy&Technology,33(1):1–3,2020.RadhikaGargandSubhasreeSengupta.Heisjustlikeme:astudyofthelong-termuseofsmartspeakersbyparentsandchildren.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies,4(1):1–24,2020.WP2:EuropeanLanguageEquality–TheFutureSituationin203055D2.14:TechnologyDeepDive–SpeechTechnologiesMahaultGarnerin,SolangeRossato,andLaurentBesacier.Investigatingtheimpactofgenderrepre-sentationinasrtrainingdata:acasestudyonlibrispeech.In3rdWorkshoponGenderBiasinNaturalLanguageProcessing,pages86–92.AssociationforComputationalLinguistics,2021.MiguelángelVerdeGarrido.Whyamilitantlydemocraticlackoftrustinstatesurveillancecanenablebetterandmoredemocraticsecurity.InTrustandTransparencyinanAgeofSurveillance,pages221–240.Routledge,2021.TheodorosGiannakopoulos,AggelosPikrakis,andSergiosTheodoridis.Adimensionalapproachtoemotionrecognitionofspeechfrommovies.In2009IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing,pages65–68.IEEE,2009.AndrewGibiansky,SercanÖmerArik,GregoryFrederickDiamos,JohnMiller,KainanPeng,WeiPing,JonathanRaiman,andYanqiZhou.Deepvoice2:Multi-speakerneuraltext-to-speech.InProceedingsofNIPS,2017.JohnJGodfrey,EdwardCHolliman,andJaneMcDaniel.Switchboard:Telephonespeechcorpusforresearchanddevelopment.InAcoustics,Speech,andSignalProcessing,IEEEInternationalConferenceon,volume1,pages517–520.IEEEComputerSociety,1992.MGoldstein.Classificationofmethodsusedforassessmentoftext-to-speechsystemsaccordingtothedemandsplacedonthelistener.Speechcommunication,16(3):225–244,1995.GáborGosztolya,VeronikaVincze,LászlóTóth,MagdolnaPákáski,JánosKálmán,andIldikóHoffmann.Identifyingmildcognitiveimpairmentandmildalzheimer’sdiseasebasedonspontaneousspeechusingasrandlinguisticfeatures.ComputerSpeech&Language,53:181–197,2019.AvashnaGovenderandSimonKing.MeasuringtheCognitiveLoadofSyntheticSpeechUsingaDualTaskParadigm.InProc.Interspeech2018,pages2843–2847,2018.NamanGoyal,JingfeiDu,MyleOtt,GiriAnantharaman,andAlexisConneau.Larger-scaletransformersformultilingualmaskedlanguagemodeling.arXivpreprintarXiv:2105.00572,2021.AlexGraves,SantiagoFernández,FaustinoGomez,andJürgenSchmidhuber.Connectionisttemporalclassification:labellingunsegmentedsequencedatawithrecurrentneuralnetworks.InProceedingsofthe23rdinternationalconferenceonMachinelearning,pages369–376,2006.FrantisekGrézl,MartinKarafiát,StanislavKontár,andJanCernocky.Probabilisticandbottle-neckfeaturesforlvcsrofmeetings.In2007IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing-ICASSP’07,volume4,pagesIV–757.IEEE,2007.DanielGriffinandJaeLim.Signalestimationfrommodifiedshort-timefouriertransform.IEEETrans-actionsonacoustics,speech,andsignalprocessing,32(2):236–243,1984.MichaelGrimm,KristianKroschel,andShrikanthNarayanan.Theveraammittaggermanaudio-visualemotionalspeechdatabase.In2008IEEEinternationalconferenceonmultimediaandexpo,pages865–868.IEEE,2008.JiuxiangGu,ZhenhuaWang,JasonKuen,LianyangMa,AmirShahroudy,BingShuai,TingLiu,XingxingWang,GangWang,JianfeiCai,etal.Recentadvancesinconvolutionalneuralnetworks.PatternRecognition,77:354–377,2018.AndreaGuidi,ClaudioGentili,EnzoPasqualeScilingo,andNicolaVanello.Analysisofspeechfea-turesandpersonalitytraits.BiomedicalSignalProcessingandControl,51:1–7,2019.ISSN1746-8094.doi:https://doi.org/10.1016/j.bspc.2019.01.027.URLhttps://www.sciencedirect.com/science/article/pii/S1746809419300230.AnmolGulati,JamesQin,Chung-ChengChiu,NikiParmar,YuZhang,JiahuiYu,WeiHan,ShiboWang,ZhengdongZhang,YonghuiWu,andRuomingPang.Conformer:Convolution-augmentedTrans-formerforSpeechRecognition.InProc.Interspeech2020,pages5036–5040,2020.doi:10.21437/Interspeech.2020-3015.URLhttp://dx.doi.org/10.21437/Interspeech.2020-3015.WP2:EuropeanLanguageEquality–TheFutureSituationin203056D2.14:TechnologyDeepDive–SpeechTechnologiesFasihHaider,SenjaPollak,PierreAlbert,andSaturninoLuz.Emotionrecognitioninlow-resourcesettings:Anevaluationofautomaticfeatureselectionmethods.ComputerSpeech&Language,65:101119,2021.MutianHe,YanDeng,andLeiHe.RobustSequence-to-SequenceAcousticModelingwithStepwiseMonotonicAttentionforNeuralTTS.InProc.Interspeech2019,pages1293–1297,2019.JamesHendler.Avoidinganotheraiwinter.IEEEIntelligentSystems,23(02):2–4,2008.HynekHermansky,DanielPWEllis,andSangitaSharma.Tandemconnectionistfeatureextractionforconventionalhmmsystems.In2000IEEEinternationalconferenceonacoustics,speech,andsignalprocessing.Proceedings(Cat.No.00CH37100),volume3,pages1635–1638.IEEE,2000.MargaretHu.Cambridgeanalytica’sblackbox.BigData&Society,7(2):2053951720938091,2020.Kun-YiHuang,Chung-HsienWu,andMing-HsiangSu.Attention-basedconvolutionalneuralnetworkandlongshort-termmemoryforshort-termdetectionofmooddisordersbasedonelicitedspeechresponses.PatternRecognition,88:668–678,2019.AndrewJaegle,SebastianBorgeaud,Jean-BaptisteAlayrac,CarlDoersch,CatalinIonescu,DavidDing,SkandaKoppula,DanielZoran,AndrewBrock,EvanShelhamer,etal.Perceiverio:Ageneralarchi-tectureforstructuredinputs&outputs.arXivpreprintarXiv:2107.14795,2021.WonJang,DanLim,JaesamYoon,BongwanKim,andJuntaeKim.UnivNet:ANeuralVocoderwithMulti-ResolutionSpectrogramDiscriminatorsforHigh-FidelityWaveformGeneration.InProc.In-terspeech2021,pages2207–2211,2021.FrederickJelinek.StatisticalMethodsforSpeechRecognition.MITPress,Cambridge,MA,USA,1998.ISBN0262100665.NalKalchbrenner,ErichElsen,KarenSimonyan,SebNoury,NormanCasagrande,EdwardLockhart,FlorianStimberg,AaronOord,SanderDieleman,andKorayKavukcuoglu.Efficientneuralaudiosynthesis.InInternationalConferenceonMachineLearning,pages2410–2419.PMLR,2018.NaoyukiKanda,YasheshGaur,XiaofeiWang,ZhongMeng,ZhuoChen,TianyanZhou,andTakuyaYosh-ioka.Jointspeakercounting,speechrecognition,andspeakeridentificationforoverlappedspeechofanynumberofspeakers.arXivpreprintarXiv:2006.10930,2020.NaoyukiKanda,XuankaiChang,YasheshGaur,XiaofeiWang,ZhongMeng,ZhuoChen,andTakuyaYoshioka.Investigationofend-to-endspeaker-attributedasrforcontinuousmulti-talkerrecordings.In2021IEEESpokenLanguageTechnologyWorkshop(SLT),pages809–816.IEEE,2021a.NaoyukiKanda,GuoliYe,YuWu,YasheshGaur,XiaofeiWang,ZhongMeng,ZhuoChen,andTakuyaYoshioka.Large-scalepre-trainingofend-to-endmulti-talkerasrformeetingtranscriptionwithsin-gledistantmicrophone.arXivpreprintarXiv:2103.16776,2021b.ShigekiKarita,NanxinChen,TomokiHayashi,TakaakiHori,HirofumiInaguma,ZiyanJiang,MasaoSomeki,NelsonEnriqueYaltaSoplin,RyuichiYamamoto,XiaofeiWang,etal.Acomparativestudyontransformervsrnninspeechapplications.In2019IEEEAutomaticSpeechRecognitionandUn-derstandingWorkshop(ASRU),pages449–456.IEEE,2019.RuhulAminKhalil,EdwardJones,MohammadInayatullahBabar,TariqullahJan,MohammadHaseebZafar,andThamerAlhussain.Speechemotionrecognitionusingdeeplearningtechniques:Areview.IEEEAccess,7:117327–117345,2019.AlexandraKonig,AharonSatt,AlexSorin,RanHoory,AlexandreDerreumaux,RenaudDavid,andPhillippeHRobert.Useofspeechanalyseswithinamobileapplicationfortheassessmentofcogni-tiveimpairmentinelderlypeople.CurrentAlzheimerResearch,15(2):120–129,2018.ShashidharGKoolagudiandKSreenivasaRao.Emotionrecognitionfromspeech:areview.Interna-tionaljournalofspeechtechnology,15(2):99–117,2012.WP2:EuropeanLanguageEquality–TheFutureSituationin203057D2.14:TechnologyDeepDive–SpeechTechnologiesJeanKossaifi,GeorgiosTzimiropoulos,SinisaTodorovic,andMajaPantic.Afew-vadatabaseforvalenceandarousalestimationin-the-wild.ImageandVisionComputing,65:23–36,2017.TheodorosKostoulas,TodorGanchev,andNikosFakotakis.Studyonspeaker-independentemotionrecognitionfromspeechonreal-worlddata.InVerbalandnonverbalfeaturesofhuman-humanandhuman-machineinteraction,pages235–242.Springer,2008.KundanKumar,RitheshKumar,ThibaultdeBoissiere,LucasGestin,WeiZhenTeoh,JoseSotelo,AlexandredeBrébisson,YoshuaBengio,andAaronCCourville.Melgan:Generativeadversarialnet-worksforconditionalwaveformsynthesis.InH.Wallach,H.Larochelle,A.Beygelzimer,F.d'Alché-Buc,E.Fox,andR.Garnett,editors,AdvancesinNeuralInformationProcessingSystems,volume32.CurranAssociates,Inc.,2019.PuneetKumar,SidharthJain,BalasubramanianRaman,ParthaPratimRoy,andMasakazuIwamura.End-to-endtripletlossbasedemotionembeddingsystemforspeechemotionrecognition.In202025thInternationalConferenceonPatternRecognition(ICPR),pages8766–8773.IEEE,2021.Oh-WookKwon,KwokleungChan,JiucangHao,andTe-WonLee.Emotionrecognitionbyspeechsig-nals.InEighthEuropeanconferenceonspeechcommunicationandtechnology,2003.Cheng-IJeffLai,YangZhang,AlexanderHLiu,ShiyuChang,Yi-LunLiao,Yung-SungChuang,KaizhiQian,SameerKhurana,DavidCox,andJamesGlass.Parp:Prune,adjustandre-pruneforself-supervisedspeechrecognition.arXivpreprintarXiv:2106.05933,2021.SiddiqueLatif,RajibRana,SaraKhalifa,RajaJurdak,JulienEpps,andBjórnWolfgangSchuller.Multi-tasksemi-supervisedadversarialautoencodingforspeechemotionrecognition.IEEETransactionsonAffectiveComputing,2020.DucLe,GilKeren,JulianChan,JayMahadeokar,ChristianFuegen,andMichaelLSeltzer.Deepshallowfusionforrnn-tpersonalization.In2021IEEESpokenLanguageTechnologyWorkshop(SLT),pages251–257.IEEE,2021.SinaeLee,JangwoonPark,andDuganUm.Speechcharacteristicsasindicatorsofpersonalitytraits.AppliedSciences,11(18):8776,2021.XiangLi,ChangheSong,JingbeiLi,ZhiyongWu,JiaJia,andHelenMeng.TowardsMulti-ScaleStyleControlforExpressiveSpeechSynthesis.InProc.Interspeech2021,pages4673–4677,2021.StevenRLivingstoneandFrankARusso.Theryersonaudio-visualdatabaseofemotionalspeechandsong(ravdess):Adynamic,multimodalsetoffacialandvocalexpressionsinnorthamericanenglish.PloSone,13(5):e0196391,2018.SaturninoLuz,FasihHaider,SofiadelaFuente,DavidaFromm,andBrianMacWhinney.Alzheimer’sdementiarecognitionthroughspontaneousspeech:theadresschallenge.arXivpreprintarXiv:2004.06833,2020.FrançoisMairesse,MarilynAWalker,MatthiasRMehl,andRogerKMoore.Usinglinguisticcuesfortheautomaticrecognitionofpersonalityinconversationandtext.Journalofartificialintelligenceresearch,30:457–500,2007.SoumiMaiti,ErikMarchi,andAlistairConkie.Generatingmultilingualvoicesusingspeakerspacetranslationbasedonbilingualspeakerdata.InICASSP2020-2020IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages7624–7628,2020.GeorgiaManiati,NikolaosEllinas,KonstantinosMarkopoulos,GeorgiosVamvoukakis,JuneSigSung,HyoungminPark,AimiliosChalamandaris,andPirrosTsiakoulis.Cross-LingualLowResourceSpeakerAdaptationUsingPhonologicalFeatures.InProc.Interspeech2021,pages1594–1598,2021.OlivierMartin,IreneKotsia,BenoitMacq,andIoannisPitas.Theenterface’05audio-visualemotiondatabase.In22ndInternationalConferenceonDataEngineeringWorkshops(ICDEW’06),pages8–8.IEEE,2006.WP2:EuropeanLanguageEquality–TheFutureSituationin203058D2.14:TechnologyDeepDive–SpeechTechnologiesGaryMcKeown,MichelValstar,RoddyCowie,MajaPantic,andMarcSchroder.Thesemainedatabase:Annotatedmultimodalrecordsofemotionallycoloredconversationsbetweenapersonandalimitedagent.IEEEtransactionsonaffectivecomputing,3(1):5–17,2011.CarlosMena,AndreaDeMarco,ClaudiaBorg,LonnekevanderPlas,andAlbertGatt.Dataaugmentationforspeechrecognitioninmaltese:Alow-resourceperspective.arXive-prints,pagesarXiv–2111,2021.GelarehMohammadiandAlessandroVinciarelli.Automaticpersonalityperception:Predictionoftraitattributionbasedonprosodicfeatures.IEEETransactionsonAffectiveComputing,3(3):273–284,2012.DevangSRamMohan,VivianHu,TianHueyTeh,AlexandraTorresquintero,ChristopherGRWallis,MarleneStaib,LorenzoFoglianti,JiamengGao,andSimonKing.Ctrl-p:Temporalcontrolofprosodicvariationforspeechsynthesis.arXivpreprintarXiv:2106.08352,2021.HannahMuckenhirn,VinayakAbrol,MathewMagimai-Doss,andSébastienMarcel.Understandingandvisualizingrawwaveform-basedcnns.InInterspeech,pages2345–2349,2019.LukeMuehlhauser.Whatshouldwelearnfrompastaiforecasts.OpenPhilanthropyProject,2016.EliyaNachmaniandLiorWolf.Unsupervisedpolyglottext-to-speech.InICASSP2019-2019IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages7055–7059,2019.YishuangNing,ShengHe,ZhiyongWu,ChunxiaoXing,andLiang-JieZhang.Areviewofdeeplearningbasedspeechsynthesis.AppliedSciences,9(19):4050,2019.CaglarOflazogluandSerdarYildirim.Recognizingemotionfromturkishspeechusingacousticfea-tures.EURASIPJournalonAudio,Speech,andMusicProcessing,2013(1):1–11,2013.VassilPanayotov,GuoguoChen,DanielPovey,andSanjeevKhudanpur.Librispeech:anasrcorpusbasedonpublicdomainaudiobooks.In2015IEEEinternationalconferenceonacoustics,speechandsignalprocessing(ICASSP),pages5206–5210.IEEE,2015.KyubyongParkandThomasMulc.Css10:Acollectionofsinglespeakerspeechdatasetsfor10lan-guages.arXivpreprintarXiv:1903.11269,2019.TaeJinPark,NaoyukiKanda,DimitriosDimitriadis,KyuJHan,ShinjiWatanabe,andShrikanthNarayanan.Areviewofspeakerdiarization:Recentadvanceswithdeeplearning.ComputerSpeech&Language,72:101317,2022.DipjyotiPaul,YannisPantazis,andYannisStylianou.SpeakerConditionalWaveRNN:TowardsUniver-salNeuralVocoderforUnseenSpeakerandRecordingConditions.InProc.Interspeech2020,pages235–239,2020.M.KathleenPichora-FullerandKateDupuis.Torontoemotionalspeechset(TESS),2020.URLhttps://doi.org/10.5683/SP2/E8H2MF.WeiPing,KainanPeng,andJitongChen.Clarinet:Parallelwavegenerationinend-to-endtext-to-speech.InInternationalConferenceonLearningRepresentations,2018a.WeiPing,KainanPeng,AndrewGibiansky,SercanOArik,AjayKannan,SharanNarang,JonathanRaiman,andJohnMiller.Deepvoice3:2000-speakerneuraltext-to-speech.InProceedingsofICLR,pages214–217,2018b.TimPolzehl,SebastianMöller,andFlorianMetze.Automaticallyassessingpersonalityfromspeech.In2010IEEEFourthInternationalConferenceonSemanticComputing,pages134–140.IEEE,2010.MartinPorcheron,JoelEFischer,StuartReeves,andSarahSharples.Voiceinterfacesineverydaylife.Inproceedingsofthe2018CHIconferenceonhumanfactorsincomputingsystems,pages1–12,2018.DanielPovey,ArnabGhoshal,GillesBoulianne,LukasBurget,OndrejGlembek,NagendraGoel,MirkoHannemann,PetrMotlicek,YanminQian,PetrSchwarz,etal.Thekaldispeechrecognitiontoolkit.InIEEE2011workshoponautomaticspeechrecognitionandunderstanding.IEEESignalProcessingSociety,2011.WP2:EuropeanLanguageEquality–TheFutureSituationin203059D2.14:TechnologyDeepDive–SpeechTechnologiesRyanPrenger,RafaelValle,andBryanCatanzaro.Waveglow:Aflow-basedgenerativenetworkforspeechsynthesis.InICASSP2019-2019IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages3617–3621.IEEE,2019.MaríaLuisaBarragánPulido,JesúsBernardinoAlonsoHernández,MiguelÁngelFerrerBallester,Car-losManuelTraviesoGonzález,JiříMekyska,andZdeněkSmékal.Alzheimer’sdiseaseandautomaticspeechanalysis:areview.Expertsystemswithapplications,150:113213,2020.JianweiQian,HaohuaDu,JiahuiHou,LinlinChen,TaehoJung,andXiang-YangLi.Hidebehind:Enjoyvoiceinputwithvoiceprintunclonabilityandanonymity.InProceedingsofthe16thACMConferenceonEmbeddedNetworkedSensorSystems,pages82–94,2018.AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal.Learningtransferablevisualmodelsfromnaturallanguagesupervision.InInternationalConferenceonMachineLearning,pages8748–8763.PMLR,2021.AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,andIlyaSutskever.Zero-shottext-to-imagegeneration.InInternationalConferenceonMachineLearning,pages8821–8831.PMLR,2021.ITUTRec.P.800.1,meanopinionscore(mos)terminology.InternationalTelecommunicationUnion,Geneva,2006.GeneralDataProtectionRegulation.Regulationeu2016/679oftheeuropeanparliamentandofthecouncilof27april2016.OfficialJournaloftheEuropeanUnion,2016.YiRen,YangjunRuan,XuTan,TaoQin,ShengZhao,ZhouZhao,andTie-YanLiu.Fastspeech:Fast,robustandcontrollabletexttospeech.InH.Wallach,H.Larochelle,A.Beygelzimer,F.d'Alché-Buc,E.Fox,andR.Garnett,editors,AdvancesinNeuralInformationProcessingSystems,volume32.CurranAssociates,Inc.,2019a.YiRen,YangjunRuan,XuTan,TaoQin,ShengZhao,ZhouZhao,andTie-YanLiu.Fastspeech:Fast,robustandcontrollabletexttospeech.arXivpreprintarXiv:1905.09263,2019b.YiRen,ChenxuHu,XuTan,TaoQin,ShengZhao,ZhouZhao,andTie-YanLiu.Fastspeech2:Fastandhigh-qualityend-to-endtexttospeech.arXivpreprintarXiv:2006.04558,2020.SlobodanRibaric,AladdinAriyaeeinia,andNikolaPavesic.De-identificationforprivacyprotectioninmultimediacontent:Asurvey.SignalProcessing:ImageCommunication,47:131–151,2016.DanielRigney.TheMattheweffect:Howadvantagebegetsfurtheradvantage.ColumbiaUniversityPress,2010.FabienRingeval,AndreasSonderegger,JuergenSauer,andDenisLalanne.Introducingtherecolamul-timodalcorpusofremotecollaborativeandaffectiveinteractions.In201310thIEEEinternationalconferenceandworkshopsonautomaticfaceandgesturerecognition(FG),pages1–8.IEEE,2013.MorganeRiviere,JadeCopet,andGabrielSynnaeve.Asr4real:Anextendedbenchmarkforspeechmodels.arXivpreprintarXiv:2110.08583,2021.PhilippVRouast,MarcAdam,andRaymondChiong.Deeplearningforhumanaffectrecognition:Insightsandnewdevelopments.IEEETransactionsonAffectiveComputing,2019.NicholasRuiz,MattiaAntoninoDiGangi,NicolaBertoldi,andMarcelloFederico.Assessingthetol-eranceofneuralmachinetranslationsystemsagainstspeechrecognitionerrors.arXivpreprintarXiv:1904.10997,2019.RalfSchlüter.SurveyTalk:ModelinginAutomaticSpeechRecognition:BeyondHiddenMarkovMod-els.InProc.Interspeech2019,2019.WP2:EuropeanLanguageEquality–TheFutureSituationin203060D2.14:TechnologyDeepDive–SpeechTechnologiesMarcSchröder.Dimensionalemotionrepresentationasabasisforspeechsynthesiswithnon-extremeemotions.InTutorialandresearchworkshoponaffectivedialoguesystems,pages209–220.Springer,2004.BjörnSchuller,AntonBatliner,StefanSteidl,andDinoSeppi.Recognisingrealisticemotionsandaffectinspeech:Stateoftheartandlessonslearntfromthefirstchallenge.Speechcommunication,53(9-10):1062–1087,2011.BjörnSchuller,AntonBatliner,ChristianBergler,Eva-MariaMessner,AntoniaHamilton,ShahinAmiriparian,AliceBaird,GeorgiosRizos,MaximilianSchmitt,LukasStappen,HaraldBaumeister,AlexisDeightonMacIntyre,andSimoneHantke.Theinterspeech2020computationalparalinguis-ticschallenge:Elderlyemotion,breathing&masks.InINTERSPEECH,2020.BjörnWSchuller,AntonBatliner,ChristianBergler,CeciliaMascolo,JingHan,IuliaLefter,HeysemKaya,ShahinAmiriparian,AliceBaird,LukasStappen,etal.Theinterspeech2021computationalparalinguisticschallenge:Covid-19cough,covid-19speech,escalation&primates.arXivpreprintarXiv:2102.13468,2021.FrankSeide,GangLi,andDongYu.Conversationalspeechtranscriptionusingcontext-dependentdeepneuralnetworks.InTwelfthannualconferenceoftheinternationalspeechcommunicationassociation,2011.RicoSennrich,BarryHaddow,andAlexandraBirch.Edinburghneuralmachinetranslationsystemsforwmt16.InProceedingsoftheFirstConferenceonMachineTranslation:Volume2,SharedTaskPapers,pages371–376,2016.BenjaminSertolli,ZhaoRen,BjörnWSchuller,andNicholasCummins.Representationtransferlearn-ingfromdeepend-to-endspeechrecognitionnetworksfortheclassificationofhealthstatesfromspeech.ComputerSpeech&Language,68:101204,2021.ZengqiangShang,ZhihuaHuang,HaozheZhang,PengyuanZhang,andYonghongYan.IncorporatingCross-SpeakerStyleTransferforMulti-LanguageText-to-Speech.InProc.Interspeech2021,pages1619–1623,2021.JonathanShen,RuomingPang,RonJWeiss,MikeSchuster,NavdeepJaitly,ZonghengYang,ZhifengChen,YuZhang,YuxuanWang,RjSkerrv-Ryan,etal.Naturalttssynthesisbyconditioningwavenetonmelspectrogrampredictions.In2018IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages4779–4783.IEEE,2018.JonathanShen,YeJia,MikeChrzanowski,YuZhang,IsaacElias,HeigaZen,andYonghuiWu.Non-attentivetacotron:Robustandcontrollableneuralttssynthesisincludingunsuperviseddurationmodeling.arXivpreprintarXiv:2010.04301,2020.OlympiaSimantiraki,MartinCooke,andSimonKing.ImpactofDifferentSpeechTypesonListeningEffort.InProc.Interspeech2018,pages2267–2271,2018.MarcinSkowron,MathiasTheunis,StefanRank,andArvidKappas.Affectandsocialprocessesinon-linecommunication–experimentswithanaffectivedialogsystem.IEEETransactionsonAffectiveComputing,4(3):267–279,2013.LilliSmal,AndreaLösch,JosefvanGenabith,MariaGiagkou,ThierryDeclerck,andStephanBusemann.Languagedatasharingineuropeanpublicservices–overcomingobstaclesandcreatingsustainabledatasharinginfrastructures.InProceedingsofthe12thLanguageResourcesandEvaluationConfer-ence,pages3443–3448,2020.DavidSnyder,DanielGarcia-Romero,GregorySell,DanielPovey,andSanjeevKhudanpur.X-vectors:Robustdnnembeddingsforspeakerrecognition.In2018IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages5329–5333.IEEE,2018.WP2:EuropeanLanguageEquality–TheFutureSituationin203061D2.14:TechnologyDeepDive–SpeechTechnologiesJoseSotelo,SoroushMehri,KundanKumar,JoaoFelipeSantos,KyleKastner,AaronCourville,andYoshuaBengio.Char2wav:End-to-endspeechsynthesis.InProceedingsof5thInternationalConfer-enceonLearningRepresentations,pages1–6,2017.BrijMohanLalSrivastava,AurélienBellet,MarcTommasi,andEmmanuelVincent.Privacy-preservingadversarialrepresentationlearninginasr:Realityorillusion?InINTERSPEECH2019-20thAnnualConferenceoftheInternationalSpeechCommunicationAssociation,2019.BrijMohanLalSrivastava,NathalieVauquier,MdSahidullah,AurélienBellet,MarcTommasi,andEmmanuelVincent.Evaluatingvoiceconversion-basedprivacyprotectionagainstinformedattack-ers.InICASSP2020-2020IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages2802–2806.IEEE,2020.TitusStahl.Indiscriminatemasssurveillanceandthepublicsphere.EthicsandInformationTechnology,18(1):33–39,2016.MartinSundermeyer,HermannNey,andRalfSchlüter.Fromfeedforwardtorecurrentlstmneuralnetworksforlanguagemodeling.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,23(3):517–529,2015.MonoramaSwain,AurobindaRoutray,andPrithvirajKabisatpathy.Databases,featuresandclassifiersforspeechemotionrecognition:areview.InternationalJournalofSpeechTechnology,21(1):93–120,2018.GabrielSynnaeve,QiantongXu,JacobKahn,TatianaLikhomanenko,EdouardGrave,VineelPratap,AnuroopSriram,VitaliyLiptchinsky,andRonanCollobert.End-to-endasr:fromsupervisedtosemi-supervisedlearningwithmodernarchitectures.arXivpreprintarXiv:1911.08460,2019.ÉvaSzékely,GustavEjeHenter,JonasBeskow,andJoakimGustafson.Spontaneousconversationalspeechsynthesisfromfounddata.InINTERSPEECH,pages4435–4439,2019.DengkeTang,JunlinZeng,andMingLi.Anend-to-enddeeplearningframeworkforspeechemotionrecognitionofatypicalindividuals.InInterspeech,pages162–166,2018.AshishTawariandMohanMTrivedi.Speechemotionanalysisinnoisyreal-worldenvironment.In201020thInternationalConferenceonPatternRecognition,pages4605–4608.IEEE,2010.JasonTaylorandKorinRichmond.ConfidenceIntervalsforASR-BasedTTSEvaluation.InProc.Inter-speech2021,pages2791–2795,2021.KatrinTomanek,FrançoiseBeaufays,JulieCattiau,AngadChandorkar,andKheChaiSim.On-devicepersonalizationofautomaticspeechrecognitionmodelsfordisorderedspeech.arXivpreprintarXiv:2106.10259,2021.NataliaTomashenko,BrijMohanLalSrivastava,XinWang,EmmanuelVincent,AndreasNautsch,Ju-nichiYamagishi,NicholasEvans,JosePatino,Jean-FrançoisBonastre,Paul-GauthierNoé,etal.Thevoiceprivacy2020challengeevaluationplan,2020.LászlóTóth,IldikóHoffmann,GáborGosztolya,VeronikaVincze,GrétaSzatlóczki,ZoltánBánréti,Mag-dolnaPákáski,andJánosKálmán.Aspeechrecognition-basedsolutionfortheautomaticdetectionofmildcognitiveimpairmentfromspontaneousspeech.CurrentAlzheimerResearch,15(2):130–138,2018.ErmalToto,MLTlachac,andElkeARundensteiner.Audibert:Adeeptransferlearningmultimodalclassificationframeworkfordepressionscreening.InProceedingsofthe30thACMInternationalConferenceonInformation&KnowledgeManagement,pages4145–4154,2021.JörgenValkandTanelAlumäe.Voxlingua107:adatasetforspokenlanguagerecognition.In2021IEEESpokenLanguageTechnologyWorkshop(SLT),pages652–658.IEEE,2021.WP2:EuropeanLanguageEquality–TheFutureSituationin203062D2.14:TechnologyDeepDive–SpeechTechnologiesRafaelValle,JasonLi,RyanPrenger,andBryanCatanzaro.Mellotron:Multispeakerexpressivevoicesynthesisbyconditioningonrhythm,pitchandglobalstyletokens.InICASSP2020-2020IEEEIn-ternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages6189–6193.IEEE,2020a.RafaelValle,KevinShih,RyanPrenger,andBryanCatanzaro.Flowtron:anautoregressiveflow-basedgenerativenetworkfortext-to-speechsynthesis.arXivpreprintarXiv:2005.05957,2020b.AäronvandenOord,SanderDieleman,HeigaZen,KarenSimonyan,OriolVinyals,AlexGraves,NalKalchbrenner,AndrewSenior,andKorayKavukcuoglu.WaveNet:AGenerativeModelforRawAu-dio.InProc.9thISCAWorkshoponSpeechSynthesisWorkshop(SSW9),page125,2016.RobvanderGoot,IbrahimSharaf,AizhanImankulova,AhmetÜstün,MarijaStepanović,AlanRam-poni,SitiOryzaKhairunnisa,MamoruKomachi,andBarbaraPlank.Frommaskedlanguagemod-elingtotranslation:Non-englishauxiliarytasksimprovezero-shotspokenlanguageunderstanding.arXivpreprintarXiv:2105.07316,2021.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.InAdvancesinneuralinformationprocessingsystems,pages5998–6008,2017.VilleVestman,TomiKinnunen,RosaGonzálezHautamäki,andMdSahidullah.Voicemimicryattacksassistedbyautomaticspeakerverification.ComputerSpeech&Language,59:36–54,2020.AlessandroVinciarelli,MajaPantic,DirkHeylen,CatherinePelachaud,IsabellaPoggi,FrancescaD’Errico,andMarcSchroeder.Bridgingthegapbetweensocialanimalandunsocialmachine:Asurveyofsocialsignalprocessing.IEEETransactionsonAffectiveComputing,3(1):69–87,2011.PetraWagner,JonasBeskow,SimonBetz,JensEdlund,JoakimGustafson,GustavEjeHenter,SébastienLeMaguer,ZofiaMalisz,ÉvaSzékely,ChristinaTånnander,andJanaVoße.SpeechSynthesisEval-uation—State-of-the-ArtAssessmentandSuggestionforaNovelResearchProgram.InProc.10thISCAWorkshoponSpeechSynthesis(SSW10),pages105–110,2019.ElectraWallington,BenjiKershenbaum,PeterBell,andOndřejKlejch.Onthelearningdynamicsofsemi-supervisedtrainingforasr.InInterspeech2021:The22ndAnnualConferenceoftheInterna-tionalSpeechCommunicationAssociation,pages716–720.InternationalSpeechCommunicationAs-sociation,2021.JianyouWang,MichaelXue,RyanCulhane,EnmaoDiao,JieDing,andVahidTarokh.Speechemotionrecognitionwithdual-sequencelstmarchitecture.InICASSP2020-2020IEEEInternationalConfer-enceonAcoustics,SpeechandSignalProcessing(ICASSP),pages6474–6478.IEEE,2020.YuxuanWang,R.J.Skerry-Ryan,DaisyStanton,YonghuiWu,RonJ.Weiss,NavdeepJaitly,ZonghengYang,YingXiao,ZhifengChen,SamyBengio,QuocLe,YannisAgiomyrgiannakis,RobClark,andRifA.Saurous.Tacotron:Towardsend-to-endspeechsynthesis.InProc.Interspeech2017,pages4006–4010,2017.YuxuanWang,DaisyStanton,YuZhang,RJ-SkerryRyan,EricBattenberg,JoelShor,YingXiao,YeJia,FeiRen,andRifASaurous.Styletokens:Unsupervisedstylemodeling,controlandtransferinend-to-endspeechsynthesis.InInternationalConferenceonMachineLearning,pages5180–5189.PMLR,2018.ElisabethWehling.PolitischesFraming:WieeineNationsichihrDenkeneinredet-unddarausPoli-tikmacht.UllsteinEbooks,2018.ISBN9783843718578.URLhttps://books.google.at/books?id=tlFaDwAAQBAJ.MirjamWester,CassiaValentini-Botinhao,andGustavEjeHenter.Areweusingenoughlisteners?no!—anempirically-supportedcritiqueofinterspeech2014TTSevaluations.InProc.Interspeech2015,pages3476–3480,2015.WP2:EuropeanLanguageEquality–TheFutureSituationin203063D2.14:TechnologyDeepDive–SpeechTechnologiesMikaWesterlund,DianeAIsabelle,andSeppoLeminen.Theacceptanceofdigitalsurveillanceinanageofbigdata.TechnologyInnovationManagementReview,11(3),2021.TerryWinograd.Shiftingviewpoints:Artificialintelligenceandhuman-computerinteraction.Artif.Intell.,170:1256–1258,2006.YunhanWu,DanielRough,AnnaBleakley,JustinEdwards,OrlaCooney,PhilipRDoyle,LeighClark,andBenjaminRCowan.Seewhati’msaying?comparingintelligentpersonalassistantusefornativeandnon-nativelanguagespeakers.In22ndInternationalConferenceonHuman-ComputerInteractionwithMobileDevicesandServices,pages1–9,2020.DetaiXin,YukiSaito,ShinnosukeTakamichi,TomokiKoriyama,andHiroshiSaruwatari.Cross-LingualSpeakerAdaptationUsingDomainAdaptationandSpeakerConsistencyLossforText-To-SpeechSyn-thesis.InProc.Interspeech2021,pages1614–1618,2021.JinhyeokYang,Jae-SungBae,TaejunBak,Young-IkKim,andHoon-YoungCho.GANSpeech:AdversarialTrainingforHigh-FidelityMulti-SpeakerSpeechSynthesis.InProc.Interspeech2021,pages2202–2206,2021.ShunzhiYang,ZhengGong,KaiYe,YungenWei,ZhenhuaHuang,andZhengHuang.Edgernn:acom-pactspeechrecognitionnetworkwithspatio-temporalfeaturesforedgecomputing.IEEEAccess,8:81468–81478,2020.ChengzhuYu,HengLu,NaHu,MengYu,ChaoWeng,KunXu,PengLiu,DeyiTuo,ShiyinKang,GuangzhiLei,DanSu,andDongYu.DurIAN:DurationInformedAttentionNetworkforSpeechSynthesis.InProc.Interspeech2020,pages2027–2031,2020.HeigaZen,VietDang,RobClark,YuZhang,RonJWeiss,YeJia,ZhifengChen,andYonghuiWu.LibriTTS:AcorpusderivedfromLibriSpeechfortext-to-speech.arXivpreprintarXiv:1904.02882,2019.AlbertZeyer,ParniaBahar,KazukiIrie,RalfSchlüter,andHermannNey.Acomparisonoftransformerandlstmencoderdecodermodelsforasr.In2019IEEEAutomaticSpeechRecognitionandUnder-standingWorkshop(ASRU),pages8–15.IEEE,2019.Ya-JieZhang,ShifengPan,LeiHe,andZhen-HuaLing.Learninglatentrepresentationsforstylecontrolandtransferinend-to-endspeechsynthesis.InICASSP2019-2019IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages6945–6949.IEEE,2019a.YuZhang,RonJ.Weiss,HeigaZen,YonghuiWu,ZhifengChen,R.J.Skerry-Ryan,YeJia,AndrewRosen-berg,andBhuvanaRamabhadran.LearningtoSpeakFluentlyinaForeignLanguage:MultilingualSpeechSynthesisandCross-LanguageVoiceCloning.InProc.Interspeech2019,pages2080–2084,2019b.YuZhang,RonJ.Weiss,HeigaZen,YonghuiWu,ZhifengChen,R.J.Skerry-Ryan,YeJia,AndrewRosen-berg,andBhuvanaRamabhadran.LearningtoSpeakFluentlyinaForeignLanguage:MultilingualSpeechSynthesisandCross-LanguageVoiceCloning.InProc.Interspeech2019,pages2080–2084,2019c.YuZhang,JamesQin,DanielSPark,WeiHan,Chung-ChengChiu,RuomingPang,QuocVLe,andYonghuiWu.Pushingthelimitsofsemi-supervisedlearningforautomaticspeechrecognition.arXivpreprintarXiv:2010.10504,2020.JianfengZhao,XiaMao,andLijiangChen.Speechemotionrecognitionusingdeep1d&2dcnnlstmnetworks.BiomedicalSignalProcessingandControl,47:312–323,2019.HaoZhou,MinlieHuang,TianyangZhang,XiaoyanZhu,andBingLiu.Emotionalchattingmachine:Emotionalconversationgenerationwithinternalandexternalmemory.InThirty-SecondAAAICon-ferenceonArtificialIntelligence,2018.XuehaoZhou,XiaohaiTian,GrandeeLee,RohanKumarDas,andHaizhouLi.End-to-endcode-switchingttswithcross-linguallanguagemodel.InICASSP2020-2020IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages7614–7618,2020.WP2:EuropeanLanguageEquality–TheFutureSituationin203064D2.14:TechnologyDeepDive–SpeechTechnologiesXiaolianZhu,YuchaoZhang,ShanYang,LiumengXue,andLeiXie.Pre-alignmentguidedattentionforimprovingtrainingefficiencyandmodelstabilityinend-to-endspeechsynthesis.IEEEAccess,7:65955–65964,2019.WP2:EuropeanLanguageEquality–TheFutureSituationin203065