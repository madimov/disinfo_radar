>FHITECHNICALREPORT<QNRsTowardLanguageforIntelligentMachinesK.EricDrexlerSeniorResearchFelloweric.drexler@oxfordmartin.ox.ac.ukTechnicalReport#2021-3Citeas:Drexler,K.E.(2021):“QNRs:TowardLanguageforIntelligentMachines”,TechnicalReport#2021-3,FutureofHumanityInstitute,UniversityofOxfordTheviewsexpressedhereinarethoseoftheauthor(s)anddonotnecessarilyreﬂecttheviewsoftheFutureofHumanityInstitute.AbstractImpoverishedsyntaxandnondiﬀerentiablevocabulariesmakenaturallanguageapoormediumforneuralrepresentationlearningandappli-cations.Learned,quasilinguisticneuralrepresentations(QNRs)canupgradewordstoembeddingsandsyntaxtographstoprovideamoreexpressiveandcomputationallytractablemedium.Graph-structured,embedding-basedquasilinguisticrepresentationscansupportformalandinformalreasoning,humanandinter-agentcommunication,andthedevelopmentofscalablequasilinguisticcorporawithcharacteristicsofbothliteraturesandassociativememory.Toachievehuman-likeintellectualcompetence,machinesmustbefullyliterate,ablenotonlytoreadandlearn,buttowritethingsworthretainingascontributionstocollectiveknowledge.Insupportofthisgoal,QNR-basedsystemscouldtranslateandprocessnaturallanguagecorporatosupporttheaggregation,reﬁnement,integration,extension,andapplicationofknowledgeatscale.IncrementaldevelopmentofQNR-basedmodelscanbuildoncurrentmethodsinneuralmachinelearning,andassystemsmature,couldpotentiallycomplementorreplacetoday’sopaque,error-prone“foundationmodels”withsystemsthataremorecapable,interpretable,andepistemicallyreliable.Potentialapplicationsandimplicationsarebroad.Tofacilitateskimming,briefsummariesofthemainsectionsofthisdocumentarecollectedinSection2.5.1Contents1Introduction51.1Abriefsummary............................51.2Some(Over)simpliﬁedDescriptions...................61.3WhatisNotProposed..........................71.4SomeConceptsandTerms........................72MotivationandOverview82.1WhyLookBeyondNaturalLanguage?..................82.2SomeMotivatingFactsandHypotheses..................92.3GeneralApproachandGoals.......................102.4FourPerspectivesThatHelpSituatetheNL+Concept...........122.5SectionOverviews............................122.6AppendixOverviews..........................143NotesonRelatedWork153.1PotentiallyUsefulModelsandTools...................163.2Symbolic,Neural,andNeurosymbolicAI.................183.3Foundationmodels...........................204Language,Cognition,andNeuralRepresentations214.1Language,Cognition,andNon-LinguisticModalities...........214.2CumulativeandStructuredKnowledge..................234.3CompositionalityinLanguageandCognition...............245ExpressiveConstructsinNLandNL+285.1VocabularyandStructureinNaturalLanguages..............285.2GrammarandSyntax..........................295.3Words,Modiﬁers,andLexical-LevelExpressions.............295.4Phrases,Sentences,Documents,andLiteratures..............345.5AttheBoundariesofLanguage:Poetry,Puns,andSong..........346DesiderataandDirectionsforNL+356.1ImproveandExtendFundamentalNLConstructs.............356.2ExploitMechanismsBeyondtheScopeofConventionalNL........366.3ExploitQNRstoSupportKnowledgeIntegration.............376.4BuildonCurrentResearch........................386.5SomeCaveats..............................3827Vector-LabeledGraphRepresentations397.1ExploitingthePowerofVectorRepresentations..............407.2ExploitingthePowerofGraph-StructuredRepresentations........437.3MappingBetweenGraphsandVectorSpaces...............478QuasilinguisticNeuralRepresentations488.1UsingGraphsasFrameworksforQuasilinguisticRepresentation......488.2UsingEmbeddingstoRepresentLexical-LevelStructure..........508.3ExpressingHigher-LevelStructureandSemantics.............518.4Regularizing,Aligning,andCombiningSemanticRepresentations.....559Scaling,Reﬁning,andExtendingQNRCorpora589.1OrganizingandExploitingContentatScale................589.2IncorporatingGeneral,Non-LinguisticContent..............619.3TranslatingandExplainingAcrossLinguisticInterfaces..........649.4IntegratingandExtendingKnowledge..................679.5Credibility,Consensus,andConsilience.................6910ArchitecturesandTraining7310.1GeneralMechanismsandApproaches..................7310.2BasicInformationFlows.........................7410.3ShapingQNRSemantics.........................7610.4AbstractingQNRRepresentationsfromNL................7710.5TrainingQNR×QNR→QNRFunctionstoRespectLatticeStructure...8010.6ProcessingandInferenceonQNRContent................8011PotentialApplicationAreas8611.1Language-CenteredTasks........................8611.2AgentCommunication,Planning,andExplanation............8911.3Science,Mathematics,andSystemDesign.................9011.4SoftwareDevelopmentandAutoML...................9412AspectsofBroaderImpact9512.1BroadKnowledgeApplications......................9512.2ProducingQNR-InformedLanguageOutputsatScale...........9712.3AgentStructure,Capabilities,andAlignment...............9913Conclusions101Acknowledgements1033Appendices104A1UniﬁcationandGeneralizationonSoftSemanticLattices104A1.1Motivation...............................105A1.2FormalDeﬁnitions...........................106A1.3LatticeStructureinNLSemantics....................108A1.4Logic,ConstraintSystems,andWeakUniﬁcation.............109A1.5ExactLatticeOperationsonRegions...................112A1.6ApproximateLatticeOperationsonRegions...............114A1.7SummaryandConclusions........................116A2Tense,Aspect,Modality,Case,andFunctionWords117TableA2.1ClassesandExamplesofFunctionwords...............117TableA2.2ExamplesofTense/AspectDistinctions...............118TableA2.3ExamplesofModalityDistinctions..................118TableA2.4ExamplesofCaseDistinctions....................119A3FromNLConstructstoNL+120A3.1UpgradingSyntacticStructure......................120A3.2UpgradingLexical-LevelExpressiveCapacity...............122A3.3SubsumingandExtendingFunction-Word/TAM-CSemantics.......125A3.4ExpressingQuantity,Frequency,Probability,andAmbiguity........126A3.5FacilitatingSemanticInterpretationandComparison...........127A4CompositionalLexicalUnits130A4.1MotivationandBasicApproach.....................130A4.2EﬃcientlyRepresentingVastVocabularies................131A4.3ParallelstoNaturalLanguageVocabularies................131A4.4ParallelstoNLPInputEncodings.....................132A4.5InductiveBiasTowardEﬃcientGeneralization..............133A4.6ANoteonDiscretizedEmbeddings....................134A5CompactQNREncodings134A5.1LevelsofRepresentationalStructure...................135A5.2ExplicitGraphObjectsvs.StringEncodings................136A5.3CompactExpressionStrings.......................136A5.4Graph-ConstructionOperators......................137A5.5VocabulariesofEmbeddings.......................138References14341IntroductionThissectionpresentsabriefsummaryandoutlineofcoreconcepts,includingboundaries(whatisnotproposed)andsometerminology.DescriptionsofthemainsectionsarecollectedinSection2.5.1.1AbriefsummaryNaturallanguage(NL)isapowerfulmediumforexpressinghumanknowl-edge,preferences,intentions,andmore,yetNLwordsandsyntaxappearimpoverishedwhencomparedtotherepresentationmechanisms(vectorem-beddings,directedgraphs)availableinmodernneuralML.TakingNLasapointofdeparture,wecanseektodeveloprepresentationsystemsthatarestrictlymoreexpressivethannaturallanguage.Theapproachproposedherecombinesgraphsandembeddingstosupportquasilinguisticneuralrepresen-tations(QNRs)shapedbyarchitecturalinductivebiasesandlearnedthroughmultitasktraining.GraphscanstronglygeneralizeNLsyntacticstructures,whilelexical-levelembeddingscanstronglygeneralizeNLvocabularies.QNRframeworkscansyntacticallyembedandwrapnon-linguisticobjects(images,datasets,etc.)andformalsymbolicrepresentations(sourcecode,mathemati-calproofs,etc.).Throughaccesstoexternalrepositories(Figure1.1),inferencesystemscandrawoncorporawithcontentthatspansscalesthatrangefromphrasesanddocumentstoscientiﬁcliteraturesandbeyond.inputsQNRencoderQNRdecoderoutputsQNRinferenceQNR repositoryFigure1.1:InformationﬂowsingenericQNRsystemssupportedbyaccesstoarepositoryofQNRcontent.Inputsandoutputsmaybemultimodal.EmbeddingscanabstractQNRcontenttoenablesemanticassociativemem-oryatscale.Neuralnetworks,potentiallyexploiting(soft)latticeoperations,canprocessretrievedQNRcontenttorecognizeanalogies,completepatterns,mergecompatibledescriptions,identifyclashes,answerquestions,andinte-grateinformationfrombothtaskinputsandrepositories.5“NL+”referstoaspirationalQNRsystemsthatoutperformnaturallan-guageasamediumforsemanticexpressionandprocessing.TheNL+visionalignswithandextendscurrentresearchdirectionsinNLP,andNL+imple-mentationscouldbuildoncurrentneuralarchitecturesandtrainingmethods.Potentialapplicationsarediverse,rangingfromfamiliarNL-to-NLfunc-tionality(interactivesearch,questionanswering,writing,translating)tonovelformsofrepresentationandreasoninginscience,engineering,softwaredevel-opment,andmathematics.Potentialadvantagesinscalability,interpretability,cost,andepistemicqualitypositionQNR-basedsystemstocomplementordisplaceopaquefoundationmodels(Bommasanietal.2021)atthefrontiersofmachinelearning.Tofacilitateskimming,briefsummariesofthemainsectionsarecollectedinSection2.5.ReaderswhoprefertostartinthemiddlemaywishtoskipaheadtoSection8:QuasilinguisticNeuralRepresentations.1.2Some(Over)simpliﬁedDescriptionsAnoversimpliﬁedproblemframing:humanintelligence:naturallanguage::machineintelligence:_______?Anoversimpliﬁedapproach:Usearchitecturalinductivebiasandrepresenta-tionlearninginneuralMLsystemstoupgradelanguagebyreplacingwordsequenceswithexplicitparsetreesandwordswithembeddingvectors.Thisisanoversimpliﬁcationbecauseit(wrongly)suggestsaclose,ﬁne-grainedcorrespondencebetweennaturallanguagesandQNRs.Alessoversimpliﬁeddescription:Usearchitecturalinductivebiasandrep-resentationlearningtodevelopmodelsthatgenerateandprocessdirectedgraphs(thatstronglygeneralizeNLsyntax)labeledwithvectorembeddings(thatstronglygeneralizebothNLwordsandphrases),therebysubsumingandextendingboththesyntacticstructuresandlexical-levelcomponentsofnaturallanguages.Theresultingrepresentationsystemscansurpassnatu-rallanguagesinexpressivecapacity,compositionality,andcomputationaltractability.Furtherobjectivesandapproaches:Learntoembedlexical-levelvectorrep-resentationsinstructuredsemanticspaces.Useinductivebiasesandmul-titasklearningtoassociatemeaningswithsemantic-spaceregions(ratherthanpoints),andexploitapproximatelatticeoperations(softuniﬁcationandanti-uniﬁcation)asmechanismsforknowledgeintegration,reﬁnement,and6generalization.Translatebroadknowledge(e.g.,fromnaturallanguagecor-pora)intolargeQNRcorporaandemployscalablealgorithmstoaccessandapplythisknowledgetoawiderangeoftasks.EnableneuralMLsystemstowriteandreadQNRcontenttoenablelearningthatisbotheﬃcientandinterpretable.1.3WhatisNotProposedSomecontrastingnegativesamplesfromthespaceofrelatedconceptscanhelpreadersreﬁnetheirinternalrepresentationsofthepresentproposal:Notaformallanguage.Formallanguagessupplementnaturallanguages,buthaveneversubsumedtheirexpressivecapacity;frameworkspro-posedherecanembedbutarenotconstrainedbyformalrepresentations.Notaconstructedlanguage.Constructedlanguages1havetypicallysoughtclarityandcomprehensibility,yetsacriﬁcedexpressivecapacity;frameworksproposedhereseektoexpandexpressivecapacity,yetasaconsequence,sacriﬁcefullhumancomprehensibility.Notasystemofhand-craftedrepresentations.Productsofneuralrepre-sentationlearningtypicallyoutperformhand-craftedrepresentations;accordingly,frameworksproposedhererely,notonhand-craftedrepre-sentations,butonrepresentationlearningshapedbyarchitecturalbiasandtrainingtasks.2NotaradicaldeparturefromcurrentneuralML.FrameworksproposedhereareinformedbyrecentdevelopmentsinneuralMLandsuggestdirectionsthatarealignedwithcurrentresearch.1.4SomeConceptsandTerms•“NL”referstonaturallanguageinagenericsense.Therepresenta-tionalcapacityofNL(inthissense)canbethoughtofasasumoftherepresentationalcapacitiesofhumanlanguages.•Representationswillbevector-labeledgraphs(VLGs);potentialarclabels(indicatingtypes,etc.)arenotexplicitlydiscussed.1.Linguageneralis,Esperanto,Loglan,etc.2.Thisdocumentoftendescribesillustrativeformsofrepresentationandfunctionality,ordescribeshowneuralcomputationcouldpotentiallyimplementthoseformsandfunctions,butalwayswiththeimplicitprovisothatlearnedneuralrepresentationsandmechanismsareapttobesurprising.7•Quasilinguisticneuralrepresentations(QNRs,implementedasVLGs)arecompositionalandlanguage-like:graphsprovideupgradedsyntacticstructure,whileembeddingsprovideupgradedlexicalcomponents.1•“NL+”referstoproposed2QNR-basedproductsofneuralrepresentationlearningthatwouldsubsumeandextendtherepresentationalcapacityofnaturallanguages.3•Theterm“lattice”andthelatticeoperationsof“meet”(here,“uniﬁca-tion”)and“join”(here,“anti-uniﬁcation”,sometimestermed“general-ization”)havetheirusualmathematicalmeanings;inthepresentcontext,however,latticesandlatticeoperationswilltypicallybeapproximate,or“soft”(AppendixA1).2MotivationandOverviewSeveralperspectivesconvergetosuggestthathigh-levelmachineintel-ligencewillrequireliteracythatisbestdevelopedinamachine-nativemediumthatismoreexpressivethannaturallanguage.Thissectionconcludeswithanoverviewofthesectionsthatfollow.Becauselanguageandmachinelearningarebroadtopicsintertwinedwitheachotherandwithahostofdisciplinesandapplicationﬁelds,itisdiﬃculttoneatlydisentanglethevarious“motivationsandperspectives”promisedbythesectiontitle.Thediscussionthatfollows(perhapsunavoidably)containssectionswithoverlappingconceptualcontent.2.1WhyLookBeyondNaturalLanguage?Whyseekalanguage-likerepresentationalmediumthatismoreexpressiveandcomputationallytractablethannaturallanguage?Thequestionalmostanswersitself.Butissuchamediumpossible,whatwoulditbelike,howmightitbedevelopedandapplied?Moregenerally,howmightwecompletetheanalogymentionedabove,1.Formallanguage-likesystems(programminglanguages,mathematicalnotations,etc.)aresometimescalled“quasilinguistic”;here,thetermisextendedtoincludelessformalsystems.2.Here,to“propose”meanstosuggestapotentialfutureobjectiveordevelopment;intheMLliterature,bycontrast,whatis“proposed”isoftenalreadydemonstrated.3.Superscripting“+”improvesestheticsinhyphenatedforms;usingtheU+207Acharactercodeimprovestypographicstability.8humanintelligence:naturallanguage::machineintelligence:__________?Itseemsunlikelythatthebestansweris“naturallanguage”(again)or“un-structuredvectorembeddings”.Humanintelligenceandhumansocietiesrelyonlanguageasaprimarymediumforcommunicatingandaccumulatingknowledge,forcoordinatingactivities,andtosomesubstantialextent,forsupportingindividualcognition.Intellectuallycompetenthumansareliterate:Theycanreadandcanwritecontentworthreading.High-levelmachineintelligencewillsurelybeabletodothesameandhaveuseforthatability.CurrentAIresearchismakingstrongprogressinreadingandwritingnaturallanguageasaninterfacetothehumanworld,yetmakeslittleuseoflanguage(-like)representationsforcommunicatingandaccumulatingknowledgewithinandbetweenmachines.Theworld’saccessibleinformationconstitutesavast,multimodalcorpusinwhichnaturallanguageservesasbothcontentandconnectivetissue.General,high-levelintelligentsystemsmustbeabletouseandextendthisinformation,anditisnaturaltoseekamediumforrepresentingknowledge,bothtranslatedandnew,thatiswell-adaptedandinsomesensenativetoneuralmachineintelligence.Whatmightamachine-adaptedlanguagebelike?ItwouldbestrangetoﬁndthatthebestlanguagesforneuralMLsystemslackbasicstructuralfeaturesofhumanlanguage—inparticular,syntaxandword-likeunits—yetperhapsequallystrangetoﬁndthatmachinesabletoshareexpressivevectorembeddingswillinsteademploysequencesoftokensthatrepresentmouthnoises.Thepresentdocumentproposesaframeworkforquasilinguisticneuralrepresentations(QNRs)that—byconstruction—couldmatchandexceedtherepresentationalcapacityofnaturallanguage.Boththepotentialvalueandgeneralrequirementsforsuchsystemsseemclearenoughtomotivateandorientfurtherinvestigation.2.2SomeMotivatingFactsandHypothesesThemotivationforpursuingQNRapproachesthatareanchoredinNLcanbegroundedbothinuncontroversialfactsandincontrastingplausibleandimplausiblehypotheses.Keymotivatingfacts1)Naturallanguageisakeyelementofhumancognitionandcommunica-tion.92)Naturallanguageprovidesexpressivecapacityofuniquebreadthandﬂexibility.3)StructuredneuralrepresentationscanbebothricherandmoreML-compatible1thansequencesofwords.Corresponding(andplausible)motivatinghypotheses+1)Quasilinguisticneuralrepresentationsofsomesortwillbekeyelementsofhuman-levelmachinecognitionandcommunication,and:+2)Theabstractfeaturesofnaturallanguage(syntacticandlexicalcon-structs)caninformthedevelopmentofQNRsthatsubsumeandextendsyntaxandwordswithgraphsandembeddings,and:+3)QNRsinformedbynaturallanguageconstructscanbemoreexpressiveandcomputationallytractablethanlanguagesthattranslateorimitateNL-likesequencesofword-liketokens.Corresponding(butimplausible)demotivatinghypothesesTheplausibil-ityoftheabovehypothesesissupportedbytheimplausibilityofcontraryhypotheses:–1)Thatlanguage-likerepresentationswillbeoflittleuseinhuman-levelmachinecognitionandcommunication,or:–2)Thatlanguage-likesyntacticandlexicalstructurescanbenobetterthanﬂatsequencesofvectorrepresentations,2or:–3)Thatembeddingsincombinationwithlanguage-likesyntacticstructurescanbenomoreexpressivethansequencesofword-liketokens.2.3GeneralApproachandGoalsInbrief,thepresentlineofinquirysuggestsaframeworkthatwould,asalreadyoutlinedinpart:1.E.g.,theycanbediﬀerentiable2.Notethatrepresentationsoftheoreticallyequivalentexpressivecapacityneednotbeequivalentin,forexample,computationaltractability,compositionality,compactness,scalabil-ity,orinductivebias.10•Replaceandgeneralizediscrete,non-diﬀerentiableNLwordsandphraseswithsemanticallyrich,diﬀerentiableembeddings.1•ReplaceandgeneralizeNLsyntaxwithgeneralgraphs(whichalsohavediﬀerentiablerepresentations).•Complementﬂatneuralrepresentationswithsyntacticstructure.•Movelinguisticcontentcloserto(quasi)cognitiverepresentations.ThisstrategystartswithNLasapointofdeparture,retaininggeneralitybysubsumingandextendingNLpiecemeal,atthelevelofunderstandableele-ments.Thealternative—toattempttocapturethewholeofNLfunctionalityinamoreformal,theory-basedframework—wouldriskthelossoffunctionalitythatwedonotfullyunderstand.Beyondthesebasicfeatures,QNRframeworkscanbeextendedto:•Exploitabstractiveembeddingsofﬁne-grainedcontent(Section8.3.4).•Exploitabstractiveembeddingsoflarge-scalecontexts(Section8.3.5).•Supportsemanticsearchatscale(Section9.1.2).•Supportsemanticnormalization,alignment,reﬁnement,andintegration(Section8.4).•Subsumeorembedformalandnon-linguisticrepresentations(Sec-tion9.2).Whatdowewantfromscalablehigh-endQNR/NL+systems?•Totranslate(andreﬁne)largeNLcorporaintomoretractableforms2•Tocombineknowledgefrommultiplesources,makinguseofrecogniz-ableconcordance,clashes,andgaps•Toprovidecomprehensive,dynamic,beyond-encyclopedicknowledgeforusebymachinesandhumans•Tosupportthegrowthofknowledgethroughmachine-aidedreasoningThelattergoalsareworthemphasizing:AkeymotivationforpursuingNL+capabilitiesistoenablesystemstolearnfrom,apply,andextendcontentthatrangesfrominformal,commonsenseknowledgetomathematicsandscientiﬁc1.Onedirectioninwhichlanguage-likerepresentationsmightdivergefromthepicturepaintedhereisinthesemanticlevelofembeddings:AsdiscussedinSection5.3,embeddingscan,throughrepresentationdiscovery,subsumethefunctionofsyntacticunitsabovethelexicallevel(e.g.,relativelycomplexphrasesandrelativelysimplesentences).Thepartialinterchange-abilityofgraphandvectorrepresentations(Section7.3)blursthepotentialsigniﬁcanceofsuchashift,however.2.WhilealsotranslatingamongalikelymultiplicityofNL+dialectsandoverlappingtask-orientedsublanguages.11literatures.WhileNL+representationshavepotentiallyimportantrolesinNL-to-NLprocessing(translation,etc.),thisisalmostincidental.Theprimaryaimistorepresent,notNL,butwhatNLitselfrepresents,andtodosobetterandwithbroaderscope.Currentlanguage-relatedmachinerepresentationsdonotprovidefullNL(muchlessNL+)functionality:Theyrangefromopaquelanguagemodelstoexplicitknowledgegraphsandformallanguages,butdespitetheirstrengths,nonecanmatch(muchlessexceed)humanlanguageinpowerandgenerality.Systemsliketheseshouldbeseenascomplements—notalternatives—toQNRframeworks.12.4FourPerspectivesThatHelpSituatetheNL+ConceptReinforcingthepointsabove,fourexternalperspectivesmayhelptosituatetheNL+conceptwithrespecttorelatedresearchtopics:•Thepowerandlimitationsofnaturallanguagesetahighbartoclearwhilesuggestingdirectionsfordevelopingmorepowerfulsystems.•Thepowerandlimitationsofsymbolicsystems2suggestaneedforcomple-mentary,lessformalrepresentationsystems.•Thepowerandlimitationsofﬂatneuralrepresentationssuggestthepoten-tialadvantagesofsystemsthatcombinetheexpressivepowerofdensevectorrepresentationswiththecompositionalstructureofNL.•ThepowerandlimitationsofcurrentNLPtools3suggestthatcurrentneuralMLtechniquescanbothsupportandbeneﬁtfromQNR-basedmechanismswithNL+applications.2.5SectionOverviewsThetopicsaddressedinthisdocumentarebroad,many-faceted,andhavealargesurfaceareaincontactwithotherdisciplines.Thetopicsarediﬃculttodisentangle,butthefollowingoverviewsprovideasketchoftheorganizationandcontentofthedocument.41.Forexample,opaqueTransformer-likemodelsmaybeusefulinQNRapplications:Ingeneral,quasicognitiveprocessingiscomplementarytoquasilinguisticrepresentation.2.Logic,mathematics,programminglanguages,knowledgerepresentationlanguages,at-temptedformalizationsofnaturallanguage,etc.3.Includingsystemsthatexploitpretrainedlanguagemodels.4.Readerswhodon’tskimwillencounterredundancyprovidedforthosewhodo.12Section1:IntroductionThissectionpresentsabriefsummaryandoutlineofcoreconcepts,includingboundaries(whatisnotproposed)andsometerminology.Section2:MotivationandOverview.Severalperspectivesconvergetosug-gestthathigh-levelmachineintelligencewillrequireliteracythatisbestdevelopedinamediummoreexpressivethannaturallanguage.Section3:NotesonRelatedWork.CurrentdevelopmentsinneuralMLprovidearchitecturesandtrainingmethodsthatcansupportQNR-orientedresearchanddevelopment.Prospectiveadvancesarelinkedtoworkinsym-bolicandneurosymboliccomputation,andtobroadtrendsindeeplearningandnaturallanguageprocessing.Section4:Language,Cognition,andNeuralRepresentations.UsingNLasamotivationandpointofdepartureforNL+motivatesareviewofitsrolesincommunication,cognition,andthegrowthofhumanknowledge.ProspectsforimprovingcompositionalitythroughQNRrepresentationsarekeyconsiderations.Section5:ExpressiveConstructsinNLandNL+.NL+mustsubsumethefunctionalityofNLconstructsidentiﬁedbylinguists,andtheshortcomingsofthoseconstructssuggestsubstantialscopeforsurpassingNL’sexpressivecapacity.Section6:DesiderataandDirectionsforNL+.Prospectsforimprovingex-pressivenessinNL+representationsincludemechanismsbothlikeandbeyondthosefoundinnaturallanguages.Researchdirectionsaimedatrealizingtheseprospectsarewell-alignedwithcurrentdirectionsinneuralML.Section7:Vector-LabeledGraphRepresentations.Inconjunctionwithto-day’sdeeplearningtoolkit,vector-labeledgraphrepresentationsprovidepowerful,diﬀerentiablemechanismsforimplementingsystemsthatrepresentandprocessstructuredsemanticinformation.Section8:QuasilinguisticNeuralRepresentations.Applicationsofvector-labeledgraphscangeneralizeNLsyntaxandupgradeNLwordstoimplementquasilinguisticneuralrepresentationsthatparallelandsurpasstheexpressivecapacityofnaturallanguageatmultiplelevelsandscales.13Section9:Scaling,Reﬁning,andExtendingQNRCorpora.ScalableQNRsystemswithNL+-levelexpressivecapacitycouldbeusedtorepresent,reﬁne,andintegratebothlinguisticandnon-linguisticcontent,enablingsystemstocompileandapplyknowledgeatinternetscale.Section10:ArchitecturesandTraining.ExtensionsofcurrentneuralMLmethodscanleveragearchitecturalinductivebiasandmultitasklearningtosupportthetrainingofquasilinguisticneuralsystemswithNL+-levelexpres-sivecapacity.Section11:PotentialApplicationAreas.PotentialapplicationsofQNR/NL+functionalityincludeandextendapplicationsofnaturallanguage.Theyin-cludehuman-orientedNLPtasks(translation,questionanswering,semanticsearch),butalsointer-agentcommunicationandtheintegrationofformalandinformalrepresentationstosupportscience,mathematics,automaticprogramming,andAutoML.Section12:AspectsofBroaderImpact.Thebreadthofpotentialapplica-tionsofQNR-basedsystemsmakesitdiﬃculttoforesee(muchlesssummarize)theirpotentialimpacts.Leadingconsiderationsincludethepotentialuseandabuseoflinguisticcapabilities,ofagentcapabilities,andofknowledgeingeneral.SystemsbasedonQNRrepresentationspromisetoberelativelytransparentandsubjecttocorrection.Section13:Conclusions.CurrentneuralMLcapabilitiescansupportthedevelopmentofsystemsbasedonquasilinguisticneuralrepresentations,alineofresearchthatpromisestoadvancearangeofresearchgoalsandapplicationsinNLPandbeyond.2.6AppendixOverviewsSeveraltopicshavebeenseparatedandplacedinappendices.Ofthese,onlytheﬁrstfocusesontopicsthatcanbeconsideredfoundational.AppendixA1:UniﬁcationandGeneralizationonSoftSemanticLattices.QNRrepresentationscansupportoperationsthatcombine,contrast,andgeneralizeinformation.Theseoperations—softapproximationsofuniﬁcationandanti-uniﬁcation—canbeusedtoimplementcontinuousrelaxationsofpowerfulmechanismsforlogicalinference.14AppendixA2:Tense,Aspect,Modality,Case,andFunctionWords.Tablesofexamplesillustrateexpressiveconstructsofnaturallanguagesthatdonotreducetonouns,verbs,andadjectives.AppendixA3:FromNLConstructstoNL+.Condensing,regularizing,andextendingthescopeofsemanticrepresentationscanimproveexpressiveca-pacityandcompositionality,andcansupporttheoreticallygroundedmethodsforcomparingandcombiningsemanticinformation.AppendixA4:CompositionalLexicalUnits.Embeddingswithexplicitcom-positionalstructuremayoﬀeradvantagesineﬃcientlearningandgeneraliza-tion.AppendixA5:CompactQNREncodings.StringrepresentationsofQNRs,inconjunctionwithdiscretizedvectorspacesandgraph-constructionopera-tors,canprovidecompactandeﬃcientQNRencodings.3NotesonRelatedWorkCurrentdevelopmentsinneuralMLprovidearchitecturesandtrainingmethodsthatcansupportQNR-orientedresearchanddevelopment.Prospectiveadvancesarelinkedtoworkinsymbolicandneurosymboliccomputation,andtobroadtrendsindeeplearningandnaturallanguageprocessing.TheprospectsexploredinthepresentdocumentincludeNLP-orientedQNRsystems,whichistosay,systemsthatread,process,andproducecontentwithinQNRdomainswhilesupportingNLinputsandoutputsatexternalinterfaces.Thediscussionfocusesonbroad,long-termgoalsandassociatedsoftwareinfrastructure.TopicsconsideredatthislevelaretooabstracttocorrespondcloselytoparticularneuralMLimplementations,precludingﬁne-grainedcomparisons.Accordingly,thissectiondiscussesconnectionstocurrentwork(usefultools,competingandcomplementaryapproaches),butonlyinoutline;moreexten-sivediscussionsofrelatedworkcanbefoundincitedpapers.153.1PotentiallyUsefulModelsandToolsPotentiallyusefulmodelsandtoolsforquasilinguisticprocessing(QLP)arecoextensivewithbroadareasofneuralML(inparticular,neuralNLP),andarangeofapplicabletools(architectures,trainingdata,trainingtasks,compu-tationalresources...)canbefoundincurrentpractice.3.1.1Vector-OrientedRepresentationsandAlgorithmsFlatneuralrepresentations—setsandsequencesofoneormoreembeddingvectors—areubiquitousinmodernneuralMLandplaykeyrolesascompo-nentsofproposedQNRarchitectures.Themostcloselyrelatedworkisinnaturallanguageprocessing.InneuralNLP,weﬁndvectorrepresentationsofwordsatinputand(of-ten)outputinterfaces;1somesystemsproduceembeddingsofhigher-levelentitiessuchassentencesanddocuments.2Semanticstructureinvectorspacesemergesspontaneouslyinwordembeddings.3End-to-endtrainingcanproducecompatiblevectorrepresentationsofimagesandtextfortasksthatincludeimagecaptioningandvisualquestionanswering.4ExtensionsofcurrentNLPrepresentations,architectures,andtrainingmethodsarenaturalcandidatesforanalogousrolesinQLP.Transformerarchitectures—successfulintasksasdiverseastranslation,questionanswer-ing,theoremproving,objectrecognition,andgraph-basedinference5—appeartohavesuﬃcientgeneralitytosupportmany(perhapsmost)aspectsofquasilinguisticprocessing.Transformerarchitectureshavebeenextendedtoreadexternalmemories,includingstoresofNLtext6andvectorembeddings.7QLPsystemscouldpo-1.Rezaeinia,Ghodsi,andRahmani(2017)andDevlinetal.(2019)2.Adietal.(2017),GangulyandPudi(2017),andConneauetal.(2018)3.Mikolov,Yih,andZweig(2013),S.Liuetal.(2018),andEthayarajh,Duvenaud,andHirst(2019).Relativetowordsinnaturallanguages,embeddingscanimprovecorrespondencebetweenrepresentationalandsemanticdistance,along-standinggoalforimprovinglinguisticsystems(Wilkins1668).4.Yuetal.(2017)andHossainetal.(2019)5.Vaswanietal.(2017),Devlinetal.(2019),Koncel-Kedziorskietal.(2019),Brownetal.(2020),PoluandSutskever(2020),andCarionetal.(2020)6.E.g.,128-tokentextpieces(Vergaetal.2020),ormoregeneralmultimodalinformation(Fanetal.2021).7.Khandelwaletal.(2020)andYogatama,Massond’Autume,andKong(2021).Modelsofthiskindfallwithinthebroadclassofmemory-augmentedneuralnetworks(Santoroetal.2016).16tentiallybebasedonbroadlysimilararchitecturesinwhichinferencesystemswriteandread,notﬂatembeddings,butQNRcontent.3.1.2GraphRepresentationsandGNNsGraphstructurescomplementvectorrepresentationsinproposedQNRs,andapplicationsofgraphrepresentationshavespurredextensiveworkinneuralML.Iterative,node-to-nodemessage-passingsystems—graphneuralnetworks1(GNNs)—aredeep,convolutionalarchitecturesthathavebeensuccessfulintasksthatrangefromsceneunderstandingtoquantumchemistryandneu-rosymboliccomputing;2theirfunctionalitymaybewell-suitedtosemanticprocessingonQNRs.Graph-orientedmodels,oftenGNNs,arewidespreadinneuralknowledge-representationsystems.3Similargraphscanbealignedforcomparisonandprocessing.4AlthoughclassicGNNsoperateonﬁxedgraphs,bothdiﬀerentiablerepresentationsandreinforcementlearninghavebeenusedtoimplementgenerativemodelsinthediscretegraph-structuredomain5(graphscan,forexample,bemappedtoandfromcontinuousem-beddings6).Withsuitablepositionalencodings,Transformerscanoperatenotonlyonsequences,butontreesorgeneralgraphs.7Therichtoolsetprovidedbycurrentgraph-orientedneuralmodelsseemssuﬃcienttosupportthedevelopmentofpowerfulQNR-basedapplications.3.1.3ComputationalInfrastructureBroadapplicationsofNL+callforscalingtolargecorpora,ﬁrsttrainingonlargeNLcorpora,thenwritingandapplyingQNRcorporathatmaybelargerstill.RoughanalogiesbetweenNLPandQLPtaskssuggestthatcomputationalcostsinbothtrainingandapplyinglarge-scalesystemscanbeinlinewiththecostsofcurrentlypracticalsystemsforlanguagemodelingandtranslation1.RecentlyreviewedinJ.Zhouetal.(2020)andWuetal.(2021).2.R.Lietal.(2017),Gilmeretal.(2017),Lambetal.(2020),andAddankietal.(2021).Labeledscenegraphs,inparticular,exemplifylearnablesemanticrelationshipsamongobjectsinwhichbothobjectsandtheirrelationshipscanbestberepresentedbyembeddings(seeFigure8.1).3.WenZhangetal.(2019)andJietal.(2021)4.Heimannetal.(2018),Caoetal.(2019),andFeyetal.(2020)5.Yunetal.(2019),J.Zhouetal.(2020),Kazietal.(2020),andWuetal.(2021)6.Cai,Zheng,andChang(2018)andPanetal.(2018)7.ShivandQuirk(2019)andChen,Barzilay,andJaakkola(2019)17(Section9.1);inparticular,algorithmsforeﬃcientembedding-basedseman-ticsearchatscale—akeyenablerforexploitinglargecorpora—havebeendemonstratedincommercialapplications.1Accordingly,currentcomputa-tionalinfrastructureseemsadequatefordevelopment,training,andpotentiallarge-scaledeploymentofNL+applications.Reductionsincomputationalcostandimprovementsinalgorithmiceﬃciencycontinue(HernandezandBrown2020).3.2Symbolic,Neural,andNeurosymbolicAIClassicsymbolicandneuralapproachestoAIprovidefurthercontextfortheproposedlineofdevelopment,whichhasconnectionstocombined,neurosym-bolicapproaches.3.2.1SymbolicAITheearlydecadesofAIcenteredonsymbolicmodelsthathavelittledirectrelevancetocurrentneuralapproaches.Symbolicsystemshadstrikingsuc-cesses,2yetproducedunimpressiveresultsinlearningandperceptualtaskslikevision.InNLP,symbolicAIfacedpersistentdiﬃcultiesstemmingfromtheinterplayofwordmeanings,syntax,andsemanticcontext,whileinakeyapplication—machinetranslation—statisticalmethodsoutperformedclassicsymbolicAI.ThequasilinguisticapproachsuggestedherediﬀersfromsymbolicAIintwoquitegeneralways:1.ProposedQNRsandQLPcomputationareintendedtosupport—notdirectlyimplement—mechanismsforinferenceandcontrol.2.ProposedQNRsandQLPcomputationaresaturatedwithneuralrepre-sentationsandlearningmechanisms.WhatsymbolicAIdoeshaveincommonwithproposedQLPistheuseofgraph-structuredrepresentations(insymbolicAI,typicallysyntaxtrees)thatareassociatedwithdistinctlexical-levelcomponents.1.J.Wangetal.(2018)andJohnson,Douze,andJégou(2019)2.Perceptionsofsuccesswere(notoriously)bluntedbyreclassiﬁcationofresearchresults(“Ifitworks,itisn’tAI”).Highlysuccessfulautomationofsymbolicmathematics,forexample,emergedfromwhathadinitiallybeenconsideredAIresearch(MartinandFateman1971;Moses2012).183.2.2NeuralMLInrecentyearsdeeplearningandneuralMLhaveadvancedrapidlyinbothscopeandperformance,successfullyaddressinganastoundingrangeofprob-lems.Becausetherangeofpotentialneuralarchitecturesandtasksisopenended,itwouldbeunwisetodrawalinearounddeeplearningandproposelimitstoitscapabilities.Thepresentproposalsarewithin,notbeyond,thescopeofmodernneuralML.Thatsaid,onecanpointtopersistentdiﬃcultieswiththemostcommonneuralMLapproaches,whichistosay,modelsthatemployﬂatneuralrep-resentations(vectors,setsofvectors,sequencesofvectors)thatoftenscalepoorly,lackclearcompositionality,andresistinterpretation.3.2.3NeurosymbolicAIDevelopmentsinneurosymbolicAIareadvancingattheintersectionbetweensymbolicandneuralML,withapproachesthatincludetheadaptationofsymbolicalgorithmstoricher,embedding-basedrepresentations.1ThisbodyofworkhasmultiplepointsofcontactwithproposedQNRapproaches,butitsdiversityresistssummarization.AppendixA1exploresconnectionswithconstraintlogicprogrammingandrelatedreasoningmechanismsbasedonneurosymbolicrepresentations.Itisimportanttodistinguishamongapproachesthatcanbecalled“neuro-symbolic”,yetdiﬀerfundamentally.GeoﬀreyHintonhasremarkedthat:Somecriticsofdeeplearningarguethatneuralnetscannotdealwithcompositionalhierarchiesandthatthereneedstobea“neurosymbolic”interfacewhichallowsneuralnetworkfront-andback-endstohandoverthehigher-levelreasoningtoamoresymbolicsystem.Ibelievethatourprimarymodeofreasoningisbyusinganalogieswhicharemadepossiblebythesimilaritiesbetweenlearnedhigh-dimensionalvectors...(Hinton2021)ThepresentproposaldiﬀersfromthosethatHintoncriticizes:WhilebothQNRsandconventionalsymbolicsystemsemployexplicitsyntacticstruc-tures,compositionalhierarchies,andword-likeunits,QNRsemployhigh-dimensionalvectors,notconventionalsymbol-tokens,inpartforthereason1.E.g.,seeRocktäschelandRiedel(2017),Minervinietal.(2020),GarcezandLamb(2020),andArabshahietal.(2021).19Hintoncites.Althoughhigher-levelreasoningseemslikelytohaveanalgo-rithmiccharacter,employingconditionalbranchesanddispatchofvaluestofunctions,1thereisgoodreasontoexpectthatthoseconditionalsandfunc-tionswilloperateonneuralrepresentationsthroughneuralmechanisms.Tostructuretheobjectsandoperationsofreasoningneednotimpoverishtheircontent.3.3FoundationmodelsTheterm“foundationmodel”hasbeenhasbeenintroduced(Bommasanietal.2021)todescribesystemsthatare“trainedonbroaddataatscaleandcanbeadapted(e.g.,ﬁne-tuned)toawiderangeofdownstreamtasks”.To-day’sleadingfoundationmodels(e.g.,BERTandGPT-32)arepretrainedonextensivecorporaofNLnext,whileothers(e.g.,CLIP3)aremultimodal;allarebasedonTransformers.Despitetheirextraordinaryrangeofapplications,currentfoundationmod-elshavesuﬀeredfromopaquerepresentations,opaqueinferencemechanisms,costlyscaling,4poorinterpretability,andlowepistemicquality,withconse-quencesreviewedandexploredindepthbyBommasanietal.(2021).QNR-orientedarchitecturescouldpotentiallyalleviateeachofthesediﬃ-cultiesbycomplementingordisplacingmodelsbasedonstand-aloneTrans-formers.Ratherthanrepresentingknowledgeinunstructured,multi-billion-parametermodels,5architecturesthatrepresentknowledgeintheformofscalableQNRcorpora(Section9)couldprovidefoundationmodelsinwhichinformationcontentiscompositional(Section4.3)andsubstantiallyinter-pretable(Section9.3.3).QuestionsofepistemicqualitycouldbeaddressedbyQNR-domainreasoningaboutexternalinformationsources(Section9.5).1.Forarecentexample,see(Fedus,Zoph,andShazeer2021).2.Devlinetal.(2019)andBrownetal.(2020)3.Radfordetal.(2021)4.EvenrelativelyscalableTransformerarchitectures(Beltagy,Peters,andCohan2020;Katharopoulosetal.2020;Zaheeretal.2020)attendonlytosectionsoftext,notliteratures.5.Inwhichknowledgerepresentationandinferencemechanismsentangleerror-pronearithmeticandinformationretrievalwithﬂuentmultilingualtranslation.204Language,Cognition,andNeuralRepresentationsUsingNLasamotivationandpointofdepartureforNL+motivatesareviewofitsrolesincommunication,cognition,andthegrowthofhumanknowledge.ProspectsforimprovingcompositionalitythroughQNRrepresentationsarekeyconsiderations.Humansaccumulateandshareinformationthroughnaturallanguage,andlan-guageiswovenintothefabricofhumancognition.TheexpressivescopeofNL,thoughlimited,isuniqueandvast.Ifweseektobuildartiﬁcialsystemsthatmatchorexceedhumancognitivecapacity,thenpursuingmachine-orientedNL-likefunctionalityseemsnecessary.Thepresentsectionreviewsthepowerandshortcomingsofnaturalandformallanguages,andfromthisperspective,considersprospectsforquasilinguisticconstructsmorepowerfulandclosertocognitiverepresentationsthanNLitself.TheexpressivecapacityofNLhasresistedformalization,anddespiteitsfamiliarity,remainspoorlyunderstood.Accordingly,inseekingmorepower-fulrepresentations,theproposedstrategywillbetoupgradetheexpressivecapacityoftherelativelywellunderstoodlinguisticcomponentsoflanguage—lexicalunitsandmeansforcomposingthem—andtotherebyupgradethelesswellunderstoodwhole.4.1Language,Cognition,andNon-LinguisticModalitiesNaturallanguage,humancognition,andthesocialaccumulationofknowledgearedeeplyintertwined.ProspectsforNL+systemsparalleltheroleofNLinsupportingbothreasoningandthegrowthofknowledge.4.1.1TheRolesandGeneralityofLanguageThroughbiologyandculture,naturallanguageevolvedtoexploitanimalvocalizations,anacousticchannelthattransmitsinformationbetweenneuralsystems,constrainedbylimitations(workingmemory,processingspeed)ofthecognitivemechanismsthatencodeanddecodemeaning.Atasocietallevel,sequencesofsymbols—writtenlanguage—encodeandextendspeech,whileataneurologicallevel,thesemanticstructuresoflanguagemeshwithcognition.Naturallanguagehasevolvedunderpressurestowardcomprehensiveex-pressivecapacity,yetitsshortcomingsarerealandpervasive.Wecanregard21NLasbothabenchmarktosurpassandasatemplateforrepresentationalarchitectures.4.1.2Complementary,Non-LinguisticModalitiesNotwords,really,betterthanwords.Thoughtsymbolsinhisbrain,communicatedthoughtsymbolsthathadshadesofmeaningwordscouldneverhave.—CliﬀordSimakCity,11952Thehumanuseofcomplementary,non-linguisticmodalitieshighlightslimitationsofNL.Itissaidthatapictureisworthathousandwords,butitwouldbemoretruetosaythatimagesandlanguageeachcanexpresssemanticcontentthattheothercannot.Anothermodality,demonstrationofskills(nowaidedbyvideo),isoftencomplementedbybothspeechandimages.Today,artifactssuchasinteractivecomputationalmodelsprovidefurthermodalities.Likelanguage,othermodalitiesmeshwithhumancognitiondowntoun-consciousdepths.Humanthoughtreliesnotonlyonlanguage,butonmentalimages,imaginedphysicalactions,andwordlesscausalmodels.NLservesasakindofgluebetweennon-linguisticmodalities—naming,linking,andexplainingthings;NL+frameworksmustandcandolikewise.NeuralMLshowsusthatvectorembeddingscandescribemuchthatwordscannot,imagesandmore.Expressiveembeddingsbeyondthescopeofhumanlanguagecanserveasintegral,insomesense“word-like”partsofquasilinguisticsemanticstructures,stretchingtheconceptofNL+.Theabilitytodirectlyreferenceandwrapafullrangeofcomputationalobjectsstretchestheconceptfurther.4.1.3HowCloselyLinkedareLanguageandCognition?Embeddingsresemblebiologicalneuralrepresentationsmorecloselythanwordsdo:Embeddingsandneuralstatevectorscontainfarmoreinformationthanmeretokenidentities,andbotharedirectlycompatiblewithneural(-like)processing.Thus,embedding-basedQNRsareclosertocognitiverepresen-tationsthanarenaturallanguages,andpresumablymorecompatiblewith(quasi)cognitiveprocessing.21.Reprinted,Simak(2016).2.Thisdoesnotarguethatinternalcognitiverepresentationsthemselveshaveaclean,graph-structuredsyntax,northatsparse,syntax-likegraphsareoptimalrepresentationsforexternalizedQNRs.TheargumentaddressesonlypropertiesofQNRsrelativetowordstrings.22Howdeeparetheconnectionsbetweenlanguageandhumancognition?Withoutplacinggreatweightonintrospectiveaccesstocognitiveprocesses,andwithoutattemptingtoresolvetheoreticalcontroversies,someaspectsoftherelationshipbetweenlanguageandcognitionseemclear:•Languageandcognitionhaveco-evolvedandhavehadampleopportu-nitytoshapeandexploitoneanother.•Languageiscompositional,andthesuccessofneuralmodelsthatparsescenesintodistinctobjectsandrelationships(Figure8.1)suggeststhatcompositionalmodelsoftheworldaremorefundamentalthanlanguageitself.1•Theexperienceoftryingto“putthoughtsintowords”(andsometimesfailing)isgoodevidencethattherearethoughtsthatarecloseto—yetnotidenticalwith—language;conversely,ﬂuentconversationshowsthatsubstantialcognitivecontentreadilytranslatestoandfromlanguage.•Externalizationofthoughtsinlanguagecanhelptostructurepersonalknowledge,whilewritingandreadingcanexpandourmentalcapacitiesbeyondthelimitsofmemory.Theseobservationssuggestthattheuseoflinguisticallystructuredyetqua-sicognitiverepresentationscouldaidthedevelopmentofmachineintelligencebyprovidingamechanismthatisknowntobeimportanttohumanintel-ligence.Conversely,prospectsforhigh-levelmachineintelligencewithoutsomethinglikelanguagearespeculativeatbest.4.2CumulativeandStructuredKnowledgeWithsomelossofnuance,speechcanbetranscribedastext,andwithsomelossofhigh-levelstructure,2textcanbemappedbacktospeech.Acentralconcernofthepresentdocument,however,isthegrowth(andreﬁnement)ofaccessibleknowledge;today,thisprocessreliesonthedevelopmentoftextcorporathatexpress(forexample)science,engineering,law,andphilosophy,togetherwithliteraturesandhistoriesthatdescribethehumancondition.Indeed,inthisdocument,“naturallanguage”tacitlyreferstolanguagecapturedinwriting.1.Applicationsofcompositionalneuralmodelsincludenotonlylanguage-linkedexpla-nationandquestionanswering(Shi,Zhang,andLi2019)butnon-linguistictaskssuchaspredictivemodelingofphysicalsystems(Wattersetal.2017).However,totheextentthatneuralMLsystemscandisplaycompetenceincompositionaltaskswithoutlanguage-likeinternalrepresentations,thiscountsagainstthenecessityoflanguage-likerepresentationsincognition.2.E.g.,duetoworking-memoryconstraints(CaplanandWaters1999).23Incommunication,NL+aimstobemoreexpressivethanNL;inconnectionwithcognition,NL+aimstobemoredirectlycompatiblewith(quasi)cognitiveprocessing;onaglobalscale,NL+aimstobemoreeﬀectiveinaccumulat-ingandapplyinggeneralknowledge.ThegrowthofNL+corporacanbecumulativeandcontentcanbestructuredforuse.Returningtoacognitiveperspective,humansnotonlyreadandwritelan-guage,butalso“readandwrite”long-termmemories.NL+contentsharescharacteristicsofbothlanguageandmemory:Liketext,NL+contentcon-stitutesexplicit,shareableinformation;likelong-termmemory,NL+-basedsystemscanstore(quasi)cognitiverepresentationsthatareaccessedthroughassociativemechanisms.14.3CompositionalityinLanguageandCognitionWhydoesthestructureoflanguagesuitittosomanytasks?Linksbetweenlanguageandcognition—theirco-evolution,theirclosecouplinginuse—arepartofthestory,butcorrespondencebetweencompositionalstructuresinlanguage,cognition,andtheworldisanother,perhapsmorefundamentalconsideration.ThecaseforthebroadutilityofNL+frameworksinAIisbasedinpartonthiscorrespondence:2Compositionalityintheworldspeaksinfavorofpursuingcompositionalrepresentationsofknowledge,situations,andactions.Likewise,compositionalityincognition(andinparticular,deliberatereasoning)speaksinfavorofstrongrolesforcompositionalrepresentationsinsupportingquasicognitiveprocessing.4.3.1DegreesofCompositionalityHere,asystem—linguistic,cognitive,oractual—willbetermed“compo-sitional”ifitcanbeusefully(thoughperhapsimperfectly)understoodormodeledasconsistingofsetsofpartsandtheirrelationships.3Usefulcompo-sitionalityrequiresthatthisunderstandingbeinsomesenselocal,emergingfrompartsthatarenottoonumerousortooremote4fromthepartsatthe1.Section9.1.2considersembedding-indexedQNRstoresasassociativememoriesaccessedthroughnear-neighborlookupinsemanticspaces.2.GoyalandBengioproposelanguage-inspiredcompositionalityasakeytodevelopingsystemsthatmorecloselymodelhuman-likeintelligence(GoyalandBengio2021);seealso(Y.Jiangetal.2019),whichmakesstrongclaimsalongsimilarlines.3.Thisisasoftercriterionthanthatofformalcompositionality.4.Inasensethatmaybeneitherspatialnortemporal.24focusofattentionoranalysis.Hard,localcompositionalityinsymbolicsys-temsrequiresthatthemeaningofexpressionsbestrictlydeterminedbytheircomponentsandsyntacticstructures;1innaturallanguage,bycontrast,com-positionalityistypicallysoft,andlocalityisamatterofdegree.Strengtheningthelocalityofcompositionalitycanmakelinguisticrepresentationsmoretractable,andisapotentialdirectionforupgradingfromNL.4.3.2CompositionalityinLanguageandtheWorldCompositionalityinlanguagemirrorscompositionalityintheworld—thoughthecompositionalityoftheworldasweseeitmaybeconditionedbylanguageandthestructureoffeasiblecognitiveprocesses.Language(andquasilinguis-ticsystemssuchasmathematicalnotationandcomputercode2)canrepresentcompositionalitybeyondnarrownotionsofdiscrete“things”and“events”.Phenomenathataredistributedinspaceandtime(e.g.,electromagneticwaves,atmosphericcirculation,thecoupledevolutionofspeciesandecosystems)canbedecomposedanddescribedintermsofdistributedentities(ﬁelds,ﬂuids,andpopulations)andrelationshipsamongtheircomponents.Entitiesthem-selvescommonlyhaveattributessuchasmass,color,velocity,energydensity,thatarecompositionalinotherways.Neuralrepresentationlearningconﬁrmsthatcompositionalityismorethanahumanmentalconstruct.ArangeofsuccessfulMLmodelsincorporatecompositionalpriorsorlearnemergentcompositionalrepresentations.3Thesesuccessesshowthatcompositionalapproachestounderstandingtheworldareusefulinnon-human,quasicognitivesystems.Representationstypicallyincludepartswithmeaningsthatareconditionedoncontext.4Inlanguage,themeaningofwordsmaydependnotonlyonsyntacticallylocalcontext,butongeneralconsiderationssuchastheleveloftechnicalityofdiscourse,theepistemicconﬁdenceofawriter,ortheﬁeld1.Scopedbindingofsymbolsstretchesbutdoesnotbreaknotionsofsyntacticlocality—iflocalityisconstruedintermsofarcsinidentiﬁablegraphs,itisnotconstrainedbysyntacticdistancesoversequencesortrees.2.Interestingly,althoughcomputerlanguagesaremodelsofcompositionality,fMRIstudiesshowthatreasoningaboutcodeisonlyweaklyfocusedonbrainregionsspecializedfornaturallanguage(Ivanovaetal.2020).Thisdistributionofneuralfunctionsupportsabroadroleforcompositionalrepresentationsinnon-linguisticcognition.3.Raposoetal.(2017),Wattersetal.(2017),Battagliaetal.(2018),Eslamietal.(2018),G.R.Yangetal.(2019),andBearetal.(2020)4.Informalsystems,deﬁnitionsandbindingswithinascopeareexamplesofapreciseformofcontextuallyconditionedcompositionality.25underdiscussion(“glass”meansonethinginakitchen,anotherinoptics,andsomethingmoregeneralinmaterialsscience).Inimages,theappearanceofanobjectmaydependonlighting,style,resolution,andcolorrendering,andscenesgeneratedfromtextualdescriptionscandiﬀergreatlybasedoncontextualattributeslike“day”or“city”.Contextsthemselves(asshownbytheseverydescriptions)cantosomeextentberepresentedbyNL,yetimagesgeneratedbyneuralvisionsystemscanbeconditionedoncontextualfeaturesthataresubstantiallycompositional(asshownbystructureinlatentspaces),andevenrecognizablebyhumans,yetnotreadilydescribedbylanguage.Alloftheseconsiderationsspeaktothevalueofcompositionalrepresenta-tionsinlanguageandbeyond.Takenasawhole,theseconsiderationssuggestthatquasilinguisticrepresentationscandescribefeaturesoftheworldthateludelanguagebasedonstringsofwords.4.3.3CompositionalityinNonlinguisticCognitionCompositionalityincognitionparallelscompositionalityinlanguageandintheworld.Whenweperceiveobjectswithpropertieslike“relativepo-sition”,orconsideractionslike“droppingabrick”,theseperceptionsandcognitivemodelsarecompositionalinthepresentsense;theyarealsofunda-mentallynonlinguisticyetoftenexpressibleinlanguage.Thus,visualthinking(Giaquinto2015)canbebothnon-linguisticandcompositional;whenthisthinkingisabstractedandexpressedindiagrammaticform,theresultingrepresentationstypicallyshowdistinctpartsandrelationships.4.3.4CompositionalityinNaturalLanguageTheconceptoflanguageasacompositionalsystemiswoventhroughtheprecedingsections,butthesehavespokenoflanguageasifcompositionalityinlanguagewereaclearanduncontroversialconcept.Itisnot.Formalsymbolicsystemsprovideabenchmarkforfullcompositionality,andbythisstandard,naturallanguagesfallfarshort.1Formalconceptsofcompositionalityhavediﬃcultyincludingcontextualfeatureslike“topic”or“historicalera”or“in1.Thelackoffullcompositionallyinlanguagehasbeenabitterpillforformalists,andnotallhaveswallowedit.ThePrincipleofCompositionality,thatthemeaningofacomplexexpressionisdeterminedbyitsstructureandthemeaningsofitsconstituents,hasbeentakentoapplytolanguage;althoughothersrecognizeapervasiveroleforcontext(enrichingwordembeddingswithcontextualinformationhasbeenasuccessfulstrategyinneuralNLP;seeLiu,Kusner,andBlunsom(2020)),someseektoapplycontexttodetermine(fullycompositional)lexical-levelmeanings,whichseemsanarbitraryandperhapsunworkablechoice.SeeSzabó(2017).26Oxford”,andstrugglewithcontextualmodulationofmeaningatthelevelof(forexample)sentencesratherthanwords.1NeuralNLPcan(andtobeeﬀective,must)incorporateinformationthatisbeyondthescopeoflocallycompositionalrepresentationsofwordstrings.Transformer-basedlanguagemodelsshowsomethinglikeunderstandingoftextinabroadworldcontext,yettheembodimentofthatknowledgeintheirweightsisnotobviouslycompositionalinitsabstractstructure,andobviouslynotcompositionalinitsconcreterepresentation.Tosaythatthemeaningoftextemergesfromacompositionofitscomponents—withparticularTrans-formercomputationalstatesasoneofthosecomponents—wouldstretchthemeaningofcompositionalitytothebreakingpoint.2Tobemeaningful,com-positionalitymustbeinsomesenselocal.Compositionalityinlanguagecanbestrengthenedbystrengtheningthelocalizationofcontextualinformation.Quasilinguisticneuralrepresentationscancontributetothisintwoways:First,bysubstitutingdescriptivevectorembeddingsforambiguouswordsandphrases,3andsecond,byincorporatingembeddingsthatlocallysummarizethemeaningofasyntacticallybroadcon-text(forexample,acontextonthescaleofabook),togetherwithembeddingsthatlocallysummarizeremotecontext,forexample,thekindreferredtoinexpressionslike“whenreadinthecontextof...”4Thesecondrolecallsforembeddingsthatarenotconventionally“lexical”.5Inbrief,theworld,cognition,andlanguagearesubstantially“composi-tional”,andrelativetonaturallanguage,quasilinguisticneuralrepresenta-tionscanimprovelocalcompositionality.Aswewillsee,improvinglocalcompositionalitycanhavearangeofadvantages.1.“CastleMoundgivesagoodviewofitssurroundings.”Isthecontextofthissentencetourisminpresent-dayOxford,ormilitaryintelligenceduringtheNormanconquest?Thenatureoftheviewanditssigniﬁcancemaybeclearinthecontextofapamphletorbook,butcannotbefoundinthequotedstringofwords.2.Thesamecanbesaidofnaturallanguageexpressionsinthecontextofhumanmemoryandneuralstates.3.Notethattheproblemisnotambiguityperse,butambiguitythatisunintentionalorcostlytoavoid.Intentionalambiguityisexpressive,and(tomeetbenchmarkcriteria)mustthereforebeexpressibleinNL+(seeSection8.3.1andSectionA3.4).4.Becauseexpressionsmayappearinmultiplecontexts,thisshouldbeseenasinformationaboutthecontextofaparticularcitationoruseofanexpression.5.AsdiscussedinSection8.3.5.275ExpressiveConstructsinNLandNL+NL+mustsubsumethefunctionalityofNLconstructsidentiﬁedbylinguists,andtheshortcomingsofthoseconstructssuggestsubstantialscopeforsurpassingNL’sexpressivecapacity.Therichexpressiveconstructsfoundinnaturallanguagesprovideabench-markandpointofdepartureforthepursuitofmorepowerfulcapabilities.ConsideringlinguisticsinthecontextofneuralMLsuggestsbothchallengesthatmustbeaddressedandchallengesthatcanbesetasideinpursuingthepromiseofNL+.Figure6.1illustratesrelationshipsamongsomeoverlappingclassesofrepresentationsystems,someofwhichstretchthedeﬁnitionof“linguistic”.AppendixA3exploresfurtheraspectsoftherelationshipbetweennaturallanguageandpotentialQNR/NL+representations,includingprospectsforcondensing,regularizing,andextendingthescopeofquasilinguisticrepresen-tations.5.1VocabularyandStructureinNaturalLanguagesWhatlinguisticconstructsenableNLtoservehumanpurposes?1Thosepur-posesarebroad:Naturallanguagesarericherthan“knowledgerepresentationlanguages”2orotherformalsystemstodate;languagecandescribecom-plexthings,relationships,andsituations,alongwithgoals,actions,abstractargumentation,epistemicuncertainty,moralconsiderations,andmore.Theproposedstrategyfordevelopingframeworksthatsubsumeandex-tendNListoupgraderepresentationalfunctionalitybyupgradingbothNLcomponentsandcompositionalmechanisms.Crucially,thisstrategyrequiresnostrong,formaltheoryofsemantics,grammar,syntax,orpragmatics,andhencenocodingofformalrules.Anapproachthat(merely)upgradescomponentsandstructuresidestepsquestionsthathavegenerateddecadesofacademiccontroversyandaddressesinsteadafarmoretractablesetofproblems.1.Notethisquestiondoesnotaskhowthoseconstructsactuallyoperate,whichisasubjectofongoingcontroversyamonglinguists.2.SeeBobrowandWinograd(1977)andMcShaneandNirenburg(2012).Knowledgerepresentationlanguagestypicallyattempttobuildonunambiguousontologies(Guarino2009),yettheabilitytoexpressambiguityisanimportantfeatureofnaturallanguages.285.2GrammarandSyntaxItiswidelyagreedthatsentencescanusefullybeparsedintotrees1deﬁnedbygrammars,yetthereareseveralcompetingapproaches.2InanNL+framework,explicitgraphscanaccommodateandgeneralizeanychoice,hencenochoiceneedbemadeattheoutset;further,becauseneuralmodelscanintegrateinfor-mationacrossextendedsyntacticregions,grammar-likechoicesoflocalgraphstructureneednotstronglyconstrainsemanticprocessing.ArchitectureswithaQNR-orientedinductivebiastogetherwithneuralrepresentationlearningonappropriatetasksshouldyieldeﬀectivesystemswithNL+functionality(Section10).Section7explorespotentialvector/graphrepresentationsingreaterdepth,whileSection8considersapplicationsofvectorembeddingsthatsubsumeelementsofNLsyntacticstructure—again,nottoproposeorpredict,butinsteadtoexplorethepotentialoflearnedNL+representations.5.3Words,Modiﬁers,andLexical-LevelExpressionsTheroleoflexical-levelunitsinNLissubsumedbyembeddingsinNL+frameworks,andprospectsforimprovingNL+expressivenessdependinpartonthepotentialadvantagesofrepresentationsincontinuous,structuredspaces.Itiseasytoarguethatembeddingscanbeasexpressiveaswords(e.g.,embeddingscansimplydesignatewords),butadeeperunderstandingoftheirpotentialcallsforconsideringwordsandword-likeentitiesinthecontextofcontinuoussemanticspaces.5.3.1Wordsandword-levelunitsWhenconsideringNL+frameworksfromanNLperspective,akeyquestionwillbetheextenttowhichmulti-wordexpressionscanbefoldedintocom-pact,tractable,single-vectorrepresentationswhilegainingratherthanlosingexpressivepower.Twoheuristicsseemreliable:1.Meaningsthatsomelanguagesexpressinasinglewordcanberepre-sentedbyasinglevector.31.OrDAGs,whencoreferenceisrepresentedexplicitly.2.SeeBorsleyandBörjars(2011).3.Meanings,notwords:Vectorrepresentationsofwordsperformpoorlywhenthosewordshavemultiplemeanings,butrepresentingmeaningsratherthanwordssidestepsthisproblem.292.Setsofword-levelunitswithrelatedmeaningscorrespondtosetsofpointsclusteredinasemanticspace.Inthepresentcontext(andadoptinganNLperspective),“lexical”(or“word-level”)unitswillberestrictedtoasinglenounorverbtogetherwithzeroormoremodiﬁers(e.g.,adjectivesoradverbs),1anda“simple”noun(orverb)phrasewillbeonethatcannotbedecomposedintomultiplenoun(orverb)phrases.2Thephrase“alarge,gray,sleepycat”isasimplenounphraseinthissense;“acatandadog”isnotsimple,butconjunctive.Aswithmanyfeaturesoflanguage,thesingle-vs.-conjunctivedistinctionisbothmeaningfulandsometimesunclear:Is“spaghettiandmeatballs”asingledish,orapairofingredients?Is“baitandswitch”adeceptivestrategyorapairofactions?Isthesemanticcontentof“ran,thenhalted”necessarilycompound,ormightalanguagehaveaverbwithaninﬂectedformdenotingapast-tenserun-then-haltaction?Notethatnothinginthepresentdiscussionhingesonthesharpnessofsuchdistinctions.Indeed,thetypicalﬂexibilityandsoftnessofmappingsbetweenmeaningsandrepresentationsspeaksagainstdiscretetokensandformalrepresentationsandinfavorofQNR/NL+systemsthatcanrepresentasofter,lessformalsemantics.3Innaturallanguage,themeaningof“word”isitselfblurry,denotingaconceptthatresistssharpdeﬁnition.Inlinguistics,“morphemes”arethesmallestmeaningfullinguisticunits,andincludenotonly(some)words,butpreﬁxes,suﬃxes,stems,andthecomponentsofcompoundwords.Inmorphologicallyrichlanguages,wordsmaycontainmorphemesthatdenotecaseortensedistinctionsthatinotherlanguageswouldbedenotedbywordsorphrases.Blurrinessagainspeaksinfavorofsoftrepresentationsandagainstlinguisticmodelsthattreat“word”asifitdenotedanaturalkind.5.3.2ContentwordrolesandrepresentationsLinguistsdistinguish“contentwords”(alsotermed“openclass”words)from“function”(or“closedclass”)words.Thesetofcontentwordsinavocabulary1.Thisuseof“lexical”diﬀersfromastandardusageinlinguistics,wheretobe“lexical”,aphrasemusthaveameaningotherthanwhatitscomponentsmightindicate,makingthephraseitselfadistinctelementofavocabulary.Thephrases“ontheotherhand”and“cat-and-mousegame”arelexicalinthissense.NLPresearchrecognizesasimilarconcept,“multi-wordexpressions”(Constantetal.2017).2.Linguistsdeﬁne“simplephrases”diﬀerently.3.Notethatpointsinavectorspaceareinherentlysharp,yetmaybetakentorepresent(potentiallysoft)regionsinalowerdimensionalspace(seeSection7.1.5).30islargeandreadilyextended;1thesetoffunctionwords(discussedbelow)issmallandslowtochange.Contentwordstypicallyrefertoobjects,properties,actions,andrelation-ships.Theyincludenouns,verbs,adjectives,andmostadverbs,andtheytypicallyhavemore-or-lessregularmarkedorinﬂectedforms.Thegrowthofhumanknowledgehasbeenaccompaniedbythegrowthofcontent-wordvocabularies.FromtheperspectiveofNLsyntaxandsemantics,adjectivesandadverbscanbeviewedasmodifyingassociatednounsandverbs;thisrelationshipmotivatesthedescriptionoftheresultingphrasesasword-level(orlexical)inthesensediscussedabove.InexploringthepotentialexpressivecapacityofNL+frameworks,willbenaturaltoconsidersemanticembeddingspacesthataccommodate(meaningslikethoseof)nounsandverbstogetherwiththelexical-levelreﬁnementsprovidedbyadjectives,adverbs,markers,andinﬂections.2Onecanthinkofcontentwordsasrepresentingbothdistinctionsofkindanddiﬀerencesinproperties.3Numbers,animals,planets,andmoleculesareofdistinctkinds,whilemagnitude,color,accessibility,andmeltingpointcorrespondtodiﬀerencesinproperties,potentiallymodeledascontinuousvariablesassociatedwiththingsofrelevantkinds.Inembeddingspaces,onecanthinkofdistinctionsofkindasrepresentedchieﬂybydistancesandclusteringamongvectors,anddiﬀerencesinpropertiesasrepresentedchieﬂybydisplacementsalongdirectionsthatcorrespondtothoseproperties.4Notethatthisperspective(kinds→clusters;properties→displacements)isprimarilyconceptual,andneednot(andlikelyshouldnot)correspondtodistinctarchitecturalfeaturesofQNR/NL+systems.1.ThevocabularyofanEnglishspeakermayinclude10,000to100,000ormorecontentwords;diﬀerentspeakersmayemploydiﬀerentblocksofspecialized(e.g.,professional)vocab-ulary.2.Notealsothepotentialvalueofexplicitlycompositionalrepresentationsofembeddings,e.g.,embeddingsbuiltbyconcatenation(AppendixA4).3.Distinctionsofkindanddiﬀerencesinpropertiesdiﬀer,yetarenotentirelydistinct.4.Thesomewhatcounter-intuitivegeometricpropertiesofhighdimensionalspacesarerelevanthere(seeSection7.1.4).Notealsothatdisplacementsinaparticulardirectionneednothavethesamemeaningindiﬀerentregionsofsemanticspace:Mostpropertiesrelevanttoplanetsdiﬀerfromthoserelevanttomusicorsoftware.315.3.3FunctionwordrolesandrepresentationsFunction(closed-class)wordsarediverse.Theyincludecoordinatingcon-junctions(and,or,but...)conjunctiveadverbs(then,therefore,however...),prepositions(in,of,without...)modalverbs(can,should,might...),determin-ers(this,that,my...),connectives(and,or,because,despite...),andmore(seeTableA2.1).Whilethesetofopen-classwordsishuge,thesetofclosed-classwordsissmall—inEnglish,only200orso.Thevocabularyofopen-classwordscanreadilybeextendedtoincludenewmeaningsbyexampleanddeﬁnition.Closed-classwords,bycontrast,typicallyplaygeneralorabstractroles,andlinguistsﬁndthatthissmallsetofwordsisnearlyﬁxed(hence“closed”).1Thestill-awkwardrepurposingof“they”asagender-neutralthird-personsingularpronounillustratesthediﬃcultyofexpandingtheclosed-classvocabulary,eventoﬁllaproblematicgap—alternativessuchas“ze”havefailedtotakehold(C.Lee2019).Functionwordsthatinthemselveshaveminimalsemanticcontentcanshapethesemanticsofcomplex,content-wordconstructs(suchasclauses,sentences,andparagraphs),eitherasmodiﬁersorbyestablishingframeworksofgrammaticalorexplanatoryrelationships.RepresentationsthatsubsumeNLmustspansemanticspacesthatincludefunctionwords.Theclosed-classnatureofNLfunctionwordssuggestsopportunitiesforenrichingthecorrespondingsemanticdomainsinNL+.InEnglish,forexam-ple,theambiguitybetweentheinclusiveandexclusivemeaningsof“or”inEnglishsuggeststhateventhemostobvious,fundamental—andinhumanaﬀairs,literallycostly—gapsinfunction-wordvocabulariescangounﬁlledforcenturies.21.Thepovertyofclosed-classvocabularyismitigatedbytheavailabilityofcompoundfunctionwords(atleast,becauseof...)thatcanbetreatedaslexicalentities.SeeKato,Shindo,andMatsumoto(2016).2.Theconstructs“Xand/orY”and“eitherXorY”canexpresstheinclusive/exclusivedistinction,yettrade-oﬀsbetweenprecisionandwordeconomy(andthecognitiveoverheadofinsteadrelyingoncontextfordisambiguation)ensurefrequentambiguityandconfusioninpractice.Asalesstrivialexample,theabilitytocompactlyexpress“possibly-inclusive-but-probably-exclusive-or”wouldbeuseful,andinacontinuousvectorspaceoffunctionwords,wouldalsobenatural.325.3.4TAM-CmodiﬁersNaturallanguagesemployarangeoftense,aspect,modality,andcase(TAM-C)modiﬁers;1somearewidelysharedacrosslanguages,othersarerare.Insomelanguages,particularTAM-Cmodiﬁersmayberepresentedbygram-maticalmarkers(aclassoffunctionwords);inothers,byinﬂections(aclassofmorphologicalfeatures).MeaningsthatinEnglishareconveyedbyfunctionwordsmaybeconveyedbyinﬂectionsinotherlanguages.2SetsofTAM-Coverlapwithclosed-classadjectivesandadverbs,andaresimilarlyresistanttochange.SomeTAM-Cmodiﬁersalterthemeaningoflexical-levelelements(bothwordsandphrases);othersoperateathighersemanticlevels.Theycanconveydistinctionsinvolvingtime,space,causality,purpose,evidence,grammaticalroles,andmore:•Tenseandaspectdistinctionstypicallyindicatetimesandtime-spansrel-ativetothepresent(ran,run,running,hadrun,willhavebeenrunning...);seeTableA2.2.•Modalitycanindicatedistinctionsbetweenquestions,commands,conﬁ-dentstatements,andpossibilities,amongmanyothers;seeTableA2.3.•Casecanindicatedistinctionsbetween(forexample)grammaticalroles(subject,object...),functionalroles(instrumentalcase),statesofpos-session(genitivecase),orrelativephysicalpositions(variouslocativecases);seeTableA2.4.Notethatmanyofthesedistinctionsareparticularlyimportanttosituatedandcooperatingagents.TAM-CmodiﬁersaresuﬃcientlygeneralandimportantthatNLsencodethemincompactlexicalrepresentations.TheroleofTAM-CmodiﬁersinNLcallsforsimilarmechanismsinNL+,and—likeadjectivesandadverbs—TAM-Cmodiﬁersarenaturalcandidatesforfoldingintolexical-levelvectorrepresentations(SectionA3.3).31.Becausetense,aspect,andmodalityoverlapandintertwine,linguistsoftengroupthemunderthelabel“TAM”;becausetheyalsooverlapwithcasedistinctions,alltheirindicators(inﬂections,markers)willbelumpedtogetherhereandsimplyreferredtoas“TAM-Cmodiﬁers”(forexamplesanddiscussion,seeAppendixA2andSectionA3.3).2.Furthermuddlingstandardlinguisticdistinctions,punctuationcanindicatecaseormodality(questionmarks,exclamationmarks)andcanplayroleslikefunctionwordsthatclarifysyntax(commas,semicolons,periods)orexpressrelationshipsofexplanation,example,orreference(colons,parentheses,quotationmarks).Inspokenlanguage,verbalemphasisandparalinguisticsignalsplaysimilarroles.3.Linguistsrecognizeasemanticspaceofmodalities(Allan2013).335.4Phrases,Sentences,Documents,andLiteraturesNL+representationslikethoseanticipatedherecondense(some)phrase-levelmeaningsintosingle,hencesyntax-free,embeddings.WithinanNL+domain,these“phrase-level”meaningsaredeﬁnitional,notmerelyapproximationsofthemeaningofahypotheticalphraseinNL.Asoutlinedabove,meaningsofkindsthatcorrespondtosimple,lexical-levelnounandverbphrases(inNL)arestrongcandidatesforsingle-embeddingrepresentation,whiletheequivalentsofnounandverbconjunctions(inNL)typicallyarenot;nonethe-less,equivalentsofNLphrasesofotherkinds(somenounclauses?)couldpotentiallybecapturedinsingleembeddings.Theboundariesoftheusefulsemanticscopeofdeﬁnitionalvectorembeddingsarepresentlyunclear,yetatsomelevelbetweenawordandadocument,deﬁnitionalembeddingsmustgivewaytovector-labeledgraphrepresentations.15.5AttheBoundariesofLanguage:Poetry,Puns,andSongDiscussionsofpotentialNL+“expressiveness”comewithacaveat:Tosaythatarepresentation“ismoreexpressive”invitesthequestion“expressivetowhom?”ThemeaningsofNLtextforhumanreadersdependonpotentiallyhuman-speciﬁccognitionandemotion,butNL+expressionscannot,ingen-eral,bereadbyhumans—indeed,systemsthat“read”NL+(e.g.,systemsforwhich“associativememory”meansscalablesearchand“NL+expressions”aresituatedwithinaspectrumofQNRs)areapttobequiteunlikehumans.Whatmightbecalled“outward-facingexpressions”—descriptions,com-mands,questions,andsoon—representakindofsemanticcontentthatmaybeaccessibletohumanminds,butisnotspeciﬁctothem.TheargumentsabovesuggestthatNL+representationscanoutperformNLinthisrole.However,NLtext—andutterances—canconveynotonlyoutward-facingsemanticcontent,buthuman-speciﬁcaﬀectivemeaning,aswellasNL-saturatedassociations,allusions,andwordplay.Punsandpoetrytranslatepoorlyevenbetweennaturallanguages,andpoetrymergesintosongwhichmergesintopuremusic,farfromwhatisusuallyconsideredsemanticexpression.Forpresentpur-poses,thesefunctionsoftextandutteranceswillbeconsideredbeyondthescopeof“language”inthesenserelevanttoNL+functionality;whatcanberepresented,however,isliteralcontent(NLtext,recordedsound)embedded1.Potentiallyaccompaniedbyabstractive,non-deﬁnitionalembeddings(Section8.3.4).Thecontoursofsuchboundariesneednotbedeterminedbyanimplementation:Thereisnoneedtoengineerrepresentationsthatneuralsystemscaninsteadlearn.34inNL+descriptionsofitseﬀectsonhumanminds(whichare,afterall,partsoftheworldtobedescribed).1Inthecontextofthisdocument,thecontent(or“meaning”)ofnaturallanguageexpressionswillbeequatedwithsemanticcontentintheoutward-facingsense.Whenapoemdeliversapunchinthegut,thisisnotitsmeaning,butitseﬀect.6DesiderataandDirectionsforNL+ProspectsforimprovingexpressivenessinNL+representationsincludemechanismsbothlikeandbeyondthosefoundinnaturallanguages.Researchdirectionsaimedatrealizingtheseprospectsarewell-alignedwithcurrentdirectionsinneuralML.BuildingontheNL-centeredconsiderationsdiscussedabove,andlookingforwardtomechanismsbeyondthescopeofnaturallanguage,thissectionoutlinesdesiderataforNL+(andmoregenerally,QNR)functionality.Ap-pendixA3furtherexploresNL+-orientedprospectsforQNRframeworks.6.1ImproveandExtendFundamentalNLConstructsDesiderataforNL+representationsincludeimprovingandextendingfunda-mentalNLconstructs:Subsume(butdonotmodel)NL:TheaimofresearchisnottomodelNL,buttomodel(andextend)itsfunctionality.Exploitdeeplearning:UseneuralMLcapabilitiestoextendNLcon-structswithouthand-craftingrepresentations.Improvecompositionality:Embeddingscanprovideeﬀectivelyinﬁnitevocabulariesandloosenthedependenceofmeaningoncontext.Useexplicitgraphrepresentations:GraphscancomposeembeddingswithouttheconstraintsofNLsyntaxandambiguitiesofcoreference.Exploitvectorembeddingspaces:Relativetowords,embeddingscanimprovebothexpressivecapacityandcomputationaltractability:–Embeddingsarenativelyneuralrepresentationsthatneednotbedecodedanddisambiguated.1.Philosophershavedissectedconsiderationsofthissorttoprovidericherdistinctionsandmorepreciseterminology.35–Embeddings,unlikewords,arediﬀerentiable,facilitatingend-to-endtraining.–Embeddingsprovideeﬀectivelyinﬁnitevocabularies,enrichingexpressivecapacity.Embrace(butdonotimpose)formalsystems:NL+frameworksandformalsystemsarecomplementary,notcompeting,modesofrepresenta-tion.Formalsystemscanbeembeddedassub-languagesinNL+,muchasmathematicalexpressionsareembeddedintextbooks(Section9.2.4).6.2ExploitMechanismsBeyondtheScopeofConventionalNLThediscussionabovehasfocusedonfeaturesofNL+frameworksthatcanberegardedasupgradesoffeaturesofNL,replacingwordswithembeddingsandimplicitsyntactictreeswithexplicit,generalgraphs.Therearealsoopportu-nitiestoexploitrepresentationsbeyondthescopeofNL:Theseincludevectorrepresentationsthatenablenovel,high-levelformsofexpression,abstraction,andsemanticsearch,aswellasQNR-basedtoolsforknowledgeintegrationatscalesyntacticallyembeddednon-linguisticcontentfarbeyondtheboundsofwhatcanbeconstruedaslanguage(Section9.2).6.2.1UseEmbeddingstoModifyandAbstractExpressionsEmbeddingscanperformsemanticrolesatthelevelofsentences,paragraphs,andbeyond.Inanadjective-likeroleSection8.3.1),theycanmodifyorreﬁnethemeaningofcomplexexpressions;inanabstractiverole,theycanenableeﬃcient,shallowprocessing(skimming)(Section8.3.4).6.2.2UseEmbeddingstoSupportScalableSemanticSearchAbstractiveembeddingscansupportsimilarity-basedsemanticsearch—ineﬀect,associativememory—overNL+corpora.Eﬃcientsimilaritysearchscalestorepositoriesindexedbybillionsofembeddings(Section9.1.5).6.2.3Reference,Embed,andWrapEverythingAtasyntacticlevel,NL+frameworkscanembednotonlyformalsystems,butalsocontentofotherkinds(Figure6.1):Non-linguisticlexical-levelunits:Neuralembeddingscanrepresentobjectsthatdiﬀerfromwords,yetcanplayasimilarrole.Forexample,36imageembeddingscanactas“nouns”,whilevectordisplacementsinalatentspacecanactas“adjectives”(Section8.2).Non-linguisticobjects:Throughhyperlinks,onlineNLexpressionscanineﬀectincorporatearbitraryinformationalorcomputationalobjects.NL+expressionscandolikewise(Section9.2).Linguisticobjects:NL+expressionscanreference,describe,andhelpindexlinguisticandlanguage-infusedobjectssuchasbooks,websites,andvideo.NL+contentcanalsocitesources(Section9.5.2). wrapped formal systemswrapped non-linguistic contentNL⁺QNR corpuswrapped NL translationsnatural languagenon-linguistic contentformal systemsFigure6.1:Approximatetransformationandcontainmentrelation-shipsamongrepresentationsystems.6.3ExploitQNRstoSupportKnowledgeIntegrationNL+representationscansupporttoolsforknowledgeintegrationbasedonsemanticsearchoverQNRcorporainconjunctionwithsoftmatching,uniﬁ-cation,andgeneralizationoverQNRrepresentations.1Theseoperationscancompareandcombineexpressionstoidentifyandrepresentareasofconcor-danceorconﬂict,aswellasstructuralanalogies,patterncompletions,and1.Uniﬁcationoftwoexpressionsproducestheleastspeciﬁc(mostgeneral)expressionthatcontainstheinformationofboth;uniﬁcationfailsifexpressionscontainconﬂictinginformation.Generalization(anti-uniﬁcation)oftwoexpressionsproducesthemostspeciﬁc(leastgeneral)expressionthatiscompatiblewith(andhenceuniﬁes)both.Uniﬁcationandanti-uniﬁcationareassociativeandcommutative,andsatisfyseveralotheraxioms.Softuniﬁcationandanti-uniﬁcationrelaxtheseconstraints.(SeeSectionA1.4.3.)37semanticoverlapsthatenabletheintegrationofnarrowexpressionstobuildsemanticstructuresspanbroaderdomains.SoftuniﬁcationofQNRstructurescansupportcontinuousrelaxationsoflogicalinferencethrough(forexample)Prolog-likecomputation(SectionA5.1).Thus,QNRrepresentationsandcor-poracansupportknowledgeintegrationthroughmechanismsbeyondthosereadilyavailableinprocessingNLtext.6.4BuildonCurrentResearchNL+-orientedresearchcanbuildoncurrentMLapplications,methods,ar-chitectures,trainingdata,andtoolsets.Potentiallyusefulcomponentsin-cludeTransformers,graphneuralnetworks,modelsthatembedorgenerategraph/vectorrepresentations,andmultitasklearningmethodsthatcanbeappliedtoshapemultipurposerepresentations.NL+representationsandcorporahavenaturalapplicationsintranslation,questionanswering,andconversationalinterfaces,aswellasreinforcementlearning(RL).1NL+-enabledsystemscouldcontributetocurrentresearchobjectivesinmathematics,science,engineering,robotics,andmachinelearningitself,bothbyhelpingtointegrateandmobilizeexistingknowledge(e.g.,mininglitera-turestoidentifycapabilitiesandopportunities),andbyfacilitatingresearchthatproducesnewknowledge.Eﬀortstoharnessandextendthepowerofnaturallanguagealignwithaspirationsforadvancedmachinelearningandartiﬁcialintelligenceingeneral.6.5SomeCaveatsThepresentdiscussiondescribesgeneralframeworks,mechanisms,andgoals,buttheproposedresearchdirectionsaresubjecttoarangeofpotential(andequallygeneral)criticisms:•ProposedNL+frameworksaretemplatedonNL,butperhapstoocloselytoprovidefundamentallynewcapabilities.•Alternatively,proposedNL+frameworksmaydiﬀertoogreatlyfromNL,undercuttingthefeasibilityofequalingNLcapabilities.•Inlightofthesurprisingpowerofﬂatneuralrepresentations,inductivebiasesthatfavorQNRsmightimpederatherthanimproveperformance.1.ForapplicationsoflanguageinRL,seeDasetal.(2017),Lazaridou,Peysakhovich,andBaroni(2017),Shahetal.(2018),andLuketinaetal.(2019).38•BothneuralMLandhumancognitionembracedomainsthataredecid-edlynon-linguistic,limitingthescopeofNL-relatedmechanisms.•AmbitiousNL+applicationsmaycallformoresemanticstructurethancanreadilybelearned.•ImplementationchallengesmayplaceambitiousNL+applicationsbe-yondpracticalreach.•Currentresearchmaynaturallysolvethekeyproblemswithnoneedtoconsiderlong-termgoals.•Theprospectsasdescribedaretoogeneraltobeusefulinguidingre-search.•Theprospectsasdescribedaretoospeciﬁctobedescriptiveoflikelydevelopments.Mostofthesecriticismsarebestregardedascautions:Linguisticmechanismshavelimitedscope;relaxingconnectionstoNLmayimproveorimpedevariousformsoffunctionality;implementationsofworkingsystemscanbediﬃculttodeveloporfallshortoftheirinitialpromise;motivationsandoutlinesofresearchdirectionsareinherentlygeneralandopen-ended;generic,short-termmotivationsoftensuﬃcetoguidedevelopmentsupagradientthatleadstocapabilitieswithfar-reachingapplications.Nonetheless,despitethesecaveats,near-termresearchchoicesinformedbyQNR/NL+conceptsseemlikelytobemorefruitfulthannot,leadingtotoolsandinsightsthatenableandinformfurtherresearch.Muchcurrentresearchisalreadywell-alignedwithQNRdevelopmentandNL+aspirations,anditisinterestingtoconsiderwherethatresearchmaylead.7Vector-LabeledGraphRepresentationsInconjunctionwithtoday’sdeeplearningtoolkit,vector-labeledgraphrepresentationsprovidepowerful,diﬀerentiablemechanismsforimple-mentingsystemsthatrepresentandprocessstructuredsemanticinfor-mation.Thispresentsectionexaminesvector-labeledgraphrepresentations(VLGs)fromtheperspectivesofrepresentationalcapacityandneuralMLtools;thefollowingsectionwillexamineprospectsforapplyingthisrepresentationalcapacitytoimplementQNRsystemsthatsurpasstheexpressivecapacityof39naturallanguage.17.1ExploitingthePowerofVectorRepresentationsInasense,thelargerepresentationalcapacityoftypicalhigh-dimensionalembeddingvectorsistrivial:Vectorscontaininghundredorthousandsofﬂoatingpointnumberscontainenoughbitstoencodelengthytextsascharac-terstrings.Whatmattershere,however,istherepresentationalcapacityofvectorsinthecontextofneuralML—thescopeandqualityofrepresentationsthatcanbediscoveredandusedbyneuralmodelsthatareshapedbysuitablearchitecturalbiases,lossfunctions,andtrainingtasks.Thisqualitativekindofcapacityisdiﬃculttoquantify,butexamplesfromcurrentpracticeareinformative.7.1.1VectorRepresentationsarePervasiveinDeepLearningDeeplearningtodayisoverwhelminglyorientedtowardprocessingcontin-uousvectorrepresentations,hencetheextraordinarycapabilitiesofdeeplearningtestifytotheirexpressivepower.7.1.2VectorRepresentationsCanEncodeLinguisticSemanticContentContinuousvectorrepresentationsinNLPshedlightonprospectsforex-pressive,tractableQNRs.2.ThetwoleadingrolesforvectorembeddingsinproposedQNRsystemsare(1)deﬁnitionalrepresentationsoflexical-levelcomponents(parallelingvectorsemanticsinNLandwordembeddingsinNLP,Section8.2)and(2)abstractiverepresentationsofhigher-levelconstructsforindexingandsummarization(Section8.3.4).7.1.3SingleVectorsCanServeasCompositionalRepresentationsInconventionalsymbolicsystems,compositionalityenablescomplexmean-ingstoberepresentedbycombinationsofcomponents.Invectorrepresen-tations,meaningscanbeattributedtoorthogonalvectorcomponents(e.g.,representingdiﬀerentpropertiesofsomething),thenthosecomponentscan1.TheserepresentationscanalsobeapproximatelyascompactasNLtext(seeAppendixA5).2.Note,however,successfulapplicationsofdiscretizedrepresentationsinwhichlearned,ﬁnitesetsofvectorsareselectedfromcontinuousspaces;see,forexample,Oord,Vinyals,andKavukcuoglu(2018)andRazavi,Oord,andVinyals(2019)40becombinedbyvectoradditionandrecoveredbyprojectionontotheircorre-spondingaxes.CondensingwhatinNLwouldbemulti-component,lexical-levelsyntacticstructuresintosingleembeddingscanreducethenumberofdistinctrepresentationalelements,retainsemanticcompositionality,andenablefacilemanipulationbyneuralcomputation.7.1.4High-DimensionalSpacesContainManyWell-SeparatedVectorsInconsideringtherepresentationalcapacityofhigh-dimensionalvectors,itisimportanttorecognizewaysinwhichtheirgeometricpropertiesdiﬀerfromthoseofvectorsinthelow-dimensionalspacesofcommonhumanexperience.Inparticular,somemeasuresof“size”areexponentialindimensionality,andarerelevanttorepresentationalcapacity.Callapairofunit-lengthvectorswithcosinesimilarity≤0.5“wellsepa-rated”.Eachofthesevectorsdeﬁnesandmarksthecenterofaset(or“cluster”)ofvectorswithcosinesimilarity≥0.86;thesesetsarelinearlyseparableanddonotoverlap.Howmanysuchwell-separatedcluster-centerscanbefoundinahigh-dimensionalspace?Inagivendimensiond,thenumberofvectorsk(d)thatarewellseparatedbythiscriterionisthe“kissingnumber”,themaximalnumberofnon-overlappingspheresthatcanbeplacedincontactwithacentralsphereofequalradius(Figure7.1).Kissingnumbersinlow-dimensionalspacesaresmall(k(2)=6,k(3)=12...),butgrowrapidlywithd.Ford=64and128,1k(d)>107and1012;ford=256,512,and1024,anasymptoticlowerbound(Edel,Rains,andSloane2002)k(d)>20.2075...dgivesk(d)>1014,1030,and1062.Thus,thenumberofneighboringyetwell-separatedcluster-centersthatcanbeembeddedinspacesofdimensionalitiescommonlyusedinneuralMLisfar(!)inexcessofanypossibleNLvocabulary.2Notethattheregionaroundacluster-centeritselfhasgreatrepresentationpowerforsub-clusters:Forexample,itscontentcanbeseparatedfromtherestofthespacebyalinearthresholdoperationandthenscaledandprojectedintoaspaceofd–1dimensions,wheresimilarconsiderationsapplyrecursively1.CitedinTorquatoandStillinger(2006).2.Notethatthenumberofvectorsthatareallnearlyorthogonal(allpairwisecosinesimi-larities(cid:28)0.5)alsogrowsexponentiallywithdandbecomesenormousinhighdimensionalspaces.41Figure7.1:Kissingspheres,d=2,k=6solongastheresidualdimensionalityremainslarge.1Theabovedescriptionisintendedtoprovideanintuitionforsomeaspectsoftheexpressivecapacityofhigh-dimensionalvectorspaces,nottopredictorsuggesthowthatcapacitywillorshouldbeusedinprospectivesystems:LearnedrepresentationsincurrentneuralMLmayoﬀerabetterguide.7.1.5PointsCanRepresentRegionsinLower-DimensionalSpaceshttps://www.overleaf.com/project/61183c4a1765f22abf2e3ebfAnembed-dingofdimensionality2dcanrepresent(forexample)acenter-pointinad-dimensionalsemanticspacetogetherwithparametersthatspecifyasur-roundingboxinthatspace.2Anembeddingmaythendesignate,notaspeciﬁcmeaning,butarangeofpotentialmeanings;alternatively,arangecanberegardedasaspeciﬁcmeaninginasemanticspacethatexplicitlyrepresentsambiguity.Intervalarithmetic3generalizessomeoperationsond-dimensionalpointstooperationsond-dimensionalboxes.1.TheuseofEuclideandistanceorcosinesimilarityisexplicitlyortacitlyassumedinmuchofthisdocument,butgrowinginterestsuggeststhathyperbolicspaces(usefulingraphandsentenceembeddings)ormixedgeometriesmayprovideattractivealternativesforembeddingQNRexpressions;seeforexamplePengetal.(2021).2.AppendixA1,SectionA1.5.3,andSectionA1.6discussbothboxesandmoreﬂexibleclassesofrepresentationsinthecontextofuniﬁcationandgeneralizationoperationsonsoftlattices.3.ArithmeticinwhichoperandsareintervalsoverR.42Thisdocumentwillusuallydiscussembeddingsasiftheyrepresentpointsinsemanticspaces,withoperationsonembeddingsdescribedasifactingonvectorsthatdesignatepoints,ratherthanregions.Theconceptofsemanticre-gionsbecomescentral,however,inconsideringsemanticlatticestructureandconstraint-basedinferencethatgeneralizeslogicprogramming(SectionA1.4).7.2ExploitingthePowerofGraph-StructuredRepresentationsGraphsareubiquitousasrepresentationsofcompositionalstructurebe-causetheycandirectlyrepresentthingsandtheirrelationshipsasnodesandarcs.Graphs(inparticular,treesandDAGs)arecentraltotraditional,word-orientednaturallanguageprocessing,whilegeneralgraphshavefoundgrowingapplicationsindiverseareasofneuralML.Thissectionoutlinessev-eralclassesofgraphsandtheirpotentialrolesinrepresentingandprocessingsemanticinformation.7.2.1Terminology,Kinds,andRolesofGraphsThecomponentsofgraphsarevariously(andsynonymously)callededges,arcs,orlinks(potentiallydirected),whichconnectvertices,points,ornodes,whichinturnmaycarrylabels,attributes,orcontents.Thepresentdiscussionwilltypicallyrefertoarcs(orlinksbetweendocument-scaleobjects)thatconnectnodesthatcarryattributesorlabels(inasemanticcontext,contentsorembeddings).1Here,“graph”typicallydenotesadirectedgraphwithattribute-bearingnodes.Labeledarcs,multigraphs,andhypergraphs2arepotentiallyusefulbutnotexplicitlydiscussed;weightedarcsareessentialinsomediﬀerentiablegraphrepresentations.(Sequencesofembeddingscanbeviewedasinstancesofaparticularlysimpleclassofgraphs.)InprospectiveQNRframeworks,vector-labeledgraphshaveatleasttwoareasofapplication:TheﬁrstareaparallelstheuseofgraphsinclassicNLP,whereexpressionsaretypicallyparsedandrepresentedassyntaxtreesor(to1.Insomeformalmodels,arcsalsocarryattributes.Withoutlossofgenerality,graphsGwithlabeledarcscanberepresentedbybipartitegraphsG(cid:48)inwhichlabeledarcsinGcorrespondtolabelednodesinG(cid:48).Forthesakeofsimplicity(anddespitetheirpotentialimportance)thepresentdiscussiondoesnotexplicitlyconsiderlabeledarcs.Ingeneral,computationalrepresentationsofgraphswillbeimplementation-dependentandwillchangedependingoncomputationalcontext(e.g.,soft,internalrepresentationsinTransformerstranslatedtoandfromhard,externalrepresentationsinexpression-stores).2.VanLierdeandChow(2019)andMenezesandRoth(2021)43representresolvedcoreferences)DAGs;VLGscanrepresentsyntactictreesexplicitly,bypassingparsing,andcanrepresentcoreferencethroughDAGs,bypassingresolution.Thesecondareaofapplicationinvolveshigher-levelsemanticrelationshipsthatinNLmightberepresentedbycitations;inanNL+context,similarrelationshipsarenaturallyrepresentedasgeneral,potentiallycyclicgraphs.(ThesetopicsarediscussedinSection8.1.)7.2.2VLGsCanProvideCapacityBeyondStand-AloneEmbeddingsFixed-lengthembeddingslackscalability,whilesequencesofembeddings(e.g.,outputsofTransformersandrecurrentnetworks),thoughpotentiallyscalable,lackexplicit,composable,andreadilymanipulatedstructure.Arcs,bycontrast,cancomposegraphstoformlargergraphsexplicitlyandrecursivelywithnoinherentlimittoscaleorcomplexity,andsubgraphcontentcanbereferencedandreusedinmultiplecontexts.Treesandgraphsarestandardrepresentationsforcompositionalstructureinahostofdomains,andarelatentinnaturallanguage.Graphswithvectorattributescanexpandtherepresentationalcapacityofsetsofembeddingsbyplacingtheminascalable,compositionalframework.17.2.3VLGsCanBeDiﬀerentiableTypicalneuraloperationsonvectorattributesaretriviallydiﬀerentiable,whilediﬀerentiableoperationsonrepresentationsofgraphtopologiesrequirespe-cialattention.Withoutemployingdiﬀerentiablerepresentations,optionsforseekinggraphsthatminimizelossfunctionsincludesearch(potentiallyemployingheuristicsorreinforcementlearning)andone-shotalgorithmicconstruction.2Withdiﬀerentiablerepresentations,moreoptionsbecomeavail-able,includingstructurediscoverythroughend-to-endtrainingorinference-timeoptimization.Conventionalrepresentationsinwhichnodesandarcsaresimplypresentorabsentcanbetermed“hardgraphs”;representationscanbemade“soft”anddiﬀerentiablebyassigningweightsintherange[0,1]toarcs.Diﬀerentiablealgorithmsthatassignweightstendingtoward{0,1}canrecoverconventional1.Singleembeddingscan,however,representsmallgraphs;thus,graph-structuredrepre-sentationdoesnotalwaysrequirereiﬁcationofnodesandarcs(SeeSection7.3).2.Yunetal.(2019),J.Zhouetal.(2020),Kazietal.(2020),andWuetal.(2021).Forap-plicationsofRLtographconstruction,seeZ.Zhouetal.(2019)andTrivedi,Yang,andZha(2020).44graphsbydiscardingedgeswhentheirweightsapproachzero,implementingstructurediscoverythroughdiﬀerentiablepruning.Diﬀerentiablepruningoperatesonaﬁxedsetofnodesandtypicallyconsid-ersallpairs,impairingscalability,1butalgorithmsthatexploitnear-neighborretrievaloperationsonwhatmaybeverylargesetsofnodes(Section9.1.5)couldimplementscalable,diﬀerentiable,semanticallyinformedalternativesthatdonotapriorirestrictpotentialtopologies.Intypicalgraphrepresenta-tions,alinkisimplementedbyasemanticallymeaninglessvalue(e.g.,anarrayindexorhash-tablekey)thatdesignatesatargetnode.Throughnear-neighborretrieval,bycontrast,avectorassociatedwithasource-nodecanserveasaqueryintoasemanticallymeaningfulspacepopulatedbykeysthatcorre-spondtocandidatetarget-nodes.Selectingtheuniquenodeassociatedwiththenearest-neighborkeyyieldsahardgraph;attendingtodistance-weightedsetsofnearneighborsyieldsasoftgraph.2Inthisapproach,queryandkeyembeddingscanmovethroughtheirjointembeddingspaceduringtraining,smoothlychangingneighbordistancesandthecorrespondingarcweightsinresponsetogradients.Mutable,soft-graphbehaviorcanberetainedatinferencetime,ormodelscanoutputconventional,hard-graphVLGs,potentiallyretaininggeometricinformation.Thus,modelsthatbuildandupdateQNRcorporacouldprovideﬂuidrepresentationsinwhichchangesinembeddingsalsochangeimpliedconnectivity,implicitlyaddinganddeleting(weighted)arcs.Insemanticallystructuredembeddingspaces,theresultingchangesintopologywillbesemanticallyinformed.Weightedgraphsmayalsobeproductsofacomputationratherthaninterme-diatesincomputinghard-graphrepresentations.Substitutingasetofweightedarcsforasinglehardarccouldrepresenteitherstructuraluncertainty(poten-tiallyresolvedbyfurtherinformation)orintentional,semanticallyinformativeambiguity.7.2.4VLGsCanSupportAlignmentandInferenceNeuralalgorithmsoverVLGscansupportbothprocessingwithinsingleexpressionsandoperationsthatlinkorcombinemultipleexpressions.These1.Toenablearbitrarygraphsasoutputsrequiresinitializingwithcompletegraphs(henceN(N−1)directedarcs).Restrictingoptimizationtolocalsubgraphs,torestrictedsearchspaces,ortoalgorithmicallygenerated“roughdrafts”canavoidthisdiﬃculty.2.Note,however,thateﬃcient,highlyscalablenear-neighborretrievalalgorithmsonun-structuredspacesaretypically“approximate”inthesensethatnearestneighborsmaywithsomeprobabilitybeomitted.Thisshortcomingmayormaynotbeimportantinagivenappli-cation,andfastalgorithmsonweaklystructuredspacescanbeexact(seeLampleetal.2019).45havepotentiallyimportantapplicationswithinQNRframeworks.Algorithmsthatalignsimilargraphs1canfacilitateknowledgeintegrationoverlargecorpora,supportingtheidentiﬁcationofclashes,concordances,andoverlapsbetweenexpressions,andtheconstructionofreﬁnements,general-izations,analogies,patterncompletions,andmergedexpressions,2aswellasmoregeneralformsofinterpretationandreasoning.Diﬀerentiablealgorithmsforgraphalignmentincludecontinuousrelaxationsofoptimalassignmentalgorithmsthatenableend-to-endlearningofalignable,semanticallymean-ingfulembeddings(Sarlinetal.2020).7.2.5VLGsCanSupport(Soft)UniﬁcationandGeneralizationIfwethinkofexpressionsasdesignatingregionsofsomesort3,thentounifyapairofexpressionsistoﬁndthelargestexpressibleintersectionoftheirregions,whiletoanti-unify(orgeneralize)apairofexpressionsistoﬁndthenarrowestexpressibleunionoftheirregions.Domainsthatsupporttheseop-erations(andsatisfyafurthersetofaxioms)constitutemathematicallattices.4Givenasetofexpressions,uniﬁcationmaybeusedtoinfernarrower,morespeciﬁcexpressions,whileanti-uniﬁcationoperationsmaybeusedtoproposebroader,moregeneralexpressions.Asdiscussedabove,QNRexpressionsthatrepresentexplicituncertaintyorambiguity(e.g.,containingvectorsrepresentinguncertainorpartiallyconstrainedvalues)mayberegardedasrepresentingregionsinasemanticspace.Thenatureof“expressibleregions”intheabovesensedependsonchoicesamongrepresentations.Notionsof“soft”or“weak”uniﬁcation(discussedinSectionA1.4.3)canreplaceequalityofsymbolswithsimilaritybetweenpoint-likesemanticembed-dings,orapproximateoverlapwhenembeddingsareinterpretedasrepresentingsemanticregions.Softuniﬁcationsupportscontinuous,diﬀerentiablerelax-ationsofthePrologbackward-chainingalgorithm,enablingsoftformsofmulti-steplogicalinference.Applicationsofsoftuniﬁcationincludequestion1.Heimannetal.(2018),Caoetal.(2019),Qu,Tang,andBengio(2019),andFeyetal.(2020)2.Softuniﬁcationandanti-uniﬁcationoperationscancontributetothisfunctionality(Ap-pendixA1).3.Inlogic,symbolscorrespondtozero-volumeregions,whilevariablescorrespondtounboundedregions.4.SeeSectionA1.2.Ingeneralizationsoflatticerepresentations,a“region”maybefuzzy,ineﬀectdeﬁningapatternofsoftattentionoverthespace,andlatticerelationshipsmaybeapproximate(SectionA1.4.3).46answering,natural-languagereasoning,andtheoremproving;1Softoper-ationscanalsoinfervariablesfromsetsofvalues(CingilliogluandRusso2020).Asnotedabove,uniﬁcationandgeneralizationhavefurtherpotentialapplicationstoknowledgeintegrationinNL+corpora.7.3MappingBetweenGraphsandVectorSpacesResearchhasyieldedarangeoftechniquesformappinggraphstoandfromvectorrepresentations.Thesetechniquesareimportantbecausetheybridgethegapbetweenhigh-capacitycompositionalstructuresandindividualem-beddingsthatlendthemselvestodirectmanipulationbyconventional(non-GNN)neuralnetworks.7.3.1EmbeddingsCanRepresentandDecodetoGraphsNeuralmodelscanbetrainedtoencodegraphs2(includingtree-structuredexpressions3)asembeddings,andtodecodeembeddingstographs.4Acom-montrainingstrategycombinesgraph-encodingandgenerativemodelswithanautoencodingobjectivefunction.5Inthedomainofsmallgraphs,embeddingscouldencoderepresentationswithconsiderableaccuracy—potentiallywithdeﬁnitionalaccuracy,forgraphsthataresmallinbothtopologyandinformationcontent.Notethatsummary,query,andkeyembeddings(Section8.3.4andSection9.1.2)canrepresentsemanticcontentwithoutsupportinggraphreconstruction.7.3.2EmbeddingsCanSupportGraphOperationsEmbeddingsthatenableaccurategraphreconstructionprovidediﬀerentiablegraphrepresentationsbeyondthosediscussedinSection7.2.3.Whetherdi-rectlyorthroughintermediate,decodedrepresentations,graphembeddingscansupportafullrangeofVLGoperations.Accordingly,thisclassofembed-dingscanbeconsideredinterchangeablewithsmall6VLGs,andneednotbe1.RocktäschelandRiedel(2017),Camperoetal.(2018),Minervinietal.(2018),Weberetal.(2019),andMinervinietal.(2020)2.Narayananetal.(2017),Grohe(2020),andPanetal.(2020)3.Allamanisetal.(2017),R.Liuetal.(2017),H.Zhangetal.(2018),andAlonetal.(2019)4.Y.Lietal.(2018)andCaoandKipf(2018)5.Panetal.(2018),SimonovskyandKomodakis(2018),andSalehiandDavulcu(2020)6.Whatcountsas“small”isanopenquestion.47consideredseparatelyhere.Potentialadvantagesincludecomputationaleﬃ-ciencyandseamlessintegrationwithotherembedding-basedrepresentations.8QuasilinguisticNeuralRepresentationsApplicationsofvector-labeledgraphscangeneralizeNLsyntaxandupgradeNLwordstoimplementquasilinguisticneuralrepresentationsthatparallelandsurpasstheexpressivecapacityofnaturallanguageatmultiplelevelsandscales.Thepresentsectiondiscusseshowvector-labeledgraphs(VLGs)canbeappliedtoimplementquasilinguisticneuralrepresentations(QNRs)thatimproveonnaturallanguagesbyupgradingexpressivecapacity,regularizingstructure,andimprovingcompositionalitytofacilitatethecompilation,extension,andintegrationoflargeknowledgecorpora.AppendixA3coversanoverlappingrangeoftopicsinmoredetailandwithgreaterattentiontoNLasamodelforpotentialNL+frameworks.Asusualinthisdocument,conceptualfeaturesanddistinctionsshouldnotbeconfusedwithactualfeaturesanddistinctionsthatinend-to-endtrainedsystemsmaybeneithersharp,norexplicit,nor(perhaps)evenrecognizable.Conceptualfeaturesanddistinctionsshouldbereadneitherasproposalsforhand-craftedstructuresnorasconﬁdentpredictionsoflearnedrepresenta-tions.Theyservetosuggest,nottheconcreteform,butthepotentialscopeofrepresentationalandfunctionalcapacity.8.1UsingGraphsasFrameworksforQuasilinguisticRepresenta-tionProposedQNRsarevector-labeledgraphs.1Attributescanincludetypeinfor-mationaswellasrichsemanticcontent;tosimplifyexposition,2attributesofwhataresemanticallyarcscanberegardedasattributesofnodesinabipartitegraph,3andwillnotbetreatedseparately.Liketrees,general(e.g.,cyclic)graphscanhavedesignatedroots.1.Attributes(labels)canpotentiallybeexpandedtoincludesetsofvectorsofexplicitlyorimplicitlydiﬀeringtypes.2.Andperhapsalsoimplementation.3.Arepresentationthatcanalsoaccommodatemultigraphsandhypergraphs.488.1.1RolesforGraphsinNL-LikeSyntaxAsalreadydiscussed,vectorattributescanrepresentlexical-levelcomponents,whilegraphscanrepresenttheirsyntacticcompositions;wheresyntaxinNLisimplicitandoftenambiguous,QNR-graphscanmakesyntacticrelation-ships(andinparticular,coreference)explicit,therebydisentanglingtheserelationshipsfromlexical-levelsemanticcontent.(SeealsoSectionA3.1.1.)8.1.2RolesforGraphsBeyondConventionalSyntaxInanextendedsense,thesyntaxofNLinthewildincludesthesyntaxof(forexample)hierarchicallystructureddocumentswithembeddedtables,crossreferences,andcitations.1QNRscannaturallyexpressthese,aswellassyntacticstructuresthatareunlikeanythatcanbedisplayedinadocument.2Stretchingtheconceptofsyntax,graphscanexpressnetworksofrelation-shipsamongobjects.Scenegraphsprovideaconcreteillustrationofhowrecognizablylinguisticrelationshipscannaturallybeexpressedbyanon-conventionalsyntax(forasimpleexample,seeFigure8.1).Figure8.1:Aneurallyinferredscenegraphthatrelatesseveralen-titiesthroughsubject–verb–objectrelationships.Anenrichedgraphforthisscenecouldrepresentmorecomplexrelationships(e.g.,man1failingtopreventman2fromthrowingaFrisbeetoman3).Anen-richedrepresentationofentitiescouldreplaceinstancesoflabelslike“man”withembeddingsthathavemeaningsmorelikeman-with-appearance(x)-posture(y)-motion(z),replaceinstancesofverbslike“catching”withembeddingsthathavemeaningsmorelikeprobably-will-catch-intended-pass,andsoon.31.Andfootnotes.2.Examplesnotincluded.3.FigurefromRabohetal.(2020),usedwithpermission.498.2UsingEmbeddingstoRepresentLexical-LevelStructureAtalexicallevel,embedding-spacegeometrycancontributetoexpressivepowerinseveralways:Proximitycanencodesemanticsimilarity.Largedisplacementscanencodediﬀerencesofkind.Smalloﬀsets(orlargerdisplacementsindistinguishablesubspaces)canencodegraded,multidimensionalsemanticdiﬀerences.WhereNLdependsoncontexttodisambiguatewords,QNRscanemploylexical-levelembeddingstoexpressmeaningsthataresubstantiallydisentan-gledfromcontext.(Forfurtherdiscussion,seeSectionA3.2.1.)8.2.1ProximityandSimilarityNeuralrepresentationlearningtypicallyplacessemanticallysimilarentitiesnearoneanotherandunrelatedentitiesfarapart.EmbeddingNLwordsworkswellenoughtobeuseful,yetitsutilityisimpairedbypolysemyandotherambiguitiesofnaturallanguage.1Byconstruction,nativeembedding-basedrepresentationsminimizethesediﬃculties.8.2.2GradedSemanticDiﬀerencesThedirectionandmagnitudeofincrementaldisplacementsinembeddingspacescanrepresentgraded,incrementaldiﬀerencesinpropertiesamongsimilarentities:Thedirectionofadisplacementaxisencodesthekindofdiﬀer-ence,themagnitudeofthedisplacementalongthataxisencodesthemagnitudeofthecorrespondingdiﬀerence,2andthe(commutative)additionofdisplace-mentscomposesmultiplediﬀerenceswithoutrecoursetosyntacticconstructs.1.Whenasingleword(e.g.,“match”)hasmultipleunrelatedmeanings(homonymy),itcorrespondstomultiple,potentiallywidelyscatteredpointsinanaturalsemanticspace;whenaword(e.g.,“love”)hasarangeofrelatedmeanings(polysemy)representationsthatmapword-meaningstopoints(ratherthansemanticregions)becomeproblematic.SeeVicente(2018).2.Whendiﬀerentregionsofavectorspacerepresentthingsofdistinctkinds,themeaningsofdirectionsmayvarywithposition:Inotherwords,becausediﬀerentkindsofthingshavediﬀerentkindsofproperties,itisnaturalformappingsofdirectionstopropertiestobeafunctionofkinds,andhenceoflocation.Indeed,theliteraturedescribesdiscrete-wordmodelsofNLsemanticsinwhichthemeaningsofadjectivesareafunctionoftheirassociatednouns;see,forexample,BaroniandZamparelli(2010)andBlacoeandLapata(2012).Inthisgeneralapproach,themeaningsofverbsarealsoafunctionoftheirassociatednouns,andthemeaningsofadverbsarefunctionsoftheirassociatedverbs.50Byavoidingdiﬃcultiesarisingfromthediscretewordsandword-sequencesofNL,theuseofcontinuous,commutativevectoroﬀsetscansubstantiallyregularizeanddisentanglelexical-levelsemanticrepresentations.8.2.3RelationshipsBetweenEntitiesofDistinctKindsDistinctionsbetweenentitiesofdiﬀerentkinds1canberepresentedasdiscretedisplacementvectors,whichcanencodedegreesofsimilarityindistancesandclusterthingsofsimilarkinds(animalswithanimals,machineswithmachines,etc.,asseeninthegeometryofwordembeddings).Displacementdirectionscanalsoencodeinformationaboutkinds.Wordembeddingstrainedonlyonmeasuresofco-occurrenceintexthavebeenfoundtorepresentrelationshipsbetweenentitiesindisplacementvectors,andanalogiesinvectordiﬀerencesandsums(AllenandHospedales2019).Inneuralknowledgegraphs,2relationshipsbetweenentitiescanbeencodedingeometriesthataredeliberatelyconstructedorinducedbylearning.3Lexical-levelQNRlabelscanprovidesimilarrepresentationalfunctionality.8.3ExpressingHigher-LevelStructureandSemanticsThediscussionaboveaddressedthevectorsemanticsofembeddingsthatplaylexical-levelroles(e.g.,nounsandverbswithmodiﬁers);thepresentdiscussionconsidersrolesforembeddingsinthesemanticsofhigher-level(supra-lexical)QNRexpressions.Someoftheserolesaredeﬁnitional:intheseroles,embeddingsmodifythemeaningsofhigher-levelconstructs.4Inotherroles,embeddingsareabstractive:Theirsemanticcontentmaycorrespondsto(quasi)cognitiveresultsofreadinghigher-levelconstructs,andarethereforesemanticallyoptional(ineﬀect,theycacheandmakeavailablecomputationalresults).Asusual,theaimhereistodescribeavailablerepresentationalfunctionality,nottopredictorproposespeciﬁcrepresentationalarchitectures.Representa-1.Where“entity”isintendedtoinclude(atleast)diﬀerentthings(horses,cows,photons,telescopes,gravitation,integers)anddiﬀerentactions(walk,run,observe,report).2.“[A]graphofdataintendedtoaccumulateandconveyknowledgeoftherealworld,whosenodesrepresententitiesofinterestandwhoseedgesrepresentrelationsbetweentheseentities”(Hoganetal.2021).3.Jietal.(2021)andAlietal.(2021)4.Expression-leveldeﬁnitionalembeddingsarenotsharplydistinguishedfromlexicalembeddings,e.g.,thosecorrespondingtofunctionwords.51tionlearningwithinaQNRframeworkneednot(andlikelywillnot)respecttheseconceptualdistinctions.8.3.1Expression-LevelModiﬁers(Deﬁnitional)Althoughlexical-levelembeddingscancondense(analoguesof)phrasesthatincludemodiﬁersof(analoguesof)words,somelexical-levelmodiﬁersoperateonsupra-lexicalexpressionsthatcannotbecondensedinthisway.Thesemodiﬁersmayresembleadjectives,butareattachedtounitsmorecomplexthanwords.Epistemicqualiﬁers(SectionA3.4)provideexamplesofthisfunctionality.Syntactically,deﬁnitionalmodiﬁersofthiskindcouldbeappliedtoexpres-sionsasvectorattributesoftheirrootnodes;semantically,expression-levelmodiﬁersactasfunctionsofasingleargument:anexpressionasawhole.8.3.2Expression-LevelRelationships(Deﬁnitional)InNL,conjunctiveelements(oftenfunctionwords)cancomposeandmodifythemeaningsofcombinedexpressions.Somecanoperateoneitherlexical-levelunitsorcomplexexpressions(examplesincludeand,or,and/or,both-and);others(forexample,however,because,anddespite1)typicallycomposemeaningacrosssubstantialsyntacticspans,operatingnotonlyatthelevelofclausesandsentences,butatthelevelofparagraphsandbeyond.Syntactically,arelationshipbetweenexpressionscouldbeappliedthroughvectorattributesoftherootnodeofacompositionofsyntacticstructures(subgraphsofatree,DAG,etc.);semantically,expression-levelrelationshipscorrespondtofunctionsoftwoormorearguments:theexpressionstheyrelate.8.3.3Expression-LevelRoles(Deﬁnitional)Expressionsandthethingstheydescribehaveroles(frequentlywithgradedproperties)inlargercontexts;featuresofrolesmayincludepurpose,impor-tance,relativetime,epistemicsupport,andsoon.Aswithexpression-levelmodiﬁersandrelationships,roledescriptionsofthiskindcouldbeappliedtoanexpressionthroughvectorattributesofitsrootnode.Ingeneral,however,arolemaybespeciﬁctoaparticularcontext.Ifanexpressioniscontained(referenced,used)inmultiplecontexts,itmayhavemultipleroles,hencerepresentationsofthoserolesmustsyntacticallystand1.Seealso:punctuation.52aboveroot-nodesandtheirattributes.Moreconcretely,multiplecontextsmaylinktoanexpressionthroughnodesthatrepresentthecorrespondingcontextualroles.8.3.4ContentSummaries(Abstractive)Inhumancognition,theactofreadingatextyieldscognitiverepresentations—ineﬀect,abstractivesummaries—thatcancontributetounderstandingdi-rectly,orbyhelpingtoformulate(whatareineﬀect)queriesthatdirectlyenableretrievalofrelevantlong-termmemories,orindirectlyenableretrievalofrelevanttexts.1InaQNRcontext,dynamicallygeneratedsummaryembed-dingscanplayasimilar,directroleatinferencetime.Whenstored,however,summariescandomore:•Amortizereadingcostsforinferencetasks.•Enableeﬃcientskimming.•Stretchthesemanticspanofattentionwindows.2•Providekeysforscalableassociativememory.Thenatureofasummarymaydependonitsuse,potentiallycallingformultiple,task-orientedsummariesofasingleexpression.3Semantically,contentsummariesarenotexpression-levelmodiﬁers:Sum-mariesapproximatesemantics,butmodiﬁershelpdeﬁnesemantics.Contentsummariesmaydependoncontextualroles,placingtheminsyntacticposi-tionsabovetherootsofsummarizedexpressions.Anaturalauxiliarytrainingobjectivewouldbeforthesimilarityofpairsofcontent-summaryvectorstopredicttheuniﬁcationscores(SectionA1.4.3)ofthecorrespondingpairsofexpressions.48.3.5ContextSummaries(Abstractive)Contextsummaries(e.g.,embeddingsthatsummarizecontext-describingQNRs)representinformationthatispotentiallyimportanttointerpretingand1.InprocessingwithaccesstoQNRcorpora,theanaloguesoflong-termmemoryandtextconverge.2.ForananalogousapplicationofsummarizationinTransformers,seeRaeetal.(2019).3.E.g.,representationsofsubstantivecontent(potentialanswerstoquestions)vs.represen-tationsofthekindsofquestionsthatthesubstantivecontentcananswer(potentiallyusefulaskeys).4.Summarieswiththispropertyseemwellsuitedforuseaskeysinsearchguidedbysemanticsimilarity.53usingaQNRexpression.1•Whatarethegeneralandimmediatetopicsofthecontext?•Isthecontextaformaloraninformaldiscussion?•Howdoesthecontextusetheexpression?•Doesthecontextsupportorcriticizethecontentoftheexpression?Atinferencetime,contextsummariescoulddirectattentiontorelevantcom-monsenseknowledgeregardingaparticularﬁeldortheworldingeneral.2Interpretinganexpressioncorrectlymayrequirecontextualinformationthatisdistributedoverlargespans;providing(quasi)cognitiverepresentationsofcontextscanmakethisinformationmorereadilyavailable.Likecontentsum-maries,contextsummariesaresemanticallyoptional,butthepotentialscaleandcomplexityofcontexts(local,domain,andglobal)maymakesomeformofcontext-summarizationunavoidable,ifextendedcontextsaretobeusedatall.Syntactically(andtosomeextentsemantically),contextsummariesresemblecontextualroles.8.3.6OriginSummaries(Abstractive)Originsummaries(potentiallysummarizingorigin-describingQNRs)canindicatehowanexpressionwasderivedandfromwhatinformation:•Whatinputswereusedinconstructingtheexpression?•Whatwastheirsource?•Whatwastheirquality?•Whatinferenceprocesswasapplied?AstructurallydistinctQNRexpressioncanbeviewedashavingasinglesource(its“author”),andhenceitsrootnodecanbelinkedtoasingleoriginsummary.Originsandtheirsummariesareimportanttotheconstruction,correction,andupdatingoflargecorpora.1.Theconsiderationsherearequitegeneral;forexample,contextrepresentationsarealsoimportantinunderstanding/recognizingobjectsinscenes(Tripathietal.2019;Carionetal.2020).2.StandardTransformersrepresentaspanofimmediatecontextintheiractivations,whilerepresentingbothrelevantandirrelevantglobalcontextintheirparameters.Bothoftheserepresentationsaresubjecttoproblemsofscaling,cost,compositionality,andinterpretability.Models(potentiallyTransformer-based)thatwriteandreadQNRscouldbeneﬁtfromexter-nalizedrepresentationsofwide-ranging,semanticallyindexedcontextualknowledge.Theapplicationofverygeneralknowledge,however,seemscontinuouswithinterpretiveskillsthatarebestembodiedinmodelparameters.548.4Regularizing,Aligning,andCombiningSemanticRepresenta-tionsDisentanglingandregularizingsemanticrepresentations—athemethatrunsthroughSection8.1andSection8.2—hasarangeofbeneﬁts.InconjunctionwithbasicQNRaﬀordances,regularizationcanfacilitatethealignmentofrepresentations,whichinturncanfacilitatesystematicinference,compari-son,combination,andrelatedformsofsemanticprocessingatthelevelofcollectionsofexpressions.8.4.1RegularizingRepresentationsInnaturallanguages,expressionshavingsimilarmeaningsmay(andoftenmust)takesubstantiallydiﬀerentforms.Wordsaredrawnfromdiscrete,coarse-grainedvocabularies,henceincrementaldiﬀerencesinmeaningforcediscretechangesinwordselection.Further,smalldiﬀerencesinmeaningareoftenencodedinstructuralchoices—activevs.passivevoice,parallelvs.non-parallelconstructs,alternativeorderingsoflists,clauses,sentences,andsoon.Theseexpressivemechanismsroutinelyentanglesemanticswithstructureinwaysthatimposeincompatiblegraphstructuresonexpressionsthatconveysimilar,seeminglyparallelmeanings.ThenatureofQNRexpressionsinvitesgreaterregularization:Meaningsofincommensuratekindsmayoftenbeencodedinsubstantiallydiﬀerentgraphtopologies,whileexpressionswithmeaningsthataresimilar—orquitediﬀer-ent,yetabstractlyparallel—canberepresentedbyidenticalgraphtopologiesinwhichdiﬀerencesareencodedinthecontinuousembeddingspacesofnodeattributes.Humanscomparemeaningsafterdecodinglanguagetoformneuralrep-resentations;nativelydisentangled,regularized,alignablerepresentationscanenablecomparisonsthataremoredirect.Byfacilitatingalignmentandcomparison,regularizationcanfacilitatesystematic(andevensemi-formal)reasoning.8.4.2AligningRepresentationsSection7.2.4tooknoteofneuralalgorithmsthatcanaligngraphsinwhichparallelsubgraphscarrycorrespondingattributes.Givenapairofaligned55graphs,furtherprocessingcanexploittheseexplicitcorrespondences.18.4.3AlignmentforPattern-DirectedActionBytriggeringpattern-dependentactions,graphalignmentcanestablishrela-tionshipsbetweenotherentitiesthatdonotthemselvesalign.Forexample,reasoningfromfactstoactionscanoftenbeformulatedintermsofproduc-tionrules2thatupdatearepresentation-state(orinsomecontexts,causeanexternalaction)whenthepatternthatdeﬁnesarule’spreconditionmatchesapatterninacurrentsituation:Wherepatternstaketheformofgraph-structuredrepresentations,this(attempted)matchingbeginswith(attempted)graphalignment.Potentialandactualapplicationsofproductionrulesrangefromperceptualinterpretationtotheoremproving.8.4.4Comparing,CombiningandExtendingContentAlignmentfacilitatescomparison.Whengraphsalign,naturaldistancemet-ricsincludesuitablyweightedandscaledsumsofdistancesbetweenpairsofvectorattributes.3Whenalignmentispartial,subgraphsinoneexpressionmaybeabsentfromsubgraphsintheother(consistentwithcompatibility),orsubgraphsmayoverlapandclash.Clashesmayindicatemutualirrelevanceorconﬂict,dependingonsemanticcontentsandproblemcontexts.Overlappinggraphscan(whencompatible)bemergedtoconstructex-tended,consistentdescriptionsofsomeentityordomain.Informalsystems,thisoperationcorrespondstouniﬁcation(SectionA1.1.1);ingeneralQNRcontexts,formaluniﬁcationcanbesupplementedorreplacedbysoft,learnedapproximations(SectionA1.4.3).Expressionsthataresimilarandcompatibleyetnotidenticalmaypro-vidediﬀerentinformationaboutasingleobjectorabstraction;uniﬁcationmechanismsthencanenrichrepresentationsbymerginginformationfrom1.Inthisconnection,considerpotentialgeneralizationsoftheimage-domainalgorithmdescribedinSarlinetal.(2020),whichemploysend-to-endtrainingtorecognize,represent,andalignfeaturesinpairsofdataobjectsthroughGNNprocessingfollowedbydiﬀerentiablegraphmatching.AnalogousprocessingofQNRrepresentationswouldenablesemantics-basedalignmentevenintheabsenceofstrictstructuralmatches.2.Inrecentworkonneuralimageinterpretation,Goyaletal.(2021)employedpropositionalknowledgeintheformofproductionrules(condition-actionpairs)thatarerepresentedbymodelparametersandselectedatinferencetimebyattentionmechanismsbasedonsimilaritybetweenconditionandactivationvectors.3.Distancemetricsmayresultfromneuralcomputationsthatconditiononbroadersemanticcontent,oronotherrelationshipsbeyondsimplepairwisesumsanddiﬀerences.56multiplesources.Forexample,uniﬁcationofmultiple,relatedabstractions(e.g.,derivedfromusesofaparticularmathematicalconstructindiversedomainsoftheoryandapplication)couldproducerepresentationswithrichersemanticsthancouldbederivedfromanysingleNLexpressionordocument—representationsthatcaptureusesandrelationshipsamongabstractions,ratherthanmerelytheirformalstructures.Alternatively,expressionsthataresimilaryetnotfullycompatiblemaydescribediﬀerentsamplesfromasingledistribution.Thedualofuniﬁcation,anti-uniﬁcation(a.k.a.generalization,SectionA1.1.2),providesthemostspe-ciﬁcrepresentationthatiscompatiblewithapairofarguments,encompassingtheirdiﬀerences(includingclashes)andretainingspeciﬁcinformationonlywhenitisprovidedbyboth.Anti-uniﬁcationofmultiplesamples1yieldsgeneralizationsthatcan(perhapsusefully)informpriorsforanunderlyingdistribution.Likeuniﬁcation,generalizationyieldsanexpressionatthesamesemanticlevelasitsinputs,buttorepresentanalogiescallsforanoutputthatcontainsrepresentationsofbothinputgraphsandtheirrelationships.Alignmentcanprovideabasisforrepresentinganalogiesasﬁrst-classsemanticobjectsthatcanbeintegratedintoanevolvingcorpusofQNRcontentLookingbackforamoment,prospectsthatweremotivatedbythemostbasicconsiderations—upgradingwordsandsyntaxtomachine-nativeembeddingsandgraphs—havenaturallyledtoprospectsthatgofarbeyondconsiderationsofexpressivenessperse.QNR/NL+frameworkslendthemselvestoapplica-tionsthatcansubsumeyetarequalitativelydiﬀerentfromthoseenabledbynaturallanguage.1.Thelatticeaxioms(SectionA1.2)ensurethatpairwisecombinationextendstomultipleargumentsinanaturalway.579Scaling,Reﬁning,andExtendingQNRCorporaScalableQNRsystemswithNL+-levelexpressivecapacitycouldbeusedtorepresent,reﬁne,andintegratebothlinguisticandnon-linguisticcontent,enablingsystemstocompileandapplyknowledgeatinternetscale.Precedingsectionsdiscussedvector/graphrepresentationsandtheirpotentialuseinconstructingQNRframeworksthatarepowerfulandtractableenoughtomeetthecriteriaforNL+.ThepresentsectionexploresthepotentialscalingofQNR/NL+systemstolargecorpora,thenconsidershowscalable,fullyfunctionalsystemscouldbeappliedtoincludeandintegratebothlinguistic,andnon-linguisticcontentwhilereﬁningandextendingthatcontentthroughjudgment,uniﬁcation,generalization,andreasoning.9.1OrganizingandExploitingContentatScaleAcoreapplicationoflanguage-orientedQNRsystemswillbetotranslateanddigestlargeNLcorporaintotractable,accessibleNL+representations,ataskthatmakesscalabilityaconcern.Currentpracticesuggeststhatthiswillbepractical.NL→NL+translationwilllikelyemploycomponentsthatresembletoday’stranslationsystems.PotentiallyanalogousNL→NLtranslationtasksareperformedatscaletoday,whileubiquitousinternet-scalesearchsuggeststhatlargecorporaofNL+knowledgecanbeexploitedforpracticaluse.9.1.1LearningbyReadingCanBeEﬃcientatScaleToestimatethemagnitudeofrequiredcomputationaltasks,wecanroughlyequatetheincrementalcostsofbuildingacorpusbyreadingNLtextsandtranslatingthemtoNL+representationswiththeincrementalcostsoftrainingalanguagemodelbyreadingNLtextsand(insomesense)“translating”theircontenttomodelparameters.Accordingtothisestimate,trainingGPT-3wasroughlyequivalenttotranslating~300billionwords1toNL+representations—about30timesthecontentofthe1.8millionpapersonthearXiv(arXiv2021)or10%ofthe40millionbooksscannedbyGoogle(H.Lee2019).Asanalternativeandperhapsmorerealisticestimate,wecanroughlyequatethecost1.Summingoverproductsoftrainingepochsandcorpussizes(Brownetal.2020).58ofNL→NL+translationtothecostofNL→NLtranslation;thethroughputofGoogleTranslate(>3trillionwordsperyear1)isthenanindicatorofaﬀordablethroughput(~40millionbooks/month).ThesecomparisonsconcurinsuggestingthefeasibilityoftranslatingNLinformationtoNL+corporaatscale.2TranslatingNLcontentintoaccessible,quasicognitiveNL+isaformoflearningthatisdistinctfromtraining.Unliketrainingmodelparameters,thetaskofreading,translating,andexpandingcorporaisbynaturefullyparallelizableandscalable.Theproductsarealsomoretransparent:Exploit-ingexternal,compositionalrepresentationsofinformationcanenablefacileinterpretation,correction,redaction,andupdateofasystem’scontent.39.1.2SemanticEmbeddingsEnableSemanticSearchNL+expressionscanbeindexedbysummaryembeddingsthatassociatesimilarexpressionswithnear-neighborpointsinasemanticspace.4Usingtheseembeddingsaskeysinnear-neighborretrievalcanprovidewhatisineﬀect“associativememory”thatsupportsnotonlysearch,buttasksinvolvingknowledgecomparisonandintegration(seeSection9.4below).Diﬀerentembeddingsofanexpressioncanserveaskeyssuitedfordiﬀerentsearchtasks.Insomeuse-contexts,wecareaboutthephysicalpropertiesofanobject;inothers,wecareaboutitscostandfunctionality.Insomeuse-contexts,wecareaboutacity’sgeography;inothers,aboutitsnightlife.Accordingly,1.Reportedin(Turovsky2016).2.Contributingtoeﬃciencyandscalability,computationsthatbothwriteandreadNL+corporacan,asnotedabove,avoidproblemsthatstemfromrepresentingknowledgesolelyintrainedmodelparameters.Largelanguagemodelslearnnotonlyknowledgeoflanguageperseandgeneralfeaturesoftheworld,butalsoarangeofspeciﬁcfactsaboutpeople,places,etc.Thestrikingimprovementsthathaveresultedfromscalinglanguagemodelshavestemmed,notonlyfromimprovementsinbroadlyapplicable,immediatelyavailablesyntactic,semantic,andreasoningskills(K.Luetal.2021),butfrommemorizationofspeciﬁc,seldom-usedfactsabouttheworld(YianZhangetal.2020).Forexample,GPT-2usesits1.5billionparameterstomemorizenames,telephonenumbers,andemailaddresses,aswellastheﬁrst500digitsofπ(Carlinietal.2021).Encodingfactualinformationofthissortinbillionsofmodelparameters—allusedincomputingeachoutputstep—isproblematic:Bothtrainingandinferenceareexpensive,andtheresultsareopaqueanddiﬃculttocorrectorupdate.Parametercountscontinuetogrow(Brownetal.2020;Fedus,Zoph,andShazeer2021).3.Forrelatedwork,seeVergaetal.(2020),Lewisetal.(2020),Guuetal.(2020),andFévryetal.(2020).4.WangandKoopman(2017),SchwenkandDouze(2017),andTranetal.(2020).Notethatretrievalbasedonsimilaritybetweensemanticembeddingscanbeeﬀectiveacrossmodalities(e.g.,Miechetal.2021).Pairwisesimilaritybetweenvectorkeyscouldpotentiallyreﬂecttheuniﬁcationscores(SectionA1.4.3)ofthecorrespondingpairsofexpressions.59whatmightberepresentedasasinglespanofcontentmaynaturallybeassociatedwithmultipledomain-orientedsummariesandkeys.WeﬁndthisgeneralpatterninTransformers,wheremulti-headattentionlayersprojecteachvaluetomultiplekeyandqueryvectors.Aquitegeneraldistinctionisbetweenrepresentingthecontentofanexpressionandrepresentingthekindsofquestionsthatitscontentcananswer—betweenwhatitsaysandwhatitisabout.Theroleofsemanticsearchoverlapswiththeroleoflinksinlarge-scaleNL+structures(forexample,generalizationsofcitationnetworks),butdiﬀersindesignatingregionsofsemanticspacethroughquery-embeddingsratherthandesignatingspeciﬁcexpressionsthroughgraphlinks.Conventionalreferencesandsyntacticrelationshipsarerepresentedbylinks,butlooserrelationshipscanberepresentedby“queryembeddings”withinexpressions,wheretheseembeddingsaretakentodenotesoftgraphlinkstoapotentialmultiplicityoftargetexpressionsinanindexedcorpus.19.1.3SemanticSearchExtendstoNon-LinguisticContentByproposedconstruction,anyitemthatcanbedescribedbyNLcanbe(better)describedbyNL+.Examplesincludecode,engineeringdesigns,legaldocuments,biomedicaldatasets,andthetargetsofcurrentrecommendersystems—images,video,apps,people,products,andsoon.Embeddingsthatrepresent(descriptionsof)non-linguisticcontent(Section9.2.2)arerelevanttoNL+inpartbecausetheycanbedirectlyreferencedbyNL+expressions,andinpartbecausetheseNL+descriptionscanprovideabasisforsemanticallyrichcontentembeddings.9.1.4NL+-MediatedEmbeddingsCouldPotentiallyImproveNLSearchSystemstrainedtomapNLtoNL+cansupporttheproductionofNLembed-dings(NL→NL+→embedding)thatarebasedondisentangled,contextuallyinformedsemanticrepresentations.Ifso,thenco-learned,NL+-mediatedkeyandqueryembeddingscouldpotentiallyimprovekNNsemanticsearchovercorrespondingNLitemsatinternetscale.1.Notethatthismechanismprovidesadiﬀerentiableandpotentiallyﬂuidgraphrepresenta-tion;seeSection7.2.3.609.1.5SemanticSearchCanBeEﬃcientatScaleExactk-nearestneighborsearchingenerichigh-dimensionalspacesiscostly,1butapproximatenearestneighborsearch(whichusuallyﬁndsthekeysnearesttoaquery)havebeenheavilyresearchedandoptimized;eﬃcient,practical,polylogarithmic-timealgorithmscansupport(forexample)billion-scalerec-ommendersystems,2andcandolikewiseforretrievalofsemanticcontentinNL+systemsatscale.9.2IncorporatingGeneral,Non-LinguisticContentHumanknowledgeincludesextensivenonlinguisticcontent,yetweusenatu-rallanguagetodescribe,discuss,index,andprovideinstructionsonhowtocreateandusethatcontent.Language-linkednonlinguisticcontentsprawlsbeyondtheboundariesofanNL-centricconceptofNL+—boundariesthatdonotconstrainlanguage-inspiredQNR/NL+applications.Examplesofnon-linguisticinformationcontentinclude:•Non-linguisticlexical-levelunits(e.g.,imageembeddings)•Expressionsinformallanguages(e.g.,mathematicalproofs)•Precisedescriptionsofobjects(e.g.,hardwaredesigns)•Formalgraph-structuredrepresentations(e.g.,probabilisticmodels)9.2.1Using“Words”BeyondtheScopeofNaturalLanguage“Nouns”representthings,butvectorembeddingscanrepresentthingsinwaysthatareinnosensetranslationsofword-likeentitiesinNL:Imageembeddings,forexample,canserveas“nouns”,3thoughthecontentofanimageembeddingneednotresemblethatofanNLnounphrase.Embeddingsthatrepresentdatasetsorobjectgeometrieshaveasimilarstatus.Similarconsiderationsapplytoverbsandrelationships:Wordsandphraseshavealimitedcapacitytodescribemotions,transformations,similarities,1.Scalingas~O(n),butlearned,structuredkeyspacescanimprovescalingtoO(n1/2)(Lampleetal.2019).2.J.Wangetal.(2018),Fuetal.(2018),Johnson,Douze,andJégou(2019),JayaramSubra-manyaetal.(2019),andSun(2020)3.Notethatrepresentationofimagesthroughrelationship-graphsamongobjectsblurstheboundarybetweenopaqueimageembeddingsandsyntacticstructures;SeeforexampleBearetal.(2020).Image-embeddingspacescanalsobealignedwithtext(e.g.,inPatashniketal.2021).61anddiﬀerences.1Aswithnouns,embeddings(andawiderangeofotherinformationobjects)canagainsubsumeaspectsofNLexpressivecapacityandextendrepresentationstorolesbeyondthescopeofpracticalNLdescriptions.ThisexpressivescopemayoftenbediﬃculttodescribeconcretelyinNL.2QNR“verbs”couldexpresstransformationsofkindsnotcompactlyexpress-ibleinconventionalNL:Forexample,displacementsofphysicalobjectscanberepresentedbymatricesthatquantitativelydescribedisplacementsandrotations,andamoregeneralrangeoftransformationscanbeexpressedbydisplacementvectorsinasuitablelatentspace.Stretchingtheconceptualframeworkfurther,verb-liketransformationscanbespeciﬁedbyexecutablefunctions.39.2.2ReferencingNon-LinguisticObjectsReferencestolinguisticandnon-linguisticobjectsarenotsharplydemarcated,butsomeinformationobjectsarebothcomplexandopaque,whilephysicalobjectsareentirelyoutsidetheinformationdomain.Allthese(indeed,essen-tiallyanything)cannonethelessbereferencedwithinthesyntacticframeworksofQNRexpressions.Asamotivatingcase,considerhowembeddedhyperlinksandmoregen-eralURIsexpandtheexpressivepowerofonlinedocuments:Theabilitytounambiguouslyreferencenotonlytext,butarbitraryinformationobjects,ispowerful,andontheinternet,thiscapabilitymeshessmoothlywithNL.Examplesofnon-linguisticinformationobjectsincludeimages,websites,datarepositories,andsoftware.Beyondthedomainofinformationobjects,domain-appropriatereferencemodalitiescanactaspropernounsindesig-natingphysicalobjects,people,places,andthelike.AnaturalpatternofusewouldplaceareferenceinaQNRwrapperthatmightinclude(forexample)asummaryembedding,conventionalmetadata,descriptions,anddocumen-tation,allofwhichcanexploittherepresentationalcapacityofNL+.QNRwrapperscanfacilitateindexing,curation,andtheselectionorrejectionofentitiesforparticularuses.1.Forexample,anNLphrasecancompactlysaythat“face_1stronglyresemblesface_2”,whilealexical-levelembeddingcancompactlysaythat:“face_1,withaspeciﬁcsetofembedding-spaceoﬀsetsinshape,color,andexpression,lookslikeface_2withsomequantiﬁedbutapproximateresidualdiﬀerences.”2.Hencethevalueofusinginterpretableyetnon-linguisticimageembeddingsasexamples.3.Inthisconnection,notethatQNRexpressionsaredata,thatneuralfunctionscanbeoftypeQNR→QNR,andthat“apply”,“eval”and“quote”functionscanplayquitegeneralroles(e.g.,inlanguageslikeSchemethatcanexpressandoperationalizehigh-levelabstractions).62Informationobjectsincludeexecutablecode,andwhenaccessedremotely,executablecodeiscontinuouswithgeneralsoftwareandhardwareservices.Interactivemodels1cancomplementNLdescriptionsofsystems.AccesstodocumentedinstancesofthecomputationalmodelsusedtoproduceparticularNL+itemscancontributetointerpretingtheitemsthemselves:Connectionsbetweenproductsandtheirsourcesoftenshouldbeexplicit.9.2.3EmbeddingGraph-StructuredContentHumanauthorsroutinelyaugmentsequentialNLtextwithgraphstructures.Eveninbasicallysequentialtext(e.g.,thisdocument),weﬁndstructuresthatexpressdeep,explicitnestingandinternalreferences.Documentsoftenincludediagramsthatlinktext-labeledelementswithnetworksofarrows,ortablesthatorganizetext-labeledelementsingrids.Thesediagramsandtablescorrespondtographs.Furtheraﬁeld,graph-structuredrepresentationscandescribecomponentassemblies,transportationsystems,andbiologicalnetworks(metabolic,reg-ulatory,genetic,etc.)Text-likedescriptionsofstatisticalandcausalrelation-shipsbecomeprobabilisticmodelsandcausalinﬂuencediagrams.InML,weﬁndformalknowledgegraphsinwhichbothelementsandrelationshipsarerepresentedbyvectorembeddings,2whileaugmentinglanguage-orientedTransformer-taskswithexplicitrepresentationsofrelationshipshasprovedfruitful.3DistinctionsbetweentheseandNL+representationsblurordisap-pearwhenweconsidergeneralizationsofconventionalsyntaxtographs,andofsymbolsandtexttoembeddingsandgeneraldataobjects.Somepotentialuse-patternscanbeconceptualizedascomprisingrestricted,graph-structuredrepresentations(e.g.,formalstructuresthatsupportcrisplydeﬁnedinferencealgorithms),intertwinedwithfullygeneralgraph-structuredrepresentations(informalstructuresthatsupportsoftinference,analogy,explicationofappli-cationscope,andsoon).9.2.4IntegratingFormalQuasilinguisticSystemsProgramminglanguagesandmathematicsareformalsystems,typicallybasedonquasilinguisticrepresentations.4Inthesesystems,theword-likeentities1.E.g.,themodelspresentedinDistillpages(DistillTeam2021).2.KazemiandPoole(2018)andHoganetal.(2021)3.CurreyandHeaﬁeld(2019),Schlagetal.(2020),andNguyenetal.(2020)4.InaQNRcontext,formalgraphrepresentations(suchasdiagramsincategorytheory)canberegardedaslanguagesthatgeneralizetext-basedsyntax.63(symbols)haveminimalsemanticcontent,yetsyntacticstructuresinconjunc-tionwithaninterpretivecontextspecifysemanticoroperationalmeaningsthatendowthesesystemswithdescriptivecapabilitiesbeyondtheconven-tionalscopeofNL.HowareformalsystemsconnectedtoNL,andbyextension,toNL+frame-works?NLcannotreplaceformalsystems,andexperiencesuggeststhatnoconventionalformalsystemcanreplaceNL.Whatweﬁndinthewildareformalstructurescombinedwithlinguisticdescriptions:mathematicsinter-leavedwithexplanatorytextinpapersandtextbooks,programsinterleavedwithcommentsanddocumentationinsourcecode,andsoon.ExperiencewithdecipheringsuchdocumentssuggeststhevalueofintimateconnectionsbetweenNL-likedescriptionsandembeddedformalexpressions.1Inonenaturalpatternofuse,formalsystems(e.g.,mathematics,code,andknowl-edgerepresentationlanguages)wouldcorrespondtodistinguished,formallyinterpretablesubsetsofnetworksofNL+expressions.Formallanguagescandescribeexecutableoperationstobeappliedintheinformalcontextofneuralcomputation.Conversely,vectorembeddingscanbeusedtoguidepremiseselectionintheformalcontextoftheoremproving.29.3TranslatingandExplainingAcrossLinguisticInterfacesProposedNL+contenthasbothsimilaritiestoNLandprofounddiﬀerences.ItisnaturaltoconsiderhowNLmightbetranslatedintoNL+representations,howNL+representationsmightbetranslatedorexplainedinNL,andhowanticipateddiﬀerencesamongNL+dialectsmightbebridged.9.3.1InterpretingNaturalLanguageInputsNL+frameworksareintendedtosupportsystemsthatlearnthroughinterac-tionwiththeworld,butﬁrstandforemost,areintendedtosupportlearningfromexistingNLcorporaandlanguage-mediatedinteractionswithhumans.TranslationfromNLsourcestoNL+iscentralbothtorepresentationlearningandtokeyapplications.ItisnaturaltoexpectthatencodersforNL→NL+translationwillsharearangeofarchitecturalcharacteristicsandtrainingmethodswithencodersforNL→NLtranslationandotherNLPtasks(Section10.2).TranslationtoNL+1.Szegedy(2020)suggestsanNL-translationapproachtoformalizingmathematicspapers.2.M.Wangetal.(2017)andMinervinietal.(2018)64couldbeappliedbothtosupportimmediatetasksand(moreimportant)toexpandcorporaofNL+-encodedknowledge.Transformer-basedNL→NLtranslationsystemscanlearnlanguage-agnosticrepresentations,1acapabilitywhichsuggeststhatNL→NL+translationwillbetractable.9.3.2TranslatingAmongNL+DialectsBecauseNL+corporacouldspanmanydomainsofknowledge—andbeen-codedbymultiple,independentlytrainedsystems—itwouldbesurprisingtoﬁnd(andperhapschallengingtodevelop)universallycompatibleNL+repre-sentations.InneuralML,diﬀerentmodelslearndiﬀerentembeddings,andtherepresentationslearnedbymodelswithdiﬀerenttrainingsets,trainingobjectives,andlatentspacesmaydivergewidely.InanNL+world,weshouldexpecttoﬁndarangeofNL+“dialects”aswellasdomain-speciﬁclanguages.Nonetheless,wheredomaincontentissharedandrepresentationalcapac-itiesareequivalent,isreasonabletoexpectfacileNL+→NL+translation.Further,regardinginteroperabilityinnon-linguistictasks,theconcretedetailsofdiﬀeringrepresentationscanbehiddenfromclientsbywhatis,ineﬀect,anabstractionbarrier.2Domain-speciﬁclanguagesmayresisttranslationatthelevelofrepresentations,yetcontributeseamlesslytoshared,cross-domainfunctionality.Losslesstranslationispossiblewhenthesemanticcapacityofonerepre-sentationfullysubsumesthecapacityofanother.GiventhecomputationaltractabilityofNL+representations,wecanexpecttranslationbetweensimilarNL+dialectstobemoreaccuratethantranslationbetweennaturallanguages.Intranslation,spacesoflexical-levelembeddingscanbemoretractablethandiscretevocabulariesinpartbecausevector-spacetransformationscanbesmoothandone-to-one.39.3.3TranslatingandExplainingNL+ContentinNLItisreasonabletoexpectthat,forarangeofNLPtasks,conventionalNL→(opaque-encoding)→NLpipelinescanbeoutperformedbyNL→NL+→1.Y.Liuetal.(2020),Tranetal.(2020),andBotha,Shan,andGillick(2020)2.Object-orientedprogrammingexploitsthisprinciple.3.Evenprojectionsofsetsofhigh-dimensionalvectorsintospacesofsubstantiallylowerdi-mensionalitycanpreservekeygeometricrelationshipsquitewell:TheJohnson–Lindenstrausslemmaimpliesgoodpreservationofbothdistancesandcosinesimilaritiesbetweenvectors.65NLpipelines;1thiswouldimplyeﬀectivetranslationoftheintermediateNL+representationstoNL.IfNL+frameworksfulﬁlltheirpotential,however,NL+corporawillcon-tainmorethantranslationsofNLexpressions.Contentwilltypicallydrawoninformationfrommultiplesources,reﬁnedthroughcompositionandinfer-ence,andenrichedwithnon-linguisticword-likeelements.Thereisnoreasontoexpectthattheresultingrepresentations—whichwillnotcorrespondtoparticularNLexpressionsoranyNLvocabularies—couldbewell-translatedintoNL.IfNL+ismoreexpressivethanNL,itfollowsthatnotallNL+contentcanbeexpressedinNL.How,then,mightNL+contentbeaccessedthroughNL,eitheringeneralorforspeciﬁchumanuses?•SomeNL+expressionswillcorrespondcloselytoNLexpressions;here,wecanexpecttoseesystemslikeconditionallanguagemodels(Keskaretal.2019)appliedtoproduceﬂuentNLtranslations.•NL+descriptionsthataredetailed,nuanced,complex,andeﬀectivelyuntranslatablecaninformNLdescriptionsthatprovidecontextuallyrelevantinformationsuitableforaparticularhumanapplication.•Similarly,ageneralabstractionexpressedinNL+mightbeeﬀectivelyuntranslatable,yetinformnarrower,moreconcreteNLdescriptionsofcontextuallyrelevantaspectsofthatabstraction.•TotheextentthatNLexpressionscould—ifsuﬃcientlyextensive—conveyfullygeneralinformation(astrongcondition),NLcouldbeusedtodescribeandexplainNL+contentinarbitrarydetail;thisapproachiscontinuouswithverbosetranslations.•NL+contentineﬀectivelynon-linguisticdomainscouldinsomein-stancesbeexpressedindiagrams,videos,interactivemodels,orotherhuman-interpretablemodalities.RelativetotheopaquerepresentationscommonincurrentneuralML,NL+representationshaveafundamentaladvantageininterpretability:BecauseQNRsarecompositional,theircomponentscanbeseparated,examined,andperhapsinterpretedpiecebypiece.Evenwhencomponentscannotbefullyinterpreted,theywilloftenrefertosomefamiliaraspectoftheworld,andknowingwhatanexpressionisaboutisitselfinformative.1.ParticularlywhenintermediateNL+processingcandrawonrelevantNL+corpora.Suc-cessfulaugmentationofTransformerswithexternalmemory(e.g.,forquestionanswering)providesevidenceforthepotentialpowerofthisapproach(Koncel-Kedziorskietal.2019;Fanetal.2021;Minetal.2020;Thorneetal.2020).669.4IntegratingandExtendingKnowledgeGraph-structuredrepresentationsinwhichsomevectorattributesdesignateregionsinsemanticspaces1lendthemselvestooperationsthatcanbeinter-pretedascontinuousrelaxationsofformaluniﬁcationandanti-uniﬁcation,whichinturncansupportreasoningandlogicalinferenceby(continuousrelaxationsof)familiaralgorithms.Theseoperationscanhelpextendcorporabycombiningexistingrepresentationsalonglinesdiscussedfromasomewhatdiﬀerentperspectiveintheprecedingsection.9.4.1CombiningKnowledgeThrough(Soft)UniﬁcationCompatiblerepresentationsneednotbeidentical:Alignmentand(successful)softuniﬁcation(AppendixA1)indicatecompatibility,andthesuccessfuluniﬁcationoftwoexpressionsdeﬁnesanewexpressionthatmaybothcombineandextendtheirgraphstructuresandsemanticallynarrowtheirattributes.Softuniﬁcationcouldpotentiallybeusedtoreﬁne,extend,compare,andlinkQNR/NL+representations.WhereQNRgraphspartiallyoverlap,successfuluniﬁcationyieldsaconsistent,extendeddescription.2Attemptstounifyincompatiblerepresentationsfailandcouldpotentiallyyieldavaluethatdescribestheirsemanticinconsistencies.3Inreﬁningcontentthroughsoftuniﬁcation,relativelyunspeciﬁcstructuresinoneQNR4maybereplacedorextendedbyrelativelyspeciﬁcstructures51.Asimpleexamplewouldberegionsimpliedbyimplicit,contextualuncertaintyinavectorvalue;richer,moreformalexamplesincludespacesinwhichvectorvalues(points)explicitlycorrespondtoregionsinlower-dimensionalspaces,orinwhichpointsaresemanticallyrelatedbytaxonomicorset-inclusionrelationships.Inalimitingcase,vectorvaluescorrespondeithertopoints(which,throughcomparisonbyequality,canmodelmathematicalsymbols)ortounknowns(which,throughco-reference,canmodelmathematicalvariables).2.Asanon-linguisticanalogy,consideroverlappingfragmentsofanimage:Whereoverlapsmatchwellenough,thefragmentscanbegluedtogethertoformamorecomprehensiveimageofascene,combininginformationfrombothfragmentsandpotentiallyrevealingnewrelationships.3.Section10.6.7suggestsgenericallyapplicabletrainingobjectivesthatwouldfavorrepre-sentationsandoperationsthat(approximately)satisfytheaxioms(SectionA1.2)foruniﬁcationandanti-uniﬁcation;inthisapproach,operationsmaybeperformedbycontextuallyinformedneuralfunctions.4.E.g.,embeddingsthatrepresentuncertainvalues;nodesthatlacklinkstooptionalcon-tent;leaf-levelnodesthatineﬀectsummarizethecontentofsomerangeofpotentialgraphextensions.5.E.g.,embeddingsthatrepresentnarrowervalues;linkstographstructurethatmaybemodiﬁedbyconditioningonacompatibleleaf-levelembedding.67fromanalignedQNR.Embeddingsthatcontaindiﬀerentinformationmaycommbinetoyieldasemanticallynarrowerembedding.Productsofsuccessfuluniﬁcation(new,moreinformativeexpressionsandrelationships)arecandidatesforadditiontoanNL+corpus.Recordsoffaileduniﬁcations—documentingspeciﬁcclashes—canprovideinformationimpor-tanttoepistemicjudgment.Thesesuccessfulandunsuccessfuluniﬁcation-productsmaycorrespondtonodesinasemanticallyhigher-levelgraphthatrepresentsrelationshipsamongexpressions.9.4.2GeneralizingKnowledgeThrough(Soft)Anti-UniﬁcationWhileuniﬁcationoftwoexpressionscanberegardedascombiningtheirinfor-mationcontent,anti-uniﬁcation(generalization)canberegardedascombiningtheiruncertainties,spanningtheirdiﬀerences,anddiscardingunsharedin-formation.Generalizationinthissensemayrepresentausefulpriorforgenerativeprocesseswithindistributionsthatincludetheinputs.9.4.3ConstructingAnalogiesOperationsbasedongeneralizationanduniﬁcationcouldbeappliedtoiden-tify,construct,andapplyanalogiesandabstractionsinQNRcorpora:•Arangeoflatentanalogieswillbereﬂectedinrecognizablyparallelstructuresbetweenoramongconcretedescriptions.1•Alignmentofparallelconcretedescriptionscanestablishconcreteanalo-gies,potentiallyreiﬁedasgraphs.•Generalizationoversetsofparalleldescriptionscanabstracttheircom-monstructuresasapatterns.•Uniﬁcationofaconcretedescriptionwithapatterncanindicateitsanal-ogytoasetofsimilarconcretedescriptionswithoutrequiringpairwisecomparison.Analogieshavepowerfulapplications.Forexample,ifadescriptionofAincludesfeaturesofakindabsentfromtheanalogousdescriptionofB,thenitisreasonabletoproposeA-likefeaturesinB.Analogiesamongmammals,forexample,underliethebiomedicalvalueofdiscoveryinanimalmodels.1.Thescopeofrecognizableparallelswilldependonlearnedrepresentationsandcom-parisonoperators.Regularization(Section8.4)canmakerepresentationsmorecomparable;usefulcomparisonoperatorscouldpotentiallyresemblerelaxeduniﬁcationandgeneralizationoperators.68Indeed,analogypermeatesscience,guidingbothhypothesesandresearchplanning.Analogyhasbeenidentiﬁedascentraltocognition,1andreiﬁednetworksofanalogiescanformgraphsinwhichrelationshipsamongabstractionsarethemselvesadomainofdiscourse.Withsuitableannotations,productsofgeneralizationandanalogy—newabstractexpressionsandrelationships—arecandidatesforadditiontoanNL+corpus.9.4.4ExtendingKnowledgeThrough(Soft)InferenceNaturallanguageinference(NLI)isamajorgoalinNLPresearch,andrecentworkdescribesasystem(basedonalargelanguagemodel)inwhichNLstate-mentsofrulesandfactsenableanswerstoNLquestions(Clark,Tafjord,andRichardson2020).InferencemechanismsthatexploitNL+→NL+operationscouldpotentiallybeusefulinNLIpipelines,andinreﬁningandextendingNL+corporaNL+→NL+inferencecouldplayacentralrole.RegularizingandnormalizingQNRrepresentations(Section8.4)canen-ableakindof“softformalization”basedoncontinuousrelaxationsofformalreasoning(modeled,forexample,onlogicprogramming,SectionA1.4.1).Rulescanberepresentedas“if-then”templates(inlogic,expressionswithunboundvariables)inwhichsuccessfuluniﬁcationofanexpressionwithan“if-condition”templatenarrowsthevaluesofattributesthat,throughcorefer-ence,theninformexpressionsconstructedfrom“then-result”templates.2Advancesinneuralmechanismsforconventionalsymbolictheorem-proving(e.g.,guidingpremiseselection)havebeensubstantial.3ItisreasonabletoexpectthatwrappingformalexpressionsinNL+descriptions—includingembeddingsandgeneralizationsofpotentialusecontexts—couldfacilitateheuristicsearchinautomatedtheoremproving.9.5Credibility,Consensus,andConsilienceHumansexamine,compare,contrast,correct,andextendinformationrepre-sentedinNLliteratures.MachinescandolikewisewithNL+content,andfor1.SeeHofstadter(2009)andGentnerandForbus(2011).2.Uniﬁcation-basedmachineryofthiskindcanimplementProlog-likecomputationwithapplicationstonaturallanguageinference(Weberetal.2019).3.RocktäschelandRiedel(2016,2017),Minervinietal.(2018),andMinervinietal.(2019)69similarpurposes,1aprocessthatcanexploituniﬁcation(andfailure),togetherwithgeneralizationandanalogy.9.5.1ModelingandExtendingScholarlyLiteraturesThestrategyofusingNLasabaselinesuggestsseekingmodelsforNL+corporainthescholarlyliterature,abodyofcontentthatincludesbothstructuresthatarebroadlyhierarchical(e.g.,summary/bodyandsection/subsectionrelationships)andstructuresthatcorrespondtomoregeneraldirectedgraphs(e.g.,citationnetworks).Inabstracts,reviewarticles,andtextbooks,scholarlyliteraturessummarizecontentatscalesthatrangefrompaperstoﬁelds.ProposedNL+constructscansupportsimilarpatternsofexpression,andcanextendcontentsummarizationtoﬁnergranularitieswithoutclutteringtypographyorhumanminds.Scholarlycitationscanlinktoinformationthatisparallel,supportive,problematic,explanatory,ormoredetailed;inNL+syntax,analogouscitationfunctionalitycanbeembodiedingraphsandlinkcontexts.Throughindexing,citationsinscholarlyliteraturescanbemadebidirec-tional,2enablingcitationgraphstobeexploredthroughbothcites-xandcited-by-xrelationships.Forsimilarreasons,NL+linksinfullyfunctionalsystemsshould(sometimes)bebi-directional.3Ingeneral,thestructureandsemanticsofcitationsandcitingcontextscanvarywidely(document-levelremotecitationsarecontinuouswithsentence-levelcoreference),andthestructureofNL+representationsmakesitnaturaltoextendcitesandcited-byrelationshipstoexpressionsﬁner-grainedthanNLpublications.Stepshavebeentakentowardapplyingdeeplearningtoimprovetheinte-grationofscholarlyliteratures.4ItwillbenaturaltobuildonthisworkusingNL+toolstoenrichNL+content.9.5.2UsingCredibility,Consensus,andConsiliencetoInformJudgmentsAswithNL,notallNL+contentwillbeequallytrustworthyandaccurate.Theoriginofinformation—itsprovenance—providesevidenceusefulinjudging1.Argumentationminingpointsinthisdirection(Moens2018;Galassietal.2020;Slonimetal.2021).2.Inanawkwardcoarse-grainedmanner.3.Note,however,thatcited-byrelationshipscanhavemassivefanout,apatternofusethatmaycallforbackward-facingstructuresricherthanlink-sets.4.Jaradehetal.(2019),M.Jiangetal.(2020),andCohanetal.(2020)70epistemicquality,aconsiderationthatbecomesobviouswhenconsideringinformationderivedfromheterogeneousNLsources.Judgmentsofepistemicqualitycanreﬂectnotonlyjudgmentsofindividualsources(theircredibility),butalsotheconsistencyofinformationfromdiﬀerentsources,consideringbothconsensusamongsourcesofsimilarkind,andconsilienceamongsourcesthatdiﬀerinkind.Searchbysemanticsimilarityandcomparisonthroughstructuralandsemanticalignmentprovideastartingpoint,butwhereepistemicqualityisinquestion,provenancewilloftenbekeytoresolvingdisagreements.Somebroaddistinctionsareimportant.9.5.2.1InformingjudgmentsthroughprovenanceandcredibilityProvenanceisanaspectofcontextthatcallsforsummarization.Inafullyfunctionalsystem,embeddings(andpotentiallyextendedQNRs1)cansumma-rizebothinformationsourcesandsubsequentprocessing,providingdescrip-tiveinformationthatcanbelinkedandaccessedtoderivedcontentfordeeperexamination.Provenanceinformationhelpsdistinguishbroadconcurrencefrommererepetitions—withouttrackingsources,repetitionsmaywronglybecountedasindicatinganinformativeconsensus.Byanalogywith(andleveraging)humanjudgment,systemscanwithsomereliabilityrecognizeproblematiccontentthatcanrangefromcommonmis-conceptionsthroughconspiracytheories,fakenews,andcomputationalpro-paganda.2Problematiccontentshouldbegivenlittleweightasasourceofinformationaboutitssubject,yetmayitselfbeanobjectofstudy.3Judg-mentsofqualitycanbeiterative:Thequalityandcoherenceofcontentcanbejudgedinpartbythequalityandcoherenceofitssources,andsoon,aprocessthatmayconvergeondescriptionsofmore-or-lesscoherentbutincompatiblemodelsoftheworldtogetherwithaccountsoftheirclashes.JudgingsourcequalityingenerallysoundNLliteraturesisafamiliarhu-mantask.Intheexperimentalsciences,forexample,weﬁndaspectrumofepistemicstatusthatrunsalongtheselines:•Uncontroversialtextbookcontent•Reviewsofwell-corroboratedresults1.Summaries(likeotherexpressions)canbeembodiedinQNRsthatprovideincreasingdetailwithincreasingsyntacticdepth.2.SeeMartinoetal.(2020).3.Inthiscontext,thediﬀerencebetweentoxictextanddiscussionsthatembedexamplesoftoxictextillustratestheimportanceofrecognizinguse-mentiondistinctions.Socialmediaﬁlterstodaymaysuppressbothadvocatesandcriticsofoﬀensiveviews.71•Reportsofrecenttheory-congruentresults•Reportsofrecentsurprisingresults•Reportsofrecenttheory-incongruentresults•Reportsofactivelydisputedresults•ReportsofsubsequentlyretractedresultsAlloftheseconsiderationsaremodulatedbythereputationsofpublishersandauthors.Broadlysimilarindicatorsofqualitycanbefoundinhistory,economics,currentaﬀairs,andmilitaryintelligence.Thereliabilityofsourcesistypicallydomain-dependent:1Nobellaureatesmayspeakwithsomeauthorityintheirﬁelds,yetbedisruptivesourcesofmisinformationbeyondit;aconspiracytheoristmaybeareliablesourceofinformationregardingsoftwareorrestau-rants.Althoughinformationaboutcredibilitycanbepropagatedthroughagraph,credibilityisnotwell-representedasascalar.9.5.2.2InformingjudgmentsthroughconsensusInjudginginformation,weoftenseekmultiplesourcesandlookforareasofagreementorconﬂict—inotherwords,degreesofconsensus.Relevantaspectsofprovenanceincludethequalityofindividualsources,butalsotheirdiversityandevidenceoftheirindependence.2Whatamounttocopyingerrorsmaybeindicatedbysporadic,conﬂictingdetails.Lackofindependencecanoftenberecognizedbyclosesimilarityinhowideasareexpressed.3Injudginginformationfrom(whatarecrediblyconsideredtobe)directobservations,experiments,andexperience,thequalityofhumansourcesmayplayonlyalimitedrole.Establishedmethodsofdataaggregationandstatisti-calanalysiswillsometimesbeappropriate,andwhileNL+representationsmaybeusefulincuratingthatdata,subsequentmethodsofinferencemayhavelittlerelationshiptoNL+aﬀordances.Inferenceprocessesthemselves,however,constituteakindofalgorithmicprovenancerelevanttodownstreamrepresentationandassessmentofresults.9.5.2.3InformingjudgmentsthroughconsilienceMorepowerfulthanconsensusamongsourcesofbroadlysimilarkindsiscon-1.Domain-basedassessmentsofcredibilityhavebeenusedinconstructingknowledgegraphsfromsocial-mediasources:Abu-Salihetal.(2021).2.SomeofthisevidenceisfoundinsurfacefeaturesofNLtexts(e.g.,usesofspeciﬁcwordsandphrases);otherevidenceisfoundinfeaturesofsemanticcontent.3.Here,linkstoNLsourcetextcanbevaluable:Literalwordingmayconveysignalsofshallowrepetition.72silience,agreementofevidencefromsourcesofqualitativelydiﬀerentkinds—forexample,agreementbetweenhistoricalrecordsandradiocarbondating,orbetweenanexperimentalresultandatheoreticalcalculation.Judgmentofwhatconstitutes“adiﬀerenceinkind”isahigh-levelsemanticoperation,butpotentiallyaccessibletosystemsthatcanrecognizesimilaritiesanddiﬀerencesamongﬁeldsthroughtheircitationstructures,focalconcerns,methodologies,andsoon.Distinguishingconsiliencefrommereconsensusisajudgmentinformedinpartbyprovenance,andiskeytorobustworldmodeling.Itcallsformodelingtheepistemicstructuresofdiverseareasofknowledge.10ArchitecturesandTrainingExtensionsofcurrentneuralMLmethodscanleveragearchitecturalinductivebiasandmultitasklearningtosupportthetrainingofquasilin-guisticneuralsystemswithNL+-levelexpressivecapacity.TheprecedingsectionssuggestthatQNRframeworkscanimplementpower-ful,tractableNL+functionality,providedthatsuitablerepresentationscanbelearned;thepresentsectionoutlinespotentiallyeﬀectiveapproachestolearningbasedonadaptationsoffamiliararchitecturesandtrainingmeth-ods.Vector-labeledgraphbottleneckscanprovideastronginductivebias,whilemultitasklearningandauxiliarylossfunctionscanshapeabstractrep-resentationsthatareanchoredin,yetsubstantiallydecoupledfrom,naturallanguages.Theﬁnalsectionoutlinespotentialarchitecturesforcomponentsthatcontrolinferencestrategies.10.1GeneralMechanismsandApproachesFollowingneuralMLpractice,thedevelopmentofQNR-centeredsystemscalls,notforhand-craftingfeatures,butforarchitecturesthatprovidesuit-ablecomponents,capacity,andinductivebias,inconjunctionwithtrainingtasksthatprovidesuitabledatasets,objectives,andlossfunctions.Generalmechanismsandapproachesinclude:•EmployingNL→QNRinterfacestoensureQNRrepresentations•EmployingQNRintermediaterepresentationsinNL→NLtrainingtasks•DecouplingQNRrepresentationsfromNLencodings•EmployingsemanticallyrichtrainingtaskswithQNRobjectives73•StructuringQNRsemanticsthroughauxiliary,lattice-orientedtraining•ApplyingQNR-domaininferencetoexploitQNRrepositoriesThesemechanismsandapproachesshouldbeseenasfacetsofmultitasklearninginwhichakeygoalistodevelopNL→QNR→NLsystems1thatsupportbroadapplications.PretrainedNL→embedding→NLmodels(e.g.,BERTandfriends)areperhapstoday’sclosestanalogues.10.2BasicInformationFlowsDevelopingQNR-centeredsystemscallsforencodersanddecodersthatcanlinkinputandoutputchannelstoQNR-basedrepresentationandinferencemechanisms(Figure10.1).inputsQNRencoderQNRdecoderoutputsFigure10.1:Informationﬂowsinminimalistic,QNR-bottlenecksys-tems.Inputsandoutputsmaybemultimodal.•Potentialinputstoencodersincludetext,butalsoimages,symbolicexpressions,multimodaldatastreams,2andsoforth.•Potentialoutputsfromdecodersincludetranslations,summaries,an-swerstoquestions,retrievedcontent,classiﬁcations,andvariousprod-uctsofdownstreamprocessing(images,engineeringdesigns,agentbehaviors,andsoon).•PotentialQNRoperationsrangefromsimplepass-through(implement-ingaQNRbottleneck3withoutQNR-domaininference)toinferencemechanismsthatcouldemployQNR-baseduniﬁcation,generalization,andreasoningmethods4whiledrawingonstoredQNRcontent(Fig-ure10.2).1.Togetherwithgeneralizationstomultimodalinputsandoutputs.2.SeeforexampleJ.Luetal.(2019)andDesaiandJohnson(2021).3.ItisatleastquestionablewhethertheinductivebiasprovidedbyasimpleQNR-bottleneckarchitecturewouldoutperformanotherwisesimilarbutunconstrainedencoder/decoderarchi-tecturesinstand-aloneNL→NLtasks.TheuseofaQNR-likebottleneckinBearetal.(2020)hasledtostrongperformance,butsmall-scaleQNRrepresentationscanbeinterchangeablewithvectorembeddingsthatlackexplicitgraphstructure(Section7.3.1).4.Notethatopen-endedreasoninglikelycallsforconditionalcomputation;potentiallyrelevantarchitecturalcomponentsandtrainingmethodsarediscussedinCasesetal.(2019),Rosenbaumetal.(2019),andBanino,Balaguer,andBlundell(2021).74PotentialarchitecturalbuildingblocksrangefromMLPsandconvolutionalnetworkstoTransformersandthemanyvarietiesofgraphneuralnetworks.1Architecturescanemploybuildingblocksofmultiplekindsthatcollectivelyenablediﬀerentiableend-to-endtraining(Section7.2.3discussesdiﬀerentiablerepresentationsofgraphtopology).ThecurrentstateoftheartsuggestsTransformer-basedbuildingblocksasanaturalchoiceforencodingNLinputsandgeneratingNLoutputs.Transformer-basedmodelshaveperformedwellinknowledge-graph→texttasks,(Ribeiroetal.2020),andcaninsomeinstancesbeneﬁtfromtrainingwithexplicitsyntax-graphrepresentations.2Encoderslikethosedevelopedforscene-graphrepresentationlearning3arenaturalcandidatesforQNR-mediatedvisiontasks.InbothNLandvisiondomains,encoderscanproducevector/graphrepresentationsthat,throughtrainingandarchitecturalbias,serveasQNRs.GNNsorgraph-orientedTransformersarenaturalchoicesbothforimplementingcomplexoperationsandforinterfacingtotask-orienteddecoders.Simplefeed-forwardnetworksarenaturalchoicesfortransformingandcombiningthevectorcomponentsofvector-labeledgraphs.SystemsthatreadandwriteexpressionsinQNRcorporacouldemployscalablenear-neighborlookupinrepositoriesindexedbyQNR-derivedsemanticembeddings(Section9.1.2).data,queriesQNRencoderQNRdecoderoutputsQNRinferenceQNR repositoryFigure10.2:InformationﬂowsingenericQNRsystemsaugmentedbyaccesstoarepositoryofQNRcontent.Inthegeneralcase,“QNRinference”includesread/writeaccesstorepositories,producingmodelsthatareinpartpre-trained,butalsopre-informed.1.ReviewedinJ.Zhouetal.(2020)andWuetal.(2021).Transformersineﬀectoperateonfullyconnectedgraphs,butonsparsegraphs,GNNscanprovidegreaterscalabilityandtask-orientedinductivebiases(Addankietal.2021),aswellasmoredirectcompatibilitywithQNRs.2.CurreyandHeaﬁeld(2019)andAkoury,Krishna,andIyyer(2019)3.Includingbothimageandjointimage-languagemodels;seeZellersetal.(2018),J.Yangetal.(2018),Leeetal.(2019),andBearetal.(2020).75TheanalysispresentedinprevioussectionssuggeststhatQNRscaninprinciplemeetthecriteriaforrepresentingNL+-levelsemantics,whilethecapabilitiesofcurrentneuralsystemssuggestthatarchitecturesbasedoncompositionsoffamiliarbuildingblockscanimplementtheoperationsre-quiredforNL+-mediatedfunctionality.Thenextquestionishowtotrainsuchsystems—howtocombinetasksandinductivebiastoproduceencoders,decoders,andprocessingmechanismsthatprovidetheintendedfunctionality.10.3ShapingQNRSemanticsBasicaspectsofintendedQNRsemanticsincludenon-trivialsyntaxinconjunc-tionwithNL-likerepresentationalcapacityandextensionstoothermodalities.Thesegoalscanbepursuedthrougharchitecturalinductivebiastogetherwithtrainingtasksinfamiliardomains.10.3.1ArchitecturalBiasTowardQuasilinguisticRepresentationsArchitecturalinductivebiascanpromotetheuseofsyntacticallynontrivialQNRs(ratherthanﬂatsequencesofembeddings)torepresentbroadlyNL-likecontent.IflearnedrepresentationsfollowthegeneralpatternanticipatedinSection8,QNRsyntaxwouldtypicallyemploy(atleast)DAGsofsubstantialdepth(Section8.1);leafattributeswouldtypicallyencode(atleast)lexical-levelsemanticinformation(Section8.2),whileattributesassociatedwithinternalnodeswouldtypicallyencoderelationships,summaries,ormodiﬁersapplicabletosubsidiaryexpressions(Section8.3).EncodersanddecoderscouldbiasQNRstowardtopologiesappropriateforexpressingquasilinguisticsemantics.ArchitecturescanpassinformationthroughQNR-processingmechanismswithfurtherinductivebiases—e.g.,architectedandtrainedtosupportsoftuniﬁcation—tofurtherpromotetheexpressionofcomputationallytractable,disentangled,quasilinguisticcontent.10.3.2AnchoringQNRsSemanticContentinNLAlthoughQNRrepresentationshavebroaderapplications,itisnaturaltofocusontaskscloselytiedtolanguage.TransformerstrainedonfamiliarNL→NLobjectives(e.g.,languagemodelingandsentenceautoencoding)haveproducedﬂatvectorrepresentations(vectorsandvector-sequences)thatsupportanextraordinaryrangeoftasks.1TrainingQNR-bottleneckarchitectures(NL→1.X.Liuetal.(2019),Brownetal.(2020),andY.Liuetal.(2020)76QNR→NL)onthesameNL→NLobjectivesshouldproducecomparable(andpotentiallysuperior)QNRrepresentationsandtaskperformance.Potentialtasksinclude:•AutoencodingNLtext•NLlanguagemodeling•Multilingualtranslation•Multi-sentencereadingcomprehension•Multi-scaleclozeandmaskedlanguagetasks1ItseemslikelythatdevelopingNL+-levelrepresentationsandmechanismswouldbestbeserved,notbyapretraining/ﬁne-tuningapproach,butbyconcurrentmultitasklearning.Inthisapproach,optimizationforindividualtasksisnotanendinitself,butameanstoenrichgradientsignalsandlearnedrepresentations.210.3.3ExtendingQNRRepresentationsBeyondLinguisticDomainsAfurtherclassoftasks,X→QNR→NL,wouldmapnon-linguisticinputsXtoNL,againmediatedby(andtraining)QNR-basedmechanisms.Potentialexamplesinclude:•Predictingdescriptionsofimages•Predictingdescriptionsofhumanactions•PredictingcommentsincodeAquitegeneralclassoftaskswouldencodeinformationfromadomain,de-codetoapotentiallydiﬀerentdomain,andtrainQNR→QNRcomponentstoperformintermediatereasoningsteps.Potentialexamplesincludethecontrolofagentbehaviorinvolvinginstruction,communication,andplanning.310.4AbstractingQNRRepresentationsfromNLIfNL+istobemorethanarepresentationofNL,thetrainingofQNRmodelsmayrequireaninductivebiastowardrepresentationsthataredeliberatelydecoupledfromNL.Lexical-levelvectorembeddingsalreadyprovideauseful1.Re.multi-scalemasking,seeJoshietal.(2020).2.SeeMcCannetal.(2018),X.Liuetal.(2019),AlexRatneretal.(2018),andAlexanderRatneretal.(2020).3.Analogouslanguage-infusedmechanismsaredescribedinShahetal.(2018),Luketinaetal.(2019),andLazaridouandBaroni(2020).77biasinthattheydecouplerepresentationsfromthepeculiaritiesofNLvocab-ularies.Massivelymultilingualtasks(translation,etc.)canfurtherencouragetheemergenceofrepresentationsthatabstractfromthefeaturesofparticularlanguages.1Incurrentpractice,combinationsofmultitasklearningandar-chitecturalbiashavebeenemployedtoseparatehigher-levelandlower-levelsemanticrepresentations.2Itmaybeuseful,however,toseekadditionalmech-anismsforlearningrepresentationsthatareabstractedfromNL.3Supportingthisidea,recentworkhasfoundthatdisentanglingsemanticsfromNLsyntaxispracticalandcanprovideadvantagesinperformingarangeofdownstreamtasks(Huang,Huang,andChang2021).10.4.1AbstractingQNRRepresentationsFromWordSequencesTasksandarchitecturescanbestructuredtofavorseparationofabstractfromword-levelrepresentations.4Ageneralapproachwouldbetosplitandrecom-bineinformationpathsinNL→NLtasks:AnabstractQNRpathcouldbetrainedtorepresentpredominantlyhigh-levelsemanticsandreasoning,whileanauxiliarypathcarrieslexical-levelinformation.Torecombinethesepaths,thehigh-levelsemanticpathcouldfeedadecoderthatisalsoprovidedwithasetofwordsfromthetargetexpressionpermutedtogetherwithdecoys.5ByreducingthetaskofproducingcorrectNLoutputstooneofselectingandarrangingelementsfromagivensetofwords,thismechanismcouldshiftalexical-level,NL-speciﬁcburden—andperhapstheassociatedlow-levelsemanticcontent—awayfromtheabstract,high-levelpath.Tostrengthenseparation,thegradient-reversaltrickfordomainadaptation6couldbeap-pliedtoactively“anti-train”theavailabilityofword-speciﬁcinformationinabstract-pathrepresentations.1.See,forexample,Arivazhaganetal.(2019),Y.Liuetal.(2020),andTranetal.(2020).2.Sanh,Wolf,andRuder(2018)andTamkin,Jurafsky,andGoodman(2020)3.Fine-tuningtoreintroduceNL-speciﬁcinformationwouldlikelybeusefulforsomeNL→NLapplications.4.Forexample,Wieting,Neubig,andBerg-Kirkpatrick(2020)separatessemanticinforma-tionfromlanguage-speciﬁcinformationinadual-languagesentence-embeddingtask.SeealsoBousmalisetal.(2016).5.Alternatively,adeep,high-levelpathcouldbetrainedtoreﬁneadistributionoverwordsprovidedbyashallow,pretrained,high-perplexitylanguagemodel.6.GaninandLempitsky(2015)andCaietal.(2019)7810.4.2StrategiesforLearningHigher-LevelAbstractionsObjectivefunctionsinNLPoftenscoreoutputsbytheircorrespondencetospeciﬁcsequencesoftargetwords.Thisobjectiveisembeddedinthedeﬁni-tionsoflanguagemodeling,maskedlanguagemodeling,andtypicalclozetasks,whilesimilarobjectivesarestandardinNLtranslation.However,asthesizeoftargetoutputsincreases—fromsingle-wordclozetaskstoﬁllinggapsonthescaleofsentences,paragraphs,andbeyond—predictingspeciﬁcwordsequencesbecomesincreasinglydiﬃcultoreﬀectivelyimpossible.Whentheactualresearchobjectiveistomanipulaterepresentationsofmeaning,lexical-levelNLtrainingobjectivesfailthetestofscalability.L(diﬀerence)complete QNR
(actual)QNR
encodingcomplete QNR
(predicted)masked NL
expressioncomplete NL
expressionQNR repositoryQNR
inferenceFigure10.3:Asemantic-completiontaskinwhichlossisbasedoncorrespondencebetweenQNRrepresentationsratherthandecodedtext.QNR-completionobjectivescanprovidesemantic,NL-basedcom-pletiontasks,e.g.,describing(notreplicating)themissingcomponentsofanexplanation,argument,story,proof,program,orneuralarchi-tecture.Toavoidcollapsingrepresentations,QNRencoderscouldbefrozenorconcurrentlyshapedbyadditionaltrainingtasks(e.g.,QNR-mediatedNLautoencoding,translation,etc.;seeSection10.3.2)Completiontasks1formulatedintheQNRdomainitselfwouldbetterservethispurpose.UsefulQNR-domaincompletiontasksrequireQNRtargetsthatrepresentrichtask-domainsemantics,butwehavealreadyseenhowNLPtaskscanbeusedforthispurpose(Section10.3.2).ProductsofsuchtrainingcanincludeNL→QNRencodersthatraisebothinferenceprocessesandtheirtargetstotheQNRdomain(Figure10.3).1.Inageneralsense,completiontaskscanincludenotonlysequencepredictionandclozetasks,butalsoquestionansweringandothercapabilitiesshownbylanguagemodelsinresponsetoprompts(seeBrownetal.(2020));predictioninabstracted(latentspace)domainscanalsosupportarangeoftasks.SeeOord,Li,andVinyals(2019).79WithtargetsraisedfromNLtoQNRrepresentations,itshouldbecomeprac-ticaltocompareoutputstotargetsevenwhenthetargetsrepresentcomplexsemanticobjectswithanenormousrangeofdistinctyetnearlyequivalentNLrepresentations.Whileitseemsdiﬃculttoconstructusefulsemantic-distancemetricsoverwordsequences,semantic-distancemetricsintheQNRdomaincanberelativelysmooth.1Ambitiousexamplesofcompletiontaskscouldincludecompletionof(descriptionsof)codewithmissingfunctions,orofmathematicaltextswithmissingequationsorproofs.10.5TrainingQNR×QNR→QNRFunctionstoRespectLatticeStructureThesemantic-latticepropertiesdiscussedinAppendixA1correspondtoanalgebraofinformation,butQNRsneednotautomaticallyrespectthisalgebra.Inparticular,absentsuitabletrainingobjectives,operationsonQNRsmaystronglyviolatethelatticeaxiomsthatconstrainuniﬁcationandgeneraliza-tion.2Learningrepresentationsandfunctionsthatapproximatelysatisfythelattice-deﬁningidentities(SectionA1.2)canpotentiallyactbothasaregu-larizerandasamechanismfortrainingoperationsthatsupportprincipledcomparison,combination,andreasoningoverQNRcontent.Becausetheexistenceof(approximate)latticeoperationsoverQNRrepre-sentationsimpliestheir(approximate)correspondenceto(whatcanbeinter-pretedas)aninformationalgebra,wecanexpectthat(approximately)enforc-ingthisconstraintcanimprovethesemanticpropertiesofarepresentationalsystem.Inaddition,predictionofsoft-uniﬁcationscores(SectionA1.4.3)canprovideanauxiliarytrainingobjectiveforcontentsummaries(Section8.3.4),providingadistancemeasurewithpotentialapplicationstostructuringlatentspacesforsimilarity-basedsemanticretrieval(Section9.1.2).10.6ProcessingandInferenceonQNRContentTheabovediscussionoutlinedcoarse-grainedinformationﬂowsandgeneraltrainingconsiderationsusingblockdiagramstorepresentunitsofhigh-levelfunctionality.Thepresentsectionexaminesthepotentialcontentsofboxes1.Aidedbystructuralregularity(Section8.4.)Asimilarapproachmightprovefruitfulintrainingmodelsthatproduceﬂatvectorrepresentations,whichnaturallyhavesmoothdistancemetrics.Thisbasicapproach(predictinglearnedrepresentationsratherthanrawinputs)isappliedinLarsenetal.(2016)andrelatedwork.2.E.g.,theymaymapapproximatelylattice-respectingtostronglylattice-incompatiblesetsofrepresentations.80task outputstask outputstask outputstask inputstask inputstask inputsQNRinferencesystemsQNR repository multi-modalinformationrepositoriesreader-encodersFigure10.4:OutlineofmultitaskarchitecturesthatincludeaccesstoexternalandQNR-basedinformationrepositories(e.g.,theinternet).Morearrowscouldbeadded.labeled“QNRinference”.Theaimhereisnottospecifyadesign,buttodescribefeaturesofplausiblearchitecturesforwhichtheimplementationchallengeswouldbeoffamiliarkinds.10.6.1Control,Selection,andRoutingTasksofdiﬀeringcomplexitywillcallfordiﬀerentQNRinferencemechanisms.Thenullcaseistheidentityfunction,single-pathpass-throughinaQNR-bottleneckarchitecture.Amoreinterestingcasewouldbeasingle-pathsystemthatperformsQNR→QNRtransformations(e.g.,usingaGNN)basedonaconditioninginput.MorepowerfulinferencemechanismscouldperformQNR×QNR→QNRoperations,potentiallybymeansofarchitecturesthatcanlearn(formsof)softuniﬁcationoranti-uniﬁcation.Towardthehighendofaspectrumofcomplexity(farfromentrylevel!),open-endedQNR-basedinferencewillrequiretheabilitytolearntask-anddata-dependentstrategiesforstoring,retrieving,andoperatingonQNRsinworkingmemoryandexternalrepositories.Thiscomplex,high-endfunc-tionalitycouldbeprovidedbyacontrollerthatroutesQNRvaluestooperatorswhileupdatingandaccessingQNRvaluesbymeansofkeyandquerybased81QNR
decoderkey-value working memoryQNR
encoder external QNR repository — QNR inference —unary
operationsunary
operationsunary
operationsbinary
operationsbinary
operationsbinary
operationsqretrievecontrol, selection, and routingstorekvvFigure10.5:Blockdiagramdecomposingaspectsofarchitecturesforcomplex,open-endedQNRinferencefunctionality.Bothworkingmemoryandanexternalrepositorystorekey-valuepairs,andgivenaquery,willreturnoneormorevaluesassociatedwithnear-neighborkeysinasemanticembeddingspace.Arrowslabeledqandk(shownexplicitlyinconnectionwithexternaloperations)representqueryandkeyembeddingsusedinstoringandretrievingQNRvalues(v).AnelaborationofFigure10.4wouldshowsimilarQNRinferencefunc-tionalityinconnection,notonlywith“QNRinferencesystems”,butalsowith“reader-encoders”(whichneednotbedistinctcomponents).82storageandretrieval.1Keysandqueries,inturn,canbeproductsofabstrac-tiveoperationsonQNRs.(Indiscussionsofretrieval,argumentpassing,etc.,“aQNR”isoperationallyareferencetoanodeinagraphthatmaybeofindeﬁniteextent.)Notethat“reasoningbasedonQNRs”canemployreasoningaboutQNRprocessingbymeansofdiﬀerentiablemechanismsthatoperateonﬂatvectorrepresentationsinacurrenttaskcontext.2Reinforcementlearningincon-junctionwithmemoryretrievalhasbeeneﬀectiveinmulti-stepreasoning(Baninoetal.2020),ashavemodelsthatperformmulti-stepreasoningoverdiﬀerentiablerepresentationsandretrieveexternalinformationtoanswerqueries(Bauer,Wang,andBansal2018).10.6.2EncodersQNRencodersaccepttaskinputs(wordsequences,images,etc.)andproducesparse-graphoutputs.NaturalimplementationchoicesincludeTransformer-likeattentionarchitecturesthatinitiallyprocessinformationonafullycon-nectedgraph(thedefaultbehaviorofattentionlayers)butapplyprogressivelysharpenedgatingfunctionsindeeperlayers.Gatingcandiﬀerentiablyweightandasymptoticallyprunearcstosparsengraphsthatcanthenbereadoutasdiscretestructures.Arangeofothermethodscouldbeappliedtothistask.Optionaldiscretizationatasparse-graphreadoutinterfacebreaksdiﬀeren-tiabilityandcannotbedirectlyoptimizedbygradientdescent.Thisdiﬃcultyhasbeenaddressedbymeansthatincludetraining-timegraphsamplingwithtoolsfromreinforcementlearning(Kazietal.2020)andothermechanismsthatlearntodiscretizeorsparsenthroughsupervisionfromperformanceondownstreamtasks(Malinowskietal.2018;Zhengetal.2020).Systemswithpotentiallyrelevantmechanismslearndynamicpatternsofconnectiv-ityonsparsegraphs(Veličkovićetal.2020)andaddressproblemsforwhichthesolutionspaceconsistsofdiscretegraphs(Cappartetal.2021).Because1.Inconsideringhowthisfunctionalitymightbestructured,analogiestocomputerarchitec-tures(bothneuralandconventional)maybeilluminating.Forexample,analogieswithstored-program(i.e.,virtuallyall)computerssuggestthatmemorystorescanusefullycontainQNRsthatdescribeexecutableinferenceprocedures.SeerelatedworkinGulcehreetal.(2018),Le,Tran,andVenkatesh(2020),andMalekmohamadi,Saﬁ-Esfahani,andKarimian-kelishadrokhi(2020).2.Apsychologicalparallelistheuseofgeneral,fundamental“thinkingskills”inreasoningaboutdeclarativememorycontent.Skillsinthissensecanbeimplicitinaprocessingmecha-nism(anactivenetworkratherthanarepository)andareappliedmoredirectlythanexplicitplans.83arcsinQNRscandeﬁnepathsforinformationﬂowincomputation(e.g.,bygraphneuralnetworks),methodsfortrainingcomputational-graphgatingfunctionsindynamicneuralnetworks1arepotentiallyapplicabletolearningQNRconstruction.10.6.3DecodersStandarddiﬀerentiableneuralarchitecturescanbeappliedtomapQNRstotypicaltask-domainoutputs.AnaturalarchitecturalpatternwouldemployGNNstoprocesssparsegraphsasinputstodownstreamTransformer-likeattentionmodels.Wheretheintendedoutputisﬂuentnaturallanguage,currentpracticesuggestsdownstreamprocessingbylargepretrainedlanguagemodelsadaptedtoconditionaltextgeneration;2potentiallyrelevantexamplesincludemodelsthatconditionoutputsonsentence-levelsemanticgraphs.310.6.4WorkingandExternalMemoryStoresWorkingmemoryandexternalrepositorieshavesimilarcharacteristicswithrespecttostorageandretrieval,butdiﬀerencesinscaleforcediﬀerencesinimplementation.Inparticular,wherestoresarelarge,computationalconsider-ationscallforstoragethatisimplementedasaneﬃcient,scalable,potentiallyshareddatabasethatisdistant(inamemory-hierarchysense)fromtask-focusedcomputations.4Intheapproachsuggestedhere,bothformsofstoragewould,however,retrievevaluesbasedonsimilaritybetweenkeyandqueryembeddings.10.6.5UnaryOperationsUnaryoperationsapplytosinglegraphs.Popularnode-convolutionalGNNsusediﬀerentiablemessage-passingschemestoupdatetheattributesofagraph,andcancombinelocalsemanticinformationtoproducecontext-informedrepresentations.Diﬀerentnetworkscouldbeappliedtonodesofdiﬀerentsemantictypes.ThevaluesreturnedbyunaryoperationsmaybeQNRsorembeddings(e.g.,keys,queries,orabstractivesummaries).1.ReviewedinHanetal.(2021)2.E.g.,Keskaretal.(2019).3.E.g.,Mageretal.(2020).4.Fuetal.(2018),J.Wangetal.(2018),Johnson,Douze,andJégou(2019),andJayaramSubramanyaetal.(2019)84Unaryoperationsmayalsotransformgraphsintographsofadiﬀerenttopologybypruningarcs(alocaloperation),orbyaddingarcs(whichingeneralmayrequireidentifyingandlinkingpotentiallyremotenodes).1Ex-amplesofneuralsystemswiththelatterfunctionalitywerenotedabove.2Alocaltopology-modifyingoperationcould(conditionally)passpotentialarcs(copiesoflocalreferences)ascomponentsofmessages.310.6.6GraphAlignmentGraphalignment(“graphmatching”)isabinaryoperationthatacceptsapairofgraphsasargumentsand(whensuccessful)returnsagraphthatrepre-sentsa(possiblypartial)node-correspondencerelationshipbetweenthem(Section7.2.4).Returnvaluescouldrangeinformfromanodethatdesignatesapairofcorrespondingnodesinthearguments,toarepresentationthatin-cludesadistinguishedsetofarcs(potentiallylabeledwithvectorembeddings)thatrepresentrelationshipsamongallpairsofcorrespondingnodes.Severalneuralmatchingmodelshavebeendemonstrated,someofwhicharerelativelyscalable.4Graphalignmentcouldbeapretrainedandﬁne-tunedfunction.10.6.7LatticeOperationsLatticeoperations(uniﬁcationandgeneralization,AppendixA1).arebinaryoperationsthatincludemechanismsforgraphalignmentandcombination.Softlatticeoperationsdiﬀerfrommatchinginthattheyreturnwhatisse-manticallyasinglegraph.Likegraphalignment,latticeoperationscouldbepretrainedandﬁne-tuned,orcouldserveasauxiliarytrainingtasksinlearningQNRinference.Neuralmodulespretrainedtomimicconventional1.Arelatedoperationwouldacceptagraphreferencedatonenodeandreturnagraphreferencedatanother,representingtheresultofagraphtraversal.2.Veličkovićetal.(2020)andCappartetal.(2021)3.Thisisthefundamentaltopology-modifyingoperationemployedbyobjectcapabilitysystems(Nobleetal.2018):AnodeAwithmessage-passingaccesstonodesBandCcanpassitsnode-Baccess(a“capability”)tonodeC;nodeAmayormaynotretainitsaccesstoCafterward.Thisoperationcanbeiteratedtoconstructarcsbetweenwhatareinitiallydistantnodesinagraph.Intuitively,access-passingissemanticallywellmotivatedifthe“need”foramoredirectconnectionfromBtoCcanbecommunicatedthroughmessagesreceivedbyA.SeealsoVeličkovićetal.(2020).4.Y.Lietal.(2019),Sarlinetal.(2020),Y.Baietal.(2020),andFeyetal.(2020)85algorithmsforuniﬁcationandgeneralizationcouldpotentiallyserveasbuild-ingblocksforarangeofinferencealgorithmsthatoperateonsoftlatticesandrichsemanticrepresentations.111PotentialApplicationAreasPotentialapplicationsofQNR/NL+functionalityincludeandextendap-plicationsofnaturallanguage.Theyincludehuman-orientedNLPtasks(translation,questionanswering,semanticsearch),butalsointer-agentcommunicationandtheintegrationofformalandinformalrepresen-tationstosupportscience,mathematics,automaticprogramming,andAutoML.QNR/NL+frameworksareintendedtosupportwide-rangingapplicationsbothwithinandbeyondthescopeofnaturallanguage.Thepresentsectionsketchesseveralpotentialapplicationareas:ﬁrst,applicationstotasksnar-rowlycenteredonlanguage—search,questionanswering,writing,translation,andlanguage-informedagentbehavior—andthenarangeofapplicationsinscience,engineering,mathematics,software,andmachinelearning,includingthegeneralgrowthandmobilizationofknowledgeinhumansociety.Thedis-cussionwillassumesuccessindevelopinghigh-levelQNR/NL+capabilities.11.1Language-CenteredTasksTasksthatmapNLinputstoNLoutputsarenaturalapplicationsofNL+-basedmodels.Thesetasksincludeinternetsearch,questionanswering,translation,andwritingassistancethatrangesfromeditingto(semi)autonomouscontentcreation.11.1.1SearchandQuestionAnsweringInsearch,NL+representationscanprovideasemanticbridgebetweenNLqueriesandNLdocumentsthatemploydiﬀerentvocabularies.Searchandquestion-answering(QA)modelscanjointlyembedqueriesandcontent,en-ablingretrievalofNLcontentbysemanticsimilaritysearch2anchoredinthe1.SeeVeličkovićandBlundell(2021)andincludedreferences.2.ReviewedinYeZhangetal.(2017)andMitraandCraswell(2018).86NL+domain;beyondcomparingembeddings,directNL+toNL+comparisonscanfurtherreﬁnesetsofpotentialsearchresults.Alternatively,languagemodelsconditionedonqueries(andpotentiallyonmodelsofreaders’stylepreferences)cantranslateretrievedNL+semanticcontenttoﬂuentNLanswers.QAﬁtswellwithdocumentsearch,asillustratedbyGoogle’sinformationboxes:Theresponsetoasearchquerycanincludenotonlyasetofdocuments,butinformationabstractedfromthecorpus.Inabroaderapplication,NL+-basedmodelscouldgenerateextendedan-swersthataremorecomprehensive,moreaccurate,andmoredirectlyrespon-sivetoaquerythananyexistingNLdocument.Withthepotentialfordenselinking(perhapspresentedasin-placeexpansionoftextandmedia),query-responsiveinformationproductscouldenablebrowsingofinternet-scaleknowledgecorporathroughpresentationsmoreattractiveandinformativethanconventionalwebpages.11.1.2TranslationandEditorialSupportTranslatingandediting,likeQA,callforinterpretingmeaningandproducingresultsconditionedoncorpus-basedcontentandpriors.Diﬀerencesincludeagreateremphasisonlengthyinputsandonoutputsthatcloselyparallelthoseinputs,withaccesstospeciﬁcknowledgeplayingasupportingratherthanprimaryrole.Duringtraining,massivelymultilingualtranslationtaskshaveproducedlanguage-invariantintermediaterepresentations(interlinguas1);wecanexpectsimilarorbetterinterlinguarepresentations—andassociatedtranslations—insystemsthatemployNL→NL+→NLarchitectures.Priorsbasedonthefrequencyofdiﬀerentpatternsofsemanticcontent(notphrases)canaiddisambiguationofNLsourcetext.Thetaskofmachine-aidededitingisrelatedtotranslation:Reproducingsemanticcontentwhiletranslatingfromlanguagetolanguagehasmuchincommonwithtransformingaroughdraftintoareﬁnedtext;stretchingthenotionofreproducingsemanticcontent,asystemmightexpandnotesintotextwhileretainingsemanticalignment.2Itisagainnaturaltoexploitpriorsoverpatternsofexpressiontohelpinterpretinputsandgenerateoutputs.Accesstoknowledgefrombroad,reﬁnedcorporacouldgreatlyenrichcontentwhenexpandingnotes.ThegraphstructureofhypertextmakesQNRsagoodﬁttoNL+-supportedauthoringofonlineNLcontent.1.SeeY.Luetal.(2018)andArivazhaganetal.(2019).2.Alimitingcaseofthistaskwouldbesemi-autonomousproductionofcontent,potentiallyonalargescale,guidedbyonlythemostgeneralindicationsofpurpose;seeSection12.2.87Asaspeciﬁc,high-leverageapplication,suchtoolscouldhelpcontributorsexpandandimproveWikipediacontent.Systemsthatcompile,reﬁne,andaccessaQNRtranslationofWikipediawouldbeanaturalextensionofcurrentresearchontheuseofexternalinformationstores.1Humancontributorscouldplaytherolesthattheydotoday,butaidedbygenerativemodelsthatdrawonreﬁnedNL+corporatosuggestcorrectedandenrichedcontent.Mostofwhatpeoplewanttoexpresseitherrepeatswhathasbeensaidelsewhere(butrephrasedandadaptedtoacontext),orexpressesnovelcon-tentthatparallelsormergeselementsofpreviouscontent.Mechanismsforabstractionandanalogy,inconjunctionwithexamplesandpriorsfromex-istingliteratures,cansupportinteractiveexpansionoftextfragmentsandhintstoprovidewhatisineﬀectamorepowerfulandintelligentformofautocomplete.Similarfunctionalitycanbeappliedatahighersemanticlevel.Responsiblewritersseektoavoidfactualerrors,whichcouldbeidentiﬁed(provisionally!)byclashesbetweentheNL+encodingofaportionofawriter’sdraftandsimilarcontentretrievedfromanepistemicallyhigh-qualityNL+corpus.2Writersoftenprefer,notonlytoavoiderrors,buttoinformtheirwritingwithknowledgethattheydonotyethave.Fillingsemanticgaps,whetherthesestemfromomissionorerrorremoval,canberegardedasacompletiontaskoverabstractrepresentations(Section10.4.2).Semanticallyinformedsearchandgenerativemodelscouldretrieveandsummarizecandidatedocumentsforanauthortoconsider,playingtheroleofaresearchassistant;3conditionallan-guagemodels,promptedwithcontextandinformedbyexternalknowledge4couldgeneratesubstantialblocksoftext,playingtheroleofacoauthor.11.1.3(Semi)AutonomousContentCreationSocialmediatodayisdegradedbytheinﬂuenceofMIsinformedandun-sourcedcontent,aproblemcaused(inpart)bythecostofﬁndinggoodinfor-1.Vergaetal.(2020),Guuetal.(2020),andXuetal.(2020)discussWikipedia-orientedsystems.2.Toenableretrievalofsimilaryetpotentiallyclashingcontent,(some)embeddingsshouldrepresent,nottheconcretesemanticcontentofexpressions(ineﬀect,answerstopotentialquestions),butthekindsofquestionsthatthecontentcananswer,animportantdistinctionnotedabove.Relevantclasheswouldthenbeindicatedbyfailuresofpartiallysuccessfulattemptsatsoftuniﬁcationbetweennewandretrievedcontent.3.Becausestatementsmayprovidesupportforotherstatements,providingsuchmaterialisrelatedtoargumentation,whereautomated,corpus-basedmethodsareanareaofactiveresearch;forexample,seeLawrenceandReed(2020)andSlonimetal.(2021).4.AprocessillustratedbyXuetal.(2020).88mationandcitingsources,and(inpart)byfact-indiﬀerentactorswithotheragendas.Well-informedrepliesarerelativelycostlyandscarce,butmob-noiseandbot-spewareabundant.Asahuman-mediatedcountermeasure,responsiblesocialmediapartic-ipantscoulddesignatetargetsforreply(perhapswithahinttosetdirec-tion)andtakepersonalresponsibilityforauthorshipwhilerelyingonsemi-autonomousmechanismsforproducing(draftsof)content.Asafullyauto-nomouscountermeasure,botscreatedbyresponsibleactorscouldscanposts,recognizeproblematiccontent,andreplywithouthumanintervention.Ac-torsthatcontrolsocialmediasystemscoulduseanalogousmechanismsinﬁltering,wherea“reply”mightbeawarningordeletion.Acceptable,fullyeﬀectivecountermeasurestotoxicmediacontentarediﬃculttoimagine,yetsubstantialimprovementsatthemarginmaybebothpracticalandquitevaluable.11.2AgentCommunication,Planning,andExplanationHumanagentsuselanguagetodescribeandcommunicategoals,situations,andplansforaction;itisreasonabletoexpectthatcomputationalagentscanlikewisebeneﬁtfrom(quasi)linguisticcommunication.1IfNL+representa-tionscanbestrictlymoreexpressivethanNL,thenNL+canbestrictlymoreeﬀectiveasameansofcommunicationamongcomputationalagents.InternalrepresentationsdevelopedbyneuralRLagentsprovideanotherpointofreferenceforpotentialagent-orientedcommunication.Somesystemsemploymemorieswithdistinct,potentiallyshareableunitsofinformationthatcanperhapsbeviewedaspre-linguisticrepresentations(e.g.,seeBan-inoetal.(2020)).ThelimitationsinherentincurrentRL-agentrepresenta-tionssuggestthepotentialforgainsfromlanguage-likesystemsinwhichthecompositionalelementsexpressdurable,shareable,stronglycompositionalabstractionsofstates,conditions,actions,eﬀects,andstrategies.QNR/NL+-basedrepresentationscancombineunnamable,lexical-levelab-stractionswithlexical-levelelementsthatdescribesemanticroles,conﬁdence,relativetime,deonticconsiderations,andthelike—inotherwords,semanticelementslikethoseoftenexpressedinNLbyfunctionwordsandTAM-Cmodiﬁers(Section5.3.3,Section5.3.4,andSectionA3.3).TheroleofNLinhumancommunicationandcognitionsuggeststhatNL+representationscan1.Forexamplesofrelatedwork,seeShahetal.(2018)andAbramsonetal.(2021).Relativelysimplelinguisticrepresentationshaveemergedspontaneously;seeMordatchandAbbeel(2018)andLazaridouandBaroni(2020).89contributetobothinter-andintra-agentperformance,sometimescompetingwithtightlycoupled,task-speciﬁcneuralrepresentations.CommunicationbetweenhumansandRLagentscanbeneﬁtfromlanguage.Althoughreinforcementlearningcanenableunaidedmachinestooutperformhumanprofessionalsevenincomplexgames,1humanadviceconveyedbyNLcanspeedandextendthescopeofreinforcementlearning.2Conversationalapplicationsprovidenaturalmechanismsforclariﬁcationandexplanation—inbothdirections—acrossmachine-humaninterfaces,potentiallyimprovingthehumanvalueandinterpretabilityofAIactions.GivensuitableNL+descriptionsandtask-relevantcorpora,similaritysearchcouldbeappliedtoidentifydescriptionsofsimilarsituations,problems,andpotentiallyapplicableplans(includinghumanprecedents);mechanismslikethoseproposedforknowledgeintegrationandreﬁnement(Section9.4)couldbeappliedtogeneralizethroughanalogyandﬁllgapsthroughsoftuniﬁca-tion.Widelyusedcontentwouldcorrespondto“commonsenseknowledge”and“standardpractice”.3Likenaturallanguage,NL+representationscouldsupportbothstrategicdeliberationandconcreteplanningatmultiplescales.Agentswithaccesstolargeknowledgecorporaresemblehumanswithaccesstotheinternet:Humansusesearchtoﬁndsolutionstoproblems(mathematics,travel,kitchenrepairs);computationalagentscandolikewise.Likehumanpopulations,agentsthataredeployedatscalecanlearnandpooltheirknowledgeatscale.Frequentproblemswill(bydeﬁnition)seldombenewlyencountered.11.3Science,Mathematics,andSystemDesignAlthoughresearchactivitiesinscience,mathematics,engineering,andsoft-waredevelopmentdiﬀerincharacter,theyshareabstracttasksthatcanbeframedassimilaritysearch,semanticalignment,analogy-building,clashde-tection,gaprecognition,andpatterncompletion.Advancesintheseﬁeldsinvolveanongoinginterplaybetween:1.Includinggamesthatrequirelong-termplanning(Vinyalsetal.2019;OpenAIetal.2019).2.Luketinaetal.(2019)reviewsprogressandcallsfor“tightintegrationofnaturallanguageunderstandingintoRL”.3.Asnotedabove,itisreasonabletoexpectthatthemostgeneralandfrequentlyusedkindsofknowledgewouldbeencoded,notindeclarativerepresentationsthatenablemulti-stepinference,butinmodelparametersthatenabledirectdecisionandaction;thisdistributionoffunctionalitywouldparallelKahneman’sSystem-1/System-2modelofhumancognition(Kahneman2011).90•Tentativeproposals(hypothesesinscience,proofgoalsinmathematics,designconceptsinengineeringandsoftwaredevelopment),•Domain-speciﬁcconstraintsandenablers(evidenceinscience,theoremsinmathematics,requirementsandavailablecomponentsinengineeringandsoftwaredevelopment),and•Competitionbetweenalternativeproposalsjudgedbytask-speciﬁccri-teriaandmetrics(maximizingaccuracyofpredictions,generalityofproofs,performanceofdesigns;minimizingrelevantformsofcostandcomplexity).Theseconsiderationshighlighttheubiquitousrolesofgenerativeprocessesandselectioncriteria,andarangeoffundamentaltasksinscience,mathemat-ics,engineering,andsoftwaredevelopmentcanbeaddressedbygenerativemodelsoverspacesofcompositionaldescriptions.ThesecanbecastintermsofQNRaﬀordances:Givenaproblem,ifacorpusofQNRscontainsdescriptionsofrelatedproblemstogetherwithknownsolutions,thensimilaritysearchonproblem-descriptions1canretrievesetsofpotentiallyrelevantsolutions.Jointsemanticalignment,generalization,andanalogy-buildingwithinproblem/solutionsetsthencansuggestaspaceofalternativesthatislikelytocontainsolutions—ornear-solutions—totheproblemathand.Inconjunctionwithaninitialproblemdescription,suchrepresentationspacescanprovidepriorsandconstraintsongenerativeprocesses,2andgeneratedcandidatesolutionscanbetestedagainsttask-speciﬁcacceptancecriteriaandqualitymetrics.Theseconsiderationsbecomemoreconcreteinthecontextofspeciﬁctaskdomains.11.3.1EngineeringDesign[T]hinkofthedesignprocessasinvolving,ﬁrst,thegenerationofalternativesand,then,thetestingofthesealternativesagainstawholearrayofrequirementsandconstraints.Thereneednotbemerelyasinglegenerate-testcycle,buttherecanbeawholenestedseriesofsuchcycles.—HerbertSimon31.Alongwithﬁlteringbasedondetailedcomparisons.2.Patterncompletionsmaysuggeststructures;samplingguidedbyembeddingsmaysuggestcomponents.3.Simon(1988).NotethatSimondescribesplanningasadesignprocess.91Typicalengineeringdomainsarestronglycompositional,andaspectsofcompositionality—modularity,separationoffunctions,standardizationofinterfaces—arewidelysharedobjectivesthataidnotonlythedesignandmod-elingofsystems,butalsoproduction,maintenance,andreuseofcomponentsacrossapplications.Representationsusedinthedesignandmodelingofengi-neeringsystemstypicallycomprisedescriptionsofcomponents(structures,circuits,motors,powersources...)andtheirinteractions(forces,signals,powertransmission,cooling...).Inengineeringpractice,naturallanguage(andprospectively,NL+)isinterwovenwithformal,physicaldescriptionsofsystem-levelrequirements,options,andactualoranticipatedperformance.AsHerbertSimonhasobserved,designcanbeseenasagenerate-and-testprocess—anaturalapplicationofgenerativemodels.1Awiderangeofpro-posedsystemscanbetestedthroughconventionalsimulation.2Inengineering,evennovelsystemsaretypicallycomposed(mostlyoren-tirely)ofhierarchiesofsubsystemsoffamiliarkinds.3TheaﬀordancesofQNRsearchandalignmentareagainapplicable:Embeddingandsimilaritysearchcanbeusedtoquerydesignlibrariesthatdescribeoptionsatvariouslevelsofabstractionandprecision;descriptionscanbebothphysicalandfunctional,andcanintegrateformalandinformalinformation.Semanticalignmentanduniﬁcationprovideaﬀordancesforﬁllinggaps—here,unﬁlledfunctionalrolesinsystemarchitectures—toreﬁnearchitecturalsketchesintoconcretedesignproposals.Thegenerationofnoveltybysoft-latticegeneralizationandcombinationoperations(AppendixA1)couldpotentiallyenablefundamentalinnovation.Becauseengineeringaimstoproducesystemsthatservehumanpurposes,designspeciﬁcations—requirements,constraints,andoptimizationcriteria—mustﬁtthosepurposes.ThedevelopmentofformalspeciﬁcationsisaninformalprocessthatcanbeneﬁtfromQNRaﬀordancesthatincludeanal-ogy,patterncompletion,andclashdetection,aswellasapplicationsofthecommonsenseknowledgeneededtochooseobviousdefaults,rejectobviousmistakes,andidentifyconsiderationsthatcallforhumanattention.1.SeediscussionsinKahng(2018),Liaoetal.(2019),andOhetal.(2019).Machine-aidedinteractivedesign(DeshpandeandPurwar2019)andimitationlearningcanalsohelptogenerateproposals;seeRaina,McComb,andCagan(2019),Raina,Cagan,andMcComb(2019),andGaninetal.(2021).2.OrusingML-basedsimulationmethods,whichareofincreasingscopeandquality.Inparticular,advancesinML-basedmolecularsimulation(reviewedinNoéetal.2020)canbeexpectedtofacilitatemolecularsystemsengineering.3.IllustratedbyworkinStumpetal.(2019),Moetal.(2019),andChenandFuge(2019).9211.3.2ScientiﬁcInquiryScienceandengineeringoftenworkcloselytogether,yettheirepistemictasksarefundamentallydiﬀerent:Engineeringseekstodiscovermultipleoptionsforachievingpurposes,whilescienceseekstodiscoveruniquelycorrectde-scriptionsofthingsthatexist.Scienceandengineeringintertwineinpractice:Scientistsexploitproductsofengineering(telescopes,microscopes,particleaccelerators,laboratoryprocedures...)whentheyperformobservationsandexperiments,whileengineersengageinsciencewhentheyaskquestionsthatcannotbeansweredbyconsultingmodels.PotentialapplicationsofQNRaﬀordancesinscienceinclude:•TranslatingNLpublicationsintouniform,searchablerepresentations•Applyinguniﬁcationtocombineandextendpartialdescriptions•Applyinguniﬁcationtoidentifyclashesbetweendescriptions•Applyinganalogiesfromdevelopedﬁeldstoidentifygapsinnewﬁelds•Applyinganalogiestosuggesthypothesesthatﬁllthosegaps•Matchingexperimentalobjectivestoexperimentalmethods•Matchingquestionsanddatatostatisticalmethods•Assessingevidencewithattentiontoconsensus•Assessingevidencewithattentiontoconsilience•EnablingongoingupdatesofinferentialdependencystructuresApplicationsliketheseneednotautomatescientiﬁcjudgment:Toprovidevalue,theyneedonlyprovideusefulsuggestionstohumanscientists.Devel-opmentsalongtheselineswouldextendcurrentdirectionsinapplyingMLtoscientiﬁcliteratures.111.3.3MathematicsYouhavetoguessamathematicaltheorembeforeyouproveit;youhavetoguesstheideaoftheproofbeforeyoucarrythroughthedetails.Youhavetocombineobservationsandfollowanalogies;youhavetotryandtryagain.—GeorgePólya2Inmathematicalapplications,proposedQNR/NL+frameworkscouldwrapformal,symbolicstructuresinsoftdescriptions3thatcanbeappliedtohelp1.E.g.,M.Jiangetal.(2020)andRaghuandSchmidt(2020)2.Pólya(1990)3.Szegedy(2020)suggestsderivingformalexpressionsfromNLtext.93recognizeanalogiesandexpresspurpose,andthesecapabilitiescanoperateatmultiplelevelsofgranularity.Pólyaobservesthatdiscoveryinmathematicsinvolvesgenerate-and-testcyclesguidedbysoftconsiderations,andmoderndeeplearningconﬁrmsthevalueofsoftmatchinginguidingtheoremproving.BetterneuralrepresentationscanimproveML-informedpremiseselection,1slowingtheexplosivegrowthofdeepprooftreesbyimprovingthesuccessrateofgenerate-and-testcycles.Graphneuralnetworksthatoperateonsyntacticstructurescanprovideusefulembeddings(M.Wangetal.2017),andenrich-ingformalsymbolicrepresentationswithsoftsemanticdescriptions(e.g.,ofknownuse-contexts)shouldenablefurthergains.Pólyaemphasizestheim-portanceofanalogy,akindofsoft,structuredgeneralization(SectionA1.1.2).Theformal(hencemorerestrictive)latticeoperationofgeneralizationbyanti-uniﬁcationhasbeenappliedtoanalogicalreasoninginsymbolicmathematics(Guheetal.2010);embeddingsymbolicstructuresinsoftrepresentationscouldextendthescopeofpotentialgeneralizations.11.4SoftwareDevelopmentandAutoMLApplicationsofneuralMLtosoftwaredevelopmentareunderintenseexplo-ration.2Languagemodelscansupportinteractive,text-basedcodecompletionandrepair;3recentworkhasdemonstratedgenerationofcodebasedondoc-strings.4GNNscouldoperateonstructuredrepresentations(syntacticandsemanticgraphs)whilealsoexploitingfunctionnames,variablenames,com-ments,anddocumentationassourcesofinformationandtargetsforpredictioninrepresentationencodinganddecoding.QNRscanprovideaﬀordancesforenrichingsyntacticstructureswithsemanticannotationsandtheresultsofstaticprogramanalysis,5andforwrappingcodeobjects(bothimplementedandproposed)indescriptionsoftheirrequirementsandfunctionality.1.SeeKucikandKorovin(2018),Bansaletal.(2019),andFerreiraandFreitas(2020).2.SeeforexamplePolosukhinandSkidanov(2018),CamachoandMcIlraith(2019),WangandChristodorescu(2019),andOdenaetal.(2020).IBMrecentlyreleasedatrainingsetthatincludes14millioncodesamplescomprisingabout500millionlinesofcode(Puri2021).3.W.Wangetal.(2020),Fengetal.(2020),Tarlowetal.(2020),andSvyatkovskiyetal.(2021)4.Transformer-basedmodelstrainedonGitHubPythoncodearegoodenoughtobeofpracticalvalue,buttheyareerror-proneandsuccessratesdeclineexponentiallywithincreasingdocstringlengthChenetal.(2021);worse,40%ofthecodehasbeenfoundtocontainpotentiallyexploitablebugs(Pearceetal.2021).5.Athoroughexploitationofpre-processinginthesymbolicdomainwouldprovideneuralnetworkswithgraph-structuredinputsthatencodenotonlysyntaxtrees,butdatastructures,inferredtypes,dataﬂow,andﬂowofcontrol.SeeAllamanis,Brockschmidt,andKhademi(2018),Cumminsetal.(2020),andGuoetal.(2021),andthediscussioninTarlowetal.(2020).94QNRrepresentationshaveadiﬀerent(andperhapscloser)relationshiptoautomatedmachinelearning(AutoML1),becauseneuralembeddingsandgraphsseemparticularlywell-suitedtorepresentingthesoftfunctionalityofneuralcomponentsingraph-structuredarchitectures.Again,generate-and-testprocessesguidedbyexamples,analogies,andpatterncompletioncouldinformsearchindesignspaces,2whilethescopeofthesespacescanembracenotonlyneuralarchitectures,buttheirtrainingmethods,softwareandhardwareinfrastructures,upstreamanddownstreamdatapipelines,andmore.12AspectsofBroaderImpactThebreadthofpotentialapplicationsofQNR-basedsystemsmakesitdiﬃculttoforesee(muchlesssummarize)theirpotentialimpacts.Leadingconsiderationsincludethepotentialuseandabuseoflinguisticcapabilities,ofagentcapabilities,andofknowledgeingeneral.SystemsbasedonQNRrepresentationspromisetoberelativelytransparentandsubjecttocorrection.PotentialrolesforQNR/NL+-enabledcapabilitiesareextraordinarilybroad,withcommensuratescopeforpotentialbeneﬁtsandharms.3ChannelsforpotentialQNR/NL+impactscanbelooselydividedintocoresemanticfunc-tionalities(applicationstoknowledgeinageneralsense),semanticfunction-alitiesatthehumaninterface(processingandproductionofnaturallanguagecontent),andpotentialrolesinAIagentimplementationandalignment.MostofthediscussionherewillbecastintermsoftheNL+spectrumofpotentialQNRfunctionality.12.1BroadKnowledgeApplicationsManyofthepotentialbeneﬁtsandharmsofQNR/NL+-enableddevelopmentsarelinkedtolargeknowledgecorporaandtheirapplications.Severalareasofpotentialimpactarecloselyrelatedtoproposedcorefunctionalitiesofknowledgeintegrationandaccess.1.Realetal.(2020)andHe,Zhao,andChu(2021)2.You,Ying,andLeskovec(2020),Radosavovicetal.(2020),andRenetal.(2021)3.Forasurveyofarangeofpotentialharms,seeBrundageetal.(2018).9512.1.1IntegratingandExtendingKnowledgeTranslationofcontentfromNLcorporatocorrespondingNL+canenabletheapplicationofQNR-domainmechanismstosearch,ﬁlter,reﬁne,integrate,andextendNL-derivedcontent,buildingknowledgeresourcesforwide-rangingapplications.Totheextentthatimprovingthequalityofknowledgeisonthewholebeneﬁcial(orharmful),weshouldexpectnetbeneﬁcial(orharmful)impacts.12.1.2MobilizingKnowledgeTranslationofNLexpressions(statements,paragraphs,documents...)tocorrespondingNL+representationspromisestoimprovesemanticembed-dingsandsimilaritysearchatscale(Section9.1.5),helpingsearchsystems“toorganizetheworld’sinformationandmakeituniversallyaccessibleanduseful”(Google2020)throughhigher-qualitysemanticindexingandqueryinterpretation.Generationofcontentthroughknowledgeintegrationcouldgobeyondsearchtodeliverinformationthatislatent(butnotexplicit)inexistingcorpora.Itisreasonabletoexpectbeneﬁcialﬁrst-orderimpacts.12.1.3FilteringInformationTotheextentthatNL→NL+translationiseﬀectiveinmappingbetweenNLcontentandmoretractablesemanticrepresentations,ﬁlteringofinformation1intheNL+domaincanbeusedtoﬁlterNLsources.Potentialapplicationsspanarangethatincludesbothreducingthetoxicityofsocialmediaandreﬁningcensorshipinauthoritarianstates.Inapplicationsoflanguagemodels,ﬁlteringbasedondisentangledrepresentationsofknowledgeandoutputscouldmitigateleakageofprivateinformation.212.1.4SurveillanceandIntelligenceAnalysisSurveillanceandintelligenceanalysisarerelativelydirectapplicationsofQNR-enabledknowledgemobilizationandintegration,andthebalanceofimpactsonsecurity,privacy,andpowerrelationshipswilldependinpartonhow1.E.g.,basedonmulti-sourceconsistency,consensus,coherence,consilience,andprove-nance(Section9.5.2).Currentﬁlteringmethodsappeartorelyheavilyonjudgmentsofsourcequality(adomain-insensitive,non-content-basedproxyforepistemicreliability),perhapsthesimplestuseofprovenance.2.AproblemdiscussedinCarlinietal.(2021).96informationisﬁltered,shared,andapplied.Mappingrawinformationintostructuredsemanticrepresentationscouldfacilitatealmostanyapplication,withobviouspotentialharms.Tomitigateharms,itwillbeimportanttoexplorehowﬁlteringofrawinformationcouldbeappliedtodiﬀerentiallyenablelegitimateapplications:Forexample,disentangledcompositionalrepresentationscouldbemoreeasilyredactedtoprotectsensitiveinformationwhileprovidinginformationnecessaryforlegitimatetasks.12.2ProducingQNR-InformedLanguageOutputsatScaleWeshouldexpecttoseesystemsthattranslateNL+contentintoNLtext1withﬂuencycomparabletomodelslikeGPT-3,anddosoatscale.Automated,NL+-informedlanguageproduction,includingsupportforhumanwriting(Section11.1.2),couldexpandquantity,improvequality,andcustomizethestyleandcontentoftextforspeciﬁcgroupsorindividualreaders.Thesecapabilitiescouldsupportarangeofapplications,bothbeneﬁcialandharmful.12.2.1ExpandingLanguageOutputQuantityTextgenerationenabledbylanguagemodelshasthepotentialtoproducetailoredcontentforsocialmediaeconomicallyandatscale:Humanwritersaretypicallypaid~0.20US$/word,2)about1,000,000timesthecostofqueryinganeﬃcientTransformervariant.3ItisreasonabletoexpectthatNL+-informedoutputswillhavebroadlysimilarcosts,ordersofmagnitudelessthanthecostsofhumanwriting,whetherthesecostsarecountedinmoneyortime.Putdiﬀerently,textoutputperunitcostcouldbescaledbyafactorontheroughorderof1,000,000.Evenwhenconstrainedbynon-computationalcostsandlimitationsofscope,thepotentialimpactofautomatedtextgenerationisenormous.12.2.2ImprovingLanguageOutputQualityApplicationsoflanguage-producingsystemswilldependinpartondomain-dependentmetricsofoutputquality:Higherqualitycanbothexpandthescopeofpotentialapplicationsanddecreasethecostsofhumansupervision,1.Translationwouldbesubjecttosemanticimprecisionduetodiﬀerencesinexpressivecapacity.2.Approximately—rangeofcompensationissubstantial(e.g.,seeTee(2021).3.Anoptimized(“FastFormer”)modelderivedfromBERTcanperforminferenceatacostofabout18US$/100millionqueries(KimandHassan2020).97whilechangingthenatureandbalanceofpotentialimpacts.Relativetoopaquelanguagemodels,systemsinformedbyNL+corporacanimproveabilities:•Tojudge,incorporate,andupdatefactualcontent•Toperformmulti-step,multi-sourceinference•ToapplyinferencetoreﬁneandexpandknowledgestoresCurrentmodelsbasedonopaque,learnedparametershavediﬃcultiesinalltheseareas;overcomingthesediﬃcultiescouldgreatlyexpandthescopeofpotentialapplications.12.2.3PotentiallyDisruptiveLanguageProductsThemostobvioussocietalthreatsfromNL+-basedlanguagecapabilitiesstemfromtheirabilitytoproducecoherentcontentthatdrawsonextensive(mis)informationresources—contentthatmimicsthemarkersofepistemicqualitywithoutthesubstance.Themagnitudeofthisthreat,however,mustbejudgedinthecontextofother,broadlysimilartechnologies.Systemsbasedonlargelanguagemodelsarebecomingﬂuentandpoten-tiallypersuasivewhileremainingfactuallyunreliable:Theycanmoreeasilybeappliedtoproduceplausiblemisinformationthaninformedcontent.Un-fortunately,thecurrentstateofsocialmediasuggeststhatﬂuent,persuasiveoutputsbasedonfalse,incoherentinformation—whetherfromconspiracyfansorcomputationalpropaganda—canbedisturbinglyeﬀectiveindegradingtheepistemicenvironment.1Thissuggeststhatthemarginalharmsofmak-ingmisinformationmorecoherent,betterreferenced,etc.,mayberelativelysmall.2Totheextentthatcapabilitiesareﬁrstdeployedbyresponsibleactors,harmscouldpotentiallybemitigatedordelayed.1.Existinglanguagemodelshavespurredconcernsregardingabuse,includingscalingofsocial-engineeringattacksoncomputersecurityandofcomputationalpropagandainpublicdiscourse(SeeWoolleyandHoward(2017)andHoward(2021)).Inpartasaconsequenceofsuchconcerns(Solaimanetal.(2019),andBrownetal.(2020),Section6.1),OpenAIrestrictedaccesstoitsGPT-3model.2.Onemayhopethatinﬂuentialaudiencesthathaveinthepastbeensusceptibletodocu-mentswithmisleadingbutapparentlyhigh-qualitycontent(e.g.,academicsandpolicymakers)wouldalsorespondtoprompt,well-targeted,high-qualitycritiquesofthosedocuments.Re-spondingpromptlywouldleavelesstimeformisleadinginformationtospreaduncheckedandbecomeentrenchedasconventionalwisdom.9812.2.4PotentiallyConstructiveLanguageProductsGeneratinglow-qualitycontentiseasyforhumansandmachines,andargu-ments(whetherforbadconclusionsorgood)cancausecollateraldamagewhentheyinadvertentlysignal-boostfalseinformation;conversely,argu-ments(regardlessofthemeritsoftheirconclusions)canproducewhatmightbedescribedas“positiveargumentationexternalities”whentheircontentsignal-boostswell-foundedknowledgeAlthoughthepotentialharmsoffacil-itatingtheproductionof(apparently)high-qualitymisinformationmaybemarginal,thepotentialbeneﬁtsoffacilitatingtheproductionofhigh-qualityinformationseemlarge.Itwouldbediﬃculttoexaggeratethepotentialvalueofevenmoderatesuccessindampingpathologicalepistemicspiralsandenablinginformationtogaintractionbasedonactualmerit.Authorswhoemployfreelyavailabletoolstoproducebetter-written,better-supported,moreabundantcontent(drawingaudiences,winningmorearguments)couldraisethebarforothers,drivingmorewidespreadadoptionofthosesametools.Epistemicspiralscanbepositive.112.3AgentStructure,Capabilities,andAlignmentSection11.2discussedNL+representationsaspotentialenablersforagentperformance—forexample,bysupportingthecompositionofplanelements,retrievalofpastsolutions,andadvice-takingfromhumans.Inconsideringpotentialimpacts,opportunitiesforimprovingtransparencyandalignmentbecomeparticularlyimportant.12.3.1AgentStructureAlong-standingmodelofadvancedAIcapabilitiestakesforgrantedacentralroleforgeneral,unitaryagents,oftenimaginedasentitiesthatlearnmuchasahumanindividualdoes.TheAI-servicesmodel2challengesthisassumption,proposingthatgeneralcapabilitiesreadilycould(andlikelywill)emergethroughtheexpansionandintegrationoftask-orientedservicesthat—cruciallyforpotentialgenerality—canincludetheserviceofdevelopingnewservices.IntheAI-servicesmodel,broadknowledgeandfunctionalityneednotbeconcentratedinopaque,mind-likeunits,butcaninsteademergethrough1.Eﬀectivealtruistspleasetakenote.2.Drexler(2019)99aggregationoverlargecorporaofknowledgeandtools,potentiallyinformedbothbypre-existinghuman-generatedcorporaandbymassivelyparallel(ratherthanindividual)experienceofinteractionwiththeworld.TheAI-servicesmodelﬁtswellwiththeQNR/NL+modelofscalable,multimodalknowledgeaggregationandintegration.12.3.2AgentCapabilitiesAlsoinalignmentwiththeAI-servicesmodelofgeneralintelligence,theabilityofrelativelysimpleagentstoaccessbroadknowledgeandtoolsets1couldamplifytheircapabilities.Thisprospectlendscredencetolong-standingthreatmodelsinwhichagentsrapidlygaingreatandpotentiallyunpredictablecapabilities;themechanismsdiﬀer,butthepotentialresultsaresimilar.ClassicAI-riskscenarioscommonlyfocusonAIcapabilitiesthatmightemergefromanimmense,opaque,undiﬀerentiatedmassoffunctionality,asituationinwhichagentsmightpursueunexpectedgoalsbyunintendedmeans.Itmaybesafertoemploytask-orientedagents(andcompositionsofagents)thatoperatewithinaction-andknowledge-spacesthatarebet-terunderstoodanddonotgrosslyexceedtaskrequirements.2Basingfunc-tionalityonbounded,diﬀerentiatedresourcesprovidesaﬀordancesforobserv-ing“whatasystemisthinkingabout”andforconstraining“whatanagentcanknowanddo”,potentiallypowerfultoolsforinterpretingandconstraininganagent’splans.3Accordingly,developerscouldseektobound,shape,andpre-dictbehaviorsbyexploitingtherelativesemantictransparencyofproposedQNR/NL+corporatodescribeandstructuretheknowledge,capabilities,constraints,andobjectivesoftask-orientedagents.12.3.3AgentAlignmentManyoftheanticipatedchallengesofaligningagents’actionswithhumanintentionshingeontheanticipateddiﬃcultyoflearninghumanpreferences.4Theabilitytoread,interpret,integrate,andgeneralizefromlargecorporaofhuman-generatedcontent(philosophy,history,news,ﬁction,courtrecords,discussionsofAIalignment...)couldsupportthedevelopmentofrichly1.E.g.,throughinternet-scalesearchandcloudservices.2.Anapplicationofthe“PrincipleofLeastPrivilege”insystemdesign.3.SeeDrexler(2019,Section9.7).4.Bostrom(2014)andRussell(2019)100informedmodelsofhumanpreferences,concerns,ethicalprinciples,andlegalsystems—andmodelsoftheirambiguities,controversies,andinconsistencies.1Conversationalsystemscouldbeusedtotestandreﬁnepredictivemodelsofhumanconcernsbyinvitinghumancommentaryonactual,proposed,andhypotheticalactions.NL+systemsthatfulﬁlltheirpromisecouldmodeltheseconsiderationsmoreeﬀectivelythanhumanlanguageitself,inawaythatisnotfullyanddirectlylegible,yetopentoinspectionthoughthewindowsofqueryandtranslation.13ConclusionsCurrentneuralMLcapabilitiescansupportthedevelopmentofsystemsbasedonquasilinguisticneuralrepresentations,alineofresearchthatpromisestoadvancearangeofresearchgoalsandapplicationsinNLPandbeyond.Naturallanguage(NL)hasunrivaledgeneralityinexpressinghumanknowl-edgeandconcerns,butisconstrainedbyitsrelianceonlimited,discretevocabulariesandsimple,tree-likesyntacticstructures.Quasilinguisticneu-ralrepresentations(QNRs)cangeneralizeNLsyntacticstructuretoexplicitgraphs(Section8.1)andcanreplacediscreteNLvocabularieswithvectorembeddingsthatconveyrichermeaningsthanwords(Section5.3,Section8.2).ByprovidingaﬀordancesforgeneralizingandupgradingthecomponentsofNL—bothitsstructureandvocabulary—QNRsystemscanenableneuralsystemstolearn“NL+”representationsthatarestrictlymoreexpressivethanNL.Machineswithhuman-likeintellectualcompetencemustbefullyliterate,ablenotonlytoread,buttowritethingsworthreadingandretainingascontributionstoaggregateknowledge.Literatemachinescanandshouldemploymachine-nativeQNR/NL+representations(Section8)thatarebothmoreexpressiveandmorecomputationallytractablethansequential,mouth-and-earorientedhumanlanguages.ProspectsforQNR/NL+systemsmakecontactwithahostofﬁelds.Theseincludelinguistics(Section5),whichoﬀersinsightsintothenatureofexpres-siveconstructsinNL(aconceptualpointofdepartureforNL+),aswellas1.AlonglinessuggestedbyStuartRussell(Wolchover2015);seealsodiscussioninDrexler(2019,Section22).101currentneuralML,inwhichvector/graphmodelsandrepresentationlearningprovideaconcretebasisforpotentialQNRimplementations(Section10).Considerationsthatincludelocalcompositionality(Section4.3)suggestthatvector/graphconstructscanprovidecomputationallytractablerepresenta-tionsofbothcomplexexpressionsandthecontextsinwhichtheyaretobeinterpreted(Section8.3).DrawingonexistingNLcorpora,QNR-basedsystemscouldenabletheconstructionofinternet-scaleNL+corporathatcanbeaccessedthroughscal-ablesemanticsearch(Section9.1),supportingapowerfulMLanalogueoflong-termmemory.Inaddition,QNR/NL+frameworkscansupportuniﬁca-tionandgeneralizationoperationson(soft,approximate)semanticlattices(AppendixA1),providingmechanismsusefulinknowledgeintegrationandreﬁnement(Section9.4,Section9.5).ApplicationsofprospectiveQNR/NL+functionalitycouldsupportnotonlyepistemicallywell-informedlanguageproduction(Section11.1),butthegrowthandmobilizationofknowledgeinscience,engineering,mathematics,andmachinelearningitself(Section11.3).Thefundamentaltechnologiesneededtoimplementsuchsystemsarealreadyinplace,incrementalpathsforwardarewell-alignedwithresearchobjectivesinMLandmachineintel-ligence,andtheirpotentialadvantagesinscalability,interpretability,cost,andepistemicqualitypositionQNR-basedsystemstocomplementordisplacecurrentfoundationmodels(Bommasanietal.2021)atthefrontiersofmachinelearning.102AcknowledgementsIwouldliketothankthemanyindividualsattheFutureofHumanityInstitute,DeepMind,andelsewhereforcommentsandquestionsthathelpedtoshapethiswork.SpecialthanksgotoDevangAgrawal,TeddyCollins,OwenCotton-Barrett,AndrewCritch,DavidDalrymple,OwainEvans,AlešFlídr,IrinaHiggins,AlexanderLerchner,AdamMarblestone,MichaelNielsen,AndrewTrask,JonathanUesato,andDaniYogatamaforhelpfulinputs,toRosaWangformanydiscussionsandunﬂaggingsupport,andtoNickBostromandtheFutureofHumanityInstituteforprovidingtheintellectualenvironmentandsupportthatmadethisworkpossible.Regardingthedocumentitself,IthankTanyaSinghforlaunchingtheLATEXconversionproject,JimmyRintjemafordoingthework,andtheBerkeleyExistentialRiskInitiative(BERI)forpayingthebills.Finally,measurestakentocounterthespreadofSARS-CoV-2deserveameasureofcredit(andperhapsblame)forthescaleandscopeofthisdocument.103A1UniﬁcationandGeneralizationonSoftSemanticLatticesQNRrepresentationscansupportoperationsthatcombine,contrast,andgeneralizeinformation.Theseoperations—softapproximationsofuniﬁcationandanti-uniﬁcation—canbeusedtoimplementcontinuousrelaxationsofpowerfulmechanismsforlogicalinference.Arangeofformalrepresentationsofmeaning,bothinlogicandlanguage,havethestructureofmathematicallattices.AlthoughthepresentproposalforQNRsystems(andaspirationalNL+systems)explicitlysetsasidetheconstraintofformality,approximatelatticestructureemergesinNLandwill(orshould,orreadilycould)bearelativelystrongpropertyofQNRs/NL+.Becauselatticescanprovideusefulproperties,itisworthconsideringthepotentialrolesandapplicationsoflatticestructureinQNR-basedsystems.NotethatthefundamentalgoalsofNL+—generalsuperioritytoNLinex-pressivenessandcomputationaltractability—donotrequirelatticepropertiesbeyondthosethatNLitselfprovides.Theabilitytoprovidestrongerlatticepropertiesisapotential(further)strengthofNL+,notarequirement.Inotherwords,latticepropertiesarenaturalanduseful,yetoptional.Thefollowingsectionsbeginbydiscussingthemotivationforconsideringandstrengtheninglatticeproperties—supportingmeetandjoin,a.k.a.uniﬁ-cationandgeneralization—inlightoftheirpotentialrolesandutility.AbriefreviewofapproximatelatticestructureinNLprovidesaninitialmotivationforapplyinglatticestructureinNL+withinthescopeofamethodologythatavoidscommitmenttoformalmodels.Considerationoflatticesinlogicandinconstraintlogicprogrammingfurthermotivatesthepursuitofapproxima-tions,andintroducesadiscussion,inpartspeculative,regardinginductivebiasandprospective,emergentlattice-orientedQNRrepresentations.Thistopicisadjacenttomanyothers,creatingalargesurfaceareathatprecludesanycompactandcomprehensivediscussionrelationshipstoexistingwork.1Asketchoftheserelationshipsandpointersintorelevantliteraturesprovidestartingpointsforfurtherreading.1.Inparticular,studiesoflatticesemanticsinNL,uniﬁcationandgeneralizationinsymboliccomputation,andlessonslearnedinthebroaderstudyofneuro-symbolicML.104A1.1MotivationTypicalexpressionscanberegardedasapproximatedescriptionsofthings,whetherambiguous(pet,ratherthancat)orpartial(greycat,ratherthanbiggreycat).Giventwoexpressions,onemaywanttocombinethemtoformeitheranarrowerdescription(bycombiningtheirinformation)orabroaderdescription(bycombiningtheirscope).Althoughmanyotheroperationsarepossible(averaging,perhaps,orextrapolation),narrowingandbroadeningareoffundamentalimportance,andinmanysemanticdomains,quiteuseful.Theycanbeconstruedasoperationsonaformalorapproximatesemanticlattice.A1.1.1WhyUniﬁcation(Meet,Intersection,Narrowing,Specialization)?Insymboliclogic,expressionscorrespondtopointsinalattice(deﬁnedbelow),anduniﬁcationisanoperationthatcombinestwoexpressionstoformamorespeciﬁcexpressionbycombiningtheircompatibleinformation.1Ingenericlattices,thecorrespondingoperationistermedmeet,whichinmanycontextscanberegardedasanintersectionofsetsorregions.Uniﬁcationcombinescompatibleinformation;failuresofuniﬁcationidentifyclashes.2TheProloglanguageillustrateshowuniﬁcationandfailurescanenablereasoningandproof.A1.1.2WhyAnti-Uniﬁcation(Join,Union,Broadening,Generalization)?Alternatively,twoexpressionsmayprovide(partial)descriptionsoftwoenti-tiesofthesamekind.Here,anaturalgoalistodescribepropertiescommontoallthingsofthatkind;clashesbetweenaspectsoftheirdescriptionsindicatethatthoseaspectsarenotdeﬁnitional.Inalatticeofexpressions,thisformofgeneralizationistermedanti-uniﬁcation,3whichincreasesgeneralitybydiscardingclashingorunsharedinformation.Ingenericlattices,thecorrespondingoperationistermedjoin,1.Moreprecisely,uniﬁcationisanoperationthatyieldsthemostgeneralinstanceofsuchanexpression.Foranapplication-orientedoverview,seeKnight(1989).2.Whenuniﬁcationattemptstocombinepartialinformationaboutasingleentity,itisnaturalforinconsistentinformationtoimplyfailure.3.Moreprecisely,amongpotentialmoregeneralexpressions,anti-uniﬁcationyieldsthemostspeciﬁcinstance.105whichinmanyinstancescorrespondstoaunionofsetsorregions.1Anti-uniﬁcationhasapplicationsinNLPandcanbeusedtoformgeneraliza-tionsandanalogiesinmathematics.2InneuralML,approximationsofanti-uniﬁcationcouldpotentiallyinformpriorsforadistributionofunseenin-stancesofaclass.A1.1.3HypothesesRegardingRolesinQNRProcessingConsiderationsexploredinthisappendixsuggestarangeofhypothesesre-gardingpotentialrolesforapproximatelatticerepresentationsandoperationsinQNRprocessing:•Thatlatticerepresentationsandoperations(“latticeproperties”)can,infact,beusefullyapproximatedinneuralsystems.•Thatlearningtoapproximatelatticepropertiesneednotimpairgeneralrepresentationalcapacity.•Thatapproximationsofmeetandjoinoperationswillhavebroadvalueinsemanticprocessing.•Thatlatticepropertiescanbeapproximatedtovaryingdegrees,provid-ingasmoothbridgebetweenformalandinformalrepresentations.•Thatlatticepropertiescanregularizerepresentationsinwaysthatareusefulbeyondenablingapproximatemeetandjoin.•Thatapproximationsoflatticerepresentationsandoperationsarebestdiscoveredbyend-to-endlearningofneuralfunctions.•Thatexplicit,approximatesatisfactionoflatticeidentitiescanprovideusefulauxiliarytrainingtasks.A1.2FormalDeﬁnitionsAmathematicallatticeisapartiallyorderedsetthatcaninmanyinstancesbeinterpretedasrepresentingthe“inclusion”of“subsumption”ofoneelementbyanother.Axiomatically,alatticehasuniquemeetandjoinoperations,∧and∨:themeetoperationmapseachpairofelementstoauniquegreatestlowerbound(inﬁmum),whilejoinmapseachpairofelementstoauniqueleastupperbound(supremum).LiketheBoolean∧and∨operators(orthesetoperators∩and∪),meetandjoinareassociative,commutative,idempotent,andabsorptive.Abounded1.Here,largersetsarelessspeciﬁcandhenceprovidelessinformation.2.Guheetal.(2010),Martinezetal.(2017),andAmiridzeandKutsia(2018)106latticewillincludeauniquebottomelement,“⊥”(inthealgebraofsets,∅;hereauniversallyincompatiblemeaning,“<nil>”),andauniquetopelement,“(cid:62)”(inthealgebraofsets,U;here,anall-embracinggeneralization,“<any>”).{x,y,z}{y,z}{x,z}{x,y}{y}{z}{x}ØFigureA1.1:ABooleanlatticeoversetsandsubsets.Inaformalinﬁxnotation,operatorsonaboundedlatticesatisfytheseidentityaxioms:Idempotence:A∧A=A∨A=ACommutativity:A∧B=B∧A,A∨B=B∨AAssociativity:A∧(B∧C)=(A∧B)∧C,A∨(B∨C)=(A∨B)∨CAbsorptivity:A∧(A∨B)=A∨(A∧B)=ABoundedness:A∧(cid:62)=A,A∨(cid:62)=(cid:62),A∨⊥=A,A∧⊥=⊥If∧and∨areimplementedasfunctions(ratherthanasstructuralfeaturesofaﬁnitedatastructure),A∧B→CandA∨B→Dwillnecessarilysatisfytheunique-inﬁmumandunique-supremumconditions.Commutativitycanbeensuredbyalgorithmicstructure,independentfromlearnedrepresentationsandoperations;idempotence,associativityandabsorptivityperhapscannot.Boundednessisstraightforward.11.Clarketal.(2021)providesamoreextensiveandformalpresentationoflatticesemanticstructureindomainscloselyalignedwiththoseconsideredhere.107A1.3LatticeStructureinNLSemanticsTheﬁtbetweenlatticeordersandNLsemanticstructuresiswellknown,1andtheterm“semanticlattice”hasbeenappliednotonlytowordandsymbol-basedrepresentations,buttoneuralrepresentationsofimages.2InstudiesofNL,both“concepts”and“properties”havebeenmodeledinlatticeframe-works,applicationsthatspeakinfavorofexplicitlatticestructureinNL+frameworks.A1.3.1ModelsofNLSemantics:ALatticeofConceptsFormalconceptanalysis3canrecover“conceptlattice”structuresfromtextcorpora,andthesestructureshavebeenarguedtobefundamentaltoinfor-mationrepresentation.Set-theoreticapproachesassociateformalobjectswithformalattributes,andconstructasubsumptionlatticeoversetsofdeﬁningattributes.Formalconceptanalysishasbeenextendedtofuzzystructuresinwhichpossessionofanattributeisamatterofdegree.4Notethatlatticerelationshipsdependoncontext:Inthecontextofhouse-holds,thejoinof“cat”and“dog”mightbe“pet”,butinthecontextoftaxon-omy,thejoinwouldbe“carnivora”.Inpractice,expressionscontaining“cat”and“dog”wouldbeconsiderednotinisolation,butinsomecontext;inNLP,contextwouldtypicallyberepresenteddynamically,aspartofacomputa-tionalstate;inQNRprocessing,contextcouldbeincludedasanabstractivevectorattribute(seeSection8.3.5).A1.3.2ModelsofNLSemantics:ALatticeofPropertiesPropertiesofthingsmayhavevaluesdistributedoveracontinuousrange,andpropertiesassociatedwithsomethingmaythemselvesspecifynotaprecisevalue,butarangewithintherange:InNL,“lightgray”doesnotdenoteapre-cisecolor,andinferencefromandescriptionofa“lightgrayobject”mayonly1.“Featurestructure”representationsareparticularlyrelevanttoQNRs;Knight(1989)reviewsfeature-structureuniﬁcationandgeneralizationinNLsemantics.2.Tousch,Herbin,andAudibert(2008),Velikovichetal.(2018),andWannenwetschetal.(2019)3.Cimiano,Hotho,andStaab(2005)4.GanterandWille(1997,1999),Cimiano,Hotho,andStaab(2005),Belohlavek(2011),Eppeetal.(2018),andClarketal.(2021)108looselyconstrainitsreﬂectance.Relationshipsamongdescriptionsthatspecifyrangesofpropertiesmaycorrespondtoanintervallattice(SectionA1.5.3).1A1.4Logic,ConstraintSystems,andWeakUniﬁcationTheprevioussectionfocusedonlatticerelationshipsamongindividualen-tities,butsuchentitiescanalsoserveasattributesinexpressionsorgeneralgraphs,enablingthedeﬁnitionofexpression-levellatticeoperations.Latticesoverexpressionsinwhichattributesthemselveshavenon-triviallatticeor-derscanprovidepowerful,tractablerepresentationsinlogicandconstraintprogramming.Computationoversuchrepresentationscanbeextendedtoincludeweakuniﬁcation.A1.4.1LogicalExpressions,LogicProgrammingLogicprogrammingperformsreasoningbasedonsyntacticuniﬁcationofexpression-treesinwhichsymbolsrepresentattributesofleafnodes(variablesorconstants)orinteriornodes(e.g.,functions,predicates,relations,andquantiﬁers).InProlog,expressionsarelimitedtoadecidablefragmentofﬁrst-orderlogic;morepowerfuluniﬁcation-basedsystemsincludeλtermsandcansupportproofinhigher-orderlogics.2Informally,ﬁrst-ordertermsAandBunifytoyieldanexpressionCpro-videdthatallcomponentsofA(subexpressionsandtheirattributes)unifywithcorrespondingfeaturesorvariablesofB;Cistheexpressionthatresultsfromthecorrespondingsubstitutions.Functionandpredicatesymbolsunifyonlywithidenticalsymbols;constantsunifywithvariablesoridenticalcon-stants;variablesunifywith(andintheresultingexpression,arereplacedby)anystructurallycorrespondingconstant,variable,orsubtree.3Asidefromvariablesthatmatchsubtrees,treestructuresmustmatch.Asrequired,uniﬁ-cationoftwoexpressionstheneitherfailsoryieldsthemostgeneralexpressionthatspecializesboth.Informally,expressionsAandBanti-unify,orgeneralize,toyieldCpro-videdthatCcontainsallfeaturesthatAandBshare,andcontainsvariableswhereverAandBdiﬀerinattributesorstructure,orwhereeithercontainsavariable.Cistheunique,mostspeciﬁcexpressionthatcanunifywithany1.Latticescanalsobeconstructedbasedonintervalswithnon-sharpboundaries;seeKehagias(2011)andSingh,AswaniKumar,andLi(2016).2.Paulson(1986)andFeltyandMiller(1988)3.Excludingsubtreesthatcontainthesamesymbolwhencyclicgraphsaredisallowed.109expressionthatcanunifywitheitherAorB.Thus,join/anti-uniﬁcationoftwoexpressionsyieldsthemostspeciﬁcexpressionthatgeneralizesboth.RelevancetoQNRsystems:QNRframeworkscanembed(atleast)ﬁrst-orderlogicexpressionsanden-abletheiruniﬁcation,providedthatsomeattributes(representingconstants,functions,etc.)canbecomparedforequality,whileothers(actingasvari-ables1)aretreatedasfeaturesthatmatchanyleaf-attributeorsubexpression.Accordingly,QNRframeworksaugmentedwithappropriatealgorithmscansupportlogicalrepresentationandreasoning.ThisisatrivialconsequenceoftheabilityofQNRstorepresentarbitraryexpressions,inconjunctionwithfreedomofinterpretationandtheTuringcompletenessofsuitableneuralmodels.Logicalexpressionsandlogicprogrammingare,however,instancesofrichersystems—alsowithinthepotentialscopeofQNRs—thatrepresentconstraintsystemsandsupportconstraintlogicprogramming.A1.4.2ConstraintsandConstraintLogicProgrammingInconstraintlogicprogramming,2representationsareextendedtoincludeconstraintsmoregeneralthanequalityofcomponentsandbindingtouncon-strainedvariables,anduniﬁcationisextendedto(orreplacedby)constraintsatisfaction.Theapplicationofconstraintsnarrowsthevariabledomains,anduniﬁcationfailswhendomainsbecomeempty.Theattributesofconstraintex-pressionshavealatticestructure,asdotheexpressionsthatcontainthem,andconstraintexpressionscanbenarrowedandgeneralizedthoughuniﬁcationandanti-uniﬁcation(YernauxandVanhoof2019).Thepotentialcomplexityofconstraintsspawnsavastmenagerieofcon-straintsystems,algorithms,andconstraintlogicprogrammingalgorithms.Providedthatexpressionscanincludebothsingle-elementdomainsandvari-ablesabletobindsubexpressions,constraintlogicprogrammingcansubsumeconventionallogicprogramming.RelevancetoQNRsystems:Aswithlogicandlogicprogramming,thegeneralityofQNRrepresen-tationsandneuralcomputationimpliestheabilitytorepresentconstraint1.ConvergentarcsinDAGexpressionsmodelthebehaviorofnamedvariablesthatoccurinmultiplelocations.Itshouldbenotedthatuniﬁcationcanproduceandoperateoncyclicgraphsunlessthisisspeciﬁcallyexcluded;seeSmolka(1992).2.JaﬀarandMaher(1994)110systemsandconstraint-basedcomputation,includingconstraintlogicpro-gramming.Thegeneralizationfromlogictoconstraintsisimportanttoseman-ticrepresentationalcapacity:Expressionscanoftenbeinterpretedasdenotingregionsinasemanticspace,1andcombinationsofexpressionscancombineconstraints.Constraintlogicprogrammingprovidesanexactformalmodelofcomputationbasedonthissemanticfoundation.A1.4.3WeakandSoftLatticeOperations“Neverexpressyourselfmoreclearlythanyouareabletothink”—NielsBohrTheliteraturedescribesarangeofmodelsofNLsemanticsandreasoningbasedonarangeofapproximateuniﬁcationoperations.Thesetypicallyre-placeequalityofconstants(functions,etc.)withsimilarity:In“softuniﬁcation”(asthetermistypicallyusedintheliterature)theoperationsucceedsifsimi-larity(forexample,cosinesimilaritybetweenvectors,Arabshahietal.(2021))isabovesomethreshold,andsuccessmayyieldeitherconventionalbindingofvariablestovalues(Camperoetal.2018),ormergedrepresentationsofvalues(CingilliogluandRusso2020).“Weakuniﬁcation”mayproducea“uniﬁcationscore”thatindicatesthequalityofuniﬁcation;thesescorescanbecarriedforwardandcombinedtoscorethequalityofmulti-stepinferenceoperations.2Asusedinthepresentcontext,theterm“softuniﬁcation”subsumesboth“weak”and“soft”uniﬁcationasusedintheliterature,andentailscombiningrepresentationsofvaluesinawaythatapproximatesuniﬁcationoperationsinconstraintlogicprogramming.Thus,“softness”allowsoperationsthatviolatestrictlatticeidentities.3TheintendedclassofQNR(soft-)uniﬁcation1.Forexample,rangesofcompatiblemeaningswithrespecttovariousproperties,implyingwhatareineﬀectintervalconstraintsonthoseproperties,afamiliar(ifperhapstoorigid)constraintstructure(seeBenhamou1995).2.E.g.,inSessa(2002),Medina,Ojeda-Aciego,andVojtáš(2004),Weberetal.(2019),andMinervinietal.(2020).3.Inaddition,latticeoperationsmaybemixed:Incombininginformation,diﬀerencesofsomekindsshouldleadtorejectionornarrowing,whilediﬀerencesofotherkindsshouldleadtogeneralization.Forexampleifwearecombiningpiecesofevidenceaboutacat(perhapsfromtwophotographs),somepropertiesshouldbeuniﬁed(picturesofspotsthatdiﬀeronlyinvisibilityandangleofviewshouldnarrowpossiblemodelsofcoloration,whileadiﬀerenceoforangevs.blackshouldleadtofailureandrejectthehypothesis“samecat”).Bycontrast,ifonephotoshowsasleepingcatandtheotheranalertcat,thecombineddescriptionshouldrepresentacatthatisnotalwaysasleep.Diﬀerencesbetweenkindsofdiﬀerencesshouldbelearnedfromandconditionedontasks.111operationsfollowsconstraintlogicprogrammingingeneralizingfromthebindingofconstantstonamed(ineﬀect,shared)unconstrainedvariablestoincludethenarrowingofshared(ineﬀectnamed),potentiallyconstrainedattributes.Asinlogicprogramming,QNRuniﬁcationwillpermittheuni-ﬁcationofsubexpressionswithvariable-likeattributes,butdiﬀersinthatconstrainedattributes(unlikevariables)mayimposesemanticallynon-trivialconstraintsonpermissibilityandonthecontentofresultingsubexpressions(SectionA1.6.4).A1.5ExactLatticeOperationsonRegionsAlthoughembeddingsinvector/graphrepresentationsdenotepointsinse-manticspaces,theirsemanticinterpretationswilltypicallycorrespondtoregionsinlower-dimensionalspaces.ComparisonstosymbolicandmoregeneralconstraintrepresentationscanprovideinsightsintopotentialQNRrepresentationsandreasonsforexpectingtheirlatticepropertiestobeinexact.A1.5.1ConventionalSymbolicExpressionsInconventionalexpressiongraphs,attributescomprisesymbolsthatrepresentpointstogetherwithsymbolsthatrepresentunboundedregionsinthespaceofexpressions.Thus,individualattributesubspacesaresimple,havenospatialstructure,andaccordinglyexhibittrivialbehaviorunderuniﬁcationandanti-uniﬁcation.A1.5.2GeneralRegionRepresentationsandOperationsLeafattributescanrepresentregionsinRn,butoptionsfortheiruniﬁca-tionandgeneralizationmayberepresentation-dependent.Theconceptuallystraightforwarddeﬁnitionisbothtrivialandproblematic:Treatingregionsassetsofpoints(A∨B=A∪B)ineﬀectdiscardsspatialstructure,andwithitthepotentialfornon-trivialgeneralization.Further,ifregionrepresentationshavelimiteddescriptivecapacity,thentheresultofgeneralizingapairofattributesbysetunioncannotingeneralberepresentedasanattribute.1Alternatively,thegeneralizationoftwovolumesmightbedeﬁnedastheirconvexhull.Generalizationofconvexregionsyieldsconvexregions,andinclusionofpointsoutsidetheinitialvolumesreﬂectsspatialstructureanda1.Considertheunionofdisjointvolumes,eachalreadyatthelimitofrepresentablecom-plexity.Intersectionscan(butdonotnecessarily)suﬀerfromasimilardiﬃculty.112plausiblenotionofsemanticgeneralization.Unfortunately,thisdeﬁnitioncanalsofallafouloflimiteddescriptivecapacity,becausetheconvexhulloftworegionscanbemorecomplexthaneither.1A1.5.3Interval(Box)RepresentationsandOperationsThereareregion-representationsforwhichlatticeoperationsareexact,forexample,one-dimensionalintervalsinR2andtheirgeneralizationtoaxis-alignedboxesinRn.3Uniﬁcationofapairofbox-regionsyieldstheirintersec-tion;anti-uniﬁcationyieldsthesmallestboxthatcontainsboth.Axis-alignedboxregionscanberepresentedbyvectorsoftwicethespatialdimensionality(forexample,bypairingintervalcenterswithintervalwidths),andlatticeoperationsyieldrepresentationsofthesameform.Interval-valuedattributeshavebeenappliedinconstraintlogicprogramming.4Generalizationthroughanti-uniﬁcationofintervalshasanaturalsemanticinterpretation:Pointsinagapbetweenintervalsrepresentplausiblemembersoftheclassfromwhichtheintervalsthemselvesaredrawn.Box regionsMeetsJoins<nil>FigureA1.2:Exactmeetsandjoinsofinterval(box)regions.1.Toillustrate,theconvexhulloftwospheresneednotbeasphere,andtheconvexhulloftwopolytopesmayhavemorefacetsthaneither.Intersectionscanalsobecomemorecomplex(seeJaulin2006).2.DiscussedforexampleinClarketal.(2021).3.Aﬃne(andother)transformationsofaspaceanditsregionscanofcoursemaintaintheseproperties.4.BenhamouandOlder(1997)andOlderandVellino(1990)113A1.6ApproximateLatticeOperationsonRegionsIfboxlatticesareexact,whyconsiderapproximateoperationsonmoregeneralregions?Thebasicintuitionisthatrepresentationsentailtrade-oﬀs,andthatgreaterﬂexibilityofformisworthsomedegreeofrelaxationintheprecisionofuniﬁcationandgeneralization.Boxeshavesharpboundaries,ﬂatfacets,andcorners;naturalsemanticrepresentationsmaynot.Intervalsinaspaceofpropertiesmaycorrespondtonaturalsemanticconcepts,yetorthogonalityandglobalalignmentofaxesmaynot.Inthepresentcontext,itisimportanttodistinguishtwokindsofap-proximation:AsdiscussedinSectionA3.4,eﬀectiveQNRframeworksmustbeabletoexpress,notonlyprecisemeanings,butrangesorconstraintsonmeaning—akindofapproximationinthesemanticdomainthatdiﬀersfromapproximationoflatticeproperties.Rangesofmeaningscanberepresentedasregionsinsemanticspaces,whileregion-representationsthatapproximateprecisemeaningscanpreciselysatisfythelatticeaxioms.Regions~Meets~Joins<nil>FigureA1.3:Approximatemeetsandjoinsofregionsfromaless-constrainedfamilyofregionshapes.A1.6.1Continuous-ValuedFunctionsInaddition,generalconsiderationsmotivaterepresentingthemembershipofpointsinsemanticclasseswithcontinuous-valuedfunctions,ratherthanfunctionshavingvaluesrestrictedto{1,0}.Suchrepresentationsinvitefurtherapproximationsoflatticeproperties.114A1.6.2RegionShapesandAlignmentItisnaturaltoexploittheﬂexibilityofneuralnetworkfunctionstorepresentgeneralizedregionshapes,andtousetheabilityoffullyconnectedlayerstofreerepresentationsfrompreferredaxisalignmentsandtherebyallowexploitationoftheabundanceofnearlyorthogonaldirectionsinhighdimen-sionalspaces.Learnedattributerepresentationsneednotdescribebox-likeregionsorsharemeaningfulalignmentindiﬀerentregionsofasemanticspace.A1.6.3SatisfactionofLatticeAxiomsasanAuxiliaryTrainingTaskGiventhevalueoflatticestructureinrepresentations,itisnaturaltospeculatethatpromotinglatticestructurethroughinductivebiasmayaidperformanceinarangeofsemantictasks.Auxiliarytrainingtasksinwhichlossesexplicitlymeasureviolationsoflatticepropertiesmaythereforebeusefulcomponentsofmultitasklearning.1A1.6.4UnifyingAttributeswithExpressionsAsnotedabove,conventionalsymbolicexpressionsallowuniﬁcationofun-constrainedvariableswithsubexpressions,whileintheQNRcontext,itisnaturaltoseektounifysubexpressionswithconstrainedvariables—attributesthatrepresentsemanticregions.Theoutlinesofdesirablebehaviorareclear,atleastinsomemotivatingcases:ConsiderapairofgraphswithsubexpressionsAandBincorrespondinglocations.LetAbeavectorrepresentation(e.g.,describingagenericgrey-stripedanimalwithsharpclaws),whileacorrespondingsubexpressionBisavector/graphrepresentation(e.g.thatcontainscomponentsthatdescribeacat’stemperament,ancestry,andappearance).Uniﬁcationshouldyieldavector/graphrepresentationinwhichthepropertiesdescribedbyvectorAconstrainrelatedpropertiesdescribedanywhereinexpressionB(e.g.,thecat’sappearance,paws,andsomeaspectsofitsancestry)IfsomecomponentofBspeciﬁesablackcat,uniﬁcationfails.Inthisinstance,generalizationshouldyieldavectorrepresentationthatdoesnotclashwithpropertiesdescribedineitherAorB,whilediscardingpropertiesthatarenotshared.1.Notethatadherencetolatticepropertiesinrepresentationalvectorspacesisimportantonlyinthoseregions/manifoldsthatareactuallyusedforrepresentation.115A1.6.5LearningLattice-OrientedAlgorithmsClassicalgorithmsforuniﬁcationandgeneralizationimplementparticularpatternsofinformationﬂow,intermediaterepresentations,anditerative,con-ditionalcomputation.Workonsupervisedneuralalgorithmiclearningil-lustratesonepotentialapproachtoadaptingsuchalgorithmstoneuralcom-putation,anapproachthatsupervisesthelearningofalgorithmicstructurewhileallowingend-to-endlearningofrichrepresentationsandcorrespondingdecisioncriteria.1A1.7SummaryandConclusionsLatticestructureisfoundinformalsystemsand(tosomeextent)inNL,anditseemsbothnaturalanddesirableinQNR/NL+representations.AlthoughlatticestructureisnotacriterionforupgradingNLtoNL+representations,substantialadherencetolatticestructurecouldpotentiallyimproveexpressivecapacityinasystemicsenseandneednotimplytherigidityoffullyformalrepresentations.Becauselinguisticrepresentationsandreasoningarethemselvesapproxi-mate,thereseemslittlereasontosacriﬁcerepresentationalﬂexibilityinordertoenforceexactanduniversalsatisfactionofthelatticeaxioms.AQNRframe-workthatembracesbothpreciseandapproximatelatticerelationshipscanenablebothformalandinformalapplicationsofthoserelationshipstofor-malandinformalreasoning.Theliteraturesonconventionalandconstraintlogicprogrammingillustratethepowerandcomputationaltractabilityofalgorithmsbasedonsystemsofthiskind.Thepotentialbeneﬁtsofapproximatelatticestructuremayemergespon-taneously,butcanalsobepursuedbyinductivebias,includingtrainingthatemployssatisfactionoflatticeaxiomsasanauxiliarytaskinmultitasklearn-ing.1.SeeVeličkovićandBlundell(2021)andincludedreferences.116A2Tense,Aspect,Modality,Case,andFunctionWordsTablesofexamplesillustrateexpressiveconstructsofnaturallanguagesthatdonotreducetonouns,verbs,andadjectives.Naturallanguagesexpressarangeofmeaningsthroughclosed-class(“func-tion”)words,andexpressdistinctionsoftense,aspect,modality,andcasethoughbothfunctionwordsandmorphologicalfeatures.Section5.3discussestherolesandimportanceoftheseconstructs;thisappendixprovidesseveralbrieftablesofexamples.TableA2.1:ClassesandExamplesofFunctionwords.Theexamplesbelowcoveraboutonethirdofthefunction-wordvocabularyoftheEnglishlanguage.Instronglyinﬂectedlanguages,therolesofsomeofthesefunctionwordsareperformedbymorphologicaldistinctions.Determiners:the,a,this,my,more,eitherPrepositions:at,in,on,of,without,betweenQualiﬁers:somewhat,maybe,enough,almostModalverbs:might,could,would,shouldAuxiliaryverbs:be,do,got,haveParticles:up,down,no,not,asPronouns:she,he,they,it,one,anyoneQuestionwords:who,what,where,why,howConjunctions:—coordinating:for,so,and,nor,but,or,yet—subordinating:if,then,thus,because,however—temporal:before,after,next,until,when,ﬁnally—correlative:both/and,either/or,not/but117TableA2.2:ExamplesofTense/AspectDistinctions.Languagescanuseinﬂectionorfunctionwordstoexpressdistinctionsthatdescribe(e.g.)relativetime,duration,orcausation.Languagesdiﬀerinthedistinctionsthattheycancompactlyexpress,whilepropertieslike“remoteness”,“completion”,and“causation”invitecontinuousrepresentations.)Perfecttenses—completedinpast,present,orfuture(“had/has/willhave”ﬁnished)Continuoustenses—ongoinginpast,present,orfuture(“was/am/willbe”working)Pastperfectcontinuous—previouslyongoinginthepast(“hadbeenworking”)Futureperfectcontinuous—previouslyongoinginthefuture(“willhavebeenworking”)Remoteperfect—completedintheremotepast(Bantulanguages1)Resultativeperfect—completedpastactioncausingpresentstate(Bantulanguages)TableA2.3:ExamplesofModalityDistinctions.Languagescanexpressmodalitiesbyinﬂectionorfunctionwords.Theexistenceofgradeddegreesandoverlapswithinandbetweenmodalitiessuggeststhepotentialvalueofcontinuousvector-spacerepresentations.Interrogative—QuestionImperative—CommandIndicative—UnqualiﬁedstatementoffactInferential—Qualiﬁed(inferred)factSubjunctive—TentativeorpotentialfactPotential—PossibleconditionConditional—PossiblebutdependentonanotherconditionHypothetical—PossiblebutcounterfactualconditionOptative—DesiredconditionDeontic—Idealorpropercondition1.Bantulanguagesincludeunusuallycomplexandexpressivesystemsoftenseandaspect(NurseandPhilippson2006;BotneandKershner2008).118TableA2.4:ExamplesofCaseDistinctions.Casedistinctionscanexpresstherolesofwordsinasentence(inafamiliargrammaticalsense)ortherolesofwhattheydenoteinasituation.Thenumberofinﬂectionalcasedistinctionsvarieswidelyamonglanguages;Englishhasthree,Tsezhasdozens,manyofwhicharelocative.Aswithmodalities,blurredboundariesandoverlapsbetweencasessuggestthepotentialvalueofcontinuousvector-spacerepre-sentations.Nominative—subjectofaverbAccusative—objectofaverbDative—indirectobjectofaverbGenitive—relationshipofpossessionComitative—relationshipofaccompanimentLative—movementtosomethingAblative—movementawayfromsomethingOrientative—orientationtowardsomethingLocative—location,orientation,directionTranslative—becomingsomethingInstrumental—meansusedforanactionCausal—causeorreasonforsomethingBenefactive—beneﬁciaryofsomethingTerminative—limitorgoalofanaction119A3FromNLConstructstoNL+Condensing,regularizing,andextendingthescopeofsemanticrepresen-tationscanimproveexpressivecapacityandcompositionality,andcansupporttheoreticallygroundedmethodsforcomparingandcombiningsemanticinformation.Section6discussedQNRarchitecturesasaframework,consideringpotentialcomponents,syntacticstructures,andtheirsemanticroles.Thepresentsec-tionextendsthisdiscussiontoexploreinmoredetailhowanticipatedQNRframeworkscouldsubsumeandextendtheexpressivecapabilitiesofnaturallanguagestofulﬁllthecriteriaforNL+.Keyconsiderationsincludefacilitatingrepresentationlearning,upgradingexpressiveness,improvingregularity,andenablingmoretractablereading,interpretation,andintegrationofcontentatscale.PotentialNL/NL+relationshipsdiscussedherearenotproposalsforhand-craftedrepresentations,noraretheystrongorconﬁdentpredictionsoftheresultsofrepresentationlearning.TheaimisinsteadtoexplorethescopeandstrengthsofQNRexpressivecapacity,withpotentialimplicationsfordesignchoicesinvolvingmodelarchitectures,inductivebias,andtrainingtasks.WhereneuralrepresentationlearningprovidesNL+functionalitybydiﬀerentmeans,weshouldexpectthosemeanstobesuperior.A3.1UpgradingSyntacticStructureTobeﬁtforpurpose,NL+syntacticstructuresmustsubsumeandextendtheirNLcounterpartswhileimprovingcomputationaltractability:•TosubsumeNLsyntacticstructures,NL+frameworkscanembedNLsyntax;asalreadydiscussed,thisisstraightforward.•ToextendNLsyntacticstructures,NL+frameworkscansupportaddi-tionalsyntacticstructure;asalreadydiscussed,thiscanbeuseful.•Toimprovetractabilityinlearningandinference,NL+frameworkscanimprovesemanticcompositionality,locality,andregularity.ThispotentialNL/NL+diﬀerentialwillcallforcloserexamination.120A3.1.1MakingSyntacticStructureExplicitExplicitrepresentationsavoidthecomputationaloverheadofinferringstruc-turefromstrings,aswellascosts(andpotentialfailures)ofnon-localdisam-biguation,e.g.,byusingDAGstorepresentcoreference.Improvedlocalityandtractabilityfollow.A3.1.2ExtendingtheExpressiveScopeofSyntacticStructureExplicitgraphscanenabletheuseofstructuresmorediverseandcomplexthanthoseenabledbynaturallanguageandaccessibletohumancognition.1Writersgrapplingwithcomplexdomainsmayemploysupplementaryrep-resentations(diagrams,formalnotation),ormaysimplyabandonattemptstoexplainsystemsandrelationshipsthatinvolvedeeplynestedstructures,heterogeneouspatternsofepistemicqualiﬁcation,andcomplexpatternsofcoreference—allofwhichmustinNLbeencodedandprocessedassequences.Moregeneralrepresentationscandirectlydescriberelationshipsthataremorecomplexyetreadilyaccessibletomachines.A3.1.3ExtendingtheConceptof“SyntacticStructure”“Naturallanguagesyntax”asunderstoodbylinguistsisofteninpracticesup-plementedwithvisuallyparsablestructuressuchasnestedtext(outlines,structureddocuments)andtablesthatrepresentgridsofrelationships.Dia-gramsmayembedtext-labelsasattributesofgraphsthatrepresentnetworksoftypedrelationships(taxonomy,control,causation,enablement,etc.);programcodeisadjacenttoNL,yetgoesbeyondNLconceptsofsyntax.Implemen-tationsthatsupportexplicitbidirectionallinkscanmodelthestructureofliteraturesthatsupportnotonly“cites”butalso“citedby”relationships.A3.1.4CollapsingSyntacticStructureInneural-networkcomputation,vectoroperationsarebasic,whilegraphoperationsmayimposeadditionalcomputationaloverheads.Thismotivatestheuseofembeddingsinpreferencetographexpressions,2whichinturn1.Whatisinsomesenseaccuratelinguisticencodingdoesnotensuresuccessfulcommuni-cation:Forexample,asaconsequenceofworking-memoryconstraints,humanssuﬀerfrom“severelimitations”insentencecomprehension(Lewis,Vasishth,andVanDyke2006).2.Whenfeasible,ofcourse.Anotherreasonisthenatural(andoftensemanticallyappropri-ate)commutativityimplicitinvectorrepresentationsviewedassumsofcomponents.121highlightsthevalueofhighlyexpressivelexical-levelunits.Inaddition,encodingdiﬀerentmeaningsasdiﬀerencesinsyntacticstructurecanimpedealignmentandcomparisonofexpressions;WhenindividualembeddingscanexpressarangeofmeaningsthatNLwouldrepresentwithdiﬀerentsyntacticstructures,thosesyntacticdiﬀerencesdisappear.A3.2UpgradingLexical-LevelExpressiveCapacityWherepossible,wewouldliketoreplacethesyntacticcompositionalityofwordswiththesimpler,arithmeticcompositionalityofvectors.Becausethebestsyntaxisnosyntax,itisworthconsideringwhatkindsofsemanticcontentcanberepresentedinvectorspaceswithoutrelyingongraphstructure.ThefollowingdiscussionnotesseveralkindsofsemanticstructurethatexistinNL,canberepresentedbyembeddings,andhaveemerged(spontaneouslyandrecognizably)inneuralmodels.A3.2.1SubsumingandExtendingContent-WordDenotationsAscarefulwritersknow,thereoftenisnosinglewordthataccuratelyconveysanintendedmeaning,whiletounpackanintendedmeaningintomultiplewordsmaybetoocostlyinword-countorcomplexity.Lexical-levelembed-dingscanshifttheambiguity/verbositytrade-oﬀtowardexpressionsthatarebothlessambiguousandmoreconcise.1Embeddingscanplacecontent-wordmeaningsinspaceswithusefulseman-ticstructure.Wordembeddingsdemonstratelearnablestructureinlexical-levelsemanticspaces.Thegeometryoflearnedwordembeddingscanrepresentnotonlysimilarity,butanalogy,2andrepresentationsofsemanticdiﬀerentialsacrossvocabularies3canbeidentiﬁedinlanguagemodels.Unfortunately,theroleofNLwordembeddings—whichmustrepresentpolysemous,context-dependentwords—precludescleanrepresentationofword-independentsemanticstructure.Vectorspacesthatrepresentmean-ingsratherthanwordscanprovidesemanticstructurethatismorereliable1.Conciseexpressionisofcourselessnecessaryinthecontextofindefatigablemachineintelligence.2.AsdiscussedinChen,Peterson,andGriﬃths(2017).3.DanielKahneman’sdoctoraldissertationexaminessemanticdiﬀerentials(Kahneman1961).SemanticstructureembeddableinvectorspaceshasbeenextensivelystudiedbybothlinguistsandMLresearchers(e.g.,seeMessick(1957),Sagaraetal.(1961),Hashimoto,Alvarez-Melis,andJaakkola(2016),andSchramowskietal.(2019)).122anduseful,hencepatternsobservedinvectorrepresentationsofnaturallan-guageprovideonlylimitedinsightintothepotentialgeneralityandutilityofsemanticallystructuredvectorspaces.Embeddingscandisambiguate,interpolate,andextendvocabulariesofcon-tentwords.BecausepointsinanembeddingspacecouldbeusedtodirectlydesignateeverywordineveryNLvocabulary—withvastcapacitytospare—embeddingscanbestrictlymoreexpressivethanNLwords.AgaintakingNLasabaseline,embeddingsoﬀertheadvantageofavoidingpolysemy,maladaptivewordambiguity,1andalarge(evencomprehensive?)rangeofrecognizablymissingwordmeanings(thefrustrating-thesaurusproblem).Insuitablesemanticspaces,pointswithinandbeyondtheroughequivalentsofNLwordclusterscan,ataminimum,interpolateandextendtheroughequivalentsofNLvocabularies.Embeddingscanextendvocabulariesbyfoldingmodiﬁer-expressionsintonounandverbspaces.Theabilitytocollapsearangeofmulti-wordexpressionsintoembeddingsisequivalenttoextendingvocabularies:Inastraightforwardexample,adjec-tivesandadverbscanmodifythemeaningsofnounsandverbstoproducediﬀerentlexical-levelmeanings.Thesemodiﬁersoftenrepresentcross-cuttingproperties2thatcandescribethingsandactionsacrossmultiple(butnotall)domains.AlthoughwordswithmodiﬁerscanbeviewedasextensionsofNLvocabularies,expressionsoflimitedsizenecessarilyleavegapsinsemanticspace;continuousvectorembeddings,bycontrast,canﬁllregionsofsemanticspacedensely.Asaconsequenceoftheaboveconsiderations,afunctionoftheformNL-encode:(lexical-NL-expression)→(lexical-embedding)canexist,butnotitsinverse.NL-encodeisneitherinjectivenorsurjective:MultipleNLexpres-sionsmaybeequivalent,andtypicalNL+expressionswillhavenoexactNLtranslation.Directionsinembeddingspacescanhaveinterpretablemeanings.LinguistsﬁndthatNLwordscanwithsubstantialdescriptiveaccuracybepositionedinspacesinwhichaxeshaveconceptualinterpretations3or1.Andconversely,maladaptiveprecisionintheformof(forexample)forcednumberandgenderdistinctions.2.E.g.,color,mass,temperature,frequency,speed,color,loudness,age,beauty,anddanger.3.Gärdenfors(2000)andLieto,Chella,andFrixione(2017)123reﬂectsemanticdiﬀerentials.InNL,modiﬁerscommonlycorrespondtodisplacementswithcomponentsalongthesesameaxes.Itisnaturaltointerpretmodiﬁer-likevectorcomponentsdiﬀerentlyindiﬀerentsemanticdomains.1Asnotedpreviously,theinterpretationofdirec-tionsina(sub)spacethatcorrespondstodiﬀerentialsapplicabletoentitieswouldnaturallydependonlocation—onwhichregionsofa(sub)spacecorrespondtowhichkindsofentities.2Inasemanticregionthatdescribespersons,forexample,directionsinasubspacemightexpressdiﬀerencesinhealth,tem-perament,age,andincome,whileinasemanticregionthatdescribesmotors,directionsinthatsamesubspacemightexpressdiﬀerencesinpower,torque,size,andeﬃciency.Lexicallevelandsyntacticcompositionalityarecomplementary.Assuggestedabove,vectoradditionofmodiﬁersandotherdiﬀerentialscanexpresscompositionalsemanticswithinembeddings.Exploitingthiscapacitydoesnot,ofcourse,precludesyntacticcompositionality:QNRscansupportcompositionalrepresentationsthroughbothvector-spaceandsyntacticstruc-ture.Withoutpredictingorengineeringspeciﬁcoutcomes,wecanexpectthatneuralrepresentationlearningwillexploitbothmechanisms.A3.2.2ImageEmbeddingsIllustrateVectorCompositionalityinSeman-ticSpacesImageembeddingscanprovideparticularlyclear,interpretableexamplesoftheexpressivepower—andpotentialcompositionality—ofcontinuousvectorrepresentations.(Section9.2discussesimageandobjectembeddings,notasexamplesofrepresentationalpower,butasactuallexicalunits.)Humansareskilledinperceivingsystematicsimilaritiesanddiﬀerencesamongfaces.Diversearchitectures(e.g.,generativeadversarialnetworks,vari-ationalautoencoders,andﬂow-basedgenerativemodels3)canproducefaceembeddingsthatspontaneouslyformstructuredsemanticspaces:Thesemod-elscanrepresentfacesinhigh-dimensionalembeddingspacesthatrepresent1.Adjectivemeaningshavebeenmodeledasfunctionsofnouns(BaroniandZamparelli(2010);seealsoBlacoeandLapata(2012)).2.Settingaside,forthemoment,usefulrelationshipsbetweenthesediﬀerentialsanddiﬀer-encesofkind.Inpractice,descriptionsofbothkindsandpropertiescannaturallybefoldedintoasingleembedding,withnoneedtoexplicitlyorcleanlyfactorrepresentationsintokind-andproperty-spaces.3.Klys,Snell,andZemel(2018),KingmaandDhariwal(2018),R.Liuetal.(2019),andShenetal.(2020)124kindsofvariationsthat(qualitatively,yetclearly)arebothrecognizableandsystematic.1Manypaperspresentrowsorarraysofimagesthatcorrespondtooﬀsetsalongdirectionsinembeddingspaces,andthesenaturallyemphasizevariationsthatcanbenamedincaptions(gender,age,aﬀect...),butacloserexaminationoftheseimagesalsorevealssystematicvariationsthatarelessreadilydescribedbywords.Wordsorphrasesofpracticallengthcannotdescribeordinaryfacessuchthateachwouldberecognizableamongmillions.Asingleembeddingcan.2Similarpowercanbebroughttobearinawiderrangeoflexical-levelrepre-sentations.3A3.3SubsumingandExtendingFunction-Word/TAM-CSemanticsAsnotedinSection5.3.4TAM-Cmeaningscanbeencodedineitherwordmorphologyorfunction(closed-class)words.SomeTAM-Cmodiﬁersaresyntacticallyassociatedwithlexical-levelunits;othersareassociatedwithhigher-levelconstructs.TAM-Cmodiﬁersthatrepresentcase(e.g.,nominative,accusative,instru-mental,benefactive;seeTableA2.4)candirectlydescribetherolesofthingsdenotedbywords(e.g.,actingvs.acteduponvs.used),butcasealsocanindirectlymodifythemeaningofaword—arockregardedasageologicalobjectdiﬀersfromarockregardedasatool.4OtherTAM-Cmodiﬁersexpresssemanticfeaturessuchasepistemiccon-ﬁdence,sentiment,anduse/mentiondistinctions.InNL,statement-levelmeaningsthatarenotcapturedbyavailableTAM-Cmodiﬁersmaybeemer-gentwithinanexpressionorimpliedbycontext;bycompactlyanddirectlyexpressingmeaningsofthiskind,expression-levelembeddingscanprovideaﬀordancesforimprovingsemanticlocality,compositionality,andclarity.51.AgoodrecentexampleisShenetal.(2020),whichﬁndsthatdiversefacescanbewell-representedin100-dimensionalspaces(Härkönenetal.2020).2.Inprinciple,todistinguishamongmillionsoffacesrequiresdistinguishingontheorderof10gradationsoneachof6dimensions,buttypicalembeddingsarefarricherinbothdistinctionsanddimensionality.3.Withinthedomainofconcrete,interpretableimages,~100dimensionalembeddingscanrepresentnotonlyfaces,butalsodiverseobjectclassesandtheirattributes,therebyrepresenting(ineﬀect)interpretable“noun-and-adjective”combinations,fewofwhichcanbecompactlyandaccuratelydescribedinNL;e.g.,seeHärkönenetal.(2020).4.Istherockhardandsharp,orapieceofﬁne-grainedultramaﬁcbasalt?5.Ontheinternet,emojihaveemergedtocompactlyexpressexpression-levelsentiment(e.g.,and),andthesecanexpressmeaningsdistributedovermorethanonedimension(consider,,and).125Somefunctionwordsconnectphrases:Theseincludewordsandcombinationsthatexpressarangeofconjunctiverelationships(and,or,and/or,therefore,then,still,however,because,despite,nonetheless...)andcapability/intentionrelatedrelationships(can,could,should,would-if,could-but,would-if-could,could-but-shouldn’t...).Considerationoftherolesofthesewordsandcon-structswillshowthattheirmeaningsaredistributedoversemanticspaces,andtheaboveremarksregardingtheuseofembeddingstointerpolateandextendNLmeaningsapply.1InNL,tense/aspectmodiﬁersexpressdistinctionsintherelativetimeanddurationofevents,andbecausethesemodiﬁersreferenceacontinuousvariable—time—theycannaturallybeexpressedbycontinuousrepresenta-tions.Likewise,epistemiccasemarkersinNL(indicative,inferential,poten-tial)implicitlyreferencecontinuousvariablesinvolvingprobability,evidence,andcausality.Notethatmuchoffunction-word/TAM-Cspacerepresentsneitherkindsnorpropertiesofthings,andissparselypopulatedbyNLexpressions.TheuseofembeddingstointerpolateandextendmeaningsintheseabstractsemanticrolescouldgreatlyimprovetheexpressivecapacityofQNR/NL+frameworksrelativetonaturallanguages.A3.4ExpressingQuantity,Frequency,Probability,andAmbiguityDiscreteNLconstructsexpressarangeofmeaningsthataremorenaturallyexpressedincontinuousspaces:Theseincludenumber,quantity,frequency,probability,strengthofevidence,andambiguityofvariouskinds.NLcanexpressspeciﬁccardinal,ordinal,andrealnumbers,andabsoluteconceptssuchasnoneorall,butmanyotherusefulexpressionsareeithercrude(grammaticalsingularvs.pluralforms)orambiguous(e.g.,several,few,many,some,most,andalmostall,orrarely,frequently,andalmostalways).Notethatintentionalambiguityisuseful:Fewdoesnotdenoteaparticularnumber,butarangeof“small”numbers,eitherabsolute(about2to5,Munroe(2012))orrelativetoexpectationsregardingsomesetofentities.Thisrangeofmeanings(likewiseforunlikely,likely,possibly,almostcertainly,etc.)invitescontinuousrepresentationsinQNRframeworks.21.E.g.,embeddingsthatgeneralizeNLconjunctive/causalexpressionscouldpresumablyexpressmeaningslike“objectX,(probably)togetherwithand(possibly)becauseofY”,anddosowithgradeddegreesofprobabilityorepistemicconﬁdence.2.ItisnaturaltowantsemanticspacesthatexpressjointprobabilitydistributionsaswellasrelationshipsinPearl’sdo-calculus;theblurrydistinctionbetweentheseandtheNL-likesemanticspacesoutlinedabovepointstothesoftboundariesofNL-centricconceptionsofNL+.126Similarremarksapplytoqualitativeandprobabilistichedges(mostly,par-tially,somewhat,tosomeextent)qualiﬁersandoftenagent-centeredepistemicqualiﬁers(presumably,ifIrecallcorrectly,itseemstome,asfarasIknow,inmyopinion,etc.).1Onewouldalsoliketobeabletocompactlyexpressquali-ﬁerslikeillustrativebutcounterfactualsimpliﬁcation,approximatedescriptionofatypicalcase,andunqualiﬁedstatementbutwithimpliedexceptions:Today,theabsenceofuniversalidiomsforexpressingthesemeaningsgivesrisetogigabytesoffruitless,argumentativenoiseininternetdiscussions.Thediscussionaboveimplicitlyframestheexpressionofambiguity(etc.)asataskforlexicalunitsinphrases,followingtheexampleofNL.Thereareadvantages,however,tofoldingambiguity(etc.)intoembeddingsthatrepresent,notpoints,butregionsinasemanticspace.Ashiftfrompoint-toregion-orientedsemanticsallowssystemsofrepresentationsthatcanap-proximatemathematicallattices(AppendixA1)andlattice-basedinferencemechanismslikePrologandconstraintlogicprogramming(SectionA1.4).Thesemechanisms,inturn,providesemi-formalapproachestomatching,uniﬁcation,andgeneralizationofrepresentations,withapplicationsoutlinedbelowandexploredfurtherinSection8.4.3.A3.5FacilitatingSemanticInterpretationandComparisonRelativetoNL,NL+frameworksoﬀerpotentialadvantagesthatinclude1.Greaterexpressivecapacity2.Moretractableinterpretation3.Moretractablecomparison.Theprecedingsectionshavediscussedadvantagesoftype(1)thatstemlargelyfromimprovementsatthelexicallevel.Thepresentsectionwillconsiderhowcondensation,localization,regularizationofexpression-levelrepresentationscanprovideadvantagesoftypes(2)and(3).Acentralthemeistheuseoftractable,uniform,embedding-basedrepresentationstoprovideexpressivecapacityofkindsthat,inNL,areembodiedinlesstractable—andoftenirregular—syntacticconstructs.1.Expressionsthatarefrequentlyreducedtoabbreviationsarelikelytorepresentbroadlyuseful,lexical-levelmeanings:IIRC,ISTM,AFAIK,IMO,etc.127A3.5.1ExploitingCondensedExpressionsAsnotedabove,embeddingscancondensemanynoun-adjective,verb-adverb,andfunctionwordconstructs,facilitatinginterpretationbymakingtheirsemanticcontentavailableinformsnotentangledwithsyntax.Further,embeddingscanbecomparedthroughdistancecomputations,1potentiallyafterprojectionortransformationintotask-andcontext-relevantsemanticspaces.TheseoperationsarenotdirectlyavailableinNLrepresentations.A3.5.2ExploitingContentSummariesContentsummaries(Section8.3.4)cancacheandamortizetheworkofin-terpretingexpressions.Thevalueofsummarizationincreasesasexpressionsbecomelarger:Anagentcanreadabook(hereconsideredan“expression”)toaccessthewholeofitsinformation,butwilltypicallypreferabookaccompa-niedbyasummaryofitstopic,scope,depth,quality,andsoon.Semanticallyoptionalsummaries(perhapsofseveralkinds)canfacilitatebothassociativememoryacrosslargecorpora(Section9.1.2)andshallowreading(“skimming”)ofretrievedcontent.Shallowreading,inturn,canenablequickrejectionoflow-relevancecontenttogetherwithfast,approximatecomparisonandrea-soningthatcanguidefurtherexploration.Whereusesofinformationdiﬀeracrossarangeoftasks,usefulsummariesofanexpressionmaylikewisediﬀer.A3.5.3ExploitingContextSummariesAlthoughcontextsummaries(Section8.3.5),likecontentsummaries,areinprinciplesemanticallyredundant,theyaresubstantiallydiﬀerentinpractice:Expressionsarebounded,butaninterpretivecontextmaybeofanysize,forexample,onthescaleofabookorabodyofdomainknowledge.Thus,absentsummarization,contextualinformation—andhencethemeaningofanexpression—maybefarfromlocal;withcontextsummarization,meaningbecomesmorelocalandhencemorestronglycompositional.21.Orintersection-andunion-likeoperationsinregion-orientedsemantics,seeAppendixA1.2.Asnotedelsewhere,currentlanguagemodelstypicallyencode(costlytolearn,diﬃculttoshare)summariesofglobalcontext—aswellasknowledgeofnarrowercontextsandevenspeciﬁcfacts—whiletheirinference-timeactivationsinclude(costlytoinfer)summariesoftextuallylocalcontext.Learningandsharingtask-orientedsummariesofbothbroadandnarrowcontextscouldprovidecomplementaryandmoreeﬃcientfunctionality.Embeddingscanprovidethemostcompactsummaries,butmoregeneralQNRscouldprovidericheryetstillabstractiveinformation.128Someuse-patternswouldplacespeciﬁcsemanticcontentinnarrowcon-textrepresentations,andschematicsemanticcontentinexpressionsthatcancontributetodescriptionsinawiderangeofcontexts.Ininterpretinganexpression,theeﬀective,interpretedmeaningsofitsembeddingswouldbestronglydependentonitscurrentcontext.Aprogramminglanguageanalogywouldbetheevaluationofexpressionsconditionedonbindingenvironments,butintheQNRcase,employingembeddingsinplaceofconventionalvaluesandvariables,1andemployingneuralmodelsinplaceofsymbolicinterpreters.A3.5.4AligningParallelandOverlappingExpressionsContentsummariescanfacilitatecomparisonandknowledgeintegrationintheabsenceoffullstructuralalignment.InthelimitingcaseofacompletestructuralmismatchbetweenQNRexpressions,theirsummaryembeddingscanstillbecompared.Totheextentthathigh-levelstructurespartiallyalign,comparisoncanproceedbasedonmatchingtosomelimiteddepth.Atpointsofstructuraldivergence,comparisoncanfallbackonsummaries:Wheresubexpressionsdiﬀer,theirsummaries(whethercachedorconstructed)canbecompared;likewise,asubexpressionsummaryinoneexpressioncanbecomparedtoalexicalembeddingintheother.AppendixA1discusseshowmatchescanberejectedorappliedthroughsoftuniﬁcation.Upgradingtheexpressivepoweroflexical-levelembeddingscanfacilitatestructuralalignmentbyshiftingburdensofsemanticexpressivenessawayfromsyntax:Condensingsimpleexpressionsintoembeddingsavoidsapo-tentialsourceofirregularity,thesequentialorderoflexical-levelelementsneednotbeusedtoencodeemphasisorsemanticpriority,andthesemanticdiﬀerencesbetweenactiveandpassivevoiceneednotbeencodedthroughdiﬀerencesinsyntaxandgrammar.Accordingly,similarmeaningsbecomeeasiertoexpressinparallelsyntacticforms.Ifexpressionswithsimilarsemanticcontent—representingsimilarthings,properties,relationships,roles—arecastinaparallelsyntacticform,theybecomeeasiertocompare.Regularizingstructureneednotsacriﬁceexpres-sivecapacity:Expression-levelnuancesthatinNLareexpressedthroughalternativesyntacticformscanquitegenerallyberepresentedbyembeddingsthatmodifyexpression-levelmeaning.1.Whileblurringthedistinctionbetweenvaluesandvariables;seeSectionA1.4.3,whichnotespotentialrelationshipsbetweenconstraint-baseduniﬁcationandvariablebindinginlogicprogramming.129Thus,structuralregularization,enabledbyexpressiveembeddingsandexplicitgraphs,canfacilitatestructuralalignmentandsemanticcomparisonofrelatedexpressions.Inaddition,however,structuralregularizationcanfacilitatetransformationsamongalternativecanonicalforms,potentiallyfa-cilitatingtranslationbetweenrepresentationaldialectsinheterogeneousNL+corpora.Regularizationneednotadheretoauniformstandard.A4CompositionalLexicalUnitsEmbeddingswithexplicitcompositionalstructuremayoﬀeradvantagesineﬃcientlearningandgeneralization.Section7.1.3notedthatthepropertiesofvectoradditioncanenablesemanticcompositionalitywithoutrecoursetosyntax;thepresentdiscussionexam-inesthepotentialroleofexplicitformsofcompositionalityinlearningandrepresentation.Amongtheconsiderationsare:•Eﬃcientlyrepresentinglargevocabularies•Parallelstonaturallanguagevocabularies•ParallelstoNLPinputencodings•InductivebiastowardeﬃcientgeneralizationTheusualdisclaimerapplies:Theaimhereisneithertopredictnorprescribeparticularrepresentations,buttoexplorewhatamountstoalowerboundonpotentialrepresentationalcapabilities.Explicitvectorcompositionality,would,however,requireexplicitarchitecturalsupport.A4.1MotivationandBasicApproachBecausefewneuralmodelswriteandreadlargestoresofneurallyencodedinformation,prospectsforbuildinglargeQNRcorporaraisenovelquestionsofstorageandpracticality.SectionA5.5outlinesanapproach(usingdictio-nariesofcomposablevectorcomponents)thatcanbecompact,eﬃcient,andexpressive.Thepresentdiscussionconsidershowandwhyexplicitcompo-sitionalitywithinvectorrepresentationsmaybeanaturalchoiceforreasonsotherthaneﬃciency.Akeyintuitionisthatsetsoflexicalcomponents(likemorphemesinnaturallanguages)canbecomposedtorepresentdistinctlexicalunits(likewordsandphrasesthatrepresentobjects,actions,classes,relationships,functions,130etc.1),andthatcompositelexicalunitscanbestberegardedandimplementedassinglevectorsinQNRs.Forconcreteness,thediscussionherewillassumethatlexical-componentembeddingsareconcatenatedtoformlexical-unitembeddings,2thenmeldedbyshallowfeed-forwardtransformationstoformuniﬁedrepresentations.Akeyunderlyingassumptionisthatdiscretevocabulariesareuseful,whethertoencodeembeddingscompactly(AppendixA5),ortoprovideaninductivebiastowardcompositionalrepresentations.(Notethatcompactencodingscancombinediscretevectorswithcontinuousscalarstodesignatepointsoncontinuousmanifolds;seeSectionA5.5.4).A4.2EﬃcientlyRepresentingVastVocabulariesTheon-boardmemoriesofGPUsandTPUscanreadilystore>107embeddingsforfastaccess.3ThiscapacityisordersofmagnitudebeyondthenumberofEnglishwords,yetusingtheseembeddingsascomponentsoflexicalunitscanprovidemuchmore.Ifsetsofpotentiallexical-unitembeddingsareCartesianproductsofsetsoflexical-componentembeddings,thenpotentialvocabulariesareenormous.Cartesian-productspacesinwhich(forexample)2to4componentsaredrawnfrom107optionswouldoﬀer1014to1028potentiallexical-unitembeddings;ofthese,onecanexpectthatatinyfraction—yetanenormousnumber—wouldbepotentiallyusefulindescribingtheworld.Torepresentexpressionsasstrings(AppendixA5),3bytesofkeyinformationperlexicalcomponentwouldbeample.A4.3ParallelstoNaturalLanguageVocabulariesLexicalunitsinNLvocabulariesarecommonlybuiltofmultiplemorphemes,includingroots,aﬃxes,4andwordsembeddedincompoundsormultiwordunits.51.Asalreadynoted,thisuseof“lexicalunits”abusesstandardterminologyinlinguistics.2.Concatenationcanbemodeledasadditionofblockwise-sparsevectors,andadditionofdensevectorswouldarguablybesuperior.However,usingadditioninplaceofconcatenationwould(inapplication)increasestoragecostsbyasmallfactor,andwould(atpresent)incurasubstantialexplanatorycost.3.SeeSectionA5.5.5.4.Englishbuildson>1300rootsandaﬃxes(preﬁxsuﬃx.com2008).5.Hereusedinthestandardlinguisticsense(alsotermed“lexicalitems”).131IfweviewNLasamodelforpotentialNL+constructs,thenitisnatu-raltoconsideranaloguesofmorphemesinembeddingspaces,andtoseeklexical-levelsemanticcompositionalitythroughexplicitcompositionofbuild-ingblocksinwhichthemeaningsofcomponentsare,asinNL,ajointresultoftheircombination.Thisapproachcanmakelexicalcomponentsthem-selvestargetsoflearningandtherebyexpandthescopeofuseful,accessiblevocabulary.Medicalterminologyillustratestheroleoflexical-levelcompositionalityinbuildingalanguageadaptedtoarichdomain.1Mostmedicaltermsarebuiltofsequencesofparts(“cardio+vascular”)orwords(“primaryvisualcortex”).Wikipedia(2021)lists510wordparts(preﬁxes,roots,andsuﬃxes)usedinmedicalterminology,whilealargemedicaldictionarydeﬁnes~125,000distinct,oftenmulti-wordterms(Dorland2007),anumberthatapproachesanestimate(~200,000)ofthenumberofwordsintheEnglishlanguageasawhole.2Reﬁningandexpandingthestoreofapplicablelexicalcomponentsfromhundredsorthousandstomillionsormorewouldgreatlyincreasethepoten-tialsemanticresolutionofmedicallanguageatalexicallevel.Medicine,ofcourse,occupiesonlyacornerofasemanticuniversethatembracesmanyﬁeldsandextendsfarbeyondwhatourwordscanreadilydescribe.A4.4ParallelstoNLPInputEncodingsTherearesubstantialparallelsandcontrastsbetweeninputencodingsincurrentNLPandcompositionalembeddingsinpotentialQNRprocessingsystemsTableA4.1):•IntheproposedmodeofQNRprocessing,inputsarelexicalcomponentsconcatenatedtoformlexicalunits;inTransformer-basedNLprocess-ing,inputsarewordsandsubwordsextractedfromstringsthroughtokenization.•IntheQNRcase,averylargevocabularyoflexicalcomponentsiscom-posedtoformavastCartesian-productspaceoflexicalunits;intheNLPcase,asmallervocabularyoflexicalunitsisbuiltfromwordfragmentsandcommonwords(inBERT,~30,000“wordpieces”).1.Avocabularywhichdescribesstructures,functions,relationships,processes,observations,evidence,interventionsandcausalityinsystemsofextraordinarycomplexityandhumanimportance.2.Acountthatomits,forexample,inﬂectedforms(Brysbaertetal.2016).132•IntheQNRcase,thecompositionoflexicalunitsisdeterminedbyrepresentationlearning;IntheNLPcase,thedecompositionofstringsisdeterminedbyatokenizationalgorithm.•NLPmodelsdynamicallyinfer(andattempttodisambiguate)lexical-levelrepresentationsfromtokenizedtext;inQNRprocessing,inputembeddingsareexplicitlexical-levelproductsofpreviousrepresenta-tionlearning.Thus,lexical-levelQNRinputsareroughlycomparabletohidden-layerrepresentationsinanNLPmodel.TableA4.1:InputrepresentationsusedincurrentNLPandprospectiveQNRprocessing.TypicalNLPmodelsProposedQNRmodelsInputunits:wordpiecetokenscomponentembeddingsVocabularysize:~104–105~107–1028Embeddingorigins:learnedrepresentationslearnedrepresentationsInitialprocessing:multipleattentionlayersMoEblendinglayer1A4.5InductiveBiasTowardEﬃcientGeneralizationTheabilitytorepresentspeciﬁclexicalunitsascompositionsofmoregeneralsemanticcomponentscouldpotentiallysupportbothsystematicgeneraliza-tionandeﬃcientlearning,includingimprovedsampleeﬃciency.Animpor-tantlexicalcomponentwilltypicallyoccurfarmorefrequentlythanthelexicalunitsthatcontainit,andlearningaboutacomponentcanprovideknowledgeregardinglexicalunitsthathavenotyetbeenencountered.2Indeed,withouttheinductivebiasprovidedbycomposition,itmightbediﬃculttolearntrulylargevocabulariesthatformwell-structuredsemanticspaces.AsanNLillustrationofthisprinciple,consider“primaryvisualcortex”again:Areaderwhoknowslittleornothingofneuroanatomywillknowthegeneralmeaningsof“primary”and“visual”and“cortex”,havingencountered1.MoE=mixtureofexperts(seeFedus,Zoph,andShazeer2021).2.Inadditiontotheseconsiderations,notethatcomponentscouldpotentiallyoccupyrelativelysimpleandwell-structuredsemanticspaces,facilitatingtheirinterpretationevenintheabsenceofspeciﬁctrainingexamples.Improvingtheinterpretabilityofnovellexicalcomponentswouldfeedthroughtoimprovementsintheinterpretabilityofnovelelementsofalexical-unitvocabulary.133thesetermsindiversecontexts.Withthisknowledge,onecanunderstandthat“primaryvisualcortex”islikelytomeansomethinglike“thepartofthebrainthatﬁrstprocessesvisualinformation”,evenifthistermhasneverbeforebeenseen.Amorereﬁnedunderstandingcanbuildonthis.ThisfamiliarprinciplecarriesovertotheworldofpotentialQNRrepresen-tations,whereexploitationofcompositionallexical-levelsemanticspromisestosupportlearningwithbroadscopeandeﬀectivegeneralization.A4.6ANoteonDiscretizedEmbeddingsIntypicalapplications,readingisfarmorefrequentthanwriting,hencemap-pingcontinuousinternalrepresentationstodiscreteexternalrepresentationsneednotbecomputationallyeﬃcient.Thisoutputtaskcanbeviewedaseithertranslationorvectorquantiﬁcation.Lexicalcomponentsthataredis-tantfromexistingembeddingsmayrepresentdiscoveriesworthrecordinginanexpandedvocabulary.Becausecomponentspopulatecontinuousvectorspaces,discretizationiscompatiblewithdiﬀerentiablerepresentationlearningofcomponentsandtheirsemantics.A5CompactQNREncodingsStringrepresentationsofQNRs,inconjunctionwithdiscretizedvec-torspacesandgraph-constructionoperators,canprovidecompactandeﬃcientQNRencodings.“Prematureoptimizationistherootofallevil.”—DonaldKnuth1NL+expressionscanbeimplementedcompactlybycombiningoperator-basedrepresentationsofgraphstructureswithextensibledictionariesofdiscretizedembeddings;thelatterprovidemechanismsforwhatcanberegardedaslossycompression,butcanalsoberegardedasprovidingausefulinductivebias(SectionA4.5).ThecontentofQNRcorporacanberepresentedasbyte1.Knuth(1974).Becausecompressionis(atmost)adownstreamresearchpriority,Knuth’swarningagainstprematureoptimizationisrelevantandsuggeststhatthevalueofthisappendixisquestionable.Thereis,however,goodreasontoexplorethescopeforeﬃcient,scalableimplementations:Asketchoffutureoptionscanhelptofreeexploratoryresearchfromprematureeﬃciencyconcerns—orworse,areluctancetoconsiderapplicationsatscale.134stringsapproximatelyascompactasNLtextbyexploitingkey–valuestoresofembeddingsandgraph-constructionoperators.1Thememoryfootprintthesestoresneednotstrainthelow-latencymemoryresourcesofcurrentmachines.Thepurposeofthisappendixisnottoargueforaparticularapproach,buttoshowthatapotentialchallenge—thescaleofQNRstoragefootprints—canbemetinatleastonepracticalway.Notethattheconsiderationsherehavenothingtodowithneuralcompu-tationperse,butareinsteadinthedomainsofalgorithmanddata-structuredesign(oftendrawingonprogramminglanguageimplementationconcepts,e.g.,environmentsandvariablebinding).Fromaneuralcomputationper-spective,themechanismsmustbydesignbetransparent,whichistosay,invisible.A5.1LevelsofRepresentationalStructureProspectiveQNRrepositoriesincluderepresentationalelementsatthreelevelsofscale:•Embeddings:vectorattributesatalevelcomparabletowords•Expressions:graphsatalevelcomparabletosentencesandparagraphs•References:graphlinksatthelevelofcitationsanddocumentstructuresInbrief,expressionsaregraphsthatbearvectorattributesandcanincludereference-linkstootherexpression-levelgraphs.Thereisnoimportantseman-ticdistinctionbetweenexpression-levelandlarger-scalegraphstructures;thekeyconsiderationsinvolveinteractionsbetweenscale,anticipatedpatternsofuse,andimplementationeﬃciency.AformalnotationwoulddistinguishbetweenQNRsasmathematicalob-jects(graphandattributes),QNRsascomputationalobjects(inference-timedatastructuresthatrepresentgraphsandattributes),andencodingsthatrepresentandevaluatetoQNRobjectsinacomputationalenvironment.Thefollowingdiscussionreliesoncontexttoclarifymeaning.1.Thesetofmechanismsoutlinedhereisintendedtobeillustrativeratherthanexhaustive,detailed,oroptimal.Thediscussiontouchesontutorialtopicsforthesakeofreaderswhomaynoticecomputationalpuzzleswithoutimmediatelyrecognizingtheirsolutions.135A5.2ExplicitGraphObjectsvs.StringEncodingsArelativelysimplecomputationalimplementationofQNRswouldrepresentembeddingsasunsharednumericalvectors,1andgraphsasexplicitdatastructures.2Repositoriesandactivecomputationswouldsharethisdirect,bulkyrepresentationalscheme.Naturallanguageexpressionsaremorecompact:NLwordsintextstringsarefarsmallerthanhigh-dimensionalvectorembeddings,andgraphstruc-turesareimplicitinNLsyntax,whichrequiresnopointer-likelinks.Inference-timerepresentationsofNL+expressionsmaywellbebulky,butsoaretheinference-timerepresentationsofNLexpressionsinneuralNLP.AndaswithNL,storedQNRexpressionscanberepresentedcompactlyasbytestrings.A5.3CompactExpressionStringsAQNRcorpuscanberepresentedasakey–valuestorethatcontainsembed-dings(numericalvectors),operators(executablecode),andencodedQNRexpressions(stringsthatareparsedintokeysandinternalreferences).Inthisscheme,expressionsencodedasbyte-stringsevaluatetoexpression-levelQNRgraphsthatcanincludereferencesthatdeﬁnegraphsatthescaleofdocumentsandcorpora.Inmoredetail:•Keysdesignateoperators,embeddings,orexpression-stringsinakey–valuestore.•Expression-stringsarebytestrings3thatareparsedintokeysandin-dices.•Indicesdesignatesubexpressionsinastring.•Operatorsaregraph-valuedfunctions4ofﬁxedaritythatactonse-quencesofsubexpressions(operands).•Subexpressionsarekeys,indicesoroperator-operandsequences.•Embeddingsaregraphattributes.1.“Unshared”inthesensethateachattribute-slotwoulddesignateadistinct,potentiallyuniquevectorobject.2.“Explicit”inthesensethateacharcwouldberepresentedbyapointer-likereference.3.Potentiallybitstrings.4.Anextendedscheme(SectionA5.5.4)allowsvector-valuedoperatorswithvectorandscalaroperands.136•Referenceisshorthandfor“expression-stringkey”(typicallyastoppingpointinlazyevaluation).A5.3.1FromStringstoSyntaxGraphsParsinganexpression-stringisstraightforward:Operatorsarefunctionsofﬁxedarity,theinitialbytesofanexpression-stringdesignateanoperator,andadherencetoastandardpreﬁxnotationenablesparsingoftherest.Aﬁrstlevelofdecodingyieldsagraphinwhichoperatornodeslinktochildnodesthatcorrespondtoembeddings,operators,andreferencestootherexpressions.1Thus,expression-stringsrepresentgraphsinwhichmostintra-expressionlinksareimplicitinthecompositionofgraph-valuedoperatorsandtheirproducts,2whiletheoverheadofdesignatingexplicit,inter-expressionlinks(severalbytesperreference-key)isineﬀectamortizedoverthelinkedex-pressions.Accordingly,QNRgraphsonascalecomparabletoNLsyntacticstructuresneednotincurper-link,pointer-likeoverhead.A5.3.2LazyEvaluationLazyevaluationenablesthepiecemealdecodingandexpansionofQNRgraphsthataretoolargetofullydecompress.Tosupportlazyevaluation,referencestoexpression-stringscanevaluatetographobjects(presentedasavectorofnodes),orcanbeleftunevaluated.Akey–valuestorethencansupportlazyevaluationbyreturningeitheranexpression-stringor,whenavailable,thecorrespondinggraphobject;aserverthatretrievesobjectsforprocessingcanqueryanexpression-storewithakeyandinvokeevaluationifthestorereturnsastring.Thismechanismalsosupportstheconstructionofsharedandcyclicgraphs.A5.4Graph-ConstructionOperatorsParsingassociateseachconstructionoperatorwithasequenceofoperandsthatcanbeevaluatedtoproduce(orprovideadecodedaccessto)embeddings,references,androotnodesofgraph-objects.Simpleconstructionoperatorscantreattheirargumentsasatomicandopaque;morepowerfuloperatorscanaccessthecontentsofevaluatedgraph-valuedoperandsorthecontentsoftheoperator’sevaluation-timecontext.1.Internal,index-encodedlinksrequireabitofadditionalbookkeeping(i.e.,rememberingsubexpressionpositions)butcandescribeDAGsandcyclicgraphs.2.Indices(~1byte?)accountfortherest.137Graphstructurecanbespeciﬁedexplicitlybyusingatreenotationinconjunctionwitharepresentation(e.g.,nodelabelsandreferences)thatcandistinguishandreferencenodestoconstructcyclicandconvergentpaths.Graphconstructionoperatorscanaugmenttheseexplicitmechanismsbyfactoringandabstractingcommongraphpatterns(e.g.,theequivalentofcommonsentencestructures);conditionalproceduraloperatorscandomore.Comprehensivesetsofoperators(“graphpieces”?)neednotgreatlyburdenstoragecapacity.A5.4.1OperatorsCanCombineOpaqueArgumentsAsimpleclassofoperatorswouldreturnacopyofagraph-template—anarbitrarilystructured“graphpatch”—inwhichvariablesareinstantiatedwithargumentsthataretreatedasopaque,atomictokens.Inthisapproach,theproductsofoperatorcompositionareconstrained:Linksbetweengraph-patchescantargetroot-nodesreturnedbyoperators.1A5.4.2PathsCanSelectGraphComponentsApaththroughaconnectedgraphcandesignatethepositionofanynoderelativetoanyother,providedthatlinkscanbedistinguishedinnavigatingfromnodetonode.2Path-basedaccesscanbeimplementedbyoperatorsthathaveaccesstodecodedgraphcontexts.A5.5VocabulariesofEmbeddingsThecomputationalcostsofprocessingNL+expressionswilldependinpartonthestoragefootprintoftheirvectorembeddings,whichinturnwilldependonthesizesbothofvocabulariesandofthevectorsthemselves.A5.5.1BasicArchitecturalConsiderationsCurrentneurallanguagemodelstokenizetextstringsandmapamodestvo-cabularyoftokens3toacorrespondingvocabularyofembeddings.Processing1.Orothernodesiflinksincludeanindexintoareturnednode-vector.Notethatdistin-guishedrootnodesareartifactsofencodingthatneednotbesemanticallyvisible.Similarremarksapplytoreferencesthattargetexpression-strings.2.E.g.,sequencesofcarandcdroperatorscannavigatearbitrarygraphsofLispconscells.Pathshavenoﬁxedsizeandcouldberepresentedbystringsinakey–valuestore.3.E.g.,~216bytepairsor~30,000wordpieces(Devlinetal.2019).138QNRsencodedasexpression-stringsrequiresasimilarmappingoftokens(keys)tolexical-componentembeddings,butprospectivevocabulariesareordersofmagnitudelarger.Theuseoflargevocabulariesmandatestheuseofscalablekey–valuestores;withthischoice,vocabularysizeaﬀectsneitherneuralmodelsizenorcomputationalcost.Theuseoflow-latencystorageforfrequentlyusedembeddingvectorscould,however,imposesubstantialburdens,withrequirementsscalingasvocabularysize×vectordimensionality×numericalprecision.1Baselinevaluesfortheseparameterscanplacestorageconcernsinaquantitativecontext.A5.5.2VocabularySizeInacontinuousvectorspace,“vocabularysize”becomesmeaningfulonlythroughvectorquantization,theuseofidenticalvectorsinmultiplecontexts.2DiscretevectorscanbecomparedtoNLwordsorwordparts,andvectorsproducedbyconcatenatingquantizedvectorscanbecomparedtowordsbuiltofcombinationsofparts.A5.5.3LexicalComponentEmbeddingsPotentialQNRrepresentationsandapplications(Section11)aresuﬃcientlydiverse(somefarfrom“linguistic”asordinarilyunderstood)thatitisdiﬃculttogeneralizeabouttheappropriategranularityofvectorquantizationortheextenttowhichitshouldbeemployed.Diﬀerentapplicationswillcallfordiﬀerenttrade-oﬀsbetweencompressionandperformance,andquantizationhasbeenfoundtoimproveratherthandegradeneuralrepresentationlearninginsomedomains.3Morecanbesaidaboutvocabularysize,however,inthecontextofNL+representationsthatconstitute(merely)stronggeneralizationsofNL.Toprovidearound-numberbaselinevalue,consideranNL+representationthatemploysavocabularyofdistinctlexicalcomponents4thatis50timeslargerthanthevocabularyofdistinctwordsintheEnglishlanguage:5Forpresent1.Valuesofotherkindsneednotimposesimilaroverheads:Setsofoperatorsneednotbelarge,whileexpressionstringscanberetrievedfromlower-cost,higher-latencystores.2.Intheliterature,theuseoflow-precisionnumericalrepresentationsisalsotermed“vectorquantization”.3.Agustssonetal.(2017),Oord,Vinyals,andKavukcuoglu(2018),KaiserandBengio(2018),Razavi,Oord,andVinyals(2019),Łańcuckietal.(2020),andZhaoetal.(2021)4.RoughlycorrespondingtomorphemesinNL.5.Iflexicalunitsaretypicallycompositionsoflexicalcomponents(AppendixA4),thenthesizeofthepotentialencodedvocabularyisfarlarger.139purposes,~200,000wordsisareasonableestimateofthelatter,1implyingabaselineNL+vocabularyof10millionlexical-componentembeddingvectors.A5.5.4ComposingComponentsAsdiscussedinAppendixA4,arichervocabularyoflexicalunitscanbeconstructedbycomposition,forexample,byconcatenatinglexical-componentembeddings.Thisgenerativemechanismparallelswhatweseeinnaturallanguages,wheremany(evenmost)wordsarecomposedofvariousword-parts,TAM-Caﬃxes,orotherwords.Discretecomposition:Expression-stringscanreadilydescribecompos-itelexicalunits:Vector-valuedoperatorswithvectorargumentscandenoteconcatenationsofanynumberofcomponents.Consideringonlydiscretecombinations,operatorsthataccept2to4argumentsfromavocabularyof107embeddingscandeﬁneproductvocabulariesof1014to1028composites.Thesearecandidatesforuseaslexicalunits,andeventinyusefulfractionscorrespondtovastvocabularies.AppendixA4explorescompositeembed-dingrepresentationsfromperspectivesthatincludeeﬃciencyinlearningandsemanticsinuse.Weightedcombination:Someentitiesaredrawnfromdistributionscharac-terizedbycontinuouspropertiesthatincludesize,color,age,andprobability.Discretevocabulariesareapoorﬁttocontinuoussemantics,butoperatorsthatacceptnumericalargumentscanspecifyweightedcombinationsofvectors,andhencecontinuousmanifoldsinsemanticspaces.Forexample,akeythatdesignatesadiscretevectorembeddingacouldbereplacedbykeysdesignatingavector-valuedoperatorfi,ascalarparameterw,andembeddingscandd,forexample,alinearcombination:fi(w,c,d)=wc+(1−w)d.Asuitablerangeofoperatorscangeneralizethisschemetononlinearfunctionsandhigher-dimensionalmanifolds.A5.5.5Dimensionality,NumericalPrecision,andStorageRequirementsTransformer-basedmodelstypicallymapsequencesoftokenstosequencesofhigh-dimensionalembeddings(inthebaselineversionofBERT,768dimen-1.Vocabularysizedependsonhow“distinctwords”aredeﬁned;reasonabledeﬁnitionsandmethodologiesyieldnumbersthatdiﬀer,butnotbyordersofmagnitude.SeeBrysbaertetal.(2016).140sions).Thoughhighdimensionalityisperhapsnecessaryforhiddenstatesthatareusedtobothrepresentandsupportprocessingofrich,non-localse-manticinformation,onemightexpectthatword-levelsemanticunitscouldberepresentedbyembeddingsoflowerdimensionality.Thisisempiricallytrue:Shrinkinginput(word-level)embeddingsbymorethananorderofmagnitude(from768to64dimensions)resultsinanegligiblelossofperformance(Lanetal.2020).Inestimatingpotentialstoragerequirements,100(butnot10or1000)dimensionsseemslikeareasonablebaselinevalueforrepresentationsofbroadlyword-likelexicalcomponents.Forminglexicalunitsbyconcatenationofcomponents(desirableonseveralgrounds;seeAppendixA4)wouldyieldlargerinputembeddingswithoutincreasingstoragerequirements.BERTmodelsaretypicallytrainedwith32-bitprecision,butforuseatinferencetime,parametersthroughoutthemodelcanbereducedto8-bit(Prato,Charlaix,andRezagholizadeh2020),ternary(WeiZhangetal.2020),andevenbinary(H.Baietal.2020)precisionwithlittlelossofperformance.Asabaseline,allowing8-bitprecisionforembeddingsattheinputinterfaceofaQNR-basedinferencesystemseemsgenerous.Whencombinedwiththebaselinevocabularysizesuggestedabove(107componentembeddings),thesenumberssuggestabaselinescaleforanNL+key–valuestore:Storage=vocabularysize×dimensionality×precision≈10,000,000elements×100dimensions×1bytes≈1GB.Forcomparison,currentGPUsandTPUstypicallyprovideon-boardmemory>10GB,andareintegratedintosystemsthatprovideterabytesofRAM.Giventheexpectedpower-law(Zif-like)distributionofusefrequencies,im-plementationsinwhichsmalllocalstoresofembeddingsarebackedbylargeremotestoreswouldpresumablyexperienceoverwhelminglylocalmemorytraﬃc.1Inmanytasks(e.g.,fetchingcontentusedtoanswerhumanqueries),occasionalmillisecond-rangelatencieswouldpresumablybeacceptable.1.AsanillustrativeNLexample,initsﬁrst107words,theGoogleBookscorpuscontainsabout104distinctwords(byagenerousdeﬁnitionthatincludesmisspellings),aratioofonenewwordin103,whileinitsﬁrst109words,itcontainsabout105distinctwords,aratioofonein104.SeeBrysbaertetal.(2016).141A5.5.6ANoteonNon-LexicalEmbeddingsProspective,fullyelaboratedNL+systemswouldemploymorethanjustlexical-levelembeddings;forexample,embeddingsthatinsomesensesum-marizeexpressionscanenableshallowprocessing(skimming),orcanserveaskeysfornear-neighborretrievalthatimplementssemanticassociativememoryoverlargecorpora.1Thecostsofsummariescaninsomesensebeamortizedovertheexpressionstheysummarize.Likenaturallanguage,NL+expressionscanberepresentedbycompactbytestrings.Storesofdiscreteembeddingscansupportlarge(andthroughcomposition,vast)vocabulariesofdiﬀerentiablesemanticrepresentationswithinanacceptablefootprintforlow-latencymemory;inotherwords,NL+corporacanberepresentedapproximatelyaseﬃcientlyandcompactlyascor-poraofNLtext.Neithernecessitynoroptimalityisclaimedfortheapproachoutlinedhere.1.Tosupportnear-neighborindexing,embeddingsshouldbeunique,notelementsofa“vocabulary”.142ReferencesAbramson,Josh,ArunAhuja,IainBarr,ArthurBrussee,FedericoCarnevale,MaryCassin,RachitaChhaparia,etal.2021.“ImitatingInteractiveIntel-ligence,”arXiv:2012.05672[cs.LG].Abu-Salih,Bilal,MarwanAl-Tawil,IbrahimAljarah,HossamFaris,PornpitWongthongtham,KitYanChan,andAminBeheshti.2021.“RelationalLearningAnalysisofSocialPoliticsusingKnowledgeGraphEmbedding.”DataMiningandKnowledgeDiscovery35(4):1497–1536.Addanki,Ravichandra,PeterW.Battaglia,DavidBudden,AndreeaDeac,Jon-athanGodwin,ThomasKeck,WaiLokSibonLi,etal.2021.“Large-scalegraphrepresentationlearningwithverydeepGNNsandself-supervision,”arXiv:2107.09422[cs.LG].Adi,Yossi,EinatKermany,YonatanBelinkov,OferLavi,andYoavGoldberg.2017.“Fine-grainedAnalysisofSentenceEmbeddingsUsingAuxiliaryPredictionTasks,”arXiv:1608.04207[cs.CL].Agustsson,Eirikur,FabianMentzer,MichaelTschannen,LukasCavigelli,RaduTimofte,LucaBenini,andLucVGool.2017.“Soft-to-HardVectorQuantizationforEnd-to-EndLearningCompressibleRepresentations.”InAdvancesinNeuralInformationProcessingSystems,editedbyI.Guyon,U.V.Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,vol.30.CurranAssociates,Inc.Akoury,Nader,KalpeshKrishna,andMohitIyyer.2019.“SyntacticallySuper-visedTransformersforFasterNeuralMachineTranslation.”InProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,1269–1281.Florence,Italy:AssociationforComputationalLinguistics.Ali,Mehdi,MaxBerrendorf,CharlesTapleyHoyt,LaurentVermue,SahandSharifzadeh,VolkerTresp,andJensLehmann.2021.“PyKEEN1.0:APythonLibraryforTrainingandEvaluatingKnowledgeGraphEmbed-dings.”JournalofMachineLearningResearch22(82):1–6.Allamanis,Miltiadis,MarcBrockschmidt,andMahmoudKhademi.2018.“LearningtoRepresentProgramswithGraphs,”arXiv:1711.00740[cs.LG].143Allamanis,Miltiadis,PankajanChanthirasegaran,PushmeetKohli,andCharlesSutton.2017.“LearningContinuousSemanticRepresentationsofSym-bolicExpressions.”InProceedingsofthe34thInternationalConferenceonMachineLearning,editedbyDoinaPrecupandYeeWhyeTeh,70:80–88.ProceedingsofMachineLearningResearch.PMLR.Allan,RutgerJ.2013.“ExploringModality’sSemanticSpaceGrammaticaliza-tion,Subjectiﬁcationandthecaseofoφ(cid:15)ιλω.”Glotta89:1–46.Allen,Carl,andTimothyHospedales.2019.“AnalogiesExplained:TowardsUnderstandingWordEmbeddings.”InProceedingsofthe36thInterna-tionalConferenceonMachineLearning,editedbyKamalikaChaudhuriandRuslanSalakhutdinov,97:223–231.ProceedingsofMachineLearningResearch.PMLR.Alon,Uri,MeitalZilberstein,OmerLevy,andEranYahav.2019.“Code2vec:LearningDistributedRepresentationsofCode.”Proc.ACMProgram.Lang.(NewYork,NY,USA)3(POPL).Amiridze,Nino,andTemurKutsia.2018.Anti-UniﬁcationandNaturalLan-guageProcessing.EasyChairPreprintno.203.Arabshahi,Forough,JenniferLee,MikaylaGawarecki,KathrynMazaitis,AmosAzaria,andTomMitchell.2021.“ConversationalNeuro-SymbolicCommonsenseReasoning,”arXiv:2006.10022[cs.AI].Arivazhagan,Naveen,AnkurBapna,OrhanFirat,RoeeAharoni,MelvinJohn-son,andWolfgangMacherey.2019.“TheMissingIngredientinZero-ShotNeuralMachineTranslation,”arXiv:1903.07091[cs.CL].arXiv.2021.“arXivMonthlySubmissions.”AccessedJanuary16,2021.https://arxiv.org/stats/monthly_submissions.Bai,Haoli,WeiZhang,LuHou,LifengShang,JingJin,XinJiang,QunLiu,MichaelLyu,andIrwinKing.2020.“BinaryBERT:PushingtheLimitofBERTQuantization,”arXiv:2012.15701[cs.CL].Bai,Yunsheng,HaoDing,KenGu,YizhouSun,andWeiWang.2020.“Learning-BasedEﬃcientGraphSimilarityComputationviaMulti-ScaleConvo-lutionalSetMatching.”ProceedingsoftheAAAIConferenceonArtiﬁcialIntelligence34(04):3219–3226.144Banino,Andrea,AdriàPuigdomènechBadia,RaphaelKöster,MartinJ.Chad-wick,ViniciusZambaldi,DemisHassabis,CaswellBarry,MatthewBot-vinick,DharshanKumaran,andCharlesBlundell.2020.“MEMO:ADeepNetworkforFlexibleCombinationofEpisodicMemories,”arXiv:2001.10913[cs.LG].Banino,Andrea,JanBalaguer,andCharlesBlundell.2021.“PonderNet:Learn-ingtoPonder,”arXiv:2107.05407[cs.LG].Bansal,Kshitij,SarahLoos,MarkusRabe,ChristianSzegedy,andStewartWilcox.2019.“HOList:AnEnvironmentforMachineLearningofHigherOrderLogicTheoremProving.”InProceedingsofthe36thInternationalConferenceonMachineLearning,editedbyKamalikaChaudhuriandRus-lanSalakhutdinov,97:454–463.ProceedingsofMachineLearningRe-search.PMLR.Baroni,Marco,andRobertoZamparelli.2010.“NounsareVectors,AdjectivesareMatrices:RepresentingAdjective-NounConstructionsinSemanticSpace.”InProceedingsofthe2010ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,1183–1193.Cambridge,MA:AssociationforComputationalLinguistics.Battaglia,PeterW.,JessicaB.Hamrick,VictorBapst,AlvaroSanchez-Gonzalez,ViniciusZambaldi,MateuszMalinowski,AndreaTacchetti,etal.2018.“Relationalinductivebiases,deeplearning,andgraphnetworks,”arXiv:1806.01261[cs.LG].Bauer,Lisa,YichengWang,andMohitBansal.2018.“CommonsenseforGen-erativeMulti-HopQuestionAnsweringTasks.”InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,4220–4230.Brussels,Belgium:AssociationforComputationalLinguistics.Bear,Daniel,ChaofeiFan,DamianMrowca,YunzhuLi,SethAlter,AranNayebi,JeremySchwartz,etal.2020.“LearningPhysicalGraphRepresen-tationsfromVisualScenes.”InAdvancesinNeuralInformationProcessingSystems,editedbyH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,33:6027–6039.CurranAssociates,Inc.Belohlavek,Radim.2011.“WhatisaFuzzyConceptLattice?II.”InProceedingsofthe13thInternationalConferenceonRoughSets,FuzzySets,DataMiningandGranularComputing,19–26.RSFDGrC’11.Moscow,Russia:Springer-Verlag.145Beltagy,Iz,MatthewE.Peters,andArmanCohan.2020.“Longformer:TheLong-DocumentTransformer,”arXiv:2004.05150[cs.CL].Benhamou,Frédéric.1995.“Intervalconstraintlogicprogramming.”InCon-straintProgramming:BasicsandTrends,editedbyAndreasPodelski,1–21.Berlin,Heidelberg:SpringerBerlinHeidelberg.Benhamou,Frédéric,andWilliamJ.Older.1997.“Applyingintervalarith-metictoreal,integer,andbooleanconstraints.”TheJournalofLogicPro-gramming32(1):1–24.Blacoe,William,andMirellaLapata.2012.“AComparisonofVector-BasedRepresentationsforSemanticComposition.”InProceedingsofthe2012JointConferenceonEmpiricalMethodsinNaturalLanguageProcessingandComputationalNaturalLanguageLearning,546–556.EMNLP-CoNLL’12.JejuIsland,Korea:AssociationforComputationalLinguistics.Bobrow,DanielG.,andTerryWinograd.1977.“AnOverviewofKRL,aKnowl-edgeRepresentationLanguage.”CognitiveScience1(1):3–46.eprint:https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog0101_2.Bommasani,Rishi,DrewAHudson,EhsanAdeli,RussAltman,SimranArora,SydneyvonArx,MichaelSBernstein,JeannetteBohg,AntoineBosse-lut,EmmaBrunskill,etal.2021.“OntheOpportunitiesandRisksofFoundationModels,”arXiv:2108.07258[cs.LG].Borsley,Robert,andKerstiBörjars.2011.Non-transformationalsyntax:Formalandexplicitmodelsofgrammar.JohnWiley&Sons.Bostrom,Nick.2014.Superintelligence:Paths,Dangers,Strategies.NewYork:OxfordUniversityPress.Botha,JanA.,ZifeiShan,andDanielGillick.2020.“EntityLinkingin100Languages.”InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),7833–7845.Online:AssociationforComputationalLinguistics.Botne,Robert,andTiﬀanyL.Kershner.2008.“Tenseandcognitivespace:Ontheorganizationoftense/aspectsystemsinBantulanguagesandbeyond.”19(2):145–218.146Bousmalis,Konstantinos,GeorgeTrigeorgis,NathanSilberman,DilipKr-ishnan,andDumitruErhan.2016.“DomainSeparationNetworks.”InAdvancesinNeuralInformationProcessingSystems,editedbyD.Lee,M.Sugiyama,U.Luxburg,I.Guyon,andR.Garnett,vol.29.CurranAsso-ciates,Inc.Brown,Tom,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,etal.2020.“LanguageModelsareFew-ShotLearners.”InAdvancesinNeuralInformationProcessingSystems,editedbyH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,33:1877–1901.CurranAssociates,Inc.Brundage,Miles,ShaharAvin,JackClark,HelenToner,PeterEckersley,BenGarﬁnkel,AllanDafoe,etal.2018.“TheMaliciousUseofArtiﬁcialIn-telligence:Forecasting,Prevention,andMitigation,”arXiv:1802.07228[cs.AI].Brysbaert,Marc,MichaëlStevens,PawełMandera,andEmmanuelKeuleers.2016.“HowManyWordsDoWeKnow?PracticalEstimatesofVocabularySizeDependentonWordDeﬁnition,theDegreeofLanguageInputandtheParticipant’sAge.”FrontiersinPsychology7:1116.Cai,HongYun,VincentW.Zheng,andKevinChen-ChuanChang.2018.“AComprehensiveSurveyofGraphEmbedding:Problems,Techniques,andApplications.”IEEETransactionsonKnowledgeandDataEngineering30(9):1616–1637.Cai,Ruichu,ZijianLi,PengfeiWei,JieQiao,KunZhang,andZhifengHao.2019.“LearningDisentangledSemanticRepresentationforDomainAdap-tation.”InProceedingsoftheTwenty-EighthInternationalJointConferenceonArtiﬁcialIntelligence,IJCAI-19,2060–2066.InternationalJointConfer-encesonArtiﬁcialIntelligenceOrganization,July.Camacho,Alberto,andSheilaA.McIlraith.2019.“TowardsNeural-GuidedProgramSynthesisforLinearTemporalLogicSpeciﬁcations,”arXiv:1912.13430[cs.AI].Campero,Andres,AldoPareja,TimKlinger,JoshTenenbaum,andSebastianRiedel.2018.“LogicalRuleInductionandTheoryLearningUsingNeuralTheoremProving,”arXiv:1809.02193[cs.AI].Cao,NicolaDe,andThomasKipf.2018.“MolGAN:Animplicitgenerativemodelforsmallmoleculargraphs,”arXiv:1805.11973[stat.ML].147Cao,Yixin,ZhiyuanLiu,ChengjiangLi,ZhiyuanLiu,JuanziLi,andTat-SengChua.2019.“Multi-ChannelGraphNeuralNetworkforEntityAlignment.”InProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,1452–1461.Florence,Italy:AssociationforComputationalLinguistics.Caplan,D,andGSWaters.1999.“VerbalWorkingMemoryandSentenceComprehension.”TheBehavioralandBrainSciences22(1):77–94.Cappart,Quentin,DidierChételat,EliasKhalil,AndreaLodi,ChristopherMorris,andPetarVeličković.2021.“Combinatorialoptimizationandreasoningwithgraphneuralnetworks,”arXiv:2102.09544[cs.LG].Carion,Nicolas,FranciscoMassa,GabrielSynnaeve,NicolasUsunier,Alexan-derKirillov,andSergeyZagoruyko.2020.“End-to-EndObjectDetectionwithTransformers.”InComputerVision–ECCV2020,editedbyAndreaVedaldi,HorstBischof,ThomasBrox,andJan-MichaelFrahm,213–229.Cham:SpringerInternationalPublishing.Carlini,Nicholas,FlorianTramer,EricWallace,MatthewJagielski,ArielHerbert-Voss,KatherineLee,AdamRoberts,etal.2021.“ExtractingTrain-ingDatafromLargeLanguageModels,”arXiv:2012.07805[cs.CR].Cases,Ignacio,ClemensRosenbaum,MatthewRiemer,AtticusGeiger,TimKlinger,AlexTamkin,OliviaLi,etal.2019.“RecursiveRoutingNetworks:LearningtoComposeModulesforLanguageUnderstanding.”InProceed-ingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongandShortPapers),3631–3648.Minneapolis,Minnesota:AssociationforComputationalLinguistics.Chen,Benson,ReginaBarzilay,andTommiJaakkola.2019.“Path-AugmentedGraphTransformerNetwork,”arXiv:1905.12712[cs.LG].Chen,Dawn,JoshuaC.Peterson,andThomasL.Griﬃths.2017.“Evaluatingvector-spacemodelsofanalogy,”arXiv:1705.04416[cs.CL].Chen,Mark,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveiraPinto,JaredKaplan,HarriEdwards,etal.2021.“EvaluatingLargeLanguageModelsTrainedonCode,”arXiv:2107.03374[cs.LG].Chen,Wei,andMarkFuge.2019.“SynthesizingDesignsWithInterpartDe-pendenciesUsingHierarchicalGenerativeAdversarialNetworks.”JournalofMechanicalDesign141(11).148Cimiano,Philipp,AndreasHotho,andSteﬀenStaab.2005.“LearningConceptHierarchiesfromTextCorporaUsingFormalConceptAnalysis.”J.Artif.Int.Res.(ElSegundo,CA,USA)24(1):305–339.Cingillioglu,Nuri,andAlessandraRusso.2020.“LearningInvariantsthroughSoftUniﬁcation.”InAdvancesinNeuralInformationProcessingSystems,editedbyH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,33:8186–8197.CurranAssociates,Inc.Clark,Peter,OyvindTafjord,andKyleRichardson.2020.“TransformersasSoftReasonersoverLanguage,”arXiv:2002.05867[cs.CL].Clark,Stephen,AlexanderLerchner,TamaravonGlehn,OlivierTieleman,RichardTanburn,MishaDashevskiy,andMatkoBosnjak.2021.“Formal-isingConceptsasGroundedAbstractions,”arXiv:2101.05125[cs.AI].Cohan,Arman,SergeyFeldman,IzBeltagy,DougDowney,andDanielWeld.2020.“SPECTER:Document-levelRepresentationLearningusingCitation-informedTransformers.”InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,2270–2282.Online:AssociationforComputationalLinguistics.Conneau,Alexis,GermanKruszewski,GuillaumeLample,LoïcBarrault,andMarcoBaroni.2018.“Whatyoucancramintoasingle$&!#*vector:Probingsentenceembeddingsforlinguisticproperties.”InProceedingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),2126–2136.Melbourne,Australia:AssociationforComputationalLinguistics.Constant,Mathieu,GülşenEryiğit,JohannaMonti,LonnekevanderPlas,Car-losRamisch,MichaelRosner,andAmaliaTodirascu.2017.“MultiwordExpressionProcessing:ASurvey.”Comput.Linguist.(Cambridge,MA,USA)43(4):837–892.Cummins,Chris,HughLeather,ZachariasFisches,TalBen-Nun,TorstenHoeﬂer,andMichaelO’Boyle.2020.“DeepDataFlowAnalysis,”arXiv:2012.01470[cs.PL].Currey,Anna,andKennethHeaﬁeld.2019.“IncorporatingSourceSyntaxintoTransformer-BasedNeuralMachineTranslation.”InProceedingsoftheFourthConferenceonMachineTranslation(Volume1:ResearchPapers),24–33.Florence,Italy:AssociationforComputationalLinguistics.149Das,Abhishek,SatwikKottur,JoséM.F.Moura,StefanLee,andDhruvBatra.2017.“LearningCooperativeVisualDialogAgentswithDeepReinforce-mentLearning.”In2017IEEEInternationalConferenceonComputerVision(ICCV),2970–2979.Desai,Karan,andJustinJohnson.2021.“VirTex:LearningVisualRepresenta-tionsfromTextualAnnotations,”arXiv:2006.06666[cs.CV].Deshpande,Shrinath,andAnuragPurwar.2019.“ComputationalCreativityViaAssistedVariationalSynthesisofMechanismsUsingDeepGenerativeModels.”JournalofMechanicalDesign141(12).Devlin,Jacob,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.“BERT:Pre-trainingofDeepBidirectionalTransformersforLanguageUn-derstanding.”InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongandShortPapers),4171–4186.Minneapolis,Minnesota:AssociationforComputationalLinguistics.DistillTeam.2021.“AboutDistill:Dedicatedtoclearexplanationsofmachinelearning.”Distill.AccessedJanuary16,2021.https://distill.pub/about/.Dorland,WilliamAlexanderNewman.2007.Dorland’sIllustratedMedicalDictionary.Dorland’sMedicalDictionarySeries.SaundersElsevier.Drexler,K.Eric.2019.ReframingSuperintelligence:ComprehensiveAIServicesasGeneralIntelligence.TechnicalReport2019-1.FutureofHumanityInstitute,UniversityofOxford.https://www.fhi.ox.ac.uk/reframing/.Edel,Yves,EMRains,andNJASloane.2002.“OnKissingNumbersinDimensions32to128,”arXiv:0207291[math.CO].Eppe,Manfred,EwenMaclean,RobertoConfalonieri,OliverKutz,MarcoSchorlemmer,EnricPlaza,andKai-UweKühnberger.2018.“Acomputa-tionalframeworkforconceptualblending.”ArtiﬁcialIntelligence256:105–129.Eslami,S.M.Ali,DaniloJimenezRezende,FredericBesse,FabioViola,AriS.Morcos,MartaGarnelo,AvrahamRuderman,etal.2018.“Neuralscenerepresentationandrendering.”Science360(6394):1204–1210.eprint:https://science.sciencemag.org/content/360/6394/1204.full.pdf.150Ethayarajh,Kawin,DavidDuvenaud,andGraemeHirst.2019.“TowardsUnderstandingLinearWordAnalogies.”InProceedingsofthe57thAn-nualMeetingoftheAssociationforComputationalLinguistics,3253–3262.Florence,Italy:AssociationforComputationalLinguistics.Fan,Angela,ClaireGardent,ChloéBraud,andAntoineBordes.2021.“Aug-mentingTransformerswithKNN-BasedCompositeMemoryforDialog.”TransactionsoftheAssociationforComputationalLinguistics9:82–99.Fedus,William,BarretZoph,andNoamShazeer.2021.“SwitchTransformers:ScalingtoTrillionParameterModelswithSimpleandEﬃcientSparsity,”arXiv:2101.03961[cs.LG].Felty,Amy,andDaleMiller.1988.“Specifyingtheoremproversinahigher-orderlogicprogramminglanguage.”In9thInternationalConferenceonAutomatedDeduction,editedbyEwingLuskandRossOverbeek,61–80.Berlin,Heidelberg:SpringerBerlinHeidelberg.Feng,Zhangyin,DayaGuo,DuyuTang,NanDuan,XiaochengFeng,MingGong,LinjunShou,etal.2020.“CodeBERT:APre-TrainedModelforProgrammingandNaturalLanguages.”InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,1536–1547.Online:AssociationforComputationalLinguistics.Ferreira,Deborah,andAndréFreitas.2020.“PremiseSelectioninNaturalLanguageMathematicalTexts.”InProceedingsofthe58thAnnualMeet-ingoftheAssociationforComputationalLinguistics,7365–7374.Online:AssociationforComputationalLinguistics.Févry,Thibault,LivioBaldiniSoares,NicholasFitzGerald,EunsolChoi,andTomKwiatkowski.2020.“EntitiesasExperts:SparseMemoryAccesswithEntitySupervision.”InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),4937–4951.Online:AssociationforComputationalLinguistics.Fey,Matthias,JanE.Lenssen,ChristopherMorris,JonathanMasci,andNilsM.Kriege.2020.“DeepGraphMatchingConsensus,”arXiv:2001.09621[cs.LG].Fu,Cong,ChaoXiang,ChangxuWang,andDengCai.2018.“FastApprox-imateNearestNeighborSearchWithTheNavigatingSpreading-outGraph,”arXiv:1707.00143[cs.LG].151Galassi,Andrea,KristianKersting,MarcoLippi,XiaotingShao,andPaoloTorroni.2020.“Neural-SymbolicArgumentationMining:AnArgumentinFavorofDeepLearningandReasoning.”FrontiersinBigData2:52.Ganguly,Soumyajit,andVikramPudi.2017.“Paper2vec:CombiningGraphandTextInformationforScientiﬁcPaperRepresentation.”InAdvancesinInformationRetrieval,editedbyJoemonMJose,ClaudiaHauﬀ,IsmailSengorAltıngovde,DaweiSong,DyaaAlbakour,StuartWatt,andJohnTait,383–395.Cham:SpringerInternationalPublishing.Ganin,Yaroslav,SergeyBartunov,YujiaLi,EthanKeller,andStefanoSal-iceti.2021.“Computer-AidedDesignasLanguage,”arXiv:2105.02769[cs.CV].Ganin,Yaroslav,andVictorLempitsky.2015.“UnsupervisedDomainAdapta-tionbyBackpropagation.”InProceedingsofthe32ndInternationalConfer-enceonMachineLearning,editedbyFrancisBachandDavidBlei,37:1180–1189.ProceedingsofMachineLearningResearch.Lille,France:PMLR.Ganter,Bernhard,andRudolfWille.1997.“AppliedLatticeTheory:For-malConceptAnalysis.”InInGeneralLatticeTheory,G.Grätzereditor,Birkhäuser.Preprints..1999.FormalConceptAnalysis:MathematicalFoundations.1sted.Trans-latedbyC.Franzke.Springer-VerlagBerlinHeidelberg.Garcez,Arturd’Avila,andLuisC.Lamb.2020.“NeurosymbolicAI:The3rdWave,”arXiv:2012.05876[cs.AI].Gärdenfors,Peter.2000.ConceptualSpaces:TheGeometryofThought.TheMITPress.Gentner,Dedre,andKennethD.Forbus.2011.“Computationalmodelsofanalogy.”WIREsCognitiveScience2(3):266–276.eprint:https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcs.105.Giaquinto,Marcus.2015.“TheEpistemologyofVisualThinkinginMathe-matics.”InStanfordEncyclopediaofPhilosophy,editedbyEdwardN.Zalta.https://plato.stanford.edu/archives/spr2020/entries/epistemology-visual-thinking/.152Gilmer,Justin,SamuelS.Schoenholz,PatrickF.Riley,OriolVinyals,andGeorgeE.Dahl.2017.“NeuralMessagePassingforQuantumChemistry.”InProceedingsofthe34thInternationalConferenceonMachineLearning-Volume70,1263–1272.ICML’17.Sydney,NSW,Australia:JMLR.org.Google.2020.“AboutGoogle.”AccessedDecember28,2020.https://about.google/.Goyal,Anirudh,andYoshuaBengio.2021.“InductiveBiasesforDeepLearn-ingofHigher-LevelCognition,”arXiv:2011.15091[cs.LG].Goyal,Anirudh,AniketDidolkar,NanRosemaryKe,CharlesBlundell,PhilippeBeaudoin,NicolasHeess,MichaelMozer,andYoshuaBengio.2021.“Neu-ralProductionSystems,”arXiv:2103.01937[cs.AI].Grohe,Martin.2020.“Word2vec,Node2vec,Graph2vec,X2vec:TowardsaTheoryofVectorEmbeddingsofStructuredData.”InProceedingsofthe39thACMSIGMOD-SIGACT-SIGAISymposiumonPrinciplesofDatabaseSystems,1–16.PODS’20.Portland,OR,USA:AssociationforComputingMachinery.Guarino,Nicola.2009.“TheOntologicalLevel:Revisiting30YearsofKnowl-edgeRepresentation.”InConceptualModeling:FoundationsandApplica-tions:EssaysinHonorofJohnMylopoulos,editedbyAlexanderT.Borgida,VinayK.Chaudhri,PaoloGiorgini,andEricS.Yu,52–67.Berlin,Heidel-berg:SpringerBerlinHeidelberg.Guhe,Markus,AlisonPease,AlanSmaill,MartinSchmidt,HelmarGust,Kai-UweKühnberger,andUlfKrumnack.2010.“Mathematicalreasoningwithhigher-orderanti-unifcation”[inEnglish].InProceedingsofthe32ndAnnualConferenceoftheCognitiveScienceSociety,1992–1997.Gulcehre,Caglar,SarathChandar,KyunghyunCho,andYoshuaBengio.2018.“DynamicNeuralTuringMachinewithContinuousandDiscreteAddress-ingSchemes.”NeuralComput.(Cambridge,MA,USA)30(4):857–884.Guo,Daya,ShuoRen,ShuaiLu,ZhangyinFeng,DuyuTang,ShujieLiu,LongZhou,etal.2021.“GraphCodeBERT:Pre-trainingCodeRepresentationswithDataFlow,”arXiv:2009.08366[cs.SE].Guu,Kelvin,KentonLee,ZoraTung,PanupongPasupat,andMing-WeiChang.2020.“REALM:Retrieval-AugmentedLanguageModelPre-Training,”arXiv:2002.08909[cs.CL].153Han,Yizeng,GaoHuang,ShijiSong,LeYang,HonghuiWang,andYulinWang.2021.“DynamicNeuralNetworks:ASurvey,”arXiv:2102.04906[cs.CV].Härkönen,Erik,AaronHertzmann,JaakkoLehtinen,andSylvainParis.2020.“GANSpace:DiscoveringInterpretableGANControls,”arXiv:2004.02546[cs.CV].Hashimoto,TatsunoriB.,DavidAlvarez-Melis,andTommiS.Jaakkola.2016.“WordEmbeddingsasMetricRecoveryinSemanticSpaces.”TransactionsoftheAssociationforComputationalLinguistics4:273–286.He,Xin,KaiyongZhao,andXiaowenChu.2021.“AutoML:Asurveyofthestate-of-the-art.”Knowledge-BasedSystems212:106622.Heimann,Mark,HaomingShen,TaraSafavi,andDanaiKoutra.2018.“RE-GAL:RepresentationLearning-BasedGraphAlignment.”InProceedingsofthe27thACMInternationalConferenceonInformationandKnowledgeMan-agement,117–126.CIKM’18.Torino,Italy:AssociationforComputingMachinery.Hernandez,Danny,andTomB.Brown.2020.“MeasuringtheAlgorithmicEﬃciencyofNeuralNetworks,”arXiv:2005.04305[cs.LG].Hinton,Geoﬀrey.2021.“Howtorepresentpart-wholehierarchiesinaneuralnetwork,”arXiv:2102.12627[cs.CV].Hofstadter,Douglas.2009.“AnalogyastheCoreofCognition.”PresidentialLecturegivenatStanfordUniversity,Stanford,CA,September10,2009.https://shc.stanford.edu/multimedia/analogy-core-cognition.Hogan,Aidan,EvaBlomqvist,MichaelCochez,Claudiad’Amato,GerarddeMelo,ClaudioGutierrez,JoséEmilioLabraGayo,etal.2021.“KnowledgeGraphs,”arXiv:2003.02320[cs.AI].Hossain,MD.Zakir,FerdousSohel,MohdFairuzShiratuddin,andHamidLaga.2019.“AComprehensiveSurveyofDeepLearningforImageCap-tioning.”ACMComput.Surv.(NewYork,NY,USA)51(6).Howard,Phil.2021.“ComputationalPropagandaResearchProject.”AccessedJanuary16,2021.https://demtech.oii.ox.ac.uk/.154Huang,JamesY.,Kuan-HaoHuang,andKai-WeiChang.2021.“Disentan-glingSemanticsandSyntaxinSentenceEmbeddingswithPre-trainedLanguageModels.”InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,1372–1379.Online:AssociationforComputationalLinguistics.Ivanova,AnnaA.,ShashankSrikant,YotaroSueoka,HopeH.Kean,RivaDhamala,Una-MayO’Reilly,MarinaU.Bers,andEvelinaFedorenko.2020.“Comprehensionofcomputercodereliesprimarilyondomain-generalexecutiveresources.”bioRxiv,eprint:https://www.biorxiv.org/content/early/2020/04/18/2020.04.16.045732.full.pdf.Jaﬀar,Joxan,andMichaelJ.Maher.1994.“Constraintlogicprogramming:asurvey.”SpecialIssue:TenYearsofLogicProgramming,TheJournalofLogicProgramming19-20:503–581.Jaradeh,MohamadYaser,AllardOelen,KheirEddineFarfar,ManuelPrinz,JenniferD’Souza,GáborKismihók,MarkusStocker,andSörenAuer.2019.“OpenResearchKnowledgeGraph:NextGenerationInfrastructureforSemanticScholarlyKnowledge.”InProceedingsofthe10thInternationalConferenceonKnowledgeCapture,243–246.K-CAP’19.MarinaDelRey,CA,USA:AssociationforComputingMachinery.Jaulin,Luc.2006.“ComputingMinimal-VolumeCredibleSetsUsingIntervalAnalysis;ApplicationtoBayesianEstimation.”IEEETransactionsonSignalProcessing54(9):3632–3636.JayaramSubramanya,Suhas,FnuDevvrit,HarshaVardhanSimhadri,Rav-ishankarKrishnawamy,andRohanKadekodi.2019.“DiskANN:FastAccurateBillion-pointNearestNeighborSearchonaSingleNode.”InAdvancesinNeuralInformationProcessingSystems,editedbyH.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-Buc,E.Fox,andR.Garnett,vol.32.CurranAssociates,Inc.Ji,Shaoxiong,ShiruiPan,ErikCambria,PekkaMarttinen,andPhilipS.Yu.2021.“ASurveyonKnowledgeGraphs:Representation,Acquisition,andApplications.”IEEETransactionsonNeuralNetworksandLearningSystems,1–21.155Jiang,Ming,JenniferD’Souza,SörenAuer,andJ.StephenDownie.2020.“ImprovingScholarlyKnowledgeRepresentation:EvaluatingBERT-BasedModelsforScientiﬁcRelationClassiﬁcation.”InDigitalLibrariesatTimesofMassiveSocietalTransition,editedbyEmiIshita,NatalieLeeSanPang,andLihongZhou,3–19.Cham:SpringerInternationalPublishing.Jiang,YiDing,Shixiang(Shane)Gu,KevinPMurphy,andChelseaFinn.2019.“LanguageasanAbstractionforHierarchicalDeepReinforcementLearn-ing.”InAdvancesinNeuralInformationProcessingSystems,editedbyH.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-Buc,E.Fox,andR.Garnett,vol.32.CurranAssociates,Inc.Johnson,Jeﬀ,MatthijsDouze,andHervéJégou.2019.“Billion-scalesimilaritysearchwithGPUs.”IEEETransactionsonBigData,1–1.Joshi,Mandar,DanqiChen,YinhanLiu,DanielS.Weld,LukeZettlemoyer,andOmerLevy.2020.“SpanBERT:ImprovingPre-trainingbyRepresentingandPredictingSpans.”TransactionsoftheAssociationforComputationalLinguistics8:64–77.Kahneman,Daniel.1961.“Ananalyticalmodelofthesemanticdiﬀerential,”UniversityofCalifornia,Berkeley..2011.Thinking,fastandslow.NewYork:Farrar,Straus/Giroux.Kahng,AndrewB.2018.“MachineLearningApplicationsinPhysicalDesign:RecentResultsandDirections.”InProceedingsofthe2018InternationalSymposiumonPhysicalDesign,68–73.ISPD’18.Monterey,California,USA:AssociationforComputingMachinery.Kaiser,Łukasz,andSamyBengio.2018.“DiscreteAutoencodersforSequenceModels,”arXiv:1801.09797[cs.LG].Katharopoulos,Angelos,ApoorvVyas,NikolaosPappas,andFrançoisFleuret.2020.“TransformersareRNNs:FastAutoregressiveTransformerswithLinearAttention,”arXiv:2006.16236[cs.LG].Kato,Akihiko,HiroyukiShindo,andYujiMatsumoto.2016.“ConstructionofanEnglishDependencyCorpusincorporatingCompoundFunctionWords.”InProceedingsoftheTenthInternationalConferenceonLanguageResourcesandEvaluation(LREC’16),1667–1671.Portorož,Slovenia:Euro-peanLanguageResourcesAssociation(ELRA).156Kazemi,SeyedMehran,andDavidPoole.2018.“SimplEEmbeddingforLinkPredictioninKnowledgeGraphs.”InAdvancesinNeuralInformationProcessingSystems,editedbyS.Bengio,H.Wallach,H.Larochelle,K.Grauman,N.Cesa-Bianchi,andR.Garnett,vol.31.CurranAssociates,Inc.Kazi,Anees,LucaCosmo,NassirNavab,andMichaelBronstein.2020.“Dif-ferentiableGraphModule(DGM)forGraphConvolutionalNetworks,”arXiv:2002.04999[cs.LG].Kehagias,Ath.2011.“Someremarksonthelatticeoffuzzyintervals.”Spe-cialIssueonInformationEngineeringApplicationsBasedonLattices,InformationSciences181(10):1863–1873.Keskar,NitishShirish,BryanMcCann,LavR.Varshney,CaimingXiong,andRichardSocher.2019.“CTRL:AConditionalTransformerLanguageModelforControllableGeneration,”arXiv:1909.05858[cs.CL].Khandelwal,Urvashi,OmerLevy,DanJurafsky,LukeZettlemoyer,andMikeLewis.2020.“GeneralizationthroughMemorization:NearestNeighborLanguageModels,”arXiv:1911.00172[cs.CL].Kim,YoungJin,andHanyHassan.2020.“FastFormers:HighlyEﬃcientTrans-formerModelsforNaturalLanguageUnderstanding.”InProceedingsofSustaiNLP:WorkshoponSimpleandEﬃcientNaturalLanguageProcessing,149–158.Online:AssociationforComputationalLinguistics.Kingma,DiederikP.,andPrafullaDhariwal.2018.“Glow:GenerativeFlowwithInvertible1x1Convolutions,”arXiv:1807.03039[stat.ML].Klys,Jack,JakeSnell,andRichardZemel.2018.“LearningLatentSubspacesinVariationalAutoencoders.”InAdvancesinNeuralInformationProcessingSystems,editedbyS.Bengio,H.Wallach,H.Larochelle,K.Grauman,N.Cesa-Bianchi,andR.Garnett,vol.31.CurranAssociates,Inc.Knight,Kevin.1989.“Uniﬁcation:AMultidisciplinarySurvey.”ACMComput.Surv.(NewYork,NY,USA)21(1):93–124.Knuth,Donald.1974.TheArtofComputerProgramming.Addison-Wesley.157Koncel-Kedziorski,Rik,DhanushBekal,YiLuan,MirellaLapata,andHan-nanehHajishirzi.2019.“TextGenerationfromKnowledgeGraphswithGraphTransformers.”InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Hu-manLanguageTechnologies,Volume1(LongandShortPapers),2284–2293.Minneapolis,Minnesota:AssociationforComputationalLinguistics.Kucik,AndrzejStanisław,andKonstantinKorovin.2018.“Premiseselectionwithneuralnetworksanddistributedrepresentationoffeatures,”arXiv:1807.10268[cs.AI].Lamb,LuísC.,Arturd’AvilaGarcez,MarcoGori,MarceloO.R.Prates,PedroH.C.Avelar,andMosheY.Vardi.2020.“GraphNeuralNetworksMeetNeural-SymbolicComputing:ASurveyandPerspective.”InProceedingsoftheTwenty-NinthInternationalJointConferenceonArtiﬁcialIntelligence,IJCAI-20,editedbyChristianBessiere,4877–4884.Surveytrack.Interna-tionalJointConferencesonArtiﬁcialIntelligenceOrganization,July.Lample,Guillaume,AlexandreSablayrolles,Marc’AurelioRanzato,LudovicDenoyer,andHerveJegou.2019.“LargeMemoryLayerswithProductKeys.”InAdvancesinNeuralInformationProcessingSystems,editedbyH.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-Buc,E.Fox,andR.Garnett,vol.32.CurranAssociates,Inc.Lan,Zhenzhong,MingdaChen,SebastianGoodman,KevinGimpel,PiyushSharma,andRaduSoricut.2020.“ALBERT:ALiteBERTforSelf-supervisedLearningofLanguageRepresentations,”arXiv:1909.11942[cs.CL].Łańcucki,Adrian,JanChorowski,GuillaumeSanchez,RicardMarxer,NanxinChen,HansJ.G.A.Dolﬁng,SameerKhurana,TanelAlumäe,andAntoineLaurent.2020.“RobustTrainingofVectorQuantizedBottleneckModels,”arXiv:2005.08520[cs.LG].Larsen,AndersBoesenLindbo,SørenKaaeSønderby,HugoLarochelle,andOleWinther.2016.“AutoencodingbeyondPixelsUsingaLearnedSimi-larityMetric.”InProceedingsofthe33rdInternationalConferenceonInter-nationalConferenceonMachineLearning-Volume48,1558–1566.ICML’16.NewYork,NY,USA:JMLR.org.Lawrence,John,andChrisReed.2020.“ArgumentMining:ASurvey.”Com-putationalLinguistics45(4):765–818.158Lazaridou,Angeliki,andMarcoBaroni.2020.“EmergentMulti-AgentCom-municationintheDeepLearningEra,”arXiv:2006.02419[cs.CL].Lazaridou,Angeliki,AlexanderPeysakhovich,andMarcoBaroni.2017.“Multi-AgentCooperationandtheEmergenceof(Natural)Language,”arXiv:1612.07182[cs.CL].Le,Hung,TruyenTran,andSvethaVenkatesh.2020.“NeuralStored-programMemory.”InInternationalConferenceonLearningRepresentations.Lee,Chelsea.2019.“Welcome,singular“they”.”APAStyle(blog),October31,2019.https://apastyle.apa.org/blog/singular-They.Lee,Haimin.2019.“15yearsofGoogleBooks.”GoogleBlog(blog),October17,2019.https://www.blog.google/products/search/15-years-google-books/.Lee,Kuang-Huei,HamidPalangi,XiChen,HoudongHu,andJianfengGao.2019.“LearningVisualRelationPriorsforImage-TextMatchingandImageCaptioningwithNeuralSceneGraphGenerators,”arXiv:1909.09953[cs.CV].Lewis,Patrick,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKar-pukhin,NamanGoyal,HeinrichKüttler,etal.2020.“Retrieval-AugmentedGenerationforKnowledge-IntensiveNLPTasks.”InAdvancesinNeuralInformationProcessingSystems,editedbyH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,33:9459–9474.CurranAssociates,Inc.Lewis,RichardL.,ShravanVasishth,andJulieA.VanDyke.2006.“Computa-tionalprinciplesofworkingmemoryinsentencecomprehension.”TrendsinCognitiveSciences10(10):447–454.Li,Ruiyu,MakarandTapaswi,RenjieLiao,JiayaJia,RaquelUrtasun,andSanjaFidler.2017.“SituationRecognitionwithGraphNeuralNetworks,”arXiv:1708.04320[cs.CV].Li,Yujia,ChenjieGu,ThomasDullien,OriolVinyals,andPushmeetKohli.2019.“GraphMatchingNetworksforLearningtheSimilarityofGraphStructuredObjects.”InProceedingsofthe36thInternationalConferenceonMachineLearning,editedbyKamalikaChaudhuriandRuslanSalakhutdi-nov,97:3835–3845.ProceedingsofMachineLearningResearch.PMLR.159Li,Yujia,OriolVinyals,ChrisDyer,RazvanPascanu,andPeterBattaglia.2018.“LearningDeepGenerativeModelsofGraphs,”arXiv:1803.03324[cs.LG].Liao,Haiguang,WentaiZhang,XuliangDong,BarnabasPoczos,KenjiShi-mada,andLeventBurakKara.2019.“ADeepReinforcementLearningApproachforGlobalRouting.”JournalofMechanicalDesign142(6).Lieto,Antonio,AntonioChella,andMarcelloFrixione.2017.“ConceptualSpacesforCognitiveArchitectures:Alinguafrancafordiﬀerentlevelsofrepresentation.”BiologicallyInspiredCognitiveArchitectures19:1–9.Liu,Qi,MattJ.Kusner,andPhilBlunsom.2020.“ASurveyonContextualEmbeddings,”arXiv:2003.07278[cs.CL].Liu,Rui,JunjieHu,WeiWei,ZiYang,andEricNyberg.2017.“StructuralEm-beddingofSyntacticTreesforMachineComprehension.”InProceedingsofthe2017ConferenceonEmpiricalMethodsinNaturalLanguageProcess-ing,815–824.Copenhagen,Denmark:AssociationforComputationalLinguistics.Liu,Rui,YuLiu,XinyuGong,XiaogangWang,andHongshengLi.2019.“Con-ditionalAdversarialGenerativeFlowforControllableImageSynthesis.”InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).Liu,Shusen,Peer-TimoBremer,JayaramanJ.Thiagarajan,VivekSrikumar,BeiWang,YardenLivnat,andValerioPascucci.2018.“VisualExplorationofSemanticRelationshipsinNeuralWordEmbeddings.”IEEETransactionsonVisualizationandComputerGraphics24(1):553–562.Liu,Xiaodong,PengchengHe,WeizhuChen,andJianfengGao.2019.“Multi-TaskDeepNeuralNetworksforNaturalLanguageUnderstanding.”InProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,4487–4496.Florence,Italy:AssociationforComputationalLinguistics.Liu,Yinhan,JiataoGu,NamanGoyal,XianLi,SergeyEdunov,MarjanGhaz-vininejad,MikeLewis,andLukeZettlemoyer.2020.“MultilingualDe-noisingPre-trainingforNeuralMachineTranslation.”TransactionsoftheAssociationforComputationalLinguistics8:726–742.160Lu,Jiasen,DhruvBatra,DeviParikh,andStefanLee.2019.“ViLBERT:Pre-trainingTask-AgnosticVisiolinguisticRepresentationsforVision-and-LanguageTasks.”InAdvancesinNeuralInformationProcessingSystems,editedbyH.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-Buc,E.Fox,andR.Garnett,vol.32.CurranAssociates,Inc.Lu,Kevin,AdityaGrover,PieterAbbeel,andIgorMordatch.2021.“PretrainedTransformersasUniversalComputationEngines,”arXiv:2103.05247[cs.LG].Lu,Yichao,PhillipKeung,FaisalLadhak,VikasBhardwaj,ShaonanZhang,andJasonSun.2018.“Aneuralinterlinguaformultilingualmachinetranslation.”InProceedingsoftheThirdConferenceonMachineTranslation:ResearchPapers,84–92.Brussels,Belgium:AssociationforComputationalLinguistics.Luketina,Jelena,NantasNardelli,GregoryFarquhar,JakobFoerster,JacobAndreas,EdwardGrefenstette,ShimonWhiteson,andTimRocktäschel.2019.“ASurveyofReinforcementLearningInformedbyNaturalLan-guage.”InProceedingsoftheTwenty-EighthInternationalJointConferenceonArtiﬁcialIntelligence,IJCAI-19,6309–6317.InternationalJointConfer-encesonArtiﬁcialIntelligenceOrganization,July.Mager,Manuel,RamónFernandezAstudillo,TahiraNaseem,MdArafatSul-tan,Young-SukLee,RaduFlorian,andSalimRoukos.2020.“GPT-too:ALanguage-Model-FirstApproachforAMR-to-TextGeneration.”InPro-ceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,1846–1852.Online:AssociationforComputationalLinguis-tics.Malekmohamadi,Soroor,FaramarzSaﬁ-Esfahani,andMortezaKarimian-kelishadrokhi.2020.“AReviewonNeuralTuringMachine(NTM).”SNComputerScience1(6):333.Malinowski,Mateusz,CarlDoersch,AdamSantoro,andPeterBattaglia.2018.“LearningVisualQuestionAnsweringbyBootstrappingHardAttention.”InProceedingsoftheEuropeanConferenceonComputerVision(ECCV).Martin,W.A.,andR.J.Fateman.1971.“TheMACSYMASystem.”InProceed-ingsoftheSecondACMSymposiumonSymbolicandAlgebraicManipulation,59–75.SYMSAC’71.LosAngeles,California,USA:AssociationforCom-putingMachinery.161Martinez,M.,A.M.H.Abdel-Fattah,U.Krumnack,D.Gómez-Ramírez,A.Smaill,T.R.Besold,A.Pease,M.Schmidt,M.Guhe,andK.-U.Kühnberger.2017.“Theoryblending:extendedalgorithmicaspectsandexamples.”AnnalsofMathematicsandArtiﬁcialIntelligence80(1):65–89.Martino,GiovanniDaSan,StefanoCresci,AlbertoBarrón-Cedeño,SeunghakYu,RobertoDiPietro,andPreslavNakov.2020.“ASurveyonCom-putationalPropagandaDetection.”InProceedingsoftheTwenty-NinthInternationalJointConferenceonArtiﬁcialIntelligence,IJCAI-20,editedbyChristianBessiere,4826–4832.Surveytrack.InternationalJointConfer-encesonArtiﬁcialIntelligenceOrganization,July.McCann,Bryan,NitishShirishKeskar,CaimingXiong,andRichardSocher.2018.“TheNaturalLanguageDecathlon:MultitaskLearningasQuestionAnswering,”arXiv:1806.08730[cs.CL].McShane,Marjorie,andSergeiNirenburg.2012.“AKnowledgeRepresenta-tionLanguageforNaturalLanguageProcessing,SimulationandReason-ing.”InternationalJournalofSemanticComputing06(01):3–23.eprint:https://doi.org/10.1142/S1793351X12400016.Medina,Jesús,ManuelOjeda-Aciego,andPeterVojtáš.2004.“Similarity-baseduniﬁcation:amulti-adjointapproach.”SelectedPapersfromEUSFLAT2001,FuzzySetsandSystems146(1):43–62.Menezes,Telmo,andCamilleRoth.2021.“SemanticHypergraphs,”arXiv:1908.10784[cs.IR].Messick,SamuelJ.1957.“MetricPropertiesoftheSemanticDiﬀerential.”EducationalandPsychologicalMeasurement17(2):200–206.Miech,Antoine,Jean-BaptisteAlayrac,IvanLaptev,JosefSivic,andAndrewZisserman.2021.“ThinkingFastandSlow:EﬃcientText-to-VisualRe-trievalwithTransformers,”arXiv:2103.16553[cs.CV].Mikolov,Tomas,Wen-tauYih,andGeoﬀreyZweig.2013.“LinguisticRegu-laritiesinContinuousSpaceWordRepresentations.”InProceedingsofthe2013ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnologies,746–751.Atlanta,Georgia:AssociationforComputationalLinguistics.Min,Sewon,DanqiChen,LukeZettlemoyer,andHannanehHajishirzi.2020.“KnowledgeGuidedTextRetrievalandReadingforOpenDomainQues-tionAnswering,”arXiv:1911.03868[cs.CL].162Minervini,Pasquale,MatkoBosnjak,TimRocktäschel,andSebastianRiedel.2018.“TowardsNeuralTheoremProvingatScale,”arXiv:1807.08204[cs.AI].Minervini,Pasquale,MatkoBošnjak,TimRocktäschel,SebastianRiedel,andEdwardGrefenstette.2019.“DiﬀerentiableReasoningonLargeKnowl-edgeBasesandNaturalLanguage,”arXiv:1912.10824[cs.LG]..2020.“Diﬀerentiablereasoningonlargeknowledgebasesandnaturallanguage.”InProceedingsoftheAAAIconferenceonartiﬁcialintelligence,34:5182–5190.04.Mitra,Bhaskar,andNickCraswell.2018.“AnIntroductiontoNeuralInfor-mationRetrieval.”FoundationsandTrends®inInformationRetrieval13(1):1–126.Mo,Kaichun,PaulGuerrero,LiYi,HaoSu,PeterWonka,NiloyJ.Mitra,andLeonidasJ.Guibas.2019.“StructureNet:HierarchicalGraphNetworksfor3DShapeGeneration.”ACMTrans.Graph.(NewYork,NY,USA)38(6).Moens,Marie-Francine.2018.“Argumentationmining:Howcanamachineacquirecommonsenseandworldknowledge?”Argument&Computation9(1):1–14.Mordatch,Igor,andPieterAbbeel.2018.“EmergenceofGroundedCom-positionalLanguageinMulti-AgentPopulations,”arXiv:1703.04908[cs.AI].Moses,Joel.2012.“Macsyma:Apersonalhistory.”JournalofSymbolicCompu-tation47(2):123–130.Munroe,Randall.2012.“WordsforSmallSets.”XKCD(blog),June20,2012.https://xkcd.com/1070/.Narayanan,Annamalai,MahinthanChandramohan,RajasekarVenkatesan,LihuiChen,YangLiu,andShantanuJaiswal.2017.“graph2vec:LearningDistributedRepresentationsofGraphs,”arXiv:1707.05005[cs.AI].Nguyen,Xuan-Phi,ShaﬁqJoty,StevenC.H.Hoi,andRichardSocher.2020.“Tree-structuredAttentionwithHierarchicalAccumulation,”arXiv:2002.08046[cs.LG].163Noble,James,AlexPotanin,TobyMurray,andMarkS.Miller.2018.“AbstractandConcreteDataTypesvsObjectCapabilities.”InPrincipledSoftwareDevelopment:EssaysDedicatedtoArndPoetzsch-HeﬀterontheOccasionofhis60thBirthday,editedbyPeterMüllerandInaSchaefer,221–240.Cham:SpringerInternationalPublishing.Noé,Frank,AlexandreTkatchenko,Klaus-RobertMüller,andCeciliaClementi.2020.“MachineLearningforMolecularSimulation.”AnnualReviewofPhysicalChemistry,361–390.Nurse,Derek,andGérardPhilippson.2006.“Commontense-aspectmarkersinBantu.”JournalofAfricanLanguagesandLinguistics27(2):155–196.Odena,Augustus,KensenShi,DavidBieber,RishabhSingh,andCharlesSut-ton.2020.“BUSTLE:Bottom-upprogram-SynthesisThroughLearning-guidedExploration,”arXiv:2007.14381[cs.PL].Oh,Sangeun,YongsuJung,SeongsinKim,IkjinLee,andNamwooKang.2019.“DeepGenerativeDesign:IntegrationofTopologyOptimizationandGenerativeModels.”JournalofMechanicalDesign141(11).Older,William,andAndréVellino.1990.“ExtendingPrologwithConstraintArithmeticonRealIntervals.”InInCanadianConferenceonComputer&ElectricalEngineering.SocietyPress.Oord,Aaronvanden,YazheLi,andOriolVinyals.2019.“RepresentationLearningwithContrastivePredictiveCoding,”arXiv:1807.03748[cs.LG].Oord,Aaronvanden,OriolVinyals,andKorayKavukcuoglu.2018.“NeuralDiscreteRepresentationLearning,”arXiv:1711.00937[cs.LG].OpenAI,ChristopherBerner,GregBrockman,BrookeChan,VickiCheung,PrzemysławDe¸biak,ChristyDennison,etal.2019.“Dota2withLargeScaleDeepReinforcementLearning,”arXiv:1912.06680[cs.LG].Pan,Shirui,RuiqiHu,Sai-FuFung,GuodongLong,JingJiang,andChengqiZhang.2020.“LearningGraphEmbeddingWithAdversarialTrainingMethods.”IEEETransactionsonCybernetics50(6):2475–2487.Pan,Shirui,RuiqiHu,GuodongLong,JingJiang,LinaYao,andChengqiZhang.2018.“AdversariallyRegularizedGraphAutoencoderforGraphEmbedding.”InProceedingsoftheTwenty-SeventhInternationalJointCon-ferenceonArtiﬁcialIntelligence,IJCAI-18,2609–2615.InternationalJointConferencesonArtiﬁcialIntelligenceOrganization,July.164Patashnik,Or,ZongzeWu,EliShechtman,DanielCohen-Or,andDaniLischin-ski.2021.“StyleCLIP:Text-DrivenManipulationofStyleGANImagery,”arXiv:2103.17249[cs.CV].Paulson,LawrenceC.1986.“Naturaldeductionashigher-orderresolution.”TheJournalofLogicProgramming3(3):237–258.Pearce,Hammond,BaleeghAhmad,BenjaminTan,BrendanDolan-Gavitt,andRameshKarri.2021.AnEmpiricalCybersecurityEvaluationofGitHubCopilot’sCodeContributions.arXiv:2108.09293[cs.CR].Peng,Wei,TuomasVaranka,AbdelrahmanMostafa,HenglinShi,andGuoyingZhao.2021.“HyperbolicDeepNeuralNetworks:ASurvey,”arXiv:2101.04562[cs.LG].Polosukhin,Illia,andAlexanderSkidanov.2018.“NeuralProgramSearch:SolvingProgrammingTasksfromDescriptionandExamples,”arXiv:1802.04335[cs.AI].Polu,Stanislas,andIlyaSutskever.2020.“GenerativeLanguageModelingforAutomatedTheoremProving,”arXiv:2009.03393[cs.LG].Pólya,G.1990.MathematicsandPlausibleReasoning:Inductionandanalogyinmathematics.InductionandAnalogyinMathematics.PrincetonUniversityPress.Prato,Gabriele,EllaCharlaix,andMehdiRezagholizadeh.2020.“FullyQuan-tizedTransformerforMachineTranslation,”arXiv:1910.10485[cs.CL].preﬁxsuﬃx.com.2008.“EnglishLanguageRoots.”AccessedMarch17,2008.http://www.prefixsuffix.com/rootchart.php.Puri,Ruchir.2021.“KickstartingAIforCode:IntroducingIBM’sProjectCodeNet.”IBMResearch.AccessedMay10,2021.https://research.ibm.com/blog/codenet-ai-for-code.Qu,Meng,JianTang,andYoshuaBengio.2019.“Weakly-supervisedKnowl-edgeGraphAlignmentwithAdversarialLearning,”arXiv:1907.03179[cs.LG].Raboh,Moshiko,RoeiHerzig,GalChechik,JonathanBerant,andAmirGlober-son.2020.“DiﬀerentiableSceneGraphs,”arXiv:1902.10200[cs.CV].165Radford,Alec,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,etal.2021.“LearningTransferableVi-sualModelsFromNaturalLanguageSupervision,”arXiv:2103.00020[cs.CV].Radosavovic,Ilija,RajPrateekKosaraju,RossGirshick,KaimingHe,andPiotrDollár.2020.“DesigningNetworkDesignSpaces,”arXiv:2003.13678[cs.CV].Rae,JackW.,AnnaPotapenko,SiddhantM.Jayakumar,andTimothyP.Lilli-crap.2019.“CompressiveTransformersforLong-RangeSequenceMod-elling,”arXiv:1911.05507[cs.LG].Raghu,Maithra,andEricSchmidt.2020.“ASurveyofDeepLearningforScientiﬁcDiscovery,”arXiv:2003.11755[cs.LG].Raina,Ayush,JonathanCagan,andChristopherMcComb.2019.“Transfer-ringDesignStrategiesFromHumantoComputerandAcrossDesignProblems.”JournalofMechanicalDesign141(11).Raina,Ayush,ChristopherMcComb,andJonathanCagan.2019.“LearningtoDesignFromHumans:ImitatingHumanDesignersThroughDeepLearning.”JournalofMechanicalDesign141(11).Raposo,David,AdamSantoro,DavidBarrett,RazvanPascanu,TimothyLilli-crap,andPeterBattaglia.2017.“Discoveringobjectsandtheirrelationsfromentangledscenerepresentations,”arXiv:1702.05068[cs.LG].Ratner,Alex,BradenHancock,JaredDunnmon,RogerGoldman,andChristo-pherRé.2018.“SnorkelMeTaL:WeakSupervisionforMulti-TaskLearn-ing.”InProceedingsoftheSecondWorkshoponDataManagementforEnd-To-EndMachineLearning.DEEM’18.Houston,TX,USA:AssociationforComputingMachinery.Ratner,Alexander,StephenH.Bach,HenryEhrenberg,JasonFries,SenWu,andChristopherRé.2020.“Snorkel:rapidtrainingdatacreationwithweaksupervision.”TheVLDBJournal29(2):709–730.Razavi,Ali,AaronvandenOord,andOriolVinyals.2019.“GeneratingDiverseHigh-FidelityImageswithVQ-VAE-2,”arXiv:1906.00446[cs.LG].166Real,Esteban,ChenLiang,DavidSo,andQuocLe.2020.“AutoML-Zero:EvolvingMachineLearningAlgorithmsFromScratch.”InProceedingsofthe37thInternationalConferenceonMachineLearning,editedbyHalDauméIIIandAartiSingh,119:8007–8019.ProceedingsofMachineLearningResearch.PMLR.Ren,Pengzhen,YunXiao,XiaojunChang,Po-yaoHuang,ZhihuiLi,XiaojiangChen,andXinWang.2021.“AComprehensiveSurveyofNeuralArchi-tectureSearch:ChallengesandSolutions.”ACMComput.Surv.(NewYork,NY,USA)54(4).Rezaeinia,SeyedMahdi,AliGhodsi,andRouhollahRahmani.2017.“Im-provingtheAccuracyofPre-trainedWordEmbeddingsforSentimentAnalysis,”arXiv:1711.08609[cs.CL].Ribeiro,LeonardoF.R.,MartinSchmitt,HinrichSchütze,andIrynaGurevych.2020.“InvestigatingPretrainedLanguageModelsforGraph-to-TextGen-eration,”arXiv:2007.08426[cs.CL].Rocktäschel,Tim,andSebastianRiedel.2016.“LearningKnowledgeBaseInferencewithNeuralTheoremProvers.”InProceedingsofthe5thWork-shoponAutomatedKnowledgeBaseConstruction,45–50.SanDiego,CA:AssociationforComputationalLinguistics..2017.“End-to-endDiﬀerentiableProving.”InAdvancesinNeuralInformationProcessingSystems,editedbyI.Guyon,U.V.Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,vol.30.CurranAssociates,Inc.Rosenbaum,Clemens,IgnacioCases,MatthewRiemer,andTimKlinger.2019.“RoutingNetworksandtheChallengesofModularandCompositionalComputation,”arXiv:1904.12774[cs.LG].Russell,Stuart.2019.Humancompatible:Artiﬁcialintelligenceandtheproblemofcontrol.Penguin.Sagara,Moriji,KazuoYamamoto,HirohikoNishimura,andHiroshiAkuto.1961.“AStudyontheSemanticStructureofJapaneseLanguagebytheSemanticDiﬀerentialMethod.”JapanesePsychologicalResearch3(3):146–156.Salehi,Amin,andHasanDavulcu.2020.“GraphAttentionAuto-Encoders.”In2020IEEE32ndInternationalConferenceonToolswithArtiﬁcialIntelligence(ICTAI),989–996.167Sanh,Victor,ThomasWolf,andSebastianRuder.2018.“AHierarchicalMulti-taskApproachforLearningEmbeddingsfromSemanticTasks,”arXiv:1811.06031[cs.CL].Santoro,Adam,SergeyBartunov,MatthewBotvinick,DaanWierstra,andTim-othyLillicrap.2016.“Meta-LearningwithMemory-AugmentedNeuralNetworks.”InProceedingsofthe33rdInternationalConferenceonInterna-tionalConferenceonMachineLearning-Volume48,1842–1850.ICML’16.NewYork,NY,USA:JMLR.org.Sarlin,Paul-Edouard,DanielDeTone,TomaszMalisiewicz,andAndrewRa-binovich.2020.“SuperGlue:LearningFeatureMatchingWithGraphNeuralNetworks.”In2020IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),4937–4946.Schlag,Imanol,PaulSmolensky,RolandFernandez,NebojsaJojic,JürgenSchmidhuber,andJianfengGao.2020.“EnhancingtheTransformerwithExplicitRelationalEncodingforMathProblemSolving,”arXiv:1910.06611[cs.LG].Schramowski,Patrick,CigdemTuran,SophieJentzsch,ConstantinRothkopf,andKristianKersting.2019.“BERThasaMoralCompass:Improvementsofethicalandmoralvaluesofmachines,”arXiv:1912.05238[cs.CL].Schwenk,Holger,andMatthijsDouze.2017.“LearningJointMultilingualSentenceRepresentationswithNeuralMachineTranslation.”InProceed-ingsofthe2ndWorkshoponRepresentationLearningforNLP,157–167.Vancouver,Canada:AssociationforComputationalLinguistics.Sessa,MariaI.2002.“Approximatereasoningbysimilarity-basedSLDresolu-tion.”TheoreticalComputerScience275(1):389–426.Shah,Pararth,MarekFiser,AleksandraFaust,J.ChaseKew,andDilekHakkani-Tur.2018.“FollowNet:RobotNavigationbyFollowingNaturalLan-guageDirectionswithDeepReinforcementLearning,”arXiv:1805.06150[cs.RO].Shen,Yujun,JinjinGu,XiaoouTang,andBoleiZhou.2020.“InterpretingtheLatentSpaceofGANsforSemanticFaceEditing.”In2020IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),9240–9249.168Shi,Jiaxin,HanwangZhang,andJuanziLi.2019.“ExplainableandExplicitVisualReasoningOverSceneGraphs.”In2019IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),8368–8376.Shiv,Vighnesh,andChrisQuirk.2019.“Novelpositionalencodingstoenabletree-basedtransformers.”InAdvancesinNeuralInformationProcessingSystems,editedbyH.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-Buc,E.Fox,andR.Garnett,vol.32.CurranAssociates,Inc.Simak,CliﬀordD.2016.City(1952).Gateway.Simon,HerbertA.1988.“TheScienceofDesign:CreatingtheArtiﬁcial.”DesignIssues4(1/2):67–82.Simonovsky,Martin,andNikosKomodakis.2018.“GraphVAE:TowardsGenerationofSmallGraphsUsingVariationalAutoencoders,”arXiv:1802.03480[cs.LG].Singh,PremKumar,C.AswaniKumar,andJinhaiLi.2016.“KnowledgeRepresentationUsingInterval-ValuedFuzzyFormalConceptLattice.”SoftComputing20(4):1485–1502.Slonim,Noam,YonatanBilu,CarlosAlzate,RoyBar-Haim,BenBogin,Fran-cescaBonin,LeshemChoshen,etal.2021.“Anautonomousdebatingsystem.”Nature591(7850):379–384.Smolka,Gert.1992.“Feature-constraintlogicsforuniﬁcationgrammars.”TheJournalofLogicProgramming12(1):51–87.Solaiman,Irene,MilesBrundage,JackClark,AmandaAskell,ArielHerbert-Voss,JeﬀWu,AlecRadford,etal.2019.“ReleaseStrategiesandtheSocialImpactsofLanguageModels,”arXiv:1908.09203[cs.CL].Stump,GaryM.,SimonW.Miller,MichaelA.Yukish,TimothyW.Simpson,andConradTucker.2019.“SpatialGrammar-BasedRecurrentNeuralNet-workforDesignFormandBehaviorOptimization.”JournalofMechanicalDesign141(12).Sun,Philip.2020.“AnnouncingScaNN:EﬃcientVectorSimilaritySearch.”GoogleAIBlog(blog),July28,2020.https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html.169Svyatkovskiy,Alexey,SebastianLee,AnnaHadjitoﬁ,MaikRiechert,JulianaFranco,andMiltiadisAllamanis.2021.“FastandMemory-EﬃcientNeu-ralCodeCompletion.”InThe2021MiningSoftwareRepositoriesConfer-ence.arXiv:2004.13651[cs.SE].Szabó,ZoltánGendler.2017.“Compositionality.”InStanfordEncyclopediaofPhilosophy,editedbyEdwardNZalta.Szegedy,Christian.2020.“Apromisingpathtowardsautoformalizationandgeneralartiﬁcialintelligence.”InInternationalConferenceonIntelligentComputerMathematics,3–20.Springer.Tamkin,Alex,DanJurafsky,andNoahGoodman.2020.“LanguageThroughaPrism:ASpectralApproachforMultiscaleLanguageRepresentations.”InAdvancesinNeuralInformationProcessingSystems,editedbyH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,33:5492–5504.CurranAssociates,Inc.Tarlow,Daniel,SubhodeepMoitra,AndrewRice,ZiminChen,Pierre-AntoineManzagol,CharlesSutton,andEdwardAftandilian.2020.“LearningtoFixBuildErrorswithGraph2DiﬀNeuralNetworks.”InProceedingsoftheIEEE/ACM42ndInternationalConferenceonSoftwareEngineeringWorkshops,19–20.ICSEW’20.Seoul,RepublicofKorea:AssociationforComputingMachinery.Tee,Naeem.2021.“What’sTheAverageFreelanceWriter’sPricePerWord?”Contentﬂy(blog),April9,2021.https://contentfly.com/blog/whats-the-average-freelance-writers-price-per-word.Thorne,James,MajidYazdani,MarziehSaeidi,FabrizioSilvestri,SebastianRiedel,andAlonHalevy.2020.“NeuralDatabases,”arXiv:2010.06973[cs.CL].Torquato,S.,andF.H.Stillinger.2006.“Exactlysolvabledisorderedsphere-packingmodelinarbitrary-dimensionalEuclideanspaces.”PhysicalRe-viewE73(3).Tousch,Anne-Marie,StéphaneHerbin,andJean-YvesAudibert.2008.“Se-manticLatticesforMultipleAnnotationofImages.”InProceedingsofthe1stACMInternationalConferenceonMultimediaInformationRetrieval,342–349.MIR’08.Vancouver,BritishColumbia,Canada:AssociationforComputingMachinery.170Tran,Chau,YuqingTang,XianLi,andJiataoGu.2020.“Cross-lingualRe-trievalforIterativeSelf-SupervisedTraining.”InAdvancesinNeuralInfor-mationProcessingSystems,editedbyH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,33:2207–2219.CurranAssociates,Inc.Tripathi,Subarna,AnahitaBhiwandiwalla,AlexeiBastidas,andHanlinTang.2019.“UsingSceneGraphContexttoImproveImageGeneration,”arXiv:1901.03762[cs.CV].Trivedi,Rakshit,JiachenYang,andHongyuanZha.2020.“GraphOpt:Learn-ingOptimizationModelsofGraphFormation,”arXiv:2007.03619[cs.LG].Turovsky,Barak.2016.“TenyearsofGoogleTranslate.”Google:TheKeyword(blog),April28,2016.https://www.blog.google/products/translate/ten-years-of-google-translate.VanLierde,H.,andTommyW.S.Chow.2019.“Query-orientedtextsum-marizationbasedonhypergraphtransversals.”InformationProcessing&Management56(4):1317–1338.Vaswani,Ashish,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,undeﬁnedukaszKaiser,andIlliaPolosukhin.2017.“AttentionisAllYouNeed.”InProceedingsofthe31stInternationalConfer-enceonNeuralInformationProcessingSystems,6000–6010.NIPS’17.LongBeach,California,USA:CurranAssociatesInc.Veličković,Petar,andCharlesBlundell.2021.“NeuralAlgorithmicReason-ing,”arXiv:2105.02761[cs.LG].Veličković,Petar,LarsBuesing,MatthewOverlan,RazvanPascanu,OriolVinyals,andCharlesBlundell.2020.“PointerGraphNetworks.”InAd-vancesinNeuralInformationProcessingSystems,editedbyH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,33:2232–2244.CurranAssociates,Inc.Velikovich,Leonid,IanWilliams,JustinScheiner,PetarAleksic,PedroMoreno,andMichaelRiley.2018.“SemanticLatticeProcessinginContextualAutomaticSpeechRecognitionforGoogleAssistant.”InInterspeech2018,2222–2226.https://www.isca-speech.org/archive/Interspeech_2018/pdfs/2453.pdf.Verga,Pat,HaitianSun,LivioBaldiniSoares,andWilliamW.Cohen.2020.“FactsasExperts:AdaptableandInterpretableNeuralMemoryoverSym-bolicKnowledge,”arXiv:2007.00849[cs.CL].171Vicente,Agustin.2018.“Polysemyandwordmeaning:anaccountoflexicalmeaningfordiﬀerentkindsofcontentwords.”PhilosophicalStudies175(4):947–968.Vinyals,Oriol,IgorBabuschkin,WojciechM.Czarnecki,MichaëlMathieu,An-drewDudzik,JunyoungChung,DavidH.Choi,etal.2019.“GrandmasterLevelinStarCraftIIUsingMulti-AgentReinforcementLearning.”Nature575(7782):350–354.Wang,Jizhe,PipeiHuang,HuanZhao,ZhiboZhang,BinqiangZhao,andDikLunLee.2018.“Billion-ScaleCommodityEmbeddingforE-CommerceRecommendationinAlibaba.”InProceedingsofthe24thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining,839–848.KDD’18.London,UnitedKingdom:AssociationforComputingMachinery.Wang,Ke,andMihaiChristodorescu.2019.“COSET:ABenchmarkforEvalu-atingNeuralProgramEmbeddings,”arXiv:1905.11445[cs.LG].Wang,Mingzhe,YiheTang,JianWang,andJiaDeng.2017.“PremiseSelectionforTheoremProvingbyDeepGraphEmbedding.”InAdvancesinNeuralInformationProcessingSystems,editedbyI.Guyon,U.V.Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,vol.30.CurranAssociates,Inc.Wang,Shenghui,andRobKoopman.2017.“Semanticembeddingforinforma-tionretrieval.”InBIR2017WorkshoponBibliometric-enhancedInformationRetrieval.Wang,Wenhan,SijieShen,GeLi,andZhiJin.2020.“TowardsFull-lineCodeCompletionwithNeuralLanguageModels,”arXiv:2009.08603[cs.SE].Wannenwetsch,AnneS.,MartinKiefel,PeterV.Gehler,andStefanRoth.2019.“LearningTask-SpeciﬁcGeneralizedConvolutionsinthePermutohedralLattice,”arXiv:1909.03677[cs.CV].Watters,Nicholas,DanielZoran,TheophaneWeber,PeterBattaglia,Raz-vanPascanu,andAndreaTacchetti.2017.“VisualInteractionNetworks:LearningaPhysicsSimulatorfromVideo.”InAdvancesinNeuralInfor-mationProcessingSystems,editedbyI.Guyon,U.V.Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,vol.30.CurranAssociates,Inc.172Weber,Leon,PasqualeMinervini,JannesMünchmeyer,UlfLeser,andTimRocktäschel.2019.“NLProlog:ReasoningwithWeakUniﬁcationforQues-tionAnsweringinNaturalLanguage.”InProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,6151–6161.Flo-rence,Italy:AssociationforComputationalLinguistics.Wieting,John,GrahamNeubig,andTaylorBerg-Kirkpatrick.2020.“ABilin-gualGenerativeTransformerforSemanticSentenceEmbedding.”InPro-ceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),1581–1594.AssociationforComputationalLinguis-tics.Wikipedia.2021.Listofmedicalroots,suﬃxesandpreﬁxes—Wikipedia,TheFreeEncyclopedia.http://en.wikipedia.org/w/index.php?title=List%20of%20medical%20roots%2C%20suffixes%20and%20prefixes&oldid=1029902137.[Online;accessed23-June-2021].Wilkins,John.1668.“AnEssayTowardsaRealCharacter,andaPhilosophicalLanguage.”Wolchover,N.2015.Concernsofanartiﬁcialintelligencepioneer.Quanta.April21.Woolley,Sam,andPhilHoward.2017.ComputationalPropagandaWorldwide:ExecutiveSummary.WorkingPaper2017.11.OxfordInternetInstitute,UniversityofOxford.https://www.oii.ox.ac.uk/blog/computational-propaganda-worldwide-executive-summary/.Wu,Zonghan,ShiruiPan,FengwenChen,GuodongLong,ChengqiZhang,andPhilipSYu.2021.“AComprehensiveSurveyonGraphNeuralNetworks.”IEEEtransactionsonneuralnetworksandlearningsystems32:4–24.Xu,Peng,MostofaPatwary,MohammadShoeybi,RaulPuri,PascaleFung,AnimaAnandkumar,andBryanCatanzaro.2020.“MEGATRON-CNTRL:ControllableStoryGenerationwithExternalKnowledgeUsingLarge-ScaleLanguageModels.”InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),2831–2845.Online:AssociationforComputationalLinguistics.Yang,GuangyuRobert,MadhuraR.Joglekar,H.FrancisSong,WilliamT.Newsome,andXiao-JingWang.2019.“TaskRepresentationsinNeuralNetworksTrainedtoPerformManyCognitiveTasks.”NatureNeuroscience22(2):297–306.173Yang,Jianwei,JiasenLu,StefanLee,DhruvBatra,andDeviParikh.2018.“GraphR-CNNforSceneGraphGeneration,”arXiv:1808.00191[cs.CV].Yernaux,Gonzague,andWimVanhoof.2019.“Anti-uniﬁcationinConstraintLogicProgramming.”TheoryandPracticeofLogicProgramming19(5-6):773–789.Yogatama,Dani,CypriendeMassond’Autume,andLingpengKong.2021.“AdaptiveSemiparametricLanguageModels.”TransactionsoftheAssocia-tionforComputationalLinguistics9:362–373.You,Jiaxuan,ZhitaoYing,andJureLeskovec.2020.“DesignSpaceforGraphNeuralNetworks.”InAdvancesinNeuralInformationProcessingSystems,editedbyH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,33:17009–17021.CurranAssociates,Inc.Yu,Dongfei,JianlongFu,TaoMei,andYongRui.2017.“Multi-levelAttentionNetworksforVisualQuestionAnswering.”In2017IEEEConferenceonComputerVisionandPatternRecognition(CVPR),4187–4195.Yun,Seongjun,MinbyulJeong,RaehyunKim,JaewooKang,andHyunwooJKim.2019.“GraphTransformerNetworks.”InAdvancesinNeuralInfor-mationProcessingSystems,editedbyH.Wallach,H.Larochelle,A.Beygelz-imer,F.d’Alché-Buc,E.Fox,andR.Garnett,vol.32.CurranAssociates,Inc.Zaheer,Manzil,GuruGuruganesh,KumarAvinavaDubey,JoshuaAinslie,ChrisAlberti,SantiagoOntanon,PhilipPham,etal.2020.“BigBird:TransformersforLongerSequences.”InAdvancesinNeuralInformationProcessingSystems,editedbyH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,33:17283–17297.CurranAssociates,Inc.Zellers,Rowan,MarkYatskar,SamThomson,andYejinChoi.2018.“NeuralMotifs:SceneGraphParsingwithGlobalContext.”In2018IEEE/CVFConferenceonComputerVisionandPatternRecognition,5831–5840.Zhang,Haijun,ShuangWang,XiaofeiXu,TommyW.S.Chow,andQ.M.Jon-athanWu.2018.“Tree2Vector:LearningaVectorialRepresentationforTree-StructuredData.”IEEETransactionsonNeuralNetworksandLearningSystems29(11):5304–5318.174Zhang,Wei,LuHou,YichunYin,LifengShang,XiaoChen,XinJiang,andQunLiu.2020.“TernaryBERT:Distillation-awareUltra-lowBitBERT.”InPro-ceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),509–521.Online:AssociationforComputationalLinguistics.Zhang,Wen,BibekPaudel,WeiZhang,AbrahamBernstein,andHuajunChen.2019.“InteractionEmbeddingsforPredictionandExplanationinKnowledgeGraphs.”InProceedingsoftheTwelfthACMInternationalConferenceonWebSearchandDataMining,96–104.WSDM’19.MelbourneVIC,Australia:AssociationforComputingMachinery.Zhang,Ye,MdMustaﬁzurRahman,AlexBraylan,BrandonDang,Heng-LuChang,HennaKim,QuintenMcNamara,etal.2017.“NeuralInformationRetrieval:ALiteratureReview,”arXiv:1611.06792[cs.IR].Zhang,Yian,AlexWarstadt,Haau-SingLi,andSamuelR.Bowman.2020.“WhenDoYouNeedBillionsofWordsofPretrainingData?,”arXiv:2011.04946[cs.CL].Zhao,Yang,PingYu,SuchismitMahapatra,QinliangSu,andChangyouChen.2021.“ImproveVariationalAutoencoderforTextGenerationwithDis-creteLatentBottleneck,”arXiv:2004.10603[cs.LG].Zheng,Cheng,BoZong,WeiCheng,DongjinSong,JingchaoNi,WenchaoYu,HaifengChen,andWeiWang.2020.“RobustGraphRepresentationLearningviaNeuralSparsiﬁcation.”InProceedingsofthe37thInternationalConferenceonMachineLearning,editedbyHalDauméIIIandAartiSingh,119:11458–11468.ProceedingsofMachineLearningResearch.PMLR.Zhou,Jie,GanquCui,ShengdingHu,ZhengyanZhang,ChengYang,ZhiyuanLiu,LifengWang,ChangchengLi,andMaosongSun.2020.“Graphneuralnetworks:Areviewofmethodsandapplications.”AIOpen1:57–81.Zhou,Zhenpeng,StevenKearnes,LiLi,RichardN.Zare,andPatrickRiley.2019.“OptimizationofMoleculesviaDeepReinforcementLearning.”ScientiﬁcReports9(1):10752.175176