AI and Disinformation

Author: Shaan Subramaniam                  Editor: Gregory Milne             Editor-in-Chief: Chandan Seth Nanda

The Bulletin of the Atomic Scientists is one of the oldest organisations dedicated to raising awareness of existential risk. Founded by biophysicists who had participated in the Manhattan Project, the Bulletin aims to provide the public with reliable information about technological developments that endanger the human species, historically focussing on the risks of nuclear war. Writing in the June 2019 issue of the Bulletin, Herbert Lin, a research fellow at the Center for International Security and Cooperation, proposed a new existential risk he described as cyber-enabled information warfare (1).

Far from simply acting as a force-multiplier for the existing risks to the species, Lin claimed that emerging information technologies posed a distinct threat in their own right, with the power to shatter the pillars of modern democratic governments: logic, truth and reality. If unmet, these forces were poised to usher in a global information dystopia, paralysing governments as waves of conspiracies fracture citizens into information siloes. A key development, Lin stressed, was that of highly accessible AI-assisted Deepfake technologies that allow almost anyone to create realistic forgeries at low cost. These technologies would serve as a powerful vector for disinformationfalsehoods deliberately created to harm individuals or organisations.

Despite these dire warnings, Deepfakesvideos that use deep learning algorithms to manipulate the identities of people featured in themdo not appear to be corroding the fabric of society just yet. Indeed, according to research from the Deepfake monitoring platform Sensity, the majority of Deepfakes produced thus far have no political valence and are made purely for entertainment (including a large NSFW genre) (2). Additionally, some of the most viral Deepfakes are educational in nature, produced to warn the public of the danger of these technologies (see, for example, Buzzfeeds potty-mouthed Obama or Sassy Justices Deepfake Trump as an investigative reporter).

With high apparent public awareness and a paucity of impactful politically-motivated Deepfakes so far, are the concerns of scholars like Lin justified? Just how good will these tools get, and what can be done to limit their negative impacts?

The technologies behind AI-assisted disinformation

Neural networks are algorithms that can perform many tasks including learning to classify images into some category (e.g. Cat or Dog) or transform a series of input numbers into a format mathematically identical to the pixels in an image.  

Technologies that have proven most effective at video and audio manipulation fall into the category of neural networks known as deep generative models. Within this group, variational autoencoders (VAEs) and generative adversarial networks (GANs) have performed best at learning how to produce images of almost any object. 

VAEs are a type of neural network used to create Deepfake videos (3). They are split into two parts called the encoder and the decoder. The encoder reduces an input image into a lower-dimensional representation called the latent space. The decoder learns how to convert this abstract representation back into an image of the original input object. Given a large dataset of images from a variety of angles and facial expressions, a VAE can be trained to encode parts of a face that are important (e.g. mouth, eyes, chin) and the decoder learns to map these representations in the latent space back into an image of a face.

To swap faces using a VAE, a neat trick is employed. Person X and Person Ys faces are used to train a single encoder, but the decoders are trained separately (each decoder only learns how to map to one of the faces) (Figure 1). To swap Person Ys face with Person Xs, the network simply makes a representation in latent space from an input image of Person Y but uses Person Xs decoder to generate an image. Note that this process only works for generating single images. To make a Deepfake video, the procedure must be performed on each individual frame in the video, and the fake face must be re-inserted into the original image. These steps are automated, however, with AI only being required for generating the swapped face. Many Deepfake generation tools, such as Faceswap and DeepFaceLab, are now freely available online and only require 20-40 minutes of training video to be effective (46).


Figure 1  Face swapping with a Variational Autoencoder (VAE). VAEs are neural networks made of two parts: an encoder and a decoder. The encoder compresses an input into a lower-dimensional representation called the latent space, and the decoder learns to reproduce the original input from this representation. To swap faces, an encoder is trained to make representations of faces from Person X and from Person Y, but their respective decoders are trained separately. Passing information encoded from an image of Person X through a Decoder trained to build images of Person Y effectively swaps their faces (and vice-versa). Image adapted from Alan Zucconis excellent explainer.

A second workhorse of digital fakery, the GAN, was first dreamed up by Ian Goodfellow (the GANfather) in 2014 following a spirited argument in a Montreal bar (7). His interlocuters did not believe that a pair of duelling neural networks could generate realistic photos on their own. One intense night of coding later, he had proved them wrong.

 The new network architecture Goodfellow proposed pitted two neural networks against each other: the Generator and the Discriminator. To illustrate how one works, we can take the example of a GAN aiming to produce realistic images of celebrities. The Discriminator is fed real images of celebrities from a database, or nonsense images from the Generator, learning to distinguish between the real and fake celebrities (Figure 2).

Through the backpropagation algorithm, the Generator learns which parts of the image it produced were most offensive to the Discriminator and adjusts its internal settings accordingly. The next image from the Generator is slightly more convincing and, over many iterations, the Generator learns to produce an image that the Discriminator cannot distinguish from the real celebrity images. In this way, GANs can learn to produce an image of almost any object and the latest generation of GANs, including ProGAN and NVIDIAs StyleGAN2, can generate incredibly realistic synthetic faces (810).


Figure 2  How Generative Adversarial Networks (GANs) can make fake celebrities. A GAN is made of two neural networks: the Generator and the Discriminator. The Generator receives random noise as an input and spits out numbers in some form, in this example a series of numbers which correspond to pixels in an image. The Discriminator receives either fake images from the generator or real images from a dataset of celebrity pictures (e.g. CelebA-HQ) and is trained to distinguish the real images from the fakes. Over many rounds of training, the generator learns to transform the random numbers it starts with into a configuration that the Discriminator cannot distinguish from the real dataset. With a large training dataset, the results can be very convincing, as shown by the fake celebrities generated by Karas et al. (2017).

New paradigms in AI-generated disinformation 


Sockpuppets are nothing new in social media influence campaigns. These are online identities which pose as real people and push biased points of view onto unsuspecting forums. The profile pictures of sockpuppet accounts are usually stolen from celebrities or from stock images on the web. This limits the influence that a sockpuppet can have as their identities can be easily compromised by a complaint from their real-life subject or by simply using a reverse image search. GANs can now generate realistic, high resolution images of people who dont exist (down to minute details like nose hairs and skin imperfections). This technology is increasingly being harnessed to aid disinformation operations by a variety of actors including governments, political parties and private companies.

Espionage agencies appear to have been among the first to adopt the technology, as the case of Katie Jones illustrates (Figure 3). Joness profile on LinkedIn was impressive, stating that she was a Russia and Eurasia fellow at the Center for Strategic and International Studies in Washington and her network boasted many high-profile U.S. policymakers and think tank employees (including one being considered for a seat on then-President Trumps Federal Reserve board). Her profile came under suspicion after real think tank employees flagged it in mid-2019, and the profile was hurriedly removed. A review conducted by the Associated Press concluded that the young womans profile image was almost certainly generated by a GAN, and was likely intended to facilitate recruitment of spies (11).

 In addition to enhancing covert operations, fake faces have seen utility in public influence campaigns supporting political parties. The first known example of this surfaced in late 2019, when Facebook announced the takedown of a network pushing pro-Trump messaging (12). Many of the admins running the groups pages appeared to have been made by a face generator similar to NVIDIAs StyleGAN (Figure 3). A similar network amplifying pro-hunting and anti-China messaging used GAN-generated faces to pose as conservative youth activists in the run-up to the 2020 US presidential elections (Figure 3) (13). Many of the fake profiles were detected manually, as GAN-generated images still suffer from a few notable defects. For example, the faces often have asymmetric features at the sides of the head, including ears and earrings. GANs can also struggle to produce convincing backgrounds, as these features are less homogenous in training datasets (Figure 3). Furthermore, as reported by the Stanford Internet Observatory, the GAN-generated faces are frequently pilfered from online databases, so a reverse image search often reveals the forgery (13).



Figure 3  Katie Jones, top left, posed as a think tank employee to infiltrate U.S. policymaking circles. Alfonso Macias, top right, was a fake admin in a large pro-Trump influence campaign (note the asymmetry of his glasses and distorted background). Bottom left, a GAN-made fake profile involved in a conservative youth groups political messaging with prominent ear asymmetry. Several accounts that were part of the same youth group network, including Jason Hoshizaki, bottom right, were stolen from a website that hosted GAN-made faces called generated.photos.

More recently still, the use of GAN-faced sockpuppets in social media influence campaigns appears to have spilled the bounds of politics. In late December 2020, an unidentified group mounted a pro-Huawei influence campaign against a law which would have restricted the companys plans to build Belgiums 5G infrastructure. During a 3-week period, at least 14 fictitious accounts posing as telecommunications experts, writers and academics shared many articlesincluding at least 2 authored by the fake accountsattacking the law on Twitter. The faces of the people in these profiles were GAN-generated, with many of their 1000+ followers appearing to be fake accounts themselves. The pro-Huawei messaging was then shared by senior members of Huaweis European leadership, although there is no indication that these individuals knew that the accounts and articles were fake (14). These developments appear to be the first instance of AI-assisted disinformation techniques being used to further the interests of a private company against a state. This trend is likely to continue, as the number of private companies offering disinformation for hire is rapidly increasing (15).

Combatting AI-assisted disinformation

Approaches to safeguard the information ecosystem fall into 3 broad categories: Media forensics, Media Education and technologies to ensure provenance. 

Media Forensics techniques aim to determine if an audio or video file has been tampered with, automatically flagging it for review or serving to help debunk the associated disinformation. One such technique, Error Level Analysis, detects parts of an image that have been subjected to different compression levels and can help detect synthetic faces (12). Another promising method uses AI to learn the subtle features that GANs leave on fake faces during synthesis, the so-called GAN fingerprint. This method can have detection accuracies up to 99.5% (16). However, research groups have showed that simply modifying the GAN-generated images to include random features reduced the accuracy of one state-of-the-art GAN fingerprint detector by a factor of over 200 (17). The competition between forgers and detection systems will continue to be an active field of innovation, with early products offering automated Deepfake video detection already on the market, including Deepware and Sensity. 

Due to the prevalence of tampered online content, the use of digital timestamping technologies to guarantee the provenance of online content is also being explored. Project Origin is an alliance of technology and media organisations (Microsoft, BBC, CBC Radio Canada and the New York Times) that have developed tools that utilise a central blockchain ledger to securely store metadata and other information about a piece of content in the form of a digital manifest (20). A copy of this manifest is embedded into the file itself and will be incomplete if the file is later tampered with. When the media is viewed, a browser extension reads the manifest and checks this with the copy in the secure ledger, alerting the viewer if the content has been manipulated. 

Technologies like Project Origins aim to guarantee that a piece of content has not been manipulated from the point of publication onwards. Similar tools are being developed by Adobe and others to guarantee the chain of provenance upstream of publication, from the camera lens through to the editing station (20,21).

Media education is an effective way to fight disinformation, as revealed by the case of Finland. Alarmed by an uptick in hostile Russian disinformation in 2014, the Finnish government moved to instil media literacy as a core component of the countrys curriculum. Students (including children as young as 6) are taught to critically evaluate information online in creative ways (Box 1) (18). Measures like this helped Finland top the Open Society Institutes Media Literacy index, a report that tracks disinformation resilience, in 2019 (19). This anti-disinformation strategy accepts that the volume and quality of disinformation is likely to increase and aims to create a society that is more resistant to misleading information despite being awash in it.

Box 1  Lessons and activities used to foster Media Literacy in Finnish classrooms

      Studying how past propaganda campaigns worked in History classes

      Practically teaching students how to manipulate images in Art classes

      Presenting how statistics can be used to deceive in Maths classes

      Hosting mock debates on current affairs, then getting students to write both factual and biased reports 

      Asking students to identify their social media bubble and think about how this influences their opinions

      Encouraging students to compare how media in different countries portray citizens of foreign states

Conclusion

Artificial intelligence is granting us the power to synthesise increasingly deceptive images ever more cheaply. These threats to factuality can be met with technical fixes, but this is likely to devolve into an arms race in which the organisations with the largest training datasets and resources win. Enhancing media literacy through education may well be the most effective tool to deploy against the spread of disinformation. If we are to stave off Herbert Lins global information dystopia, one thing is certainwe have our work cut out for us.