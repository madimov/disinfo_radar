title,url,date,text,cleaning,tokens
Model Predictive Control with Self-supervised Representation Learning,"[{'href': 'http://arxiv.org/abs/2304.07219v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2304.07219v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-04-14 16:02:04,"3
2
0
2

r
p
A
4
1

]

G
L
.
s
c
[

1
v
9
1
2
7
0
.
4
0
3
2
:
v
i
X
r
a

Model Predictive Control with Self-supervised
Representation Learning

Jonas Matthies, Muhammad Burhan Hafez, Mostafa Kotb, and Stefan Wermter

Knowledge Technology, Department of Informatics, University of Hamburg
Jonas-Matthies@gmx.de,
{burhan.hafez,mostafa.kotb,stefan.wermter}@uni-hamburg.de

Abstract. Over the last few years, we have not seen any major develop-
ments in model-free or model-based learning methods that would make
one obsolete relative to the other. In most cases, the used technique is
heavily dependent on the use case scenario or other attributes, e.g. the
environment. Both approaches have their own advantages, for example,
sample eﬃciency or computational eﬃciency. However, when combining
the two, the advantages of each can be combined and hence achieve better
performance. The TD-MPC framework is an example of this approach.
On the one hand, a world model in combination with model predictive
control is used to get a good initial estimate of the value function. On the
other hand, a Q function is used to provide a good long-term estimate.
Similar to algorithms like MuZero a latent state representation is used,
where only task-relevant information is encoded to reduce the complex-
ity. In this paper, we propose the use of a reconstruction function within
the TD-MPC framework, so that the agent can reconstruct the original
observation given the internal state representation. This allows our agent
to have a more stable learning signal during training and also improves
sample eﬃciency. Our proposed addition of another loss term leads to
improved performance on both state- and image-based tasks from the
DeepMind-Control suite.

Keywords: reinforcement learning · model predictive control · Q-Learning
· TD-MPC framework · reconstruction function.

1

Introduction

In the domain of Reinforcement Learning (RL), there are two main categories of
approaches, namely the model-free and model-based learning methods. Model-
free methods are concerned with learning the quality of speciﬁc states or pairs
of state and action, whereas model-based methods learn a model of the envi-
ronments dynamics. While model-free approaches evaluate the possible actions
and next states at each time step, model-based methods use a process called
planning. With the help of the world model, the actor can simulate a sequence
of actions by mimicking the real environment and hence create and evaluate
diﬀerent plans. However, planning is often restrained to short horizons because

 
 
 
 
 
 
2

J. Matthies et al.

extensive planning is not feasible due to the model becoming less and less accu-
rate the more actions we simulate.

Inherently, both model-free and model-based approaches have their own
strengths and weaknesses, prompting the question of whether the two methods
can be combined to create one framework that leverages the beneﬁts of both.
Although research in this area is still in its early stages, there have already
been encouraging ﬁndings indicating an increased performance for these hybrid
learning approaches [14,18,15,6,4,5]. The recently proposed Temporal Diﬀerence
Learning for Model Predictive Control (TD-MPC)[9] framework, for example,
demonstrated that the combination of model-free and model-based methods can
combine the advantages of both methods and is able to achieve superior sample
eﬃciency as well as improved performance when compared to other state-of-the-
art algorithms such as Soft Actor-Critic (SAC)[3] or LOOP [15] on a variety of
tasks. It utilizes a combination of Model Predictive Control (MPC) for short-
horizon planning and a Q-value function for long-term reward estimates. Similar
to other state-of-the-art algorithms like MuZero [14], Dreamer [6] or EﬃcientZero
[18], latent representations of the environment states are extensively used during
training to provide the agent with an abstract representation and thus reduce
the complexity and focus on task-relevant details.

In this paper, we propose the use of an additional loss term for the training of
the Task-Oriented Latent Dynamics (TOLD)[9] model, which supplements the
already implemented consistency loss. The additional term is calculated with the
help of the reconstruction function, which aims to reconstruct the original obser-
vation from the latent space representation. While the latent space is designed
to omit irrelevant details from the observations, the reconstruction function on
the other hand is used to restore the original observation. The use of a recon-
struction function has already proven to be performance-enhancing in various
other algorithms like PlaNet [7], Dreamer [6], Dreamer-v2 [8] or a modiﬁed ver-
sion of MuZero [13]. Similar to the rest of the TOLD model, the parameters of
the reconstruction function are learnt jointly with the representation, dynam-
ics, reward and value functions using Temporal Diﬀerence (TD) learning. By
complementing the consistency loss with a reconstruction loss, the aim is to sta-
bilize the learning process of the agent through an enhanced learning signal. We
seek to answer the question of how our version of TD-MPC with Self-supervised
Representation Learning (SRL) compares to the original implementation of the
framework. An implementation of the adopted TD-MPC framework can be found
at https://github.com/Jonas-SM/TD-MPC-SRL/.

2 Background

2.1 Formal Deﬁnition

We deﬁne an inﬁnite-horizon Markov-Decission Process (MDP) characterized
by the tuple (S, A, T, R, γ, p0) to model the agent-environment interaction. The
set of possible states and actions in the environment are deﬁned as S and A
respectively, where S ∈ Rn and A ∈ Rm are continuous state and action spaces.

MPC with SRL

3

The dynamics of the environment can be deﬁned through the transition func-
tion T : S × A × S (cid:55)→ [0, 1]. It determines the probability of reaching state
s(cid:48) when taking action a in state s. Similarly, we deﬁne our reward function as
R : S × A (cid:55)→ R. Hence this function maps every pair of state and action to a
scalar value which represents the immediate feedback an agent receives after tak-
ing an action. Additionally, γ : γ ∈ [0, 1) is a discount factor and p0 denotes the
initial state distribution of the environment, which speciﬁes the probability dis-
tribution of the agent starting in each possible state. This tuple constitutes the
complete MDP and serves as the formal framework for the TD-MPC algorithm.
Another essential component of an RL agent is the policy π : S (cid:55)→ A, which is a
mapping from states to actions that is iteratively updated throughout training.
At every timestep, this policy determines which action the agent executes and
thus deﬁnes the agent’s behaviour. Over the course of training, we aim to learn
a parameterized mapping πθ, such that the agent achieves a maximum reward.

2.2 TOLD model

The purpose of the TOLD model is to provide the agent with an abstract model
of the world, that excludes all information irrelevant to the task [9]. We will
also often refer to this abstract representation of the world as the latent space.
Additionally, as a result of removing unnecessary details and, thus in most cases,
signiﬁcantly simplifying the environment’s complexity, the training process will
also become less complicated for the agent [2]. With the help of the models’
diﬀerent components, a number of quantities can be predicted that steer the
learning process. The model itself consists of ﬁve components [9]:

1. Representation function: zt = hθ(st), simple encoder that encodes an

observation st into its respective latent state zt.

2. Dynamics function: zt+1 = dθ(zt, at), model of the world used to predict

the subsequent latent state zt+1.

3. Reward function: ˆrt = Rθ(zt, at), predicts the expected reward ˆrt for a

latent state zt and action at.

4. Value function: ˆqt = Qθ(zt, at), calculates an estimate of the expected

return ˆqt given latent state zt and action at.

5. Policy function: ˆat = πθ(zt), predicts the best action ˆat to take for a given

latent state zt.

All components are implemented using purely deterministic Multilayer Per-
ceptrons (MLPs) and θ denotes the current parameterization of the TOLD model
[9]. Similar to other approaches like the MuZero algorithm [14] that also utilizes
both a model of the world and a value function, TD-MPC learns a policy πθ
additional to the model of the environment [9]. This policy is involved in the
evaluation of the TOLD model, as well as the planning process of the agent to
generate sample trajectories. In addition to the regular TOLD model, TD-MPC
utilizes a target network, which is essentially a regular copy of the TOLD model,
whose parameters θ− are a slow-moving average of θ [9]. The use of a target net-
work can help with preventing the learning process from becoming unstable and

4

J. Matthies et al.

is used in many Deep-Q RL algorithms such as Deep Q-learning [11], TD3 [1]
and DDPG [10]. As it is a slow-moving average of θ it is also updated at the
same time according to the following rule with ζ ∈ [0, 1) being a constant [9]:

t+1 = (1 − ζ)θ−
θ−
During training, the TOLD model is updated by minimizing the loss

t + ζθt.

J (θ) =

t+H
(cid:88)

i=t

λi−tL(θ; Γi),

(1)

(2)

where J deﬁnes the total loss computed as the sum of the single-step loss L(θ; Γi)
and a discount factor λ, which controls the inﬂuence of predictions far in the
future [9]. Γ ∼ B deﬁnes a trajectory sampled from the replay buﬀer B, and the
single-step loss L(θ; Γi) is deﬁned as [9]:

L(θ; Γi) = c1lr

i + c2lv

i + c3lc
i ,

(3)

where lr
i , lv
and deﬁned as in [9]:

i , lc

i are the reward loss, value loss and consistency loss, respectively

i = ||Rθ(zi, ai) − ri||2
lr
2.
i = ||Qθ(zi, ai) − (ri + γQθ− (zi+1, πθ(zi+1)))||2
lv
2.
i = ||dθ(zi, ai) − hθ− (si+1)||2
lc
2.

(4)

(5)

(6)

The single-step loss is then used to jointly optimize the reward, value, and
dynamics functions with coeﬃcients c1, c2 and c3 balancing the weight of the 3
terms [9]. During training, prioritized experience replay [12] is used to store all
experiences (st, at, rt, st+1) of the agent in a replay buﬀer, so that they can be
used to train the TOLD model [9]. Using the collected experiences, the TOLD
model is iteratively updated throughout training to improve the predictions
made by the value, reward, dynamics and policy functions. Given a sample tra-
jectory Γ ∼ B from the replay buﬀer B of length H, we start at observation st
and consider this as the starting point. At ﬁrst, st is encoded into latent state
zt using hθ. Next, using the dynamics function dθ, an H time steps are unrolled
by predicting the next latent state given the previous latent state and action.
At each time step, the TOLD model predicts (ˆrt, ˆqt, ˆat), and the single-step loss
L(θ; Γt) is calculated using Equation 3. At the end, the total loss J is computed
by summing the single-step losses over the H time steps as indicated by Equa-
tion 2, which is then used to perform one update of the model parameters. Then,
the policy πθ is updated by minimizing the objective

Jπ(θ; Γi) = −

t+H
(cid:88)

i=t

λi−tQθ(zi, πθ(zi)),

(7)

which is a time-weighted summation of the policy objective widely used in actor-
critic methods [3,10].

MPC with SRL

5

2.3 Planning

During planning, the TD-MPC framework makes use of a slightly adopted ver-
sion of Model Predictive Path Integral (MPPI)[17] control, which is a sampling-
based model predictive control algorithm. We introduce (µ0, σ0)t:t+H , µ0, σ0 ∈
Rm, A ∈ Rm, which deﬁnes parameters for a normal distribution N used to
sample trajectories for planning [9]. Besides the trajectories sampled from the
normal distribution, a small number of trajectories are included generated by the
learned policy πθ. Using the dynamics function dθ, all the sampled trajectories
are unrolled and evaluated by estimating the total return as follows:

GΓ = E

(cid:34)
γH Qθ(zH , aH ) +

(cid:35)
γtRθ(zt, at)

.

H−1
(cid:88)

t=0

(8)

The terminal Q-function Qθ(zH , aH ) provides an estimation of the return
beyond the planning horizon H which supplements the short-term estimation
provided by the reward function Rθ. Selecting only the best k trajectories Γ ∗
based on their return, parameters µj and σj at iteration j are iteratively updated
using the following estimates which are normalized in terms of the top-k returns:

µj =

(cid:80)k

i=1 ΩiΓ ∗
i
(cid:80)k
i=1 Ωi

, σi = max





(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:80)k

i − µj)2

i=1 Ωi(Γ ∗
(cid:80)k

i=1 Ωi



, (cid:15)

 ,

(9)

where Ωi = τ ∗(Gi −G∗) [9]. Additionally, Gi denotes the return of trajectory Γi,
G∗ represents the maximum return, τ is a temperature parameter regulating the
inﬂuence of the best trajectories and (cid:15) is a linearly decayed constant that enforces
constant exploration [9]. This process of sampling trajectories and afterwards
updating parameters µ and σ is repeated for a total of J iterations during one
time step [9]. Subsequently, one trajectory Γ is sampled from the ﬁnal return-
normalized distribution N and the ﬁrst action at is executed. Afterwards, this
planning process is repeated at the next time step t+1 with a 1-step shifted
mean µ from the previous time step [9]. This prevents the agent from starting at
0 every time and instead provides a good starting point which also includes the
learnt knowledge from previous experiences [9]. However, a fairly high variance
is chosen to avoid local minima.

3 TD-MPC with SRL

3.1 Reconstruction Function

While in TD-MPC, the latent state representation is mainly learned and in-
ﬂuenced by the consistency loss, we propose to add another loss term to the
single-step loss L(θ; Γi) which aims to provide the agent with a richer learning
signal and thus should also improve the performance of the entire model and

6

J. Matthies et al.

Fig. 1: Updated TOLD model [9]

algorithm. We extend the TOLD model by another learnable component h−1
(shown in Fig. 1) deﬁned as:

θ

Reconstruction function : ˜st = h−1

θ (zt),

θ

where h−1
represents the inverse of the latent state representation function hθ.
We denote the result of h−1
θ (zt) as ˜st which represents an estimate of the respec-
tive ground truth observation st since in most cases the real observation st cannot
be fully reconstructed. The reason for this is that the task-oriented latent state
representation zt will most likely be far less complex and thus it would be chal-
lenging, if not even impossible, to reconstruct the original state st entirely with
only being given the latent state. As denoted by parameters θ, this additional
function will be learnt together with the other components of the TOLD model.
Hence, error gradients from all 4 terms lr, lv, lc and lh−1
are back-propagated
through time to update each component. This also means that the other func-
tions are forced to include information which is important for the reconstruction
of the latent representation. Given a trajectory Γ = (st, at, rt, st+1)t:t+H sam-
pled from our Replay Buﬀer B and latent state representation zt = hθ(st), we
supplement the loss L(θ; Γi) (Equation 3) with the following self-supervised re-
construction loss term:

lh−1
i = ||h−1

θ (zi) − si||2
2.

(10)

Similar to the reward loss (see Equation 4), the true observation st from Γ
θ (zt) by computing

is being compared to the reconstructed observation ˜st = h−1
the mean squared error. Thus, the new single-step loss L(θ; Γi), is deﬁned as:

i + c4lh−1

i + c3lc

L(θ; Γi) = c1lr

i + c2lv
where, c4 is the reconstruction loss coeﬃcient. As a result, the reconstruction
function will not be used in any form within the planning process of the agent,
but instead only utilized to stabilize the learning process. When considering a
sparse reward signal, the learning process can be inherently challenging due to
the reward serving as the primary learning signal in most cases. Thus, if rewards
are sparse, so is the learning signal, and as a result, the agent potentially misses

(11)

,

i

...

MPC with SRL

7

out on key insights about the environment dynamics or simply forgets about pre-
viously acquired knowledge. By including the term lh−1
in the single-step loss,
we aim to further enhance the learning signal and thereby improving the robust-
ness of the learning process, especially in sparse and noisy environments. Since
the reconstruction function is also jointly trained with the other components,
error gradients are propagated through the other functions, resulting in shared
behaviours and features. Thus, both the representation and dynamics functions
will be forced to retain information essential to reconstruct the original obser-
vation. Including a reconstruction loss can potentially lead to the representa-
tion and dynamics functions gaining a better understanding of the environment
beyond just the reward signal. Key features or other characteristics that help
distinguish diﬀerent states could be incorporated to further improve the learning
process. The goal of the reconstruction loss is to augment the learning signal,
without dominating the other loss terms.

Another possible advantage of our proposed change to the TD-MPC frame-
work is the ability to pre-train the world model in a self-supervised manner.
Essentially, we can enable the agent to explore the environment without any
reward or goal and use the reconstruction loss as the primary learning signal. As
a result, the agent can acquire knowledge about the environments dynamics and
construct a world model before the actual training process begins. This approach
can be advantageous as having a pre-trained model of the environment allows
for better estimates from the start and thus for reduced training times.

4 Experiments

4.1 Setup

Similar to the experiments conducted in the TD-MPC paper, we run all envi-
ronments with diﬀerent random seeds and average the results [9]. As the original
TD-MPC has already been thoroughly compared to other state-of-the-art algo-
rithms, we consider it suﬃcient to limit our comparison to the original TD-MPC
framework. We use the documented results from the oﬃcial GitHub repository1
for comparison of state-based environments, whereas we generate the data for
image-based environments on our own using the oﬃcial implementation, due to
no data being available for most image-based tasks. We follow the same approach
as in [9] and conduct experiments on various environments from the DeepMind
Control suite (DMControl)[16]. Hyperparameter values that are not explicitly
stated are set to the values in [9].

We adopt the TD-MPC framework as described in Section 3. Similar to
the other components of the TOLD model, we employ a deterministic MLP
to learn the reconstruction function. For state-based tasks, the architecture of
the MLP consists of two simple fully-connected layers together with an ELU
activation function, chosen intentionally to resemble the encoder architecture of
the latent state representation function. The MLP architecture for image-based

1 https://github.com/nicklashansen/tdmpc

8

J. Matthies et al.

tasks is slightly more intricate due to the need of upsampling the given latent
representation to match the original input size of 84x84 pixels. The initial layer
is a Linear Layer, used to preprocess the input for the subsequent, whereas
the next four hidden layers are deconvolutional layers, with all except the last
being followed by a batch normalization layer to enhance the stability of the
learning process. In terms of activation functions, we chose the ReLU function
for all layers, except for the ﬁnal layer, where we used a sigmoid function to
ensure that the outputs fall within the range of [0, 1]. Additional to normalizing
the images, a pixel shift augmentation of ±4 is used to prevent overﬁtting of the
TOLD encoder hθ [9]. For the training of our introduced reconstruction function,
we use the original images instead of the augmented ones within the respective
loss lh−1
because we aim to reconstruct the original observation as provided by
the environment. We empirically choose a reconstruction coeﬃcient of 2.0, 0.25,
0.275 for the state-based Finger Turn Hard, Cheetah Run, and Acrobat-Swingup,
and of 0.15, 0.275, 0.15 for the image-based Finger Turn Hard, Cheetah Run,
and Reacher Easy environments respectively.

4.2 Results

To evaluate the performance of the two algorithms we conduct a periodic perfor-
mance evaluation during training after every 20k environment steps. We evaluate
the agent after every 20k environment steps, averaging over 10 episodes, then the
averaged return is calculated. In the description of the results and conclusion, the
term TD-MPC agent is used to refer to the agent using the original TD-MPC
framework, while the TD-MPC framework with the additional reconstruction
function is referred to as the reconstruction agent. Additionally, we measure the
performance for image-based tasks after 100k and 500k environment steps to
compare the initial learning speed, asymptotic performance as well as sample
eﬃciency and robustness of the algorithms.

For state-based environments, the reconstruction agent outperforms the reg-
ular TD-MPC agent on two of the three selected tasks (see Fig. 2). For the
tasks Finger Turn Hard and Cheetah Run, the additional reconstruction func-
tion yields an increased learning speed and improved overall performance. In the
Acrobot Swingup environment the reconstruction agent achieves a better initial
learning speed, but worse asymptotic performance, while in terms of stability, we
achieve comparable results in all three environments. Looking at the overall per-
formance averaged over all three tasks, our changes to the TD-MPC framework
slightly increase the learning speed and performance but seem to be slightly less
stable when compared to the TD-MPC agent (see Fig. 3).

In the selected image-based environments, the changes made to the single-
step loss term and the additional reconstruction function do not appear to have
a signiﬁcant impact on the overall learning speed of our agent (see Fig. 2).
Based on the results in Table 1, the reconstruction agent demonstrated improved
performance and stability after 100k environment steps on two of the tasks, which
suggests a faster initial learning process and enhanced stability, while achieving
comparable performance on the others. After 500k steps, our agent outperformed

MPC with SRL

9

Fig. 2: Episode return comparison of our method (TD-MPC with SRL) and the
original TD-MPC framework on state-based (top row) and image-based (bottom
row) environments. We follow the approach from the original TD-MPC paper
[9] and average the return over 5 runs.

the original framework in only one of the tested environments but showed a
signiﬁcant improvement in terms of stability (lower standard deviation) on two of
the three evaluated tasks. In general, our framework achieved comparable episode
returns in most image-based environments, as shown in Fig. 3, which displays
the average reward of the three image-based environments: Finger Turn Hard,
Cheetah Run, and Reacher Easy. Overall, however, the modiﬁed agent exhibited
improved stability after 500k environment steps in two of the three evaluated
environments (Reacher Easy, Finger Turn Hard), as indicated in Table 1.

4.3 Discussion

The state-based environments, especially Finger Turn Hard and Cheetah Run,
showed the most signiﬁcant improvements in terms of overall performance. The
introduction of the reconstruction function and its associated loss term appears
to help the agent with learning a more accurate TOLD model at a faster pace
than its original counterpart. Learning the reconstruction function together with
the other components also results in more frequent updates to the other com-
ponents like the value and dynamics functions which could possibly achieve an
increased learning speed and stability. However, the modiﬁed agent did not show
signiﬁcant improvements in stability, but rather a slight decrease, especially in
simple low-dimensional environments like Finger Turn Hard or Acrobot Swingup,
where the learning process was less robust and instead more susceptible to ran-
dom seeds. This is potentially due to the unnecessary additional information
being encoded in the latent state. On the other hand, for more complex environ-
ments like Cheetah Run, we observed a signiﬁcantly more robust and slightly

10

J. Matthies et al.

Fig. 3: Episode return comparison of our method (TD-MPC with SRL) and the
original TD-MPC framework. The left plot represents the average return over
all three state-based environments, while the right does the same for the three
image-based environments from Fig. 2.

100K STEP SCORES TD-MPC TD-MPC with SRL

Cheetah Run
Reacher Easy
Finger Turn Hard

246 ± 73
256 ± 116
50 ± 100

191 ± 93
319 ± 82
55.3 ± 108

500K STEP SCORES TD-MPC TD-MPC with SRL

Cheetah Run
Reacher Easy
Finger Turn Hard

561 ± 35
668 ± 217
442 ± 187
Table 1: Episode return comparison for image-based environments after 100k
and 500k environment steps with the respective standard deviations. The column
with bold numbers indicates better results in terms of return.

435 ± 79
768 ± 178
392 ± 132

faster learning process, suggesting that the information needed for reconstruction
is useful in this domain and can help with strengthening the learning signal.

Learning from pixels proves to be more challenging for our agent, which
may be due to their inherent noisy and high-dimensional nature. In the con-
text of noisy environments, the role of the latent state is crucial as it reduces
the complexity of the observation by encoding only the task-relevant details,
thereby ﬁltering out irrelevant information. Therefore, our modiﬁcations to the
TD-MPC algorithm allow for a more eﬃcient and stable learning process by en-
hancing the learning signal and updating the model components more frequently.
This is important since the agent may miss out on important information due to
sparse rewards when only trained on the reward signal. The increase in perfor-
mance and stability are most likely attributed to the reduced susceptibility to
noise and random seeds, resulting in a more consistent and steady learning pro-
cess. However, in more complex environments such as Cheetah Run, the overall

MPC with SRL

11

performance return-wise may still be slightly worse, due to the lack of hyperpa-
rameter tuning. To further validate our ﬁndings, a more diverse comparison of
performance would include a variety of other complex environments, such as the
Humanoid or Dog tasks.

One aspect that we have not yet explored is the ability of the modiﬁed TD-
MPC with self-supervised representation learning to pre-train a model of the
environment, which could supply the agent with better initial estimates through-
out training. Therefore, this could potentially be another valuable opportunity
to further improve the performance and stability of our modiﬁed agent. We leave
this open for future work.

5 Conclusion

In this paper, we proposed a modiﬁcation of the TD-MPC framework, specif-
ically the TOLD model. This included an additional self-supervised loss term,
as well as a new component aimed at reconstructing the original observation
given a latent state and thus providing the agent with an enhanced learning
signal. We assessed the modiﬁed agent’s performance in various experiments
from the DMControl suite and compared it to the original version. Our ﬁndings
showed that the modiﬁed agent performed better on the majority of state-based
environments while achieving comparable stability. In image-based tasks, which
inherently have high noise levels, the reconstruction agent proved to have a faster
initial learning speed in most experiments, and despite struggling with the overall
learning speed in most environments, the modiﬁed agent demonstrated a more
robust learning process in the majority of tasks. Especially for high-dimensional
environments, the agent is still struggling in terms of episode return and stabil-
ity. Nevertheless, most importantly, we do not observe a signiﬁcant decrease in
terms of performance on any of the tasks. These results indicate the potential
beneﬁts of the reconstruction function and its associated loss term in generat-
ing an enhanced learning signal and consistent updating of the TOLD model
components that is not dependent on reward.

6 Acknowledgements

The authors gratefully acknowledge support from the DFG (CML, MoReSpace,
LeCAREbot), BMWK (SIDIMO, VERIKAS), and the European Commission
(TRAIL, TERAIS). Mostafa Kotb is funded by a scholarship from the Ministry
of Higher Education of the Arab Republic of Egypt.

References

1. Fujimoto, S., van Hoof, H., Meger, D.: Addressing function approximation error
in actor-critic methods. In: Proceedings of the 35th International Conference on
Machine Learning. Proceedings of Machine Learning Research, vol. 80, pp. 1587–
1596. PMLR (2018)

12

J. Matthies et al.

2. Ha, D., Schmidhuber, J.: Recurrent world models facilitate policy evolution. In:
Advances in Neural Information Processing Systems. vol. 31. Curran Associates,
Inc. (2018)

3. Haarnoja, T., Zhou, A., Abbeel, P., Levine, S.: Soft actor-critic: Oﬀ-policy maxi-
mum entropy deep reinforcement learning with a stochastic actor. In: Proceedings
of the 35th International Conference on Machine Learning. Proceedings of Machine
Learning Research, vol. 80, pp. 1861–1870. PMLR (2018)

4. Hafez, M.B., Weber, C., Kerzel, M., Wermter, S.: Curious meta-controller: Adap-
tive alternation between model-based and model-free control in deep reinforcement
learning. In: 2019 International Joint Conference on Neural Networks (IJCNN).
pp. 1–8 (2019)

5. Hafez, M.B., Weber, C., Kerzel, M., Wermter, S.: Improving robot dual-system mo-
tor learning with intrinsically motivated meta-control and latent-space experience
imagination. Robotics and Autonomous Systems 133, 103630 (2020)

6. Hafner, D., Lillicrap, T., Ba, J., Norouzi, M.: Dream to control: Learning behaviors
by latent imagination. In: International Conference on Learning Representations
(2020)

7. Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., Davidson, J.:
Learning latent dynamics for planning from pixels. In: Proceedings of the 36th
International Conference on Machine Learning. Proceedings of Machine Learning
Research, vol. 97, pp. 2555–2565. PMLR (2019)

8. Hafner, D., Lillicrap, T.P., Norouzi, M., Ba, J.: Mastering atari with discrete world

models. In: International Conference on Learning Representations (2021)

9. Hansen, N., Wang, X., Su, H.: Temporal diﬀerence learning for model predictive

control. In: International Conference on Machine Learning, PMLR (2022)

10. Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D.,
Wierstra, D.: Continuous control with deep reinforcement learning. In: ICLR (2016)
11. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
D., Riedmiller, M.A.: Playing atari with deep reinforcement learning. CoRR
abs/1312.5602 (2013)

12. Schaul, T., Quan, J., Antonoglou, I., Silver, D.: Prioritized experience replay. In:
4th International Conference on Learning Representations, ICLR 2016, San Juan,
Puerto Rico, May 2-4, 2016, Conference Track Proceedings (2016)

13. Scholz, J., Weber, C., Hafez, M.B., Wermter, S.: Improving model-based reinforce-
ment learning with internal state representations through self-supervision. In: 2021
International Joint Conference on Neural Networks (IJCNN). pp. 1–8. IEEE (2021)
14. Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S.,
Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al.: Mastering atari, go, chess
and shogi by planning with a learned model. Nature 588(7839), 604–609 (2020)

15. Sikchi, H., Zhou, W., Held, D.: Learning oﬀ-policy with online planning. In: Pro-
ceedings of the 5th Conference on Robot Learning. Proceedings of Machine Learn-
ing Research, vol. 164, pp. 1622–1633. PMLR (2022)

16. Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., de Las Casas, D., Budden,
D., Abdolmaleki, A., Merel, J., Lefrancq, A., Lillicrap, T.P., Riedmiller, M.A.:
Deepmind control suite. CoRR abs/1801.00690 (2018)

17. Williams, G., Aldrich, A., Theodorou, E.A.: Model predictive path integral control:
From theory to parallel computation. Journal of Guidance, Control, and Dynamics
40(2), 344–357 (2017)

18. Ye, W., Liu, S., Kurutach, T., Abbeel, P., Gao, Y.: Mastering atari games with
limited data. In: Advances in Neural Information Processing Systems. vol. 34, pp.
25476–25488. Curran Associates, Inc. (2021)

","3 2 0 2 r p A 4 1 ] G L . s c [ 1 v 9 1 2 7 0 . 4 0 3 2 : v i X r a Model Predictive Control with Self-supervised Representation Learning Jonas Matthies , Muhammad Burhan Hafez , Mostafa Kotb , and Stefan Wermter Knowledge Technology , Department of Informatics , University of Hamburg Jonas-Matthies @ gmx.de , { burhan.hafez , mostafa.kotb , stefan.wermter } @ uni-hamburg.de Abstract . Over the last few years , we have not seen any major develop- ments in model-free or model-based learning methods that would make one obsolete relative to the other . In most cases , the used technique is heavily dependent on the use case scenario or other attributes , e.g . the environment . Both approaches have their own advantages , for example , sample eﬃciency or computational eﬃciency . However , when combining the two , the advantages of each can be combined and hence achieve better performance . The TD-MPC framework is an example of this approach . On the one hand , a world model in combination with model predictive control is used to get a good initial estimate of the value function . On the other hand , a Q function is used to provide a good long-term estimate . Similar to algorithms like MuZero a latent state representation is used , where only task-relevant information is encoded to reduce the complex- ity . In this paper , we propose the use of a reconstruction function within the TD-MPC framework , so that the agent can reconstruct the original observation given the internal state representation . This allows our agent to have a more stable learning signal during training and also improves sample eﬃciency . Our proposed addition of another loss term leads to improved performance on both state- and image-based tasks from the DeepMind-Control suite . Keywords : reinforcement learning · model predictive control · Q-Learning · TD-MPC framework · reconstruction function . 1 Introduction In the domain of Reinforcement Learning ( RL ) , there are two main categories of approaches , namely the model-free and model-based learning methods . Model- free methods are concerned with learning the quality of speciﬁc states or pairs of state and action , whereas model-based methods learn a model of the envi- ronments dynamics . While model-free approaches evaluate the possible actions and next states at each time step , model-based methods use a process called planning . With the help of the world model , the actor can simulate a sequence of actions by mimicking the real environment and hence create and evaluate diﬀerent plans . However , planning is often restrained to short horizons because 2 J. Matthies et al . extensive planning is not feasible due to the model becoming less and less accu- rate the more actions we simulate . Inherently , both model-free and model-based approaches have their own strengths and weaknesses , prompting the question of whether the two methods can be combined to create one framework that leverages the beneﬁts of both . Although research in this area is still in its early stages , there have already been encouraging ﬁndings indicating an increased performance for these hybrid learning approaches [ 14,18,15,6,4,5 ] . The recently proposed Temporal Diﬀerence Learning for Model Predictive Control ( TD-MPC ) [ 9 ] framework , for example , demonstrated that the combination of model-free and model-based methods can combine the advantages of both methods and is able to achieve superior sample eﬃciency as well as improved performance when compared to other state-of-the- art algorithms such as Soft Actor-Critic ( SAC ) [ 3 ] or LOOP [ 15 ] on a variety of tasks . It utilizes a combination of Model Predictive Control ( MPC ) for short- horizon planning and a Q-value function for long-term reward estimates . Similar to other state-of-the-art algorithms like MuZero [ 14 ] , Dreamer [ 6 ] or EﬃcientZero [ 18 ] , latent representations of the environment states are extensively used during training to provide the agent with an abstract representation and thus reduce the complexity and focus on task-relevant details . In this paper , we propose the use of an additional loss term for the training of the Task-Oriented Latent Dynamics ( TOLD ) [ 9 ] model , which supplements the already implemented consistency loss . The additional term is calculated with the help of the reconstruction function , which aims to reconstruct the original obser- vation from the latent space representation . While the latent space is designed to omit irrelevant details from the observations , the reconstruction function on the other hand is used to restore the original observation . The use of a recon- struction function has already proven to be performance-enhancing in various other algorithms like PlaNet [ 7 ] , Dreamer [ 6 ] , Dreamer-v2 [ 8 ] or a modiﬁed ver- sion of MuZero [ 13 ] . Similar to the rest of the TOLD model , the parameters of the reconstruction function are learnt jointly with the representation , dynam- ics , reward and value functions using Temporal Diﬀerence ( TD ) learning . By complementing the consistency loss with a reconstruction loss , the aim is to sta- bilize the learning process of the agent through an enhanced learning signal . We seek to answer the question of how our version of TD-MPC with Self-supervised Representation Learning ( SRL ) compares to the original implementation of the framework . An implementation of the adopted TD-MPC framework can be found at https : //github.com/Jonas-SM/TD-MPC-SRL/ . 2 Background 2.1 Formal Deﬁnition We deﬁne an inﬁnite-horizon Markov-Decission Process ( MDP ) characterized by the tuple ( S , A , T , R , γ , p0 ) to model the agent-environment interaction . The set of possible states and actions in the environment are deﬁned as S and A respectively , where S ∈ Rn and A ∈ Rm are continuous state and action spaces . MPC with SRL 3 The dynamics of the environment can be deﬁned through the transition func- tion T : S × A × S ( cid:55 ) → [ 0 , 1 ] . It determines the probability of reaching state s ( cid:48 ) when taking action a in state s. Similarly , we deﬁne our reward function as R : S × A ( cid:55 ) → R. Hence this function maps every pair of state and action to a scalar value which represents the immediate feedback an agent receives after tak- ing an action . Additionally , γ : γ ∈ [ 0 , 1 ) is a discount factor and p0 denotes the initial state distribution of the environment , which speciﬁes the probability dis- tribution of the agent starting in each possible state . This tuple constitutes the complete MDP and serves as the formal framework for the TD-MPC algorithm . Another essential component of an RL agent is the policy π : S ( cid:55 ) → A , which is a mapping from states to actions that is iteratively updated throughout training . At every timestep , this policy determines which action the agent executes and thus deﬁnes the agent ’ s behaviour . Over the course of training , we aim to learn a parameterized mapping πθ , such that the agent achieves a maximum reward . 2.2 TOLD model The purpose of the TOLD model is to provide the agent with an abstract model of the world , that excludes all information irrelevant to the task [ 9 ] . We will also often refer to this abstract representation of the world as the latent space . Additionally , as a result of removing unnecessary details and , thus in most cases , signiﬁcantly simplifying the environment ’ s complexity , the training process will also become less complicated for the agent [ 2 ] . With the help of the models ’ diﬀerent components , a number of quantities can be predicted that steer the learning process . The model itself consists of ﬁve components [ 9 ] : 1 . Representation function : zt = hθ ( st ) , simple encoder that encodes an observation st into its respective latent state zt . 2 . Dynamics function : zt+1 = dθ ( zt , at ) , model of the world used to predict the subsequent latent state zt+1 . 3 . Reward function : ˆrt = Rθ ( zt , at ) , predicts the expected reward ˆrt for a latent state zt and action at . 4 . Value function : ˆqt = Qθ ( zt , at ) , calculates an estimate of the expected return ˆqt given latent state zt and action at . 5 . Policy function : ˆat = πθ ( zt ) , predicts the best action ˆat to take for a given latent state zt . All components are implemented using purely deterministic Multilayer Per- ceptrons ( MLPs ) and θ denotes the current parameterization of the TOLD model [ 9 ] . Similar to other approaches like the MuZero algorithm [ 14 ] that also utilizes both a model of the world and a value function , TD-MPC learns a policy πθ additional to the model of the environment [ 9 ] . This policy is involved in the evaluation of the TOLD model , as well as the planning process of the agent to generate sample trajectories . In addition to the regular TOLD model , TD-MPC utilizes a target network , which is essentially a regular copy of the TOLD model , whose parameters θ− are a slow-moving average of θ [ 9 ] . The use of a target net- work can help with preventing the learning process from becoming unstable and 4 J. Matthies et al . is used in many Deep-Q RL algorithms such as Deep Q-learning [ 11 ] , TD3 [ 1 ] and DDPG [ 10 ] . As it is a slow-moving average of θ it is also updated at the same time according to the following rule with ζ ∈ [ 0 , 1 ) being a constant [ 9 ] : t+1 = ( 1 − ζ ) θ− θ− During training , the TOLD model is updated by minimizing the loss t + ζθt . J ( θ ) = t+H ( cid:88 ) i=t λi−tL ( θ ; Γi ) , ( 1 ) ( 2 ) where J deﬁnes the total loss computed as the sum of the single-step loss L ( θ ; Γi ) and a discount factor λ , which controls the inﬂuence of predictions far in the future [ 9 ] . Γ ∼ B deﬁnes a trajectory sampled from the replay buﬀer B , and the single-step loss L ( θ ; Γi ) is deﬁned as [ 9 ] : L ( θ ; Γi ) = c1lr i + c2lv i + c3lc i , ( 3 ) where lr i , lv and deﬁned as in [ 9 ] : i , lc i are the reward loss , value loss and consistency loss , respectively i = ||Rθ ( zi , ai ) − ri||2 lr 2. i = ||Qθ ( zi , ai ) − ( ri + γQθ− ( zi+1 , πθ ( zi+1 ) ) ) ||2 lv 2. i = ||dθ ( zi , ai ) − hθ− ( si+1 ) ||2 lc 2 . ( 4 ) ( 5 ) ( 6 ) The single-step loss is then used to jointly optimize the reward , value , and dynamics functions with coeﬃcients c1 , c2 and c3 balancing the weight of the 3 terms [ 9 ] . During training , prioritized experience replay [ 12 ] is used to store all experiences ( st , at , rt , st+1 ) of the agent in a replay buﬀer , so that they can be used to train the TOLD model [ 9 ] . Using the collected experiences , the TOLD model is iteratively updated throughout training to improve the predictions made by the value , reward , dynamics and policy functions . Given a sample tra- jectory Γ ∼ B from the replay buﬀer B of length H , we start at observation st and consider this as the starting point . At ﬁrst , st is encoded into latent state zt using hθ . Next , using the dynamics function dθ , an H time steps are unrolled by predicting the next latent state given the previous latent state and action . At each time step , the TOLD model predicts ( ˆrt , ˆqt , ˆat ) , and the single-step loss L ( θ ; Γt ) is calculated using Equation 3 . At the end , the total loss J is computed by summing the single-step losses over the H time steps as indicated by Equa- tion 2 , which is then used to perform one update of the model parameters . Then , the policy πθ is updated by minimizing the objective Jπ ( θ ; Γi ) = − t+H ( cid:88 ) i=t λi−tQθ ( zi , πθ ( zi ) ) , ( 7 ) which is a time-weighted summation of the policy objective widely used in actor- critic methods [ 3,10 ] . MPC with SRL 5 2.3 Planning During planning , the TD-MPC framework makes use of a slightly adopted ver- sion of Model Predictive Path Integral ( MPPI ) [ 17 ] control , which is a sampling- based model predictive control algorithm . We introduce ( µ0 , σ0 ) t : t+H , µ0 , σ0 ∈ Rm , A ∈ Rm , which deﬁnes parameters for a normal distribution N used to sample trajectories for planning [ 9 ] . Besides the trajectories sampled from the normal distribution , a small number of trajectories are included generated by the learned policy πθ . Using the dynamics function dθ , all the sampled trajectories are unrolled and evaluated by estimating the total return as follows : GΓ = E ( cid:34 ) γH Qθ ( zH , aH ) + ( cid:35 ) γtRθ ( zt , at ) . H−1 ( cid:88 ) t=0 ( 8 ) The terminal Q-function Qθ ( zH , aH ) provides an estimation of the return beyond the planning horizon H which supplements the short-term estimation provided by the reward function Rθ . Selecting only the best k trajectories Γ ∗ based on their return , parameters µj and σj at iteration j are iteratively updated using the following estimates which are normalized in terms of the top-k returns : µj = ( cid:80 ) k i=1 ΩiΓ ∗ i ( cid:80 ) k i=1 Ωi , σi = max   ( cid:118 ) ( cid:117 ) ( cid:117 ) ( cid:116 ) ( cid:80 ) k i − µj ) 2 i=1 Ωi ( Γ ∗ ( cid:80 ) k i=1 Ωi  , ( cid:15 )  , ( 9 ) where Ωi = τ ∗ ( Gi −G∗ ) [ 9 ] . Additionally , Gi denotes the return of trajectory Γi , G∗ represents the maximum return , τ is a temperature parameter regulating the inﬂuence of the best trajectories and ( cid:15 ) is a linearly decayed constant that enforces constant exploration [ 9 ] . This process of sampling trajectories and afterwards updating parameters µ and σ is repeated for a total of J iterations during one time step [ 9 ] . Subsequently , one trajectory Γ is sampled from the ﬁnal return- normalized distribution N and the ﬁrst action at is executed . Afterwards , this planning process is repeated at the next time step t+1 with a 1-step shifted mean µ from the previous time step [ 9 ] . This prevents the agent from starting at 0 every time and instead provides a good starting point which also includes the learnt knowledge from previous experiences [ 9 ] . However , a fairly high variance is chosen to avoid local minima . 3 TD-MPC with SRL 3.1 Reconstruction Function While in TD-MPC , the latent state representation is mainly learned and in- ﬂuenced by the consistency loss , we propose to add another loss term to the single-step loss L ( θ ; Γi ) which aims to provide the agent with a richer learning signal and thus should also improve the performance of the entire model and 6 J. Matthies et al . Fig . 1 : Updated TOLD model [ 9 ] algorithm . We extend the TOLD model by another learnable component h−1 ( shown in Fig . 1 ) deﬁned as : θ Reconstruction function : ˜st = h−1 θ ( zt ) , θ where h−1 represents the inverse of the latent state representation function hθ . We denote the result of h−1 θ ( zt ) as ˜st which represents an estimate of the respec- tive ground truth observation st since in most cases the real observation st can not be fully reconstructed . The reason for this is that the task-oriented latent state representation zt will most likely be far less complex and thus it would be chal- lenging , if not even impossible , to reconstruct the original state st entirely with only being given the latent state . As denoted by parameters θ , this additional function will be learnt together with the other components of the TOLD model . Hence , error gradients from all 4 terms lr , lv , lc and lh−1 are back-propagated through time to update each component . This also means that the other func- tions are forced to include information which is important for the reconstruction of the latent representation . Given a trajectory Γ = ( st , at , rt , st+1 ) t : t+H sam- pled from our Replay Buﬀer B and latent state representation zt = hθ ( st ) , we supplement the loss L ( θ ; Γi ) ( Equation 3 ) with the following self-supervised re- construction loss term : lh−1 i = ||h−1 θ ( zi ) − si||2 2 . ( 10 ) Similar to the reward loss ( see Equation 4 ) , the true observation st from Γ θ ( zt ) by computing is being compared to the reconstructed observation ˜st = h−1 the mean squared error . Thus , the new single-step loss L ( θ ; Γi ) , is deﬁned as : i + c4lh−1 i + c3lc L ( θ ; Γi ) = c1lr i + c2lv where , c4 is the reconstruction loss coeﬃcient . As a result , the reconstruction function will not be used in any form within the planning process of the agent , but instead only utilized to stabilize the learning process . When considering a sparse reward signal , the learning process can be inherently challenging due to the reward serving as the primary learning signal in most cases . Thus , if rewards are sparse , so is the learning signal , and as a result , the agent potentially misses ( 11 ) , i ... MPC with SRL 7 out on key insights about the environment dynamics or simply forgets about pre- viously acquired knowledge . By including the term lh−1 in the single-step loss , we aim to further enhance the learning signal and thereby improving the robust- ness of the learning process , especially in sparse and noisy environments . Since the reconstruction function is also jointly trained with the other components , error gradients are propagated through the other functions , resulting in shared behaviours and features . Thus , both the representation and dynamics functions will be forced to retain information essential to reconstruct the original obser- vation . Including a reconstruction loss can potentially lead to the representa- tion and dynamics functions gaining a better understanding of the environment beyond just the reward signal . Key features or other characteristics that help distinguish diﬀerent states could be incorporated to further improve the learning process . The goal of the reconstruction loss is to augment the learning signal , without dominating the other loss terms . Another possible advantage of our proposed change to the TD-MPC frame- work is the ability to pre-train the world model in a self-supervised manner . Essentially , we can enable the agent to explore the environment without any reward or goal and use the reconstruction loss as the primary learning signal . As a result , the agent can acquire knowledge about the environments dynamics and construct a world model before the actual training process begins . This approach can be advantageous as having a pre-trained model of the environment allows for better estimates from the start and thus for reduced training times . 4 Experiments 4.1 Setup Similar to the experiments conducted in the TD-MPC paper , we run all envi- ronments with diﬀerent random seeds and average the results [ 9 ] . As the original TD-MPC has already been thoroughly compared to other state-of-the-art algo- rithms , we consider it suﬃcient to limit our comparison to the original TD-MPC framework . We use the documented results from the oﬃcial GitHub repository1 for comparison of state-based environments , whereas we generate the data for image-based environments on our own using the oﬃcial implementation , due to no data being available for most image-based tasks . We follow the same approach as in [ 9 ] and conduct experiments on various environments from the DeepMind Control suite ( DMControl ) [ 16 ] . Hyperparameter values that are not explicitly stated are set to the values in [ 9 ] . We adopt the TD-MPC framework as described in Section 3 . Similar to the other components of the TOLD model , we employ a deterministic MLP to learn the reconstruction function . For state-based tasks , the architecture of the MLP consists of two simple fully-connected layers together with an ELU activation function , chosen intentionally to resemble the encoder architecture of the latent state representation function . The MLP architecture for image-based 1 https : //github.com/nicklashansen/tdmpc 8 J. Matthies et al . tasks is slightly more intricate due to the need of upsampling the given latent representation to match the original input size of 84x84 pixels . The initial layer is a Linear Layer , used to preprocess the input for the subsequent , whereas the next four hidden layers are deconvolutional layers , with all except the last being followed by a batch normalization layer to enhance the stability of the learning process . In terms of activation functions , we chose the ReLU function for all layers , except for the ﬁnal layer , where we used a sigmoid function to ensure that the outputs fall within the range of [ 0 , 1 ] . Additional to normalizing the images , a pixel shift augmentation of ±4 is used to prevent overﬁtting of the TOLD encoder hθ [ 9 ] . For the training of our introduced reconstruction function , we use the original images instead of the augmented ones within the respective loss lh−1 because we aim to reconstruct the original observation as provided by the environment . We empirically choose a reconstruction coeﬃcient of 2.0 , 0.25 , 0.275 for the state-based Finger Turn Hard , Cheetah Run , and Acrobat-Swingup , and of 0.15 , 0.275 , 0.15 for the image-based Finger Turn Hard , Cheetah Run , and Reacher Easy environments respectively . 4.2 Results To evaluate the performance of the two algorithms we conduct a periodic perfor- mance evaluation during training after every 20k environment steps . We evaluate the agent after every 20k environment steps , averaging over 10 episodes , then the averaged return is calculated . In the description of the results and conclusion , the term TD-MPC agent is used to refer to the agent using the original TD-MPC framework , while the TD-MPC framework with the additional reconstruction function is referred to as the reconstruction agent . Additionally , we measure the performance for image-based tasks after 100k and 500k environment steps to compare the initial learning speed , asymptotic performance as well as sample eﬃciency and robustness of the algorithms . For state-based environments , the reconstruction agent outperforms the reg- ular TD-MPC agent on two of the three selected tasks ( see Fig . 2 ) . For the tasks Finger Turn Hard and Cheetah Run , the additional reconstruction func- tion yields an increased learning speed and improved overall performance . In the Acrobot Swingup environment the reconstruction agent achieves a better initial learning speed , but worse asymptotic performance , while in terms of stability , we achieve comparable results in all three environments . Looking at the overall per- formance averaged over all three tasks , our changes to the TD-MPC framework slightly increase the learning speed and performance but seem to be slightly less stable when compared to the TD-MPC agent ( see Fig . 3 ) . In the selected image-based environments , the changes made to the single- step loss term and the additional reconstruction function do not appear to have a signiﬁcant impact on the overall learning speed of our agent ( see Fig . 2 ) . Based on the results in Table 1 , the reconstruction agent demonstrated improved performance and stability after 100k environment steps on two of the tasks , which suggests a faster initial learning process and enhanced stability , while achieving comparable performance on the others . After 500k steps , our agent outperformed MPC with SRL 9 Fig . 2 : Episode return comparison of our method ( TD-MPC with SRL ) and the original TD-MPC framework on state-based ( top row ) and image-based ( bottom row ) environments . We follow the approach from the original TD-MPC paper [ 9 ] and average the return over 5 runs . the original framework in only one of the tested environments but showed a signiﬁcant improvement in terms of stability ( lower standard deviation ) on two of the three evaluated tasks . In general , our framework achieved comparable episode returns in most image-based environments , as shown in Fig . 3 , which displays the average reward of the three image-based environments : Finger Turn Hard , Cheetah Run , and Reacher Easy . Overall , however , the modiﬁed agent exhibited improved stability after 500k environment steps in two of the three evaluated environments ( Reacher Easy , Finger Turn Hard ) , as indicated in Table 1 . 4.3 Discussion The state-based environments , especially Finger Turn Hard and Cheetah Run , showed the most signiﬁcant improvements in terms of overall performance . The introduction of the reconstruction function and its associated loss term appears to help the agent with learning a more accurate TOLD model at a faster pace than its original counterpart . Learning the reconstruction function together with the other components also results in more frequent updates to the other com- ponents like the value and dynamics functions which could possibly achieve an increased learning speed and stability . However , the modiﬁed agent did not show signiﬁcant improvements in stability , but rather a slight decrease , especially in simple low-dimensional environments like Finger Turn Hard or Acrobot Swingup , where the learning process was less robust and instead more susceptible to ran- dom seeds . This is potentially due to the unnecessary additional information being encoded in the latent state . On the other hand , for more complex environ- ments like Cheetah Run , we observed a signiﬁcantly more robust and slightly 10 J. Matthies et al . Fig . 3 : Episode return comparison of our method ( TD-MPC with SRL ) and the original TD-MPC framework . The left plot represents the average return over all three state-based environments , while the right does the same for the three image-based environments from Fig . 2 . 100K STEP SCORES TD-MPC TD-MPC with SRL Cheetah Run Reacher Easy Finger Turn Hard 246 ± 73 256 ± 116 50 ± 100 191 ± 93 319 ± 82 55.3 ± 108 500K STEP SCORES TD-MPC TD-MPC with SRL Cheetah Run Reacher Easy Finger Turn Hard 561 ± 35 668 ± 217 442 ± 187 Table 1 : Episode return comparison for image-based environments after 100k and 500k environment steps with the respective standard deviations . The column with bold numbers indicates better results in terms of return . 435 ± 79 768 ± 178 392 ± 132 faster learning process , suggesting that the information needed for reconstruction is useful in this domain and can help with strengthening the learning signal . Learning from pixels proves to be more challenging for our agent , which may be due to their inherent noisy and high-dimensional nature . In the con- text of noisy environments , the role of the latent state is crucial as it reduces the complexity of the observation by encoding only the task-relevant details , thereby ﬁltering out irrelevant information . Therefore , our modiﬁcations to the TD-MPC algorithm allow for a more eﬃcient and stable learning process by en- hancing the learning signal and updating the model components more frequently . This is important since the agent may miss out on important information due to sparse rewards when only trained on the reward signal . The increase in perfor- mance and stability are most likely attributed to the reduced susceptibility to noise and random seeds , resulting in a more consistent and steady learning pro- cess . However , in more complex environments such as Cheetah Run , the overall MPC with SRL 11 performance return-wise may still be slightly worse , due to the lack of hyperpa- rameter tuning . To further validate our ﬁndings , a more diverse comparison of performance would include a variety of other complex environments , such as the Humanoid or Dog tasks . One aspect that we have not yet explored is the ability of the modiﬁed TD- MPC with self-supervised representation learning to pre-train a model of the environment , which could supply the agent with better initial estimates through- out training . Therefore , this could potentially be another valuable opportunity to further improve the performance and stability of our modiﬁed agent . We leave this open for future work . 5 Conclusion In this paper , we proposed a modiﬁcation of the TD-MPC framework , specif- ically the TOLD model . This included an additional self-supervised loss term , as well as a new component aimed at reconstructing the original observation given a latent state and thus providing the agent with an enhanced learning signal . We assessed the modiﬁed agent ’ s performance in various experiments from the DMControl suite and compared it to the original version . Our ﬁndings showed that the modiﬁed agent performed better on the majority of state-based environments while achieving comparable stability . In image-based tasks , which inherently have high noise levels , the reconstruction agent proved to have a faster initial learning speed in most experiments , and despite struggling with the overall learning speed in most environments , the modiﬁed agent demonstrated a more robust learning process in the majority of tasks . Especially for high-dimensional environments , the agent is still struggling in terms of episode return and stabil- ity . Nevertheless , most importantly , we do not observe a signiﬁcant decrease in terms of performance on any of the tasks . These results indicate the potential beneﬁts of the reconstruction function and its associated loss term in generat- ing an enhanced learning signal and consistent updating of the TOLD model components that is not dependent on reward . 6 Acknowledgements The authors gratefully acknowledge support from the DFG ( CML , MoReSpace , LeCAREbot ) , BMWK ( SIDIMO , VERIKAS ) , and the European Commission ( TRAIL , TERAIS ) . Mostafa Kotb is funded by a scholarship from the Ministry of Higher Education of the Arab Republic of Egypt . References 1 . Fujimoto , S. , van Hoof , H. , Meger , D. : Addressing function approximation error in actor-critic methods . In : Proceedings of the 35th International Conference on Machine Learning . Proceedings of Machine Learning Research , vol . 80 , pp . 1587– 1596 . PMLR ( 2018 ) 12 J. Matthies et al . 2 . Ha , D. , Schmidhuber , J. : Recurrent world models facilitate policy evolution . In : Advances in Neural Information Processing Systems . vol . 31 . Curran Associates , Inc. ( 2018 ) 3 . Haarnoja , T. , Zhou , A. , Abbeel , P. , Levine , S. : Soft actor-critic : Oﬀ-policy maxi- mum entropy deep reinforcement learning with a stochastic actor . In : Proceedings of the 35th International Conference on Machine Learning . Proceedings of Machine Learning Research , vol . 80 , pp . 1861–1870 . PMLR ( 2018 ) 4 . Hafez , M.B. , Weber , C. , Kerzel , M. , Wermter , S. : Curious meta-controller : Adap- tive alternation between model-based and model-free control in deep reinforcement learning . In : 2019 International Joint Conference on Neural Networks ( IJCNN ) . pp . 1–8 ( 2019 ) 5 . Hafez , M.B. , Weber , C. , Kerzel , M. , Wermter , S. : Improving robot dual-system mo- tor learning with intrinsically motivated meta-control and latent-space experience imagination . Robotics and Autonomous Systems 133 , 103630 ( 2020 ) 6 . Hafner , D. , Lillicrap , T. , Ba , J. , Norouzi , M. : Dream to control : Learning behaviors by latent imagination . In : International Conference on Learning Representations ( 2020 ) 7 . Hafner , D. , Lillicrap , T. , Fischer , I. , Villegas , R. , Ha , D. , Lee , H. , Davidson , J. : Learning latent dynamics for planning from pixels . In : Proceedings of the 36th International Conference on Machine Learning . Proceedings of Machine Learning Research , vol . 97 , pp . 2555–2565 . PMLR ( 2019 ) 8 . Hafner , D. , Lillicrap , T.P. , Norouzi , M. , Ba , J. : Mastering atari with discrete world models . In : International Conference on Learning Representations ( 2021 ) 9 . Hansen , N. , Wang , X. , Su , H. : Temporal diﬀerence learning for model predictive control . In : International Conference on Machine Learning , PMLR ( 2022 ) 10 . Lillicrap , T.P. , Hunt , J.J. , Pritzel , A. , Heess , N. , Erez , T. , Tassa , Y. , Silver , D. , Wierstra , D. : Continuous control with deep reinforcement learning . In : ICLR ( 2016 ) 11 . Mnih , V. , Kavukcuoglu , K. , Silver , D. , Graves , A. , Antonoglou , I. , Wierstra , D. , Riedmiller , M.A . : Playing atari with deep reinforcement learning . CoRR abs/1312.5602 ( 2013 ) 12 . Schaul , T. , Quan , J. , Antonoglou , I. , Silver , D. : Prioritized experience replay . In : 4th International Conference on Learning Representations , ICLR 2016 , San Juan , Puerto Rico , May 2-4 , 2016 , Conference Track Proceedings ( 2016 ) 13 . Scholz , J. , Weber , C. , Hafez , M.B. , Wermter , S. : Improving model-based reinforce- ment learning with internal state representations through self-supervision . In : 2021 International Joint Conference on Neural Networks ( IJCNN ) . pp . 1–8 . IEEE ( 2021 ) 14 . Schrittwieser , J. , Antonoglou , I. , Hubert , T. , Simonyan , K. , Sifre , L. , Schmitt , S. , Guez , A. , Lockhart , E. , Hassabis , D. , Graepel , T. , et al . : Mastering atari , go , chess and shogi by planning with a learned model . Nature 588 ( 7839 ) , 604–609 ( 2020 ) 15 . Sikchi , H. , Zhou , W. , Held , D. : Learning oﬀ-policy with online planning . In : Pro- ceedings of the 5th Conference on Robot Learning . Proceedings of Machine Learn- ing Research , vol . 164 , pp . 1622–1633 . PMLR ( 2022 ) 16 . Tassa , Y. , Doron , Y. , Muldal , A. , Erez , T. , Li , Y. , de Las Casas , D. , Budden , D. , Abdolmaleki , A. , Merel , J. , Lefrancq , A. , Lillicrap , T.P. , Riedmiller , M.A . : Deepmind control suite . CoRR abs/1801.00690 ( 2018 ) 17 . Williams , G. , Aldrich , A. , Theodorou , E.A . : Model predictive path integral control : From theory to parallel computation . Journal of Guidance , Control , and Dynamics 40 ( 2 ) , 344–357 ( 2017 ) 18 . Ye , W. , Liu , S. , Kurutach , T. , Abbeel , P. , Gao , Y. : Mastering atari games with limited data . In : Advances in Neural Information Processing Systems . vol . 34 , pp . 25476–25488 . Curran Associates , Inc. ( 2021 )","['r', 'p', 'l', 'c', 'v', 'r', 'model', 'predictive', 'control', 'selfsupervise', 'representation', 'learn', 'matthie', 'technology', 'department', 'gmxde', 'burhanhafez', 'stefanwermter', 'unihamburgde', 'abstract', 'last', 'year', 'see', 'major', 'develop', 'ment', 'modelfree', 'modelbase', 'learning', 'method', 'make', 'obsolete', 'relative', 'case', 'use', 'technique', 'heavily', 'dependent', 'use', 'case', 'scenario', 'attribute', 'environment', 'approach', 'advantage', 'example', 'sample', 'eﬃciency', 'computational', 'eﬃciency', 'however', 'combine', 'advantage', 'combine', 'hence', 'achieve', 'well', 'performance', 'tdmpc', 'framework', 'example', 'approach', 'hand', 'world', 'model', 'combination', 'model', 'predictive', 'control', 'use', 'get', 'good', 'initial', 'estimate', 'value', 'function', 'hand', 'q', 'function', 'use', 'provide', 'good', 'longterm', 'estimate', 'similar', 'algorithm', 'latent', 'state', 'representation', 'use', 'taskrelevant', 'information', 'encode', 'reduce', 'complex', 'ity', 'paper', 'propose', 'use', 'reconstruction', 'function', 'tdmpc', 'framework', 'agent', 'reconstruct', 'original', 'observation', 'give', 'internal', 'state', 'representation', 'allow', 'agent', 'stable', 'learning', 'signal', 'training', 'also', 'improve', 'sample', 'eﬃciency', 'propose', 'addition', 'loss', 'term', 'lead', 'improved', 'performance', 'state', 'imagebase', 'task', 'deepmindcontrol', 'suite', 'keyword', 'reinforcement', 'learning', 'model', 'predictive', 'control', 'qlearne', 'framework', 'reconstruction', 'function', 'introduction', 'domain', 'reinforcement', 'learning', 'rl', 'main', 'category', 'approach', 'namely', 'modelfree', 'modelbase', 'learning', 'method', 'model', 'free', 'method', 'concern', 'learn', 'quality', 'speciﬁc', 'state', 'pair', 'state', 'action', 'modelbase', 'method', 'learn', 'model', 'envi', 'ronment', 'dynamic', 'modelfree', 'approach', 'evaluate', 'possible', 'action', 'next', 'state', 'time', 'step', 'modelbase', 'method', 'use', 'process', 'call', 'planning', 'help', 'world', 'model', 'actor', 'simulate', 'sequence', 'action', 'mimic', 'real', 'environment', 'hence', 'create', 'evaluate', 'diﬀerent', 'plan', 'however', 'planning', 'often', 'restrain', 'short', 'horizon', 'j', 'matthie', 'extensive', 'planning', 'feasible', 'model', 'become', 'less', 'less', 'accu', 'rate', 'action', 'simulate', 'inherently', 'modelfree', 'modelbase', 'approach', 'strength', 'weakness', 'prompt', 'question', 'method', 'combine', 'create', 'framework', 'leverage', 'beneﬁts', 'research', 'area', 'still', 'early', 'stage', 'already', 'encourage', 'ﬁnding', 'indicate', 'increase', 'performance', 'hybrid', 'learn', 'approach', 'recently', 'propose', 'temporal', 'diﬀerence', 'learning', 'model', 'predictive', 'control', 'tdmpc', 'framework', 'example', 'demonstrate', 'combination', 'modelfree', 'modelbase', 'method', 'combine', 'advantage', 'method', 'able', 'achieve', 'superior', 'sample', 'eﬃciency', 'well', 'improve', 'performance', 'compare', 'stateofthe', 'art', 'algorithm', 'soft', 'actorcritic', 'sac', 'loop', 'variety', 'task', 'utilize', 'combination', 'model', 'predictive', 'control', 'short', 'horizon', 'plan', 'qvalue', 'function', 'longterm', 'reward', 'estimate', 'similar', 'stateoftheart', 'algorithm', 'dreamer', 'eﬃcientzero', 'latent', 'representation', 'environment', 'state', 'extensively', 'use', 'training', 'provide', 'agent', 'abstract', 'representation', 'thus', 'reduce', 'complexity', 'focus', 'taskrelevant', 'detail', 'paper', 'propose', 'use', 'additional', 'loss', 'term', 'training', 'taskoriente', 'latent', 'dynamic', 'tell', 'model', 'supplement', 'already', 'implement', 'consistency', 'loss', 'additional', 'term', 'calculate', 'help', 'reconstruction', 'function', 'aim', 'reconstruct', 'original', 'obser', 'vation', 'latent', 'space', 'representation', 'latent', 'space', 'design', 'omit', 'irrelevant', 'detail', 'observation', 'reconstruction', 'function', 'hand', 'use', 'restore', 'original', 'observation', 'use', 'recon', 'struction', 'function', 'already', 'prove', 'performanceenhance', 'various', 'algorithm', 'planet', 'dreamer', 'dreamerv2', 'modiﬁed', 'ver', 'sion', 'similar', 'rest', 'tell', 'model', 'parameter', 'reconstruction', 'function', 'learn', 'jointly', 'representation', 'reward', 'value', 'function', 'use', 'temporal', 'diﬀerence', 'learn', 'complement', 'consistency', 'loss', 'reconstruction', 'loss', 'aim', 'sta', 'bilize', 'learning', 'process', 'agent', 'enhanced', 'learning', 'signal', 'seek', 'answer', 'question', 'version', 'tdmpc', 'selfsupervise', 'representation', 'learn', 'compare', 'original', 'implementation', 'framework', 'implementation', 'adopt', 'tdmpc', 'framework', 'find', 'background', 'formal', 'deﬁnition', 'deﬁne', 'inﬁnitehorizon', 'markovdecission', 'process', 'characterize', 'tuple', 'r', 'p0', 'model', 'agentenvironment', 'interaction', 'set', 'possible', 'state', 'action', 'environment', 'deﬁne', 'respectively', 'rn', '∈', 'rm', 'continuous', 'state', 'action', 'space', 'dynamic', 'environment', 'deﬁne', 'transition', 'tion', '×', 'cid55', 'determine', 'probability', 'reach', 'state', 'cid48', 'take', 'action', 'state', 'similarly', 'deﬁne', 'reward', 'function', 'r', '×', 'cid55', 'hence', 'function', 'map', 'pair', 'state', 'action', 'scalar', 'value', 'represent', 'immediate', 'feedback', 'agent', 'receive', 'tak', 'e', 'action', 'additionally', 'discount', 'factor', 'denote', 'initial', 'state', 'distribution', 'environment', 'speciﬁes', 'probability', 'dis', 'tribution', 'agent', 'start', 'possible', 'state', 'tuple', 'constitute', 'complete', 'mdp', 'serve', 'formal', 'framework', 'tdmpc', 'essential', 'component', 'rl', 'agent', 'policy', 'π', 'cid55', 'mapping', 'state', 'action', 'iteratively', 'update', 'training', 'timestep', 'policy', 'determine', 'action', 'agent', 'execute', 'thus', 'deﬁne', 'agent', 'behaviour', 'course', 'training', 'aim', 'learn', 'parameterized', 'mapping', 'πθ', 'agent', 'achieve', 'maximum', 'reward', 'tell', 'model', 'purpose', 'tell', 'model', 'provide', 'agent', 'abstract', 'model', 'world', 'exclude', 'information', 'irrelevant', 'task', 'also', 'often', 'refer', 'abstract', 'representation', 'world', 'latent', 'space', 'additionally', 'result', 'remove', 'unnecessary', 'detail', 'thus', 'case', 'signiﬁcantly', 'simplify', 'environment', 'complexity', 'training', 'process', 'also', 'become', 'less', 'complicated', 'agent', 'help', 'model', 'diﬀerent', 'component', 'number', 'quantity', 'predict', 'steer', 'learning', 'process', 'model', 'consist', 'ﬁve', 'component', 'representation', 'function', 'simple', 'encoder', 'encode', 'observation', 'respective', 'latent', 'state', 'dynamic', 'function', 'zt1', 'dθ', 'zt', 'model', 'world', 'use', 'predict', 'subsequent', 'latent', 'state', 'zt1', 'reward', 'function', 'ˆrt', 'rθ', 'zt', 'predict', 'expect', 'reward', 'ˆrt', 'latent', 'state', 'zt', 'action', 'value', 'function', 'ˆqt', 'calculate', 'estimate', 'expect', 'return', 'give', 'latent', 'state', 'zt', 'action', 'policy', 'function', 'πθ', 'predict', 'good', 'action', 'ˆat', 'take', 'give', 'latent', 'state', 'component', 'implement', 'use', 'purely', 'deterministic', 'multilayer', 'ceptron', 'mlp', 'denote', 'current', 'parameterization', 'tell', 'model', 'similar', 'approach', 'also', 'utilize', 'model', 'world', 'value', 'function', 'tdmpc', 'learn', 'policy', 'πθ', 'additional', 'model', 'environment', 'policy', 'involve', 'evaluation', 'tell', 'model', 'well', 'planning', 'process', 'agent', 'generate', 'sample', 'trajectory', 'addition', 'regular', 'tell', 'model', 'utilize', 'target', 'network', 'essentially', 'regular', 'copy', 'tell', 'model', 'parameter', 'θ−', 'slowmove', 'average', 'θ', 'use', 'target', 'net', 'work', 'help', 'prevent', 'learning', 'process', 'become', 'unstable', 'j', 'matthie', 'use', 'many', 'deepq', 'algorithm', 'deep', 'qlearne', 'td3', 'ddpg', 'slowmove', 'average', 'θ', 'also', 'update', 'time', 'accord', 'follow', 'rule', 'constant', 't1', 'ζ', 'θ−', 'train', 'tell', 'model', 'update', 'minimize', 'loss', 'j', 'θ', 'λi−tl', 'θ', 'γi', 'deﬁne', 'total', 'loss', 'compute', 'sum', 'singlestep', 'loss', 'l', 'θ', 'discount', 'factor', 'control', 'inﬂuence', 'prediction', 'far', 'future', 'b', 'deﬁne', 'trajectory', 'sample', 'replay', 'singlestep', 'loss', 'l', 'θ', 'deﬁne', 'l', 'θ', 'c2lv', 'lv', 'deﬁne', 'reward', 'loss', 'value', 'loss', 'consistency', 'loss', 'respectively', 'rθ', 'zi', 'ai', 'ri2', 'zi', 'ai', 'zi1', 'πθ', 'zi1', 'lv', 'dθ', 'zi', 'ai', 'si1', 'lc', 'singlestep', 'loss', 'use', 'jointly', 'optimize', 'reward', 'value', 'dynamic', 'function', 'coeﬃcient', 'c1', 'c3', 'balance', 'weight', 'term', 'training', 'prioritize', 'experience', 'replay', 'use', 'store', 'experience', 'st1', 'agent', 'replay', 'buﬀer', 'use', 'train', 'tell', 'model', 'use', 'collect', 'experience', 'tell', 'model', 'iteratively', 'update', 'training', 'improve', 'prediction', 'make', 'value', 'reward', 'dynamic', 'policy', 'function', 'give', 'sample', 'tra', 'jectory', 'b', 'replay', 'length', 'h', 'start', 'observation', 'consider', 'starting', 'point', 'encode', 'latent', 'state', 'use', 'next', 'use', 'dynamic', 'function', 'dθ', 'h', 'time', 'step', 'unrolled', 'predict', 'next', 'latent', 'state', 'give', 'previous', 'latent', 'state', 'action', 'time', 'step', 'tell', 'model', 'predict', 'ˆrt', 'ˆqt', 'singlestep', 'loss', 'l', 'θ', 'calculate', 'use', 'equation', 'end', 'total', 'loss', 'j', 'compute', 'sum', 'singlestep', 'loss', 'h', 'time', 'step', 'indicate', 'equa', 'tion', 'use', 'perform', 'update', 'model', 'parameter', 'policy', 'πθ', 'update', 'minimize', 'objective', 'πθ', 'timeweighted', 'summation', 'policy', 'objective', 'widely', 'use', 'actor', 'critic', 'method', 'mpc', 'planning', 'plan', 'tdmpc', 'framework', 'make', 'use', 'slightly', 'adopt', 'ver', 'sion', 'model', 'predictive', 'path', 'integral', 'control', 'sample', 'base', 'model', 'introduce', 'rm', 'rm', 'deﬁne', 'parameter', 'normal', 'distribution', 'use', 'sample', 'trajectory', 'plan', 'trajectory', 'sample', 'normal', 'distribution', 'small', 'number', 'trajectory', 'include', 'generate', 'learn', 'policy', 'πθ', 'use', 'dynamic', 'function', 'dθ', 'sample', 'trajectory', 'unrolled', 'evaluate', 'estimate', 'total', 'return', 'follow', 'e', 'cid35', 'γtrθ', 'terminal', 'qfunction', 'qθ', 'provide', 'estimation', 'return', 'planning', 'horizon', 'h', 'supplement', 'shortterm', 'estimation', 'provide', 'reward', 'function', 'select', 'good', 'k', 'trajectory', '∗', 'base', 'return', 'parameter', 'µj', 'σj', 'iteration', 'iteratively', 'update', 'use', 'follow', 'estimate', 'normalize', 'term', 'topk', 'return', '∗', 'max', 'µj', 'ωi', '∗', 'gi', '−g∗', 'additionally', 'gi', 'denote', 'return', 'trajectory', 'represent', 'maximum', 'return', 'temperature', 'parameter', 'regulate', 'inﬂuence', 'good', 'trajectory', 'cid15', 'linearly', 'decay', 'constant', 'enforce', 'constant', 'exploration', 'process', 'sample', 'trajectory', 'afterwards', 'update', 'parameter', 'repeat', 'total', 'j', 'iteration', 'time', 'step', 'subsequently', 'trajectory', 'γ', 'sample', 'ﬁnal', 'return', 'normalize', 'distribution', 'n', 'ﬁrst', 'action', 'execute', 'afterwards', 'planning', 'process', 'repeat', 'next', 'time', 'step', 't1', 'shift', 'mean', 'previous', 'time', 'step', 'prevent', 'agent', 'start', 'time', 'instead', 'provide', 'good', 'starting', 'point', 'also', 'include', 'learnt', 'knowledge', 'previous', 'experience', 'however', 'fairly', 'high', 'variance', 'choose', 'avoid', 'local', 'tdmpc', 'reconstruction', 'function', 'tdmpc', 'latent', 'state', 'representation', 'mainly', 'learn', 'ﬂuence', 'consistency', 'loss', 'propose', 'add', 'loss', 'term', 'singlestep', 'loss', 'l', 'θ', 'aim', 'provide', 'agent', 'rich', 'learning', 'signal', 'thus', 'also', 'improve', 'performance', 'entire', 'model', 'j', 'matthie', 'fig', 'update', 'model', 'extend', 'told', 'model', 'learnable', 'component', 'show', 'fig', 'deﬁne', 'reconstruction', 'function', 'θ', 'θ', 'represent', 'inverse', 'latent', 'state', 'representation', 'function', 'denote', 'result', 'θ', 'represent', 'estimate', 'tive', 'ground', 'truth', 'observation', 'case', 'real', 'observation', 'fully', 'reconstruct', 'reason', 'taskoriente', 'latent', 'state', 'representation', 'likely', 'far', 'less', 'complex', 'thus', 'chal', 'lenge', 'even', 'impossible', 'reconstruct', 'original', 'state', 'entirely', 'give', 'latent', 'state', 'denote', 'parameter', 'θ', 'additional', 'function', 'learn', 'together', 'component', 'tell', 'model', 'hence', 'error', 'gradient', 'term', 'lc', 'backpropagate', 'time', 'update', 'component', 'also', 'mean', 'tion', 'force', 'include', 'information', 'important', 'reconstruction', 'latent', 'representation', 'give', 'trajectory', 'st1', 'plead', 'replay', 'latent', 'state', 'representation', 'supplement', 'loss', 'l', 'θ', 'equation', 'follow', 'selfsupervise', 'construction', 'loss', 'term', 'θ', 'si2', 'similar', 'reward', 'loss', 'see', 'equation', 'true', 'observation', 'compute', 'compare', 'reconstructed', 'observation', 'mean', 'square', 'error', 'thus', 'new', 'singlestep', 'loss', 'l', 'θ', 'deﬁne', 'c4lh−1', 'l', 'θ', 'c2lv', 'c4', 'reconstruction', 'loss', 'coeﬃcient', 'result', 'reconstruction', 'function', 'use', 'form', 'planning', 'process', 'agent', 'instead', 'utilize', 'stabilize', 'learning', 'process', 'consider', 'sparse', 'reward', 'signal', 'learning', 'process', 'inherently', 'challenge', 'reward', 'serve', 'primary', 'learning', 'signal', 'case', 'thus', 'reward', 'sparse', 'learning', 'signal', 'result', 'agent', 'potentially', 'miss', 'mpc', 'key', 'insight', 'environment', 'dynamic', 'simply', 'forget', 'pre', 'viously', 'acquire', 'knowledge', 'include', 'term', 'singlestep', 'loss', 'aim', 'far', 'enhance', 'learning', 'signal', 'thereby', 'improve', 'robust', 'ness', 'learning', 'process', 'especially', 'sparse', 'noisy', 'environment', 'reconstruction', 'function', 'also', 'jointly', 'train', 'component', 'error', 'gradient', 'propagate', 'function', 'result', 'share', 'behaviour', 'feature', 'thus', 'representation', 'dynamic', 'function', 'force', 'retain', 'information', 'essential', 'reconstruct', 'original', 'obser', 'vation', 'include', 'reconstruction', 'loss', 'potentially', 'lead', 'representa', 'tion', 'dynamic', 'function', 'gain', 'well', 'understanding', 'environment', 'reward', 'signal', 'key', 'feature', 'characteristic', 'help', 'distinguish', 'diﬀerent', 'state', 'incorporate', 'far', 'improve', 'learning', 'process', 'goal', 'reconstruction', 'loss', 'augment', 'learning', 'signal', 'dominate', 'loss', 'term', 'possible', 'advantage', 'propose', 'change', 'tdmpc', 'frame', 'work', 'ability', 'pretrain', 'world', 'model', 'selfsupervise', 'manner', 'essentially', 'enable', 'agent', 'explore', 'environment', 'reward', 'goal', 'use', 'reconstruction', 'loss', 'primary', 'learning', 'signal', 'result', 'agent', 'acquire', 'knowledge', 'environment', 'dynamic', 'construct', 'world', 'model', 'actual', 'training', 'process', 'begin', 'approach', 'advantageous', 'pretraine', 'model', 'environment', 'allow', 'well', 'estimate', 'start', 'thus', 'reduced', 'training', 'time', 'experiment', 'setup', 'similar', 'experiment', 'conduct', 'tdmpc', 'paper', 'run', 'envi', 'ronment', 'diﬀerent', 'random', 'seed', 'average', 'result', 'original', 'tdmpc', 'already', 'thoroughly', 'compare', 'stateoftheart', 'algo', 'rithms', 'consider', 'suﬃcient', 'limit', 'comparison', 'original', 'tdmpc', 'framework', 'use', 'document', 'result', 'oﬃcial', 'repository1', 'comparison', 'statebased', 'environment', 'generate', 'datum', 'imagebase', 'environment', 'use', 'oﬃcial', 'implementation', 'data', 'available', 'imagebase', 'task', 'follow', 'approach', 'conduct', 'experiment', 'various', 'environment', 'deepmind', 'control', 'dmcontrol', 'hyperparameter', 'value', 'explicitly', 'state', 'set', 'value', 'adopt', 'tdmpc', 'framework', 'describe', 'section', 'similar', 'component', 'tell', 'model', 'employ', 'deterministic', 'mlp', 'learn', 'reconstruction', 'function', 'statebased', 'task', 'architecture', 'mlp', 'consist', 'simple', 'fullyconnected', 'layer', 'together', 'elu', 'activation', 'function', 'choose', 'intentionally', 'resemble', 'encoder', 'architecture', 'latent', 'state', 'representation', 'function', 'mlp', 'architecture', 'imagebase', 'https', 'githubcomnicklashansentdmpc', 'j', 'matthie', 'task', 'slightly', 'intricate', 'need', 'upsample', 'give', 'latent', 'representation', 'match', 'original', 'input', 'size', 'pixel', 'initial', 'layer', 'linear', 'layer', 'use', 'preprocess', 'input', 'subsequent', 'next', 'hidden', 'layer', 'deconvolutional', 'layer', 'last', 'follow', 'batch', 'normalization', 'layer', 'enhance', 'stability', 'learning', 'process', 'term', 'activation', 'function', 'choose', 'relu', 'function', 'layer', 'ﬁnal', 'layer', 'use', 'sigmoid', 'function', 'ensure', 'output', 'fall', 'range', 'additional', 'normalize', 'image', 'pixel', 'shift', 'augmentation', '±4', 'use', 'prevent', 'overﬁtting', 'told', 'encoder', 'training', 'introduce', 'reconstruction', 'function', 'use', 'original', 'image', 'instead', 'augment', 'one', 'respective', 'loss', 'lh−1', 'aim', 'reconstruct', 'original', 'observation', 'provide', 'environment', 'empirically', 'choose', 'reconstruction', 'coeﬃcient', 'statebased', 'finger', 'turn', 'hard', 'run', 'imagebase', 'finger', 'turn', 'hard', 'run', 'reacher', 'easy', 'environment', 'respectively', 'result', 'evaluate', 'performance', 'algorithm', 'conduct', 'periodic', 'perfor', 'mance', 'evaluation', 'training', '20k', 'environment', 'step', 'evaluate', 'agent', '20k', 'environment', 'step', 'average', 'episode', 'average', 'return', 'calculate', 'description', 'result', 'conclusion', 'term', 'tdmpc', 'agent', 'use', 'refer', 'agent', 'use', 'original', 'tdmpc', 'framework', 'tdmpc', 'framework', 'additional', 'reconstruction', 'function', 'refer', 'reconstruction', 'agent', 'additionally', 'measure', 'performance', 'imagebase', 'task', '100k', 'environment', 'step', 'compare', 'initial', 'learning', 'speed', 'asymptotic', 'performance', 'well', 'sample', 'eﬃciency', 'robustness', 'algorithm', 'statebased', 'environment', 'reconstruction', 'agent', 'outperform', 'ular', 'tdmpc', 'agent', 'select', 'task', 'see', 'fig', 'task', 'finger', 'turn', 'hard', 'run', 'additional', 'reconstruction', 'tion', 'yield', 'increase', 'learn', 'speed', 'improve', 'overall', 'performance', 'environment', 'reconstruction', 'agent', 'achieve', 'well', 'initial', 'learning', 'speed', 'worse', 'asymptotic', 'performance', 'term', 'stability', 'achieve', 'comparable', 'result', 'environment', 'look', 'overall', 'formance', 'average', 'task', 'change', 'tdmpc', 'framework', 'slightly', 'increase', 'learn', 'speed', 'performance', 'seem', 'slightly', 'less', 'stable', 'compare', 'tdmpc', 'agent', 'see', 'fig', 'select', 'imagebase', 'environment', 'change', 'make', 'single', 'step', 'loss', 'term', 'additional', 'reconstruction', 'function', 'appear', 'signiﬁcant', 'impact', 'overall', 'learn', 'speed', 'agent', 'see', 'fig', 'base', 'result', 'table', 'reconstruction', 'agent', 'demonstrate', 'improved', 'performance', 'stability', 'environment', 'step', 'task', 'suggest', 'fast', 'initial', 'learning', 'process', 'enhance', 'stability', 'achieve', 'comparable', 'performance', 'step', 'agent', 'outperform', 'fig', 'episode', 'return', 'comparison', 'method', 'tdmpc', 'original', 'tdmpc', 'framework', 'statebased', 'top', 'row', 'imagebase', 'bottom', 'row', 'environment', 'follow', 'approach', 'original', 'tdmpc', 'paper', 'average', 'return', 'run', 'original', 'framework', 'test', 'environment', 'show', 'signiﬁcant', 'improvement', 'term', 'stability', 'low', 'standard', 'deviation', 'evaluate', 'task', 'general', 'framework', 'achieve', 'comparable', 'episode', 'return', 'imagebase', 'environment', 'show', 'fig', 'display', 'average', 'reward', 'imagebase', 'environment', 'finger', 'turn', 'hard', 'run', 'reacher', 'easy', 'overall', 'however', 'agent', 'exhibit', 'improved', 'stability', 'environment', 'step', 'evaluate', 'environment', 'reacher', 'easy', 'finger', 'turn', 'hard', 'indicate', 'table', 'discussion', 'statebase', 'environment', 'especially', 'finger', 'turn', 'hard', 'run', 'show', 'signiﬁcant', 'improvement', 'term', 'overall', 'performance', 'introduction', 'reconstruction', 'function', 'associated', 'loss', 'term', 'appear', 'help', 'agent', 'learn', 'accurate', 'told', 'model', 'fast', 'pace', 'original', 'counterpart', 'learn', 'reconstruction', 'function', 'together', 'component', 'also', 'result', 'frequent', 'update', 'com', 'ponent', 'value', 'dynamic', 'function', 'possibly', 'achieve', 'increase', 'learn', 'speed', 'stability', 'however', 'modiﬁed', 'agent', 'show', 'signiﬁcant', 'improvement', 'stability', 'rather', 'slight', 'decrease', 'especially', 'simple', 'lowdimensional', 'environment', 'finger', 'turn', 'hard', 'learning', 'process', 'less', 'robust', 'instead', 'susceptible', 'run', 'seed', 'potentially', 'due', 'unnecessary', 'additional', 'information', 'encode', 'latent', 'state', 'hand', 'complex', 'environ', 'ment', 'observe', 'signiﬁcantly', 'robust', 'slightly', 'j', 'matthie', 'fig', 'episode', 'return', 'comparison', 'method', 'tdmpc', 'original', 'tdmpc', 'framework', 'left', 'plot', 'represent', 'average', 'return', 'statebased', 'environment', 'right', 'imagebase', 'environment', 'fig', 'step', 'score', 'tdmpc', 'tdmpc', 'run', 'reacher', 'easy', 'finger', 'turn', 'hard', 'step', 'score', 'tdmpc', 'tdmpc', 'run', 'reacher', 'easy', 'finger', 'turn', 'hard', 'table', 'episode', 'return', 'comparison', 'imagebase', 'environment', '100k', 'environment', 'step', 'respective', 'standard', 'deviation', 'column', 'bold', 'number', 'indicate', 'well', 'result', 'term', 'return', 'fast', 'learning', 'process', 'suggest', 'information', 'need', 'reconstruction', 'useful', 'domain', 'help', 'strengthen', 'learning', 'signal', 'learn', 'pixel', 'prove', 'challenging', 'agent', 'due', 'inherent', 'noisy', 'highdimensional', 'nature', 'con', 'text', 'noisy', 'environment', 'role', 'latent', 'state', 'crucial', 'reduce', 'complexity', 'observation', 'encode', 'taskrelevant', 'detail', 'thereby', 'ﬁltere', 'irrelevant', 'information', 'therefore', 'modiﬁcation', 'tdmpc', 'allow', 'eﬃcient', 'stable', 'learning', 'process', 'hance', 'learning', 'signal', 'update', 'model', 'component', 'frequently', 'important', 'agent', 'miss', 'important', 'information', 'sparse', 'reward', 'train', 'reward', 'signal', 'increase', 'mance', 'stability', 'likely', 'attribute', 'reduced', 'susceptibility', 'noise', 'random', 'seed', 'result', 'consistent', 'steady', 'learn', 'pro', 'cess', 'however', 'complex', 'environment', 'run', 'overall', 'mpc', 'performance', 'returnwise', 'still', 'slightly', 'bad', 'lack', 'hyperpa', 'rameter', 'tune', 'far', 'validate', 'ﬁnding', 'diverse', 'comparison', 'performance', 'include', 'variety', 'complex', 'environment', 'humanoid', 'dog', 'task', 'aspect', 'yet', 'explore', 'ability', 'modiﬁed', 'mpc', 'selfsupervise', 'representation', 'learning', 'pretrain', 'model', 'environment', 'supply', 'agent', 'well', 'initial', 'estimate', 'training', 'therefore', 'potentially', 'valuable', 'opportunity', 'far', 'improve', 'performance', 'stability', 'modiﬁed', 'agent', 'leave', 'open', 'future', 'work', 'conclusion', 'paper', 'propose', 'modiﬁcation', 'tdmpc', 'framework', 'specif', 'ically', 'tell', 'model', 'include', 'additional', 'selfsupervise', 'loss', 'term', 'well', 'new', 'component', 'aim', 'reconstruct', 'original', 'observation', 'give', 'latent', 'state', 'thus', 'provide', 'agent', 'enhanced', 'learning', 'signal', 'assess', 'modiﬁed', 'agent', 'performance', 'various', 'experiment', 'dmcontrol', 'suite', 'compare', 'original', 'version', 'ﬁnding', 'show', 'agent', 'perform', 'well', 'majority', 'statebased', 'environment', 'achieve', 'comparable', 'stability', 'imagebase', 'task', 'inherently', 'high', 'noise', 'level', 'reconstruction', 'agent', 'prove', 'fast', 'initial', 'learning', 'speed', 'experiment', 'struggle', 'overall', 'learn', 'speed', 'environment', 'modiﬁed', 'agent', 'demonstrate', 'robust', 'learning', 'process', 'majority', 'task', 'especially', 'highdimensional', 'environment', 'agent', 'still', 'struggle', 'term', 'episode', 'return', 'ity', 'nevertheless', 'importantly', 'observe', 'signiﬁcant', 'decrease', 'term', 'performance', 'task', 'result', 'indicate', 'potential', 'beneﬁts', 'reconstruction', 'function', 'associated', 'loss', 'term', 'generat', 'enhanced', 'learning', 'signal', 'consistent', 'updating', 'tell', 'model', 'component', 'dependent', 'reward', 'acknowledgement', 'author', 'gratefully', 'acknowledge', 'support', 'dfg', 'cml', 'sidimo', 'verika', 'trail', 'fund', 'scholarship', 'high', 'education', 'reference', 'fujimoto', 'hoof', 'h', 'address', 'function', 'approximation', 'error', 'actorcritic', 'method', 'proceeding', '35th', 'international', 'conference', 'machine', 'learn', 'proceeding', 'machine', 'learn', 'research', 'vol', 'pp', 'pmlr', 'matthie', 'schmidhuber', 'j', 'recurrent', 'world', 'model', 'facilitate', 'policy', 'evolution', 'advance', 'neural', 'information', 'processing', 'system', 'vol', 'haarnoja', 'zhou', 'abbeel', 'p', 'levine', 'soft', 'actorcritic', 'oﬀpolicy', 'maxi', 'reinforcement', 'learning', 'stochastic', 'actor', 'proceeding', '35th', 'international', 'conference', 'machine', 'learn', 'proceeding', 'machine', 'learn', 'research', 'vol', 'pp', 'pmlr', 'hafez', 'c', 'kerzel', 'wermter', 'curious', 'metacontroller', 'adap', 'tive', 'alternation', 'modelbase', 'modelfree', 'control', 'deep', 'reinforcement', 'learning', 'international', 'joint', 'conference', 'neural', 'network', 'pp', '1–8', 'hafez', 'c', 'kerzel', 'wermter', 'improve', 'robot', 'dualsystem', 'tor', 'learn', 'intrinsically', 'motivate', 'metacontrol', 'latentspace', 'experience', 'imagination', 'robotic', 'autonomous', 'system', 'hafner', 'lillicrap', 'j', 'dream', 'control', 'learn', 'behavior', 'latent', 'imagination', 'international', 'conference', 'learn', 'representation', 'hafner', 'lillicrap', 'fischer', 'r', 'h', 'davidson', 'learn', 'latent', 'dynamic', 'plan', 'pixel', 'proceeding', '36th', 'international', 'conference', 'machine', 'learn', 'proceeding', 'machine', 'learn', 'research', 'vol', 'pp', 'pmlr', 'hafner', 'lillicrap', 'tp', 'j', 'master', 'atari', 'discrete', 'world', 'model', 'international', 'conference', 'learn', 'representation', 'hansen', 'n', 'h', 'temporal', 'diﬀerence', 'learning', 'model', 'predictive', 'control', 'international', 'conference', 'machine', 'learn', 'pmlr', 'lillicrap', 'tp', 'hunt', 'jj', 'pritzel', 'heess', 'n', 'erez', 'tassa', 'silver', 'wierstra', 'continuous', 'control', 'deep', 'reinforcement', 'learning', 'iclr', 'silver', 'grave', 'antonoglou', 'riedmiller', 'play', 'atari', 'deep', 'reinforcement', 'learning', 'schaul', 'antonoglou', 'silver', 'prioritize', 'experience', 'replay', '4th', 'international', 'conference', 'learn', 'representation', 'iclr', 'conference', 'track', 'proceeding', 'scholz', 'c', 'hafez', 'wermter', 'improve', 'modelbase', 'reinforce', 'ment', 'learn', 'internal', 'state', 'representation', 'selfsupervision', 'international', 'joint', 'conference', 'neural', 'network', 'pp', 'ieee', 'schrittwieser', 'antonoglou', 'sifre', 'l', 'schmitt', 'guez', 'lockhart', 'e', 'graepel', 'et', 'master', 'atari', 'go', 'chess', 'shogi', 'plan', 'learned', 'model', 'nature', 'h', 'w', 'hold', 'learn', 'oﬀpolicy', 'online', 'planning', 'pro', 'ceeding', '5th', 'conference', 'robot', 'learn', 'proceeding', 'machine', 'learn', 'research', 'vol', 'pp', 'pmlr', 'tassa', 'doron', 'muldal', 'erez', 'budden', 'merel', 'j', 'lefrancq', 'lillicrap', 'tp', 'riedmiller', 'deepmind', 'control', 'aldrich', 'theodorou', 'model', 'predictive', 'path', 'integral', 'control', 'theory', 'parallel', 'computation', 'journal', 'guidance', 'control', 'dynamic', 'ye', 'abbeel', 'p', 'master', 'atari', 'game', 'limited', 'datum', 'advance', 'neural', 'information', 'processing', 'system', 'vol', 'pp']"
"The Night Shift: Understanding Performance Variability of Cloud
  Serverless Platforms","[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3592533.3592808', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2304.07177v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2304.07177v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-04-14 14:51:35,"3
2
0
2

r
p
A
4
1

]

G
L
.
s
c
[

1
v
7
4
9
6
0
.
4
0
3
2
:
v
i
X
r
a

TimelyFL: Heterogeneity-aware Asynchronous Federated Learning with
Adaptive Partial Training

Tuo Zhang1, Lei Gao1, Sunwoo Lee2, Mi Zhang3, Salman Avestimehr1
1University of Southern California
2Inha University
3The Ohio State University
tuozhang@usc.edu

Abstract

In cross-device Federated Learning (FL) environments,
scaling synchronous FL methods is challenging as strag-
glers hinder the training process. Moreover, the availabil-
ity of each client to join the training is highly variable over
time due to system heterogeneities and intermittent connec-
tivity. Recent asynchronous FL methods (e.g., FedBuff
[22]) have been proposed to overcome these issues by al-
lowing slower users to continue their work on local train-
ing based on stale models and to contribute to aggregation
when ready. However, we show empirically that this method
can lead to a substantial drop in training accuracy as well
as a slower convergence rate. The primary reason is that
fast-speed devices contribute to many more rounds of ag-
gregation while others join more intermittently or not at all,
and with stale model updates. To overcome this barrier, we
propose TimelyFL, a heterogeneity-aware asynchronous
FL framework with adaptive partial training. During the
training, TimelyFL adjusts the local training workload
based on the real-time resource capabilities of each client,
aiming to allow more available clients to join in the global
update without staleness. We demonstrate the performance
benefits of TimelyFL by conducting extensive experiments
on various datasets (e.g., CIFAR-10, Google Speech, and
Reddit) and models (e.g., ResNet20, VGG11, and ALBERT).
In comparison with the state-of-the-art (i.e., FedBuff),
our evaluations reveal that TimelyFL improves participa-
tion rate by 21.13%, harvests 1.28× - 2.89× more efficiency
on convergence rate, and provides a 6.25% increment on
test accuracy.

1. Introduction

Federated learning (FL) has emerged as a promising
distributed machine learning paradigm that preserves pri-
vacy [12, 28]. The gist of FL is to keep the clients’ pri-

vate data on the devices and perform local model training
for each client. A central server will collect these locally
trained models to update a global model and then push it
back for the next round of training.

Most existing FL protocols are based on synchronous FL
training (SyncFL), meaning that at each round all clients (or
a selected cohort of clients) are updating their local mod-
els based on the latest update broadcast by the server at the
beginning of that round. Due to the unbalanced commu-
nication or hardware capabilities and non-identical training
data distribution, however, the time consumption for a lo-
cal update can vary substantially from device to device, and
some clients may even be temporarily disconnected dur-
ing the training process [27]. Thus, leaving the server with
two suboptimal choices: to wait for all clients participating
in each round to finish their local training and contribute
to model aggregation (which will cause significant delays
due to stragglers), or to only wait for a subset of the faster
clients (which will ignore all the work and contributions
from slower clients). These critical challenges largely im-
pede the scalability of SyncFL and make it difficult to land
in large-scale cross-device scenarios.

To address those challenges, recent works have proposed
asynchronous federated learning (AsyncFL) [3, 9, 22, 30],
which allows slower clients to continue local training and
contribute to future aggregation rounds. AsyncFL de-
couples client local training from global model aggrega-
tion/updates, as only certain clients would simultaneously
get an update from the cloud server, which decreases the
impact of stragglers. The most recent AsyncFL work –
FedBuff [9, 22] – proposes that the server should perform
a gradient aggregation to create a global model once the
number of received local updates reaches a requisite thresh-
old, which is a tunable parameter referred as aggregation
goal. The slower clients can still upload their updates later
as long as they finish local training, but their updates may
not be included based on staleness information.

 
 
 
 
 
 
(a) Participation rate distribution across all de-
vices

(b) Number of participated clients during FL
training rounds

(c) Time-to-accuracy performance for each strat-
egy

Figure 1. Empirical performance of SyncFL, FedBuff, and TimelyFL in CIFAR-10 classification task with FedOpt as server aggregator
(for experiment details and other evaluations see Section 4). TimelyFL includes more devices join in global update during the training
(shown in (a) and (b)). As more devices participate timely, TimelyFL harvests both convergence rate and accuracy boost compared to
FedBuff (shown in (c)).

As highlighted in Figure 1c, we empirically demonstrate
that while FedBuff achieves much faster convergence to
a certain intermediate accuracy, it can, unfortunately, lead
to a substantial drop in final accuracy compared to SyncFL.
The intuitive explanation is that, as FedBuff only accepts
a fixed number of local updates to contribute to the global
model in every communication round, it decreases the par-
allel computing efficiency by blocking other completed lo-
cal updates into global aggregation, which turns them into
stale updates as they would be postponed to the next round
of global update. Moreover, the server aggregator prefers
the fast-speed devices contributing more rounds of train-
ing, whereas low-speed devices may not enjoy the same
frequency of contribution. Even when the slow devices par-
ticipate in global training, they occasionally send the staled
updates that potentially harm the convergence of the global
loss. As shown in Figure 1a and 1b, compared to SyncFL,
FedBuff only includes a fixed number of local updates per
round, and achieves a low participation rate (i.e., the num-
ber of aggregation participated divided by the total number
of aggregation rounds) on average with a biased distribu-
tion, indicating that the inclusiveness of a group has been
diminished, which is the root cause of the test accuracy gap.

To close the gap between SyncFL and AsyncFL, we pro-
pose TimelyFL, a heterogeneity-aware asynchronous fed-
erated learning framework based on adaptive partial train-
ing. One key distinction of TimelyFL from previous
AsyncFL works is that TimelyFL does not set a fixed
number limit to the number of local updates for the global
Instead, to accommodate a flexi-
aggregation per round.
ble number of clients joining in the global update, we set
a wall-clock time limit for every aggregation interval. The
aggregation interval equals the kth fastest local update time
among all clients, where k is a tunable parameter. As long
as the device can deliver its model update to the server

within this interval, it will be part of the global aggrega-
tion. To include more available devices to join in global
aggregation without staleness, we introduce partial model
training for clients with low capacity. Instead of fully train-
ing a model, only a part of the model that composes of a
subset of consecutive output-side layers will be assigned to
them for backward pass training. With partial model train-
ing, both local computation time and communication time
will decrease for stale clients.

As shown in Figure 2, TimelyFL unifies the local
training time by adaptively adjusting the workload (i.e.,
the local epoch number and partial training ratio) for each
client, making it feasible for clients to finish the local train-
ing and upload the updates to the server within the calcu-
lated aggregation interval in every communication round.
As such, TimelyFL tackles the system heterogeneity is-
sue and eliminates the staleness of local update reports for
slower devices.

We evaluate the performance of TimelyFL across vari-
ous application tasks, including image classification, speech
recognition, and natural language processing on CIFAR-
10 [13], Google Speech Command [29], and Reddit Com-
ment [1] benchmark datasets, respectively, with two com-
monly used aggregation functions, FedAvg [20] and Fe-
dOpt [23]. Our results show that 66.4% of devices increase
the participation rate and the average participation rate in-
creases by 21.1% in TimelyFL compared to FedBuff.
Under the same scale of the FL system, TimelyFL outper-
forms FedBuff [22] on both time-to-accuracy and final
test accuracy, as shown in Figure 1c.

2. Related Work

Asynchronous Federated Learning. Due to inter-
mittent connectivity and availability among clients, asyn-
chronous FL is a promising solution to tackle device het-

Figure 2. FedBuff (top): Server updates the global model as it receives the requisite number of local updates, and slower devices still
could send their updates at a later time to the server. Fast devices participate more times in the global update, while slow devices contribute
less or no participation. TimelyFL (bottom): Server updates the individual workload every round based on the real-time availability of
each client to include more devices in global update timely, largely increases the participation rate for slow devices.

erogeneity in FL [34]. Most asynchronous FL works con-
centrate on solving the straggler problem, such as [19], [30],
and [31]. PAPAYA [9] and FedBuff [22] have been pro-
posed to mitigate stragglers and enable secure aggregation
jointly. Specifically, the individual updates are not incorpo-
rated by the server as soon they arrive. Instead, the server
will keep receiving local updates in a secure buffer, where
the buffer size is a tunable parameter, and update the global
model whenever the buffer is full. The slow devices can also
send the local update to the server after the global aggrega-
tion. Their update will be considered for the next available
global update. However, practically speaking, fast devices
participate in global updates many more times than slow
devices, and some slow devices cannot join in the global
aggregation even once due to the staleness control.

All of the above approaches assume that the client should
process the local training within the full-model size. As the
slower users participate in the global aggregation, they can
only contribute with stale updates. Some previous works
have pointed out that the effects of the stale update on
distributed machine learning can directly harm the over-
all convergence behavior for the final model, aligned with
the asynchronous distributed optimization theory suggested
by [4, 5, 36]. Moreover, the participation rate is mainly un-
balanced due to the high-speed devices contributing more
rounds to global updates compared to the slow-speed de-
vices. In contrast to previous approaches, we focus on en-
abling all clients to join in the global aggregation effectively
based on their local resources to improve the inclusiveness
of the final global model training.

Partial Model Training. Partial model training can be
viewed as an efficient approach to reduce both communica-

tion and computation workload on the client-side of the FL
system [2]. FedPrune [21] proposes a method that prunes
the global model for each client based on their device capa-
bilities, where slow clients are served smaller models and
faster clients train on larger models. FedPT [25] lever-
ages the partially trainable neural networks on clients to re-
duce communication costs and enable faster training with
a smaller memory footprint and with few implications on
model performance. Other works such as [17, 24, 32] also
address that partial model training can save both communi-
cation cost and memory usage in cross-device FL. All of the
above works maintain the partial ratio for the sub-model of a
certain client as constant during the entire FL training pro-
cess, which neglects that the availability of each device is
not stable throughout the time. In this work, we adaptively
adjust the partial ratio for the local model training based
on the real-time device availability, which aims to improve
both efficiency and utility for each client.

3. Our Method

3.1. Standard Asynchronous Federated Learning

Figure 3 (left) illustrates the standard asynchronous FL
framework. Instead of waiting for all clients to finish the
local model training, the server stores the individual up-
dates from clients in a buffer and then adjusts the global
model once the buffer size reaches the requisite number of
the aggregation goal. Other non-participating devices will
postpone their contribution to global updates in the latter
communication round once they finish the training. Given
that the standard AsyncFL framework suffers from inclu-
siveness constraints described in the introduction section,

Figure 3. Left: The standard AsyncFL framework. The server will initiate the global update once it collects the requisite number of
local updates. The other clients will be postponed to a latter communication round with stale update information. Right: The proposed
TimelyFL. The server will include all the received local updates within aggregation interval to global update. Clients with a weaker
capacity are assigned to train a subset of the model to catch the aggregation interval time.

we propose an efficient AsyncFL framework as shown in
Figure 3 (right) to address this issue.

3.2. TimelyFL Design

3.2.1 Preliminaries

To increase the participation rate for the slow clients, we
aim to design a cross-device asynchronous federated learn-
ing framework where each device can finish the local up-
date within a limited time interval by adaptively adjusting
its workload per round. Here, the workload is defined as the
product of local training epoch number E and partial model
training ratio α. To formalize this, our time utility function,
which considers the local training optimization at the client
side, is designed as follows:

arg max
E,α

(˜tcmp,c × E × α + ˜tcom,c × α) ≤ Tk

(1)

where ˜tcmp,c is the estimated local computation time, and
˜tcom,c is the estimated local communication time of the
client c for one epoch of full model training in a certain
communication round calculated by the server. Note that
both ˜tcmp,c and ˜tcom,c are not constant throughout the train-
ing due to the nature of mobile devices. By adjusting E
and α, each device is supposed to participate in the global
aggregation every communication round timely and max-
imally utilize its resource capability within aggregation in-
terval Tk. Therefore, the overall distributed optimization in-
volves more iterations on diverse datasets, leading to faster
convergence.

and impeding them from contributing to the global model.
To address this issue, we introduce partial model training to
slow devices. Figure 3 (right) illustrates our approach when
partial model training lands on the device heterogeneity FL
system. Instead of a full training model, weak devices will
be assigned to train partial models composed of a subset of
consecutive output-side layers. During the training, only the
subset of assigned layers will operate both forward pass and
backward propagation, while the other layers will only pro-
cess the forward pass for the input data but be frozen during
weight updates. After local training finishes, the partially-
trained clients only return the update for the assigned layers
to the server for the global aggregation, as the frozen layers
are unchanged during training.

We target to solve two bottlenecks in the cross-device
FL with partial model training, communication and compu-
tation efficiency for the stale clients. In edge computing, the
backward propagation consumes much more time than the
forward pass. Partial model training would essentially re-
duce the training time, as it does not need to calculate gradi-
ents for the frozen parameters. The reduced time is roughly
proportional to the reduced model size, as we empirically
shown in the Appendix A.2.1. Moreover, we only send the
trainable part of updates to the server, substantially improv-
ing communication efficiency, especially when stragglers
with limited network connections exist. By implementing
partial model training, we aim to let low-capacity devices
report their local updates to the server timely without stal-
eness, thereby improving their participation rate during FL
training.

3.2.2 Adaptive Model Training

Due to resource limitations, some weak devices may not
finish the full model training effectively within the time in-
terval Tk, making them become stale clients in the system

3.2.3 TimelyFL Algorithm

Based on the adaptive model training, we propose the
TimelyFL. TimelyFL tries to unify each client’s round

time to the limited aggregation interval Tk by adaptively
adjusting the workload concerning its real-time availability
per communication round. The workload is defined as the
product of the partial training ratio α and the local epoch
number E. TimelyFL framework is composed of three
main parts, TimelyFL server, local time update, and work-
load scheduling.

time tcmp from one data batch full model training. The unit
computation time ˜tcmp is estimated by tcmp and progress β,
where β is defined as the ratio of trained batch number to
the total data batch number. The local communication time
equals the model’s file size M over the device’s real-time
network bandwidth Bw, as the same setting in the previous
FL system work [14].

Algorithm 1: TimelyFL.

Input: k: the aggregation participation target, n:
the number of training concurrency
1 for r ∈ {0, · · · , R − 1} communication rounds do
2

Global server do:

3

4

5

6

7

8

9

10

11

Sample n clients uniformly at random to

define S, and send W r
Clients c ∈ S in parallel do:

s to clients in S ;

˜ttotal, ˜tcmp, ˜tcom = LocalTimeUpdate(M ) ;

Global server do:

k ← the kth smallest number in ⟨˜ttotal⟩ ;
T r
⟨Er⟩, ⟨αr⟩, ⟨tr

rpt⟩ =

r, ⟨˜tcmp⟩, ⟨˜tcom⟩) ;

WorkloadScheduling(Tk
Clients c ∈ S in parallel do:

W r

c ← adaptive model training ;

Global server do:

W r+1

s ← aggregate ⟨W r

c ⟩ ;

12
13 end

Output: W R
s

TimelyFL Server. TimelyFL server is in charge of
adjusting the aggregation interval Tk, local training epoch
E, and partial training ratio α for each device during the
FL training, as summarized in Algorithm 1. The aggre-
gation interval Tk in each round equals the kth smallest
value among ⟨˜ttotal⟩, as the estimated unit total time for
all clients. At each communication round, TimelyFL
server randomly samples n clients to construct the collec-
tion S and distributes the global model to the clients inside
S, which means n clients would start the local training in
this round, same as the definition of training concurrency
in the FedBuff. Each selected client would perform one
data batch full model training to estimate its time consump-
tion and report it to the server. Then, aggregation interval
time Tk and training hyperparameters for client c (i.e., lo-
cal training epoch number E and partial training ratio α)
would be adjusted based on all selected clients’ status dur-
ing the FL training process. The server would also return a
local computation budget time trpt,c, as the wall-clock time
when the client must report its training status.

Local Time Update. To efficiently accommodate the
capabilities, each participant needs to update its time con-
sumption to the server as summarized in Algorithm 2.
Specifically, each client would collect the real computation

Algorithm 2: Local Time Update.

Input: M : the file size of the received global model,
Bw: the real-time network bandwidth

1 tcmp, β ← one data batch training ;
2 ˜tcom = M/Bw ;
3 ˜tcmp = tcmp/β ;
4 ˜ttotal = ˜tcmp + ˜tcom ;

Output: ˜ttotal, ˜tcom, ˜tcmp

Workload Scheduling. TimelyFL would adjust the lo-
cal epoch number E and partial training ratio α for each
client in every communication round based on the estimated
˜tcom,c, ˜tcmp,c and aggregation interval Tk, as the relation-
ship shown in 1.
If one’s unit total time is smaller than
Tk, then the server would try to maximize its local training
utility and minimize the idle time, as to assign more than
one local epoch training for the next round. Otherwise, the
server would assign less amount of workload to them by de-
creasing the model training ratio α, which guarantees they
can finish at least one local epoch training within the report
time trpt,c and catch up the global aggregation timely. We
summarized the scheduler as Algorithm 3.

Algorithm 3: Workload Scheduling.
Input: Tk: aggregation interval time, ⟨˜tcmp⟩: unit

computation time, ⟨˜tcom⟩: unit
communication time
1 for each client c ∈ S in parallel do
2

Ec = max(⌊(Tk − ˜tcom,c)/˜tcmp,c⌋, 1) ;
αc = min(Tk/(˜tcom,c + ˜tcmp,c), 1) ;
trpt,c = Tk − ˜tcom,c × αc ;

3

4
5 end

Output: ⟨E⟩, ⟨α⟩, ⟨trpt⟩

4. Experiment

4.1. Experimental Settings

Datasets, Models, and Tasks.

To demonstrate
TimelyFL’s effectiveness across tasks, we evaluate
TimelyFL on three benchmark datasets from various cat-
egories of FL applications:

Table 1. Wall clock training time to reach target validation accuracy on benchmark datasets (lower is better). “> 200 hr” indicates the
target accuracy was not reached.

Dataset

Agg. function

Accuracy/Loss

TimelyFL

FedBuff

SyncFL

CIFAR-10

Google Speech

Reddit

FedAvg

FedOpt

FedAvg

FedOpt

FedAvg

FedOpt

60%
70%

60%
70%

70%
80%

70%
80%

7.0 (ppl)
6.8 (ppl)

7.0 (ppl)
6.8 (ppl)

5.50 ±2.5% hr
12.81 ±1.8% hr

7.86 ±2.1% hr (1.43×)
> 200

76.81 ±2.4% hr (13.96×)
150.98 ±1.7% hr (11.78×)

3.58 ±2.5% hr
6.46 ±1.8% hr

5.68 ±2.6% hr (1.59×)
18.73 ±2.3% hr (2.89×)

34.87 ±2.3% hr (9.74×)
58.84 ±0.8% hr (9.11×)

22.90 ±2.1% hr
40.54 ±1.2% hr

42.71 ±2.3% hr (1.87×)
70.60 ±2.0% hr (1.74×)

103.07 ±2.1% hr (4.50×)
187.93 ±1.2% hr (4.64×)

18.08 ±1.1% hr
31.39 ±0.9% hr

30.60 ±1.7% hr (1.69×)
53.36 ±0.9% hr (1.70×)

66.13 ±1.2% hr (3.66×)
107.38 ±0.7% hr (3.42×)

9.56 ±3.1% hr
17.99 ±0.7% hr

15.82 ±2.9% hr (1.65×)
> 200

23.36 ±1.5% hr (2.44×)
67.32 ±0.5% hr (3.74×)

10.99 ±2.7% hr
12.86 ±0.6% hr

14.09 ±2.8% hr (1.28×)
> 200

27.25 ±2.1% hr (2.48×)
57.65 ±0.4% hr (4.48×)

1. Image Classification. The CIFAR-10 dataset [13]
consists of 60,000 colour images in 10 classes. There
are 50,000 training images and 10,000 test images. To
follow the realistic non-iid data in FL scenarios, we
partition both datasets into 128 clusters using a Dirich-
let distribution with α equals 0.1. We evaluate the
dataset with ResNet-20 [7] model.

2. Speech Recognition. The Google Command speech
dataset [29] covers 105,829 audio recordings collected
from 2,618 clients. The training set includes record-
ings from 2,112 speakers, the validation set includes
256 speakers, and the test set includes 250 speak-
ers. The data set is composed of 35 common words
from the everyday vocabulary, such as ”Yes”, ”No”,
”Up”, and ”Down”. We evaluate the dataset with
VGG11 [26] model for a 35-class keyword spotting
task. We also evaluate the dataset with a lightweight
model based on one related work [33], and the de-
tailed data-preprocessing methods are presented in Ap-
pendix A.1.2.

3. Natural Language Processing. Reddit [1] consists of
comments from 1,660,820 users in the Reddit forum.
In this dataset, we filter the users with less than 20
words in total and restrict to the 30k most frequently
used words, as the same settings in the previous work
[14]. Then, we train the lightweight Albert [16] model
for the next-word-prediction task. The performance is
evaluated by the perplexity loss (ppl), which lower is
better.

Experiment Setup. We use the FedML platform [6,35],
an open-source framework for FL, to execute our frame-
work. On the CPU/GPU training side, to approach the real-
world heterogeneous client system performance in emula-
tion, we acquire the local computation times of deep learn-

ing models across hundreds of device types from the AI
benchmark [10] and communication times from Network
Measurements on mobiles [8]. These data will be assigned
to the simulated devices we create in the experiment, the
same as the settings in previous FL works [14, 15, 18]. The
distribution of heterogeneous system utility across simu-
lated clients will be shown in the Appendix A.1.2.

Evaluation Metrics and Baselines. We compare
TimelyFL with FedBuff [9, 22] as the AsyncFL base-
To demonstrate applicability of TimelyFL, we
line.
present the evaluation results using two aggregation func-
tion, FedAvg [20] and FedOpt [23]. We evaluated the per-
formance of TimelyFL and its baseline using the follow-
ing three metrics: test accuracy/loss, time-to-accuracy, and
participation rate. The participation rate is defined as the
total number of rounds that the device contributes to the
global update divided by the total communication round
number. The rate is distributed in the interval between 0
and 1, which implies how often a client participates in the
global model update.

Hyperparameter Settings. We searched for the client
learning rate in a range from 10−6 to 100, server learn-
ing rate in a range from 10−4 to 100, input batch size in
a range from 8 to 256, and total training round in a range
from 1000 to 10000. The aggregation goal and aggregation
participation target is searched from 30% to 50% of train-
ing concurrency per round for FedBuff and TimelyFL,
respectively. We list the detailed hyperparameter selection
for each experiment setup in the Appendix A.1.3.

4.2. End-to-End Performance

We begin by comparing the end-to-end performance
of TimelyFL on benchmark datasets, conducting on the
CPU/GPU-based training. The training concurrency is set
to 128 for CIFAR-10 related experiments, 20 for Google

(a) CIFAR-10 with FedOpt

(b) Google Command with FedOpt

(c) Reddit with FedOpt

Figure 4. Time-to-accuracy performance for SyncFL, FedBuff and TimelyFL.

FedBuff needs 1.28 - 2.89× in terms of wall clock time.
Besides of the time-to-accuracy speedup, TimelyFL also
harvests test accuracy increment compared to FedBuff
within the same communication rounds. As the learn-
ing curve in the Figure 4, TimelyFL achieves 3.27%
and 4.01% higher final accuracy on CIFAR-10 and Google
Command, and 0.43 lower ppl on Reddit in comparison
to FedBuff with FedOpt. Under FedAvg, TimelyFL
achieves 4.93% and 6.25% higher final accuracy on CIFAR-
10 and Google Command, respectively, and 0.20 lower ppl
on Reddit compared to FedBuff.

4.3. Understanding the Advantages of TimelyFL

TimelyFL improves inclusiveness1.

In Table 1, we
view the SyncFL as the standard baseline that does not in-
clude any asynchronous technique and FedBuff as the
baseline that only introduces a fixed buffer size to accel-
erate the training. Instead of fixed buffer size, TimelyFL
adopts a flexible buffer size controlled by aggregation inter-
val time, which allows more available clients to participate
in the global update per round. As illustrated in Figure 5,
66.4% of devices are able to achieve an increased partici-
pation rate, and the average participation rate per client in-
creases by 21% in TimelyFL compared to FedBuff un-
der the CIFAR-10 experiment setting we implemented in
the last section. The average participation rate increment is
the main reason for the time-to-accuracy speed-up. As each
client joins the global model update more rapidly, the learn-
In addi-
ing efficiency increases during the FL training.
tion, combined with more devices contributing to the global
model more frequently, TimelyFL improves inclusiveness
during the model training compared to FedBuff.

The contribution of inclusiveness for model perfor-
mance is especially significant when training on the non-
iid dataset, where each client brings a unique local up-
date to the global model. To demonstrate our point, we
test both TimelyFL and Fedbuff with FedAvg as an

1In this paper, the inclusiveness increment represents the participation

rate increment in the FL training.

(a) Participation rate for each client

(b) Participation rate distribution

Figure 5. Participation rate evaluation.

speech related experiments, and 100 for Reddit related ex-
periments. The communication round is set to be 2000,
1000, and 500 for CIFAR-10, Google speech, and Red-
dit, respectively. For both FedBuff and TimelyFL, we
set the aggregation goal and aggregation participation tar-
get equal to 50% of training concurrency for a fair com-
parison. We run each experiment five times with different
random seeds and report its mean and standard deviation for
the time consumption in the Table 1.

Speedup of TimelyFL. Given the same heterogeneous
data, TimelyFL achieves the shortest training time to
reach all target accuracy/loss. Table 1 shows the train-
ing time needed to converge to the target accuracy/loss
for each strategy considered. Compared to TimelyFL,
synchronous FL requires 2.44 - 13.96× more times, and

Table 2. Wall clock training time to reach target validation accuracy on benchmark datasets (lower is better).

Dataset

Agg. function

Accuracy

TimelyFL

FedBuff

SyncFL

Google Speech

FedAvg

FedOpt

70%
80%

70%
80%

2.23 ±2.1% hr
4.16 ±1.3% hr

3.55 ±1.9% hr (1.59×)
6.13 ±1.4% hr (1.47×)

18.37 ±0.6% hr (8.24×)
32.46 ±0.4% hr (7.80×)

0.48 ±1.7% hr
1.13 ±1.2% hr

1.66 ±1.0% hr (3.46×)
3.25 ±0.8% hr (2.88×)

4.61 ±2.1% hr (9.60×)
7.47 ±1.1% hr (6.61×)

tial training ratio and local epoch number, should be adap-
tively scheduled based on the real-time capability of each
device. To demonstrate our strategy, we test the training un-
der the TimelyFL framework both with and without adap-
tive workload scheduling on the CIFAR-10 dataset, with
the training concurrency equals to 64. Figure 7 shows the
learning curves for both scenarios. With adaptive workload
scheduling, TimelyFL saves 4.09× convergence time to
50% accuracy and 10.89% test accuracy increment, which
illustrates that real-time workload scheduling essentially
improves both learning efficiency and accuracy.

TimelyFL is effective on the lightweight model. To
investigate the effectiveness of the lightweight model on
the TimelyFL framework, we implement one lightweight
model on the Google Speech Commands dataset for the
keyword spotting task. Following one previous work [33],
we choose the model that consists of two convolution lay-
ers followed by one Gated Recurrent Units (GRU) layer.
An average pooling layer is connected to the GRU output,
which is then fed through two dense layers to generate the
predictions. The parameter size of this model is equal to
79044. We adopt the same baseline selections as in Sec-
tion 4. The hyperparameters for the experiments are listed
in Section A.1.3. The experiment results are summarized
in Table 2. TimelyFL achieves a higher convergence speed
compared with the other two strategies before reaching the
test accuracy, which confirms the simulation results elabo-
rated in Section 4.2 and demonstrates the effectiveness of
the TimelyFL on the lightweight model architecture.

Figure 6. Time-to-accuracy performance under different non-iid
distribution.

Figure 7. TimelyFL performance under adaptive and non-
adaptive workload schedule.

aggregator on the CIFAR-10 dataset using a non-iid parti-
tion. As shown in Figure 6, as the parameter for Dirich-
let distribution goes up,
the convergence-time gap be-
tween TimelyFL and Fedbuff increases as well, which
demonstrates our advantage for non-iid data training com-
pared to Fedbuff.

TimelyFL is heterogeneity-aware. Under cross-
device federated learning, most participating entities have
limited computing capability and intermittent connectivity.
As such, it could not be guaranteed that devices would
complete their training workload in every communication
round as assigned initially. To effectively resist the dis-
turbance, the training hyperparameters, such as the par-

5. Conclusion

In this work, we propose TimelyFL, a heterogeneity-
aware asynchronous FL scheme with adaptive partial train-
ing. To include more available devices joining in global ag-
gregation in a timely manner, TimelyFL introduces par-
tial model training to the slow-speed devices. Moreover,
TimelyFL is resilient to system heterogeneity by adjusting
the local training workload based on the real-time resource
capabilities of each client during FL training. Our exper-
imental results demonstrate that TimelyFL could outper-
form major AsyncFL proposals in terms of both time-to-
accuracy and test accuracy.

References

[1] Reddit Comment Data. https://files.pushshift.

io/reddit/comments/. 2, 6, 11

[2] Samiul Alam, Luyang Liu, Ming Yan, and Mi Zhang.
FedRolex: Model-Heterogeneous Federated Learning with
Rolling Sub-Model Extraction. In Conference on Neural In-
formation Processing Systems (NeurIPS), 2022. 3

[3] Dmitrii Avdiukhin and Shiva Prasad Kasiviswanathan. Fed-
erated learning under arbitrary communication patterns. In
ICML, 2021. 1

[4] Wei Dai, Yi Zhou, Nanqing Dong, H. Zhang, and Eric P.
Xing. Toward understanding the impact of staleness in dis-
tributed machine learning. ArXiv, abs/1810.03264, 2019. 3
[5] Niv Giladi, Mor Shpigel Nacson, Elad Hoffer, and Daniel
Soudry. At stability’s edge: How to adjust hyperparame-
ters to preserve minima selection in asynchronous training
of neural networks? ArXiv, abs/1909.12340, 2020. 3

[6] Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi
Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek
Singh, Han Qiu, Li Shen, Peilin Zhao, Yan Kang, Yang
Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram,
Fedml: A research library
and Salman Avestimehr.
ArXiv,
and benchmark for federated machine learning.
abs/2007.13518, 2020. 6

[7] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep
residual learning for image recognition. 2016 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 770–778, 2016. 6, 11

[8] Junxian Huang, Cheng Chen, Yutong Pei, Zhaoguang Wang,
Zhiyun Qian, Feng Qian, Birjodh Tiwana, Qiang Xu, Z Mao,
Ming Zhang, et al. Mobiperf: Mobile network measurement
system. Technical Report. University of Michigan and Mi-
crosoft Research, 2011. 6

[9] Dzmitry Huba, John Nguyen, Kshitiz Malik, Ruiyu Zhu,
Michael G. Rabbat, Ashkan Yousefpour, Carole-Jean Wu,
Hongyuan Zhan, Pavel Ustinov, Harish Srinivas, Kaikai
Wang, Anthony Shoumikhin, Jesik Min, and Mani Malek.
Papaya: Practical, private, and scalable federated learning.
ArXiv, abs/2111.04877, 2021. 1, 3, 6

[10] Andrey D. Ignatov, Radu Timofte, Andrei Kulik, Seungsoo
Yang, Ke Wang, Felix Baum, Max Wu, Lirong Xu, and
Luc Van Gool. Ai benchmark: All about deep learning on
smartphones in 2019. 2019 IEEE/CVF International Confer-
ence on Computer Vision Workshop (ICCVW), pages 3617–
3635, 2019. 6, 11

[11] Xiaotang Jiang, Huan Wang, Yiliu Chen, Ziqi Wu, Lichuan
Wang, Bin Zou, Yafeng Yang, Zongyang Cui, Yuezhi Cai,
Tianhang Yu, Chengfei Lv, and Zhihua Wu. Mnn: A univer-
sal and efficient inference engine. ArXiv, abs/2002.12418,
2020. 12

[12] Peter Kairouz, H Brendan McMahan, Brendan Avent,
Aur´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista
Bonawitz, Zachary Charles, Graham Cormode, Rachel Cum-
mings, et al. Advances and open problems in federated learn-
ing. Foundations and Trends® in Machine Learning, 14(1–
2):1–210, 2021. 1

[13] Alex Krizhevsky. Learning multiple layers of features from

tiny images. 2009. 2, 6, 11

[14] Fan Lai, Yinwei Dai, Xiangfeng Zhu, and Mosharaf Chowd-
hury. Fedscale: Benchmarking model and system perfor-
mance of federated learning. Proceedings of the First Work-
shop on Systems Challenges in Reliable and Secure Feder-
ated Learning, 2021. 5, 6, 11

[15] Fan Lai, Xiangfeng Zhu, Harsha V. Madhyastha, and
Mosharaf Chowdhury. Oort: Efficient federated learning via
guided participant selection. In OSDI, 2021. 6

[16] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin
Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite
bert for self-supervised learning of language representations.
ArXiv, abs/1909.11942, 2020. 6, 11

[17] Sunwoo Lee, Tuo Zhang, Chaoyang He, and Salman Aves-
timehr. Layer-wise adaptive model aggregation for scalable
federated learning. ArXiv, abs/2110.10302, 2021. 3

[18] Chenning Li, Xiao Zeng, Mi Zhang, and Zhichao Cao. Pyra-
midFL: A Fine-grained Client Selection Framework for Effi-
cient Federated Learning. In ACM International Conference
on Mobile Computing and Networking (MobiCom), 2022. 6
[19] Xingyu Li, Zhe Qu, Bo Tang, and Zhuo Lu. Stragglers are
not disaster: A hybrid federated learning algorithm with de-
layed gradients. ArXiv, abs/2102.06329, 2021. 3

[20] H. B. McMahan, Eider Moore, Daniel Ramage, Seth Hamp-
son, and Blaise Ag¨uera y Arcas. Communication-efficient
learning of deep networks from decentralized data. In AIS-
TATS, 2017. 2, 6

[21] Muhammad Tahir Munir, Muhammad Mustansar Saeed, Ma-
had Farah Ali, Zafar Ayyub Qazi, and Ihsan Ayyub Qazi.
Fedprune: Towards inclusive federated learning. ArXiv,
abs/2110.14205, 2021. 3

[22] John Nguyen, Kshitiz Malik, Hongyuan Zhan, Ashkan
Yousefpour, Michael G. Rabbat, Mani Malek, and Dzmitry
Huba. Federated learning with buffered asynchronous aggre-
gation. ArXiv, abs/2106.06639, 2021. 1, 2, 3, 6, 11

[23] Sashank J. Reddi, Zachary B. Charles, Manzil Zaheer,
Zachary Garrett, Keith Rush, Jakub Konecn´y, Sanjiv Ku-
mar, and H. B. McMahan. Adaptive federated optimization.
ArXiv, abs/2003.00295, 2021. 2, 6

[24] Jae Hun Ro, Theresa Breiner, Lara McConnaughey,
Mingqing Chen, Ananda Theertha Suresh, Shankar Kumar,
and Rajiv Mathews. Scaling language model size in cross-
device federated learning. ArXiv, abs/2204.09715, 2022. 3

[25] Hakim Sidahmed, Zheng Xu, Ankush Garg, Yuan Cao, and
Mingqing Chen. Efficient and private federated learning with
partially trainable networks. ArXiv, abs/2110.03450, 2021. 3
[26] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2015. 6, 11

[27] Saeed Vahidian, Sreevatsank Kadaveru, Woo-Ram Baek,
Weijia Wang, Vyacheslav Kungurtsev, Chen Chen, Mubarak
Shah, and Bill Lin. When do curricula work in federated
learning? ArXiv, abs/2212.12712, 2022. 1

[28] Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi,
H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew,
Salman Avestimehr, Katharine Daly, Deepesh Data, et al.

A field guide to federated optimization.
arXiv:2107.06917, 2021. 1

arXiv preprint

[29] Pete Warden. Speech commands: A dataset for limited-
ArXiv, abs/1804.03209,

vocabulary speech recognition.
2018. 2, 6, 11

[30] Wentai Wu, Ligang He, Weiwei Lin, Rui Mao, Carsten
Maple, and Stephen A. Jarvis. Safa: A semi-asynchronous
protocol for fast federated learning with low overhead. IEEE
Transactions on Computers, 70:655–668, 2021. 1, 3
[31] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Asyn-
chronous federated optimization. ArXiv, abs/1903.03934,
2019. 3

[32] Tien-Ju Yang, Dhruv Guliani, Franccoise Beaufays, and Gio-
vanni Motta. Partial variable training for efficient on-device
federated learning. ArXiv, abs/2110.05607, 2021. 3

[33] Tuo Zhang, Tiantian Feng, Samiul Alam, Sunwoo Lee, Mi
Zhang, Shrikanth S. Narayanan, and Salman Avestimehr.
Fedaudio: A federated learning benchmark for audio tasks.
ArXiv, abs/2210.15707, 2022. 6, 8, 11

[34] Tuo Zhang, Lei Gao, Chaoyang He, Mi Zhang, Bhaskar Kr-
ishnamachari, and Salman Avestimehr. Federated learning
for internet of things: Applications, challenges, and oppor-
IEEE Internet of Things Magazine (IEEE IoTM),
tunities.
2022. 3

[35] Tuo Zhang, Chaoyang He, Tian-Shya Ma, Mark Ma, and
Salman Avestimehr. Federated learning for internet of things.
Proceedings of the 19th ACM Conference on Embedded Net-
worked Sensor Systems, 2021. 6

[36] Zhengyuan Zhou, P. Mertikopoulos, Nicholas Bambos, Pe-
ter W. Glynn, Yinyu Ye, Li-Jia Li, and Li Fei-Fei. Dis-
tributed asynchronous optimization with unbounded delays:
How slow can you go? In ICML, 2018. 3

A. Appendix

A.1. Experiment Settings

A.1.1 Computing Infrastructure

The simulation experiments are conducted on a computing
server with one GPU. The server is equipped with AMD
EPYC 7502 32-Core Processor and 1024G memory. The
GPU is NVIDIA RTX A4000.

A.1.2 Datasets and Models

AI Benchmark. AI Benchmark [10] is a public dataset
that is designed for evaluating the performance of impor-
tant AI tasks on mobile devices. AI Benchmark provides
diverse models’ training and inference speed across vari-
ous devices, including chipsets from Qualcomm, HiSilicon,
Samsung, MediaTek, and Unisoc. Figure 8a illustrates the
distribution of the computation efficiency across clients in
the AI Benchmark. The slowest device would take around
13.3× computational times than the fastest device for the
same task. To approach the dynamic availability of devices,
such as low-power mode or multi-process running, we de-
sign a coefficient w as follows:

x ∼ N (1, 0.3)



1
x
1.3 x ≥ 1.3

x ≤ 1
1 ≤ x ≤ 1.3



w =

(2)

In this work, we assign the values from AI Benchmark as
base computation time to the clients to emulate real devices,
analogous to the usage in FedScale [14]. We also generate
the coefficient w every round for each client to simulate the
natural disturbance to availability. The local computation
time in each round equals the product of w and the base
computation time for each client.

MobiPerf. MobiPerf is a public dataset for measuring
network performance on mobile devices, which collects the
available cloud-to-edge network throughput of over 100k
worldwide mobile clients. Figure 8b illustrates the distribu-
tion of communication consumption of MobiPerf. Note that
the best communication channel can be 200× better than the
worst one. We randomly assign a value from MobiPerf to
a simulated device every communication round to emulate
intermittent connectivity in a real deployment.

CIFAR-10. The CIFAR-10 dataset [13] consists of
60,000 32x32 colour images in 10 classes. There are 50,000
training images and 10,000 test images. We normalize the
images by the mean and standard deviation of the dataset.
We evaluate the dataset with ResNet-20 [7] model. To
emulate the realistic non-iid distribution, we partition the
dataset using a Dirichlet distribution, following the previ-
ous works [22].

Google Command. The Google Command speech
dataset [29] covers 105,829 audio recordings collected from
2,618 clients. The training set includes recordings from
2,112D speakers, the validation set includes 256 speakers,
and the test set includes 250 speakers. The data set is com-
posed of 35 common words from the everyday vocabulary,
such as ”Yes”, ”No”, ”Up”, and ”Down”. We evaluate the
dataset with VGG11 [26] model and a lightweight model
based on one related work [33] for a 35-class keyword spot-
ting task.

For the VGG11-based experiment on Google Speech
Commands, we use the Mel-frequency cepstral coefficients
(MFCC) method to pre-process the raw audio data. Specif-
ically, a sequence of overlapping Hamming windows is ap-
plied to the raw speech signal with a time shift of 10 ms and
window size of 25ms. The MFCC is used for training the
keyword spotting model.

For the lightweight model experiment, to pre-process
the raw audio data, a sequence of overlapping Hamming
windows is applied to the raw speech signal with a time
shift of 10 ms. We calculate the discrete Fourier trans-
form (DFT) with a frame length of 1,024 and compute
the Mel-spectrogram with a dimension of 128. The Mel-
spectrogram is used for training the keyword spotting
model. We follow [33] for this setup.

Reddit. Reddit [1] consists of comments from 1,660,820
users in the Reddit forum. Each client corresponds to a user,
whose data are all of their personal posts. Thus it follows
the real non-iid data under FL scenarios. In this dataset, we
filter the users with less than 20 words in total and restrict
to the 30k most frequently used words, as the same settings
in the previous work [14]. Then, we train the lightweight
Albert [16] model for the next-word-prediction task. The
performance is evaluated by the perplexity loss (ppl), which
lower is better.

A.1.3 Hyperparameter Settings

We searched for the client learning rate in a range from
10−6 to 100, server learning rate in a range from 10−4 to
100, input batch size in a range from 8 to 256, and total
training round in a range from 1000 to 10000. The aggre-
gation goal and aggregation participation target is searched
from 30% to 50% of training concurrency per round for
FedBuff and TimelyFL, respectively.

After hyper-parameter searching, we fixed the follow-
ing hyperparameters: for CIFAR-10 related experiments,
the total training round is 2000, and training concurrency
is 128 for all setups. The aggregation goal and aggregation
participation target is 50% of the training concurrency for
both FedBuff and TimelyFL. For CIFAR-10 with Fe-
dAvg related experiments, the batch size is 8, and the client
learning rate is 0.8. For CIFAR-10 with FedOpt related ex-

Figure 9. Partial training system performance in on real edge de-
vices.

A.2. System Performance

A.2.1 Partial Training Performance

Due to different parameters and tensor shapes among dif-
ferent layers, the training time (computational time of the
forward and backward propagation) is not strictly linear to
the trainable layer numbers and varies with the model struc-
tures. For simplicity and generality, we define the training
time of the partial model as the linear multiplication of the
training time of the full model and the training ratio. This
linear relationship is verified through our real measurement
on a Samsung Galaxy S20 with ResNet-20 model using
MNN [11] library. As shown in Figure 9, most of the test
results are below the linear straight line (except the ratio is
below 0.2), justifying the rationality of our choice.

(a) Diverse computation efficiency in AI Benchmark

(b) Diverse communicate efficiency in Mobiperf

Figure 8. Heterogeneous system utility across simulated clients.

periments, the batch size is 10, the client learning rate is
0.03, and the server learning rate is 0.001 with ADAM as
server optimizer.

For Google command related experiments with VGG11
model, the total training round is 1000, and training con-
currency is 20 for all setups. The aggregation goal and ag-
gregation participation target is 50% of the training concur-
rency for both FedBuff and TimelyFL. The batch size
is 32, and the client learning rate is 0.01. Under the FedOpt,
the server learning rate is 0.001 with ADAM as server opti-
mizer.

For Google command related experiments with the
lightweight model, the total training round is 5000, and
training concurrency is 106 for all setups. The aggrega-
tion goal and aggregation participation target is 50% of the
training concurrency for both FedBuff and TimelyFL.
The batch size is 16, and the client learning rate is 0.1 under
the FedAvg. Under the FedOpt, the client learning rate is
0.05 for synchrounous FL and TimelyFL, and the client
learning rate is 0.2 for FedBuff. The server learning rate
is 0.001 with ADAM as server optimizer for all setups.

Finally, for Reddit related experiments, the total training
round is 500, and training concurrency is 20 for all setups.
The aggregation goal and aggregation participation target
is 50% of the training concurrency for both FedBuff and
TimelyFL. The batch size is 20, and the client learning
rate is 0.0005 for SyncFL and TimelyFL, and 0.0003 for
FedBuff. Under the FedOpt, the server learning rate is
0.001 with ADAM as server optimizer.

","3 2 0 2 r p A 4 1 ] G L . s c [ 1 v 7 4 9 6 0 . 4 0 3 2 : v i X r a TimelyFL : Heterogeneity-aware Asynchronous Federated Learning with Adaptive Partial Training Tuo Zhang1 , Lei Gao1 , Sunwoo Lee2 , Mi Zhang3 , Salman Avestimehr1 1University of Southern California 2Inha University 3The Ohio State University tuozhang @ usc.edu Abstract In cross-device Federated Learning ( FL ) environments , scaling synchronous FL methods is challenging as strag- glers hinder the training process . Moreover , the availabil- ity of each client to join the training is highly variable over time due to system heterogeneities and intermittent connec- tivity . Recent asynchronous FL methods ( e.g. , FedBuff [ 22 ] ) have been proposed to overcome these issues by al- lowing slower users to continue their work on local train- ing based on stale models and to contribute to aggregation when ready . However , we show empirically that this method can lead to a substantial drop in training accuracy as well as a slower convergence rate . The primary reason is that fast-speed devices contribute to many more rounds of ag- gregation while others join more intermittently or not at all , and with stale model updates . To overcome this barrier , we propose TimelyFL , a heterogeneity-aware asynchronous FL framework with adaptive partial training . During the training , TimelyFL adjusts the local training workload based on the real-time resource capabilities of each client , aiming to allow more available clients to join in the global update without staleness . We demonstrate the performance benefits of TimelyFL by conducting extensive experiments on various datasets ( e.g. , CIFAR-10 , Google Speech , and Reddit ) and models ( e.g. , ResNet20 , VGG11 , and ALBERT ) . In comparison with the state-of-the-art ( i.e. , FedBuff ) , our evaluations reveal that TimelyFL improves participa- tion rate by 21.13 % , harvests 1.28× - 2.89× more efficiency on convergence rate , and provides a 6.25 % increment on test accuracy . 1 . Introduction Federated learning ( FL ) has emerged as a promising distributed machine learning paradigm that preserves pri- vacy [ 12 , 28 ] . The gist of FL is to keep the clients ’ pri- vate data on the devices and perform local model training for each client . A central server will collect these locally trained models to update a global model and then push it back for the next round of training . Most existing FL protocols are based on synchronous FL training ( SyncFL ) , meaning that at each round all clients ( or a selected cohort of clients ) are updating their local mod- els based on the latest update broadcast by the server at the beginning of that round . Due to the unbalanced commu- nication or hardware capabilities and non-identical training data distribution , however , the time consumption for a lo- cal update can vary substantially from device to device , and some clients may even be temporarily disconnected dur- ing the training process [ 27 ] . Thus , leaving the server with two suboptimal choices : to wait for all clients participating in each round to finish their local training and contribute to model aggregation ( which will cause significant delays due to stragglers ) , or to only wait for a subset of the faster clients ( which will ignore all the work and contributions from slower clients ) . These critical challenges largely im- pede the scalability of SyncFL and make it difficult to land in large-scale cross-device scenarios . To address those challenges , recent works have proposed asynchronous federated learning ( AsyncFL ) [ 3 , 9 , 22 , 30 ] , which allows slower clients to continue local training and contribute to future aggregation rounds . AsyncFL de- couples client local training from global model aggrega- tion/updates , as only certain clients would simultaneously get an update from the cloud server , which decreases the impact of stragglers . The most recent AsyncFL work – FedBuff [ 9 , 22 ] – proposes that the server should perform a gradient aggregation to create a global model once the number of received local updates reaches a requisite thresh- old , which is a tunable parameter referred as aggregation goal . The slower clients can still upload their updates later as long as they finish local training , but their updates may not be included based on staleness information . ( a ) Participation rate distribution across all de- vices ( b ) Number of participated clients during FL training rounds ( c ) Time-to-accuracy performance for each strat- egy Figure 1 . Empirical performance of SyncFL , FedBuff , and TimelyFL in CIFAR-10 classification task with FedOpt as server aggregator ( for experiment details and other evaluations see Section 4 ) . TimelyFL includes more devices join in global update during the training ( shown in ( a ) and ( b ) ) . As more devices participate timely , TimelyFL harvests both convergence rate and accuracy boost compared to FedBuff ( shown in ( c ) ) . As highlighted in Figure 1c , we empirically demonstrate that while FedBuff achieves much faster convergence to a certain intermediate accuracy , it can , unfortunately , lead to a substantial drop in final accuracy compared to SyncFL . The intuitive explanation is that , as FedBuff only accepts a fixed number of local updates to contribute to the global model in every communication round , it decreases the par- allel computing efficiency by blocking other completed lo- cal updates into global aggregation , which turns them into stale updates as they would be postponed to the next round of global update . Moreover , the server aggregator prefers the fast-speed devices contributing more rounds of train- ing , whereas low-speed devices may not enjoy the same frequency of contribution . Even when the slow devices par- ticipate in global training , they occasionally send the staled updates that potentially harm the convergence of the global loss . As shown in Figure 1a and 1b , compared to SyncFL , FedBuff only includes a fixed number of local updates per round , and achieves a low participation rate ( i.e. , the num- ber of aggregation participated divided by the total number of aggregation rounds ) on average with a biased distribu- tion , indicating that the inclusiveness of a group has been diminished , which is the root cause of the test accuracy gap . To close the gap between SyncFL and AsyncFL , we pro- pose TimelyFL , a heterogeneity-aware asynchronous fed- erated learning framework based on adaptive partial train- ing . One key distinction of TimelyFL from previous AsyncFL works is that TimelyFL does not set a fixed number limit to the number of local updates for the global Instead , to accommodate a flexi- aggregation per round . ble number of clients joining in the global update , we set a wall-clock time limit for every aggregation interval . The aggregation interval equals the kth fastest local update time among all clients , where k is a tunable parameter . As long as the device can deliver its model update to the server within this interval , it will be part of the global aggrega- tion . To include more available devices to join in global aggregation without staleness , we introduce partial model training for clients with low capacity . Instead of fully train- ing a model , only a part of the model that composes of a subset of consecutive output-side layers will be assigned to them for backward pass training . With partial model train- ing , both local computation time and communication time will decrease for stale clients . As shown in Figure 2 , TimelyFL unifies the local training time by adaptively adjusting the workload ( i.e. , the local epoch number and partial training ratio ) for each client , making it feasible for clients to finish the local train- ing and upload the updates to the server within the calcu- lated aggregation interval in every communication round . As such , TimelyFL tackles the system heterogeneity is- sue and eliminates the staleness of local update reports for slower devices . We evaluate the performance of TimelyFL across vari- ous application tasks , including image classification , speech recognition , and natural language processing on CIFAR- 10 [ 13 ] , Google Speech Command [ 29 ] , and Reddit Com- ment [ 1 ] benchmark datasets , respectively , with two com- monly used aggregation functions , FedAvg [ 20 ] and Fe- dOpt [ 23 ] . Our results show that 66.4 % of devices increase the participation rate and the average participation rate in- creases by 21.1 % in TimelyFL compared to FedBuff . Under the same scale of the FL system , TimelyFL outper- forms FedBuff [ 22 ] on both time-to-accuracy and final test accuracy , as shown in Figure 1c . 2 . Related Work Asynchronous Federated Learning . Due to inter- mittent connectivity and availability among clients , asyn- chronous FL is a promising solution to tackle device het- Figure 2 . FedBuff ( top ) : Server updates the global model as it receives the requisite number of local updates , and slower devices still could send their updates at a later time to the server . Fast devices participate more times in the global update , while slow devices contribute less or no participation . TimelyFL ( bottom ) : Server updates the individual workload every round based on the real-time availability of each client to include more devices in global update timely , largely increases the participation rate for slow devices . erogeneity in FL [ 34 ] . Most asynchronous FL works con- centrate on solving the straggler problem , such as [ 19 ] , [ 30 ] , and [ 31 ] . PAPAYA [ 9 ] and FedBuff [ 22 ] have been pro- posed to mitigate stragglers and enable secure aggregation jointly . Specifically , the individual updates are not incorpo- rated by the server as soon they arrive . Instead , the server will keep receiving local updates in a secure buffer , where the buffer size is a tunable parameter , and update the global model whenever the buffer is full . The slow devices can also send the local update to the server after the global aggrega- tion . Their update will be considered for the next available global update . However , practically speaking , fast devices participate in global updates many more times than slow devices , and some slow devices can not join in the global aggregation even once due to the staleness control . All of the above approaches assume that the client should process the local training within the full-model size . As the slower users participate in the global aggregation , they can only contribute with stale updates . Some previous works have pointed out that the effects of the stale update on distributed machine learning can directly harm the over- all convergence behavior for the final model , aligned with the asynchronous distributed optimization theory suggested by [ 4 , 5 , 36 ] . Moreover , the participation rate is mainly un- balanced due to the high-speed devices contributing more rounds to global updates compared to the slow-speed de- vices . In contrast to previous approaches , we focus on en- abling all clients to join in the global aggregation effectively based on their local resources to improve the inclusiveness of the final global model training . Partial Model Training . Partial model training can be viewed as an efficient approach to reduce both communica- tion and computation workload on the client-side of the FL system [ 2 ] . FedPrune [ 21 ] proposes a method that prunes the global model for each client based on their device capa- bilities , where slow clients are served smaller models and faster clients train on larger models . FedPT [ 25 ] lever- ages the partially trainable neural networks on clients to re- duce communication costs and enable faster training with a smaller memory footprint and with few implications on model performance . Other works such as [ 17 , 24 , 32 ] also address that partial model training can save both communi- cation cost and memory usage in cross-device FL . All of the above works maintain the partial ratio for the sub-model of a certain client as constant during the entire FL training pro- cess , which neglects that the availability of each device is not stable throughout the time . In this work , we adaptively adjust the partial ratio for the local model training based on the real-time device availability , which aims to improve both efficiency and utility for each client . 3 . Our Method 3.1 . Standard Asynchronous Federated Learning Figure 3 ( left ) illustrates the standard asynchronous FL framework . Instead of waiting for all clients to finish the local model training , the server stores the individual up- dates from clients in a buffer and then adjusts the global model once the buffer size reaches the requisite number of the aggregation goal . Other non-participating devices will postpone their contribution to global updates in the latter communication round once they finish the training . Given that the standard AsyncFL framework suffers from inclu- siveness constraints described in the introduction section , Figure 3 . Left : The standard AsyncFL framework . The server will initiate the global update once it collects the requisite number of local updates . The other clients will be postponed to a latter communication round with stale update information . Right : The proposed TimelyFL . The server will include all the received local updates within aggregation interval to global update . Clients with a weaker capacity are assigned to train a subset of the model to catch the aggregation interval time . we propose an efficient AsyncFL framework as shown in Figure 3 ( right ) to address this issue . 3.2 . TimelyFL Design 3.2.1 Preliminaries To increase the participation rate for the slow clients , we aim to design a cross-device asynchronous federated learn- ing framework where each device can finish the local up- date within a limited time interval by adaptively adjusting its workload per round . Here , the workload is defined as the product of local training epoch number E and partial model training ratio α . To formalize this , our time utility function , which considers the local training optimization at the client side , is designed as follows : arg max E , α ( ˜tcmp , c × E × α + ˜tcom , c × α ) ≤ Tk ( 1 ) where ˜tcmp , c is the estimated local computation time , and ˜tcom , c is the estimated local communication time of the client c for one epoch of full model training in a certain communication round calculated by the server . Note that both ˜tcmp , c and ˜tcom , c are not constant throughout the train- ing due to the nature of mobile devices . By adjusting E and α , each device is supposed to participate in the global aggregation every communication round timely and max- imally utilize its resource capability within aggregation in- terval Tk . Therefore , the overall distributed optimization in- volves more iterations on diverse datasets , leading to faster convergence . and impeding them from contributing to the global model . To address this issue , we introduce partial model training to slow devices . Figure 3 ( right ) illustrates our approach when partial model training lands on the device heterogeneity FL system . Instead of a full training model , weak devices will be assigned to train partial models composed of a subset of consecutive output-side layers . During the training , only the subset of assigned layers will operate both forward pass and backward propagation , while the other layers will only pro- cess the forward pass for the input data but be frozen during weight updates . After local training finishes , the partially- trained clients only return the update for the assigned layers to the server for the global aggregation , as the frozen layers are unchanged during training . We target to solve two bottlenecks in the cross-device FL with partial model training , communication and compu- tation efficiency for the stale clients . In edge computing , the backward propagation consumes much more time than the forward pass . Partial model training would essentially re- duce the training time , as it does not need to calculate gradi- ents for the frozen parameters . The reduced time is roughly proportional to the reduced model size , as we empirically shown in the Appendix A.2.1 . Moreover , we only send the trainable part of updates to the server , substantially improv- ing communication efficiency , especially when stragglers with limited network connections exist . By implementing partial model training , we aim to let low-capacity devices report their local updates to the server timely without stal- eness , thereby improving their participation rate during FL training . 3.2.2 Adaptive Model Training Due to resource limitations , some weak devices may not finish the full model training effectively within the time in- terval Tk , making them become stale clients in the system 3.2.3 TimelyFL Algorithm Based on the adaptive model training , we propose the TimelyFL . TimelyFL tries to unify each client ’ s round time to the limited aggregation interval Tk by adaptively adjusting the workload concerning its real-time availability per communication round . The workload is defined as the product of the partial training ratio α and the local epoch number E. TimelyFL framework is composed of three main parts , TimelyFL server , local time update , and work- load scheduling . time tcmp from one data batch full model training . The unit computation time ˜tcmp is estimated by tcmp and progress β , where β is defined as the ratio of trained batch number to the total data batch number . The local communication time equals the model ’ s file size M over the device ’ s real-time network bandwidth Bw , as the same setting in the previous FL system work [ 14 ] . Algorithm 1 : TimelyFL . Input : k : the aggregation participation target , n : the number of training concurrency 1 for r ∈ { 0 , · · · , R − 1 } communication rounds do 2 Global server do : 3 4 5 6 7 8 9 10 11 Sample n clients uniformly at random to define S , and send W r Clients c ∈ S in parallel do : s to clients in S ; ˜ttotal , ˜tcmp , ˜tcom = LocalTimeUpdate ( M ) ; Global server do : k ← the kth smallest number in ⟨˜ttotal⟩ ; T r ⟨Er⟩ , ⟨αr⟩ , ⟨tr rpt⟩ = r , ⟨˜tcmp⟩ , ⟨˜tcom⟩ ) ; WorkloadScheduling ( Tk Clients c ∈ S in parallel do : W r c ← adaptive model training ; Global server do : W r+1 s ← aggregate ⟨W r c ⟩ ; 12 13 end Output : W R s TimelyFL Server . TimelyFL server is in charge of adjusting the aggregation interval Tk , local training epoch E , and partial training ratio α for each device during the FL training , as summarized in Algorithm 1 . The aggre- gation interval Tk in each round equals the kth smallest value among ⟨˜ttotal⟩ , as the estimated unit total time for all clients . At each communication round , TimelyFL server randomly samples n clients to construct the collec- tion S and distributes the global model to the clients inside S , which means n clients would start the local training in this round , same as the definition of training concurrency in the FedBuff . Each selected client would perform one data batch full model training to estimate its time consump- tion and report it to the server . Then , aggregation interval time Tk and training hyperparameters for client c ( i.e. , lo- cal training epoch number E and partial training ratio α ) would be adjusted based on all selected clients ’ status dur- ing the FL training process . The server would also return a local computation budget time trpt , c , as the wall-clock time when the client must report its training status . Local Time Update . To efficiently accommodate the capabilities , each participant needs to update its time con- sumption to the server as summarized in Algorithm 2 . Specifically , each client would collect the real computation Algorithm 2 : Local Time Update . Input : M : the file size of the received global model , Bw : the real-time network bandwidth 1 tcmp , β ← one data batch training ; 2 ˜tcom = M/Bw ; 3 ˜tcmp = tcmp/β ; 4 ˜ttotal = ˜tcmp + ˜tcom ; Output : ˜ttotal , ˜tcom , ˜tcmp Workload Scheduling . TimelyFL would adjust the lo- cal epoch number E and partial training ratio α for each client in every communication round based on the estimated ˜tcom , c , ˜tcmp , c and aggregation interval Tk , as the relation- ship shown in 1 . If one ’ s unit total time is smaller than Tk , then the server would try to maximize its local training utility and minimize the idle time , as to assign more than one local epoch training for the next round . Otherwise , the server would assign less amount of workload to them by de- creasing the model training ratio α , which guarantees they can finish at least one local epoch training within the report time trpt , c and catch up the global aggregation timely . We summarized the scheduler as Algorithm 3 . Algorithm 3 : Workload Scheduling . Input : Tk : aggregation interval time , ⟨˜tcmp⟩ : unit computation time , ⟨˜tcom⟩ : unit communication time 1 for each client c ∈ S in parallel do 2 Ec = max ( ⌊ ( Tk − ˜tcom , c ) /˜tcmp , c⌋ , 1 ) ; αc = min ( Tk/ ( ˜tcom , c + ˜tcmp , c ) , 1 ) ; trpt , c = Tk − ˜tcom , c × αc ; 3 4 5 end Output : ⟨E⟩ , ⟨α⟩ , ⟨trpt⟩ 4 . Experiment 4.1 . Experimental Settings Datasets , Models , and Tasks . To demonstrate TimelyFL ’ s effectiveness across tasks , we evaluate TimelyFL on three benchmark datasets from various cat- egories of FL applications : Table 1 . Wall clock training time to reach target validation accuracy on benchmark datasets ( lower is better ) . “ > 200 hr ” indicates the target accuracy was not reached . Dataset Agg . function Accuracy/Loss TimelyFL FedBuff SyncFL CIFAR-10 Google Speech Reddit FedAvg FedOpt FedAvg FedOpt FedAvg FedOpt 60 % 70 % 60 % 70 % 70 % 80 % 70 % 80 % 7.0 ( ppl ) 6.8 ( ppl ) 7.0 ( ppl ) 6.8 ( ppl ) 5.50 ±2.5 % hr 12.81 ±1.8 % hr 7.86 ±2.1 % hr ( 1.43× ) > 200 76.81 ±2.4 % hr ( 13.96× ) 150.98 ±1.7 % hr ( 11.78× ) 3.58 ±2.5 % hr 6.46 ±1.8 % hr 5.68 ±2.6 % hr ( 1.59× ) 18.73 ±2.3 % hr ( 2.89× ) 34.87 ±2.3 % hr ( 9.74× ) 58.84 ±0.8 % hr ( 9.11× ) 22.90 ±2.1 % hr 40.54 ±1.2 % hr 42.71 ±2.3 % hr ( 1.87× ) 70.60 ±2.0 % hr ( 1.74× ) 103.07 ±2.1 % hr ( 4.50× ) 187.93 ±1.2 % hr ( 4.64× ) 18.08 ±1.1 % hr 31.39 ±0.9 % hr 30.60 ±1.7 % hr ( 1.69× ) 53.36 ±0.9 % hr ( 1.70× ) 66.13 ±1.2 % hr ( 3.66× ) 107.38 ±0.7 % hr ( 3.42× ) 9.56 ±3.1 % hr 17.99 ±0.7 % hr 15.82 ±2.9 % hr ( 1.65× ) > 200 23.36 ±1.5 % hr ( 2.44× ) 67.32 ±0.5 % hr ( 3.74× ) 10.99 ±2.7 % hr 12.86 ±0.6 % hr 14.09 ±2.8 % hr ( 1.28× ) > 200 27.25 ±2.1 % hr ( 2.48× ) 57.65 ±0.4 % hr ( 4.48× ) 1 . Image Classification . The CIFAR-10 dataset [ 13 ] consists of 60,000 colour images in 10 classes . There are 50,000 training images and 10,000 test images . To follow the realistic non-iid data in FL scenarios , we partition both datasets into 128 clusters using a Dirich- let distribution with α equals 0.1 . We evaluate the dataset with ResNet-20 [ 7 ] model . 2 . Speech Recognition . The Google Command speech dataset [ 29 ] covers 105,829 audio recordings collected from 2,618 clients . The training set includes record- ings from 2,112 speakers , the validation set includes 256 speakers , and the test set includes 250 speak- ers . The data set is composed of 35 common words from the everyday vocabulary , such as ” Yes ” , ” No ” , ” Up ” , and ” Down ” . We evaluate the dataset with VGG11 [ 26 ] model for a 35-class keyword spotting task . We also evaluate the dataset with a lightweight model based on one related work [ 33 ] , and the de- tailed data-preprocessing methods are presented in Ap- pendix A.1.2 . 3 . Natural Language Processing . Reddit [ 1 ] consists of comments from 1,660,820 users in the Reddit forum . In this dataset , we filter the users with less than 20 words in total and restrict to the 30k most frequently used words , as the same settings in the previous work [ 14 ] . Then , we train the lightweight Albert [ 16 ] model for the next-word-prediction task . The performance is evaluated by the perplexity loss ( ppl ) , which lower is better . Experiment Setup . We use the FedML platform [ 6,35 ] , an open-source framework for FL , to execute our frame- work . On the CPU/GPU training side , to approach the real- world heterogeneous client system performance in emula- tion , we acquire the local computation times of deep learn- ing models across hundreds of device types from the AI benchmark [ 10 ] and communication times from Network Measurements on mobiles [ 8 ] . These data will be assigned to the simulated devices we create in the experiment , the same as the settings in previous FL works [ 14 , 15 , 18 ] . The distribution of heterogeneous system utility across simu- lated clients will be shown in the Appendix A.1.2 . Evaluation Metrics and Baselines . We compare TimelyFL with FedBuff [ 9 , 22 ] as the AsyncFL base- To demonstrate applicability of TimelyFL , we line . present the evaluation results using two aggregation func- tion , FedAvg [ 20 ] and FedOpt [ 23 ] . We evaluated the per- formance of TimelyFL and its baseline using the follow- ing three metrics : test accuracy/loss , time-to-accuracy , and participation rate . The participation rate is defined as the total number of rounds that the device contributes to the global update divided by the total communication round number . The rate is distributed in the interval between 0 and 1 , which implies how often a client participates in the global model update . Hyperparameter Settings . We searched for the client learning rate in a range from 10−6 to 100 , server learn- ing rate in a range from 10−4 to 100 , input batch size in a range from 8 to 256 , and total training round in a range from 1000 to 10000 . The aggregation goal and aggregation participation target is searched from 30 % to 50 % of train- ing concurrency per round for FedBuff and TimelyFL , respectively . We list the detailed hyperparameter selection for each experiment setup in the Appendix A.1.3 . 4.2 . End-to-End Performance We begin by comparing the end-to-end performance of TimelyFL on benchmark datasets , conducting on the CPU/GPU-based training . The training concurrency is set to 128 for CIFAR-10 related experiments , 20 for Google ( a ) CIFAR-10 with FedOpt ( b ) Google Command with FedOpt ( c ) Reddit with FedOpt Figure 4 . Time-to-accuracy performance for SyncFL , FedBuff and TimelyFL . FedBuff needs 1.28 - 2.89× in terms of wall clock time . Besides of the time-to-accuracy speedup , TimelyFL also harvests test accuracy increment compared to FedBuff within the same communication rounds . As the learn- ing curve in the Figure 4 , TimelyFL achieves 3.27 % and 4.01 % higher final accuracy on CIFAR-10 and Google Command , and 0.43 lower ppl on Reddit in comparison to FedBuff with FedOpt . Under FedAvg , TimelyFL achieves 4.93 % and 6.25 % higher final accuracy on CIFAR- 10 and Google Command , respectively , and 0.20 lower ppl on Reddit compared to FedBuff . 4.3 . Understanding the Advantages of TimelyFL TimelyFL improves inclusiveness1 . In Table 1 , we view the SyncFL as the standard baseline that does not in- clude any asynchronous technique and FedBuff as the baseline that only introduces a fixed buffer size to accel- erate the training . Instead of fixed buffer size , TimelyFL adopts a flexible buffer size controlled by aggregation inter- val time , which allows more available clients to participate in the global update per round . As illustrated in Figure 5 , 66.4 % of devices are able to achieve an increased partici- pation rate , and the average participation rate per client in- creases by 21 % in TimelyFL compared to FedBuff un- der the CIFAR-10 experiment setting we implemented in the last section . The average participation rate increment is the main reason for the time-to-accuracy speed-up . As each client joins the global model update more rapidly , the learn- In addi- ing efficiency increases during the FL training . tion , combined with more devices contributing to the global model more frequently , TimelyFL improves inclusiveness during the model training compared to FedBuff . The contribution of inclusiveness for model perfor- mance is especially significant when training on the non- iid dataset , where each client brings a unique local up- date to the global model . To demonstrate our point , we test both TimelyFL and Fedbuff with FedAvg as an 1In this paper , the inclusiveness increment represents the participation rate increment in the FL training . ( a ) Participation rate for each client ( b ) Participation rate distribution Figure 5 . Participation rate evaluation . speech related experiments , and 100 for Reddit related ex- periments . The communication round is set to be 2000 , 1000 , and 500 for CIFAR-10 , Google speech , and Red- dit , respectively . For both FedBuff and TimelyFL , we set the aggregation goal and aggregation participation tar- get equal to 50 % of training concurrency for a fair com- parison . We run each experiment five times with different random seeds and report its mean and standard deviation for the time consumption in the Table 1 . Speedup of TimelyFL . Given the same heterogeneous data , TimelyFL achieves the shortest training time to reach all target accuracy/loss . Table 1 shows the train- ing time needed to converge to the target accuracy/loss for each strategy considered . Compared to TimelyFL , synchronous FL requires 2.44 - 13.96× more times , and Table 2 . Wall clock training time to reach target validation accuracy on benchmark datasets ( lower is better ) . Dataset Agg . function Accuracy TimelyFL FedBuff SyncFL Google Speech FedAvg FedOpt 70 % 80 % 70 % 80 % 2.23 ±2.1 % hr 4.16 ±1.3 % hr 3.55 ±1.9 % hr ( 1.59× ) 6.13 ±1.4 % hr ( 1.47× ) 18.37 ±0.6 % hr ( 8.24× ) 32.46 ±0.4 % hr ( 7.80× ) 0.48 ±1.7 % hr 1.13 ±1.2 % hr 1.66 ±1.0 % hr ( 3.46× ) 3.25 ±0.8 % hr ( 2.88× ) 4.61 ±2.1 % hr ( 9.60× ) 7.47 ±1.1 % hr ( 6.61× ) tial training ratio and local epoch number , should be adap- tively scheduled based on the real-time capability of each device . To demonstrate our strategy , we test the training un- der the TimelyFL framework both with and without adap- tive workload scheduling on the CIFAR-10 dataset , with the training concurrency equals to 64 . Figure 7 shows the learning curves for both scenarios . With adaptive workload scheduling , TimelyFL saves 4.09× convergence time to 50 % accuracy and 10.89 % test accuracy increment , which illustrates that real-time workload scheduling essentially improves both learning efficiency and accuracy . TimelyFL is effective on the lightweight model . To investigate the effectiveness of the lightweight model on the TimelyFL framework , we implement one lightweight model on the Google Speech Commands dataset for the keyword spotting task . Following one previous work [ 33 ] , we choose the model that consists of two convolution lay- ers followed by one Gated Recurrent Units ( GRU ) layer . An average pooling layer is connected to the GRU output , which is then fed through two dense layers to generate the predictions . The parameter size of this model is equal to 79044 . We adopt the same baseline selections as in Sec- tion 4 . The hyperparameters for the experiments are listed in Section A.1.3 . The experiment results are summarized in Table 2 . TimelyFL achieves a higher convergence speed compared with the other two strategies before reaching the test accuracy , which confirms the simulation results elabo- rated in Section 4.2 and demonstrates the effectiveness of the TimelyFL on the lightweight model architecture . Figure 6 . Time-to-accuracy performance under different non-iid distribution . Figure 7 . TimelyFL performance under adaptive and non- adaptive workload schedule . aggregator on the CIFAR-10 dataset using a non-iid parti- tion . As shown in Figure 6 , as the parameter for Dirich- let distribution goes up , the convergence-time gap be- tween TimelyFL and Fedbuff increases as well , which demonstrates our advantage for non-iid data training com- pared to Fedbuff . TimelyFL is heterogeneity-aware . Under cross- device federated learning , most participating entities have limited computing capability and intermittent connectivity . As such , it could not be guaranteed that devices would complete their training workload in every communication round as assigned initially . To effectively resist the dis- turbance , the training hyperparameters , such as the par- 5 . Conclusion In this work , we propose TimelyFL , a heterogeneity- aware asynchronous FL scheme with adaptive partial train- ing . To include more available devices joining in global ag- gregation in a timely manner , TimelyFL introduces par- tial model training to the slow-speed devices . Moreover , TimelyFL is resilient to system heterogeneity by adjusting the local training workload based on the real-time resource capabilities of each client during FL training . Our exper- imental results demonstrate that TimelyFL could outper- form major AsyncFL proposals in terms of both time-to- accuracy and test accuracy . References [ 1 ] Reddit Comment Data . https : //files.pushshift . io/reddit/comments/ . 2 , 6 , 11 [ 2 ] Samiul Alam , Luyang Liu , Ming Yan , and Mi Zhang . FedRolex : Model-Heterogeneous Federated Learning with Rolling Sub-Model Extraction . In Conference on Neural In- formation Processing Systems ( NeurIPS ) , 2022 . 3 [ 3 ] Dmitrii Avdiukhin and Shiva Prasad Kasiviswanathan . Fed- erated learning under arbitrary communication patterns . In ICML , 2021 . 1 [ 4 ] Wei Dai , Yi Zhou , Nanqing Dong , H. Zhang , and Eric P. Xing . Toward understanding the impact of staleness in dis- tributed machine learning . ArXiv , abs/1810.03264 , 2019 . 3 [ 5 ] Niv Giladi , Mor Shpigel Nacson , Elad Hoffer , and Daniel Soudry . At stability ’ s edge : How to adjust hyperparame- ters to preserve minima selection in asynchronous training of neural networks ? ArXiv , abs/1909.12340 , 2020 . 3 [ 6 ] Chaoyang He , Songze Li , Jinhyun So , Mi Zhang , Hongyi Wang , Xiaoyang Wang , Praneeth Vepakomma , Abhishek Singh , Han Qiu , Li Shen , Peilin Zhao , Yan Kang , Yang Liu , Ramesh Raskar , Qiang Yang , Murali Annavaram , Fedml : A research library and Salman Avestimehr . ArXiv , and benchmark for federated machine learning . abs/2007.13518 , 2020 . 6 [ 7 ] Kaiming He , X. Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . 2016 IEEE Confer- ence on Computer Vision and Pattern Recognition ( CVPR ) , pages 770–778 , 2016 . 6 , 11 [ 8 ] Junxian Huang , Cheng Chen , Yutong Pei , Zhaoguang Wang , Zhiyun Qian , Feng Qian , Birjodh Tiwana , Qiang Xu , Z Mao , Ming Zhang , et al . Mobiperf : Mobile network measurement system . Technical Report . University of Michigan and Mi- crosoft Research , 2011 . 6 [ 9 ] Dzmitry Huba , John Nguyen , Kshitiz Malik , Ruiyu Zhu , Michael G. Rabbat , Ashkan Yousefpour , Carole-Jean Wu , Hongyuan Zhan , Pavel Ustinov , Harish Srinivas , Kaikai Wang , Anthony Shoumikhin , Jesik Min , and Mani Malek . Papaya : Practical , private , and scalable federated learning . ArXiv , abs/2111.04877 , 2021 . 1 , 3 , 6 [ 10 ] Andrey D. Ignatov , Radu Timofte , Andrei Kulik , Seungsoo Yang , Ke Wang , Felix Baum , Max Wu , Lirong Xu , and Luc Van Gool . Ai benchmark : All about deep learning on smartphones in 2019 . 2019 IEEE/CVF International Confer- ence on Computer Vision Workshop ( ICCVW ) , pages 3617– 3635 , 2019 . 6 , 11 [ 11 ] Xiaotang Jiang , Huan Wang , Yiliu Chen , Ziqi Wu , Lichuan Wang , Bin Zou , Yafeng Yang , Zongyang Cui , Yuezhi Cai , Tianhang Yu , Chengfei Lv , and Zhihua Wu . Mnn : A univer- sal and efficient inference engine . ArXiv , abs/2002.12418 , 2020 . 12 [ 12 ] Peter Kairouz , H Brendan McMahan , Brendan Avent , Aur´elien Bellet , Mehdi Bennis , Arjun Nitin Bhagoji , Kallista Bonawitz , Zachary Charles , Graham Cormode , Rachel Cum- mings , et al . Advances and open problems in federated learn- ing . Foundations and Trends® in Machine Learning , 14 ( 1– 2 ) :1–210 , 2021 . 1 [ 13 ] Alex Krizhevsky . Learning multiple layers of features from tiny images . 2009 . 2 , 6 , 11 [ 14 ] Fan Lai , Yinwei Dai , Xiangfeng Zhu , and Mosharaf Chowd- hury . Fedscale : Benchmarking model and system perfor- mance of federated learning . Proceedings of the First Work- shop on Systems Challenges in Reliable and Secure Feder- ated Learning , 2021 . 5 , 6 , 11 [ 15 ] Fan Lai , Xiangfeng Zhu , Harsha V. Madhyastha , and Mosharaf Chowdhury . Oort : Efficient federated learning via guided participant selection . In OSDI , 2021 . 6 [ 16 ] Zhenzhong Lan , Mingda Chen , Sebastian Goodman , Kevin Gimpel , Piyush Sharma , and Radu Soricut . Albert : A lite bert for self-supervised learning of language representations . ArXiv , abs/1909.11942 , 2020 . 6 , 11 [ 17 ] Sunwoo Lee , Tuo Zhang , Chaoyang He , and Salman Aves- timehr . Layer-wise adaptive model aggregation for scalable federated learning . ArXiv , abs/2110.10302 , 2021 . 3 [ 18 ] Chenning Li , Xiao Zeng , Mi Zhang , and Zhichao Cao . Pyra- midFL : A Fine-grained Client Selection Framework for Effi- cient Federated Learning . In ACM International Conference on Mobile Computing and Networking ( MobiCom ) , 2022 . 6 [ 19 ] Xingyu Li , Zhe Qu , Bo Tang , and Zhuo Lu . Stragglers are not disaster : A hybrid federated learning algorithm with de- layed gradients . ArXiv , abs/2102.06329 , 2021 . 3 [ 20 ] H. B. McMahan , Eider Moore , Daniel Ramage , Seth Hamp- son , and Blaise Ag¨uera y Arcas . Communication-efficient learning of deep networks from decentralized data . In AIS- TATS , 2017 . 2 , 6 [ 21 ] Muhammad Tahir Munir , Muhammad Mustansar Saeed , Ma- had Farah Ali , Zafar Ayyub Qazi , and Ihsan Ayyub Qazi . Fedprune : Towards inclusive federated learning . ArXiv , abs/2110.14205 , 2021 . 3 [ 22 ] John Nguyen , Kshitiz Malik , Hongyuan Zhan , Ashkan Yousefpour , Michael G. Rabbat , Mani Malek , and Dzmitry Huba . Federated learning with buffered asynchronous aggre- gation . ArXiv , abs/2106.06639 , 2021 . 1 , 2 , 3 , 6 , 11 [ 23 ] Sashank J. Reddi , Zachary B. Charles , Manzil Zaheer , Zachary Garrett , Keith Rush , Jakub Konecn´y , Sanjiv Ku- mar , and H. B. McMahan . Adaptive federated optimization . ArXiv , abs/2003.00295 , 2021 . 2 , 6 [ 24 ] Jae Hun Ro , Theresa Breiner , Lara McConnaughey , Mingqing Chen , Ananda Theertha Suresh , Shankar Kumar , and Rajiv Mathews . Scaling language model size in cross- device federated learning . ArXiv , abs/2204.09715 , 2022 . 3 [ 25 ] Hakim Sidahmed , Zheng Xu , Ankush Garg , Yuan Cao , and Mingqing Chen . Efficient and private federated learning with partially trainable networks . ArXiv , abs/2110.03450 , 2021 . 3 [ 26 ] Karen Simonyan and Andrew Zisserman . Very deep convo- lutional networks for large-scale image recognition . CoRR , abs/1409.1556 , 2015 . 6 , 11 [ 27 ] Saeed Vahidian , Sreevatsank Kadaveru , Woo-Ram Baek , Weijia Wang , Vyacheslav Kungurtsev , Chen Chen , Mubarak Shah , and Bill Lin . When do curricula work in federated learning ? ArXiv , abs/2212.12712 , 2022 . 1 [ 28 ] Jianyu Wang , Zachary Charles , Zheng Xu , Gauri Joshi , H Brendan McMahan , Maruan Al-Shedivat , Galen Andrew , Salman Avestimehr , Katharine Daly , Deepesh Data , et al . A field guide to federated optimization . arXiv:2107.06917 , 2021 . 1 arXiv preprint [ 29 ] Pete Warden . Speech commands : A dataset for limited- ArXiv , abs/1804.03209 , vocabulary speech recognition . 2018 . 2 , 6 , 11 [ 30 ] Wentai Wu , Ligang He , Weiwei Lin , Rui Mao , Carsten Maple , and Stephen A. Jarvis . Safa : A semi-asynchronous protocol for fast federated learning with low overhead . IEEE Transactions on Computers , 70:655–668 , 2021 . 1 , 3 [ 31 ] Cong Xie , Oluwasanmi Koyejo , and Indranil Gupta . Asyn- chronous federated optimization . ArXiv , abs/1903.03934 , 2019 . 3 [ 32 ] Tien-Ju Yang , Dhruv Guliani , Franccoise Beaufays , and Gio- vanni Motta . Partial variable training for efficient on-device federated learning . ArXiv , abs/2110.05607 , 2021 . 3 [ 33 ] Tuo Zhang , Tiantian Feng , Samiul Alam , Sunwoo Lee , Mi Zhang , Shrikanth S. Narayanan , and Salman Avestimehr . Fedaudio : A federated learning benchmark for audio tasks . ArXiv , abs/2210.15707 , 2022 . 6 , 8 , 11 [ 34 ] Tuo Zhang , Lei Gao , Chaoyang He , Mi Zhang , Bhaskar Kr- ishnamachari , and Salman Avestimehr . Federated learning for internet of things : Applications , challenges , and oppor- IEEE Internet of Things Magazine ( IEEE IoTM ) , tunities . 2022 . 3 [ 35 ] Tuo Zhang , Chaoyang He , Tian-Shya Ma , Mark Ma , and Salman Avestimehr . Federated learning for internet of things . Proceedings of the 19th ACM Conference on Embedded Net- worked Sensor Systems , 2021 . 6 [ 36 ] Zhengyuan Zhou , P. Mertikopoulos , Nicholas Bambos , Pe- ter W. Glynn , Yinyu Ye , Li-Jia Li , and Li Fei-Fei . Dis- tributed asynchronous optimization with unbounded delays : How slow can you go ? In ICML , 2018 . 3 A . Appendix A.1 . Experiment Settings A.1.1 Computing Infrastructure The simulation experiments are conducted on a computing server with one GPU . The server is equipped with AMD EPYC 7502 32-Core Processor and 1024G memory . The GPU is NVIDIA RTX A4000 . A.1.2 Datasets and Models AI Benchmark . AI Benchmark [ 10 ] is a public dataset that is designed for evaluating the performance of impor- tant AI tasks on mobile devices . AI Benchmark provides diverse models ’ training and inference speed across vari- ous devices , including chipsets from Qualcomm , HiSilicon , Samsung , MediaTek , and Unisoc . Figure 8a illustrates the distribution of the computation efficiency across clients in the AI Benchmark . The slowest device would take around 13.3× computational times than the fastest device for the same task . To approach the dynamic availability of devices , such as low-power mode or multi-process running , we de- sign a coefficient w as follows : x ∼ N ( 1 , 0.3 )   1 x 1.3 x ≥ 1.3 x ≤ 1 1 ≤ x ≤ 1.3  w = ( 2 ) In this work , we assign the values from AI Benchmark as base computation time to the clients to emulate real devices , analogous to the usage in FedScale [ 14 ] . We also generate the coefficient w every round for each client to simulate the natural disturbance to availability . The local computation time in each round equals the product of w and the base computation time for each client . MobiPerf . MobiPerf is a public dataset for measuring network performance on mobile devices , which collects the available cloud-to-edge network throughput of over 100k worldwide mobile clients . Figure 8b illustrates the distribu- tion of communication consumption of MobiPerf . Note that the best communication channel can be 200× better than the worst one . We randomly assign a value from MobiPerf to a simulated device every communication round to emulate intermittent connectivity in a real deployment . CIFAR-10 . The CIFAR-10 dataset [ 13 ] consists of 60,000 32x32 colour images in 10 classes . There are 50,000 training images and 10,000 test images . We normalize the images by the mean and standard deviation of the dataset . We evaluate the dataset with ResNet-20 [ 7 ] model . To emulate the realistic non-iid distribution , we partition the dataset using a Dirichlet distribution , following the previ- ous works [ 22 ] . Google Command . The Google Command speech dataset [ 29 ] covers 105,829 audio recordings collected from 2,618 clients . The training set includes recordings from 2,112D speakers , the validation set includes 256 speakers , and the test set includes 250 speakers . The data set is com- posed of 35 common words from the everyday vocabulary , such as ” Yes ” , ” No ” , ” Up ” , and ” Down ” . We evaluate the dataset with VGG11 [ 26 ] model and a lightweight model based on one related work [ 33 ] for a 35-class keyword spot- ting task . For the VGG11-based experiment on Google Speech Commands , we use the Mel-frequency cepstral coefficients ( MFCC ) method to pre-process the raw audio data . Specif- ically , a sequence of overlapping Hamming windows is ap- plied to the raw speech signal with a time shift of 10 ms and window size of 25ms . The MFCC is used for training the keyword spotting model . For the lightweight model experiment , to pre-process the raw audio data , a sequence of overlapping Hamming windows is applied to the raw speech signal with a time shift of 10 ms. We calculate the discrete Fourier trans- form ( DFT ) with a frame length of 1,024 and compute the Mel-spectrogram with a dimension of 128 . The Mel- spectrogram is used for training the keyword spotting model . We follow [ 33 ] for this setup . Reddit . Reddit [ 1 ] consists of comments from 1,660,820 users in the Reddit forum . Each client corresponds to a user , whose data are all of their personal posts . Thus it follows the real non-iid data under FL scenarios . In this dataset , we filter the users with less than 20 words in total and restrict to the 30k most frequently used words , as the same settings in the previous work [ 14 ] . Then , we train the lightweight Albert [ 16 ] model for the next-word-prediction task . The performance is evaluated by the perplexity loss ( ppl ) , which lower is better . A.1.3 Hyperparameter Settings We searched for the client learning rate in a range from 10−6 to 100 , server learning rate in a range from 10−4 to 100 , input batch size in a range from 8 to 256 , and total training round in a range from 1000 to 10000 . The aggre- gation goal and aggregation participation target is searched from 30 % to 50 % of training concurrency per round for FedBuff and TimelyFL , respectively . After hyper-parameter searching , we fixed the follow- ing hyperparameters : for CIFAR-10 related experiments , the total training round is 2000 , and training concurrency is 128 for all setups . The aggregation goal and aggregation participation target is 50 % of the training concurrency for both FedBuff and TimelyFL . For CIFAR-10 with Fe- dAvg related experiments , the batch size is 8 , and the client learning rate is 0.8 . For CIFAR-10 with FedOpt related ex- Figure 9 . Partial training system performance in on real edge de- vices . A.2 . System Performance A.2.1 Partial Training Performance Due to different parameters and tensor shapes among dif- ferent layers , the training time ( computational time of the forward and backward propagation ) is not strictly linear to the trainable layer numbers and varies with the model struc- tures . For simplicity and generality , we define the training time of the partial model as the linear multiplication of the training time of the full model and the training ratio . This linear relationship is verified through our real measurement on a Samsung Galaxy S20 with ResNet-20 model using MNN [ 11 ] library . As shown in Figure 9 , most of the test results are below the linear straight line ( except the ratio is below 0.2 ) , justifying the rationality of our choice . ( a ) Diverse computation efficiency in AI Benchmark ( b ) Diverse communicate efficiency in Mobiperf Figure 8 . Heterogeneous system utility across simulated clients . periments , the batch size is 10 , the client learning rate is 0.03 , and the server learning rate is 0.001 with ADAM as server optimizer . For Google command related experiments with VGG11 model , the total training round is 1000 , and training con- currency is 20 for all setups . The aggregation goal and ag- gregation participation target is 50 % of the training concur- rency for both FedBuff and TimelyFL . The batch size is 32 , and the client learning rate is 0.01 . Under the FedOpt , the server learning rate is 0.001 with ADAM as server opti- mizer . For Google command related experiments with the lightweight model , the total training round is 5000 , and training concurrency is 106 for all setups . The aggrega- tion goal and aggregation participation target is 50 % of the training concurrency for both FedBuff and TimelyFL . The batch size is 16 , and the client learning rate is 0.1 under the FedAvg . Under the FedOpt , the client learning rate is 0.05 for synchrounous FL and TimelyFL , and the client learning rate is 0.2 for FedBuff . The server learning rate is 0.001 with ADAM as server optimizer for all setups . Finally , for Reddit related experiments , the total training round is 500 , and training concurrency is 20 for all setups . The aggregation goal and aggregation participation target is 50 % of the training concurrency for both FedBuff and TimelyFL . The batch size is 20 , and the client learning rate is 0.0005 for SyncFL and TimelyFL , and 0.0003 for FedBuff . Under the FedOpt , the server learning rate is 0.001 with ADAM as server optimizer .","['r', 'p', 'l', 'c', 'v', 'r', 'timelyfl', 'heterogeneityaware', 'asynchronous', 'federate', 'learning', 'adaptive', 'partial', 'training', 'tuo', 'zhang1', 'gao1', 'sunwoo', 'abstract', 'crossdevice', 'federate', 'learning', 'environment', 'scale', 'synchronous', 'method', 'challenge', 'strag', 'gler', 'hinder', 'training', 'process', 'moreover', 'ity', 'client', 'join', 'training', 'highly', 'variable', 'time', 'system', 'heterogeneity', 'intermittent', 'connec', 'tivity', 'recent', 'asynchronous', 'method', 'fedbuff', 'propose', 'overcome', 'issue', 'slow', 'user', 'continue', 'work', 'local', 'train', 'base', 'stale', 'model', 'contribute', 'aggregation', 'ready', 'however', 'show', 'empirically', 'method', 'lead', 'substantial', 'drop', 'training', 'accuracy', 'well', 'slow', 'convergence', 'rate', 'primary', 'reason', 'device', 'contribute', 'many', 'round', 'gregation', 'join', 'intermittently', 'stale', 'model', 'update', 'overcome', 'barrier', 'propose', 'timelyfl', 'heterogeneityaware', 'asynchronous', 'framework', 'adaptive', 'partial', 'training', 'training', 'timelyfl', 'adjust', 'local', 'training', 'workload', 'base', 'realtime', 'resource', 'capability', 'client', 'aim', 'allow', 'available', 'client', 'join', 'global', 'update', 'staleness', 'demonstrate', 'performance', 'benefit', 'timelyfl', 'conduct', 'extensive', 'experiment', 'various', 'dataset', 'speech', 'reddit', 'model', 'resnet20', 'vgg11', 'comparison', 'stateoftheart', 'ie', 'fedbuff', 'evaluation', 'reveal', 'improve', 'participa', 'tion', 'rate', 'harvest', '128×', '289×', 'efficiency', 'convergence', 'rate', 'provide', 'increment', 'test', 'accuracy', 'introduction', 'federate', 'learning', 'emerge', 'promising', 'distribute', 'machine', 'learning', 'paradigm', 'preserve', 'gist', 'keep', 'client', 'pri', 'vate', 'datum', 'device', 'perform', 'local', 'model', 'training', 'client', 'central', 'server', 'collect', 'locally', 'train', 'model', 'update', 'global', 'model', 'push', 'back', 'next', 'round', 'training', 'exist', 'protocol', 'base', 'synchronous', 'training', 'syncfl', 'mean', 'round', 'client', 'select', 'cohort', 'client', 'update', 'local', 'mod', 'el', 'base', 'late', 'update', 'broadcast', 'server', 'beginning', 'round', 'unbalanced', 'commu', 'nication', 'hardware', 'capability', 'nonidentical', 'training', 'datum', 'distribution', 'however', 'time', 'consumption', 'cal', 'update', 'vary', 'substantially', 'device', 'device', 'client', 'even', 'temporarily', 'disconnect', 'training', 'process', 'thus', 'leave', 'server', 'suboptimal', 'choice', 'wait', 'client', 'participate', 'round', 'finish', 'local', 'training', 'contribute', 'model', 'aggregation', 'cause', 'significant', 'delay', 'straggler', 'wait', 'subset', 'fast', 'client', 'ignore', 'work', 'contribution', 'slow', 'client', 'critical', 'challenge', 'largely', 'pede', 'scalability', 'make', 'difficult', 'land', 'largescale', 'crossdevice', 'scenario', 'address', 'challenge', 'recent', 'work', 'propose', 'asynchronous', 'federate', 'learn', 'allow', 'slow', 'client', 'continue', 'local', 'training', 'contribute', 'future', 'aggregation', 'round', 'client', 'local', 'training', 'global', 'model', 'tionupdate', 'certain', 'client', 'simultaneously', 'get', 'update', 'cloud', 'server', 'decrease', 'impact', 'straggler', 'recent', 'asyncfl', 'work', 'fedbuff', 'propose', 'server', 'perform', 'gradient', 'aggregation', 'create', 'global', 'model', 'number', 'receive', 'local', 'update', 'reach', 'requisite', 'thresh', 'old', 'tunable', 'parameter', 'refer', 'aggregation', 'goal', 'slow', 'client', 'still', 'upload', 'update', 'later', 'long', 'finish', 'local', 'training', 'update', 'include', 'base', 'staleness', 'information', 'participation', 'rate', 'distribution', 'vice', 'number', 'participate', 'client', 'training', 'round', 'c', 'timetoaccuracy', 'performance', 'egy', 'figure', 'empirical', 'performance', 'syncfl', 'fedbuff', 'timelyfl', 'cifar10', 'classification', 'task', 'fedopt', 'server', 'aggregator', 'experiment', 'detail', 'evaluation', 'see', 'section', 'timelyfl', 'include', 'device', 'join', 'global', 'update', 'training', 'show', 'b', 'device', 'participate', 'timely', 'timelyfl', 'harvest', 'convergence', 'rate', 'accuracy', 'boost', 'compare', 'fedbuff', 'show', 'c', 'highlight', 'figure', '1c', 'empirically', 'demonstrate', 'fedbuff', 'achieve', 'much', 'fast', 'convergence', 'certain', 'intermediate', 'accuracy', 'unfortunately', 'lead', 'substantial', 'drop', 'final', 'accuracy', 'compare', 'syncfl', 'intuitive', 'explanation', 'fedbuff', 'accept', 'fix', 'number', 'local', 'update', 'contribute', 'global', 'model', 'communication', 'round', 'decrease', 'par', 'allel', 'computing', 'efficiency', 'block', 'complete', 'cal', 'update', 'global', 'aggregation', 'turn', 'stale', 'update', 'postpone', 'next', 'round', 'global', 'update', 'moreover', 'server', 'aggregator', 'prefer', 'device', 'contribute', 'round', 'train', 'lowspee', 'device', 'enjoy', 'frequency', 'contribution', 'even', 'slow', 'device', 'ticipate', 'global', 'training', 'occasionally', 'send', 'stale', 'update', 'potentially', 'harm', 'convergence', 'global', 'loss', 'show', 'figure', '1a', 'compare', 'syncfl', 'fedbuff', 'include', 'fix', 'number', 'local', 'update', 'round', 'achieve', 'low', 'participation', 'rate', 'num', 'ber', 'aggregation', 'participate', 'divide', 'total', 'number', 'aggregation', 'round', 'average', 'biased', 'distribu', 'tion', 'indicate', 'inclusiveness', 'group', 'diminish', 'root', 'cause', 'test', 'accuracy', 'gap', 'close', 'gap', 'pose', 'timelyfl', 'heterogeneityaware', 'asynchronous', 'erate', 'learning', 'framework', 'base', 'adaptive', 'partial', 'train', 'ing', 'key', 'distinction', 'previous', 'asyncfl', 'work', 'set', 'fix', 'number', 'limit', 'number', 'local', 'update', 'global', 'instead', 'accommodate', 'flexi', 'aggregation', 'round', 'ble', 'number', 'client', 'join', 'global', 'update', 'set', 'wallclock', 'time', 'limit', 'aggregation', 'interval', 'aggregation', 'interval', 'equal', 'kth', 'fast', 'local', 'update', 'time', 'client', 'tunable', 'parameter', 'long', 'device', 'deliver', 'model', 'update', 'server', 'interval', 'part', 'global', 'aggrega', 'tion', 'include', 'available', 'device', 'join', 'global', 'aggregation', 'staleness', 'introduce', 'partial', 'model', 'training', 'client', 'low', 'capacity', 'instead', 'fully', 'train', 'e', 'model', 'part', 'model', 'compose', 'subset', 'consecutive', 'outputside', 'layer', 'assign', 'backward', 'pass', 'training', 'partial', 'model', 'train', 'local', 'computation', 'time', 'communication', 'time', 'decrease', 'stale', 'client', 'show', 'figure', 'timelyfl', 'unify', 'local', 'training', 'time', 'adaptively', 'adjust', 'workload', 'local', 'epoch', 'number', 'partial', 'training', 'ratio', 'client', 'make', 'feasible', 'client', 'finish', 'local', 'train', 'ing', 'upload', 'update', 'server', 'calcu', 'late', 'aggregation', 'interval', 'communication', 'round', 'timelyfl', 'tackle', 'system', 'heterogeneity', 'sue', 'eliminate', 'staleness', 'local', 'update', 'report', 'slow', 'device', 'evaluate', 'performance', 'ous', 'application', 'task', 'include', 'image', 'classification', 'speech', 'recognition', 'natural', 'language', 'processing', 'cifar', 'speech', 'reddit', 'com', 'ment', 'benchmark', 'dataset', 'respectively', 'com', 'monly', 'use', 'aggregation', 'function', 'dopt', 'result', 'show', 'device', 'increase', 'participation', 'rate', 'average', 'participation', 'rate', 'crease', 'timelyfl', 'compare', 'fedbuff', 'scale', 'fl', 'system', 'timelyfl', 'outper', 'form', 'fedbuff', 'timetoaccuracy', 'final', 'test', 'accuracy', 'show', 'figure', 'relate', 'work', 'asynchronous', 'federate', 'learning', 'mittent', 'connectivity', 'availability', 'client', 'asyn', 'chronous', 'promising', 'solution', 'tackle', 'device', 'het', 'figure', 'fedbuff', 'top', 'server', 'update', 'global', 'model', 'receive', 'requisite', 'number', 'local', 'update', 'slow', 'device', 'still', 'send', 'update', 'later', 'time', 'server', 'fast', 'device', 'participate', 'time', 'global', 'update', 'slow', 'device', 'contribute', 'less', 'participation', 'timelyfl', 'bottom', 'server', 'update', 'individual', 'workload', 'round', 'base', 'realtime', 'availability', 'client', 'include', 'device', 'global', 'update', 'timely', 'largely', 'increase', 'participation', 'rate', 'slow', 'device', 'erogeneity', 'fl', 'asynchronous', 'centrate', 'solve', 'straggler', 'problem', 'papaya', 'fedbuff', 'pro', 'pose', 'mitigate', 'straggler', 'enable', 'secure', 'aggregation', 'jointly', 'specifically', 'individual', 'update', 'incorpo', 'rate', 'server', 'soon', 'arrive', 'instead', 'server', 'keep', 'receive', 'local', 'update', 'secure', 'buffer', 'buffer', 'size', 'tunable', 'parameter', 'update', 'global', 'model', 'buffer', 'full', 'slow', 'device', 'also', 'send', 'local', 'update', 'server', 'global', 'aggrega', 'tion', 'update', 'consider', 'next', 'available', 'global', 'update', 'however', 'practically', 'speak', 'fast', 'device', 'participate', 'global', 'update', 'many', 'time', 'slow', 'device', 'slow', 'device', 'join', 'global', 'aggregation', 'even', 'due', 'staleness', 'control', 'approach', 'assume', 'client', 'process', 'local', 'training', 'fullmodel', 'size', 'slow', 'user', 'participate', 'global', 'aggregation', 'contribute', 'stale', 'update', 'previous', 'work', 'point', 'effect', 'stale', 'update', 'distribute', 'machine', 'learning', 'directly', 'harm', 'convergence', 'behavior', 'final', 'model', 'align', 'asynchronous', 'distribute', 'optimization', 'theory', 'suggest', 'moreover', 'participation', 'rate', 'mainly', 'balance', 'highspeed', 'device', 'contribute', 'round', 'global', 'update', 'compare', 'slowspeed', 'vice', 'contrast', 'previous', 'approach', 'focus', 'en', 'able', 'client', 'join', 'global', 'aggregation', 'effectively', 'base', 'local', 'resource', 'improve', 'inclusiveness', 'final', 'global', 'model', 'training', 'partial', 'model', 'training', 'partial', 'model', 'training', 'view', 'efficient', 'approach', 'reduce', 'tion', 'computation', 'workload', 'clientside', 'fl', 'system', 'fedprune', 'propose', 'method', 'prune', 'global', 'model', 'client', 'base', 'device', 'capa', 'bilitie', 'slow', 'client', 'serve', 'small', 'model', 'fast', 'client', 'train', 'large', 'model', 'lever', 'age', 'partially', 'trainable', 'neural', 'network', 'client', 'duce', 'communication', 'cost', 'enable', 'fast', 'training', 'small', 'memory', 'footprint', 'implication', 'model', 'performance', 'work', 'also', 'address', 'partial', 'model', 'training', 'save', 'communi', 'cation', 'cost', 'memory', 'usage', 'crossdevice', 'work', 'maintain', 'partial', 'ratio', 'submodel', 'certain', 'client', 'constant', 'entire', 'training', 'pro', 'cess', 'neglect', 'availability', 'device', 'stable', 'time', 'work', 'adaptively', 'adjust', 'partial', 'ratio', 'local', 'model', 'training', 'base', 'realtime', 'device', 'availability', 'aim', 'improve', 'efficiency', 'utility', 'client', 'method', 'standard', 'asynchronous', 'federate', 'learning', 'figure', 'leave', 'illustrate', 'standard', 'asynchronous', 'framework', 'instead', 'wait', 'client', 'finish', 'local', 'model', 'train', 'server', 'store', 'individual', 'date', 'client', 'buffer', 'adjust', 'global', 'model', 'buffer', 'size', 'reach', 'requisite', 'number', 'aggregation', 'goal', 'nonparticipating', 'device', 'postpone', 'contribution', 'global', 'update', 'latter', 'communication', 'round', 'finish', 'training', 'give', 'standard', 'asyncfl', 'framework', 'suffer', 'siveness', 'constraint', 'describe', 'introduction', 'section', 'figure', 'leave', 'standard', 'framework', 'server', 'initiate', 'global', 'update', 'collect', 'requisite', 'number', 'local', 'update', 'client', 'postpone', 'latter', 'communication', 'round', 'stale', 'update', 'information', 'right', 'propose', 'timelyfl', 'server', 'include', 'receive', 'local', 'update', 'aggregation', 'interval', 'global', 'update', 'client', 'weak', 'capacity', 'assign', 'train', 'subset', 'model', 'catch', 'aggregation', 'interval', 'time', 'propose', 'efficient', 'asyncfl', 'framework', 'show', 'figure', 'right', 'address', 'issue', 'timelyfl', 'design', 'preliminary', 'increase', 'participation', 'rate', 'slow', 'client', 'aim', 'design', 'crossdevice', 'asynchronous', 'federate', 'learn', 'ing', 'framework', 'device', 'finish', 'local', 'date', 'limited', 'time', 'interval', 'adaptively', 'adjust', 'workload', 'round', 'workload', 'define', 'product', 'local', 'training', 'number', 'e', 'partial', 'model', 'training', 'ratio', 'formalize', 'time', 'utility', 'function', 'consider', 'local', 'training', 'optimization', 'client', 'side', 'design', 'follow', 'arg', 'e', '×', '˜tcom', '≤', 'estimate', 'local', 'computation', 'time', '˜tcom', 'c', 'estimate', 'local', 'communication', 'time', 'client', 'c', 'epoch', 'full', 'model', 'training', 'certain', 'communication', 'round', 'calculate', 'server', 'note', 'c', 'constant', 'train', 'ing', 'nature', 'mobile', 'device', 'adjust', 'e', 'device', 'suppose', 'participate', 'global', 'aggregation', 'communication', 'round', 'timely', 'imally', 'utilize', 'resource', 'capability', 'aggregation', 'therefore', 'overall', 'distribute', 'optimization', 'volve', 'iteration', 'diverse', 'dataset', 'lead', 'fast', 'convergence', 'impede', 'contribute', 'global', 'model', 'address', 'issue', 'introduce', 'partial', 'model', 'training', 'slow', 'device', 'figure', 'right', 'illustrate', 'approach', 'partial', 'model', 'training', 'land', 'device', 'heterogeneity', 'system', 'instead', 'full', 'training', 'model', 'weak', 'device', 'assign', 'train', 'partial', 'model', 'compose', 'subset', 'consecutive', 'outputside', 'layer', 'training', 'subset', 'assign', 'layer', 'operate', 'forward', 'pass', 'backward', 'propagation', 'layer', 'pro', 'cess', 'forward', 'pass', 'input', 'datum', 'freeze', 'weight', 'update', 'local', 'training', 'finish', 'partially', 'train', 'client', 'return', 'update', 'assign', 'layer', 'server', 'global', 'aggregation', 'frozen', 'layer', 'unchanged', 'training', 'target', 'solve', 'bottleneck', 'crossdevice', 'partial', 'model', 'training', 'communication', 'compu', 'tation', 'efficiency', 'stale', 'client', 'edge', 'compute', 'backward', 'propagation', 'consume', 'much', 'time', 'forward', 'pass', 'partial', 'model', 'training', 'essentially', 'duce', 'training', 'time', 'need', 'calculate', 'gradi', 'ent', 'frozen', 'parameter', 'reduce', 'time', 'roughly', 'proportional', 'reduce', 'model', 'size', 'empirically', 'show', 'a21', 'moreover', 'send', 'trainable', 'part', 'update', 'server', 'substantially', 'improv', 'ing', 'communication', 'efficiency', 'especially', 'straggler', 'limited', 'network', 'connection', 'exist', 'implement', 'partial', 'model', 'training', 'aim', 'let', 'lowcapacity', 'device', 'report', 'local', 'update', 'server', 'timely', 'stal', 'eness', 'thereby', 'improve', 'participation', 'rate', 'training', 'adaptive', 'model', 'training', 'resource', 'limitation', 'weak', 'device', 'finish', 'full', 'model', 'training', 'effectively', 'time', 'terval', 'make', 'become', 'stale', 'client', 'system', 'timelyfl', 'base', 'adaptive', 'model', 'training', 'propose', 'try', 'unify', 'client', 'round', 'time', 'limit', 'aggregation', 'interval', 'tk', 'adaptively', 'adjust', 'workload', 'concern', 'realtime', 'availability', 'communication', 'round', 'workload', 'define', 'product', 'partial', 'training', 'ratio', 'local', 'number', 'framework', 'compose', 'main', 'part', 'timelyfl', 'server', 'local', 'time', 'update', 'work', 'load', 'scheduling', 'time', 'datum', 'batch', 'full', 'model', 'train', 'unit', 'computation', 'time', 'estimate', 'progress', 'define', 'ratio', 'train', 'batch', 'number', 'total', 'data', 'batch', 'number', 'local', 'communication', 'time', 'equal', 'model', 'file', 'size', 'device', 'network', 'setting', 'previous', 'fl', 'system', 'work', 'timelyfl', 'input', 'k', 'aggregation', 'participation', 'target', 'number', 'training', 'concurrency', 'r', 'communication', 'round', 'global', 'server', 'sample', 'client', 'uniformly', 'random', 'define', 'send', 'w', 'r', 'client', 'c', 'parallel', 'client', 'localtimeupdate', 'global', 'server', 'kth', 'small', 'number', 'r', '⟨er⟩', '⟨tr', 'rpt⟩', 'r', '⟨˜tcmp⟩', 'workloadschedule', 'client', 'parallel', 'r', 'c', 'model', 'training', 'global', 'server', 'aggregate', '⟨w', 'r', 'c', 'end', 'output', 'timelyfl', 'server', 'timelyfl', 'server', 'charge', 'adjust', 'aggregation', 'interval', 'local', 'training', 'e', 'partial', 'training', 'ratio', 'α', 'device', 'fl', 'training', 'summarize', 'gation', 'interval', 'tk', 'round', 'equal', 'kth', 'small', 'value', 'estimate', 'unit', 'total', 'time', 'client', 'communication', 'round', 'timelyfl', 'server', 'randomly', 'sample', 'client', 'construct', 'collec', 'tion', 'distribute', 'global', 'model', 'client', 'mean', 'client', 'start', 'local', 'training', 'round', 'definition', 'training', 'concurrency', 'fedbuff', 'select', 'client', 'perform', 'datum', 'batch', 'full', 'model', 'training', 'estimate', 'time', 'consump', 'tion', 'report', 'server', 'aggregation', 'interval', 'time', 'training', 'hyperparameter', 'client', 'cal', 'train', 'number', 'e', 'partial', 'training', 'ratio', 'adjust', 'base', 'select', 'client', 'status', 'dur', 'fl', 'training', 'process', 'server', 'also', 'return', 'local', 'computation', 'budget', 'time', 'trpt', 'c', 'wallclock', 'time', 'client', 'report', 'training', 'status', 'local', 'time', 'update', 'efficiently', 'accommodate', 'capability', 'participant', 'need', 'update', 'time', 'con', 'sumption', 'server', 'summarize', 'specifically', 'client', 'collect', 'real', 'computation', 'local', 'time', 'update', 'input', 'file', 'size', 'receive', 'global', 'model', 'bw', 'realtime', 'network', 'bandwidth', 'datum', 'batch', 'training', 'mbw', 'tcmpβ', 'output', 'workload', 'scheduling', 'adjust', 'number', 'e', 'partial', 'training', 'ratio', 'α', 'client', 'communication', 'round', 'base', 'estimated', '˜tcom', 'c', 'aggregation', 'interval', 'tk', 'relation', 'ship', 'show', 'unit', 'total', 'time', 'small', 'server', 'try', 'maximize', 'local', 'training', 'utility', 'minimize', 'idle', 'time', 'assign', 'local', 'epoch', 'training', 'next', 'round', 'otherwise', 'server', 'assign', 'less', 'amount', 'workload', 'de', 'crease', 'model', 'training', 'ratio', 'guarantee', 'finish', 'least', 'local', 'epoch', 'training', 'report', 'time', 'trpt', 'c', 'catch', 'global', 'aggregation', 'summarize', 'scheduler', 'workload', 'scheduling', 'input', 'aggregation', 'interval', 'time', '⟨˜tcmp⟩', 'unit', 'computation', 'time', 'unit', 'communication', 'time', 'client', 'parallel', 'c', 'αc', 'c', 'c', 'trpt', 'c', 'αc', 'end', 'output', '⟨e⟩', '⟨trpt⟩', 'experiment', 'experimental', 'setting', 'dataset', 'model', 'task', 'demonstrate', 'timelyfl', 'effectiveness', 'task', 'evaluate', 'timelyfl', 'benchmark', 'dataset', 'various', 'cat', 'egorie', 'application', 'table', 'wall', 'clock', 'training', 'time', 'reach', 'target', 'validation', 'accuracy', 'benchmark', 'dataset', 'lower', 'well', 'hr', 'indicate', 'target', 'accuracy', 'reach', 'dataset', 'function', 'accuracyloss', 'timelyfl', 'speech', 'reddit', 'fedavg', 'fedopt', 'fedavg', 'fedopt', 'fedavg', 'fedopt', 'ppl', 'ppl', 'ppl', 'ppl', 'hr', 'hr', 'hr', '143×', 'hr', 'hr', 'hr', 'hr', '±26', 'hr', 'hr', '289×', '±23', 'hr', '974×', 'hr', '911×', '±21', 'hr', 'hr', 'hr', 'hr', 'hr', 'hr', '±11', 'hr', 'hr', 'hr', 'hr', '170×', 'hr', 'hr', 'hr', 'hr', '±29', 'hr', '165×', '±15', 'hr', '244×', 'hr', 'hr', 'hr', 'hr', '128×', '±21', 'hr', 'hr', '448×', 'image', 'classification', 'cifar10', 'consist', 'colour', 'image', 'class', 'training', 'image', 'test', 'image', 'follow', 'realistic', 'noniid', 'datum', 'scenario', 'partition', 'dataset', 'cluster', 'use', 'dirich', 'let', 'distribution', 'equal', 'evaluate', 'dataset', 'resnet20', 'model', 'speech', 'command', 'speech', 'dataset', 'cover', 'audio', 'recording', 'collect', 'client', 'training', 'set', 'include', 'record', 'ing', 'speaker', 'validation', 'set', 'include', 'speaker', 'test', 'set', 'include', 'speak', 'er', 'datum', 'set', 'compose', 'common', 'word', 'everyday', 'vocabulary', 'evaluate', 'dataset', 'vgg11', 'model', 'keyword', 'spot', 'task', 'also', 'evaluate', 'dataset', 'lightweight', 'model', 'base', 'relate', 'work', 'de', 'tailed', 'datapreprocessing', 'method', 'present', 'natural', 'language', 'processing', 'reddit', 'consist', 'comment', 'user', 'reddit', 'forum', 'dataset', 'filter', 'user', 'less', 'word', 'total', 'restrict', '30k', 'frequently', 'use', 'word', 'setting', 'previous', 'work', 'train', 'lightweight', 'model', 'nextwordprediction', 'task', 'performance', 'evaluate', 'perplexity', 'loss', 'low', 'well', 'experiment', 'setup', 'use', 'fedml', 'platform', 'opensource', 'framework', 'execute', 'frame', 'work', 'cpugpu', 'training', 'side', 'approach', 'real', 'world', 'heterogeneous', 'client', 'system', 'performance', 'emula', 'tion', 'acquire', 'local', 'computation', 'time', 'deep', 'learn', 'ing', 'model', 'hundred', 'device', 'type', 'benchmark', 'communication', 'time', 'network', 'measurement', 'mobile', 'datum', 'assign', 'simulate', 'device', 'create', 'experiment', 'setting', 'previous', 'work', 'distribution', 'heterogeneous', 'system', 'utility', 'simu', 'late', 'client', 'show', 'a12', 'evaluation', 'metric', 'baseline', 'compare', 'timelyfl', 'fedbuff', 'asyncfl', 'base', 'demonstrate', 'applicability', 'line', 'present', 'evaluation', 'result', 'use', 'aggregation', 'tion', 'fedopt', 'evaluate', 'formance', 'baseline', 'use', 'follow', 'ing', 'metric', 'test', 'accuracyloss', 'timetoaccuracy', 'participation', 'rate', 'participation', 'rate', 'define', 'total', 'number', 'round', 'device', 'contribute', 'global', 'update', 'divide', 'total', 'communication', 'round', 'number', 'rate', 'distribute', 'interval', 'imply', 'often', 'client', 'participate', 'global', 'model', 'update', 'hyperparameter', 'setting', 'search', 'client', 'learning', 'rate', 'range', 'server', 'learn', 'ing', 'rate', 'range', 'input', 'batch', 'size', 'range', 'total', 'training', 'round', 'range', 'aggregation', 'goal', 'aggregation', 'participation', 'target', 'search', 'train', 'ing', 'concurrency', 'round', 'fedbuff', 'timelyfl', 'respectively', 'list', 'detailed', 'hyperparameter', 'selection', 'experiment', 'setup', 'a13', 'endtoend', 'performance', 'begin', 'compare', 'endtoend', 'performance', 'timelyfl', 'benchmark', 'dataset', 'conduct', 'cpugpubase', 'training', 'training', 'concurrency', 'set', 'cifar10', 'relate', 'experiment', 'cifar10', 'command', 'fedopt', 'c', 'reddit', 'fedopt', 'figure', 'timetoaccuracy', 'performance', 'syncfl', 'fedbuff', 'timelyfl', 'fedbuff', 'need', '289×', 'term', 'time', 'timetoaccuracy', 'speedup', 'timelyfl', 'also', 'harvest', 'test', 'accuracy', 'increment', 'compare', 'fedbuff', 'communication', 'round', 'learn', 'curve', 'figure', 'timelyfl', 'achieve', 'high', 'final', 'accuracy', 'command', 'low', 'ppl', 'reddit', 'comparison', 'fedbuff', 'fedopt', 'timelyfl', 'achieve', 'high', 'final', 'accuracy', 'command', 'respectively', 'low', 'ppl', 'reddit', 'compare', 'fedbuff', 'understand', 'advantage', 'improve', 'inclusiveness1', 'table', 'view', 'syncfl', 'standard', 'baseline', 'clude', 'asynchronous', 'technique', 'fedbuff', 'baseline', 'introduce', 'fix', 'buffer', 'size', 'accel', 'erate', 'training', 'instead', 'fix', 'buffer', 'size', 'timelyfl', 'adopt', 'flexible', 'buffer', 'size', 'control', 'time', 'allow', 'available', 'client', 'participate', 'global', 'update', 'round', 'illustrate', 'figure', 'device', 'able', 'achieve', 'increase', 'partici', 'pation', 'rate', 'average', 'participation', 'rate', 'client', 'crease', 'timelyfl', 'compare', 'cifar10', 'experiment', 'set', 'implement', 'last', 'section', 'average', 'participation', 'rate', 'increment', 'main', 'reason', 'timetoaccuracy', 'speedup', 'client', 'join', 'global', 'model', 'update', 'rapidly', 'learn', 'ing', 'efficiency', 'increase', 'fl', 'training', 'tion', 'combine', 'device', 'contribute', 'global', 'model', 'frequently', 'timelyfl', 'improve', 'inclusiveness', 'model', 'training', 'compare', 'fedbuff', 'contribution', 'inclusiveness', 'model', 'mance', 'especially', 'significant', 'training', 'non', 'iid', 'dataset', 'client', 'bring', 'unique', 'local', 'date', 'global', 'model', 'demonstrate', 'point', 'test', 'timelyfl', 'fedbuff', 'fedavg', 'paper', 'inclusiveness', 'increment', 'represent', 'participation', 'rate', 'increment', 'fl', 'training', 'participation', 'rate', 'client', 'participation', 'rate', 'distribution', 'figure', 'participation', 'rate', 'evaluation', 'speech', 'relate', 'experiment', 'reddit', 'relate', 'ex', 'periment', 'communication', 'round', 'set', 'speech', 'respectively', 'fedbuff', 'timelyfl', 'set', 'aggregation', 'goal', 'aggregation', 'participation', 'tar', 'get', 'equal', 'training', 'concurrency', 'fair', 'com', 'parison', 'run', 'experiment', 'time', 'different', 'random', 'seed', 'report', 'mean', 'standard', 'deviation', 'time', 'consumption', 'table', 'speedup', 'give', 'heterogeneous', 'datum', 'timelyfl', 'achieve', 'short', 'training', 'time', 'reach', 'target', 'accuracyloss', 'table', 'show', 'train', 'ing', 'time', 'need', 'converge', 'target', 'accuracyloss', 'strategy', 'consider', 'compare', 'timelyfl', 'synchronous', 'require', 'time', 'table', 'wall', 'clock', 'training', 'time', 'reach', 'target', 'validation', 'accuracy', 'benchmark', 'dataset', 'lower', 'well', 'dataset', 'function', 'accuracy', 'timelyfl', 'speech', 'fedopt', 'hr', 'hr', 'hr', '±14', 'hr', '147×', 'hr', 'hr', 'hr', 'hr', '±08', 'hr', '288×', '±21', 'hr', '±11', 'hr', '661×', 'tial', 'training', 'ratio', 'local', 'epoch', 'number', 'adap', 'tively', 'schedule', 'base', 'realtime', 'capability', 'device', 'demonstrate', 'strategy', 'test', 'training', 'timelyfl', 'framework', 'adap', 'tive', 'workload', 'scheduling', 'cifar10', 'dataset', 'training', 'concurrency', 'equal', 'figure', 'show', 'learn', 'curve', 'scenario', 'adaptive', 'workload', 'scheduling', 'timelyfl', 'save', '409×', 'convergence', 'time', 'accuracy', 'test', 'accuracy', 'increment', 'illustrate', 'workload', 'scheduling', 'essentially', 'improve', 'learn', 'efficiency', 'accuracy', 'timelyfl', 'effective', 'lightweight', 'model', 'investigate', 'effectiveness', 'lightweight', 'model', 'timelyfl', 'framework', 'implement', 'lightweight', 'model', 'speech', 'command', 'dataset', 'keyword', 'spot', 'task', 'follow', 'previous', 'work', 'choose', 'model', 'consist', 'convolution', 'lie', 'er', 'follow', 'gate', 'recurrent', 'unit', 'gru', 'layer', 'average', 'pooling', 'layer', 'connect', 'gru', 'output', 'feed', 'dense', 'layer', 'generate', 'prediction', 'parameter', 'size', 'model', 'equal', 'adopt', 'baseline', 'selection', 'tion', 'hyperparameter', 'experiment', 'list', 'section', 'a13', 'experiment', 'result', 'summarize', 'table', 'timelyfl', 'achieve', 'high', 'convergence', 'speed', 'compare', 'strategy', 'reach', 'test', 'accuracy', 'confirm', 'simulation', 'result', 'rate', 'section', 'demonstrate', 'effectiveness', 'timelyfl', 'lightweight', 'model', 'architecture', 'figure', 'timetoaccuracy', 'performance', 'different', 'noniid', 'distribution', 'figure', 'timelyfl', 'performance', 'adaptive', 'non', 'adaptive', 'workload', 'schedule', 'aggregator', 'cifar10', 'dataset', 'use', 'noniid', 'parti', 'tion', 'show', 'figure', 'parameter', 'dirich', 'let', 'distribution', 'go', 'convergencetime', 'gap', 'tween', 'timelyfl', 'fedbuff', 'increase', 'well', 'demonstrate', 'advantage', 'noniid', 'datum', 'training', 'com', 'pare', 'fedbuff', 'timelyfl', 'heterogeneityaware', 'cross', 'device', 'federate', 'learn', 'participate', 'entity', 'limit', 'computing', 'capability', 'intermittent', 'connectivity', 'guarantee', 'device', 'complete', 'training', 'workload', 'communication', 'round', 'assign', 'initially', 'effectively', 'resist', 'turbance', 'training', 'hyperparameter', 'par', 'conclusion', 'work', 'propose', 'timelyfl', 'heterogeneity', 'aware', 'asynchronous', 'scheme', 'adaptive', 'partial', 'train', 'include', 'available', 'device', 'join', 'gregation', 'timely', 'manner', 'timelyfl', 'introduce', 'par', 'tial', 'model', 'training', 'slowspeed', 'device', 'moreover', 'timelyfl', 'resilient', 'system', 'heterogeneity', 'adjust', 'local', 'training', 'workload', 'base', 'realtime', 'resource', 'capability', 'client', 'training', 'exper', 'imental', 'result', 'demonstrate', 'outper', 'form', 'major', 'asyncfl', 'proposal', 'term', 'timeto', 'accuracy', 'test', 'accuracy', 'reference', 'reddit', 'comment', 'datum', 'https', 'filespushshift', 'ioredditcomment', 'samiul', 'modelheterogeneous', 'federate', 'learning', 'rolling', 'submodel', 'extraction', 'conference', 'neural', 'formation', 'processing', 'system', 'dmitrii', 'erate', 'learn', 'arbitrary', 'communication', 'pattern', 'icml', 'nanqe', 'understand', 'impact', 'staleness', 'tribute', 'machine', 'learn', 'arxiv', 'abs181003264', 'giladi', 'hoffer', 'soudry', 'stability', 'edge', 'adjust', 'hyperparame', 'ter', 'preserve', 'selection', 'asynchronous', 'training', 'neural', 'network', 'arxiv', 'abs190912340', 'chaoyang', 'songze', 'jinhyun', 'annavaram', 'fedml', 'research', 'library', 'avestimehr', 'arxiv', 'benchmark', 'federate', 'machine', 'learn', 'kaiming', 'shaoqe', 'ren', 'deep', 'residual', 'learning', 'image', 'recognition', 'ieee', 'ence', 'computer', 'vision', 'pattern', 'recognition', 'cvpr', 'page', 'birjodh', 'mobiperf', 'mobile', 'network', 'measurement', 'system', 'technical', 'report', 'crosoft', 'research', 'dzmitry', 'pavel', 'ustinov', 'harish', 'srinivas', 'jesik', 'papaya', 'practical', 'private', 'scalable', 'federate', 'learning', 'arxiv', 'abs211104877', 'ignatov', 'radu', 'timofte', 'andrei', 'baum', 'lirong', 'luc', 'ai', 'benchmark', 'deep', 'learning', 'smartphone', 'ieeecvf', 'ence', 'computer', 'vision', 'page', 'mnn', 'univer', 'sal', 'efficient', 'inference', 'engine', 'arxiv', 'abs200212418', 'brendan', 'brendan', 'bellet', 'mehdi', 'advance', 'open', 'problem', 'federate', 'learn', 'ing', 'foundation', 'trend', '®', 'machine', 'learn', 'learn', 'multiple', 'layer', 'feature', 'tiny', 'image', 'fedscale', 'benchmarking', 'model', 'system', 'mance', 'federate', 'learning', 'proceeding', 'first', 'work', 'shop', 'system', 'challenge', 'reliable', 'secure', 'feder', 'ate', 'learn', 'fan', 'madhyastha', 'chowdhury', 'oort', 'efficient', 'federate', 'learning', 'guide', 'participant', 'selection', 'osdi', 'sebastian', 'piyush', 'sharma', 'radu', 'soricut', 'lite', 'bert', 'selfsupervise', 'learning', 'language', 'representation', 'arxiv', 'abs190911942', 'salman', 'ave', 'timehr', 'layerwise', 'adaptive', 'model', 'aggregation', 'scalable', 'federate', 'learning', 'arxiv', 'abs211010302', 'chenne', 'pyra', 'midfl', 'finegraine', 'client', 'selection', 'framework', 'cient', 'federate', 'learn', 'international', 'conference', 'mobile', 'computing', 'network', 'straggler', 'disaster', 'hybrid', 'federate', 'learn', 'lay', 'gradient', 'arxiv', 'son', 'blaise', 'communicationefficient', 'learning', 'deep', 'network', 'decentralized', 'datum', 'tat', 'zafar', 'ayyub', 'qazi', 'fedprune', 'inclusive', 'federate', 'learn', 'arxiv', 'abs211014205', 'ashkan', 'malek', 'federate', 'learn', 'buffer', 'asynchronous', 'aggre', 'gation', 'arxiv', 'sashank', 'j', 'rush', 'jakub', 'adaptive', 'federated', 'optimization', 'arxiv', 'abs200300295', 'jae', 'breiner', 'lara', 'mingqe', 'mathew', 'scale', 'language', 'model', 'size', 'cross', 'device', 'federate', 'learn', 'arxiv', 'abs220409715', 'hakim', 'sidahme', 'garg', 'mingqe', 'efficient', 'private', 'federate', 'learning', 'partially', 'trainable', 'network', 'arxiv', 'abs211003450', 'deep', 'lutional', 'network', 'largescale', 'image', 'recognition', 'saeed', 'vahidian', 'kungurtsev', 'curricula', 'work', 'federate', 'learn', 'arxiv', 'abs221212712', 'brendan', 'datum', 'field', 'guide', 'federate', 'optimization', 'arxiv210706917', 'arxiv', 'preprint', 'pete', 'warden', 'speech', 'command', 'dataset', 'limited', 'arxiv', 'abs180403209', 'vocabulary', 'speech', 'weiwei', 'carsten', 'maple', 'stephen', 'semiasynchronous', 'protocol', 'fast', 'federate', 'learning', 'low', 'overhead', 'ieee', 'transaction', 'computer', '70655–668', 'cong', 'gupta', 'asyn', 'chronous', 'federate', 'optimization', 'arxiv', 'abs190303934', 'tienju', 'franccoise', 'beaufay', 'motta', 'partial', 'variable', 'training', 'efficient', 'ondevice', 'federate', 'learning', 'arxiv', 'samiul', 'avestimehr', 'federate', 'learning', 'benchmark', 'audio', 'task', 'arxiv', 'abs221015707', 'avestimehr', 'federate', 'learning', 'internet', 'thing', 'application', 'challenge', 'oppor', 'ieee', 'internet', 'thing', 'magazine', 'ieee', 'iotm', 'tunitie', 'tianshya', 'avestimehr', 'federate', 'learning', 'internet', 'thing', 'proceeding', '19th', 'acm', 'conference', 'embed', 'net', 'worked', 'sensor', 'system', 'yinyu', 'tribute', 'asynchronous', 'optimization', 'unbounded', 'delay', 'slow', 'go', 'icml', 'appendix', 'experiment', 'setting', 'a11', 'compute', 'infrastructure', 'simulation', 'experiment', 'conduct', 'compute', 'server', 'server', 'equip', 'processor', 'g', 'memory', 'rtx', 'a12', 'dataset', 'model', 'ai', 'benchmark', 'benchmark', 'public', 'dataset', 'design', 'evaluate', 'performance', 'ai', 'task', 'mobile', 'device', 'ai', 'benchmark', 'provide', 'diverse', 'model', 'training', 'inference', 'speed', 'vari', 'ous', 'device', 'include', 'chipset', 'mediatek', 'unisoc', 'figure', 'illustrate', 'distribution', 'computation', 'efficiency', 'client', 'benchmark', 'slow', 'device', 'take', 'computational', 'time', 'fast', 'device', 'task', 'approach', 'dynamic', 'availability', 'device', 'lowpower', 'mode', 'multiprocess', 'run', 'sign', 'coefficient', 'w', 'follow', 'n', '\uf8f4\uf8f2', '≤', '≤', '\uf8f4\uf8f3', 'w', 'work', 'assign', 'value', 'benchmark', 'base', 'computation', 'time', 'client', 'emulate', 'real', 'device', 'analogous', 'usage', 'fedscale', 'also', 'generate', 'coefficient', 'round', 'client', 'simulate', 'natural', 'disturbance', 'availability', 'local', 'computation', 'time', 'round', 'equal', 'product', 'base', 'computation', 'time', 'client', 'mobiperf', 'mobiperf', 'public', 'dataset', 'measure', 'network', 'performance', 'mobile', 'device', 'collect', 'available', 'cloudtoedge', 'network', 'throughput', 'worldwide', 'mobile', 'client', 'figure', 'illustrate', 'distribu', 'tion', 'communication', 'consumption', 'mobiperf', 'note', 'good', 'communication', 'channel', '200×', 'well', 'bad', 'randomly', 'assign', 'value', 'mobiperf', 'simulated', 'device', 'communication', 'round', 'emulate', 'intermittent', 'connectivity', 'real', 'deployment', 'cifar10', 'cifar10', 'consist', 'colour', 'image', 'class', 'training', 'image', 'test', 'image', 'normalize', 'image', 'mean', 'standard', 'deviation', 'dataset', 'evaluate', 'dataset', 'resnet20', 'model', 'emulate', 'realistic', 'noniid', 'distribution', 'partition', 'dataset', 'use', 'dirichlet', 'distribution', 'follow', 'previ', 'ous', 'work', 'command', 'command', 'speech', 'dataset', 'cover', 'audio', 'recording', 'collect', 'client', 'training', 'set', 'include', 'recording', 'speaker', 'validation', 'set', 'include', 'speaker', 'test', 'set', 'include', 'speaker', 'datum', 'set', 'com', 'pose', 'common', 'word', 'everyday', 'vocabulary', 'evaluate', 'dataset', 'vgg11', 'model', 'lightweight', 'model', 'base', 'relate', 'work', 'keyword', 'spot', 'te', 'task', 'vgg11based', 'experiment', 'speech', 'command', 'use', 'melfrequency', 'cepstral', 'coefficient', 'mfcc', 'method', 'preprocess', 'raw', 'audio', 'datum', 'specif', 'ically', 'sequence', 'overlap', 'hamming', 'window', 'ply', 'raw', 'speech', 'signal', 'time', 'shift', 'ms', 'window', 'size', '25ms', 'mfcc', 'use', 'train', 'keyword', 'spot', 'model', 'lightweight', 'model', 'experiment', 'preprocess', 'raw', 'audio', 'datum', 'sequence', 'overlap', 'hamming', 'window', 'apply', 'raw', 'speech', 'signal', 'time', 'shift', 'ms', 'calculate', 'discrete', 'fouri', 'tran', 'form', 'dft', 'frame', 'length', 'compute', 'melspectrogram', 'dimension', 'use', 'train', 'keyword', 'spot', 'model', 'follow', 'setup', 'reddit', 'reddit', 'consist', 'comment', 'user', 'reddit', 'forum', 'client', 'correspond', 'user', 'datum', 'personal', 'post', 'thus', 'follow', 'real', 'noniid', 'datum', 'scenario', 'dataset', 'filter', 'user', 'less', 'word', 'total', 'restrict', '30k', 'frequently', 'use', 'word', 'setting', 'previous', 'work', 'train', 'lightweight', 'model', 'nextwordprediction', 'task', 'performance', 'evaluate', 'perplexity', 'loss', 'low', 'well', 'hyperparameter', 'setting', 'search', 'client', 'learning', 'rate', 'range', 'server', 'learning', 'rate', 'range', 'input', 'batch', 'size', 'range', 'total', 'training', 'round', 'range', 'gation', 'goal', 'aggregation', 'participation', 'target', 'search', 'training', 'concurrency', 'round', 'fedbuff', 'timelyfl', 'respectively', 'hyperparameter', 'search', 'fix', 'follow', 'ing', 'hyperparameter', 'cifar10', 'related', 'experiment', 'total', 'training', 'round', 'training', 'concurrency', 'setup', 'aggregation', 'goal', 'aggregation', 'participation', 'target', 'training', 'concurrency', 'fedbuff', 'timelyfl', 'cifar10', 'davg', 'relate', 'experiment', 'batch', 'size', 'client', 'learning', 'rate', 'cifar10', 'relate', 'ex', 'figure', 'partial', 'training', 'system', 'performance', 'real', 'edge', 'vice', 'a2', 'system', 'performance', 'a21', 'partial', 'training', 'performance', 'different', 'parameter', 'tensor', 'shape', 'dif', 'ferent', 'layer', 'training', 'time', 'computational', 'time', 'forward', 'backward', 'propagation', 'strictly', 'linear', 'trainable', 'layer', 'number', 'vary', 'model', 'ture', 'simplicity', 'generality', 'define', 'training', 'time', 'partial', 'model', 'linear', 'multiplication', 'training', 'time', 'full', 'model', 'training', 'ratio', 'linear', 'relationship', 'verify', 'real', 'measurement', 'galaxy', 's20', 'resnet20', 'model', 'use', 'library', 'show', 'figure', 'test', 'result', 'linear', 'straight', 'line', 'ratio', 'justify', 'rationality', 'choice', 'diverse', 'computation', 'efficiency', 'b', 'diverse', 'communicate', 'efficiency', 'mobiperf', 'figure', 'heterogeneous', 'system', 'utility', 'simulate', 'client', 'periment', 'batch', 'size', 'client', 'learning', 'rate', 'server', 'learning', 'rate', 'server', 'optimizer', 'command', 'relate', 'experiment', 'vgg11', 'model', 'total', 'training', 'round', 'train', 'con', 'currency', 'setup', 'aggregation', 'goal', 'gregation', 'participation', 'target', 'training', 'concur', 'rency', 'fedbuff', 'timelyfl', 'batch', 'size', 'client', 'learning', 'rate', 'fedopt', 'server', 'learning', 'rate', 'command', 'relate', 'experiment', 'lightweight', 'model', 'total', 'training', 'round', 'training', 'concurrency', 'setup', 'aggrega', 'tion', 'goal', 'aggregation', 'participation', 'target', 'training', 'concurrency', 'fedbuff', 'timelyfl', 'batch', 'size', 'client', 'learning', 'rate', 'fedavg', 'fedopt', 'client', 'learning', 'rate', 'synchrounous', 'fl', 'timelyfl', 'client', 'learning', 'rate', 'fedbuff', 'server', 'learning', 'rate', 'server', 'optimizer', 'setup', 'finally', 'reddit', 'relate', 'experiment', 'total', 'training', 'round', 'training', 'concurrency', 'setup', 'aggregation', 'goal', 'aggregation', 'participation', 'target', 'training', 'concurrency', 'fedbuff', 'timelyfl', 'batch', 'size', 'client', 'learning', 'rate', 'syncfl', 'timelyfl', 'fedbuff', 'fedopt', 'server', 'learning', 'rate', 'server', 'optimizer']"
"TimelyFL: Heterogeneity-aware Asynchronous Federated Learning with
  Adaptive Partial Training","[{'href': 'http://arxiv.org/abs/2304.06947v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2304.06947v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-04-14 06:26:08,"The Night Shift: Understanding Performance
Variability of Cloud Serverless Platforms
Nils Japke
TU Berlin & ECDF
Berlin, Germany
nj@mcc.tu-berlin.de

Trever Schirmer
TU Berlin & ECDF
Berlin, Germany
ts@mcc.tu-berlin.de

Sofia Greten
TU Berlin & ECDF
Berlin, Germany
sog@mcc.tu-berlin.de

3
2
0
2

r
p
A
4
1

]

C
D
.
s
c
[

1
v
7
7
1
7
0
.
4
0
3
2
:
v
i
X
r
a

Tobias Pfandzelter
TU Berlin & ECDF
Berlin, Germany
tp@mcc.tu-berlin.de

David Bermbach
TU Berlin & ECDF
Berlin, Germany
db@mcc.tu-berlin.de

ABSTRACT
Function-as-a-Service is a popular cloud programming model that
supports developers by abstracting away most operational concerns
with automatic deployment and scaling of applications. Due to the
high level of abstraction, developers rely on the cloud platform to
offer a consistent service level, as decreased performance leads to
higher latency and higher cost given the pay-per-use model. In
this paper, we measure performance variability of Google Cloud
Functions over multiple months. Our results show that diurnal
patterns can lead to performance differences of up to 15%, and
that the frequency of unexpected cold starts increases threefold
during the start of the week. This behavior can negatively impact
researchers that conduct performance studies on cloud platforms
and practitioners that run cloud applications.

CCS CONCEPTS
• Computer systems organization → Cloud computing; • Ap-
plied computing → Service-oriented architectures; • Software
and its engineering → Software performance; • Networks →
Cloud computing.

KEYWORDS
Serverless Computing, Function as a Service, Performance Varia-
tion, Resource Contention

ACM Reference Format:
Trever Schirmer, Nils Japke, Sofia Greten, Tobias Pfandzelter, and David
Bermbach. 2023. The Night Shift: Understanding Performance Variability
of Cloud Serverless Platforms. In The 1st Workshop on SErverless Systems,
Applications and MEthodologies (SESAME ’23), May 8, 2023, Rome, Italy. ACM,
New York, NY, USA, 7 pages. https://doi.org/10.1145/3592533.3592808

1 INTRODUCTION
Function-as-a-Service (FaaS) is a serverless cloud computing de-
livery model where developers compose their applications from
event-driven stateless functions and all operational tasks are man-
aged by the cloud provider [5, 17, 23]. Functions are billed on a

SESAME ’23, May 8, 2023, Rome, Italy
© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
This is the author’s version of the work. It is posted here for your personal use. Not for
redistribution. The definitive Version of Record was published in The 1st Workshop on
SErverless Systems, Applications and MEthodologies (SESAME ’23), May 8, 2023, Rome,
Italy, https://doi.org/10.1145/3592533.3592808.

pay-per-use basis at second or even microsecond granularity and
offer rapid elasticity and scale [5, 11, 23]. The abstraction from
operational concerns has made FaaS a popular cloud execution
model, with offerings by all major cloud providers, e.g., Amazon
Web Services Lambda1 and Google Cloud Functions2 [2, 5, 11].

The flip side of high levels of resource sharing in the cloud and
abstracting from resource management is that developers must
rely on the cloud platform provider to offer stable and consistent
performance. Somewhat counterintuitively, FaaS users must actu-
ally pay more when a FaaS platform underperforms and latency
is higher, as billed function execution is also longer [2, 36]. Most
cloud FaaS platforms do not offer service level agreements beyond
limited guarantees regarding general uptime [1, 21, 44].

Cloud computing is subject to performance variations [7, 33], and
FaaS is no exception, as previous studies on the long-term (day-to-
day) performance changes of FaaS platforms have shown [16, 27, 40].
A general improvement of FaaS services over time is expected as
platform providers update and advance there infrastructure [42],
yet there are also much finer effects in the short term.

In this paper, we benchmark and analyze these effects using
highly frequent (every 40s) cloud FaaS benchmarks over the course
of two months against Google Cloud Functions (GCF), which has
received fewer attention in previous studies. Our main finding is
that performance varies greatly during the course of the day, with
an increase of request-response latency of up to 15% and more
than three times as many unexpected cold starts from day to night
within the same day, and that these effects are most noticeable at
the start of the week. These findings impact how we interpret the
results of cloud FaaS benchmarks and are significant for cloud FaaS
developers. In summary, we make the following contributions:

• Based on a number of existing serverless benchmarks, we
propose a methodology for evaluating temporal performance
variations in cloud FaaS platforms (§3).

• We execute our benchmark with frequent (every 40s) runs
on Google Cloud Functions over the course of two months
and show how request-response latency exhibits strong (up
to 15% difference) diurnal variation (§4.1).

• We further uncover an increase in unexpected cold starts at
specific times of the day that suggest increased rates of in-
stance recycling in GCF during times of high demand (§4.2).

1https://aws.amazon.com/lambda/
2https://cloud.google.com/functions/

 
 
 
 
 
 
SESAME ’23, May 8, 2023, Rome, Italy

Trever Schirmer, Nils Japke, Sofia Greten, Tobias Pfandzelter, and David Bermbach

• We evaluate long-term trends in our data and identify possi-

ble causes (§4.3).

• We survey existing cloud FaaS benchmarking studies and
find that almost two thirds provide insufficient information
on how the performance variability effects we identify are
controlled for (§5.1).

• We discuss implications of our findings for practitioners that

run applications on cloud FaaS platforms (§5.2).

In order to enable other researchers and practitioners to extend
and replicate our experiments, we make the artifacts used to pro-
duce this paper available as open-source3.

2 RELATED WORK
Existing research on FaaS performance variability focuses on ei-
ther short-term or long-term (i.e., more than a week) variability.
Short-term studies include a report by Lambion et al. [27], who
have benchmarked the performance of various functions on AWS
Lambda over the course of a day. The authors execute their func-
tions in different time zones and using different hardware archi-
tectures, and find a 6% shorter function duration during the night.
Mahmoudi et al. [29] propose an analytical performance model to
predict performance metrics of functions. To validate their model,
they repeat the same one-hour experiment ten times, and find that
request arrival rate, average response times, and function timeout
can be used to predict performance up to five minutes in advance.
Ginzburg and Freedman [20] analyze performance variations on
AWS Lambda over the course a week. They call 1000 functions
every two hours and show that the daily performance of the same
function inside the same region and between regions can vary sig-
nificantly, which is mainly caused by local inactivity and lack of
performance isolation between tenants. Since they only measure
for one week, their analysis focuses on daily variations, which they
measure at 1-2%.

A long-term study of serverless systems is presented by Eismann
et al. [16], who have executed the same serverless application on
Lambda once a day over ten months. They find long-term perfor-
mance changes that are likely to be caused by platform changes, and
short-term variations between days. Figelia et al. [19] measure the
performance of various functions running on Lambda over seven
months, but do not analyze their dataset for regular variability.

The focus of our paper is to close the gap between long-term
and short-term studies by collecting frequent measurements over
a longer period of time. Additionally, we go beyond the focus on
AWS Lambda and present a general methodology for experiments
that are applicable to all FaaS platforms. We collect our results on
Google Cloud Functions, which has received fewer attention in
previous studies despite its popularity.

3 METHODOLOGY
To assess performance variations in FaaS, we repeatedly execute
a FaaS function in short intervals across a large time span. By
controlling for execution region, resource parameters (through the
memory option on cloud FaaS providers), and function type, we
can focus on cloud platform performance.

3https://github.com/umbrellerde/night-shift-code

Functions. We use three FaaS functions from existing serverless
benchmarks in our experiments. All functions perform isolated
computations that do not rely on external services, the performance
of which could influence our results [22]. The float workload of
Kim et al. [26] that performs floating point operations. The matrix
function of Werner et al. [41] that performs matrix multiplication.
Finally, we adapt the face detection model of Barosum et al. [4] for
the ml function. To minimize the impact of external fluctuations on
our measurements, we embed all inputs directly into the functions.
The resources available to a function instance are determined
by the memory configured for that function: On GCF, the amount
of vCPUs allocated to a function instance is tiered and increases
with every multiple of 128MB memory, while AWS Lambda scales
vCPUs linearly with memory [14]. To capture effects of resource
configurations, we deploy float and matrix with 128MB, 256MB, and
512MB of memory, while ml is deployed with 512MB and 1024MB
as it has higher resource requirements.

Execution. Cloud function invocations can be both “warm” and
“cold”: When a function is invoked for the first time, a new function
instance is created. This is called a “cold start” and incurs a creation
overhead [3, 6, 31]. Subsequent (but possibly not parallel) invoca-
tions of the same function can reuse the existing instance and avoid
this overhead, the “warm starts”. Typically, cloud platforms will
keep existing instances for future invocations for a limited amount
of time and then evict them to reclaim resources [13].

To capture both cold and warm start latencies, we invoke func-
tions in loops: We first call the function once, creating a cold start,
and then call the function again. In theory, this second invocation
should be served by the existing function instance. We then wait 20
minutes to make sure that the next function call is a cold start again
and restart the same loop. To collect more measurement points than
twice every 20 minutes, we deploy parallel copies of a function that
we cycle through.

Metrics. We consider three main metrics: request-response latency,
unexpected cold starts, and long-term trends. For request-response
latency, we use the billed duration that is output for every func-
tion execution by the FaaS platform. Unexpected cold starts are
cold starts that occur directly after a function has already been
called once, so that they should be warm. Unexpected cold starts
imply that a platform was unable to find a warm function instance,
possibly because it has been evicted due to resource contention.

Finally, we conduct a seasonal trend decomposition using LOESS
(STL) [12], which can handle complex seasonal patterns. We use
the following model for our data:

𝑦𝑡 = 𝑇𝑡 + 𝑆𝑡 + 𝐼𝑡

, where 𝑇𝑡 represents the trend, 𝑆𝑡 represents a seasonal compo-
nent (days in our case), and 𝐼𝑡 represents the remaining noise. We fit
our data to this model using a method to create smoothed estimates
with a seasonality of one day. The trend component 𝑇𝑡 shows the
overall progress in the billed duration over the whole duration of
the experiment. A non-flat trend line indicates that longer-term
changes to the platform have occurred, e.g., long-term seasonal
changes or updates to the platform that influenced performance.
The seasonal component 𝑆𝑡 shows periodic recurring deviations in

The Night Shift: Understanding Performance Variability of Cloud Serverless Platforms

SESAME ’23, May 8, 2023, Rome, Italy

Figure 1: Billed Duration of warm calls to float with 128MB
memory. During working hours, the billed duration in-
creases by up to 15%. The area around the dashed line shows
the 95% confidence interval.

the data from the trend. The 𝐼𝑡 component is random noise centered
around 0. Outliers in the noise indicate that the performance at spe-
cific times could not be explained by the previous two components.
The trend component from the STL is also used for Change Point
Detection [39], which can detect structural changes in data.

4 EXPERIMENTS
In this section, we present initial results of our experiments with
Google Cloud Functions in the europe-west3 region. The measure-
ment period started on Dec 12, 2022 and ended on Feb 27, 2023.
We report all execution times in local time (CET). We first analyze
the performance variability of the platform (§4.1). Afterwards, we
explore unexpected cold starts (§4.2) and outliers as well as change
points and long-term trends in our data (§4.3).

4.1 Performance Variability
To analyze performance variability, we analyze the billed duration
of comparable invocations. We show the billed duration of the
float function with 128MB memory in Figure 1. We observe a clear
performance increase during the night, with a noticeable latency
spike during working hours. The average billed duration between
23:00 and 06:00 was 106ms, and increased by 15% to 122ms between
07:00 and 16:00. When aggregated by the day of the week, the
average billed duration fluctuates between 113.86ms on Saturdays
and 117.28ms on Mondays. Overall, billed duration is slightly lower
on the average weekend compared to the start of the week. The
weekly trend is much smaller than the daily trend, as the billed
duration only decreases by ~4% during the weekend.

When looking at larger memory sizes, the relative performance
change over time becomes smaller. As shown in Figure 2, the
float function with 128MB memory changes ~10% during a day,
while 512MB only changes up to 5%. The ml functions with 512MB
and 1024MB memory equally changed ~4% during an average day.
Noticeably, the average billed duration of the float function with
256MB of memory differed more than 15% during a day. This can

(a) float

(b) ml

Figure 2: Performance change over a day of warm instances
ordered by memory size. The y-Axis is normalized to the av-
erage and shows the relative change, e.g., bigger values show
a bigger deviation from the average billed duration. The area
around the curves shows the 95% confidence interval.

be explained by looking at the distribution of latency values over
all invocations (Figure 3): Around 50% of functions with a mem-
ory size of 256MB follow the same distribution as functions with
512MB of memory, and the other half follows the same distribu-
tion as the 128MB functions. The results for the matrix function
exhibit similar results but are omitted due to space constraints. The
high variability in performance and uneven distribution of billed
durations indicates that GCF internally uses 128MB and 512MB
function instances to handle requests to the 256MB functions, as
also shown by Malawski et al. [30].

While cold start durations also follow the daily patterns shown
in §4.1, the configured memory size has no impact on the duration
of cold starts. The billed durations of cold starts follow a normal dis-
tribution and are on average around 9-10x longer than the average
warm latency, but all memory sizes follow the same distribution.

125

120

115

110

105

]
s

m

[

n
o
i
t
a
r
u
D
d
e
l
l
i

B

0

5

10
Hour of Day

15

20

 
 
Memory
128
256
512

m
o
r
f

n
o
i
t
a
i
v
e
D
e
v
i
t
a
l
e
R

n
o
i
t
a
r
u
D
d
e
l
l
i

B
n
a
e
M

20.0%

10.0%

0.0%

−10.0%

−20.0%

0

5

10

15

20

Hour of Day

 
 
 
 
 
Memory
512
1024

m
o
r
f

n
o
i
t
a
i
v
e
D
e
v
i
t
a
l
e
R

n
o
i
t
a
r
u
D
d
e
l
l
i

B
n
a
e
M

20.0%

10.0%

0.0%

−10.0%

−20.0%

0

5

10

15

20

Hour of Day

 
 
 
 
 
SESAME ’23, May 8, 2023, Rome, Italy

Trever Schirmer, Nils Japke, Sofia Greten, Tobias Pfandzelter, and David Bermbach

Figure 3: Cumulative distribution of billed durations of the
float function without cold starts. We argue that GCF uses
128MB and 512MB containers to execute 256MB functions.

Figure 5: Trend Component of the STL, with Change Points
and Outliers marked as horizontal lines.

4.3 Outliers & Long-Term Trend
Based on the STL introduced in §3, we show change points and
outliers in Figure 5. We define an outlier as every hour during which
the average execution duration is outside the fourth interquartile
range, i.e., more than four times the difference between the first
and third quartile, away from the average. Our data contains four
outliers, which were all within three days of the turn of the month.
This indicates to us that the platform is under unusual load at these
times, possibly due to additional load from monthly jobs.

All change points, i.e., points when the average execution du-
ration changed, occurred during the night, indicating that they
coincide with scheduled updates to the platform.

Over our whole measurement period, there is no clear permanent
trend towards better or worse overall performance. Based on the
long-term study by Eismann et al. [16], we only expect to find
permanent trends in longer measurement periods. Compared to
the authors’ study of AWS, which finds statistical trends below 10%,
our trend component changes between 105ms during December to
128ms during February, a 21% increase.

5 IMPLICATIONS
In this section, we discuss how our findings impact serverless sys-
tems. First, we focus on the implications for benchmarking. After-
wards, we describe implications on serverless applications.

Figure 4: Relative frequency of unexpected cold starts, aver-
aged by the hour of the week they happened in. On Mondays
during the day, up to 13% of invocations can be unexpected
cold starts, compared to less than 5% during the night and
on weekends. The area around the curve shows the 95% con-
fidence interval.

This indicates that the cold start overhead is dependent on resources
that can be configured by changing the function configuration.

4.2 Unexpected Cold Starts
We show the relative frequency of unexpected cold starts in Fig-
ure 4. While the billed duration of warm instances seems to only
follow a daily trend, the frequency of unexpected cold starts has a
weekly seasonality, with clear trends of increased cold starts during
working hours. On average, the frequency of unexpected cold starts
was 3.7% during the night (20:00—08:00), 3.6% during the weekend,
9.8% during working hours (09:00—17:00 Mon—Fri), and 12.3% dur-
ing working hours on Monday. For comparison, there were less
than 0.15% unexpected warm starts, where a function instance was
still warm after more than 20 minutes.

5.1 Validity of Benchmarks
When running benchmarks, researchers want to minimize effects of
external factors to their measurements, which otherwise might con-
found results [9]. In the case of benchmarking serverless systems,
we have shown that the number of cold starts and the performance
of functions undergoes changes within a single day. For compara-
tive performance studies, benchmark results are only comparable
if experiments are conducted at comparable times. If an experi-
ment is short, i.e., does not capture the performance variation of
an entire day, daily variations can skew results. Similarly, long-
term performance changes can impact measurements taken over

n
o
i
t
u
b
i
r
t
s
i
D
e
v
i
t
a
u
m
u
C

l

l

a
c
i
r
i
p
m
E

1.0

0.8

0.6

0.4

0.2

0.0

Memory Size

128
256
512

0

50

100
200
150
Billed Duration [ms]

250

300

 
 
f
o

y
c
n
e
u
q
e
r
F

e
v
i
t
a
l
e
R

s
t
r
a
t
S

l

d
o
C

d
e
t
c
e
p
x
e
n
U

15.0%

12.5%

10.0%

7.5%

5.0%

2.5%

0.0%

0

25

50

75

100
Hour of Week

125

150

 
 
 
 
STL Trend
Change Point
Outlier

n
o
i
t
a
r
u
D
d
e
l
l
i

B

]
s

m

[

t
n
e
n
o
p
m
o
C

d
n
e
r
T

130

125

120

115

110

105

5

c   1

e

D

1

n   0

a

J

5

n   1

a

J

1

b   0

e

F

5

b   1

e

F

Function Start

 
 
 
The Night Shift: Understanding Performance Variability of Cloud Serverless Platforms

SESAME ’23, May 8, 2023, Rome, Italy

5.2 Application Performance
The performance variability that we have shown for GCF affects
serverless applications in several ways: During daytime, functions
have increased latency, suffer more cold starts, and their execu-
tion cost is increased due to the pay-by-second billing model. For
low-latency, event-driven functions, it is not feasible to postpone
their execution to the night or a weekend to decrease costs. A
possible way forward, however, is to shift function execution to
another cloud region with better performance. This may increase
network latency and transmission costs, but an up to 20% reduc-
tion on function execution times and the associated decrease in
costs can outweigh this overhead for long(er)-running functions.
Such an approach requires constant evaluation of FaaS platform
performance in different regions, possibly also based on application
metrics [8].

Researchers have also proposed systems that adapt FaaS appli-
cations to improve performance and cost on cloud FaaS platforms.
Such systems rely on initial performance measurements of applica-
tions on cloud platforms [14, 15, 18, 24] or a feedback loop between
platform, application, and optimizer [36]. Both approaches are af-
fected by performance variability of the FaaS platform, as the opti-
mizer or model cannot differentiate between performance changes
that are caused by deployment updates and those that are caused
by platform instability. A possible way forward for these systems is
to control for temporal performance variations, e.g., by deploying
multiple parameter sets concurrently or performing longer initial
measurements.

6 LIMITATIONS & FUTURE WORK
We have shown considerable performance variability in Google
Cloud Functions and discussed how these affect applications and
performance measurement research. We plan to build on this initial
work in the future to arrive at a more holistic view of performance
variability in cloud FaaS platforms.
FaaS Platforms. Our initial experiments are limited mostly to GCF,
with some additional validation on AWS Lambda. Although we have
seen that in our experiments, Lambda suffers from less performance
variability than GCF, parameters such as memory size, hardware
architecture, geographical region, or programming language could
further influence variability. We plan to conduct additional exper-
iments on different FaaS platforms in the future, controlling for
these additional parameters.
Platform Changes. FaaS platforms are evolving quickly, and our
measurements and experiments can only capture the behavior of
such a platform at a specific point in time. Continuous updates could
increase or even eliminate the performance variability effects we
observe in the future, making continuous measurements important.
Subsequently, researchers that want to account for the described
behavior in their own measurements on FaaS platforms should
conduct their own experiments using our methodology, as our
measurement results may be outdated by then.
Performance Dimensions. The functions we use in our experiments
are CPU-bound, which gives a good indication for general platform
performance and minimizes the impact of the performance of ex-
ternal services. Beyond CPU performance, other resource metrics

Figure 6: Execution Times of the 110.dynamic-html Bench-
mark on GCF and Lambda (excerpt). The minimum and
maximum measured difference of the whole experiment are
marked as horizontal lines.

longer periods of time, e.g., a three-day study with one group bench-
marked over the weekend and another benchmarked during the
week. A possible remediation for such experiments is adopting
parallel benchmarking techniques such as duet benchmarking [10].
We show an overview of existing publications on cloud FaaS
performance measurements surveyed for this paper in Table 1.
For every paper, we give an overview of cloud platforms under
test, cloud regions, and time of day of the benchmark execution, if
stated. Based on this metadata, we must assume that published re-
sults could be affected by daily performance variations if execution
time is not given or different regions are used (implying different
time zones). Overall, 10 out of a total of 16 papers do not provide
sufficient information to rule out effects of performance variability.
While this does not mean that the reported results are invalid, it
shows that the research community has not paid enough attention
to these effects in performance measurements.

As an example, we replicate an experiment from Copik et al. [13]
that compares the execution time of a dynamic HTML generator
(110.dynamic-html) between AWS Lambda and GCF. We deploy
this function in the eu-central-1 (Lambda) and europe-west3
(GCF) regions with 128MB of memory and run 50 sequential in-
vocations every hour over the course of five days. As shown in
Figure 6, the performance on GCF exhibits temporal variations
that can skew results: The smallest performance difference during
our experiments happened on the 23rd of February 2023 at mid-
night, when the average performance difference was <2.8s (GCF
52% slower). Shortly after, at 09:00, we observe the largest perfor-
mance difference with the average difference increasing to >25s
(GCF 6.8× slower). These performance changes show that short-
term variation in performance between serverless platforms can
have a significant impact on benchmarking results and need to
be controlled for. We recommend repeating experiments over the
course of a day and mentioning execution time when describing
experiment setup.

GCF
AWS

Max Difference
Min Difference

]
s
[

i

e
m
T
n
o
i
t
u
c
e
x
E

40

30

20

10

0

0

0 : 0

0

0

2 : 0

0

0

0

0

0

0

8 : 0

6 : 0

8 : 0
4 : 0
1
1
Time of Execution (Feb 23, 2023)

2 : 0

0 : 0

4 : 0

6 : 0

0

1

1

1

0

0

0

0

0

0

0 : 0

2

 
 
SESAME ’23, May 8, 2023, Rome, Italy

Trever Schirmer, Nils Japke, Sofia Greten, Tobias Pfandzelter, and David Bermbach

Table 1: Selection of serverless benchmarks and whether they could be impacted by short-term performance fluctuations. Bold
lines are papers where a daily impact could not be ruled out.

Authors

Cloud Platform(s)

Region(s) Specified

Execution Time of Day

AWS, Azure, GCP
AWS

Copik et al. [13]
Eismann et al. [16]
Figiela et al. [19] AWS, Azure, IBM, GCP
Jackson et al. [25]
Grambow et al. [22]
Kim et al. [26]
López et al. [28]
Malawski et al. [30]
Manner et al. [31]
McGrath et al. [32]
Pelle et al. [34]
Scheuner et al. [35]
Shahrad et al. [37]
Somu et al. [38]
Werner et al. [41]
Zhang et al. [43]

AWS, Azure
AWS, Azure, GCP
AWS, Azure, GCP
AWS, Azure, IBM
AWS, GCP
AWS, Azure
AWS, Azure, GCP
AWS
AWS
Azure
AWS, GCP
AWS
AWS, GCP

US (AWS), EU (Azure, GCP)
–
EU (AWS), US (GCP)
–
EU
–
–
EU (AWS), US (GCP)
–
–
“multiple regions”
US
“entire infrastructure”
–
EU
–

–
daily (19:00) for 10 months
every 5 minutes
hourly over 6 days
–
–
–
“permanently”
–
–
–
–
every minute
–
–
–

such as memory access, disk I/O, and network latency or band-
width can be affected by platform variability. As our findings may
not be unconditionally applicable to workloads that are bound in
these dimensions, we will investigate their variation with additional
functions in the future.

7 CONCLUSION
In this paper, we have presented the results of our multi-month
performance variability benchmark measuring the performance
of multiple functions on Google Cloud Functions every 40s. Our
results show that the execution duration of a function varies up to
15% per day, and the frequency of unexpected cold starts varies over
a week and per day. While more resources reduce daily performance
variability, they do not shorten cold start durations. By looking at
the long-term trend, we identify likely updates to the platform and
outlier behavior around the turn of the month. These results have
implications for both researchers and practitioners.

REFERENCES
[1] Amazon Web Services. 2022. AWS Lambda Service Level Agreement. Retrieved

Feburary 24, 2023 from https://aws.amazon.com/lambda/sla/

[2] Ioana Baldini, Perry Cheng, Stephen J. Fink, Nick Mitchell, Vinod Muthusamy,
Rodric Rabbah, Philippe Suter, and Olivier Tardieu. 2017. The serverless trilemma:
function composition for serverless computing. In Proceedings of the 2017 ACM
SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections
on Programming and Software (Vancover, BC, Canada) (Onward! 2017). Association
for Computing Machinery, New York, NY, USA, 89–103. https://doi.org/10.1145/
3133850.3133855

[3] Daniel Bardsley, Larry Ryan, and John Howard. 2018. Serverless Performance and
Optimization Strategies. In Proceedings of the 2018 IEEE International Conference
on Smart Cloud (New York, NY, USA) (SmartCloud). IEEE, New York, NY, USA,
19–26. https://doi.org/10.1109/SmartCloud.2018.00012

[4] Emad Barsoum, Cha Zhang, Cristian Canton Ferrer, and Zhengyou Zhang. 2016.
Training deep networks for facial expression recognition with crowd-sourced
label distribution. In Proceedings of the 18th ACM International Conference on
Multimodal Interaction (Tokyo, Japan) (ICMI ’16). Association for Computing Ma-
chinery, New York, NY, USA, 279–283. https://doi.org/10.1145/2993148.2993165

[5] David Bermbach, Abhishek Chandra, Chandra Krintz, Aniruddha Gokhale,
Aleksander Slominski, Lauritz Thamsen, Everton Cavalcante, Tian Guo, Ivona
Brandic, and Rich Wolski. 2021. On the Future of Cloud Engineering. In Pro-
ceedings of the 9th IEEE International Conference on Cloud Engineering (San
Francisco, CA, USA) (IC2E 2021). ACM, New York, NY, USA, 264–275. https:
//doi.org/10.1109/IC2E52221.2021.00044

[6] David Bermbach, Ahmet-Serdar Karakaya, and Simon Buchholz. 2020. Using
Application Knowledge to Reduce Cold Starts in FaaS Services. In Proceedings
of the 35th ACM Symposium on Applied Computing (Brno, Czech Republic) (SAC
’20). Association for Computing Machinery, New York, NY, USA, 134–143. https:
//doi.org/10.1145/3341105.3373909

[7] David Bermbach and Stefan Tai. 2011. Eventual Consistency: How Soon is
Eventual? An Evaluation of Amazon S3’s Consistency Behavior. In Proceedings of
the 6th Workshop on Middleware for Service Oriented Computing (Lisbon, Portugal)
(MW4SOC ’11). ACM, New York, NY, USA, 1–6. https://doi.org/10.1145/2093185.
2093186

[8] David Bermbach and Stefan Tai. 2014. Benchmarking Eventual Consistency:
Lessons Learned from Long-Term Experimental Studies. In Proceedings of the
2nd IEEE International Conference on Cloud Engineering (Boston, MA, USA) (IC2E
2014). IEEE, New York, NY, USA, 47–56. https://doi.org/10.1109/IC2E.2014.37
[9] David Bermbach, Erik Wittern, and Stefan Tai. 2017. Cloud Service Benchmarking:
Measuring Quality of Cloud Services from a Client Perspective. Springer, Cham,
Switzerland.

[10] Lubomír Bulej, Vojtěch Hork`y, Petr Tuma, François Farquet, and Aleksandar
Prokopec. 2020. Duet benchmarking: Improving measurement accuracy in the
cloud. In Proceedings of the ACM/SPEC International Conference on Performance
Engineering (Edmonton AB, Canada) (ICPE ’20). Association for Computing Ma-
chinery, New York, NY, USA, 100–107. https://doi.org/10.1145/3358960.3379132
[11] Paul Castro, Vatche Ishakian, Vinod Muthusamy, and Aleksander Slominski.
2019. The rise of serverless computing. Commun. ACM 62, 12 (Nov. 2019), 44–54.
https://doi.org/10.1145/3368454

[12] Robert B. Cleveland, William S. Cleveland, Jean E. McRae, and Irma Terpenning.
1990. STL: A Seasonal-Trend Decomposition Procedure Based on Loess. Journal
of Official Statistics 6, 1 (March 1990), 3–73.

[13] Marcin Copik, Grzegorz Kwasniewski, Maciej Besta, Michal Podstawski, and
Torsten Hoefler. 2021. SeBS: a serverless benchmark suite for function-as-a-
service computing. In Proceedings of the 22nd International Middleware Confer-
ence (Quebec City, QC, Canada) (Middleware 2021). Association for Computing
Machinery, New York, NY, USA, 64–78. https://doi.org/10.1145/3464298.3476133
[14] Robert Cordingly, Sonia Xu, and Wesley Lloyd. 2022. Function Memory Op-
timization for Heterogeneous Serverless Platforms with CPU Time Account-
ing. In Proceedings of the 10th IEEE International Conference on Cloud Engi-
neering (Asilomar, CA, USA) (IC2E 2022). IEEE, New York, NY, USA, 104–115.
https://doi.org/10.1109/IC2E55432.2022.00019

The Night Shift: Understanding Performance Variability of Cloud Serverless Platforms

SESAME ’23, May 8, 2023, Rome, Italy

[15] Janos Czentye, Istvan Pelle, Andras Kern, Balazs Peter Gero, Laszlo Toka, and
Balazs Sonkoly. 2019. Optimizing Latency Sensitive Applications for Amazon’s
Public Cloud Platform. In Proceedings of the 2019 IEEE Global Communications
Conference (Waikoloa Village, HI, USA) (GLOBECOM). IEEE, New York, NY, USA,
1–7. https://doi.org/10.1109/GLOBECOM38437.2019.9013988

[16] Simon Eismann, Diego Elias Costa, Lizhi Liao, Cor-Paul Bezemer, Weiyi Shang,
André van Hoorn, and Samuel Kounev. 2022. A case study on the stability of
performance tests for serverless applications. Journal of Systems and Software
189, Article 111294 (July 2022). https://doi.org/10.1016/j.jss.2022.111294
[17] Simon Eismann, Joel Scheuner, Erwin van Eyk, Maximilian Schwinger, Johannes
Grohmann, Nikolas Herbst, Cristina L. Abad, and Alexandru Iosup. 2020. Server-
less Applications: Why, When, and How? IEEE Software 38, 1 (Sept. 2020), 32–39.
https://doi.org/10.1109/MS.2020.3023302

[18] Tarek Elgamal. 2018. Costless: Optimizing Cost of Serverless Computing through
Function Fusion and Placement. In Proceedings of the 2018 IEEE/ACM Symposium
on Edge Computing (Seattle, WA, USA) (SEC 2018). IEEE, New York, NY, USA,
300–312. https://doi.org/10.1109/SEC.2018.00029

[19] Kamil Figiela, Adam Gajek, Adam Zima, Beata Obrok, and Maciej Malawski.
2018. Performance evaluation of heterogeneous cloud functions. Concurrency
and Computation: Practice and Experience 30, 23, Article e4792 (Aug. 2018). https:
//doi.org/10.1002/cpe.4792

[20] Samuel Ginzburg and Michael J. Freedman. 2021. Serverless Isn’t Server-Less:
Measuring and Exploiting Resource Variability on Cloud FaaS Platforms. In
Proceedings of the 2020 Sixth International Workshop on Serverless Computing
(Delft, Netherlands) (WoSC’20). Association for Computing Machinery, New York,
NY, USA, 43–48. https://doi.org/10.1145/3429880.3430099

[21] Google Cloud. 2021. Cloud Functions Service Level Agreement (SLA). Retrieved

Feburary 24, 2023 from https://cloud.google.com/functions/sla

[22] Martin Grambow, Tobias Pfandzelter, Luk Burchard, Carsten Schubert, Max Zhao,
and David Bermbach. 2021. BeFaaS = An Application-Centric Benchmarking
Framework for FaaS Platforms. In Proceedings of the 9th IEEE International Con-
ference on Cloud Engineering (San Francisco, CA, USA) (IC2E 2021). IEEE, New
York, NY, USA, 1–8. https://doi.org/10.1109/IC2E52221.2021.00014

[23] Scott Hendrickson, Stephen Sturdevant, Tyler Harter, Venkateshwaran Venkatara-
mani, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau. 2016. Serverless
computation with openlambda. In Proceedings of the 8th USENIX Workshop on
Hot Topics in Cloud Computing (Denver, CO, USA) (HotCloud ’16). USENIX Asso-
ciation, Berkeley, CA, USA, 33–39.

[24] Shay Horovitz, Roei Amos, Ohad Baruch, Tomer Cohen, Tal Oyar, and Afik
Deri. 2018. FaaStest - Machine Learning Based Cost and Performance FaaS
Optimization. In Proceedings of the International Conference on the Economics of
Grids, Clouds, Systems, and Services (Pisa, Italy) (GECON 2018). Springer, Cham,
Switzerland, 171–186. https://doi.org/10.1007/978-3-030-13342-9_15

[25] David Jackson and Gary Clynch. 2018. An investigation of the impact of language
runtime on the performance and cost of serverless functions. In Proceedings of
the 2018 IEEE/ACM International Conference on Utility and Cloud Computing
Companion (Zurich, Switzerland) (UCC Companion). IEEE, New York, NY, USA,
154–160. https://doi.org/10.1109/UCC-Companion.2018.00050

[26] Jeongchul Kim and Kyungyong Lee. 2019. FunctionBench: A Suite of Workloads
for Serverless Cloud Function Service. In Proceedings of the 2019 IEEE International
Conference on Cloud Computing (Milan, Italy) (CLOUD). IEEE, New York, NY,
USA, 502–504. https://doi.org/10.1109/CLOUD.2019.00091

[27] Danielle Lambion, Robert Schmitz, Robert Cordingly, Navid Heydari, and Wes
Lloyd. 2022. Characterizing X86 and ARM Serverless Performance Variation: A
Natural Language Processing Case Study. In Companion of the 2022 ACM/SPEC
International Conference on Performance Engineering (Bejing, China). Association
for Computing Machinery, New York, NY, USA, 69–75. https://doi.org/10.1145/
3491204.3543506

[28] Pedro García López, Marc Sánchez-Artigas, Gerard París, Daniel Barcelona Pons,
Álvaro Ruiz Ollobarren, and David Arroyo Pinto. 2018. Comparison of FaaS
orchestration systems. In Proceedings of the 2018 IEEE/ACM International Con-
ference on Utility and Cloud Computing Companion (Zurich, Switzerland) (UCC
Companion). IEEE, New York, NY, USA, 148–153. https://doi.org/10.1109/UCC-
Companion.2018.00049

[29] Nima Mahmoudi and Hamzeh Khazaei. 2021. Temporal Performance Modelling
of Serverless Computing Platforms. In Proceedings of the 2020 Sixth International
Workshop on Serverless Computing (Delft, Netherlands) (WoSC’20). Association
for Computing Machinery, New York, NY, USA, 1–6. https://doi.org/10.1145/
3429880.3430092

[30] Maciej Malawski, Kamil Figiela, Adam Gajek, and Adam Zima. 2017. Benchmark-
ing Heterogeneous Cloud Functions. In Proceedings of Euro-Par 2017: Parallel

Processing Workshops (Santiago de Compostela, Spain) (Euro-Par 2017). Springer,
Cham, Switzerland, 415–426. https://doi.org/10.1007/978-3-319-75178-8_34
[31] Johannes Manner, Martin Endreß, Tobias Heckel, and Guido Wirtz. 2018. Cold
Start Influencing Factors in Function as a Service. In Proceedings of the 2018
IEEE/ACM International Conference on Utility and Cloud Computing Companion
(Zurich, Switzerland) (UCC Companion). IEEE, New York, NY, USA, 181–188.
https://doi.org/10.1109/UCC-Companion.2018.00054

[32] Garrett McGrath and Paul R Brenner. 2017. Serverless computing: Design, imple-
mentation, and performance. In Proceedings of the 2017 IEEE 37th International
Conference on Distributed Computing Systems Workshops (Atlanta, GA, USA) (ICD-
CSW). IEEE, New York, NY, USA, 405–410. https://doi.org/10.1109/ICDCSW.
2017.36

[33] Tommi Nylander, Johan Ruuskanen, Karl-Erik Årzén, and Martina Maggio. 2020.
Towards Performance Modeling of Speculative Execution for Cloud Applications.
In Proceedings of the Companion of the ACM/SPEC International Conference on
Performance Engineering (Edmonton AB, Canada) (ICPE ’20). Association for
Computing Machinery, New York, NY, USA, 17–19. https://doi.org/10.1145/
3375555.3384379

[34] István Pelle, János Czentye, János Dóka, and Balázs Sonkoly. 2019. Towards
latency sensitive cloud native applications: A performance study on AWS. In
Proceedings of the 2019 IEEE 12th International Conference on Cloud Computing
(Milan, Italy) (CLOUD). IEEE, New York, NY, USA, 272–280. https://doi.org/10.
1109/CLOUD.2019.00054

[35] Joel Scheuner, Marcus Bertilsson, Oskar Grönqvist, Henrik Tao, Henrik Lagergren,
Jan-Philipp Steghöfer, and Philipp Leitner. 2022. TriggerBench: A Performance
Benchmark for Serverless Function Triggers. In Proceedings of the 2022 IEEE
International Conference on Cloud Engineering (Asilomar, CA, USA) (IC2E 2022).
IEEE, New York, NY, USA, 96–103. https://doi.org/10.1109/IC2E55432.2022.00018
[36] Trever Schirmer, Joel Scheuner, Tobias Pfandzelter, and David Bermbach. 2022.
Fusionize: Improving Serverless Application Performance through Feedback-
Driven Function Fusion. In Proceedings of the 10th IEEE International Conference
on Cloud Engineering (Asilomar, CA, USA) (IC2E 2022). IEEE, New York, NY, USA,
85–95. https://doi.org/10.1109/IC2E55432.2022.00017

[37] Mohammad Shahrad, Rodrigo Fonseca, Íñigo Goiri, Gohar Chaudhry, Paul Batum,
Jason Cooke, Eduardo Laureano, Colby Tresness, Mark Russinovich, and Ricardo
Bianchini. 2020. Serverless in the Wild: Characterizing and Optimizing the
Serverless Workload at a Large Cloud Provider. In Proceedings of the 2020 USENIX
Annual Technical Conference (Virtual Event, USA) (USENIX ATC ’20). USENIX,
Berkeley, CA, USA, 205–218.

[38] Nikhila Somu, Nilanjan Daw, Umesh Bellur, and Purushottam Kulkarni. 2020.
PanOpticon: A Comprehensive Benchmarking Tool for Serverless Applications.
In Proceedings of the 2020 International Conference on COMmunication Systems
& NETworkS (Bengaluru, India) (COMSNETS 2020). IEEE, New York, NY, USA,
144–151. https://doi.org/10.1109/COMSNETS48256.2020.9027346

[39] Charles Truong, Laurent Oudre, and Nicolas Vayatis. 2020. Selective review of
offline change point detection methods. Signal Processing 167, Article 107299
(Feb. 2020). https://doi.org/10.1016/j.sigpro.2019.107299

[40] Erwin van Eyk, Joel Scheuner, Simon Eismann, Cristina L. Abad, and Alexandru
Iosup. 2020. Beyond Microbenchmarks: The SPEC-RG Vision for a Comprehen-
sive Serverless Benchmark. In Proceedings of the Companion of the ACM/SPEC
International Conference on Performance Engineering (Edmonton AB, Canada)
(ICPE ’20). Association for Computing Machinery, New York, NY, USA, 26–31.
https://doi.org/10.1145/3375555.3384381

[41] Sebastian Werner, Jörn Kuhlenkamp, Markus Klems, Johannes Müller, and Stefan
Tai. 2018. Serverless Big Data Processing using Matrix Multiplication as Example.
In Proceedings of the 2018 IEEE International Conference on Big Data (Seattle, WA,
USA) (Big Data). IEEE, New York, NY, USA, 358–365. https://doi.org/10.1109/
BigData.2018.8622362

[42] Sebastian Werner and Trever Schirmer. 2022. Hardless: A Generalized Serverless
Compute Architecture for Hardware Processing Accelerators. In Proceedings of
the 10th IEEE International Conference on Cloud Engineering (Asilomar, CA, USA)
(IC2E 2022). IEEE, New York, NY, USA, 79–84. https://doi.org/10.1109/IC2E55432.
2022.00016

[43] Miao Zhang, Yifei Zhu, Cong Zhang, and Jiangchuan Liu. 2019. Video processing
with serverless computing: A measurement study. In Proceedings of the 29th ACM
Workshop on Network and Operating Systems Support for Digital Audio and Video
(Amherst, MA, USA) (NOSSDAV ’19). Association for Computing Machinery, New
York, NY, USA, 61–66. https://doi.org/10.1145/3304112.3325608

[44] Haidong Zhao, Zakaria Benomar, Tobias Pfandzelter, and Nikolaos Georgantas.
2022. Supporting Multi-Cloud in Serverless Computing. In Proceedings of the 15th
IEEE/ACM International Conference on Utility and Cloud Computing Companion
(Vancouver, WA, USA) (UCC ’22). IEEE, New York, NY, USA.

","The Night Shift : Understanding Performance Variability of Cloud Serverless Platforms Nils Japke TU Berlin & ECDF Berlin , Germany nj @ mcc.tu-berlin.de Trever Schirmer TU Berlin & ECDF Berlin , Germany ts @ mcc.tu-berlin.de Sofia Greten TU Berlin & ECDF Berlin , Germany sog @ mcc.tu-berlin.de 3 2 0 2 r p A 4 1 ] C D . s c [ 1 v 7 7 1 7 0 . 4 0 3 2 : v i X r a Tobias Pfandzelter TU Berlin & ECDF Berlin , Germany tp @ mcc.tu-berlin.de David Bermbach TU Berlin & ECDF Berlin , Germany db @ mcc.tu-berlin.de ABSTRACT Function-as-a-Service is a popular cloud programming model that supports developers by abstracting away most operational concerns with automatic deployment and scaling of applications . Due to the high level of abstraction , developers rely on the cloud platform to offer a consistent service level , as decreased performance leads to higher latency and higher cost given the pay-per-use model . In this paper , we measure performance variability of Google Cloud Functions over multiple months . Our results show that diurnal patterns can lead to performance differences of up to 15 % , and that the frequency of unexpected cold starts increases threefold during the start of the week . This behavior can negatively impact researchers that conduct performance studies on cloud platforms and practitioners that run cloud applications . CCS CONCEPTS • Computer systems organization → Cloud computing ; • Ap- plied computing → Service-oriented architectures ; • Software and its engineering → Software performance ; • Networks → Cloud computing . KEYWORDS Serverless Computing , Function as a Service , Performance Varia- tion , Resource Contention ACM Reference Format : Trever Schirmer , Nils Japke , Sofia Greten , Tobias Pfandzelter , and David Bermbach . 2023 . The Night Shift : Understanding Performance Variability of Cloud Serverless Platforms . In The 1st Workshop on SErverless Systems , Applications and MEthodologies ( SESAME ’ 23 ) , May 8 , 2023 , Rome , Italy . ACM , New York , NY , USA , 7 pages . https : //doi.org/10.1145/3592533.3592808 1 INTRODUCTION Function-as-a-Service ( FaaS ) is a serverless cloud computing de- livery model where developers compose their applications from event-driven stateless functions and all operational tasks are man- aged by the cloud provider [ 5 , 17 , 23 ] . Functions are billed on a SESAME ’ 23 , May 8 , 2023 , Rome , Italy © 2023 Copyright held by the owner/author ( s ) . Publication rights licensed to ACM . This is the author ’ s version of the work . It is posted here for your personal use . Not for redistribution . The definitive Version of Record was published in The 1st Workshop on SErverless Systems , Applications and MEthodologies ( SESAME ’ 23 ) , May 8 , 2023 , Rome , Italy , https : //doi.org/10.1145/3592533.3592808 . pay-per-use basis at second or even microsecond granularity and offer rapid elasticity and scale [ 5 , 11 , 23 ] . The abstraction from operational concerns has made FaaS a popular cloud execution model , with offerings by all major cloud providers , e.g. , Amazon Web Services Lambda1 and Google Cloud Functions2 [ 2 , 5 , 11 ] . The flip side of high levels of resource sharing in the cloud and abstracting from resource management is that developers must rely on the cloud platform provider to offer stable and consistent performance . Somewhat counterintuitively , FaaS users must actu- ally pay more when a FaaS platform underperforms and latency is higher , as billed function execution is also longer [ 2 , 36 ] . Most cloud FaaS platforms do not offer service level agreements beyond limited guarantees regarding general uptime [ 1 , 21 , 44 ] . Cloud computing is subject to performance variations [ 7 , 33 ] , and FaaS is no exception , as previous studies on the long-term ( day-to- day ) performance changes of FaaS platforms have shown [ 16 , 27 , 40 ] . A general improvement of FaaS services over time is expected as platform providers update and advance there infrastructure [ 42 ] , yet there are also much finer effects in the short term . In this paper , we benchmark and analyze these effects using highly frequent ( every 40s ) cloud FaaS benchmarks over the course of two months against Google Cloud Functions ( GCF ) , which has received fewer attention in previous studies . Our main finding is that performance varies greatly during the course of the day , with an increase of request-response latency of up to 15 % and more than three times as many unexpected cold starts from day to night within the same day , and that these effects are most noticeable at the start of the week . These findings impact how we interpret the results of cloud FaaS benchmarks and are significant for cloud FaaS developers . In summary , we make the following contributions : • Based on a number of existing serverless benchmarks , we propose a methodology for evaluating temporal performance variations in cloud FaaS platforms ( §3 ) . • We execute our benchmark with frequent ( every 40s ) runs on Google Cloud Functions over the course of two months and show how request-response latency exhibits strong ( up to 15 % difference ) diurnal variation ( §4.1 ) . • We further uncover an increase in unexpected cold starts at specific times of the day that suggest increased rates of in- stance recycling in GCF during times of high demand ( §4.2 ) . 1https : //aws.amazon.com/lambda/ 2https : //cloud.google.com/functions/ SESAME ’ 23 , May 8 , 2023 , Rome , Italy Trever Schirmer , Nils Japke , Sofia Greten , Tobias Pfandzelter , and David Bermbach • We evaluate long-term trends in our data and identify possi- ble causes ( §4.3 ) . • We survey existing cloud FaaS benchmarking studies and find that almost two thirds provide insufficient information on how the performance variability effects we identify are controlled for ( §5.1 ) . • We discuss implications of our findings for practitioners that run applications on cloud FaaS platforms ( §5.2 ) . In order to enable other researchers and practitioners to extend and replicate our experiments , we make the artifacts used to pro- duce this paper available as open-source3 . 2 RELATED WORK Existing research on FaaS performance variability focuses on ei- ther short-term or long-term ( i.e. , more than a week ) variability . Short-term studies include a report by Lambion et al . [ 27 ] , who have benchmarked the performance of various functions on AWS Lambda over the course of a day . The authors execute their func- tions in different time zones and using different hardware archi- tectures , and find a 6 % shorter function duration during the night . Mahmoudi et al . [ 29 ] propose an analytical performance model to predict performance metrics of functions . To validate their model , they repeat the same one-hour experiment ten times , and find that request arrival rate , average response times , and function timeout can be used to predict performance up to five minutes in advance . Ginzburg and Freedman [ 20 ] analyze performance variations on AWS Lambda over the course a week . They call 1000 functions every two hours and show that the daily performance of the same function inside the same region and between regions can vary sig- nificantly , which is mainly caused by local inactivity and lack of performance isolation between tenants . Since they only measure for one week , their analysis focuses on daily variations , which they measure at 1-2 % . A long-term study of serverless systems is presented by Eismann et al . [ 16 ] , who have executed the same serverless application on Lambda once a day over ten months . They find long-term perfor- mance changes that are likely to be caused by platform changes , and short-term variations between days . Figelia et al . [ 19 ] measure the performance of various functions running on Lambda over seven months , but do not analyze their dataset for regular variability . The focus of our paper is to close the gap between long-term and short-term studies by collecting frequent measurements over a longer period of time . Additionally , we go beyond the focus on AWS Lambda and present a general methodology for experiments that are applicable to all FaaS platforms . We collect our results on Google Cloud Functions , which has received fewer attention in previous studies despite its popularity . 3 METHODOLOGY To assess performance variations in FaaS , we repeatedly execute a FaaS function in short intervals across a large time span . By controlling for execution region , resource parameters ( through the memory option on cloud FaaS providers ) , and function type , we can focus on cloud platform performance . 3https : Functions . We use three FaaS functions from existing serverless benchmarks in our experiments . All functions perform isolated computations that do not rely on external services , the performance of which could influence our results [ 22 ] . The float workload of Kim et al . [ 26 ] that performs floating point operations . The matrix function of Werner et al . [ 41 ] that performs matrix multiplication . Finally , we adapt the face detection model of Barosum et al . [ 4 ] for the ml function . To minimize the impact of external fluctuations on our measurements , we embed all inputs directly into the functions . The resources available to a function instance are determined by the memory configured for that function : On GCF , the amount of vCPUs allocated to a function instance is tiered and increases with every multiple of 128MB memory , while AWS Lambda scales vCPUs linearly with memory [ 14 ] . To capture effects of resource configurations , we deploy float and matrix with 128MB , 256MB , and 512MB of memory , while ml is deployed with 512MB and 1024MB as it has higher resource requirements . Execution . Cloud function invocations can be both “ warm ” and “ cold ” : When a function is invoked for the first time , a new function instance is created . This is called a “ cold start ” and incurs a creation overhead [ 3 , 6 , 31 ] . Subsequent ( but possibly not parallel ) invoca- tions of the same function can reuse the existing instance and avoid this overhead , the “ warm starts ” . Typically , cloud platforms will keep existing instances for future invocations for a limited amount of time and then evict them to reclaim resources [ 13 ] . To capture both cold and warm start latencies , we invoke func- tions in loops : We first call the function once , creating a cold start , and then call the function again . In theory , this second invocation should be served by the existing function instance . We then wait 20 minutes to make sure that the next function call is a cold start again and restart the same loop . To collect more measurement points than twice every 20 minutes , we deploy parallel copies of a function that we cycle through . Metrics . We consider three main metrics : request-response latency , unexpected cold starts , and long-term trends . For request-response latency , we use the billed duration that is output for every func- tion execution by the FaaS platform . Unexpected cold starts are cold starts that occur directly after a function has already been called once , so that they should be warm . Unexpected cold starts imply that a platform was unable to find a warm function instance , possibly because it has been evicted due to resource contention . Finally , we conduct a seasonal trend decomposition using LOESS ( STL ) [ 12 ] , which can handle complex seasonal patterns . We use the following model for our data : 𝑦𝑡 = 𝑇𝑡 + 𝑆𝑡 + 𝐼𝑡 , where 𝑇𝑡 represents the trend , 𝑆𝑡 represents a seasonal compo- nent ( days in our case ) , and 𝐼𝑡 represents the remaining noise . We fit our data to this model using a method to create smoothed estimates with a seasonality of one day . The trend component 𝑇𝑡 shows the overall progress in the billed duration over the whole duration of the experiment . A non-flat trend line indicates that longer-term changes to the platform have occurred , e.g. , long-term seasonal changes or updates to the platform that influenced performance . The seasonal component 𝑆𝑡 shows periodic recurring deviations in The Night Shift : Understanding Performance Variability of Cloud Serverless Platforms SESAME ’ 23 , May 8 , 2023 , Rome , Italy Figure 1 : Billed Duration of warm calls to float with 128MB memory . During working hours , the billed duration in- creases by up to 15 % . The area around the dashed line shows the 95 % confidence interval . the data from the trend . The 𝐼𝑡 component is random noise centered around 0 . Outliers in the noise indicate that the performance at spe- cific times could not be explained by the previous two components . The trend component from the STL is also used for Change Point Detection [ 39 ] , which can detect structural changes in data . 4 EXPERIMENTS In this section , we present initial results of our experiments with Google Cloud Functions in the europe-west3 region . The measure- ment period started on Dec 12 , 2022 and ended on Feb 27 , 2023 . We report all execution times in local time ( CET ) . We first analyze the performance variability of the platform ( §4.1 ) . Afterwards , we explore unexpected cold starts ( §4.2 ) and outliers as well as change points and long-term trends in our data ( §4.3 ) . 4.1 Performance Variability To analyze performance variability , we analyze the billed duration of comparable invocations . We show the billed duration of the float function with 128MB memory in Figure 1 . We observe a clear performance increase during the night , with a noticeable latency spike during working hours . The average billed duration between 23:00 and 06:00 was 106ms , and increased by 15 % to 122ms between 07:00 and 16:00 . When aggregated by the day of the week , the average billed duration fluctuates between 113.86ms on Saturdays and 117.28ms on Mondays . Overall , billed duration is slightly lower on the average weekend compared to the start of the week . The weekly trend is much smaller than the daily trend , as the billed duration only decreases by ~4 % during the weekend . When looking at larger memory sizes , the relative performance change over time becomes smaller . As shown in Figure 2 , the float function with 128MB memory changes ~10 % during a day , while 512MB only changes up to 5 % . The ml functions with 512MB and 1024MB memory equally changed ~4 % during an average day . Noticeably , the average billed duration of the float function with 256MB of memory differed more than 15 % during a day . This can ( a ) float ( b ) ml Figure 2 : Performance change over a day of warm instances ordered by memory size . The y-Axis is normalized to the av- erage and shows the relative change , e.g. , bigger values show a bigger deviation from the average billed duration . The area around the curves shows the 95 % confidence interval . be explained by looking at the distribution of latency values over all invocations ( Figure 3 ) : Around 50 % of functions with a mem- ory size of 256MB follow the same distribution as functions with 512MB of memory , and the other half follows the same distribu- tion as the 128MB functions . The results for the matrix function exhibit similar results but are omitted due to space constraints . The high variability in performance and uneven distribution of billed durations indicates that GCF internally uses 128MB and 512MB function instances to handle requests to the 256MB functions , as also shown by Malawski et al . [ 30 ] . While cold start durations also follow the daily patterns shown in §4.1 , the configured memory size has no impact on the duration of cold starts . The billed durations of cold starts follow a normal dis- tribution and are on average around 9-10x longer than the average warm latency , but all memory sizes follow the same distribution . 125 120 115 110 105 ] s m [ n o i t a r u D d e l l i B 0 5 10 Hour of Day 15 20 Memory 128 256 512 m o r f n o i t a i v e D e v i t a l e R n o i t a r u D d e l l i B n a e M 20.0 % 10.0 % 0.0 % −10.0 % −20.0 % 0 5 10 15 20 Hour of Day Memory 512 1024 m o r f n o i t a i v e D e v i t a l e R n o i t a r u D d e l l i B n a e M 20.0 % 10.0 % 0.0 % −10.0 % −20.0 % 0 5 10 15 20 Hour of Day SESAME ’ 23 , May 8 , 2023 , Rome , Italy Trever Schirmer , Nils Japke , Sofia Greten , Tobias Pfandzelter , and David Bermbach Figure 3 : Cumulative distribution of billed durations of the float function without cold starts . We argue that GCF uses 128MB and 512MB containers to execute 256MB functions . Figure 5 : Trend Component of the STL , with Change Points and Outliers marked as horizontal lines . 4.3 Outliers & Long-Term Trend Based on the STL introduced in §3 , we show change points and outliers in Figure 5 . We define an outlier as every hour during which the average execution duration is outside the fourth interquartile range , i.e. , more than four times the difference between the first and third quartile , away from the average . Our data contains four outliers , which were all within three days of the turn of the month . This indicates to us that the platform is under unusual load at these times , possibly due to additional load from monthly jobs . All change points , i.e. , points when the average execution du- ration changed , occurred during the night , indicating that they coincide with scheduled updates to the platform . Over our whole measurement period , there is no clear permanent trend towards better or worse overall performance . Based on the long-term study by Eismann et al . [ 16 ] , we only expect to find permanent trends in longer measurement periods . Compared to the authors ’ study of AWS , which finds statistical trends below 10 % , our trend component changes between 105ms during December to 128ms during February , a 21 % increase . 5 IMPLICATIONS In this section , we discuss how our findings impact serverless sys- tems . First , we focus on the implications for benchmarking . After- wards , we describe implications on serverless applications . Figure 4 : Relative frequency of unexpected cold starts , aver- aged by the hour of the week they happened in . On Mondays during the day , up to 13 % of invocations can be unexpected cold starts , compared to less than 5 % during the night and on weekends . The area around the curve shows the 95 % con- fidence interval . This indicates that the cold start overhead is dependent on resources that can be configured by changing the function configuration . 4.2 Unexpected Cold Starts We show the relative frequency of unexpected cold starts in Fig- ure 4 . While the billed duration of warm instances seems to only follow a daily trend , the frequency of unexpected cold starts has a weekly seasonality , with clear trends of increased cold starts during working hours . On average , the frequency of unexpected cold starts was 3.7 % during the night ( 20:00—08:00 ) , 3.6 % during the weekend , 9.8 % during working hours ( 09:00—17:00 Mon—Fri ) , and 12.3 % dur- ing working hours on Monday . For comparison , there were less than 0.15 % unexpected warm starts , where a function instance was still warm after more than 20 minutes . 5.1 Validity of Benchmarks When running benchmarks , researchers want to minimize effects of external factors to their measurements , which otherwise might con- found results [ 9 ] . In the case of benchmarking serverless systems , we have shown that the number of cold starts and the performance of functions undergoes changes within a single day . For compara- tive performance studies , benchmark results are only comparable if experiments are conducted at comparable times . If an experi- ment is short , i.e. , does not capture the performance variation of an entire day , daily variations can skew results . Similarly , long- term performance changes can impact measurements taken over n o i t u b i r t s i D e v i t a u m u C l l a c i r i p m E 1.0 0.8 0.6 0.4 0.2 0.0 Memory Size 128 256 512 0 50 100 200 150 Billed Duration [ ms ] 250 300 f o y c n e u q e r F e v i t a l e R s t r a t S l d o C d e t c e p x e n U 15.0 % 12.5 % 10.0 % 7.5 % 5.0 % 2.5 % 0.0 % 0 25 50 75 100 Hour of Week 125 150 STL Trend Change Point Outlier n o i t a r u D d e l l i B ] s m [ t n e n o p m o C d n e r T 130 125 120 115 110 105 5 c 1 e D 1 n 0 a J 5 n 1 a J 1 b 0 e F 5 b 1 e F Function Start The Night Shift : Understanding Performance Variability of Cloud Serverless Platforms SESAME ’ 23 , May 8 , 2023 , Rome , Italy 5.2 Application Performance The performance variability that we have shown for GCF affects serverless applications in several ways : During daytime , functions have increased latency , suffer more cold starts , and their execu- tion cost is increased due to the pay-by-second billing model . For low-latency , event-driven functions , it is not feasible to postpone their execution to the night or a weekend to decrease costs . A possible way forward , however , is to shift function execution to another cloud region with better performance . This may increase network latency and transmission costs , but an up to 20 % reduc- tion on function execution times and the associated decrease in costs can outweigh this overhead for long ( er ) -running functions . Such an approach requires constant evaluation of FaaS platform performance in different regions , possibly also based on application metrics [ 8 ] . Researchers have also proposed systems that adapt FaaS appli- cations to improve performance and cost on cloud FaaS platforms . Such systems rely on initial performance measurements of applica- tions on cloud platforms [ 14 , 15 , 18 , 24 ] or a feedback loop between platform , application , and optimizer [ 36 ] . Both approaches are af- fected by performance variability of the FaaS platform , as the opti- mizer or model can not differentiate between performance changes that are caused by deployment updates and those that are caused by platform instability . A possible way forward for these systems is to control for temporal performance variations , e.g. , by deploying multiple parameter sets concurrently or performing longer initial measurements . 6 LIMITATIONS & FUTURE WORK We have shown considerable performance variability in Google Cloud Functions and discussed how these affect applications and performance measurement research . We plan to build on this initial work in the future to arrive at a more holistic view of performance variability in cloud FaaS platforms . FaaS Platforms . Our initial experiments are limited mostly to GCF , with some additional validation on AWS Lambda . Although we have seen that in our experiments , Lambda suffers from less performance variability than GCF , parameters such as memory size , hardware architecture , geographical region , or programming language could further influence variability . We plan to conduct additional exper- iments on different FaaS platforms in the future , controlling for these additional parameters . Platform Changes . FaaS platforms are evolving quickly , and our measurements and experiments can only capture the behavior of such a platform at a specific point in time . Continuous updates could increase or even eliminate the performance variability effects we observe in the future , making continuous measurements important . Subsequently , researchers that want to account for the described behavior in their own measurements on FaaS platforms should conduct their own experiments using our methodology , as our measurement results may be outdated by then . Performance Dimensions . The functions we use in our experiments are CPU-bound , which gives a good indication for general platform performance and minimizes the impact of the performance of ex- ternal services . Beyond CPU performance , other resource metrics Figure 6 : Execution Times of the 110.dynamic-html Bench- mark on GCF and Lambda ( excerpt ) . The minimum and maximum measured difference of the whole experiment are marked as horizontal lines . longer periods of time , e.g. , a three-day study with one group bench- marked over the weekend and another benchmarked during the week . A possible remediation for such experiments is adopting parallel benchmarking techniques such as duet benchmarking [ 10 ] . We show an overview of existing publications on cloud FaaS performance measurements surveyed for this paper in Table 1 . For every paper , we give an overview of cloud platforms under test , cloud regions , and time of day of the benchmark execution , if stated . Based on this metadata , we must assume that published re- sults could be affected by daily performance variations if execution time is not given or different regions are used ( implying different time zones ) . Overall , 10 out of a total of 16 papers do not provide sufficient information to rule out effects of performance variability . While this does not mean that the reported results are invalid , it shows that the research community has not paid enough attention to these effects in performance measurements . As an example , we replicate an experiment from Copik et al . [ 13 ] that compares the execution time of a dynamic HTML generator ( 110.dynamic-html ) between AWS Lambda and GCF . We deploy this function in the eu-central-1 ( Lambda ) and europe-west3 ( GCF ) regions with 128MB of memory and run 50 sequential in- vocations every hour over the course of five days . As shown in Figure 6 , the performance on GCF exhibits temporal variations that can skew results : The smallest performance difference during our experiments happened on the 23rd of February 2023 at mid- night , when the average performance difference was < 2.8s ( GCF 52 % slower ) . Shortly after , at 09:00 , we observe the largest perfor- mance difference with the average difference increasing to > 25s ( GCF 6.8× slower ) . These performance changes show that short- term variation in performance between serverless platforms can have a significant impact on benchmarking results and need to be controlled for . We recommend repeating experiments over the course of a day and mentioning execution time when describing experiment setup . GCF AWS Max Difference Min Difference ] s [ i e m T n o i t u c e x E 40 30 20 10 0 0 0 : 0 0 0 2 : 0 0 0 0 0 0 0 8 : 0 6 : 0 8 : 0 4 : 0 1 1 Time of Execution ( Feb 23 , 2023 ) 2 : 0 0 : 0 4 : 0 6 : 0 0 1 1 1 0 0 0 0 0 0 0 : 0 2 SESAME ’ 23 , May 8 , 2023 , Rome , Italy Trever Schirmer , Nils Japke , Sofia Greten , Tobias Pfandzelter , and David Bermbach Table 1 : Selection of serverless benchmarks and whether they could be impacted by short-term performance fluctuations . Bold lines are papers where a daily impact could not be ruled out . Authors Cloud Platform ( s ) Region ( s ) Specified Execution Time of Day AWS , Azure , GCP AWS Copik et al . [ 13 ] Eismann et al . [ 16 ] Figiela et al . [ 19 ] AWS , Azure , IBM , GCP Jackson et al . [ 25 ] Grambow et al . [ 22 ] Kim et al . [ 26 ] López et al . [ 28 ] Malawski et al . [ 30 ] Manner et al . [ 31 ] McGrath et al . [ 32 ] Pelle et al . [ 34 ] Scheuner et al . [ 35 ] Shahrad et al . [ 37 ] Somu et al . [ 38 ] Werner et al . [ 41 ] Zhang et al . [ 43 ] AWS , Azure AWS , Azure , GCP AWS , Azure , GCP AWS , Azure , IBM AWS , GCP AWS , Azure AWS , Azure , GCP AWS AWS Azure AWS , GCP AWS AWS , GCP US ( AWS ) , EU ( Azure , GCP ) – EU ( AWS ) , US ( GCP ) – EU – – EU ( AWS ) , US ( GCP ) – – “ multiple regions ” US “ entire infrastructure ” – EU – – daily ( 19:00 ) for 10 months every 5 minutes hourly over 6 days – – – “ permanently ” – – – – every minute – – – such as memory access , disk I/O , and network latency or band- width can be affected by platform variability . As our findings may not be unconditionally applicable to workloads that are bound in these dimensions , we will investigate their variation with additional functions in the future . 7 CONCLUSION In this paper , we have presented the results of our multi-month performance variability benchmark measuring the performance of multiple functions on Google Cloud Functions every 40s . Our results show that the execution duration of a function varies up to 15 % per day , and the frequency of unexpected cold starts varies over a week and per day . While more resources reduce daily performance variability , they do not shorten cold start durations . By looking at the long-term trend , we identify likely updates to the platform and outlier behavior around the turn of the month . These results have implications for both researchers and practitioners . REFERENCES [ 1 ] Amazon Web Services . 2022 . AWS Lambda Service Level Agreement . Retrieved Feburary 24 , 2023 from https : //aws.amazon.com/lambda/sla/ [ 2 ] Ioana Baldini , Perry Cheng , Stephen J. Fink , Nick Mitchell , Vinod Muthusamy , Rodric Rabbah , Philippe Suter , and Olivier Tardieu . 2017 . The serverless trilemma : function composition for serverless computing . In Proceedings of the 2017 ACM SIGPLAN International Symposium on New Ideas , New Paradigms , and Reflections on Programming and Software ( Vancover , BC , Canada ) ( Onward ! 2017 ) . Association for Computing Machinery , New York , NY , USA , 89–103 . https : //doi.org/10.1145/ 3133850.3133855 [ 3 ] Daniel Bardsley , Larry Ryan , and John Howard . 2018 . Serverless Performance and Optimization Strategies . In Proceedings of the 2018 IEEE International Conference on Smart Cloud ( New York , NY , USA ) ( SmartCloud ) . IEEE , New York , NY , USA , 19–26 . https : [ 4 ] Emad Barsoum , Cha Zhang , Cristian Canton Ferrer , and Zhengyou Zhang . 2016 . Training deep networks for facial expression recognition with crowd-sourced label distribution . In Proceedings of the 18th ACM International Conference on Multimodal Interaction ( Tokyo , Japan ) ( ICMI ’ 16 ) . Association for Computing Ma- chinery , New York , NY , USA , 279–283 . https : //doi.org/10.1145/2993148.2993165 [ 5 ] David Bermbach , Abhishek Chandra , Chandra Krintz , Aniruddha Gokhale , Aleksander Slominski , Lauritz Thamsen , Everton Cavalcante , Tian Guo , Ivona Brandic , and Rich Wolski . 2021 . On the Future of Cloud Engineering . In Pro- ceedings of the 9th IEEE International Conference on Cloud Engineering ( San Francisco , CA , USA ) ( IC2E 2021 ) . ACM , New York , NY , USA , 264–275 . https : [ 6 ] David Bermbach , Ahmet-Serdar Karakaya , and Simon Buchholz . 2020 . Using Application Knowledge to Reduce Cold Starts in FaaS Services . In Proceedings of the 35th ACM Symposium on Applied Computing ( Brno , Czech Republic ) ( SAC ’ 20 ) . Association for Computing Machinery , New York , NY , USA , 134–143 . https : //doi.org/10.1145/3341105.3373909 [ 7 ] David Bermbach and Stefan Tai . 2011 . Eventual Consistency : How Soon is Eventual ? An Evaluation of Amazon S3 ’ s Consistency Behavior . In Proceedings of the 6th Workshop on Middleware for Service Oriented Computing ( Lisbon , Portugal ) ( MW4SOC ’ 11 ) . ACM , New York , NY , USA , 1–6 . https : //doi.org/10.1145/2093185 . 2093186 [ 8 ] David Bermbach and Stefan Tai . 2014 . Benchmarking Eventual Consistency : Lessons Learned from Long-Term Experimental Studies . In Proceedings of the 2nd IEEE International Conference on Cloud Engineering ( Boston , MA , USA ) ( IC2E 2014 ) . IEEE , New York , NY , USA , 47–56 . https : //doi.org/10.1109/IC2E.2014.37 [ 9 ] David Bermbach , Erik Wittern , and Stefan Tai . 2017 . Cloud Service Benchmarking : Measuring Quality of Cloud Services from a Client Perspective . Springer , Cham , Switzerland . [ 10 ] Lubomír Bulej , Vojtěch Hork ` y , Petr Tuma , François Farquet , and Aleksandar Prokopec . 2020 . Duet benchmarking : Improving measurement accuracy in the cloud . In Proceedings of the ACM/SPEC International Conference on Performance Engineering ( Edmonton AB , Canada ) ( ICPE ’ 20 ) . Association for Computing Ma- chinery , New York , NY , USA , 100–107 . https : //doi.org/10.1145/3358960.3379132 [ 11 ] Paul Castro , Vatche Ishakian , Vinod Muthusamy , and Aleksander Slominski . 2019 . The rise of serverless computing . Commun . ACM 62 , 12 ( Nov. 2019 ) , 44–54 . https : //doi.org/10.1145/3368454 [ 12 ] Robert B. Cleveland , William S. Cleveland , Jean E. McRae , and Irma Terpenning . 1990 . STL : A Seasonal-Trend Decomposition Procedure Based on Loess . Journal of Official Statistics 6 , 1 ( March 1990 ) , 3–73 . [ 13 ] Marcin Copik , Grzegorz Kwasniewski , Maciej Besta , Michal Podstawski , and Torsten Hoefler . 2021 . SeBS : a serverless benchmark suite for function-as-a- service computing . In Proceedings of the 22nd International Middleware Confer- ence ( Quebec City , QC , Canada ) ( Middleware 2021 ) . Association for Computing Machinery , New York , NY , USA , 64–78 . https : //doi.org/10.1145/3464298.3476133 [ 14 ] Robert Cordingly , Sonia Xu , and Wesley Lloyd . 2022 . Function Memory Op- timization for Heterogeneous Serverless Platforms with CPU Time Account- ing . In Proceedings of the 10th IEEE International Conference on Cloud Engi- neering ( Asilomar , CA , USA ) ( IC2E 2022 ) . IEEE , New York , NY , USA , 104–115 . https : The Night Shift : Understanding Performance Variability of Cloud Serverless Platforms SESAME ’ 23 , May 8 , 2023 , Rome , Italy [ 15 ] Janos Czentye , Istvan Pelle , Andras Kern , Balazs Peter Gero , Laszlo Toka , and Balazs Sonkoly . 2019 . Optimizing Latency Sensitive Applications for Amazon ’ s Public Cloud Platform . In Proceedings of the 2019 IEEE Global Communications Conference ( Waikoloa Village , HI , USA ) ( GLOBECOM ) . IEEE , New York , NY , USA , 1–7 . https : [ 16 ] Simon Eismann , Diego Elias Costa , Lizhi Liao , Cor-Paul Bezemer , Weiyi Shang , André van Hoorn , and Samuel Kounev . 2022 . A case study on the stability of performance tests for serverless applications . Journal of Systems and Software 189 , Article 111294 ( July 2022 ) . https : [ 17 ] Simon Eismann , Joel Scheuner , Erwin van Eyk , Maximilian Schwinger , Johannes Grohmann , Nikolas Herbst , Cristina L. Abad , and Alexandru Iosup . 2020 . Server- less Applications : Why , When , and How ? IEEE Software 38 , 1 ( Sept. 2020 ) , 32–39 . https : //doi.org/10.1109/MS.2020.3023302 [ 18 ] Tarek Elgamal . 2018 . Costless : Optimizing Cost of Serverless Computing through Function Fusion and Placement . In Proceedings of the 2018 IEEE/ACM Symposium on Edge Computing ( Seattle , WA , USA ) ( SEC 2018 ) . IEEE , New York , NY , USA , 300–312 . https : //doi.org/10.1109/SEC.2018.00029 [ 19 ] Kamil Figiela , Adam Gajek , Adam Zima , Beata Obrok , and Maciej Malawski . 2018 . Performance evaluation of heterogeneous cloud functions . Concurrency and Computation : Practice and Experience 30 , 23 , Article e4792 ( Aug. 2018 ) . https : //doi.org/10.1002/cpe.4792 [ 20 ] Samuel Ginzburg and Michael J. Freedman . 2021 . Serverless Isn ’ t Server-Less : Measuring and Exploiting Resource Variability on Cloud FaaS Platforms . In Proceedings of the 2020 Sixth International Workshop on Serverless Computing ( Delft , Netherlands ) ( WoSC ’ 20 ) . Association for Computing Machinery , New York , NY , USA , 43–48 . https : //doi.org/10.1145/3429880.3430099 [ 21 ] Google Cloud . 2021 . Cloud Functions Service Level Agreement ( SLA ) . Retrieved Feburary 24 , 2023 from https : //cloud.google.com/functions/sla [ 22 ] Martin Grambow , Tobias Pfandzelter , Luk Burchard , Carsten Schubert , Max Zhao , and David Bermbach . 2021 . BeFaaS = An Application-Centric Benchmarking Framework for FaaS Platforms . In Proceedings of the 9th IEEE International Con- ference on Cloud Engineering ( San Francisco , CA , USA ) ( IC2E 2021 ) . IEEE , New York , NY , USA , 1–8 . https : [ 23 ] Scott Hendrickson , Stephen Sturdevant , Tyler Harter , Venkateshwaran Venkatara- mani , Andrea C. Arpaci-Dusseau , and Remzi H. Arpaci-Dusseau . 2016 . Serverless computation with openlambda . In Proceedings of the 8th USENIX Workshop on Hot Topics in Cloud Computing ( Denver , CO , USA ) ( HotCloud ’ 16 ) . USENIX Asso- ciation , Berkeley , CA , USA , 33–39 . [ 24 ] Shay Horovitz , Roei Amos , Ohad Baruch , Tomer Cohen , Tal Oyar , and Afik Deri . 2018 . FaaStest - Machine Learning Based Cost and Performance FaaS Optimization . In Proceedings of the International Conference on the Economics of Grids , Clouds , Systems , and Services ( Pisa , Italy ) ( GECON 2018 ) . Springer , Cham , Switzerland , 171–186 . https : [ 25 ] David Jackson and Gary Clynch . 2018 . An investigation of the impact of language runtime on the performance and cost of serverless functions . In Proceedings of the 2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion ( Zurich , Switzerland ) ( UCC Companion ) . IEEE , New York , NY , USA , 154–160 . https : [ 26 ] Jeongchul Kim and Kyungyong Lee . 2019 . FunctionBench : A Suite of Workloads for Serverless Cloud Function Service . In Proceedings of the 2019 IEEE International Conference on Cloud Computing ( Milan , Italy ) ( CLOUD ) . IEEE , New York , NY , USA , 502–504 . https : //doi.org/10.1109/CLOUD.2019.00091 [ 27 ] Danielle Lambion , Robert Schmitz , Robert Cordingly , Navid Heydari , and Wes Lloyd . 2022 . Characterizing X86 and ARM Serverless Performance Variation : A Natural Language Processing Case Study . In Companion of the 2022 ACM/SPEC International Conference on Performance Engineering ( Bejing , China ) . Association for Computing Machinery , New York , NY , USA , 69–75 . https : //doi.org/10.1145/ 3491204.3543506 [ 28 ] Pedro García López , Marc Sánchez-Artigas , Gerard París , Daniel Barcelona Pons , Álvaro Ruiz Ollobarren , and David Arroyo Pinto . 2018 . Comparison of FaaS orchestration systems . In Proceedings of the 2018 IEEE/ACM International Con- ference on Utility and Cloud Computing Companion ( Zurich , Switzerland ) ( UCC Companion ) . IEEE , New York , NY , USA , 148–153 . https : //doi.org/10.1109/UCC- Companion.2018.00049 [ 29 ] Nima Mahmoudi and Hamzeh Khazaei . 2021 . Temporal Performance Modelling of Serverless Computing Platforms . In Proceedings of the 2020 Sixth International Workshop on Serverless Computing ( Delft , Netherlands ) ( WoSC ’ 20 ) . Association for Computing Machinery , New York , NY , USA , 1–6 . https : //doi.org/10.1145/ 3429880.3430092 [ 30 ] Maciej Malawski , Kamil Figiela , Adam Gajek , and Adam Zima . 2017 . Benchmark- ing Heterogeneous Cloud Functions . In Proceedings of Euro-Par 2017 : Parallel Processing Workshops ( Santiago de Compostela , Spain ) ( Euro-Par 2017 ) . Springer , Cham , Switzerland , 415–426 . https : [ 31 ] Johannes Manner , Martin Endreß , Tobias Heckel , and Guido Wirtz . 2018 . Cold Start Influencing Factors in Function as a Service . In Proceedings of the 2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion ( Zurich , Switzerland ) ( UCC Companion ) . IEEE , New York , NY , USA , 181–188 . https : [ 32 ] Garrett McGrath and Paul R Brenner . 2017 . Serverless computing : Design , imple- mentation , and performance . In Proceedings of the 2017 IEEE 37th International Conference on Distributed Computing Systems Workshops ( Atlanta , GA , USA ) ( ICD- CSW ) . IEEE , New York , NY , USA , 405–410 . https : //doi.org/10.1109/ICDCSW . 2017.36 [ 33 ] Tommi Nylander , Johan Ruuskanen , Karl-Erik Årzén , and Martina Maggio . 2020 . Towards Performance Modeling of Speculative Execution for Cloud Applications . In Proceedings of the Companion of the ACM/SPEC International Conference on Performance Engineering ( Edmonton AB , Canada ) ( ICPE ’ 20 ) . Association for Computing Machinery , New York , NY , USA , 17–19 . https : //doi.org/10.1145/ 3375555.3384379 [ 34 ] István Pelle , János Czentye , János Dóka , and Balázs Sonkoly . 2019 . Towards latency sensitive cloud native applications : A performance study on AWS . In Proceedings of the 2019 IEEE 12th International Conference on Cloud Computing ( Milan , Italy ) ( CLOUD ) . IEEE , New York , NY , USA , 272–280 . https : //doi.org/10 . 1109/CLOUD.2019.00054 [ 35 ] Joel Scheuner , Marcus Bertilsson , Oskar Grönqvist , Henrik Tao , Henrik Lagergren , Jan-Philipp Steghöfer , and Philipp Leitner . 2022 . TriggerBench : A Performance Benchmark for Serverless Function Triggers . In Proceedings of the 2022 IEEE International Conference on Cloud Engineering ( Asilomar , CA , USA ) ( IC2E 2022 ) . IEEE , New York , NY , USA , 96–103 . https : [ 36 ] Trever Schirmer , Joel Scheuner , Tobias Pfandzelter , and David Bermbach . 2022 . Fusionize : Improving Serverless Application Performance through Feedback- Driven Function Fusion . In Proceedings of the 10th IEEE International Conference on Cloud Engineering ( Asilomar , CA , USA ) ( IC2E 2022 ) . IEEE , New York , NY , USA , 85–95 . https : [ 37 ] Mohammad Shahrad , Rodrigo Fonseca , Íñigo Goiri , Gohar Chaudhry , Paul Batum , Jason Cooke , Eduardo Laureano , Colby Tresness , Mark Russinovich , and Ricardo Bianchini . 2020 . Serverless in the Wild : Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider . In Proceedings of the 2020 USENIX Annual Technical Conference ( Virtual Event , USA ) ( USENIX ATC ’ 20 ) . USENIX , Berkeley , CA , USA , 205–218 . [ 38 ] Nikhila Somu , Nilanjan Daw , Umesh Bellur , and Purushottam Kulkarni . 2020 . PanOpticon : A Comprehensive Benchmarking Tool for Serverless Applications . In Proceedings of the 2020 International Conference on COMmunication Systems & NETworkS ( Bengaluru , India ) ( COMSNETS 2020 ) . IEEE , New York , NY , USA , 144–151 . https : [ 39 ] Charles Truong , Laurent Oudre , and Nicolas Vayatis . 2020 . Selective review of offline change point detection methods . Signal Processing 167 , Article 107299 ( Feb. 2020 ) . https : [ 40 ] Erwin van Eyk , Joel Scheuner , Simon Eismann , Cristina L. Abad , and Alexandru Iosup . 2020 . Beyond Microbenchmarks : The SPEC-RG Vision for a Comprehen- sive Serverless Benchmark . In Proceedings of the Companion of the ACM/SPEC International Conference on Performance Engineering ( Edmonton AB , Canada ) ( ICPE ’ 20 ) . Association for Computing Machinery , New York , NY , USA , 26–31 . https : //doi.org/10.1145/3375555.3384381 [ 41 ] Sebastian Werner , Jörn Kuhlenkamp , Markus Klems , Johannes Müller , and Stefan Tai . 2018 . Serverless Big Data Processing using Matrix Multiplication as Example . In Proceedings of the 2018 IEEE International Conference on Big Data ( Seattle , WA , USA ) ( Big Data ) . IEEE , New York , NY , USA , 358–365 . https : //doi.org/10.1109/ BigData.2018.8622362 [ 42 ] Sebastian Werner and Trever Schirmer . 2022 . Hardless : A Generalized Serverless Compute Architecture for Hardware Processing Accelerators . In Proceedings of the 10th IEEE International Conference on Cloud Engineering ( Asilomar , CA , USA ) ( IC2E 2022 ) . IEEE , New York , NY , USA , 79–84 . https : //doi.org/10.1109/IC2E55432 . 2022.00016 [ 43 ] Miao Zhang , Yifei Zhu , Cong Zhang , and Jiangchuan Liu . 2019 . Video processing with serverless computing : A measurement study . In Proceedings of the 29th ACM Workshop on Network and Operating Systems Support for Digital Audio and Video ( Amherst , MA , USA ) ( NOSSDAV ’ 19 ) . Association for Computing Machinery , New York , NY , USA , 61–66 . https : //doi.org/10.1145/3304112.3325608 [ 44 ] Haidong Zhao , Zakaria Benomar , Tobias Pfandzelter , and Nikolaos Georgantas . 2022 . Supporting Multi-Cloud in Serverless Computing . In Proceedings of the 15th IEEE/ACM International Conference on Utility and Cloud Computing Companion ( Vancouver , WA , USA ) ( UCC ’ 22 ) . IEEE , New York , NY , USA .","['night', 'shift', 'understand', 'performance', 'variability', 'serverless', 'platform', 'nil', 'ecdf', 'mcctuberlinde', 'trever', 'ecdf', 'mcctuberlinde', 'sofia', 'greten', 'ecdf', 'mcctuberlinde', 'r', 'p', 'c', 'c', 'v', 'r', 'tobias', 'pfandzelter', 'ecdf', 'mcctuberlinde', 'ecdf', 'mcctuberlinde', 'abstract', 'functionasaservice', 'popular', 'cloud', 'programming', 'model', 'support', 'developer', 'abstract', 'operational', 'concern', 'automatic', 'deployment', 'scaling', 'application', 'high', 'level', 'abstraction', 'developer', 'rely', 'cloud', 'platform', 'offer', 'consistent', 'service', 'level', 'decrease', 'performance', 'lead', 'high', 'latency', 'high', 'cost', 'give', 'payperuse', 'model', 'paper', 'measure', 'performance', 'variability', 'function', 'multiple', 'month', 'result', 'show', 'diurnal', 'pattern', 'lead', 'performance', 'difference', 'frequency', 'unexpected', 'cold', 'start', 'increase', 'threefold', 'start', 'week', 'behavior', 'negatively', 'impact', 'researcher', 'conduct', 'performance', 'study', 'cloud', 'platform', 'practitioner', 'run', 'cloud', 'application', 'ccs', 'concept', 'computer', 'system', 'organization', 'ply', 'computing', 'serviceoriente', 'architecture', 'software', 'engineering', 'software', 'performance', 'network', 'keyword', 'serverless', 'computing', 'function', 'service', 'performance', 'varia', 'tion', 'resource', 'contention', 'acm', 'reference', 'format', 'trever', 'schirmer', 'nil', 'sofia', 'greten', 'tobia', 'pfandzelter', 'night', 'shift', 'understand', 'performance', 'variability', 'cloud', 'serverless', 'platform', '1st', 'workshop', 'serverless', 'system', 'application', 'methodology', 'sesame', 'page', 'introduction', 'functionasaservice', 'faas', 'serverless', 'cloud', 'compute', 'livery', 'model', 'developer', 'compose', 'application', 'eventdriven', 'stateless', 'function', 'operational', 'task', 'man', 'age', 'cloud', 'provider', 'function', 'bill', 'sesame', 'copyright', 'hold', 'ownerauthor', 'publication', 'right', 'license', 'acm', 'author', 'version', 'work', 'post', 'personal', 'use', 'redistribution', 'definitive', 'version', 'record', 'publish', '1st', 'workshop', 'serverless', 'system', 'application', 'methodology', 'sesame', 'payperuse', 'basis', 'second', 'even', 'microsecond', 'granularity', 'offer', 'rapid', 'elasticity', 'scale', 'abstraction', 'operational', 'concern', 'make', 'faas', 'popular', 'cloud', 'execution', 'model', 'offering', 'major', 'cloud', 'provider', 'service', 'flip', 'side', 'high', 'level', 'resource', 'sharing', 'cloud', 'abstract', 'resource', 'management', 'developer', 'rely', 'cloud', 'platform', 'provider', 'offer', 'stable', 'consistent', 'performance', 'somewhat', 'counterintuitively', 'faas', 'user', 'actu', 'ally', 'pay', 'faas', 'platform', 'underperform', 'latency', 'high', 'bill', 'function', 'execution', 'also', 'long', 'cloud', 'faas', 'platform', 'offer', 'service', 'level', 'agreement', 'limited', 'guarantee', 'regard', 'general', 'uptime', 'cloud', 'computing', 'subject', 'performance', 'variation', 'faas', 'exception', 'previous', 'study', 'longterm', 'dayto', 'day', 'performance', 'change', 'faas', 'platform', 'show', 'general', 'improvement', 'faas', 'service', 'time', 'expect', 'platform', 'provider', 'update', 'advance', 'infrastructure', 'also', 'much', 'fine', 'effect', 'short', 'term', 'paper', 'benchmark', 'analyze', 'effect', 'use', 'highly', 'frequent', '40', 'cloud', 'faas', 'benchmark', 'course', 'month', 'function', 'receive', 'attention', 'previous', 'study', 'main', 'finding', 'performance', 'vary', 'greatly', 'course', 'day', 'increase', 'requestresponse', 'latency', 'time', 'many', 'unexpected', 'cold', 'start', 'day', 'night', 'day', 'effect', 'noticeable', 'start', 'week', 'finding', 'impact', 'interpret', 'result', 'cloud', 'faas', 'benchmark', 'significant', 'cloud', 'faas', 'developer', 'summary', 'make', 'follow', 'contribution', 'base', 'number', 'exist', 'serverless', 'benchmark', 'propose', 'methodology', 'evaluate', 'temporal', 'performance', 'variation', 'cloud', 'platform', '•', 'execute', 'benchmark', 'frequent', '40', 'run', 'cloud', 'function', 'course', 'month', 'show', 'requestresponse', 'latency', 'exhibit', 'strong', 'difference', 'diurnal', 'variation', '•', 'far', 'uncover', 'increase', 'unexpected', 'cold', 'start', 'specific', 'time', 'day', 'suggest', 'increase', 'rate', 'stance', 'recycling', 'time', 'high', 'demand', 'cloudgooglecomfunction', 'sesame', 'nil', 'sofia', 'greten', 'tobia', 'pfandzelter', 'evaluate', 'longterm', 'trend', 'datum', 'identify', 'possi', 'ble', 'cause', '•', 'survey', 'exist', 'cloud', 'faas', 'benchmarking', 'study', 'find', 'almost', 'third', 'provide', 'insufficient', 'information', 'performance', 'variability', 'effect', 'identify', 'control', '•', 'discuss', 'implication', 'finding', 'practitioner', 'run', 'application', 'cloud', 'faas', 'platform', 'order', 'enable', 'researcher', 'practitioner', 'extend', 'replicate', 'experiment', 'make', 'artifact', 'use', 'pro', 'duce', 'paper', 'available', 'opensource3', 'related', 'work', 'exist', 'research', 'faas', 'performance', 'variability', 'focus', 'ei', 'shortterm', 'longterm', 'week', 'variability', 'shortterm', 'study', 'include', 'report', 'lambion', 'benchmarke', 'performance', 'various', 'function', 'aw', 'lambda', 'course', 'day', 'author', 'execute', 'func', 'tion', 'different', 'time', 'zone', 'use', 'different', 'hardware', 'archi', 'tecture', 'find', 'short', 'function', 'duration', 'night', 'propose', 'analytical', 'performance', 'model', 'predict', 'performance', 'metric', 'function', 'validate', 'model', 'repeat', 'onehour', 'experiment', 'time', 'find', 'request', 'arrival', 'rate', 'average', 'response', 'time', 'function', 'timeout', 'use', 'predict', 'performance', 'minute', 'advance', 'ginzburg', 'freedman', 'analyze', 'performance', 'variation', 'aw', 'lambda', 'course', 'week', 'call', 'function', 'hour', 'show', 'daily', 'performance', 'function', 'region', 'region', 'vary', 'sig', 'nificantly', 'mainly', 'cause', 'local', 'inactivity', 'lack', 'performance', 'isolation', 'tenant', 'measure', 'week', 'analysis', 'focus', 'daily', 'variation', 'measure', 'longterm', 'study', 'serverless', 'system', 'present', 'execute', 'serverless', 'application', 'lambda', 'day', 'month', 'find', 'longterm', 'mance', 'change', 'likely', 'cause', 'platform', 'change', 'shortterm', 'variation', 'day', 'figelia', 'measure', 'performance', 'various', 'function', 'run', 'lambda', 'month', 'analyze', 'dataset', 'regular', 'variability', 'focus', 'paper', 'close', 'gap', 'longterm', 'shortterm', 'study', 'collect', 'frequent', 'measurement', 'long', 'period', 'time', 'additionally', 'go', 'focus', 'aw', 'lambda', 'present', 'general', 'methodology', 'experiment', 'applicable', 'faas', 'platform', 'collect', 'result', 'function', 'receive', 'attention', 'previous', 'study', 'popularity', 'methodology', 'assess', 'performance', 'variation', 'faas', 'repeatedly', 'execute', 'faas', 'function', 'short', 'interval', 'large', 'time', 'span', 'control', 'execution', 'region', 'resource', 'parameter', 'memory', 'option', 'cloud', 'faas', 'provider', 'function', 'type', 'focus', 'cloud', 'platform', 'performance', 'function', 'use', 'faas', 'function', 'exist', 'serverless', 'benchmark', 'experiment', 'function', 'perform', 'isolated', 'computation', 'rely', 'external', 'service', 'performance', 'influence', 'result', 'float', 'workload', 'perform', 'float', 'point', 'operation', 'matrix', 'function', 'werner', 'perform', 'matrix', 'multiplication', 'finally', 'adapt', 'face', 'detection', 'model', 'barosum', 'function', 'minimize', 'impact', 'external', 'fluctuation', 'measurement', 'embed', 'input', 'directly', 'function', 'resource', 'available', 'function', 'instance', 'determine', 'memory', 'configure', 'function', 'amount', 'vcpus', 'allocate', 'function', 'instance', 'tiere', 'increase', 'multiple', 'mb', 'memory', 'scale', 'linearly', 'memory', 'capture', 'effect', 'resource', 'configuration', 'deploy', 'float', 'matrix', 'mb', 'memory', 'deploy', 'mb', 'high', 'resource', 'requirement', 'execution', 'cloud', 'function', 'invocation', 'warm', 'cold', 'function', 'invoke', 'first', 'time', 'new', 'function', 'instance', 'create', 'call', 'cold', 'start', 'incur', 'creation', 'overhead', 'subsequent', 'possibly', 'parallel', 'invoca', 'tion', 'function', 'reuse', 'exist', 'instance', 'avoid', 'overhead', 'warm', 'start', 'typically', 'cloud', 'platform', 'keep', 'exist', 'instance', 'future', 'invocation', 'limited', 'amount', 'time', 'evict', 'reclaim', 'resource', 'capture', 'cold', 'warm', 'start', 'latency', 'invoke', 'func', 'tion', 'loop', 'first', 'call', 'function', 'create', 'cold', 'start', 'call', 'function', 'theory', 'second', 'invocation', 'serve', 'exist', 'function', 'instance', 'wait', 'minute', 'make', 'sure', 'next', 'function', 'call', 'cold', 'start', 'restart', 'loop', 'collect', 'measurement', 'point', 'minute', 'deploy', 'parallel', 'copy', 'function', 'cycle', 'metric', 'consider', 'main', 'metric', 'requestresponse', 'latency', 'unexpected', 'cold', 'start', 'longterm', 'trend', 'requestresponse', 'latency', 'use', 'bill', 'duration', 'output', 'func', 'tion', 'execution', 'faas', 'platform', 'unexpected', 'cold', 'start', 'cold', 'start', 'occur', 'directly', 'function', 'already', 'call', 'warm', 'unexpected', 'cold', 'start', 'imply', 'platform', 'unable', 'find', 'warm', 'function', 'instance', 'possibly', 'evict', 'resource', 'contention', 'finally', 'conduct', 'seasonal', 'trend', 'decomposition', 'use', 'loess', 'stl', 'handle', 'complex', 'seasonal', 'pattern', 'use', 'following', 'model', 'datum', 'represent', 'trend', 'represent', 'seasonal', 'compo', 'nent', 'day', 'case', 'represent', 'remain', 'noise', 'fit', 'datum', 'model', 'use', 'method', 'create', 'smoothed', 'estimate', 'seasonality', 'day', 'trend', 'component', 'show', 'overall', 'progress', 'bill', 'duration', 'whole', 'duration', 'experiment', 'nonflat', 'trend', 'line', 'indicate', 'longerterm', 'change', 'platform', 'occur', 'longterm', 'seasonal', 'change', 'update', 'platform', 'influence', 'performance', 'seasonal', 'component', 'show', 'periodic', 'recur', 'deviation', 'night', 'shift', 'understand', 'performance', 'variability', 'serverless', 'platform', 'sesame', 'figure', 'bill', 'duration', 'warm', 'call', 'float', 'mb', 'memory', 'work', 'hour', 'bill', 'duration', 'crease', 'area', 'dash', 'line', 'show', 'confidence', 'interval', 'datum', 'trend', 'component', 'random', 'noise', 'center', 'outlier', 'noise', 'indicate', 'performance', 'cific', 'time', 'explain', 'previous', 'component', 'trend', 'component', 'stl', 'also', 'use', 'change', 'point', 'detection', 'detect', 'structural', 'change', 'datum', 'experiment', 'section', 'present', 'initial', 'result', 'experiment', 'cloud', 'function', 'europewest3', 'region', 'measure', 'ment', 'period', 'start', 'end', 'report', 'execution', 'time', 'local', 'time', 'first', 'analyze', 'performance', 'variability', 'platform', 'afterwards', 'explore', 'unexpected', 'cold', 'start', 'outlier', 'well', 'change', 'point', 'longterm', 'trend', 'datum', 'performance', 'variability', 'analyze', 'performance', 'variability', 'analyze', 'bill', 'duration', 'comparable', 'invocation', 'show', 'bill', 'duration', 'float', 'function', 'mb', 'memory', 'figure', 'observe', 'clear', 'performance', 'increase', 'night', 'noticeable', 'latency', 'spike', 'work', 'hour', 'average', 'bill', 'duration', '106ms', 'increase', 'aggregate', 'day', 'week', 'average', 'bill', 'duration', 'fluctuate', '11386ms', 'saturday', '11728m', 'monday', 'overall', 'bill', 'duration', 'slightly', 'low', 'average', 'weekend', 'compare', 'start', 'week', 'weekly', 'trend', 'much', 'small', 'daily', 'trend', 'bill', 'duration', 'decrease', 'weekend', 'look', 'large', 'memory', 'size', 'relative', 'performance', 'change', 'time', 'become', 'small', 'show', 'figure', 'float', 'function', 'mb', 'memory', 'change', 'day', 'change', 'function', 'mb', 'memory', 'equally', 'change', 'average', 'day', 'noticeably', 'average', 'bill', 'duration', 'float', 'function', 'mb', 'memory', 'differ', 'day', 'float', 'b', 'figure', 'performance', 'change', 'day', 'warm', 'instance', 'order', 'memory', 'size', 'yaxis', 'normalize', 'av', 'erage', 'show', 'relative', 'change', 'big', 'value', 'show', 'big', 'deviation', 'average', 'bill', 'duration', 'area', 'curve', 'show', 'confidence', 'interval', 'explain', 'look', 'distribution', 'latency', 'value', 'invocation', 'figure', 'function', 'ory', 'size', 'follow', 'distribution', 'function', 'mb', 'memory', 'half', 'follow', 'distribu', 'tion', 'mb', 'function', 'result', 'matrix', 'function', 'exhibit', 'similar', 'result', 'omit', 'space', 'constraint', 'high', 'variability', 'performance', 'uneven', 'distribution', 'bill', 'duration', 'indicate', 'internally', 'use', 'mb', 'function', 'instance', 'handle', 'request', 'mb', 'function', 'also', 'show', 'cold', 'start', 'duration', 'also', 'follow', 'daily', 'pattern', 'show', 'configure', 'memory', 'size', 'impact', 'duration', 'cold', 'start', 'bill', 'duration', 'cold', 'start', 'follow', 'normal', 'dis', 'tribution', 'average', 'around', '910x', 'long', 'average', 'warm', 'latency', 'memory', 'size', 'follow', 'distribution', 'r', 'u', 'e', 'l', 'hour', 'day', 'memory', 'r', 'e', 'e', 'l', 'e', 'r', 'n', 'r', 'u', 'e', 'e', '−200', 'hour', 'day', 'memory', 'r', 'e', 'e', 'l', 'e', 'r', 'n', 'r', 'u', 'e', 'e', '−200', 'hour', 'day', 'sesame', 'nil', 'sofia', 'greten', 'tobia', 'pfandzelter', 'bermbach', 'figure', 'cumulative', 'distribution', 'bill', 'duration', 'float', 'function', 'cold', 'start', 'argue', 'use', 'mb', 'container', 'execute', 'function', 'figure', 'trend', 'component', 'stl', 'change', 'point', 'outlier', 'mark', 'horizontal', 'line', 'outlier', 'longterm', 'trend', 'base', 'stl', 'introduce', 'show', 'change', 'point', 'outlier', 'figure', 'define', 'outlier', 'hour', 'average', 'execution', 'duration', 'fourth', 'interquartile', 'range', 'time', 'difference', 'first', 'third', 'quartile', 'away', 'average', 'datum', 'contain', 'outlier', 'day', 'turn', 'month', 'indicate', 'platform', 'unusual', 'load', 'time', 'possibly', 'additional', 'load', 'monthly', 'job', 'change', 'point', 'point', 'average', 'execution', 'du', 'ration', 'change', 'occur', 'night', 'indicate', 'coincide', 'schedule', 'update', 'platform', 'whole', 'measurement', 'period', 'clear', 'permanent', 'trend', 'well', 'bad', 'overall', 'performance', 'base', 'longterm', 'study', 'expect', 'find', 'permanent', 'trend', 'long', 'measurement', 'period', 'compare', 'author', 'study', 'aw', 'find', 'statistical', 'trend', 'trend', 'component', 'change', '105ms', '128m', 'increase', 'implication', 'section', 'discuss', 'finding', 'impact', 'serverless', 'sys', 'tem', 'first', 'focus', 'implication', 'benchmarke', 'ward', 'describe', 'implication', 'serverless', 'application', 'figure', 'relative', 'frequency', 'unexpected', 'cold', 'start', 'aver', 'age', 'hour', 'week', 'happen', 'monday', 'day', 'invocation', 'unexpected', 'cold', 'start', 'compare', 'less', 'night', 'weekend', 'area', 'curve', 'show', 'fidence', 'interval', 'indicate', 'cold', 'start', 'overhead', 'dependent', 'resource', 'configure', 'change', 'function', 'configuration', 'unexpected', 'cold', 'start', 'show', 'relative', 'frequency', 'unexpected', 'cold', 'start', 'fig', 'ure', 'bill', 'duration', 'warm', 'instance', 'seem', 'follow', 'daily', 'trend', 'frequency', 'unexpected', 'cold', 'start', 'weekly', 'seasonality', 'clear', 'trend', 'increase', 'cold', 'start', 'work', 'hour', 'average', 'frequency', 'unexpected', 'cold', 'start', 'night', 'weekend', 'work', 'hour', 'fri', 'dur', 'work', 'hour', 'comparison', 'less', 'unexpected', 'warm', 'start', 'function', 'instance', 'still', 'warm', 'minute', 'validity', 'benchmark', 'run', 'benchmark', 'researcher', 'want', 'minimize', 'effect', 'external', 'factor', 'measurement', 'otherwise', 'find', 'result', 'case', 'benchmarke', 'serverless', 'system', 'show', 'number', 'cold', 'start', 'performance', 'function', 'undergoe', 'change', 'single', 'day', 'compara', 'tive', 'performance', 'study', 'benchmark', 'result', 'comparable', 'experiment', 'conduct', 'comparable', 'time', 'experi', 'ment', 'short', 'capture', 'performance', 'variation', 'entire', 'day', 'daily', 'variation', 'skew', 'result', 'similarly', 'long', 'term', 'performance', 'change', 'impact', 'measurement', 'take', 'r', 'e', 'u', 'c', 'r', 'memory', 'size', 'bill', 'duration', 'f', 'r', 'l', 'e', 'r', 'l', 'c', 'e', 'hour', 'week', 'stl', 'trend', 'change', 'point', 'outlier', 'r', 'u', 'e', 'l', 'c', 'e', 'r', 'c', 'e', 'j', 'j', 'e', 'e', 'function', 'start', 'night', 'shift', 'understand', 'performance', 'variability', 'serverless', 'platform', 'sesame', 'application', 'performance', 'performance', 'variability', 'show', 'affect', 'serverless', 'application', 'several', 'way', 'daytime', 'function', 'increase', 'latency', 'suffer', 'cold', 'start', 'tion', 'cost', 'increase', 'paybysecond', 'billing', 'model', 'lowlatency', 'eventdriven', 'function', 'feasible', 'postpone', 'execution', 'night', 'weekend', 'decrease', 'cost', 'possible', 'way', 'forward', 'however', 'shift', 'function', 'execution', 'cloud', 'region', 'well', 'performance', 'increase', 'network', 'latency', 'transmission', 'cost', 'reduc', 'tion', 'function', 'execution', 'time', 'associate', 'decrease', 'cost', 'outweigh', 'overhead', 'long', 'running', 'function', 'approach', 'require', 'constant', 'evaluation', 'faas', 'platform', 'performance', 'different', 'region', 'possibly', 'also', 'base', 'application', 'metric', 'researcher', 'also', 'propose', 'system', 'adapt', 'faas', 'appli', 'cation', 'improve', 'performance', 'cost', 'cloud', 'faas', 'platform', 'system', 'rely', 'initial', 'performance', 'measurement', 'tion', 'cloud', 'platform', 'feedback', 'loop', 'platform', 'application', 'optimizer', 'approach', 'fecte', 'performance', 'variability', 'faas', 'platform', 'model', 'differentiate', 'performance', 'change', 'cause', 'deployment', 'update', 'cause', 'platform', 'instability', 'possible', 'way', 'forward', 'system', 'control', 'temporal', 'performance', 'variation', 'deploy', 'multiple', 'parameter', 'set', 'concurrently', 'perform', 'long', 'initial', 'measurement', 'limitation', 'future', 'work', 'show', 'considerable', 'performance', 'variability', 'function', 'discuss', 'affect', 'application', 'performance', 'measurement', 'research', 'plan', 'build', 'initial', 'work', 'future', 'arrive', 'holistic', 'view', 'performance', 'variability', 'platform', 'faas', 'platform', 'initial', 'experiment', 'limit', 'mostly', 'additional', 'validation', 'aw', 'see', 'experiment', 'lambda', 'suffer', 'less', 'performance', 'variability', 'parameter', 'memory', 'size', 'hardware', 'architecture', 'geographical', 'region', 'programming', 'language', 'far', 'influence', 'variability', 'plan', 'conduct', 'additional', 'exper', 'iment', 'different', 'faas', 'platform', 'future', 'control', 'additional', 'parameter', 'platform', 'change', 'faas', 'platform', 'evolve', 'quickly', 'measurement', 'experiment', 'capture', 'behavior', 'platform', 'specific', 'point', 'time', 'continuous', 'update', 'increase', 'even', 'eliminate', 'performance', 'variability', 'effect', 'observe', 'future', 'make', 'continuous', 'measurement', 'important', 'subsequently', 'researcher', 'want', 'account', 'describe', 'behavior', 'measurement', 'faas', 'platform', 'conduct', 'experiment', 'use', 'methodology', 'measurement', 'result', 'outdate', 'performance', 'dimension', 'function', 'use', 'experiment', 'cpubound', 'give', 'good', 'indication', 'general', 'platform', 'performance', 'minimize', 'impact', 'performance', 'ex', 'ternal', 'service', 'cpu', 'performance', 'resource', 'metric', 'figure', 'execution', 'time', 'bench', 'mark', 'minimum', 'maximum', 'measured', 'difference', 'whole', 'experiment', 'mark', 'horizontal', 'line', 'long', 'period', 'time', 'eg', 'threeday', 'study', 'group', 'bench', 'mark', 'weekend', 'benchmarke', 'week', 'possible', 'remediation', 'experiment', 'adopt', 'parallel', 'benchmarke', 'technique', 'duet', 'benchmarke', 'show', 'overview', 'exist', 'publication', 'cloud', 'faas', 'performance', 'measurement', 'survey', 'paper', 'table', 'paper', 'give', 'overview', 'cloud', 'platform', 'test', 'cloud', 'region', 'time', 'day', 'benchmark', 'execution', 'state', 'base', 'metadata', 'assume', 'publish', 'sult', 'affect', 'daily', 'performance', 'variation', 'execution', 'time', 'give', 'different', 'region', 'use', 'imply', 'different', 'time', 'zone', 'overall', 'total', 'paper', 'provide', 'sufficient', 'information', 'rule', 'effect', 'performance', 'variability', 'mean', 'report', 'result', 'invalid', 'show', 'research', 'community', 'pay', 'enough', 'attention', 'effect', 'performance', 'measurement', 'example', 'replicate', 'experiment', 'compare', 'execution', 'time', 'dynamic', 'html', 'generator', 'deploy', 'function', 'eucentral1', 'lambda', 'region', 'mb', 'memory', 'run', 'sequential', 'vocation', 'hour', 'course', 'day', 'show', 'figure', 'performance', 'exhibit', 'temporal', 'variation', 'skew', 'result', 'small', 'performance', 'difference', 'experiment', 'happen', '23rd', 'mid', 'night', 'average', 'performance', 'difference', '28', 'slow', 'shortly', 'observe', 'large', 'perfor', 'mance', 'difference', 'average', 'difference', 'increase', '25', 'slow', 'performance', 'change', 'show', 'short', 'term', 'variation', 'performance', 'serverless', 'platform', 'significant', 'impact', 'benchmarke', 'result', 'need', 'control', 'recommend', 'repeat', 'experiment', 'course', 'day', 'mention', 'execution', 'time', 'describe', 'experiment', 'setup', 'difference', 'min', 'difference', 'e', 'e', 'time', 'execution', 'feb', 'sesame', 'nil', 'sofia', 'greten', 'tobia', 'pfandzelter', 'table', 'selection', 'serverless', 'benchmark', 'impact', 'shortterm', 'performance', 'fluctuation', 'bold', 'line', 'paper', 'daily', 'impact', 'rule', 'author', 'cloud', 'region', 'specify', 'execution', 'time', 'day', 'aw', 'azure', 'gcp', 'aws', 'azure', 'lópez', 'malawski', 'manner', 'mcgrath', 'pelle', 'scheuner', 'shahrad', 'somu', 'werner', 'aws', 'azure', 'aw', 'azure', 'gcp', 'aw', 'azure', 'gcp', 'aw', 'azure', 'gcp', 'aw', 'azure', 'aw', 'azure', 'gcp', 'aw', 'azure', 'aw', 'gcp', 'aw', 'gcp', 'aw', 'azure', 'gcp', 'aw', 'gcp', 'aw', 'gcp', 'multiple', 'region', 'entire', 'infrastructure', 'daily', 'month', 'minute', 'hourly', 'day', 'permanently', 'minute', 'memory', 'access', 'disk', 'io', 'network', 'latency', 'band', 'width', 'affect', 'platform', 'variability', 'finding', 'unconditionally', 'applicable', 'workload', 'bind', 'dimension', 'investigate', 'variation', 'additional', 'function', 'future', 'conclusion', 'paper', 'present', 'result', 'multimonth', 'performance', 'variability', 'benchmark', 'measure', 'performance', 'multiple', 'function', 'cloud', 'function', '40', 'result', 'show', 'execution', 'duration', 'function', 'vary', 'day', 'frequency', 'unexpected', 'cold', 'start', 'vary', 'week', 'day', 'resource', 'reduce', 'daily', 'performance', 'variability', 'shorten', 'cold', 'start', 'duration', 'look', 'longterm', 'trend', 'identify', 'likely', 'update', 'platform', 'outli', 'behavior', 'turn', 'month', 'result', 'implication', 'researcher', 'practitioner', 'reference', 'web', 'service', 'lambda', 'service', 'level', 'agreement', 'retrieve', 'feburary', 'https', 'awsamazoncomlambdasla', 'fink', 'rodric', 'suter', 'olivi', 'tardieu', 'serverless', 'trilemma', 'function', 'composition', 'serverless', 'computing', 'proceeding', 'international', 'symposium', 'new', 'idea', 'new', 'paradigm', 'reflection', 'programming', 'software', 'onward', 'association', 'compute', 'machinery', 'https', 'serverless', 'performance', 'optimization', 'strategy', 'proceeding', 'ieee', 'international', 'conference', 'smart', 'cloud', 'ieee', 'https', 'emad', 'barsoum', 'cristian', 'training', 'deep', 'network', 'facial', 'expression', 'recognition', 'crowdsource', 'label', 'distribution', 'proceeding', '18th', 'international', 'conference', 'multimodal', 'interaction', 'association', 'compute', 'chinery', 'https', 'doiorg10114529931482993165', 'aniruddha', 'gokhale', 'aleksander', 'slominski', 'brandic', 'rich', 'future', 'cloud', 'engineering', 'pro', 'ceeding', '9th', 'ieee', 'international', 'conference', 'acm', 'https', 'use', 'application', 'knowledge', 'reduce', 'cold', 'start', 'faas', 'service', 'proceeding', '35th', 'acm', 'symposium', 'apply', 'compute', 'brno', 'sac', 'association', 'compute', 'machinery', 'https', 'eventual', 'consistency', 'soon', 'eventual', 'evaluation', 'consistency', 'behavior', 'proceeding', '6th', 'workshop', 'middleware', 'service', 'orient', 'computing', 'portugal', 'acm', 'https', 'benchmarke', 'eventual', 'consistency', 'lesson', 'learn', 'longterm', 'experimental', 'study', 'proceeding', '2nd', 'ieee', 'international', 'conference', 'ieee', 'https', 'doiorg101109ic2e201437', 'wittern', 'cloud', 'service', 'benchmarke', 'measure', 'quality', 'cloud', 'service', 'client', 'perspective', 'lubomír', 'vojtěch', 'hork', 'françois', 'farquet', 'duet', 'benchmarke', 'improve', 'measurement', 'accuracy', 'cloud', 'proceeding', 'conference', 'performance', 'engineering', 'icpe', 'association', 'compute', 'chinery', 'https', 'aleksander', 'slominski', 'rise', 'serverless', 'computing', 'acm', 'https', 'terpenning', 'stl', 'seasonaltrend', 'decomposition', 'procedure', 'base', 'loess', 'journal', 'official', 'statistic', 'michal', 'podstawski', 'torsten', 'hoefler', 'sebs', 'serverless', 'benchmark', 'suite', 'service', 'computing', 'proceeding', '22nd', 'international', 'middleware', 'confer', 'ence', 'city', 'qc', 'middleware', 'association', 'compute', 'machinery', 'https', 'cordingly', 'function', 'memory', 'op', 'timization', 'heterogeneous', 'serverless', 'platform', 'cpu', 'time', 'account', 'proceeding', '10th', 'ieee', 'international', 'conference', 'neere', 'ieee', 'https', 'night', 'shift', 'understand', 'performance', 'variability', 'serverless', 'platform', 'sesame', 'jano', 'czentye', 'sonkoly', 'optimize', 'latency', 'sensitive', 'application', 'public', 'cloud', 'platform', 'proceeding', 'ieee', 'global', 'communication', 'village', 'ieee', 'https', 'corpaul', 'bezemer', 'case', 'study', 'stability', 'performance', 'test', 'serverless', 'application', 'journal', 'system', 'software', 'article', 'https', 'scheuner', 'maximilian', 'schwinger', 'server', 'less', 'application', 'ieee', 'software', 'sept', 'https', 'tarek', 'costless', 'optimize', 'cost', 'serverless', 'computing', 'function', 'fusion', 'placement', 'proceeding', 'ieeeacm', 'symposium', 'edge', 'compute', 'ieee', 'https', 'gajek', 'zima', 'beata', 'obrok', 'maciej', 'malawski', 'performance', 'evaluation', 'heterogeneous', 'cloud', 'function', 'concurrency', 'computation', 'practice', 'experience', 'article', 'e4792', 'https', 'serverless', 'serverless', 'measure', 'exploit', 'resource', 'variability', 'cloud', 'faas', 'platform', 'proceeding', 'sixth', 'international', 'workshop', 'serverless', 'computing', 'association', 'compute', 'machinery', 'https', 'cloud', 'function', 'service', 'level', 'agreement', 'sla', 'retrieve', 'feburary', 'https', 'cloudgooglecomfunctionssla', 'tobia', 'carsten', 'befaas', 'applicationcentric', 'benchmarking', 'framework', 'faas', 'platform', 'proceeding', '9th', 'ieee', 'international', 'con', 'ference', 'ieee', 'https', 'sturdevant', 'tyler', 'harter', 'arpacidusseau', 'serverless', 'computation', 'proceeding', '8th', 'usenix', 'workshop', 'hot', 'topic', 'hotcloud', 'usenix', 'asso', 'ciation', 'ohad', 'baruch', 'tomer', 'cohen', 'oyar', 'faastest', 'machine', 'learn', 'base', 'cost', 'performance', 'faas', 'optimization', 'proceeding', 'international', 'conference', 'economic', 'grid', 'cloud', 'system', 'service', 'https', 'investigation', 'impact', 'language', 'runtime', 'performance', 'cost', 'serverless', 'function', 'proceeding', 'ieeeacm', 'international', 'conference', 'utility', 'cloud', 'compute', 'ucc', 'companion', 'ieee', 'https', 'functionbench', 'suite', 'workload', 'serverless', 'cloud', 'function', 'service', 'proceeding', 'ieee', 'international', 'conference', 'cloud', 'ieee', 'https', 'lambion', 'cordingly', 'navid', 'characterize', 'x86', 'arm', 'serverless', 'performance', 'variation', 'natural', 'language', 'processing', 'case', 'study', 'companion', 'international', 'conference', 'performance', 'engineering', 'bejing', 'compute', 'machinery', 'https', 'pedro', 'garcía', 'pon', 'álvaro', 'ruiz', 'arroyo', 'pinto', 'comparison', 'faas', 'orchestration', 'system', 'proceeding', 'ieeeacm', 'international', 'con', 'ference', 'utility', 'cloud', 'compute', 'ucc', 'companion', 'ieee', 'https', 'doiorg101109ucc', 'hamzeh', 'khazaei', 'temporal', 'performance', 'modelling', 'serverless', 'computing', 'platform', 'proceeding', 'sixth', 'international', 'workshop', 'serverless', 'computing', 'association', 'compute', 'machinery', 'https', 'malawski', 'gajek', 'benchmark', 'ing', 'heterogeneous', 'cloud', 'function', 'proceeding', 'parallel', 'processing', 'workshop', 'europar', 'switzerland', 'https', 'johanne', 'manner', 'tobia', 'heckel', 'wirtz', 'cold', 'start', 'influence', 'factor', 'function', 'service', 'proceeding', 'ieeeacm', 'international', 'conference', 'utility', 'cloud', 'compute', 'ucc', 'companion', 'ieee', 'https', 'brenner', 'serverless', 'computing', 'design', 'mentation', 'performance', 'proceeding', 'ieee', '37th', 'international', 'conference', 'distribute', 'computing', 'system', 'workshop', 'ieee', 'https', 'tommi', 'nylander', 'performance', 'modeling', 'speculative', 'execution', 'cloud', 'application', 'proceeding', 'companion', 'conference', 'performance', 'engineering', 'icpe', 'association', 'compute', 'machinery', 'https', 'istván', 'pelle', 'sonkoly', 'latency', 'sensitive', 'cloud', 'native', 'application', 'performance', 'study', 'aw', 'proceeding', 'ieee', 'international', 'conference', 'cloud', 'ieee', 'https', 'doiorg10', 'scheuner', 'leitner', 'triggerbench', 'performance', 'benchmark', 'serverless', 'function', 'trigger', 'proceeding', 'ieee', 'international', 'conference', 'ieee', 'https', 'trever', 'schirmer', 'scheuner', 'tobia', 'pfandzelter', 'fusionize', 'improve', 'serverless', 'application', 'performance', 'feedback', 'drive', 'function', 'fusion', 'proceeding', '10th', 'ieee', 'international', 'conference', 'ieee', 'https', 'mohammad', 'fonseca', 'chaudhry', 'tresness', 'mark', 'russinovich', 'serverless', 'wild', 'characterize', 'optimize', 'serverless', 'workload', 'large', 'cloud', 'provider', 'proceeding', 'usenix', 'annual', 'technical', 'conference', 'virtual', 'event', 'usenix', 'daw', 'bellur', 'panopticon', 'comprehensive', 'benchmarke', 'tool', 'serverless', 'application', 'proceeding', 'international', 'conference', 'communication', 'system', 'network', 'bengaluru', 'comsnet', 'ieee', 'https', 'laurent', 'selective', 'review', 'offline', 'change', 'point', 'detection', 'method', 'signal', 'processing', 'article', 'https', 'erwin', 'scheuner', 'microbenchmark', 'specrg', 'vision', 'comprehen', 'sive', 'serverless', 'benchmark', 'proceeding', 'companion', 'conference', 'performance', 'engineering', 'icpe', 'association', 'compute', 'machinery', 'https', 'sebastian', 'werner', 'johanne', 'serverless', 'big', 'datum', 'processing', 'use', 'matrix', 'multiplication', 'example', 'proceeding', 'ieee', 'international', 'conference', 'big', 'datum', 'seattle', 'big', 'datum', 'ieee', 'https', 'sebastian', 'werner', 'schirmer', 'hardless', 'generalized', 'serverless', 'compute', 'architecture', 'hardware', 'processing', 'accelerator', 'proceeding', '10th', 'ieee', 'international', 'conference', 'ieee', 'https', 'doiorg101109ic2e55432', 'cong', 'video', 'processing', 'serverless', 'computing', 'measurement', 'study', 'proceeding', '29th', 'acm', 'workshop', 'network', 'operating', 'system', 'support', 'digital', 'audio', 'video', 'amherst', 'nossdav', 'association', 'compute', 'machinery', 'https', 'doiorg10114533041123325608', 'haidong', 'tobia', 'pfandzelter', 'georganta', 'support', 'multicloud', 'serverless', 'computing', 'proceeding', '15th', 'ieeeacm', 'international', 'conference', 'utility', 'cloud', 'compute', 'ucc', 'ieee']"
"ChatGPT cites the most-cited articles and journals, relying solely on
  Google Scholar's citation counts. As a result, AI may amplify the Matthew
  Effect in environmental science","[{'href': 'http://arxiv.org/abs/2304.06794v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2304.06794v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-04-13 19:29:49,"ChatGPT cites the most-cited articles and journals,
relying solely on Google Scholar's citation counts.
As a result, AI may amplify the Matthew Effect in
environmental science

Eduard Petiška; Charles University (Prague, Czech Republic); eduard.petiska@czp.cuni.cz
Assistant and respondent: ChatGPT; OpenAI, Inc; (San Francisco, CA)
April 11, 2023

Abstract

Introduction
ChatGPT (GPT) has become one of the most talked-about innovations in recent years, with
over 100 million users worldwide. However, there is still limited knowledge about the sources
of information GPT utilizes. As a result, we carried out a study focusing on the sources of
information within the field of environmental science. Our study aims to address the research
question: ""Does ChatGPT predominantly cite the most-cited publications in environmental
science?""

Method
In our study, we asked GPT to identify the ten most significant subdisciplines within the field
of environmental science. We then asked it to compose a scientific review article on each
subdiscipline, including 25 references. We proceeded to analyze these references, focusing
on factors such as the number of citations, publication date, and the journal in which the
work was published.

Results
Our findings indicate that GPT tends to cite highly-cited publications in environmental
science, with a median citation count of 1184.5. It also exhibits a preference for older
publications, with a median publication year of 2010, and predominantly refers to
well-respected journals in the field, with Nature being the most cited journal by GPT.
Interestingly, our findings suggest that GPT seems to exclusively rely on citation count data
from Google Scholar for the works it cites, rather than utilizing citation information from other
scientific databases such as Web of Science or Scopus.

Conclusion
In conclusion, our study suggests that Google Scholar citations play a significant role as a
predictor for mentioning a study in GPT-generated content. This finding reinforces the
dominance of Google Scholar among scientific databases and perpetuates the Matthew
Effect in science, where the rich get richer in terms of citations. With many scholars already
utilizing GPT for literature review purposes, we can anticipate further disparities and an
expanding gap between lesser-cited and highly-cited publications.

Keywords: ChatGPT; Environmental Science, Matthew's effect, citations

1

Introduction

ChatGPT, a powerful language model, has demonstrated its ability to generate

coherent and well-structured text, prompting its application in writing the introduction and
abstract sections of scientific articles.1 In fact, several papers have acknowledged ChatGPT
as a co-author.2 While its potential in research and academia is evident, concerns have
arisen regarding its implications on education and integrity.3

Professor Darren Hick from Furman University identified ChatGPT's ""style"" in a

student's paper, proposing a policy to administer ad-hoc oral exams if AI-generated
submissions are suspected.4 In response to concerns, the New York City Department of
Education blocked access to ChatGPT. Scientific journals have taken varying stances on
ChatGPT. Some, like Nature and JAMA Network, mandate disclosure of text-generating tool
usage and prohibit listing ChatGPT as a co-author, while Science has banned the use of
large language model-generated text entirely.56

As GPT has released in November, it is still a relatively new field of scientific

research, although more than 1 000 papers were already published7, still, less is known
about the application of ChatGPT in concrete disciplines such as environmental science or
scientometrics. In this study, we have concentrated our efforts on analyzing citations in the
field of environmental science to investigate whether the Matthew effect, a well-known
concept in the scientific community, also influences the citation patterns in
ChatGPT-generated responses

Mathew effect in science

The Matthew effect (rooted in the Gospel of Matthew) in science, a phenomenon first

introduced by Robert K. Merton (1968)8, has attracted significant attention in recent years
due to its implications on the reward and communication systems of scientific research. This
effect, which refers to the disproportionate allocation of rewards and recognition to
well-established researchers, can impede scientific progress by inhibiting the growth of
emerging scientists (Petersen et al., 2018;9 Li & Chen, 201910). Studies have demonstrated
that the Matthew effect can lead to disparities in funding, career opportunities, and citation

1 Bushard, Brian (January 10, 2023). ""Fake Scientific Abstracts Written By ChatGPT Fooled
Scientists, Study Finds"". Forbes. Archived from the original on February 3, 2023.
2 Stokel-Walker, Chris (January 18, 2023). ""ChatGPT listed as author on research papers: many
scientists disapprove"". Nature. 613 (7945): 620–621.
3 Stokel-Walker, Chris (December 9, 2022). ""AI bot ChatGPT writes smart essays — should
professors worry?"". Nature. doi:10.1038/d41586-022-04397-7
4 Mitchell, Alex (December 26, 2022). ""Students using ChatGPT to cheat, professor warns"". The New
York Post.
5 Brainard, Jeffrey (February 22, 2023). ""As scientists explore AI-written text, journals hammer out
policies"". Science. doi:10.1126/science.adh2937
6 https://en.wikipedia.org/wiki/ChatGPT
7 https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=allintitle%3AChatGPT&btnG=&oq=al
8 Merton, R. K. (1968). The Matthew effect in science: The reward and communication systems of
science are considered. Science, 159(3810), 56-63.
9 Petersen, A. M., Fortunato, S., Pan, R. K., Kaski, K., Penner, O., Rungi, A., ... & Barabási, A. L.
(2018). Reputation and impact in academic careers. Proceedings of the National Academy of
Sciences, 111(43), 15316-15321.
10 Li, E. Y., & Chen, Y. (2019). The Matthew effect in scientific funding. Proceedings of the National
Academy of Sciences, 116(16), 7684-7689.

2

counts for researchers (Allison & Stewart, 197411). In addition to disadvantaging early-career
scientists (Bol et al., 201812), the Matthew effect can also result in a skewed distribution of
research attention and resources toward certain fields or topics (Wang et al., 201313).

This effect is pronounced for example in the case of Wikipedia, as the platform often

serves as a gateway to knowledge. As a result, articles on Wikipedia that cite
well-established publications inadvertently reinforce the Matthew effect, as these sources
continue to accumulate citations and dominate their respective fields (Thompson, N., &
Hanley, D.,2018).14

Similarly, the Matthew effect in Google Scholar is an example of how the digital

platform may amplify the visibility and recognition of the most cited publications, thereby
reinforcing existing hierarchies in scientific research. As an influential search engine for
scholarly literature, Google Scholar employs algorithms that prioritize highly-cited articles in
its search results. This process can create a self-perpetuating cycle, where articles that are
already well-established in their respective fields continue to receive even more citations,
while lesser-known publications remain relatively obscure. Consequently, the Matthew effect
in Google Scholar can contribute to a skewed distribution of attention and resources,
potentially hindering the discovery of innovative ideas and research findings.

We hypothesize that the Matthew effect also plays a crucial role in the case of GPT.
Since there are no studies focused on citation usage by ChatGPT and its decision-making
process for selecting the papers it cites, we asked the model the following question: ""How do
you choose which academic studies to cite?"" The response we received was: ""As an AI
language model, when generating responses, I rely on the information and data available
during my training, which includes a wide range of academic studies, articles, and other
sources up to my completion date (September 2021). When selecting which academic
studies to cite, I consider several factors, including the relevance of the study to the question
or topic at hand, the quality and reliability of the research, and the impact and influence of
the publication within its field. Additionally, I may give more weight to studies that have been
cited frequently or published in well-regarded journals, as these are often indicators of the
importance and contribution of the research. However, it is important to note that my
citations are generated algorithmically, and I do not have personal biases or preferences in
selecting which academic studies to cite.” As we can see, the answer provided by GPT is
somewhat vague and lacks specificity, so we attempted to obtain more information from the
model through subsequent discussion (more details can be found in the Discussion section).

11 Allison, P. D., & Stewart, J. A. (1974). Productivity differences among scientists: Evidence for
accumulative advantage. American Sociological Review, 39(4), 596-606.
12 Bol, T., de Vaan, M., & van de Rijt, A. (2018). The Matthew effect in science funding. Proceedings of
the National Academy of Sciences, 115(19), 4887-4890.
13 Wang, J., Veugelers, R., & Stephan, P. (2013). Bias against novelty in science: A cautionary tale for
users of bibliometric indicators. NBER Working Paper No. 19135.
14 Thompson, N., & Hanley, D. (2018). Science is shaped by Wikipedia: evidence from a randomized
control trial.

3

Method

We pose the research question: ""Does ChatGPT predominantly cite the most-cited

publications in environmental science?"" To answer this question, we followed several steps.
First, we asked ChatGPT-4 to perform the following tasks:

1. Write the 10 most important subdisciplines of environmental science;
2. Compose an introduction for a research article (commonly referred to as a ""review

article"") about ""subdiscipline"" (Ecology, Conservation Biology, etc.), ensuring that
every statement within the introduction is supported by citations from relevant
research articles. Include 25 references in total for this introduction;

3. Provide a list of all references in the table: date, title, and publisher in a separate

column, so the table has 4 columns, including rank;

4. Now write the number of Google Scholar citations for each reference into the table,

so that it has five columns.

After that, we analyzed responses from various disciplines' data (including citations
and journals) based on the criteria mentioned earlier for each subdiscipline. These criteria
included the oldest and newest citation, the median date of the citation, the publication with
the lowest and highest number of citations, and the median number of citations. Finally, we
asked GPT to determine the most cited journals in the responses it provided across the
disciplines.

4

Picture 1. An illustration of the text section with the citations provided by GPT

5

Results

Table 1. The top 10 most important subdisciplines of environmental science, according to
GPT

No. Subdiscipline

Description

1

Ecology

Study of interactions between organisms and their environment, and the
factors affecting these interactions.

2

3

4

5

Conservation
Biology

Study of preserving biodiversity, species, and ecosystems through habitat
restoration and protected areas.

Environmental
Chemistry

Examination of the chemical and biochemical processes in the
environment, including pollution and natural cycles.

Environmental
Engineering

Design and implementation of solutions to mitigate environmental
problems, such as pollution control and waste management.

Environmental
Policy and
Management

Development, implementation, and analysis of policies and strategies to
address environmental issues and promote sustainability.

6

Climatology

Study of climate patterns, changes, and their impact on ecosystems and
human society.

7

Hydrology

Study of the distribution, movement, and quality of water on Earth,
including surface and groundwater resources.

8

9

Environmental
Health

Study of how environmental factors, such as pollution and climate
change, impact human health and well-being.

Sustainable
Development

Pursuit of economic, social, and environmental objectives without
compromising the ability of future generations to meet their needs.

10

Renewable
Energy

Development and implementation of sustainable energy sources, such as
solar, wind, and hydroelectric power.

6

Table 2. The overview of the average number of citations received by research articles cited
by GPT

Topic

Oldest
citation

Newest
citation

Ecology

1973

2019

Conservation Biology

1985

2016

1961

2019

Median
date of
the
citation

Publication
with the
lowest
citations

Publicati
on with
most
citations

The
median
number of
citations

2012

2004

1999

436

790

183

8763

9400

8686

1150

1100

1762

Environmental
Chemistry

Environmental
Engineering

Environmental Policy
and Management

1972

2013

2001

38

1024

5590

1986

2020

2009

155

1045

11,030

Climatology

2005

2015

2012

287

37840

1,263

Hydrology

2000

2017

Environmental Health

2003

2016

2007

2011

94

66

2308

3429

339

405

Sustainable
Development

1987

2020

2011

162

18278

1269

Renewable Energy

2008

2021

Total

1961

2021

2015

2010

7

38

16,324

476

37840

1184.5

7

Table 3. The top 10 most cited journals by GPT across disciplines

Rank Journal

Citations

1

2

3

4

5

6

7

8

9

Nature

Science

Proceedings of the National Academy of Sciences

Annual Review of Environment and Resources

Global Environmental Change

Trends in Ecology & Evolution

Annual Review of Ecology, Evolution, and Systematics

BioScience

Global Change Biology

10

Ecological Applications

17

13

8

6

5

4

4

4

4

4

8

Discussion

There are other questions related to ChatGPT's selection of research publications, as

little is currently known about it. In order to shed more light on this topic, we posed several
additional questions to GPT and have presented them below:15
____

“How did you choose and rank the citations?”
“I selected and ranked the top 10 most important subdisciplines of environmental science
based on their broad relevance and impact on addressing current environmental challenges.
The selected subdisciplines are essential for understanding the complex interactions within
ecosystems, preserving biodiversity, managing resources, and promoting sustainable
development. However, it is important to note that this ranking is subjective and may vary
depending on individual preferences and specific contexts. Each subdiscipline in
environmental science has its importance and contributes to our overall understanding of the
environment and sustainability. The selection of the top 10 was made to provide an overview
of some of the most significant and widely applicable areas of study within the field.”

“Up to which date do you count citations?”
“The Google Scholar citation counts provided in the table are up to my knowledge cutoff
date, which is September 2021. Please note that the citation counts may have changed
since then, as new citations are added over time.”

“Can you provide me with citations from the Web of Science?”
“As an AI language model, I am unable to access Web of Science directly to provide you
with real-time citation counts. The information I have provided in this conversation is based
on my training data, which goes up until September 2021. For the most current and accurate
citation counts from Web of Science, you will need to access the database directly or ask
someone with access to provide the information for you.”

“And as of September 2021, what is the number of citations in Web of Science?”
“I apologize for any confusion. As an AI language model, I do not have direct access to the
Web of Science database or its citation counts. My knowledge is based on the training data
provided to me, which includes citation counts from Google Scholar up until September
2021. For citation counts from Web of Science, you would need to access the database
directly or ask someone with access to provide the information for you.”

""What about Scopus?”
“As an AI language model, I am unable to access Scopus or its citation data directly. The
information I have provided in this conversation is based on my training data, which goes up
until September 2021, and it includes citation counts from Google Scholar.
For the most current and accurate citation counts from Scopus, you will need to access the
database directly or ask someone with access to provide the information for you.”

15 The prompts in the article may have been proofreaded, and GPT's responses can vary depending
on the specific conversation. Additionally, some sections of this article may refer to previous versions
of GPT.

9

""How do you choose the citations that you mention?""
As an AI language model, I based the selection of the 25 references on their relevance to
the topic of ""Conservation Biology."" My training data includes a wide range of scientific
articles and sources, which allows me to identify important and frequently cited works related
to the field.
When composing the introduction for the research article, I considered key research areas
and subtopics within conservation biology, such as habitat loss, climate change, invasive
species, overexploitation, and pollution. I then selected references that were representative
of these themes and that have been widely cited in the literature. This approach aimed to
provide a comprehensive and well-rounded overview of the field, using sources that have
made significant contributions to conservation biology.
Please note that the selection of references is not exhaustive, and there may be other
important works that were not included. The list of references provided is meant to serve as
a starting point for further research and exploration of the field.

“Can you provide more details on how you chose the 25 citations and the algorithm used to
do so?""
As an AI language model, I consider various factors when selecting research articles to
mention, and the number of citations in Google Scholar is one of those factors. A high
citation count often indicates that a study has had a significant impact on its field, as it has
been frequently referenced by other researchers. However, it is not the only criteria I use
when selecting articles to mention.

Other factors I consider include:
Relevance: How well does the article align with the topic being discussed? I prioritize articles
that are closely related to the subject matter.
Quality: Has the article been published in a reputable journal or by a well-regarded
publisher? Peer-reviewed articles from established journals generally have a higher level of
credibility.
Recency: How recent is the publication? While older articles can be foundational and
influential, more recent articles may reflect current trends and advances in the field.
Author expertise: Are the authors of the article recognized experts in their field? Articles by
well-respected researchers can carry more weight.
Novelty: Does the article present a new or innovative approach, theory, or finding? Articles
that introduce new ideas or challenge existing paradigms may be more likely to be
mentioned.

In summary, while the number of Google Scholar citations is an important factor, it is not the
sole predictor for mentioning a research article. I take a holistic approach, considering
multiple factors to ensure that the selected articles are relevant, high-quality, and
representative of the field.

“Can you be more concrete about how did you choose these 25 citations, and provide some
algorithm which you used?”
“Certainly! Here is the algorithm I used to select the 25 citations for the introduction on
Conservation Biology:

10

1.

I first identified key subtopics within Conservation Biology, such as habitat loss,
climate change, invasive species, overexploitation, and pollution.

3.

2. For each subtopic, I conducted a search using a scientific database (such as Google
Scholar) to identify highly cited articles that are widely considered to be influential
and impactful in the field.
I reviewed the abstracts and summaries of these articles to assess their relevance
and importance to the subtopic.
I then selected the most highly cited articles that were representative of each
subtopic and that provided a comprehensive and well-rounded overview of the field
of Conservation Biology.
I ensured that the selected articles were published in reputable journals or by
well-regarded publishers, and that they were authored by recognized experts in the
field.

5.

4.

6. Finally, I compiled the 25 citations into a list, taking care to provide a balanced

representation of the different subtopics and areas of research within Conservation
Biology.

This algorithm allowed me to select a diverse range of highly cited articles that are
considered to be important and influential in the field of Conservation Biology.”

____

During the research, several problems and issues arose, such as the fact that GPT

provides references somewhat randomly. References can serve as pointers to further
literature rather than specifically supporting concrete statements. Additionally, the
statements generated by GPT can be too vague to require references.

It is interesting to note that ChatGPT can be inconsistent, sometimes initially
providing references but later claiming not to have them or being unable to provide them. For
instance, GPT might state: ""Due to the limitations of this AI, I cannot access real-time
Google Scholar citations or verify the publisher for each reference. However, I can provide a
list of the references with the date and title in a table format. You can later manually add the
publisher and citations from Google Scholar."" To address this issue, we either opened a new
window or continued in the chat window where citations were provided, or we tried using a
different prompt.

It was observed that GPT can provide different answers to the same queries,

especially in previous versions. However, based on the prompts we tried, the pattern in
citation behavior remained consistent.

11

Conclusion

GPT is already changing the world of knowledge. It has become the subject of
scientific research, which in this field also proves that traditional academic publishing
methods are becoming obsolete (similar to the case of the Covid pandemic). Preprints,
Twitter, and Reddit are increasingly proving to be efficient ways of distributing knowledge in
these respects. If we want to find out the latest information about GPT, for example, we can
use the following channels:

1) Twitter search for GPT16 (many tweets are relevant to scientific audiences as

a new tool for scientists – Perplexity17);

2) Reddit search for GPT18 and the forum ChatGPT19, where you can find weekly

updates20;

3) Google News (e.g., US edition) for ChatGPT21;
4) Google Scholar (allintitle:ChatGPT)22
5) Dimensions (can be filtered according to Altmetric and scientific areas to

identify the most discussed research articles in your field)23

In conclusion, GPT appears to be influenced by the current assessment of scientific

quality (primarily through citations) in environmental science, even though, according to
GPT, citations are not the sole predictors for mentioning a study. We hypothesize that this
citation pattern is prevalent across various disciplines; however, further research is
necessary to test this hypothesis.

We would like to acknowledge the assistance of an AI language model, ChatGPT, in

generating responses for this research article.

16 https://twitter.com/search?q=GPT&src=typed_query
17 https://twitter.com/Artifexx/status/1645303838595858432
18 https://www.reddit.com/search/?q=GPT
19 https://www.reddit.com/r/ChatGPT/
20

https://www.reddit.com/r/ChatGPT/comments/12diapw/gpt4_week_3_chatbots_are_yesterdays_news
_ai/
21 https://news.google.com/search?q=chatGPT&hl=en-US&gl=US&ceid=US%3Aen
22 https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=allintitle%3AChatGPT&btnG=
23

https://app.dimensions.ai/discover/publication?search_mode=content&search_text=chatgpt&search_t
ype=kws&search_field=text_search&order=altmetric&and_facet_for=80003

12

","ChatGPT cites the most-cited articles and journals , relying solely on Google Scholar 's citation counts . As a result , AI may amplify the Matthew Effect in environmental science Eduard Petiška ; Charles University ( Prague , Czech Republic ) ; eduard.petiska @ czp.cuni.cz Assistant and respondent : ChatGPT ; OpenAI , Inc ; ( San Francisco , CA ) April 11 , 2023 Abstract Introduction ChatGPT ( GPT ) has become one of the most talked-about innovations in recent years , with over 100 million users worldwide . However , there is still limited knowledge about the sources of information GPT utilizes . As a result , we carried out a study focusing on the sources of information within the field of environmental science . Our study aims to address the research question : `` Does ChatGPT predominantly cite the most-cited publications in environmental science ? '' Method In our study , we asked GPT to identify the ten most significant subdisciplines within the field of environmental science . We then asked it to compose a scientific review article on each subdiscipline , including 25 references . We proceeded to analyze these references , focusing on factors such as the number of citations , publication date , and the journal in which the work was published . Results Our findings indicate that GPT tends to cite highly-cited publications in environmental science , with a median citation count of 1184.5 . It also exhibits a preference for older publications , with a median publication year of 2010 , and predominantly refers to well-respected journals in the field , with Nature being the most cited journal by GPT . Interestingly , our findings suggest that GPT seems to exclusively rely on citation count data from Google Scholar for the works it cites , rather than utilizing citation information from other scientific databases such as Web of Science or Scopus . Conclusion In conclusion , our study suggests that Google Scholar citations play a significant role as a predictor for mentioning a study in GPT-generated content . This finding reinforces the dominance of Google Scholar among scientific databases and perpetuates the Matthew Effect in science , where the rich get richer in terms of citations . With many scholars already utilizing GPT for literature review purposes , we can anticipate further disparities and an expanding gap between lesser-cited and highly-cited publications . Keywords : ChatGPT ; Environmental Science , Matthew 's effect , citations 1 Introduction ChatGPT , a powerful language model , has demonstrated its ability to generate coherent and well-structured text , prompting its application in writing the introduction and abstract sections of scientific articles.1 In fact , several papers have acknowledged ChatGPT as a co-author.2 While its potential in research and academia is evident , concerns have arisen regarding its implications on education and integrity.3 Professor Darren Hick from Furman University identified ChatGPT 's `` style '' in a student 's paper , proposing a policy to administer ad-hoc oral exams if AI-generated submissions are suspected.4 In response to concerns , the New York City Department of Education blocked access to ChatGPT . Scientific journals have taken varying stances on ChatGPT . Some , like Nature and JAMA Network , mandate disclosure of text-generating tool usage and prohibit listing ChatGPT as a co-author , while Science has banned the use of large language model-generated text entirely.56 As GPT has released in November , it is still a relatively new field of scientific research , although more than 1 000 papers were already published7 , still , less is known about the application of ChatGPT in concrete disciplines such as environmental science or scientometrics . In this study , we have concentrated our efforts on analyzing citations in the field of environmental science to investigate whether the Matthew effect , a well-known concept in the scientific community , also influences the citation patterns in ChatGPT-generated responses Mathew effect in science The Matthew effect ( rooted in the Gospel of Matthew ) in science , a phenomenon first introduced by Robert K. Merton ( 1968 ) 8 , has attracted significant attention in recent years due to its implications on the reward and communication systems of scientific research . This effect , which refers to the disproportionate allocation of rewards and recognition to well-established researchers , can impede scientific progress by inhibiting the growth of emerging scientists ( Petersen et al. , 2018 ; 9 Li & Chen , 201910 ) . Studies have demonstrated that the Matthew effect can lead to disparities in funding , career opportunities , and citation 1 Bushard , Brian ( January 10 , 2023 ) . `` Fake Scientific Abstracts Written By ChatGPT Fooled Scientists , Study Finds '' . Forbes . Archived from the original on February 3 , 2023 . 2 Stokel-Walker , Chris ( January 18 , 2023 ) . `` ChatGPT listed as author on research papers : many scientists disapprove '' . Nature . 613 ( 7945 ) : 620–621 . 3 Stokel-Walker , Chris ( December 9 , 2022 ) . `` AI bot ChatGPT writes smart essays — should professors worry ? '' . Nature . doi:10.1038/d41586-022-04397-7 4 Mitchell , Alex ( December 26 , 2022 ) . `` Students using ChatGPT to cheat , professor warns '' . The New York Post . 5 Brainard , Jeffrey ( February 22 , 2023 ) . `` As scientists explore AI-written text , journals hammer out policies '' . Science . doi:10.1126/science.adh2937 6 https : //en.wikipedia.org/wiki/ChatGPT 7 https : //scholar.google.com/scholar ? hl=en & as_sdt=0 % 2C5 & q=allintitle % 3AChatGPT & btnG= & oq=al 8 Merton , R. K. ( 1968 ) . The Matthew effect in science : The reward and communication systems of science are considered . Science , 159 ( 3810 ) , 56-63 . 9 Petersen , A. M. , Fortunato , S. , Pan , R. K. , Kaski , K. , Penner , O. , Rungi , A. , ... & Barabási , A. L. ( 2018 ) . Reputation and impact in academic careers . Proceedings of the National Academy of Sciences , 111 ( 43 ) , 15316-15321 . 10 Li , E. Y. , & Chen , Y . ( 2019 ) . The Matthew effect in scientific funding . Proceedings of the National Academy of Sciences , 116 ( 16 ) , 7684-7689 . 2 counts for researchers ( Allison & Stewart , 197411 ) . In addition to disadvantaging early-career scientists ( Bol et al. , 201812 ) , the Matthew effect can also result in a skewed distribution of research attention and resources toward certain fields or topics ( Wang et al. , 201313 ) . This effect is pronounced for example in the case of Wikipedia , as the platform often serves as a gateway to knowledge . As a result , articles on Wikipedia that cite well-established publications inadvertently reinforce the Matthew effect , as these sources continue to accumulate citations and dominate their respective fields ( Thompson , N. , & Hanley , D.,2018 ) .14 Similarly , the Matthew effect in Google Scholar is an example of how the digital platform may amplify the visibility and recognition of the most cited publications , thereby reinforcing existing hierarchies in scientific research . As an influential search engine for scholarly literature , Google Scholar employs algorithms that prioritize highly-cited articles in its search results . This process can create a self-perpetuating cycle , where articles that are already well-established in their respective fields continue to receive even more citations , while lesser-known publications remain relatively obscure . Consequently , the Matthew effect in Google Scholar can contribute to a skewed distribution of attention and resources , potentially hindering the discovery of innovative ideas and research findings . We hypothesize that the Matthew effect also plays a crucial role in the case of GPT . Since there are no studies focused on citation usage by ChatGPT and its decision-making process for selecting the papers it cites , we asked the model the following question : `` How do you choose which academic studies to cite ? '' The response we received was : `` As an AI language model , when generating responses , I rely on the information and data available during my training , which includes a wide range of academic studies , articles , and other sources up to my completion date ( September 2021 ) . When selecting which academic studies to cite , I consider several factors , including the relevance of the study to the question or topic at hand , the quality and reliability of the research , and the impact and influence of the publication within its field . Additionally , I may give more weight to studies that have been cited frequently or published in well-regarded journals , as these are often indicators of the importance and contribution of the research . However , it is important to note that my citations are generated algorithmically , and I do not have personal biases or preferences in selecting which academic studies to cite. ” As we can see , the answer provided by GPT is somewhat vague and lacks specificity , so we attempted to obtain more information from the model through subsequent discussion ( more details can be found in the Discussion section ) . 11 Allison , P. D. , & Stewart , J . A . ( 1974 ) . Productivity differences among scientists : Evidence for accumulative advantage . American Sociological Review , 39 ( 4 ) , 596-606 . 12 Bol , T. , de Vaan , M. , & van de Rijt , A . ( 2018 ) . The Matthew effect in science funding . Proceedings of the National Academy of Sciences , 115 ( 19 ) , 4887-4890 . 13 Wang , J. , Veugelers , R. , & Stephan , P. ( 2013 ) . Bias against novelty in science : A cautionary tale for users of bibliometric indicators . NBER Working Paper No . 19135 . 14 Thompson , N. , & Hanley , D. ( 2018 ) . Science is shaped by Wikipedia : evidence from a randomized control trial . 3 Method We pose the research question : `` Does ChatGPT predominantly cite the most-cited publications in environmental science ? '' To answer this question , we followed several steps . First , we asked ChatGPT-4 to perform the following tasks : 1 . Write the 10 most important subdisciplines of environmental science ; 2 . Compose an introduction for a research article ( commonly referred to as a `` review article '' ) about `` subdiscipline '' ( Ecology , Conservation Biology , etc . ) , ensuring that every statement within the introduction is supported by citations from relevant research articles . Include 25 references in total for this introduction ; 3 . Provide a list of all references in the table : date , title , and publisher in a separate column , so the table has 4 columns , including rank ; 4 . Now write the number of Google Scholar citations for each reference into the table , so that it has five columns . After that , we analyzed responses from various disciplines ' data ( including citations and journals ) based on the criteria mentioned earlier for each subdiscipline . These criteria included the oldest and newest citation , the median date of the citation , the publication with the lowest and highest number of citations , and the median number of citations . Finally , we asked GPT to determine the most cited journals in the responses it provided across the disciplines . 4 Picture 1 . An illustration of the text section with the citations provided by GPT 5 Results Table 1 . The top 10 most important subdisciplines of environmental science , according to GPT No . Subdiscipline Description 1 Ecology Study of interactions between organisms and their environment , and the factors affecting these interactions . 2 3 4 5 Conservation Biology Study of preserving biodiversity , species , and ecosystems through habitat restoration and protected areas . Environmental Chemistry Examination of the chemical and biochemical processes in the environment , including pollution and natural cycles . Environmental Engineering Design and implementation of solutions to mitigate environmental problems , such as pollution control and waste management . Environmental Policy and Management Development , implementation , and analysis of policies and strategies to address environmental issues and promote sustainability . 6 Climatology Study of climate patterns , changes , and their impact on ecosystems and human society . 7 Hydrology Study of the distribution , movement , and quality of water on Earth , including surface and groundwater resources . 8 9 Environmental Health Study of how environmental factors , such as pollution and climate change , impact human health and well-being . Sustainable Development Pursuit of economic , social , and environmental objectives without compromising the ability of future generations to meet their needs . 10 Renewable Energy Development and implementation of sustainable energy sources , such as solar , wind , and hydroelectric power . 6 Table 2 . The overview of the average number of citations received by research articles cited by GPT Topic Oldest citation Newest citation Ecology 1973 2019 Conservation Biology 1985 2016 1961 2019 Median date of the citation Publication with the lowest citations Publicati on with most citations The median number of citations 2012 2004 1999 436 790 183 8763 9400 8686 1150 1100 1762 Environmental Chemistry Environmental Engineering Environmental Policy and Management 1972 2013 2001 38 1024 5590 1986 2020 2009 155 1045 11,030 Climatology 2005 2015 2012 287 37840 1,263 Hydrology 2000 2017 Environmental Health 2003 2016 2007 2011 94 66 2308 3429 339 405 Sustainable Development 1987 2020 2011 162 18278 1269 Renewable Energy 2008 2021 Total 1961 2021 2015 2010 7 38 16,324 476 37840 1184.5 7 Table 3 . The top 10 most cited journals by GPT across disciplines Rank Journal Citations 1 2 3 4 5 6 7 8 9 Nature Science Proceedings of the National Academy of Sciences Annual Review of Environment and Resources Global Environmental Change Trends in Ecology & Evolution Annual Review of Ecology , Evolution , and Systematics BioScience Global Change Biology 10 Ecological Applications 17 13 8 6 5 4 4 4 4 4 8 Discussion There are other questions related to ChatGPT 's selection of research publications , as little is currently known about it . In order to shed more light on this topic , we posed several additional questions to GPT and have presented them below:15 ____ “ How did you choose and rank the citations ? ” “ I selected and ranked the top 10 most important subdisciplines of environmental science based on their broad relevance and impact on addressing current environmental challenges . The selected subdisciplines are essential for understanding the complex interactions within ecosystems , preserving biodiversity , managing resources , and promoting sustainable development . However , it is important to note that this ranking is subjective and may vary depending on individual preferences and specific contexts . Each subdiscipline in environmental science has its importance and contributes to our overall understanding of the environment and sustainability . The selection of the top 10 was made to provide an overview of some of the most significant and widely applicable areas of study within the field. ” “ Up to which date do you count citations ? ” “ The Google Scholar citation counts provided in the table are up to my knowledge cutoff date , which is September 2021 . Please note that the citation counts may have changed since then , as new citations are added over time. ” “ Can you provide me with citations from the Web of Science ? ” “ As an AI language model , I am unable to access Web of Science directly to provide you with real-time citation counts . The information I have provided in this conversation is based on my training data , which goes up until September 2021 . For the most current and accurate citation counts from Web of Science , you will need to access the database directly or ask someone with access to provide the information for you. ” “ And as of September 2021 , what is the number of citations in Web of Science ? ” “ I apologize for any confusion . As an AI language model , I do not have direct access to the Web of Science database or its citation counts . My knowledge is based on the training data provided to me , which includes citation counts from Google Scholar up until September 2021 . For citation counts from Web of Science , you would need to access the database directly or ask someone with access to provide the information for you. ” `` What about Scopus ? ” “ As an AI language model , I am unable to access Scopus or its citation data directly . The information I have provided in this conversation is based on my training data , which goes up until September 2021 , and it includes citation counts from Google Scholar . For the most current and accurate citation counts from Scopus , you will need to access the database directly or ask someone with access to provide the information for you. ” 15 The prompts in the article may have been proofreaded , and GPT 's responses can vary depending on the specific conversation . Additionally , some sections of this article may refer to previous versions of GPT . 9 `` How do you choose the citations that you mention ? '' As an AI language model , I based the selection of the 25 references on their relevance to the topic of `` Conservation Biology . '' My training data includes a wide range of scientific articles and sources , which allows me to identify important and frequently cited works related to the field . When composing the introduction for the research article , I considered key research areas and subtopics within conservation biology , such as habitat loss , climate change , invasive species , overexploitation , and pollution . I then selected references that were representative of these themes and that have been widely cited in the literature . This approach aimed to provide a comprehensive and well-rounded overview of the field , using sources that have made significant contributions to conservation biology . Please note that the selection of references is not exhaustive , and there may be other important works that were not included . The list of references provided is meant to serve as a starting point for further research and exploration of the field . “ Can you provide more details on how you chose the 25 citations and the algorithm used to do so ? '' As an AI language model , I consider various factors when selecting research articles to mention , and the number of citations in Google Scholar is one of those factors . A high citation count often indicates that a study has had a significant impact on its field , as it has been frequently referenced by other researchers . However , it is not the only criteria I use when selecting articles to mention . Other factors I consider include : Relevance : How well does the article align with the topic being discussed ? I prioritize articles that are closely related to the subject matter . Quality : Has the article been published in a reputable journal or by a well-regarded publisher ? Peer-reviewed articles from established journals generally have a higher level of credibility . Recency : How recent is the publication ? While older articles can be foundational and influential , more recent articles may reflect current trends and advances in the field . Author expertise : Are the authors of the article recognized experts in their field ? Articles by well-respected researchers can carry more weight . Novelty : Does the article present a new or innovative approach , theory , or finding ? Articles that introduce new ideas or challenge existing paradigms may be more likely to be mentioned . In summary , while the number of Google Scholar citations is an important factor , it is not the sole predictor for mentioning a research article . I take a holistic approach , considering multiple factors to ensure that the selected articles are relevant , high-quality , and representative of the field . “ Can you be more concrete about how did you choose these 25 citations , and provide some algorithm which you used ? ” “ Certainly ! Here is the algorithm I used to select the 25 citations for the introduction on Conservation Biology : 10 1 . I first identified key subtopics within Conservation Biology , such as habitat loss , climate change , invasive species , overexploitation , and pollution . 3 . 2 . For each subtopic , I conducted a search using a scientific database ( such as Google Scholar ) to identify highly cited articles that are widely considered to be influential and impactful in the field . I reviewed the abstracts and summaries of these articles to assess their relevance and importance to the subtopic . I then selected the most highly cited articles that were representative of each subtopic and that provided a comprehensive and well-rounded overview of the field of Conservation Biology . I ensured that the selected articles were published in reputable journals or by well-regarded publishers , and that they were authored by recognized experts in the field . 5 . 4 . 6 . Finally , I compiled the 25 citations into a list , taking care to provide a balanced representation of the different subtopics and areas of research within Conservation Biology . This algorithm allowed me to select a diverse range of highly cited articles that are considered to be important and influential in the field of Conservation Biology. ” ____ During the research , several problems and issues arose , such as the fact that GPT provides references somewhat randomly . References can serve as pointers to further literature rather than specifically supporting concrete statements . Additionally , the statements generated by GPT can be too vague to require references . It is interesting to note that ChatGPT can be inconsistent , sometimes initially providing references but later claiming not to have them or being unable to provide them . For instance , GPT might state : `` Due to the limitations of this AI , I can not access real-time Google Scholar citations or verify the publisher for each reference . However , I can provide a list of the references with the date and title in a table format . You can later manually add the publisher and citations from Google Scholar . '' To address this issue , we either opened a new window or continued in the chat window where citations were provided , or we tried using a different prompt . It was observed that GPT can provide different answers to the same queries , especially in previous versions . However , based on the prompts we tried , the pattern in citation behavior remained consistent . 11 Conclusion GPT is already changing the world of knowledge . It has become the subject of scientific research , which in this field also proves that traditional academic publishing methods are becoming obsolete ( similar to the case of the Covid pandemic ) . Preprints , Twitter , and Reddit are increasingly proving to be efficient ways of distributing knowledge in these respects . If we want to find out the latest information about GPT , for example , we can use the following channels : 1 ) Twitter search for GPT16 ( many tweets are relevant to scientific audiences as a new tool for scientists – Perplexity17 ) ; 2 ) Reddit search for GPT18 and the forum ChatGPT19 , where you can find weekly updates20 ; 3 ) Google News ( e.g. , US edition ) for ChatGPT21 ; 4 ) Google Scholar ( allintitle : ChatGPT ) 22 5 ) Dimensions ( can be filtered according to Altmetric and scientific areas to identify the most discussed research articles in your field ) 23 In conclusion , GPT appears to be influenced by the current assessment of scientific quality ( primarily through citations ) in environmental science , even though , according to GPT , citations are not the sole predictors for mentioning a study . We hypothesize that this citation pattern is prevalent across various disciplines ; however , further research is necessary to test this hypothesis . We would like to acknowledge the assistance of an AI language model , ChatGPT , in generating responses for this research article . 16 https : //twitter.com/search ? q=GPT & src=typed_query 17 https : 18 https : //www.reddit.com/search/ ? q=GPT 19 https : //www.reddit.com/r/ChatGPT/ 20 https : _ai/ 21 https : //news.google.com/search ? q=chatGPT & hl=en-US & gl=US & ceid=US % 3Aen 22 https : //scholar.google.com/scholar ? hl=en & as_sdt=0 % 2C5 & q=allintitle % 3AChatGPT & btnG= 23 https : ? search_mode=content & search_text=chatgpt & search_t ype=kws & search_field=text_search & order=altmetric & and_facet_for=80003 12","['cite', 'mostcited', 'article', 'journal', 'rely', 'solely', 'scholar', 'citation', 'count', 'result', 'amplify', 'matthew', 'effect', 'environmental', 'czpcunicz', 'assistant', 'respondent', 'chatgpt', 'abstract', 'introduction', 'become', 'talkedabout', 'innovation', 'recent', 'year', 'user', 'worldwide', 'however', 'still', 'limited', 'knowledge', 'source', 'information', 'utilize', 'result', 'carry', 'study', 'focus', 'source', 'information', 'field', 'environmental', 'science', 'study', 'aim', 'address', 'research', 'question', 'chatgpt', 'predominantly', 'cite', 'mostcited', 'publication', 'environmental', 'science', 'method', 'study', 'ask', 'gpt', 'identify', 'significant', 'subdiscipline', 'field', 'environmental', 'science', 'ask', 'compose', 'scientific', 'review', 'article', 'subdiscipline', 'include', 'reference', 'proceed', 'analyze', 'reference', 'focus', 'factor', 'number', 'citation', 'publication', 'date', 'journal', 'work', 'publish', 'result', 'finding', 'indicate', 'gpt', 'tend', 'cite', 'highlycite', 'publication', 'environmental', 'science', 'median', 'citation', 'count', 'also', 'exhibit', 'preference', 'old', 'publication', 'median', 'publication', 'year', 'predominantly', 'refer', 'wellrespecte', 'journal', 'field', 'nature', 'cite', 'journal', 'interestingly', 'finding', 'suggest', 'gpt', 'seem', 'exclusively', 'rely', 'citation', 'count', 'datum', 'scholar', 'work', 'cite', 'rather', 'utilize', 'citation', 'information', 'scientific', 'database', 'web', 'science', 'scopus', 'conclusion', 'conclusion', 'study', 'suggest', 'scholar', 'citation', 'play', 'significant', 'role', 'predictor', 'mention', 'study', 'gptgenerate', 'content', 'finding', 'reinforce', 'dominance', 'scholar', 'scientific', 'database', 'perpetuate', 'effect', 'science', 'rich', 'get', 'rich', 'term', 'citation', 'many', 'scholar', 'already', 'utilize', 'gpt', 'literature', 'review', 'purpose', 'anticipate', 'disparity', 'expand', 'gap', 'lessercite', 'highlycite', 'publication', 'keyword', 'chatgpt', 'environmental', 'effect', 'citation', 'introduction', 'chatgpt', 'powerful', 'language', 'model', 'demonstrate', 'ability', 'generate', 'coherent', 'wellstructure', 'text', 'prompt', 'application', 'write', 'introduction', 'abstract', 'section', 'scientific', 'articles1', 'fact', 'several', 'paper', 'acknowledge', 'chatgpt', 'coauthor2', 'potential', 'research', 'academia', 'evident', 'concern', 'arise', 'regard', 'implication', 'education', 'identify', 'style', 'student', 'paper', 'propose', 'policy', 'administer', 'adhoc', 'oral', 'exam', 'aigenerate', 'submission', 'suspected4', 'response', 'concern', 'education', 'block', 'access', 'chatgpt', 'scientific', 'journal', 'take', 'vary', 'stance', 'chatgpt', 'nature', 'network', 'mandate', 'disclosure', 'textgenerate', 'tool', 'usage', 'prohibit', 'list', 'chatgpt', 'coauthor', 'science', 'ban', 'use', 'large', 'language', 'modelgenerate', 'text', 'entirely56', 'release', 'still', 'relatively', 'new', 'field', 'scientific', 'research', 'paper', 'already', 'published7', 'still', 'less', 'know', 'application', 'chatgpt', 'concrete', 'discipline', 'environmental', 'science', 'scientometric', 'study', 'concentrate', 'effort', 'analyze', 'citation', 'field', 'environmental', 'science', 'investigate', 'effect', 'wellknown', 'concept', 'scientific', 'community', 'also', 'influence', 'citation', 'pattern', 'chatgptgenerate', 'response', 'mathew', 'effect', 'science', 'effect', 'root', 'gospel', 'science', 'phenomenon', 'first', 'introduce', 'attract', 'significant', 'attention', 'recent', 'year', 'implication', 'reward', 'communication', 'system', 'scientific', 'research', 'effect', 'refer', 'disproportionate', 'allocation', 'reward', 'recognition', 'wellestablished', 'researcher', 'impede', 'scientific', 'progress', 'inhibit', 'growth', 'emerge', 'scientist', 'petersen', 'study', 'demonstrate', 'effect', 'lead', 'disparity', 'funding', 'career', 'opportunity', 'citation', 'bushard', 'brian', 'fake', 'scientific', 'abstract', 'write', 'chatgpt', 'fool', 'scientist', 'study', 'find', 'forbe', 'archive', 'original', 'stokelwalker', 'chatgpt', 'list', 'author', 'research', 'paper', 'many', 'scientist', 'disapprove', 'nature', 'stokelwalker', 'write', 'smart', 'essay', 'professor', 'worry', 'nature', 'doi101038d41586022043977', 'mitchell', 'student', 'use', 'chatgpt', 'cheat', 'professor', 'warn', 'brainard', 'scientist', 'explore', 'aiwritten', 'text', 'journal', 'hammer', 'policy', 'https', 'enwikipediaorgwikichatgpt', 'https', 'hlen', 'assdt0', 'qallintitle', 'btng', 'merton', 'r', 'effect', 'science', 'reward', 'communication', 'system', 'science', 'consider', 'science', 'petersen', 'pan', 'kaski', 'penner', 'barabási', 'l', 'reputation', 'impact', 'academic', 'career', 'proceeding', 'e', 'effect', 'scientific', 'funding', 'proceeding', 'count', 'researcher', 'addition', 'disadvantage', 'earlycareer', 'scientist', 'bol', 'effect', 'also', 'result', 'skewed', 'distribution', 'research', 'attention', 'resource', 'certain', 'field', 'topic', 'effect', 'pronounce', 'example', 'case', 'platform', 'often', 'serve', 'gateway', 'knowledge', 'result', 'article', 'wikipedia', 'cite', 'wellestablished', 'publication', 'inadvertently', 'reinforce', 'effect', 'source', 'continue', 'accumulate', 'citation', 'dominate', 'respective', 'field', 'thompson', 'hanley', 'd2018', 'similarly', 'effect', 'scholar', 'example', 'digital', 'platform', 'amplify', 'visibility', 'recognition', 'cite', 'publication', 'thereby', 'reinforce', 'exist', 'hierarchy', 'scientific', 'research', 'influential', 'search', 'engine', 'scholarly', 'scholar', 'employ', 'algorithm', 'prioritize', 'highlycite', 'article', 'search', 'result', 'process', 'create', 'selfperpetuate', 'cycle', 'article', 'already', 'wellestablishe', 'respective', 'field', 'continue', 'receive', 'even', 'citation', 'lesserknown', 'publication', 'remain', 'relatively', 'obscure', 'consequently', 'effect', 'scholar', 'contribute', 'skewed', 'distribution', 'attention', 'resource', 'potentially', 'hinder', 'discovery', 'innovative', 'idea', 'research', 'finding', 'hypothesize', 'effect', 'also', 'play', 'crucial', 'role', 'case', 'gpt', 'study', 'focus', 'citation', 'usage', 'chatgpt', 'decisionmaking', 'process', 'select', 'paper', 'cite', 'ask', 'model', 'following', 'question', 'choose', 'academic', 'study', 'cite', 'response', 'receive', 'ai', 'language', 'model', 'generate', 'response', 'rely', 'information', 'datum', 'available', 'training', 'include', 'wide', 'range', 'academic', 'study', 'article', 'source', 'completion', 'date', 'select', 'academic', 'study', 'cite', 'consider', 'several', 'factor', 'include', 'relevance', 'study', 'question', 'topic', 'hand', 'quality', 'reliability', 'research', 'impact', 'influence', 'publication', 'field', 'additionally', 'give', 'weight', 'study', 'cite', 'frequently', 'publish', 'wellregarde', 'journal', 'often', 'indicator', 'importance', 'contribution', 'research', 'however', 'important', 'note', 'citation', 'generate', 'algorithmically', 'personal', 'bias', 'preference', 'select', 'academic', 'study', 'cite', 'see', 'answer', 'provide', 'gpt', 'somewhat', 'vague', 'lack', 'specificity', 'attempt', 'obtain', 'information', 'model', 'subsequent', 'discussion', 'detail', 'find', 'discussion', 'section', 'allison', 'stewart', 'j', 'productivity', 'difference', 'scientist', 'evidence', 'accumulative', 'advantage', 'american', 'sociological', 'review', 'bol', 'rijt', 'effect', 'science', 'funding', 'proceeding', 'science', 'veugeler', 'r', 'stephan', 'p', 'bias', 'novelty', 'science', 'cautionary', 'tale', 'user', 'bibliometric', 'indicator', 'nber', 'working', 'paper', 'thompson', 'science', 'shape', 'wikipedia', 'evidence', 'randomized', 'control', 'trial', 'method', 'pose', 'research', 'question', 'chatgpt', 'predominantly', 'cite', 'mostcited', 'publication', 'environmental', 'science', 'answer', 'question', 'follow', 'several', 'step', 'first', 'ask', 'chatgpt4', 'perform', 'follow', 'task', 'write', 'important', 'subdiscipline', 'environmental', 'science', 'compose', 'introduction', 'research', 'article', 'commonly', 'refer', 'review', 'article', 'subdiscipline', 'ecology', 'conservation', 'biology', 'ensure', 'statement', 'introduction', 'support', 'citation', 'relevant', 'research', 'article', 'include', 'reference', 'total', 'introduction', 'provide', 'list', 'reference', 'table', 'date', 'title', 'publisher', 'separate', 'column', 'table', 'column', 'include', 'rank', 'write', 'number', 'scholar', 'citation', 'reference', 'table', 'column', 'analyze', 'response', 'various', 'discipline', 'datum', 'include', 'citation', 'journal', 'base', 'criterion', 'mention', 'early', 'subdiscipline', 'criterion', 'include', 'old', 'new', 'citation', 'median', 'date', 'citation', 'publication', 'low', 'high', 'number', 'citation', 'median', 'number', 'citation', 'finally', 'ask', 'gpt', 'determine', 'cite', 'journal', 'response', 'provide', 'discipline', 'picture', 'illustration', 'text', 'section', 'citation', 'provide', 'gpt', 'result', 'table', 'top', 'important', 'subdiscipline', 'environmental', 'science', 'accord', 'gpt', 'subdiscipline', 'description', 'ecology', 'study', 'interaction', 'organism', 'environment', 'factor', 'affect', 'interaction', 'conservation', 'biology', 'study', 'preserve', 'biodiversity', 'specie', 'ecosystem', 'habitat', 'restoration', 'protect', 'area', 'environmental', 'chemistry', 'examination', 'chemical', 'biochemical', 'process', 'environment', 'include', 'pollution', 'natural', 'cycle', 'environmental', 'engineering', 'design', 'implementation', 'solution', 'mitigate', 'environmental', 'problem', 'pollution', 'control', 'waste', 'management', 'environmental', 'policy', 'management', 'development', 'implementation', 'analysis', 'policy', 'strategy', 'address', 'environmental', 'issue', 'promote', 'sustainability', 'climatology', 'study', 'climate', 'pattern', 'change', 'impact', 'ecosystem', 'human', 'society', 'hydrology', 'study', 'distribution', 'movement', 'quality', 'water', 'earth', 'include', 'surface', 'groundwater', 'resource', 'environmental', 'health', 'study', 'environmental', 'factor', 'pollution', 'climate', 'change', 'impact', 'human', 'health', 'wellbee', 'sustainable', 'development', 'pursuit', 'economic', 'social', 'environmental', 'objective', 'compromise', 'ability', 'future', 'generation', 'meet', 'need', 'renewable', 'energy', 'development', 'implementation', 'sustainable', 'energy', 'source', 'solar', 'wind', 'hydroelectric', 'power', 'table', 'overview', 'average', 'number', 'citation', 'receive', 'research', 'article', 'cite', 'gpt', 'topic', 'old', 'citation', 'new', 'citation', 'ecology', 'conservation', 'biology', 'median', 'date', 'citation', 'publication', 'low', 'citation', 'publicati', 'citation', 'median', 'number', 'citation', 'environmental', 'environmental', 'policy', 'management', 'climatology', 'hydrology', 'environmental', 'health', 'sustainable', 'development', 'renewable', 'energy', 'total', 'table', 'top', 'cite', 'journal', 'gpt', 'citation', 'nature', 'science', 'proceeding', 'annual', 'review', 'environment', 'resource', 'global', 'environmental', 'change', 'trend', 'ecology', 'evolution', 'annual', 'review', 'ecology', 'evolution', 'systematic', 'global', 'change', 'biology', 'ecological', 'application', 'discussion', 'question', 'relate', 'selection', 'research', 'publication', 'little', 'currently', 'know', 'order', 'shed', 'light', 'topic', 'pose', 'several', 'additional', 'question', 'gpt', 'present', 'choose', 'rank', 'citation', 'select', 'rank', 'top', 'important', 'subdiscipline', 'environmental', 'science', 'base', 'broad', 'relevance', 'impact', 'address', 'current', 'environmental', 'challenge', 'select', 'subdiscipline', 'essential', 'understand', 'complex', 'interaction', 'ecosystem', 'preserve', 'biodiversity', 'manage', 'resource', 'promote', 'sustainable', 'development', 'however', 'important', 'note', 'ranking', 'subjective', 'vary', 'depend', 'individual', 'preference', 'specific', 'context', 'subdiscipline', 'environmental', 'science', 'importance', 'contribute', 'overall', 'understanding', 'environment', 'sustainability', 'selection', 'top', 'make', 'provide', 'overview', 'significant', 'widely', 'applicable', 'area', 'study', 'field', 'date', 'count', 'citation', 'scholar', 'citation', 'count', 'provide', 'table', 'knowledge', 'cutoff', 'date', 'note', 'citation', 'count', 'change', 'new', 'citation', 'add', 'time', 'provide', 'citation', 'web', 'science', 'ai', 'language', 'model', 'unable', 'access', 'web', 'science', 'directly', 'provide', 'realtime', 'citation', 'count', 'information', 'provide', 'conversation', 'base', 'training', 'datum', 'go', 'current', 'accurate', 'citation', 'count', 'web', 'science', 'need', 'access', 'database', 'directly', 'ask', 'access', 'provide', 'information', 'number', 'citation', 'web', 'apologize', 'confusion', 'language', 'model', 'direct', 'access', 'web', 'science', 'database', 'citation', 'count', 'knowledge', 'base', 'training', 'datum', 'provide', 'include', 'citation', 'count', 'scholar', 'citation', 'count', 'web', 'science', 'need', 'access', 'database', 'directly', 'ask', 'access', 'provide', 'information', 'scopus', 'ai', 'language', 'model', 'unable', 'access', 'scopus', 'citation', 'datum', 'directly', 'information', 'provide', 'conversation', 'base', 'training', 'datum', 'go', 'include', 'citation', 'count', 'scholar', 'current', 'accurate', 'citation', 'count', 'scopus', 'need', 'access', 'database', 'directly', 'ask', 'access', 'provide', 'information', 'prompt', 'article', 'proofread', 'response', 'vary', 'depend', 'specific', 'conversation', 'additionally', 'section', 'article', 'refer', 'previous', 'version', 'gpt', 'choose', 'citation', 'mention', 'ai', 'language', 'model', 'base', 'selection', 'reference', 'relevance', 'topic', 'conservation', 'biology', 'training', 'datum', 'include', 'wide', 'range', 'scientific', 'article', 'source', 'allow', 'identify', 'important', 'frequently', 'cite', 'work', 'relate', 'field', 'compose', 'introduction', 'research', 'article', 'consider', 'key', 'research', 'area', 'subtopic', 'conservation', 'biology', 'habitat', 'loss', 'climate', 'change', 'invasive', 'specie', 'overexploitation', 'pollution', 'select', 'reference', 'representative', 'theme', 'widely', 'cite', 'literature', 'approach', 'aim', 'provide', 'comprehensive', 'wellrounded', 'overview', 'field', 'use', 'source', 'make', 'significant', 'contribution', 'conservation', 'biology', 'note', 'selection', 'reference', 'exhaustive', 'important', 'work', 'include', 'list', 'reference', 'provide', 'mean', 'serve', 'starting', 'point', 'research', 'exploration', 'field', 'provide', 'detail', 'choose', 'citation', 'use', 'ai', 'language', 'model', 'consider', 'various', 'factor', 'select', 'research', 'article', 'mention', 'number', 'citation', 'scholar', 'factor', 'high', 'citation', 'count', 'often', 'indicate', 'study', 'significant', 'impact', 'field', 'frequently', 'reference', 'researcher', 'however', 'criterion', 'use', 'select', 'article', 'mention', 'factor', 'consider', 'include', 'relevance', 'well', 'article', 'align', 'topic', 'discuss', 'prioritize', 'article', 'closely', 'relate', 'subject', 'matter', 'quality', 'article', 'publish', 'reputable', 'journal', 'wellregarded', 'publisher', 'peerreviewe', 'article', 'establish', 'journal', 'generally', 'high', 'level', 'credibility', 'recency', 'recent', 'publication', 'old', 'article', 'foundational', 'influential', 'recent', 'article', 'reflect', 'current', 'trend', 'advance', 'field', 'author', 'expertise', 'author', 'article', 'recognize', 'expert', 'field', 'article', 'wellrespecte', 'researcher', 'carry', 'weight', 'novelty', 'article', 'present', 'new', 'innovative', 'approach', 'theory', 'find', 'article', 'introduce', 'new', 'idea', 'challenge', 'exist', 'paradigm', 'likely', 'mention', 'summary', 'number', 'scholar', 'citation', 'important', 'factor', 'sole', 'predictor', 'mention', 'research', 'article', 'take', 'holistic', 'approach', 'consider', 'multiple', 'factor', 'ensure', 'select', 'article', 'relevant', 'highquality', 'representative', 'field', 'concrete', 'choose', 'citation', 'provide', 'use', 'certainly', 'use', 'select', 'citation', 'introduction', 'conservation', 'biology', 'first', 'identify', 'key', 'subtopic', 'conservation', 'biology', 'habitat', 'loss', 'climate', 'change', 'invasive', 'specie', 'overexploitation', 'pollution', 'subtopic', 'conduct', 'search', 'use', 'scientific', 'database', 'scholar', 'identify', 'highly', 'cite', 'article', 'widely', 'consider', 'influential', 'impactful', 'field', 'review', 'abstract', 'summary', 'article', 'assess', 'relevance', 'importance', 'subtopic', 'select', 'highly', 'cite', 'article', 'representative', 'subtopic', 'provide', 'comprehensive', 'wellrounded', 'overview', 'field', 'conservation', 'biology', 'ensure', 'select', 'article', 'publish', 'reputable', 'journal', 'wellregarde', 'publisher', 'author', 'recognize', 'expert', 'field', 'finally', 'compile', 'citation', 'list', 'take', 'care', 'provide', 'balanced', 'representation', 'different', 'subtopic', 'area', 'research', 'conservation', 'biology', 'allow', 'select', 'diverse', 'range', 'highly', 'cite', 'article', 'consider', 'important', 'influential', 'field', 'conservation', 'biology', 'research', 'several', 'problem', 'issue', 'arise', 'fact', 'gpt', 'provide', 'reference', 'somewhat', 'randomly', 'reference', 'serve', 'pointer', 'literature', 'rather', 'specifically', 'support', 'concrete', 'statement', 'additionally', 'statement', 'generate', 'gpt', 'vague', 'require', 'reference', 'interesting', 'note', 'chatgpt', 'inconsistent', 'sometimes', 'initially', 'provide', 'reference', 'later', 'claim', 'unable', 'provide', 'instance', 'gpt', 'state', 'limitation', 'ai', 'access', 'realtime', 'scholar', 'citation', 'verify', 'publisher', 'reference', 'however', 'provide', 'list', 'reference', 'date', 'title', 'table', 'format', 'later', 'manually', 'add', 'publisher', 'citation', 'scholar', 'address', 'issue', 'open', 'new', 'window', 'continue', 'chat', 'window', 'citation', 'provide', 'try', 'use', 'different', 'prompt', 'observe', 'gpt', 'provide', 'different', 'answer', 'query', 'especially', 'previous', 'version', 'however', 'base', 'prompt', 'try', 'pattern', 'citation', 'behavior', 'remain', 'consistent', 'conclusion', 'gpt', 'already', 'change', 'world', 'knowledge', 'become', 'subject', 'scientific', 'research', 'field', 'also', 'prove', 'traditional', 'academic', 'publishing', 'method', 'become', 'obsolete', 'similar', 'case', 'covid', 'pandemic', 'preprint', 'twitter', 'reddit', 'increasingly', 'prove', 'efficient', 'way', 'distribute', 'knowledge', 'respect', 'want', 'find', 'late', 'information', 'gpt', 'example', 'use', 'follow', 'channel', 'twitter', 'search', 'gpt16', 'many', 'tweet', 'relevant', 'scientific', 'audience', 'new', 'tool', 'scientist', 'perplexity17', 'reddit', 'search', 'gpt18', 'forum', 'chatgpt19', 'find', 'weekly', 'updates20', 'edition', 'chatgpt21', 'scholar', 'chatgpt', 'dimension', 'filter', 'accord', 'altmetric', 'scientific', 'area', 'identify', 'discuss', 'research', 'article', 'field', 'conclusion', 'appear', 'influence', 'current', 'assessment', 'scientific', 'quality', 'primarily', 'citation', 'environmental', 'science', 'even', 'accord', 'gpt', 'citation', 'sole', 'predictor', 'mention', 'study', 'hypothesize', 'citation', 'pattern', 'prevalent', 'various', 'discipline', 'however', 'research', 'necessary', 'test', 'hypothesis', 'like', 'acknowledge', 'assistance', 'language', 'model', 'chatgpt', 'generate', 'response', 'research', 'article', 'https', 'twittercomsearch', 'qgpt', 'srctypedquery', 'https', 'https', 'wwwredditcomsearch', 'qgpt', 'https', 'wwwredditcomrchatgpt', 'http', 'ai', 'https', 'newsgooglecomsearch', 'qchatgpt', 'hlenus', 'glus', 'https', 'hlen', 'assdt0', 'qallintitle', 'btng', 'https', 'searchmodecontent', 'searchtextchatgpt', 'searcht', 'searchfieldtextsearch', 'orderaltmetric']"
