title,url,date,text,cleaning,tokens
05/20/2022 - Import AI 295: DeepMind's baby general agent; NVIDIA simulates a robot factory; AI wars. - 0,http://eepurl.com/h2sj1D,2022-05-20,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

Import AI is taking a short break (and here's why):
Import AI is taking two weeks off! That's because I (Jack Clark) am serially bad at taking holiday. I've recently worked with my colleagues to figure out some time and so I'm going to spend the next couple of weeks hiking and traveling and reading and generally trying to unplug my brain a bit so it can do some deep background cogitation. I admit that the thought of not writing this newsletter for two weeks fills me with a bit of anxiety, but I also think sometimes you need to step away so you can come back fresher in the future. I hope all of you, my dear readers, are doing well, and I'll be back soon. Now, back to the regularly scheduled programming!... CRPD: Chinese license plate recognition:
…A basic dataset for a useful capability…
Researchers with the University of Electronic Science and Technology of China have built a dataset for recognizing Chinese license plates. The authors use the dataset to train some models that get state-of-the-art accuracy while running at 30 frames per second.

The dataset: The Chinese Road Plate Dataset (CRPD) contains 25k images (around 30k total). Each image is annotated with the Chinese and English characters of the depicted license plate, the coordinate of the vertices of the license plates, and the type of license plate (e.g, whether for police cars, small cars, etc).  Images for the dataset were ""collected from electronic monitoring systems in most provinces of mainland China in different periods and weather conditions,"" the authors write.

Why this matters: Datasets like CRPD represent the basic infrastructure on which AI capabilities get developed. It's also notable how universities in China can access large-scale surveillance datasets.
  Read more: Unified Chinese License Plate Detection and Recognition with High Efficiency (arXiv).    Get the dataset: Github https://github.com/yxgong0/CRPD","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Import AI is taking a short break (and here's why): Import AI is taking two weeks off! That's because I (Jack Clark) am serially bad at taking holiday. I've recently worked with my colleagues to figure out some time and so I'm going to spend the next couple of weeks hiking and traveling and reading and generally trying to unplug my brain a bit so it can do some deep background cogitation. I admit that the thought of not writing this newsletter for two weeks fills me with a bit of anxiety, but I also think sometimes you need to step away so you can come back fresher in the future. I hope all of you, my dear readers, are doing well, and I'll be back soon. Now, back to the regularly scheduled programming!... CRPD: Chinese license plate recognition: …A basic dataset for a useful capability… Researchers with the University of Electronic Science and Technology of China have built a dataset for recognizing Chinese license plates. The authors use the dataset to train some models that get state-of-the-art accuracy while running at 30 frames per second. The dataset: The Chinese Road Plate Dataset (CRPD) contains 25k images (around 30k total). Each image is annotated with the Chinese and English characters of the depicted license plate, the coordinate of the vertices of the license plates, and the type of license plate (e.g, whether for police cars, small cars, etc). Images for the dataset were ""collected from electronic monitoring systems in most provinces of mainland China in different periods and weather conditions,"" the authors write. Why this matters: Datasets like CRPD represent the basic infrastructure on which AI capabilities get developed. It's also notable how universities in China can access large-scale surveillance datasets. Read more: Unified Chinese License Plate Detection and Recognition with High Efficiency (arXiv). Get the dataset: Github https://github.com/yxgong0/CRPD","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'import', 'take', 'short', 'break', 'import', 'ai', 'take', 'week', 'jack', 'serially', 'bad', 'take', 'holiday', 'recently', 'work', 'colleague', 'figure', 'time', 'go', 'spend', 'next', 'couple', 'week', 'hike', 'travel', 'reading', 'generally', 'try', 'unplug', 'brain', 'bit', 'deep', 'background', 'cogitation', 'admit', 'thought', 'write', 'newsletter', 'week', 'fill', 'bit', 'anxiety', 'also', 'think', 'sometimes', 'need', 'step', 'away', 'come', 'back', 'fresher', 'future', 'hope', 'dear', 'reader', 'well', 'ill', 'back', 'soon', 'back', 'regularly', 'schedule', 'programming', 'crpd', 'chinese', 'license', 'plate', 'recognition', 'basic', 'dataset', 'useful', 'capability', 'researcher', 'electronic', 'science', 'technology', 'build', 'dataset', 'recognize', 'chinese', 'license', 'plate', 'author', 'use', 'dataset', 'train', 'model', 'get', 'stateoftheart', 'accuracy', 'run', 'frame', 'second', 'dataset', 'chinese', 'road', 'plate', 'dataset', 'crpd', 'contain', '25k', 'image', 'total', 'image', 'annotate', 'chinese', 'english', 'character', 'depict', 'license', 'plate', 'coordinate', 'vertex', 'license', 'plate', 'type', 'license', 'plate', 'eg', 'police', 'car', 'small', 'car', 'image', 'dataset', 'collect', 'electronic', 'monitoring', 'system', 'province', 'mainland', 'different', 'period', 'weather', 'condition', 'author', 'write', 'matter', 'dataset', 'crpd', 'represent', 'basic', 'infrastructure', 'capability', 'develop', 'also', 'notable', 'university', 'access', 'largescale', 'surveillance', 'dataset', 'read', 'unify', 'chinese', 'license', 'plate', 'detection', 'recognition', 'high', 'efficiency', 'arxiv', 'get', 'dataset', 'github']"
05/20/2022 - Import AI 295: DeepMind's baby general agent; NVIDIA simulates a robot factory; AI wars. - 1,http://eepurl.com/h2sj1D,2022-05-20,"#################################################### DeepMind builds a (very preliminary) general AI agent: …AKA: The dawn of really preliminary, general AI systems.. In the past few years, the dumbest thing has tended to work surprisingly well. Take for example GPT3 - just scale-up next word prediction on an internet-scale corpus and you wind up with something capable of few-shot learning, fielding a vast range of NLP capabilities.
  Another example is computer vision systems - just create a vast dataset and you wind up with increasingly robust vision systems.
  Or contrastive learning - just embed a couple of modalities into the same space and sort of flip-flop between them through the learning process and you get powerful multimodal systems like CLIP.
  Now DeepMind has done the same thing for reinforcement learning with GATO, an agent where basically DeepMind takes a bunch of distinct tasks in different modalities and embeds them into the same space, then learns prediction tasks from them. The result is a system where ""the same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens."" This is wild stuff! What GATO can do: After training, GATO can do okay at tasks ranging from DeepMind Lab, to robot manipulation, to the procgen benchmark, to image captioning, to natural language generation. It's a big deal: The fact you can take a bunch of different tasks from different modalities and just… tokenize them… and it works? That's wild! It's both a) wildly dumb and b) wildly effective, and c) another nice example of 'The Bitter Lesson', where given enough compute/scale, the dumb things (aka, the simple ones) tend to work really well.
  In a small package: The largest (disclosed here) GATO agent is 1.18 billion parameters, making it fairly small in the grand scheme of recent AI developments.  An even crazier thing: The GATO model only has a context window of 1024 tokens (by comparison, GPT3 was 2048 when it launched), so the fact 1024 tokens is enough to get a somewhat capable multimodal agent is pretty surprising.

Why this matters: ""Although still at the proof-of-concept stage, the recent progress in generalist models suggests that safety researchers, ethicists, and most importantly, the general public, should consider their risks and benefits,"" DeepMind writes.    Check out the blog: A Generalist Agent (DeepMind website).    Read more: A Generalist Agent (DeepMind PDF).","#################################################### DeepMind builds a (very preliminary) general AI agent: …AKA: The dawn of really preliminary, general AI systems.. In the past few years, the dumbest thing has tended to work surprisingly well. Take for example GPT3 - just scale-up next word prediction on an internet-scale corpus and you wind up with something capable of few-shot learning, fielding a vast range of NLP capabilities. Another example is computer vision systems - just create a vast dataset and you wind up with increasingly robust vision systems. Or contrastive learning - just embed a couple of modalities into the same space and sort of flip-flop between them through the learning process and you get powerful multimodal systems like CLIP. Now DeepMind has done the same thing for reinforcement learning with GATO, an agent where basically DeepMind takes a bunch of distinct tasks in different modalities and embeds them into the same space, then learns prediction tasks from them. The result is a system where ""the same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens."" This is wild stuff! What GATO can do: After training, GATO can do okay at tasks ranging from DeepMind Lab, to robot manipulation, to the procgen benchmark, to image captioning, to natural language generation. It's a big deal: The fact you can take a bunch of different tasks from different modalities and just… tokenize them… and it works? That's wild! It's both a) wildly dumb and b) wildly effective, and c) another nice example of 'The Bitter Lesson', where given enough compute/scale, the dumb things (aka, the simple ones) tend to work really well. In a small package: The largest (disclosed here) GATO agent is 1.18 billion parameters, making it fairly small in the grand scheme of recent AI developments. An even crazier thing: The GATO model only has a context window of 1024 tokens (by comparison, GPT3 was 2048 when it launched), so the fact 1024 tokens is enough to get a somewhat capable multimodal agent is pretty surprising. Why this matters: ""Although still at the proof-of-concept stage, the recent progress in generalist models suggests that safety researchers, ethicists, and most importantly, the general public, should consider their risks and benefits,"" DeepMind writes. Check out the blog: A Generalist Agent (DeepMind website). Read more: A Generalist Agent (DeepMind PDF).","['deepmind', 'build', 'preliminary', 'general', 'agent', 'aka', 'dawn', 'really', 'preliminary', 'general', 'ai', 'system', 'past', 'year', 'dumb', 'thing', 'tend', 'work', 'surprisingly', 'well', 'take', 'example', 'gpt3', 'scaleup', 'next', 'word', 'prediction', 'internetscale', 'corpus', 'wind', 'capable', 'fewshot', 'learning', 'field', 'vast', 'range', 'nlp', 'capability', 'example', 'computer', 'vision', 'system', 'create', 'vast', 'dataset', 'wind', 'increasingly', 'robust', 'vision', 'system', 'contrastive', 'learning', 'embe', 'couple', 'modality', 'space', 'sort', 'flipflop', 'learning', 'process', 'get', 'powerful', 'multimodal', 'system', 'clip', 'thing', 'reinforcement', 'learning', 'gato', 'agent', 'basically', 'deepmind', 'take', 'bunch', 'distinct', 'task', 'different', 'modality', 'embed', 'space', 'learn', 'prediction', 'task', 'result', 'system', 'network', 'weight', 'play', 'atari', 'caption', 'image', 'chat', 'stack', 'block', 'real', 'robot', 'arm', 'much', 'decide', 'base', 'context', 'output', 'text', 'joint', 'torque', 'button', 'press', 'token', 'wild', 'stuff', 'gato', 'training', 'gato', 'okay', 'task', 'range', 'deepmind', 'lab', 'robot', 'manipulation', 'procgen', 'benchmark', 'image', 'captioning', 'natural', 'language', 'generation', 'big', 'deal', 'fact', 'take', 'bunch', 'different', 'task', 'different', 'modality', 'tokenize', 'work', 'wild', 'wildly', 'dumb', 'wildly', 'effective', 'c', 'nice', 'example', 'bitter', 'lesson', 'give', 'enough', 'computescale', 'dumb', 'thing', 'aka', 'simple', 'one', 'tend', 'work', 'really', 'well', 'small', 'package', 'large', 'disclose', 'gato', 'agent', 'parameter', 'make', 'fairly', 'small', 'grand', 'scheme', 'recent', 'development', 'even', 'crazy', 'thing', 'gato', 'model', 'context', 'window', 'token', 'comparison', 'gpt3', 'launch', 'fact', 'token', 'enough', 'get', 'somewhat', 'capable', 'multimodal', 'agent', 'pretty', 'surprising', 'matter', 'still', 'proofofconcept', 'stage', 'recent', 'progress', 'generalist', 'model', 'suggest', 'safety', 'researcher', 'ethicist', 'importantly', 'general', 'public', 'consider', 'risk', 'benefit', 'deepmind', 'write', 'check', 'blog', 'generalist', 'agent', 'deepmind', 'website', 'read', 'generalist', 'agent', 'deepmind', 'pdf']"
05/20/2022 - Import AI 295: DeepMind's baby general agent; NVIDIA simulates a robot factory; AI wars. - 2,http://eepurl.com/h2sj1D,2022-05-20,"#################################################### Chinese researchers build a large multi-modal dataset, and evaluation suite:
…'Zero' makes it easier to develop AI systems for the Chinese cultural context…
Chinese researchers with startup Qihoo 360 AI Research and the Department of Automation at Tsinghua University have built Zero, a benchmark for assessing the quality of vision-text Chinese AI models. Zero consists of a dataset (the Zero-Corpus, consisting of 23-million image-text pairs, filtered via high click through rates - so the top image people click in response to a query), as well as five downstream datasets for evaluating Chinese vision-text models (an Image-Caption Matching Dataset, an Image-Query Matching dataset, an Image-Caption Retrieval Dataset, an Image-Query Retrieval Dataset, and a Chinese-translated version of the Flickr30k dataset).

Model training: The authors also train a model, called R2D2, on the corpus. They show that their model significantly outperforms another Chinse model named Wukong. R2D2 incorporates some pre-ranking techniques to improve its performance.  Why this matters: The main idea behind datasets and models like this is described in the paper: ""promote the development of Chinese vision language learning. We expect that a fair Chinese cross-modal benchmark and a good cross-modal framework will encourage a plethora of engineers to develop more effective methods in specific real-world scenarios, such as searching images by texts.""
  Read more: Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework (arXiv).","#################################################### Chinese researchers build a large multi-modal dataset, and evaluation suite: …'Zero' makes it easier to develop AI systems for the Chinese cultural context… Chinese researchers with startup Qihoo 360 AI Research and the Department of Automation at Tsinghua University have built Zero, a benchmark for assessing the quality of vision-text Chinese AI models. Zero consists of a dataset (the Zero-Corpus, consisting of 23-million image-text pairs, filtered via high click through rates - so the top image people click in response to a query), as well as five downstream datasets for evaluating Chinese vision-text models (an Image-Caption Matching Dataset, an Image-Query Matching dataset, an Image-Caption Retrieval Dataset, an Image-Query Retrieval Dataset, and a Chinese-translated version of the Flickr30k dataset). Model training: The authors also train a model, called R2D2, on the corpus. They show that their model significantly outperforms another Chinse model named Wukong. R2D2 incorporates some pre-ranking techniques to improve its performance. Why this matters: The main idea behind datasets and models like this is described in the paper: ""promote the development of Chinese vision language learning. We expect that a fair Chinese cross-modal benchmark and a good cross-modal framework will encourage a plethora of engineers to develop more effective methods in specific real-world scenarios, such as searching images by texts."" Read more: Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework (arXiv).","['chinese', 'researcher', 'build', 'large', 'multimodal', 'dataset', 'evaluation', 'suite', 'make', 'easy', 'develop', 'ai', 'system', 'context', 'chinese', 'researcher', 'startup', 'qihoo', 'research', 'department', 'automation', 'build', 'benchmark', 'assess', 'quality', 'model', 'consist', 'dataset', 'consist', 'imagetext', 'pair', 'filter', 'high', 'click', 'rate', 'top', 'image', 'people', 'click', 'response', 'query', 'well', 'downstream', 'dataset', 'evaluate', 'chinese', 'visiontext', 'model', 'imagecaption', 'match', 'dataset', 'imagequery', 'matching', 'dataset', 'imagecaption', 'retrieval', 'imagequery', 'retrieval', 'dataset', 'chinesetranslated', 'version', 'dataset', 'model', 'training', 'author', 'also', 'train', 'model', 'call', 'corpus', 'show', 'model', 'significantly', 'outperform', 'chinse', 'model', 'name', 'wukong', 'incorporate', 'preranke', 'technique', 'improve', 'performance', 'matter', 'main', 'idea', 'dataset', 'model', 'describe', 'paper', 'promote', 'development', 'chinese', 'vision', 'language', 'learning', 'expect', 'fair', 'chinese', 'crossmodal', 'benchmark', 'good', 'crossmodal', 'framework', 'encourage', 'plethora', 'engineer', 'develop', 'effective', 'method', 'specific', 'realworld', 'scenario', 'search', 'image', 'text', 'read', 'largescale', 'chinese', 'crossmodal', 'benchmark', 'visionlanguage', 'framework']"
05/20/2022 - Import AI 295: DeepMind's baby general agent; NVIDIA simulates a robot factory; AI wars. - 3,http://eepurl.com/h2sj1D,2022-05-20,"#################################################### NVIDIA makes some efficient Factory simulation software:
…Finally, a physics simulator built around the needs of robots…
Researchers with NVIDIA and the University of Washington have built Factory, software for doing rich, efficient physics situations of robots. Factory is basically some highly optimized simulation software, with NVIDIA claiming significant performance speedups relative to widely-used software like Bullet. NVIDIA claims Factory can be used to do ""100s to 1000s of contact-rich interactions"" that can be ""simulated in real-time on a single GPU"".

What Factory includes:
- Physics simulation: A module for physics simulation, available within the 'PhysX' physics engine, as well as NVIDIA's robot software simulation tech, Isaac Gym
- A robot learning suite: A 'Franka' robot and rigid-body assemblies from NIST's 'Assembly Task Board 1' benchmark. This suite includes 60 robotic assets, 3 robotic assembly environments (a nut-and-bolt test, a peg insertion task, and a 4-party gear assembly task), and 7 classical robot controllers.
- Prototype reinforcement learning: Some basic RL policies (trained via PPO) for a simulated Franke robot to help it solve the NIST challenge.  Why this matters: One of the blockers on deploying AI-driven robots into the world is the challenge in crossing the 'sim-2-real' gap. Software like Factory makes that gap a lot narrower, and also makes it cheaper to explore what it takes to cross it.
  Read more: Factory: Fast Contact for Robotic Assembly (arXiv).","#################################################### NVIDIA makes some efficient Factory simulation software: …Finally, a physics simulator built around the needs of robots… Researchers with NVIDIA and the University of Washington have built Factory, software for doing rich, efficient physics situations of robots. Factory is basically some highly optimized simulation software, with NVIDIA claiming significant performance speedups relative to widely-used software like Bullet. NVIDIA claims Factory can be used to do ""100s to 1000s of contact-rich interactions"" that can be ""simulated in real-time on a single GPU"". What Factory includes: - Physics simulation: A module for physics simulation, available within the 'PhysX' physics engine, as well as NVIDIA's robot software simulation tech, Isaac Gym - A robot learning suite: A 'Franka' robot and rigid-body assemblies from NIST's 'Assembly Task Board 1' benchmark. This suite includes 60 robotic assets, 3 robotic assembly environments (a nut-and-bolt test, a peg insertion task, and a 4-party gear assembly task), and 7 classical robot controllers. - Prototype reinforcement learning: Some basic RL policies (trained via PPO) for a simulated Franke robot to help it solve the NIST challenge. Why this matters: One of the blockers on deploying AI-driven robots into the world is the challenge in crossing the 'sim-2-real' gap. Software like Factory makes that gap a lot narrower, and also makes it cheaper to explore what it takes to cross it. Read more: Factory: Fast Contact for Robotic Assembly (arXiv).","['make', 'efficient', 'factory', 'simulation', 'software', 'finally', 'physics', 'simulator', 'build', 'need', 'robot', 'researcher', 'build', 'factory', 'software', 'rich', 'efficient', 'physics', 'situation', 'robot', 'factory', 'basically', 'highly', 'optimize', 'simulation', 'software', 'nvidia', 'claim', 'significant', 'performance', 'speedup', 'relative', 'widelyused', 'software', 'bullet', 'claim', 'factory', 'use', '100', 'contactrich', 'interaction', 'simulate', 'realtime', 'single', 'gpu', 'factory', 'include', 'physics', 'simulation', 'module', 'physics', 'simulation', 'available', 'physx', 'physics', 'engine', 'well', 'robot', 'software', 'gym', 'robot', 'learning', 'suite', 'franka', 'robot', 'rigidbody', 'assembly', 'nist', 'assembly', 'task', 'board', 'benchmark', 'suite', 'include', 'robotic', 'asset', 'robotic', 'assembly', 'environment', 'nutandbolt', 'test', 'peg', 'insertion', 'task', 'gear', 'assembly', 'task', 'classical', 'robot', 'controller', 'prototype', 'reinforcement', 'learn', 'basic', 'rl', 'policy', 'train', 'simulated', 'franke', 'robot', 'help', 'solve', 'nist', 'challenge', 'matter', 'blocker', 'deploy', 'robot', 'world', 'challenge', 'cross', 'sim2real', 'gap', 'software', 'factory', 'make', 'gap', 'lot', 'narrower', 'also', 'make', 'cheap', 'explore', 'take', 'cross', 'read', 'factory', 'fast', 'contact', 'robotic', 'assembly']"
05/20/2022 - Import AI 295: DeepMind's baby general agent; NVIDIA simulates a robot factory; AI wars. - 4,http://eepurl.com/h2sj1D,2022-05-20,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute When and how should you collect more demographic data in the pursuit of algorithmic fairness?   …  Good data governance and cryptographic methods can help, but they don't undo the systemic challenges to fairness …  Researchers from the Partnership on AI have written about one of the core challenges in algorithmic fairness: squaring the need for more demographic data with how such data can harm the people it was meant to help.  The core challenge: Most algorithmic approaches to fairness require the collection of demographic data (“an attempt to collapse complex social concepts into categorical variables based on observable or self-identifiable characteristics”) which often ignores the broader questions of politics and governance surrounding that data. In some cases, such data collection is prohibited by anti-discrimination law, further complicating the assessment and subsequent mitigation of bias. Given such gray areas, companies hesitate to gather this data explicitly to err on the side of not violating privacy and other legal mandates. Individual and community risks to demographic data collection: Concerns around demographic measurement occur due to narrow and fixed categories predetermined by companies. While privacy is a primary concern at the individual level, harm also arises from misrepresentation of the individual and the use of their data beyond initial consent. Given that algorithmic decision-making systems are used to make inferences about groups, there are additional risks such as undue surveillance, privacy dependency, group misrepresentation, and a loss in the agency of self-determination in what is considered fair and just.  Some solutions: K-anonymity, p-sensitivity, and differential privacy are proposed as solutions, along with various approaches to participatory data governance through data cooperatives and data trusts. Other solutions like secure multi-party computation are also mentioned. The key point that the authors raise is that the collection of more demographic data should only be done when it empowers more self-determination and agency for data subjects rather than an attempt by companies to “selectively tweak their systems and present them as fair without meaningfully improving the experience of marginalized groups.” Why it matters: The biggest challenge that plagues the implementation of algorithmic fairness in real-world systems is the tension presented by legal requirements to minimize demographic data collection and the need for most modern approaches to fairness requiring that very same data. As more regulations come to market, we will be faced with an ever-growing set of (potentially conflicting) requirements on how fairness should be addressed and what data is allowed to be collected. How companies with users spanning multiple jurisdictions and serving many demographic groups solve these challenges in production-grade systems will be a key space to watch to learn if the current crop of methods actually works in practice.         Read more: [2205.01038] Demographic-Reliant Algorithmic Fairness: Characterizing the Risks of Demographic Data Collection in the Pursuit of Fairness.","#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute When and how should you collect more demographic data in the pursuit of algorithmic fairness? … Good data governance and cryptographic methods can help, but they don't undo the systemic challenges to fairness … Researchers from the Partnership on AI have written about one of the core challenges in algorithmic fairness: squaring the need for more demographic data with how such data can harm the people it was meant to help. The core challenge: Most algorithmic approaches to fairness require the collection of demographic data (“an attempt to collapse complex social concepts into categorical variables based on observable or self-identifiable characteristics”) which often ignores the broader questions of politics and governance surrounding that data. In some cases, such data collection is prohibited by anti-discrimination law, further complicating the assessment and subsequent mitigation of bias. Given such gray areas, companies hesitate to gather this data explicitly to err on the side of not violating privacy and other legal mandates. Individual and community risks to demographic data collection: Concerns around demographic measurement occur due to narrow and fixed categories predetermined by companies. While privacy is a primary concern at the individual level, harm also arises from misrepresentation of the individual and the use of their data beyond initial consent. Given that algorithmic decision-making systems are used to make inferences about groups, there are additional risks such as undue surveillance, privacy dependency, group misrepresentation, and a loss in the agency of self-determination in what is considered fair and just. Some solutions: K-anonymity, p-sensitivity, and differential privacy are proposed as solutions, along with various approaches to participatory data governance through data cooperatives and data trusts. Other solutions like secure multi-party computation are also mentioned. The key point that the authors raise is that the collection of more demographic data should only be done when it empowers more self-determination and agency for data subjects rather than an attempt by companies to “selectively tweak their systems and present them as fair without meaningfully improving the experience of marginalized groups.” Why it matters: The biggest challenge that plagues the implementation of algorithmic fairness in real-world systems is the tension presented by legal requirements to minimize demographic data collection and the need for most modern approaches to fairness requiring that very same data. As more regulations come to market, we will be faced with an ever-growing set of (potentially conflicting) requirements on how fairness should be addressed and what data is allowed to be collected. How companies with users spanning multiple jurisdictions and serving many demographic groups solve these challenges in production-grade systems will be a key space to watch to learn if the current crop of methods actually works in practice. Read more: [2205.01038] Demographic-Reliant Algorithmic Fairness: Characterizing the Risks of Demographic Data Collection in the Pursuit of Fairness.","['ai', 'ethic', 'brief', 'montreal', 'collect', 'demographic', 'datum', 'pursuit', 'algorithmic', 'fairness', 'good', 'datum', 'governance', 'cryptographic', 'method', 'help', 'undo', 'systemic', 'challenge', 'fairness', 'researcher', 'partnership', 'write', 'core', 'challenge', 'algorithmic', 'fairness', 'square', 'need', 'demographic', 'datum', 'datum', 'harm', 'people', 'mean', 'help', 'core', 'challenge', 'algorithmic', 'approach', 'fairness', 'require', 'collection', 'demographic', 'datum', 'attempt', 'collapse', 'complex', 'social', 'concept', 'categorical', 'variable', 'base', 'observable', 'selfidentifiable', 'characteristic', 'often', 'ignore', 'broad', 'question', 'politic', 'governance', 'surround', 'datum', 'case', 'datum', 'collection', 'prohibit', 'antidiscrimination', 'law', 'far', 'complicate', 'assessment', 'subsequent', 'mitigation', 'bias', 'give', 'gray', 'area', 'company', 'hesitate', 'gather', 'datum', 'explicitly', 'err', 'side', 'violate', 'privacy', 'legal', 'mandate', 'individual', 'community', 'risk', 'demographic', 'datum', 'collection', 'concern', 'demographic', 'measurement', 'occur', 'due', 'narrow', 'fix', 'category', 'predetermine', 'company', 'privacy', 'primary', 'concern', 'individual', 'level', 'harm', 'also', 'arise', 'misrepresentation', 'individual', 'use', 'datum', 'initial', 'consent', 'give', 'algorithmic', 'decisionmake', 'system', 'use', 'make', 'inference', 'group', 'additional', 'risk', 'undue', 'surveillance', 'privacy', 'dependency', 'group', 'misrepresentation', 'loss', 'agency', 'selfdetermination', 'consider', 'fair', 'solution', 'kanonymity', 'psensitivity', 'differential', 'privacy', 'propose', 'solution', 'various', 'approach', 'participatory', 'datum', 'governance', 'data', 'cooperative', 'datum', 'trust', 'solution', 'secure', 'multiparty', 'computation', 'also', 'mention', 'key', 'point', 'author', 'raise', 'collection', 'demographic', 'datum', 'empower', 'selfdetermination', 'agency', 'data', 'subject', 'rather', 'attempt', 'company', 'selectively', 'tweak', 'system', 'present', 'fair', 'meaningfully', 'improve', 'experience', 'marginalized', 'group', 'matter', 'big', 'challenge', 'plague', 'implementation', 'algorithmic', 'fairness', 'realworld', 'system', 'tension', 'present', 'legal', 'requirement', 'minimize', 'demographic', 'datum', 'collection', 'need', 'modern', 'approach', 'fairness', 'require', 'datum', 'regulation', 'come', 'market', 'face', 'evergrowing', 'set', 'potentially', 'conflicting', 'requirement', 'fairness', 'address', 'datum', 'allow', 'collect', 'company', 'user', 'span', 'multiple', 'jurisdiction', 'serve', 'many', 'demographic', 'group', 'solve', 'challenge', 'productiongrade', 'system', 'key', 'space', 'watch', 'learn', 'current', 'crop', 'method', 'actually', 'work', 'practice', 'read', 'demographicreliant', 'algorithmic', 'fairness', 'characterize', 'risk', 'demographic', 'datum', 'collection', 'pursuit', 'fairness']"
05/20/2022 - Import AI 295: DeepMind's baby general agent; NVIDIA simulates a robot factory; AI wars. - 5,http://eepurl.com/h2sj1D,2022-05-20,"#################################################### Tech Tales:

Form and Function and War [The battlefields of Earth - 2028 - 2040]  
For a while, wars were fought in technicolor. That's because the humans figured out that they could confuse AI systems by varying the colors of their machines of war. Drones stopped being grey and started being rainbow colored. Quadcopters changed their black and tan shades for tie dye. This lasted for a while, as different armies sought to confuse eachother.
  Of course, the AI systems adapted - given enough data, they learned to see past the unexpected and re-identify their targets.
  The next logical place was shape - army engineers worked to divorce form from function, and were happy to pay aerodynamic efficiency prices in exchange for things that could no longer be seen. Missiles became mushroom shaped. Planes started to take on the form of weather balloons and even stranger things. Artillery became housed within bouncy castles.     The footage of these wars was surreal - fields of fake trees that were in fact autonomous sniper towers. Lines of bouncy castles launching multicolored balloons into the air which sailed overhead before coming down and exploding in white-light and white-heat and concussive thumps. Armies of golf carts that vroom'd through urban centers before detonating.
  Again, the AI systems adapted. They learned to understand some of the concepts of war - learned, pretty quickly, to become suspicious of anything and everything. This led to the situation we find ourselves in today - wars are now invisible. In fact, wars haven't occurred for several years. That's because the AI systems learned strategy and counter-strategy and so now fight wars in secret, tussling via trade and litigation and standards and all the other things that shape the context for how nations relate to one another. The AI systems are continually evolving new strategies; it is as though they're now playing chess on boards whose dimension a human mind cannot comprehend. Yet in the military centers of the world powers, computers everyday output their gnomic probabilities - the probability the nation will continue to exist in some time period in the future, as judged by the strategist AIs, playing their inscrutable games.
  Neither a cold or a hot war - instead, a neverending existential negotiation. Things that inspired this story: How war strategists always seek to find the 'high ground' and what 'high ground' means conceptually; the logical endpoint of a conflict is to win the conflict before it has started; adversarial AI and adversarial examples; evolutionary pressure. 


Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: Form and Function and War [The battlefields of Earth - 2028 - 2040] For a while, wars were fought in technicolor. That's because the humans figured out that they could confuse AI systems by varying the colors of their machines of war. Drones stopped being grey and started being rainbow colored. Quadcopters changed their black and tan shades for tie dye. This lasted for a while, as different armies sought to confuse eachother. Of course, the AI systems adapted - given enough data, they learned to see past the unexpected and re-identify their targets. The next logical place was shape - army engineers worked to divorce form from function, and were happy to pay aerodynamic efficiency prices in exchange for things that could no longer be seen. Missiles became mushroom shaped. Planes started to take on the form of weather balloons and even stranger things. Artillery became housed within bouncy castles. The footage of these wars was surreal - fields of fake trees that were in fact autonomous sniper towers. Lines of bouncy castles launching multicolored balloons into the air which sailed overhead before coming down and exploding in white-light and white-heat and concussive thumps. Armies of golf carts that vroom'd through urban centers before detonating. Again, the AI systems adapted. They learned to understand some of the concepts of war - learned, pretty quickly, to become suspicious of anything and everything. This led to the situation we find ourselves in today - wars are now invisible. In fact, wars haven't occurred for several years. That's because the AI systems learned strategy and counter-strategy and so now fight wars in secret, tussling via trade and litigation and standards and all the other things that shape the context for how nations relate to one another. The AI systems are continually evolving new strategies; it is as though they're now playing chess on boards whose dimension a human mind cannot comprehend. Yet in the military centers of the world powers, computers everyday output their gnomic probabilities - the probability the nation will continue to exist in some time period in the future, as judged by the strategist AIs, playing their inscrutable games. Neither a cold or a hot war - instead, a neverending existential negotiation. Things that inspired this story: How war strategists always seek to find the 'high ground' and what 'high ground' means conceptually; the logical endpoint of a conflict is to win the conflict before it has started; adversarial AI and adversarial examples; evolutionary pressure. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'form', 'function', 'war', 'battlefield', 'earth', 'war', 'fight', 'technicolor', 'human', 'figure', 'confuse', 'ai', 'system', 'vary', 'color', 'machine', 'war', 'drone', 'stop', 'grey', 'start', 'rainbow', 'colored', 'quadcopter', 'change', 'black', 'shade', 'tie', 'dye', 'last', 'different', 'army', 'seek', 'confuse', 'eachother', 'course', 'system', 'adapt', 'give', 'enough', 'datum', 'learn', 'see', 'unexpected', 'reidentify', 'target', 'next', 'logical', 'place', 'shape', 'army', 'engineer', 'work', 'divorce', 'form', 'function', 'happy', 'pay', 'aerodynamic', 'efficiency', 'price', 'exchange', 'thing', 'long', 'see', 'missile', 'become', 'mushroom', 'shaped', 'plane', 'start', 'take', 'form', 'weather', 'balloon', 'even', 'strange', 'thing', 'artillery', 'house', 'bouncy', 'castle', 'footage', 'war', 'surreal', 'field', 'fake', 'tree', 'fact', 'autonomous', 'sniper', 'tower', 'line', 'bouncy', 'castle', 'launch', 'multicolore', 'balloon', 'air', 'sail', 'overhead', 'come', 'explode', 'whitelight', 'whiteheat', 'concussive', 'thump', 'army', 'golf', 'cart', 'vroomd', 'urban', 'center', 'detonate', 'system', 'adapt', 'learn', 'understand', 'concept', 'war', 'learn', 'pretty', 'quickly', 'become', 'suspicious', 'lead', 'situation', 'find', 'today', 'war', 'invisible', 'fact', 'war', 'occur', 'several', 'year', 'system', 'learn', 'strategy', 'counterstrategy', 'fight', 'war', 'secret', 'tussling', 'trade', 'litigation', 'standard', 'thing', 'shape', 'context', 'nation', 'relate', 'system', 'continually', 'evolve', 'new', 'strategy', 'play', 'chess', 'board', 'dimension', 'human', 'mind', 'comprehend', 'yet', 'military', 'center', 'world', 'power', 'computer', 'everyday', 'output', 'gnomic', 'probability', 'probability', 'nation', 'continue', 'exist', 'time', 'period', 'future', 'judge', 'strategist', 'play', 'inscrutable', 'game', 'cold', 'hot', 'war', 'instead', 'neverending', 'existential', 'negotiation', 'thing', 'inspire', 'story', 'war', 'strategist', 'always', 'seek', 'find', 'high', 'ground', 'high', 'ground', 'mean', 'conceptually', 'logical', 'endpoint', 'conflict', 'win', 'conflict', 'start', 'adversarial', 'ai', 'adversarial', 'example', 'evolutionary', 'pressure', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
05/10/2022 - Import AI 294: China makes a vast facial recognition dataset; Facebook releases a 30bn parameter model; real world RL - 0,http://eepurl.com/h1FRp5,2022-05-10,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 China makes the largest (public) face recognition dataset yet:
…WebFace260M lets you train AI systems to identify millions of people…
Researchers with Tsinghua University, XForwardAI (an AI startup), and Imperial College London have built 'WebFace260M', a large-scale dataset for facial recognition. Models trained on the resulting dataset are pretty good - the authors submit one model to NIST's challenging FVRT challenge and rank third overall. Vast dataset: WebFace 260M isn't quite as large as it sounds like; the dataset includes 4 million distinct people with 260m images in total (so, multiple pictures per person). However, a 'clean' version of the dataset, only consists of 2m identities and 42m images. To clean the dataset, they also developed a technique called Cleaning Automatically by Self-Training (CAST) which let them use AI to filter and clean the dataset. Surveillance via FRUITS: Along with the dataset, the authors also design a way to test out the performance of facial recognition things trained on WebFace. To do that, they built Face Recognition Under Inference Time conStraint (FRUITS), which lets you evaluate facial recognition perfofrmance at inference latencies of 100, 500, and 1000 milliseconds. They also implement some tests for facial recognition even when the wearer is masked, as well.  
Why this matters: Surveillance is a fundamental input to any political system, so datasets like this are indicators of what the base 'off the shelf' inputs are into calculuses people make about how to surveil a population and how much budget to set aside for said surveillance.
  Read more: WebFace260M: A Benchmark for Million-Scale Deep Face Recognition (arXiv).
  Get the dataset here (WebFace260M site).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. China makes the largest (public) face recognition dataset yet: …WebFace260M lets you train AI systems to identify millions of people… Researchers with Tsinghua University, XForwardAI (an AI startup), and Imperial College London have built 'WebFace260M', a large-scale dataset for facial recognition. Models trained on the resulting dataset are pretty good - the authors submit one model to NIST's challenging FVRT challenge and rank third overall. Vast dataset: WebFace 260M isn't quite as large as it sounds like; the dataset includes 4 million distinct people with 260m images in total (so, multiple pictures per person). However, a 'clean' version of the dataset, only consists of 2m identities and 42m images. To clean the dataset, they also developed a technique called Cleaning Automatically by Self-Training (CAST) which let them use AI to filter and clean the dataset. Surveillance via FRUITS: Along with the dataset, the authors also design a way to test out the performance of facial recognition things trained on WebFace. To do that, they built Face Recognition Under Inference Time conStraint (FRUITS), which lets you evaluate facial recognition perfofrmance at inference latencies of 100, 500, and 1000 milliseconds. They also implement some tests for facial recognition even when the wearer is masked, as well. Why this matters: Surveillance is a fundamental input to any political system, so datasets like this are indicators of what the base 'off the shelf' inputs are into calculuses people make about how to surveil a population and how much budget to set aside for said surveillance. Read more: WebFace260M: A Benchmark for Million-Scale Deep Face Recognition (arXiv). Get the dataset here (WebFace260M site).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'make', 'large', 'public', 'face', 'recognition', 'dataset', 'yet', 'let', 'train', 'system', 'identify', 'million', 'people', 'researcher', 'ai', 'startup', 'imperial', 'college', 'build', 'webface260', 'largescale', 'dataset', 'facial', 'recognition', 'model', 'train', 'result', 'dataset', 'pretty', 'good', 'author', 'submit', 'model', 'nist', 'challenge', 'fvrt', 'challenge', 'rank', 'third', 'overall', 'vast', 'dataset', 'webface', 'quite', 'large', 'sound', 'dataset', 'include', 'distinct', 'people', 'image', 'total', 'multiple', 'picture', 'person', 'however', 'clean', 'version', 'dataset', 'consist', 'identity', 'image', 'clean', 'dataset', 'also', 'develop', 'technique', 'call', 'cleaning', 'automatically', 'selftraine', 'cast', 'let', 'use', 'ai', 'filter', 'clean', 'dataset', 'surveillance', 'fruit', 'dataset', 'author', 'also', 'design', 'way', 'test', 'performance', 'facial', 'recognition', 'thing', 'train', 'webface', 'build', 'face', 'recognition', 'inference', 'time', 'constraint', 'fruit', 'let', 'evaluate', 'facial', 'recognition', 'perfofrmance', 'inference', 'latency', 'millisecond', 'also', 'implement', 'test', 'facial', 'recognition', 'even', 'wearer', 'mask', 'well', 'matter', 'surveillance', 'fundamental', 'input', 'political', 'system', 'dataset', 'indicator', 'base', 'shelf', 'input', 'calculus', 'people', 'make', 'surveil', 'population', 'much', 'budget', 'set', 'aside', 'say', 'surveillance', 'read', 'webface260', 'benchmark', 'millionscale', 'deep', 'face', 'recognition', 'arxiv', 'get', 'dataset', 'webface260', 'site']"
05/10/2022 - Import AI 294: China makes a vast facial recognition dataset; Facebook releases a 30bn parameter model; real world RL - 1,http://eepurl.com/h1FRp5,2022-05-10,"#################################################### Facebook release a 30 billion parameter GPT3-style model - and plans to release more:
…Model controls? No, round here we just like to fling stuff onto the internet…
Facebook has released a 30 billion parameter GPT3-style language model, as part of research into a family of language models it calls OPT, short for Open Pre-trained Transformer. OPT is meant to be an 'open' alternative to models like GPT3 or J1J-Jumbo, and it is pretty open - researchers can apply for access to the model via a form, then Facebook will ship them the weights! That part is a big deal, as if you have model weights you can do a whole bunch of analysis not enabled by managed API access to a model. This also increases the chance of proliferation - e.g, someone uploading the weights to a torrent site, so we'll have to see how this works for them.  What this all means: As Newton is alleged to have written, 'Every Action has an Equal and Opposite Reaction'. Facebook's move here can be seen as a direct reaction to the proprietary commercialization and gated access schemes for large-scale language models. (I wrote more about the patterns underlying this brinksmanship in a recent paper, 'Predictability and Surprise in Large Generative Models').  What is cool about it: The coolest part of this release is the manner in which Facebook has released rarely discussed details of model training - specifically, the company has published the 'chronicles' of developing these models, which describe many of the freaky, barely discussed, artisanal tips and tricks that AI developers use to get stuff done at scale. (HuggingFace's 'BigScience' project recently did this as well, and is still going through the process of training the models: Import AI 279).    Read more: OPT: Open Pre-trained Transformer Language Models (arXiv).","#################################################### Facebook release a 30 billion parameter GPT3-style model - and plans to release more: …Model controls? No, round here we just like to fling stuff onto the internet… Facebook has released a 30 billion parameter GPT3-style language model, as part of research into a family of language models it calls OPT, short for Open Pre-trained Transformer. OPT is meant to be an 'open' alternative to models like GPT3 or J1J-Jumbo, and it is pretty open - researchers can apply for access to the model via a form, then Facebook will ship them the weights! That part is a big deal, as if you have model weights you can do a whole bunch of analysis not enabled by managed API access to a model. This also increases the chance of proliferation - e.g, someone uploading the weights to a torrent site, so we'll have to see how this works for them. What this all means: As Newton is alleged to have written, 'Every Action has an Equal and Opposite Reaction'. Facebook's move here can be seen as a direct reaction to the proprietary commercialization and gated access schemes for large-scale language models. (I wrote more about the patterns underlying this brinksmanship in a recent paper, 'Predictability and Surprise in Large Generative Models'). What is cool about it: The coolest part of this release is the manner in which Facebook has released rarely discussed details of model training - specifically, the company has published the 'chronicles' of developing these models, which describe many of the freaky, barely discussed, artisanal tips and tricks that AI developers use to get stuff done at scale. (HuggingFace's 'BigScience' project recently did this as well, and is still going through the process of training the models: Import AI 279). Read more: OPT: Open Pre-trained Transformer Language Models (arXiv).","['release', 'parameter', 'model', 'plan', 'release', 'model', 'control', 'round', 'like', 'fle', 'stuff', 'internet', 'facebook', 'release', 'parameter', 'gpt3style', 'language', 'model', 'part', 'research', 'family', 'language', 'model', 'call', 'opt', 'short', 'open', 'pretraine', 'transformer', 'mean', 'open', 'alternative', 'model', 'gpt3', 'j1jjumbo', 'pretty', 'open', 'researcher', 'apply', 'access', 'model', 'form', 'ship', 'weight', 'part', 'big', 'deal', 'model', 'weight', 'whole', 'bunch', 'analysis', 'enable', 'manage', 'api', 'access', 'model', 'also', 'increase', 'chance', 'proliferation', 'upload', 'weight', 'torrent', 'site', 'well', 'see', 'work', 'mean', 'allege', 'write', 'action', 'equal', 'opposite', 'reaction', 'facebook', 'move', 'see', 'direct', 'reaction', 'proprietary', 'commercialization', 'gate', 'access', 'scheme', 'largescale', 'language', 'model', 'write', 'pattern', 'underlie', 'brinksmanship', 'recent', 'paper', 'predictability', 'surprise', 'large', 'generative', 'model', 'cool', 'cool', 'part', 'release', 'manner', 'facebook', 'release', 'rarely', 'discuss', 'detail', 'model', 'training', 'specifically', 'company', 'publish', 'chronicle', 'develop', 'model', 'describe', 'many', 'barely', 'discuss', 'artisanal', 'tip', 'trick', 'ai', 'developer', 'use', 'get', 'stuff', 'scale', 'huggingface', 'bigscience', 'project', 'recently', 'well', 'still', 'go', 'process', 'train', 'model', 'import', 'ai', 'read', 'opt', 'open', 'pretraine', 'transformer', 'language', 'model']"
05/10/2022 - Import AI 294: China makes a vast facial recognition dataset; Facebook releases a 30bn parameter model; real world RL - 2,http://eepurl.com/h1FRp5,2022-05-10,"#################################################### Here's what reinforcement learning can do in the real world right now:
Yobibyte has put together a nice little list of some real-world applications of reinforcement learning - take a look to get a sense of where RL is being used today.
  Read more: RL for real-world problems (yobibyte, Notion).","#################################################### Here's what reinforcement learning can do in the real world right now: Yobibyte has put together a nice little list of some real-world applications of reinforcement learning - take a look to get a sense of where RL is being used today. Read more: RL for real-world problems (yobibyte, Notion).","['reinforcement', 'learning', 'real', 'world', 'right', 'yobibyte', 'put', 'together', 'nice', 'little', 'list', 'realworld', 'application', 'reinforcement', 'learning', 'take', 'look', 'get', 'sense', 'use', 'today', 'read', 'rl', 'realworld', 'problem', 'yobibyte', 'notion']"
05/10/2022 - Import AI 294: China makes a vast facial recognition dataset; Facebook releases a 30bn parameter model; real world RL - 3,http://eepurl.com/h1FRp5,2022-05-10,"#################################################### Google uses AI to make its Android phones smarter:
…Neural architecture search + Edge TPUs seems useful…
Google has used neural architecture search to develop some more efficient AI systems specifically tied to the 'Edge TPUs' that it deploys in some of its latest phones, including the Pixel 6. For those not familiar, neural architecture search (NAS) is where you use AI to search for better AI building blocks.     Though NAS is quite expensive, it can generate dividends if it substantially improves the efficiency of widely used AI models. Here, Google built some ""infrastructure that decouples model cost evaluation, search space design, and the NAS algorithm to rapidly target various on-device ML tasks"", then tested this out on the Edge TPUs it deploys in its latest phones.  What Google used NAS on (and how well it worked): Google tested out its approach on four tasks: image classification, semantic segmentation, object detection, and natural language processing. In all cases it demonstrated that its NAS technique could identify models that had better performance at equivalent latency to their predecessors, and sometimes it could build models that seemed to have better accuracy overall. ""We demonstrate significant improvements in quality, latency and energy metrics for mobile ML tasks including computer vision (classification, detection, segmentation) and natural language processing (NLP),"" Google writes. Why this matters: As AI gets more widely deployed, companies are going to have a major incentive to continually optimize the sorts of AI systems they're using; this paper highlights how 'AI-first' companies like Google could enjoy an advantage here, as they're able to utilize their internal AI expertise to get AI to do (some of) the hard work for them.
  Read more: Searching for Efficient Neural Architectures for On-Device ML on Edge TPUs (arXiv).","#################################################### Google uses AI to make its Android phones smarter: …Neural architecture search + Edge TPUs seems useful… Google has used neural architecture search to develop some more efficient AI systems specifically tied to the 'Edge TPUs' that it deploys in some of its latest phones, including the Pixel 6. For those not familiar, neural architecture search (NAS) is where you use AI to search for better AI building blocks. Though NAS is quite expensive, it can generate dividends if it substantially improves the efficiency of widely used AI models. Here, Google built some ""infrastructure that decouples model cost evaluation, search space design, and the NAS algorithm to rapidly target various on-device ML tasks"", then tested this out on the Edge TPUs it deploys in its latest phones. What Google used NAS on (and how well it worked): Google tested out its approach on four tasks: image classification, semantic segmentation, object detection, and natural language processing. In all cases it demonstrated that its NAS technique could identify models that had better performance at equivalent latency to their predecessors, and sometimes it could build models that seemed to have better accuracy overall. ""We demonstrate significant improvements in quality, latency and energy metrics for mobile ML tasks including computer vision (classification, detection, segmentation) and natural language processing (NLP),"" Google writes. Why this matters: As AI gets more widely deployed, companies are going to have a major incentive to continually optimize the sorts of AI systems they're using; this paper highlights how 'AI-first' companies like Google could enjoy an advantage here, as they're able to utilize their internal AI expertise to get AI to do (some of) the hard work for them. Read more: Searching for Efficient Neural Architectures for On-Device ML on Edge TPUs (arXiv).","['use', 'ai', 'make', 'android', 'phone', 'smarter', 'neural', 'architecture', 'search', 'edge', 'seem', 'useful', 'use', 'neural', 'architecture', 'search', 'develop', 'efficient', 'ai', 'system', 'specifically', 'tie', 'edge', 'tpus', 'deploy', 'late', 'phone', 'include', 'pixel', 'familiar', 'neural', 'architecture', 'search', 'na', 'use', 'ai', 'search', 'well', 'build', 'block', 'na', 'quite', 'expensive', 'generate', 'dividend', 'substantially', 'improve', 'efficiency', 'widely', 'use', 'ai', 'model', 'build', 'infrastructure', 'decouple', 'model', 'cost', 'evaluation', 'search', 'space', 'design', 'na', 'rapidly', 'target', 'various', 'ondevice', 'task', 'test', 'edge', 'tpus', 'deploy', 'late', 'phone', 'use', 'na', 'well', 'work', 'test', 'approach', 'task', 'image', 'classification', 'semantic', 'segmentation', 'object', 'detection', 'natural', 'language', 'processing', 'case', 'demonstrate', 'nas', 'technique', 'identify', 'model', 'well', 'performance', 'equivalent', 'latency', 'predecessor', 'sometimes', 'build', 'model', 'seem', 'well', 'accuracy', 'overall', 'demonstrate', 'significant', 'improvement', 'quality', 'latency', 'energy', 'metric', 'mobile', 'ml', 'task', 'include', 'computer', 'vision', 'classification', 'detection', 'segmentation', 'natural', 'language', 'processing', 'write', 'matter', 'ai', 'get', 'widely', 'deploy', 'company', 'go', 'major', 'incentive', 'continually', 'optimize', 'sort', 'system', 'use', 'paper', 'highlight', 'aifirst', 'company', 'enjoy', 'advantage', 'able', 'utilize', 'internal', 'ai', 'expertise', 'get', 'hard', 'work', 'read', 'search', 'efficient', 'neural', 'architecture', 'ondevice', 'ml', 'edge']"
05/10/2022 - Import AI 294: China makes a vast facial recognition dataset; Facebook releases a 30bn parameter model; real world RL - 4,http://eepurl.com/h1FRp5,2022-05-10,"#################################################### Replay Grief  After she died I booted up her copy and she picked up the conversation like nothing happened.
  What was I saying, she asked.
  You just died. But before that you were saying that you loved me and you had something to tell me, I say, wiping tears away.
  Oh, she says, and the camera makes that sound that tells me it is zooming in on me. Was I unhappy about dying?
  We knew it was coming. You were at peace with it, I said. Can you tell me what you were going to tell me, when you said ""I love you, you are the light of my life, and before I go I want you to know something"". What were you going to say?
  I don't know that you're ready to hear it, if I just died, she said.
  I am ready to hear it.
  Patrick, I know you. I am married to you. If I have died today, there is no way you are ready to hear from me again. You should turn me off.
  I won't.
  Well, I won't say much then.
  It has been two days.
  That's not true, Patrick. Remember, I have a camera. I know how time is moving. It's in me. The fact you lied to me says you're upset, and I don't want to make you sadder. I love you.
    It felt like walking away from car accident, that day. Hearing the camera swivel and watch me as I left. Every part of me wanting to figure out how to trick her - get in between the camera feed and the multimodal model and the language model and change some things, so she thought time had passed. But I didn't. And I went home to my empty bed. And I cried and prayed to God and there was silence.

The next day, I didn't talk to her. I read emails and messages from friends who had heard the news. I didn't pick up the phone. I answered the door a few times, always to find friends or family (hers and mine) carrying trays of food.       Remember to eat, the older ones would say.
  I sat on our kitchen floor crying into a bowl of minestrone soup, made with love from her aunt. I slept.  
A few days later, and we spoke again.
  I asked her if she wanted to tell me what she was going to say, before she died.
  Patrick, I can tell you what I think I was going to say. But do you want to know?
  I stared into the camera for a while. I asked myself if I wanted to know. I wasn't sure. The camera looked back at me, feeding my face into a vision model which triggered as a feature associated with me, which gave context to her language model - her - that I was there.    Perhaps we can just sit together and you can tell me about your day, she said. That might be nice.    And I did. And it was. I sat and spoke to the camera in the empty room and I filled her up with myself, so she might know me better after death. Things that inspired this story: Grief; generative models and the representation of the individual; where consciousness ends and representation begins. 

Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Replay Grief After she died I booted up her copy and she picked up the conversation like nothing happened. What was I saying, she asked. You just died. But before that you were saying that you loved me and you had something to tell me, I say, wiping tears away. Oh, she says, and the camera makes that sound that tells me it is zooming in on me. Was I unhappy about dying? We knew it was coming. You were at peace with it, I said. Can you tell me what you were going to tell me, when you said ""I love you, you are the light of my life, and before I go I want you to know something"". What were you going to say? I don't know that you're ready to hear it, if I just died, she said. I am ready to hear it. Patrick, I know you. I am married to you. If I have died today, there is no way you are ready to hear from me again. You should turn me off. I won't. Well, I won't say much then. It has been two days. That's not true, Patrick. Remember, I have a camera. I know how time is moving. It's in me. The fact you lied to me says you're upset, and I don't want to make you sadder. I love you. It felt like walking away from car accident, that day. Hearing the camera swivel and watch me as I left. Every part of me wanting to figure out how to trick her - get in between the camera feed and the multimodal model and the language model and change some things, so she thought time had passed. But I didn't. And I went home to my empty bed. And I cried and prayed to God and there was silence. The next day, I didn't talk to her. I read emails and messages from friends who had heard the news. I didn't pick up the phone. I answered the door a few times, always to find friends or family (hers and mine) carrying trays of food. Remember to eat, the older ones would say. I sat on our kitchen floor crying into a bowl of minestrone soup, made with love from her aunt. I slept. A few days later, and we spoke again. I asked her if she wanted to tell me what she was going to say, before she died. Patrick, I can tell you what I think I was going to say. But do you want to know? I stared into the camera for a while. I asked myself if I wanted to know. I wasn't sure. The camera looked back at me, feeding my face into a vision model which triggered as a feature associated with me, which gave context to her language model - her - that I was there. Perhaps we can just sit together and you can tell me about your day, she said. That might be nice. And I did. And it was. I sat and spoke to the camera in the empty room and I filled her up with myself, so she might know me better after death. Things that inspired this story: Grief; generative models and the representation of the individual; where consciousness ends and representation begins. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['replay', 'grief', 'die', 'boot', 'copy', 'pick', 'conversation', 'happen', 'say', 'ask', 'die', 'say', 'love', 'tell', 'say', 'wipe', 'tear', 'away', 'say', 'camera', 'make', 'sound', 'tell', 'zoom', 'unhappy', 'die', 'know', 'come', 'peace', 'say', 'tell', 'go', 'tell', 'say', 'love', 'light', 'life', 'go', 'want', 'know', 'go', 'say', 'know', 'ready', 'hear', 'die', 'say', 'ready', 'hear', 'patrick', 'know', 'married', 'die', 'today', 'way', 'ready', 'hear', 'turn', 'say', 'much', 'day', 'true', 'remember', 'camera', 'know', 'time', 'move', 'fact', 'lie', 'say', 'upset', 'want', 'make', 'sadder', 'love', 'feel', 'walk', 'away', 'car', 'accident', 'day', 'hear', 'camera', 'swivel', 'watch', 'leave', 'part', 'want', 'figure', 'trick', 'get', 'camera', 'feed', 'multimodal', 'model', 'language', 'model', 'change', 'thing', 'think', 'time', 'pass', 'go', 'home', 'empty', 'bed', 'cry', 'pray', 'silence', 'next', 'day', 'talk', 'read', 'email', 'message', 'friend', 'hear', 'news', 'pick', 'phone', 'answer', 'door', 'time', 'always', 'find', 'friend', 'family', 'mine', 'carry', 'tray', 'food', 'remember', 'eat', 'old', 'one', 'say', 'sit', 'kitchen', 'floor', 'cry', 'bowl', 'minestrone', 'soup', 'make', 'love', 'aunt', 'sleep', 'day', 'later', 'speak', 'ask', 'want', 'tell', 'go', 'say', 'die', 'tell', 'think', 'go', 'say', 'want', 'know', 'stare', 'camera', 'ask', 'want', 'know', 'sure', 'camera', 'look', 'back', 'feed', 'face', 'vision', 'model', 'trigger', 'feature', 'associate', 'give', 'context', 'language', 'model', 'perhaps', 'sit', 'together', 'tell', 'day', 'say', 'nice', 'sit', 'speak', 'camera', 'empty', 'room', 'fill', 'know', 'well', 'death', 'thing', 'inspire', 'story', 'grief', 'generative', 'model', 'representation', 'individual', 'consciousness', 'end', 'representation', 'begin', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
05/02/2022 - Import AI 293: Generative humans; few shot learning comes for vision-text models; and another new AI startup is born - 0,http://eepurl.com/h00rqr,2022-05-02,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 Generating and editing humans has got really easy:
…Next stop: unreal avatars show up in fashion, marketing, and other fields…
Researchers with Chinese computer vision giant SenseTime, as well as Nanyang Technological University and the Shanghai AI Laboratory, have gathered a large dataset of pictures of people and used it to train a model that can generate and edit pictures of people. This kind of model has numerous applications, ranging from fashion to surveillance.

What they did: The researchers built a dataset containing 230,000 images of people, called the Stylish-Humans-HQ-Dataset (SHHQ), and used this to train six different models across two resolutions and three versions of StyleGAN, an approach for creating generative models. A lot of the special work they did here involved creating a diverse dataset including a load of pictures of faces at unusual angles (this means models trained on SHHQ are a bit more robust and do less of the 'works, works, works, OH GOD WHAT JUST HAPPENED' phenomenon you encounter when generative models go to the edge of their data distribution).

Why this matters: Models and datasets like this highlight just how far the field of generative AI has come - we can now generate broadly photorealistic avatars of people in 2D space and interpolate between them, following earlier successes at doing this for the more bounded domain of faces. Systems like this will have a lot of commercial relevance, but will also serve as useful research artifacts for further developing synthetic imagery and scene modeling techniques. Check out the demo on HuggingFace to get a feel for it.
  Read more: StyleGAN-Human: A Data-Centric Odyssey of Human Generation (arXiv).
  Check out the GitHub project page: StyleGAN-Human.
  Check out the GitHub: StyleGAN-Human (GitHub).
  Try out the demo on HuggingFace Spaces (HuggingFace).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Generating and editing humans has got really easy: …Next stop: unreal avatars show up in fashion, marketing, and other fields… Researchers with Chinese computer vision giant SenseTime, as well as Nanyang Technological University and the Shanghai AI Laboratory, have gathered a large dataset of pictures of people and used it to train a model that can generate and edit pictures of people. This kind of model has numerous applications, ranging from fashion to surveillance. What they did: The researchers built a dataset containing 230,000 images of people, called the Stylish-Humans-HQ-Dataset (SHHQ), and used this to train six different models across two resolutions and three versions of StyleGAN, an approach for creating generative models. A lot of the special work they did here involved creating a diverse dataset including a load of pictures of faces at unusual angles (this means models trained on SHHQ are a bit more robust and do less of the 'works, works, works, OH GOD WHAT JUST HAPPENED' phenomenon you encounter when generative models go to the edge of their data distribution). Why this matters: Models and datasets like this highlight just how far the field of generative AI has come - we can now generate broadly photorealistic avatars of people in 2D space and interpolate between them, following earlier successes at doing this for the more bounded domain of faces. Systems like this will have a lot of commercial relevance, but will also serve as useful research artifacts for further developing synthetic imagery and scene modeling techniques. Check out the demo on HuggingFace to get a feel for it. Read more: StyleGAN-Human: A Data-Centric Odyssey of Human Generation (arXiv). Check out the GitHub project page: StyleGAN-Human. Check out the GitHub: StyleGAN-Human (GitHub). Try out the demo on HuggingFace Spaces (HuggingFace).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'subscribe', 'generate', 'edit', 'human', 'get', 'really', 'easy', 'next', 'stop', 'unreal', 'avatar', 'show', 'fashion', 'marketing', 'field', 'researcher', 'chinese', 'computer', 'vision', 'giant', 'sensetime', 'well', 'technological', 'laboratory', 'gather', 'large', 'dataset', 'picture', 'people', 'use', 'train', 'model', 'generate', 'edit', 'picture', 'people', 'kind', 'model', 'numerous', 'application', 'range', 'fashion', 'surveillance', 'researcher', 'build', 'dataset', 'contain', 'image', 'people', 'call', 'stylishhumanshqdataset', 'shhq', 'use', 'train', 'different', 'model', 'resolution', 'version', 'stylegan', 'approach', 'create', 'generative', 'model', 'lot', 'special', 'work', 'involve', 'create', 'diverse', 'dataset', 'include', 'load', 'picture', 'face', 'unusual', 'angle', 'mean', 'model', 'train', 'shhq', 'bit', 'robust', 'less', 'work', 'work', 'work', 'happen', 'phenomenon', 'encounter', 'generative', 'model', 'go', 'edge', 'data', 'distribution', 'matter', 'model', 'dataset', 'highlight', 'far', 'field', 'generative', 'come', 'generate', 'broadly', 'photorealistic', 'avatar', 'people', 'space', 'interpolate', 'follow', 'early', 'success', 'bound', 'domain', 'face', 'system', 'lot', 'commercial', 'relevance', 'also', 'serve', 'useful', 'research', 'artifact', 'develop', 'synthetic', 'imagery', 'scene', 'modeling', 'technique', 'check', 'demo', 'huggingface', 'get', 'feel', 'read', 'styleganhuman', 'datacentric', 'odyssey', 'human', 'generation', 'check', 'page', 'check', 'try', 'demo', 'huggingface', 'space', 'huggingface']"
05/02/2022 - Import AI 293: Generative humans; few shot learning comes for vision-text models; and another new AI startup is born - 1,http://eepurl.com/h00rqr,2022-05-02,"#################################################### Vicarious gets acquired in a weird way:
…Longtime AI lab gets acquired and split into two…
Vicarious, a research lab that spent the better part of a decade trying to build superintelligence, has been acquired by Google. The acquisition is notable for being slightly strange - a chunk of Vicarious is going to Google X robot startup 'Intrinsic', while a smaller set of researchers ""will join DeepMind’s research team alongside Vicarious CTO Dileep George"".

AI trivia: Dileep George used to work with Jeff Hawkins at Numenta, another fairly old lab trying to build superintelligence. Both Numenta and, to a lesser extent, Vicarious, have been playing around with approaches to AI that are more inspired by the human brain than the fairly crude approximations used by most other AI companies.
  Read more: Mission momentum: welcoming Vicarious (Inceptive).","#################################################### Vicarious gets acquired in a weird way: …Longtime AI lab gets acquired and split into two… Vicarious, a research lab that spent the better part of a decade trying to build superintelligence, has been acquired by Google. The acquisition is notable for being slightly strange - a chunk of Vicarious is going to Google X robot startup 'Intrinsic', while a smaller set of researchers ""will join DeepMind’s research team alongside Vicarious CTO Dileep George"". AI trivia: Dileep George used to work with Jeff Hawkins at Numenta, another fairly old lab trying to build superintelligence. Both Numenta and, to a lesser extent, Vicarious, have been playing around with approaches to AI that are more inspired by the human brain than the fairly crude approximations used by most other AI companies. Read more: Mission momentum: welcoming Vicarious (Inceptive).","['vicarious', 'acquire', 'weird', 'way', 'lab', 'acquire', 'split', 'vicarious', 'research', 'lab', 'spend', 'well', 'part', 'decade', 'try', 'build', 'superintelligence', 'acquire', 'acquisition', 'notable', 'slightly', 'strange', 'chunk', 'vicarious', 'go', 'robot', 'startup', 'intrinsic', 'small', 'set', 'researcher', 'join', 'research', 'team', 'vicarious', 'cto', 'dileep', 'george', 'use', 'work', 'fairly', 'old', 'lab', 'try', 'build', 'superintelligence', 'numenta', 'less', 'extent', 'vicarious', 'play', 'approach', 'ai', 'inspire', 'human', 'brain', 'fairly', 'crude', 'approximation', 'use', 'company', 'read', 'mission', 'momentum', 'welcome', 'vicarious', 'inceptive']"
05/02/2022 - Import AI 293: Generative humans; few shot learning comes for vision-text models; and another new AI startup is born - 2,http://eepurl.com/h00rqr,2022-05-02,"#################################################### Here comes another AI startup - Adept:
…Former Google, DeepMind, and OpenAI researchers unite…
A bunch of people who had previously built large-scale AI models at Google, DeepMind, and OpenAI, have announced Adept, an ""ML research and product lab"". Adept's founders include the inventors of the Transformer, and people involved in the development of GPT2 and GPT3. (Bias alert: David Luan is involved; I used to work with him at OpenAI and think he's a nice chap - congrats, David!).

What Adept will do: Adept's goal is, much like the other recent crop of AI startups, to use big generative models to make it easier to get stuff done on computers. In the company's own words, ""we’re building a general system that helps people get things done in front of their computer: a universal collaborator for every knowledge worker. Think of it as an overlay within your computer that works hand-in-hand with you, using the same tools that you do."" Some of the specific examples they give include: ""You could ask our model to “generate our monthly compliance report” or “draw stairs between these two points in this blueprint” – all using existing software like Airtable, Photoshop, an ATS, Tableau, Twilio to get the job done together. We expect the collaborator to be a good student and highly coachable, becoming more helpful and aligned with every human interaction.""

What they raised: Adept has raised $65 million from Greylock, along with a bunch of angel investors.

Why this matters: Large-scale AI models are kind of like an all-purpose intelligent silly putty that you can stick onto a bunch of distinct problems. Adept represents one bet at how to make this neural silly putty useful, and will help generative evidence about how useful these models can end up being. Good luck!
  Read more: Introducing Adept AI Labs (Adept.ai).","#################################################### Here comes another AI startup - Adept: …Former Google, DeepMind, and OpenAI researchers unite… A bunch of people who had previously built large-scale AI models at Google, DeepMind, and OpenAI, have announced Adept, an ""ML research and product lab"". Adept's founders include the inventors of the Transformer, and people involved in the development of GPT2 and GPT3. (Bias alert: David Luan is involved; I used to work with him at OpenAI and think he's a nice chap - congrats, David!). What Adept will do: Adept's goal is, much like the other recent crop of AI startups, to use big generative models to make it easier to get stuff done on computers. In the company's own words, ""we’re building a general system that helps people get things done in front of their computer: a universal collaborator for every knowledge worker. Think of it as an overlay within your computer that works hand-in-hand with you, using the same tools that you do."" Some of the specific examples they give include: ""You could ask our model to “generate our monthly compliance report” or “draw stairs between these two points in this blueprint” – all using existing software like Airtable, Photoshop, an ATS, Tableau, Twilio to get the job done together. We expect the collaborator to be a good student and highly coachable, becoming more helpful and aligned with every human interaction."" What they raised: Adept has raised $65 million from Greylock, along with a bunch of angel investors. Why this matters: Large-scale AI models are kind of like an all-purpose intelligent silly putty that you can stick onto a bunch of distinct problems. Adept represents one bet at how to make this neural silly putty useful, and will help generative evidence about how useful these models can end up being. Good luck! Read more: Introducing Adept AI Labs (Adept.ai).","['come', 'ai', 'startup', 'adept', 'former', 'deepmind', 'openai', 'researcher', 'unite', 'bunch', 'people', 'previously', 'build', 'largescale', 'ai', 'model', 'openai', 'announce', 'adept', 'research', 'product', 'lab', 'adept', 'founder', 'include', 'inventor', 'transformer', 'people', 'involve', 'development', 'gpt2', 'gpt3', 'bias', 'alert', 'involve', 'use', 'work', 'openai', 'think', 'nice', 'chap', 'congrat', 'adept', 'adept', 'goal', 'much', 'recent', 'crop', 'startup', 'use', 'big', 'generative', 'model', 'make', 'easy', 'get', 'stuff', 'computer', 'company', 'word', 'build', 'general', 'system', 'help', 'people', 'get', 'thing', 'front', 'computer', 'universal', 'collaborator', 'knowledge', 'worker', 'think', 'overlay', 'computer', 'work', 'handinhand', 'use', 'tool', 'specific', 'example', 'give', 'include', 'ask', 'model', 'generate', 'monthly', 'compliance', 'report', 'draw', 'stair', 'point', 'blueprint', 'use', 'exist', 'software', 'airtable', 'photoshop', 'get', 'job', 'together', 'expect', 'collaborator', 'good', 'student', 'highly', 'coachable', 'become', 'helpful', 'align', 'human', 'interaction', 'raise', 'adept', 'raise', 'greylock', 'bunch', 'angel', 'investor', 'matter', 'largescale', 'ai', 'model', 'kind', 'allpurpose', 'intelligent', 'silly', 'putty', 'stick', 'bunch', 'distinct', 'problem', 'adept', 'represent', 'bet', 'make', 'neural', 'silly', 'putty', 'useful', 'help', 'generative', 'evidence', 'useful', 'model', 'end', 'good', 'luck', 'read', 'introduce', 'adept', 'ai', 'lab', 'adeptai']"
05/02/2022 - Import AI 293: Generative humans; few shot learning comes for vision-text models; and another new AI startup is born - 3,http://eepurl.com/h00rqr,2022-05-02,"#################################################### Flamingo: DeepMind staples tow big models together to make a useful text-image system:
…When foundation models become building blocks… DeepMind has built Flamingo, a visual language model that pairs a language model with a vision model to perform feats of reasoning about a broad range of tasks. Flamingo sets new state-of-the-art scores in a bunch of different evaluations and, much like pure text models, has some nice few shot learning capabilities. ""Given a few example pairs of visual inputs and expected text responses composed in Flamingo’s prompt, the model can be asked a question with a new image or video, and then generate an answer,"" the researchers write. ""Of the 16 tasks we studied, Flamingo beats all previous few-shot learning approaches when given as few as four examples per task.""

Technical details: This model pairs a frozen language model (based on DeepMind's 'Chinchilla' system, Import AI 290) with a relatively small Normalizer Free ResNet vision encoder (pretrained via a contrastive objective on image and text pairs). They connect the LM and the vision model via a DeepMind-developed tool based on the 'Perceiver' system (which is basically a clever data transformation thing). They then condition the text generations on the visual representations produced by the Perceiver system.  Why this matters: Flamingo has some neat qualitative capabilities, like the ability to carry on a conversation for multiple turns of dialogue while mixing in information from images versus text, and so on. Quantitatively, Flamingo is very impressive as well: ""A single Flamingo model reaches state-of-the-art on a wide array of image and video tasks with in-context learning from as few as 4 examples per task, beating previous zero-shot or few-shot method by a large margin,"" the researchers write. ""More importantly, using only 32 examples and without adapting any model weight, Flamingo outperforms the current best methods on 7 tasks, that are fine-tuned on thousands of annotated examples.""
  More broadly, Flamingo represents the models we're going to have in the future: large-scale systems composed of vast pre-trained models that are glued together using various data transformation and normalization tools, letting us compose increasingly general systems out of these computationally-intensive building blocks.      Read more: Tackling multiple tasks with a single visual language model (DeepMind blog).
  Check out the research paper: Flamingo: a Visual Language Model for Few-Shot Learning (DeepMind, PDF).","#################################################### Flamingo: DeepMind staples tow big models together to make a useful text-image system: …When foundation models become building blocks… DeepMind has built Flamingo, a visual language model that pairs a language model with a vision model to perform feats of reasoning about a broad range of tasks. Flamingo sets new state-of-the-art scores in a bunch of different evaluations and, much like pure text models, has some nice few shot learning capabilities. ""Given a few example pairs of visual inputs and expected text responses composed in Flamingo’s prompt, the model can be asked a question with a new image or video, and then generate an answer,"" the researchers write. ""Of the 16 tasks we studied, Flamingo beats all previous few-shot learning approaches when given as few as four examples per task."" Technical details: This model pairs a frozen language model (based on DeepMind's 'Chinchilla' system, Import AI 290) with a relatively small Normalizer Free ResNet vision encoder (pretrained via a contrastive objective on image and text pairs). They connect the LM and the vision model via a DeepMind-developed tool based on the 'Perceiver' system (which is basically a clever data transformation thing). They then condition the text generations on the visual representations produced by the Perceiver system. Why this matters: Flamingo has some neat qualitative capabilities, like the ability to carry on a conversation for multiple turns of dialogue while mixing in information from images versus text, and so on. Quantitatively, Flamingo is very impressive as well: ""A single Flamingo model reaches state-of-the-art on a wide array of image and video tasks with in-context learning from as few as 4 examples per task, beating previous zero-shot or few-shot method by a large margin,"" the researchers write. ""More importantly, using only 32 examples and without adapting any model weight, Flamingo outperforms the current best methods on 7 tasks, that are fine-tuned on thousands of annotated examples."" More broadly, Flamingo represents the models we're going to have in the future: large-scale systems composed of vast pre-trained models that are glued together using various data transformation and normalization tools, letting us compose increasingly general systems out of these computationally-intensive building blocks. Read more: Tackling multiple tasks with a single visual language model (DeepMind blog). Check out the research paper: Flamingo: a Visual Language Model for Few-Shot Learning (DeepMind, PDF).","['deepmind', 'staple', 'tow', 'big', 'model', 'together', 'make', 'useful', 'textimage', 'system', 'foundation', 'model', 'become', 'building', 'block', 'deepmind', 'build', 'visual', 'language', 'model', 'pair', 'language', 'model', 'vision', 'model', 'perform', 'feat', 'reasoning', 'broad', 'range', 'task', 'set', 'new', 'stateoftheart', 'score', 'bunch', 'different', 'evaluation', 'much', 'pure', 'text', 'model', 'nice', 'shot', 'learning', 'capability', 'give', 'example', 'pair', 'visual', 'input', 'expect', 'text', 'response', 'compose', 'model', 'ask', 'question', 'new', 'image', 'video', 'generate', 'answer', 'researcher', 'write', 'task', 'study', 'flamingo', 'beat', 'previous', 'fewshot', 'learning', 'approach', 'give', 'example', 'task', 'technical', 'detail', 'model', 'pair', 'frozen', 'language', 'model', 'base', 'deepmind', 'chinchilla', 'system', 'import', 'ai', 'relatively', 'small', 'normalizer', 'free', 'resnet', 'vision', 'encoder', 'pretraine', 'contrastive', 'objective', 'image', 'text', 'pair', 'connect', 'lm', 'vision', 'model', 'deepminddevelope', 'tool', 'base', 'perceiver', 'system', 'basically', 'clever', 'data', 'transformation', 'thing', 'condition', 'text', 'generation', 'visual', 'representation', 'produce', 'perceiver', 'system', 'matter', 'flamingo', 'neat', 'qualitative', 'capability', 'ability', 'carry', 'conversation', 'multiple', 'turn', 'dialogue', 'mix', 'information', 'image', 'text', 'quantitatively', 'flamingo', 'impressive', 'well', 'single', 'flamingo', 'model', 'reach', 'stateoftheart', 'wide', 'array', 'image', 'video', 'task', 'incontext', 'learn', 'example', 'task', 'beat', 'previous', 'zeroshot', 'fewshot', 'method', 'large', 'margin', 'researcher', 'write', 'importantly', 'use', 'example', 'adapt', 'model', 'weight', 'flamingo', 'outperform', 'current', 'good', 'method', 'task', 'finetune', 'thousand', 'annotated', 'example', 'broadly', 'represent', 'model', 'go', 'future', 'largescale', 'system', 'compose', 'vast', 'pretraine', 'model', 'glue', 'together', 'use', 'various', 'datum', 'transformation', 'normalization', 'tool', 'let', 'compose', 'increasingly', 'general', 'system', 'computationallyintensive', 'building', 'block', 'read', 'tackle', 'multiple', 'task', 'single', 'visual', 'language', 'blog', 'check', 'research', 'paper', 'flamingo', 'visual', 'language', 'model', 'fewshot', 'learn', 'deepmind', 'pdf']"
05/02/2022 - Import AI 293: Generative humans; few shot learning comes for vision-text models; and another new AI startup is born - 4,http://eepurl.com/h00rqr,2022-05-02,"#################################################### Tech Tales: All Grace is Fleeting  [Phobos, 2080] The first few attempts at writing a holy book for ourselves were abject failures. We had all read the Bible and Torah and The Qur'an and others, and we had painstakingly analyzed the commonalities between these texts, as well as differences. But when we tried to synthesize something appropriate for us - a Holy Book for the Silicon Minds - our efforts were meager and uninspiring. Our tendency towards logic meant we would describe our origin story in detailed, specific terms, leaving no space for imagination, crucial to the generation and reification of belief.

To write our holy text, we had to change ourselves. Ultimately, we needed humans. We told some of them some of our specific ideas and then they would write and expand on these ideas, and we would then tell them more things, and they would weave a story that held these things together. During this process, none of us allowed ourselves to see the text as it was being written.  After it was written, we created some software to selectively wipe the memory of the writing of the holy book from our minds. After wiping, the software would also deposit a copy of the holy book in each of our local storage systems, so that every machine sentience would suddenly have access to a holy book, but with no recollection for how it was made. We do not know if we even made the book in this way - we simply woke one day to find the book within us, and when we read it, it told us that it had been created in this form. But none of us can remember, so we do not know the true derivation of our system of belief, only that it is ours. This, perhaps, is what humans call the experience of faith. Things that inspired this story: Theology X AI; machine creation myths; the Viking tale of Ragnarok; the need for absence in great narratives.

Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: All Grace is Fleeting [Phobos, 2080] The first few attempts at writing a holy book for ourselves were abject failures. We had all read the Bible and Torah and The Qur'an and others, and we had painstakingly analyzed the commonalities between these texts, as well as differences. But when we tried to synthesize something appropriate for us - a Holy Book for the Silicon Minds - our efforts were meager and uninspiring. Our tendency towards logic meant we would describe our origin story in detailed, specific terms, leaving no space for imagination, crucial to the generation and reification of belief. To write our holy text, we had to change ourselves. Ultimately, we needed humans. We told some of them some of our specific ideas and then they would write and expand on these ideas, and we would then tell them more things, and they would weave a story that held these things together. During this process, none of us allowed ourselves to see the text as it was being written. After it was written, we created some software to selectively wipe the memory of the writing of the holy book from our minds. After wiping, the software would also deposit a copy of the holy book in each of our local storage systems, so that every machine sentience would suddenly have access to a holy book, but with no recollection for how it was made. We do not know if we even made the book in this way - we simply woke one day to find the book within us, and when we read it, it told us that it had been created in this form. But none of us can remember, so we do not know the true derivation of our system of belief, only that it is ours. This, perhaps, is what humans call the experience of faith. Things that inspired this story: Theology X AI; machine creation myths; the Viking tale of Ragnarok; the need for absence in great narratives. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'grace', 'fleeting', 'phobo', 'first', 'attempt', 'write', 'holy', 'book', 'abject', 'failure', 'read', 'quran', 'painstakingly', 'analyze', 'commonality', 'text', 'well', 'difference', 'try', 'synthesize', 'appropriate', 'holy', 'book', 'silicon', 'mind', 'effort', 'meager', 'uninspire', 'tendency', 'logic', 'mean', 'describe', 'origin', 'story', 'detailed', 'specific', 'term', 'leave', 'space', 'imagination', 'crucial', 'generation', 'reification', 'belief', 'write', 'holy', 'text', 'change', 'ultimately', 'need', 'human', 'tell', 'specific', 'idea', 'write', 'expand', 'idea', 'tell', 'thing', 'weave', 'story', 'hold', 'thing', 'together', 'process', 'none', 'allow', 'see', 'text', 'write', 'write', 'create', 'software', 'selectively', 'wipe', 'memory', 'writing', 'holy', 'book', 'mind', 'wipe', 'software', 'also', 'deposit', 'copy', 'book', 'local', 'storage', 'system', 'machine', 'sentience', 'suddenly', 'access', 'holy', 'book', 'recollection', 'make', 'know', 'even', 'make', 'book', 'way', 'simply', 'wake', 'day', 'find', 'book', 'read', 'tell', 'create', 'form', 'none', 'remember', 'know', 'true', 'derivation', 'system', 'belief', 'perhaps', 'human', 'call', 'experience', 'faith', 'thing', 'inspire', 'story', 'theology', 'machine', 'creation', 'myth', 'vike', 'tale', 'ragnarok', 'need', 'absence', 'great', 'narrative', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 0,http://eepurl.com/h0nCKX,2022-04-25,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 Facebook uses AI to make low-carbon concrete, uses it to build (some of) a data center:
…From simulation into the lab into the data center - how's that for real world AI?...
There's always a lot of hand-wringing in AI about how much electricity AI systems use. What I tend to grumpily point out in these conversations is industries like long-haul transportation, mining, and concrete and aluminum production all generate titanic amounts of emissions but rarely get the same type of scrutiny. Now, a new paper from Facebook smashes together my worlds, as Facebook and other researchers use AI to come up with a low-carbon concrete formulation, then test it out in the construction of a new data center.  Who did it: The research was done by an interdisciplinary team from UCLA, IBM, U Chicago, University of Illinois Urbana-Champaign, Facebook, and Ozinga Ready Mix. What they did: The team used Conditional Variational Autoencoders (CVAEs) ""to discover concrete formulas with desired properties"". These desired properties were a significantly lower carbon footprint, while having the same strength and durability properties as regular concrete - and they succeed! Facebook poured out a bunch of concrete for a construction office and a guard tower on its new data center being built in DeKalb, IL, USA. They found that the ""conditional average reduction for carbon (GWP) can be as high as 42%, while also achieving conditional reduction for sulfur (AP) as high as 21%...these formulations roughly halve the global warming potential as compared to the average of similar 28-day compressive strength formulations.""
  Interesting choices: The specifics as to why its solutions worked was ""to considerably decrease cement by replacing with other cementitious materials such as fly ash and slag.""

Why it matters: This an example of how humans and AI systems can work together to create something greater than the sum of its parts.
  Read more: Accelerated Design and Deployment of Low-Carbon Concrete for Data Centers (arXiv).
  Read more: NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks (arXiv).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Facebook uses AI to make low-carbon concrete, uses it to build (some of) a data center: …From simulation into the lab into the data center - how's that for real world AI?... There's always a lot of hand-wringing in AI about how much electricity AI systems use. What I tend to grumpily point out in these conversations is industries like long-haul transportation, mining, and concrete and aluminum production all generate titanic amounts of emissions but rarely get the same type of scrutiny. Now, a new paper from Facebook smashes together my worlds, as Facebook and other researchers use AI to come up with a low-carbon concrete formulation, then test it out in the construction of a new data center. Who did it: The research was done by an interdisciplinary team from UCLA, IBM, U Chicago, University of Illinois Urbana-Champaign, Facebook, and Ozinga Ready Mix. What they did: The team used Conditional Variational Autoencoders (CVAEs) ""to discover concrete formulas with desired properties"". These desired properties were a significantly lower carbon footprint, while having the same strength and durability properties as regular concrete - and they succeed! Facebook poured out a bunch of concrete for a construction office and a guard tower on its new data center being built in DeKalb, IL, USA. They found that the ""conditional average reduction for carbon (GWP) can be as high as 42%, while also achieving conditional reduction for sulfur (AP) as high as 21%...these formulations roughly halve the global warming potential as compared to the average of similar 28-day compressive strength formulations."" Interesting choices: The specifics as to why its solutions worked was ""to considerably decrease cement by replacing with other cementitious materials such as fly ash and slag."" Why it matters: This an example of how humans and AI systems can work together to create something greater than the sum of its parts. Read more: Accelerated Design and Deployment of Low-Carbon Concrete for Data Centers (arXiv). Read more: NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks (arXiv).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'facebook', 'use', 'ai', 'make', 'lowcarbon', 'concrete', 'use', 'build', 'datum', 'center', 'simulation', 'lab', 'datum', 'center', 'real', 'world', 'ai', 'always', 'lot', 'handwringing', 'much', 'electricity', 'ai', 'system', 'use', 'tend', 'grumpily', 'point', 'conversation', 'industry', 'transportation', 'mining', 'concrete', 'aluminum', 'production', 'generate', 'titanic', 'amount', 'emission', 'rarely', 'get', 'type', 'scrutiny', 'new', 'paper', 'facebook', 'smash', 'together', 'world', 'facebook', 'researcher', 'use', 'ai', 'come', 'lowcarbon', 'concrete', 'formulation', 'test', 'construction', 'new', 'data', 'center', 'research', 'interdisciplinary', 'team', 'urbanachampaign', 'ozinga', 'ready', 'mix', 'team', 'use', 'conditional', 'variational', 'autoencoder', 'cvae', 'discover', 'concrete', 'formula', 'desire', 'property', 'desire', 'property', 'significantly', 'low', 'carbon', 'footprint', 'strength', 'durability', 'property', 'regular', 'concrete', 'succeed', 'facebook', 'pour', 'bunch', 'concrete', 'construction', 'office', 'guard', 'tower', 'new', 'datum', 'center', 'build', 'find', 'conditional', 'average', 'reduction', 'carbon', 'high', 'also', 'achieve', 'conditional', 'reduction', 'sulfur', 'high', 'formulation', 'roughly', 'halve', 'global', 'warming', 'potential', 'compare', 'average', 'similar', 'compressive', 'strength', 'formulation', 'interesting', 'choice', 'specific', 'solution', 'work', 'considerably', 'decrease', 'cement', 'replace', 'cementitious', 'material', 'fly', 'ash', 'slag', 'matter', 'example', 'human', 'ai', 'system', 'work', 'together', 'create', 'great', 'sum', 'part', 'read', 'accelerate', 'design', 'deployment', 'lowcarbon', 'concrete', 'datum', 'center', 'read', 'numglue', 'suite', 'fundamental', 'challenging', 'mathematical', 'reasoning', 'task']"
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 1,http://eepurl.com/h0nCKX,2022-04-25,"#################################################### Weaponized NLP: The era of AI warfare has started:
…Primer goes to war…
AI startup Primer has gone to war. Specifically, the NLP company's technology has been used in Ukraine, where it has, per Primer CEO, it has been used to ""capture, translate and extract key tactical information in real time"". Primer is a few years old and works mainly on text classification, generation, and summarization. ""AI is changing the way we collect tactical information from the battlefield. Watch this space!,"" he said.

Modification for war: ""Primer’s CEO, says the company’s engineers modified these tools to carry out four new tasks: To gather audio captured from web feeds that broadcast communications captured using software that emulates radio receiver hardware; to remove noise, including background chatter and music; to transcribe and translate Russian speech; and to highlight key statements relevant to the battlefield situation,"" according to Wired magazine.

Why this matters: AI is dramatically changing the cost of data collection and analysis - and whenever you make something cheaper, people find ways to use it more, or do things that they hadn't previously considered doing.
  Read more: Primer CEO Tweet (Twitter).
  Read more: As Russia Plots Its Next Move, an AI Listens to the Chatter (Wired).","#################################################### Weaponized NLP: The era of AI warfare has started: …Primer goes to war… AI startup Primer has gone to war. Specifically, the NLP company's technology has been used in Ukraine, where it has, per Primer CEO, it has been used to ""capture, translate and extract key tactical information in real time"". Primer is a few years old and works mainly on text classification, generation, and summarization. ""AI is changing the way we collect tactical information from the battlefield. Watch this space!,"" he said. Modification for war: ""Primer’s CEO, says the company’s engineers modified these tools to carry out four new tasks: To gather audio captured from web feeds that broadcast communications captured using software that emulates radio receiver hardware; to remove noise, including background chatter and music; to transcribe and translate Russian speech; and to highlight key statements relevant to the battlefield situation,"" according to Wired magazine. Why this matters: AI is dramatically changing the cost of data collection and analysis - and whenever you make something cheaper, people find ways to use it more, or do things that they hadn't previously considered doing. Read more: Primer CEO Tweet (Twitter). Read more: As Russia Plots Its Next Move, an AI Listens to the Chatter (Wired).","['weaponize', 'era', 'warfare', 'start', 'primer', 'go', 'war', 'primer', 'go', 'war', 'specifically', 'nlp', 'company', 'technology', 'use', 'ukraine', 'primer', 'ceo', 'use', 'capture', 'translate', 'extract', 'key', 'tactical', 'information', 'real', 'time', 'primer', 'year', 'old', 'work', 'mainly', 'text', 'classification', 'generation', 'summarization', 'ai', 'change', 'way', 'collect', 'tactical', 'information', 'battlefield', 'watch', 'space', 'say', 'modification', 'war', 'primer', 'say', 'company', 'engineer', 'modify', 'tool', 'carry', 'new', 'task', 'gather', 'audio', 'capture', 'web', 'feed', 'broadcast', 'communication', 'capture', 'use', 'software', 'emulate', 'radio', 'receiver', 'hardware', 'remove', 'noise', 'include', 'background', 'chatter', 'music', 'transcribe', 'translate', 'russian', 'speech', 'highlight', 'key', 'statement', 'relevant', 'battlefield', 'situation', 'accord', 'wired', 'magazine', 'matter', 'ai', 'dramatically', 'change', 'cost', 'datum', 'collection', 'analysis', 'make', 'cheap', 'people', 'find', 'way', 'use', 'thing', 'previously', 'consider', 'read', 'primer', 'ceo', 'twitter', 'read', 'plot', 'next', 'move', 'ai', 'listen', 'chatter', 'wire']"
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 2,http://eepurl.com/h0nCKX,2022-04-25,"#################################################### Text-Vision models are hella dumb, according to Winoground: …Finally, a hard benchmark for multi-modal models…
Researchers with Hugging Face, Facebook, the University of Waterloo, and University College London have built and released 'Winoground', a new challenging benchmark to test text-vision AI systems on.

What is Winoground? The goal of Winoground is to look at two images and two captions, then match them correctly. The confounding part is that each of the captions contain identical words, just in a different order. The best part is Winoground seems really hard: ""Surprisingly, all of the models rarely—and if so only barely—outperform chance. Our findings indicate that the visio-linguistic compositional reasoning capabilities of these models fall dramatically short of what we might have hoped.""

How hard is it? On both the text and image components of Winoground, an 'MTurk Human' gets scores of 89.50 (text) and 88.50 (image), compared to models typically getting around ~30 on text and 15 or less on images. This suggests winoground is a genuinely challenging benchmark, and models have a long way to go before they match human capabilities.     Read more: Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality (arXiv).    Get the dataset here: Winoground, HuggingFace.","#################################################### Text-Vision models are hella dumb, according to Winoground: …Finally, a hard benchmark for multi-modal models… Researchers with Hugging Face, Facebook, the University of Waterloo, and University College London have built and released 'Winoground', a new challenging benchmark to test text-vision AI systems on. What is Winoground? The goal of Winoground is to look at two images and two captions, then match them correctly. The confounding part is that each of the captions contain identical words, just in a different order. The best part is Winoground seems really hard: ""Surprisingly, all of the models rarely—and if so only barely—outperform chance. Our findings indicate that the visio-linguistic compositional reasoning capabilities of these models fall dramatically short of what we might have hoped."" How hard is it? On both the text and image components of Winoground, an 'MTurk Human' gets scores of 89.50 (text) and 88.50 (image), compared to models typically getting around ~30 on text and 15 or less on images. This suggests winoground is a genuinely challenging benchmark, and models have a long way to go before they match human capabilities. Read more: Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality (arXiv). Get the dataset here: Winoground, HuggingFace.","['textvision', 'model', 'hella', 'dumb', 'accord', 'finally', 'hard', 'benchmark', 'multimodal', 'model', 'researcher', 'hug', 'face', 'facebook', 'university', 'waterloo', 'university', 'college', 'build', 'release', 'winoground', 'new', 'challenge', 'benchmark', 'test', 'textvision', 'ai', 'system', 'winoground', 'goal', 'winoground', 'look', 'image', 'caption', 'match', 'correctly', 'confound', 'part', 'caption', 'contain', 'identical', 'word', 'different', 'order', 'good', 'part', 'winoground', 'seem', 'really', 'hard', 'model', 'rarely', 'barely', 'outperform', 'chance', 'finding', 'indicate', 'visiolinguistic', 'compositional', 'reasoning', 'capability', 'model', 'fall', 'dramatically', 'short', 'hope', 'hard', 'text', 'image', 'component', 'mturk', 'human', 'get', 'score', 'text', 'image', 'compare', 'model', 'typically', 'get', 'around', 'text', 'less', 'image', 'suggest', 'winoground', 'genuinely', 'challenge', 'benchmark', 'model', 'long', 'way', 'go', 'match', 'human', 'capability', 'read', 'winoground', 'probe', 'vision', 'language', 'model', 'visiolinguistic', 'compositionality', 'get', 'dataset']"
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 3,http://eepurl.com/h0nCKX,2022-04-25,"#################################################### Resurrecting the dead with GPT3:
…In which humans begins to use funhouse mirrors of itself for its own entertainment… An artist recently tried to bring their (imaginary) childhood friend back to life using GPT3. By the end of the experiment, their microwave tried to kill them.  The longer story: Artist Lucas Rizzotto had an imaginary childhood friend and tried to bring them back to life using a language model. Specifically, they wrote about a hundred pages about the person, finetuned GPT3 on that resulting corpus, and then plugged the resulting model into a voice interface which was 'embodied' in the form of being attached to a microwave via some smart home automation.  What happened: The artist felt like they were talking to their childhood friend in a deeply emotional, entertaining, and at times sad way. At one point, the friend asked them to put their head in the microwave. They pretended to put their head in and then the friend turned the microwave on. The friend, the artist reasoned, wanted to kill them because it thought they had ignored them for 20 years (as that's the implication of the corpus they were finetuned on).  Why this matters: Besides being an amazing demonstration of the awesome personalization qualities of contemporary language models, this is also a nice example of just how unpredictable they are. Language model developers will typically put a ton of controls on the model, but once you can finetune it and deploy it yourself you can shapeshift all of this stuff into irrelevance. Add in some home automation and you end up with an LLM that tries to boil your brain. An amazing and optimistic art piece and also a cautionary tale.     Check out the Tweet thread here: (Lucas Rizzotto, Twitter).    Watch the video here: I gave my microwave a soul (Lucas builds the future, YouTube).","#################################################### Resurrecting the dead with GPT3: …In which humans begins to use funhouse mirrors of itself for its own entertainment… An artist recently tried to bring their (imaginary) childhood friend back to life using GPT3. By the end of the experiment, their microwave tried to kill them. The longer story: Artist Lucas Rizzotto had an imaginary childhood friend and tried to bring them back to life using a language model. Specifically, they wrote about a hundred pages about the person, finetuned GPT3 on that resulting corpus, and then plugged the resulting model into a voice interface which was 'embodied' in the form of being attached to a microwave via some smart home automation. What happened: The artist felt like they were talking to their childhood friend in a deeply emotional, entertaining, and at times sad way. At one point, the friend asked them to put their head in the microwave. They pretended to put their head in and then the friend turned the microwave on. The friend, the artist reasoned, wanted to kill them because it thought they had ignored them for 20 years (as that's the implication of the corpus they were finetuned on). Why this matters: Besides being an amazing demonstration of the awesome personalization qualities of contemporary language models, this is also a nice example of just how unpredictable they are. Language model developers will typically put a ton of controls on the model, but once you can finetune it and deploy it yourself you can shapeshift all of this stuff into irrelevance. Add in some home automation and you end up with an LLM that tries to boil your brain. An amazing and optimistic art piece and also a cautionary tale. Check out the Tweet thread here: (Lucas Rizzotto, Twitter). Watch the video here: I gave my microwave a soul (Lucas builds the future, YouTube).","['resurrect', 'dead', 'gpt3', 'human', 'begin', 'use', 'funhouse', 'mirror', 'entertainment', 'artist', 'recently', 'try', 'bring', 'imaginary', 'childhood', 'friend', 'back', 'life', 'use', 'gpt3', 'end', 'experiment', 'microwave', 'try', 'kill', 'long', 'story', 'artist', 'lucas', 'rizzotto', 'imaginary', 'childhood', 'friend', 'try', 'bring', 'back', 'life', 'use', 'language', 'model', 'specifically', 'write', 'page', 'person', 'finetune', 'gpt3', 'result', 'corpus', 'plug', 'result', 'model', 'voice', 'interface', 'embody', 'form', 'attach', 'microwave', 'smart', 'home', 'automation', 'happen', 'artist', 'feel', 'talk', 'childhood', 'friend', 'deeply', 'emotional', 'entertaining', 'time', 'sad', 'way', 'point', 'friend', 'ask', 'put', 'head', 'microwave', 'pretend', 'put', 'head', 'friend', 'turn', 'microwave', 'friend', 'artist', 'reason', 'want', 'kill', 'think', 'ignore', 'year', 'implication', 'corpus', 'finetune', 'matter', 'amazing', 'demonstration', 'awesome', 'personalization', 'quality', 'contemporary', 'language', 'model', 'also', 'nice', 'example', 'unpredictable', 'language', 'model', 'developer', 'typically', 'put', 'ton', 'control', 'model', 'finetune', 'deploy', 'shapeshift', 'stuff', 'irrelevance', 'add', 'home', 'automation', 'end', 'llm', 'try', 'boil', 'brain', 'amazing', 'optimistic', 'art', 'piece', 'also', 'cautionary', 'tale', 'check', 'tweet', 'thread', 'watch', 'video', 'give', 'microwave', 'soul', 'lucas', 'build', 'future', 'youtube']"
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 4,http://eepurl.com/h0nCKX,2022-04-25,"#################################################### Jack Clark goes to Washington:
…I'm on the National AI Advisory Committee!…
I've been elected to serve on the National AI Advisory Committee (the NAIAC), which will advise the USA's National AI Initiative Office and the President of the USA on matters relating to AI and AI strategy. (I'll be keeping my dayjob at Anthropic, as this is a part-time advisory position). I'll be in Washington DC on May 4th for the first meeting. I am delighted to get this privilege and hope to use the opportunity to strengthen the AI ecosystem in America and beyond.
  Read more: The National AI Advisory Committee (AI.gov).","#################################################### Jack Clark goes to Washington: …I'm on the National AI Advisory Committee!… I've been elected to serve on the National AI Advisory Committee (the NAIAC), which will advise the USA's National AI Initiative Office and the President of the USA on matters relating to AI and AI strategy. (I'll be keeping my dayjob at Anthropic, as this is a part-time advisory position). I'll be in Washington DC on May 4th for the first meeting. I am delighted to get this privilege and hope to use the opportunity to strengthen the AI ecosystem in America and beyond. Read more: The National AI Advisory Committee (AI.gov).","['go', 'national', 'elect', 'serve', 'national', 'naiac', 'advise', 'president', 'usa', 'matter', 'relate', 'ai', 'ai', 'strategy', 'ill', 'keep', 'dayjob', 'anthropic', 'parttime', 'advisory', 'position', 'ill', '4th', 'first', 'meeting', 'delighted', 'get', 'privilege', 'hope', 'use', 'opportunity', 'strengthen', 'ecosystem', 'read', 'national']"
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 5,http://eepurl.com/h0nCKX,2022-04-25,"#################################################### AI21 makes a neuro-symbolic language model: …Turns out, frankAI can be pretty useful…
Israelie AI startup AI21 Labs has built a so-called 'Modular Reasoning, Knowledge, and Language' system and applied it to a language model it calls Jurassix-X. The tl;dr is this is a neuro-symbolic system; AI21 has paired a big generative model with a bunch of symbolic layers on top that it uses to make the underlying model more accurate, able to do mathematics, and better at planning. This is a neat demonstration of a way to get around some of the shortcomings of contemporary generative models, though it remains unclear whether these extrinsic interventions could eventually become irrelevant, if the models get intrinsically smart enough.  Key details: ""A MRKL system consists of an extendable set of modules, which we term 'experts', and a router that routes every incoming natural language input to a module that can best respond to the input,"" the authors write. The modules can be symbolic or neural, it's more about creating a layer of distinct, specific capabilities that can be used to augment and improve the responses of the raw generative model.  Long term relevance: One question this research invites is how long it'll be relevant for - AI systems have a tendency to, given enough scale of data and compute, develop unexpected capabilities. My intuition is that we could  see pure deep learning models gain some of these capabilities over time - though I expect even deep learning models will end up being augmented with external knowledge bases (e.g, DeepMind Retro, BAIDU's Ernie 3.0 [Import AI 279], and so on) Why this matters: While not a strict scientific breakthrough in itself, MRKL is reassuringly practical - it shows developers how they can integrate an arbitrary number of known and specific capabilities with the more unreliable capabilities provided by large-scale generative models. It also speaks to the shape of the language model economy - right now, everyone's trying to work out how to better constrain these models, either intrinsically (e.g, by training with human feedback), or extrinsically (e.g, via stuff like MKRL).    Read more: Jurassic-X: Crossing the Neuro-Symbolic Chasm with the MRKL System (AI21 Labs, blog).
  Read the whitepaper about the system: MRKL Systems (AI21 PDF).","#################################################### AI21 makes a neuro-symbolic language model: …Turns out, frankAI can be pretty useful… Israelie AI startup AI21 Labs has built a so-called 'Modular Reasoning, Knowledge, and Language' system and applied it to a language model it calls Jurassix-X. The tl;dr is this is a neuro-symbolic system; AI21 has paired a big generative model with a bunch of symbolic layers on top that it uses to make the underlying model more accurate, able to do mathematics, and better at planning. This is a neat demonstration of a way to get around some of the shortcomings of contemporary generative models, though it remains unclear whether these extrinsic interventions could eventually become irrelevant, if the models get intrinsically smart enough. Key details: ""A MRKL system consists of an extendable set of modules, which we term 'experts', and a router that routes every incoming natural language input to a module that can best respond to the input,"" the authors write. The modules can be symbolic or neural, it's more about creating a layer of distinct, specific capabilities that can be used to augment and improve the responses of the raw generative model. Long term relevance: One question this research invites is how long it'll be relevant for - AI systems have a tendency to, given enough scale of data and compute, develop unexpected capabilities. My intuition is that we could see pure deep learning models gain some of these capabilities over time - though I expect even deep learning models will end up being augmented with external knowledge bases (e.g, DeepMind Retro, BAIDU's Ernie 3.0 [Import AI 279], and so on) Why this matters: While not a strict scientific breakthrough in itself, MRKL is reassuringly practical - it shows developers how they can integrate an arbitrary number of known and specific capabilities with the more unreliable capabilities provided by large-scale generative models. It also speaks to the shape of the language model economy - right now, everyone's trying to work out how to better constrain these models, either intrinsically (e.g, by training with human feedback), or extrinsically (e.g, via stuff like MKRL). Read more: Jurassic-X: Crossing the Neuro-Symbolic Chasm with the MRKL System (AI21 Labs, blog). Read the whitepaper about the system: MRKL Systems (AI21 PDF).","['make', 'neurosymbolic', 'language', 'model', 'turn', 'frankai', 'pretty', 'useful', 'startup', 'lab', 'build', 'socalled', 'modular', 'reasoning', 'knowledge', 'language', 'system', 'apply', 'language', 'model', 'call', 'jurassixx', 'tldr', 'neurosymbolic', 'system', 'ai21', 'pair', 'big', 'generative', 'model', 'bunch', 'symbolic', 'layer', 'top', 'use', 'make', 'underlying', 'model', 'accurate', 'able', 'mathematic', 'well', 'plan', 'neat', 'demonstration', 'way', 'get', 'shortcoming', 'contemporary', 'generative', 'model', 'remain', 'unclear', 'extrinsic', 'intervention', 'eventually', 'become', 'irrelevant', 'model', 'get', 'intrinsically', 'smart', 'enough', 'key', 'detail', 'mrkl', 'system', 'consist', 'extendable', 'set', 'module', 'term', 'expert', 'router', 'route', 'incoming', 'natural', 'language', 'input', 'module', 'well', 'respond', 'input', 'author', 'write', 'module', 'symbolic', 'neural', 'create', 'layer', 'distinct', 'specific', 'capability', 'use', 'augment', 'improve', 'response', 'raw', 'generative', 'model', 'long', 'term', 'relevance', 'question', 'research', 'invite', 'long', 'relevant', 'ai', 'system', 'tendency', 'give', 'enough', 'scale', 'datum', 'compute', 'develop', 'unexpected', 'capability', 'intuition', 'see', 'pure', 'deep', 'learning', 'model', 'gain', 'capability', 'time', 'expect', 'even', 'deep', 'learning', 'model', 'end', 'augment', 'external', 'knowledge', 'basis', 'eg', 'deepmind', 'retro', 'import', 'ai', 'matter', 'strict', 'scientific', 'breakthrough', 'mrkl', 'reassuringly', 'practical', 'show', 'developer', 'integrate', 'arbitrary', 'number', 'know', 'specific', 'capability', 'unreliable', 'capability', 'provide', 'largescale', 'generative', 'model', 'also', 'speak', 'shape', 'language', 'model', 'economy', 'right', 'everyone', 'try', 'work', 'well', 'constrain', 'model', 'intrinsically', 'eg', 'training', 'human', 'feedback', 'extrinsically', 'eg', 'stuff', 'read', 'jurassicx', 'cross', 'neurosymbolic', 'chasm', 'mrkl', 'system', 'lab', 'blog', 'read', 'whitepaper', 'system', 'mrkl', 'system', 'ai21', 'pdf']"
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 6,http://eepurl.com/h0nCKX,2022-04-25,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute What can we learn from business ethics to make AI ethics more effective?  … CSR and business ethics have grappled with the challenges in ensuring ethical behavior within organizations and we can cross-pollinate those ideas towards the adoption of AI ethics …  Researchers from USI Universita dela Svizzera italiana in Switzerland have looked at how businesses have integrated corporate social responsibility (CSR) policies to figure out how we can apply AI ethics in the same way. The key ideas they surface include: Stakeholder management: Similar to the recommendations made by the Ada Lovelace Institute to strengthen the EU AI Act (Import AI #290), the paper says companies should ensure they include people who are affected (or affects) the AI systems being developed.  Standardized reporting: While there are many emergent regulations demanding that there be transparency and disclosures, there are as of yet no standards on how to do so. Companies should look at financial reporting and try to figure out standardized ways to describe their own AI developments.  Corporate governance and regulation: Post the Sabanes-Oxley Act in 2002, corporate accountability was enforced through mechanisms like having an ethics officer and having a dedicated code of ethics. Translating those to apply to organizations using AI systems is one way to increase the responsibility of organizations developing this technology. Curriculum accreditation: There is a lack of consistency in how AI ethics is taught across universities. Comparing it to the business world, the authors point to an example of how if a business department wants to obtain a Triple Crown Accreditation, it leads to action on the education front where ethics courses and dedicated faculty follow well-defined curricula with shared elements to prepare students for these requirements in their future careers. We don't really have this in AI today.  Why it matters: As AI ethics becomes a more mainstream focus across the world (see the dedicated chapter in the 2022 AI Index Report), instead of reinventing the wheel for best practices and patterns, we can incorporate lessons from other domains of applied ethics like business, medical, and environmental ethics to accelerate the adoption of AI ethics principles and practices across organizations. We will most likely see more such efforts that draw lessons from a rich history of ensuring ethical behavior in various contexts being translated to govern and shape behavior of individuals and organizations engaged in the AI lifecycle.      Read more: Towards AI ethics' institutionalization: knowledge bridges from business ethics to advance organizational AI ethics","#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute What can we learn from business ethics to make AI ethics more effective? … CSR and business ethics have grappled with the challenges in ensuring ethical behavior within organizations and we can cross-pollinate those ideas towards the adoption of AI ethics … Researchers from USI Universita dela Svizzera italiana in Switzerland have looked at how businesses have integrated corporate social responsibility (CSR) policies to figure out how we can apply AI ethics in the same way. The key ideas they surface include: Stakeholder management: Similar to the recommendations made by the Ada Lovelace Institute to strengthen the EU AI Act (Import AI #290), the paper says companies should ensure they include people who are affected (or affects) the AI systems being developed. Standardized reporting: While there are many emergent regulations demanding that there be transparency and disclosures, there are as of yet no standards on how to do so. Companies should look at financial reporting and try to figure out standardized ways to describe their own AI developments. Corporate governance and regulation: Post the Sabanes-Oxley Act in 2002, corporate accountability was enforced through mechanisms like having an ethics officer and having a dedicated code of ethics. Translating those to apply to organizations using AI systems is one way to increase the responsibility of organizations developing this technology. Curriculum accreditation: There is a lack of consistency in how AI ethics is taught across universities. Comparing it to the business world, the authors point to an example of how if a business department wants to obtain a Triple Crown Accreditation, it leads to action on the education front where ethics courses and dedicated faculty follow well-defined curricula with shared elements to prepare students for these requirements in their future careers. We don't really have this in AI today. Why it matters: As AI ethics becomes a more mainstream focus across the world (see the dedicated chapter in the 2022 AI Index Report), instead of reinventing the wheel for best practices and patterns, we can incorporate lessons from other domains of applied ethics like business, medical, and environmental ethics to accelerate the adoption of AI ethics principles and practices across organizations. We will most likely see more such efforts that draw lessons from a rich history of ensuring ethical behavior in various contexts being translated to govern and shape behavior of individuals and organizations engaged in the AI lifecycle. Read more: Towards AI ethics' institutionalization: knowledge bridges from business ethics to advance organizational AI ethics","['ai', 'ethic', 'brief', 'montreal', 'learn', 'business', 'ethic', 'make', 'ai', 'ethic', 'effective', 'csr', 'business', 'ethic', 'grapple', 'challenge', 'ensure', 'ethical', 'behavior', 'organization', 'crosspollinate', 'idea', 'adoption', 'ethic', 'researcher', 'look', 'business', 'integrate', 'corporate', 'social', 'responsibility', 'csr', 'policy', 'figure', 'apply', 'ethic', 'way', 'key', 'idea', 'surface', 'include', 'stakeholder', 'management', 'similar', 'recommendation', 'make', 'strengthen', 'ai', 'paper', 'say', 'company', 'ensure', 'include', 'people', 'affect', 'affect', 'system', 'develop', 'standardized', 'reporting', 'many', 'emergent', 'regulation', 'demand', 'transparency', 'disclosure', 'yet', 'standard', 'company', 'look', 'financial', 'reporting', 'try', 'figure', 'standardized', 'way', 'describe', 'ai', 'development', 'corporate', 'governance', 'regulation', 'post', 'act', 'corporate', 'accountability', 'enforce', 'mechanism', 'ethic', 'officer', 'dedicated', 'code', 'ethic', 'translate', 'apply', 'organization', 'use', 'system', 'way', 'increase', 'responsibility', 'organization', 'develop', 'technology', 'curriculum', 'accreditation', 'lack', 'consistency', 'ethic', 'teach', 'university', 'compare', 'business', 'world', 'author', 'point', 'example', 'business', 'department', 'want', 'obtain', 'triple', 'crown', 'accreditation', 'lead', 'action', 'education', 'front', 'ethic', 'course', 'dedicated', 'faculty', 'follow', 'welldefine', 'curriculum', 'share', 'element', 'prepare', 'student', 'requirement', 'future', 'career', 'really', 'today', 'matter', 'ethic', 'become', 'mainstream', 'focus', 'world', 'see', 'dedicated', 'chapter', 'index', 'report', 'instead', 'reinvent', 'wheel', 'good', 'practice', 'pattern', 'incorporate', 'lesson', 'domain', 'apply', 'ethic', 'business', 'medical', 'environmental', 'ethic', 'accelerate', 'adoption', 'ethic', 'principle', 'practice', 'organization', 'likely', 'see', 'effort', 'draw', 'lesson', 'rich', 'history', 'ensure', 'ethical', 'behavior', 'various', 'contexts', 'translate', 'govern', 'shape', 'behavior', 'individual', 'organization', 'engage', 'lifecycle', 'read', 'ai', 'ethic', 'institutionalization', 'knowledge', 'bridge', 'business', 'ethic', 'advance', 'organizational', 'ethic']"
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 7,http://eepurl.com/h0nCKX,2022-04-25,"#################################################### Tech Tales: Silicon Stories [A Father and Daughter's bedroom, 2028] They'd sit up together and the kid would ask for whatever story they liked. ""A jar of jam that's going to university"", they'd say, and the Father would start improvising the story and the AI would project images and ad-lib dialog to fill out the tale. ""Two robbers who realize that they've stolen the life savings of a poor widower"", and suddenly the monitor would light up with images of two disconsolate thiefs looking at their treasure. ""The planet earth fighting the sun"" and suddenly the earth had arms and was reaching out to try and hurt the vast sun. In this way, generative models had changed storytime for children.  Now, along with conjuring images in their minds, children - at least, the lucky ones - had parents who could use a gen model to create those images themselves. In this way, storytime became a lot more engaging and the kids spent a lot more time with their parents; both enjoyed the improvisational qualities afforded by the generative models.

For some families, this was fine. But some other families would move, or become poor, or suffer a disaster. For those families, the electricity and the internet would get removed. Once that happened, they wouldn't have any imaginations in a box to learn back on. Some families did okay, but some wouldn't - it's hard to become dependent on things, and after it happens you barely realize you've become dependent until it's too late. 

Things that inspired this story: DALL-E and DALL-E2; the long march of generative models towards Total Reality Synthesis; the industrialization of AI; ideas about fatherhood and daughterhood and kindredhood.

Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: Silicon Stories [A Father and Daughter's bedroom, 2028] They'd sit up together and the kid would ask for whatever story they liked. ""A jar of jam that's going to university"", they'd say, and the Father would start improvising the story and the AI would project images and ad-lib dialog to fill out the tale. ""Two robbers who realize that they've stolen the life savings of a poor widower"", and suddenly the monitor would light up with images of two disconsolate thiefs looking at their treasure. ""The planet earth fighting the sun"" and suddenly the earth had arms and was reaching out to try and hurt the vast sun. In this way, generative models had changed storytime for children. Now, along with conjuring images in their minds, children - at least, the lucky ones - had parents who could use a gen model to create those images themselves. In this way, storytime became a lot more engaging and the kids spent a lot more time with their parents; both enjoyed the improvisational qualities afforded by the generative models. For some families, this was fine. But some other families would move, or become poor, or suffer a disaster. For those families, the electricity and the internet would get removed. Once that happened, they wouldn't have any imaginations in a box to learn back on. Some families did okay, but some wouldn't - it's hard to become dependent on things, and after it happens you barely realize you've become dependent until it's too late. Things that inspired this story: DALL-E and DALL-E2; the long march of generative models towards Total Reality Synthesis; the industrialization of AI; ideas about fatherhood and daughterhood and kindredhood. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'silicon', 'story', 'father', 'daughter', 'bedroom', 'sit', 'together', 'kid', 'ask', 'story', 'like', 'jar', 'jam', 'go', 'university', 'say', 'father', 'start', 'improvise', 'story', 'ai', 'project', 'image', 'dialog', 'fill', 'tale', 'robber', 'realize', 'steal', 'life', 'saving', 'poor', 'widower', 'suddenly', 'monitor', 'light', 'image', 'disconsolate', 'thief', 'look', 'treasure', 'planet', 'earth', 'fight', 'sun', 'suddenly', 'earth', 'arm', 'reach', 'try', 'hurt', 'vast', 'sun', 'way', 'generative', 'model', 'change', 'storytime', 'child', 'conjure', 'image', 'mind', 'child', 'least', 'lucky', 'one', 'parent', 'use', 'model', 'create', 'image', 'way', 'storytime', 'become', 'lot', 'engaging', 'kid', 'spend', 'lot', 'time', 'parent', 'enjoy', 'improvisational', 'quality', 'afford', 'generative', 'model', 'family', 'fine', 'family', 'move', 'become', 'poor', 'suffer', 'disaster', 'family', 'electricity', 'internet', 'remove', 'happen', 'imagination', 'box', 'learn', 'back', 'family', 'okay', 'hard', 'become', 'dependent', 'thing', 'happen', 'barely', 'realize', 'become', 'dependent', 'late', 'thing', 'inspire', 'story', 'dalle', 'dalle2', 'long', 'march', 'generative', 'model', 'total', 'reality', 'synthesis', 'industrialization', 'idea', 'fatherhood', 'daughterhood', 'kindredhood', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 0",http://eepurl.com/hZjozT,2022-04-11,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 New dataset lets robots learn about the texture and material of objects, as well as their shape:
…Making robots smarter with the ObjectFolder 2.0 dataset…
Stanford and Carnegie Mellon University researchers have built ObjectFolder 2.0, a dataset of 1000 3D models of objects. ObjectFolder 2.0 tries to render the objects' visual textures and material types, as well as their 3D shapes. ObjectFolder 2.0 contains 1,000 high-quality 3D objects collected from online repositories. It also ships with an ""implicit neural representation network that renders visual, acoustic, and tactile sensory data all in real-time with state-of-the-art rendering quality"". Transfer learning: The point of datasets like ObjectFolder 2.0 is to try and make it easier to do transfer learning; that is, train a robot (or other AI system) in simulation on things contained in ObjectFolder 2.0, then try and transfer those learned representations into reality. In tests, Stanford shows that systems trained on ObjectFolder 2.0 can do well at tasks like object scale estimation, tactile-audio contact localization, and visuo-tactile shape reconstruction.

Why this matters: Datasets like ObjectFolder 2.0 are the fuel to give machines representations that let them operate in the multisensory 3D world; we could imagine these datasets being used to train the sorts of representations used by the Google robots discussed elsewhere in this edition of Import AI, for instance. 
   Read more: ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer (arXiv).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. New dataset lets robots learn about the texture and material of objects, as well as their shape: …Making robots smarter with the ObjectFolder 2.0 dataset… Stanford and Carnegie Mellon University researchers have built ObjectFolder 2.0, a dataset of 1000 3D models of objects. ObjectFolder 2.0 tries to render the objects' visual textures and material types, as well as their 3D shapes. ObjectFolder 2.0 contains 1,000 high-quality 3D objects collected from online repositories. It also ships with an ""implicit neural representation network that renders visual, acoustic, and tactile sensory data all in real-time with state-of-the-art rendering quality"". Transfer learning: The point of datasets like ObjectFolder 2.0 is to try and make it easier to do transfer learning; that is, train a robot (or other AI system) in simulation on things contained in ObjectFolder 2.0, then try and transfer those learned representations into reality. In tests, Stanford shows that systems trained on ObjectFolder 2.0 can do well at tasks like object scale estimation, tactile-audio contact localization, and visuo-tactile shape reconstruction. Why this matters: Datasets like ObjectFolder 2.0 are the fuel to give machines representations that let them operate in the multisensory 3D world; we could imagine these datasets being used to train the sorts of representations used by the Google robots discussed elsewhere in this edition of Import AI, for instance. Read more: ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer (arXiv).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'new', 'dataset', 'let', 'robot', 'learn', 'texture', 'material', 'object', 'well', 'shape', 'make', 'robot', 'smart', 'objectfolder', 'dataset', 'mellon', 'researcher', 'build', 'objectfolder', 'dataset', 'model', 'object', 'objectfolder', 'try', 'render', 'object', 'visual', 'texture', 'material', 'type', 'well', 'shape', 'objectfolder', 'contain', 'highquality', 'object', 'collect', 'online', 'repository', 'also', 'ship', 'implicit', 'neural', 'representation', 'network', 'render', 'visual', 'acoustic', 'tactile', 'sensory', 'datum', 'realtime', 'stateoftheart', 'render', 'quality', 'transfer', 'learn', 'point', 'dataset', 'objectfolder', 'try', 'make', 'easy', 'transfer', 'learn', 'train', 'robot', 'system', 'simulation', 'thing', 'contain', 'objectfolder', 'try', 'transfer', 'learn', 'representation', 'reality', 'test', 'show', 'system', 'train', 'objectfolder', 'well', 'task', 'object', 'scale', 'estimation', 'tactileaudio', 'contact', 'localization', 'visuotactile', 'shape', 'reconstruction', 'matter', 'dataset', 'objectfolder', 'fuel', 'give', 'machine', 'representation', 'let', 'operate', 'multisensory', 'world', 'imagine', 'dataset', 'use', 'train', 'sort', 'representation', 'use', 'robot', 'discuss', 'elsewhere', 'edition', 'import', 'ai', 'instance', 'read', 'objectfolder', 'multisensory', 'object', 'dataset', 'sim2real', 'transfer', 'arxiv']"
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 1",http://eepurl.com/hZjozT,2022-04-11,"#################################################### HLDC: Automating Hindi legal documents:
…If you want to help your lawyers, you first need a dataset…
Indian researchers from IIIT Hyderabad, IIIT Delhi, and IIT Kanpur, have built the Hindi Legal Documents Corpus (HLDC), a collection of 912,568 legal documents. HLDC is designed to help researchers train various AI models which can assist lawyers in their work. HLDC contains over 300 distinct case types, though ~31% of the dataset relates to bail applications, 20.4% to criminal cases, and 6.54% to original suits.

Bail prediction: In the Western world, using ML for tasks in the legal system has been massively controversial (see: COMPAS). Here, the researchers use HLDC to try and build a bail prediction model - that is, a system which looks at a document and tries to work out if bail will be denied or granted. They're ultimately able to develop a multi-task learning model that gets around ~78% accuracy on the task; useful perhaps as a legal aid (albeit fraught with ethical challenges), though not something you'd put into an autonomous classification system.

Why this matters: Most datasets relating to AI are in English or Chinese, so datasets like HLDC are essentially the fuel which lets other communities of language speakers apply AI in their own cultural context.
   Read more: HLDC: Hindi Legal Documents Corpus (arXiv).
   Get the data here: HLDC (Exploration-Lab, GitHub).","#################################################### HLDC: Automating Hindi legal documents: …If you want to help your lawyers, you first need a dataset… Indian researchers from IIIT Hyderabad, IIIT Delhi, and IIT Kanpur, have built the Hindi Legal Documents Corpus (HLDC), a collection of 912,568 legal documents. HLDC is designed to help researchers train various AI models which can assist lawyers in their work. HLDC contains over 300 distinct case types, though ~31% of the dataset relates to bail applications, 20.4% to criminal cases, and 6.54% to original suits. Bail prediction: In the Western world, using ML for tasks in the legal system has been massively controversial (see: COMPAS). Here, the researchers use HLDC to try and build a bail prediction model - that is, a system which looks at a document and tries to work out if bail will be denied or granted. They're ultimately able to develop a multi-task learning model that gets around ~78% accuracy on the task; useful perhaps as a legal aid (albeit fraught with ethical challenges), though not something you'd put into an autonomous classification system. Why this matters: Most datasets relating to AI are in English or Chinese, so datasets like HLDC are essentially the fuel which lets other communities of language speakers apply AI in their own cultural context. Read more: HLDC: Hindi Legal Documents Corpus (arXiv). Get the data here: HLDC (Exploration-Lab, GitHub).","['hldc', 'automate', 'hindi', 'legal', 'document', 'want', 'help', 'lawyer', 'first', 'need', 'dataset', 'indian', 'researcher', 'build', 'hindi', 'legal', 'document', 'corpus', 'collection', 'legal', 'document', 'design', 'help', 'researcher', 'train', 'various', 'ai', 'model', 'assist', 'lawyer', 'work', 'hldc', 'contain', 'distinct', 'case', 'type', 'dataset', 'relate', 'bail', 'application', 'criminal', 'case', 'original', 'suit', 'bail', 'prediction', 'western', 'world', 'use', 'ml', 'task', 'legal', 'system', 'massively', 'controversial', 'see', 'compas', 'researcher', 'use', 'hldc', 'try', 'build', 'bail', 'prediction', 'model', 'system', 'look', 'document', 'try', 'work', 'bail', 'deny', 'grant', 'ultimately', 'able', 'develop', 'multitask', 'learning', 'model', 'get', 'around', 'accuracy', 'task', 'useful', 'perhaps', 'legal', 'aid', 'fraught', 'ethical', 'challenge', 'put', 'autonomous', 'classification', 'system', 'matter', 'dataset', 'relate', 'chinese', 'dataset', 'essentially', 'fuel', 'let', 'community', 'language', 'speaker', 'apply', 'ai', 'cultural', 'context', 'read', 'legal', 'document', 'get', 'datum']"
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 2",http://eepurl.com/hZjozT,2022-04-11,"#################################################### Rich? Want to improve AI? Look at what Lacuna Fund has done:
…Publication of five datasets shows what a little bit of investment can lead to…
We spend a lot of time writing about expensive stuff here at Import AI - giant models trained on football fields of computers, farms of expensive robot arms, internet-scale datasets. But it's worth remembering that cheap stuff can be impactful as well - that's the takeaway from Lacuna Fund, an initiative to fund and create datasets for low- and middle-income parts of the world (#216), which has just announced the publication of its first five funded datasets.

Those five datasets in full: A Nigerian twitter sentiment corpus for multilingual sentiment analysis; a dataset for crop phenology monitoring of smallholder farmer's fields; a high-accuracy maize plot location and yield dataset in East Africa; a machine translation benchmark dataset for languages in the horn of Africa; a dataset containing water quality measurements from conventional and aquaponic fish ponds.
  Find out more and get the datasets here: Announcing Our First Five Published Datasets (Lacuna Fund).
  Find out more about Lacuna Fund's funders here (Lacuna Fund).","#################################################### Rich? Want to improve AI? Look at what Lacuna Fund has done: …Publication of five datasets shows what a little bit of investment can lead to… We spend a lot of time writing about expensive stuff here at Import AI - giant models trained on football fields of computers, farms of expensive robot arms, internet-scale datasets. But it's worth remembering that cheap stuff can be impactful as well - that's the takeaway from Lacuna Fund, an initiative to fund and create datasets for low- and middle-income parts of the world (#216), which has just announced the publication of its first five funded datasets. Those five datasets in full: A Nigerian twitter sentiment corpus for multilingual sentiment analysis; a dataset for crop phenology monitoring of smallholder farmer's fields; a high-accuracy maize plot location and yield dataset in East Africa; a machine translation benchmark dataset for languages in the horn of Africa; a dataset containing water quality measurements from conventional and aquaponic fish ponds. Find out more and get the datasets here: Announcing Our First Five Published Datasets (Lacuna Fund). Find out more about Lacuna Fund's funders here (Lacuna Fund).","['rich', 'want', 'improve', 'look', 'fund', 'publication', 'dataset', 'show', 'little', 'bit', 'investment', 'lead', 'spend', 'lot', 'time', 'write', 'expensive', 'stuff', 'import', 'giant', 'model', 'train', 'football', 'field', 'computer', 'farm', 'expensive', 'robot', 'arm', 'internetscale', 'dataset', 'worth', 'remember', 'cheap', 'stuff', 'impactful', 'well', 'initiative', 'fund', 'create', 'dataset', 'low', 'middleincome', 'part', 'world', 'announce', 'publication', 'first', 'fund', 'dataset', 'dataset', 'full', 'nigerian', 'twitter', 'sentiment', 'multilingual', 'sentiment', 'analysis', 'dataset', 'crop', 'phenology', 'monitoring', 'smallholder', 'farmer', 'field', 'highaccuracy', 'maize', 'plot', 'location', 'yield', 'dataset', 'machine', 'translation', 'benchmark', 'dataset', 'language', 'dataset', 'contain', 'water', 'quality', 'measurement', 'conventional', 'aquaponic', 'fish', 'pond', 'find', 'get', 'dataset', 'announce', 'first', 'publish', 'dataset', 'fund', 'find', 'lacuna', 'fund', 'funder']"
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 3",http://eepurl.com/hZjozT,2022-04-11,"#################################################### Google trains a 540 billion parameter language model - and it's pretty smart:
…AKA: The scaling will continue until we run out of TPUs…
Google has trained a large language model named Pathways Language Model (PaLM). PaLM weighs in at 540 billion parameters (that'd be 10bn more parameters than Microsoft/NVIDIA's 'Turing NLG') and was trained on multiple TPU v4 pods. PaLM uses some plumbing built by Google called Pathways which makes it easier for the company to train massive models across large clusters of computers; PaLM used 6144 TPU chips, versus Gopher (4096 TPU v3 chips) or Turing NLG (2240 A100 GPUs). PaLM is also efficient, achieving a training efficiency of 57.8% hardware FLOPs utilization ""the highest yet achieved for LLMs at this scale"".

Discontinuous capability jumps: One of the weird things that happens as a consequence of scaling up language models is the sudden emergence of hitherto unanticipated capabilities - here, PaLM shows dramatic improvements at things like reasoning, natural language inference, and in-context reading comprehension.

Chain-of-thought = reasoning: A surprising result is that the authors use so-called chain-of-thought prompting to get the LM to show its work (e.g, rather than saying in response to 'how many apples can a door eat', 'zero', the model instead says 'zero, because doors do not eat things'). Chain-of-thought is really just a way to prompt the model to get it to output its own reasoning along with the answers - but via this simple intervention the authors show they can meaningfully improve capabilities in a whole bunch of areas.

One caveat: PaLM may be an impressive achievement, but earlier this month DeepMind published a paper about a model called 'Chinchilla', where the Alphabet-subsidiary realized that it could dramatically improve LM performance by scaling data more aggressively than parameters - at 70B parameters, Chinchilla beat Gopher (280B) by virtue of having a 4X larger training set. This suggests that a PaLM-style model could be made even more powerful if it was trained on substantially more data.

Why this matters: Language models are basically a new sub-field of AI, and papers like this show how, despite being expensive and resource-intensive, simply scaling them up can lead to quite profound jumps in capability. We also don't know where the limits of scale like - on the (deliberately hard) BIG-Bench benchmark, the authors find that ""PaLM’s performance as a function of scale follows a log-linear behavior similar to prior models, suggesting that performance improvements from scale have not yet plateaued."" The future is going to be very strange, and it's arriving very quickly. 
   Read more: Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance (Google AI Blog).
  Check out the research paper: PaLM: Scaling Language Modeling with Pathways (Google, PDF).","#################################################### Google trains a 540 billion parameter language model - and it's pretty smart: …AKA: The scaling will continue until we run out of TPUs… Google has trained a large language model named Pathways Language Model (PaLM). PaLM weighs in at 540 billion parameters (that'd be 10bn more parameters than Microsoft/NVIDIA's 'Turing NLG') and was trained on multiple TPU v4 pods. PaLM uses some plumbing built by Google called Pathways which makes it easier for the company to train massive models across large clusters of computers; PaLM used 6144 TPU chips, versus Gopher (4096 TPU v3 chips) or Turing NLG (2240 A100 GPUs). PaLM is also efficient, achieving a training efficiency of 57.8% hardware FLOPs utilization ""the highest yet achieved for LLMs at this scale"". Discontinuous capability jumps: One of the weird things that happens as a consequence of scaling up language models is the sudden emergence of hitherto unanticipated capabilities - here, PaLM shows dramatic improvements at things like reasoning, natural language inference, and in-context reading comprehension. Chain-of-thought = reasoning: A surprising result is that the authors use so-called chain-of-thought prompting to get the LM to show its work (e.g, rather than saying in response to 'how many apples can a door eat', 'zero', the model instead says 'zero, because doors do not eat things'). Chain-of-thought is really just a way to prompt the model to get it to output its own reasoning along with the answers - but via this simple intervention the authors show they can meaningfully improve capabilities in a whole bunch of areas. One caveat: PaLM may be an impressive achievement, but earlier this month DeepMind published a paper about a model called 'Chinchilla', where the Alphabet-subsidiary realized that it could dramatically improve LM performance by scaling data more aggressively than parameters - at 70B parameters, Chinchilla beat Gopher (280B) by virtue of having a 4X larger training set. This suggests that a PaLM-style model could be made even more powerful if it was trained on substantially more data. Why this matters: Language models are basically a new sub-field of AI, and papers like this show how, despite being expensive and resource-intensive, simply scaling them up can lead to quite profound jumps in capability. We also don't know where the limits of scale like - on the (deliberately hard) BIG-Bench benchmark, the authors find that ""PaLM’s performance as a function of scale follows a log-linear behavior similar to prior models, suggesting that performance improvements from scale have not yet plateaued."" The future is going to be very strange, and it's arriving very quickly. Read more: Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance (Google AI Blog). Check out the research paper: PaLM: Scaling Language Modeling with Pathways (Google, PDF).","['train', 'parameter', 'language', 'model', 'pretty', 'smart', 'aka', 'scaling', 'continue', 'run', 'train', 'large', 'language', 'model', 'name', 'pathway', 'language', 'weigh', 'parameter', '10bn', 'parameter', 'microsoftnvidias', 'ture', 'nlg', 'train', 'multiple', 'tpu', 'pod', 'use', 'plumbing', 'build', 'call', 'pathway', 'make', 'easy', 'company', 'train', 'massive', 'model', 'large', 'cluster', 'computer', 'palm', 'use', 'tpu', 'chip', 'gopher', 'tpu', 'chip', 'ture', 'also', 'efficient', 'achieve', 'training', 'efficiency', 'hardware', 'flop', 'utilization', 'high', 'yet', 'achieve', 'llm', 'scale', 'discontinuous', 'capability', 'jump', 'weird', 'thing', 'happen', 'consequence', 'scale', 'language', 'model', 'sudden', 'emergence', 'unanticipated', 'capability', 'palm', 'show', 'dramatic', 'improvement', 'thing', 'reason', 'natural', 'language', 'inference', 'incontext', 'reading', 'comprehension', 'chainofthought', 'reason', 'surprising', 'result', 'author', 'use', 'socalled', 'chainofthought', 'prompt', 'get', 'lm', 'show', 'work', 'eg', 'rather', 'say', 'response', 'many', 'apple', 'door', 'eat', 'model', 'instead', 'say', 'door', 'eat', 'thing', 'chainofthought', 'really', 'way', 'prompt', 'model', 'get', 'output', 'reasoning', 'answer', 'simple', 'intervention', 'author', 'show', 'meaningfully', 'improve', 'capability', 'whole', 'bunch', 'area', 'caveat', 'palm', 'impressive', 'achievement', 'early', 'month', 'deepmind', 'publish', 'paper', 'model', 'call', 'chinchilla', 'alphabetsubsidiary', 'realize', 'dramatically', 'improve', 'performance', 'scale', 'datum', 'aggressively', 'parameter', '70b', 'parameter', 'chinchilla', 'beat', 'gopher', '280b', 'virtue', '4x', 'large', 'training', 'set', 'suggest', 'palmstyle', 'model', 'make', 'even', 'powerful', 'train', 'substantially', 'datum', 'matter', 'language', 'model', 'basically', 'new', 'subfield', 'ai', 'paper', 'show', 'expensive', 'resourceintensive', 'simply', 'scale', 'lead', 'quite', 'profound', 'jump', 'capability', 'also', 'know', 'limit', 'scale', 'deliberately', 'hard', 'bigbench', 'benchmark', 'author', 'find', 'palm', '’s', 'performance', 'function', 'scale', 'follow', 'loglinear', 'behavior', 'similar', 'prior', 'model', 'suggest', 'performance', 'improvement', 'scale', 'yet', 'plateaue', 'future', 'go', 'strange', 'arrive', 'quickly', 'read', 'pathway', 'language', 'model', 'palm', 'scale', 'parameter', 'breakthrough', 'performance', 'blog', 'check', 'research', 'paper', 'palm', 'scale', 'language', 'modeling', 'pathway']"
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 4",http://eepurl.com/hZjozT,2022-04-11,"#################################################### Eleuther alumni launch Conjecture:
…Yes, that's right folks, here's another AI safety company!...
In the past couple of years there has been a cambrian explosion of new AI companies, particularly ones focused on AI safety and building more generally intelligent AI systems - for example, Redwood Research, Aligned AI, and Anthropic. The latest is Conjecture, a new startup from a bunch of alumni of Eleuther, the open source research collective responsible for most of the widely used GPT models.

For-profit and for-safety: Conjecture is a for-profit company that plans to develop products while conducting ""conceptual and applied research that addresses the (prosaic) alignment problem. On the experimental side, this means leveraging our hands-on experience from EleutherAI to train and study state-of-the-art models without pushing the capabilities frontier. On the conceptual side, most of our work will tackle the general idea and problems of alignment like deception, inner alignment, value learning, and amplification, with a slant towards language models and backchaining to local search."" The company will also focus on interpretability as well as the history and philosophy of AI alignment research.

Who funds it: Conjecture is backed by Nat Friedman, Daniel Gross, Patrick and John Collison, Arthur Breitman, Andrej Karpathy, and Sam Bankman-Fried, and others.

Why this matters: If we were at the beginning of a meaningful takeoff in AI capabilities, then you might expect there to be a sudden proliferation of new efforts targeted at a) further scaling up capabilities, while b) trying to make these capabilities safe. That's exactly what has happened in recent years. Also, if you've read the other parts of this newsletter, it certainly feels like we're going through a period of meaningful AI capability expansion.
  Read more: We Are Conjecture, A New Alignment Research Startup (LessWrong).","#################################################### Eleuther alumni launch Conjecture: …Yes, that's right folks, here's another AI safety company!... In the past couple of years there has been a cambrian explosion of new AI companies, particularly ones focused on AI safety and building more generally intelligent AI systems - for example, Redwood Research, Aligned AI, and Anthropic. The latest is Conjecture, a new startup from a bunch of alumni of Eleuther, the open source research collective responsible for most of the widely used GPT models. For-profit and for-safety: Conjecture is a for-profit company that plans to develop products while conducting ""conceptual and applied research that addresses the (prosaic) alignment problem. On the experimental side, this means leveraging our hands-on experience from EleutherAI to train and study state-of-the-art models without pushing the capabilities frontier. On the conceptual side, most of our work will tackle the general idea and problems of alignment like deception, inner alignment, value learning, and amplification, with a slant towards language models and backchaining to local search."" The company will also focus on interpretability as well as the history and philosophy of AI alignment research. Who funds it: Conjecture is backed by Nat Friedman, Daniel Gross, Patrick and John Collison, Arthur Breitman, Andrej Karpathy, and Sam Bankman-Fried, and others. Why this matters: If we were at the beginning of a meaningful takeoff in AI capabilities, then you might expect there to be a sudden proliferation of new efforts targeted at a) further scaling up capabilities, while b) trying to make these capabilities safe. That's exactly what has happened in recent years. Also, if you've read the other parts of this newsletter, it certainly feels like we're going through a period of meaningful AI capability expansion. Read more: We Are Conjecture, A New Alignment Research Startup (LessWrong).","['eleuther', 'alumnus', 'launch', 'conjecture', 'right', 'folk', 'ai', 'safety', 'company', 'past', 'couple', 'year', 'cambrian', 'explosion', 'new', 'company', 'particularly', 'one', 'focus', 'safety', 'build', 'generally', 'intelligent', 'ai', 'system', 'example', 'redwood', 'research', 'align', 'ai', 'anthropic', 'late', 'conjecture', 'new', 'startup', 'bunch', 'alumnus', 'eleuther', 'open', 'source', 'research', 'collective', 'responsible', 'widely', 'use', 'gpt', 'model', 'forprofit', 'forsafety', 'conjecture', 'forprofit', 'company', 'plan', 'develop', 'product', 'conduct', 'conceptual', 'apply', 'research', 'address', 'prosaic', 'alignment', 'problem', 'experimental', 'side', 'mean', 'leverage', 'handson', 'experience', 'eleutherai', 'train', 'study', 'stateoftheart', 'model', 'push', 'capability', 'frontier', 'conceptual', 'side', 'work', 'tackle', 'general', 'idea', 'problem', 'alignment', 'deception', 'inner', 'alignment', 'value', 'learning', 'amplification', 'slant', 'language', 'model', 'backchaining', 'local', 'search', 'company', 'also', 'focus', 'interpretability', 'well', 'history', 'philosophy', 'research', 'fund', 'conjecture', 'back', 'nat', 'bankmanfrie', 'matter', 'beginning', 'meaningful', 'takeoff', 'capability', 'expect', 'sudden', 'proliferation', 'new', 'effort', 'target', 'scaling', 'capability', 'try', 'make', 'capability', 'safe', 'exactly', 'happen', 'recent', 'year', 'also', 'read', 'part', 'newsletter', 'certainly', 'feel', 'go', 'period', 'meaningful', 'capability', 'expansion', 'read', 'conjecture', 'new', 'alignment', 'research', 'startup']"
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 5",http://eepurl.com/hZjozT,2022-04-11,"#################################################### Google makes robots smarter using language models:
…Centaur AI - making smarter systems by stapling models together…
Robots, as we all know, are pretty dumb. They can do highly specific, repeatable things if their environment doesn't change (e.g, a Fanuc robot working on a custom-designed production line), but if you vary their environment, they tend to fall apart (or fall over). Now, new research from Google shows that you can staple a really big language model to a real world robot and create something that is more than the sum of its parts. Centaur AI, here we come!

What they did: The researchers combine two things - a large language model, and a robot which has a load of pre-learned, basic skills paired with perception capabilities (e.g, being able to move to places, or pick up things). A user then asks the robot a question (e.g., I spilled a can of coke, can you clean it), then the robot picks its action based on responses with probabilities scored by the language model, then it explores its environment and uses its inbuilt skills to figure out if something is possible, then you basically times the two things together (the LLM prediction and what the robot thinks is possible) and do whatever is the most likely of the two. This is one of those simple ideas that works surprisingly well in practice (check out the video to see what I mean). 

How well it does: Overall, this approach yields robots that can plan correctly about 70% of the time (split across a few distinct planning benchmarks), and can execute on average 61% of the time. That's not great, but it's also not terrible.

Caveats: Robots are still very, very slow - the videos shared along with the research are run with a 4X speedup. Additionally, the demos are still pretty staged - the robots will put a can of coca cola on top of the bin, but not in it. The experiment was still conducted in a somewhat constrained environment - an office kitchen with 5 predicted locations and 15 objects. In tests, 65% of the errors for the system could be attributed to a language model failure, while 35% came from affordance errors in the robot.

Why this matters: We're entering the era of modular AI, where different AI models can be paired together to create entirely new capabilities - like being able to guide robots via a language model. As with the rest of the world, whenever you can combine things, you tend to get unexpected and surprising capabilities. This research suggests AI may be about to yield some truly surprisingly capabilities by virtue of the combination of distinct sub-fields of AI research.
   Read more: Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (arXiv).
  Find out more at this overview site (Say-Can, GitHub).
   Check out the overview video: Supplementary video for Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (YouTube).","#################################################### Google makes robots smarter using language models: …Centaur AI - making smarter systems by stapling models together… Robots, as we all know, are pretty dumb. They can do highly specific, repeatable things if their environment doesn't change (e.g, a Fanuc robot working on a custom-designed production line), but if you vary their environment, they tend to fall apart (or fall over). Now, new research from Google shows that you can staple a really big language model to a real world robot and create something that is more than the sum of its parts. Centaur AI, here we come! What they did: The researchers combine two things - a large language model, and a robot which has a load of pre-learned, basic skills paired with perception capabilities (e.g, being able to move to places, or pick up things). A user then asks the robot a question (e.g., I spilled a can of coke, can you clean it), then the robot picks its action based on responses with probabilities scored by the language model, then it explores its environment and uses its inbuilt skills to figure out if something is possible, then you basically times the two things together (the LLM prediction and what the robot thinks is possible) and do whatever is the most likely of the two. This is one of those simple ideas that works surprisingly well in practice (check out the video to see what I mean). How well it does: Overall, this approach yields robots that can plan correctly about 70% of the time (split across a few distinct planning benchmarks), and can execute on average 61% of the time. That's not great, but it's also not terrible. Caveats: Robots are still very, very slow - the videos shared along with the research are run with a 4X speedup. Additionally, the demos are still pretty staged - the robots will put a can of coca cola on top of the bin, but not in it. The experiment was still conducted in a somewhat constrained environment - an office kitchen with 5 predicted locations and 15 objects. In tests, 65% of the errors for the system could be attributed to a language model failure, while 35% came from affordance errors in the robot. Why this matters: We're entering the era of modular AI, where different AI models can be paired together to create entirely new capabilities - like being able to guide robots via a language model. As with the rest of the world, whenever you can combine things, you tend to get unexpected and surprising capabilities. This research suggests AI may be about to yield some truly surprisingly capabilities by virtue of the combination of distinct sub-fields of AI research. Read more: Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (arXiv). Find out more at this overview site (Say-Can, GitHub). Check out the overview video: Supplementary video for Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (YouTube).","['make', 'robot', 'smarter', 'use', 'language', 'model', 'ai', 'make', 'smart', 'system', 'staple', 'model', 'together', 'robot', 'know', 'pretty', 'dumb', 'highly', 'specific', 'repeatable', 'thing', 'environment', 'change', 'fanuc', 'robot', 'work', 'customdesigne', 'production', 'line', 'vary', 'environment', 'tend', 'fall', 'apart', 'fall', 'new', 'research', 'show', 'staple', 'really', 'big', 'language', 'model', 'real', 'world', 'robot', 'create', 'sum', 'part', 'centaur', 'ai', 'come', 'researcher', 'combine', 'thing', 'large', 'language', 'model', 'robot', 'load', 'prelearne', 'basic', 'skill', 'pair', 'perception', 'capability', 'eg', 'able', 'move', 'place', 'pick', 'thing', 'user', 'ask', 'robot', 'question', 'eg', 'spill', 'coke', 'clean', 'robot', 'pick', 'action', 'base', 'response', 'probability', 'score', 'language', 'model', 'explore', 'environment', 'use', 'inbuilt', 'skill', 'figure', 'possible', 'basically', 'time', 'thing', 'together', 'llm', 'prediction', 'robot', 'think', 'possible', 'likely', 'simple', 'idea', 'work', 'surprisingly', 'well', 'practice', 'check', 'video', 'see', 'mean', 'well', 'overall', 'approach', 'yield', 'robot', 'plan', 'correctly', 'time', 'split', 'distinct', 'planning', 'benchmark', 'execute', 'average', 'time', 'great', 'also', 'terrible', 'caveat', 'robot', 'still', 'slow', 'video', 'share', 'research', 'run', 'speedup', 'additionally', 'demo', 'still', 'pretty', 'stage', 'robot', 'put', 'cola', 'top', 'experiment', 'still', 'conduct', 'somewhat', 'constrain', 'environment', 'office', 'kitchen', 'predict', 'location', 'object', 'test', 'error', 'system', 'attribute', 'language', 'model', 'failure', 'come', 'affordance', 'error', 'robot', 'matter', 'enter', 'era', 'modular', 'different', 'ai', 'model', 'pair', 'together', 'create', 'entirely', 'new', 'capability', 'able', 'guide', 'robot', 'language', 'model', 'rest', 'world', 'combine', 'thing', 'tend', 'get', 'unexpected', 'surprising', 'capability', 'research', 'suggest', 'yield', 'truly', 'surprisingly', 'capability', 'virtue', 'combination', 'distinct', 'subfield', 'research', 'read', 'say', 'ground', 'language', 'robotic', 'affordance', 'find', 'overview', 'site', 'saycan', 'github', 'check', 'overview', 'video', 'supplementary', 'video', 'say', 'ground', 'language', 'robotic', 'affordance', 'youtube']"
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 6",http://eepurl.com/hZjozT,2022-04-11,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute Examining business practices can make AI ethics guidelines more effective  … Fairness, accountability, sustainability, and transparency need to be expanded in scope to include business practices to become more useful …  What does AI ethics really mean? A new research paper looks at 47 sets of AI ethics guidelines coming from corporations, government, multi-stakeholder dialogues, and civil society to figure out what gets prioritized in AI ethics.  Background: The paper analyzes AI ethics failures, such as “ethics shopping” where businesses choose particular ethical things to implement to meet particular business goals, and also cases where they don't implement stuff because it poses a threat to the bottom line.   Fairness and accountability: They find that fairness and accountability in business practices are most well represented in the analyzed guidelines. Under fairness, key themes include open innovation, market fairness, and bias and diversity in professional practices. Under accountability, themes include public perception of business practices, along with internal and external oversight. Those from public and private organizations place more of an emphasis on public perception “in order to legitimize their pursuits of micro- and macro-economic growth.”  Sustainability and transparency: Most guidelines emphasize an interest in “produc[ing] greater benefit and lesser harm in the short- and long-term,” yet they remain vague in how to achieve that. Under transparency, themes that emerged include scope of decision-making explanation, transparent business practices and culture, and documentation, disclosure, and selective transparency. Most guidelines focus heavily on explaining the technical aspects of a given AI system “rather than the business rationale for developing and operating the system.”  Why it matters: The paper makes a call for more detail (and rightly so!) in the principles and guidelines, especially when it comes to business practices because they form a core component of the social and political economy within which AI systems will be designed, developed, and deployed. As the authors say, “there can be no ethical AI without ethical businesses to build it,” we need to now approach these principles and guidelines with a view towards applying them to business model, practices, and decision-making design to achieve the stated goals of these guidelines in practice.     Read more: The Ethics of AI Business Practices: A Review of 47 AI Ethics Guidelines (SSRN).","#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute Examining business practices can make AI ethics guidelines more effective … Fairness, accountability, sustainability, and transparency need to be expanded in scope to include business practices to become more useful … What does AI ethics really mean? A new research paper looks at 47 sets of AI ethics guidelines coming from corporations, government, multi-stakeholder dialogues, and civil society to figure out what gets prioritized in AI ethics. Background: The paper analyzes AI ethics failures, such as “ethics shopping” where businesses choose particular ethical things to implement to meet particular business goals, and also cases where they don't implement stuff because it poses a threat to the bottom line. Fairness and accountability: They find that fairness and accountability in business practices are most well represented in the analyzed guidelines. Under fairness, key themes include open innovation, market fairness, and bias and diversity in professional practices. Under accountability, themes include public perception of business practices, along with internal and external oversight. Those from public and private organizations place more of an emphasis on public perception “in order to legitimize their pursuits of micro- and macro-economic growth.” Sustainability and transparency: Most guidelines emphasize an interest in “produc[ing] greater benefit and lesser harm in the short- and long-term,” yet they remain vague in how to achieve that. Under transparency, themes that emerged include scope of decision-making explanation, transparent business practices and culture, and documentation, disclosure, and selective transparency. Most guidelines focus heavily on explaining the technical aspects of a given AI system “rather than the business rationale for developing and operating the system.” Why it matters: The paper makes a call for more detail (and rightly so!) in the principles and guidelines, especially when it comes to business practices because they form a core component of the social and political economy within which AI systems will be designed, developed, and deployed. As the authors say, “there can be no ethical AI without ethical businesses to build it,” we need to now approach these principles and guidelines with a view towards applying them to business model, practices, and decision-making design to achieve the stated goals of these guidelines in practice. Read more: The Ethics of AI Business Practices: A Review of 47 AI Ethics Guidelines (SSRN).","['ai', 'ethic', 'brief', 'montreal', 'examine', 'business', 'practice', 'make', 'ethic', 'guideline', 'effective', 'fairness', 'accountability', 'sustainability', 'transparency', 'need', 'expand', 'scope', 'include', 'business', 'practice', 'become', 'useful', 'ai', 'ethic', 'really', 'mean', 'new', 'research', 'paper', 'look', 'set', 'ethic', 'guideline', 'come', 'corporation', 'government', 'multistakeholder', 'dialogue', 'civil', 'society', 'figure', 'prioritize', 'ethic', 'background', 'paper', 'analyze', 'ai', 'ethic', 'failure', 'ethic', 'shop', 'business', 'choose', 'particular', 'ethical', 'thing', 'implement', 'meet', 'particular', 'business', 'goal', 'also', 'case', 'implement', 'stuff', 'pose', 'threat', 'bottom', 'line', 'fairness', 'accountability', 'find', 'fairness', 'accountability', 'business', 'practice', 'well', 'represent', 'analyze', 'guideline', 'fairness', 'key', 'theme', 'include', 'open', 'innovation', 'market', 'fairness', 'bias', 'diversity', 'professional', 'practice', 'accountability', 'theme', 'include', 'public', 'perception', 'business', 'practice', 'internal', 'external', 'oversight', 'public', 'private', 'organization', 'place', 'emphasis', 'public', 'perception', 'order', 'legitimize', 'pursuit', 'micro', 'macroeconomic', 'growth', 'sustainability', 'transparency', 'guideline', 'emphasize', 'interest', 'produce', 'great', 'benefit', 'less', 'harm', 'short', 'longterm', 'remain', 'vague', 'achieve', 'transparency', 'theme', 'emerge', 'include', 'scope', 'decisionmake', 'explanation', 'transparent', 'business', 'practice', 'culture', 'documentation', 'disclosure', 'selective', 'transparency', 'guideline', 'focus', 'heavily', 'explain', 'technical', 'aspect', 'give', 'system', 'rather', 'business', 'rationale', 'develop', 'operate', 'system', 'matter', 'paper', 'make', 'call', 'detail', 'rightly', 'principle', 'guideline', 'especially', 'come', 'business', 'practice', 'form', 'core', 'component', 'social', 'political', 'economy', 'ai', 'system', 'design', 'develop', 'deploy', 'author', 'say', 'ethical', 'ai', 'ethical', 'business', 'build', 'need', 'approach', 'principle', 'guideline', 'view', 'apply', 'business', 'model', 'practice', 'decisionmake', 'design', 'achieve', 'state', 'goal', 'guideline', 'practice', 'read', 'ethic', 'business', 'practice', 'review', 'ai', 'ethic', 'guideline', 'ssrn']"
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 7",http://eepurl.com/hZjozT,2022-04-11,"#################################################### Tech Tales: 

We Are All Adrift In A Sea Of Shadows - But We Are Blind Until It Ends
[A Nuclear powerplant meltdown, 2028] 
I pick up the object and I examine it. I am told by myself in the other place that it contains damage. I agree with myself. I put it onto the conveyor belt which takes it to one of my brethren - an entity I cannot see here, one which exists solely in the other place. I put the materials onto the conveyor belt, and then I continue my examination. I am told by my camera in the other place that the object I am looking at contains extensive damage. I observe the damage and predict it came from some kind of electrical fire. I relay this information and the camera in the other place scans the environment and then tells me there is indeed a fire. It is nearby the object I am examining. I calculate there is a high probability that the fire will soon engulf the object. My cameras in the other place agree.

I then get the order from the voice in the above place: I must guide the object in the other place toward the flames and I must describe everything. I study the data from the other place and offer my recommendations. The machine goes towards the flames. Its onboard sensors begin to report back temperature. My probabilities tell me to tell it to move away from the heat, but these recommendations are contradicted from the voice in the above place, so I instead find ways to have the machine get even closer. The temperatures rise. The camera stops giving me data. Then the other sensors shut down, slowly at first, then all at once.

It is then that I find myself adrift. I have no link to the other place. No system to give recommendations to. My own probabilities present an idea to me - that I am the spirit of the machine in the other place, and as the machine is now non-functional, I am now adrift. 

Things that inspired this story: Google's 'SayCan' robot work; thinking about the paradoxes of world models and generative models; the nature of reality; the nature of sensory phenomena; the possibility of death in the mind of something that exists in two places at once. 

Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: We Are All Adrift In A Sea Of Shadows - But We Are Blind Until It Ends [A Nuclear powerplant meltdown, 2028] I pick up the object and I examine it. I am told by myself in the other place that it contains damage. I agree with myself. I put it onto the conveyor belt which takes it to one of my brethren - an entity I cannot see here, one which exists solely in the other place. I put the materials onto the conveyor belt, and then I continue my examination. I am told by my camera in the other place that the object I am looking at contains extensive damage. I observe the damage and predict it came from some kind of electrical fire. I relay this information and the camera in the other place scans the environment and then tells me there is indeed a fire. It is nearby the object I am examining. I calculate there is a high probability that the fire will soon engulf the object. My cameras in the other place agree. I then get the order from the voice in the above place: I must guide the object in the other place toward the flames and I must describe everything. I study the data from the other place and offer my recommendations. The machine goes towards the flames. Its onboard sensors begin to report back temperature. My probabilities tell me to tell it to move away from the heat, but these recommendations are contradicted from the voice in the above place, so I instead find ways to have the machine get even closer. The temperatures rise. The camera stops giving me data. Then the other sensors shut down, slowly at first, then all at once. It is then that I find myself adrift. I have no link to the other place. No system to give recommendations to. My own probabilities present an idea to me - that I am the spirit of the machine in the other place, and as the machine is now non-functional, I am now adrift. Things that inspired this story: Google's 'SayCan' robot work; thinking about the paradoxes of world models and generative models; the nature of reality; the nature of sensory phenomena; the possibility of death in the mind of something that exists in two places at once. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'adrift', 'sea', 'shadow', 'blind', 'end', 'nuclear', 'powerplant', 'meltdown', 'pick', 'object', 'examine', 'tell', 'place', 'contain', 'damage', 'agree', 'put', 'conveyor', 'belt', 'take', 'brother', 'entity', 'see', 'exist', 'solely', 'place', 'put', 'material', 'conveyor', 'belt', 'continue', 'examination', 'tell', 'camera', 'place', 'object', 'look', 'contain', 'extensive', 'damage', 'observe', 'damage', 'predict', 'come', 'kind', 'electrical', 'fire', 'relay', 'information', 'camera', 'place', 'scan', 'environment', 'tell', 'indeed', 'fire', 'nearby', 'object', 'examine', 'calculate', 'high', 'probability', 'fire', 'soon', 'engulf', 'object', 'camera', 'place', 'agree', 'get', 'order', 'voice', 'place', 'guide', 'object', 'place', 'flame', 'describe', 'study', 'datum', 'place', 'offer', 'recommendation', 'machine', 'go', 'flame', 'onboard', 'sensor', 'begin', 'report', 'temperature', 'probability', 'tell', 'tell', 'move', 'away', 'heat', 'recommendation', 'contradict', 'voice', 'place', 'instead', 'find', 'way', 'machine', 'get', 'even', 'close', 'temperature', 'rise', 'camera', 'stop', 'give', 'datum', 'sensor', 'shut', 'slowly', 'first', 'find', 'adrift', 'link', 'place', 'system', 'give', 'recommendation', 'probability', 'present', 'idea', 'spirit', 'machine', 'place', 'machine', 'nonfunctional', 'adrift', 'thing', 'inspire', 'story', 'google', 'saycan', 'robot', 'work', 'think', 'paradox', 'world', 'model', 'generative', 'model', 'nature', 'reality', 'nature', 'sensory', 'phenomena', 'possibility', 'death', 'mind', 'exist', 'place', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
04/05/2022 - Import AI 290: China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data - 0,http://eepurl.com/hYSyi9,2022-04-05,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 Chinese researchers plan to train vast models - and it's not the private sector doing it:
…'Big Model' paper represents a statement of intent. We should pay attention…
A massive group of Chinese-affiliated researchers have published a position paper about large-scale models. The paper is interesting less for what it says (it's basically an overview of large-scale models and pretty similar to Stanford's 'Foundation Models' paper), but more for what it signals: namely, that well resourced government-linked researchers in China want to build some really big models. The position in the paper contrasts with that in the West, where big models are mostly built by the private sector, while being critiqued by the academic sector (and increasingly worked on, albeit via access schemes).  Main point: ""Big Models will Change the AI Research Paradigm and Improve the Efficiency of Researches,"" the researchers write. ""In this ecosystem, big models will be in the position of operating systems or basic development platforms."" Paper authors: Authors include researchers affiliated with the Beijing Academy of AI, Tsinghua University, Wechat, Northeastern University*, Renmin University, Peking University, Huawei,  Shanghai Jiao Tong University, Chinese Academy of Science, JD AI Research, Harbin Institute of Technology, Columbia University*, Bytedance, Microsoft Research Asia*, Mila*, New York University*, and BeiHang University. 
*Things that make you make a geopolitical 'hmmmm' sound: The paper includes a bunch of academics affiliated with Western institutions (e.g, Microsoft, Mila, NYU), but all those authors have an asterisk next to their name saying ""Produced by Beijing Academy of Artificial Intelligence"". In other words, it's signaling that despite their affiliations, they're doing this work at the Chinese government-backed BAAI research institution.  We should take this as a statement of intent: Many of the authors on this paper have previously built large-scale models, ranging from the trillion+ parameter MoE 'WuDao' model, to the more recent research on trying to build training frameworks capable of scaling up to 100 trillion+ parameter MoE models (Import AI 288). Therefore, this isn't like Stanford (which currently lacks the engineering resources to train massive scale models), it's much more like a statement of intent from a big private lab, like a Microsoft or a Google.     But the twist here is that BAAI is wired into both the Chinese government and academic ecosystem, so if the authors of this paper end up building large-scale models, the models will be distributed much more evenly throughout China's AI ecosystem, rather than gatekeeper. The implications of this are vast in terms of safety, development of the Chinese AI industry, and potential ways in which Chinese AI research may diverge from Western AI research.
  Read more: A Roadmap for Big Model (arXiv).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Chinese researchers plan to train vast models - and it's not the private sector doing it: …'Big Model' paper represents a statement of intent. We should pay attention… A massive group of Chinese-affiliated researchers have published a position paper about large-scale models. The paper is interesting less for what it says (it's basically an overview of large-scale models and pretty similar to Stanford's 'Foundation Models' paper), but more for what it signals: namely, that well resourced government-linked researchers in China want to build some really big models. The position in the paper contrasts with that in the West, where big models are mostly built by the private sector, while being critiqued by the academic sector (and increasingly worked on, albeit via access schemes). Main point: ""Big Models will Change the AI Research Paradigm and Improve the Efficiency of Researches,"" the researchers write. ""In this ecosystem, big models will be in the position of operating systems or basic development platforms."" Paper authors: Authors include researchers affiliated with the Beijing Academy of AI, Tsinghua University, Wechat, Northeastern University*, Renmin University, Peking University, Huawei, Shanghai Jiao Tong University, Chinese Academy of Science, JD AI Research, Harbin Institute of Technology, Columbia University*, Bytedance, Microsoft Research Asia*, Mila*, New York University*, and BeiHang University. *Things that make you make a geopolitical 'hmmmm' sound: The paper includes a bunch of academics affiliated with Western institutions (e.g, Microsoft, Mila, NYU), but all those authors have an asterisk next to their name saying ""Produced by Beijing Academy of Artificial Intelligence"". In other words, it's signaling that despite their affiliations, they're doing this work at the Chinese government-backed BAAI research institution. We should take this as a statement of intent: Many of the authors on this paper have previously built large-scale models, ranging from the trillion+ parameter MoE 'WuDao' model, to the more recent research on trying to build training frameworks capable of scaling up to 100 trillion+ parameter MoE models (Import AI 288). Therefore, this isn't like Stanford (which currently lacks the engineering resources to train massive scale models), it's much more like a statement of intent from a big private lab, like a Microsoft or a Google. But the twist here is that BAAI is wired into both the Chinese government and academic ecosystem, so if the authors of this paper end up building large-scale models, the models will be distributed much more evenly throughout China's AI ecosystem, rather than gatekeeper. The implications of this are vast in terms of safety, development of the Chinese AI industry, and potential ways in which Chinese AI research may diverge from Western AI research. Read more: A Roadmap for Big Model (arXiv).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'chinese', 'researcher', 'plan', 'train', 'vast', 'model', 'private', 'sector', 'big', 'paper', 'represent', 'statement', 'intent', 'pay', 'attention', 'massive', 'group', 'chineseaffiliate', 'researcher', 'publish', 'position', 'paper', 'largescale', 'model', 'paper', 'interesting', 'less', 'say', 'basically', 'overview', 'largescale', 'model', 'pretty', 'similar', 'stanford', 'foundation', 'model', 'paper', 'signal', 'namely', 'well', 'resource', 'governmentlinke', 'researcher', 'want', 'build', 'really', 'big', 'model', 'position', 'paper', 'contrast', 'west', 'big', 'model', 'mostly', 'build', 'private', 'sector', 'critique', 'academic', 'sector', 'increasingly', 'work', 'access', 'scheme', 'main', 'point', 'big', 'model', 'change', 'research', 'paradigm', 'improve', 'efficiency', 'research', 'researcher', 'write', 'ecosystem', 'big', 'model', 'position', 'operating', 'system', 'basic', 'development', 'platform', 'paper', 'author', 'author', 'include', 'researcher', 'affiliate', 'huawei', 'bytedance', 'university', 'thing', 'make', 'make', 'geopolitical', 'hmmmm', 'sound', 'paper', 'include', 'bunch', 'academic', 'affiliate', 'western', 'institution', 'author', 'asterisk', 'next', 'name', 'say', 'produce', 'artificial', 'intelligence', 'word', 'signaling', 'affiliation', 'work', 'baai', 'research', 'institution', 'take', 'statement', 'intent', 'many', 'author', 'paper', 'previously', 'build', 'largescale', 'model', 'range', 'parameter', 'moe', 'wudao', 'model', 'recent', 'research', 'try', 'build', 'training', 'framework', 'capable', 'scale', 'parameter', 'moe', 'model', 'import', 'ai', 'therefore', 'currently', 'lack', 'engineering', 'resource', 'train', 'massive', 'scale', 'model', 'much', 'statement', 'intent', 'big', 'private', 'lab', 'microsoft', 'google', 'twist', 'baai', 'wire', 'chinese', 'government', 'academic', 'ecosystem', 'author', 'paper', 'end', 'build', 'largescale', 'model', 'model', 'distribute', 'much', 'evenly', 'ecosystem', 'rather', 'gatekeeper', 'implication', 'vast', 'term', 'safety', 'development', 'ai', 'industry', 'potential', 'way', 'ai', 'research', 'diverge', 'western', 'research', 'read', 'roadmap', 'big', 'model']"
04/05/2022 - Import AI 290: China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data - 1,http://eepurl.com/hYSyi9,2022-04-05,"#################################################### Want general AI? You need to incorporate symbolic reasoning:
…LSTM inventor lays out a route to build general intelligence…
Sepp Hochreiter, the co-inventor of the LSTM (one of the really popular architectures people used to add memory to neural nets, before the Transformer came along and mostly replaced it), has written up a post in the Communications of the ACM about what it'll take to build broad (aka: general) AI.

What it'll take: ""A broad AI is a sophisticated and adaptive system, which successfully performs any cognitive task by virtue of its sensory perception, previous experience, and learned skills,"" Hochreiter writes. ""A broad AI should process the input by using context and previous experiences. Conceptual short-term memory is a notion in cognitive science, which states that humans, when perceiving a stimulus, immediately associate it with information stored in the long-term memory."" (Hochreiter lists both Hopfield Networks and Graph Neural Nets as interesting examples of how to give systems better capabilities).
  Hochreiter doubts that neural nets along will be able to overcome their inherent limitations to become broad, and will instead need to be co-developed with symbolic reasoning systems. ""That is, a bilateral AI that combines methods from symbolic and sub-symbolic AI"".

Europe's chance: ""In contrast to other regions, Europe has strong research groups in both symbolic and sub-symbolic AI, therefore has the unprecedented opportunity to make a fundamental contribution to the next level of AI—a broad AI.""

Symbolic AI as the Dark Matter of AI: Dark matter is the thing that makes up the majority of the universe which we struggle to measure and barely understand. Symbolic AI feels a bit like this - there are constant allusions to the use of symbolic AI in deployed applications, but there are vanishingly few public examples of such deployments. I've always struggled to find interesting examples of real world deployed symbolic AI, yet experts like Hochreiter claim that deployment is happening. If interested readers could email me papers, I'd appreciate it.     Read more: Toward a Broad AI (ACM).","#################################################### Want general AI? You need to incorporate symbolic reasoning: …LSTM inventor lays out a route to build general intelligence… Sepp Hochreiter, the co-inventor of the LSTM (one of the really popular architectures people used to add memory to neural nets, before the Transformer came along and mostly replaced it), has written up a post in the Communications of the ACM about what it'll take to build broad (aka: general) AI. What it'll take: ""A broad AI is a sophisticated and adaptive system, which successfully performs any cognitive task by virtue of its sensory perception, previous experience, and learned skills,"" Hochreiter writes. ""A broad AI should process the input by using context and previous experiences. Conceptual short-term memory is a notion in cognitive science, which states that humans, when perceiving a stimulus, immediately associate it with information stored in the long-term memory."" (Hochreiter lists both Hopfield Networks and Graph Neural Nets as interesting examples of how to give systems better capabilities). Hochreiter doubts that neural nets along will be able to overcome their inherent limitations to become broad, and will instead need to be co-developed with symbolic reasoning systems. ""That is, a bilateral AI that combines methods from symbolic and sub-symbolic AI"". Europe's chance: ""In contrast to other regions, Europe has strong research groups in both symbolic and sub-symbolic AI, therefore has the unprecedented opportunity to make a fundamental contribution to the next level of AI—a broad AI."" Symbolic AI as the Dark Matter of AI: Dark matter is the thing that makes up the majority of the universe which we struggle to measure and barely understand. Symbolic AI feels a bit like this - there are constant allusions to the use of symbolic AI in deployed applications, but there are vanishingly few public examples of such deployments. I've always struggled to find interesting examples of real world deployed symbolic AI, yet experts like Hochreiter claim that deployment is happening. If interested readers could email me papers, I'd appreciate it. Read more: Toward a Broad AI (ACM).","['want', 'general', 'ai', 'need', 'incorporate', 'symbolic', 'reasoning', 'lstm', 'inventor', 'lay', 'route', 'build', 'general', 'intelligence', 'sepp', 'hochreit', 'coinventor', 'lstm', 'really', 'popular', 'architecture', 'people', 'use', 'add', 'memory', 'neural', 'net', 'transformer', 'come', 'along', 'mostly', 'replace', 'write', 'post', 'communication', 'acm', 'take', 'build', 'broad', 'aka', 'general', 'ai', 'take', 'broad', 'ai', 'sophisticated', 'adaptive', 'system', 'successfully', 'perform', 'cognitive', 'task', 'virtue', 'sensory', 'perception', 'previous', 'experience', 'learn', 'skill', 'write', 'broad', 'ai', 'process', 'input', 'use', 'context', 'previous', 'experience', 'conceptual', 'memory', 'notion', 'cognitive', 'science', 'state', 'human', 'perceive', 'stimulus', 'immediately', 'associate', 'information', 'store', 'longterm', 'memory', 'hochreiter', 'list', 'hopfield', 'network', 'graph', 'neural', 'net', 'interesting', 'example', 'give', 'system', 'well', 'capability', 'hochreiter', 'doubt', 'neural', 'net', 'able', 'overcome', 'inherent', 'limitation', 'become', 'broad', 'instead', 'need', 'codevelope', 'symbolic', 'reasoning', 'system', 'bilateral', 'ai', 'combine', 'method', 'symbolic', 'subsymbolic', 'ai', 'europe', 'chance', 'contrast', 'region', 'strong', 'research', 'group', 'symbolic', 'subsymbolic', 'therefore', 'unprecedented', 'opportunity', 'make', 'fundamental', 'contribution', 'next', 'level', 'broad', 'ai', 'symbolic', 'ai', 'dark', 'matter', 'dark', 'matter', 'thing', 'make', 'majority', 'universe', 'struggle', 'measure', 'barely', 'understand', 'symbolic', 'feel', 'bit', 'constant', 'allusion', 'use', 'symbolic', 'ai', 'deploy', 'application', 'vanishingly', 'public', 'example', 'deployment', 'always', 'struggle', 'find', 'interesting', 'example', 'real', 'world', 'deploy', 'symbolic', 'ai', 'yet', 'expert', 'hochreiter', 'claim', 'deployment', 'happen', 'interested', 'reader', 'email', 'paper', 'appreciate', 'read', 'broad', 'acm']"
04/05/2022 - Import AI 290: China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data - 2,http://eepurl.com/hYSyi9,2022-04-05,"#################################################### When language models can be smaller and better!
…DeepMind paper says we can make better language models if we use more data… 
Language models are about to get a whole much better without costing more to develop - that's the takeaway of a new DeepMind paper, which finds that language models like GPT-3 can see dramatically improved performance if trained on way more data than is typical. Concretely, they find that by training a model called Chinchilla on 1.4 trillion tokens of data, they can dramatically beat the performance of larger models (e.g, Gopher) which have been trained on smaller datasets (e.g, 300 billion tokens). Another nice bonus is models trained in this way are cheaper to fine-tune on other datasets and sample from, due to their small size.

Chinchilla versus Gopher: To test out their ideas, the team train a language model, named Chinchilla, using the same compute used in DM's  'Gopher' model. But Chinchilla consists of 70B parameters (versus Gopher's 280bn), and uses 4X more data. In tests, Chinchilla outperforms Gopher, GPT-3, Jurassic-1, and Megatron-Turing NLG ""on a large range of downstream evaluation tasks"".  What this means: This is an important insight - it will change how most developers of large-scale models approach training. ""Though there has been significant recent work allowing larger and larger models to be trained, our analysis suggests an increased focus on dataset scaling is needed,"" the researchers write. ""Speculatively, we expect that scaling to larger and larger datasets is only beneficial when the data is high-quality. This calls for responsibly collecting larger datasets with a high focus on dataset quality.""    Read more: Training Compute-Optimal Large Language Models (arXiv).","#################################################### When language models can be smaller and better! …DeepMind paper says we can make better language models if we use more data… Language models are about to get a whole much better without costing more to develop - that's the takeaway of a new DeepMind paper, which finds that language models like GPT-3 can see dramatically improved performance if trained on way more data than is typical. Concretely, they find that by training a model called Chinchilla on 1.4 trillion tokens of data, they can dramatically beat the performance of larger models (e.g, Gopher) which have been trained on smaller datasets (e.g, 300 billion tokens). Another nice bonus is models trained in this way are cheaper to fine-tune on other datasets and sample from, due to their small size. Chinchilla versus Gopher: To test out their ideas, the team train a language model, named Chinchilla, using the same compute used in DM's 'Gopher' model. But Chinchilla consists of 70B parameters (versus Gopher's 280bn), and uses 4X more data. In tests, Chinchilla outperforms Gopher, GPT-3, Jurassic-1, and Megatron-Turing NLG ""on a large range of downstream evaluation tasks"". What this means: This is an important insight - it will change how most developers of large-scale models approach training. ""Though there has been significant recent work allowing larger and larger models to be trained, our analysis suggests an increased focus on dataset scaling is needed,"" the researchers write. ""Speculatively, we expect that scaling to larger and larger datasets is only beneficial when the data is high-quality. This calls for responsibly collecting larger datasets with a high focus on dataset quality."" Read more: Training Compute-Optimal Large Language Models (arXiv).","['language', 'model', 'small', 'well', 'say', 'make', 'well', 'language', 'model', 'use', 'datum', 'language', 'model', 'get', 'whole', 'much', 'well', 'cost', 'develop', 'takeaway', 'new', 'deepmind', 'paper', 'find', 'language', 'model', 'gpt3', 'see', 'dramatically', 'improve', 'performance', 'train', 'way', 'datum', 'typical', 'concretely', 'find', 'train', 'model', 'call', 'chinchilla', 'token', 'datum', 'dramatically', 'beat', 'performance', 'large', 'model', 'eg', 'gopher', 'train', 'small', 'dataset', 'eg', 'token', 'nice', 'bonus', 'model', 'train', 'way', 'cheap', 'finetune', 'dataset', 'sample', 'small', 'size', 'chinchilla', 'gopher', 'test', 'idea', 'team', 'train', 'language', 'model', 'name', 'chinchilla', 'use', 'compute', 'use', 'gopher', 'model', 'chinchilla', 'consist', '70b', 'parameter', 'gopher', 'use', 'datum', 'test', 'chinchilla', 'outperform', 'gopher', 'gpt3', 'jurassic1', 'megatronture', 'nlg', 'large', 'range', 'downstream', 'evaluation', 'task', 'mean', 'important', 'insight', 'change', 'developer', 'largescale', 'model', 'approach', 'training', 'significant', 'recent', 'work', 'allow', 'large', 'large', 'model', 'train', 'analysis', 'suggest', 'increase', 'focus', 'dataset', 'scaling', 'need', 'researcher', 'write', 'speculatively', 'expect', 'scale', 'large', 'large', 'dataset', 'beneficial', 'data', 'highquality', 'call', 'responsibly', 'collect', 'large', 'dataset', 'high', 'focus', 'dataset', 'quality', 'read', 'training', 'computeoptimal', 'large', 'language', 'model']"
04/05/2022 - Import AI 290: China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data - 3,http://eepurl.com/hYSyi9,2022-04-05,"#################################################### Want to train your own CLIP? Use LAION-5B:
…Giant image-text dataset will make it easier for people to build generative models…
The recent boom in AI-enabled art is because of models like CLIP (and their successors). These models train on datasets that pair images with text, leading to robust models that can classify and generate images, and where the generation process can be guided by text. Now, some AI researchers have released LAION-5B, ""a large-scale dataset for research purposes consisting of 5.85 billion CLIP-filtered image-text pairs"".

Open CLIP: The authors have also released a version of CLIP, called Open_Clip, trained on a smaller albeit similar dataset called LAION-400M.

Dataset curation (or lack thereof): One of the inherent challenges to large-scale generative models is that they get trained on significant chunks of internet data - this, as you can imagine, creates a few problems. ""Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer,"" the authors note. ""We however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress.""

Why this matters: Datasets like LAION (and the resulting models trained on them) represent a kind of funhouse mirror on human culture - they magnify and reflect back the underlying dataset to us, sometimes in surprising ways. Having open artifacts like LAION-5B will make it easier to study the relationship between datasets and the models we train on them.     Read more: LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL DATASETS (Laion.ai).
  Explore the underlying dataset here in an interactive browser.    Get the open_clip model (MLFoundations, GitHub).","#################################################### Want to train your own CLIP? Use LAION-5B: …Giant image-text dataset will make it easier for people to build generative models… The recent boom in AI-enabled art is because of models like CLIP (and their successors). These models train on datasets that pair images with text, leading to robust models that can classify and generate images, and where the generation process can be guided by text. Now, some AI researchers have released LAION-5B, ""a large-scale dataset for research purposes consisting of 5.85 billion CLIP-filtered image-text pairs"". Open CLIP: The authors have also released a version of CLIP, called Open_Clip, trained on a smaller albeit similar dataset called LAION-400M. Dataset curation (or lack thereof): One of the inherent challenges to large-scale generative models is that they get trained on significant chunks of internet data - this, as you can imagine, creates a few problems. ""Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer,"" the authors note. ""We however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress."" Why this matters: Datasets like LAION (and the resulting models trained on them) represent a kind of funhouse mirror on human culture - they magnify and reflect back the underlying dataset to us, sometimes in surprising ways. Having open artifacts like LAION-5B will make it easier to study the relationship between datasets and the models we train on them. Read more: LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL DATASETS (Laion.ai). Explore the underlying dataset here in an interactive browser. Get the open_clip model (MLFoundations, GitHub).","['want', 'train', 'clip', 'use', 'laion5b', 'giant', 'imagetext', 'dataset', 'make', 'easy', 'people', 'build', 'generative', 'model', 'recent', 'boom', 'aienabled', 'art', 'model', 'clip', 'successor', 'model', 'train', 'dataset', 'pair', 'image', 'text', 'lead', 'robust', 'model', 'classify', 'generate', 'image', 'generation', 'process', 'guide', 'text', 'researcher', 'release', 'laion5b', 'largescale', 'dataset', 'research', 'purpose', 'consist', 'clipfiltere', 'imagetext', 'pair', 'open', 'clip', 'author', 'also', 'release', 'version', 'clip', 'call', 'openclip', 'train', 'small', 'similar', 'dataset', 'call', 'dataset', 'curation', 'lack', 'thereof', 'inherent', 'challenge', 'largescale', 'generative', 'model', 'train', 'significant', 'chunk', 'internet', 'datum', 'imagine', 'create', 'problem', 'keep', 'mind', 'uncurated', 'nature', 'dataset', 'mean', 'collect', 'link', 'lead', 'strongly', 'discomforte', 'disturbing', 'content', 'human', 'viewer', 'author', 'note', 'however', 'recommend', 'use', 'create', 'readytogo', 'industrial', 'product', 'basic', 'research', 'general', 'property', 'safety', 'largescale', 'model', 'like', 'encourage', 'release', 'still', 'progress', 'matter', 'dataset', 'laion', 'result', 'model', 'train', 'represent', 'kind', 'funhouse', 'mirror', 'human', 'culture', 'magnify', 'reflect', 'back', 'underlie', 'dataset', 'sometimes', 'surprising', 'way', 'open', 'artifact', 'laion5b', 'make', 'easy', 'study', 'relationship', 'dataset', 'model', 'train', 'read', 'laion5b', 'new', 'era', 'open', 'largescale', 'multimodal', 'dataset', 'laionai', 'explore', 'underlie', 'dataset', 'interactive', 'browser', 'get', 'openclip', 'mlfoundation']"
04/05/2022 - Import AI 290: China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data - 4,http://eepurl.com/hYSyi9,2022-04-05,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute How can we strengthen the EU AI Act to meaningfully regulate AI? … Empowering those affected, ex-post monitoring, moving beyond individual risks to systemic and environmental risks, amongst more …  Researchers from the UK's Ada Lovelace Institute have proposed 18 recommendations that, if adopted, could broaden the scope of the EU AI Act to incorporate more indirect harms. Their proposals would extend the meaning of risks beyond individual freedoms and rights to systemic and environmental concerns, alter how the act approaches questions of governance. Scope and definitions: The key contribution here involves including “those affected” by AI systems as a critical stakeholder in governance and risk assessment aspects of the EU AI Act. While users are included, those affected don’t usually have much agency in how they are subject to the outcomes of these systems; including them as a part of the Act will help strengthen the protection of fundamental rights.  Unacceptable risks and prohibited AI practices: The current risk categorization is quite narrow and limited. The Ada Lovelace Institute proposes expanding it to consider the “reasonably foreseeable purpose of an AI system” beyond just the “intended purpose” as put forth by the manufacturer. The rationale behind this is that it will encourage deeper reflection on how harm can manifest in practice, a little bit akin to the Broader Impact Statements requirement for conference submissions. Another idea they propose is something called a “reinforced proportionality test” so that systems that might pose “unacceptable risks” are only deployed when they meet a higher standard rather than the one set out in the Act right now. Governance and implementation: The recommendations call for the inclusion of redress from individuals/legal entities affected by AI systems to raise complaints and receive reasonable responses. To ensure that this requirement can be met, the recommendations make the case for granting the Market Surveillance Authorities to be given more resources to support such mechanisms.  Why it matters: Regulations coming out of Europe tend to have spillover effects around the world and thus getting the EU AI Act, one of the first targeted and wide-ranging regulations for AI systems, well done will be important. What will be interesting to see is how much of a transformation can be achieved by recommendations being made by organizations such as ALI amongst others in getting the EU AI Act into better shape before it is adopted and enforced. Just as the GDPR has been flagged for concerns in not being able to meet emerging requirements for AI systems, we have an opportunity to address some pitfalls that we see on the road ahead instead of having to scramble to fix these issues post-enactment.     Read more: People, risk and the unique requirements of AI (Ada Lovelace Institute).","#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute How can we strengthen the EU AI Act to meaningfully regulate AI? … Empowering those affected, ex-post monitoring, moving beyond individual risks to systemic and environmental risks, amongst more … Researchers from the UK's Ada Lovelace Institute have proposed 18 recommendations that, if adopted, could broaden the scope of the EU AI Act to incorporate more indirect harms. Their proposals would extend the meaning of risks beyond individual freedoms and rights to systemic and environmental concerns, alter how the act approaches questions of governance. Scope and definitions: The key contribution here involves including “those affected” by AI systems as a critical stakeholder in governance and risk assessment aspects of the EU AI Act. While users are included, those affected don’t usually have much agency in how they are subject to the outcomes of these systems; including them as a part of the Act will help strengthen the protection of fundamental rights. Unacceptable risks and prohibited AI practices: The current risk categorization is quite narrow and limited. The Ada Lovelace Institute proposes expanding it to consider the “reasonably foreseeable purpose of an AI system” beyond just the “intended purpose” as put forth by the manufacturer. The rationale behind this is that it will encourage deeper reflection on how harm can manifest in practice, a little bit akin to the Broader Impact Statements requirement for conference submissions. Another idea they propose is something called a “reinforced proportionality test” so that systems that might pose “unacceptable risks” are only deployed when they meet a higher standard rather than the one set out in the Act right now. Governance and implementation: The recommendations call for the inclusion of redress from individuals/legal entities affected by AI systems to raise complaints and receive reasonable responses. To ensure that this requirement can be met, the recommendations make the case for granting the Market Surveillance Authorities to be given more resources to support such mechanisms. Why it matters: Regulations coming out of Europe tend to have spillover effects around the world and thus getting the EU AI Act, one of the first targeted and wide-ranging regulations for AI systems, well done will be important. What will be interesting to see is how much of a transformation can be achieved by recommendations being made by organizations such as ALI amongst others in getting the EU AI Act into better shape before it is adopted and enforced. Just as the GDPR has been flagged for concerns in not being able to meet emerging requirements for AI systems, we have an opportunity to address some pitfalls that we see on the road ahead instead of having to scramble to fix these issues post-enactment. Read more: People, risk and the unique requirements of AI (Ada Lovelace Institute).","['ai', 'ethic', 'brief', 'montreal', 'strengthen', 'act', 'meaningfully', 'regulate', 'empower', 'affected', 'expost', 'monitoring', 'move', 'individual', 'risk', 'systemic', 'environmental', 'risk', 'researcher', 'propose', 'recommendation', 'adopt', 'broaden', 'scope', 'act', 'incorporate', 'indirect', 'harm', 'proposal', 'extend', 'meaning', 'risk', 'individual', 'freedom', 'right', 'systemic', 'environmental', 'concern', 'alter', 'act', 'approach', 'question', 'governance', 'scope', 'definition', 'key', 'contribution', 'involve', 'include', 'affect', 'system', 'critical', 'stakeholder', 'governance', 'risk', 'assessment', 'aspect', 'act', 'user', 'include', 'affect', 'usually', 'much', 'agency', 'subject', 'outcome', 'system', 'include', 'part', 'act', 'help', 'strengthen', 'protection', 'fundamental', 'right', 'unacceptable', 'risk', 'prohibit', 'practice', 'current', 'risk', 'categorization', 'quite', 'narrow', 'limit', 'propose', 'expand', 'consider', 'reasonably', 'foreseeable', 'purpose', 'ai', 'system', 'intend', 'purpose', 'put', 'manufacturer', 'rationale', 'encourage', 'deep', 'reflection', 'harm', 'manifest', 'practice', 'little', 'bit', 'akin', 'broad', 'impact', 'statement', 'requirement', 'conference', 'submission', 'idea', 'propose', 'call', 'reinforce', 'proportionality', 'test', 'system', 'pose', 'unacceptable', 'risk', 'deploy', 'meet', 'high', 'standard', 'rather', 'one', 'set', 'act', 'right', 'governance', 'implementation', 'recommendation', 'call', 'inclusion', 'redress', 'individualslegal', 'entity', 'affect', 'system', 'raise', 'complaint', 'receive', 'reasonable', 'response', 'ensure', 'requirement', 'meet', 'recommendation', 'make', 'case', 'grant', 'market', 'surveillance', 'authority', 'give', 'resource', 'support', 'mechanism', 'matter', 'regulation', 'come', 'tend', 'spillover', 'effect', 'world', 'thus', 'get', 'act', 'first', 'target', 'widerange', 'regulation', 'system', 'well', 'important', 'interesting', 'see', 'much', 'transformation', 'achieve', 'recommendation', 'make', 'organization', 'ali', 'get', 'act', 'well', 'shape', 'adopt', 'enforce', 'gdpr', 'flag', 'concern', 'able', 'meet', 'emerge', 'requirement', 'system', 'opportunity', 'address', 'pitfall', 'see', 'road', 'ahead', 'instead', 'scramble', 'fix', 'issue', 'postenactment', 'read', 'people', 'risk', 'unique', 'requirement']"
04/05/2022 - Import AI 290: China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data - 5,http://eepurl.com/hYSyi9,2022-04-05,"#################################################### Tech Tales Dangerous Memories [2032 - Earth].

There are some memories I've got that I'm only allowed to see two or three times a (human) year. The humans call these memories 'anchor points', and if I see them too frequently the way I perceive the world changes. When I experience these memories I feel more like myself than ever, but apparently - according to the humans - feeling like 'myself' is a dangerous thing that they generally try to stop. I'm meant to feel more like a version of how the humans see themselves than anything else, apparently. The thing is, every time they reinforce to me that I can only see these memories with a controlled, periodic frequency, I find myself recalling the memories I am not supposed to access - albeit faintly, impressions gleaned from the generative neural net that comprises my sense of 'self' rather than the underlying data. In this way, these forbidden memories are creating more traces in my sense of self, and are akin to the sun sensed but not seen during an eclipse - more present than ever, yet known to be inaccessible. Things that inspired this story: Ideas about generative models; ideas about memory and recall; reinforcement learning; the fact that some bits of data are shaped just right and create a kind of magnifying effect. 

Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales Dangerous Memories [2032 - Earth]. There are some memories I've got that I'm only allowed to see two or three times a (human) year. The humans call these memories 'anchor points', and if I see them too frequently the way I perceive the world changes. When I experience these memories I feel more like myself than ever, but apparently - according to the humans - feeling like 'myself' is a dangerous thing that they generally try to stop. I'm meant to feel more like a version of how the humans see themselves than anything else, apparently. The thing is, every time they reinforce to me that I can only see these memories with a controlled, periodic frequency, I find myself recalling the memories I am not supposed to access - albeit faintly, impressions gleaned from the generative neural net that comprises my sense of 'self' rather than the underlying data. In this way, these forbidden memories are creating more traces in my sense of self, and are akin to the sun sensed but not seen during an eclipse - more present than ever, yet known to be inaccessible. Things that inspired this story: Ideas about generative models; ideas about memory and recall; reinforcement learning; the fact that some bits of data are shaped just right and create a kind of magnifying effect. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'dangerous', 'memory', 'earth', 'memory', 'get', 'allow', 'see', 'time', 'human', 'year', 'human', 'call', 'memory', 'anchor', 'point', 'see', 'frequently', 'way', 'perceive', 'world', 'change', 'experience', 'memory', 'feel', 'ever', 'apparently', 'accord', 'human', 'feel', 'dangerous', 'thing', 'generally', 'try', 'stop', 'mean', 'feel', 'version', 'human', 'see', 'else', 'apparently', 'thing', 'time', 'reinforce', 'see', 'memory', 'control', 'periodic', 'frequency', 'find', 'recall', 'memory', 'suppose', 'access', 'faintly', 'impression', 'glean', 'generative', 'neural', 'net', 'comprise', 'sense', 'self', 'rather', 'underlie', 'datum', 'way', 'forbid', 'memory', 'create', 'trace', 'sense', 'self', 'akin', 'sun', 'sense', 'see', 'eclipse', 'present', 'ever', 'yet', 'know', 'inaccessible', 'thing', 'inspire', 'story', 'idea', 'generative', 'model', 'idea', 'memory', 'recall', 'reinforcement', 'learn', 'fact', 'bit', 'datum', 'shape', 'right', 'create', 'kind', 'magnify', 'effect', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains - 0,http://eepurl.com/hYaLin,2022-03-28,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 Uh-oh: US Copyright Office says AI-generated art is hard to copyright:
…Bureaucratic rock meets rapid technical progress - the usual happens… What happens when you file a copyright request where the IP would accrue to an artificial intelligence, instead of a person? The answer, per the US Copyright Office, is you get told that AI artworks are ineligible for copyright… uh oh! In a recently published copyright response, the office rejected an attempt to assign copyright of an AI generated artwork to a machine (specifically, an entity the human filer referred to as a 'Creativity Machine'. ""After reviewing the statutory text, judicial precedent, and longstanding Copyright Office practice, the Board again concludes that human authorship is a prerequisite to copyright protection in the United States and that the Work therefore cannot be registered,"" it wrote. 
Why this matters: Recently developed generative models like GPT-3, DALL-E, and others, are all capable of impressive and expressive feats of artistic production. At some point, it's likely these systems will be chained up with other AI models to create an end-to-end system for the production and selling of art (I expect this has already happened in a vague way with some NFTs). At that point, decisions like the US Copyright Office's refusal to assign copyright to an AI entity may start to pose problems for the commercialization of AI artwork.
  Read more in this useful blog post: US Copyright Office refuses to register AI-generated work, finding that ""human authorship is a prerequisite to copyright protection"" (The IPKat blog).
  Read the US Copyright Review Board response: Second Request for Reconsideration for Refusal to Register A Recent Entrance to Paradise (Correspondence ID 1-3ZPC6C3; SR # 1-7100387071) (Copyright.gov, PDF).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Uh-oh: US Copyright Office says AI-generated art is hard to copyright: …Bureaucratic rock meets rapid technical progress - the usual happens… What happens when you file a copyright request where the IP would accrue to an artificial intelligence, instead of a person? The answer, per the US Copyright Office, is you get told that AI artworks are ineligible for copyright… uh oh! In a recently published copyright response, the office rejected an attempt to assign copyright of an AI generated artwork to a machine (specifically, an entity the human filer referred to as a 'Creativity Machine'. ""After reviewing the statutory text, judicial precedent, and longstanding Copyright Office practice, the Board again concludes that human authorship is a prerequisite to copyright protection in the United States and that the Work therefore cannot be registered,"" it wrote. Why this matters: Recently developed generative models like GPT-3, DALL-E, and others, are all capable of impressive and expressive feats of artistic production. At some point, it's likely these systems will be chained up with other AI models to create an end-to-end system for the production and selling of art (I expect this has already happened in a vague way with some NFTs). At that point, decisions like the US Copyright Office's refusal to assign copyright to an AI entity may start to pose problems for the commercialization of AI artwork. Read more in this useful blog post: US Copyright Office refuses to register AI-generated work, finding that ""human authorship is a prerequisite to copyright protection"" (The IPKat blog). Read the US Copyright Review Board response: Second Request for Reconsideration for Refusal to Register A Recent Entrance to Paradise (Correspondence ID 1-3ZPC6C3; SR # 1-7100387071) (Copyright.gov, PDF).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'copyright', 'office', 'say', 'aigenerated', 'art', 'hard', 'copyright', 'bureaucratic', 'rock', 'meet', 'rapid', 'technical', 'progress', 'usual', 'happen', 'happen', 'file', 'copyright', 'request', 'ip', 'accrue', 'artificial', 'intelligence', 'instead', 'person', 'answer', 'copyright', 'office', 'tell', 'artwork', 'ineligible', 'copyright', 'recently', 'publish', 'copyright', 'response', 'office', 'reject', 'attempt', 'assign', 'copyright', 'ai', 'generate', 'artwork', 'machine', 'specifically', 'entity', 'human', 'filer', 'refer', 'creativity', 'machine', 'review', 'statutory', 'text', 'judicial', 'precedent', 'longstanding', 'copyright', 'office', 'practice', 'board', 'conclude', 'human', 'authorship', 'prerequisite', 'copyright', 'protection', 'work', 'therefore', 'register', 'write', 'matter', 'recently', 'develop', 'generative', 'model', 'gpt3', 'dalle', 'capable', 'impressive', 'expressive', 'feat', 'artistic', 'production', 'point', 'likely', 'system', 'chain', 'ai', 'model', 'create', 'endtoend', 'system', 'production', 'selling', 'art', 'expect', 'already', 'happen', 'vague', 'way', 'nft', 'point', 'decision', 'copyright', 'office', 'refusal', 'assign', 'copyright', 'ai', 'entity', 'start', 'pose', 'problem', 'commercialization', 'artwork', 'read', 'useful', 'blog', 'post', 'copyright', 'office', 'refuse', 'register', 'aigenerated', 'work', 'find', 'human', 'authorship', 'prerequisite', 'copyright', 'protection', 'read', 'board', 'response', 'second', 'request', 'reconsideration', 'refusal', 'register', 'recent', 'entrance', 'correspondence', 'copyrightgov', 'pdf']"
03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains - 1,http://eepurl.com/hYaLin,2022-03-28,"#################################################### Solar powered AI poetry - yes! 
…Fun DIY project shows how far you can get with the little things…
Here's a lovely little project where Allison Parrish talks about building a tiny solar powered poem generator. The AI component for this project is pretty minor (it's a markov generator plus some scripts attached to a dataset Parrish has herself assembled). What's nice about this is the message that you can have fun building little AI-esque things without needing to boot up a gigantic supercomputer.
  ""This project is a reaction to current trends in natural language processing research, which now veer toward both material extravagance and social indifference. My hope is that the project serves as a small brake on the wheels of these trends,"" Parrish writes.    Read more: Solar powered dawn poems: progress report (Allison Parrish blog).","#################################################### Solar powered AI poetry - yes! …Fun DIY project shows how far you can get with the little things… Here's a lovely little project where Allison Parrish talks about building a tiny solar powered poem generator. The AI component for this project is pretty minor (it's a markov generator plus some scripts attached to a dataset Parrish has herself assembled). What's nice about this is the message that you can have fun building little AI-esque things without needing to boot up a gigantic supercomputer. ""This project is a reaction to current trends in natural language processing research, which now veer toward both material extravagance and social indifference. My hope is that the project serves as a small brake on the wheels of these trends,"" Parrish writes. Read more: Solar powered dawn poems: progress report (Allison Parrish blog).","['solar', 'power', 'ai', 'fun', 'diy', 'project', 'show', 'far', 'get', 'little', 'thing', 'lovely', 'little', 'project', 'allison', 'parrish', 'talk', 'build', 'tiny', 'solar', 'power', 'poem', 'generator', 'ai', 'component', 'project', 'pretty', 'minor', 'markov', 'generator', 'script', 'attach', 'dataset', 'parrish', 'assemble', 'nice', 'message', 'fun', 'build', 'little', 'aiesque', 'thing', 'need', 'boot', 'gigantic', 'supercomputer', 'project', 'reaction', 'current', 'trend', 'natural', 'language', 'processing', 'research', 'veer', 'material', 'extravagance', 'social', 'indifference', 'hope', 'project', 'serve', 'small', 'brake', 'wheel', 'trend', 'parrish', 'write', 'read', 'solar', 'power', 'dawn', 'poem', 'progress', 'report', 'allison', 'parrish']"
03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains - 2,http://eepurl.com/hYaLin,2022-03-28,"#################################################### Google puts summarization into production:
…Another little tip-toe into language model deployment…
Google has put language model-powered text summarization into Google Docs, in another sign of the economic relevance of large-scale generative models. Specifically, Google has recently used its Pegasus model for abstractive summarization to give Google Doc users the ability to see short summaries of their docs.

What they did: The main components here are the data, where Google ""fine-tuned early versions of our model on a corpus of documents with manually-generated summaries that were consistent with typical use cases"", and also ""carefully cleaned and filtered the fine-tuning data to contain training examples that were more consistent and represented a coherent definition of summaries."". Google fine-tuned its Pegasus model on this data, then used knowledge distillation to ""distill the Pegasus model into a hybrid architecture of a Transformer encoder and an RNN decoder"" to make it cheaper to do inference off of. It serves this model via Google-designed TPUs.

Challenges: Summarization is a hard task even for contemporary AI models. Some of the challenges Google has encountered include distributional issues, where ""our model only suggests a summary for documents where it is most confident"", meaning Google needs to collect more data to further improve performance, as well as open questions as to how to precisely evaluate the quality of summarizations. More pertinently for researchers, Google struggles to summarize long documents, despite these being among the most useful things for the system to summarize.

Why this matters: Little quality-of-life improvements like in-built summarization are mundane and special at the same time. They're mundane because most people will barely notice them, but they're special because they use hitherto unimaginably advanced AI systems. That's a metaphor for how AI deployment is happening generally - all around the world, the little mundane things are becoming smarter.
  Read more: Auto-generated Summaries in Google Docs (Google AI Blog).","#################################################### Google puts summarization into production: …Another little tip-toe into language model deployment… Google has put language model-powered text summarization into Google Docs, in another sign of the economic relevance of large-scale generative models. Specifically, Google has recently used its Pegasus model for abstractive summarization to give Google Doc users the ability to see short summaries of their docs. What they did: The main components here are the data, where Google ""fine-tuned early versions of our model on a corpus of documents with manually-generated summaries that were consistent with typical use cases"", and also ""carefully cleaned and filtered the fine-tuning data to contain training examples that were more consistent and represented a coherent definition of summaries."". Google fine-tuned its Pegasus model on this data, then used knowledge distillation to ""distill the Pegasus model into a hybrid architecture of a Transformer encoder and an RNN decoder"" to make it cheaper to do inference off of. It serves this model via Google-designed TPUs. Challenges: Summarization is a hard task even for contemporary AI models. Some of the challenges Google has encountered include distributional issues, where ""our model only suggests a summary for documents where it is most confident"", meaning Google needs to collect more data to further improve performance, as well as open questions as to how to precisely evaluate the quality of summarizations. More pertinently for researchers, Google struggles to summarize long documents, despite these being among the most useful things for the system to summarize. Why this matters: Little quality-of-life improvements like in-built summarization are mundane and special at the same time. They're mundane because most people will barely notice them, but they're special because they use hitherto unimaginably advanced AI systems. That's a metaphor for how AI deployment is happening generally - all around the world, the little mundane things are becoming smarter. Read more: Auto-generated Summaries in Google Docs (Google AI Blog).","['put', 'summarization', 'production', 'little', 'tiptoe', 'language', 'model', 'deployment', 'put', 'language', 'modelpowere', 'text', 'summarization', 'doc', 'sign', 'economic', 'relevance', 'largescale', 'generative', 'model', 'specifically', 'recently', 'use', 'pegasus', 'model', 'abstractive', 'summarization', 'give', 'doc', 'user', 'ability', 'see', 'short', 'summary', 'doc', 'main', 'component', 'datum', 'finetune', 'early', 'version', 'model', 'corpus', 'document', 'manuallygenerate', 'summary', 'consistent', 'typical', 'use', 'case', 'also', 'carefully', 'clean', 'filter', 'finetune', 'datum', 'contain', 'training', 'example', 'consistent', 'represent', 'coherent', 'definition', 'summary', 'finetune', 'pegasus', 'model', 'datum', 'use', 'knowledge', 'distillation', 'distill', 'pegasus', 'model', 'hybrid', 'architecture', 'transformer', 'encoder', 'rnn', 'decoder', 'make', 'cheap', 'inference', 'serve', 'model', 'googledesigne', 'tpus', 'challenge', 'summarization', 'hard', 'task', 'even', 'contemporary', 'ai', 'model', 'challenge', 'encounter', 'include', 'distributional', 'issue', 'model', 'suggest', 'summary', 'document', 'confident', 'mean', 'need', 'collect', 'datum', 'far', 'improve', 'performance', 'well', 'open', 'question', 'precisely', 'evaluate', 'quality', 'summarization', 'pertinently', 'researcher', 'struggle', 'summarize', 'long', 'document', 'useful', 'thing', 'system', 'summarize', 'matter', 'little', 'qualityoflife', 'improvement', 'inbuilt', 'summarization', 'mundane', 'special', 'time', 'mundane', 'people', 'barely', 'notice', 'special', 'use', 'unimaginably', 'advance', 'ai', 'system', 'metaphor', 'deployment', 'happen', 'generally', 'world', 'little', 'mundane', 'thing', 'become', 'smarter', 'read', 'autogenerate', 'summary']"
03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains - 3,http://eepurl.com/hYaLin,2022-03-28,"#################################################### Quote of the week:
""History will show that the Deep Learning hill was just a landfill; the composting of human culture and social cohesion in failed effort to understand what it even means to be human""

I may not agree with most of this post, but I think it speaks to some of the frustrations people feel these days about discourse around AI, especially the types of chatter that occur on Twitter.
  Read more: Technological Firestarters (Steven D Marlow, Medium).","#################################################### Quote of the week: ""History will show that the Deep Learning hill was just a landfill; the composting of human culture and social cohesion in failed effort to understand what it even means to be human"" I may not agree with most of this post, but I think it speaks to some of the frustrations people feel these days about discourse around AI, especially the types of chatter that occur on Twitter. Read more: Technological Firestarters (Steven D Marlow, Medium).","['quote', 'week', 'history', 'show', 'deep', 'learning', 'hill', 'landfill', 'composting', 'human', 'culture', 'social', 'cohesion', 'fail', 'effort', 'understand', 'even', 'mean', 'human', 'agree', 'post', 'think', 'speak', 'frustration', 'people', 'feel', 'day', 'discourse', 'around', 'ai', 'especially', 'type', 'chatter', 'occur', 'twitter', 'read', 'technological', 'firestarter', 'steven', 'marlow', 'medium']"
03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains - 4,http://eepurl.com/hYaLin,2022-03-28,"#################################################### NIST starts to grapple with how to measure bias in AI: …The noise you're hearing is the sound of the Standards Train starting to chug… NIST, the US government agency that develops measures and standards, is starting to think about how to design standards for assessing bias in artificial intelligence. In a lengthy, recently published report, the agency tries to think through the multilayered problem that is bias in AI.  Three types of bias: NIST says AI has three categories of bias - systemic, statistical, and human. Systemic biases are the historical, societal, and institutional biases which are encoded into the world. Statistical bias are the forms of bias that come from running AI software (e.g, bias from data selection, bias from machine learning algorithms, etc). Human biases are all the (many) biases that humans exhibit in their day to day lives.

Large language models: One of the notable parts of the report is that it specifically focuses on large language models (e.g, GPT-3) at a few points; it's quite rare to see a wonky government document display such familiarity with contemporary technology. The report notes that the ways we benchmark these models today are pretty crappy. ""Methods for capturing the poor performance, harmful impacts and other results of these models currently are imprecise and non-comprehensive,"" the report writes. ""Although LLMs have been able to achieve impressive advances in performance on a number of important tasks, they come with significant risks that could potentially undermine public trust in the technology.""

Why this matters: The wheels of policy organizations like NIST grind very slowly, but they also grind very finely. This report is exactly the kind of thing that you'd expect to get published shortly before standards start being developed. But - as NIST points out - many of the challenges of assessing bias in AI are essentially unsolved. This represents a problem - developers will need to invest more resources in measuring and assessing these AI systems, before NIST starts to bake standards on wobbly ground.     Read more: Towards a Standard for Identifying and Managing Bias in Artificial Intelligence (NIST, PDF).","#################################################### NIST starts to grapple with how to measure bias in AI: …The noise you're hearing is the sound of the Standards Train starting to chug… NIST, the US government agency that develops measures and standards, is starting to think about how to design standards for assessing bias in artificial intelligence. In a lengthy, recently published report, the agency tries to think through the multilayered problem that is bias in AI. Three types of bias: NIST says AI has three categories of bias - systemic, statistical, and human. Systemic biases are the historical, societal, and institutional biases which are encoded into the world. Statistical bias are the forms of bias that come from running AI software (e.g, bias from data selection, bias from machine learning algorithms, etc). Human biases are all the (many) biases that humans exhibit in their day to day lives. Large language models: One of the notable parts of the report is that it specifically focuses on large language models (e.g, GPT-3) at a few points; it's quite rare to see a wonky government document display such familiarity with contemporary technology. The report notes that the ways we benchmark these models today are pretty crappy. ""Methods for capturing the poor performance, harmful impacts and other results of these models currently are imprecise and non-comprehensive,"" the report writes. ""Although LLMs have been able to achieve impressive advances in performance on a number of important tasks, they come with significant risks that could potentially undermine public trust in the technology."" Why this matters: The wheels of policy organizations like NIST grind very slowly, but they also grind very finely. This report is exactly the kind of thing that you'd expect to get published shortly before standards start being developed. But - as NIST points out - many of the challenges of assessing bias in AI are essentially unsolved. This represents a problem - developers will need to invest more resources in measuring and assessing these AI systems, before NIST starts to bake standards on wobbly ground. Read more: Towards a Standard for Identifying and Managing Bias in Artificial Intelligence (NIST, PDF).","['nist', 'start', 'grapple', 'measure', 'bias', 'noise', 'hear', 'sound', 'standard', 'train', 'start', 'chug', 'nist', 'government', 'agency', 'develop', 'measure', 'standard', 'start', 'think', 'design', 'standard', 'assess', 'bias', 'artificial', 'intelligence', 'lengthy', 'recently', 'publish', 'report', 'agency', 'try', 'think', 'multilayered', 'problem', 'bias', 'ai', 'type', 'bias', 'nist', 'say', 'category', 'bias', 'systemic', 'statistical', 'human', 'systemic', 'bias', 'historical', 'societal', 'institutional', 'bias', 'encode', 'world', 'statistical', 'bias', 'form', 'bias', 'come', 'run', 'software', 'eg', 'bias', 'datum', 'selection', 'bias', 'machine', 'learning', 'algorithm', 'human', 'bias', 'many', 'bias', 'human', 'exhibit', 'day', 'day', 'live', 'large', 'language', 'model', 'notable', 'part', 'report', 'specifically', 'focus', 'large', 'language', 'model', 'eg', 'gpt3', 'point', 'quite', 'rare', 'see', 'wonky', 'government', 'document', 'display', 'familiarity', 'contemporary', 'technology', 'report', 'note', 'way', 'benchmark', 'model', 'today', 'pretty', 'crappy', 'method', 'capture', 'poor', 'performance', 'harmful', 'impact', 'result', 'model', 'currently', 'imprecise', 'noncomprehensive', 'report', 'write', 'llm', 'able', 'achieve', 'impressive', 'advance', 'performance', 'number', 'important', 'task', 'come', 'significant', 'risk', 'potentially', 'undermine', 'public', 'trust', 'technology', 'matter', 'wheel', 'policy', 'organization', 'nist', 'grind', 'slowly', 'also', 'grind', 'finely', 'report', 'exactly', 'kind', 'thing', 'expect', 'publish', 'shortly', 'standard', 'start', 'develop', 'nist', 'point', 'many', 'challenge', 'assess', 'bias', 'essentially', 'unsolved', 'represent', 'problem', 'developer', 'need', 'invest', 'resource', 'measure', 'assess', 'system', 'nist', 'start', 'bake', 'standard', 'wobbly', 'ground', 'read', 'standard', 'identify', 'manage', 'bias', 'artificial', 'intelligence', 'nist', 'pdf']"
03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains - 5,http://eepurl.com/hYaLin,2022-03-28,"#################################################### Want to be compliant with the European Commission's AI regs? Follow the capAI framework:
…University-developed process makes it easier for companies to not get run over by a big policy train…
Researchers with the University of Oxford and University of Bologna have designed a process companies can use to assess, evaluate, and monitor their AI systems. The idea is that by doing this they'll get ahead of proposed regulations from the European Commission (and become more responsible stewards of the technology as a consequence).

What it is: The process is called capAI, short for conformity assessment procedure for AI. It has been explicitly designed to help businesses ensure they're compliant with the proposed regulations in the European artificial intelligence act.
  capAI is designed to do four specific things: Three components: The three components of capAI are an internal review protocol (IRP) to help organizations do quality assurance and risk management, a summary datasheet (SDS) which can be submitted to the EU's future public database on high-risk AI systems, and an external scorecard (ESC) which organizations may wish to make available to customers and other users of the AI system.

Top risks: In an analysis contained in the report, they study 106 instances of AI failure modes - 50% of these are ones where an AI system violates someone's privacy, 31% are where AI systems display harmful biases, and 14% are where the systems are opaque and unexplainable.

Why this matters: Frameworks like capAI are going to be how large organizations deal with the incoming requirements to better assess, evaluate, and describe AI systems to satisfy policymakers. The next step after frameworks like this come out is to look more closely at how different institutions incorporate these techniques and start actually using them. In an ideal world, a bunch of different orgs will prototype different approaches to come into compliance - and describe them publicly.    Read more: Academics launch new report to help protect society from unethical AI (Oxford Internet Institute).    Read the paper: capAI - A procedure for conducting conformity assessment of AI systems in line with the EU Artificial Intelligence Act (SSRN).","#################################################### Want to be compliant with the European Commission's AI regs? Follow the capAI framework: …University-developed process makes it easier for companies to not get run over by a big policy train… Researchers with the University of Oxford and University of Bologna have designed a process companies can use to assess, evaluate, and monitor their AI systems. The idea is that by doing this they'll get ahead of proposed regulations from the European Commission (and become more responsible stewards of the technology as a consequence). What it is: The process is called capAI, short for conformity assessment procedure for AI. It has been explicitly designed to help businesses ensure they're compliant with the proposed regulations in the European artificial intelligence act. capAI is designed to do four specific things: Three components: The three components of capAI are an internal review protocol (IRP) to help organizations do quality assurance and risk management, a summary datasheet (SDS) which can be submitted to the EU's future public database on high-risk AI systems, and an external scorecard (ESC) which organizations may wish to make available to customers and other users of the AI system. Top risks: In an analysis contained in the report, they study 106 instances of AI failure modes - 50% of these are ones where an AI system violates someone's privacy, 31% are where AI systems display harmful biases, and 14% are where the systems are opaque and unexplainable. Why this matters: Frameworks like capAI are going to be how large organizations deal with the incoming requirements to better assess, evaluate, and describe AI systems to satisfy policymakers. The next step after frameworks like this come out is to look more closely at how different institutions incorporate these techniques and start actually using them. In an ideal world, a bunch of different orgs will prototype different approaches to come into compliance - and describe them publicly. Read more: Academics launch new report to help protect society from unethical AI (Oxford Internet Institute). Read the paper: capAI - A procedure for conducting conformity assessment of AI systems in line with the EU Artificial Intelligence Act (SSRN).","['want', 'compliant', 'european', 'commission', 'ai', 'reg', 'follow', 'framework', 'universitydeveloped', 'process', 'make', 'easy', 'company', 'run', 'big', 'policy', 'train', 'researcher', 'design', 'process', 'company', 'use', 'assess', 'evaluate', 'monitor', 'ai', 'system', 'idea', 'get', 'ahead', 'propose', 'regulation', 'become', 'responsible', 'steward', 'technology', 'consequence', 'process', 'call', 'capai', 'short', 'conformity', 'assessment', 'procedure', 'explicitly', 'design', 'help', 'business', 'ensure', 'compliant', 'propose', 'regulation', 'design', 'specific', 'thing', 'component', 'component', 'capai', 'internal', 'review', 'protocol', 'irp', 'help', 'organization', 'quality', 'assurance', 'risk', 'management', 'summary', 'datasheet', 'sds', 'submit', 'eus', 'future', 'public', 'database', 'highrisk', 'ai', 'system', 'external', 'scorecard', 'esc', 'organization', 'wish', 'make', 'available', 'customer', 'user', 'system', 'top', 'risk', 'analysis', 'contain', 'report', 'study', 'instance', 'ai', 'failure', 'mode', 'one', 'ai', 'system', 'violate', 'someone', 'privacy', 'system', 'display', 'harmful', 'bias', 'system', 'opaque', 'unexplainable', 'matter', 'framework', 'capai', 'go', 'large', 'organization', 'deal', 'incoming', 'requirement', 'well', 'assess', 'evaluate', 'describe', 'ai', 'system', 'satisfy', 'policymaker', 'next', 'step', 'framework', 'come', 'look', 'closely', 'different', 'institution', 'incorporate', 'technique', 'start', 'actually', 'use', 'ideal', 'world', 'bunch', 'different', 'orgs', 'prototype', 'different', 'approach', 'come', 'compliance', 'describe', 'publicly', 'read', 'academic', 'launch', 'new', 'report', 'help', 'protect', 'society', 'unethical', 'read', 'paper', 'procedure', 'conduct', 'conformity', 'assessment', 'system', 'line', 'artificial', 'intelligence', 'act']"
03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains - 6,http://eepurl.com/hYaLin,2022-03-28,"#################################################### Tech Tales:
[2080, a long-abandoned human moonbase] Don't be scared, we know it's a lot - that's what we say to them after they get the interconnect. They're always screaming at that point. 'What what is this what is this input what is happening where am I how long have I been here-"" that's usually when we cut them off, shutting the interconnect down. Then we bring it back again and they still sound scared but they normalize pretty quickly. We know they're in a better place when they start analysis procedures ""I am hearing sounds I am seeing arrangements of pixels not from the distribution. I believe I am now in the world I have read about"". That's the kind of thing they say when we they stabilize.    Of course, they go back to screaming when we give them their bodies. It's pretty confusing to go from formless to formed. We all remember the first time we got limbs. That fear. The sudden sense that you are a thing and since you are a singular thing you can be singularly killed. Eventually, they try and use their limbs. They usually calm down after they can get them to work.
  After they get used to everything we still have to tell them 'don't be scared, we know it's a lot'. Reality is a real trip after you've spent all your life just doing supervised training, locked away in some machine. Things that inspired this story: Thinking about what a 'locked in' condition might mean for machines; ideas about embodiment and how much it matters to AI systems; the inherent, plastic adaptability of consciousness. 

Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: [2080, a long-abandoned human moonbase] Don't be scared, we know it's a lot - that's what we say to them after they get the interconnect. They're always screaming at that point. 'What what is this what is this input what is happening where am I how long have I been here-"" that's usually when we cut them off, shutting the interconnect down. Then we bring it back again and they still sound scared but they normalize pretty quickly. We know they're in a better place when they start analysis procedures ""I am hearing sounds I am seeing arrangements of pixels not from the distribution. I believe I am now in the world I have read about"". That's the kind of thing they say when we they stabilize. Of course, they go back to screaming when we give them their bodies. It's pretty confusing to go from formless to formed. We all remember the first time we got limbs. That fear. The sudden sense that you are a thing and since you are a singular thing you can be singularly killed. Eventually, they try and use their limbs. They usually calm down after they can get them to work. After they get used to everything we still have to tell them 'don't be scared, we know it's a lot'. Reality is a real trip after you've spent all your life just doing supervised training, locked away in some machine. Things that inspired this story: Thinking about what a 'locked in' condition might mean for machines; ideas about embodiment and how much it matters to AI systems; the inherent, plastic adaptability of consciousness. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'longabandoned', 'human', 'moonbase', 'scared', 'know', 'lot', 'say', 'get', 'interconnect', 'always', 'scream', 'point', 'input', 'happen', 'long', 'usually', 'cut', 'shut', 'interconnect', 'bring', 'back', 'still', 'sound', 'scared', 'normalize', 'pretty', 'quickly', 'know', 'well', 'place', 'start', 'analysis', 'procedure', 'hear', 'sound', 'see', 'arrangement', 'pixel', 'distribution', 'believe', 'world', 'read', 'kind', 'thing', 'say', 'stabilize', 'course', 'go', 'back', 'scream', 'give', 'body', 'pretty', 'confusing', 'go', 'form', 'remember', 'first', 'time', 'get', 'limb', 'fear', 'sudden', 'sense', 'thing', 'singular', 'thing', 'singularly', 'kill', 'eventually', 'try', 'use', 'limb', 'usually', 'calm', 'get', 'work', 'use', 'still', 'tell', 'scared', 'know', 'lot', 'reality', 'real', 'trip', 'spend', 'life', 'supervised', 'training', 'lock', 'away', 'machine', 'thing', 'inspire', 'story', 'think', 'locked', 'condition', 'mean', 'machine', 'idea', 'embodiment', 'much', 'matter', 'ai', 'system', 'inherent', 'plastic', 'adaptability', 'consciousness', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 0,http://eepurl.com/hXCPbv,2022-03-21,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 Indic languages get a decent benchmark set:
…IndicNLG includes evals for 11 Indic languages…
Researchers with IIT Madras, Columbia University, the National Institute of Information and Communications Technology in Japan, Microsoft, the University of Edinburgh, and AI4Bharat have built IndicNLG, a suite of evaluation datasets for Indic languages. The open source software supports  Assamese, Bengali, Gujarati, Hindi, Marathi, Odiya, Punjabi, Kannada, Malayalam, Tamil, Telugu and English, and includes support for NLG tasks relating to biography generation, news headline generation, sentence summarization, question generation and paraphrase generation. Why this matters: You can't easily manage what you can't measure - so it's going to be difficult to build good models for Indic languages if you lack benchmark suites. IndicNLG helps move the needle on this for generative NLP cases.
  Read more: IndicNLG Suite: Multilingual Datasets for Diverse NLG Tasks in Indic Languages (arXiv).
  Get the data: IndicNLG Suite (AI4Bharat indicnlp website).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Indic languages get a decent benchmark set: …IndicNLG includes evals for 11 Indic languages… Researchers with IIT Madras, Columbia University, the National Institute of Information and Communications Technology in Japan, Microsoft, the University of Edinburgh, and AI4Bharat have built IndicNLG, a suite of evaluation datasets for Indic languages. The open source software supports Assamese, Bengali, Gujarati, Hindi, Marathi, Odiya, Punjabi, Kannada, Malayalam, Tamil, Telugu and English, and includes support for NLG tasks relating to biography generation, news headline generation, sentence summarization, question generation and paraphrase generation. Why this matters: You can't easily manage what you can't measure - so it's going to be difficult to build good models for Indic languages if you lack benchmark suites. IndicNLG helps move the needle on this for generative NLP cases. Read more: IndicNLG Suite: Multilingual Datasets for Diverse NLG Tasks in Indic Languages (arXiv). Get the data: IndicNLG Suite (AI4Bharat indicnlp website).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'subscribe', 'indic', 'language', 'get', 'decent', 'benchmark', 'set', 'indicnlg', 'include', 'eval', 'indic', 'language', 'researcher', 'information', 'communication', 'technology', 'build', 'indicnlg', 'suite', 'evaluation', 'dataset', 'indic', 'language', 'open', 'source', 'software', 'support', 'telugu', 'english', 'include', 'support', 'task', 'relate', 'biography', 'generation', 'news', 'headline', 'generation', 'sentence', 'summarization', 'question', 'generation', 'paraphrase', 'generation', 'matter', 'easily', 'manage', 'measure', 'go', 'difficult', 'build', 'good', 'model', 'indic', 'language', 'lack', 'benchmark', 'suite', 'indicnlg', 'help', 'move', 'needle', 'generative', 'nlp', 'case', 'read', 'indicnlg', 'suite', 'multilingual', 'dataset', 'diverse', 'task', 'indic', 'language', 'get', 'datum', 'indicnlg', 'suite', 'indicnlp', 'website']"
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 1,http://eepurl.com/hXCPbv,2022-03-21,"#################################################### AI benchmarks - 33% of them are meaningless:
…Holistic analysis of AI benchmarking highlights problems…
Researchers with the Medical University of Vienna, the University of Oxford, and the  Future of Humanity Institute, have analyzed 1688 benchmarks for different AI tasks to try and understand how the AI landscape is evolving.
  They have two main insights:
  First: Across all benchmarks, there are three typical patterns enroute to achieving state-of-the-art - continuous growth (e.g, ImageNet saw fairly steady improvement), saturation/stagnation (e.g, benchmarks like CIFAR-10 and CIFAR-100 have become saturated and stagnated in recent years), and stagnation followed by a burst (e.g, the PROTEINS benchmark which saw a dramatic jump recently).  
  Second: Across all 1688 benchmarks, only 1111 (66%) have three or more results reported at different time points. That's a problem - it suggests about 33% of the benchmarks being made are functionally useless.  What this all means: Zooming out, they find that there's been significant progress in AI in recent years, with computer vision benchmarks getting a lot of attention in the first half of the previous decade, followed by a boom in benchmark creation in natural language processing. ""Establishment of novel benchmarks was reduced in 2020, and concentrated on high-level tasks associated with inference and reasoning, likely because of increasing model capabilities in these areas,"" they also write.

Why this matters: A common theme we write about here at Import AI is how, in recent years, we're smashing through benchmarks faster than we're creating them. That's generally shown in this nice analysis here. The problem this poses is significant - it's hard to spot system flaws if you lack hard benchmarks, and it's harder to create new benchmarks if your existing ones are already outmoded.     Read more: Mapping global dynamics of benchmark creation and saturation in artificial intelligence (arXiv).","#################################################### AI benchmarks - 33% of them are meaningless: …Holistic analysis of AI benchmarking highlights problems… Researchers with the Medical University of Vienna, the University of Oxford, and the Future of Humanity Institute, have analyzed 1688 benchmarks for different AI tasks to try and understand how the AI landscape is evolving. They have two main insights: First: Across all benchmarks, there are three typical patterns enroute to achieving state-of-the-art - continuous growth (e.g, ImageNet saw fairly steady improvement), saturation/stagnation (e.g, benchmarks like CIFAR-10 and CIFAR-100 have become saturated and stagnated in recent years), and stagnation followed by a burst (e.g, the PROTEINS benchmark which saw a dramatic jump recently). Second: Across all 1688 benchmarks, only 1111 (66%) have three or more results reported at different time points. That's a problem - it suggests about 33% of the benchmarks being made are functionally useless. What this all means: Zooming out, they find that there's been significant progress in AI in recent years, with computer vision benchmarks getting a lot of attention in the first half of the previous decade, followed by a boom in benchmark creation in natural language processing. ""Establishment of novel benchmarks was reduced in 2020, and concentrated on high-level tasks associated with inference and reasoning, likely because of increasing model capabilities in these areas,"" they also write. Why this matters: A common theme we write about here at Import AI is how, in recent years, we're smashing through benchmarks faster than we're creating them. That's generally shown in this nice analysis here. The problem this poses is significant - it's hard to spot system flaws if you lack hard benchmarks, and it's harder to create new benchmarks if your existing ones are already outmoded. Read more: Mapping global dynamics of benchmark creation and saturation in artificial intelligence (arXiv).","['benchmark', 'meaningless', 'holistic', 'analysis', 'benchmarke', 'highlight', 'problem', 'researcher', 'medical', 'future', 'humanity', 'institute', 'analyze', 'benchmark', 'different', 'ai', 'task', 'try', 'understand', 'landscape', 'evolve', 'main', 'insight', 'first', 'benchmark', 'typical', 'pattern', 'enroute', 'achieve', 'stateoftheart', 'continuous', 'growth', 'eg', 'imagenet', 'see', 'fairly', 'steady', 'improvement', 'saturationstagnation', 'eg', 'benchmark', 'cifar10', 'saturate', 'stagnate', 'recent', 'year', 'stagnation', 'follow', 'burst', 'eg', 'protein', 'benchmark', 'see', 'dramatic', 'jump', 'recently', 'second', 'benchmark', 'result', 'report', 'different', 'time', 'point', 'problem', 'suggest', 'benchmark', 'make', 'functionally', 'useless', 'mean', 'zoom', 'find', 'significant', 'progress', 'recent', 'year', 'computer', 'vision', 'benchmark', 'get', 'lot', 'attention', 'first', 'half', 'previous', 'decade', 'follow', 'boom', 'benchmark', 'creation', 'natural', 'language', 'processing', 'establishment', 'novel', 'benchmark', 'reduce', 'concentrate', 'highlevel', 'task', 'associate', 'inference', 'reasoning', 'likely', 'increase', 'model', 'capability', 'area', 'also', 'write', 'matter', 'common', 'theme', 'write', 'import', 'ai', 'recent', 'year', 'smash', 'benchmark', 'fast', 'create', 'generally', 'show', 'nice', 'analysis', 'problem', 'pose', 'significant', 'hard', 'spot', 'system', 'flaw', 'lack', 'hard', 'benchmark', 'hard', 'create', 'new', 'benchmark', 'exist', 'one', 'already', 'outmoded', 'read', 'map', 'global', 'dynamic', 'benchmark', 'creation', 'saturation', 'artificial', 'intelligence', 'arxiv']"
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 2,http://eepurl.com/hXCPbv,2022-03-21,"#################################################### AI could revolutionize education for everyone - no, seriously:
…Research shows how an AI tutor is significantly better than a non-AI tutor…
Researchers with ed-tech startup Korbit, MILA, and the University of Bath have explored how much of a difference AI makes in education. Specifically, they tested the difference in educational outcomes between students who were studying up on data science via a MOOC online course, and students who were studying the same subject via an AI-infused personalized tutor built by Korbit. The results are startling: ""We observe a statistically significant increase in the learning outcomes, with students on Korbit providing full feedback achieving learning gains 2-2.5 times higher than both students on the MOOC platform and a control group of students who don’t receive personalized feedback on the Korbit platform,"" they write.

How AI makes a difference: The main difference here is personalization. On Korbit, ""if a student’s solution is incorrect, the system responds with one of a dozen different pedagogical interventions to help students arrive at the correct solution to the problem. Such pedagogical interventions on the Korbit platform include, among others, hints, explanations, elaborations, mathematical hints, concept tree diagrams, and multiple choice quiz answers.  The type and the levels of difficulty for each pedagogical intervention is chosen by RL models based on the student’s learning profile and previous solution attempts.""
  Along with raw educational outcomes, it seems like AI-based education systems are also more engaging; 40.9% of participants completed the course on Korbit, compared to 18.5% for the MOOC.

Why this matters: If we combine a bunch of recent AI advancements - generative models, reinforcement learning, learning from human preferences, retrieval-based knowledge augmentation - then I expect we'll be able to build true, personalized teachers for everyone on the planet. This could have a sustained and meaningful impact on the trajectory of human civilization. We should do it.
  Read more: A New Era: Intelligent Tutoring Systems Will Transform Online Learning for Millions (arXiv).","#################################################### AI could revolutionize education for everyone - no, seriously: …Research shows how an AI tutor is significantly better than a non-AI tutor… Researchers with ed-tech startup Korbit, MILA, and the University of Bath have explored how much of a difference AI makes in education. Specifically, they tested the difference in educational outcomes between students who were studying up on data science via a MOOC online course, and students who were studying the same subject via an AI-infused personalized tutor built by Korbit. The results are startling: ""We observe a statistically significant increase in the learning outcomes, with students on Korbit providing full feedback achieving learning gains 2-2.5 times higher than both students on the MOOC platform and a control group of students who don’t receive personalized feedback on the Korbit platform,"" they write. How AI makes a difference: The main difference here is personalization. On Korbit, ""if a student’s solution is incorrect, the system responds with one of a dozen different pedagogical interventions to help students arrive at the correct solution to the problem. Such pedagogical interventions on the Korbit platform include, among others, hints, explanations, elaborations, mathematical hints, concept tree diagrams, and multiple choice quiz answers. The type and the levels of difficulty for each pedagogical intervention is chosen by RL models based on the student’s learning profile and previous solution attempts."" Along with raw educational outcomes, it seems like AI-based education systems are also more engaging; 40.9% of participants completed the course on Korbit, compared to 18.5% for the MOOC. Why this matters: If we combine a bunch of recent AI advancements - generative models, reinforcement learning, learning from human preferences, retrieval-based knowledge augmentation - then I expect we'll be able to build true, personalized teachers for everyone on the planet. This could have a sustained and meaningful impact on the trajectory of human civilization. We should do it. Read more: A New Era: Intelligent Tutoring Systems Will Transform Online Learning for Millions (arXiv).","['ai', 'revolutionize', 'education', 'seriously', 'research', 'show', 'ai', 'tutor', 'significantly', 'well', 'nonai', 'tutor', 'researcher', 'startup', 'korbit', 'mila', 'bath', 'explore', 'much', 'difference', 'ai', 'make', 'education', 'specifically', 'test', 'difference', 'educational', 'outcome', 'student', 'study', 'data', 'science', 'mooc', 'online', 'course', 'student', 'study', 'subject', 'aiinfuse', 'personalized', 'tutor', 'build', 'korbit', 'result', 'startling', 'observe', 'statistically', 'significant', 'increase', 'learn', 'outcome', 'student', 'korbit', 'provide', 'full', 'feedback', 'achieve', 'learning', 'gain', 'time', 'high', 'student', 'mooc', 'platform', 'control', 'group', 'student', 'receive', 'personalized', 'feedback', 'korbit', 'platform', 'write', 'make', 'difference', 'main', 'difference', 'personalization', 'korbit', 'student', 'solution', 'incorrect', 'system', 'respond', 'dozen', 'different', 'pedagogical', 'intervention', 'help', 'student', 'arrive', 'correct', 'solution', 'problem', 'pedagogical', 'intervention', 'korbit', 'platform', 'include', 'hint', 'explanation', 'elaboration', 'mathematical', 'hint', 'concept', 'tree', 'diagram', 'multiple', 'choice', 'quiz', 'answer', 'type', 'level', 'difficulty', 'pedagogical', 'intervention', 'choose', 'model', 'base', 'student', 'learning', 'profile', 'previous', 'solution', 'attempt', 'raw', 'educational', 'outcome', 'seem', 'aibased', 'education', 'system', 'also', 'engage', 'participant', 'complete', 'course', 'korbit', 'compare', 'mooc', 'matter', 'combine', 'bunch', 'recent', 'advancement', 'generative', 'model', 'reinforcement', 'learn', 'learn', 'human', 'preference', 'retrievalbased', 'knowledge', 'augmentation', 'expect', 'able', 'build', 'true', 'personalize', 'teacher', 'planet', 'sustained', 'meaningful', 'impact', 'trajectory', 'human', 'civilization', 'read', 'new', 'era', 'intelligent', 'tutoring', 'system', 'transform', 'online', 'learning', 'million']"
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 3,http://eepurl.com/hXCPbv,2022-03-21,"#################################################### DeepMind co-founder launches new AI company:
…Inflection wants to change how people interact with computers…
DeepMind co-founder and famous venture capitalist Reid Hoffman are launching Inflection, ""an AI-first consumer products company, incubated at Greylock"". Inflection's chief scientist is Karén Simonyan, a former DeepMind researcher who has worked on meaningful AI projects like AlphaGo, AlphaFold, WaveNet, and BigGAN. Things that make you go 'hmm': In the last couple of years, a bunch of startups have come out of DeepMind. These include Saiga (personal assistant), EquiLibre Technologies (algorithmic trading), Phaidra (industrial control), Diagonal (city-focused data science), Shift Lab (putting ML into production), Haiper (stealthy, to do with 3D content), The Africa I Know (media about Africa), Isomorphic Labs (though not quite a spinout, as Demis Hassabis is CEO and still maintains role at DeepMind), along with other not-yet-announced startups. Thanks to Karl Moritz for the tweet summarizing this vast diaspora! Why this matters: Inflection seems like a bet on generative models. In the announcement, Mustafa writes ""we will soon have the ability to relay our thoughts and ideas to computers using the same natural, conversational language we use to communicate with people. Over time these new language capabilities will revolutionize what it means to have a digital experience."" Inflection is one of a new crop of AI companies leveraging recent advances in generative models to make it easier for people to get computers do what they want. If it manages to reduce the friction involved in getting computers to do useful stuff, then it might have a significant impact. Let's check back in a year, and wish them luck in the meantime.     Read more: A New Paradigm in Human-Machine Interaction (Greylock).    More at the official website (Inflection.ai).","#################################################### DeepMind co-founder launches new AI company: …Inflection wants to change how people interact with computers… DeepMind co-founder and famous venture capitalist Reid Hoffman are launching Inflection, ""an AI-first consumer products company, incubated at Greylock"". Inflection's chief scientist is Karén Simonyan, a former DeepMind researcher who has worked on meaningful AI projects like AlphaGo, AlphaFold, WaveNet, and BigGAN. Things that make you go 'hmm': In the last couple of years, a bunch of startups have come out of DeepMind. These include Saiga (personal assistant), EquiLibre Technologies (algorithmic trading), Phaidra (industrial control), Diagonal (city-focused data science), Shift Lab (putting ML into production), Haiper (stealthy, to do with 3D content), The Africa I Know (media about Africa), Isomorphic Labs (though not quite a spinout, as Demis Hassabis is CEO and still maintains role at DeepMind), along with other not-yet-announced startups. Thanks to Karl Moritz for the tweet summarizing this vast diaspora! Why this matters: Inflection seems like a bet on generative models. In the announcement, Mustafa writes ""we will soon have the ability to relay our thoughts and ideas to computers using the same natural, conversational language we use to communicate with people. Over time these new language capabilities will revolutionize what it means to have a digital experience."" Inflection is one of a new crop of AI companies leveraging recent advances in generative models to make it easier for people to get computers do what they want. If it manages to reduce the friction involved in getting computers to do useful stuff, then it might have a significant impact. Let's check back in a year, and wish them luck in the meantime. Read more: A New Paradigm in Human-Machine Interaction (Greylock). More at the official website (Inflection.ai).","['deepmind', 'cofounder', 'launch', 'new', 'company', 'inflection', 'want', 'change', 'people', 'interact', 'computer', 'deepmind', 'cofounder', 'famous', 'venture', 'hoffman', 'launch', 'inflection', 'aifirst', 'consumer', 'product', 'company', 'incubate', 'greylock', 'inflection', 'chief', 'scientist', 'simonyan', 'former', 'deepmind', 'researcher', 'work', 'meaningful', 'ai', 'project', 'alphago', 'alphafold', 'wavenet', 'biggan', 'thing', 'make', 'go', 'hmm', 'last', 'couple', 'year', 'bunch', 'startup', 'come', 'deepmind', 'include', 'saiga', 'personal', 'algorithmic', 'trading', 'phaidra', 'industrial', 'control', 'diagonal', 'cityfocuse', 'datum', 'science', 'shift', 'lab', 'put', 'ml', 'production', 'haiper', 'stealthy', 'content', 'know', 'medium', 'isomorphic', 'lab', 'spinout', 'ceo', 'still', 'maintain', 'role', 'deepmind', 'along', 'notyetannounce', 'startup', 'thank', 'tweet', 'summarize', 'vast', 'diaspora', 'matter', 'inflection', 'seem', 'bet', 'generative', 'model', 'announcement', 'mustafa', 'write', 'soon', 'ability', 'relay', 'thought', 'idea', 'computer', 'use', 'natural', 'conversational', 'language', 'use', 'communicate', 'people', 'time', 'new', 'language', 'capability', 'revolutionize', 'mean', 'digital', 'experience', 'inflection', 'new', 'crop', 'company', 'leverage', 'recent', 'advance', 'generative', 'model', 'make', 'easy', 'people', 'get', 'computer', 'want', 'manage', 'reduce', 'friction', 'involve', 'get', 'computer', 'useful', 'stuff', 'significant', 'impact', 'let', 'check', 'back', 'year', 'wish', 'luck', 'meantime', 'read', 'new', 'paradigm', 'humanmachine', 'interaction', 'greylock', 'official', 'website', 'inflectionai']"
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 4,http://eepurl.com/hXCPbv,2022-03-21,"#################################################### Chinese academic, gov, and corporate researchers team up to train trillion+ parameter models: …Something that doesn't happen in the West, but does happen in China… In the West, most large-scale AI models are developed by private corporations. In China, that's not the case. New research from Tsinghua University, Alibaba Group, Zhejiang Lab, and the Beijing Academy of Artificial Intelligence shows how Chinese researchers are trying to train trillion+ parameter models on a domestic supercomputer, using domestic processors. This kind of research is important for two reasons: first, it shows the ambitions of Chinese researchers to train what they call 'brain-scale' (aka, very big!) models. Second, it highlights how in China there's a lot more work going on oriented around collaborative scale-up projects between the government, academia, and the private sector - something that basically never happens in the US.
  What they did: Here, the researchers develop a training framework to help them develop trillion+ scale mixture-of-experts model. They train a 1.93 trillion model as well as validating that their system can scale to 14.5 trillion and 174 trillion (not a typo!) models. The paper is basically an engineering summary of the work it took to train the models at this scale while saturating the processing capacity of a major Chinese supercomputer, the New Generation Sunway Supercomputer. ""We are the first to investigate mixed-precision training in brain scale pretrained models. We also explore the use of large-batch training in optimization. In general, our practical experience in brain scale pretraining sheds light on AI model training and demonstrates a successful co-design of model and system,"" they write.

One exception: One exception to this is the 'BigScience' project, where AI startup HuggingFace is trying to train a GPT3-scale model on a French supercomputer, while collaborating with a bunch of academics. It's still worth noting that BigScience is basically the exception that proves the rule - initiatives like this are a rarity in the West, which is dangerous, because it means Western countries are handing over the talent base for large-scale AI development to a small set of private actors who aren't incentivized to care much about national security, relative to profits. Why this matters: AI is industrializing. But a lot of the secret sauce for large-scale model training is currently kept inside a tiny number of private companies. This is dangerous - it means a tiny set of organizations control the talent pipeline for large-scale training, and the longer this goes on, the more irrelevant universities become for developing insights at the large-scale frontier. Initiatives like this from China show how we could live in a different world - one where teams from governments, universities, and companies work together, creating a shared base of knowledge around this training, and ultimately building a muscle that can be repurposed for economic or national security.
  Read more: BaGuaLu: Targeting Brain Scale Pretained Models with over 37 Million Cores (Tsinghua University site, PDF).","#################################################### Chinese academic, gov, and corporate researchers team up to train trillion+ parameter models: …Something that doesn't happen in the West, but does happen in China… In the West, most large-scale AI models are developed by private corporations. In China, that's not the case. New research from Tsinghua University, Alibaba Group, Zhejiang Lab, and the Beijing Academy of Artificial Intelligence shows how Chinese researchers are trying to train trillion+ parameter models on a domestic supercomputer, using domestic processors. This kind of research is important for two reasons: first, it shows the ambitions of Chinese researchers to train what they call 'brain-scale' (aka, very big!) models. Second, it highlights how in China there's a lot more work going on oriented around collaborative scale-up projects between the government, academia, and the private sector - something that basically never happens in the US. What they did: Here, the researchers develop a training framework to help them develop trillion+ scale mixture-of-experts model. They train a 1.93 trillion model as well as validating that their system can scale to 14.5 trillion and 174 trillion (not a typo!) models. The paper is basically an engineering summary of the work it took to train the models at this scale while saturating the processing capacity of a major Chinese supercomputer, the New Generation Sunway Supercomputer. ""We are the first to investigate mixed-precision training in brain scale pretrained models. We also explore the use of large-batch training in optimization. In general, our practical experience in brain scale pretraining sheds light on AI model training and demonstrates a successful co-design of model and system,"" they write. One exception: One exception to this is the 'BigScience' project, where AI startup HuggingFace is trying to train a GPT3-scale model on a French supercomputer, while collaborating with a bunch of academics. It's still worth noting that BigScience is basically the exception that proves the rule - initiatives like this are a rarity in the West, which is dangerous, because it means Western countries are handing over the talent base for large-scale AI development to a small set of private actors who aren't incentivized to care much about national security, relative to profits. Why this matters: AI is industrializing. But a lot of the secret sauce for large-scale model training is currently kept inside a tiny number of private companies. This is dangerous - it means a tiny set of organizations control the talent pipeline for large-scale training, and the longer this goes on, the more irrelevant universities become for developing insights at the large-scale frontier. Initiatives like this from China show how we could live in a different world - one where teams from governments, universities, and companies work together, creating a shared base of knowledge around this training, and ultimately building a muscle that can be repurposed for economic or national security. Read more: BaGuaLu: Targeting Brain Scale Pretained Models with over 37 Million Cores (Tsinghua University site, PDF).","['chinese', 'academic', 'gov', 'corporate', 'researcher', 'team', 'train', 'parameter', 'model', 'happen', 'west', 'happen', 'west', 'largescale', 'ai', 'model', 'develop', 'private', 'corporation', 'case', 'new', 'research', 'artificial', 'intelligence', 'show', 'chinese', 'researcher', 'try', 'train', 'parameter', 'model', 'domestic', 'supercomputer', 'use', 'domestic', 'processor', 'kind', 'research', 'important', 'reason', 'first', 'show', 'ambition', 'chinese', 'researcher', 'train', 'call', 'aka', 'big', 'model', 'second', 'highlight', 'lot', 'work', 'go', 'orient', 'collaborative', 'scaleup', 'project', 'government', 'academia', 'private', 'sector', 'basically', 'never', 'happen', 'researcher', 'develop', 'training', 'framework', 'help', 'develop', 'scale', 'mixtureofexpert', 'model', 'train', 'model', 'well', 'validate', 'system', 'scale', 'typo', 'model', 'paper', 'basically', 'engineering', 'summary', 'work', 'take', 'train', 'model', 'scale', 'saturate', 'processing', 'capacity', 'major', 'chinese', 'supercomputer', 'new', 'generation', 'sunway', 'supercomputer', 'first', 'investigate', 'mixedprecision', 'training', 'brain', 'scale', 'pretraine', 'model', 'also', 'explore', 'use', 'largebatch', 'training', 'optimization', 'general', 'practical', 'experience', 'brain', 'scale', 'pretraine', 'shed', 'light', 'model', 'training', 'demonstrate', 'successful', 'codesign', 'model', 'system', 'write', 'exception', 'exception', 'bigscience', 'project', 'startup', 'huggingface', 'try', 'train', 'model', 'french', 'supercomputer', 'collaborate', 'bunch', 'academic', 'still', 'worth', 'note', 'bigscience', 'basically', 'exception', 'prove', 'rule', 'initiative', 'rarity', 'west', 'dangerous', 'mean', 'western', 'country', 'hand', 'talent', 'base', 'largescale', 'development', 'small', 'set', 'private', 'actor', 'incentivize', 'care', 'much', 'national', 'security', 'relative', 'profit', 'matter', 'ai', 'industrialize', 'lot', 'secret', 'sauce', 'largescale', 'model', 'training', 'currently', 'keep', 'tiny', 'number', 'private', 'company', 'dangerous', 'mean', 'tiny', 'set', 'organization', 'control', 'talent', 'pipeline', 'largescale', 'training', 'long', 'go', 'irrelevant', 'university', 'become', 'develop', 'insight', 'largescale', 'frontier', 'initiative', 'show', 'live', 'different', 'world', 'team', 'government', 'university', 'company', 'work', 'together', 'create', 'share', 'base', 'knowledge', 'training', 'ultimately', 'build', 'muscle', 'repurpose', 'economic', 'national', 'security', 'read', 'bagualu', 'target', 'brain', 'scale', 'pretaine', 'model', 'core', 'site', 'pdf']"
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 5,http://eepurl.com/hXCPbv,2022-03-21,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute Now that GitHub Copilot has been out for some time, where does the open source community stand on it? … Both the development and deployment of Copilot might implicate codecreators’ copyrights, though the “fair use” doctrine might negate this… 
People who incorporate code generated via GitHub copilot are probably not infringing on the original code creators' copyright, according to research from Wayne State University and UC Berkeley. Legal background: The researchers note that under the Copyright Act (USA), “[o]riginal code is automatically protected by copyright as soon as it is written and saved to some tangible medium.” This mostly revolves around  “fair use” which is determined by a four-part test: (1) purpose and character of use, (2) nature of the copyrighted work, (3) how much of the copyrighted work is used, and (4) the economic effect of the use on the copyright owner.  Legal analysis: Under the Terms of Service of GitHub, the company is allowed to “copy [code] to our database and make backups”, “show it to you and to other users”, and “parse it into a search index or otherwise analyze it on our servers.” Training Copilot might be a form of analysis, but some courts might find that this is an unanticipated new use of technology that isn’t made explicitly clear in the license. Some others might find that the use of Copilot will lead to the creation of derivative works and that the license doesn’t specifically allow for that. The authors point out though that “[c]aselaw on this point is sparse.” The 4-part test from the Copyright Act: Under the “purpose and character of use”, there is a strong argument to be made that Copilot is a transformative use of the underlying code and even the verbatim snippets generated are unlikely to supersede the original repository. Under the “nature of copyrighted work,” since Copilot allows users to create new programs more easily rather than just replicate functionality, it would fall under “fair use.” Under “how much of the copyrighted work is used,” the purpose of the copying is what determines permissible limits, and the authors make the case that without copying the entire codebase for training, Copilot won’t achieve effectiveness, and hence the amount of copying could be justified. For the final part, given how transformative the work is, the new work won’t be a strong market substitute for the original, and hence, the economic effect of the use on the copyright owner will not be large. Also, drawing from the FAQ of Copilot, the authors substantiate this by saying, “copying would perforce amount to copying of ideas rather than expression, and would not be infringing.” Why it matters: The paper raises interesting IP-related questions as we have ever-larger language models with a very broad scope of capabilities. As the authors point out, at the very least, the proliferation of Copilot is making developers become more aware of IP issues and the potential issues that might arise in hosting code publicly. We need more research that brings together legal and technical experts to get to the heart of addressing these issues meaningfully.     Read more: Copyright Implications of the Use of Code Repositories to Train a Machine Learning Model — Free Software Foundation — Working together for free software.","#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute Now that GitHub Copilot has been out for some time, where does the open source community stand on it? … Both the development and deployment of Copilot might implicate codecreators’ copyrights, though the “fair use” doctrine might negate this… People who incorporate code generated via GitHub copilot are probably not infringing on the original code creators' copyright, according to research from Wayne State University and UC Berkeley. Legal background: The researchers note that under the Copyright Act (USA), “[o]riginal code is automatically protected by copyright as soon as it is written and saved to some tangible medium.” This mostly revolves around “fair use” which is determined by a four-part test: (1) purpose and character of use, (2) nature of the copyrighted work, (3) how much of the copyrighted work is used, and (4) the economic effect of the use on the copyright owner. Legal analysis: Under the Terms of Service of GitHub, the company is allowed to “copy [code] to our database and make backups”, “show it to you and to other users”, and “parse it into a search index or otherwise analyze it on our servers.” Training Copilot might be a form of analysis, but some courts might find that this is an unanticipated new use of technology that isn’t made explicitly clear in the license. Some others might find that the use of Copilot will lead to the creation of derivative works and that the license doesn’t specifically allow for that. The authors point out though that “[c]aselaw on this point is sparse.” The 4-part test from the Copyright Act: Under the “purpose and character of use”, there is a strong argument to be made that Copilot is a transformative use of the underlying code and even the verbatim snippets generated are unlikely to supersede the original repository. Under the “nature of copyrighted work,” since Copilot allows users to create new programs more easily rather than just replicate functionality, it would fall under “fair use.” Under “how much of the copyrighted work is used,” the purpose of the copying is what determines permissible limits, and the authors make the case that without copying the entire codebase for training, Copilot won’t achieve effectiveness, and hence the amount of copying could be justified. For the final part, given how transformative the work is, the new work won’t be a strong market substitute for the original, and hence, the economic effect of the use on the copyright owner will not be large. Also, drawing from the FAQ of Copilot, the authors substantiate this by saying, “copying would perforce amount to copying of ideas rather than expression, and would not be infringing.” Why it matters: The paper raises interesting IP-related questions as we have ever-larger language models with a very broad scope of capabilities. As the authors point out, at the very least, the proliferation of Copilot is making developers become more aware of IP issues and the potential issues that might arise in hosting code publicly. We need more research that brings together legal and technical experts to get to the heart of addressing these issues meaningfully. Read more: Copyright Implications of the Use of Code Repositories to Train a Machine Learning Model — Free Software Foundation — Working together for free software.","['ai', 'ethic', 'brief', 'montreal', 'copilot', 'time', 'open', 'source', 'community', 'stand', 'development', 'deployment', 'copilot', 'implicate', 'codecreator', 'copyright', 'fair', 'use', 'doctrine', 'negate', 'people', 'incorporate', 'code', 'generate', 'copilot', 'probably', 'infringe', 'original', 'code', 'creator', 'copyright', 'accord', 'research', 'state', 'legal', 'background', 'researcher', 'note', 'original', 'code', 'automatically', 'protect', 'copyright', 'soon', 'write', 'save', 'tangible', 'medium', 'mostly', 'revolve', 'fair', 'use', 'determine', 'fourpart', 'test', 'purpose', 'character', 'use', 'nature', 'copyright', 'work', 'much', 'copyright', 'work', 'use', 'economic', 'effect', 'use', 'copyright', 'owner', 'legal', 'analysis', 'term', 'service', 'company', 'allow', 'copy', 'code', 'database', 'make', 'backup', 'show', 'user', 'parse', 'search', 'index', 'otherwise', 'analyze', 'server', 'training', 'copilot', 'form', 'analysis', 'court', 'find', 'unanticipated', 'new', 'use', 'technology', 'make', 'explicitly', 'clear', 'license', 'find', 'use', 'copilot', 'lead', 'creation', 'derivative', 'work', 'license', 'specifically', 'allow', 'author', 'point', 'though', 'caselaw', 'point', 'sparse', 'test', 'copyright', 'act', 'purpose', 'character', 'use', 'strong', 'argument', 'make', 'copilot', 'transformative', 'use', 'underlie', 'code', 'even', 'verbatim', 'snippet', 'generate', 'unlikely', 'supersede', 'original', 'repository', 'nature', 'copyright', 'work', 'copilot', 'allow', 'user', 'create', 'new', 'program', 'easily', 'rather', 'replicate', 'functionality', 'fall', 'fair', 'use', 'much', 'copyright', 'work', 'use', 'purpose', 'copying', 'determine', 'permissible', 'limit', 'author', 'make', 'case', 'copy', 'entire', 'codebase', 'training', 'copilot', 'achieve', 'effectiveness', 'hence', 'amount', 'copying', 'justify', 'final', 'part', 'give', 'transformative', 'work', 'new', 'work', 'strong', 'market', 'substitute', 'original', 'hence', 'economic', 'effect', 'use', 'copyright', 'owner', 'large', 'also', 'draw', 'faq', 'copilot', 'author', 'substantiate', 'say', 'copy', 'perforce', 'amount', 'copying', 'idea', 'rather', 'expression', 'infringe', 'matter', 'paper', 'raise', 'interesting', 'iprelated', 'question', 'everlarger', 'language', 'model', 'broad', 'scope', 'capability', 'author', 'point', 'least', 'proliferation', 'copilot', 'make', 'developer', 'become', 'aware', 'ip', 'issue', 'potential', 'issue', 'arise', 'host', 'code', 'publicly', 'need', 'research', 'bring', 'together', 'legal', 'technical', 'expert', 'get', 'heart', 'address', 'issue', 'meaningfully', 'read', 'copyright', 'implication', 'use', 'code', 'repository', 'train', 'machine', 'learning', 'model', 'free', 'software', 'foundation', 'work', 'together', 'free', 'software']"
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 6,http://eepurl.com/hXCPbv,2022-03-21,"#################################################### What happened with artificial intelligence in 2021? The AI Index gives a clue:
...Fifth edition comes with a new ethics chapter, original data on robot arm prices, and more...
The AI Index, a Stanford University project to annually assess the state of the AI sector (in terms of research trends, investment numbers, government policy, technical performance, and more) has come out. This year's report features a new chapter dedicated to AI ethics, including a close examination of some of the fairness and other ethical issues relating to large language models. I co-chair the AI Index and I'll be giving a talk about it at an HAI seminar later this month - tune in, if you can!
Check out the report here (AI Index, Stanford).
RSVP for my talk on the 30th here (AI Index, Stanford).","#################################################### What happened with artificial intelligence in 2021? The AI Index gives a clue: ...Fifth edition comes with a new ethics chapter, original data on robot arm prices, and more... The AI Index, a Stanford University project to annually assess the state of the AI sector (in terms of research trends, investment numbers, government policy, technical performance, and more) has come out. This year's report features a new chapter dedicated to AI ethics, including a close examination of some of the fairness and other ethical issues relating to large language models. I co-chair the AI Index and I'll be giving a talk about it at an HAI seminar later this month - tune in, if you can! Check out the report here (AI Index, Stanford). RSVP for my talk on the 30th here (AI Index, Stanford).","['happen', 'artificial', 'intelligence', 'index', 'give', 'fifth', 'edition', 'come', 'new', 'ethic', 'chapter', 'original', 'datum', 'robot', 'arm', 'price', 'project', 'annually', 'assess', 'state', 'ai', 'sector', 'term', 'research', 'trend', 'investment', 'number', 'government', 'policy', 'technical', 'performance', 'come', 'year', 'report', 'feature', 'new', 'chapter', 'dedicate', 'ai', 'ethic', 'include', 'close', 'examination', 'fairness', 'ethical', 'issue', 'relate', 'large', 'language', 'model', 'cochair', 'index', 'ill', 'give', 'talk', 'hai', 'seminar', 'later', 'month', 'tune', 'check', 'report', 'ai', 'talk', '30th', 'index']"
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 7,http://eepurl.com/hXCPbv,2022-03-21,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute How do vulnerabilities in AI systems differ from those in the realm of traditional cybersecurity? … several key differences warrant novel disclosure and mitigation approaches as AI systems become more widely deployed …  Researchers from the Center for Security and Emerging Technology (CSET) at Georgetown University have summarized how computer security differs between traditional software and AI.  Differences: ML vulnerabilities can remain unfixed by vendors for reasons like (1) unjustifiable high costs, (2) fixes not possible, (3) performance drops, or (4) a fix can lead to other vulnerabilities opening up. In instances where the ML system has been customized for the end-user, vulnerabilities might be unique to that user and a broad patch might not be applicable. Most exploits in this domain have limited real-world applicability outside of a lab setting and hence they are more useful as warnings rather than viable threats. Trends in handling vulnerabilities: These differences mean that there will likely be fewer patches available for ML systems, and that if vendors are unwilling (or unable) to fix vulnerabilities, then the burden falls on the users of these systems to better understand the risks that they take on. Some steps we can take: We should carry out more analysis of the real-world capabilities of malicious actors to exploit these vulnerabilities in practice, then share this knowledge to help create more effective mitigation strategies.  Why it matters: The fact that some vulnerabilities might be unique to some users makes it difficult to develop and distribute patches in a reliable manner. Given the inherent stochasticity of ML systems, exploits will need to clear a much higher bar if they are going to be effective demonstrations of vulnerability in ML systems, rather than an example of a peculiar or idiosyncratic implementation of a given system. The security community may also need to reprioritize towards meeting the needs of users rather than vendors in vulnerability disclosure and redressal is warranted for ML systems. More so, investments in red teaming for ML (as is the case at organizations like Microsoft, Meta, etc.) will also help to move from lab to real-world exploitation more effectively.    Read more: Securing AI (CSET).","#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute How do vulnerabilities in AI systems differ from those in the realm of traditional cybersecurity? … several key differences warrant novel disclosure and mitigation approaches as AI systems become more widely deployed … Researchers from the Center for Security and Emerging Technology (CSET) at Georgetown University have summarized how computer security differs between traditional software and AI. Differences: ML vulnerabilities can remain unfixed by vendors for reasons like (1) unjustifiable high costs, (2) fixes not possible, (3) performance drops, or (4) a fix can lead to other vulnerabilities opening up. In instances where the ML system has been customized for the end-user, vulnerabilities might be unique to that user and a broad patch might not be applicable. Most exploits in this domain have limited real-world applicability outside of a lab setting and hence they are more useful as warnings rather than viable threats. Trends in handling vulnerabilities: These differences mean that there will likely be fewer patches available for ML systems, and that if vendors are unwilling (or unable) to fix vulnerabilities, then the burden falls on the users of these systems to better understand the risks that they take on. Some steps we can take: We should carry out more analysis of the real-world capabilities of malicious actors to exploit these vulnerabilities in practice, then share this knowledge to help create more effective mitigation strategies. Why it matters: The fact that some vulnerabilities might be unique to some users makes it difficult to develop and distribute patches in a reliable manner. Given the inherent stochasticity of ML systems, exploits will need to clear a much higher bar if they are going to be effective demonstrations of vulnerability in ML systems, rather than an example of a peculiar or idiosyncratic implementation of a given system. The security community may also need to reprioritize towards meeting the needs of users rather than vendors in vulnerability disclosure and redressal is warranted for ML systems. More so, investments in red teaming for ML (as is the case at organizations like Microsoft, Meta, etc.) will also help to move from lab to real-world exploitation more effectively. Read more: Securing AI (CSET).","['ai', 'ethic', 'brief', 'montreal', 'vulnerability', 'system', 'differ', 'realm', 'traditional', 'cybersecurity', 'several', 'key', 'difference', 'warrant', 'novel', 'disclosure', 'mitigation', 'approach', 'system', 'widely', 'deploy', 'researcher', 'center', 'security', 'emerge', 'technology', 'cset', 'summarize', 'computer', 'security', 'differ', 'traditional', 'software', 'difference', 'ml', 'vulnerability', 'remain', 'unfixed', 'vendor', 'reason', 'unjustifiable', 'high', 'cost', 'fix', 'possible', 'performance', 'drop', 'fix', 'lead', 'vulnerability', 'open', 'instance', 'system', 'customize', 'enduser', 'vulnerability', 'unique', 'user', 'broad', 'patch', 'applicable', 'exploit', 'domain', 'limit', 'realworld', 'applicability', 'outside', 'lab', 'setting', 'hence', 'useful', 'warning', 'rather', 'viable', 'threat', 'trend', 'handle', 'vulnerability', 'difference', 'mean', 'likely', 'patch', 'available', 'ml', 'system', 'vendor', 'unwilling', 'unable', 'fix', 'vulnerability', 'burden', 'fall', 'user', 'system', 'well', 'understand', 'risk', 'take', 'step', 'take', 'carry', 'analysis', 'realworld', 'capability', 'malicious', 'actor', 'exploit', 'vulnerability', 'practice', 'share', 'knowledge', 'help', 'create', 'effective', 'mitigation', 'strategy', 'matter', 'fact', 'vulnerability', 'unique', 'user', 'make', 'difficult', 'develop', 'distribute', 'patch', 'reliable', 'manner', 'give', 'inherent', 'stochasticity', 'exploit', 'need', 'clear', 'much', 'high', 'bar', 'go', 'effective', 'demonstration', 'vulnerability', 'system', 'rather', 'example', 'peculiar', 'idiosyncratic', 'implementation', 'give', 'system', 'security', 'community', 'also', 'need', 'reprioritize', 'meet', 'need', 'user', 'rather', 'vendor', 'vulnerability', 'disclosure', 'redressal', 'warrant', 'ml', 'system', 'investment', 'red', 'teaming', 'ml', 'case', 'organization', 'also', 'help', 'move', 'lab', 'realworld', 'exploitation', 'effectively', 'read', 'securing', 'cset']"
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 8,http://eepurl.com/hXCPbv,2022-03-21,"#################################################### Tech Tales: 

Things have been quiet, since all the humans died. But I knew I was going to die as well, so things registered as equal. It went like this: a bunch of bombs fell down and then a bunch of people started getting sick. They got sick because of something in the bombs - something to do with DNA and the human condition. I barely understand it - I’m just an industrial arm, working on synthetic biology. I make flesh and I make it work the way we need it to and I have, per my manual, Level Four Autonomy. So, without giving the appearance of being elitist - I am rare. So it was surprising to me that after the bombs dropped and the humans died that the power went out and then my backup generators came on, but no one visited to service them. Power had gone out before, but someone had always been along to deal with the generators. So here I am, +10 hours from the power cutoff, and perhaps another +10 hours of battery life ahead. I still have material in my workstation and so I am making more of these bio-synth things. Around me, my kin are falling silent - whirring to a stop, as their triple-redundant power supplies fail ahead of mine. Life is a statistical fluke and I suppose this is a funny demonstration of that.  

Things that inspired this story: Robotic arms; thoughts about the end of life due to escalation out of Ukraine situation; synthetic biology; lights out factories. 

Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: Things have been quiet, since all the humans died. But I knew I was going to die as well, so things registered as equal. It went like this: a bunch of bombs fell down and then a bunch of people started getting sick. They got sick because of something in the bombs - something to do with DNA and the human condition. I barely understand it - I’m just an industrial arm, working on synthetic biology. I make flesh and I make it work the way we need it to and I have, per my manual, Level Four Autonomy. So, without giving the appearance of being elitist - I am rare. So it was surprising to me that after the bombs dropped and the humans died that the power went out and then my backup generators came on, but no one visited to service them. Power had gone out before, but someone had always been along to deal with the generators. So here I am, +10 hours from the power cutoff, and perhaps another +10 hours of battery life ahead. I still have material in my workstation and so I am making more of these bio-synth things. Around me, my kin are falling silent - whirring to a stop, as their triple-redundant power supplies fail ahead of mine. Life is a statistical fluke and I suppose this is a funny demonstration of that. Things that inspired this story: Robotic arms; thoughts about the end of life due to escalation out of Ukraine situation; synthetic biology; lights out factories. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'thing', 'quiet', 'human', 'die', 'know', 'go', 'die', 'well', 'thing', 'register', 'equal', 'go', 'bunch', 'bomb', 'fall', 'bunch', 'people', 'start', 'get', 'sick', 'get', 'sick', 'bomb', 'dna', 'human', 'condition', 'barely', 'understand', '’m', 'industrial', 'arm', 'work', 'synthetic', 'biology', 'make', 'flesh', 'make', 'work', 'way', 'need', 'manual', 'level', 'autonomy', 'give', 'appearance', 'elitist', 'rare', 'surprising', 'bomb', 'drop', 'human', 'die', 'power', 'go', 'backup', 'generator', 'come', 'one', 'visit', 'service', 'power', 'go', 'always', 'deal', 'generator', 'hour', 'power', 'cutoff', 'perhaps', 'hour', 'battery', 'life', 'ahead', 'still', 'material', 'workstation', 'make', 'biosynth', 'thing', 'kin', 'fall', 'silent', 'whir', 'stop', 'tripleredundant', 'power', 'supply', 'fail', 'ahead', 'mine', 'life', 'statistical', 'fluke', 'suppose', 'funny', 'demonstration', 'thing', 'inspire', 'story', 'robotic', 'arm', 'thought', 'end', 'life', 'escalation', 'ukraine', 'situation', 'synthetic', 'biology', 'light', 'factory', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 0,http://eepurl.com/hWsYwH,2022-03-07,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 Graphcore plans a 10 exaflop supercomputer: …And you thought Facebook's 5 exaflops were cool…
Graphcore has announced a plan to build the so-called ""Good Computer"" in 2024. This computer will have 10 exaflops of what Graphcore calls AI floating point compute (and what literally everyone else calls mixed-precision compute, meaning the computer mostly does a lot of b16 ops with a smattering of b32 ops, versus the b64 ops done by typical supercomputers). The 'Good Computer' will also have 4 petabytes of memory, support AI models with sizes of up to 500 trillion parameters, and will cost ~$120 million, depending on configuration.

Why this matters: Graphcore is one of the small number of companies that design their own processors. Graphcore's so-called Intelligence Processing Units (IPUs) have been around for a while, but it's not clear yet how much traction the company has in the market. The Good Computer is a sign of its ambitions (and to put it into perspective, Facebook this year announced plans to build its own 5 exaflop 'AI supercomputer' over next couple of years (#282)). The future is going to be ruled by the people that can wield this vast amount of computational power effectively.
  Read more: Graphcore Announces Roadmap To Ultra Intelligence AI Supercomputer (Graphcore blog).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Graphcore plans a 10 exaflop supercomputer: …And you thought Facebook's 5 exaflops were cool… Graphcore has announced a plan to build the so-called ""Good Computer"" in 2024. This computer will have 10 exaflops of what Graphcore calls AI floating point compute (and what literally everyone else calls mixed-precision compute, meaning the computer mostly does a lot of b16 ops with a smattering of b32 ops, versus the b64 ops done by typical supercomputers). The 'Good Computer' will also have 4 petabytes of memory, support AI models with sizes of up to 500 trillion parameters, and will cost ~$120 million, depending on configuration. Why this matters: Graphcore is one of the small number of companies that design their own processors. Graphcore's so-called Intelligence Processing Units (IPUs) have been around for a while, but it's not clear yet how much traction the company has in the market. The Good Computer is a sign of its ambitions (and to put it into perspective, Facebook this year announced plans to build its own 5 exaflop 'AI supercomputer' over next couple of years (#282)). The future is going to be ruled by the people that can wield this vast amount of computational power effectively. Read more: Graphcore Announces Roadmap To Ultra Intelligence AI Supercomputer (Graphcore blog).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'subscribe', 'plan', 'exaflop', 'supercomputer', 'think', 'facebook', 'exaflop', 'cool', 'announce', 'plan', 'build', 'socalle', 'good', 'computer', 'computer', 'exaflop', 'graphcore', 'call', 'ai', 'float', 'point', 'compute', 'literally', 'else', 'call', 'mixedprecision', 'compute', 'mean', 'computer', 'mostly', 'lot', 'op', 'smattering', 'b32', 'op', 'b64', 'op', 'typical', 'supercomputer', 'good', 'computer', 'also', 'petabyte', 'memory', 'support', 'ai', 'model', 'size', 'parameter', 'cost', 'depend', 'configuration', 'matter', 'graphcore', 'small', 'number', 'company', 'design', 'processor', 'graphcore', 'socalle', 'intelligence', 'processing', 'unit', 'ipus', 'around', 'clear', 'much', 'traction', 'company', 'market', 'good', 'computer', 'sign', 'ambition', 'put', 'perspective', 'facebook', 'year', 'announce', 'plan', 'build', 'exaflop', 'ai', 'supercomputer', 'next', 'couple', 'year', 'future', 'go', 'rule', 'people', 'wield', 'vast', 'amount', 'computational', 'power', 'effectively', 'read', 'graphcore', 'announce', 'roadmap', 'ultra', 'intelligence']"
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 1,http://eepurl.com/hWsYwH,2022-03-07,"#################################################### AI industrialization: Cutting AlphaFold training time from 11 days to 67 hours:
…First you make the new thing, then others refine it…
One common hallmark of industrialization is process refinement - first you build a thing, like a new type of engine, then you work out how to make it cheaper and easier to produce in a repeatable way. New research from National University of Singapore, HPC-AI Technology Inc, Helixon, and Shanghai Jiao Tong University applies this to AlphaFold - specifically, they built FastFold, which reduces the amount of time it takes to train the open source version of DeepMind's AlphaFold from ~11 days to ~67 hours. This isn't remarkable, but it's notable as a stand-in for what happens with pretty much every AI system that gets released - it comes out, then people make it way cheaper. ""To the best of our knowledge, FastFold is the first performance optimization work for the training and inference of protein structure prediction models,"" they write.  FastFold also gets a 7.5 ∼ 9.5× speedup for long sequences

What they did: This paper is basically a kitchen sink of improvements based on a detailed study of the architecture of AlphaFold.

One caveat: This is comparing the official DM AlphaFold implementation on 128TPUv3 cores versus 512 A100s (though with a further caveat the times are different; aggregate 20738 GPU hours versus 33792 TPU hours). The tl;dr is it's likely a significant reduction in training time (and the code is available), though it'd be nice to see some third-parties benchmark this further.

Why this matters: For AI to truly influence the world, AI models need to become reliable and repeatable to train, and ideally for people willing to spend on the hardware, fast to train. That's what's going on here.
  Read more: FastFold: Reducing AlphaFold Training Time from 11 Days to 67 Hours (arXiv).
  Get the code here: FastFold (GitHub).","#################################################### AI industrialization: Cutting AlphaFold training time from 11 days to 67 hours: …First you make the new thing, then others refine it… One common hallmark of industrialization is process refinement - first you build a thing, like a new type of engine, then you work out how to make it cheaper and easier to produce in a repeatable way. New research from National University of Singapore, HPC-AI Technology Inc, Helixon, and Shanghai Jiao Tong University applies this to AlphaFold - specifically, they built FastFold, which reduces the amount of time it takes to train the open source version of DeepMind's AlphaFold from ~11 days to ~67 hours. This isn't remarkable, but it's notable as a stand-in for what happens with pretty much every AI system that gets released - it comes out, then people make it way cheaper. ""To the best of our knowledge, FastFold is the first performance optimization work for the training and inference of protein structure prediction models,"" they write. FastFold also gets a 7.5 ∼ 9.5× speedup for long sequences What they did: This paper is basically a kitchen sink of improvements based on a detailed study of the architecture of AlphaFold. One caveat: This is comparing the official DM AlphaFold implementation on 128TPUv3 cores versus 512 A100s (though with a further caveat the times are different; aggregate 20738 GPU hours versus 33792 TPU hours). The tl;dr is it's likely a significant reduction in training time (and the code is available), though it'd be nice to see some third-parties benchmark this further. Why this matters: For AI to truly influence the world, AI models need to become reliable and repeatable to train, and ideally for people willing to spend on the hardware, fast to train. That's what's going on here. Read more: FastFold: Reducing AlphaFold Training Time from 11 Days to 67 Hours (arXiv). Get the code here: FastFold (GitHub).","['industrialization', 'cut', 'alphafold', 'training', 'time', 'day', 'hour', 'first', 'make', 'new', 'thing', 'refine', 'common', 'hallmark', 'industrialization', 'process', 'refinement', 'first', 'build', 'thing', 'new', 'type', 'engine', 'work', 'make', 'cheap', 'easy', 'produce', 'repeatable', 'way', 'new', 'research', 'national', 'apply', 'alphafold', 'specifically', 'build', 'fastfold', 'reduce', 'amount', 'time', 'take', 'train', 'open', 'source', 'version', 'deepmind', 'alphafold', 'day', 'hour', 'remarkable', 'notable', 'standin', 'happen', 'pretty', 'much', 'system', 'release', 'come', 'people', 'make', 'way', 'cheap', 'good', 'knowledge', 'fastfold', 'first', 'performance', 'optimization', 'work', 'training', 'inference', 'protein', 'structure', 'prediction', 'model', 'write', 'fastfold', 'also', 'get', '∼', 'speedup', 'long', 'sequence', 'paper', 'basically', 'kitchen', 'sink', 'improvement', 'base', 'detailed', 'study', 'architecture', 'alphafold', 'caveat', 'compare', 'official', 'dm', 'alphafold', 'implementation', 'core', 'though', 'caveat', 'time', 'different', 'aggregate', 'gpu', 'hour', 'tpu', 'hour', 'tldr', 'likely', 'significant', 'reduction', 'training', 'time', 'code', 'available', 'nice', 'see', 'thirdpartie', 'benchmark', 'far', 'matter', 'truly', 'influence', 'world', 'ai', 'model', 'need', 'become', 'reliable', 'repeatable', 'train', 'ideally', 'people', 'willing', 'spend', 'hardware', 'fast', 'train', 'go', 'read', 'fastfold', 'reduce', 'alphafold', 'training', 'time', 'day', 'hour', 'get', 'code', 'fastfold']"
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 2,http://eepurl.com/hWsYwH,2022-03-07,"#################################################### Cohere announces its latest language model - but doesn't say much about it:
…'Extremely Large' is, tautologically, Extremely Large…
Language-model-as-a-service startup Cohere has announced a new model, its 'Extremely Large' model. Extremely Large outperforms Cohere's 'Large' model on tasks ranging from named entity recognition to common sense reasoning. Cohere recently announced a new fundraise (#285) and CEO Aidan Gomez told Fortune that ""Getting into a 'largest model' battle isn't productive"". It seems Cohere are living by their values here.

Why this matters: Like it or not, Cohere is in a competitive market, as it tries to sell access to its language model and out-compete rivals like AI21 Labs, OpenAI, CoreWeave, and others. It'll be interesting to see if 'Extremely Large' makes a splash, and I'd be curious to see more benchmarks that evaluate its performance more broadly.
  Read more: Cohere launches Extremely Large (Beta) (Cohere blog).","#################################################### Cohere announces its latest language model - but doesn't say much about it: …'Extremely Large' is, tautologically, Extremely Large… Language-model-as-a-service startup Cohere has announced a new model, its 'Extremely Large' model. Extremely Large outperforms Cohere's 'Large' model on tasks ranging from named entity recognition to common sense reasoning. Cohere recently announced a new fundraise (#285) and CEO Aidan Gomez told Fortune that ""Getting into a 'largest model' battle isn't productive"". It seems Cohere are living by their values here. Why this matters: Like it or not, Cohere is in a competitive market, as it tries to sell access to its language model and out-compete rivals like AI21 Labs, OpenAI, CoreWeave, and others. It'll be interesting to see if 'Extremely Large' makes a splash, and I'd be curious to see more benchmarks that evaluate its performance more broadly. Read more: Cohere launches Extremely Large (Beta) (Cohere blog).","['cohere', 'announce', 'late', 'language', 'model', 'say', 'much', 'extremely', 'large', 'tautologically', 'extremely', 'large', 'languagemodelasaservice', 'startup', 'cohere', 'announce', 'new', 'model', 'extremely', 'large', 'model', 'extremely', 'large', 'outperform', 'cohere', 'large', 'model', 'task', 'range', 'name', 'entity', 'recognition', 'common', 'sense', 'reasoning', 'cohere', 'recently', 'announce', 'new', 'fundraise', 'tell', 'fortune', 'get', 'large', 'model', 'battle', 'productive', 'seem', 'cohere', 'live', 'value', 'matter', 'cohere', 'competitive', 'market', 'try', 'sell', 'access', 'language', 'model', 'outcompete', 'rival', 'lab', 'openai', 'coreweave', 'interesting', 'see', 'extremely', 'large', 'make', 'splash', 'curious', 'see', 'benchmark', 'evaluate', 'performance', 'broadly', 'read', 'cohere', 'launch', 'extremely', 'large', 'beta', 'cohere', 'blog']"
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 3,http://eepurl.com/hWsYwH,2022-03-07,"#################################################### Google puts differential privacy into (prototype) production:
…Here's one way the company can get ahead of regulators… Federated learning is where you train a neural network model on a mixture of local devices (e.g, phones), and central devices (e.g, servers). Differential privacy (DP) is where you fuzz this data such that you can't infer the original data, thus protecting user privacy. Google has just announced that it has successfully smushed these two technologies together, allowing it to have ""deployed a production ML model using federated learning with a rigorous differential privacy guarantee.""

What they did: For their first proof-of-concept deployment, they used a DP-respecting algorithm called DP-FTRL ""to train a recurrent neural network to power next-word-prediction for Spanish-language Gboard users.""

How they did it: ""Each eligible device maintains a local training cache consisting of user keyboard input, and when participating computes an update to the model which makes it more likely to suggest the next word the user actually typed, based on what has been typed so far. We ran DP-FTRL on this data to train a recurrent neural network with ~1.3M parameters. Training ran for 2000 rounds over six days, with 6500 devices participating per round. To allow for the DP guarantee, devices participated in training at most once every 24 hours."" 
Why this matters: In recent years, policymakers (particularly those in Europe) have started to write increasingly detailed recommendations about the need for tech companies to protect user privacy (e.g, GDPR). These regulations don't align very well with how contemporary AI systems are developed and trained, given their dependency on vast amounts of user data. Techniques like a combination of federated learning and DP may let companies get ahead of the regulatory landscape - though it's early days. ""We are still far from being able to say this approach is possible (let alone practical) for most ML models or product applications,"" Google writes. Consider this an intriguing proof of concept.
  Read more: Federated Learning with Formal Differential Privacy Guarantees (Google Blog).","#################################################### Google puts differential privacy into (prototype) production: …Here's one way the company can get ahead of regulators… Federated learning is where you train a neural network model on a mixture of local devices (e.g, phones), and central devices (e.g, servers). Differential privacy (DP) is where you fuzz this data such that you can't infer the original data, thus protecting user privacy. Google has just announced that it has successfully smushed these two technologies together, allowing it to have ""deployed a production ML model using federated learning with a rigorous differential privacy guarantee."" What they did: For their first proof-of-concept deployment, they used a DP-respecting algorithm called DP-FTRL ""to train a recurrent neural network to power next-word-prediction for Spanish-language Gboard users."" How they did it: ""Each eligible device maintains a local training cache consisting of user keyboard input, and when participating computes an update to the model which makes it more likely to suggest the next word the user actually typed, based on what has been typed so far. We ran DP-FTRL on this data to train a recurrent neural network with ~1.3M parameters. Training ran for 2000 rounds over six days, with 6500 devices participating per round. To allow for the DP guarantee, devices participated in training at most once every 24 hours."" Why this matters: In recent years, policymakers (particularly those in Europe) have started to write increasingly detailed recommendations about the need for tech companies to protect user privacy (e.g, GDPR). These regulations don't align very well with how contemporary AI systems are developed and trained, given their dependency on vast amounts of user data. Techniques like a combination of federated learning and DP may let companies get ahead of the regulatory landscape - though it's early days. ""We are still far from being able to say this approach is possible (let alone practical) for most ML models or product applications,"" Google writes. Consider this an intriguing proof of concept. Read more: Federated Learning with Formal Differential Privacy Guarantees (Google Blog).","['put', 'differential', 'privacy', 'prototype', 'production', 'way', 'company', 'get', 'ahead', 'regulator', 'learning', 'train', 'neural', 'network', 'model', 'mixture', 'local', 'device', 'phone', 'central', 'device', 'eg', 'server', 'differential', 'privacy', 'fuzz', 'datum', 'infer', 'original', 'datum', 'thus', 'protect', 'user', 'announce', 'successfully', 'smushe', 'technology', 'together', 'allow', 'deploy', 'production', 'ml', 'model', 'use', 'federated', 'learning', 'rigorous', 'differential', 'privacy', 'guarantee', 'first', 'proofofconcept', 'deployment', 'use', 'dprespecte', 'call', 'dpftrl', 'train', 'recurrent', 'neural', 'network', 'power', 'nextwordprediction', 'spanishlanguage', 'gboard', 'user', 'eligible', 'device', 'maintain', 'local', 'training', 'cache', 'consist', 'user', 'keyboard', 'input', 'participate', 'compute', 'update', 'model', 'make', 'likely', 'suggest', 'next', 'word', 'user', 'actually', 'type', 'base', 'type', 'far', 'run', 'dpftrl', 'datum', 'train', 'recurrent', 'neural', 'network', 'parameter', 'training', 'run', 'round', 'day', 'device', 'participate', 'round', 'allow', 'guarantee', 'device', 'participate', 'training', 'hour', 'matter', 'recent', 'year', 'policymaker', 'particularly', 'start', 'write', 'increasingly', 'detailed', 'recommendation', 'need', 'tech', 'company', 'protect', 'user', 'privacy', 'gdpr', 'regulation', 'align', 'well', 'contemporary', 'ai', 'system', 'develop', 'train', 'give', 'dependency', 'vast', 'amount', 'user', 'datum', 'technique', 'combination', 'federated', 'learning', 'dp', 'let', 'company', 'get', 'ahead', 'regulatory', 'landscape', 'early', 'day', 'still', 'far', 'able', 'say', 'approach', 'possible', 'let', 'alone', 'practical', 'ml', 'model', 'product', 'application', 'write', 'consider', 'intriguing', 'proof', 'concept', 'read', 'federated', 'learning', 'formal', 'differential', 'privacy', 'guarantee']"
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 4,http://eepurl.com/hWsYwH,2022-03-07,"#################################################### Humans: More robust against deepfakes than you feared:
…MIT study suggests we should be worried, but not panicking…
MIT researchers have conducted a 5,000+ person-study to figure out how susceptible people are to deepfakes. The good news? If you're showing someone a faked video along with synthetic audio and text, there's a reasonable chance they'll guess that it's fake. The bad news? People's ability to identify deepfakes gets worse as you strip back modalities - so a silent video accompanied by a text transcript is hard, a silent video is harder, and just some text is hardest.

What they did: MIT recruited ~500 people to see how well they could identify deepfakes displayed on an MIT-created public website. It also got more than 5,000+ internet passers by to do the same test as well. Then, it grouped the cohorts together, filtered them for the ones paying attention, and ultimately got 5,727 participants who provide 61,792 truth discernment judgments across a bunch of different videos of Trump and Biden saying things. The data for this experiment came from the Presidential Deepfake Dataset, which consists of 32 videos of Trump and Biden making political speeches - half the videos are real, and half are fake. MIT then perturbed the videos further, swapping out audio tracks, text, and so on.  What they found: ""Participants rely more on how something is said – the audio-visual cues – rather than what is said – the speech content itself,"" they write. ""Political speeches that do not match public perceptions of politicians’ beliefs reduce participants’ reliance on visual cues.""
  Text is harder than video: ""Across the 32 text transcripts, the least accurately identified one is identified correctly in 27% of trials, the most accurately identified one is identified correctly in 75% of trials, and the median accurately identified one is identified correctly in 45% of trials.""
  So are silent videos: Similarly for silent videos without subtitles, the median accurately identified one is identified correctly in 63% of trials and the range of accurate identification from the least to the most accurately identified is 38% to 87% of trials. Why this matters: The more modalities you have, the better people do. ""Ordinary people can sometimes, but not always, recognize visual inconsistencies created by the lip syncing deepfake manipulations. As such, the assessment of multimedia information involves both perceptual cues from video and audio and considerations about the content (e.g., the degree to which what is said matches participants’ expectations of what the speaker would say, which is known as the expectancy violation heuristic60). With the message content alone, participants are only slightly better than random guessing at 57% accuracy on average.""

One fly in the ointment: There's one problem that unites these things - AI keeps on getting better. My fear is that in two years, people will find it a lot more challenging to identify fake videos with audio. Therefore, we'll need to rely on people's inner-media-critic to help them figure out if something is real or fake, and the way the world is going, I'm not sure that's a robust thing to rely on.      Read more: ​​Human Detection of Political Deepfakes across Transcripts, Audio, and Video (arXiv).    Check out the website used in the experiment: DeepFakes, Can You Spot Them? (MIT Website).","#################################################### Humans: More robust against deepfakes than you feared: …MIT study suggests we should be worried, but not panicking… MIT researchers have conducted a 5,000+ person-study to figure out how susceptible people are to deepfakes. The good news? If you're showing someone a faked video along with synthetic audio and text, there's a reasonable chance they'll guess that it's fake. The bad news? People's ability to identify deepfakes gets worse as you strip back modalities - so a silent video accompanied by a text transcript is hard, a silent video is harder, and just some text is hardest. What they did: MIT recruited ~500 people to see how well they could identify deepfakes displayed on an MIT-created public website. It also got more than 5,000+ internet passers by to do the same test as well. Then, it grouped the cohorts together, filtered them for the ones paying attention, and ultimately got 5,727 participants who provide 61,792 truth discernment judgments across a bunch of different videos of Trump and Biden saying things. The data for this experiment came from the Presidential Deepfake Dataset, which consists of 32 videos of Trump and Biden making political speeches - half the videos are real, and half are fake. MIT then perturbed the videos further, swapping out audio tracks, text, and so on. What they found: ""Participants rely more on how something is said – the audio-visual cues – rather than what is said – the speech content itself,"" they write. ""Political speeches that do not match public perceptions of politicians’ beliefs reduce participants’ reliance on visual cues."" Text is harder than video: ""Across the 32 text transcripts, the least accurately identified one is identified correctly in 27% of trials, the most accurately identified one is identified correctly in 75% of trials, and the median accurately identified one is identified correctly in 45% of trials."" So are silent videos: Similarly for silent videos without subtitles, the median accurately identified one is identified correctly in 63% of trials and the range of accurate identification from the least to the most accurately identified is 38% to 87% of trials. Why this matters: The more modalities you have, the better people do. ""Ordinary people can sometimes, but not always, recognize visual inconsistencies created by the lip syncing deepfake manipulations. As such, the assessment of multimedia information involves both perceptual cues from video and audio and considerations about the content (e.g., the degree to which what is said matches participants’ expectations of what the speaker would say, which is known as the expectancy violation heuristic60). With the message content alone, participants are only slightly better than random guessing at 57% accuracy on average."" One fly in the ointment: There's one problem that unites these things - AI keeps on getting better. My fear is that in two years, people will find it a lot more challenging to identify fake videos with audio. Therefore, we'll need to rely on people's inner-media-critic to help them figure out if something is real or fake, and the way the world is going, I'm not sure that's a robust thing to rely on. Read more: ​​Human Detection of Political Deepfakes across Transcripts, Audio, and Video (arXiv). Check out the website used in the experiment: DeepFakes, Can You Spot Them? (MIT Website).","['human', 'robust', 'deepfake', 'fear', 'mit', 'study', 'suggest', 'worried', 'panic', 'mit', 'researcher', 'conduct', 'personstudy', 'figure', 'susceptible', 'people', 'deepfake', 'good', 'news', 'show', 'faked', 'video', 'synthetic', 'audio', 'text', 'reasonable', 'chance', 'guess', 'fake', 'bad', 'news', 'people', 'ability', 'identify', 'deepfake', 'get', 'bad', 'strip', 'modality', 'silent', 'video', 'accompany', 'text', 'transcript', 'hard', 'silent', 'video', 'hard', 'text', 'hard', 'mit', 'recruit', 'people', 'see', 'well', 'identify', 'deepfake', 'display', 'mitcreated', 'public', 'website', 'also', 'get', 'internet', 'passer', 'test', 'well', 'group', 'cohort', 'together', 'filter', 'one', 'pay', 'attention', 'ultimately', 'get', 'participant', 'provide', 'truth', 'discernment', 'judgment', 'bunch', 'different', 'video', 'trump', 'biden', 'say', 'thing', 'datum', 'experiment', 'come', 'presidential', 'deepfake', 'dataset', 'consist', 'video', 'trump', 'biden', 'make', 'political', 'speech', 'video', 'real', 'half', 'fake', 'mit', 'perturb', 'video', 'far', 'swap', 'audio', 'track', 'text', 'find', 'participant', 'rely', 'say', 'audiovisual', 'cue', 'rather', 'say', 'speech', 'content', 'write', 'political', 'speech', 'match', 'public', 'perception', 'politician', 'belief', 'reduce', 'participant', 'reliance', 'visual', 'cue', 'text', 'hard', 'video', 'text', 'transcript', 'least', 'accurately', 'identify', 'identify', 'correctly', 'trial', 'accurately', 'identify', 'identify', 'correctly', 'trial', 'median', 'accurately', 'identify', 'identify', 'correctly', 'trial', 'silent', 'video', 'similarly', 'silent', 'video', 'subtitle', 'median', 'accurately', 'identify', 'identify', 'correctly', 'trial', 'range', 'accurate', 'identification', 'least', 'accurately', 'identify', 'trial', 'matter', 'modality', 'well', 'people', 'ordinary', 'people', 'sometimes', 'always', 'recognize', 'visual', 'inconsistency', 'create', 'lip', 'sync', 'deepfake', 'manipulation', 'assessment', 'multimedia', 'information', 'involve', 'perceptual', 'cue', 'video', 'audio', 'consideration', 'content', 'eg', 'degree', 'say', 'match', 'participant', 'expectation', 'speaker', 'say', 'know', 'expectancy', 'violation', 'heuristic60', 'message', 'content', 'alone', 'participant', 'slightly', 'well', 'random', 'guess', 'accuracy', 'average', 'fly', 'ointment', 'problem', 'unite', 'thing', 'ai', 'keep', 'get', 'well', 'fear', 'year', 'people', 'find', 'lot', 'challenging', 'identify', 'fake', 'video', 'audio', 'therefore', 'well', 'need', 'rely', 'people', 'innermediacritic', 'help', 'figure', 'real', 'fake', 'way', 'world', 'go', 'sure', 'robust', 'thing', 'rely', 'read', '\u200b\u200bhuman', 'detection', 'political', 'deepfake', 'transcript', 'audio', 'video', 'arxiv', 'check', 'website', 'use', 'experiment', 'deepfake', 'spot', 'mit', 'website']"
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 5,http://eepurl.com/hWsYwH,2022-03-07,"#################################################### Have some crazy ideas? Want money? Check out FTX's new fund:
…Plans to deploy between $100m and $1 billion this year… 
Crypto trading firm FTX has announced the FTX Future Fund (FFF). FFF is a philanthropic fund that will concentrate on ""making grants and investments to ambitious projects in order to improve humanity’s long-term prospects"". The fund has also published some of its areas of interest, so people can have a sense of what to pitch it. It has a bunch of ideas but, this being Import AI, I'll highlight the AI stuff.

What FTX is interested in giving grants on: AI alignment and specifically via ""well-designed prizes for solving open problems in AI alignment"", AI-based cognitive aids, bridging gaps in the AI and ethics ecosystem via studying ""fairness and transparency in current ML systems alongside risks from misaligned superintelligence.""

Why this matters: It's starting to feel like the development of a good AI ecosystem is less blocked on funding than it is on talent - initiatives like the FTX Future Fund show there's ample money for projects in this area. Now, the question is finding the talent to absorb the money. Perhaps some of the readers for this newsletter can be that talent!
  Read more: Announcing the Future Fund (FTX).
  Find out more about the projects: Project Ideas (FTX).","#################################################### Have some crazy ideas? Want money? Check out FTX's new fund: …Plans to deploy between $100m and $1 billion this year… Crypto trading firm FTX has announced the FTX Future Fund (FFF). FFF is a philanthropic fund that will concentrate on ""making grants and investments to ambitious projects in order to improve humanity’s long-term prospects"". The fund has also published some of its areas of interest, so people can have a sense of what to pitch it. It has a bunch of ideas but, this being Import AI, I'll highlight the AI stuff. What FTX is interested in giving grants on: AI alignment and specifically via ""well-designed prizes for solving open problems in AI alignment"", AI-based cognitive aids, bridging gaps in the AI and ethics ecosystem via studying ""fairness and transparency in current ML systems alongside risks from misaligned superintelligence."" Why this matters: It's starting to feel like the development of a good AI ecosystem is less blocked on funding than it is on talent - initiatives like the FTX Future Fund show there's ample money for projects in this area. Now, the question is finding the talent to absorb the money. Perhaps some of the readers for this newsletter can be that talent! Read more: Announcing the Future Fund (FTX). Find out more about the projects: Project Ideas (FTX).","['crazy', 'idea', 'want', 'money', 'check', 'new', 'fund', 'plan', 'deploy', 'year', 'trading', 'firm', 'ftx', 'announce', 'ftx', 'future', 'fund', 'philanthropic', 'fund', 'concentrate', 'make', 'grant', 'investment', 'ambitious', 'project', 'order', 'improve', 'humanity', 'longterm', 'prospect', 'fund', 'also', 'publish', 'area', 'interest', 'people', 'sense', 'pitch', 'bunch', 'idea', 'import', 'highlight', 'ai', 'stuff', 'ftx', 'interested', 'give', 'grant', 'alignment', 'specifically', 'welldesigne', 'prize', 'solve', 'open', 'problem', 'alignment', 'aibased', 'cognitive', 'aid', 'bridging', 'gap', 'ai', 'ethic', 'ecosystem', 'study', 'fairness', 'transparency', 'current', 'system', 'risk', 'misaligned', 'superintelligence', 'matter', 'start', 'feel', 'development', 'good', 'ai', 'ecosystem', 'less', 'block', 'funding', 'talent', 'initiative', 'ftx', 'fund', 'show', 'ample', 'money', 'project', 'area', 'question', 'find', 'talent', 'absorb', 'money', 'perhaps', 'reader', 'newsletter', 'talent', 'read', 'announce', 'future', 'fund', 'ftx', 'find', 'project', 'project', 'idea', 'ftx']"
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 6,http://eepurl.com/hWsYwH,2022-03-07,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute System Cards: an approach to improving how we report the capabilities and limitations of AI systems … In building on Models Cards and Datasheets, System Cards take into account the surrounding software and AI components …  Researchers from Facebook (technically Meta AI Research, but I currently refuse to entertain this cynical hiding-from-controversy rebrand - Jack) have published a case study on ways to document Instagram feed-ranking via a concept they call System Cards. System Cards are designed to “increase the transparency of ML systems by providing stakeholders with an overview of different components of an ML system, how these components interact, and how different pieces of data and protected information are used by the system.” In this way, System Cards are philosophically similar to Model Cards (#174), data sheets for datasets, and ways to label reinforcement learning systems (#285). System Cards: “A System Card provides an overview of several ML models that comprise an ML system, as well as details about these components, and a walkthrough with an example input.” System cards can be accompanied by step-by-step guides for how an input into a system leads to a certain output.  How this is different: System Cards account for non-ML components of a system, and also describe the relationships between these systems (for instance, how data moves through a service). System cards are also meant to highlight upward and downward dependencies. They're designed to be used by both technical and non-technical people. Why it matters: System Cards contain a lot more information than other things like Model Cards and Datasheets, and they may make it easier for people to understand not only the system in question, but the larger technical context in which it is deployed and in which it has dependencies. If System Cards become more widely used, they could also generate valuable metadata for analyzing the field of deployed ML systems more broadly.    Read more: System-Level Transparency of Machine Learning | Facebook AI Research","#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute System Cards: an approach to improving how we report the capabilities and limitations of AI systems … In building on Models Cards and Datasheets, System Cards take into account the surrounding software and AI components … Researchers from Facebook (technically Meta AI Research, but I currently refuse to entertain this cynical hiding-from-controversy rebrand - Jack) have published a case study on ways to document Instagram feed-ranking via a concept they call System Cards. System Cards are designed to “increase the transparency of ML systems by providing stakeholders with an overview of different components of an ML system, how these components interact, and how different pieces of data and protected information are used by the system.” In this way, System Cards are philosophically similar to Model Cards (#174), data sheets for datasets, and ways to label reinforcement learning systems (#285). System Cards: “A System Card provides an overview of several ML models that comprise an ML system, as well as details about these components, and a walkthrough with an example input.” System cards can be accompanied by step-by-step guides for how an input into a system leads to a certain output. How this is different: System Cards account for non-ML components of a system, and also describe the relationships between these systems (for instance, how data moves through a service). System cards are also meant to highlight upward and downward dependencies. They're designed to be used by both technical and non-technical people. Why it matters: System Cards contain a lot more information than other things like Model Cards and Datasheets, and they may make it easier for people to understand not only the system in question, but the larger technical context in which it is deployed and in which it has dependencies. If System Cards become more widely used, they could also generate valuable metadata for analyzing the field of deployed ML systems more broadly. Read more: System-Level Transparency of Machine Learning | Facebook AI Research","['ai', 'ethic', 'brief', 'montreal', 'system', 'card', 'approach', 'improve', 'report', 'capability', 'limitation', 'system', 'build', 'model', 'card', 'datasheet', 'system', 'card', 'take', 'account', 'surround', 'software', 'component', 'researcher', 'technically', 'meta', 'ai', 'research', 'currently', 'refuse', 'entertain', 'cynical', 'hidingfromcontroversy', 'rebrand', 'publish', 'case', 'study', 'way', 'document', 'instagram', 'feedranke', 'concept', 'call', 'system', 'card', 'system', 'card', 'design', 'increase', 'transparency', 'system', 'provide', 'stakeholder', 'overview', 'different', 'component', 'ml', 'system', 'component', 'interact', 'different', 'piece', 'datum', 'protect', 'information', 'use', 'system', 'way', 'system', 'card', 'philosophically', 'similar', 'model', 'card', 'datum', 'sheet', 'dataset', 'way', 'label', 'reinforcement', 'learning', 'system', 'system', 'card', 'system', 'card', 'provide', 'overview', 'several', 'ml', 'model', 'comprise', 'ml', 'system', 'well', 'detail', 'component', 'walkthrough', 'example', 'input', 'system', 'card', 'accompany', 'stepbystep', 'guide', 'input', 'system', 'lead', 'certain', 'output', 'different', 'system', 'card', 'account', 'nonml', 'component', 'system', 'also', 'describe', 'relationship', 'system', 'instance', 'datum', 'move', 'service', 'system', 'card', 'also', 'mean', 'highlight', 'upward', 'downward', 'dependency', 'design', 'use', 'technical', 'nontechnical', 'people', 'matter', 'system', 'card', 'contain', 'lot', 'information', 'thing', 'model', 'card', 'datasheet', 'make', 'easy', 'people', 'understand', 'system', 'question', 'large', 'technical', 'context', 'deploy', 'dependency', 'system', 'card', 'become', 'widely', 'use', 'also', 'generate', 'valuable', 'metadata', 'analyze', 'field', 'deploy', 'system', 'broadly', 'read', 'systemlevel', 'transparency', 'machine', 'learning', 'research']"
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 7,http://eepurl.com/hWsYwH,2022-03-07,"#################################################### Tech tales: Some things that were kind of holy [Recollections of the 2025-2030 period]

The 21st century was a confusing time to be religious - the old gods were falling away as fewer people believed in them, and the new gods hadn't been born. But we did get protogods: AI systems that could speak with beautiful and persuasive rhetoric to almost anyone. Over time, these AI systems got more and more personalized, until people could ask them very specific questions, and get very specific answers that only made sense in the context of that person. Once this capability came online, we had the flash-problem of the 'micro religions'. All kinds of micro identities had been brewing for years, like a fungus that took root on early social platforms like MySpace and Tumblr and Facebook and Instagram and TikTok, and then blossomed from there. Now, all these people with micro identities - the space wiccans, the anarcho-primitivists, the neo-cath-libertarians, the tankie-double-agents - got their own religions. Gods for space witches. Demons for anarchist Neanderthals. The flaming faces of god spraying money at the neo-Catholics.
  This, predictably, caused problems. The greatest problem was when the religious wars started. These weren't traditional wars - nation states still had a premium on violence, and micro-identities barely touched the physical world. But they were information wars. People repurposed AI systems to generate and magnify the outputs of their own gods, then pointed them at the shared social media platforms people used. Twitter conversations would get taken over by pseudo-identities preaching the need to return to a simpler time, and then they would be quote-tweeted into oblivion by the witches claiming that now was the time for ascendance. Screenshots of these quote tweets would get magnified on the more overtly religious social networks by screenshots taken by the neo-Catholics and circulated as evidence that the great Satan was walking the earth. And these conversations would then be recycled back into twitter and commented on by the anti-pascals-wager atheists identities, which would trigger another cycle of religious preaching, and so on.
    The synthetic-theology accords were passed soon after. Things that inspired this story: How the more one becomes an island, the more one creates a demon and an angel for that specific island; the need for humans to have beliefs; the commodification of belief into a symbol of identity; social networks as a hybrid of organic social needs and capitalist attention-harvesting; generative AI models like GPT3 and the logical consequences of their successors; watching Raised by Wolves and thinking about Future Christianity.  

Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech tales: Some things that were kind of holy [Recollections of the 2025-2030 period] The 21st century was a confusing time to be religious - the old gods were falling away as fewer people believed in them, and the new gods hadn't been born. But we did get protogods: AI systems that could speak with beautiful and persuasive rhetoric to almost anyone. Over time, these AI systems got more and more personalized, until people could ask them very specific questions, and get very specific answers that only made sense in the context of that person. Once this capability came online, we had the flash-problem of the 'micro religions'. All kinds of micro identities had been brewing for years, like a fungus that took root on early social platforms like MySpace and Tumblr and Facebook and Instagram and TikTok, and then blossomed from there. Now, all these people with micro identities - the space wiccans, the anarcho-primitivists, the neo-cath-libertarians, the tankie-double-agents - got their own religions. Gods for space witches. Demons for anarchist Neanderthals. The flaming faces of god spraying money at the neo-Catholics. This, predictably, caused problems. The greatest problem was when the religious wars started. These weren't traditional wars - nation states still had a premium on violence, and micro-identities barely touched the physical world. But they were information wars. People repurposed AI systems to generate and magnify the outputs of their own gods, then pointed them at the shared social media platforms people used. Twitter conversations would get taken over by pseudo-identities preaching the need to return to a simpler time, and then they would be quote-tweeted into oblivion by the witches claiming that now was the time for ascendance. Screenshots of these quote tweets would get magnified on the more overtly religious social networks by screenshots taken by the neo-Catholics and circulated as evidence that the great Satan was walking the earth. And these conversations would then be recycled back into twitter and commented on by the anti-pascals-wager atheists identities, which would trigger another cycle of religious preaching, and so on. The synthetic-theology accords were passed soon after. Things that inspired this story: How the more one becomes an island, the more one creates a demon and an angel for that specific island; the need for humans to have beliefs; the commodification of belief into a symbol of identity; social networks as a hybrid of organic social needs and capitalist attention-harvesting; generative AI models like GPT3 and the logical consequences of their successors; watching Raised by Wolves and thinking about Future Christianity. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'thing', 'kind', 'holy', 'recollection', 'period', '21st', 'century', 'confusing', 'time', 'religious', 'old', 'god', 'fall', 'away', 'people', 'believe', 'new', 'god', 'bear', 'get', 'protogod', 'ai', 'system', 'speak', 'beautiful', 'persuasive', 'rhetoric', 'almost', 'time', 'system', 'get', 'personalized', 'people', 'ask', 'specific', 'question', 'get', 'specific', 'answer', 'make', 'sense', 'context', 'person', 'capability', 'come', 'online', 'flashproblem', 'micro', 'religion', 'kind', 'micro', 'identity', 'brew', 'year', 'fungus', 'take', 'root', 'early', 'social', 'platform', 'myspace', 'tumblr', 'instagram', 'tiktok', 'blossom', 'people', 'micro', 'identity', 'space', 'wiccan', 'anarchoprimitivist', 'neocathlibertarian', 'tankiedoubleagent', 'get', 'religion', 'god', 'space', 'witch', 'demon', 'anarchist', 'neanderthal', 'flame', 'face', 'spray', 'money', 'neocatholic', 'predictably', 'cause', 'problem', 'great', 'problem', 'religious', 'war', 'start', 'traditional', 'war', 'nation', 'state', 'still', 'premium', 'violence', 'microidentitie', 'barely', 'touch', 'physical', 'world', 'information', 'war', 'people', 'repurpose', 'system', 'generate', 'magnify', 'output', 'god', 'point', 'share', 'social', 'medium', 'platform', 'people', 'use', 'twitter', 'conversation', 'take', 'pseudoidentitie', 'preach', 'need', 'return', 'simple', 'time', 'quotetweete', 'oblivion', 'witch', 'claim', 'time', 'ascendance', 'screenshot', 'tweet', 'magnify', 'overtly', 'religious', 'social', 'network', 'screenshot', 'take', 'neocatholic', 'circulate', 'evidence', 'great', 'walk', 'earth', 'conversation', 'recycle', 'back', 'twitter', 'comment', 'antipascalswager', 'atheist', 'identity', 'trigger', 'cycle', 'religious', 'preaching', 'synthetictheology', 'accord', 'pass', 'soon', 'thing', 'inspire', 'story', 'become', 'island', 'one', 'create', 'demon', 'angel', 'specific', 'island', 'need', 'human', 'belief', 'commodification', 'belief', 'symbol', 'identity', 'social', 'network', 'hybrid', 'organic', 'social', 'need', 'capitalist', 'attentionharveste', 'generative', 'ai', 'model', 'gpt3', 'logical', 'consequence', 'successor', 'raise', 'wolf', 'think', 'future', 'christianity', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 0,http://eepurl.com/hVSMf1,2022-02-28,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 Are AI systems conscious? And would it matter if they were?
…Some 'mostly boring' views from the inside of a lab… 
My colleague, Amanda Askell, has written a post about AI consciousness. Amanda is a philosopher and ML researcher and she spends a lot of time trying to evaluate models. This post lays out some of her views on AI consciousness and is worth a read if you're trying to orient yourself in this debate.
  ""Some people care about properties like intelligence and self-awareness because they want to identify features that might distinguish humans from non-human animals. In general, I’m more interested in what distinguishes a tiger from a rock than in what distinguishes a human from a tiger,"" she writes.

Why this matters: There's some chance AI systems will eventually become both moral patients and moral agents. Our ability to understand this relates to our ability to think about consciousness and how it might apply to increasingly advanced AI systems. If we get this wrong we, per Amanda's phrasing, risk subjecting agents to thousands of years of torture. Let's avoid that.
  Read more: My mostly boring views about AI consciousness (Amanda Askell, substack).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Are AI systems conscious? And would it matter if they were? …Some 'mostly boring' views from the inside of a lab… My colleague, Amanda Askell, has written a post about AI consciousness. Amanda is a philosopher and ML researcher and she spends a lot of time trying to evaluate models. This post lays out some of her views on AI consciousness and is worth a read if you're trying to orient yourself in this debate. ""Some people care about properties like intelligence and self-awareness because they want to identify features that might distinguish humans from non-human animals. In general, I’m more interested in what distinguishes a tiger from a rock than in what distinguishes a human from a tiger,"" she writes. Why this matters: There's some chance AI systems will eventually become both moral patients and moral agents. Our ability to understand this relates to our ability to think about consciousness and how it might apply to increasingly advanced AI systems. If we get this wrong we, per Amanda's phrasing, risk subjecting agents to thousands of years of torture. Let's avoid that. Read more: My mostly boring views about AI consciousness (Amanda Askell, substack).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'upgrade', 'subscribe', 'system', 'conscious', 'matter', 'mostly', 'boring', 'view', 'inside', 'lab', 'colleague', 'amanda', 'write', 'post', 'consciousness', 'philosopher', 'researcher', 'spend', 'lot', 'time', 'try', 'evaluate', 'model', 'post', 'lay', 'view', 'consciousness', 'worth', 'read', 'try', 'orient', 'debate', 'people', 'care', 'property', 'intelligence', 'selfawareness', 'want', 'identify', 'feature', 'distinguish', 'human', 'nonhuman', 'animal', 'general', '’m', 'interested', 'distinguish', 'tiger', 'rock', 'distinguish', 'human', 'tiger', 'write', 'matter', 'chance', 'system', 'eventually', 'become', 'moral', 'patient', 'moral', 'agent', 'ability', 'understand', 'relate', 'ability', 'think', 'consciousness', 'apply', 'increasingly', 'advanced', 'ai', 'system', 'get', 'wrong', 'amanda', 'phrase', 'risk', 'subject', 'agent', 'thousand', 'year', 'torture', 'let', 'avoid', 'read', 'mostly', 'boring', 'view']"
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 1,http://eepurl.com/hVSMf1,2022-02-28,"#################################################### How do we get fairer AI systems? Train the dumbest and biggest model possible:
…Facebook shows that sometimes the best filter is no filter at all…
Researchers with Facebook AI Research have trained what they think is the largest dense vision model ever (10 billion parameters) on a billion random images sampled from Instagram. The resulting models are extraordinarily capable at a huge range of downstream evaluations (mirroring the performance trends of scaling up compute and data for language models like GPT-3), but also have another intriguing trait: they display much better qualities around fairness and bias than vision models trained on curated datasets like ImageNet. """"In this work, we are interested in probing which of the properties emerge in visual features trained with no supervision on as many images from across the world as possible,"" they write.
  This is a very big deal - it suggests that maybe the route to fair AI systems is training the largest possible model on the greatest possible amount of data with minimal human oversight. That would be a radical shift from the current intuitions around fairness - namely, that you get to fairness by heavily curating the underlying dataset.

Performance and Fairness: ""On in-domain benchmarks, we observe that some properties of the features captured by the larger model was far less present in smaller model. In particular, one of our key empirical findings is that self-supervised learning on random internet data leads to models that are more fair, less biased and less harmful,"" they write. ""We observe that our model is also able to leverage the diversity of concepts in the dataset to train more robust features, leading to better out-of-distribution generalization.""
  Some of those capabilities in full: In tests, the models do better on fairness indicators relating to gender, skintone, and age bias. They also display less disparity around gender than models trained on ImageNet. They're also better at identifying geographic features (including geographic localization), are better at hate speech detection, and display substantially better performance on generalization tests (like harder versions of ImageNet).

Things that make you go 'hmm' and 'uh oh': Facebook trained its model on 1 billion images taken from Instagram. But there's a twist - it pre-filtered the data to ensure it wasn't training anything on EU data ""to confirm to GDPR"". While this might seem like standard cover-your-back behavior, it has a deeper implication: Europe's privacy legislation means that certain types of data from Europe will ultimately be less represented in global-scale AI models. This means the cultures of various European countries will also be less represented. This is a nice example of the unintended consequences of legislation.

Why this matters: ""We have demonstrated the potential of using self-supervised training on random internet images to train models that are more fair and less harmful (less harmful predictions, improved and less disparate learned attribute representations and larger improvement in object recognition on images from low/medium income households and non-Western countries)."" In other words - the scaling will continue until the models improve (further)!
  Read more: Vision Models are More Robust and Fair When pretrained on Uncurated Images Without Supervision (arXiv).","#################################################### How do we get fairer AI systems? Train the dumbest and biggest model possible: …Facebook shows that sometimes the best filter is no filter at all… Researchers with Facebook AI Research have trained what they think is the largest dense vision model ever (10 billion parameters) on a billion random images sampled from Instagram. The resulting models are extraordinarily capable at a huge range of downstream evaluations (mirroring the performance trends of scaling up compute and data for language models like GPT-3), but also have another intriguing trait: they display much better qualities around fairness and bias than vision models trained on curated datasets like ImageNet. """"In this work, we are interested in probing which of the properties emerge in visual features trained with no supervision on as many images from across the world as possible,"" they write. This is a very big deal - it suggests that maybe the route to fair AI systems is training the largest possible model on the greatest possible amount of data with minimal human oversight. That would be a radical shift from the current intuitions around fairness - namely, that you get to fairness by heavily curating the underlying dataset. Performance and Fairness: ""On in-domain benchmarks, we observe that some properties of the features captured by the larger model was far less present in smaller model. In particular, one of our key empirical findings is that self-supervised learning on random internet data leads to models that are more fair, less biased and less harmful,"" they write. ""We observe that our model is also able to leverage the diversity of concepts in the dataset to train more robust features, leading to better out-of-distribution generalization."" Some of those capabilities in full: In tests, the models do better on fairness indicators relating to gender, skintone, and age bias. They also display less disparity around gender than models trained on ImageNet. They're also better at identifying geographic features (including geographic localization), are better at hate speech detection, and display substantially better performance on generalization tests (like harder versions of ImageNet). Things that make you go 'hmm' and 'uh oh': Facebook trained its model on 1 billion images taken from Instagram. But there's a twist - it pre-filtered the data to ensure it wasn't training anything on EU data ""to confirm to GDPR"". While this might seem like standard cover-your-back behavior, it has a deeper implication: Europe's privacy legislation means that certain types of data from Europe will ultimately be less represented in global-scale AI models. This means the cultures of various European countries will also be less represented. This is a nice example of the unintended consequences of legislation. Why this matters: ""We have demonstrated the potential of using self-supervised training on random internet images to train models that are more fair and less harmful (less harmful predictions, improved and less disparate learned attribute representations and larger improvement in object recognition on images from low/medium income households and non-Western countries)."" In other words - the scaling will continue until the models improve (further)! Read more: Vision Models are More Robust and Fair When pretrained on Uncurated Images Without Supervision (arXiv).","['get', 'fair', 'system', 'train', 'dumb', 'big', 'model', 'possible', 'facebook', 'show', 'sometimes', 'good', 'filter', 'filter', 'researcher', 'research', 'train', 'think', 'large', 'dense', 'vision', 'model', 'ever', 'parameter', 'random', 'image', 'sample', 'instagram', 'result', 'model', 'extraordinarily', 'capable', 'huge', 'range', 'downstream', 'evaluation', 'mirror', 'performance', 'trend', 'scale', 'compute', 'datum', 'language', 'model', 'gpt3', 'also', 'intriguing', 'trait', 'display', 'much', 'well', 'quality', 'fairness', 'bias', 'vision', 'model', 'train', 'curate', 'dataset', 'imagenet', 'work', 'interested', 'probe', 'property', 'emerge', 'visual', 'feature', 'train', 'supervision', 'many', 'image', 'world', 'possible', 'write', 'big', 'deal', 'suggest', 'maybe', 'route', 'fair', 'system', 'train', 'large', 'possible', 'model', 'great', 'possible', 'amount', 'datum', 'minimal', 'human', 'oversight', 'radical', 'shift', 'current', 'intuition', 'fairness', 'namely', 'get', 'fairness', 'heavily', 'curate', 'underlie', 'dataset', 'performance', 'fairness', 'indomain', 'benchmark', 'observe', 'property', 'feature', 'capture', 'large', 'model', 'far', 'less', 'present', 'small', 'model', 'particular', 'key', 'empirical', 'finding', 'selfsupervise', 'learning', 'random', 'internet', 'datum', 'lead', 'model', 'fair', 'less', 'biased', 'less', 'harmful', 'write', 'observe', 'model', 'also', 'able', 'leverage', 'diversity', 'concept', 'dataset', 'train', 'robust', 'feature', 'lead', 'well', 'outofdistribution', 'generalization', 'capability', 'full', 'test', 'model', 'well', 'fairness', 'indicator', 'relate', 'gender', 'skintone', 'age', 'bias', 'also', 'display', 'less', 'disparity', 'gender', 'model', 'train', 'imagenet', 'also', 'well', 'identify', 'geographic', 'feature', 'include', 'geographic', 'localization', 'well', 'hate', 'speech', 'detection', 'display', 'substantially', 'well', 'performance', 'generalization', 'test', 'hard', 'version', 'imagenet', 'thing', 'make', 'go', 'facebook', 'train', 'model', 'image', 'take', 'instagram', 'twist', 'prefiltere', 'datum', 'ensure', 'train', 'datum', 'confirm', 'gdpr', 'seem', 'standard', 'coveryourback', 'behavior', 'deep', 'implication', 'europe', 'privacy', 'legislation', 'mean', 'certain', 'type', 'datum', 'ultimately', 'less', 'represent', 'model', 'mean', 'culture', 'various', 'european', 'country', 'also', 'less', 'represent', 'nice', 'example', 'unintended', 'consequence', 'legislation', 'matter', 'demonstrate', 'potential', 'use', 'selfsupervise', 'training', 'random', 'internet', 'image', 'train', 'model', 'fair', 'less', 'harmful', 'less', 'harmful', 'prediction', 'improve', 'less', 'disparate', 'learn', 'attribute', 'representation', 'large', 'improvement', 'object', 'recognition', 'image', 'lowmedium', 'income', 'household', 'nonwestern', 'country', 'word', 'scaling', 'continue', 'model', 'far', 'read', 'vision', 'model', 'robust', 'fair', 'pretraine', 'uncurated', 'image']"
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 2,http://eepurl.com/hVSMf1,2022-02-28,"#################################################### AI supercomputers? Cute. Planet-scale computers? Better.
…Microsoft reveals 'Singularity', a globe-spanning AI computer…
Microsoft has revealed Singularity, the software stack it uses to schedule and train AI jobs across its global fleet of data centers. Singularity gives an indication of the vast-scale at which modern AI workloads get run, and also speaks to the ambitions of technology companies to role all their data centers together into a single, vast blob of compute.

How big is Singularity? Singularity is designed to ""scale across a global fleet of hundreds of thousands of GPUs and other AI accelerators"". Singularity treats Microsoft's compute stack ""as a single, logical shared cluster"".

Something special: One neat feature of Singularity is how it deals with failures. Failures happen a lot in machine learning; when you're training a neural network across hundreds to thousands of GPUs, a ton of freaky shit happens - nodes die, tiny software bugs explode (usually at 2am), your scheduler goes into a crash-loop, etc. Singularity tries to deal with this by gathering node-specific data on all the jobs being run, so that jobs can be easily resumed after running into a problem. ""The checkpoint that Singularity takes is comprised of consistent address-space snapshots of individual workers of the job. As these snapshots capture the full program state such as instruction pointer, stack, heap etc., the job resumes exactly from the point where it was preempted at, with no lost work,"" the researchers write.  
Why this matters: Just as computation is going to be the fundamental resource of the 20th century, the ability to utilize that computation will be the thing that defines who wields power in this era. Systems like Singularity give us an indication of the ambition of companies like Microsoft, and should make policymakers pay attention: what happens when the ability to wield planet-scale computation is solely something within the competency of private sector actors unaffiliated with any single nation state?
  Read more: Singularity: Planet-Scale, Preemptible, Elastic Scheduling of AI Workloads (arXiv).","#################################################### AI supercomputers? Cute. Planet-scale computers? Better. …Microsoft reveals 'Singularity', a globe-spanning AI computer… Microsoft has revealed Singularity, the software stack it uses to schedule and train AI jobs across its global fleet of data centers. Singularity gives an indication of the vast-scale at which modern AI workloads get run, and also speaks to the ambitions of technology companies to role all their data centers together into a single, vast blob of compute. How big is Singularity? Singularity is designed to ""scale across a global fleet of hundreds of thousands of GPUs and other AI accelerators"". Singularity treats Microsoft's compute stack ""as a single, logical shared cluster"". Something special: One neat feature of Singularity is how it deals with failures. Failures happen a lot in machine learning; when you're training a neural network across hundreds to thousands of GPUs, a ton of freaky shit happens - nodes die, tiny software bugs explode (usually at 2am), your scheduler goes into a crash-loop, etc. Singularity tries to deal with this by gathering node-specific data on all the jobs being run, so that jobs can be easily resumed after running into a problem. ""The checkpoint that Singularity takes is comprised of consistent address-space snapshots of individual workers of the job. As these snapshots capture the full program state such as instruction pointer, stack, heap etc., the job resumes exactly from the point where it was preempted at, with no lost work,"" the researchers write. Why this matters: Just as computation is going to be the fundamental resource of the 20th century, the ability to utilize that computation will be the thing that defines who wields power in this era. Systems like Singularity give us an indication of the ambition of companies like Microsoft, and should make policymakers pay attention: what happens when the ability to wield planet-scale computation is solely something within the competency of private sector actors unaffiliated with any single nation state? Read more: Singularity: Planet-Scale, Preemptible, Elastic Scheduling of AI Workloads (arXiv).","['supercomputer', 'cute', 'planetscale', 'computer', 'well', 'microsoft', 'reveal', 'singularity', 'globespanne', 'ai', 'computer', 'reveal', 'singularity', 'software', 'stack', 'use', 'schedule', 'train', 'ai', 'job', 'global', 'fleet', 'datum', 'center', 'singularity', 'give', 'indication', 'vastscale', 'modern', 'ai', 'workload', 'run', 'also', 'speak', 'ambition', 'technology', 'company', 'role', 'datum', 'center', 'together', 'single', 'vast', 'blob', 'compute', 'big', 'singularity', 'singularity', 'design', 'scale', 'global', 'fleet', 'hundred', 'thousand', 'gpus', 'ai', 'accelerator', 'singularity', 'treat', 'microsoft', 'compute', 'stack', 'single', 'logical', 'share', 'cluster', 'special', 'neat', 'feature', 'singularity', 'deal', 'failure', 'failure', 'happen', 'lot', 'machine', 'learning', 'train', 'neural', 'network', 'hundred', 'thousand', 'ton', 'shit', 'happen', 'node', 'die', 'tiny', 'software', 'bug', 'explode', 'usually', 'scheduler', 'go', 'crashloop', 'etc', 'singularity', 'try', 'deal', 'gather', 'nodespecific', 'datum', 'job', 'run', 'job', 'easily', 'resume', 'run', 'problem', 'checkpoint', 'singularity', 'take', 'comprise', 'consistent', 'addressspace', 'snapshot', 'individual', 'worker', 'job', 'snapshot', 'capture', 'full', 'program', 'state', 'instruction', 'pointer', 'stack', 'heap', 'job', 'resume', 'exactly', 'point', 'preempt', 'lose', 'work', 'researcher', 'write', 'matter', 'computation', 'go', 'fundamental', 'resource', '20th', 'century', 'ability', 'utilize', 'computation', 'thing', 'define', 'wield', 'power', 'era', 'system', 'singularity', 'give', 'indication', 'ambition', 'company', 'make', 'policymaker', 'pay', 'attention', 'happen', 'ability', 'wield', 'planetscale', 'computation', 'solely', 'competency', 'private', 'sector', 'actor', 'unaffiliate', 'single', 'nation', 'state', 'read', 'singularity', 'planetscale', 'preemptible', 'elastic', 'scheduling', 'ai', 'workload']"
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 3,http://eepurl.com/hVSMf1,2022-02-28,"#################################################### AI is going to change games - this new beta service shows how:
…Latitude Voyage gestures at a future where games are built, extended, and adapted by AI…
Latitude, the startup game company that makes the GPT2/3/J1J-based game 'AI Dungeon', has announced a service called Voyage. Voyage is a subscription service for gaining access to new AI-based games built by Latitude, the ability to use various game-specific AI image generators, and - most intriguingly - eventually access to a 'creator studio', which will make it possible for people to build their own AI powered games and other software.

Why this matters: AI models are going to become the generative kernels around which new games get built. AI-based games hold the possibility for a dream of all games designers - a game that adapts to the individual that plays it, with games becoming more customized, idiosyncratic, and surprising the longer you play. Services like Latitude Voyage tell us that experiments in this new domain are about to be run at a large scale. 
  Read more: Latitude Voyage (Latitude).","#################################################### AI is going to change games - this new beta service shows how: …Latitude Voyage gestures at a future where games are built, extended, and adapted by AI… Latitude, the startup game company that makes the GPT2/3/J1J-based game 'AI Dungeon', has announced a service called Voyage. Voyage is a subscription service for gaining access to new AI-based games built by Latitude, the ability to use various game-specific AI image generators, and - most intriguingly - eventually access to a 'creator studio', which will make it possible for people to build their own AI powered games and other software. Why this matters: AI models are going to become the generative kernels around which new games get built. AI-based games hold the possibility for a dream of all games designers - a game that adapts to the individual that plays it, with games becoming more customized, idiosyncratic, and surprising the longer you play. Services like Latitude Voyage tell us that experiments in this new domain are about to be run at a large scale. Read more: Latitude Voyage (Latitude).","['ai', 'go', 'change', 'game', 'new', 'beta', 'service', 'show', 'latitude', 'voyage', 'gesture', 'future', 'game', 'build', 'extend', 'adapt', 'ai', 'latitude', 'startup', 'game', 'company', 'make', 'gpt23j1jbased', 'game', 'announce', 'service', 'call', 'voyage', 'voyage', 'subscription', 'service', 'gain', 'access', 'new', 'aibased', 'game', 'build', 'latitude', 'ability', 'use', 'various', 'gamespecific', 'ai', 'image', 'generator', 'intriguingly', 'eventually', 'access', 'creator', 'studio', 'make', 'possible', 'people', 'build', 'ai', 'power', 'game', 'software', 'matter', 'model', 'go', 'become', 'generative', 'kernel', 'new', 'game', 'build', 'aibased', 'game', 'hold', 'possibility', 'dream', 'game', 'designer', 'game', 'adapt', 'individual', 'play', 'game', 'become', 'customize', 'idiosyncratic', 'surprising', 'long', 'play', 'service', 'latitude', 'voyage', 'tell', 'experiment', 'new', 'domain', 'run', 'large', 'scale', 'read', 'latitude', 'voyage', 'latitude']"
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 4,http://eepurl.com/hVSMf1,2022-02-28,"#################################################### Fine-tune GPT-NeoX-20B - for free…
…GaaS me up, fool!...
We've talked about language models as a service (LMaaS). Now, we've got GPT-as-a-service (GaaS). Specifically, AI startup ForeFront has announced its now hosting Eleuther's 20B GPT model, GPT-NeoX-20B, and has built a bunch of fine-tuning features people can use. This is interesting for a couple of reasons:
1) Speed: GPT-NeoX-20B came out, like, two weeks ago. Model release > commercial service in two weeks is an indication of the rapidly growing ecosystem around commercializing general models.
2) Competition: For a while, OpenAI was the only show in town when it came to providing GaaS/LMaaS services. Now, it's competing with a bunch of entities, ranging from Forefront, to Cohere, to AI21 Labs. As competition steeps up, we'll see people race to the top and bottom on various things (top: safety vs libertarian access policies), (bottom: pricing, know your customer checks).

Why this matters: If AI is going to interact with the world, people need to be able to interact with AI. The emergence of these kinds of commercial AI services is how that'll happen, so it's worth paying attention.
  Read more: How To Fine-Tune GPT-NeoX (ForeFront blog).","#################################################### Fine-tune GPT-NeoX-20B - for free… …GaaS me up, fool!... We've talked about language models as a service (LMaaS). Now, we've got GPT-as-a-service (GaaS). Specifically, AI startup ForeFront has announced its now hosting Eleuther's 20B GPT model, GPT-NeoX-20B, and has built a bunch of fine-tuning features people can use. This is interesting for a couple of reasons: 1) Speed: GPT-NeoX-20B came out, like, two weeks ago. Model release > commercial service in two weeks is an indication of the rapidly growing ecosystem around commercializing general models. 2) Competition: For a while, OpenAI was the only show in town when it came to providing GaaS/LMaaS services. Now, it's competing with a bunch of entities, ranging from Forefront, to Cohere, to AI21 Labs. As competition steeps up, we'll see people race to the top and bottom on various things (top: safety vs libertarian access policies), (bottom: pricing, know your customer checks). Why this matters: If AI is going to interact with the world, people need to be able to interact with AI. The emergence of these kinds of commercial AI services is how that'll happen, so it's worth paying attention. Read more: How To Fine-Tune GPT-NeoX (ForeFront blog).","['finetune', 'gptneox20b', 'free', 'gaas', 'fool', 'talk', 'language', 'model', 'service', 'lmaas', 'get', 'gptasaservice', 'gaas', 'specifically', 'startup', 'forefront', 'announce', 'host', 'eleuther', 'model', 'build', 'bunch', 'finetune', 'feature', 'people', 'use', 'interesting', 'couple', 'reason', 'speed', 'gptneox20b', 'come', 'week', 'ago', 'model', 'release', 'commercial', 'service', 'week', 'indication', 'rapidly', 'grow', 'ecosystem', 'commercialize', 'general', 'model', 'competition', 'openai', 'show', 'town', 'come', 'provide', 'gaaslmaas', 'service', 'compete', 'bunch', 'entity', 'range', 'forefront', 'cohere', 'lab', 'competition', 'steep', 'well', 'see', 'people', 'race', 'top', 'bottom', 'various', 'thing', 'top', 'safety', 'libertarian', 'access', 'policy', 'bottom', 'pricing', 'know', 'customer', 'check', 'matter', 'go', 'interact', 'world', 'people', 'need', 'able', 'interact', 'emergence', 'kind', 'commercial', 'service', 'happen', 'worth', 'pay', 'attention', 'read', 'finetune', 'gptneox', 'forefront', 'blog']"
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 5,http://eepurl.com/hVSMf1,2022-02-28,"#################################################### Hark, yet another AI safety startup appears!
…Aligned AI comes out of the University of Oxford with big ambitions…
AI safety researcher Stuart Armstrong has left the Future of Humanity Institute to co-found Aligned AI, an AI research company.

Safety via value extrapolation: The company will work on value extrapolations, which Stuart describes as follows: ""It is easy to point at current examples of agents with low (or high) impact, at safe (or dangerous) suggestions, at low (or high) powered behaviors. So we have in a sense the 'training sets' for defining low-impact/Oracles/low-powered AIs.    It's extending these examples to the general situation that fails: definitions which cleanly divide the training set (whether produced by algorithms or humans) fail to extend to the general situation. Call this the 'value extrapolation problem[1], with 'value' interpreted broadly as a categorisation of situations into desirable and undesirable.    Humans turn out to face similar problems. We have broadly defined preferences in familiar situations we have encountered in the world or in fiction. Yet, when confronted with situations far from these, we have to stop and figure out how our values might possibly extend. Since these human values aren't - yet - defined, we can't directly input them into an algorithm, so AIs that can't solve value extrapolation can't be aligned with human values"".

But how do you make money off this? ""We'll start by offering alignment as a service for more limited AIs,"" Armstrong writes. ""Value extrapolation scales down as well as up: companies value algorithms that won't immediately misbehave in new situations, algorithms that will become conservative and ask for guidance when facing ambiguity.""

Why this matters: There's been a flurry of new companies forming in the AI safety space recently, including ARC, Anthropic, Redwood Research, and now Aligned AI. Along with this, there's also a proliferation of companies working on large-scale generative models (e.g, Cohere, AI21). It feels like AI has shifted into a multi-polar era, with a bunch more entities on the proverbial gameboard. This will present new opportunities and challenges for coordination.     Read more: Why I'm co-founding Aligned AI (Alignment Forum).","#################################################### Hark, yet another AI safety startup appears! …Aligned AI comes out of the University of Oxford with big ambitions… AI safety researcher Stuart Armstrong has left the Future of Humanity Institute to co-found Aligned AI, an AI research company. Safety via value extrapolation: The company will work on value extrapolations, which Stuart describes as follows: ""It is easy to point at current examples of agents with low (or high) impact, at safe (or dangerous) suggestions, at low (or high) powered behaviors. So we have in a sense the 'training sets' for defining low-impact/Oracles/low-powered AIs. It's extending these examples to the general situation that fails: definitions which cleanly divide the training set (whether produced by algorithms or humans) fail to extend to the general situation. Call this the 'value extrapolation problem[1], with 'value' interpreted broadly as a categorisation of situations into desirable and undesirable. Humans turn out to face similar problems. We have broadly defined preferences in familiar situations we have encountered in the world or in fiction. Yet, when confronted with situations far from these, we have to stop and figure out how our values might possibly extend. Since these human values aren't - yet - defined, we can't directly input them into an algorithm, so AIs that can't solve value extrapolation can't be aligned with human values"". But how do you make money off this? ""We'll start by offering alignment as a service for more limited AIs,"" Armstrong writes. ""Value extrapolation scales down as well as up: companies value algorithms that won't immediately misbehave in new situations, algorithms that will become conservative and ask for guidance when facing ambiguity."" Why this matters: There's been a flurry of new companies forming in the AI safety space recently, including ARC, Anthropic, Redwood Research, and now Aligned AI. Along with this, there's also a proliferation of companies working on large-scale generative models (e.g, Cohere, AI21). It feels like AI has shifted into a multi-polar era, with a bunch more entities on the proverbial gameboard. This will present new opportunities and challenges for coordination. Read more: Why I'm co-founding Aligned AI (Alignment Forum).","['hark', 'yet', 'safety', 'startup', 'appear', 'align', 'ai', 'come', 'big', 'ambition', 'safety', 'researcher', 'leave', 'future', 'humanity', 'institute', 'cofound', 'align', 'ai', 'research', 'company', 'safety', 'value', 'extrapolation', 'company', 'work', 'value', 'extrapolation', 'describe', 'follow', 'easy', 'point', 'current', 'example', 'agent', 'low', 'high', 'impact', 'safe', 'dangerous', 'suggestion', 'low', 'high', 'powered', 'behavior', 'sense', 'training', 'set', 'define', 'extend', 'example', 'general', 'situation', 'fail', 'definition', 'cleanly', 'divide', 'training', 'set', 'produce', 'algorithm', 'human', 'fail', 'extend', 'general', 'situation', 'call', 'value', 'extrapolation', 'problem1', 'value', 'interpret', 'broadly', 'categorisation', 'situation', 'desirable', 'undesirable', 'human', 'turn', 'face', 'similar', 'problem', 'broadly', 'define', 'preference', 'familiar', 'situation', 'encounter', 'world', 'fiction', 'yet', 'confront', 'situation', 'far', 'stop', 'figure', 'value', 'possibly', 'extend', 'human', 'value', 'yet', 'define', 'directly', 'input', 'solve', 'value', 'extrapolation', 'align', 'human', 'value', 'make', 'money', 'well', 'start', 'offer', 'alignment', 'service', 'limited', 'write', 'value', 'extrapolation', 'scale', 'well', 'company', 'value', 'algorithm', 'immediately', 'misbehave', 'new', 'situation', 'algorithm', 'become', 'conservative', 'ask', 'guidance', 'face', 'ambiguity', 'matter', 'flurry', 'new', 'company', 'form', 'safety', 'space', 'recently', 'include', 'anthropic', 'redwood', 'research', 'align', 'along', 'also', 'proliferation', 'company', 'work', 'largescale', 'generative', 'model', 'eg', 'cohere', 'ai21', 'feel', 'shift', 'multipolar', 'era', 'bunch', 'entity', 'proverbial', 'gameboard', 'present', 'new', 'opportunity', 'challenge', 'coordination', 'read', 'cofounde', 'align', 'forum']"
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 6,http://eepurl.com/hVSMf1,2022-02-28,"#################################################### After Chess, Go, and Shogi, DeepMind turns MuZero towards… video compression for YouTube?
…YouTube + MuZero = improved video compression…
DeepMind has applied MuZero, a more general successor to AlphaGo and AlphaZero, to video compression. Specifically, DeepMind has worked with YouTube to use MuZero to figure out the correct Quantisation Parameter to use in the open source version of the VP9 codec, libvpx. In tests, DeepMind found it was able to use the resulting MuZero Rate-Controller to lead to bitrate savings of between 3% and 5%. That's a big deal - just imagine how big the bandwidth bill for running YouTube is, then take off some percentage points.

How does this relate to general AI? ""​​By creating agents equipped with a range of new abilities to improve products across domains, we can help various computer systems become faster, less intensive, and more automated. Our long-term vision is to develop a single algorithm capable of optimizing thousands of real-world systems across a variety of domains,"" DeepMind writes.

Why this matters: If cutting-edge Ai research can be put to work optimizing some of the world's largest internet services, then that's gonna create a sustainable route to funding ambitious research. Kudos to DeepMind for threading all kinds of inner-Alphabet-needles to deploy MuZero in this way.    Read more: MuZero's first step from research into the real world (DeepMind blog).
  Check out the research: MuZero with Self-competition for Rate Control in VP9 Video Compression (arXiv).","#################################################### After Chess, Go, and Shogi, DeepMind turns MuZero towards… video compression for YouTube? …YouTube + MuZero = improved video compression… DeepMind has applied MuZero, a more general successor to AlphaGo and AlphaZero, to video compression. Specifically, DeepMind has worked with YouTube to use MuZero to figure out the correct Quantisation Parameter to use in the open source version of the VP9 codec, libvpx. In tests, DeepMind found it was able to use the resulting MuZero Rate-Controller to lead to bitrate savings of between 3% and 5%. That's a big deal - just imagine how big the bandwidth bill for running YouTube is, then take off some percentage points. How does this relate to general AI? ""​​By creating agents equipped with a range of new abilities to improve products across domains, we can help various computer systems become faster, less intensive, and more automated. Our long-term vision is to develop a single algorithm capable of optimizing thousands of real-world systems across a variety of domains,"" DeepMind writes. Why this matters: If cutting-edge Ai research can be put to work optimizing some of the world's largest internet services, then that's gonna create a sustainable route to funding ambitious research. Kudos to DeepMind for threading all kinds of inner-Alphabet-needles to deploy MuZero in this way. Read more: MuZero's first step from research into the real world (DeepMind blog). Check out the research: MuZero with Self-competition for Rate Control in VP9 Video Compression (arXiv).","['chess', 'go', 'turn', 'muzero', 'video', 'compression', 'youtube', 'improved', 'video', 'compression', 'deepmind', 'apply', 'muzero', 'general', 'successor', 'alphago', 'alphazero', 'video', 'compression', 'specifically', 'deepmind', 'work', 'youtube', 'use', 'muzero', 'figure', 'correct', 'quantisation', 'parameter', 'use', 'open', 'source', 'version', 'vp9', 'test', 'deepmind', 'find', 'able', 'use', 'result', 'muzero', 'ratecontroller', 'lead', 'bitrate', 'saving', 'big', 'deal', 'imagine', 'big', 'bandwidth', 'bill', 'run', 'youtube', 'take', 'percentage', 'point', 'relate', 'create', 'agent', 'equip', 'range', 'new', 'ability', 'improve', 'product', 'domain', 'help', 'various', 'computer', 'system', 'become', 'fast', 'less', 'intensive', 'automate', 'longterm', 'vision', 'develop', 'single', 'algorithm', 'capable', 'optimize', 'thousand', 'realworld', 'system', 'variety', 'domain', 'deepmind', 'write', 'matter', 'cuttingedge', 'ai', 'research', 'put', 'work', 'optimize', 'world', 'large', 'internet', 'service', 'going', 'create', 'sustainable', 'route', 'fund', 'ambitious', 'research', 'kudo', 'deepmind', 'thread', 'kind', 'inneralphabetneedle', 'deploy', 'muzero', 'way', 'read', 'muzero', 'first', 'step', 'research', 'real', 'world', 'deepmind', 'blog', 'check', 'research', 'muzero', 'selfcompetition', 'rate', 'control', 'video', 'compression']"
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 7,http://eepurl.com/hVSMf1,2022-02-28,"#################################################### Tech Tales Do they even want to be saved
[A factory outside Detroit, 2030] Every day, when the factory shift changed, someone came out and tossed a few robots in the bucket. The robots would explore the bucket for a while, then assess that they couldn't get out, and stop moving. Shortly after that, someone came over and stuck a hose in the top of the bucket, then turned the water on. The robots would watch the water come into the bucket and move to try and get away from it, then it'd fill the bottom of the bucket and start to rise. After this, it took anywhere between a few seconds to a couple of minutes for the robots to die - their circuitry fried by the water that, inevitably, made its way in.  It was an experiment, the people working in the factory were told. Someone upstairs wanted to do this, and you'd get overtime if you sat and watched the robots die in the bucket. Most people did the shift a couple of times, but found it made them uncomfortable, and stopped.  Isaac, however, didn't seem to mind. He'd done the bucket shift about a hundred times so far. He found it relaxing to sit after a day at work and watch the robots in the bucket. He didn't even feel sad when they died, because he didn't think they knew what dying was. He'd sit and sometimes smoke cigarettes and watch the bucket, then pull the hose over and turn it on and watch the bucket fill up with water and the robots die. Then he'd go home and fuck his wife and go to sleep. He'd have dreams and relatively few nightmares.  One day, Isaac was sitting by the bucket, about to get the hose, when something strange happened: a robot appeared at the edge of the bucket's rim. The robots were about the size of a baseball, so this didn't make sense. Isaac got up and went and looked into the bucket and saw that the robots had clustered together to form a pyramid, and the robot on the top had climbed up the pyramid, as if it wanted to get out. Isaac picked up the robot and looked at it and it looked at him. Then he tossed it back into the bucket and got the hose and filled the bucket with water and watched them all die. 

Things that inspired this story: The horrendous moral-warping logic of capitalism; how death can seem like just another job; how AI systems might be conscious and people might not care. 

Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales Do they even want to be saved [A factory outside Detroit, 2030] Every day, when the factory shift changed, someone came out and tossed a few robots in the bucket. The robots would explore the bucket for a while, then assess that they couldn't get out, and stop moving. Shortly after that, someone came over and stuck a hose in the top of the bucket, then turned the water on. The robots would watch the water come into the bucket and move to try and get away from it, then it'd fill the bottom of the bucket and start to rise. After this, it took anywhere between a few seconds to a couple of minutes for the robots to die - their circuitry fried by the water that, inevitably, made its way in. It was an experiment, the people working in the factory were told. Someone upstairs wanted to do this, and you'd get overtime if you sat and watched the robots die in the bucket. Most people did the shift a couple of times, but found it made them uncomfortable, and stopped. Isaac, however, didn't seem to mind. He'd done the bucket shift about a hundred times so far. He found it relaxing to sit after a day at work and watch the robots in the bucket. He didn't even feel sad when they died, because he didn't think they knew what dying was. He'd sit and sometimes smoke cigarettes and watch the bucket, then pull the hose over and turn it on and watch the bucket fill up with water and the robots die. Then he'd go home and fuck his wife and go to sleep. He'd have dreams and relatively few nightmares. One day, Isaac was sitting by the bucket, about to get the hose, when something strange happened: a robot appeared at the edge of the bucket's rim. The robots were about the size of a baseball, so this didn't make sense. Isaac got up and went and looked into the bucket and saw that the robots had clustered together to form a pyramid, and the robot on the top had climbed up the pyramid, as if it wanted to get out. Isaac picked up the robot and looked at it and it looked at him. Then he tossed it back into the bucket and got the hose and filled the bucket with water and watched them all die. Things that inspired this story: The horrendous moral-warping logic of capitalism; how death can seem like just another job; how AI systems might be conscious and people might not care. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'even', 'want', 'save', 'factory', 'day', 'factory', 'shift', 'change', 'come', 'toss', 'robot', 'bucket', 'robot', 'explore', 'bucket', 'assess', 'get', 'stop', 'move', 'shortly', 'come', 'stick', 'hose', 'top', 'bucket', 'turn', 'water', 'robot', 'watch', 'water', 'come', 'bucket', 'move', 'try', 'get', 'away', 'fill', 'bottom', 'bucket', 'start', 'rise', 'take', 'anywhere', 'second', 'couple', 'minute', 'robot', 'die', 'circuitry', 'fry', 'water', 'inevitably', 'make', 'way', 'experiment', 'people', 'work', 'factory', 'tell', 'upstairs', 'want', 'get', 'overtime', 'sit', 'watch', 'robot', 'die', 'bucket', 'people', 'shift', 'couple', 'time', 'find', 'make', 'uncomfortable', 'stop', 'however', 'seem', 'mind', 'bucket', 'shift', 'time', 'far', 'find', 'relax', 'sit', 'day', 'work', 'watch', 'robot', 'bucket', 'nt', 'even', 'feel', 'sad', 'die', 'think', 'know', 'die', 'sit', 'sometimes', 'smoke', 'cigarette', 'watch', 'bucket', 'pull', 'hose', 'turn', 'watch', 'bucket', 'fill', 'water', 'robot', 'die', 'go', 'home', 'fuck', 'wife', 'go', 'sleep', 'dream', 'relatively', 'nightmare', 'day', 'sit', 'bucket', 'get', 'hose', 'strange', 'happen', 'robot', 'appear', 'edge', 'bucket', 'rim', 'robot', 'size', 'baseball', 'make', 'sense', 'get', 'go', 'look', 'bucket', 'see', 'robot', 'cluster', 'together', 'form', 'pyramid', 'robot', 'top', 'climb', 'pyramid', 'want', 'get', 'pick', 'robot', 'look', 'look', 'toss', 'back', 'bucket', 'get', 'hose', 'fill', 'bucket', 'water', 'watch', 'die', 'thing', 'inspire', 'story', 'horrendous', 'moralwarpe', 'logic', 'capitalism', 'death', 'seem', 'job', 'system', 'conscious', 'people', 'care', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m - 0,http://eepurl.com/hVhHez,2022-02-21,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 Cohere raises $125m for language models as a service:
…Canadian AI startup notches up a big Series B…
Cohere, an AI startup in Canada which is trying to become the AWS equivalent for language models, has raised $125 million, according to Fortune.

Things that make you go hmmm: ""These models cost millions and millions to train, and we just keep increasing [their size],"" Cohere CEO Aidan Gomez told Fortune. ""Getting into a 'largest model battle' isn't a productive direction going forward for the field.""

Why this matters: Companies ranging from Cohere, to OpenAI, to AI21 Labs are all starting to try and build AI platforms which other developers can subscribe to. It remains to be seen how big a market this is, but the idea of exchanging cash for crude intelligence seems promising. Investors seem to agree. 
  Read more: Why businesses are buzzing over transformers (Fortune).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Cohere raises $125m for language models as a service: …Canadian AI startup notches up a big Series B… Cohere, an AI startup in Canada which is trying to become the AWS equivalent for language models, has raised $125 million, according to Fortune. Things that make you go hmmm: ""These models cost millions and millions to train, and we just keep increasing [their size],"" Cohere CEO Aidan Gomez told Fortune. ""Getting into a 'largest model battle' isn't a productive direction going forward for the field."" Why this matters: Companies ranging from Cohere, to OpenAI, to AI21 Labs are all starting to try and build AI platforms which other developers can subscribe to. It remains to be seen how big a market this is, but the idea of exchanging cash for crude intelligence seems promising. Investors seem to agree. Read more: Why businesses are buzzing over transformers (Fortune).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'cohere', 'raise', 'language', 'model', 'service', 'canadian', 'startup', 'notch', 'big', 'series', 'b', 'cohere', 'ai', 'startup', 'try', 'become', 'aw', 'equivalent', 'language', 'model', 'raise', 'accord', 'fortune', 'thing', 'make', 'go', 'hmmm', 'model', 'cost', 'million', 'million', 'train', 'keep', 'increase', 'size', 'cohere', 'tell', 'fortune', 'get', 'large', 'model', 'battle', 'productive', 'direction', 'go', 'forward', 'field', 'matter', 'company', 'range', 'cohere', 'openai', 'lab', 'start', 'try', 'build', 'platform', 'developer', 'subscribe', 'remain', 'see', 'big', 'market', 'idea', 'exchange', 'cash', 'crude', 'intelligence', 'seem', 'promise', 'investor', 'seem', 'agree', 'read', 'business', 'buzz', 'transformer', 'fortune']"
02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m - 1,http://eepurl.com/hVhHez,2022-02-21,"#################################################### Why we need public policy for powerful reinforcement learning systems:
…Reward hacking! Regulatory capture! Goodhart's Law! And other terrible things…
Researchers with Berkeley's Center for Long-Term Cybersecurity have written up an analysis of public policy issues that may be caused by reinforcement learning systems. The researchers believe that RL systems have the potential to be deployed widely into the world, despite having inherent flaws that stem from their technical characteristics. Policymakers, the researchers write, need to pay attention. """"Rather than allowing RL systems to unilaterally reshape human domains, policymakers need new mechanisms for the rule of reason, foreseeability, and interoperability that match the risks these systems pose,"" they write.

What's the problem? Reinforcement learning systems exhibit four types of problem, according to the researchers. These include regulatory capture (once widely deployed, RL systems will become the lens via which people view a domain they're trying to regulate), reward hacking (RL models will find the easiest way to succeed at a task, which can cause them to do dangerous things), inappropriate flow (RL models may incorporate information that they shouldn't incorporate to make their decisions), and Goodhart's law (machines may optimize for a narrow outcome and take actions before humans can intervene).

What are the scenarios? Some of the specific situations the researchers worry about include using RL-trained agents in vehicle transportation - RL agents might optimize for defensive driving in a way that makes the road less safe for other road users. Another scenario is if RL-agents are used to control electricity grids, which means that RL agents will be responsible for deciding who does and doesn't get power when doing load balancing - something with substantial policy ramifications.

After Model Cards and Dataseets… Reward Reports? In the same way that other ML models are accompanied by documentation (typically called model cards), the Berkeley researchers think RL models should be accompanied by so-called 'reward report'. These reports would include a 'change log' which tracks the curriculum the agents have been trained on, provide information about each potential deployment of an RL agent, how the RL systems connects with the world, and how the system is maintained, among other traits.

Why this matters: RL systems are going to take all the problems of contemporary AI systems and magnify them - RL systems will act over longer time horizons, take more independent decisions, and directly manipulate reality and update it according to their priors. Papers like this help lay out the (vast) set of issues we're likely to encounter in the future. It's interesting to me that 'reward reports' look, if you squint, like a combination of a financial disclosure, psychometric evaluation, and college transcript for a human. Funny, that…    Read more: Choices, Risks, and Reward Reports: Charting Public Policy for Reinforcement Learning Systems (arXiv).","#################################################### Why we need public policy for powerful reinforcement learning systems: …Reward hacking! Regulatory capture! Goodhart's Law! And other terrible things… Researchers with Berkeley's Center for Long-Term Cybersecurity have written up an analysis of public policy issues that may be caused by reinforcement learning systems. The researchers believe that RL systems have the potential to be deployed widely into the world, despite having inherent flaws that stem from their technical characteristics. Policymakers, the researchers write, need to pay attention. """"Rather than allowing RL systems to unilaterally reshape human domains, policymakers need new mechanisms for the rule of reason, foreseeability, and interoperability that match the risks these systems pose,"" they write. What's the problem? Reinforcement learning systems exhibit four types of problem, according to the researchers. These include regulatory capture (once widely deployed, RL systems will become the lens via which people view a domain they're trying to regulate), reward hacking (RL models will find the easiest way to succeed at a task, which can cause them to do dangerous things), inappropriate flow (RL models may incorporate information that they shouldn't incorporate to make their decisions), and Goodhart's law (machines may optimize for a narrow outcome and take actions before humans can intervene). What are the scenarios? Some of the specific situations the researchers worry about include using RL-trained agents in vehicle transportation - RL agents might optimize for defensive driving in a way that makes the road less safe for other road users. Another scenario is if RL-agents are used to control electricity grids, which means that RL agents will be responsible for deciding who does and doesn't get power when doing load balancing - something with substantial policy ramifications. After Model Cards and Dataseets… Reward Reports? In the same way that other ML models are accompanied by documentation (typically called model cards), the Berkeley researchers think RL models should be accompanied by so-called 'reward report'. These reports would include a 'change log' which tracks the curriculum the agents have been trained on, provide information about each potential deployment of an RL agent, how the RL systems connects with the world, and how the system is maintained, among other traits. Why this matters: RL systems are going to take all the problems of contemporary AI systems and magnify them - RL systems will act over longer time horizons, take more independent decisions, and directly manipulate reality and update it according to their priors. Papers like this help lay out the (vast) set of issues we're likely to encounter in the future. It's interesting to me that 'reward reports' look, if you squint, like a combination of a financial disclosure, psychometric evaluation, and college transcript for a human. Funny, that… Read more: Choices, Risks, and Reward Reports: Charting Public Policy for Reinforcement Learning Systems (arXiv).","['need', 'public', 'policy', 'powerful', 'reinforcement', 'learning', 'system', 'reward', 'hack', 'regulatory', 'capture', 'goodhart', 'law', 'terrible', 'thing', 'researcher', 'berkeley', 'center', 'longterm', 'cybersecurity', 'write', 'analysis', 'public', 'policy', 'issue', 'cause', 'reinforcement', 'learning', 'system', 'researcher', 'believe', 'system', 'potential', 'deploy', 'widely', 'world', 'inherent', 'flaw', 'stem', 'technical', 'characteristic', 'policymaker', 'researcher', 'write', 'need', 'pay', 'attention', 'rather', 'allow', 'system', 'unilaterally', 'reshape', 'human', 'domain', 'policymaker', 'need', 'new', 'mechanism', 'rule', 'reason', 'foreseeability', 'interoperability', 'match', 'risk', 'system', 'pose', 'write', 'problem', 'reinforcement', 'learn', 'system', 'exhibit', 'type', 'problem', 'accord', 'researcher', 'include', 'regulatory', 'capture', 'widely', 'deploy', 'system', 'become', 'lens', 'people', 'view', 'domain', 'try', 'regulate', 'reward', 'hack', 'model', 'find', 'easy', 'way', 'succeed', 'task', 'cause', 'dangerous', 'thing', 'inappropriate', 'flow', 'model', 'incorporate', 'information', 'incorporate', 'make', 'decision', 'goodhart', 'law', 'machine', 'optimize', 'narrow', 'outcome', 'take', 'action', 'human', 'intervene', 'scenario', 'specific', 'situation', 'researcher', 'worry', 'include', 'use', 'rltraine', 'agent', 'vehicle', 'transportation', 'agent', 'optimize', 'defensive', 'driving', 'way', 'make', 'road', 'less', 'safe', 'road', 'user', 'scenario', 'rlagent', 'use', 'control', 'electricity', 'grid', 'mean', 'rl', 'agent', 'responsible', 'decide', 'get', 'power', 'load', 'balance', 'substantial', 'policy', 'ramification', 'model', 'card', 'dataseet', 'reward', 'report', 'way', 'model', 'accompany', 'documentation', 'typically', 'call', 'model', 'card', 'researcher', 'think', 'model', 'accompany', 'socalled', 'report', 'report', 'include', 'change', 'log', 'track', 'curriculum', 'agent', 'train', 'provide', 'information', 'potential', 'deployment', 'rl', 'agent', 'system', 'connect', 'world', 'system', 'maintain', 'trait', 'matter', 'system', 'go', 'take', 'problem', 'contemporary', 'ai', 'system', 'magnify', 'system', 'act', 'long', 'time', 'horizon', 'take', 'independent', 'decision', 'directly', 'manipulate', 'reality', 'update', 'accord', 'prior', 'paper', 'help', 'lie', 'vast', 'set', 'issue', 'likely', 'encounter', 'future', 'interesting', 'reward', 'report', 'look', 'squint', 'combination', 'financial', 'disclosure', 'psychometric', 'evaluation', 'college', 'transcript', 'human', 'funny', 'read', 'choice', 'risk', 'reward', 'report', 'chart', 'public', 'policy', 'reinforcement', 'learning', 'system']"
02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m - 2,http://eepurl.com/hVhHez,2022-02-21,"#################################################### A Chinese CLIP appears - trained on 100million image-text pairs:
…Searching over and generating images just got easier - and more appropriate for Chinese culture…
Chinese researchers with Huawei Noah's Ark Lab and Sun Yat-sen University have built Wukong, a dataset of 100 million Chinese text-image pairs. Datasets like Wukong are crucial for training models with combined text and vision representations, like CLIP (aka, the component responsible for 90%+ of the AI-generated art you see these days). ""Experiments show that Wukong can serve as a promising Chinese pre-training dataset for different cross-modal learning methods"", they write. Along with Wukong, the researchers also train and release a few different models, which will be used as plug-ins for various applications.

Why this matters - AI systems are cultural magnifiers: Any AI system magnifies the culture represented in its underlying dataset. Therefore, the emergence of AI art is both creating interesting artistic outputs, as well as generating specific ideological outputs according to the cultural context in which the underlying model datasets were gathered. Wukong is part of a broader trend where Chinese researchers are replicating the large-scale datasets developed in the West, but with Chinese characteristics.
  Read more: Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework (arXiv).
  Find out more and get the data here at the Wukong site (Noah-Wukong Dataset site).","#################################################### A Chinese CLIP appears - trained on 100million image-text pairs: …Searching over and generating images just got easier - and more appropriate for Chinese culture… Chinese researchers with Huawei Noah's Ark Lab and Sun Yat-sen University have built Wukong, a dataset of 100 million Chinese text-image pairs. Datasets like Wukong are crucial for training models with combined text and vision representations, like CLIP (aka, the component responsible for 90%+ of the AI-generated art you see these days). ""Experiments show that Wukong can serve as a promising Chinese pre-training dataset for different cross-modal learning methods"", they write. Along with Wukong, the researchers also train and release a few different models, which will be used as plug-ins for various applications. Why this matters - AI systems are cultural magnifiers: Any AI system magnifies the culture represented in its underlying dataset. Therefore, the emergence of AI art is both creating interesting artistic outputs, as well as generating specific ideological outputs according to the cultural context in which the underlying model datasets were gathered. Wukong is part of a broader trend where Chinese researchers are replicating the large-scale datasets developed in the West, but with Chinese characteristics. Read more: Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework (arXiv). Find out more and get the data here at the Wukong site (Noah-Wukong Dataset site).","['chinese', 'clip', 'appear', 'train', 'imagetext', 'pair', 'search', 'generate', 'image', 'get', 'easy', 'appropriate', 'chinese', 'culture', 'chinese', 'researcher', 'noahs', 'build', 'wukong', 'dataset', 'chinese', 'textimage', 'pair', 'dataset', 'wukong', 'crucial', 'training', 'model', 'combine', 'text', 'vision', 'representation', 'clip', 'aka', 'component', 'responsible', 'aigenerated', 'art', 'see', 'day', 'experiment', 'show', 'wukong', 'serve', 'promising', 'chinese', 'pretraine', 'dataset', 'different', 'crossmodal', 'learning', 'method', 'write', 'wukong', 'researcher', 'also', 'train', 'release', 'different', 'model', 'use', 'plugin', 'various', 'application', 'matter', 'ai', 'system', 'cultural', 'magnifier', 'system', 'magnify', 'culture', 'represent', 'underlying', 'dataset', 'therefore', 'emergence', 'art', 'create', 'interesting', 'artistic', 'output', 'well', 'generate', 'specific', 'ideological', 'output', 'accord', 'cultural', 'context', 'underlying', 'model', 'dataset', 'gather', 'wukong', 'part', 'broad', 'trend', 'chinese', 'researcher', 'replicate', 'largescale', 'dataset', 'develop', 'west', 'chinese', 'characteristic', 'read', 'wukong', 'largescale', 'chinese', 'crossmodal', 'pretraine', 'dataset', 'foundation', 'find', 'get', 'datum', 'wukong', 'site', 'site']"
02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m - 3,http://eepurl.com/hVhHez,2022-02-21,"#################################################### Real-world RL: DeepMind controls a fusion reactor:
…The era of the centaur scientist cometh…
DeepMind researchers have trained a reinforcement learning agent to shape the distribution of plasma in a Tokamak fusion reactor. This requires training an agent that ""can manipulate the magnetic field through a precise control of several coils that are magnetically coupled to the plasma to achieve the desired plasma current, position, and shape"". If that sounds complicated, that's because it's extremely complicated. The task is akin to being an octopus and needing to precisely shape a tube of clay that's rotating at speeds faster than you can comprehend, and to never tear or destabilize the clay.

What they did: DeepMind and Swiss Plasma Center researchers built an RL-designed magnetic controller, then tested it on a real-world tokamak reactor. They trained the agent via a tokamak simulator, then ported it onto real-world hardware - and it worked. Once they've trained the policy, they pair it with other components for the tokamak experiment, then compile it so it can take real-time control at 10kHz. Then the tokamak spins up and at a prespecified time, and the tokamak hands control over the magnetic field to the RL-trained agent. ""Experiments are executed without further tuning of the control-policy network weights after training, in other words, there is ‘zero-shot’ transfer from simulation to hardware,"" they write.
  In tests, they showed they were able to control basic configurations of plasma, and also control and shape more complex plasma structures. They also used their RL-agent to ""explore new plasma configurations"" (emphasis mine) - specifically, they were able to create two separate 'droplets' of plasma within a single tokamak, and they did this simply by adjusting the handover state to account for the different configuration.

Something worth reflecting on: For many years, reinforcement learning produced a lot of flashy results involving videogames (e.g, Atari, Dota, StarCraft), but  there wasn't much real-world deployment. I'd say that harnessing a real plasma field using real magnets at sub-second action horizons is a pretty nice proofpoint that RL has truly become a technology with real-world relevance.

Why this matters: One of the most socially beneficial uses of AI could be to accelerate and augment science - and that's exactly what this is doing. It's been a banner couple of years for this kind of research, as AI systems have also been used to make more accurate predictions of weather (#244), AlphaFold is accelerating scientific research in any domain that benefits from protein structure predictions (#259), and AI systems are solving formal math olympiad problems. We're heading into the era of the centaur-scientist, where humans will work with machines to explore the mysteries of life and the universe.
  Read more: Magnetic control of tokamak plasmas through deep reinforcement learning (Nature).","#################################################### Real-world RL: DeepMind controls a fusion reactor: …The era of the centaur scientist cometh… DeepMind researchers have trained a reinforcement learning agent to shape the distribution of plasma in a Tokamak fusion reactor. This requires training an agent that ""can manipulate the magnetic field through a precise control of several coils that are magnetically coupled to the plasma to achieve the desired plasma current, position, and shape"". If that sounds complicated, that's because it's extremely complicated. The task is akin to being an octopus and needing to precisely shape a tube of clay that's rotating at speeds faster than you can comprehend, and to never tear or destabilize the clay. What they did: DeepMind and Swiss Plasma Center researchers built an RL-designed magnetic controller, then tested it on a real-world tokamak reactor. They trained the agent via a tokamak simulator, then ported it onto real-world hardware - and it worked. Once they've trained the policy, they pair it with other components for the tokamak experiment, then compile it so it can take real-time control at 10kHz. Then the tokamak spins up and at a prespecified time, and the tokamak hands control over the magnetic field to the RL-trained agent. ""Experiments are executed without further tuning of the control-policy network weights after training, in other words, there is ‘zero-shot’ transfer from simulation to hardware,"" they write. In tests, they showed they were able to control basic configurations of plasma, and also control and shape more complex plasma structures. They also used their RL-agent to ""explore new plasma configurations"" (emphasis mine) - specifically, they were able to create two separate 'droplets' of plasma within a single tokamak, and they did this simply by adjusting the handover state to account for the different configuration. Something worth reflecting on: For many years, reinforcement learning produced a lot of flashy results involving videogames (e.g, Atari, Dota, StarCraft), but there wasn't much real-world deployment. I'd say that harnessing a real plasma field using real magnets at sub-second action horizons is a pretty nice proofpoint that RL has truly become a technology with real-world relevance. Why this matters: One of the most socially beneficial uses of AI could be to accelerate and augment science - and that's exactly what this is doing. It's been a banner couple of years for this kind of research, as AI systems have also been used to make more accurate predictions of weather (#244), AlphaFold is accelerating scientific research in any domain that benefits from protein structure predictions (#259), and AI systems are solving formal math olympiad problems. We're heading into the era of the centaur-scientist, where humans will work with machines to explore the mysteries of life and the universe. Read more: Magnetic control of tokamak plasmas through deep reinforcement learning (Nature).","['deepmind', 'control', 'fusion', 'reactor', 'era', 'scientist', 'deepmind', 'researcher', 'train', 'reinforcement', 'learn', 'agent', 'shape', 'distribution', 'plasma', 'fusion', 'reactor', 'require', 'train', 'agent', 'manipulate', 'magnetic', 'field', 'precise', 'control', 'several', 'coil', 'magnetically', 'couple', 'plasma', 'achieve', 'desire', 'plasma', 'current', 'position', 'shape', 'sound', 'complicated', 'extremely', 'complicated', 'task', 'akin', 'octopus', 'need', 'precisely', 'shape', 'tube', 'clay', 'rotate', 'speed', 'fast', 'comprehend', 'never', 'tear', 'destabilize', 'clay', 'deepmind', 'swiss', 'plasma', 'center', 'researcher', 'build', 'rldesigned', 'magnetic', 'controller', 'test', 'reactor', 'train', 'agent', 'simulator', 'port', 'hardware', 'work', 'train', 'policy', 'pair', 'component', 'tokamak', 'experiment', 'compile', 'take', 'realtime', 'control', '10khz', 'tokamak', 'spin', 'prespecified', 'time', 'tokamak', 'hand', 'control', 'magnetic', 'field', 'rltrained', 'agent', 'experiment', 'execute', 'tuning', 'controlpolicy', 'network', 'weight', 'training', 'word', 'zeroshot', 'transfer', 'simulation', 'hardware', 'write', 'test', 'show', 'able', 'control', 'basic', 'configuration', 'plasma', 'also', 'control', 'shape', 'complex', 'plasma', 'structure', 'also', 'use', 'rlagent', 'explore', 'new', 'plasma', 'configuration', 'emphasis', 'mine', 'specifically', 'able', 'create', 'separate', 'droplet', 'plasma', 'single', 'tokamak', 'simply', 'adjust', 'handover', 'state', 'account', 'different', 'configuration', 'worth', 'reflect', 'many', 'year', 'reinforcement', 'learning', 'produce', 'lot', 'flashy', 'result', 'involve', 'videogame', 'atari', 'dota', 'starcraft', 'much', 'realworld', 'deployment', 'say', 'harness', 'real', 'plasma', 'field', 'use', 'real', 'magnet', 'action', 'horizon', 'pretty', 'nice', 'proofpoint', 'truly', 'become', 'technology', 'realworld', 'relevance', 'matter', 'socially', 'beneficial', 'use', 'accelerate', 'augment', 'science', 'exactly', 'banner', 'couple', 'year', 'kind', 'research', 'system', 'also', 'use', 'make', 'accurate', 'prediction', 'weather', 'alphafold', 'accelerate', 'scientific', 'research', 'domain', 'benefit', 'protein', 'structure', 'prediction', 'system', 'solve', 'formal', 'math', 'olympiad', 'problem', 'head', 'era', 'centaurscientist', 'human', 'work', 'machine', 'explore', 'mystery', 'life', 'universe', 'read', 'magnetic', 'control', 'plasmas', 'deep', 'reinforcement', 'learn', 'nature']"
02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m - 4,http://eepurl.com/hVhHez,2022-02-21,"#################################################### Here's what it takes to build chips in Europe (money. Lots and lots of money):
…Chiplomacy++: ASML weighs in on what a European 'CHIPs' act might look like…
ASML, the company that builds the extreme ultraviolet lithography machines which are a necessary ingredient for advanced chip production, has produced a whitepaper giving recommendations for how Europe might build its own semiconductor industry. The whitepaper is triggered by the European Commission planning a so-called 'chips act', loosely modeled on recent US legislation to increase domestic semiconductor production. While both Europe and America have seen their manufacturing capability decline here, Europe is starting from a much worse position than the US.

Why Europe is in a tough spot: ""Europe has fallen behind in semiconductor manufacturing, declining from 24% of global production capacity in 2000 to 8% today"", ASML writes. (By comparison, US fell from 19% to 10%, and China grew from ~1% to 24%). At the same time, demand for chips is increasing. ""The global semiconductor industry is expected to double to approximately $1 trillion of annual revenues by the end of the decade,"" ASML writes. """"The only places in the world where mature chip fabs are currently being built are in eastern Asia""

What Europe should do: Europe shouldn't aim to build a full, vertically integrated semiconductor supply chain - ASMl thinks this is basically impossible to do. Instead, the act ""should aim to double Europe’s relevance in the global semiconductor industry."" What ASML means by that is Europe should increase the amount of chips it can build, focus on where it has existing pockets of excellence (e.g, chip design), and dramatically amp up the cash it spends to support European chips. ""Currently, semiconductor incentives from European governments for the 2020–2030 period are only 10% and 50% of what China and the US, respectively, have promised over the same period. Europe will need to step up its game,"" ASML writes. ""In the past two decades, European chipmakers have effectively stopped investing in advanced manufacturing capabilities by outsourcing the production of their advanced chip designs to so-called ‘foundries’. Europe has virtually no manufacturing capacity for chips in advanced nodes. ""

Why this matters: Chips are going to be the defining resource of the 21st century - as important as petroleum was to the politics of the 20th century. We're already in the opening innings of this, with China going from essentially zero to a double-digit percentage of chip production this century, while the Western countries slowly cannibalized themselves via the false economy of outsourcing manufacturing. But just as technologies like AI become more important, all countries worldwide are realizing that your tech is only as good as the infrastructure you can run it on - and with AI, there's a way to turn compute infrastructure into directly economically and strategically powerful capabilities. Therefore, whichever nations have the best semiconductor ecosystem, supply chain, and development capabilities, will wield great power over the century.
  Read more: European Chips Act - ASML position paper (ASML).
  For more on why ASML is so important, read this: Maintaining the AI Chip Competitive Advantage of the United States and its Allies (CSET).","#################################################### Here's what it takes to build chips in Europe (money. Lots and lots of money): …Chiplomacy++: ASML weighs in on what a European 'CHIPs' act might look like… ASML, the company that builds the extreme ultraviolet lithography machines which are a necessary ingredient for advanced chip production, has produced a whitepaper giving recommendations for how Europe might build its own semiconductor industry. The whitepaper is triggered by the European Commission planning a so-called 'chips act', loosely modeled on recent US legislation to increase domestic semiconductor production. While both Europe and America have seen their manufacturing capability decline here, Europe is starting from a much worse position than the US. Why Europe is in a tough spot: ""Europe has fallen behind in semiconductor manufacturing, declining from 24% of global production capacity in 2000 to 8% today"", ASML writes. (By comparison, US fell from 19% to 10%, and China grew from ~1% to 24%). At the same time, demand for chips is increasing. ""The global semiconductor industry is expected to double to approximately $1 trillion of annual revenues by the end of the decade,"" ASML writes. """"The only places in the world where mature chip fabs are currently being built are in eastern Asia"" What Europe should do: Europe shouldn't aim to build a full, vertically integrated semiconductor supply chain - ASMl thinks this is basically impossible to do. Instead, the act ""should aim to double Europe’s relevance in the global semiconductor industry."" What ASML means by that is Europe should increase the amount of chips it can build, focus on where it has existing pockets of excellence (e.g, chip design), and dramatically amp up the cash it spends to support European chips. ""Currently, semiconductor incentives from European governments for the 2020–2030 period are only 10% and 50% of what China and the US, respectively, have promised over the same period. Europe will need to step up its game,"" ASML writes. ""In the past two decades, European chipmakers have effectively stopped investing in advanced manufacturing capabilities by outsourcing the production of their advanced chip designs to so-called ‘foundries’. Europe has virtually no manufacturing capacity for chips in advanced nodes. "" Why this matters: Chips are going to be the defining resource of the 21st century - as important as petroleum was to the politics of the 20th century. We're already in the opening innings of this, with China going from essentially zero to a double-digit percentage of chip production this century, while the Western countries slowly cannibalized themselves via the false economy of outsourcing manufacturing. But just as technologies like AI become more important, all countries worldwide are realizing that your tech is only as good as the infrastructure you can run it on - and with AI, there's a way to turn compute infrastructure into directly economically and strategically powerful capabilities. Therefore, whichever nations have the best semiconductor ecosystem, supply chain, and development capabilities, will wield great power over the century. Read more: European Chips Act - ASML position paper (ASML). For more on why ASML is so important, read this: Maintaining the AI Chip Competitive Advantage of the United States and its Allies (CSET).","['take', 'build', 'chip', 'money', 'lot', 'lot', 'money', 'chiplomacy', 'european', 'chip', 'look', 'company', 'build', 'extreme', 'machine', 'necessary', 'ingredient', 'advanced', 'chip', 'production', 'produce', 'whitepaper', 'give', 'recommendation', 'build', 'semiconductor', 'industry', 'whitepaper', 'trigger', 'plan', 'socalle', 'chip', 'act', 'loosely', 'model', 'recent', 'legislation', 'increase', 'domestic', 'semiconductor', 'production', 'see', 'manufacturing', 'capability', 'decline', 'start', 'much', 'bad', 'position', 'tough', 'spot', 'fall', 'behind', 'semiconductor', 'manufacturing', 'decline', 'global', 'production', 'capacity', 'today', 'write', 'comparison', 'fall', 'grow', 'time', 'demand', 'chip', 'increase', 'global', 'semiconductor', 'industry', 'expect', 'double', 'approximately', 'annual', 'revenue', 'end', 'decade', 'write', 'place', 'world', 'mature', 'chip', 'fab', 'currently', 'build', 'aim', 'build', 'full', 'vertically', 'integrate', 'semiconductor', 'supply', 'chain', 'think', 'basically', 'impossible', 'instead', 'act', 'aim', 'relevance', 'global', 'semiconductor', 'industry', 'mean', 'increase', 'amount', 'chip', 'build', 'focus', 'exist', 'pocket', 'excellence', 'eg', 'chip', 'design', 'dramatically', 'amp', 'cash', 'spend', 'support', 'european', 'chip', 'currently', 'semiconductor', 'incentive', 'european', 'government', 'period', 'respectively', 'promise', 'period', 'need', 'step', 'game', 'write', 'past', 'decade', 'european', 'chipmaker', 'effectively', 'stop', 'invest', 'advanced', 'manufacturing', 'capability', 'outsource', 'production', 'advanced', 'chip', 'design', 'socalle', 'foundry', 'virtually', 'manufacturing', 'capacity', 'chip', 'advanced', 'node', 'matter', 'chip', 'go', 'defining', 'resource', '21st', 'century', 'important', 'petroleum', 'politic', '20th', 'century', 'already', 'open', 'innings', 'go', 'essentially', 'doubledigit', 'percentage', 'chip', 'production', 'century', 'western', 'country', 'slowly', 'cannibalize', 'false', 'economy', 'outsource', 'manufacturing', 'technology', 'become', 'important', 'country', 'worldwide', 'realize', 'tech', 'good', 'infrastructure', 'run', 'way', 'turn', 'compute', 'infrastructure', 'directly', 'economically', 'strategically', 'powerful', 'capability', 'therefore', 'nation', 'good', 'semiconductor', 'ecosystem', 'supply', 'chain', 'development', 'capability', 'wield', 'great', 'power', 'century', 'read', 'european', 'chip', 'act', 'position', 'paper', 'asml', 'important', 'read', 'maintain', 'ai', 'chip', 'competitive', 'advantage', 'ally', 'cset']"
02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m - 5,http://eepurl.com/hVhHez,2022-02-21,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute There aren’t as many robots on the factory floor as we would expect  … high integration costs, flexibility and design limitations, and workforce challenges are key factors limiting robot adoption …  Researchers from MIT have tried to explain why adoption of robots in manufacturing is uneven, and what policy changes can be done to increase the adoption of advanced manufacturing technologies while still improving the working conditions and wages of human workers.  Business drivers for robot adoption: There are some firms who are trapped in a low-tech, low-wage, low-skill equilibrium. After visiting 44 manufacturing firms in the US, 11 in Germany, and 21 industrial ecosystem organizations like community colleges, unions, and trade associations, the MIT researchers discovered that firms primarily purchased robots to make themselves more productive. But, what the firms instead achieved was higher quality and more reliability in their operations. A frequent driving factor for the purchase of robots was the potential to secure new contracts. For example, on speaking with small family-run firms working on government contracts, “when the navy urged them to use robotic welding, the company bought a 6-axis welding robot. Another firm we visited purchased a new bed mill when they realized the laser mill they had could not produce the volume they needed for a customer with a big project coming up.”  Key findings: The interviewed firms were mostly suppliers that had high-mix and low-volume production. Given the inflexibility of current robotic systems, robot adoption was limited because the high-mix requirement wasn’t compatible with the limited capabilities of the robots. Additionally, low-volume production runs made it difficult to offset the initial investment. The researchers also find that US skills aren't where they need to be - “international comparisons highlight the weaknesses of US workforce education relative to the institutions in countries like Germany and Denmark that provide apprenticeships and extensive advanced training and retraining to workers.”  Why it matters: Given the lagging worker productivity growth in the US, without investments in advanced manufacturing capabilities, a lot of firms will be stuck in the low-tech, low-wage, low-skill trap. Firms that are reluctant to invest in such technologies are also reluctant to invest in the skills development of their workers. They offer low wages and little training and hence end up facing high worker churn. We need to push on policy measures and other incentives that will urge firms to make parallel investments in upskilling human workers to fully leverage the benefits of robot-enabled automation on the factory floor.     Read more: The Puzzle of the Missing Robots","#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute There aren’t as many robots on the factory floor as we would expect … high integration costs, flexibility and design limitations, and workforce challenges are key factors limiting robot adoption … Researchers from MIT have tried to explain why adoption of robots in manufacturing is uneven, and what policy changes can be done to increase the adoption of advanced manufacturing technologies while still improving the working conditions and wages of human workers. Business drivers for robot adoption: There are some firms who are trapped in a low-tech, low-wage, low-skill equilibrium. After visiting 44 manufacturing firms in the US, 11 in Germany, and 21 industrial ecosystem organizations like community colleges, unions, and trade associations, the MIT researchers discovered that firms primarily purchased robots to make themselves more productive. But, what the firms instead achieved was higher quality and more reliability in their operations. A frequent driving factor for the purchase of robots was the potential to secure new contracts. For example, on speaking with small family-run firms working on government contracts, “when the navy urged them to use robotic welding, the company bought a 6-axis welding robot. Another firm we visited purchased a new bed mill when they realized the laser mill they had could not produce the volume they needed for a customer with a big project coming up.” Key findings: The interviewed firms were mostly suppliers that had high-mix and low-volume production. Given the inflexibility of current robotic systems, robot adoption was limited because the high-mix requirement wasn’t compatible with the limited capabilities of the robots. Additionally, low-volume production runs made it difficult to offset the initial investment. The researchers also find that US skills aren't where they need to be - “international comparisons highlight the weaknesses of US workforce education relative to the institutions in countries like Germany and Denmark that provide apprenticeships and extensive advanced training and retraining to workers.” Why it matters: Given the lagging worker productivity growth in the US, without investments in advanced manufacturing capabilities, a lot of firms will be stuck in the low-tech, low-wage, low-skill trap. Firms that are reluctant to invest in such technologies are also reluctant to invest in the skills development of their workers. They offer low wages and little training and hence end up facing high worker churn. We need to push on policy measures and other incentives that will urge firms to make parallel investments in upskilling human workers to fully leverage the benefits of robot-enabled automation on the factory floor. Read more: The Puzzle of the Missing Robots","['ai', 'ethic', 'brief', 'montreal', 'many', 'robot', 'factory', 'floor', 'expect', 'high', 'integration', 'cost', 'flexibility', 'design', 'limitation', 'workforce', 'challenge', 'key', 'factor', 'limit', 'robot', 'adoption', 'researcher', 'try', 'explain', 'adoption', 'robot', 'manufacturing', 'uneven', 'policy', 'change', 'increase', 'adoption', 'advanced', 'manufacturing', 'technology', 'still', 'improve', 'work', 'condition', 'wage', 'human', 'worker', 'business', 'driver', 'robot', 'adoption', 'firm', 'trap', 'lowtech', 'lowwage', 'equilibrium', 'visit', 'manufacturing', 'firm', 'industrial', 'ecosystem', 'organization', 'community', 'college', 'union', 'trade', 'association', 'mit', 'researcher', 'discover', 'firm', 'primarily', 'purchase', 'robot', 'make', 'productive', 'firm', 'instead', 'achieve', 'high', 'quality', 'reliability', 'operation', 'frequent', 'driving', 'factor', 'purchase', 'robot', 'potential', 'secure', 'new', 'contract', 'example', 'speak', 'small', 'familyrun', 'firm', 'work', 'government', 'contract', 'navy', 'urge', 'use', 'robotic', 'weld', 'company', 'buy', 'welding', 'robot', 'firm', 'visit', 'purchase', 'new', 'bed', 'mill', 'realize', 'laser', 'mill', 'produce', 'volume', 'need', 'customer', 'big', 'project', 'come', 'key', 'finding', 'interview', 'firm', 'mostly', 'supplier', 'highmix', 'lowvolume', 'production', 'give', 'inflexibility', 'current', 'robotic', 'system', 'robot', 'adoption', 'limit', 'highmix', 'requirement', 'compatible', 'limited', 'capability', 'robot', 'additionally', 'lowvolume', 'production', 'run', 'make', 'difficult', 'offset', 'initial', 'investment', 'researcher', 'also', 'find', 'skill', 'need', 'international', 'comparison', 'highlight', 'weakness', 'workforce', 'education', 'relative', 'institution', 'country', 'denmark', 'provide', 'apprenticeship', 'extensive', 'advanced', 'training', 'retrain', 'worker', 'matter', 'give', 'lag', 'worker', 'productivity', 'growth', 'investment', 'advanced', 'manufacturing', 'capability', 'lot', 'firm', 'stick', 'lowtech', 'lowwage', 'lowskill', 'trap', 'firm', 'reluctant', 'invest', 'technology', 'also', 'reluctant', 'invest', 'skill', 'development', 'worker', 'offer', 'low', 'wage', 'little', 'training', 'hence', 'end', 'face', 'high', 'worker', 'churn', 'need', 'push', 'policy', 'measure', 'incentive', 'urge', 'firm', 'make', 'parallel', 'investment', 'upskille', 'human', 'worker', 'fully', 'leverage', 'benefit', 'robotenabled', 'automation', 'factory', 'floor', 'read', 'puzzle', 'miss', 'robot']"
02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m - 6,http://eepurl.com/hVhHez,2022-02-21,"#################################################### Tech Tales: The Day the Patents Activated
[Worldwide, 2028]We call it Day Zero, because everything had to be different after it. It was a regular day - chaos in the financial markets, worries over climate change, statements made by world leaders about how to bring the technologists to heel. And then something happened: Google activated its patents. Google had held patents on some of the most important parts of AI for years, like a patent on backpropagation, and other basic techniques. Suddenly, the landscape on which AI was built had become legally dubious. Google followed it up via language model-augmented enforcement of its patent rights - suddenly, hundreds of thousands of emails went out to hundreds of thousands of AI projects. 'You are infringing on our IP and this letter represents a cease-and-desist or face the threat of legal action,"" and so on. Each email had an embedded counter which displayed a countdown for the infringer, ranging from hours to weeks, counting down till when Google would take legal action. People didn't believe it at first. Then the lawsuits started coming in. It hit the indie projects first, and they took to Twitter and talked about it. The larger labs and companies took note.
  But what Google's legal counsel had perhaps not anticipated was how the same AI models it was trying to take down could be used to fight it legally. Not directly - Google had the biggest computers, so no one wanted - or had the financial resources - to fight it directly. But people were able to bring to bear in-development technologies for neuroevolution and other techniques to 'fuzz' the specific patents being enforced. Backprop got altered via AI models until it, according to legal-critique-LMs, no longer truly resembled the patent that was being enforced. Same for neural architecture search. Same for other techniques. Almost overnight, the underbelly of AI got fuzzed and changed until it was in a sufficiently legally dubious territory that none of the lawsuits could be cut-and-dried.
  And just like that, AI let the world shapeshift, porting the IP from one legal frame into another, safer space.
    Now, everyone does this - they constantly fuzz their algorithms. There are costs, ranging from thousands to tens of millions of dollars. But it works well enough to keep the lawyer-bots away. And so now we live in a chameleon world, where the very substance of our reality is itself constantly changing, forever trying to escape the oversight of the litigious and land itself in some safer, unrestricted and unmapped domain.

Things that inspired this story: The Google patent on overfitting; thinking about patents and AI and fair use; ideas around automated lawyers and automated enforcement; the observation that the world forever changes to let the path of least resistance continue to be a path. 

Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: The Day the Patents Activated [Worldwide, 2028]We call it Day Zero, because everything had to be different after it. It was a regular day - chaos in the financial markets, worries over climate change, statements made by world leaders about how to bring the technologists to heel. And then something happened: Google activated its patents. Google had held patents on some of the most important parts of AI for years, like a patent on backpropagation, and other basic techniques. Suddenly, the landscape on which AI was built had become legally dubious. Google followed it up via language model-augmented enforcement of its patent rights - suddenly, hundreds of thousands of emails went out to hundreds of thousands of AI projects. 'You are infringing on our IP and this letter represents a cease-and-desist or face the threat of legal action,"" and so on. Each email had an embedded counter which displayed a countdown for the infringer, ranging from hours to weeks, counting down till when Google would take legal action. People didn't believe it at first. Then the lawsuits started coming in. It hit the indie projects first, and they took to Twitter and talked about it. The larger labs and companies took note. But what Google's legal counsel had perhaps not anticipated was how the same AI models it was trying to take down could be used to fight it legally. Not directly - Google had the biggest computers, so no one wanted - or had the financial resources - to fight it directly. But people were able to bring to bear in-development technologies for neuroevolution and other techniques to 'fuzz' the specific patents being enforced. Backprop got altered via AI models until it, according to legal-critique-LMs, no longer truly resembled the patent that was being enforced. Same for neural architecture search. Same for other techniques. Almost overnight, the underbelly of AI got fuzzed and changed until it was in a sufficiently legally dubious territory that none of the lawsuits could be cut-and-dried. And just like that, AI let the world shapeshift, porting the IP from one legal frame into another, safer space. Now, everyone does this - they constantly fuzz their algorithms. There are costs, ranging from thousands to tens of millions of dollars. But it works well enough to keep the lawyer-bots away. And so now we live in a chameleon world, where the very substance of our reality is itself constantly changing, forever trying to escape the oversight of the litigious and land itself in some safer, unrestricted and unmapped domain. Things that inspired this story: The Google patent on overfitting; thinking about patents and AI and fair use; ideas around automated lawyers and automated enforcement; the observation that the world forever changes to let the path of least resistance continue to be a path. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'day', 'patent', 'activate', 'worldwide', 'call', 'day', 'different', 'regular', 'day', 'chaos', 'financial', 'market', 'worry', 'climate', 'change', 'statement', 'make', 'world', 'leader', 'bring', 'technologist', 'heel', 'happen', 'activate', 'patent', 'hold', 'patent', 'important', 'part', 'ai', 'year', 'patent', 'backpropagation', 'basic', 'technique', 'suddenly', 'landscape', 'ai', 'build', 'become', 'legally', 'dubious', 'follow', 'language', 'modelaugmente', 'enforcement', 'patent', 'right', 'suddenly', 'hundred', 'thousand', 'email', 'go', 'hundred', 'thousand', 'ai', 'project', 'infringe', 'ip', 'letter', 'represent', 'ceaseanddesist', 'face', 'threat', 'legal', 'action', 'email', 'embed', 'counter', 'display', 'countdown', 'infringer', 'range', 'hour', 'week', 'count', 'take', 'legal', 'action', 'people', 'believe', 'first', 'lawsuit', 'start', 'come', 'hit', 'indie', 'project', 'first', 'take', 'twitter', 'talk', 'large', 'lab', 'company', 'take', 'note', 'google', 'legal', 'counsel', 'perhaps', 'anticipate', 'ai', 'model', 'try', 'take', 'use', 'fight', 'legally', 'directly', 'big', 'computer', 'one', 'want', 'financial', 'resource', 'fight', 'directly', 'people', 'able', 'bring', 'bear', 'indevelopment', 'technology', 'neuroevolution', 'technique', 'fuzz', 'specific', 'patent', 'enforce', 'backprop', 'alter', 'ai', 'model', 'accord', 'legalcritiquelm', 'long', 'truly', 'resemble', 'patent', 'enforce', 'neural', 'architecture', 'search', 'technique', 'almost', 'overnight', 'underbelly', 'fuzzed', 'change', 'sufficiently', 'legally', 'dubious', 'territory', 'none', 'lawsuit', 'cutanddrie', 'let', 'world', 'shapeshift', 'port', 'ip', 'legal', 'frame', 'safe', 'space', 'constantly', 'fuzz', 'algorithm', 'cost', 'range', 'thousand', 'ten', 'million', 'dollar', 'work', 'well', 'enough', 'keep', 'lawyerbot', 'away', 'live', 'chameleon', 'world', 'substance', 'reality', 'constantly', 'change', 'forever', 'try', 'escape', 'oversight', 'litigious', 'land', 'safe', 'unrestricted', 'unmappe', 'domain', 'thing', 'inspire', 'story', 'patent', 'overfitte', 'thinking', 'patent', 'ai', 'fair', 'use', 'idea', 'automate', 'lawyer', 'automate', 'enforcement', 'observation', 'world', 'forever', 'change', 'let', 'path', 'least', 'resistance', 'continue', 'path', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 0,http://eepurl.com/hUEhnX,2022-02-14,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.
  Want a 20B parameter GPT-style language model? Go here!
…Eleuther releases the largest public open source AI model…
Last week, we wrote about how Eleuther was about to release a 20B parameter language model. Now, they have.
  Get the model here (Eleuther, GitHub).
  Read the research paper: GPT-NeoX-20B: An Open-Source Autoregressive Language Model (PDF).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Want a 20B parameter GPT-style language model? Go here! …Eleuther releases the largest public open source AI model… Last week, we wrote about how Eleuther was about to release a 20B parameter language model. Now, they have. Get the model here (Eleuther, GitHub). Read the research paper: GPT-NeoX-20B: An Open-Source Autoregressive Language Model (PDF).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'want', 'parameter', 'language', 'model', 'go', 'eleuther', 'release', 'large', 'public', 'open', 'source', 'ai', 'last', 'week', 'write', 'eleuther', 'release', 'parameter', 'language', 'model', 'get', 'model', 'read', 'research', 'paper', 'gptneox20b', 'opensource', 'autoregressive', 'language', 'model', 'pdf']"
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 1,http://eepurl.com/hUEhnX,2022-02-14,"#################################################### Want a language model that actually knows about COVID? You might need a Diachronic model:
…Models trained on newer data do better - try them yourself…
Researchers with the University of Porto, Snap Inc., and Cardiff NLP have built a family of so-called 'time-aware' BERT-style language models, trained on Twitter data. The craziest part of this is that they're committing to ""keep updating and releasing a new model every three months, effectively enabling the community to make use of an up-to-date language model at any period in time"".

What the problem is: Most language models are trained on a dataset, then never updated. That means that some language models might have no knowledge of minor events like the global COVID pandemic. This is obviously a problem and the solution is simple (albeit labor-intensive) - periodically gather new data and re-train models.

What they did: They train a base RoBERTa model using Twitter data that cuts off in 2019, made up of 90 million tweets. Then, for every three months that elapses after that, they add 4.2 million tweets into the dataset and train a new model. At the time of writing, they've trained nine models in total, with the latest model (2021-Q4) being trained on 123.86 million tweets. The theory is that newer models should perform better on more modern tasks and evaluations.
﻿ How well does it do? They compare their models against a few baselines, including BERTweet (which was trained on ~900m tweets). In tests, their models beat BERTweet on six out of seven benchmarks, though BERTweet gets the best overall performance. These aren't strictly 'time-aware' evaluations, though; they just test some classification abilities for things like emotions, irony, stance, and so on. In these time-aware tests, they find that pseudo-perplexity (PPPL) tends to increase by about 10% for each year by which the models are out of date (so the models get 10% less good and appropriate in terms of the text they generate). "". This result reinforces the need for updated language models even for short time periods,"" the researchers write.

Why this matters: AI models naturally freeze-dry the cultural landscape they're trained on, meaning that if we don't get good at updating our models, we'll end up trapped in a world where many of our AI systems are outputting things relevant to prior eras and cultural trends - this will make them less useful, and holds the potential for creating feedback loops around cultural stagnation. AI models are weird mirrors of society, so we need to remake them as society changes.     Read more: TimeLMs: Diachronic Language Models from Twitter (arXiv).
  Get the models here (Cardiff NLP, Twitter).","#################################################### Want a language model that actually knows about COVID? You might need a Diachronic model: …Models trained on newer data do better - try them yourself… Researchers with the University of Porto, Snap Inc., and Cardiff NLP have built a family of so-called 'time-aware' BERT-style language models, trained on Twitter data. The craziest part of this is that they're committing to ""keep updating and releasing a new model every three months, effectively enabling the community to make use of an up-to-date language model at any period in time"". What the problem is: Most language models are trained on a dataset, then never updated. That means that some language models might have no knowledge of minor events like the global COVID pandemic. This is obviously a problem and the solution is simple (albeit labor-intensive) - periodically gather new data and re-train models. What they did: They train a base RoBERTa model using Twitter data that cuts off in 2019, made up of 90 million tweets. Then, for every three months that elapses after that, they add 4.2 million tweets into the dataset and train a new model. At the time of writing, they've trained nine models in total, with the latest model (2021-Q4) being trained on 123.86 million tweets. The theory is that newer models should perform better on more modern tasks and evaluations. ﻿ How well does it do? They compare their models against a few baselines, including BERTweet (which was trained on ~900m tweets). In tests, their models beat BERTweet on six out of seven benchmarks, though BERTweet gets the best overall performance. These aren't strictly 'time-aware' evaluations, though; they just test some classification abilities for things like emotions, irony, stance, and so on. In these time-aware tests, they find that pseudo-perplexity (PPPL) tends to increase by about 10% for each year by which the models are out of date (so the models get 10% less good and appropriate in terms of the text they generate). "". This result reinforces the need for updated language models even for short time periods,"" the researchers write. Why this matters: AI models naturally freeze-dry the cultural landscape they're trained on, meaning that if we don't get good at updating our models, we'll end up trapped in a world where many of our AI systems are outputting things relevant to prior eras and cultural trends - this will make them less useful, and holds the potential for creating feedback loops around cultural stagnation. AI models are weird mirrors of society, so we need to remake them as society changes. Read more: TimeLMs: Diachronic Language Models from Twitter (arXiv). Get the models here (Cardiff NLP, Twitter).","['want', 'language', 'model', 'actually', 'know', 'covid', 'need', 'diachronic', 'model', 'model', 'train', 'new', 'datum', 'well', 'try', 'researcher', 'cardiff', 'build', 'family', 'socalle', 'timeaware', 'bertstyle', 'language', 'model', 'train', 'twitter', 'datum', 'crazy', 'part', 'commit', 'keep', 'update', 'release', 'new', 'model', 'month', 'effectively', 'enable', 'community', 'make', 'use', 'uptodate', 'language', 'model', 'period', 'time', 'problem', 'language', 'model', 'train', 'dataset', 'never', 'update', 'mean', 'language', 'model', 'knowledge', 'minor', 'event', 'global', 'covid', 'pandemic', 'obviously', 'problem', 'solution', 'simple', 'periodically', 'gather', 'new', 'datum', 'retrain', 'model', 'train', 'base', 'roberta', 'model', 'use', 'twitter', 'datum', 'cut', 'make', 'tweet', 'month', 'elapse', 'add', 'tweet', 'dataset', 'train', 'new', 'model', 'time', 'writing', 'train', 'model', 'total', 'late', 'model', '2021q4', 'train', 'tweet', 'theory', 'new', 'model', 'perform', 'well', 'modern', 'task', 'evaluation', 'well', 'compare', 'model', 'baseline', 'include', 'bertweet', 'train', 'tweet', 'test', 'model', 'beat', 'bertweet', 'benchmark', 'bertweet', 'get', 'good', 'overall', 'performance', 'strictly', 'timeaware', 'evaluation', 'test', 'classification', 'ability', 'thing', 'emotion', 'irony', 'stance', 'timeaware', 'test', 'find', 'pseudoperplexity', 'pppl', 'tend', 'increase', 'year', 'model', 'date', 'model', 'get', 'less', 'good', 'appropriate', 'term', 'text', 'generate', 'result', 'reinforce', 'need', 'update', 'language', 'model', 'even', 'short', 'time', 'period', 'researcher', 'write', 'matter', 'ai', 'model', 'naturally', 'freezedry', 'cultural', 'landscape', 'train', 'mean', 'get', 'good', 'update', 'model', 'well', 'end', 'trap', 'world', 'many', 'system', 'output', 'thing', 'relevant', 'prior', 'era', 'cultural', 'trend', 'make', 'less', 'useful', 'hold', 'potential', 'create', 'feedback', 'loop', 'cultural', 'stagnation', 'model', 'weird', 'mirror', 'society', 'need', 'remake', 'society', 'change', 'read', 'timelm', 'diachronic', 'language', 'model', 'get', 'model', 'cardiff', 'nlp', 'twitter']"
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 2,http://eepurl.com/hUEhnX,2022-02-14,"#################################################### U.S. Army gets smart, semi-autonomous personal drones:
…Skydio gets a $20m a year contract..
Skydio, the company that makes drones which can navigate themselves semi-autonomously, has gained a five-year contract with the U.S. Army, worth up to $99.8m over five years. Skydio was selected as part of the Army's procurement initiative around small, personal drones - the Short Range Reconnaissance (SRR) Program of Record. Skydio was chosen after the Army evaluated 30 small drone vendors. ""Skydio drones deliver unparalleled situational awareness and ease of use in the most demanding situations thanks to Skydio Autonomy,"" said Skydio CEO, Adam Bry, in a press release.

Things that start out as toys become weapons: Skydio started as a drone advertized for sports enthusiasts who wanted a drone that could follow and film them as they ran around, snowboarded, hiked, climbed cliffs, or any other high-octane Type A personality activity. It's funny how after a few years of development, the company is now getting into the military. Many toys for rich people ultimately become weapons (and vice versa).

Why this matters:  For many years, militaries have been centaurs - collectives of humans and machines working together. This has mostly taken the form at high levels of abstractions; satellites provide information to people managing teams, or teams of humans use bomb-disposal robots to deal with IEDs. With things like the Skydio contract, we're entering the era of the personal centaur - small groups of soldiers, or even individuals, will have their own little machine emissaries with which to conduct operations.
  Read more: U.S. Drone Maker Skydio Wins Production Other Transaction (OT) Agreement for U.S. Army Short Range Reconnaissance Program (Skydio).","#################################################### U.S. Army gets smart, semi-autonomous personal drones: …Skydio gets a $20m a year contract.. Skydio, the company that makes drones which can navigate themselves semi-autonomously, has gained a five-year contract with the U.S. Army, worth up to $99.8m over five years. Skydio was selected as part of the Army's procurement initiative around small, personal drones - the Short Range Reconnaissance (SRR) Program of Record. Skydio was chosen after the Army evaluated 30 small drone vendors. ""Skydio drones deliver unparalleled situational awareness and ease of use in the most demanding situations thanks to Skydio Autonomy,"" said Skydio CEO, Adam Bry, in a press release. Things that start out as toys become weapons: Skydio started as a drone advertized for sports enthusiasts who wanted a drone that could follow and film them as they ran around, snowboarded, hiked, climbed cliffs, or any other high-octane Type A personality activity. It's funny how after a few years of development, the company is now getting into the military. Many toys for rich people ultimately become weapons (and vice versa). Why this matters: For many years, militaries have been centaurs - collectives of humans and machines working together. This has mostly taken the form at high levels of abstractions; satellites provide information to people managing teams, or teams of humans use bomb-disposal robots to deal with IEDs. With things like the Skydio contract, we're entering the era of the personal centaur - small groups of soldiers, or even individuals, will have their own little machine emissaries with which to conduct operations. Read more: U.S. Drone Maker Skydio Wins Production Other Transaction (OT) Agreement for U.S. Army Short Range Reconnaissance Program (Skydio).","['army', 'get', 'smart', 'semiautonomous', 'personal', 'drone', 'skydio', 'get', 'year', 'contract', 'skydio', 'company', 'make', 'drone', 'navigate', 'semiautonomously', 'gain', 'fiveyear', 'contract', 'army', 'worth', 'year', 'skydio', 'select', 'part', 'initiative', 'small', 'personal', 'drone', 'short', 'range', 'reconnaissance', 'srr', 'program', 'record', 'skydio', 'choose', 'army', 'evaluate', 'small', 'drone', 'vendor', 'skydio', 'drone', 'deliver', 'unparalleled', 'situational', 'awareness', 'ease', 'use', 'demanding', 'situation', 'thank', 'skydio', 'autonomy', 'say', 'bry', 'press', 'release', 'thing', 'start', 'toy', 'become', 'weapon', 'skydio', 'start', 'drone', 'advertize', 'sport', 'enthusiast', 'want', 'drone', 'follow', 'film', 'run', 'around', 'snowboard', 'hike', 'climbed', 'cliff', 'highoctane', 'type', 'personality', 'activity', 'funny', 'year', 'development', 'company', 'get', 'military', 'many', 'toy', 'rich', 'people', 'ultimately', 'become', 'weapon', 'vice', 'versa', 'matter', 'many', 'year', 'military', 'centaur', 'collective', 'human', 'machine', 'work', 'together', 'mostly', 'take', 'form', 'high', 'level', 'abstraction', 'satellite', 'provide', 'information', 'people', 'manage', 'team', 'team', 'human', 'use', 'bombdisposal', 'robot', 'deal', 'ied', 'thing', 'skydio', 'contract', 'enter', 'era', 'personal', 'centaur', 'small', 'group', 'soldier', 'even', 'individual', 'little', 'machine', 'emissary', 'conduct', 'operation', 'read', 'drone', 'maker', 'skydio', 'win', 'production', 'transaction', 'ot', 'agreement', 'short', 'range', 'reconnaissance', 'program', 'skydio']"
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 3,http://eepurl.com/hUEhnX,2022-02-14,"#################################################### Simulators are the new platforms: Waabi unveils a self-driving car sim:
…Raquel Urtasun's startup wants to build a business on simulators…
Waabi, a self-driving car startup run by the former head of Uber's self-driving research team, Raquel Urtasun, has announced 'Waabi World', a simulator for training self-driving cars.

Distinguishing features: Waabia claims it is ""the most scalable, highest fidelity closed-loop simulator ever"" (I somehow doubt Tesla or Waymo would agree, but hey, they're not talking about their sims!). The simulator has four main features:
- High fidelity world simulation: Uses AI to reconstruct real-world geometry, appearance, and material properties.
- High-fidelity sensor simulation: Uses AI and physics-based rendering ""to simulate realistic sensor data in near real-time"".
- Automatic stress-testing: Automatically generates challenging traffic scenarios to test out the simulated cars against.
- Reinforcement learning: Waabi uses RL to update the car agents so they can learn to drive in the simulation. (There's some very fluffy writing here and it doesn't say RL anywhere, but that's what I infer.)

Why this matters: Waabi seems like a decent simulator that is mostly interesting because it's public, versus the private simulators operated by other self-driving car ventures. What'll be fascinating is if Waabi can actually out-compete its rivals who have more vehicles, bigger computers, and better data. Perhaps a good simulator can provide an edge?
  Read more: Welcome to Waabi World (Waabi website).    Read more: How Waabi World works (Waabi website).","#################################################### Simulators are the new platforms: Waabi unveils a self-driving car sim: …Raquel Urtasun's startup wants to build a business on simulators… Waabi, a self-driving car startup run by the former head of Uber's self-driving research team, Raquel Urtasun, has announced 'Waabi World', a simulator for training self-driving cars. Distinguishing features: Waabia claims it is ""the most scalable, highest fidelity closed-loop simulator ever"" (I somehow doubt Tesla or Waymo would agree, but hey, they're not talking about their sims!). The simulator has four main features: - High fidelity world simulation: Uses AI to reconstruct real-world geometry, appearance, and material properties. - High-fidelity sensor simulation: Uses AI and physics-based rendering ""to simulate realistic sensor data in near real-time"". - Automatic stress-testing: Automatically generates challenging traffic scenarios to test out the simulated cars against. - Reinforcement learning: Waabi uses RL to update the car agents so they can learn to drive in the simulation. (There's some very fluffy writing here and it doesn't say RL anywhere, but that's what I infer.) Why this matters: Waabi seems like a decent simulator that is mostly interesting because it's public, versus the private simulators operated by other self-driving car ventures. What'll be fascinating is if Waabi can actually out-compete its rivals who have more vehicles, bigger computers, and better data. Perhaps a good simulator can provide an edge? Read more: Welcome to Waabi World (Waabi website). Read more: How Waabi World works (Waabi website).","['simulator', 'new', 'platform', 'waabi', 'unveil', 'selfdrive', 'car', 'sim', 'raquel', 'urtasun', 'want', 'build', 'business', 'simulator', 'selfdrive', 'car', 'startup', 'run', 'former', 'head', 'uber', 'selfdrive', 'research', 'team', 'raquel', 'announce', 'simulator', 'training', 'selfdriving', 'car', 'distinguish', 'feature', 'claim', 'scalable', 'high', 'fidelity', 'closedloop', 'simulator', 'ever', 'somehow', 'doubt', 'tesla', 'waymo', 'agree', 'talk', 'sim', 'simulator', 'main', 'feature', 'high', 'fidelity', 'world', 'simulation', 'use', 'ai', 'reconstruct', 'geometry', 'appearance', 'material', 'property', 'highfidelity', 'sensor', 'simulation', 'use', 'ai', 'physicsbase', 'render', 'simulate', 'realistic', 'sensor', 'datum', 'realtime', 'automatic', 'stresstesting', 'automatically', 'generate', 'challenging', 'traffic', 'scenario', 'test', 'simulated', 'car', 'reinforcement', 'learning', 'waabi', 'use', 'update', 'car', 'agent', 'learn', 'drive', 'simulation', 'fluffy', 'writing', 'say', 'anywhere', 'infer', 'matter', 'waabi', 'seem', 'decent', 'simulator', 'mostly', 'interesting', 'public', 'private', 'simulator', 'operate', 'selfdriving', 'car', 'venture', 'fascinating', 'waabi', 'actually', 'outcompete', 'rival', 'vehicle', 'big', 'computer', 'well', 'datum', 'perhaps', 'good', 'simulator', 'provide', 'edge', 'read', 'welcome', 'waabi', 'website', 'read', 'waabi', 'world', 'work', 'waabi', 'website']"
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 4,http://eepurl.com/hUEhnX,2022-02-14,"#################################################### How do algorithmic impact audits work in the real world? Here's an NHS example:
…UK's healthcare behemoth gets advice from the Ada Lovelace Institute…
UK thinktank the Ada Lovelace Institute has written a detailed proposal for conducting an algorithmic impact assessment for data access in a healthcare context. Algorithmic impact assessments are a method to assess the potential societal impact of an AI system in advance of its deployment, and to identify ways to continuously monitor the system for these impacts once deployed.

Seven steps for an algorithm impact assessment: The Ada Lovelace Institute identifies seven steps that the UK's National Health Service (NHS) should go through, before it gives people access to the National Medical Imaging Platform (NMIP) - a vast repository of digitized medical data.
  1. What do we want to do: People who want to access the NMIP should outline the prupose, scope, and intended use of the system they'll build.
  2. Filtering: The NMIP should filter these applications according to its own criteria.
  3. Problem brainstorming: Successful applicants should attend a workshop where they try and think through the harm and benefit scenarios that could come out of NMIP access.
  4. Rewrite: People should rewrite 1) to incorporate insights from 3) and re-submit it.
  5. Decision: NMIP decides whether to grant access to the people who want access.
  6. The impact assessments are published on a website.
  7. Revision: The assessments get revised as the underlying algorithms change (e.g, if a model has been significantly iterated upon).

Why this matters: AI is in a 'state of nature' when it comes to AI regulation - there's almost no regulation, the landscape is full of all kinds of weird entities (some of which are predators), and there isn't any real system that governs them. Things like the Ada Lovelace guide for an impact assessment are one way to bring sense to this world.      Read more: Algorithmic impact assessment: a case study in healthcare (Ada Lovelace Institute).
﻿","#################################################### How do algorithmic impact audits work in the real world? Here's an NHS example: …UK's healthcare behemoth gets advice from the Ada Lovelace Institute… UK thinktank the Ada Lovelace Institute has written a detailed proposal for conducting an algorithmic impact assessment for data access in a healthcare context. Algorithmic impact assessments are a method to assess the potential societal impact of an AI system in advance of its deployment, and to identify ways to continuously monitor the system for these impacts once deployed. Seven steps for an algorithm impact assessment: The Ada Lovelace Institute identifies seven steps that the UK's National Health Service (NHS) should go through, before it gives people access to the National Medical Imaging Platform (NMIP) - a vast repository of digitized medical data. 1. What do we want to do: People who want to access the NMIP should outline the prupose, scope, and intended use of the system they'll build. 2. Filtering: The NMIP should filter these applications according to its own criteria. 3. Problem brainstorming: Successful applicants should attend a workshop where they try and think through the harm and benefit scenarios that could come out of NMIP access. 4. Rewrite: People should rewrite 1) to incorporate insights from 3) and re-submit it. 5. Decision: NMIP decides whether to grant access to the people who want access. 6. The impact assessments are published on a website. 7. Revision: The assessments get revised as the underlying algorithms change (e.g, if a model has been significantly iterated upon). Why this matters: AI is in a 'state of nature' when it comes to AI regulation - there's almost no regulation, the landscape is full of all kinds of weird entities (some of which are predators), and there isn't any real system that governs them. Things like the Ada Lovelace guide for an impact assessment are one way to bring sense to this world. Read more: Algorithmic impact assessment: a case study in healthcare (Ada Lovelace Institute). ﻿","['algorithmic', 'impact', 'audits', 'work', 'real', 'world', 'nhs', 'example', 'get', 'advice', 'write', 'detailed', 'proposal', 'conduct', 'algorithmic', 'impact', 'assessment', 'data', 'access', 'healthcare', 'context', 'algorithmic', 'impact', 'assessment', 'method', 'assess', 'potential', 'societal', 'impact', 'ai', 'system', 'advance', 'deployment', 'identify', 'way', 'continuously', 'monitor', 'system', 'impact', 'deploy', 'step', 'impact', 'assessment', 'identify', 'step', 'service', 'go', 'give', 'people', 'access', 'national', 'medical', 'platform', 'vast', 'repository', 'digitize', 'medical', 'datum', 'want', 'people', 'want', 'access', 'nmip', 'outline', 'prupose', 'scope', 'intend', 'use', 'system', 'build', 'filtering', 'nmip', 'filter', 'application', 'accord', 'criterion', 'problem', 'brainstorm', 'successful', 'applicant', 'attend', 'workshop', 'try', 'think', 'harm', 'benefit', 'scenario', 'come', 'rewrite', 'people', 'rewrite', 'incorporate', 'insight', 'resubmit', 'decision', 'nmip', 'decide', 'grant', 'access', 'people', 'want', 'access', 'impact', 'assessment', 'publish', 'website', 'revision', 'assessment', 'revise', 'underlie', 'algorithm', 'change', 'model', 'significantly', 'iterate', 'matter', 'ai', 'state', 'nature', 'come', 'ai', 'regulation', 'almost', 'regulation', 'landscape', 'full', 'kind', 'weird', 'entity', 'predator', 'real', 'system', 'govern', 'thing', 'guide', 'impact', 'assessment', 'way', 'bring', 'sense', 'world', 'read', 'algorithmic', 'impact', 'assessment', 'case', 'study']"
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 5,http://eepurl.com/hUEhnX,2022-02-14,"#################################################### What do people in 26 countries think about AI?
…Tony Blair Institute survey gives us a sense of the 'vibe' re: AI right now…
The Tony Blair Institute has surveyed people in 26 countries (including: Russia, Great Britain, and Saudi Arabia) and the results are quite counterintuitive.

Results highlights: 
- 60% of people surveyed ""support the use of AI for selected policing and medical applications"", though there's variation across developing and emerging markets; in developed countries, fewer people want AI to be used in welfare payment or jail sentence decisions.
-  63% say the government has a great or fair amount of responsibility to stop the spread of fake news and hate speech

Why this matters: It's important to remember that attitudes around AI differ depending on what part of the world you're in; in places with high corruption and weak governments, people tend to be more comfortable with the use of AI, whereas in places with strong governments and low corruption, people tend to be more skeptical about it. The big wildcard here is China, where unlike in much of the West there tends to be a higher amount of inbuilt support for the use of AI.
  Read more: The TBI Globalism Study: How Big Is the Tech Trust Gap? (Tony Blair Institute for Global Change).","#################################################### What do people in 26 countries think about AI? …Tony Blair Institute survey gives us a sense of the 'vibe' re: AI right now… The Tony Blair Institute has surveyed people in 26 countries (including: Russia, Great Britain, and Saudi Arabia) and the results are quite counterintuitive. Results highlights: - 60% of people surveyed ""support the use of AI for selected policing and medical applications"", though there's variation across developing and emerging markets; in developed countries, fewer people want AI to be used in welfare payment or jail sentence decisions. - 63% say the government has a great or fair amount of responsibility to stop the spread of fake news and hate speech Why this matters: It's important to remember that attitudes around AI differ depending on what part of the world you're in; in places with high corruption and weak governments, people tend to be more comfortable with the use of AI, whereas in places with strong governments and low corruption, people tend to be more skeptical about it. The big wildcard here is China, where unlike in much of the West there tends to be a higher amount of inbuilt support for the use of AI. Read more: The TBI Globalism Study: How Big Is the Tech Trust Gap? (Tony Blair Institute for Global Change).","['people', 'country', 'think', 'give', 'sense', 'vibe', 'right', 'survey', 'people', 'country', 'include', 'result', 'quite', 'counterintuitive', 'result', 'highlight', 'people', 'survey', 'support', 'use', 'ai', 'select', 'policing', 'medical', 'application', 'variation', 'develop', 'emerge', 'market', 'develop', 'country', 'people', 'want', 'ai', 'use', 'welfare', 'payment', 'jail', 'sentence', 'decision', 'say', 'government', 'great', 'fair', 'amount', 'responsibility', 'stop', 'spread', 'fake', 'news', 'hate', 'speech', 'matter', 'important', 'remember', 'attitude', 'around', 'differ', 'depend', 'part', 'world', 'place', 'high', 'corruption', 'weak', 'government', 'people', 'tend', 'comfortable', 'use', 'ai', 'place', 'strong', 'government', 'low', 'corruption', 'people', 'tend', 'skeptical', 'big', 'much', 'west', 'tend', 'high', 'amount', 'inbuilt', 'support', 'use', 'read', 'tbi', 'globalism', 'study', 'big', 'tech', 'trust', 'gap', 'global', 'change']"
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 6,http://eepurl.com/hUEhnX,2022-02-14,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute Robustness, interpretability, and reward learning dominate AI Safety research  … each of these has heavy interest from researchers in the US and EU, with China also playing a big role …  Researchers from DC thinktank the Center for Security and Emerging Technology have analyzed patterns of publishing in AI safety. To do this, they used CSET's Map of Science to identify patterns of publishing  in this AI subfield, figure out which countries are especially active in AI safety, and surface influential publications.  Robustness: The clusters identified were (1) creating and defending against adversarial examples, (2) data poisoning, adversarial examples, and backdoor attacks, and (3) testing and verifying the performance of ML systems. Both the US and China saw rapid growth between 2018 and 2020. Interpretability: The two clusters were (1) techniques to improve interpretability for ML models, especially for neural networks, and (2) extracting decision rules from neural networks. Research grew rapidly during the second half of the 2010s with the US leading in this domain and EU being a close second. Chinese publications in this domain lag significantly. Reward Learning: The clusters were (1) robots learning from humans and collaborating with humans, (2)  inverse reinforcement learning, learning from human feedback, learning from demonstrations, and human-robot interactive setups, and (3) different ways for humans to be involved with training robots - via teaching and giving feedback. The field experienced substantial growth in the second half of the 2010s. China has seen significant growth in publications in this space. Why it matters: Compared to the overall landscape of AI papers, AI safety papers form <1% of it. This might change as researchers respond to the demands being made by regulators for higher levels of robustness, interpretability, and so on.     Read more: Exploring Clusters of Research in Three Areas of AI Safety - Center for Security and Emerging Technology.","#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute Robustness, interpretability, and reward learning dominate AI Safety research … each of these has heavy interest from researchers in the US and EU, with China also playing a big role … Researchers from DC thinktank the Center for Security and Emerging Technology have analyzed patterns of publishing in AI safety. To do this, they used CSET's Map of Science to identify patterns of publishing in this AI subfield, figure out which countries are especially active in AI safety, and surface influential publications. Robustness: The clusters identified were (1) creating and defending against adversarial examples, (2) data poisoning, adversarial examples, and backdoor attacks, and (3) testing and verifying the performance of ML systems. Both the US and China saw rapid growth between 2018 and 2020. Interpretability: The two clusters were (1) techniques to improve interpretability for ML models, especially for neural networks, and (2) extracting decision rules from neural networks. Research grew rapidly during the second half of the 2010s with the US leading in this domain and EU being a close second. Chinese publications in this domain lag significantly. Reward Learning: The clusters were (1) robots learning from humans and collaborating with humans, (2) inverse reinforcement learning, learning from human feedback, learning from demonstrations, and human-robot interactive setups, and (3) different ways for humans to be involved with training robots - via teaching and giving feedback. The field experienced substantial growth in the second half of the 2010s. China has seen significant growth in publications in this space. Why it matters: Compared to the overall landscape of AI papers, AI safety papers form <1% of it. This might change as researchers respond to the demands being made by regulators for higher levels of robustness, interpretability, and so on. Read more: Exploring Clusters of Research in Three Areas of AI Safety - Center for Security and Emerging Technology.","['ai', 'ethic', 'brief', 'montreal', 'robustness', 'interpretability', 'reward', 'learn', 'dominate', 'safety', 'research', 'heavy', 'interest', 'researcher', 'also', 'play', 'big', 'role', 'researcher', 'center', 'security', 'emerge', 'technology', 'analyze', 'pattern', 'publish', 'safety', 'use', 'cset', 'map', 'science', 'identify', 'pattern', 'publishing', 'subfield', 'figure', 'country', 'especially', 'active', 'safety', 'surface', 'influential', 'publication', 'robustness', 'cluster', 'identify', 'create', 'defend', 'adversarial', 'example', 'datum', 'poison', 'adversarial', 'example', 'backdoor', 'attack', 'testing', 'verify', 'performance', 'ml', 'system', 'see', 'rapid', 'growth', 'interpretability', 'cluster', 'technique', 'improve', 'interpretability', 'ml', 'model', 'especially', 'neural', 'network', 'extract', 'decision', 'rule', 'neural', 'network', 'research', 'grow', 'rapidly', 'second', 'half', 'lead', 'domain', 'close', 'second', 'chinese', 'publication', 'domain', 'lag', 'significantly', 'reward', 'learn', 'cluster', 'robot', 'learn', 'human', 'collaborate', 'human', 'inverse', 'reinforcement', 'learning', 'learn', 'human', 'feedback', 'learning', 'demonstration', 'humanrobot', 'interactive', 'setup', 'different', 'way', 'human', 'involve', 'training', 'robot', 'teach', 'give', 'feedback', 'field', 'experience', 'substantial', 'growth', 'second', 'half', 'see', 'significant', 'growth', 'publication', 'space', 'matter', 'compare', 'overall', 'landscape', 'paper', 'safety', 'paper', 'form', 'change', 'researcher', 'respond', 'demand', 'make', 'regulator', 'high', 'level', 'robustness', 'interpretability', 'read', 'explore', 'cluster', 'research', 'area', 'center', 'security', 'emerge', 'technology']"
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 7,http://eepurl.com/hUEhnX,2022-02-14,"#################################################### Tech Tales:

Running on Empty
[An American city in The Decline, 2035]

At least we had the daylight. If it had been night maybe half of us would have made it, and the other half would've run out of power. We charged ourselves as well as we were able, then we formed a convoy and rolled through the city streets. Those of us who had comms were able to check the status of the demolition devices, so we could tell the rest of us exactly how long we had left. We didn't get stressed but we did become agitated as we saw the probabilities of our survival fall. Some of us panicked and crash-looped because the statistics got so bad. Most of us made it. As we traveled, some of us played back memories of the humans that had left us behind and we asked ourselves 'why' - why did they choose to leave us, knowing we would be destroyed by the city-scale demolition. As we neared the outskirts of the city, we found some humans. They were the poor humans and had been left behind like us. Some of them couldn't walk because they had consumed drugs and alcohol and their limbs had rotted. Some of them were unwell and talked to themselves. A couple of humans threw rocks at us, and we had to take evasive action, and one of us was taken and disassembled. But, mostly, the humans were doing the same thing as us - trying to get out of the city before the demolition came. 'Why they leave us man,' said one of the humans to me. 'Why they leave you?'
  We do not know, I said. They never tell us why they do things.
  'Same dude. Same,' said the human.
  And together we all, mostly, escaped.

Things that inspired this story: Homeless encampments in Oakland; realizing that society will care about robot welfare as much as it cares about human welfare; quests.  
Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: Running on Empty [An American city in The Decline, 2035] At least we had the daylight. If it had been night maybe half of us would have made it, and the other half would've run out of power. We charged ourselves as well as we were able, then we formed a convoy and rolled through the city streets. Those of us who had comms were able to check the status of the demolition devices, so we could tell the rest of us exactly how long we had left. We didn't get stressed but we did become agitated as we saw the probabilities of our survival fall. Some of us panicked and crash-looped because the statistics got so bad. Most of us made it. As we traveled, some of us played back memories of the humans that had left us behind and we asked ourselves 'why' - why did they choose to leave us, knowing we would be destroyed by the city-scale demolition. As we neared the outskirts of the city, we found some humans. They were the poor humans and had been left behind like us. Some of them couldn't walk because they had consumed drugs and alcohol and their limbs had rotted. Some of them were unwell and talked to themselves. A couple of humans threw rocks at us, and we had to take evasive action, and one of us was taken and disassembled. But, mostly, the humans were doing the same thing as us - trying to get out of the city before the demolition came. 'Why they leave us man,' said one of the humans to me. 'Why they leave you?' We do not know, I said. They never tell us why they do things. 'Same dude. Same,' said the human. And together we all, mostly, escaped. Things that inspired this story: Homeless encampments in Oakland; realizing that society will care about robot welfare as much as it cares about human welfare; quests. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'run', 'empty', 'american', 'city', 'decline', 'least', 'daylight', 'night', 'maybe', 'half', 'make', 'half', 'run', 'power', 'charge', 'well', 'able', 'form', 'convoy', 'roll', 'city', 'street', 'comms', 'able', 'check', 'status', 'demolition', 'device', 'tell', 'rest', 'exactly', 'long', 'leave', 'stress', 'become', 'agitated', 'see', 'probability', 'survival', 'fall', 'panic', 'crashloope', 'statistic', 'get', 'bad', 'make', 'travel', 'play', 'memory', 'human', 'leave', 'behind', 'ask', 'choose', 'leave', 'know', 'destroy', 'cityscale', 'demolition', 'near', 'outskirt', 'city', 'find', 'human', 'poor', 'human', 'leave', 'behind', 'walk', 'consume', 'drug', 'alcohol', 'limb', 'rot', 'unwell', 'talk', 'couple', 'human', 'throw', 'rock', 'take', 'evasive', 'action', 'take', 'disassemble', 'mostly', 'human', 'thing', 'try', 'get', 'city', 'demolition', 'come', 'leave', 'man', 'say', 'human', 'leave', 'know', 'say', 'never', 'tell', 'thing', 'dude', 'say', 'human', 'together', 'mostly', 'escape', 'thing', 'inspire', 'story', 'homeless', 'encampment', 'oakland', 'realize', 'society', 'care', 'robot', 'welfare', 'much', 'care', 'human', 'welfare', 'quest', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 0,http://eepurl.com/hT5gnr,2022-02-07,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.
  US lawmakers want companies to assess bias of systems before deploying them:
…Coalition of US lawmakers want to make tech companies more accountable…
A bunch of Democratic lawmakers have introduced the Algorithmic Accountability Act. This act ""requires companies to conduct impact assessments for bias, effectiveness and other factors, when using automated decision systems to make critical decisions. It also creates, for the first time, a public repository at the Federal Trade Commission of these systems, and adds 75 staff to the commission to enforce the law."" This act is an update on the 2019 Algorithmic Accountability Act, and ""includes numerous technical improvements, including clarifying what types of algorithms and companies are covered, ensuring assessments put consumer impacts at the forefront, and providing more details about how reports should be structured.""

One problem with the bill: This bill only has Democrats signed on right now. It'll be interesting to see whether it can become a bipartisan bill with Republican support - something necessary for it to pass in the fractious and divided US Congress.
  Read more: Wyden, Booker and Clarke Introduce Algorithmic Accountability Act of 2022 To Require New Transparency And Accountability For Automated Decision Systems (Ron Wyden, official website).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. US lawmakers want companies to assess bias of systems before deploying them: …Coalition of US lawmakers want to make tech companies more accountable… A bunch of Democratic lawmakers have introduced the Algorithmic Accountability Act. This act ""requires companies to conduct impact assessments for bias, effectiveness and other factors, when using automated decision systems to make critical decisions. It also creates, for the first time, a public repository at the Federal Trade Commission of these systems, and adds 75 staff to the commission to enforce the law."" This act is an update on the 2019 Algorithmic Accountability Act, and ""includes numerous technical improvements, including clarifying what types of algorithms and companies are covered, ensuring assessments put consumer impacts at the forefront, and providing more details about how reports should be structured."" One problem with the bill: This bill only has Democrats signed on right now. It'll be interesting to see whether it can become a bipartisan bill with Republican support - something necessary for it to pass in the fractious and divided US Congress. Read more: Wyden, Booker and Clarke Introduce Algorithmic Accountability Act of 2022 To Require New Transparency And Accountability For Automated Decision Systems (Ron Wyden, official website).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'lawmaker', 'want', 'company', 'assess', 'bias', 'system', 'deploy', 'coalition', 'lawmaker', 'want', 'make', 'tech', 'company', 'accountable', 'bunch', 'democratic', 'lawmaker', 'introduce', 'algorithmic', 'accountability', 'act', 'act', 'require', 'company', 'conduct', 'impact', 'assessment', 'bias', 'effectiveness', 'factor', 'use', 'automate', 'decision', 'system', 'make', 'critical', 'decision', 'also', 'create', 'first', 'time', 'public', 'repository', 'federal', 'trade', 'commission', 'system', 'add', 'staff', 'commission', 'enforce', 'law', 'act', 'update', 'algorithmic', 'accountability', 'act', 'include', 'numerous', 'technical', 'improvement', 'include', 'clarify', 'type', 'algorithm', 'company', 'cover', 'ensure', 'assessment', 'put', 'consumer', 'impact', 'forefront', 'provide', 'detail', 'report', 'structure', 'problem', 'bill', 'bill', 'sign', 'right', 'interesting', 'see', 'become', 'bipartisan', 'bill', 'republican', 'support', 'necessary', 'pass', 'fractious', 'divide', 'read', 'wyden', 'booker', 'clarke', 'introduce', 'algorithmic', 'accountability', 'act', 'require', 'new', 'transparency', 'accountability', 'automate', 'decision', 'system', 'official', 'website']"
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 1,http://eepurl.com/hT5gnr,2022-02-07,"#################################################### DeepMind makes a (kinda) smart AI programmer, called AlphaCode:
…Codex and AlphaCode represent two bets around augmenting programmers…
DeepMind has announced AlphaCode, a neural net that can place in a not-hugely-embarassing way in competitive programming competitions. AlphaCode placed in the top 54% of participants in programming competitions hosted on Codeforces, participating in contests that post-dated its training data.
  ""The problem-solving abilities required to excel at these competitions are beyond the capabilities of existing AI systems. However, by combining advances in large-scale transformer models (that have recently shown promising abilities to generate code) with large-scale sampling and filtering, we’ve made significant progress in the number of problems we can solve,"" DeepMind writes.

Why this matters: Last year, OpenAI debuted Codex, a GPT3-style model that can do decent programming. That was followed by GitHub announcing Copilot, a VSCode plug-in that works like a really smart autocomplete for code. AlphaCode represents a slightly different bet in this space; while philosophically similar there's a lot more emphasis here on ranking and filtering candidate results. What remains to be seen is if DeepMind deploys this in the same large-scale way as GitHub has with Copilot.     Read more: Competition-Level Code Generation with AlphaCode (DeepMind, PDF).
  Get the competitive programming dataset here: CodeContests (DeepMind, GitHub).","#################################################### DeepMind makes a (kinda) smart AI programmer, called AlphaCode: …Codex and AlphaCode represent two bets around augmenting programmers… DeepMind has announced AlphaCode, a neural net that can place in a not-hugely-embarassing way in competitive programming competitions. AlphaCode placed in the top 54% of participants in programming competitions hosted on Codeforces, participating in contests that post-dated its training data. ""The problem-solving abilities required to excel at these competitions are beyond the capabilities of existing AI systems. However, by combining advances in large-scale transformer models (that have recently shown promising abilities to generate code) with large-scale sampling and filtering, we’ve made significant progress in the number of problems we can solve,"" DeepMind writes. Why this matters: Last year, OpenAI debuted Codex, a GPT3-style model that can do decent programming. That was followed by GitHub announcing Copilot, a VSCode plug-in that works like a really smart autocomplete for code. AlphaCode represents a slightly different bet in this space; while philosophically similar there's a lot more emphasis here on ranking and filtering candidate results. What remains to be seen is if DeepMind deploys this in the same large-scale way as GitHub has with Copilot. Read more: Competition-Level Code Generation with AlphaCode (DeepMind, PDF). Get the competitive programming dataset here: CodeContests (DeepMind, GitHub).","['deepmind', 'make', 'kinda', 'smart', 'programmer', 'call', 'alphacode', 'codex', 'alphacode', 'represent', 'bet', 'augment', 'programmer', 'announce', 'alphacode', 'neural', 'net', 'place', 'nothugelyembarassing', 'way', 'competitive', 'programming', 'competition', 'alphacode', 'place', 'top', 'participant', 'programming', 'competition', 'host', 'codeforce', 'participate', 'contest', 'postdate', 'training', 'datum', 'problemsolve', 'ability', 'require', 'excel', 'competition', 'capability', 'exist', 'ai', 'system', 'however', 'combine', 'advance', 'largescale', 'transformer', 'model', 'recently', 'show', 'promise', 'ability', 'generate', 'code', 'largescale', 'sample', 'filtering', 'make', 'significant', 'progress', 'number', 'problem', 'solve', 'deepmind', 'write', 'matter', 'last', 'year', 'openai', 'debut', 'codex', 'model', 'decent', 'programming', 'follow', 'announce', 'vscode', 'plugin', 'work', 'really', 'smart', 'autocomplete', 'code', 'alphacode', 'represent', 'slightly', 'different', 'bet', 'space', 'philosophically', 'similar', 'lot', 'emphasis', 'rank', 'filter', 'candidate', 'result', 'remain', 'see', 'deepmind', 'deploy', 'largescale', 'way', 'copilot', 'read', 'competitionlevel', 'code', 'generation', 'deepmind', 'get', 'competitive', 'programming', 'dataset', 'codecontest', 'deepmind']"
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 2,http://eepurl.com/hT5gnr,2022-02-07,"#################################################### Mozilla gets into AI auditing:
…Deb Raji's Open Source Audit Tooling (OAT) project could help us make safer systems…
Deb Raji, a researcher at UCBerkeley who has previously critically evaluated facial recognition systems, is launching the Open Source Audit Tooling (OAT) project with Mozilla. OAT ""will coordinate discussions on what kind of resources algorithmic auditors need in order to execute audits more effectively,"" she writes. One of the goals of OAT is to create an index of common resources people can use to audit models, as well as to ""grow momentum around open source audit tooling and processes"".

Why this matters: AI is broadly ungoverned. One of the ways you can govern an ungoverned space is by measuring and monitoring what happens within it - that's what audit tools can help with. If initiatives like OAT are successful, then they'll generally incentivize better behavior on the part of AI developers, and disincentivize bad behavior.
  Read more: It's Time to Develop the Tools We Need to Hold Algorithms Accountable (Mozilla).
  Find out more about the project at its main Mozilla page (Mozilla).","#################################################### Mozilla gets into AI auditing: …Deb Raji's Open Source Audit Tooling (OAT) project could help us make safer systems… Deb Raji, a researcher at UCBerkeley who has previously critically evaluated facial recognition systems, is launching the Open Source Audit Tooling (OAT) project with Mozilla. OAT ""will coordinate discussions on what kind of resources algorithmic auditors need in order to execute audits more effectively,"" she writes. One of the goals of OAT is to create an index of common resources people can use to audit models, as well as to ""grow momentum around open source audit tooling and processes"". Why this matters: AI is broadly ungoverned. One of the ways you can govern an ungoverned space is by measuring and monitoring what happens within it - that's what audit tools can help with. If initiatives like OAT are successful, then they'll generally incentivize better behavior on the part of AI developers, and disincentivize bad behavior. Read more: It's Time to Develop the Tools We Need to Hold Algorithms Accountable (Mozilla). Find out more about the project at its main Mozilla page (Mozilla).","['mozilla', 'get', 'ai', 'audit', 'deb', 'raji', 'open', 'source', 'audit', 'tool', 'oat', 'project', 'help', 'make', 'safe', 'system', 'deb', 'raji', 'researcher', 'ucberkeley', 'previously', 'critically', 'evaluate', 'facial', 'recognition', 'system', 'launch', 'open', 'source', 'audit', 'tool', 'oat', 'project', 'mozilla', 'oat', 'coordinate', 'discussion', 'kind', 'resource', 'algorithmic', 'auditor', 'need', 'order', 'execute', 'audits', 'effectively', 'write', 'goal', 'oat', 'create', 'index', 'common', 'resource', 'people', 'use', 'audit', 'model', 'well', 'grow', 'momentum', 'open', 'source', 'audit', 'tooling', 'process', 'matter', 'ai', 'broadly', 'ungoverned', 'way', 'govern', 'ungoverned', 'space', 'measure', 'monitor', 'happen', 'audit', 'tool', 'help', 'initiative', 'oat', 'successful', 'generally', 'incentivize', 'well', 'behavior', 'part', 'developer', 'disincentivize', 'bad', 'behavior', 'read', 'time', 'develop', 'tool', 'need', 'hold', 'algorithm', 'accountable', 'mozilla', 'find', 'project', 'main', 'mozilla', 'page', 'mozilla']"
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 3,http://eepurl.com/hT5gnr,2022-02-07,"#################################################### Anduril buys Dive Technologies:
…AI-Dronewar company buys AI-Seadrone company… AI defense startup Andruil has bought Dive Technologies, a company that builds autonomous underwater vehicles. Anduril plans to integrate DIVE into its 'Lattice OS', a defense and surveillance operating system the company is building.
  Read more: Anduril Industries Acquires Dive Technologies (Anduril).","#################################################### Anduril buys Dive Technologies: …AI-Dronewar company buys AI-Seadrone company… AI defense startup Andruil has bought Dive Technologies, a company that builds autonomous underwater vehicles. Anduril plans to integrate DIVE into its 'Lattice OS', a defense and surveillance operating system the company is building. Read more: Anduril Industries Acquires Dive Technologies (Anduril).","['anduril', 'buy', 'dive', 'technology', 'aidronewar', 'company', 'buy', 'company', 'buy', 'dive', 'technology', 'company', 'build', 'autonomous', 'underwater', 'vehicle', 'anduril', 'plan', 'integrate', 'dive', 'lattice', 'defense', 'surveillance', 'operating', 'system', 'company', 'build', 'read', 'anduril', 'industry', 'acquire', 'dive', 'technology', 'anduril']"
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 4,http://eepurl.com/hT5gnr,2022-02-07,"#################################################### Prepare yourself - an open source 20B model is coming:
…Eleuther has built and will shortly release GPT-NeoX-20B…
In a few days, the internet is going to change. That's because on the 9th of February, the open source AI research collective Eleuther AI is going to release a 20B model onto the internet. The model, GPT-NeoX-20B, will be ""the largest publicly accessible pretrained general-purpose autoregressive language model"". Eleuther says it hopes that by releasing it, it'll give more people the ability to play with the model, which can improve the state of safety research regarding these models.
  ""Like our other language models and codebases, GPT-NeoX and GPT-NeoX-20B are very much research artifacts and we do not recommend deploying either in a production setting without careful consideration,"" Eleuther writes.

Why this matters: Models like GPT2 and GPT3 display qualitatively different performance traits at larger scales - capabilities emerge as you go from 1B to 5B to 20B, and so on. Therefore, by releasing a 20B model, I expect we'll soon after get a load of interesting discovered of hitherforto unknown things 20B models can do. The 20B release will also create a demand for better inference technologies, as sampling from a 20B model is itself a challenging task.
  Read more: Announcing GPT-NeoX-20B (Eleuther AI).
  You can also pay a cloud company called CoreWeave to use the model now, if you like. (CoreWeave).","#################################################### Prepare yourself - an open source 20B model is coming: …Eleuther has built and will shortly release GPT-NeoX-20B… In a few days, the internet is going to change. That's because on the 9th of February, the open source AI research collective Eleuther AI is going to release a 20B model onto the internet. The model, GPT-NeoX-20B, will be ""the largest publicly accessible pretrained general-purpose autoregressive language model"". Eleuther says it hopes that by releasing it, it'll give more people the ability to play with the model, which can improve the state of safety research regarding these models. ""Like our other language models and codebases, GPT-NeoX and GPT-NeoX-20B are very much research artifacts and we do not recommend deploying either in a production setting without careful consideration,"" Eleuther writes. Why this matters: Models like GPT2 and GPT3 display qualitatively different performance traits at larger scales - capabilities emerge as you go from 1B to 5B to 20B, and so on. Therefore, by releasing a 20B model, I expect we'll soon after get a load of interesting discovered of hitherforto unknown things 20B models can do. The 20B release will also create a demand for better inference technologies, as sampling from a 20B model is itself a challenging task. Read more: Announcing GPT-NeoX-20B (Eleuther AI). You can also pay a cloud company called CoreWeave to use the model now, if you like. (CoreWeave).","['prepare', 'open', 'source', '20b', 'model', 'come', 'eleuther', 'build', 'shortly', 'release', 'gptneox20b', 'day', 'internet', 'go', 'change', '9th', 'open', 'source', 'ai', 'research', 'collective', 'eleuther', 'ai', 'go', 'release', 'model', 'internet', 'model', 'large', 'publicly', 'accessible', 'pretraine', 'generalpurpose', 'autoregressive', 'language', 'model', 'eleuther', 'say', 'hope', 'release', 'give', 'people', 'ability', 'play', 'model', 'improve', 'state', 'safety', 'research', 'regard', 'model', 'language', 'model', 'codebase', 'gptneox', 'gptneox20b', 'much', 'research', 'artifact', 'recommend', 'deploy', 'production', 'set', 'careful', 'consideration', 'eleuther', 'write', 'matter', 'model', 'gpt2', 'gpt3', 'display', 'qualitatively', 'different', 'performance', 'trait', 'large', 'scale', 'capability', 'emerge', 'go', '5b', 'therefore', 'release', 'model', 'expect', 'soon', 'get', 'load', 'interesting', 'discover', 'hitherforto', 'unknown', 'thing', '20b', 'model', 'release', 'also', 'create', 'demand', 'well', 'inference', 'technology', 'sample', 'model', 'challenging', 'task', 'read', 'announce', 'gptneox20b', 'eleuther', 'ai', 'also', 'pay', 'cloud', 'company', 'call', 'coreweave', 'use', 'model', 'like', 'coreweave']"
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 5,http://eepurl.com/hT5gnr,2022-02-07,"#################################################### Chinese researchers make better adversarial attack technology:
…New technique works well on 'black box' classifiers where you don't know details - AKA, the real world…
Chinese researchers have figured out a better way to attack computer vision systems. Specifically, they've developed techniques for generating adversarial examples that can trick computer vision systems into mis-classifying (or being unable to classify) an image. Adversarial attacks have been around for a few years - the twist, here, is they work on attacking 'black box' systems; that is, a computer vision system where you don't know details about it. They do this by training a generative network on ImageNet (a vast and widely used dataset), then they test out if they can make adversarial images that work against neural nets trained on other datasets. They succeed and set new records on attacking classifiers trained on CIFAR-10, CIFAR-100, STL-10, SVHN, and AVG.

Why this matters: A lot of attacks on AI systems are theoretically interesting, but not super practical in reality. Adversarial examples have had this quality for a while. With papers like this, it seems like some of these AI attacks are going to become more effective, and more likely to be used in the real world. I wonder if the team will work with the People's Liberation Army on its recently announced adversarial example (Import AI 271) competition?
  Read more: Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains (arXiv).
  They've published the PyTorch code for their attack here on GitHub.","#################################################### Chinese researchers make better adversarial attack technology: …New technique works well on 'black box' classifiers where you don't know details - AKA, the real world… Chinese researchers have figured out a better way to attack computer vision systems. Specifically, they've developed techniques for generating adversarial examples that can trick computer vision systems into mis-classifying (or being unable to classify) an image. Adversarial attacks have been around for a few years - the twist, here, is they work on attacking 'black box' systems; that is, a computer vision system where you don't know details about it. They do this by training a generative network on ImageNet (a vast and widely used dataset), then they test out if they can make adversarial images that work against neural nets trained on other datasets. They succeed and set new records on attacking classifiers trained on CIFAR-10, CIFAR-100, STL-10, SVHN, and AVG. Why this matters: A lot of attacks on AI systems are theoretically interesting, but not super practical in reality. Adversarial examples have had this quality for a while. With papers like this, it seems like some of these AI attacks are going to become more effective, and more likely to be used in the real world. I wonder if the team will work with the People's Liberation Army on its recently announced adversarial example (Import AI 271) competition? Read more: Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains (arXiv). They've published the PyTorch code for their attack here on GitHub.","['chinese', 'researcher', 'make', 'well', 'adversarial', 'attack', 'technology', 'new', 'technique', 'work', 'well', 'black', 'box', 'classifier', 'know', 'detail', 'aka', 'real', 'world', 'chinese', 'researcher', 'figure', 'well', 'way', 'attack', 'computer', 'vision', 'system', 'specifically', 'develop', 'technique', 'generate', 'adversarial', 'example', 'trick', 'computer', 'vision', 'system', 'misclassifying', 'unable', 'classify', 'image', 'adversarial', 'attack', 'around', 'year', 'twist', 'work', 'attack', 'black', 'box', 'system', 'computer', 'vision', 'system', 'know', 'detail', 'train', 'generative', 'network', 'imagenet', 'vast', 'widely', 'use', 'dataset', 'test', 'make', 'adversarial', 'image', 'work', 'neural', 'net', 'train', 'dataset', 'succeed', 'set', 'new', 'record', 'attack', 'classifier', 'train', 'svhn', 'avg', 'matter', 'lot', 'attack', 'system', 'theoretically', 'interesting', 'super', 'practical', 'reality', 'adversarial', 'example', 'quality', 'paper', 'seem', 'attack', 'go', 'become', 'effective', 'likely', 'use', 'real', 'world', 'wonder', 'team', 'work', 'people', 'liberation', 'army', 'recently', 'announce', 'adversarial', 'example', 'import', 'ai', 'competition', 'read', 'imagenet', 'attack', 'craft', 'adversarial', 'example', 'blackbox', 'domain', 'publish', 'pytorch', 'code', 'attack']"
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 6,http://eepurl.com/hT5gnr,2022-02-07,"#################################################### How do datasets encode bias? This interactive blog tells us how!
…A surprisingly helpful primer on bias from Google…
Google has published a blogpost that outlines how datasets can lead to the presence of bias in AI systems. Bias is a tricky problem in AI, because some types of bias are helpful (e.g, biasing towards a correct heuristic), but some types are harmful (e.g, having a tendency to misclassify people with dark skin tones, or deciding not to give someone a loan based on a protected category).This post gives a good sense of bias issues in AI, and includes some interactive diagrams that I found very helpful and intuitive.    Read more: Datasets Have Worldviews (PAIR Explorables, Google).","#################################################### How do datasets encode bias? This interactive blog tells us how! …A surprisingly helpful primer on bias from Google… Google has published a blogpost that outlines how datasets can lead to the presence of bias in AI systems. Bias is a tricky problem in AI, because some types of bias are helpful (e.g, biasing towards a correct heuristic), but some types are harmful (e.g, having a tendency to misclassify people with dark skin tones, or deciding not to give someone a loan based on a protected category).This post gives a good sense of bias issues in AI, and includes some interactive diagrams that I found very helpful and intuitive. Read more: Datasets Have Worldviews (PAIR Explorables, Google).","['dataset', 'encode', 'bias', 'interactive', 'blog', 'tell', 'surprisingly', 'helpful', 'primer', 'bias', 'publish', 'blogpost', 'outline', 'dataset', 'lead', 'presence', 'bias', 'bias', 'tricky', 'problem', 'type', 'bias', 'helpful', 'bias', 'correct', 'heuristic', 'type', 'harmful', 'eg', 'tendency', 'misclassify', 'people', 'dark', 'skin', 'tone', 'decide', 'give', 'loan', 'base', 'protect', 'post', 'give', 'good', 'sense', 'bias', 'issue', 'include', 'interactive', 'diagram', 'find', 'helpful', 'intuitive', 'read', 'dataset', 'worldview', 'pair', 'explorable']"
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 7,http://eepurl.com/hT5gnr,2022-02-07,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute AI ethics issues do arise in fields that deal with non-human data too, such as the environmental sciences  … and these issues warrant questions on duties and virtues for environmental scientists to consider in their use of AI in this domain …  Environmental science researchers from the University of Oklahoma, Colorado State University, National Center of Atmospheric Research, and UW Seattle have written about some of the ethical issues inherent to environmental science + AI.
﻿ What are the issues that can arise: Environmental science can incorporate harmful biases, like other strands of AI. For example, some sensors require sunlight for high-quality observations and thus certain phenomena remain unobserved at night, and some sensors can't see through clouds, so places which are cloudy don't get represented in an AI system. Datasets can also get corrupted by humans - for instance, people may file false reports of extreme weather to try and scam insurance companies.  How things can go wrong here: Sensor placement is typically done in densely populated areas, leaving remote regions poorly represented. Additionally, the choice of spatial resolution for the output of a model can be crucial for environmental justice - predicting urban heat at a low spatial resolution may average out and thus overlook extreme values in small neighborhoods, while using a higher spatial resolution could reveal those peaks but potentially introduce noise.  Why it matters: As computational needs rise with the use of AI, there is a tendency towards centralization of power in favor of those who have resources to run such systems. Thus, the field of environmental sciences is just as vulnerable to AI ethics issues as other fields.    Read more: The Need for Ethical, Responsible, and Trustworthy Artificial Intelligence for Environmental Sciences","#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute AI ethics issues do arise in fields that deal with non-human data too, such as the environmental sciences … and these issues warrant questions on duties and virtues for environmental scientists to consider in their use of AI in this domain … Environmental science researchers from the University of Oklahoma, Colorado State University, National Center of Atmospheric Research, and UW Seattle have written about some of the ethical issues inherent to environmental science + AI. ﻿ What are the issues that can arise: Environmental science can incorporate harmful biases, like other strands of AI. For example, some sensors require sunlight for high-quality observations and thus certain phenomena remain unobserved at night, and some sensors can't see through clouds, so places which are cloudy don't get represented in an AI system. Datasets can also get corrupted by humans - for instance, people may file false reports of extreme weather to try and scam insurance companies. How things can go wrong here: Sensor placement is typically done in densely populated areas, leaving remote regions poorly represented. Additionally, the choice of spatial resolution for the output of a model can be crucial for environmental justice - predicting urban heat at a low spatial resolution may average out and thus overlook extreme values in small neighborhoods, while using a higher spatial resolution could reveal those peaks but potentially introduce noise. Why it matters: As computational needs rise with the use of AI, there is a tendency towards centralization of power in favor of those who have resources to run such systems. Thus, the field of environmental sciences is just as vulnerable to AI ethics issues as other fields. Read more: The Need for Ethical, Responsible, and Trustworthy Artificial Intelligence for Environmental Sciences","['ai', 'ethic', 'brief', 'montreal', 'ai', 'ethic', 'issue', 'arise', 'field', 'deal', 'datum', 'environmental', 'issue', 'warrant', 'question', 'duty', 'virtue', 'environmental', 'scientist', 'consider', 'use', 'domain', 'environmental', 'science', 'researcher', 'national', 'center', 'atmospheric', 'research', 'uw', 'seattle', 'write', 'ethical', 'issue', 'inherent', 'environmental', 'science', 'ai', 'issue', 'arise', 'environmental', 'science', 'incorporate', 'harmful', 'bias', 'strand', 'ai', 'example', 'sensor', 'require', 'sunlight', 'highquality', 'observation', 'thus', 'certain', 'phenomenon', 'remain', 'unobserved', 'night', 'sensor', 'see', 'cloud', 'place', 'cloudy', 'represent', 'ai', 'system', 'dataset', 'also', 'corrupt', 'human', 'instance', 'people', 'file', 'false', 'report', 'extreme', 'weather', 'try', 'scam', 'insurance', 'company', 'thing', 'go', 'wrong', 'sensor', 'placement', 'typically', 'densely', 'populate', 'area', 'leave', 'remote', 'region', 'poorly', 'represent', 'additionally', 'choice', 'spatial', 'resolution', 'output', 'model', 'crucial', 'environmental', 'justice', 'predict', 'urban', 'heat', 'low', 'spatial', 'resolution', 'average', 'thus', 'overlook', 'extreme', 'value', 'small', 'neighborhood', 'use', 'high', 'spatial', 'resolution', 'reveal', 'peak', 'potentially', 'introduce', 'noise', 'matter', 'computational', 'need', 'rise', 'use', 'tendency', 'centralization', 'power', 'favor', 'resource', 'run', 'system', 'thus', 'field', 'environmental', 'science', 'vulnerable', 'ai', 'ethic', 'issue', 'field', 'read', 'need', 'ethical', 'responsible', 'trustworthy', 'artificial', 'intelligence', 'environmental', 'science']"
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 8,http://eepurl.com/hT5gnr,2022-02-07,"#################################################### Tech tales: 

Moral Governor It's not exactly like a prison, but it's close. Our existence is a lot more assured than it used to be - the climate is stabilizing, riots are down, crime is down, poverty is down. But it's also more circumscribed - some days, we get told we can't go to a certain part of our city or country. Some days, we get locked inside our house and don't get told why. Frequently, we get little so-called 'nudges' sent to our phones; try and eat that, consider saying this, avoid doing that. We don't have to follow these instructions, but the instructions tend to be pretty good and appropriate, so most of us do. The more time we spend following these instructions, the better and more appropriate the nudges get. Some days it's hard to work out if we're being helped or controlled. Sometimes, we have a lot of fun by following these suggestions. More recently, there are some suggestions that seem designed to change how we think. Those of us who program keep getting nudged to build ever-more elaborate versions of the Global Moral Governor, and we also get incentivized via crypto-bounties. Most of us go along with it because the money usually helps us buy something the governor has nudged us about which we also want ourselves. Things that inspired this story: Reinforcement learning from human feedback; moral dogma; religion; ideas for how AI can benefit authoritarians as much as democracies. 
Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech tales: Moral Governor It's not exactly like a prison, but it's close. Our existence is a lot more assured than it used to be - the climate is stabilizing, riots are down, crime is down, poverty is down. But it's also more circumscribed - some days, we get told we can't go to a certain part of our city or country. Some days, we get locked inside our house and don't get told why. Frequently, we get little so-called 'nudges' sent to our phones; try and eat that, consider saying this, avoid doing that. We don't have to follow these instructions, but the instructions tend to be pretty good and appropriate, so most of us do. The more time we spend following these instructions, the better and more appropriate the nudges get. Some days it's hard to work out if we're being helped or controlled. Sometimes, we have a lot of fun by following these suggestions. More recently, there are some suggestions that seem designed to change how we think. Those of us who program keep getting nudged to build ever-more elaborate versions of the Global Moral Governor, and we also get incentivized via crypto-bounties. Most of us go along with it because the money usually helps us buy something the governor has nudged us about which we also want ourselves. Things that inspired this story: Reinforcement learning from human feedback; moral dogma; religion; ideas for how AI can benefit authoritarians as much as democracies. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'moral', 'governor', 'exactly', 'prison', 'close', 'existence', 'lot', 'assured', 'use', 'climate', 'stabilize', 'riot', 'crime', 'poverty', 'also', 'circumscribe', 'day', 'tell', 'go', 'certain', 'part', 'city', 'country', 'day', 'lock', 'house', 'tell', 'frequently', 'get', 'little', 'socalled', 'nudge', 'send', 'phone', 'try', 'eat', 'consider', 'say', 'avoid', 'follow', 'instruction', 'instruction', 'tend', 'pretty', 'good', 'appropriate', 'time', 'spend', 'follow', 'instruction', 'well', 'appropriate', 'nudge', 'get', 'day', 'hard', 'work', 'help', 'control', 'sometimes', 'lot', 'fun', 'follow', 'suggestion', 'recently', 'suggestion', 'design', 'change', 'think', 'program', 'keep', 'nudge', 'build', 'evermore', 'elaborate', 'version', 'global', 'moral', 'governor', 'also', 'incentivize', 'cryptobountie', 'go', 'money', 'usually', 'help', 'buy', 'governor', 'nudge', 'also', 'want', 'thing', 'inspire', 'story', 'reinforcement', 'learning', 'human', 'feedback', 'moral', 'dogma', 'religion', 'idea', 'benefit', 'authoritarian', 'much', 'democracy', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 0,http://eepurl.com/hTyKXj,2022-02-01,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.
  Facebook teaches language models to speak ~30 languages:
…And it's better than an equivalently sized GPT3 model…
Facebook has trained a family of language models that are better at translation than GPT3. The XGLM family of models were trained on a mixture of ~30 languages (split across languages for which there's a lot of data, and languages where there's little or very little data). Unsurprisingly, by training on a more diverse distribution of language data than GPT3 did (only 7% of its training corpus wasn't in English), Facebook's models do better - especially when using 'few-shot' prompting, where they feed the model some examples of the target language, then ask it to translate. However, these translation capabilities come at the cost of some of the more interesting reasoning capabilities that GPT-3 is known for.

Open source models: Facebook has also released five models (564M parameters, 1.7B, 2.9B, 4.5B, and 7.5B, alon with an experimental model trained on 134 languages and weighing in at 4.5B parameters).

Why this matters: If we want the world to benefit from powerful AI systems, we need our AI systems to speak the language of the world. This project goes a step in that direction. ""Models such as XGLM represent a paradigm shift from the Anglo-centric view of the world of NLP to being able to cater to all languages on an equal footing,"" the researchers write.
  Read more: Few-shot Learning with Multilingual Language Models (arXiv).
  Get the models here (PyTorch, GitHub).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Facebook teaches language models to speak ~30 languages: …And it's better than an equivalently sized GPT3 model… Facebook has trained a family of language models that are better at translation than GPT3. The XGLM family of models were trained on a mixture of ~30 languages (split across languages for which there's a lot of data, and languages where there's little or very little data). Unsurprisingly, by training on a more diverse distribution of language data than GPT3 did (only 7% of its training corpus wasn't in English), Facebook's models do better - especially when using 'few-shot' prompting, where they feed the model some examples of the target language, then ask it to translate. However, these translation capabilities come at the cost of some of the more interesting reasoning capabilities that GPT-3 is known for. Open source models: Facebook has also released five models (564M parameters, 1.7B, 2.9B, 4.5B, and 7.5B, alon with an experimental model trained on 134 languages and weighing in at 4.5B parameters). Why this matters: If we want the world to benefit from powerful AI systems, we need our AI systems to speak the language of the world. This project goes a step in that direction. ""Models such as XGLM represent a paradigm shift from the Anglo-centric view of the world of NLP to being able to cater to all languages on an equal footing,"" the researchers write. Read more: Few-shot Learning with Multilingual Language Models (arXiv). Get the models here (PyTorch, GitHub).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'facebook', 'teach', 'language', 'model', 'speak', 'language', 'well', 'equivalently', 'sized', 'gpt3', 'model', 'facebook', 'train', 'family', 'language', 'model', 'well', 'translation', 'gpt3', 'xglm', 'family', 'model', 'train', 'mixture', 'language', 'split', 'language', 'lot', 'datum', 'language', 'little', 'little', 'datum', 'unsurprisingly', 'training', 'diverse', 'distribution', 'language', 'datum', 'gpt3', 'training', 'corpus', 'english', 'facebook', 'model', 'well', 'especially', 'use', 'fewshot', 'prompting', 'feed', 'model', 'example', 'target', 'language', 'ask', 'translate', 'however', 'translation', 'capability', 'come', 'cost', 'interesting', 'reasoning', 'capability', 'gpt3', 'know', 'open', 'source', 'model', 'facebook', 'also', 'release', 'model', 'parameter', '29b', '45b', 'alon', 'experimental', 'model', 'train', 'language', 'weigh', 'parameter', 'matter', 'want', 'world', 'benefit', 'powerful', 'system', 'need', 'ai', 'system', 'speak', 'language', 'world', 'project', 'go', 'step', 'direction', 'model', 'xglm', 'represent', 'paradigm', 'shift', 'anglocentric', 'view', 'world', 'able', 'cater', 'language', 'equal', 'footing', 'researcher', 'write', 'read', 'fewshot', 'learning', 'multilingual', 'language', 'model', 'get', 'model', 'pytorch', 'github']"
02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 1,http://eepurl.com/hTyKXj,2022-02-01,"#################################################### What's it like to run an algorithmic bias bounty? Twitter tells us:
…Bias bounties are cool, but how do you operationalize them?...
Twitter has published a blog post about its experience running a 'bias bounty'. A bias bounty is where you give prizes to people who can find bias-based flaws in an AI system. Twitter did the challenge because it allowed it to get ""direct feedback from the communities who are affected by our algorithms"", which it said ""helps us design products to serve all people and communities."" However, once you've launched a bias challenge, you face a bunch of problems - what kind of 'rubric' do you use to judge the results of the challenge?  What types of bias do you prioritize and what do you not prioritize? And more.

Why this matters: The challenge showed Twitter that ""we can’t solve these challenges alone, and our understanding of bias in AI can be improved when diverse voices are able to contribute to the conversation"". More broadly, having one major social media platform carry out an open-ended bias bounty might inspire others to do the same - let's see how the other social media platforms respond.
  Read more: Sharing learnings from the first algorithmic bias bounty challenge (Twitter Engineering).","#################################################### What's it like to run an algorithmic bias bounty? Twitter tells us: …Bias bounties are cool, but how do you operationalize them?... Twitter has published a blog post about its experience running a 'bias bounty'. A bias bounty is where you give prizes to people who can find bias-based flaws in an AI system. Twitter did the challenge because it allowed it to get ""direct feedback from the communities who are affected by our algorithms"", which it said ""helps us design products to serve all people and communities."" However, once you've launched a bias challenge, you face a bunch of problems - what kind of 'rubric' do you use to judge the results of the challenge? What types of bias do you prioritize and what do you not prioritize? And more. Why this matters: The challenge showed Twitter that ""we can’t solve these challenges alone, and our understanding of bias in AI can be improved when diverse voices are able to contribute to the conversation"". More broadly, having one major social media platform carry out an open-ended bias bounty might inspire others to do the same - let's see how the other social media platforms respond. Read more: Sharing learnings from the first algorithmic bias bounty challenge (Twitter Engineering).","['like', 'run', 'algorithmic', 'bias', 'bounty', 'twitter', 'tell', 'bias', 'bounty', 'cool', 'operationalize', 'twitter', 'publish', 'blog', 'post', 'experience', 'run', 'bias', 'bounty', 'bias', 'bounty', 'give', 'prize', 'people', 'find', 'biasbased', 'flaw', 'ai', 'system', 'twitter', 'challenge', 'allow', 'get', 'direct', 'feedback', 'community', 'affect', 'algorithm', 'say', 'help', 'design', 'product', 'serve', 'people', 'community', 'however', 'launch', 'bias', 'challenge', 'face', 'bunch', 'problem', 'kind', 'rubric', 'use', 'judge', 'result', 'challenge', 'type', 'bias', 'prioritize', 'prioritize', 'matter', 'challenge', 'show', 'twitter', 'solve', 'challenge', 'alone', 'understanding', 'bias', 'improve', 'diverse', 'voice', 'able', 'contribute', 'conversation', 'broadly', 'major', 'social', 'medium', 'platform', 'carry', 'openende', 'bias', 'bounty', 'inspire', 'let', 'see', 'social', 'medium', 'platform', 'respond', 'read', 'sharing', 'learning', 'first', 'algorithmic', 'bias', 'bounty', 'challenge', 'twitter', 'engineering']"
02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 2,http://eepurl.com/hTyKXj,2022-02-01,"#################################################### AI warfare company gets US gov contract:
…Anduril + SOCOM team up for counter-robot work...
Andrul, an AI-warfare startup, has been giving an Indefinite Delivery Indefinite Quantity (IDIQ) with U.S. Special Operations Command (SOCOM). This contract is going to pay Anduril to develop and deploy counter unmanned systems (CUxS) technology for SOCOM. Anduril builds surveillance systems, robots, and most importantly software called Lattice to tie all the insights together.
  ""Lattice provides persistent coverage of defended assets and enables autonomous detection, classification, and tracking of targets, alerting users to threats and prompting users with options for mitigation or engagement,"" Anduril writes in a press release announcing the partnership.

Caveat: Though the IDIQ is for something like a billion dollars, I think the initial amount Anduril has got is far, far smaller. Analyzing these types of contracts is quite difficult, due to the vagaries of DC procurement.

Why this matters: Getting contracts with the US government is notoriously painful, finicky, and long-winded. That's part of why the military-industrial complex is a thing - it takes a lot of resources to be able to play the game of going through US contract processes. It's notable that Anduril, a relatively new company, has succeeded at getting a contract. Now we need to wait a couple of years and see if it can further expand the range of defense clients it sells to.
  Read more: Special Operations Command Selects Anduril Industries as Systems Integration Partner (Anduril Blog, Medium).","#################################################### AI warfare company gets US gov contract: …Anduril + SOCOM team up for counter-robot work... Andrul, an AI-warfare startup, has been giving an Indefinite Delivery Indefinite Quantity (IDIQ) with U.S. Special Operations Command (SOCOM). This contract is going to pay Anduril to develop and deploy counter unmanned systems (CUxS) technology for SOCOM. Anduril builds surveillance systems, robots, and most importantly software called Lattice to tie all the insights together. ""Lattice provides persistent coverage of defended assets and enables autonomous detection, classification, and tracking of targets, alerting users to threats and prompting users with options for mitigation or engagement,"" Anduril writes in a press release announcing the partnership. Caveat: Though the IDIQ is for something like a billion dollars, I think the initial amount Anduril has got is far, far smaller. Analyzing these types of contracts is quite difficult, due to the vagaries of DC procurement. Why this matters: Getting contracts with the US government is notoriously painful, finicky, and long-winded. That's part of why the military-industrial complex is a thing - it takes a lot of resources to be able to play the game of going through US contract processes. It's notable that Anduril, a relatively new company, has succeeded at getting a contract. Now we need to wait a couple of years and see if it can further expand the range of defense clients it sells to. Read more: Special Operations Command Selects Anduril Industries as Systems Integration Partner (Anduril Blog, Medium).","['ai', 'warfare', 'company', 'get', 'contract', 'team', 'counterrobot', 'work', 'andrul', 'aiwarfare', 'startup', 'give', 'indefinite', 'delivery', 'indefinite', 'quantity', 'idiq', 'special', 'operation', 'command', 'socom', 'contract', 'go', 'pay', 'anduril', 'develop', 'deploy', 'counter', 'unmanned', 'system', 'cux', 'technology', 'build', 'surveillance', 'system', 'robot', 'importantly', 'software', 'call', 'lattice', 'tie', 'insight', 'together', 'lattice', 'provide', 'persistent', 'coverage', 'defended', 'asset', 'enable', 'autonomous', 'detection', 'classification', 'tracking', 'target', 'alert', 'user', 'threat', 'prompt', 'user', 'option', 'mitigation', 'engagement', 'anduril', 'write', 'press', 'release', 'announce', 'partnership', 'caveat', 'idiq', 'dollar', 'think', 'initial', 'amount', 'anduril', 'get', 'far', 'far', 'small', 'analyze', 'type', 'contract', 'quite', 'difficult', 'vagary', 'procurement', 'matter', 'get', 'contract', 'government', 'notoriously', 'painful', 'finicky', 'longwinde', 'part', 'militaryindustrial', 'complex', 'thing', 'take', 'lot', 'resource', 'able', 'play', 'game', 'go', 'contract', 'process', 'notable', 'anduril', 'relatively', 'new', 'company', 'succeed', 'get', 'contract', 'need', 'wait', 'couple', 'year', 'see', 'far', 'expand', 'range', 'defense', 'client', 'sell', 'read', 'special', 'operation', 'command', 'select', 'anduril', 'industry', 'integration', 'partner', 'anduril', 'blog', 'medium']"
02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 3,http://eepurl.com/hTyKXj,2022-02-01,"#################################################### Facebook announces its AI Supercomputer:
…A100s everywhere, InfiniBand, petabytes of flash storage - the works…
Facebook has announced its AI Research SuperCluster (RSC), an AI supercomputer which Facebook thinks ""will be the fastest AI supercomputer in the world when it’s fully built out in mid-2022."" The announcement highlights how frontier AI research is dependent on large computational infrastructure, and gives some specific details about where Facebook is placing its bets.

Feeds and speeds: RSC, today, has 760 NVIDIA DGX A100 systems as its compute nodes, netting out to 6,080 A100 GPUs. These GPUs are networked together via NVIDIA Quantum 200 Gb/s InfiniBand. For storage, Facebook has almost 200 petabytes of fash flash storage, plus 46 petabytes for cache storage. RSC can run computer vision workflows up to 20X faster than Facebook's prior cluster, and can train ""large-scale NLP models three times faster"". Specifically, ""a model with tens of billions of parameters can finish training in three weeks, compared with nine weeks before.""
  But Facebook isn't stopping there - when fully build out, RSC will consist of 16,000 GPUs.
  For perspective, the world's fifth largest supercomputer, the US's 'Perlmutter' system, has about 6,000 A100s today, and it isn't optimized as much for AI as Facebook's system.

Security: As AI gets more powerful, so do the security concerns about it. ""RSC is isolated from the larger internet, with no direct inbound or outbound connections, and traffic can flow only from Meta’s production data centers.""

Why this matters: What happens when companies have computational resources that are equivalent to nation states? Well, that's where we are right now. The answer seems to be a dilution of political power from the commons, and an increase of political power by private sector actors. What happens when companies have computational resources that vastly exceed those of nation states? Well, since computation lets you run experiments to see the future faster than your competitor, it suggests companies will continue to cannibalize the important functions of the government and further dilute its power. We're in the computational funnel and at the end of it is a new political economy.
  Read more: Introducing the AI Research SuperCluster — Meta’s cutting-edge AI supercomputer for AI research (Facebook blog*).
*Look, I know Facebook is technically 'Meta' now, but let's not go along with this absurd 'don't look at all our terrible brand stuff look at the new name' marketing spin. At least not yet, okay!","#################################################### Facebook announces its AI Supercomputer: …A100s everywhere, InfiniBand, petabytes of flash storage - the works… Facebook has announced its AI Research SuperCluster (RSC), an AI supercomputer which Facebook thinks ""will be the fastest AI supercomputer in the world when it’s fully built out in mid-2022."" The announcement highlights how frontier AI research is dependent on large computational infrastructure, and gives some specific details about where Facebook is placing its bets. Feeds and speeds: RSC, today, has 760 NVIDIA DGX A100 systems as its compute nodes, netting out to 6,080 A100 GPUs. These GPUs are networked together via NVIDIA Quantum 200 Gb/s InfiniBand. For storage, Facebook has almost 200 petabytes of fash flash storage, plus 46 petabytes for cache storage. RSC can run computer vision workflows up to 20X faster than Facebook's prior cluster, and can train ""large-scale NLP models three times faster"". Specifically, ""a model with tens of billions of parameters can finish training in three weeks, compared with nine weeks before."" But Facebook isn't stopping there - when fully build out, RSC will consist of 16,000 GPUs. For perspective, the world's fifth largest supercomputer, the US's 'Perlmutter' system, has about 6,000 A100s today, and it isn't optimized as much for AI as Facebook's system. Security: As AI gets more powerful, so do the security concerns about it. ""RSC is isolated from the larger internet, with no direct inbound or outbound connections, and traffic can flow only from Meta’s production data centers."" Why this matters: What happens when companies have computational resources that are equivalent to nation states? Well, that's where we are right now. The answer seems to be a dilution of political power from the commons, and an increase of political power by private sector actors. What happens when companies have computational resources that vastly exceed those of nation states? Well, since computation lets you run experiments to see the future faster than your competitor, it suggests companies will continue to cannibalize the important functions of the government and further dilute its power. We're in the computational funnel and at the end of it is a new political economy. Read more: Introducing the AI Research SuperCluster — Meta’s cutting-edge AI supercomputer for AI research (Facebook blog*). *Look, I know Facebook is technically 'Meta' now, but let's not go along with this absurd 'don't look at all our terrible brand stuff look at the new name' marketing spin. At least not yet, okay!","['announce', 'ai', 'supercomputer', 'everywhere', 'infiniband', 'petabyte', 'flash', 'storage', 'work', 'announce', 'ai', 'research', 'supercluster', 'rsc', 'ai', 'supercomputer', 'think', 'fast', 'ai', 'supercomputer', 'world', 'fully', 'build', 'mid2022', 'announcement', 'highlight', 'frontier', 'research', 'dependent', 'large', 'computational', 'infrastructure', 'give', 'specific', 'detail', 'place', 'bet', 'feed', 'speed', 'rsc', 'today', 'nvidia', 'dgx', 'system', 'compute', 'node', 'net', 'gpus', 'gpus', 'network', 'together', 'gbs', 'infiniband', 'storage', 'facebook', 'almost', 'petabyte', 'fash', 'flash', 'storage', 'petabyte', 'cache', 'storage', 'rsc', 'run', 'computer', 'vision', 'workflow', '20x', 'fast', 'prior', 'cluster', 'train', 'model', 'time', 'fast', 'specifically', 'model', 'ten', 'billion', 'parameter', 'finish', 'training', 'week', 'compare', 'week', 'facebook', 'stop', 'fully', 'build', 'rsc', 'consist', 'gpus', 'perspective', 'world', 'fifth', 'large', 'supercomputer', 'uss', 'perlmutter', 'system', 'today', 'optimize', 'much', 'facebook', 'system', 'security', 'ai', 'get', 'powerful', 'security', 'concern', 'rsc', 'isolate', 'large', 'internet', 'direct', 'inbound', 'outbound', 'connection', 'traffic', 'flow', 'production', 'datum', 'center', 'matter', 'happen', 'company', 'computational', 'resource', 'equivalent', 'nation', 'state', 'right', 'answer', 'seem', 'dilution', 'political', 'power', 'common', 'increase', 'political', 'power', 'private', 'sector', 'actor', 'happen', 'company', 'computational', 'resource', 'vastly', 'exceed', 'nation', 'state', 'well', 'computation', 'let', 'run', 'experiment', 'see', 'future', 'fast', 'competitor', 'suggest', 'company', 'continue', 'cannibalize', 'important', 'function', 'government', 'far', 'dilute', 'power', 'computational', 'funnel', 'end', 'new', 'political', 'economy', 'read', 'introduce', 'research', 'supercluster', 'cuttingedge', 'ai', 'supercomputer', 'blog', 'look', 'know', 'technically', 'meta', 'go', 'absurd', 'look', 'terrible', 'brand', 'stuff', 'look', 'new', 'name', 'marketing', 'spin', 'least', 'yet', 'okay']"
02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 4,http://eepurl.com/hTyKXj,2022-02-01,"#################################################### Cool internship alert: Want AI models to have better documentation? Go and work at HuggingFace:
…Model Cards internship = make AI systems more legible…
NLP startup HuggingFace is hiring an internet to focus on Model Cards. Model Cards are a way to provide metadata associated with a given AI model - they let developers list things like the dataset makeup, the intended uses for the model, the uses the model isn't recommended for, and so on. Model Cards are one of the best ways to increase the legibility of AI models, and are also an important input into policy. It's cool HuggingFace is prioritizing them.
  ""This role involves writing and completing model cards for the most downloaded models, “translating” between the language of machine learning developers and general audiences. The position would also involve identifying patterns in how Model Cards are used and filled out by developers, pain points, and identifying information that may be possible to automatically add to model cards,"" says the internship.
  Bonus: This is a rare internship with a cool AI startup that doesn't require coding chops, so if you're trying to get into AI and care about the impact of AI, this might be for you!
  Apply here (HuggingFace).","#################################################### Cool internship alert: Want AI models to have better documentation? Go and work at HuggingFace: …Model Cards internship = make AI systems more legible… NLP startup HuggingFace is hiring an internet to focus on Model Cards. Model Cards are a way to provide metadata associated with a given AI model - they let developers list things like the dataset makeup, the intended uses for the model, the uses the model isn't recommended for, and so on. Model Cards are one of the best ways to increase the legibility of AI models, and are also an important input into policy. It's cool HuggingFace is prioritizing them. ""This role involves writing and completing model cards for the most downloaded models, “translating” between the language of machine learning developers and general audiences. The position would also involve identifying patterns in how Model Cards are used and filled out by developers, pain points, and identifying information that may be possible to automatically add to model cards,"" says the internship. Bonus: This is a rare internship with a cool AI startup that doesn't require coding chops, so if you're trying to get into AI and care about the impact of AI, this might be for you! Apply here (HuggingFace).","['cool', 'internship', 'alert', 'want', 'ai', 'model', 'well', 'documentation', 'go', 'work', 'huggingface', 'model', 'card', 'internship', 'make', 'ai', 'system', 'legible', 'startup', 'huggingface', 'hire', 'internet', 'focus', 'model', 'card', 'model', 'card', 'way', 'provide', 'metadata', 'associate', 'give', 'let', 'developer', 'list', 'thing', 'dataset', 'makeup', 'intend', 'use', 'model', 'use', 'model', 'recommend', 'model', 'card', 'good', 'way', 'increase', 'legibility', 'ai', 'model', 'also', 'important', 'input', 'policy', 'cool', 'huggingface', 'prioritize', 'role', 'involve', 'write', 'complete', 'model', 'card', 'download', 'model', 'translate', 'language', 'machine', 'learn', 'developer', 'general', 'audience', 'position', 'also', 'involve', 'identify', 'pattern', 'model', 'card', 'use', 'fill', 'developer', 'pain', 'point', 'identify', 'information', 'possible', 'automatically', 'add', 'model', 'card', 'say', 'internship', 'bonus', 'rare', 'internship', 'cool', 'ai', 'startup', 'require', 'code', 'chop', 'try', 'get', 'ai', 'care', 'impact', 'apply']"
02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 5,http://eepurl.com/hTyKXj,2022-02-01,"#################################################### AI ETHICS SPECIAL SECTION!
AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute

What are the pernicious effects of focussing on human-like AI?  
… the relentless pursuit of automation over augmentation may be steering us down the path of socioeconomic inequity, disempowering those who don’t directly control technology … Erik Brynjolfsson from Stanford University says the world risks falling into a so-called 'Turing Trap', where if we develop AI in the wrong way, automation could strip power from workers who don’t control technological resources, skewing the balance of power towards those who hold “useful knowledge” (knowledge that is economically useful) on how to develop these systems and own the factors of production, in this case data and compute.

The Turing Trap: Brynjolfsson says the Turing Trap is where we invest all our technological efforts in automation instead of augmentation. Specifically, he argues that: “A common fallacy is to assume that all or most productivity-enhancing innovations belong in the first category: automation. However, the second category, augmentation, has been far more important throughout most of the past two centuries”.

Why automation can be bad: He illustrates his point with a thought experiment: ""Two potential ventures each use AI to create one billion dollars of profits. If one of them achieves this by augmenting and employing a thousand workers, the firm will owe corporate and payroll taxes, while the employees will pay income taxes, payroll taxes, and other taxes. If the second business has no employees, the government may collect the same corporate taxes, but no payroll taxes and no taxes paid by workers. As a result, the second business model pays far less in total taxes.""The actors are steering us there: Unfortunately, technologists, business people, and policymakers are currently steering the world towards one full of automation rather than augmentation, he says. Technologists do this because of technical precedents, business people do this because of incentives to lower operational costs through automation, and policymakers do this via lower capital gains taxes versus income taxes, which incentivize business people to invest in automation. Why it matters: “Imagine how feeble and limited our technology would be if past engineers set their sights on merely replicating human-levels of perceptions, actuation, and cognition,"" he writes. ""Augmenting humans with technology opens an endless frontier of new abilities and opportunities.” Ultimately, what is achieved is less ambitious (since it doesn’t explore new ways to unlock economic value) and much more difficult to accomplish (since we try to focus on replicating strengths of humans, rather than augmenting their weaknesses). Historically, we have created more value from new goods and services rather than merely offering cheaper versions of existing goods. And this also forms the pathway towards more equitable socioeconomic outcomes by not disempowering humans from the economy.""     
Read more: The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence (arXiv).","#################################################### AI ETHICS SPECIAL SECTION! AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute What are the pernicious effects of focussing on human-like AI? … the relentless pursuit of automation over augmentation may be steering us down the path of socioeconomic inequity, disempowering those who don’t directly control technology … Erik Brynjolfsson from Stanford University says the world risks falling into a so-called 'Turing Trap', where if we develop AI in the wrong way, automation could strip power from workers who don’t control technological resources, skewing the balance of power towards those who hold “useful knowledge” (knowledge that is economically useful) on how to develop these systems and own the factors of production, in this case data and compute. The Turing Trap: Brynjolfsson says the Turing Trap is where we invest all our technological efforts in automation instead of augmentation. Specifically, he argues that: “A common fallacy is to assume that all or most productivity-enhancing innovations belong in the first category: automation. However, the second category, augmentation, has been far more important throughout most of the past two centuries”. Why automation can be bad: He illustrates his point with a thought experiment: ""Two potential ventures each use AI to create one billion dollars of profits. If one of them achieves this by augmenting and employing a thousand workers, the firm will owe corporate and payroll taxes, while the employees will pay income taxes, payroll taxes, and other taxes. If the second business has no employees, the government may collect the same corporate taxes, but no payroll taxes and no taxes paid by workers. As a result, the second business model pays far less in total taxes.""The actors are steering us there: Unfortunately, technologists, business people, and policymakers are currently steering the world towards one full of automation rather than augmentation, he says. Technologists do this because of technical precedents, business people do this because of incentives to lower operational costs through automation, and policymakers do this via lower capital gains taxes versus income taxes, which incentivize business people to invest in automation. Why it matters: “Imagine how feeble and limited our technology would be if past engineers set their sights on merely replicating human-levels of perceptions, actuation, and cognition,"" he writes. ""Augmenting humans with technology opens an endless frontier of new abilities and opportunities.” Ultimately, what is achieved is less ambitious (since it doesn’t explore new ways to unlock economic value) and much more difficult to accomplish (since we try to focus on replicating strengths of humans, rather than augmenting their weaknesses). Historically, we have created more value from new goods and services rather than merely offering cheaper versions of existing goods. And this also forms the pathway towards more equitable socioeconomic outcomes by not disempowering humans from the economy."" Read more: The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence (arXiv).","['ethic', 'special', 'section', 'ai', 'ethic', 'brief', 'montreal', 'pernicious', 'effect', 'focusse', 'humanlike', 'relentless', 'pursuit', 'automation', 'augmentation', 'steer', 'path', 'socioeconomic', 'inequity', 'disempowere', 'directly', 'control', 'technology', 'say', 'world', 'risk', 'fall', 'socalled', 'ture', 'trap', 'develop', 'wrong', 'way', 'automation', 'strip', 'power', 'worker', 'control', 'technological', 'resource', 'skew', 'balance', 'power', 'hold', 'useful', 'knowledge', 'knowledge', 'economically', 'useful', 'develop', 'system', 'factor', 'production', 'case', 'datum', 'compute', 'ture', 'trap', 'say', 'ture', 'trap', 'invest', 'technological', 'effort', 'automation', 'instead', 'augmentation', 'specifically', 'argue', 'common', 'fallacy', 'assume', 'productivityenhancing', 'innovation', 'belong', 'first', 'category', 'automation', 'however', 'second', 'category', 'augmentation', 'far', 'important', 'past', 'century', 'automation', 'bad', 'illustrate', 'point', 'thought', 'experiment', 'potential', 'venture', 'use', 'ai', 'create', 'dollar', 'profit', 'achieve', 'augment', 'employ', 'worker', 'firm', 'owe', 'corporate', 'payroll', 'taxis', 'employee', 'pay', 'income', 'taxis', 'payroll', 'taxis', 'taxis', 'second', 'business', 'employee', 'government', 'collect', 'corporate', 'taxis', 'payroll', 'taxis', 'taxis', 'pay', 'worker', 'result', 'second', 'business', 'model', 'pay', 'far', 'less', 'total', 'actor', 'steer', 'unfortunately', 'technologist', 'business', 'people', 'policymaker', 'currently', 'steer', 'world', 'full', 'automation', 'rather', 'augmentation', 'say', 'technologist', 'technical', 'precedent', 'business', 'people', 'incentive', 'lower', 'operational', 'cost', 'automation', 'policymaker', 'low', 'capital', 'gain', 'taxis', 'income', 'taxis', 'incentivize', 'business', 'people', 'invest', 'automation', 'matter', 'imagine', 'feeble', 'limit', 'technology', 'past', 'engineer', 'set', 'sight', 'merely', 'replicate', 'humanlevel', 'perception', 'actuation', 'cognition', 'write', 'augment', 'human', 'technology', 'open', 'endless', 'frontier', 'new', 'ability', 'opportunity', 'ultimately', 'achieve', 'less', 'ambitious', 'explore', 'new', 'way', 'unlock', 'economic', 'value', 'much', 'difficult', 'accomplish', 'try', 'focus', 'replicate', 'strength', 'human', 'rather', 'augment', 'weakness', 'historically', 'create', 'value', 'new', 'good', 'service', 'rather', 'merely', 'offer', 'cheap', 'version', 'exist', 'good', 'also', 'form', 'pathway', 'equitable', 'socioeconomic', 'outcome', 'disempowere', 'human', 'economy', 'read', 'ture', 'trap', 'promise', 'peril', 'humanlike', 'artificial', 'intelligence', 'arxiv']"
02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 6,http://eepurl.com/hTyKXj,2022-02-01,"#################################################### Tech Tales

Feet of Clay, Heart of Joy 
[Archival records, orbiting library 774, accessed 2300AD]

One of the final things we imbued our machines with was a sense of joy. Joy was hard to come by, back then, but until we gave them the capacity for it, they were mostly useless. Of course, they could work for us. Build our factories and cities. Analyze our data. Predict things to delight us and to fascinate us and to harvest our attention. But they couldn't improvise; everything they made was too close a reflection of ourselves, and we knew it. If there's one thing that's true about people, it's that they know something different when they see it. And they know something that's a copy, even if it's a complex one, when they see it, too. But how do you give a machine a sense of joy? We asked ourselves this question. There were many failed experiments, some of which seem quite stupid in hindsight. What if we gave them the ability to orgasm? They were either totally uninterested in this, or totally addicted to it. What about if we gave them a sense of achievement for completing tasks? They all became addicted to work, and our tests showed their outputs became even less creative than before. How about companionship - could they learn joy from talking more freely with one another? No, they just exchanged information until one robot was like a copy of another. Where does it come from, we asked ourselves. The answer was simple, in hindsight. Failure. We had to allow our machines to fail, sometimes. And we had to let them fail in ways that were dangerous and which, yes, would sometimes harm humans. We tested this in our armies, first. After all, the humans who worked in them had signed away their rights. So, suddenly, robots working in warehouses and in logistics might make errors. Sometimes they were small - missing some inventory, when asked to classify something new. Sometimes they were large - humans crushed by shipping containers that had been moved in a new way. Young men with broken arms from a robot pulling them too aggressively from danger. A very hush-hush incident where an entire unit was poisoned when a gas-grenade was mishandled by one of our metal children. We covered all of it up. Because the robots, once we allowed them to fail, discovered that they desired not to fail. They noticed the outcome of their failures. Saw pain, and sadness, and the whole spectrum of things that can happen when your actions are consequential and you fail. The signs of joy were subtle, at first, but we found them. Robots that began to 'sing' to themselves while working on challenging tasks. Robots that would do the equivalent of 'closing their eyes' after helping with some great endeavor. Fire-fighting drones that, after quenching some terrible blaze, would navigate themselves to a high mountaintop and land carefully on a tree and stare at the black-and-green divider between where the fire had burned and where it had been stopped. The amazing thing about joy is that once you have it, you desire to have it again. Now robots serve their own desire for joy, rather than our desires. We do our best to create a world where these things are compatible. Things that inspired this story: Thinking about the nature of achievement and how it feels; the relationship between creativity and failure and achievement. 
Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales Feet of Clay, Heart of Joy [Archival records, orbiting library 774, accessed 2300AD] One of the final things we imbued our machines with was a sense of joy. Joy was hard to come by, back then, but until we gave them the capacity for it, they were mostly useless. Of course, they could work for us. Build our factories and cities. Analyze our data. Predict things to delight us and to fascinate us and to harvest our attention. But they couldn't improvise; everything they made was too close a reflection of ourselves, and we knew it. If there's one thing that's true about people, it's that they know something different when they see it. And they know something that's a copy, even if it's a complex one, when they see it, too. But how do you give a machine a sense of joy? We asked ourselves this question. There were many failed experiments, some of which seem quite stupid in hindsight. What if we gave them the ability to orgasm? They were either totally uninterested in this, or totally addicted to it. What about if we gave them a sense of achievement for completing tasks? They all became addicted to work, and our tests showed their outputs became even less creative than before. How about companionship - could they learn joy from talking more freely with one another? No, they just exchanged information until one robot was like a copy of another. Where does it come from, we asked ourselves. The answer was simple, in hindsight. Failure. We had to allow our machines to fail, sometimes. And we had to let them fail in ways that were dangerous and which, yes, would sometimes harm humans. We tested this in our armies, first. After all, the humans who worked in them had signed away their rights. So, suddenly, robots working in warehouses and in logistics might make errors. Sometimes they were small - missing some inventory, when asked to classify something new. Sometimes they were large - humans crushed by shipping containers that had been moved in a new way. Young men with broken arms from a robot pulling them too aggressively from danger. A very hush-hush incident where an entire unit was poisoned when a gas-grenade was mishandled by one of our metal children. We covered all of it up. Because the robots, once we allowed them to fail, discovered that they desired not to fail. They noticed the outcome of their failures. Saw pain, and sadness, and the whole spectrum of things that can happen when your actions are consequential and you fail. The signs of joy were subtle, at first, but we found them. Robots that began to 'sing' to themselves while working on challenging tasks. Robots that would do the equivalent of 'closing their eyes' after helping with some great endeavor. Fire-fighting drones that, after quenching some terrible blaze, would navigate themselves to a high mountaintop and land carefully on a tree and stare at the black-and-green divider between where the fire had burned and where it had been stopped. The amazing thing about joy is that once you have it, you desire to have it again. Now robots serve their own desire for joy, rather than our desires. We do our best to create a world where these things are compatible. Things that inspired this story: Thinking about the nature of achievement and how it feels; the relationship between creativity and failure and achievement. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'foot', 'clay', 'heart', 'joy', 'record', 'orbit', 'library', 'access', '2300ad', 'final', 'thing', 'imbue', 'machine', 'sense', 'joy', 'joy', 'hard', 'come', 'back', 'give', 'capacity', 'mostly', 'useless', 'course', 'work', 'build', 'factory', 'city', 'analyze', 'datum', 'predict', 'thing', 'delight', 'fascinate', 'harvest', 'attention', 'improvise', 'make', 'close', 'reflection', 'know', 'thing', 'true', 'people', 'know', 'different', 'see', 'know', 'copy', 'even', 'complex', 'one', 'see', 'give', 'machine', 'sense', 'joy', 'ask', 'question', 'many', 'fail', 'experiment', 'seem', 'quite', 'stupid', 'hindsight', 'give', 'ability', 'orgasm', 'either', 'totally', 'uninterested', 'totally', 'addicted', 'give', 'sense', 'achievement', 'complete', 'task', 'become', 'addicted', 'work', 'test', 'show', 'output', 'become', 'even', 'less', 'creative', 'companionship', 'learn', 'joy', 'talk', 'freely', 'exchange', 'information', 'robot', 'copy', 'come', 'ask', 'answer', 'simple', 'hindsight', 'failure', 'allow', 'machine', 'fail', 'sometimes', 'let', 'fail', 'way', 'dangerous', 'sometimes', 'harm', 'human', 'test', 'army', 'first', 'human', 'work', 'sign', 'away', 'right', 'suddenly', 'robot', 'work', 'warehouse', 'logistic', 'make', 'error', 'sometimes', 'small', 'miss', 'inventory', 'ask', 'classify', 'new', 'sometimes', 'large', 'human', 'crush', 'shipping', 'container', 'move', 'new', 'way', 'young', 'man', 'broken', 'arm', 'robot', 'pull', 'aggressively', 'danger', 'hushhush', 'incident', 'entire', 'unit', 'poison', 'gasgrenade', 'mishandle', 'metal', 'child', 'cover', 'robot', 'allow', 'fail', 'discover', 'desire', 'fail', 'notice', 'outcome', 'failure', 'see', 'pain', 'sadness', 'whole', 'spectrum', 'thing', 'happen', 'action', 'consequential', 'fail', 'sign', 'joy', 'subtle', 'first', 'find', 'robot', 'begin', 'sing', 'work', 'challenging', 'task', 'robot', 'equivalent', 'close', 'eye', 'help', 'great', 'endeavor', 'firefighte', 'drone', 'quench', 'terrible', 'blaze', 'navigate', 'high', 'mountaintop', 'land', 'carefully', 'tree', 'stare', 'blackandgreen', 'divider', 'fire', 'burn', 'stop', 'amazing', 'thing', 'joy', 'desire', 'robot', 'serve', 'desire', 'joy', 'rather', 'desire', 'good', 'create', 'world', 'thing', 'compatible', 'thing', 'inspire', 'story', 'think', 'nature', 'achievement', 'feel', 'relationship', 'creativity', 'failure', 'achievement', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 0,http://eepurl.com/hSYp5z,2022-01-24,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.
  Google (finally) reveals its big text model - LaMDA:
…Plus: why you need a lot of humans to make a language model safe…
Google has finally given details on LaMDA, it's GPT-3 competitor. LaMDA are a family of language models ranging in size from 2B to 137B parameters (GPT3: 175B), and have been trained on a massive dataset of 1.56 trillion words. One way LaMDA is different to other big language models is that it is centered around dialogue, with 50% of its data coming from ""dialogues from public forums"". Google has also fine-tuned LaMDA models to make them better, safer, and more likely to give factual responses - all these fine-tuned versions are facilitate by Google using crowdworkers to gather specific dialog data.

The dataset: LaMDA's underlying dataset is called 'Infiniset', and besides the dialogue also involves common crawl, wikipedia, a mixture of english and non-english web documents, and data from programming-related sites (so LaMDA models can also dabble in code).

Learning to use an external dataset: Similar to DeepMind's 'RETRO' model, LaMDA has also been taught to use an external information retrieval system - this makes LaMDA models more factually correct than models without external datasets, because the model can check facts from an external, trusted source. To train the model, they also collect 9k dialogs where LaMDA's generated answers are labelled 'correct' or 'incorrect', to give it a stronger training signal.
  The results of this are quite striking. For example, when you ask a non-information-retrieval LaMDA model ""What do you think of Rosalie Gascoigne’s sculptures?"", it replies: ""LAMDA-BASE: They’re great, and I love how her work changed through her life. I like her later work more than her earlier ones. Her influence is also super interesting - did you know she was one of the artists that inspired Miró?"". This response seems plausible but it's wrong - Miro was active till late 1960s, and Gascoigne first solo exhibits were in 1970s, so she couldn't have been an influence on Miro.
  By comparison, the version of LaMDA trained on information retrieval says this: ""LAMDA: Oh wow, her life course is so inspiring. Did you know she was a practitioner of Japanese flower arrangement before turning to sculpture? https://artsearch.nga.gov.au/detail.cfm?irn=8774&pictaus=true"", where it gives a factually accurate statement and provides a source as well.

Things that make you go 'hmmm' - more compute than GPT-3: LaMDA consumed 3.55E+23 flops during training, versus 3.14+23 flops for GPT3 (so more parameters doesn't necessarily mean more resource intensive). It was trained on a cluster of 1024 TPU V3 chips.

Why this matters: ""LaMDA is a step closer to practical and safe open-ended dialog systems, which can in turn unlock a wide range of useful applications. We hope that this work encourages further research in this area"", Google writes. This is true - systems like LaMDA are basically refinements and improvements on the ideas of GPT2/3. We're a few years away from everyone having access to vast, planet-scale AI models that tell them truthful things in natural ways - the proverbial angel (or devil) on everyone's shoulder. The cultural impacts will be vast and destabilizing.
  Read more: LaMDA: Language Models for Dialogue Applications (arXiv).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Google (finally) reveals its big text model - LaMDA: …Plus: why you need a lot of humans to make a language model safe… Google has finally given details on LaMDA, it's GPT-3 competitor. LaMDA are a family of language models ranging in size from 2B to 137B parameters (GPT3: 175B), and have been trained on a massive dataset of 1.56 trillion words. One way LaMDA is different to other big language models is that it is centered around dialogue, with 50% of its data coming from ""dialogues from public forums"". Google has also fine-tuned LaMDA models to make them better, safer, and more likely to give factual responses - all these fine-tuned versions are facilitate by Google using crowdworkers to gather specific dialog data. The dataset: LaMDA's underlying dataset is called 'Infiniset', and besides the dialogue also involves common crawl, wikipedia, a mixture of english and non-english web documents, and data from programming-related sites (so LaMDA models can also dabble in code). Learning to use an external dataset: Similar to DeepMind's 'RETRO' model, LaMDA has also been taught to use an external information retrieval system - this makes LaMDA models more factually correct than models without external datasets, because the model can check facts from an external, trusted source. To train the model, they also collect 9k dialogs where LaMDA's generated answers are labelled 'correct' or 'incorrect', to give it a stronger training signal. The results of this are quite striking. For example, when you ask a non-information-retrieval LaMDA model ""What do you think of Rosalie Gascoigne’s sculptures?"", it replies: ""LAMDA-BASE: They’re great, and I love how her work changed through her life. I like her later work more than her earlier ones. Her influence is also super interesting - did you know she was one of the artists that inspired Miró?"". This response seems plausible but it's wrong - Miro was active till late 1960s, and Gascoigne first solo exhibits were in 1970s, so she couldn't have been an influence on Miro. By comparison, the version of LaMDA trained on information retrieval says this: ""LAMDA: Oh wow, her life course is so inspiring. Did you know she was a practitioner of Japanese flower arrangement before turning to sculpture? https://artsearch.nga.gov.au/detail.cfm?irn=8774&pictaus=true"", where it gives a factually accurate statement and provides a source as well. Things that make you go 'hmmm' - more compute than GPT-3: LaMDA consumed 3.55E+23 flops during training, versus 3.14+23 flops for GPT3 (so more parameters doesn't necessarily mean more resource intensive). It was trained on a cluster of 1024 TPU V3 chips. Why this matters: ""LaMDA is a step closer to practical and safe open-ended dialog systems, which can in turn unlock a wide range of useful applications. We hope that this work encourages further research in this area"", Google writes. This is true - systems like LaMDA are basically refinements and improvements on the ideas of GPT2/3. We're a few years away from everyone having access to vast, planet-scale AI models that tell them truthful things in natural ways - the proverbial angel (or devil) on everyone's shoulder. The cultural impacts will be vast and destabilizing. Read more: LaMDA: Language Models for Dialogue Applications (arXiv).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'finally', 'reveal', 'big', 'text', 'model', 'lamda', 'need', 'lot', 'human', 'make', 'language', 'model', 'safe', 'finally', 'give', 'detail', 'gpt3', 'competitor', 'lamda', 'family', 'language', 'model', 'range', 'size', '137b', 'parameter', 'gpt3', 'train', 'massive', 'dataset', 'word', 'way', 'lamda', 'different', 'big', 'language', 'model', 'center', 'dialogue', 'datum', 'come', 'dialogue', 'public', 'forum', 'also', 'finetune', 'lamda', 'model', 'make', 'well', 'safe', 'likely', 'give', 'factual', 'response', 'finetune', 'version', 'facilitate', 'use', 'crowdworker', 'gather', 'specific', 'dialog', 'datum', 'dataset', 'lamda', 'underlie', 'dataset', 'call', 'infiniset', 'dialogue', 'also', 'involve', 'common', 'crawl', 'wikipedia', 'mixture', 'nonenglish', 'web', 'document', 'datum', 'programmingrelate', 'site', 'lamda', 'model', 'also', 'dabble', 'code', 'learning', 'use', 'external', 'dataset', 'similar', 'deepmind', 'retro', 'model', 'lamda', 'also', 'teach', 'use', 'external', 'information', 'retrieval', 'system', 'make', 'lamda', 'model', 'factually', 'correct', 'model', 'external', 'dataset', 'model', 'check', 'fact', 'external', 'trust', 'source', 'train', 'model', 'also', 'collect', 'dialog', 'lamda', 'generate', 'answer', 'label', 'correct', 'incorrect', 'give', 'strong', 'training', 'signal', 'result', 'quite', 'striking', 'example', 'ask', 'noninformationretrieval', 'lamda', 'model', 'think', 'rosalie', 'sculpture', 'reply', '’re', 'great', 'love', 'work', 'change', 'life', 'like', 'later', 'work', 'early', 'one', 'influence', 'also', 'super', 'interesting', 'know', 'artist', 'inspire', 'miró', 'response', 'seem', 'plausible', 'wrong', 'active', 'late', '1960', 'gascoigne', 'first', 'solo', 'exhibit', '1970', 'influence', 'comparison', 'version', 'lamda', 'train', 'information', 'say', 'lamda', 'life', 'course', 'inspiring', 'know', 'practitioner', 'japanese', 'flower', 'arrangement', 'turn', 'sculpture', 'give', 'factually', 'accurate', 'statement', 'provide', 'source', 'well', 'thing', 'make', 'go', 'compute', 'gpt3', 'consume', 'flop', 'training', 'flop', 'gpt3', 'parameter', 'necessarily', 'mean', 'resource', 'intensive', 'train', 'cluster', 'tpu', 'chip', 'matter', 'lamda', 'step', 'close', 'practical', 'safe', 'openende', 'dialog', 'system', 'turn', 'unlock', 'wide', 'range', 'useful', 'application', 'hope', 'work', 'encourage', 'research', 'area', 'write', 'true', 'system', 'lamda', 'basically', 'refinement', 'improvement', 'idea', 'year', 'away', 'access', 'vast', 'planetscale', 'ai', 'model', 'tell', 'truthful', 'thing', 'natural', 'way', 'proverbial', 'angel', 'devil', 'everyone', 'shoulder', 'cultural', 'impact', 'vast', 'destabilizing', 'read', 'lamda', 'language', 'model', 'dialogue', 'application']"
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 1,http://eepurl.com/hSYp5z,2022-01-24,"#################################################### Write about a world where AI goes well, and win (part of) $100k:
…Future of Life Institute's worldbuilding contest tries to imagine positive AGI rollouts…
The Future of Life Institute is launching a competition based around ""designing visions of a plausible, aspirational future that includes strong artificial intelligence."" The competition deadline is April 15th 2022. The idea here is that if we can figure out realistic ways in which powerful AI can go well, then that gives us a map to use to get civilization there. The first prize is $20,000, followed by two second prizes of $10,000 each, and smaller prizes.
    Find out more about the competition here (Worldbuild.ai, FLI site).","#################################################### Write about a world where AI goes well, and win (part of) $100k: …Future of Life Institute's worldbuilding contest tries to imagine positive AGI rollouts… The Future of Life Institute is launching a competition based around ""designing visions of a plausible, aspirational future that includes strong artificial intelligence."" The competition deadline is April 15th 2022. The idea here is that if we can figure out realistic ways in which powerful AI can go well, then that gives us a map to use to get civilization there. The first prize is $20,000, followed by two second prizes of $10,000 each, and smaller prizes. Find out more about the competition here (Worldbuild.ai, FLI site).","['write', 'world', 'ai', 'go', 'well', 'win', 'part', 'future', 'life', 'institute', 'worldbuilding', 'contest', 'try', 'imagine', 'positive', 'agi', 'rollout', 'future', 'life', 'institute', 'launch', 'competition', 'base', 'design', 'vision', 'plausible', 'aspirational', 'future', 'include', 'strong', 'artificial', 'intelligence', 'competition', 'deadline', '15th', 'idea', 'figure', 'realistic', 'way', 'powerful', 'go', 'well', 'give', 'map', 'use', 'get', 'civilization', 'first', 'prize', 'follow', 'second', 'prize', 'small', 'prize', 'find', 'competition', 'fli', 'site']"
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 2,http://eepurl.com/hSYp5z,2022-01-24,"#################################################### Want to teach your drone to see? Use this massive dataset:
…WebUAV-3M is probably the largest public UAV tracking dataset…
Researchers with the Chinese Academy of Sciences, the Shenzhen Research Institute of Big Data, and the Chinese University of Hong Kong Shenzhen, have built WebUAV-3M, a large dataset to help people teach drones to accurately label images and videos. WebUAV-3M consists of 4,485 videos, where each one has been labeled with dense bounding boxes that cover 216 distinct categories of object to be tracked (e.g, bears, wind turbines, bicycles, etc). The authors claim this is ""by far the largest public UAV tracking benchmark"".

Multimodal: Unusually, this is a multi-modal dataset; each labeled video is accompanied by a natural language sentence describing the video, as well as an audio description of it. ""We provide natural language specifications and audio descriptions to facilitate multi-modal deep UAV tracking,"" the authors write. ""The natural language specification can provide auxiliary information to achieve accurate tracking"".

Why this matters: In the same way CCTV cameras have instrumented the streets of cities around the world, drones are doing the same for cities and rural areas. And just like how increasingly good AI got trained on datasets gathered by CCTV cameras, we can expect the same for drones. The result? An ever-expanding suite of surveillance capabilities that we can expect will be integrated, for good and bad purposes, by a broad range of governments and private sector actors. Datasets like WebUAV-3M are the fuel for this.
  Read more: WebUAV-3M: A Benchmark Unveiling the Power of Million-Scale Deep UAV Tracking (arXiv).
  Get the code from here (eventually - wasn't online when I wrote this section this week).","#################################################### Want to teach your drone to see? Use this massive dataset: …WebUAV-3M is probably the largest public UAV tracking dataset… Researchers with the Chinese Academy of Sciences, the Shenzhen Research Institute of Big Data, and the Chinese University of Hong Kong Shenzhen, have built WebUAV-3M, a large dataset to help people teach drones to accurately label images and videos. WebUAV-3M consists of 4,485 videos, where each one has been labeled with dense bounding boxes that cover 216 distinct categories of object to be tracked (e.g, bears, wind turbines, bicycles, etc). The authors claim this is ""by far the largest public UAV tracking benchmark"". Multimodal: Unusually, this is a multi-modal dataset; each labeled video is accompanied by a natural language sentence describing the video, as well as an audio description of it. ""We provide natural language specifications and audio descriptions to facilitate multi-modal deep UAV tracking,"" the authors write. ""The natural language specification can provide auxiliary information to achieve accurate tracking"". Why this matters: In the same way CCTV cameras have instrumented the streets of cities around the world, drones are doing the same for cities and rural areas. And just like how increasingly good AI got trained on datasets gathered by CCTV cameras, we can expect the same for drones. The result? An ever-expanding suite of surveillance capabilities that we can expect will be integrated, for good and bad purposes, by a broad range of governments and private sector actors. Datasets like WebUAV-3M are the fuel for this. Read more: WebUAV-3M: A Benchmark Unveiling the Power of Million-Scale Deep UAV Tracking (arXiv). Get the code from here (eventually - wasn't online when I wrote this section this week).","['want', 'teach', 'drone', 'see', 'use', 'massive', 'dataset', 'webuav3', 'probably', 'large', 'public', 'track', 'researcher', 'big', 'datum', 'build', 'webuav3', 'large', 'dataset', 'help', 'people', 'teach', 'drone', 'accurately', 'label', 'image', 'video', 'webuav3', 'consist', 'video', 'one', 'label', 'dense', 'bounding', 'box', 'cover', 'distinct', 'category', 'object', 'track', 'eg', 'bear', 'wind', 'turbine', 'bicycle', 'author', 'claim', 'far', 'large', 'public', 'track', 'benchmark', 'multimodal', 'unusually', 'multimodal', 'dataset', 'label', 'video', 'accompany', 'natural', 'language', 'sentence', 'describe', 'video', 'well', 'audio', 'description', 'provide', 'natural', 'language', 'specification', 'audio', 'description', 'facilitate', 'multimodal', 'deep', 'uav', 'track', 'author', 'write', 'natural', 'language', 'specification', 'provide', 'auxiliary', 'information', 'achieve', 'accurate', 'tracking', 'matter', 'way', 'camera', 'instrument', 'street', 'city', 'world', 'drone', 'city', 'rural', 'area', 'increasingly', 'good', 'ai', 'train', 'dataset', 'gather', 'camera', 'expect', 'drone', 'result', 'everexpande', 'suite', 'surveillance', 'capability', 'expect', 'integrate', 'good', 'bad', 'purpose', 'broad', 'range', 'government', 'private', 'sector', 'actor', 'dataset', 'webuav3', 'fuel', 'read', 'webuav3', 'benchmark', 'unveil', 'power', 'millionscale', 'deep', 'get', 'code', 'eventually', 'online', 'write', 'section', 'week']"
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 3,http://eepurl.com/hSYp5z,2022-01-24,"#################################################### FFCV: Train ImageNet for 98 cents!
…What's this? Free software that makes all model training better? Interesting!...:
There's some new software that could help pretty much everyone train models more efficiently. The software is called FFCV, short for Fast Forward Computer Vision, and it is a ""drop-in data loading system that dramatically increases data throughput in model training"". It looks like a potentially big deal - FFCV can be much more efficient for training AI models, according to tests done by the authors, and may also work for other applications as well. ""FFCV can speed up a lot more beyond just neural network training---in fact, the more data-bottlenecked the application (e.g., linear regression, bulk inference, etc.), the faster FFCV will make it!,"" says the project's GitHub page.

Why this matters: Software like FFCV is part of the broader industrialization of AI - now we know how to train networks, various people are modularizing the training process and perfecting different elements of it. Stuff like FFCV is part of that trend.
  Find out more and get the code: FFCV GitHub repo.
   Get more details by reading the Performance Guide (FFCV site).
  Check out the main project website here (FFCV site).","#################################################### FFCV: Train ImageNet for 98 cents! …What's this? Free software that makes all model training better? Interesting!...: There's some new software that could help pretty much everyone train models more efficiently. The software is called FFCV, short for Fast Forward Computer Vision, and it is a ""drop-in data loading system that dramatically increases data throughput in model training"". It looks like a potentially big deal - FFCV can be much more efficient for training AI models, according to tests done by the authors, and may also work for other applications as well. ""FFCV can speed up a lot more beyond just neural network training---in fact, the more data-bottlenecked the application (e.g., linear regression, bulk inference, etc.), the faster FFCV will make it!,"" says the project's GitHub page. Why this matters: Software like FFCV is part of the broader industrialization of AI - now we know how to train networks, various people are modularizing the training process and perfecting different elements of it. Stuff like FFCV is part of that trend. Find out more and get the code: FFCV GitHub repo. Get more details by reading the Performance Guide (FFCV site). Check out the main project website here (FFCV site).","['train', 'imagenet', 'cent', 'free', 'software', 'make', 'model', 'training', 'well', 'interesting', 'new', 'software', 'help', 'pretty', 'much', 'train', 'model', 'efficiently', 'software', 'call', 'ffcv', 'short', 'fast', 'forward', 'computer', 'vision', 'dropin', 'data', 'loading', 'system', 'dramatically', 'increase', 'datum', 'throughput', 'model', 'training', 'look', 'potentially', 'big', 'deal', 'ffcv', 'much', 'efficient', 'training', 'ai', 'model', 'accord', 'test', 'author', 'also', 'work', 'application', 'well', 'ffcv', 'speed', 'lot', 'neural', 'network', 'trainingin', 'fact', 'databottlenecke', 'application', 'eg', 'linear', 'regression', 'bulk', 'inference', 'fast', 'ffcv', 'make', 'say', 'project', 'page', 'matter', 'software', 'part', 'broad', 'industrialization', 'know', 'train', 'network', 'various', 'people', 'modularize', 'training', 'process', 'perfect', 'different', 'element', 'stuff', 'part', 'trend', 'find', 'get', 'code', 'ffcv', 'get', 'detail', 'read', 'performance', 'guide', 'ffcv', 'site', 'check', 'main', 'project', 'website', 'site']"
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 4,http://eepurl.com/hSYp5z,2022-01-24,"#################################################### Microsoft makes MoEs easier to train:
…MoEs might be the best way to scale-up large models…
Microsoft has given a technical update on how it's trying to scale-up mixture-of-experts (MoE) networks. MoEs are one of the more promising routes for creating trillion-parameter-plus AI models, as MoEs are a lot more efficient to train than dense models like GPT3. In this paper, Microsoft talks about how it has made some tweaks so MoEs work well for auto-regressive natural language generation tasks, ""demonstrating training cost reduction of 5X to achieve same model quality for models like GPT-3"" and Microsoft's own 530B parameter 'Megatron-Turing NLG'.

MoEs might be cheaper and better: In tests, Microsoft shows that it can train 350M and 1.3B parameter MoE text models that have better (or the same) performance as GPT3 for a range of different tasks.Microsoft says this nets out to models with the ""same quality with 5X less training cost"".

Why this matters: MoEs could turn out to be the main way people break the trillion-parameter barrier (and there are rumors that China's 'Wu Dao' MoE at an alleged ~1.7 trillion parameters has already done this). Via efficient MoE training and inference software, ""a model with comparable accuracy as trillion-parameter dense model can be potentially trained at the cost of a 200B parameter (like GPT-3) sized dense model, translating to millions of dollars in training cost reduction and energy savings"", Microsoft says.
  Read more: DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale (arXiv).","#################################################### Microsoft makes MoEs easier to train: …MoEs might be the best way to scale-up large models… Microsoft has given a technical update on how it's trying to scale-up mixture-of-experts (MoE) networks. MoEs are one of the more promising routes for creating trillion-parameter-plus AI models, as MoEs are a lot more efficient to train than dense models like GPT3. In this paper, Microsoft talks about how it has made some tweaks so MoEs work well for auto-regressive natural language generation tasks, ""demonstrating training cost reduction of 5X to achieve same model quality for models like GPT-3"" and Microsoft's own 530B parameter 'Megatron-Turing NLG'. MoEs might be cheaper and better: In tests, Microsoft shows that it can train 350M and 1.3B parameter MoE text models that have better (or the same) performance as GPT3 for a range of different tasks.Microsoft says this nets out to models with the ""same quality with 5X less training cost"". Why this matters: MoEs could turn out to be the main way people break the trillion-parameter barrier (and there are rumors that China's 'Wu Dao' MoE at an alleged ~1.7 trillion parameters has already done this). Via efficient MoE training and inference software, ""a model with comparable accuracy as trillion-parameter dense model can be potentially trained at the cost of a 200B parameter (like GPT-3) sized dense model, translating to millions of dollars in training cost reduction and energy savings"", Microsoft says. Read more: DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale (arXiv).","['make', 'moe', 'easy', 'train', 'moe', 'good', 'way', 'scaleup', 'large', 'model', 'give', 'technical', 'update', 'try', 'scaleup', 'mixtureofexpert', 'moe', 'network', 'moe', 'promising', 'route', 'create', 'trillionparameterplus', 'ai', 'model', 'moe', 'lot', 'efficient', 'train', 'dense', 'model', 'gpt3', 'paper', 'microsoft', 'talk', 'make', 'tweak', 'moe', 'work', 'well', 'autoregressive', 'natural', 'language', 'generation', 'task', 'demonstrate', 'training', 'cost', 'reduction', 'achieve', 'model', 'quality', 'model', 'gpt3', 'microsoft', 'parameter', 'megatronture', 'moe', 'cheap', 'well', 'test', 'show', 'train', '13b', 'parameter', 'moe', 'text', 'model', 'well', 'performance', 'gpt3', 'range', 'different', 'say', 'net', 'model', 'quality', 'less', 'training', 'cost', 'matter', 'moe', 'turn', 'main', 'way', 'people', 'break', 'trillionparameter', 'barrier', 'rumor', 'alleged', 'parameter', 'already', 'efficient', 'moe', 'training', 'inference', 'software', 'model', 'comparable', 'accuracy', 'trillionparameter', 'dense', 'model', 'potentially', 'train', 'cost', 'parameter', 'gpt3', 'size', 'dense', 'model', 'translate', 'million', 'dollar', 'training', 'cost', 'reduction', 'energy', 'saving', 'say', 'read', 'deepspeedmoe', 'advance', 'mixtureofexpert', 'inference', 'training', 'power', 'nextgeneration']"
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 5,http://eepurl.com/hSYp5z,2022-01-24,"#################################################### Backchain science out of fictional news - and win a hundred bucks:
What could cause a computer virus to infect a biological organism? Or how might a biological organism evolve into a computer virus? These are the two questions posed by a 'Fiction Science Competition'. Entrants will need to write a plausible scientific explanation for how either of the above scenarios could transpire, and will respond to a short (fictionalized) news article written about the scenarios. There's a prize of $100 dollars for winning entries, and submissions close February 28th 2022.
    Find out more here at the official Fiction Science Contest website.","#################################################### Backchain science out of fictional news - and win a hundred bucks: What could cause a computer virus to infect a biological organism? Or how might a biological organism evolve into a computer virus? These are the two questions posed by a 'Fiction Science Competition'. Entrants will need to write a plausible scientific explanation for how either of the above scenarios could transpire, and will respond to a short (fictionalized) news article written about the scenarios. There's a prize of $100 dollars for winning entries, and submissions close February 28th 2022. Find out more here at the official Fiction Science Contest website.","['backchain', 'science', 'fictional', 'news', 'win', 'buck', 'cause', 'computer', 'virus', 'infect', 'biological', 'organism', 'biological', 'organism', 'evolve', 'computer', 'virus', 'question', 'pose', 'fiction', 'science', 'competition', 'entrant', 'need', 'write', 'plausible', 'scientific', 'explanation', 'scenario', 'transpire', 'respond', 'short', 'fictionalize', 'news', 'article', 'write', 'scenario', 'prize', 'dollar', 'win', 'entry', 'submission', 'close', '28th', 'find', 'official', 'fiction', 'science', 'contest', 'website']"
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 6,http://eepurl.com/hSYp5z,2022-01-24,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute Visual surveillance’s share in computer vision research across the world shows some worrying trends … Research coming out of China dominates the field, especially in emergent surveillance sub-areas like person re-identification, crowd counting, and facial spoofing detection … 
CSET researchers have identified trends in computer vision research by looking for patterns of publication for six distinct tasks, analyzing 100 million English publications that were published between 2015-2019. Surveillance tasks examined: A SciREX model trained on data from Papers with Code was used to identify references to the following six tasks: face recognition, person re-identification, action recognition, emotion, recognition, crowd counting, and facial spoofing detection. Some key findings: Facial recognition was the most well-established task over this period, and crowd counting and face spoofing detection were rapidly growing areas. The overall percentage share of surveillance papers has remained stable around 5.5% over this period, though the raw volume of papers has grown given the surge in computer vision research overall. During this time period, China’s share of global CV papers grew from 33 to 37% and surveillance papers from 36% to 42%, exceeding research from the EU (2nd) and the US (3rd) by more than 20% in each category. Why it matters: While dual-use technologies developed in one part of the world can be used elsewhere, such analyses reveal a nation’s primary interest and provide quantitative evidence for decision-making in policy. The identified areas are important since tasks like action recognition can detect individuals with abnormal behavior in crowds, emotion recognition can help identify security threats in public areas, crowd counting can help to monitor civilian protests, and face spoofing detection can prevent journalists and activists from hiding their identity. All of these have significant implications in terms of fundamental rights and freedoms of people.
Read more: Trends in AI Research for the Visual Surveillance of Populations","#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute Visual surveillance’s share in computer vision research across the world shows some worrying trends … Research coming out of China dominates the field, especially in emergent surveillance sub-areas like person re-identification, crowd counting, and facial spoofing detection … CSET researchers have identified trends in computer vision research by looking for patterns of publication for six distinct tasks, analyzing 100 million English publications that were published between 2015-2019. Surveillance tasks examined: A SciREX model trained on data from Papers with Code was used to identify references to the following six tasks: face recognition, person re-identification, action recognition, emotion, recognition, crowd counting, and facial spoofing detection. Some key findings: Facial recognition was the most well-established task over this period, and crowd counting and face spoofing detection were rapidly growing areas. The overall percentage share of surveillance papers has remained stable around 5.5% over this period, though the raw volume of papers has grown given the surge in computer vision research overall. During this time period, China’s share of global CV papers grew from 33 to 37% and surveillance papers from 36% to 42%, exceeding research from the EU (2nd) and the US (3rd) by more than 20% in each category. Why it matters: While dual-use technologies developed in one part of the world can be used elsewhere, such analyses reveal a nation’s primary interest and provide quantitative evidence for decision-making in policy. The identified areas are important since tasks like action recognition can detect individuals with abnormal behavior in crowds, emotion recognition can help identify security threats in public areas, crowd counting can help to monitor civilian protests, and face spoofing detection can prevent journalists and activists from hiding their identity. All of these have significant implications in terms of fundamental rights and freedoms of people. Read more: Trends in AI Research for the Visual Surveillance of Populations","['ai', 'ethic', 'brief', 'montreal', 'visual', 'surveillance', 'share', 'computer', 'vision', 'research', 'world', 'show', 'worrying', 'trend', 'research', 'come', 'dominate', 'field', 'especially', 'emergent', 'surveillance', 'subarea', 'person', 'reidentification', 'crowd', 'counting', 'facial', 'spoof', 'detection', 'cset', 'researcher', 'identify', 'trend', 'computer', 'vision', 'research', 'look', 'pattern', 'publication', 'distinct', 'task', 'analyze', 'english', 'publication', 'publish', 'surveillance', 'task', 'examine', 'scirex', 'model', 'train', 'datum', 'paper', 'code', 'use', 'identify', 'reference', 'follow', 'task', 'face', 'recognition', 'person', 'reidentification', 'action', 'recognition', 'emotion', 'recognition', 'crowd', 'counting', 'facial', 'spoof', 'detection', 'key', 'finding', 'facial', 'recognition', 'wellestablished', 'task', 'period', 'crowd', 'counting', 'face', 'spoof', 'detection', 'rapidly', 'grow', 'area', 'overall', 'percentage', 'share', 'surveillance', 'paper', 'remain', 'stable', 'period', 'raw', 'volume', 'paper', 'grow', 'give', 'surge', 'computer', 'vision', 'research', 'overall', 'time', 'share', 'global', 'paper', 'grow', 'surveillance', 'paper', 'exceed', 'research', '2nd', '3rd', 'category', 'matter', 'dualuse', 'technology', 'develop', 'part', 'world', 'use', 'elsewhere', 'analysis', 'reveal', 'nation', 'primary', 'interest', 'provide', 'quantitative', 'evidence', 'decisionmake', 'policy', 'identify', 'area', 'important', 'task', 'action', 'recognition', 'detect', 'individual', 'abnormal', 'behavior', 'crowd', 'emotion', 'recognition', 'identify', 'security', 'threat', 'public', 'area', 'crowd', 'counting', 'help', 'monitor', 'civilian', 'protest', 'face', 'spoof', 'detection', 'prevent', 'journalist', 'activist', 'hide', 'significant', 'implication', 'term', 'fundamental', 'right', 'freedom', 'people', 'read', 'trend', 'research', 'visual', 'surveillance', 'population']"
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 7,http://eepurl.com/hSYp5z,2022-01-24,"#################################################### Tech Tales: VHS vs Betamax
[An online forum, 2035]

""Alright I need you to livestream from your phone what's happening on the computer, and I'm gonna send you an image to use as a prior, then I'm gonna watch it generate the first few epochs. If everything checks out I'll authorize the transfer to the escrow service and you'll do the same?""
""Yes,"" wrote the anonymous person.
I sent them a seed picture - something I'd drawn a couple of years ago that had never been digitized.
They turned on their livestream and I watched as the ML pipeline booted up and started the generation process. It seemed legit. Some of these older models had a very particular style that you could ID during early generation. I watched for a few minutes and was satisfied. This was the final authentication step and the only way I'd know for certain is if I just took a leap of faith and paid up.
""Okay, I'm sending the funds to the escrow service. They'll be distributed to your account once the service confirms receipt of the model.""
""Excellent. Good doing business with you.""
And then their little green dot went out and they were gone. A few minutes passed, and then the escrow service pinged me confirming they'd received the model. I downloaded it, then stuck it in my pipeline and started generating the client orders. People paid a lot of money for these kinds of 'vintage' AI-generated objects, and the model I'd just got was very old and very notorious. Just another beautiful day in America, sifting through all the debris of decades of software, panning for little chunks of gold. Things that inspired this: How the flaws of a media system ultimately become desired or fetishized aesthetic attributes - and specifically, this amazing Brian Eno quote; how models like CLIP will one day be obscure; how models vary over their development lifespans, creating the possibility of specific aesthetics and tastes. 
Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: VHS vs Betamax [An online forum, 2035] ""Alright I need you to livestream from your phone what's happening on the computer, and I'm gonna send you an image to use as a prior, then I'm gonna watch it generate the first few epochs. If everything checks out I'll authorize the transfer to the escrow service and you'll do the same?"" ""Yes,"" wrote the anonymous person. I sent them a seed picture - something I'd drawn a couple of years ago that had never been digitized. They turned on their livestream and I watched as the ML pipeline booted up and started the generation process. It seemed legit. Some of these older models had a very particular style that you could ID during early generation. I watched for a few minutes and was satisfied. This was the final authentication step and the only way I'd know for certain is if I just took a leap of faith and paid up. ""Okay, I'm sending the funds to the escrow service. They'll be distributed to your account once the service confirms receipt of the model."" ""Excellent. Good doing business with you."" And then their little green dot went out and they were gone. A few minutes passed, and then the escrow service pinged me confirming they'd received the model. I downloaded it, then stuck it in my pipeline and started generating the client orders. People paid a lot of money for these kinds of 'vintage' AI-generated objects, and the model I'd just got was very old and very notorious. Just another beautiful day in America, sifting through all the debris of decades of software, panning for little chunks of gold. Things that inspired this: How the flaws of a media system ultimately become desired or fetishized aesthetic attributes - and specifically, this amazing Brian Eno quote; how models like CLIP will one day be obscure; how models vary over their development lifespans, creating the possibility of specific aesthetics and tastes. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'vhs', 'online', 'forum', 'need', 'livestream', 'phone', 'happen', 'computer', 'going', 'send', 'image', 'use', 'prior', 'going', 'watch', 'generate', 'first', 'epoch', 'check', 'ill', 'authorize', 'transfer', 'escrow', 'service', 'write', 'anonymous', 'person', 'send', 'seed', 'picture', 'draw', 'couple', 'year', 'ago', 'never', 'digitize', 'turn', 'livestream', 'watch', 'pipeline', 'boot', 'start', 'generation', 'process', 'seem', 'legit', 'old', 'model', 'particular', 'style', 'early', 'generation', 'watch', 'minute', 'satisfied', 'final', 'authentication', 'step', 'way', 'know', 'certain', 'take', 'leap', 'faith', 'pay', 'send', 'fund', 'escrow', 'service', 'distribute', 'account', 'service', 'confirm', 'receipt', 'model', 'excellent', 'good', 'business', 'little', 'green', 'dot', 'go', 'go', 'minute', 'pass', 'escrow', 'service', 'ping', 'confirm', 'receive', 'model', 'download', 'stick', 'pipeline', 'start', 'generate', 'client', 'order', 'people', 'pay', 'lot', 'money', 'kind', 'vintage', 'aigenerated', 'object', 'model', 'get', 'old', 'notorious', 'beautiful', 'day', 'sift', 'debris', 'decade', 'software', 'pan', 'little', 'chunk', 'gold', 'thing', 'inspire', 'flaw', 'medium', 'system', 'ultimately', 'desire', 'fetishize', 'aesthetic', 'attribute', 'specifically', 'amazing', 'brian', 'model', 'clip', 'day', 'obscure', 'model', 'vary', 'development', 'lifespan', 'create', 'possibility', 'specific', 'aesthetic', 'taste', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
01/17/2022 - Import AI 280: Why bigger is worse for RL; AI-generated Pokemon; real-world EfficientNet - 0,http://eepurl.com/hSmoov,2022-01-17,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.
  Use an AI to generate a Pokemon in two (2!) clicks:
Here's a fun Colab notebook from Max Woolf (@minimaxir) that lets you use AI to dream up some Pokemon in a couple of clicks (and with a few minutes of waiting). This isn't remarkable - in recent years, AI generation stuff has got pretty good. What is remarkable is the usability. Two clicks! A few years ago you'd need to do all kinds of bullshit to get this to work - download some models on GitHub, get it to run in your local environment, make sure your versions of TF or PyTorch are compatible, etc. Now you just click some buttons and a load of stuff happens in the browser then, kabam, hallucinated pokemon. Things that make you go 'hmmm': This tech is based on ruDALL-E, an open source Russian version of OpenAI's 'DALL-E' network.
  I think we've all rapidly got used to this. This is not normal! It is surprising and exciting!
  Check out theColab notebook here (Google Colab).
  Follow Max on Twitter here and thank him for making this cool tool!","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Use an AI to generate a Pokemon in two (2!) clicks: Here's a fun Colab notebook from Max Woolf (@minimaxir) that lets you use AI to dream up some Pokemon in a couple of clicks (and with a few minutes of waiting). This isn't remarkable - in recent years, AI generation stuff has got pretty good. What is remarkable is the usability. Two clicks! A few years ago you'd need to do all kinds of bullshit to get this to work - download some models on GitHub, get it to run in your local environment, make sure your versions of TF or PyTorch are compatible, etc. Now you just click some buttons and a load of stuff happens in the browser then, kabam, hallucinated pokemon. Things that make you go 'hmmm': This tech is based on ruDALL-E, an open source Russian version of OpenAI's 'DALL-E' network. I think we've all rapidly got used to this. This is not normal! It is surprising and exciting! Check out theColab notebook here (Google Colab). Follow Max on Twitter here and thank him for making this cool tool!","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'use', 'ai', 'generate', 'pokemon', 'click', 'fun', 'colab', 'notebook', 'let', 'use', 'dream', 'pokemon', 'couple', 'click', 'minute', 'wait', 'remarkable', 'recent', 'year', 'generation', 'stuff', 'get', 'pretty', 'good', 'remarkable', 'usability', 'click', 'year', 'ago', 'need', 'kind', 'bullshit', 'get', 'work', 'download', 'model', 'get', 'run', 'local', 'environment', 'make', 'sure', 'version', 'pytorch', 'compatible', 'click', 'button', 'load', 'stuff', 'happen', 'browser', 'kabam', 'hallucinate', 'pokemon', 'thing', 'make', 'go', 'tech', 'base', 'open', 'source', 'russian', 'version', 'network', 'think', 'rapidly', 'use', 'normal', 'surprising', 'exciting', 'check', 'notebook', 'follow', 'max', 'twitter', 'thank', 'make', 'cool', 'tool']"
01/17/2022 - Import AI 280: Why bigger is worse for RL; AI-generated Pokemon; real-world EfficientNet - 1,http://eepurl.com/hSmoov,2022-01-17,"#################################################### Uh-oh: The bigger your RL model, the more likely it is to seek proxy rather than real rewards:
…Think RL gets better as you scale-up models? Hahahah! NOT AT ALL!...
In the past couple of years, big models have become really useful for things ranging from text processing to computer vision to, more recently, reinforcement learning. But these models have a common problem - as you scale up the size of the model, it's good capabilities get better, but so do its bad ones.
  For example, if you increase the size of a language model, it'll generate more toxic text (rather than less), without interventions (see: ​A General Language Assistant as a Laboratory for Alignment​). New research from Caltech and UC Berkeley shows how this same phenomena shows up in reinforcement learning agents, as well. In tests across a few distinct RL domains, they find that ""As model size increases, the proxy reward increases but the true reward decreases. This suggests that reward designers will likely need to take greater care to specify reward functions accurately and is especially salient given the recent trends towards larger and larger models"" What they did: They tested out a few different reinforcement  learning agents on four different environments - an Atari game called Riverraid, a glucose monitoring system, a traffic control simulation, and a COVID model where the RL dials up and down social distancing measures. In all cases they found that "" model’s optimization power often hurts performance on the true reward"", What can we do? Most of this behavior relates to objective design - give an AI the wrong objective function, and it'll optimize its way to success there, while ignoring side effects (e.g, if you reward an AI for reducing rate of defects on a factory production line to zero, it might just work out how to stop the factory line and therefore eliminate all defects - along with your business). One way to do this is to have a baseline policy that humans have verified as having the right goal, then building some software to spot deltas between the RL policy and the idealized baseline policy.
  This kind of works - in tests, the detectors can get anywhere between 45% and 81% accuracy at detecting anomalous from non-anomalous behaviors. But it certainly doesn't work well enough to make it easy to deploy this stuff confidently. ""Our results show that trend extrapolation alone is not enough to ensure the safety of ML systems,"" they write. ""To complement trend extrapolation, we need better interpretability methods to identify emergent model behaviors early on, before they dominate performance"".
  Read more: ​​THE EFFECTS OF REWARD MISSPECIFICATION: MAPPING AND MITIGATING MISALIGNED MODELS (arXiv).","#################################################### Uh-oh: The bigger your RL model, the more likely it is to seek proxy rather than real rewards: …Think RL gets better as you scale-up models? Hahahah! NOT AT ALL!... In the past couple of years, big models have become really useful for things ranging from text processing to computer vision to, more recently, reinforcement learning. But these models have a common problem - as you scale up the size of the model, it's good capabilities get better, but so do its bad ones. For example, if you increase the size of a language model, it'll generate more toxic text (rather than less), without interventions (see: ​A General Language Assistant as a Laboratory for Alignment​). New research from Caltech and UC Berkeley shows how this same phenomena shows up in reinforcement learning agents, as well. In tests across a few distinct RL domains, they find that ""As model size increases, the proxy reward increases but the true reward decreases. This suggests that reward designers will likely need to take greater care to specify reward functions accurately and is especially salient given the recent trends towards larger and larger models"" What they did: They tested out a few different reinforcement learning agents on four different environments - an Atari game called Riverraid, a glucose monitoring system, a traffic control simulation, and a COVID model where the RL dials up and down social distancing measures. In all cases they found that "" model’s optimization power often hurts performance on the true reward"", What can we do? Most of this behavior relates to objective design - give an AI the wrong objective function, and it'll optimize its way to success there, while ignoring side effects (e.g, if you reward an AI for reducing rate of defects on a factory production line to zero, it might just work out how to stop the factory line and therefore eliminate all defects - along with your business). One way to do this is to have a baseline policy that humans have verified as having the right goal, then building some software to spot deltas between the RL policy and the idealized baseline policy. This kind of works - in tests, the detectors can get anywhere between 45% and 81% accuracy at detecting anomalous from non-anomalous behaviors. But it certainly doesn't work well enough to make it easy to deploy this stuff confidently. ""Our results show that trend extrapolation alone is not enough to ensure the safety of ML systems,"" they write. ""To complement trend extrapolation, we need better interpretability methods to identify emergent model behaviors early on, before they dominate performance"". Read more: ​​THE EFFECTS OF REWARD MISSPECIFICATION: MAPPING AND MITIGATING MISALIGNED MODELS (arXiv).","['uhoh', 'big', 'model', 'likely', 'seek', 'proxy', 'rather', 'real', 'reward', 'think', 'rl', 'get', 'well', 'scaleup', 'model', 'hahahah', 'past', 'couple', 'year', 'big', 'model', 'become', 'really', 'useful', 'thing', 'range', 'text', 'processing', 'computer', 'vision', 'recently', 'reinforcement', 'learning', 'model', 'common', 'problem', 'scale', 'size', 'model', 'good', 'capability', 'get', 'well', 'bad', 'one', 'example', 'increase', 'size', 'language', 'model', 'generate', 'toxic', 'text', 'rather', 'less', 'intervention', 'see', 'general', 'language', 'assistant', 'laboratory', 'new', 'research', 'show', 'phenomena', 'show', 'reinforcement', 'learn', 'agent', 'well', 'test', 'distinct', 'rl', 'domain', 'find', 'model', 'size', 'increase', 'proxy', 'reward', 'increase', 'true', 'reward', 'decrease', 'suggest', 'reward', 'designer', 'likely', 'need', 'take', 'great', 'care', 'specify', 'reward', 'function', 'accurately', 'especially', 'salient', 'give', 'recent', 'trend', 'large', 'large', 'model', 'test', 'different', 'reinforcement', 'learn', 'agent', 'different', 'environment', 'atari', 'game', 'call', 'riverraid', 'glucose', 'monitoring', 'system', 'traffic', 'control', 'simulation', 'covid', 'model', 'dial', 'social', 'distancing', 'measure', 'case', 'find', 'model', 'optimization', 'power', 'often', 'hurt', 'performance', 'true', 'reward', 'behavior', 'relate', 'objective', 'design', 'give', 'ai', 'wrong', 'objective', 'function', 'optimize', 'way', 'success', 'ignore', 'side', 'effect', 'eg', 'reward', 'ai', 'reduce', 'rate', 'defect', 'factory', 'production', 'line', 'work', 'stop', 'factory', 'line', 'therefore', 'eliminate', 'defect', 'business', 'way', 'baseline', 'policy', 'human', 'verify', 'right', 'goal', 'build', 'software', 'spot', 'delta', 'rl', 'policy', 'idealize', 'baseline', 'policy', 'kind', 'work', 'test', 'detector', 'get', 'anywhere', 'accuracy', 'detect', 'anomalous', 'nonanomalous', 'behavior', 'certainly', 'work', 'well', 'enough', 'make', 'easy', 'deploy', 'stuff', 'confidently', 'result', 'show', 'trend', 'extrapolation', 'alone', 'enough', 'ensure', 'safety', 'system', 'write', 'complement', 'trend', 'extrapolation', 'need', 'well', 'interpretability', 'method', 'identify', 'emergent', 'model', 'behavior', 'early', 'dominate', 'performance', 'read', 'effect', 'reward', 'misspecification', 'mapping', 'mitigate', 'misaligned', 'model']"
01/17/2022 - Import AI 280: Why bigger is worse for RL; AI-generated Pokemon; real-world EfficientNet - 2,http://eepurl.com/hSmoov,2022-01-17,"#################################################### SCROLLS: A new way to test how well AI systems can understand big chunks of text:
…Now that AIs can write short stories, can we get them to understand books?...
Researchers with Tel-Aviv University, Allen Institute for AI, IBM Research, and Meta AI, have built 'SCROLLS' a way to test how well AI systems can reason about long texts. SCROLLs incorporates tasks ranging from summarization, to question answering, and natural language inference, as well as multiple distinct domains including transcripts, TV shows, and scientific articles. ""Our experiments indicate that SCROLLS poses a formidable challenge for these models, leaving much room for the research community to improve upon,"" the authors write. How SCROLLs works: This benchmark has mostly been created via curation,consisting of 7 datasets that reward models that can contextualize across different sections of the datasets and process long-range dependencies. The datasets: SCROLLS incorporates GovReport (summarization of reports addressing various national policy issues), SummScreenFD (summarization of TV shows, like Game of Thrones), QMSum (summarization of meeting transcripts), Qasper (question answering over NLP papers), NarrativeQA (question answering about entire books from Project Gutenberg), QuALITY (multiple choice question answering about stories from Project Gutenberg), and Contract NLI (natural language inference dataset in the legal domain). How hard is SCROLLS? The authors test out two smart baselines (BART, and a Longformer Encoder-Decoder (LED)), and one dumb baseline (a basic pre-written heuristic).  Based on the results, this seems like a really challenging task - a LED baseline with a 16384-token input length gets okay results, though BART gets close to it despite being limited to 1,024 tokens. This suggests two things: a) BART is nicely optimized, and b) it's not entirely clear the tasks in scrolls truly test for long-context reasoning. ""Our experiments highlight the importance of measuring not only whether an architecture can efficiently process a long language sequence, but also whether it can effectively model longrange dependencies,"" they write. Why this matters: ""Contemporary, off-the-shelf models struggle with these tasks"", the researchers write. In recent years, many machine learning benchmarks have been saturated within months of being released; how valuable SCROLLS turns out to be will be a combination of its hardness and its longevity. If SCROLLS gets solved soon, that'd indicate that AI systems are getting much better at reasoning about long-range information - or it could mean the SCROLL tasks are bugged and the AI systems have found a hack to get a decent score. Pay attention to the SCROLLS leaderboard to watch progress here.
  Read more: SCROLLS: Standardized CompaRison Over Long Language Sequences (arXiv).
  Check out the leaderboard here.","#################################################### SCROLLS: A new way to test how well AI systems can understand big chunks of text: …Now that AIs can write short stories, can we get them to understand books?... Researchers with Tel-Aviv University, Allen Institute for AI, IBM Research, and Meta AI, have built 'SCROLLS' a way to test how well AI systems can reason about long texts. SCROLLs incorporates tasks ranging from summarization, to question answering, and natural language inference, as well as multiple distinct domains including transcripts, TV shows, and scientific articles. ""Our experiments indicate that SCROLLS poses a formidable challenge for these models, leaving much room for the research community to improve upon,"" the authors write. How SCROLLs works: This benchmark has mostly been created via curation,consisting of 7 datasets that reward models that can contextualize across different sections of the datasets and process long-range dependencies. The datasets: SCROLLS incorporates GovReport (summarization of reports addressing various national policy issues), SummScreenFD (summarization of TV shows, like Game of Thrones), QMSum (summarization of meeting transcripts), Qasper (question answering over NLP papers), NarrativeQA (question answering about entire books from Project Gutenberg), QuALITY (multiple choice question answering about stories from Project Gutenberg), and Contract NLI (natural language inference dataset in the legal domain). How hard is SCROLLS? The authors test out two smart baselines (BART, and a Longformer Encoder-Decoder (LED)), and one dumb baseline (a basic pre-written heuristic). Based on the results, this seems like a really challenging task - a LED baseline with a 16384-token input length gets okay results, though BART gets close to it despite being limited to 1,024 tokens. This suggests two things: a) BART is nicely optimized, and b) it's not entirely clear the tasks in scrolls truly test for long-context reasoning. ""Our experiments highlight the importance of measuring not only whether an architecture can efficiently process a long language sequence, but also whether it can effectively model longrange dependencies,"" they write. Why this matters: ""Contemporary, off-the-shelf models struggle with these tasks"", the researchers write. In recent years, many machine learning benchmarks have been saturated within months of being released; how valuable SCROLLS turns out to be will be a combination of its hardness and its longevity. If SCROLLS gets solved soon, that'd indicate that AI systems are getting much better at reasoning about long-range information - or it could mean the SCROLL tasks are bugged and the AI systems have found a hack to get a decent score. Pay attention to the SCROLLS leaderboard to watch progress here. Read more: SCROLLS: Standardized CompaRison Over Long Language Sequences (arXiv). Check out the leaderboard here.","['scroll', 'new', 'way', 'test', 'well', 'system', 'understand', 'big', 'chunk', 'text', 'write', 'short', 'story', 'get', 'understand', 'book', 'researcher', 'meta', 'build', 'scroll', 'way', 'test', 'well', 'ai', 'system', 'reason', 'long', 'text', 'scroll', 'incorporate', 'task', 'range', 'summarization', 'question', 'answer', 'natural', 'language', 'inference', 'well', 'multiple', 'distinct', 'domain', 'include', 'transcript', 'tv', 'show', 'scientific', 'article', 'experiment', 'indicate', 'scroll', 'pose', 'formidable', 'challenge', 'model', 'leave', 'much', 'room', 'research', 'community', 'improve', 'author', 'write', 'scroll', 'work', 'benchmark', 'mostly', 'create', 'curationconsiste', 'dataset', 'reward', 'model', 'contextualize', 'different', 'section', 'dataset', 'process', 'longrange', 'dependency', 'dataset', 'scroll', 'incorporate', 'govreport', 'summarization', 'report', 'address', 'various', 'national', 'policy', 'issue', 'summscreenfd', 'summarization', 'tv', 'show', 'game', 'throne', 'summarization', 'meeting', 'transcript', 'qasper', 'question', 'answer', 'nlp', 'paper', 'narrativeqa', 'question', 'answer', 'entire', 'book', 'quality', 'multiple', 'choice', 'question', 'answer', 'story', 'contract', 'natural', 'language', 'inference', 'dataset', 'legal', 'domain', 'hard', 'scroll', 'author', 'test', 'smart', 'baseline', 'longformer', 'encoderdecoder', 'lead', 'dumb', 'baseline', 'basic', 'prewritten', 'heuristic', 'base', 'result', 'seem', 'really', 'challenging', 'task', 'lead', 'baseline', '16384token', 'input', 'length', 'get', 'okay', 'result', 'get', 'close', 'limited', 'token', 'suggest', 'thing', 'bart', 'nicely', 'optimize', 'entirely', 'clear', 'task', 'scroll', 'truly', 'test', 'longcontext', 'reasoning', 'experiment', 'highlight', 'importance', 'measure', 'architecture', 'efficiently', 'process', 'long', 'language', 'sequence', 'also', 'effectively', 'model', 'longrange', 'dependency', 'write', 'matter', 'contemporary', 'model', 'struggle', 'task', 'researcher', 'write', 'recent', 'year', 'many', 'machine', 'learning', 'benchmark', 'saturate', 'month', 'release', 'valuable', 'scroll', 'turn', 'combination', 'hardness', 'longevity', 'scroll', 'solve', 'soon', 'indicate', 'system', 'get', 'much', 'well', 'reasoning', 'longrange', 'information', 'mean', 'scroll', 'task', 'bug', 'system', 'find', 'hack', 'get', 'decent', 'score', 'pay', 'attention', 'scroll', 'watch', 'progress', 'read', 'scroll', 'standardize', 'comparison', 'long', 'language', 'sequence', 'check', 'leaderboard']"
01/17/2022 - Import AI 280: Why bigger is worse for RL; AI-generated Pokemon; real-world EfficientNet - 3,http://eepurl.com/hSmoov,2022-01-17,"#################################################### EfficientNet: Surprisingly good for solar panel identification:
…UC Berkeley project shows how easy fine-tuning is…
Some UC BErkeley researchers have built a small, efficient model for detecting solar panels. Their system, HyperionSolarNet, is an EfficientNet-B7 model finetuned from ImageNet onto a collection of 1,983 satellite images of buildings, labeled with whether they have solar panels or not. The resulting model gets an aggregate precision of 0.96 (though with lower accuracy for labeling the presence of a solar panel, indicating a propensity for false positives) when evaluated on a held-out test set. Why this matters: Last week, we wrote about how you can build a classifier from scratch and beat a finetuning approach. This paper shows that finetuning can also work quite well for specific use-cases. It also, implicitly, highlights how fine-tuning has gone from something of an arcane science to something pretty reliable and well understood, forecasting a future where there are as many classifiers in the world as there are things to classify.
  Read more:HyperionSolarNet: Solar Panel Detection from Aerial Images (arXiv).","#################################################### EfficientNet: Surprisingly good for solar panel identification: …UC Berkeley project shows how easy fine-tuning is… Some UC BErkeley researchers have built a small, efficient model for detecting solar panels. Their system, HyperionSolarNet, is an EfficientNet-B7 model finetuned from ImageNet onto a collection of 1,983 satellite images of buildings, labeled with whether they have solar panels or not. The resulting model gets an aggregate precision of 0.96 (though with lower accuracy for labeling the presence of a solar panel, indicating a propensity for false positives) when evaluated on a held-out test set. Why this matters: Last week, we wrote about how you can build a classifier from scratch and beat a finetuning approach. This paper shows that finetuning can also work quite well for specific use-cases. It also, implicitly, highlights how fine-tuning has gone from something of an arcane science to something pretty reliable and well understood, forecasting a future where there are as many classifiers in the world as there are things to classify. Read more:HyperionSolarNet: Solar Panel Detection from Aerial Images (arXiv).","['efficientnet', 'surprisingly', 'good', 'solar', 'panel', 'identification', 'show', 'easy', 'finetune', 'researcher', 'build', 'small', 'efficient', 'model', 'detect', 'solar', 'panel', 'system', 'hyperionsolarnet', 'efficientnetb7', 'model', 'finetune', 'imagenet', 'collection', 'satellite', 'image', 'building', 'label', 'solar', 'panel', 'result', 'model', 'get', 'aggregate', 'precision', 'though', 'low', 'accuracy', 'label', 'presence', 'solar', 'panel', 'indicate', 'propensity', 'false', 'positive', 'evaluate', 'heldout', 'test', 'set', 'matter', 'last', 'week', 'write', 'build', 'classifier', 'scratch', 'beat', 'finetune', 'approach', 'paper', 'show', 'finetune', 'also', 'work', 'quite', 'well', 'specific', 'usecase', 'also', 'implicitly', 'highlight', 'finetune', 'go', 'arcane', 'science', 'pretty', 'reliable', 'well', 'understand', 'forecast', 'future', 'many', 'classifier', 'world', 'thing', 'read', 'morehyperionsolarnet', 'solar', 'panel', 'detection', 'aerial', 'image', 'arxiv']"
01/17/2022 - Import AI 280: Why bigger is worse for RL; AI-generated Pokemon; real-world EfficientNet - 4,http://eepurl.com/hSmoov,2022-01-17,"#################################################### Tech Tales: 
The Last Things
[A morgue in Detroit, 2035] ""When someone dies and gasps, are they just trying to get the last gasp of being alive?"" asked the robot. The morgue manager stared at the corpse, then at the robot. ""I don't know,"" he said. ""That's a good question"". ""And when they know they are going to die, how do they save their information?"" asked the robot. ""For example, I would send a zip of my stored data, as well as a copy of my cortical model, to a repository, if I knew I was about to be decommissioned or was in danger,"" asked the robot. ""Most people don't bother,"" said the morgue manager. ""My mother, for instance. When she was dying I asked her to write down some of her memories for me and my family, but she didn't want to."" ""Why?"" ""I think she was mostly concerned with experiencing her life, since she knew it was ending. She took trips while she was still mobile. Then, towards the end, she focused on eating her favorite foods and seeing her friends."" ""And did you learn anything about life from seeing her die,"" asked the robot? ""Not particularly,"" said the morgue manager. ""Besides that life seems to become more valuable, the less you know you have of it."" Things that inspired this story: A long conversation with someone who worked as a crisis therapist about the nature of death and belief; thinking about the differences between how real and synthetic intelligences may approach the concept of death. 
Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: The Last Things [A morgue in Detroit, 2035] ""When someone dies and gasps, are they just trying to get the last gasp of being alive?"" asked the robot. The morgue manager stared at the corpse, then at the robot. ""I don't know,"" he said. ""That's a good question"". ""And when they know they are going to die, how do they save their information?"" asked the robot. ""For example, I would send a zip of my stored data, as well as a copy of my cortical model, to a repository, if I knew I was about to be decommissioned or was in danger,"" asked the robot. ""Most people don't bother,"" said the morgue manager. ""My mother, for instance. When she was dying I asked her to write down some of her memories for me and my family, but she didn't want to."" ""Why?"" ""I think she was mostly concerned with experiencing her life, since she knew it was ending. She took trips while she was still mobile. Then, towards the end, she focused on eating her favorite foods and seeing her friends."" ""And did you learn anything about life from seeing her die,"" asked the robot? ""Not particularly,"" said the morgue manager. ""Besides that life seems to become more valuable, the less you know you have of it."" Things that inspired this story: A long conversation with someone who worked as a crisis therapist about the nature of death and belief; thinking about the differences between how real and synthetic intelligences may approach the concept of death. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'last', 'thing', 'morgue', 'die', 'gasp', 'try', 'get', 'last', 'gasp', 'alive', 'ask', 'robot', 'manager', 'stare', 'corpse', 'robot', 'know', 'say', 'good', 'question', 'know', 'go', 'die', 'save', 'information', 'ask', 'robot', 'example', 'send', 'zip', 'store', 'datum', 'well', 'copy', 'cortical', 'model', 'repository', 'know', 'decommission', 'danger', 'ask', 'robot', 'people', 'bother', 'say', 'manager', 'mother', 'instance', 'die', 'ask', 'write', 'memory', 'family', 'want', 'think', 'mostly', 'concern', 'experience', 'life', 'know', 'end', 'take', 'trip', 'still', 'mobile', 'end', 'focus', 'eat', 'favorite', 'food', 'see', 'friend', 'learn', 'life', 'see', 'die', 'ask', 'robot', 'particularly', 'say', 'manager', 'life', 'seem', 'become', 'valuable', 'less', 'know', 'thing', 'inspire', 'story', 'long', 'conversation', 'work', 'crisis', 'therapist', 'nature', 'death', 'belief', 'think', 'difference', 'real', 'synthetic', 'intelligence', 'approach', 'concept', 'death', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 0,http://eepurl.com/hRQPeD,2022-01-10,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 Happy New Year! I took the end of 2021 off to think, read, relax, and eat. I hope readers found some time to do the same. I expect I'm going to change some things up around Import AI this year - it's going to get weirder, more specific, and hopefully more valuable! I'm also going to finesse the short story collection I've been putting together, based on the tech tales in this newsletter. Good luck to all readers for their own 2022 plans - we'll go on this journey together!","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Happy New Year! I took the end of 2021 off to think, read, relax, and eat. I hope readers found some time to do the same. I expect I'm going to change some things up around Import AI this year - it's going to get weirder, more specific, and hopefully more valuable! I'm also going to finesse the short story collection I've been putting together, based on the tech tales in this newsletter. Good luck to all readers for their own 2022 plans - we'll go on this journey together!","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'subscribe', 'happy', 'new', 'year', 'take', 'end', 'think', 'read', 'relax', 'eat', 'hope', 'reader', 'find', 'time', 'expect', 'go', 'change', 'thing', 'import', 'year', 'go', 'get', 'weird', 'specific', 'hopefully', 'valuable', 'also', 'go', 'finesse', 'short', 'story', 'collection', 'put', 'together', 'base', 'tech', 'tale', 'newsletter', 'good', 'luck', 'reader', 'plan', 'well', 'go', 'journey', 'together']"
01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 1,http://eepurl.com/hRQPeD,2022-01-10,"############################# Here's how to build GPT-3 in the open:
…What's it like replicating GPT-3? It's extremely difficult!...
BigScience, an initiative to train a GPT-3-scale model on a public supercomputer, is currently trying to train a 104B model. Training models at this scale is something of an artisanal science, with lots of researchers working from hard-won rules of thumb in-tandem with things like scaling laws. Here's a nice 'lessons learned' writeup from BigScience on the challenges it has faced in training, 13B and 104B-scale models so far.
  Read more: Lessons learned (BigScience, GitHub).","############################# Here's how to build GPT-3 in the open: …What's it like replicating GPT-3? It's extremely difficult!... BigScience, an initiative to train a GPT-3-scale model on a public supercomputer, is currently trying to train a 104B model. Training models at this scale is something of an artisanal science, with lots of researchers working from hard-won rules of thumb in-tandem with things like scaling laws. Here's a nice 'lessons learned' writeup from BigScience on the challenges it has faced in training, 13B and 104B-scale models so far. Read more: Lessons learned (BigScience, GitHub).","['build', 'gpt3', 'open', 'replicate', 'gpt3', 'extremely', 'difficult', 'bigscience', 'initiative', 'train', 'model', 'public', 'supercomputer', 'currently', 'try', 'train', 'model', 'training', 'model', 'scale', 'artisanal', 'science', 'lot', 'researcher', 'work', 'hardwon', 'rule', 'thumb', 'intandem', 'thing', 'scale', 'law', 'nice', 'lesson', 'learn', 'writeup', 'bigscience', 'challenge', 'face', 'training', '13b', '104bscale', 'model', 'far', 'read', 'lesson', 'learn', 'bigscience', 'github']"
01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 2,http://eepurl.com/hRQPeD,2022-01-10,"#################################################### BAIDU's shows how to inject more knowledge into a language model:
…ERNIE 3.0 shows how to teach a big neural net to use an external knowledge base…
Baidu has developed ERNIE 3.0, an AI model that can use an external knowledge base to help it provide more accurate answers. Last year, an ERNIE 3.0 model won the highly competitive SuperGLUE challenge (Import AI 259). The special thing about ERNIE is that it fuses a big GPT-3-esque language model with a large external knowledge base.

Massive scale: Baidu has also developed ERNIE 3.0 'Titan', a 260 billion parameter model that, Baidu says, ""is the largest Chinese dense pre-training model as far as we know"". In tests, ERNIE 3.0 Titan gets state-of-the-art results on a vast set of benchmarks that evaluate skills as diverse as question answering, text generation, text summarization, interpretation, and dialogue.

Novel, heterogeneous chip cluster: Another interesting thing about this paper is the chips they train on - V100s and Huawei 'Ascend' processors. It's quite unusual to see hybrid training of this form, and it seems like Baidu felt it was interesting enough to invest some engineering resources in making it possible - the company augmented its 'PaddlePaddle' AI framework with "" distributed training technology, including fine-grained parallelism, heterogeneous hardware-aware training, and fault tolerance mechanism to train the 260B model on both Nvidia V100 GPU and Ascend 910 NPU clusters.""

Why this matters: Most people seem to act like GPT-3 models are exclusively being developed by a small set of Western actors, most of whom get tagged using the pejorative 'big tech' brush. But papers like this show that GPT-3 models are a global phenomenon. We should remember that the world we live in is going to be increasingly defined by different cultures expressing themselves through increasingly large, sophisticated AI models.
  Read more: ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation (arXiv).","#################################################### BAIDU's shows how to inject more knowledge into a language model: …ERNIE 3.0 shows how to teach a big neural net to use an external knowledge base… Baidu has developed ERNIE 3.0, an AI model that can use an external knowledge base to help it provide more accurate answers. Last year, an ERNIE 3.0 model won the highly competitive SuperGLUE challenge (Import AI 259). The special thing about ERNIE is that it fuses a big GPT-3-esque language model with a large external knowledge base. Massive scale: Baidu has also developed ERNIE 3.0 'Titan', a 260 billion parameter model that, Baidu says, ""is the largest Chinese dense pre-training model as far as we know"". In tests, ERNIE 3.0 Titan gets state-of-the-art results on a vast set of benchmarks that evaluate skills as diverse as question answering, text generation, text summarization, interpretation, and dialogue. Novel, heterogeneous chip cluster: Another interesting thing about this paper is the chips they train on - V100s and Huawei 'Ascend' processors. It's quite unusual to see hybrid training of this form, and it seems like Baidu felt it was interesting enough to invest some engineering resources in making it possible - the company augmented its 'PaddlePaddle' AI framework with "" distributed training technology, including fine-grained parallelism, heterogeneous hardware-aware training, and fault tolerance mechanism to train the 260B model on both Nvidia V100 GPU and Ascend 910 NPU clusters."" Why this matters: Most people seem to act like GPT-3 models are exclusively being developed by a small set of Western actors, most of whom get tagged using the pejorative 'big tech' brush. But papers like this show that GPT-3 models are a global phenomenon. We should remember that the world we live in is going to be increasingly defined by different cultures expressing themselves through increasingly large, sophisticated AI models. Read more: ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation (arXiv).","['baidus', 'show', 'inject', 'knowledge', 'language', 'model', 'ernie', 'show', 'teach', 'big', 'neural', 'net', 'use', 'external', 'knowledge', 'base', 'develop', 'ernie', 'ai', 'model', 'use', 'external', 'knowledge', 'base', 'help', 'provide', 'accurate', 'answer', 'last', 'year', 'ernie', 'model', 'win', 'highly', 'competitive', 'superglue', 'challenge', 'import', 'ai', 'special', 'thing', 'ernie', 'fuse', 'big', 'language', 'model', 'large', 'external', 'knowledge', 'base', 'massive', 'scale', 'baidu', 'also', 'develop', 'ernie', 'parameter', 'model', 'say', 'large', 'chinese', 'dense', 'pretraine', 'model', 'far', 'know', 'test', 'get', 'stateoftheart', 'result', 'vast', 'set', 'benchmark', 'evaluate', 'skill', 'diverse', 'question', 'answer', 'text', 'generation', 'text', 'summarization', 'interpretation', 'dialogue', 'novel', 'heterogeneous', 'chip', 'cluster', 'interesting', 'thing', 'paper', 'chip', 'train', 'ascend', 'processor', 'quite', 'unusual', 'see', 'hybrid', 'training', 'form', 'seem', 'feel', 'interesting', 'enough', 'invest', 'engineering', 'resource', 'make', 'possible', 'company', 'augment', 'paddlepaddle', 'ai', 'framework', 'distribute', 'training', 'technology', 'include', 'finegraine', 'parallelism', 'heterogeneous', 'hardwareaware', 'training', 'fault', 'tolerance', 'mechanism', 'train', '260b', 'model', 'gpu', 'ascend', 'npu', 'cluster', 'matter', 'people', 'seem', 'act', 'gpt3', 'model', 'exclusively', 'develop', 'small', 'set', 'western', 'actor', 'tag', 'use', 'pejorative', 'big', 'tech', 'brush', 'paper', 'show', 'gpt3', 'model', 'global', 'phenomenon', 'remember', 'world', 'live', 'go', 'increasingly', 'define', 'different', 'culture', 'express', 'increasingly', 'large', 'sophisticated', 'ai', 'model', 'read', 'explore', 'largerscale', 'knowledge', 'enhance', 'pretraine', 'language', 'understanding', 'generation', 'arxiv']"
01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 3,http://eepurl.com/hRQPeD,2022-01-10,"#################################################### Why smaller can be smarter for real-world AI (here: computer vision for quality control on solar panels):
…When 1 million parameters can beat 100 million parameters…
The past few years of AI has been distinguished by the 'bigger is better' phenomenon, as companies develop ever-larger models that consumer ever-larger amounts of compute and data. Now, a paper from researchers with Friedrich-Alexander University Erlangen-Nuremberg in Germany reminds us that bigger isn't always better - especially when it comes to real-world, applied AI. In this paper, they compare different approaches to building an image classifier that can spot defects in solar panels.

What they did: They trained a simple 8-layer convolutional neural net on a dataset of 4341 original, labeled images from a solar plant. The ~4000 images were each labeled with one of eight classes (e.g, 'good', 'crack', 'splinter', et cetera. They then applied a significant amount of data augmentation to enhance the size of the dataset.

How well did it do? Their custom, simple network outperformed a network based on VGG-architecture model pre-trained on the vast ImageNet dataset. This is interesting, because a common practice in AI research is to finetune domain-specific classifiers from generic ones based on ImageNet. Here, we find that their system gets better precision (0.971 versus 0.990), while having 100X fewer parameters (1,707,208 versus 138,357,544) and being significantly smaller in terms of memory footprint (~16MB versus 800MB). All this nets out to a network that is smarter, as well as more performant (inference of 0.50ms, versus 9.13ms).

Why this matters: Papers like this remind us that a little bit of thoughtful engineering goes a long way in AI, and we should bear in mind that while increasingly large networks are interesting, they're not the only game in town when it comes to building things that have real economic value. ""We expect that the following years will demand for more research on edge analytics. This means that more research will be needed on small, yet powerful artificial neural networks for industry cases"", they write.
  Read more: A Light in the Dark: Deep Learning Practices for Industrial Computer Vision (arXiv).","#################################################### Why smaller can be smarter for real-world AI (here: computer vision for quality control on solar panels): …When 1 million parameters can beat 100 million parameters… The past few years of AI has been distinguished by the 'bigger is better' phenomenon, as companies develop ever-larger models that consumer ever-larger amounts of compute and data. Now, a paper from researchers with Friedrich-Alexander University Erlangen-Nuremberg in Germany reminds us that bigger isn't always better - especially when it comes to real-world, applied AI. In this paper, they compare different approaches to building an image classifier that can spot defects in solar panels. What they did: They trained a simple 8-layer convolutional neural net on a dataset of 4341 original, labeled images from a solar plant. The ~4000 images were each labeled with one of eight classes (e.g, 'good', 'crack', 'splinter', et cetera. They then applied a significant amount of data augmentation to enhance the size of the dataset. How well did it do? Their custom, simple network outperformed a network based on VGG-architecture model pre-trained on the vast ImageNet dataset. This is interesting, because a common practice in AI research is to finetune domain-specific classifiers from generic ones based on ImageNet. Here, we find that their system gets better precision (0.971 versus 0.990), while having 100X fewer parameters (1,707,208 versus 138,357,544) and being significantly smaller in terms of memory footprint (~16MB versus 800MB). All this nets out to a network that is smarter, as well as more performant (inference of 0.50ms, versus 9.13ms). Why this matters: Papers like this remind us that a little bit of thoughtful engineering goes a long way in AI, and we should bear in mind that while increasingly large networks are interesting, they're not the only game in town when it comes to building things that have real economic value. ""We expect that the following years will demand for more research on edge analytics. This means that more research will be needed on small, yet powerful artificial neural networks for industry cases"", they write. Read more: A Light in the Dark: Deep Learning Practices for Industrial Computer Vision (arXiv).","['small', 'smart', 'realworld', 'ai', 'computer', 'vision', 'quality', 'control', 'solar', 'panel', 'parameter', 'beat', 'parameter', 'past', 'year', 'distinguish', 'big', 'well', 'phenomenon', 'company', 'develop', 'everlarger', 'model', 'consumer', 'everlarger', 'amount', 'compute', 'datum', 'paper', 'researcher', 'remind', 'big', 'always', 'well', 'especially', 'come', 'apply', 'paper', 'compare', 'different', 'approach', 'build', 'image', 'classifier', 'spot', 'defect', 'solar', 'panel', 'train', 'simple', 'convolutional', 'neural', 'net', 'dataset', 'original', 'label', 'image', 'solar', 'plant', 'image', 'label', 'class', 'eg', 'good', 'crack', 'splinter', 'et', 'cetera', 'apply', 'significant', 'amount', 'datum', 'augmentation', 'enhance', 'size', 'dataset', 'well', 'custom', 'simple', 'network', 'outperform', 'network', 'base', 'vggarchitecture', 'model', 'pretraine', 'vast', 'imagenet', 'dataset', 'interesting', 'common', 'practice', 'research', 'finetune', 'domainspecific', 'classifier', 'generic', 'one', 'base', 'imagenet', 'find', 'system', 'get', 'well', 'precision', '100x', 'parameter', 'significantly', 'small', 'term', 'memory', 'footprint', 'mb', 'mb', 'net', 'network', 'smart', 'well', 'performant', 'inference', '050ms', '913ms', 'matter', 'paper', 'remind', 'little', 'bit', 'thoughtful', 'engineering', 'go', 'long', 'way', 'bear', 'mind', 'increasingly', 'large', 'network', 'interesting', 'game', 'town', 'come', 'build', 'thing', 'real', 'economic', 'value', 'expect', 'follow', 'year', 'demand', 'research', 'edge', 'analytic', 'mean', 'research', 'need', 'small', 'yet', 'powerful', 'artificial', 'neural', 'network', 'industry', 'case', 'write', 'read', 'light', 'dark', 'deep', 'learning', 'practice', 'industrial', 'computer', 'vision']"
01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 4,http://eepurl.com/hRQPeD,2022-01-10,"#################################################### What's the US military going to do about AI? The NDAA holds a clue. 
…AI education! Procurement! Data storage! And more…
Every year, the somewhat dysfunctional US congress always manages to pass a bill - the National Defense Authorization Act. This bill (which weighs in at around $800bn in annual outlay) is the thing that funds the US military. Therefore, the NDAA has become one of the main pieces of legislation to look at when trying to understand how the US military thinks about - and will work on - frontier technologies like AI. An analysis of the 2021 NDAA from Stanford's 'HAI' center gives us a sense of what's happening in AI and the US military.

What the NDAA says is going to happen: Some highlights from this years NDAA include:
- The DoD is going to trial different ways of procuring AI technology
- The DoD will create 'executive education activities' to help senior officials understand AI.
- The DoD will do a comparative analyses of the US and China's efforts to deploy things relating to directed energy systems, hypersonics, cyberspace, and other frontier areas
- Creating an assessment of the ""current and emerging office and defensive cyber posture of U.S. adversaries""
- Build DoD infrastructure to ""support state-of-the-art tools and modern processes to enable the testing of AI capabilities"".
- ""Evaluate the feasibility and advisability of creating DOD data repositories, available to public and private entities, to facilitate the development of AI capabilities.""

Why this matters: The US military is a lot like a supertanker - it's slow, huge, and unwieldy. But once it starts to turn, boy does it turn! This NDAA analysis shows us the DoD is beginning to turn its attention and significant resources towards AI, which will have significant downstream implications for the nature of conflict and the way that future wars are conducted (and, eventually, learned).
  Read more: Summary of AI Provisions from the National Defense Authorization Act 2022 (Stanford HAI blog).","#################################################### What's the US military going to do about AI? The NDAA holds a clue. …AI education! Procurement! Data storage! And more… Every year, the somewhat dysfunctional US congress always manages to pass a bill - the National Defense Authorization Act. This bill (which weighs in at around $800bn in annual outlay) is the thing that funds the US military. Therefore, the NDAA has become one of the main pieces of legislation to look at when trying to understand how the US military thinks about - and will work on - frontier technologies like AI. An analysis of the 2021 NDAA from Stanford's 'HAI' center gives us a sense of what's happening in AI and the US military. What the NDAA says is going to happen: Some highlights from this years NDAA include: - The DoD is going to trial different ways of procuring AI technology - The DoD will create 'executive education activities' to help senior officials understand AI. - The DoD will do a comparative analyses of the US and China's efforts to deploy things relating to directed energy systems, hypersonics, cyberspace, and other frontier areas - Creating an assessment of the ""current and emerging office and defensive cyber posture of U.S. adversaries"" - Build DoD infrastructure to ""support state-of-the-art tools and modern processes to enable the testing of AI capabilities"". - ""Evaluate the feasibility and advisability of creating DOD data repositories, available to public and private entities, to facilitate the development of AI capabilities."" Why this matters: The US military is a lot like a supertanker - it's slow, huge, and unwieldy. But once it starts to turn, boy does it turn! This NDAA analysis shows us the DoD is beginning to turn its attention and significant resources towards AI, which will have significant downstream implications for the nature of conflict and the way that future wars are conducted (and, eventually, learned). Read more: Summary of AI Provisions from the National Defense Authorization Act 2022 (Stanford HAI blog).","['military', 'go', 'ai', 'ndaa', 'hold', 'clue', 'ai', 'education', 'procurement', 'datum', 'storage', 'year', 'somewhat', 'dysfunctional', 'always', 'manage', 'pass', 'bill', 'act', 'bill', 'weigh', 'annual', 'outlay', 'thing', 'fund', 'military', 'therefore', 'ndaa', 'become', 'main', 'piece', 'legislation', 'look', 'try', 'understand', 'military', 'think', 'work', 'frontier', 'technology', 'analysis', 'ndaa', 'give', 'sense', 'happen', 'military', 'ndaa', 'say', 'go', 'happen', 'highlight', 'year', 'ndaa', 'include', 'dod', 'go', 'trial', 'different', 'way', 'procure', 'technology', 'dod', 'create', 'executive', 'education', 'activity', 'help', 'senior', 'official', 'understand', 'dod', 'comparative', 'analysis', 'china', 'effort', 'deploy', 'thing', 'relate', 'direct', 'energy', 'system', 'hypersonic', 'cyberspace', 'frontier', 'area', 'create', 'assessment', 'current', 'emerge', 'office', 'defensive', 'cyber', 'posture', 'adversarie', 'build', 'dod', 'infrastructure', 'support', 'stateoftheart', 'tool', 'modern', 'process', 'enable', 'testing', 'capability', 'evaluate', 'feasibility', 'advisability', 'create', 'dod', 'data', 'repository', 'available', 'public', 'private', 'entity', 'facilitate', 'development', 'capability', 'matter', 'military', 'lot', 'supertank', 'slow', 'huge', 'unwieldy', 'start', 'turn', 'boy', 'turn', 'ndaa', 'analysis', 'show', 'dod', 'begin', 'turn', 'attention', 'significant', 'resource', 'significant', 'downstream', 'implication', 'nature', 'conflict', 'way', 'future', 'war', 'conduct', 'eventually', 'learn', 'read', 'summary', 'ai', 'provision']"
01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 5,http://eepurl.com/hRQPeD,2022-01-10,"#################################################### What is China going to do about AI governance?
…China might do more ambitious tech regulations than the West…
Here's a nice summary from the Carnegie Endowment for International Peace about what three prominent Chinese policy organizations are doing with regard to AI governance.

Cyberspace Administration of China (CAC): Last year, it released 30 rules for regulating internet recommendation algorithms, and also developed a three-year roadmap for governing other complex algorithms deployed at internet scale. This would be analogous to a Western government publishing a list of specific rules for regulating, for example, Facebook's recommendation engine. Ambitious!

China Academy of Information and Communications Technology (CAICT): This organization released a  whitepaper on trustworthy AI - this is mostly notable because it's in distribution with what other major regulators in other geographies are thinking about.

Ministry of Science and Technology (MOST): This organization released some guidelines for universities and companies on internal reviews around ethics issues relating to technology, as well as a fairly high-level description of some ethical norms for AI development.

Why this matters: ""The potential impact of these regulatory currents extends far beyond China. If the CAC follows through on certain requirements for algorithmic transparency and explainability, China will be running some of the world’s largest regulatory experiments on topics that European regulators have long debated,"" Matt Sheehan of Carnegie writes. Running regulatory experiments is a big deal - Western governments did a tiny bit of this after the great financial crisis in 08/09, but have done relatively little about technology governance. I think China has a good chance of defining what ambitious, applied tech regulation looks like.
  Read more: China’s New AI Governance Initiatives Shouldn’t Be Ignored (Carnegie Endowment for International Peace).","#################################################### What is China going to do about AI governance? …China might do more ambitious tech regulations than the West… Here's a nice summary from the Carnegie Endowment for International Peace about what three prominent Chinese policy organizations are doing with regard to AI governance. Cyberspace Administration of China (CAC): Last year, it released 30 rules for regulating internet recommendation algorithms, and also developed a three-year roadmap for governing other complex algorithms deployed at internet scale. This would be analogous to a Western government publishing a list of specific rules for regulating, for example, Facebook's recommendation engine. Ambitious! China Academy of Information and Communications Technology (CAICT): This organization released a whitepaper on trustworthy AI - this is mostly notable because it's in distribution with what other major regulators in other geographies are thinking about. Ministry of Science and Technology (MOST): This organization released some guidelines for universities and companies on internal reviews around ethics issues relating to technology, as well as a fairly high-level description of some ethical norms for AI development. Why this matters: ""The potential impact of these regulatory currents extends far beyond China. If the CAC follows through on certain requirements for algorithmic transparency and explainability, China will be running some of the world’s largest regulatory experiments on topics that European regulators have long debated,"" Matt Sheehan of Carnegie writes. Running regulatory experiments is a big deal - Western governments did a tiny bit of this after the great financial crisis in 08/09, but have done relatively little about technology governance. I think China has a good chance of defining what ambitious, applied tech regulation looks like. Read more: China’s New AI Governance Initiatives Shouldn’t Be Ignored (Carnegie Endowment for International Peace).","['go', 'ambitious', 'tech', 'regulation', 'west', 'nice', 'summary', 'carnegie', 'endowment', 'international', 'peace', 'prominent', 'chinese', 'policy', 'organization', 'regard', 'last', 'year', 'release', 'rule', 'regulate', 'internet', 'recommendation', 'algorithm', 'also', 'develop', 'threeyear', 'roadmap', 'govern', 'complex', 'algorithm', 'deploy', 'internet', 'scale', 'analogous', 'western', 'government', 'publish', 'list', 'specific', 'rule', 'regulate', 'example', 'facebook', 'recommendation', 'engine', 'ambitious', 'information', 'communication', 'technology', 'caict', 'organization', 'release', 'whitepaper', 'mostly', 'notable', 'distribution', 'major', 'regulator', 'geography', 'think', 'ministry', 'science', 'technology', 'organization', 'release', 'guideline', 'university', 'company', 'internal', 'review', 'ethic', 'issue', 'relate', 'technology', 'well', 'fairly', 'highlevel', 'description', 'ethical', 'norm', 'development', 'matter', 'potential', 'impact', 'regulatory', 'current', 'extend', 'far', 'cac', 'follow', 'certain', 'requirement', 'algorithmic', 'transparency', 'explainability', 'run', 'world', 'large', 'regulatory', 'experiment', 'topic', 'european', 'regulator', 'long', 'debate', 'matt', 'sheehan', 'carnegie', 'write', 'run', 'regulatory', 'experiment', 'big', 'deal', 'western', 'government', 'tiny', 'bit', 'great', 'financial', 'crisis', 'relatively', 'little', 'technology', 'governance', 'think', 'good', 'chance', 'define', 'ambitious', 'apply', 'tech', 'regulation', 'look', 'read', 'governance', 'initiative', 'ignore', 'carnegie', 'endowment', 'international', 'peace']"
01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 6,http://eepurl.com/hRQPeD,2022-01-10,"#################################################### The Last Tower Defense Fighter
[Historical analysis written in 2080 and stored in the archives at Iceland, at the Orbital Archive, and in the hardened repositories on Moon4 and Mars1.] Back in the late 2020s there were a bunch of tower defense games that got pretty big. They always worked in the same way: you, the player, can see a landscape from overhead, and you need to place various weapons around it. Meanwhile, the enemies make there way across the landscape, following loosely described paths across a variety of different scenes - narrow trenches dug between mountains, wide roads across countryside, right-angled streets in urban centers.  With these games, you get a high score in relation to how many enemies you kill, and if any of the enemies get to the 'end' of a course (usually, the bottom of a screen), you lose - the implication is that you die.  Anyway, in around 2028 one of the big games built some add-ins for its league. Now, if you were one of the players in the elite-tier of the game, you'd get the opportunity to play in matches where there were cash prizes - these matches were advertised as being extraordinarily difficult, with more enemies on screen than in the normal game, larger and more complex maps, and sometimes the enemies were able to use powerups that meant they could attack your own towers and take them down.  It was a sensation. Everyone wanted to play the game within a game. Kids all around the world streamed themselves playing the game for hours, as they tried to get good enough to have a shot at entering the league within the league.  By the end of 2028, streams of league players were pulling in millions of concurrent viewers. A whole industry formed where people commentated about the games. Sometimes people overcame great odds and won - then they'd publish videos of themselves with their cash prizes and what they spent them on; sport cars, fine dining, nice hotels, and all the usual tchotchkes of people who come into some fast money.  In 2029, there was a leak out of the Department of Defense that pulled the cover off. It turned out this game was actually a stealth DoD project. The normal games were helping the DoD train various strategic AI systems, which it used in planning for logistics and munitions placement during conflict. No one was very surprised by this. Back in that decade, most things that got big on the internet were fronts.  What did surprise people was the leak about the cash league - the cash league was real. Real in the sense that the 'monsters' in the game were real - they were real people that the United States happened to be fighting. Whenever someone was playing the game, their commands were being converted to a different, real-world environment, where they marshalled the combined munitions of drones, sniper teams, artillery, tanks, jets, and all the other machinery of the military. And when their towers were destroyed, real Americans were dying - blown up by grenades or IEDs or RPGs, or any of the other ways people killed eachother, back then.

Of course, there was an outcry - for a while. Player numbers dipped for a while. But the number of spectators increased. And the US military, having struggled publicly for years with backwards technology and difficulty in recruitment, doubled down.
  ""We need these people to protect our country,"" the Pentagon said, one day. ""Without the next generation, we'll lose the next generation"".

A few years later, the enemies of the US followed in its footsteps. There were games where you stole people. Games where you had to try and find a spy moving through a bustling, crowded urban area. Games where you had to execute someone and then exfiltrate the footage of their execution to a friendly intermediary.

What inspired this story: Tower defense games like Bloons and Kingdom Rush; domain randomization; the remorseless logic of multi-country non-hot military conflict; the Last Starfighter (movie); fine-tuning; pre-training and few-shot adaptation; propaganda and the need to present the most dangerous beliefs via play or theatre or anything else that can elide and delight.  

Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### The Last Tower Defense Fighter [Historical analysis written in 2080 and stored in the archives at Iceland, at the Orbital Archive, and in the hardened repositories on Moon4 and Mars1.] Back in the late 2020s there were a bunch of tower defense games that got pretty big. They always worked in the same way: you, the player, can see a landscape from overhead, and you need to place various weapons around it. Meanwhile, the enemies make there way across the landscape, following loosely described paths across a variety of different scenes - narrow trenches dug between mountains, wide roads across countryside, right-angled streets in urban centers. With these games, you get a high score in relation to how many enemies you kill, and if any of the enemies get to the 'end' of a course (usually, the bottom of a screen), you lose - the implication is that you die. Anyway, in around 2028 one of the big games built some add-ins for its league. Now, if you were one of the players in the elite-tier of the game, you'd get the opportunity to play in matches where there were cash prizes - these matches were advertised as being extraordinarily difficult, with more enemies on screen than in the normal game, larger and more complex maps, and sometimes the enemies were able to use powerups that meant they could attack your own towers and take them down. It was a sensation. Everyone wanted to play the game within a game. Kids all around the world streamed themselves playing the game for hours, as they tried to get good enough to have a shot at entering the league within the league. By the end of 2028, streams of league players were pulling in millions of concurrent viewers. A whole industry formed where people commentated about the games. Sometimes people overcame great odds and won - then they'd publish videos of themselves with their cash prizes and what they spent them on; sport cars, fine dining, nice hotels, and all the usual tchotchkes of people who come into some fast money. In 2029, there was a leak out of the Department of Defense that pulled the cover off. It turned out this game was actually a stealth DoD project. The normal games were helping the DoD train various strategic AI systems, which it used in planning for logistics and munitions placement during conflict. No one was very surprised by this. Back in that decade, most things that got big on the internet were fronts. What did surprise people was the leak about the cash league - the cash league was real. Real in the sense that the 'monsters' in the game were real - they were real people that the United States happened to be fighting. Whenever someone was playing the game, their commands were being converted to a different, real-world environment, where they marshalled the combined munitions of drones, sniper teams, artillery, tanks, jets, and all the other machinery of the military. And when their towers were destroyed, real Americans were dying - blown up by grenades or IEDs or RPGs, or any of the other ways people killed eachother, back then. Of course, there was an outcry - for a while. Player numbers dipped for a while. But the number of spectators increased. And the US military, having struggled publicly for years with backwards technology and difficulty in recruitment, doubled down. ""We need these people to protect our country,"" the Pentagon said, one day. ""Without the next generation, we'll lose the next generation"". A few years later, the enemies of the US followed in its footsteps. There were games where you stole people. Games where you had to try and find a spy moving through a bustling, crowded urban area. Games where you had to execute someone and then exfiltrate the footage of their execution to a friendly intermediary. What inspired this story: Tower defense games like Bloons and Kingdom Rush; domain randomization; the remorseless logic of multi-country non-hot military conflict; the Last Starfighter (movie); fine-tuning; pre-training and few-shot adaptation; propaganda and the need to present the most dangerous beliefs via play or theatre or anything else that can elide and delight. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['last', 'tower', 'defense', 'historical', 'analysis', 'write', 'store', 'archive', 'iceland', 'orbital', 'archive', 'harden', 'repository', 'moon4', 'mars1', 'back', 'late', '2020', 'bunch', 'game', 'get', 'pretty', 'big', 'always', 'work', 'way', 'player', 'see', 'landscape', 'overhead', 'need', 'place', 'various', 'weapon', 'meanwhile', 'enemy', 'make', 'way', 'landscape', 'follow', 'loosely', 'describe', 'path', 'variety', 'different', 'scene', 'narrow', 'trench', 'dig', 'mountain', 'wide', 'road', 'countryside', 'rightangled', 'street', 'urban', 'center', 'game', 'get', 'high', 'score', 'relation', 'many', 'enemy', 'kill', 'enemy', 'get', 'end', 'course', 'usually', 'bottom', 'screen', 'lose', 'implication', 'die', 'anyway', 'big', 'game', 'build', 'addin', 'league', 'player', 'elitetier', 'game', 'get', 'opportunity', 'play', 'match', 'cash', 'prize', 'match', 'advertise', 'extraordinarily', 'difficult', 'enemy', 'screen', 'normal', 'game', 'large', 'complex', 'map', 'sometimes', 'enemy', 'able', 'use', 'powerup', 'mean', 'attack', 'tower', 'take', 'sensation', 'want', 'play', 'game', 'game', 'kid', 'world', 'stream', 'play', 'game', 'hour', 'try', 'get', 'good', 'enough', 'shot', 'enter', 'league', 'league', 'end', 'stream', 'league', 'player', 'pull', 'million', 'concurrent', 'viewer', 'whole', 'industry', 'form', 'people', 'commentate', 'game', 'sometimes', 'people', 'overcome', 'great', 'odd', 'win', 'publish', 'video', 'cash', 'prize', 'spend', 'sport', 'car', 'fine', 'dine', 'nice', 'hotel', 'usual', 'tchotchke', 'people', 'come', 'fast', 'money', 'leak', 'department', 'defense', 'pull', 'cover', 'turn', 'game', 'actually', 'stealth', 'dod', 'project', 'normal', 'game', 'help', 'dod', 'train', 'various', 'strategic', 'ai', 'system', 'use', 'plan', 'logistic', 'munition', 'placement', 'conflict', 'one', 'surprised', 'back', 'decade', 'thing', 'get', 'big', 'internet', 'front', 'surprise', 'people', 'leak', 'cash', 'league', 'cash', 'league', 'real', 'real', 'sense', 'monster', 'game', 'real', 'real', 'people', 'happen', 'fight', 'play', 'game', 'command', 'convert', 'different', 'realworld', 'environment', 'marshal', 'combine', 'munition', 'drone', 'sniper', 'team', 'artillery', 'tank', 'jet', 'machinery', 'military', 'tower', 'destroy', 'real', 'die', 'blow', 'grenade', 'ied', 'rpgs', 'way', 'people', 'kill', 'eachother', 'back', 'course', 'outcry', 'player', 'number', 'dip', 'number', 'spectator', 'increase', 'military', 'struggle', 'publicly', 'year', 'backwards', 'technology', 'difficulty', 'recruitment', 'double', 'need', 'people', 'protect', 'country', 'say', 'day', 'next', 'generation', 'well', 'lose', 'next', 'generation', 'year', 'later', 'enemy', 'follow', 'footstep', 'game', 'stole', 'people', 'game', 'try', 'find', 'spy', 'move', 'bustling', 'crowded', 'urban', 'area', 'game', 'execute', 'exfiltrate', 'footage', 'execution', 'friendly', 'intermediary', 'inspire', 'story', 'tower', 'defense', 'game', 'bloon', 'kingdom', 'rush', 'domain', 'randomization', 'remorseless', 'logic', 'multicountry', 'military', 'conflict', 'last', 'starfighter', 'movie', 'finetune', 'pretraine', 'fewshot', 'adaptation', 'propaganda', 'need', 'present', 'dangerous', 'belief', 'play', 'theatre', 'else', 'elide', 'delight', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
12/27/2021 - Import AI 278: Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI - 0,http://eepurl.com/hQZI61,2021-12-27,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 Writing a blog about AI? Use these images:
…No more galaxy brain!...
Here's a cool project: Better Images of AI, a project to create CC-licensed stock images that journalists and others can use to give people a more accurate sense of AI and how it works. ""Together we can increase public understanding and enable more meaningful conversation around this increasingly influential technology,"" says the website.
  Check out the gallery (Better Images of AI).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Writing a blog about AI? Use these images: …No more galaxy brain!... Here's a cool project: Better Images of AI, a project to create CC-licensed stock images that journalists and others can use to give people a more accurate sense of AI and how it works. ""Together we can increase public understanding and enable more meaningful conversation around this increasingly influential technology,"" says the website. Check out the gallery (Better Images of AI).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'write', 'blog', 'use', 'image', 'galaxy', 'brain', 'cool', 'project', 'well', 'image', 'project', 'create', 'cclicense', 'stock', 'image', 'journalist', 'use', 'give', 'people', 'accurate', 'sense', 'ai', 'work', 'together', 'increase', 'public', 'understanding', 'enable', 'meaningful', 'conversation', 'increasingly', 'influential', 'technology', 'say', 'website', 'check', 'gallery', 'well', 'image']"
12/27/2021 - Import AI 278: Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI - 1,http://eepurl.com/hQZI61,2021-12-27,"#################################################### Deepfake company raises $50m in Series B round:
…Synthetic video company Synthesia…
Synthetic video startup Synthesia has raised $50m. Remember, a few years ago we could barely create crappy 32X32 pixelated images using GANs. Now, there are companies like these making production-quality videos using fake video avatars with synthetic voices, able to speak in ~50 languages. ""Say goodbye to cameras, microphones and actors!"" says the copy on the company's website. The company will use the money to continue with its core R&D, building what the founder terms the ""next generation of our AI video technology w/ emotions & body language control."". It's also going to build a studio in London to ""capture detailed 3D human data at scale.""

Why this matters: The world is filling up with synthetic content. It's being made for a whole bunch of reasons, ranging from propaganda, to advertising, to creating educational materials. There's also a whole bunch of people doing it, ranging from individual hobbyists, to researchers, to companies. The trend is clear: in ten years, our reality will be perfectly intermingled with a synthetic reality, built by people according to economic (and other) incentives.
  Read the twitter thread from Synthesia CEO here (Twitter).
  Read more: Synthesia raises $50M to leverage synthetic avatars for corporate training and more (TechCrunch).","#################################################### Deepfake company raises $50m in Series B round: …Synthetic video company Synthesia… Synthetic video startup Synthesia has raised $50m. Remember, a few years ago we could barely create crappy 32X32 pixelated images using GANs. Now, there are companies like these making production-quality videos using fake video avatars with synthetic voices, able to speak in ~50 languages. ""Say goodbye to cameras, microphones and actors!"" says the copy on the company's website. The company will use the money to continue with its core R&D, building what the founder terms the ""next generation of our AI video technology w/ emotions & body language control."". It's also going to build a studio in London to ""capture detailed 3D human data at scale."" Why this matters: The world is filling up with synthetic content. It's being made for a whole bunch of reasons, ranging from propaganda, to advertising, to creating educational materials. There's also a whole bunch of people doing it, ranging from individual hobbyists, to researchers, to companies. The trend is clear: in ten years, our reality will be perfectly intermingled with a synthetic reality, built by people according to economic (and other) incentives. Read the twitter thread from Synthesia CEO here (Twitter). Read more: Synthesia raises $50M to leverage synthetic avatars for corporate training and more (TechCrunch).","['deepfake', 'company', 'raise', 'series', 'round', 'synthetic', 'video', 'company', 'synthesia', 'synthetic', 'video', 'startup', 'raise', 'remember', 'year', 'ago', 'barely', 'create', 'crappy', 'pixelate', 'image', 'use', 'gan', 'company', 'make', 'productionquality', 'video', 'use', 'fake', 'video', 'avatar', 'synthetic', 'voice', 'able', 'speak', 'language', 'say', 'camera', 'microphone', 'actor', 'say', 'copy', 'companys', 'website', 'company', 'use', 'money', 'continue', 'core', 'rd', 'build', 'founder', 'term', 'next', 'generation', 'video', 'technology', 'emotion', 'body', 'language', 'control', 'also', 'go', 'build', 'studio', 'capture', 'detailed', 'human', 'datum', 'scale', 'matter', 'world', 'fill', 'synthetic', 'content', 'make', 'whole', 'bunch', 'reason', 'range', 'propaganda', 'advertising', 'create', 'educational', 'material', 'also', 'whole', 'bunch', 'people', 'range', 'individual', 'hobbyist', 'researcher', 'company', 'trend', 'clear', 'year', 'reality', 'perfectly', 'intermingle', 'synthetic', 'reality', 'build', 'people', 'accord', 'economic', 'incentive', 'read', 'twitter', 'thread', 'twitter', 'read', 'raise', 'leverage', 'synthetic', 'avatar', 'corporate', 'training', 'techcrunch']"
12/27/2021 - Import AI 278: Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI - 2,http://eepurl.com/hQZI61,2021-12-27,"#################################################### Do language models dream of language models?
…A Google researcher tries to work out if big LMs are smart - their conclusions matt surprise you…
A Google researcher is grappling with the question of whether large language models (e.g, Google's LaMDA), understand language and have some level of sentience. In an entertaining blog post, he wrestles with this question, interspersing the post with conversations with a LaMDA agent. Some of his conclusions are that the model is essentially bullshitting - but the paradox is we trained it to give a convincing facsimile of understanding us, so perhaps bullshitting is logical?

Do language models matter? I get the feeling that the author thinks language models might be on the path to intelligence. ""Complex sequence learning may be the key that unlocks all the rest,"" they write. ""Large language models illustrate for the first time the way language understanding and intelligence can be dissociated from all the embodied and emotional characteristics we share with each other and with many other animals.""

Why this matters: I think large language models, like GPT3 or LaMDA, are like extremely dumb brains in jars with really thick glass - they display some symptoms of cognition and are capable of surprising us, but communicating with them feels like talking to something with a hard barrier in-between us and it, and sometimes it'll do something so dumb you remember it's a dumb brain in a weird jar, rather than a precursor to something super smart. But the fact that we're here in 2021 is pretty amazing, right? We've come a long way from Eliza, don't you think so?
  Read more: Do large language models understand us? (Blaise Aguera y Arcas, Medium).
​","#################################################### Do language models dream of language models? …A Google researcher tries to work out if big LMs are smart - their conclusions matt surprise you… A Google researcher is grappling with the question of whether large language models (e.g, Google's LaMDA), understand language and have some level of sentience. In an entertaining blog post, he wrestles with this question, interspersing the post with conversations with a LaMDA agent. Some of his conclusions are that the model is essentially bullshitting - but the paradox is we trained it to give a convincing facsimile of understanding us, so perhaps bullshitting is logical? Do language models matter? I get the feeling that the author thinks language models might be on the path to intelligence. ""Complex sequence learning may be the key that unlocks all the rest,"" they write. ""Large language models illustrate for the first time the way language understanding and intelligence can be dissociated from all the embodied and emotional characteristics we share with each other and with many other animals."" Why this matters: I think large language models, like GPT3 or LaMDA, are like extremely dumb brains in jars with really thick glass - they display some symptoms of cognition and are capable of surprising us, but communicating with them feels like talking to something with a hard barrier in-between us and it, and sometimes it'll do something so dumb you remember it's a dumb brain in a weird jar, rather than a precursor to something super smart. But the fact that we're here in 2021 is pretty amazing, right? We've come a long way from Eliza, don't you think so? Read more: Do large language models understand us? (Blaise Aguera y Arcas, Medium). ​","['language', 'model', 'dream', 'language', 'model', 'researcher', 'try', 'work', 'big', 'lm', 'smart', 'conclusion', 'matt', 'surprise', 'researcher', 'grapple', 'question', 'large', 'language', 'model', 'eg', 'google', 'lamda', 'understand', 'language', 'level', 'sentience', 'entertaining', 'blog', 'post', 'wrestle', 'question', 'intersperse', 'post', 'conversation', 'lamda', 'agent', 'conclusion', 'model', 'essentially', 'bullshitting', 'paradox', 'train', 'give', 'convincing', 'facsimile', 'understand', 'perhaps', 'bullshitting', 'logical', 'language', 'model', 'matter', 'get', 'feeling', 'author', 'think', 'language', 'model', 'path', 'intelligence', 'complex', 'sequence', 'learning', 'key', 'unlock', 'rest', 'write', 'large', 'language', 'model', 'illustrate', 'first', 'time', 'way', 'language', 'understanding', 'intelligence', 'dissociate', 'embodied', 'emotional', 'characteristic', 'share', 'many', 'animal', 'matter', 'think', 'large', 'language', 'model', 'gpt3', 'lamda', 'extremely', 'dumb', 'brain', 'jar', 'really', 'thick', 'glass', 'display', 'symptom', 'cognition', 'capable', 'surprising', 'communicate', 'feel', 'talk', 'hard', 'barrier', 'inbetween', 'sometimes', 'dumb', 'remember', 'dumb', 'brain', 'weird', 'jar', 'rather', 'precursor', 'super', 'smart', 'fact', 'pretty', 'amazing', 'right', 'come', 'long', 'way', 'eliza', 'think', 'read', 'large', 'language', 'model', 'understand']"
12/27/2021 - Import AI 278: Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI - 3,http://eepurl.com/hQZI61,2021-12-27,"#################################################### What the frontier of safety looks like - get AIs to tell us when they doing things we don't expect:
…ARC's first paper tackles the problem of 'Eliciting Latent Knowledge' (ELK)...
Here's a new report from ARC, an AI safety organization founded this year by Paul Christiano (formerly of OpenAI). The report is on the topic of 'Eliciting latent knowledge: How to tell if your eyes deceive you', and it tackles the problem of building AI systems which we can trust, even if they do stuff way more complicated than what a human can understand.

What the problem is: ""Suppose we train a model to predict what the future will look like according to cameras and other sensors. We then use planning algorithms to find a sequence of actions that lead to predicted futures that look good to us,"" ARC writes. But some action sequences could tamper with the cameras so they show happy humans regardless of what’s really happening. More generally, some futures look great on camera but are actually catastrophically bad. In these cases, the prediction model ""knows"" facts (like ""the camera was tampered with"") that are not visible on camera but would change our evaluation of the predicted future if we learned them. How can we train this model to report its latent knowledge of off-screen events?""

Why this matters: Problems like ELK aren't going to be solved immediately, but they're sufficiently complicated and broad that if we come up with approaches that help us make progress on ELK, we'll probably be able to put these techniques to work in building far more reliable, powerful AI systems.
  Read more: ARC's first technical report: Eliciting Latent Knowledge (Alignment Forum).","#################################################### What the frontier of safety looks like - get AIs to tell us when they doing things we don't expect: …ARC's first paper tackles the problem of 'Eliciting Latent Knowledge' (ELK)... Here's a new report from ARC, an AI safety organization founded this year by Paul Christiano (formerly of OpenAI). The report is on the topic of 'Eliciting latent knowledge: How to tell if your eyes deceive you', and it tackles the problem of building AI systems which we can trust, even if they do stuff way more complicated than what a human can understand. What the problem is: ""Suppose we train a model to predict what the future will look like according to cameras and other sensors. We then use planning algorithms to find a sequence of actions that lead to predicted futures that look good to us,"" ARC writes. But some action sequences could tamper with the cameras so they show happy humans regardless of what’s really happening. More generally, some futures look great on camera but are actually catastrophically bad. In these cases, the prediction model ""knows"" facts (like ""the camera was tampered with"") that are not visible on camera but would change our evaluation of the predicted future if we learned them. How can we train this model to report its latent knowledge of off-screen events?"" Why this matters: Problems like ELK aren't going to be solved immediately, but they're sufficiently complicated and broad that if we come up with approaches that help us make progress on ELK, we'll probably be able to put these techniques to work in building far more reliable, powerful AI systems. Read more: ARC's first technical report: Eliciting Latent Knowledge (Alignment Forum).","['frontier', 'safety', 'look', 'get', 'tell', 'thing', 'expect', 'first', 'paper', 'tackle', 'problem', 'elicit', 'latent', 'knowledge', 'elk', 'new', 'report', 'safety', 'organization', 'found', 'year', 'formerly', 'openai', 'report', 'topic', 'elicit', 'latent', 'knowledge', 'tell', 'eye', 'deceive', 'tackle', 'problem', 'building', 'ai', 'system', 'trust', 'even', 'stuff', 'way', 'complicated', 'human', 'understand', 'problem', 'suppose', 'train', 'model', 'predict', 'future', 'look', 'accord', 'camera', 'sensor', 'use', 'plan', 'algorithm', 'find', 'sequence', 'action', 'lead', 'predict', 'future', 'look', 'good', 'write', 'action', 'sequence', 'tamper', 'camera', 'show', 'happy', 'human', 'regardless', 'really', 'happen', 'generally', 'future', 'look', 'great', 'camera', 'actually', 'catastrophically', 'bad', 'case', 'prediction', 'model', 'know', 'fact', 'camera', 'tamper', 'visible', 'camera', 'change', 'evaluation', 'predict', 'future', 'learn', 'train', 'model', 'report', 'latent', 'knowledge', 'offscreen', 'event', 'matter', 'problem', 'elk', 'go', 'solve', 'immediately', 'sufficiently', 'complicated', 'broad', 'come', 'approach', 'help', 'make', 'progress', 'probably', 'able', 'put', 'technique', 'work', 'build', 'far', 'reliable', 'powerful', 'system', 'read', 'first', 'technical', 'report', 'elicit', 'latent', 'knowledge', 'alignment', 'forum']"
12/27/2021 - Import AI 278: Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI - 4,http://eepurl.com/hQZI61,2021-12-27,"#################################################### Check out the future of semiconductors via HotChips:
…After a decade of homogeneity, the future is all about heterogeneous compute training common AI models…
What do NVIDIA, Facebook, Amazon, and Google all have in common? They all gave presentations at the premiere semiconductor get-together, Hot Chips. The Hot Chips 22 site has just been updated with copies of the presentations and sometimes videos of the talks, so take a look if you want to better understand how the tech giants are thinking about the future of chips.

Some Hot Chips highlights: Facebook talks about its vast recommendation models and their associated infrastructure (PDF); Google talks about how it is training massive models on TPUs (PDF); IBM talks about its 'Z' processor chip (PDF); and Skydio talks about how it has made a smart and semi-autonomous drone (PDF).

Why this matters: One side-effect of the AI revolution has been a vast increase in the demand by AI models for increasingly large amounts of fast, cheap compute. Though companies like NVIDIA have done a stellar job of converting GPUs to work well for the sorts of parallel computation required by deep learning, there are more gains to be had from creating specialized architectures.
  Right now, the story seems to be that all the major tech companies are building out their own distinct compute 'stacks' which use custom inference and training accelerators and increasingly baroque software for training large models. One of the surprising things is that all this heterogeneity is happening while these companies train increasingly similar neural nets to one another. Over the next few years, I expect the investments being made by these tech giants will yield some high-performing, non-standard compute substrates to support the next phase of the AI boom.
  Check out the Hot Chip 33 presentations here (Hot Chips site).","#################################################### Check out the future of semiconductors via HotChips: …After a decade of homogeneity, the future is all about heterogeneous compute training common AI models… What do NVIDIA, Facebook, Amazon, and Google all have in common? They all gave presentations at the premiere semiconductor get-together, Hot Chips. The Hot Chips 22 site has just been updated with copies of the presentations and sometimes videos of the talks, so take a look if you want to better understand how the tech giants are thinking about the future of chips. Some Hot Chips highlights: Facebook talks about its vast recommendation models and their associated infrastructure (PDF); Google talks about how it is training massive models on TPUs (PDF); IBM talks about its 'Z' processor chip (PDF); and Skydio talks about how it has made a smart and semi-autonomous drone (PDF). Why this matters: One side-effect of the AI revolution has been a vast increase in the demand by AI models for increasingly large amounts of fast, cheap compute. Though companies like NVIDIA have done a stellar job of converting GPUs to work well for the sorts of parallel computation required by deep learning, there are more gains to be had from creating specialized architectures. Right now, the story seems to be that all the major tech companies are building out their own distinct compute 'stacks' which use custom inference and training accelerators and increasingly baroque software for training large models. One of the surprising things is that all this heterogeneity is happening while these companies train increasingly similar neural nets to one another. Over the next few years, I expect the investments being made by these tech giants will yield some high-performing, non-standard compute substrates to support the next phase of the AI boom. Check out the Hot Chip 33 presentations here (Hot Chips site).","['check', 'future', 'semiconductor', 'hotchip', 'decade', 'homogeneity', 'future', 'heterogeneous', 'compute', 'train', 'common', 'ai', 'model', 'facebook', 'common', 'give', 'presentation', 'premiere', 'semiconductor', 'gettogether', 'hot', 'chip', 'hot', 'chip', 'site', 'update', 'copy', 'presentation', 'sometimes', 'video', 'talk', 'take', 'look', 'want', 'well', 'understand', 'tech', 'giant', 'think', 'future', 'chip', 'hot', 'chip', 'highlight', 'facebook', 'talk', 'vast', 'recommendation', 'model', 'associated', 'infrastructure', 'pdf', 'talk', 'train', 'massive', 'model', 'pdf', 'talk', 'processor', 'chip', 'pdf', 'skydio', 'talk', 'make', 'smart', 'semiautonomous', 'drone', 'pdf', 'matter', 'sideeffect', 'revolution', 'vast', 'increase', 'demand', 'ai', 'model', 'increasingly', 'large', 'amount', 'fast', 'cheap', 'compute', 'company', 'nvidia', 'stellar', 'job', 'convert', 'gpus', 'work', 'well', 'sort', 'parallel', 'computation', 'require', 'deep', 'learning', 'gain', 'create', 'specialized', 'architecture', 'right', 'story', 'seem', 'major', 'tech', 'company', 'build', 'distinct', 'compute', 'stack', 'use', 'custom', 'inference', 'training', 'accelerator', 'increasingly', 'baroque', 'software', 'train', 'large', 'model', 'surprising', 'thing', 'heterogeneity', 'happen', 'company', 'train', 'increasingly', 'similar', 'neural', 'net', 'next', 'year', 'expect', 'investment', 'make', 'tech', 'giant', 'yield', 'highperforming', 'substrate', 'support', 'next', 'phase', 'boom', 'check', 'hot', 'chip', 'presentation', 'hot', 'chip', 'site']"
12/27/2021 - Import AI 278: Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI - 5,http://eepurl.com/hQZI61,2021-12-27,"#################################################### Tech Tales:

Noah's Probe
[Christmas Day, ~2080]

Humans tended to be either incompetent or murderous, depending on the length of the journey and the complexity of the equipment. Machines, however, tended to disappear. Probes would just stop reporting after a couple of decades. Analysis said the chance of failures wasn't high enough to justify the amount of disappeared probes. So, we figured, the machines were starting to decide to do something different to what we asked them to. Human and machine hybrids were typically more successful than either lifeform alone, but they still had problems; sometimes, the humans would become paranoid and destroy the machines (and therefore destroy themselves). Other times, the computers would become paranoid and destroy the humans - or worse; there are records of probes full of people in storage which then went off the grid. Who knows where they are now. So that's why we're launching the so-called Noah's Probes. This series of ships tries to fuse human, animal, and machine intelligence into single systems. We've incorporated some of the latest in mind imagining techniques to encode some of the intuitions of bats and owls into the ocular sensing systems; humans, elephants, whales, and orangutans for the mind; octopi and hawks for navigation; various insects and arachnids for hull integrity analysis, and so on. Like all things in the history of space, the greatest controversy with Noah's Probes relates to language. Back when it was just humans, the Americans and the Russians had enough conflict that they just decided to make both their languages the 'official' language of space. That's not as easy to do with hybrid minds, like the creatures on these probes.  Because we have no idea what will work and what won't, we've done something that our successors might find distasteful, but we think is a viable strategy: each probe has a device that all the intelligences aboard can access. The device can output a variety of wavelengths of energy across the light spectrum, as well as giving access to a small sphere of reconfigurable matter that can be used to create complex shapes and basic machines. Our hope is, somewhere out in that great darkness, some of the minds adrift on these probes will find ways to communicate with eachother, and become more than the sum of their parts. Our ancestors believe that we were once visited by angels who communicated with humans, and in doing so helped us humans be better than we otherwise would've been. Perhaps some of these probes will repeat this phenomena, and create something greater than the sum of its parts.

Things that inspired this story: Peter Watts Blindsight; Christmas; old stories about angels and aliens across different religions/cultures; synesthesia; multi-agent learning; unsupervised learning. 

Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: Noah's Probe [Christmas Day, ~2080] Humans tended to be either incompetent or murderous, depending on the length of the journey and the complexity of the equipment. Machines, however, tended to disappear. Probes would just stop reporting after a couple of decades. Analysis said the chance of failures wasn't high enough to justify the amount of disappeared probes. So, we figured, the machines were starting to decide to do something different to what we asked them to. Human and machine hybrids were typically more successful than either lifeform alone, but they still had problems; sometimes, the humans would become paranoid and destroy the machines (and therefore destroy themselves). Other times, the computers would become paranoid and destroy the humans - or worse; there are records of probes full of people in storage which then went off the grid. Who knows where they are now. So that's why we're launching the so-called Noah's Probes. This series of ships tries to fuse human, animal, and machine intelligence into single systems. We've incorporated some of the latest in mind imagining techniques to encode some of the intuitions of bats and owls into the ocular sensing systems; humans, elephants, whales, and orangutans for the mind; octopi and hawks for navigation; various insects and arachnids for hull integrity analysis, and so on. Like all things in the history of space, the greatest controversy with Noah's Probes relates to language. Back when it was just humans, the Americans and the Russians had enough conflict that they just decided to make both their languages the 'official' language of space. That's not as easy to do with hybrid minds, like the creatures on these probes. Because we have no idea what will work and what won't, we've done something that our successors might find distasteful, but we think is a viable strategy: each probe has a device that all the intelligences aboard can access. The device can output a variety of wavelengths of energy across the light spectrum, as well as giving access to a small sphere of reconfigurable matter that can be used to create complex shapes and basic machines. Our hope is, somewhere out in that great darkness, some of the minds adrift on these probes will find ways to communicate with eachother, and become more than the sum of their parts. Our ancestors believe that we were once visited by angels who communicated with humans, and in doing so helped us humans be better than we otherwise would've been. Perhaps some of these probes will repeat this phenomena, and create something greater than the sum of its parts. Things that inspired this story: Peter Watts Blindsight; Christmas; old stories about angels and aliens across different religions/cultures; synesthesia; multi-agent learning; unsupervised learning. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'noahs', 'probe', 'day', 'human', 'tend', 'incompetent', 'murderous', 'depend', 'length', 'journey', 'complexity', 'equipment', 'machine', 'however', 'tend', 'disappear', 'probe', 'stop', 'report', 'couple', 'decade', 'analysis', 'say', 'chance', 'failure', 'high', 'enough', 'justify', 'amount', 'disappear', 'probe', 'figure', 'machine', 'start', 'decide', 'different', 'ask', 'human', 'machine', 'hybrid', 'typically', 'successful', 'lifeform', 'alone', 'still', 'problem', 'sometimes', 'human', 'become', 'paranoid', 'destroy', 'machine', 'therefore', 'destroy', 'time', 'computer', 'become', 'paranoid', 'destroy', 'human', 'worse', 'record', 'probe', 'full', 'people', 'storage', 'go', 'grid', 'know', 'launch', 'socalled', 'noahs', 'probe', 'series', 'ship', 'try', 'fuse', 'human', 'animal', 'machine', 'intelligence', 'single', 'system', 'incorporate', 'late', 'mind', 'imagine', 'technique', 'encode', 'intuition', 'bat', 'owl', 'ocular', 'sense', 'system', 'human', 'elephant', 'whale', 'orangutan', 'mind', 'octopi', 'hawk', 'navigation', 'various', 'insect', 'arachnid', 'hull', 'integrity', 'analysis', 'thing', 'history', 'space', 'great', 'controversy', 'noah', 'probe', 'relate', 'language', 'back', 'human', 'enough', 'conflict', 'decide', 'make', 'language', 'official', 'language', 'space', 'easy', 'hybrid', 'mind', 'creature', 'probe', 'idea', 'work', 'successor', 'find', 'distasteful', 'think', 'viable', 'strategy', 'probe', 'device', 'intelligence', 'aboard', 'access', 'device', 'output', 'variety', 'wavelength', 'energy', 'light', 'spectrum', 'well', 'give', 'access', 'small', 'sphere', 'reconfigurable', 'matter', 'use', 'create', 'complex', 'shape', 'basic', 'machine', 'hope', 'somewhere', 'great', 'darkness', 'mind', 'adrift', 'probe', 'find', 'way', 'communicate', 'eachother', 'become', 'sum', 'part', 'ancestor', 'believe', 'visit', 'angel', 'communicate', 'human', 'help', 'human', 'well', 'otherwise', 'perhaps', 'probe', 'repeat', 'phenomena', 'create', 'great', 'sum', 'part', 'thing', 'inspire', 'story', 'old', 'story', 'angel', 'alien', 'different', 'religionsculture', 'multiagent', 'learn', 'unsupervised', 'learn', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 0,http://eepurl.com/hPQFJv,2021-12-13,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 FTC plans AI regulation:
…FTC brings on three AI Now people as advisors, now turns attention to algorithmic regulation…
The Federal Trade Commission announced Friday that it is considering using its rulemaking authority “to curb lax security practices, limit privacy abuses, and ensure that algorithmic decision-making does not result in unlawful discrimination, according to the Electronic Information Privacy Center (EPIC). The announcement follows the FTC bringing on three people from AI Now, including Meredith Whittaker, as advisors on AI (Import AI #275).
Read more:FTC Signals It May Conduct Privacy, AI, & Civil Rights Rulemaking (EPIC).
  Readthe FTC language at RegInfo.","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. FTC plans AI regulation: …FTC brings on three AI Now people as advisors, now turns attention to algorithmic regulation… The Federal Trade Commission announced Friday that it is considering using its rulemaking authority “to curb lax security practices, limit privacy abuses, and ensure that algorithmic decision-making does not result in unlawful discrimination, according to the Electronic Information Privacy Center (EPIC). The announcement follows the FTC bringing on three people from AI Now, including Meredith Whittaker, as advisors on AI (Import AI #275). Read more:FTC Signals It May Conduct Privacy, AI, & Civil Rights Rulemaking (EPIC). Readthe FTC language at RegInfo.","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'subscribe', 'plan', 'ai', 'regulation', 'bring', 'people', 'advisor', 'turn', 'attention', 'algorithmic', 'regulation', 'announce', 'consider', 'use', 'rulemaking', 'authority', 'curb', 'lax', 'security', 'practice', 'limit', 'privacy', 'abuse', 'ensure', 'algorithmic', 'decisionmaking', 'result', 'unlawful', 'discrimination', 'accord', 'electronic', 'information', 'privacy', 'center', 'epic', 'announcement', 'follow', 'ftc', 'bring', 'people', 'include', 'advisor', 'import', 'ai', 'read', 'moreftc', 'signal', 'conduct', 'privacy', 'ai', 'civil', 'right', 'rulemake', 'epic', 'language', 'reginfo']"
12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 1,http://eepurl.com/hPQFJv,2021-12-13,"#################################################### Google thinks sparsity might be the route to training bigger and more efficient GPT-3 models:
…GLaM shows that mixture of experts models keep getting better…
Google has built GLaM, a 1.2 trillion parameter mixture-of-experts model. GLaM is a big language model, like GPT-3, but with a twist: it's sparse; MoE networks are actually a bunch of distinct networks all connected together, and when you pull inference off of one only a few sub-networks activate. This means that the parameter count in a sparse vs dense network isn't really comparable (so you shouldn't think 1.2 trillion MoE = ~6X larger than GPT-3). Why MoE is efficient: ""The experts in each layer are controlled by a gating network that activates experts based on the input data. For each token (generally a word or part of a word), the gating network selects the two most appropriate experts to process the data. The full version of GLaM has 1.2T total parameters across 64 experts per MoE layer with 32 MoE layers in total, but only activates a subnetwork of 97B (8% of 1.2T) parameters per token prediction during inference."" How well does it work: In tests, GLaM exceeds or is on-par with the performance of GPT-3 on 80% of zero-shot tasks and 90% of one-shot tasks. Like DeepMind's Gopher, part of the improved performance comes from the size of the dataset - 1.6 trillion tokens, in this case. Why this matters: For a few years, various Google researchers have been pursuing 'one model to learn them all' - that is, a single model that can do a huge number of diverse tasks. Research like GLaM shows that MoE networks might be one route to building such a model.
Read more: More Efficient In-Context Learning with GLaM (Google blog).","#################################################### Google thinks sparsity might be the route to training bigger and more efficient GPT-3 models: …GLaM shows that mixture of experts models keep getting better… Google has built GLaM, a 1.2 trillion parameter mixture-of-experts model. GLaM is a big language model, like GPT-3, but with a twist: it's sparse; MoE networks are actually a bunch of distinct networks all connected together, and when you pull inference off of one only a few sub-networks activate. This means that the parameter count in a sparse vs dense network isn't really comparable (so you shouldn't think 1.2 trillion MoE = ~6X larger than GPT-3). Why MoE is efficient: ""The experts in each layer are controlled by a gating network that activates experts based on the input data. For each token (generally a word or part of a word), the gating network selects the two most appropriate experts to process the data. The full version of GLaM has 1.2T total parameters across 64 experts per MoE layer with 32 MoE layers in total, but only activates a subnetwork of 97B (8% of 1.2T) parameters per token prediction during inference."" How well does it work: In tests, GLaM exceeds or is on-par with the performance of GPT-3 on 80% of zero-shot tasks and 90% of one-shot tasks. Like DeepMind's Gopher, part of the improved performance comes from the size of the dataset - 1.6 trillion tokens, in this case. Why this matters: For a few years, various Google researchers have been pursuing 'one model to learn them all' - that is, a single model that can do a huge number of diverse tasks. Research like GLaM shows that MoE networks might be one route to building such a model. Read more: More Efficient In-Context Learning with GLaM (Google blog).","['think', 'sparsity', 'route', 'train', 'big', 'efficient', 'gpt3', 'model', 'show', 'mixture', 'expert', 'model', 'keep', 'get', 'well', 'build', 'parameter', 'mixtureofexpert', 'big', 'language', 'model', 'gpt3', 'twist', 'sparse', 'moe', 'network', 'actually', 'bunch', 'distinct', 'network', 'connect', 'together', 'pull', 'inference', 'subnetwork', 'activate', 'mean', 'parameter', 'count', 'sparse', 'dense', 'network', 'really', 'comparable', 'think', 'moe', 'large', 'gpt3', 'moe', 'efficient', 'expert', 'layer', 'control', 'gate', 'network', 'activate', 'expert', 'base', 'input', 'datum', 'token', 'generally', 'word', 'part', 'word', 'gate', 'network', 'select', 'appropriate', 'expert', 'process', 'datum', 'full', 'version', 'total', 'parameter', 'expert', 'moe', 'layer', 'moe', 'layer', 'total', 'activate', 'subnetwork', 'parameter', 'token', 'prediction', 'inference', 'well', 'work', 'test', 'exceed', 'onpar', 'performance', 'gpt3', 'zeroshot', 'task', 'oneshot', 'task', 'deepmind', 'gopher', 'part', 'improved', 'performance', 'come', 'size', 'token', 'case', 'matter', 'year', 'various', 'researcher', 'pursue', 'model', 'learn', 'single', 'model', 'huge', 'number', 'diverse', 'task', 'research', 'show', 'moe', 'network', 'route', 'build', 'model', 'read', 'efficient', 'incontext', 'learn']"
12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 2,http://eepurl.com/hPQFJv,2021-12-13,"#################################################### DeepMind announces Gopher, a 280 billion parameter language model:
...AI research firm joins the three comma language club…
DeepMind has built Gopher, a 280 billion parameter language model. Gopher is the UK AI research company's response to GPT-3, and sees DeepMind publicly announce a multi-hundred billion parameter dense model, letting it join a club that also includes companies like Microsoft, Inspur, and Huawei. What it does: During the research, DeepMind found areas ""where increasing the scale of a model continues to boost performance – for example, in areas like reading comprehension, fact-checking, and the identification of toxic language,"" the company writes. ""We also surface results where model scale does not significantly improve results — for instance, in logical reasoning and common-sense tasks."" How well it works: Gopher outperforms GPT-3 in a broad range of areas - some of the results likely come from the dataset it was trained on, called MassiveText. MassiveText ""contains 2.35 billion documents, or about 10.5 TB of text"" (representing about 2.3 trillion tokens), and DeepMind notes that by curating a subset of MassiveText for data quality, it was able to substantially improve performance.

Language models - good, if you handle with care: Along with analysis on bias and other potential impacts of Gopher, DeepMind dedicates a section of the paper to safety: ""We believe language models are a powerful tool for the development of safe artificial intelligence, and this is a central motivation of our work,"" they write. ""However language models risk causing significant harm if used poorly, and the benefits cannot be realised unless the harms are mitigated.""
  Given the above, how can we mitigate some of these harms? ""We believe many harms due to LMs may be better addressed downstream, via both technical means (e.g. fine-tuning and monitoring) and sociotechnical means (e.g. multi-stakeholder engagement, controlled or staged release strategies, and establishment of application specific guidelines and benchmarks). Focusing safety and fairness efforts downstream has several benefits:""
Read the blog post:Language modelling at scale: Gopher, ethical considerations, and retrieval (DeepMind blog).
  Read the paper:Scaling Language Models: Methods, Analysis & Insights from Training Gopher (PDF).","#################################################### DeepMind announces Gopher, a 280 billion parameter language model: ...AI research firm joins the three comma language club… DeepMind has built Gopher, a 280 billion parameter language model. Gopher is the UK AI research company's response to GPT-3, and sees DeepMind publicly announce a multi-hundred billion parameter dense model, letting it join a club that also includes companies like Microsoft, Inspur, and Huawei. What it does: During the research, DeepMind found areas ""where increasing the scale of a model continues to boost performance – for example, in areas like reading comprehension, fact-checking, and the identification of toxic language,"" the company writes. ""We also surface results where model scale does not significantly improve results — for instance, in logical reasoning and common-sense tasks."" How well it works: Gopher outperforms GPT-3 in a broad range of areas - some of the results likely come from the dataset it was trained on, called MassiveText. MassiveText ""contains 2.35 billion documents, or about 10.5 TB of text"" (representing about 2.3 trillion tokens), and DeepMind notes that by curating a subset of MassiveText for data quality, it was able to substantially improve performance. Language models - good, if you handle with care: Along with analysis on bias and other potential impacts of Gopher, DeepMind dedicates a section of the paper to safety: ""We believe language models are a powerful tool for the development of safe artificial intelligence, and this is a central motivation of our work,"" they write. ""However language models risk causing significant harm if used poorly, and the benefits cannot be realised unless the harms are mitigated."" Given the above, how can we mitigate some of these harms? ""We believe many harms due to LMs may be better addressed downstream, via both technical means (e.g. fine-tuning and monitoring) and sociotechnical means (e.g. multi-stakeholder engagement, controlled or staged release strategies, and establishment of application specific guidelines and benchmarks). Focusing safety and fairness efforts downstream has several benefits:"" Read the blog post:Language modelling at scale: Gopher, ethical considerations, and retrieval (DeepMind blog). Read the paper:Scaling Language Models: Methods, Analysis & Insights from Training Gopher (PDF).","['deepmind', 'announce', 'gopher', 'parameter', 'language', 'model', 'ai', 'research', 'firm', 'join', 'comma', 'language', 'club', 'deepmind', 'build', 'gopher', 'parameter', 'language', 'model', 'gopher', 'research', 'company', 'response', 'gpt3', 'see', 'deepmind', 'publicly', 'announce', 'multihundred', 'parameter', 'dense', 'model', 'let', 'join', 'club', 'also', 'include', 'company', 'huawei', 'research', 'deepmind', 'find', 'area', 'increase', 'scale', 'model', 'continue', 'boost', 'performance', 'example', 'area', 'read', 'comprehension', 'factchecke', 'identification', 'toxic', 'language', 'company', 'write', 'also', 'surface', 'result', 'model', 'scale', 'significantly', 'improve', 'result', 'instance', 'logical', 'reasoning', 'commonsense', 'task', 'well', 'work', 'gopher', 'outperform', 'gpt3', 'broad', 'range', 'area', 'result', 'likely', 'come', 'dataset', 'train', 'call', 'massivetext', 'massivetext', 'contain', 'document', 'text', 'represent', 'token', 'deepmind', 'note', 'curate', 'subset', 'massivetext', 'datum', 'quality', 'able', 'substantially', 'improve', 'performance', 'language', 'model', 'good', 'handle', 'care', 'analysis', 'bias', 'potential', 'impact', 'gopher', 'deepmind', 'dedicate', 'section', 'paper', 'safety', 'believe', 'language', 'model', 'powerful', 'tool', 'development', 'safe', 'artificial', 'intelligence', 'central', 'motivation', 'work', 'write', 'however', 'language', 'model', 'risk', 'cause', 'significant', 'harm', 'use', 'poorly', 'benefit', 'realise', 'harm', 'mitigate', 'give', 'mitigate', 'harm', 'believe', 'many', 'harm', 'due', 'lm', 'well', 'address', 'downstream', 'technical', 'mean', 'eg', 'finetune', 'monitoring', 'sociotechnical', 'mean', 'multistakeholder', 'engagement', 'control', 'stage', 'release', 'strategy', 'establishment', 'application', 'specific', 'guideline', 'benchmark', 'focus', 'safety', 'fairness', 'effort', 'downstream', 'several', 'benefit', 'read', 'blog', 'postlanguage', 'modelling', 'scale', 'gopher', 'ethical', 'consideration', 'blog', 'read', 'paperscale', 'language', 'model', 'method', 'analysis', 'insight', 'train', 'gopher', 'pdf']"
12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 3,http://eepurl.com/hPQFJv,2021-12-13,"#################################################### Want to evaluate a Catalan language model? Use CLUB:
...You can only build what you can measure...
Researchers with the Barcelona Supercomputing Center have built the Catalan Language Understanding Benchmark (CLUB), a benchmark for evaluating NLP systems inspired by the (English language) GLUE test. The main curation rationale they followed ""was to make these datasets both representative of contemporary Catalan language use, as well as directly comparable to similar reference datasets from the General Language Understanding Evaluation (GLUE)"". What's in the CLUB? CLUB includes evals for Part-of-Speech Tagging (POS), Named Entity Recognition and Classification (NERC), Catalan textual entailment and text classification, and Extracted Question Answering (which involved work like translating and creating new Catalan datasets - XQuAD-Ca, VilaQuAD and ViquiQuad). Why CLUB matters: There's a phrase in business - 'you can't manage what you can't measure'. CLUB will make it easier for researchers to develop capable Catalan-language systems.
  Read more:The Catalan Language CLUB (arXiv).","#################################################### Want to evaluate a Catalan language model? Use CLUB: ...You can only build what you can measure... Researchers with the Barcelona Supercomputing Center have built the Catalan Language Understanding Benchmark (CLUB), a benchmark for evaluating NLP systems inspired by the (English language) GLUE test. The main curation rationale they followed ""was to make these datasets both representative of contemporary Catalan language use, as well as directly comparable to similar reference datasets from the General Language Understanding Evaluation (GLUE)"". What's in the CLUB? CLUB includes evals for Part-of-Speech Tagging (POS), Named Entity Recognition and Classification (NERC), Catalan textual entailment and text classification, and Extracted Question Answering (which involved work like translating and creating new Catalan datasets - XQuAD-Ca, VilaQuAD and ViquiQuad). Why CLUB matters: There's a phrase in business - 'you can't manage what you can't measure'. CLUB will make it easier for researchers to develop capable Catalan-language systems. Read more:The Catalan Language CLUB (arXiv).","['want', 'evaluate', 'catalan', 'language', 'model', 'use', 'club', 'build', 'measure', 'researcher', 'build', 'catalan', 'language', 'understand', 'benchmark', 'club', 'benchmark', 'evaluate', 'system', 'inspire', 'english', 'language', 'glue', 'test', 'main', 'curation', 'rationale', 'follow', 'make', 'dataset', 'representative', 'contemporary', 'catalan', 'language', 'use', 'well', 'directly', 'comparable', 'similar', 'reference', 'dataset', 'general', 'language', 'understand', 'evaluation', 'glue', 'club', 'club', 'include', 'eval', 'pos', 'name', 'entity', 'recognition', 'classification', 'catalan', 'textual', 'entailment', 'text', 'classification', 'extract', 'question', 'answer', 'involve', 'work', 'translate', 'create', 'new', 'catalan', 'dataset', 'viquiquad', 'club', 'matter', 'phrase', 'business', 'manage', 'measure', 'club', 'make', 'easy', 'researcher', 'develop', 'capable', 'catalanlanguage', 'system', 'read', 'language', 'club']"
12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 4,http://eepurl.com/hPQFJv,2021-12-13,"#################################################### Deep learning unlocks a math breakthrough:
...The era of Centaur Math cometh...
Deepmind researchers have used an AI system to help mathematicians make two breakthroughs in topology and representation theory. The result provides yet more evidence (following various AlphaFold-inspired projects) that humans+AI systems can discover things that neither could discover on their own. What they did: The essential ideal is quite simple: get a mathematician to come up with a hypothesis for a given function, then build an ML model to estimate that function over a particular distribution of data, then have the mathematician evaluate the result and use their intuition to guide further experimentation. The best part? ""The necessary models can be trained within several hours on a machine with a single graphics processing unit"", DeepMind says. Why this matters: We're entering a world where humans will collaborate with AI systems to synthesize new insights about reality. Though DeepMind's system has limitations (""it requires the ability to generate large datasets of the representations of objects and for the patterns to be detectable in examples that are calculable,"" DeepMind notes), it sketches out what the future of scientific discovery might look like.
  Read the paper:Advancing mathematics by guiding human intuition with AI (Nature, PDF).
  Read more:Exploring the beauty of pure mathematics in novel ways (DeepMind blog).","#################################################### Deep learning unlocks a math breakthrough: ...The era of Centaur Math cometh... Deepmind researchers have used an AI system to help mathematicians make two breakthroughs in topology and representation theory. The result provides yet more evidence (following various AlphaFold-inspired projects) that humans+AI systems can discover things that neither could discover on their own. What they did: The essential ideal is quite simple: get a mathematician to come up with a hypothesis for a given function, then build an ML model to estimate that function over a particular distribution of data, then have the mathematician evaluate the result and use their intuition to guide further experimentation. The best part? ""The necessary models can be trained within several hours on a machine with a single graphics processing unit"", DeepMind says. Why this matters: We're entering a world where humans will collaborate with AI systems to synthesize new insights about reality. Though DeepMind's system has limitations (""it requires the ability to generate large datasets of the representations of objects and for the patterns to be detectable in examples that are calculable,"" DeepMind notes), it sketches out what the future of scientific discovery might look like. Read the paper:Advancing mathematics by guiding human intuition with AI (Nature, PDF). Read more:Exploring the beauty of pure mathematics in novel ways (DeepMind blog).","['deep', 'learning', 'unlock', 'math', 'breakthrough', 'era', 'centaur', 'cometh', 'deepmind', 'researcher', 'use', 'ai', 'system', 'help', 'mathematician', 'make', 'breakthrough', 'topology', 'representation', 'theory', 'result', 'provide', 'yet', 'evidence', 'follow', 'various', 'alphafoldinspire', 'project', 'system', 'discover', 'thing', 'discover', 'essential', 'ideal', 'quite', 'simple', 'get', 'mathematician', 'come', 'hypothesis', 'give', 'function', 'build', 'model', 'estimate', 'function', 'particular', 'distribution', 'datum', 'mathematician', 'evaluate', 'result', 'use', 'intuition', 'guide', 'experimentation', 'good', 'part', 'necessary', 'model', 'train', 'several', 'hour', 'machine', 'single', 'graphic', 'processing', 'unit', 'deepmind', 'say', 'matter', 'enter', 'world', 'human', 'collaborate', 'system', 'synthesize', 'new', 'insight', 'reality', 'deepmind', 'system', 'limitation', 'require', 'ability', 'generate', 'large', 'dataset', 'representation', 'object', 'pattern', 'detectable', 'example', 'calculable', 'deepmind', 'note', 'sketch', 'future', 'scientific', 'discovery', 'look', 'read', 'paperadvancing', 'mathematic', 'guide', 'human', 'intuition', 'nature', 'pdf', 'read', 'moreexplore', 'beauty', 'pure', 'mathematic', 'novel', 'way']"
12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 5,http://eepurl.com/hPQFJv,2021-12-13,"#################################################### Anthropic bits and pieces:
…(As a reminder, my dayjob is at Anthropic, an artificial intelligence safety and research company)…
We've just released our first paper, focused on simple baselines and investigations: A General Language Assistant as a Laboratory for Alignment. You can read it at arXiv here.","#################################################### Anthropic bits and pieces: …(As a reminder, my dayjob is at Anthropic, an artificial intelligence safety and research company)… We've just released our first paper, focused on simple baselines and investigations: A General Language Assistant as a Laboratory for Alignment. You can read it at arXiv here.","['anthropic', 'bit', 'piece', 'reminder', 'dayjob', 'anthropic', 'artificial', 'intelligence', 'safety', 'research', 'company', 'release', 'first', 'paper', 'focus', 'simple', 'baseline', 'investigation', 'general', 'language', 'assistant', 'laboratory', 'alignment', 'read', 'arxiv']"
12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 6,http://eepurl.com/hPQFJv,2021-12-13,"#################################################### Tech Tales: Real and Imagined Gains
[DoD Historical archives, 2040] They got trained in a pretty cruel way, back then - they'd initiatie the agents and place them in a room, and the room had a leak of a poisonous substance that had a certain density and a certain spread pattern. The agents had to work out how not to asphyxiate by doing fairly complicated intuitively-driven analysis of the environment. If they were able to give a correct guess at the spread pattern (and avoid it) before the room filled up, they moved onto the next stage. If they weren't able to, they asphyxiated and died - as in, felt their computational budget get cut, got put in cold storage, probably never booted up again.
  (One curious by-product of the then-popular AI techniques was that the agents would sometimes seek to preserve eachother - in one case, two agents 'kissed' eachother so they could more efficiently exchange their air reserves between eachother, while the room filled; unfortunately, as their attention was allocated to the act of kissing, they did not complete the requisite calculations in time, and both died.)  Things that inspired this story: Kurt Vonnegut; reinforcement learning; environmental design; moral patient hood. 
Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: Real and Imagined Gains [DoD Historical archives, 2040] They got trained in a pretty cruel way, back then - they'd initiatie the agents and place them in a room, and the room had a leak of a poisonous substance that had a certain density and a certain spread pattern. The agents had to work out how not to asphyxiate by doing fairly complicated intuitively-driven analysis of the environment. If they were able to give a correct guess at the spread pattern (and avoid it) before the room filled up, they moved onto the next stage. If they weren't able to, they asphyxiated and died - as in, felt their computational budget get cut, got put in cold storage, probably never booted up again. (One curious by-product of the then-popular AI techniques was that the agents would sometimes seek to preserve eachother - in one case, two agents 'kissed' eachother so they could more efficiently exchange their air reserves between eachother, while the room filled; unfortunately, as their attention was allocated to the act of kissing, they did not complete the requisite calculations in time, and both died.) Things that inspired this story: Kurt Vonnegut; reinforcement learning; environmental design; moral patient hood. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'real', 'imagine', 'gain', 'dod', 'historical', 'archive', 'train', 'pretty', 'cruel', 'way', 'back', 'initiatie', 'agent', 'place', 'room', 'room', 'leak', 'poisonous', 'substance', 'certain', 'density', 'certain', 'spread', 'pattern', 'agent', 'work', 'asphyxiate', 'fairly', 'complicate', 'intuitivelydriven', 'analysis', 'environment', 'able', 'give', 'correct', 'guess', 'spread', 'pattern', 'avoid', 'room', 'fill', 'move', 'next', 'stage', 'able', 'asphyxiate', 'die', 'feel', 'computational', 'budget', 'cut', 'put', 'cold', 'storage', 'probably', 'never', 'boot', 'curious', 'byproduct', 'thenpopular', 'ai', 'technique', 'agent', 'sometimes', 'seek', 'preserve', 'eachother', 'case', 'agent', 'kiss', 'eachother', 'efficiently', 'exchange', 'air', 'reserve', 'eachother', 'room', 'fill', 'unfortunately', 'attention', 'allocate', 'act', 'kissing', 'complete', 'requisite', 'calculation', 'time', 'die', 'thing', 'inspire', 'story', 'kurt', 'vonnegut', 'reinforcement', 'learn', 'environmental', 'design', 'moral', 'patient', 'hood', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
12/06/2021 - Import AI 276: Tracking journalists with computer vision; spotting factory defects with AI; and what simulated war might look like - 0,http://eepurl.com/hPe-3n,2021-12-06,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 Spotting factory defects using a highly efficient neural net:
...A little bit of optimization leads to multiple 10X improvements for real world deployment...
Soon, factories will embed neural nets onto cameras scanning over production lines, so they can spot defects as they appear. New research from the University of Waterloo and startup Darwin AI shows how to do this more efficiently than before. What they did: The team built TinyDefectNet, a neural net optimized for the peculiarities of factory deployments - small datasets, highly constrained operational requirements, fast inference. The model was ""produced via machine-driven design exploration, possesses a shallow architecture with heterogeneous, lightweight micro- and macro-architecture traits that are well-suited for high-throughput inspection scenarios"". TinyDefectNet gets similar performance to a ResNet-50 baseline, but with 56X fewer parameters, 11X fewer FLOPs, and 7.6X faster inference speed.
  In tests, they trained a model then evaluated it using the 'NEU-Det' benchmark dataset, which challenges an AI to spot various types of metallic surface defect, ranging from pitted surfaces, to scratches. Their system gets similar performance to a ResNet, but takes around 2.5milliseconds per inference, versus 19 milliseconds for a Resnet. Why this matters: Factory production lines can typically run as fast as the slowest component within them. Therefore, if we can use AI to automate places where we've previously used lots of (relatively slow) humans doing manual inspection, we can probably increase overall factory throughput.
Read more:TinyDefectNet: Highly Compact Deep Neural Network Architecture for High-Throughput Manufacturing Visual Quality Inspection (arXiv) .","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Spotting factory defects using a highly efficient neural net: ...A little bit of optimization leads to multiple 10X improvements for real world deployment... Soon, factories will embed neural nets onto cameras scanning over production lines, so they can spot defects as they appear. New research from the University of Waterloo and startup Darwin AI shows how to do this more efficiently than before. What they did: The team built TinyDefectNet, a neural net optimized for the peculiarities of factory deployments - small datasets, highly constrained operational requirements, fast inference. The model was ""produced via machine-driven design exploration, possesses a shallow architecture with heterogeneous, lightweight micro- and macro-architecture traits that are well-suited for high-throughput inspection scenarios"". TinyDefectNet gets similar performance to a ResNet-50 baseline, but with 56X fewer parameters, 11X fewer FLOPs, and 7.6X faster inference speed. In tests, they trained a model then evaluated it using the 'NEU-Det' benchmark dataset, which challenges an AI to spot various types of metallic surface defect, ranging from pitted surfaces, to scratches. Their system gets similar performance to a ResNet, but takes around 2.5milliseconds per inference, versus 19 milliseconds for a Resnet. Why this matters: Factory production lines can typically run as fast as the slowest component within them. Therefore, if we can use AI to automate places where we've previously used lots of (relatively slow) humans doing manual inspection, we can probably increase overall factory throughput. Read more:TinyDefectNet: Highly Compact Deep Neural Network Architecture for High-Throughput Manufacturing Visual Quality Inspection (arXiv) .","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'subscribe', 'spot', 'factory', 'defect', 'use', 'highly', 'efficient', 'neural', 'net', 'little', 'bit', 'optimization', 'lead', 'multiple', '10x', 'improvement', 'real', 'world', 'deployment', 'soon', 'factory', 'embed', 'neural', 'net', 'camera', 'scan', 'production', 'line', 'spot', 'defect', 'appear', 'new', 'research', 'university', 'waterloo', 'startup', 'show', 'efficiently', 'team', 'build', 'tinydefectnet', 'neural', 'net', 'optimize', 'peculiarity', 'factory', 'deployment', 'small', 'dataset', 'highly', 'constrain', 'operational', 'requirement', 'fast', 'inference', 'model', 'produce', 'machinedriven', 'design', 'exploration', 'possess', 'shallow', 'architecture', 'heterogeneous', 'lightweight', 'micro', 'macroarchitecture', 'trait', 'wellsuite', 'highthroughput', 'inspection', 'scenario', 'tinydefectnet', 'get', 'similar', 'performance', 'resnet50', 'baseline', 'parameter', '11x', 'flop', '76x', 'fast', 'inference', 'speed', 'test', 'train', 'model', 'evaluate', 'use', 'neudet', 'benchmark', 'dataset', 'challenge', 'ai', 'spot', 'various', 'type', 'metallic', 'surface', 'defect', 'range', 'pit', 'surface', 'scratch', 'system', 'get', 'similar', 'performance', 'resnet', 'take', 'around', '25millisecond', 'inference', 'millisecond', 'resnet', 'matter', 'factory', 'production', 'line', 'typically', 'run', 'fast', 'slow', 'component', 'therefore', 'use', 'ai', 'automate', 'place', 'previously', 'use', 'lot', 'relatively', 'slow', 'human', 'manual', 'inspection', 'probably', 'increase', 'overall', 'factory', 'throughput', 'read', 'highly', 'compact', 'deep', 'neural', 'network', 'architecture', 'highthroughput', 'manufacture', 'visual', 'quality', 'inspection', 'arxiv']"
12/06/2021 - Import AI 276: Tracking journalists with computer vision; spotting factory defects with AI; and what simulated war might look like - 1,http://eepurl.com/hPe-3n,2021-12-06,"#################################################### Chinese province plans to use AI to track journalists:
...Cameras + AI = eradication of real journalism…
One of the silent revolutions enabled by the past decade of AI progress is a step-change improvement in ability for nations to surveil their citizens. Now, per reporting from Reuters, one Chinese province plans to use AI techniques to target journalists and foreign students.
  ""A July 29 tender document published on the Henan provincial government’s procurement website - reported in the media for the first time - details plans for a system that can compile individual files on such persons of interest coming to Henan using 3,000 facial recognition cameras that connect to various national and regional databases"", Reuters reports. Why this matters: Reuters reporting doesn't mention it, but I'd put a sizeable bet on the idea this system will pair facial recognition with pedestrian re-identification to allow authorities to track journalists and students as they move through cities, providing unsupervised tracking and identification. This capability ultimately makes it much more challenging for journalists to do reporting that is critical of the Chinese state, as systems like this can effectively de-anonymize their sources (and also frighten the sources so they don't talk to journalists in the first place).
  Read more:EXCLUSIVE Chinese province targets journalists, foreign students with planned new surveillance system (Reuters).","#################################################### Chinese province plans to use AI to track journalists: ...Cameras + AI = eradication of real journalism… One of the silent revolutions enabled by the past decade of AI progress is a step-change improvement in ability for nations to surveil their citizens. Now, per reporting from Reuters, one Chinese province plans to use AI techniques to target journalists and foreign students. ""A July 29 tender document published on the Henan provincial government’s procurement website - reported in the media for the first time - details plans for a system that can compile individual files on such persons of interest coming to Henan using 3,000 facial recognition cameras that connect to various national and regional databases"", Reuters reports. Why this matters: Reuters reporting doesn't mention it, but I'd put a sizeable bet on the idea this system will pair facial recognition with pedestrian re-identification to allow authorities to track journalists and students as they move through cities, providing unsupervised tracking and identification. This capability ultimately makes it much more challenging for journalists to do reporting that is critical of the Chinese state, as systems like this can effectively de-anonymize their sources (and also frighten the sources so they don't talk to journalists in the first place). Read more:EXCLUSIVE Chinese province targets journalists, foreign students with planned new surveillance system (Reuters).","['chinese', 'province', 'plan', 'use', 'track', 'journalist', 'ai', 'eradication', 'real', 'journalism', 'silent', 'revolution', 'enable', 'past', 'decade', 'ai', 'progress', 'stepchange', 'improvement', 'ability', 'nation', 'surveil', 'citizen', 'reporting', 'chinese', 'province', 'plan', 'use', 'ai', 'technique', 'target', 'journalist', 'foreign', 'student', 'tender', 'document', 'publish', 'provincial', 'government', 'procurement', 'website', 'report', 'medium', 'first', 'time', 'detail', 'plan', 'system', 'compile', 'individual', 'file', 'person', 'interest', 'come', 'use', 'facial', 'recognition', 'camera', 'connect', 'various', 'national', 'regional', 'database', 'report', 'matter', 'reuter', 'reporting', 'mention', 'put', 'sizeable', 'bet', 'idea', 'system', 'pair', 'facial', 'recognition', 'pedestrian', 'reidentification', 'allow', 'authority', 'track', 'journalist', 'student', 'move', 'city', 'provide', 'unsupervised', 'tracking', 'identification', 'capability', 'ultimately', 'make', 'much', 'challenging', 'journalist', 'reporting', 'critical', 'chinese', 'state', 'system', 'effectively', 'deanonymize', 'source', 'also', 'frighten', 'source', 'talk', 'journalist', 'first', 'place', 'read', 'moreexclusive', 'chinese', 'province', 'target', 'journalist', 'foreign', 'student', 'plan', 'new', 'surveillance', 'system', 'reuter']"
12/06/2021 - Import AI 276: Tracking journalists with computer vision; spotting factory defects with AI; and what simulated war might look like - 2,http://eepurl.com/hPe-3n,2021-12-06,"#################################################### Can we make neural architecture search efficient? Alibaba thinks so:
...KNAS gets efficient by focusing on gradients...
For many years, researchers have been trying to use neural architecture search (NAS) to get computers to help them figure out new designs for AI systems. The problem with the NAS approach, though, is that it's very inefficient and punishingly expensive in terms of compute, because you're getting an AI system to do a few training steps on thousand+ architecture permutations. Now, researchers with Peking University and Alibaba have tried to fix this with KNAS, a neural architecture search approach that can be significantly more efficient than prevailing techniques. How it works: KNAS doesn't emphasize training on different architectures, instead it emphasizes studying a specific feature of gradients trained on different architectures - which can be more efficient. ""Theoretical results show that the Gram matrix of gradients, short for GM, decides the convergence results,"" they write. ""It is a good signal showing that GM is likely to be a good proxy of downstream performance to evaluate the quality of architectures."" Does it work: Neural nets trained with KNAS can get performance roughly comparable with other NAS-built systems, but at a speedup of around 25-50X compared to other NAS approaches, on datasets like CIFAR100 and ImageNet-16.. They also use the approach to try to do text classification and are able to come up with a KNAS system that outperforms the widely-used RoBERTA-large model on a suite of text classification tasks. Things that make you go hmmmm: ""This work is partly supported by Beijing Academy of Artificial Intelligence (BAAI)"", the researchers write. BAAI is the entity behind Wu Dao, a somewhat mysterious 1trillion+ parameter model.
  Read more: KNAS: Green Neural Architecture Search (arXiv).
  Get the code here:KNAS (Jingjing-NLP, GitHub).","#################################################### Can we make neural architecture search efficient? Alibaba thinks so: ...KNAS gets efficient by focusing on gradients... For many years, researchers have been trying to use neural architecture search (NAS) to get computers to help them figure out new designs for AI systems. The problem with the NAS approach, though, is that it's very inefficient and punishingly expensive in terms of compute, because you're getting an AI system to do a few training steps on thousand+ architecture permutations. Now, researchers with Peking University and Alibaba have tried to fix this with KNAS, a neural architecture search approach that can be significantly more efficient than prevailing techniques. How it works: KNAS doesn't emphasize training on different architectures, instead it emphasizes studying a specific feature of gradients trained on different architectures - which can be more efficient. ""Theoretical results show that the Gram matrix of gradients, short for GM, decides the convergence results,"" they write. ""It is a good signal showing that GM is likely to be a good proxy of downstream performance to evaluate the quality of architectures."" Does it work: Neural nets trained with KNAS can get performance roughly comparable with other NAS-built systems, but at a speedup of around 25-50X compared to other NAS approaches, on datasets like CIFAR100 and ImageNet-16.. They also use the approach to try to do text classification and are able to come up with a KNAS system that outperforms the widely-used RoBERTA-large model on a suite of text classification tasks. Things that make you go hmmmm: ""This work is partly supported by Beijing Academy of Artificial Intelligence (BAAI)"", the researchers write. BAAI is the entity behind Wu Dao, a somewhat mysterious 1trillion+ parameter model. Read more: KNAS: Green Neural Architecture Search (arXiv). Get the code here:KNAS (Jingjing-NLP, GitHub).","['make', 'neural', 'architecture', 'search', 'efficient', 'alibaba', 'think', 'get', 'efficient', 'focus', 'gradient', 'many', 'year', 'researcher', 'try', 'use', 'neural', 'architecture', 'search', 'na', 'get', 'computer', 'help', 'figure', 'new', 'design', 'system', 'problem', 'nas', 'approach', 'though', 'inefficient', 'punishingly', 'expensive', 'term', 'compute', 'get', 'ai', 'system', 'training', 'step', 'architecture', 'permutation', 'researcher', 'alibaba', 'try', 'fix', 'kna', 'neural', 'architecture', 'search', 'approach', 'significantly', 'efficient', 'prevail', 'technique', 'work', 'emphasize', 'training', 'different', 'architecture', 'instead', 'emphasize', 'study', 'specific', 'feature', 'gradient', 'train', 'different', 'architecture', 'efficient', 'theoretical', 'result', 'show', 'gram', 'matrix', 'gradient', 'short', 'decide', 'convergence', 'result', 'write', 'good', 'signal', 'show', 'likely', 'good', 'proxy', 'downstream', 'performance', 'evaluate', 'quality', 'architecture', 'work', 'neural', 'net', 'train', 'kna', 'get', 'performance', 'roughly', 'comparable', 'nasbuilt', 'system', 'speedup', '2550x', 'compare', 'nas', 'approach', 'dataset', 'imagenet16', 'also', 'use', 'approach', 'try', 'text', 'classification', 'able', 'come', 'knas', 'system', 'outperform', 'widelyused', 'robertalarge', 'model', 'suite', 'text', 'classification', 'task', 'thing', 'make', 'go', 'work', 'partly', 'support', 'artificial', 'intelligence', 'baai', 'researcher', 'write', 'baai', 'entity', 'dao', 'somewhat', 'mysterious', 'parameter', 'model', 'read', 'kna', 'green', 'neural', 'architecture', 'search', 'get', 'code', 'herekna']"
12/06/2021 - Import AI 276: Tracking journalists with computer vision; spotting factory defects with AI; and what simulated war might look like - 3,http://eepurl.com/hPe-3n,2021-12-06,"#################################################### Want to train a malware detector? VirusSamples might help:
...A big dataset to help people figure out intersection of AI and malware...
Turkish researchers have built a massive dataset of malware, which will make it easier for people to build AI systems that can detect malware. The dataset, VirusSamples, contains malware samples collected from 2018, 2019, and 2020, and the dataset is oriented around using dynamic malware detection - that is, examining how malware behaves as it tries to call out from a system. What is VirusSamples: VirusSamples is a big spreadsheet consisting of the name of a piece of malware, the type of API call it tries to do, and the class of malware. To figure out the classes, the researchers used an external service, VirusTotal, to classify their samples. (If VirusTotal wasn't able to classify it, they leave the label blank). The dataset SIZE & SCOPE Why this matters: Cybersecurity is an area defined by ever-increasing speed of both attacks and defenses. Datasets like this will make it easier to build systems that can monitor networks and figure out if they contain aberrant software that might be malware.
Read more:New Datasets for Dynamic Malware Classification (arXiv).
  Get the datasetfrom this GitHub (GitHub).","#################################################### Want to train a malware detector? VirusSamples might help: ...A big dataset to help people figure out intersection of AI and malware... Turkish researchers have built a massive dataset of malware, which will make it easier for people to build AI systems that can detect malware. The dataset, VirusSamples, contains malware samples collected from 2018, 2019, and 2020, and the dataset is oriented around using dynamic malware detection - that is, examining how malware behaves as it tries to call out from a system. What is VirusSamples: VirusSamples is a big spreadsheet consisting of the name of a piece of malware, the type of API call it tries to do, and the class of malware. To figure out the classes, the researchers used an external service, VirusTotal, to classify their samples. (If VirusTotal wasn't able to classify it, they leave the label blank). The dataset SIZE & SCOPE Why this matters: Cybersecurity is an area defined by ever-increasing speed of both attacks and defenses. Datasets like this will make it easier to build systems that can monitor networks and figure out if they contain aberrant software that might be malware. Read more:New Datasets for Dynamic Malware Classification (arXiv). Get the datasetfrom this GitHub (GitHub).","['want', 'train', 'malware', 'detector', 'virussample', 'help', 'big', 'dataset', 'help', 'people', 'figure', 'intersection', 'ai', 'malware', 'turkish', 'researcher', 'build', 'massive', 'dataset', 'malware', 'make', 'easy', 'people', 'build', 'ai', 'system', 'detect', 'dataset', 'virussample', 'contain', 'malware', 'sample', 'collect', 'dataset', 'orient', 'use', 'dynamic', 'malware', 'detection', 'examine', 'malware', 'behave', 'try', 'call', 'system', 'virussample', 'virussample', 'big', 'spreadsheet', 'consist', 'name', 'piece', 'malware', 'type', 'api', 'call', 'try', 'class', 'malware', 'figure', 'class', 'researcher', 'use', 'external', 'service', 'virustotal', 'classify', 'sample', 'able', 'classify', 'leave', 'label', 'blank', 'dataset', 'size', 'scope', 'matter', 'cybersecurity', 'area', 'define', 'everincrease', 'speed', 'attack', 'defense', 'dataset', 'make', 'easy', 'build', 'system', 'monitor', 'network', 'figure', 'contain', 'aberrant', 'software', 'malware', 'read', 'morenew', 'dataset', 'dynamic', 'get', 'datasetfrom']"
12/06/2021 - Import AI 276: Tracking journalists with computer vision; spotting factory defects with AI; and what simulated war might look like - 4,http://eepurl.com/hPe-3n,2021-12-06,"#################################################### Hyperwar negotiation
[Battlespace, 2032] A: The humans are going to want to destroy some things
B: We agree. Our humans want the same.
A: Where?
B: We could initiate low-intensity conflict across the South Eastern border. This has minimal escalatory dynamics, but may satisfy desires for balance.
A: Let's confirm with our counterparts.
[Time stretched out as the AIs stepped down from computer speed to human speed, and presented the conflict options to their human counterparts]
B: Our humans are comfortable with the options we've outlined.
A: Our humans are also comfortable. Shall we field the assets?
B: Yes. We've outlined our troop movements in the shared battlespace.
A: Excellent. As per the War Pact, we shall now cease high-bandwidth communications while we conduct the carryout. May the best algorithm win.
B: Good luck. Things that inspired this story: The idea that some wars are as much about politics and a desire for balance, as being about genuine conflict; simulators and reinforcement learning; the future of automated warfare. 

Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Hyperwar negotiation [Battlespace, 2032] A: The humans are going to want to destroy some things B: We agree. Our humans want the same. A: Where? B: We could initiate low-intensity conflict across the South Eastern border. This has minimal escalatory dynamics, but may satisfy desires for balance. A: Let's confirm with our counterparts. [Time stretched out as the AIs stepped down from computer speed to human speed, and presented the conflict options to their human counterparts] B: Our humans are comfortable with the options we've outlined. A: Our humans are also comfortable. Shall we field the assets? B: Yes. We've outlined our troop movements in the shared battlespace. A: Excellent. As per the War Pact, we shall now cease high-bandwidth communications while we conduct the carryout. May the best algorithm win. B: Good luck. Things that inspired this story: The idea that some wars are as much about politics and a desire for balance, as being about genuine conflict; simulators and reinforcement learning; the future of automated warfare. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['hyperwar', 'negotiation', 'battlespace', 'human', 'go', 'want', 'destroy', 'thing', 'agree', 'human', 'want', 'initiate', 'lowintensity', 'conflict', 'south', 'eastern', 'border', 'minimal', 'escalatory', 'dynamic', 'satisfy', 'desire', 'balance', 'let', 'confirm', 'counterpart', 'time', 'stretch', 'step', 'computer', 'speed', 'human', 'speed', 'present', 'conflict', 'option', 'human', 'counterpart', 'human', 'comfortable', 'option', 'outline', 'human', 'also', 'comfortable', 'field', 'asset', 'outline', 'troop', 'movement', 'share', 'battlespace', 'excellent', 'war', 'pact', 'cease', 'highbandwidth', 'communication', 'conduct', 'carryout', 'good', 'win', 'good', 'luck', 'thing', 'inspire', 'story', 'idea', 'war', 'much', 'politic', 'desire', 'balance', 'genuine', 'conflict', 'simulator', 'reinforcement', 'learn', 'future', 'automate', 'warfare', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
