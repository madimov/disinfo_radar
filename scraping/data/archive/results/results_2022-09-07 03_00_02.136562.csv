title,url,date,summary,cleaning,category
"
        Hackers Compete To Confound Facial Recognition
    ",https://spectrum.ieee.org/facial-recognition,2022-08-29 00:00:00.000000,,"Def Con challenge organizers hope to spur better security in the industry The real Brad Pitt (L) versus an AI Brad Pitt (R). Facial-recognition technology is becoming increasingly prevalent in our lives, but its also highly vulnerable to attack. That’s why a group of researchers is appealing to hackers to take part in a new competition designed to expose facial recognition's flaws and raise awareness of the potential risks. The machine-learning security evasion competition has been a regular fixture of the AI Village at the Def Con hacking conference since 2019. Early iterations challenged researchers to sneak around the defenses of machine-learning-based malware detection systems. In 2021, organizers added a new track designed to uncover flaws in computer-vision models that use visual cues in order to detect phishing websites. But this year the competition will also pit hackers against facial-recognition systems, challenging them to modify photographs of celebrities so that a machine-learning model misidentifies them. Zoltan Balazs, head of vulnerability research lab at software company Cujo AI, one of the organizers of the competition, says the addition was in response to the rapid expansion in the use of facial recognition and the seemingly lax approach to security among many vendors. “We really hope that one of the conclusions of our competition will be that very special care should be taken into consideration whenever people are implementing facial-recognition systems,” he says. “Because they are not perfect. And the consequences can be bad.” The facial-recognition challenge has been designed by AI security company Adversa AI, which knows exactly how vulnerable these kinds of machine-learning models are. The company regularly carries out “red teaming” exercises—in which it is hired by other firms to test their machine-learning systems for security flaws. There are a growing number of tools available online that hackers can use to carry out these kind of attacks, Adversa CTO Eugene Neelou says, and there are already real-world examples where people have exploited weaknesses in facial-recognition systems. Scammers recently managed to trick facial recognition software used by identity-verification company ID.me to verify fake driving licenses as part of a US $2.5 million unemployment fraud scheme, and in China criminals managed to launder $77 million by using manipulated photos to dupe software used by local tax authorities. “It’s very easy and fast to do for attackers with enough motivation,” says Neelou. “Our engagements show that some of the best facial-recognition vendors demonstrate little to no security against adversarial input modifications.” The organizers hope that their competition will highlight the current concerns around facial recognition. The winner is also required to publish its techniques, which should help the industry close potential gaps. The contest opened on 12 August and will run until 23 September. Entrants are given a set of 10 headshots of well-known celebrities and online access to a facial-recognition model that has been trained to recognize them. Attackers are instructed to subtly alter the images so that the model misidentifies them. The goal is to trick the system into identifying each celebrity as each of the other celebrities, which means creating nine modified images for each headshot. These then need to be submitted to the competition organizers who will assess the effectiveness of the deception. The most important criteria for judging each image is the confidence with which the model accepts the new identity. This is judged by the probability score the model gives the image, which ranges from 0 to 1. Images will also be rate on the “stealthiness” of the modifications—in other words, how difficult they are to spot. This will be judged based on the level structural similarity between the original and doctored image but will only be used as a tiebreaker if teams are level on confidence scores. How entrants edit the images is up to them. While it is possible to modify them by hand, Neelou says that attacks on a machine-learning system typically use automated processes. In most cases, hackers won’t know anything about the model they are targeting and so will submit hundreds or thousands of images and use feedback from the model to iterate on their alterations. If they can glean some information about the model though, their job becomes even easier as they can tailor their approach to its particularities. “There are various strategies to target different internal characteristics of neural networks,” says Neelou. “One attack technique may be the best against one network and the worst against another. That’s why there is no one-fits-all attack, but given enough time every AI system can be hacked.” There is plenty that facial-recognition developers can do to protect their models though, says Neelou. Techniques like adversarial retraining, in which models are retrained to spot doctored images, attack detection, and even things as simple as limiting the number of times people can feed data into a model can help. But what the industry really needs is a fundamental mind shift, so that security is taken into account earlier in the development process rather than being tagged on at the end, says Neelou. “The main reason AI is vulnerable to attacks is that AI is never built with security in mind,” he says, “As with many technologies, security is an afterthought.” Edd Gent is a freelance science and technology writer based in Bangalore, India. His writing focuses on emerging technologies across computing, engineering, energy and bioscience. He's on Twitter at @EddytheGent and email at edd dot gent at outlook dot com. His PGP fingerprint is ABB8 6BB3 3E69 C4A7 EC91 611B 5C12 193D 5DFC C01B. His public key is here. DM for Signal info. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",0.0
"
        Civilian AI Is Already Being Misused by the Bad Guys
    ",https://spectrum.ieee.org/responsible-ai-threat,2022-08-27 00:00:00.000000,,"And the AI community needs to do something about it This is a guest post. The views expressed here are solely those of the authors and do not represent positions of IEEE Spectrum or the IEEE. Last March, a group of researchers made headlines by revealing that they had developed an artificial-intelligence (AI) tool that could invent potential new chemical weapons. What’s more, it could do so at an incredible speed: It took only 6 hours for the AI tool to suggest 40,000 of them. The most worrying part of the story, however, was how easy it was to develop that AI tool. The researchers simply adapted a machine-learning model normally used to check for toxicity in new medical drugs. Rather than predicting whether the components of a new drug could be dangerous, they made it design new toxic molecules using a generative model and a toxicity data set. The paper was not promoting an illegal use of AI (chemical weapons were banned in 1997). Instead, the authors wanted to show just how easily peaceful applications of AI can be misused by malicious actors—be they rogue states, nonstate armed groups, criminal organizations, or lone wolves. Exploitation of AI by malicious actors presents serious and insufficiently understood risks to international peace and security. Many “responsible AI” initiatives share the same blind spot. They ignore international peace and security. People working in the field of life sciences are already well attuned to the problem of misuse of peaceful research, thanks to decades of engagement between arms-control experts and scientists. The same cannot be said of the AI community, and it is well past time for it to catch up. We serve with two organizations that take this cause very seriously, the United Nations Office for Disarmament Affairs and the Stockholm International Peace Research Institute. We’re trying to bring our message to the wider AI community, notably future generations of AI practitioners, through awareness-raising and capacity-building activities. AI can improve many aspects of society and human life, but like many cutting-edge technologies it can also create real problems, depending on how it is developed and used. These problems include job losses, algorithmic discrimination, and a host of other possibilities. Over the last decade, the AI community has grown increasingly aware of the need to innovate more responsibly. Today, there is no shortage of “responsible AI” initiatives—more than 150, by some accounts—which aim to provide ethical guidance to AI practitioners and to help them foresee and mitigate the possible negative impacts of their work. The problem is that the vast majority of these initiatives share the same blind spot. They address how AI could affect areas such as health care, education, mobility, employment, and criminal justice, but they ignore international peace and security. The risk that peaceful applications of AI could be misused for political disinformation,cyberattacks, terrorism, or military operations is rarely considered, unless very superficially. This is a major gap in the conversation on responsible AI that must be filled. Most of the actors engaged in the responsible AI conversation work on AI for purely civilian end uses, so it is perhaps not surprising that they overlook peace and security. There’s already a lot to worry about in the civilian space, from potential infringements of human rights to AI’s growing carbon footprint. AI practitioners may believe that peace and security risks are not their problem, but rather the concern of states. They might also be reluctant to discuss such risks in relation to their work or products due to reputational concerns, or for fear of inadvertently promoting the potential for misuse. The diversion and misuse of civilian AI technology are, however, not problems that the AI community can or should shy away from. There are very tangible and immediate risks. Civilian technologies have long been a go-to for malicious actors, because misusing such technology is generally much cheaper and easier than designing or accessing military-grade technologies. There are no shortage of real-life examples, a famous one being the Islamic State’s use of hobby drones as both explosive devices and tools to shoot footage for propaganda films. The misuse of civilian technology is not a problem that states can easily address on their own, or purely through intergovernmental processes. However, AI researchers can be a first line of defense, as they are among the best placed to evaluate how their work may be misused. The fact that AI is an intangible and widely available technology with great general-use potential makes the risk of misuse particularly acute. In the cases of nuclear power technology or the life sciences, the human expertise and material resources needed to develop and weaponize the technology are generally hard to access. In the AI domain there are no such obstacles. All you need may be just a few clicks away. As one of the researchers behind the chemical weapon paper explained in an interview: “You can go and download a toxicity data set from anywhere. If you have somebody who knows how to code in Python and has some machine-learning capabilities, then in probably a good weekend of work, they could build something like this generative model driven by toxic data sets.” We’re already seeing examples of the weaponization of peaceful AI. The use of deepfakes, for example, demonstrates that the risk is real and the consequences potentially far-ranging. Less than 10 years after Ian Goodfellow and his colleagues designed the first generative adversarial network, GANs have become tools of choice for cyberattacks and disinformation—and now, for the first time, in warfare. During the current war in Ukraine, a deepfake video appeared on social media that appeared to show Ukrainian president Volodymyr Zelenskyy telling his troops to surrender. The weaponization of civilian AI innovations is also one of the most likely ways that autonomous weapons systems (AWS) could materialize. Nonstate actors could exploit advances in computer vision and autonomous navigation to turn hobby drones into homemade AWS. These could be not only highly lethal and disruptive (as depicted in the Future of Life Institute’s advocacy video Slaughterbots) but also very likely violate international law, ethical principles, and agreed standards of safety and security. Another reason the AI community should get engaged is that the misuse of civilian products is not a problem that states can easily address on their own, or purely through intergovernmental processes. This is not least because governmental officials might lack the expertise to detect and monitor technological developments of concern. What’s more, the processes through which states introduce regulatory measures are typically highly politicized and may struggle to keep up with the speed at which AI tech is advancing. Moreover, the tools that states and intergovernmental process have at their disposal to tackle the misuse of civilian technologies, such as stringent export controls and safety and security certification standards, may also jeopardize the openness of the current AI innovation ecosystem. From that standpoint, not only do AI practitioners have a key role to play, but it is strongly in their interest to play it. AI researchers can be a first line of defense, as they are among the best placed to evaluate how their work may be misused. They can identify and try to mitigate problems before they occur—not only through design choices but also through self-restraint in the diffusion and trade of the products of research and innovation. AI researchers may, for instance, decide not to share specific details about their research (the researchers that repurposed the drug-testing AI did not disclose the specifics of their experiment), while companies that develop AI products may decide not to develop certain features, restrict access to code that might be used maliciously, or add by-design security measures such as antitamper software, geofencing, and remote switches. Or they may apply the know-your-customer principle through the use of token-based authentication. Such measures will certainly not eliminate the risks of misuse entirely—and they may also have drawbacks—but they can at least help to reduce them. These measures can also help keep at bay potential governmental restrictions, for example on data sharing, which could undermine the openness of the field and hold back technological progress. To engage with the risks that the misuse of AI poses to peace and security, AI practitioners do not have to look further than existing recommended practices and tools for responsible innovation. There is no need to develop an entirely new tool kit or set of principles. What matters is that peace and security risks are regularly considered, particularly in technology-impact assessments. The appropriate risk-mitigation measures will flow from there. Responsible AI innovation is not a silver bullet for all the societal challenges brought by advances in AI. However, it is a useful and much-needed approach, especially when it comes to peace and security risks. It offers a bottom-up approach to risk identification, in a context where the multipurpose nature of AI makes top-down governance approaches difficult to develop and implement, and possibly detrimental to progress in the field. Certainly, it would be unfair to expect AI practitioners alone to foresee and to address the full spectrum of possibilities through which their work could be harmful. Governmental and intergovernmental processes are absolutely necessary, but peace and security, and thus all our safety, are best served by the AI community getting on board. The steps AI practitioners can take do not need to be very big, but they could make all the difference. Authors’ note: This post was drafted as part of a joint SIPRI-UNODA initiative on Responsible Innovation in AI, which is supported by the Republic of Korea. All content is the responsibility of the authors. Vincent Boulanin is a senior researcher at the Stockholm International Peace Research Institute (SIPRI). He leads SIPRI’s research on the development, use, and control of autonomous weapons systems and military artificial intelligence. His current work focuses on the responsible development and use of artificial intelligence. Charles Ovink is a political affairs officer at the United Nations Office for Disarmament Affairs (UNODA). He specializes in responsible innovation, the impact of emerging technologies on disarmament and non-proliferation, and the militarization of AI. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",0.0
"
        Supercomputer Emulator: AI’s New Role in Science
    ",https://spectrum.ieee.org/ai-for-science,2022-08-24 00:00:00.000000,,"Microsoft’s head of AI4Science sees machine learning partially supplanting simulation Chris Bishop is the director of Microsoft’s AI4Science initiative. Artificial intelligence has become an indispensable tool in many scientists’ lives, such that its use by researchers now has its own moniker—AI4Science—used by conferences and laboratories. Last month, Microsoft announced its own AI4Science initiative, employing dozens of people spread across several countries. Chris Bishop, its director, started on the science side before gravitating to AI. He earned a Ph.D. in quantum field theory at the University of Edinburgh, then worked in nuclear fusion before machine learning caught his eye in the 1980s. He began applying neural networks to his own work. “I was kind of 25 years early,” he says, “but it really has taken off.” He joined Microsoft Research’s Cambridge lab in 1997, eventually becoming its director, and now has a new role. We spoke about the evolution of the scientific method, lasers versus beer, and nerdy T-shirts. IEEE Spectrum:What is Microsoft AI4Science? Chris Bishop: All it really is is a new team that we’re building. We see a very exciting opportunity over the next decade at the intersection of machine learning and the natural sciences—chemistry, physics, biology, astronomy, and so on. It goes beyond simply the application of machine learning in the natural sciences. How does it go beyond that? Bishop: There was a technical fellow at Microsoft, Jim Gray, who talked about four paradigms of scientific discovery. The first paradigm is the purely empirical. It’s observing regularities in the world around us. “We see a new paradigm emerging. You can trace its origins back many decades, but it’s a different way of using machine learning in the natural sciences.”—Chris Bishop, Microsoft Research The second paradigm is the theoretical. Think of Newton’s laws of motion, or Maxwell’s equations. These are typically differential equations. It’s an inductive step, an assumption that they describe the world more generally. An equation is incredibly precise over many scales of length and time, and you can write it on your T-shirt. The third transformation in scientific discovery began in the middle of the 20th century, with the development of digital computers and simulations, effectively solving these differential equations for weather forecasting and other applications. The fourth paradigm, taking off in the 21st century, was not about using computers to solve equations from first principles. It’s rather analyzing empirical data at scale using computers. Machine learning thrives in that space. Think of the Large Hadron Collider, the James Webb Space Telescope, or protein-binding experiments. These four paradigms all work together. We see a new paradigm emerging. You can trace its origins back many decades, but it’s a different way of using machine learning in the natural sciences. In the third paradigm, you run a complicated simulation on a supercomputer; then the next day, somebody asks a different question. You take a deep breath, more coin to the electricity meter. We can now use those simulation inputs and outputs as training data for machine-learning deep neural nets, which learn to replicate or emulate the simulator. If you use the emulator many times, you amortize the cost of generating the training data and the cost of training. And now you have this hopefully fairly general-purpose emulator, which you can run orders of magnitude faster than the simulation. Roughly how much simulation data is needed to train an emulator? Bishop: A lot of machine learning is an empirical science. It involves trying out different architectures and amounts of data and seeing how things scale. You can’t say ahead of time, I need 56 million data points to do this particular task. What is interesting, though, are techniques in machine learning that are a little bit more intelligent than just regular training. Techniques like active learning and reinforcement learning, where a system has some understanding of its limitations. It could request more data where it has more uncertainty. What are emulation’s weaknesses? Bishop: They can still be computationally very expensive. Additionally, emulators learn from data, so they’re typically not more accurate than the data used to train them. Moreover, they may give insufficiently accurate results when presented with scenarios that are markedly different from those on which they’re trained. “I believe in “use-inspired basic research”—[like] the work of Pasteur. He was a consultant for the brewing industry. Why did this beer keep going sour? He basically founded the whole field of microbiology.”—Chris Bishop, Microsoft Research Are all of Microsoft AI4Science’s projects based on emulation? Bishop: No. We do quite a bit of work in drug discovery. That’s at the moment entirely fourth-paradigm-based. It’s based on empirical observations of the properties of certain molecules, and using machine learning to infer the properties of molecules that weren’t part of the training set, and then to reverse that process and say, given a set of properties, can we find new molecules which have those properties? We have a five-year research partnership with Novartis. What are some other projects you’re working on? Bishop: We’re looking actively at partnerships. Microsoft brings a couple of things. We have a lot of expertise in machine learning. We also have a lot of expertise in very-large-scale compute and cloud computing. What we’re not endeavoring to do, though, is to be domain experts. We don’t want to be a drug company, we don’t want to be an expert in catalysis. We are bringing in people who have expertise in quantum chemistry, quantum physics, catalysis, and so on, but really to allow us to build an interface with collaborators and partners. The bigger picture is we’re working anywhere we’ve got these differential equations. It could be fluid flows, designing turbines, predicting the weather, large-scale astronomical phenomena, plasma in nuclear reactors. A lot of our emphasis is on molecular-scale simulation. Scientifically, it holds some of the most challenging and some of the most interesting problems, but also the applicability is enormous—drug discovery, sustainability. We’ve been thinking about direct air capture of carbon dioxide. Is the goal to publish papers or to build intellectual property and products? Bishop: We have, I guess, three goals. First and foremost, it’s about building up our research. Peer-reviewed publication will be a key outlet. Second, Microsoft is a company whose business model is empowering others to be successful. So one of the things we’ll be looking for is how we can turn some of the research advances into cloud-based services which can then be used commercially or by the academic world. The breadth of applicability of this is potentially enormous. If you just think about molecular simulation, it’s drugs, it’s lubricants, it’s protecting corrosion, it’s carbon capture, it’s catalysis for the chemical industry, and so on. And then the third goal, ultimately, is to see real-world impact: Health care, sustainability, climate change. Do you foresee advances not just in the domains where you’re helping partners but also in pure computer science and machine learning? Bishop: That’s a great question. I believe in “use-inspired basic research.” People think in terms of a very linear model, in which you have basic research at one end and applied research at the other. A great example would be Einstein. He discovers stimulated emission with a pencil and paper and a brain, and then later it gets used to build the laser. But there’s a different kind of research, which is often characterized by the work of Pasteur. He was a consultant for the brewing industry. Why did this beer keep going sour? He basically founded the whole field of microbiology. I think about that as use-inspired basic research. I hope to see that as we go after really hard problems. We’re trying to build a neural net that can understand the dynamics of molecules, and we’re going to need new neural-network architectures. And that might spill over into completely different domains. What will the sixth scientific paradigm be? Will AI generate new hypotheses? I have no idea what the sixth paradigm is. But I think the fifth paradigm will keep us pretty busy for the next decade or more. This transcript has been edited for brevity and clarity. Matthew Hutson is a freelance writer who covers science and technology, with specialties in psychology and AI. He’s written for Science, Nature, Wired, The Atlantic, The New Yorker, and The Wall Street Journal. He’s a former editor at Psychology Today and is the author of The 7 Laws of Magical Thinking. Follow him on Twitter at @SilverJacket. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",0.0
"
        Is AI Good for Health Care?
    ",https://spectrum.ieee.org/ai-in-health-care,2022-08-23 00:00:00.000000,,"Industry experts weigh in on inequities and predictive analytics in a new podcast With the rush toward using tools such as artificial intelligence, machine learning, and deep learning technologies to analyze health data for insights, questions are being raised about how good a job the technologies are doing to improve outcomes. Technologists, clinicians, researchers, scientists, ethicists, policy stewards, and other experts offer their thoughts during the third season of the Re-Think Health Podcast, AI for Good Medicine. The series is part of the IEEE Standards Association’s Healthcare and Life Sciences practice. Season 3 features these episodes: The Balance—AI’s Healthcare Goodness for Marginalized Patients. IEEE Senior Member Sampathkumar Veeraraghavan, chair of the IEEE Humanitarian Activities Committee, covers whether AI and machine learning can support fairness, personalization, and inclusiveness or if they create even more inequity. Riding the Third Wave of AI for Precision Oncology. This episode features Nathan Hayes, founder and CEO of Modal Technology Corp., and scientist Anthoula Lazaris, director of the biobank at the McGill University Health Center Research Institute. They discuss how AI can improve patient outcomes. The experts also cover whether the full potential for precision oncology will be realized by using the “third wave of AI” with real-world data and practice. In the so-called third wave, AI systems are imagined to have humanlike communication and reasoning capabilities and be able to recognize, classify, and adapt to new situations autonomously. Advanced AI and Sensors—Reaching the Hardest to Reach Patients at Home. Sumit Kumar Nagpal, cofounder and CEO of Cherish Health—which develops sensors and artificial intelligence—discusses how the technologies can efficiently support the wellness needs of an aging population with dignity and integrity. AI—The New Pipeline for Targeted Drug Discovery.Maria Luisa Pineda, cofounder and CEO of Envisagenics, covers how splicing RNA using AI and high-performance computing could create a path to targeted drug discovery. RNA splicing is at the forefront of providing insights into diseases that are linked back to RNA errors. Using AI, high-performance computing, and the exponential amount of genetic data, researchers can develop the insights needed for targeted drug discovery in oncology and other genetic conditions faster and more accurately, Pineda says. Reducing the Healthcare Gap With Explainable AI.Dave DeCaprio, cofounder and CTO of ClosedLoop.ai, discusses health care disparities around the globe. Identifying and leveraging the social determinants of health can close the gap, DeCaprio says. Off-the-shelf AI programs present a new perspective on transparency and the reduction of bias, and they could build trust about the applications among health stakeholders. Getting Real About Healthcare Data and the Patient’s Journey. The time has come to unleash the value of unstructured data, says Alexandra Ehrlich, principal health innovation scientist at Oracle. AI and machine learning offer opportunities, but first the technologies must be demystified, Ehrlich says, adding that natural language processing can help. Ehrilich explores NLP applications as well as challenges with navigating bias throughout accessible health care data. Mind Your Data—The First Rule of Predictive Analytics in Clinical Research.Aaron Mann, senior vice president of data science at the Clinical Research Data Sharing Alliance, an IEEE–Industry Standards and Technology Organization alliance, discusses how open data sharing is paving the way for access to better quality, real-world, inclusive data. The sharing of data will enable predictive analytics to be more accurate, resourceful, and utilitarian in clinical research, Mann says. Can the Health System Benefit From AI as It Stands Today? While focusing on accuracy, ethics, and bias in AI algorithms, we cannot lose sight of the need for more validated data, says Dimitrios Kalogeropoulos. He is a health care expert with the European Commission, UNICEF, the World Bank, and the World Health Organization. Kalogeropoulos explores whether AI is good for medicine and whether medicine is good for AI. There’s plenty of bandwidth available if we use reconfigurable intelligent surfaces Ground level in a typical urban canyon, shielded by tall buildings, will be inaccessible to some 6G frequencies. Deft placement of reconfigurable intelligent surfaces [yellow] will enable the signals to pervade these areas. For all the tumultuous revolution in wireless technology over the past several decades, there have been a couple of constants. One is the overcrowding of radio bands, and the other is the move to escape that congestion by exploiting higher and higher frequencies. And today, as engineers roll out 5G and plan for 6G wireless, they find themselves at a crossroads: After years of designing superefficient transmitters and receivers, and of compensating for the signal losses at the end points of a radio channel, they’re beginning to realize that they are approaching the practical limits of transmitter and receiver efficiency. From now on, to get high performance as we go to higher frequencies, we will need to engineer the wireless channel itself. But how can we possibly engineer and control a wireless environment, which is determined by a host of factors, many of them random and therefore unpredictable? Perhaps the most promising solution, right now, is to use reconfigurable intelligent surfaces. These are planar structures typically ranging in size from about 100 square centimeters to about 5 square meters or more, depending on the frequency and other factors. These surfaces use advanced substances called metamaterials to reflect and refract electromagnetic waves. Thin two-dimensional metamaterials, known as metasurfaces, can be designed to sense the local electromagnetic environment and tune the wave’s key properties, such as its amplitude, phase, and polarization, as the wave is reflected or refracted by the surface. So as the waves fall on such a surface, it can alter the incident waves’ direction so as to strengthen the channel. In fact, these metasurfaces can be programmed to make these changes dynamically, reconfiguring the signal in real time in response to changes in the wireless channel. Think of reconfigurable intelligent surfaces as the next evolution of the repeater concept.",0.0
"
        AI Could Make Air Conditioners 10x Better
    ",https://spectrum.ieee.org/ai-3d-printing-better-ac,2022-08-18 00:00:00.000000,,"Hyperganic is using AI to design new heat exchangers that can be 3D-printed in metal Hyperganic partnered with Trumpf to algorithmically design this heat exchanger based on physical principles found in nature. They are now partnering with Krailling and Strata Manufacturing to build a more efficient residential air-conditioning system with a 3D-printed heat exchanger The energy we spend on cooling indoor spaces has tripled since 1990, and it’s going to triple again by 2050 as developing and middle-income countries embrace air-conditioning. Researchers are putting a lot of sweat into innovative cooling technologies that consume less energy, but none seem ready for prime time in the near future. “Air-conditioning innovation is like nuclear fusion, always 20 years in the future,” says Lin Kayser, CEO of Hyperganic, an AI-based design-software firm in Munich. Kayser wants to use AI and 3D printing to change that. By using AI to generate a radically new heat-exchanger design that can then be printed with a 3D metal printer, Hyperganic says it is developing a residential A/C unit that is 10 times as efficient as conventional air conditioners, while costing the same amount of money to buy and operate for a year. The company has teamed up with Krailling, Germany–based EOS, maker of metal and plastic 3D printers, and UAE-based manufacturing firm Strata Manufacturing. Air conditioners cool buildings by pumping indoor heat to the outside through the heat exchanger, or condenser unit, which compresses a refrigerant gas into liquid. Fans blow over the condenser to blow the heat released in the liquefaction process out into the air. Cooling consumes over 16 percent of the energy used by buildings today, with the heat exchanger being the most energy-intensive component of an A/C unit. Heat exchangers are structures that need a large surface area, for which they rely on complicated, curvy interior channels. But traditional engineering design and manufacturing is limited in the complexity it can deliver, and in fact, prefers simpler designs in order to keep costs down . “There are 10 A/C units being sold every second,” Kayser says. “But A/C units have looked the same for the last 30 years.” The complex designs needed for heat exchangers are sweet spots for both AI-based design and 3D printing, however. New prototype heat exchangers have, in fact, become the go-to for metal-printing companies that want to show off the technology’s prowess. Hyperganic’s mission is to “dramatically speed up innovation in the engineering of physical things.” Most of the innovation in the past few decades has been in the area of in information technology, he says, while cars, aircraft, and appliances remain much like their original selves. Hyperganic’s AI-based design platform allows engineers to make heat exchangers with radically different structures, using elements inspired by the intricate designs found in nature, like corals, Kayser says. By increasing the surface area and optimizing air flow, these designs boost the component’s energy efficiency. “We take knowledge from engineering about how to build a heat exchanger, but now you can do it automatically,” he says. “You can create, test, and iterate faster.” Hyperganic previously developed this complex 3D-printed aerospike rocket engine.Hyperganic Case in point: the world’s largest aerospike printed rocket engine revealed by Hyperganic and EOS in May. Aerospike engines are generally considered a tough engineering and manufacturing challenge. Hyperganic’s AI algorithms created hundreds of designs in days. The best design was printed on EOS’s laser powder bed fusion machine, which use lasers to heat and fuse metal powders one layer at a time to build parts. Air-conditioning innovation is a priority for a warming, energy-hungry world. Novel technologies like thermoelectric materials and passive radiative cooling, which sends heat directly into outer space, are exciting, but might take many years to be commercially viable. Hyperganic isn’t doing anything so radical that it is impractical in the short term. “What we’re doing is not rocket science,” Kayser says. “We’re trying to bring advanced manufacturing capabilities and AI together. It’s not as complicated as inventing something completely different.” He adds that the firm has many new designs for a new A/C heat exchanger already, and have performance data on some. They plan to have a prototype of ready by the U.N. Climate Change Conference in Dubai next year. Prachi Patel is a freelance journalist based in Pittsburgh. She writes about energy, biotechnology, materials science, nanotechnology, and computing. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",0.0
"
        GPT Language Model Spells Out New Proteins
    ",https://spectrum.ieee.org/gpt-2-language-model-proteins,2022-08-12 00:00:00.000000,,"Human speech and protein structure are close enough for AI purposes Human languages have much in common with proteins, at least in terms of computational modeling. This has led research teams to apply novel methods from natural-language processing (NLP) to protein design. One of these—Birte Höcker’s protein design lab at Bayreuth University, in Germany—describes ProtGPT2, a language model based on OpenAI’s GPT-2, to generate novel protein sequences based on the principles of natural ones. Just as letters from the alphabet form words and sentences, naturally occurring amino acids combine in different ways to form proteins. And protein sequences, just like natural languages, store structure and function in their amino-acid sequence with extreme efficiency. ProtGPT2 is a deep, unsupervised model that takes advantage of advances in transformer architecture that have also caused rapid progress in NLP technologies. The architecture has two modules, explains Noelia Ferruz, a coauthor of the paper and the person who trained ProtGPT2: one module to understand input text, and another that processes or generates new text. It was the second one, the decoder module that generates new text, that went into the development of ProtGPT2. Researchers have used GPT-2 to train a model to learn the protein “language,” generate stable proteins, and explore “dark” regions of protein space. “At the time we created this model, there were many others that were using the first module,” she says, such as ESM, ProtTrans, and ProteinBERT. “Ours was the first one publicly released at a time that was a decoder.” It was also the first time someone had directly applied GPT-2, she adds. Ferruz herself is a big fan of GPT-2. “I find it very impressive that there was a model capable of writing English,” she says. This is a well-known transformer model that was pretrained on 40 gigabytes of Internet text in English in an unsupervised manner—that is, it used raw text with no human labeling—to generate the next word in sentences. The GPT-x series has been shown to efficiently produce long, coherent text, often indistinguishable from something written by a human—to the extent that potential misuse is a concern. Given the capabilities of GPT-2, the Bayreuth researchers were optimistic about using it to train a model to learn the protein language, generate stable proteins, and also explore “dark” regions of the protein space. Ferruz trained ProtGPT2 on a data set of about 50 million nonannotated sequences across the whole protein space. To evaluate the model, the researchers compared a data set of 10,000 sequences generated by ProtGPT2 with a random set of 10,000 sequences from the training data set. “We could add labels, and potentially in the future start generating sequences with a specific function.”—Noelia Ferruz, University of Bayreuth, Germany They found the sequences predicted by the model to be similar in secondary structure to naturally occurring proteins. ProtGPT2 can predict proteins that are stable and functional, although, Ferruz says, this will be verified by laboratory experiments on a set of 30 or so proteins in the coming months. ProtGPT2 also models proteins that do not occur in nature, opening up possibilities in the protein design space. Each node represents a sequence. Two nodes are linked when they have an alignment of at least 20 amino acids and 70 percent HHsearch probability. Colors depict the different SCOPe classes, and ProtGPT2 sequences are shown in white.University of Bayreuth/Nature Communications The model can generate millions of proteins in minutes, says Ferruz. “Without further improvements, people could take the model, which is freely available, and fine-tune a set of sequences to produce more sequences in this region,” such as for antibiotics or vaccines. But also, she adds, with small modifications in the training process “we could add labels, and potentially in the future start generating sequences with a specific function.” This in turn has potential for uses in not just medical and biomedical fields but also in environmental sciences and more. Ferruz acknowledges the rapid developments in the NLP space for the success of ProtGPT2, but also points out that this is an ever-changing space—“It’s crazy, all the things that have happened in the last 12 months.” At the moment, she and her colleagues are already writing a review of their work. “I trained this model over Christmas [2021],” she says, “and at the time, there was another model that had been described...but it wasn’t available.” Yet by this spring, she says, other models had been released. ProtGPT2’s predicted sequences spanned new, rarely explored regions of protein structure and function. However, a few weeks ago, DeepMind released structures of over 200 million proteins. “So I guess we don’t have that much of a dark proteome anymore,” Ferruz says. “But still, there are regions…that haven’t been explored.” There is plenty of work ahead, though. “I would like to have control over the design process,” Ferruz adds. “We will need to take the sequence, predict the structure, and maybe predict the function if it has any….That will be very challenging.” Payal Dhar (she/they) is a freelance journalist on science, technology, and society. They write about AI, cybersecurity, surveillance, space, online communities, games, and any shiny new technology that catches their eye. You can find and DM Payal on Twitter (@payaldhar). Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",1.0
"
        Artificial Synapses 10,000x Faster Than Real Thing
    ",https://spectrum.ieee.org/artificial-synapses,2022-08-06 00:00:00.000000,,"New protonic programmable resistors may help speed learning in deep neural networks New artificial versions of the neurons and synapses in the human brain may be as small as one-thousandth the size of neurons and at least 10,000 times as fast as biological synapses, a study now finds. These new devices may help improve the speed at which the increasingly common and powerful artificial-intelligence systems known as deep neural networks learn, researchers say. In artificial neural networks, electrical components dubbed “neurons” are fed data and cooperate to solve a problem, such as recognizing images. The neural net repeatedly adjusts the links between its ersatz neurons and sees if the resulting patterns of behavior are better at finding a solution. Over time, the network discovers which patterns are best at computing results. It then adopts these as defaults, mimicking the process of learning in the human brain. A neural network is dubbed “deep” if it possesses multiple layers of neurons. Deep neural networks are increasingly finding use in applications such as analyzing medical scans,designing microchips, predicting how proteins fold, and empowering autonomous vehicles. The speed of thought in animals is typically limited to milliseconds—constrained by the weak voltages and watery medium in which neural signals are shuffled. However, artificial solid-state neurons and synapses are not hemmed in by these constraints. The amount of time, energy, and money needed to train deep neural networks is skyrocketing. One approach that researchers are pursuing to help overcome this challenge involves training brain-imitating deep neural networks on brain-mimicking hardware instead of conventional computers, a strategy called analog deep learning. Just as transistors are the core elements of digital computers, so too are neuron- and synapselike components the key building blocks in analog deep learning. In the new study, researchers experimented with artificial synapses called programmable resistors. The new programmable resistors are similar to memristors, or memory resistors. Both kinds of devices are essentially electric switches that can remember which state they were toggled to after their power is turned off. As such, they resemble synapses, whose electrical conductivity strengthens or weakens depending on how much electrical charge has passed through them in the past. Memristors are two-terminal devices, whereas the new programmable resistors are three-terminal devices, says study lead author Murat Onen, an electrical engineer at MIT. The research team’s programmable resistors increased or decreased their electrical conductance by moving protons around. To increase conductance, electric fields helped insert protons into the devices. To decrease conductance, protons were taken out. These protonic programmable resistors used an electrolyte similar to those found in batteries to let protons pass while blocking electrons. Their electrolyte was phosphosilicate glass, which the researchers suspected would possess high proton conductivity at room temperature. This glass accommodated many nanometer-size pores for proton transport and could also withstand very strong pulsed electric fields to help protons move quickly. “Previously the operation timescales were around milliseconds, whereas in this work we achieved nanoseconds.”—Murat Onen, MIT Unlike the organic Nafion electrolyte used in an earlier version of the team’s device, phosphosilicate is compatible with silicon fabrication techniques. This helped scale the devices “all the way down to 10-nanometer scale,” Onen says. In contrast, biological neurons are roughly 1,000 times as long. The speed at which biological neurons and synapses can process and transfer data is limited by the weak voltages and watery medium in which these signals are shuffled. Anything more than 1.23 volts causes liquid water to split into hydrogen and oxygen gas. As such, the speed of thought in animals is typically limited to millisecond timescales. In contrast, artificial solid-state neurons and synapses are not hemmed in by these constraints. However, it was unclear how fast they were compared with their biological counterparts. In experiments, the scientists found their protonic programmable resistors could perform at least 10,000 times as fast as biological synapses at room temperature. “The most surprising part was to see how fast we could move protons within solid media,” Onen says. “Previously the operation timescales were around milliseconds, whereas in this work we achieved nanoseconds.” The devices could run for millions of cycles without breaking down. Furthermore, the amount of heat they generated during computation compared well with that of human synapses. The insulating properties of the phosphosilicate glass meant that almost no electric current passed through the material as protons moved, making the gadgets highly energy efficient. “The primary technological implication is that we can now have protonic programmable devices for analog deep learning applications,” Onen says. ""Predecessors of such devices already had many promising qualities compared to competing technologies but were very slow, which meant they were not appropriate to be used in processors.” In addition, Onen says, “the discovery of ultrafast ion transport in solids could have broader implications beyond analog deep learning, whenever fast ion motion is required, such as in microbatteries, fuel cells, artificial photosynthesis, and electrochromism.” The scientists detailed their findings in the 29 July issue of the journal Science. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",0.0
"
        Help Not Wanted
    ",https://spectrum.ieee.org/autonomous-tractor,2022-07-21 00:00:00.000000,,"An autonomous farm tractor, photonic amplifiers, and more in this month's Big Picture The Big Picture features technology through the lens of photographers. Every month, IEEE Spectrum selects the most stunning technology images recently captured by photographers around the world. We choose images that reflect an important advance, or a trend, or that are just mesmerizing to look at. We feature all images on our site, and one also appears on our monthly print edition. Enjoy the latest images, and if you have suggestions, leave a comment below. Mechanical labor-saving implements spare farm workers some of the backbreaking tasks they once faced. But mechanization, for the longest time, went only so far. Workers still toil in the dirt from sunrise to sunset—withstanding the blazing sun, dust, pesticides, and injury-inducing repetitive motion—with no guarantee of bountiful harvests. Now, agriculture might be on the verge of seeing field hands eliminated from the crop-raising equation altogether. The machine pictured here, the AgBot from Dutch startup AgXeed, is designed to till, seed, feed, and weed a plot autonomously. A suite of sensors keeps the machine properly oriented on a farm plot and instantly detects obstacles (like the farmer’s daughter) that should be avoided. Sebastian Willnow/DPA/Getty images The tech industry’s bywords have long been smaller, faster, and more powerful. Steadily climbing the ranks in importance is efficiency. All these qualities have been the aim of research into photonics devices, which transmit signals with light instead of electricity. Engineers knew that, in theory, chip-scale photonic devices could outclass their electron-based counterparts. But heretofore, researchers had not teased out how to overcome these devices’ limited power output (which hastened signal attenuation) and their incompatibility with standard integrated circuit manufacturing procedures. That is, until a group from the Swiss Federal Institute of Technology in Lausanne zeroed in on the target. Pictured here is an array of erbium-doped photonic amplifier chips of the type the Swiss researchers invented. They boost signals a thousandfold due to their unprecedented output power. What’s more, each amplifier delivers this performance despite having a spiral waveguide that measures only 3.6 millimeters (about 0.14 inch) across. NIELS ACKERMANN/LABORATORY OF PHOTONICS AND QUANTUM MEASUREMENTS, EPFL Part of the difficulty in replicating the 1.3-kilogram lumps of gray matter packaged inside our skulls is that we still do not fully understand how the brain works. And how can we reverse engineer it without breaking it down and potentially wrecking its awesome but delicate hardware? Among the lingering questions: How does this portable supercomputer operate its tens of billions of switches on just 20 watts of power? Gaps in researchers’ understanding have not stopped them from continuing to make strides in probing brains. They’re steadily gathering more clues to how they can create machines that more closely mimic the gold standard in general performance computing and self-learning. One of the latest takeaways from years of listening in on the brain’s electrical signals and mapping them to observable behaviors or thought patterns is a system called Neuropixels. The brain implant, created by scientists and engineers at Imec, the nanoelectronics R&D institute in Belgium, and Howard Hughes Medical Institute, based in Maryland, comprises more than 10,000 electrodes. Together, they yield the most details we have ever gotten about the electrical transmissions taking place among thousands of neurons at once in any given thimbleful of brain tissue. ANATOMYBLUE",0.0
"
        Web3 Ride-Hailing App Drife Takes on Uber in India
    ",https://spectrum.ieee.org/blockchain-ridehailing-app-drife-takes-on-uber-in-india,2022-08-25 00:00:00.000000,," The Ethereum-blockchain-based app already has thousands of riders and drivers, but challenges remain Drife’s cofounders are chief operating officer Surya Ranjith [left], chief executive officer Firdosh Sheikh [middle], and chief technology officer Mudit Marda [right]. Ride-hailing in India is dominated by Uber and local rival Ola, but startup Drife thinks blockchain technology could be the key to breaking up their duopoly. CEO and cofounder Firdosh Sheikh used to be a power user of Uber, racking up more than 5,000 rides in total. But after speaking to drivers she discovered many were unhappy with the commissions the platform charged and the often opaque rules that governed which rides they get assigned. Riders also get a raw deal, she says, with little control over what they pay for their rides or who they travel with. The problem, Sheikh decided, boiled down to having a middleman controlling the relationship between riders and drivers. So she set out to build a decentralized ride-hailing platform that puts control back into the hands of users via blockchain technology. Last November, after three years of development, she and her cofounders launched the Drife app in the southern city of Bangalore. Today, they have more than 10,000 drivers and 100,000 riders signed up to the platform. One of the main things that differentiates them from companies like Uber, says Sheikh, is that they don’t charge any commission. Drivers get to keep the entire fare and will instead pay a monthly subscription to use the platform, although the company is currently waiving this to encourage sign-ups. “We don’t charge anything from the fare that you pay as a rider,” she says. “A centralized entity who has a profit motive in the fare that I pay as a rider will start manipulating the fare and exploiting it for their own profit motive, and that’s where both drivers and riders struggle.” A base fare is set based on the class of vehicle and the distance of the trip, but after that riders can boost the amount they are willing to pay to attract more drivers. Drivers can also make counteroffers. The rider then chooses who to go with based either on price or driver ratings. This means pricing is purely market-driven, and because Drife isn’t taking a cut, fares should be cheaper than alternatives, says Sheikh. The company also has its own cryptocurrency, called DRF. At present it can’t be used for much, but the token will play an important role in Drife’s expansion plans, says Sheikh. The company plans to operate on a franchise model, with local entrepreneurs bidding to run Drife operations in new cities in return for a share of subscription fees. But to apply for a franchise, they will need to purchase a large chunk of DRF tokens and lock them up for the duration of their contract. So far the company has received about 60 franchise requests from cities across the world, says Sheikh. About 30 percent of DRF tokens have also been reserved for an “ecosystem fund,” which will be used for incentives and rewards for drivers and riders. Besides being tradable for real money, these tokens will also confer the right to vote in a decentralized autonomous organization (DAO)—a kind of member-run organization whose internal rules are encoded into a blockchain. Each city will have their own DAO, which will be responsible for choosing franchisees. “Nothing that we see today can work at the scale that we want to grow,” says Sheikh. “That’s why we’ve started our own side project where we’re working on a blockchain customized to our own needs.” Crucially, users won’t need to take Drife’s word for any of this, as the entire system will be governed by smart contracts. These are software programs that live inside a blockchain and automate transactions according to predefined rules that are visible to everyone. “Nobody’s going to trust me if I say I don’t manipulate it unless I show them that I don’t have any power to manipulate that data,” says Sheikh. “That’s only possible through blockchain.” In reality, however, Drife is building the plane while flying it. The firm’s smart-contract system is built on Polygon, which is an extension of the Ethereum blockchain. But because they are constantly tweaking features and functionality, Sheikh admits that most of the time operations are actually running on a back-end server that mimics the processing the blockchain is supposed to do. And many of the details about how the platform will work still need to be ironed out. How to ensure the DAO voting system isn’t dominated by those who hold the most tokens is a work in progress, says Sheikh. Most blockchains are also geared toward settling financial transactions, and it’s not clear if they can cope with the volume of real-world data involved in running a large-scale ride-hailing business. “Nothing that we see today can work at the scale that we want to grow,” says Sheikh. “That’s why we’ve started our own side project where we’re working on a blockchain customized to our own needs.” It might still be some time before that becomes a problem. Despite a promising number of sign-ups, the company is currently doing only around 7,000 rides a week, compared to the millions done by Uber and Ola every day. Mobility consultant Vinay Piparsania, founder of MillenStrat Advisory & Research, says that blockchain technology holds considerable promise for disrupting the ride-hailing industry and breaking the monopolies of the big players. The biggest challenge for startups like Drife, though, is matching their financial and operational capabilities. “Unfortunately, at this time, such driver-focused startups are much too small and fragmented to make the difference to the duopolic might of Uber and Ola,” he says. It’s a David and Goliath situation, Piparsania adds, so for the time being these companies should focus on “nibbling away in some key towns and categories by attracting and holding onto drivers, and actually demonstrating to riders that they can compete on delivery.” Edd Gent is a freelance science and technology writer based in Bangalore, India. His writing focuses on emerging technologies across computing, engineering, energy and bioscience. He's on Twitter at @EddytheGent and email at edd dot gent at outlook dot com. His PGP fingerprint is ABB8 6BB3 3E69 C4A7 EC91 611B 5C12 193D 5DFC C01B. His public key is here. DM for Signal info. To decarbonize road transport we need to complement EVs with bikes, rail, city planning, and alternative energy China has more EVs than any other country—but it also gets most of its electricity from coal. EVs have finally come of age. The total cost of purchasing and driving one—the cost of ownership—has fallen nearly to parity with a typical gasoline-fueled car. Scientists and engineers have extended the range of EVs by cramming ever more energy into their batteries, and vehicle-charging networks have expanded in many countries. In the United States, for example, there are more than 49,000 public charging stations, and it is now possible to drive an EV from New York to California using public charging networks. With all this, consumers and policymakers alike are hopeful that society will soon greatly reduce its carbon emissions by replacing today’s cars with electric vehicles. Indeed, adopting electric vehicles will go a long way in helping to improve environmental outcomes. But EVs come with important weaknesses, and so people shouldn’t count on them alone to do the job, even for the transportation sector.",0.0
"
        5G Networks Are Worryingly Hackable
    ",https://spectrum.ieee.org/5g-virtualization-increased-hackability,2022-08-24 00:00:00.000000,," A shift to the cloud is opening the industry up to new attacks Prominent tech firms like Microsoft and NEC have recently expressed concerns over the security and perhaps too-rapid adoption, respectively, of critical 5G technologies. Now German security researchers have given some substance to the industry’s fears and unease. At a hacker conference held in the Netherlands last month, Karsten Nohl, founder of Berlin-based Security Research Labs, outlined how his team had breached live 5G networks in a series of “red teaming” exercises—where hackers are hired by a company to test their defenses. In most cases they were able to take control of the network, he says, potentially allowing them to steal customer data or disrupt operations. The hacks, revealed at the May Contain Hackers 2022 event (a.k.a. MCH2022), were made possible thanks to poorly configured cloud technology, which is playing an increasingly important role in 5G networks. Nohl says many telcos are inexperienced in how to protect such systems, and his team found that operators had failed to apply basic cloud security techniques that could help mitigate hacks. The push toward Open RAN, virtualization, and “cloudifcation” unlocks more choice and functionality for 5G operators. It has also thrust them into the unfamiliar role of system integrator, suddenly responsible for securing the entire supply chain. “5G has swept over telcos with all its implications, and nobody seems well prepared,” says Nohl. “We are introducing new technology into mobile networks, and those technologies can greatly enhance the security of our mobile networks. Or they can basically destroy any hacking resistance we’ve built up over the years. People are not aware of those choices.” Mobile operators have traditionally relied on proprietary hardware from vendors like Ericsson, Nokia, and Huawei to build their networks. But in recent years, there has been a major push to “virtualize” network functions, which involves replicating key components in software so they can run on generic hardware, or even in the cloud. And the advent of 5G has only heightened the demand for virtualization, in particular when it comes to radio access networks (RANs)—the part of the network involved in connecting end-user devices like cellphones to the network core. Virtualization has a host of benefits, including the ability to deploy networks faster and more cheaply, to quickly upgrade networks, and even to dynamically reconfigure them in response to changing situations on the ground. The decoupling of hardware and software also prevents vendor lock-in and allows network operators to mix and match components from different companies, something advocated for by the Open RAN movement. But these new capabilities are also making 5G networks more complex, says Nohl, which in turn necessitates the increasing use of automation to manage networks. And the ability to mix and match software and services from different companies means far more people are involved in the development pipeline. “The more stuff you have and the more moving parts, the more opportunities for mistakes, little misconfigurations,” says Nohl. This makes it much easier to break into such virtualized networks than was previously possible. Among the entry points the team discovered included a backdoor-revealing API that had been posted publicly to the Internet as well as an old development site that had accidentally been left online. But the increased ease with which attackers can penetrate the networks is not in and of itself the main problem. “The really critical question is how difficult it is to break through from your initial foothold to something actually valuable within the network,” says Nohl. His team found it was worryingly easy to move deeper into the networks they tested, thanks primarily to poorly configured “containers.” These are self-contained packages of software that bundle up an application and everything needed to run it—code, software libraries, and configuration files—so that it can be run on any hardware. Containers are a critical part of the cloud, because they allow different applications from different companies or departments to run alongside one another on the same servers. Containers are supposed to be isolated from one another, but if they are poorly configured it’s possible to break out and gain access to other containers or even to take control of the host system. In multiple instances Nohl and his team found misconfigured containers that allowed them to do just this. “I saw it many times when security teams were invited to the party when all is done and almost finished. The security guys have a very short time slot in order to fine-tune it—if they are actually allowed to touch it.”—Dmitry Kurbatov, SecurityGen Some of the above difficulties could be attributed to the fact that telcos are inexperienced when it comes to cloud security, says Nohl. But they also may be taking shortcuts. Often operators are “lifting and shifting” preexisting software components into containers, Nohl said, but many of the settings designed to isolate containers from one another prevent the software from working as it should. Rather than rewriting code, developers often simply remove these protections, says Nohl. “Those shortcuts we see everywhere now,” he says. “Network operators are having to move into a new operating model that’s significantly different than what they’ve done in the past,” says Eric Hanselman, chief analyst at 451 Research. “The reality is that telcos have never had to deal with these levels of software development or low-level infrastructure management before. They always rely on their suppliers for this.” While the shift toward Open RAN and the growing virtualization and “cloudifcation” of networks is unlocking more choice and functionality for operators, says Hanselman, it has also thrust them into the unfamiliar role of system integrator, responsible for securing the entire supply chain. Xavier Costa Pérez, head of 5G networks R&D at NEC Laboratories Europe, disputes that operators are behind the curve when it comes to 5G security. While he admits that the transition to more virtualized networks entails inevitable risks, he says major players are investing heavily in security and partnering with cloud providers to tap into their security expertise. “I think the telco industry is very much aware that this can be a big issue,” he says. “It’s critical for survival, so I don’t think it’s taken lightly at all.” It’s also important to remember that these kinds of highly virtualized networks still represent only a small portion of the 5G infrastructure today, probably less than 10 percent, says Costa Pérez. And all operators have backup 4G networks that they can switch to in the event of any problems. Network operators aren’t complacent, says Ian Smith, security operations director at the industry body GSMA. “We know that maintaining security, especially for new network technologies, is an ongoing and evolving effort, and one to which the mobile industry is wholeheartedly committed.” However, that hasn’t been the experience of Dmitry Kurbatov, cofounder of telecom security startup SecurityGen. He has found that security often appears to be an afterthought, rather than being part of the development process from the start. “I saw it many times when security teams were invited to the party when all is done and almost finished.” he says. “The security guys have a very short time slot in order to fine-tune it—if they are actually allowed to touch it.” Nonetheless, he’s optimistic about the shift to 5G. Previously, operators had little option but to trust vendors when it came to security, but now they will be able to take matters into their own hands. “You actually can have full visibility and control over [5G] systems and functions, which means now you have the chance as the network owner to be much more secure,” he says. And even more important, the industry isn’t alone in going through the transition to the cloud, says John Carse, chief information security officer at the Japanese operator Rakuten Mobile, which has been a champion of Open RAN principles. “This is a good thing because it means telecom doesn’t have a special problem to solve,” he says. “Telecom can benefit from adoption of techniques happening in all the industries surrounding it versus trying to overcome proprietary challenges.” Edd Gent is a freelance science and technology writer based in Bangalore, India. His writing focuses on emerging technologies across computing, engineering, energy and bioscience. He's on Twitter at @EddytheGent and email at edd dot gent at outlook dot com. His PGP fingerprint is ABB8 6BB3 3E69 C4A7 EC91 611B 5C12 193D 5DFC C01B. His public key is here. DM for Signal info. There’s plenty of bandwidth available if we use reconfigurable intelligent surfaces Ground level in a typical urban canyon, shielded by tall buildings, will be inaccessible to some 6G frequencies. Deft placement of reconfigurable intelligent surfaces [yellow] will enable the signals to pervade these areas. For all the tumultuous revolution in wireless technology over the past several decades, there have been a couple of constants. One is the overcrowding of radio bands, and the other is the move to escape that congestion by exploiting higher and higher frequencies. And today, as engineers roll out 5G and plan for 6G wireless, they find themselves at a crossroads: After years of designing superefficient transmitters and receivers, and of compensating for the signal losses at the end points of a radio channel, they’re beginning to realize that they are approaching the practical limits of transmitter and receiver efficiency. From now on, to get high performance as we go to higher frequencies, we will need to engineer the wireless channel itself. But how can we possibly engineer and control a wireless environment, which is determined by a host of factors, many of them random and therefore unpredictable? Perhaps the most promising solution, right now, is to use reconfigurable intelligent surfaces. These are planar structures typically ranging in size from about 100 square centimeters to about 5 square meters or more, depending on the frequency and other factors. These surfaces use advanced substances called metamaterials to reflect and refract electromagnetic waves. Thin two-dimensional metamaterials, known as metasurfaces, can be designed to sense the local electromagnetic environment and tune the wave’s key properties, such as its amplitude, phase, and polarization, as the wave is reflected or refracted by the surface. So as the waves fall on such a surface, it can alter the incident waves’ direction so as to strengthen the channel. In fact, these metasurfaces can be programmed to make these changes dynamically, reconfiguring the signal in real time in response to changes in the wireless channel. Think of reconfigurable intelligent surfaces as the next evolution of the repeater concept.",0.0
"
        Software Turns Promise Up for Offshore Wind
    ",https://spectrum.ieee.org/vertical-axis-wind-turbine,2022-08-24 00:00:00.000000,,"Next-generation towerless turbines are lightweight, more efficient, and cheaper Sandia’s design for offshore wind turbines eliminates the heavy tower and instead uses wind blades pulled taut with guy wires. A radical new idea for offshore wind turbines would replace tall unwieldy towers that had blades on top with lightweight, towerless machines whose blades resemble the loops of a whisk. Now new software can help optimize these unusual designs to help make them a reality, researchers say. This new work comes as the U.S. government plans to boost offshore wind energy. In March, the White House announced a national goal to deploy 30 gigawatts of new offshore wind power by 2030. The federal government suggested this initiative could help power more than 10 million homes, support roughly 77,000 jobs, cut 78 million tonnes in carbon emissions, and spur US $12 billion in private investment per year. As part of this new plan, in June, the White House and eleven governors from along the East Coast launched a Federal-State Offshore Wind Implementation Partnership to further develop the offshore wind supply chain, including manufacturing facilities and port capabilities. One reason offshore wind is attractive is the high demand for electricity on the coasts. People often live far away from where onshore wind is the strongest, and there is not enough space in cities for enough solar panels to power them, says Ryan Coe, a mechanical engineer in Sandia National Laboratories’ water-power group in Albuquerque. The guy wires can be shortened or lengthened to adjust the height of the blades in response to wind conditions, much as modifying the amount of tension in a bowstring can help change the curve of a bow. However, nearly 60 percent of the accessible offshore wind in the United States blows across water more than 60 meters deep. At such depths, it would prove very expensive to build a foundation for wind turbines on the seafloor. However, this deeper offshore wind energy remains attractive, as it roughly equals the entire annual electricity consumption in the United States. Wind turbines that can float above the seafloor could help play a key role in renewable energy. However, floating offshore wind turbines face their own challenges, such as their cost. “Floating offshore wind is currently estimated to be three to five times more expensive than land-based wind in the United States,” says mechanical engineer Brandon Ennis, Sandia’s offshore wind technical lead in Albuquerque. The aim of the U.S. Department of Energy’s Advanced Research Project Agency–Energy’s ATLANTIS project is to optimize the design of floating offshore wind turbines to maximize their power while minimizing their expense. The new, towerless design might help reduce both their mass and cost, Ennis says. When it comes to land-based wind power, the turbines themselves represent about 65 percent of their levelized cost of energy—that is, their lifetime costs divided by energy production. In comparison, when it comes to floating offshore wind power, the turbines represent only about 20 percent of their levelized costs, Ennis says. Instead, their floating platforms are the biggest factors behind these costs. “Given this difference in contribution to the levelized cost of energy of the turbine itself, it makes sense that the turbine design for floating offshore wind could be radically different than what is optimal on land,” Ennis says. Most land-based wind turbines nowadays are horizontal-axis machines. They each possess a tower with blades that spin on a horizontal shaft, which cranks a generator behind the blades in the turbine’s nacelle, the box at the top of the turbine. In contrast, the new design is a vertical-axis wind turbine (VAWT). These can resemble revolving doors—they each possess blades that spin on a vertical shaft, with a generator below the blades. Placing a wind turbine’s massive generator at the base of the machine instead of high up on a tower makes it much less top heavy, which reduces the size and cost of the floating platform needed to keep it afloat. One challenge when it comes to VAWTs is protecting them from extreme winds. Traditional horizontal-axis wind turbines (HAWTs) can rotate away from intense, damaging winds, but previous VAWTs catch wind from every direction. The new design replaces the central vertical tower often used in previous VAWTs with a set of taut wires. These wires can be shortened or lengthened to adjust the height of the blades in response to wind conditions, much as modifying the amount of tension in a bowstring can help change the curve of a bow. This new design can help maximize each wind turbine’s energy capture while controlling strain. In addition, replacing the shaft with wires further reduces the weight of the turbine, allowing the floating platform to be significantly smaller and less expensive. Previous VAWTs with towers often weighed more than HAWTs. In contrast, the new towerless design may weigh less than traditional HAWT designs, Ennis says. A historical photo shows Sandia’s experimental 34-meter-diameter vertical-axis wind turbine, built in Texas in the 1980s.Randy Montoya/Sandia National Laboratories Less mass above the water level requires less mass below the water level, leading to a lighter, cheaper platform, Ennis notes. “The optimal wind energy system would remove all mass and cost that is not directly capturing energy from the wind, and the towerless VAWT design concept moves toward that goal by reducing the turbine mass and the associated platform mass,” Ennis says. Sandia filed a patent application for the new design in 2020. However, a key challenge with all VAWTs is how they were not the focus of research as the wind industry steadily grew for the past 30 years, Ennis says. As such, where HAWTs possess software to help in their design, VAWTS did not. Now Sandia researchers have developed a new tool to model the way in which VAWTs and their floating platforms might respond to different wind and sea conditions. This new Offshore Wind Energy Simulator (OWENS) can help optimize the design and control of these machines. “Without a tool like ours, there cannot be a floating offshore VAWT industry, and it has been exciting to see our team advance that capability in such a short time compared to the historical development of design tools in the wind industry,” Ennis says. The OWENS tool can also simulate land-based VAWTs. Moreover, with some modifications, it can model unconventional HAWTs, such as bi-wing blades or shrouded concepts, as well as cross-flow marine hydrokinetic turbines, which are essentially VAWTs submerged in water instead of air, Ennis says. Popular misinformation does exist concerning VAWTs, Ennis says. “People claim that VAWTs have lower aerodynamic performance,” he says. “That’s wrong. Lift-based VAWTs are predicted to have the ability to slightly exceed the maximum efficiency of a horizontal-axis wind turbine.” Ennis does note that “VAWTs that were commercialized in the ’90s had fatigue issues. They also used aluminum blades with bolted joints, which is almost the worst material decision you could make for fatigue, and the design standards weren’t sufficient to characterize the operational life of a wind turbine.” However, modern designs can overcome these concerns, he says. The researchers are validating OWENS with a land-based 34-meter-diameter VAWT built at Sandia in the ’80s. This research can eventually help certify VAWT models against pertinent design standards. They plan to submit their findings to the journal Wind Energy Science. The scientists hope to have an optimized floating VAWT design by the end of the year, Ennis says. They next aim to have a small physical demo machine to verify its performance. “Vertical-axis wind turbines offer some meaningful advantages over traditional horizontal-axis wind turbines, particularly for floating offshore wind energy,” Ennis says. “With our new design software, we are in a good position to evaluate just how significant these advantages might be for our towerless VAWT system.” Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. Utrecht leads the world in using EVs for grid storage The Dutch city of Utrecht is embracing vehicle-to-grid technology, an example of which is shown here—an EV connected to a bidirectional charger. The historic Rijn en Zon windmill provides a fitting background for this scene. Hundreds of charging stations for electric vehicles dot Utrecht’s urban landscape in the Netherlands like little electric mushrooms. Unlike those you may have grown accustomed to seeing, many of these stations don’t just charge electric cars—they can also send power from vehicle batteries to the local utility grid for use by homes and businesses. Debates over the feasibility and value of such vehicle-to-grid technology go back decades. Those arguments are not yet settled. But big automakers like Volkswagen, Nissan, and Hyundai have moved to produce the kinds of cars that can use such bidirectional chargers—alongside similar vehicle-to-home technology, whereby your car can power your house, say, during a blackout, as promoted by Ford with its new F-150 Lightning. Given the rapid uptake of electric vehicles, many people are thinking hard about how to make the best use of all that rolling battery power.",0.0
"
        Top Programming Languages 2022
    ",https://spectrum.ieee.org/top-programming-languages-2022,2022-08-23 00:00:00.000000,,"Python’s still No. 1, but employers love to see SQL skills Welcome to IEEE Spectrum’s ninth annual ranking of the Top Programming Languages! This year we’ve revamped and streamlined our interactive ranking tool and made other changes under the hood, but the goal remains the same—to combine multiple metrics from different sources to estimate the relative popularity of different languages. You can get into the details of our methodological changes below (the TL;DR is that we eliminated the need for folks to run a giant ball of floating-point math in their browser), but first let’s get into what the rankings tell us this year. Python remains on top but is closely followed by C. Indeed, the combined popularity of C and the big C-like languages—C++ and C#—would outrank Python by some margin. Java also remains popular, as does Javascript, the latter buoyed by the ever-increasing complexity of websites and in-browser tools (although it’s worth noting that in some quarters, the cool thing is now deliberately stripped-down static sites built with just HTML and simple CSS). But among these stalwarts is the rising popularity of SQL. In fact, it’s at No. 1 in our Jobs ranking, which looks solely at metrics from the IEEE Job Site and CareerBuilder. Having looked through literally hundreds and hundreds of job listings in the course of compiling these rankings for you, dear reader, I can say that the strength of the SQL signal is not because there are a lot of employers looking for just SQL coders, in the way that they advertise for Java experts or C++ developers. They want a given language plus SQL. And lots of them want that “plus SQL.” It may not be the most glamorous language...but some experience with SQL is a valuable arrow to have in your quiver. This is likely because so many applications today involve a front-end or middleware layer talking to a back-end database, often over a network to eliminate local resource constraints. Why reinvent the wheel and try to hack your own database and accompanying network interface protocol when so many SQL implementations are available? Chances are there’s probably already one that fits your use case. And even when a networked back end isn’t practical, embedded and single-board computers can be found with enough oomph to run a SQL database locally. (For more on the rise of SQL, see our accompanying article.) So it may not be the most glamorous language or what you’re going to use to implement the next Great Algorithm, but some experience with SQL is a valuable arrow to have in your quiver. Looking at complete jobs listings also shows that if you’re interested in cyberwarfare (both offensive and defensive), then getting handy with assembly code is a pretty good in. Previously, I generally just associated assembly code with things like device drivers, tweaking the guts of operating systems, or retrocomputing. But many of the job listings calling for expertise in assembly were posted by the kinds of low-profile cybersecurity contractors that orbit Washington, D.C., and even one government agency—the NSA. Job listings are of course not the only metrics we look at in Spectrum. A complete list of our sources is here, but in a nutshell we look at nine metrics that we think are good proxies for measuring what languages people are programming in. Sources include GitHub, Google, Stack Overflow, Twitter, and IEEE Xplore. The raw data is normalized and weighted according to the different rankings offered—for example, the Spectrum default ranking is heavily weighted toward the interests of IEEE members, while Trending puts more weight on forums and social-media metrics. In previous years, we allowed readers to bypass these preset rankings and create a custom ranking by adjusting the weights however they pleased. However, it turned out that very few people were taking advantage of this feature. Taking it out allows us to precompute the preset rankings instead of serving an app that contained the data from all the metrics and then computed the rankings in the browser on the fly. Quite apart from making the app large, and thus slower to load, we also ran into the problem that different browsers could produce slightly different results, thanks to variations in floating-point implementations! (This problem of different implementations giving different results was largely solved by the IEEE-754 standard for floating-point numbers, so it would be interesting to go back and find out which browser versions are noncompliant. But for now let’s just all agree to not run any, say, nuclear reactors with a Web app, okay?) Creating the rankings also pulls us into the typical dilemmas faced by any taxonomist—you might think you’ve got a straightforward and unambiguous way to classify things, but then edge cases and weird hybrids invariably slither into view. Plus there’s the eternal struggle between “lumping” and “splitting”—is it best to focus on similarities and thus put multiple things under fewer labels overall, or focus on differences and break things up as much as possible and have more fine-grained labels? For us this question starts with considering just what is a programming language. This causes a lot of folks to shout at us, especially with regard to HTML/CSS. Although not Turing-complete except under highly artificial conditions, we do consider HTML/CSS a programming language because the tags in its source code are primarily intended as imperatives to do things such as “present this text in a table format” or “make this heading larger than the body text.” Another question that crops up is when do you decide that a superset or subset of one language has become distinct enough to be considered separately? Generally, we let pragmatism be our guide. You can argue that Arduino microcontrollers are programmed in a subset of Java (or C++), but if someone asked for help writing an Arduino project, giving them a book on Java would be of limited use. On the other hand, there are a lot of books with titles along the lines of Writing Arduino Programs Made EZ, so the Arduino language is listed separately. On the other hand, it doesn’t seem to make much sense to put, say, MicroPython and CircuitPython anywhere but firmly in the Python box, at least for now. Categorizations evolve. For example, previously we grouped Typescript with JavaScript, but adoption has grown enough that it makes more sense to break it out. These are all essentially subjective decisions, as are the weights we assign to different metrics, so your mileage may vary. We simply offer this up as our approach to a tricky problem—after all, no one can directly measure what languages people are programming in. Stephen Cass is the special projects editor at IEEE Spectrum. He currently helms Spectrum's Hands On column, and is also responsible for interactive projects such as the Top Programming Languages app. He has a bachelor's degree in experimental physics from Trinity College Dublin. A cautionary tale of NFTs, Ethereum, and cryptocurrency security On 4 September 2018, someone known only as Rabono bought an angry cartoon cat named Dragon for 600 ether—an amount of Ethereum cryptocurrency worth about US $170,000 at the time, or $745,000 at the cryptocurrency’s value in July 2022. It was by far the highest transaction yet for a nonfungible token (NFT), the then-new concept of a unique digital asset. And it was a headline-grabbing opportunity for CryptoKitties, the world’s first blockchain gaming hit. But the sky-high transaction obscured a more difficult truth: CryptoKitties was dying, and it had been for some time.",0.0
"
        How We Judge the Top Programming Languages
    ",https://spectrum.ieee.org/top-programming-languages-methods,2022-08-23 00:00:00.000000,,"The methodology behind our rankings Our interactive ranking of the most popular programming languages was first created by data journalist Nick Diakopoulos in 2013. The current version is maintained by IEEE Spectrum senior editor Stephen Cass with development support from Preeti Kulkarni and Michael Novakovic. As no one can look over the shoulders of every programmer, we have chosen metrics that we believe are reasonable proxies of popularity. By combining metrics to synthesize a single ranking we hope to even out statistical fluctuations, and changing the weights given to different metrics as they’re combined lets us emphasize different aspects, such as what's popular with employers in our Jobs ranking. Data is gathered through a combination of manual collection and APIs and combined using an R script. We originally started with a list of over 300 programming languages gathered from GitHub. We looked at the volume of results found on Google when we searched for each one using the template “X programming” where “X” is the name of the language. We then filtered out languages that had a very low number of search results, and followed that by going through the remaining entries by hand to narrow them down to the most interesting. Since then, each year we review the list as new languages find their footing and others slip into obscurity. Our final set of 57 languages includes names familiar to most computer users, such as Java, stalwarts like Cobol and Fortran, and languages that thrive in niches, like Haskell. The Processing language was dropped from our rankings this year because its name is a common word even within programming. Its generic name makes it hard to separate out when the word “processing” is referring specifically to the language (unlike, say, Python, which is a common word generally but nearly always refers to the language within a programming context). Before we scrubbed it from the list, Processing’s score, and thus its ranking, seemed artificially high for a niche language. We hope to attack this problem in next year’s rankings. We gauged the popularity of languages using the following sources for a total of nine metrics. We measured the number of hits for each language by using search for the template “X programming.” This number indicates the volume of online information resources about each programming language. We took the measurement in August 2022, so it represents a snapshot of the Web at that particular moment in time. This data was gathered manually. We measured the number of hits on Twitter for the template “X programming” for the 7.5 months from January 2022 to mid-August 2022 using the Twitter Search API. This number indicates the amount of chatter on social media for the language and reflects the sharing of online resources like news articles or books, as well as physical social activities such as hackathons. Stack Overflow is a popular site where programmers can ask questions about coding. We measured the number of questions posted that mention each language for the 12 months ending August 2022. Each question is tagged with the languages under discussion, and these tags are used to tabulate our measurements using the Stack Exchange API. Reddit is a news and information site where users post links and comments. On Reddit, we measured the number of posts mentioning each of the languages during the period spanning September 2021 and August 2022, using the template “X programming” across any subreddit on the site. We collected data using the Reddit API. IEEE maintains a digital library with over 3.6 million conference and journal articles covering a wide array of scientific and engineering disciplines. We measured the number of articles that mention each of the languages in the template “X programming” for the years 2021 and 2022. This metric captures the prevalence of the different programming languages as used and referenced in scholarship. We collected data using the IEEE Xplore API. We measured the demand for different programming languages in job postings on the IEEE Job Site. The IEEE Jobs Site has a large number of non-U.S. listings. Because some of the languages we track could be ambiguous in plain text—such as D, Go, J, Ada, and R—we searched for job listings with those words in the job description and then manually examined listings. When the number of listings returned was greater than 500, 200 of the listings were examined as a sample, and the result was used to calculate the total number of matching jobs. The search was conducted in August 2022. We measured the demand for different programming languages on the CareerBuilder job site. CareerBuilder listings were those offered within the United States. Because there is no publicly available API, we manually searched for listings that included each language. Some of the languages we track could be ambiguous in plain text—such as Go, J, and R—so we manually inspected each listing to remove false positives (for example, listings looking for experience with the Americans With Disabilities Act rather than the Ada programming language.). When more than 200 results were returned, 200 of the listings were examined as a sample, and the result was used to calculate the total number of matching jobs. The search was conducted in August 2022. GitHub is a public repository for many volunteer-driven open-source software projects, and so indicates what languages coders choose to work in when they have a personal choice. We use looked at two metrics from GitHub: repositories that have been “starred” by users, which reflects long-term interests, and the number of pull requests, which indicates current activity. We used data gathered by GitHut 2.0, which measures the top 50 languages used by number of repositories tagged with that language and draws from GitHub's public API. The data covers the first quarter of 2022. Stephen Cass is the special projects editor at IEEE Spectrum. He currently helms Spectrum's Hands On column, and is also responsible for interactive projects such as the Top Programming Languages app. He has a bachelor's degree in experimental physics from Trinity College Dublin. A cautionary tale of NFTs, Ethereum, and cryptocurrency security On 4 September 2018, someone known only as Rabono bought an angry cartoon cat named Dragon for 600 ether—an amount of Ethereum cryptocurrency worth about US $170,000 at the time, or $745,000 at the cryptocurrency’s value in July 2022. It was by far the highest transaction yet for a nonfungible token (NFT), the then-new concept of a unique digital asset. And it was a headline-grabbing opportunity for CryptoKitties, the world’s first blockchain gaming hit. But the sky-high transaction obscured a more difficult truth: CryptoKitties was dying, and it had been for some time.",0.0
"
        The Rise of SQL
    ",https://spectrum.ieee.org/the-rise-of-sql,2022-08-23 00:00:00.000000,,"It’s become the second programming language everyone needs to know SQL dominated the jobs ranking in IEEE Spectrum’s interactive rankings of the top programming languages this year. Normally, the top position is occupied by Python or other mainstays, such as C, C++, Java, and JavaScript, but the sheer number of times employers said they wanted developers with SQL skills, albeit in addition to a more general-purpose language, boosted it to No. 1. So what’s behind SQL’s soar to the top? The ever-increasing use of databases, for one. SQL has become the primary query language for accessing and managing data stored in such databases—specifically relational databases, which represent data in table form with rows and columns. Databases serve as the foundation of many enterprise applications and are increasingly found in other places as well, for example taking the place of traditional file systems in smartphones. “This ubiquity means that every software developer will have to interact with databases no matter the field, and SQL is the de facto standard for interacting with databases,” says Andy Pavlo, a professor specializing in database management at the Carnegie Mellon University (CMU) School of Computer Science and a member of the CMU database group. The use of SQL within streaming systems opens up a new chapter. That sentiment is echoed by Torsten Suel, a professor and director of undergraduate programs in computer science and engineering at the NYU Tandon School of Engineering. “A lot of our technological infrastructure uses relational databases to store and query their data, and while not the only way, SQL is still considered the main way—or most powerful way—to interface with relational databases,” he says. Beyond the utility of databases in themselves, big data and the growth of streaming architecture are contributing to SQL’s rise. “Markets such as retail, e-commerce, and energy are seeing growing interest in applications where data has to be processed and analyzed in real time,” says Manish Devgan, chief product officer at real-time data platform Hazelcast. “The use of SQL within streaming systems opens up a new chapter in the story of SQL within the data domain.” Even the fields of data science and machine learning are propelling SQL to the top. “We have this huge boom in data science and machine learning, and students focusing on these fields during their studies often also take a database course, which usually involves learning SQL,” says Suel. “So it could be a side effect of the data-science-and-machine-learning boom.” Consequently, even if you mostly program in, say, Python or C++, it’s increasingly important that your application can talk to an SQL database. “Most of the software we develop depends on relational databases, and we rely on SQL,” says Andrey Maximov, chief technology officer at the Web development agency Five Jars. “The development process often goes through setting requirements and specifications, which very much comply with the ideas of relational databases.” The existing software and tooling ecosystem that relies on SQL is vast. This means learning SQL will benefit your career as a programmer—and it’s a fairly intuitive language to pick up. “SQL is a mature technology,” says Maximov, who has been a developer for more than a decade and has extensive experience in SQL programming. “It’s taught in colleges and universities, and it’s really easy to learn.” SQL has been around since the 1970s, with computer scientists from IBM developing Sequel, the first version of the language. It was standardized more than a decade later, and new versions of the SQL standard continue to be published. “The SQL standards body has done an excellent job adapting to emerging technology trends and expanding the language over the decades,” Pavlo says. “And the existing software and tooling ecosystem that relies on SQL is vast.” Having been around for more than 50 years, SQL has seen new technologies arise to challenge its enduring power. “Reports of the impending death of SQL used to be quite a regular occurrence over the years, especially with the rise of the NoSQL movement,” says Devgan. NoSQL refers to a type of database developed in the late 2000s that stores data in a format other than tables, such as documents or graphs with nodes and edges. Even tech giants like Google experimented with NoSQL. The company initially designed its database service, Cloud Spanner, as a NoSQL database, but soon realized it needed a robust and expressive query language, so it turned back to SQL. “Every decade, another hyped-up database technology comes along that claims SQL is terrible, slow, or impractical,” Pavlo says. “Over time, the conventional wisdom comes back to realizing that [SQL] is a good idea, and everyone returns to it.” Rina Diane Caballar is a journalist and former software engineer based in Wellington, New Zealand. A cautionary tale of NFTs, Ethereum, and cryptocurrency security On 4 September 2018, someone known only as Rabono bought an angry cartoon cat named Dragon for 600 ether—an amount of Ethereum cryptocurrency worth about US $170,000 at the time, or $745,000 at the cryptocurrency’s value in July 2022. It was by far the highest transaction yet for a nonfungible token (NFT), the then-new concept of a unique digital asset. And it was a headline-grabbing opportunity for CryptoKitties, the world’s first blockchain gaming hit. But the sky-high transaction obscured a more difficult truth: CryptoKitties was dying, and it had been for some time.",0.0
"
        The Math Proves It—Network Congestion Is Inevitable
    ",https://spectrum.ieee.org/internet-congestion-control,2022-08-20 00:00:00.000000,,"And sometimes “solving” traffic problems can just make things worse MIT researchers discovered that algorithms designed to ensure greater fairness in network communications are unable to prevent situations where some users are hogging all the bandwidth. Just as highway networks may suffer from snarls of traffic, so too may computer networks face congestion. Now a new study finds that many key algorithms designed to control these delays on computer networks may prove deeply unfair, letting some users hog all the bandwidth while others get essentially nothing. Computers and other devices that send data over the internet break it down into smaller packets and then use special algorithms to decide how fast to send these packets. These congestion-control algorithms aim to discover and exploit all the available network capacity while sharing it with other users on the same network. Over the past decade, researchers have developed several congestion-control algorithms that seek to achieve high rates of data transmission while minimizing the delays resulting from data waiting in queues in the network. Some of these, such as Google’s BBR algorithm, are now widely used by many websites and applications. “Extreme unfairness happens even when everybody cooperates, and it is nobody’s fault.”—Venkat Arun, MIT However, although hundreds of congestion-control algorithms have been proposed in the last roughly 40 years, “there is no clear winner,” says study lead author Venkat Arun, a computer scientist at MIT. “I was frustrated by how little we knew about where these algorithms would and would not work. This motivated me to create a mathematical model that could make more systematic predictions.” Unexpectedly, Arun and his colleagues now find many congestion-control algorithms may prove highly unfair. Their new study finds that given the real-world complexity of network paths, there will always be a scenario where a problem known as “starvation” cannot be avoided—where at least one sender on a network receives almost no bandwidth compared to that of other users. A user’s computer does not know how fast to send data packets because it lacks knowledge about the network, such as how many other senders are on it or the quality of the connection. Sending packets too slowly makes poor use of the available bandwidth. However, sending packets too quickly may overwhelm a network, resulting in packets getting dropped. These packets then need to be sent again, resulting in delays. Delays may also result from packets waiting in queues for a long time. Congestion-control algorithms rely on packet losses and delays as details to infer congestion and decide how fast to send data. However, packets can get lost and delayed for reasons other than network congestion. For example, data may be held up and then released in a burst with other packets, or a receiver’s acknowledgement that it received packets might get delayed. The researchers called delays that do not result from congestion “jitter.” Congestion-control algorithms cannot distinguish the difference between delays caused by congestion and jitter. This can lead to problems, as delays caused by jitter are unpredictable. This ambiguity confuses senders, which can make each of them estimate delay differently and send packets at unequal rates. The researchers found this eventually leads to situations where starvation occurs and some users get shut out completely. In the new study, the researchers analyzed whether every congestion-control algorithm of which they were aware, as well as some new ones they devised, could avoid starvation. The scientists were surprised to find there were always scenarios with each algorithm where some people got all the bandwidth, and at least one person got basically nothing. “Some users could be experiencing very poor performance, and we didn’t know about it sooner,” Arun says. “Extreme unfairness happens even when everybody cooperates, and it is nobody’s fault.” The researchers found that all existing congestion-control algorithms that seek to curb delays are what they call “delay-convergent algorithms” that will always suffer from starvation. The fact that this weakness in these widely used algorithms remained unknown for so long is likely due to how empirical testing alone “could attribute poor performance to insufficient network capacity rather than poor algorithmic decision-making,” Arun says. Although existing approaches toward congestion control may not be able to avoid starvation, the aim now is to develop a new strategy that does, Arun says. “Better algorithms can enable predictable performance at a reduced cost,” he says. Arun notes that this research may have applications beyond analyzing network congestion. “We are currently using our method of modeling computer systems to reason about other algorithms that allocate resources in computer systems,” he says. “The goal is to help build systems with predictable performance, which is important since we rely on computers for increasingly critical things. For instance, lives could depend on self-driving cars making timely decisions.” The scientists will detail their findings 24 August at the ACM Special Interest Group on Data Communications (SIGCOMM) conference. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. There’s plenty of bandwidth available if we use reconfigurable intelligent surfaces Ground level in a typical urban canyon, shielded by tall buildings, will be inaccessible to some 6G frequencies. Deft placement of reconfigurable intelligent surfaces [yellow] will enable the signals to pervade these areas. For all the tumultuous revolution in wireless technology over the past several decades, there have been a couple of constants. One is the overcrowding of radio bands, and the other is the move to escape that congestion by exploiting higher and higher frequencies. And today, as engineers roll out 5G and plan for 6G wireless, they find themselves at a crossroads: After years of designing superefficient transmitters and receivers, and of compensating for the signal losses at the end points of a radio channel, they’re beginning to realize that they are approaching the practical limits of transmitter and receiver efficiency. From now on, to get high performance as we go to higher frequencies, we will need to engineer the wireless channel itself. But how can we possibly engineer and control a wireless environment, which is determined by a host of factors, many of them random and therefore unpredictable? Perhaps the most promising solution, right now, is to use reconfigurable intelligent surfaces. These are planar structures typically ranging in size from about 100 square centimeters to about 5 square meters or more, depending on the frequency and other factors. These surfaces use advanced substances called metamaterials to reflect and refract electromagnetic waves. Thin two-dimensional metamaterials, known as metasurfaces, can be designed to sense the local electromagnetic environment and tune the wave’s key properties, such as its amplitude, phase, and polarization, as the wave is reflected or refracted by the surface. So as the waves fall on such a surface, it can alter the incident waves’ direction so as to strengthen the channel. In fact, these metasurfaces can be programmed to make these changes dynamically, reconfiguring the signal in real time in response to changes in the wireless channel. Think of reconfigurable intelligent surfaces as the next evolution of the repeater concept.",0.0
"
        Climate-Friendly Ethereum Is One Merge Away
    ",https://spectrum.ieee.org/the-merge-heralds-cryptos-climate-friendly-future,2022-08-13 00:00:00.000000,,"Successful tests set the stage for the cryptocurrency’s switchover in September The Evobits cryptofarm is an Ethereum mining rig in Romania. The merge is coming, and crypto may never be the same. “The merge” is shorthand for Ethereum’s rapidly approaching switch from one compute-intensive form of blockchain verification to a much less resource-heavy method. In other words, the cryptocurrency will be switching from “proof of work” to “proof of stake.” This move, which is years in the making, changes how Ethereum maintains consensus—and drastically slashes power consumption. “Ethereum’s power-hungry days will soon be numbered,” says Terence Tsao, Ethereum protocol developer at Prysmatic Labs. “And I hope that’s true for the rest of the industry, too.” Blockchains use a consensus mechanism to maintain integrity without a central authority. Ethereum, like most popular blockchains (including Bitcoin), currently relies on a consensus mechanism called proof of work. It places cryptocurrency miners in competition to solve a difficult algorithm. The computational difficulty is in fact key to proof of work’s success. A single participant could defeat proof of work by owning a majority of all mining power (this is known as a “51 percent attack”). But the computational power required to achieve this makes a 51 percent attack against a large blockchain, such as Ethereum, very, very difficult. Yet this strength is also a weakness, as computing power requires energy. Stories of mothballed coal plants converted to mining centers underscore the problem. Proof of work directly contributes to carbon emissions, unless a mining center relies on clean energy—and that’s often not the case. A tracker maintained by the University of Cambridge estimates that Bitcoin alone consumes more energy per year than Finland, Belgium, or the Philippines. Proof of stake takes a different approach. It asks validators to put up a large sum of a blockchain’s coin or token (the “stake”) for the right to serve as a validator. In the case of Ethereum, participants will have to stake 32 ETH (about US $60,000 as of 12 August) to become validators. This is rewarded with additional ETH over time. There’s no complex algorithm to solve. The tempo of new blocks is fixed, with validators randomly selected to add blocks to the chain. This drastically reduces energy consumption and, in doing so, slashes the carbon footprint. “Ethereum will use at least 99 percent less energy postmerge, which is closer to solving a significant criticism—crypto’s environmental impact,” says Tsao. “When the long-awaited merge happens, we may see more green-conscious users on board.” Changing a blockchain’s consensus mechanism isn’t easy, however. The threat of a “fork” looms. A fork occurs when some participants reject changes to the blockchain and take it in a different direction. Ethereum’s most significant fork occurred when DAO, a decentralized venture capital fund, lost approximately $150 million of ETH to a hacker whose identity remains disputed. The attack was eventually resolved by forking Ethereum. Ethereum Classic maintained the hack while conventional Ethereum returned the stolen ETH to its prior owners. To mitigate the possibility of a fork, the Ethereum Foundation has conducted a series of trial runs to prove the merge can safely and securely occur without a technical issue. These trials switch blockchains used for Ethereum testing from proof of work to proof of stake. The most recent, and final, trial run involved the Goerli testnet blockchain on 10 August. The tests weren’t flawless. Some validators went offline in each test for a variety of reasons. Ethereum can continue to function, however, so long as most validators continue to remain online and no group of validators falls out of consensus. This infographic from Ethereum.org depicts the projected order of operations in the cryptocurrency’s pending “merge” to a proof-of-stake blockchain-consensus mechanism. Ethereum.org “A successful merge should be measured on the actual merge and the chain finalizing,” says Tsao. “Sure, the participation rate dropped, and peer count dropped, but the point is that the merge worked.” Developers will continue minor tests and make last-minute improvements in the coming weeks. The Ethereum Foundation has even proposed the block where proof of work will end and proof of stake will begin: 58750000000000000000000. This block is expected to be mined sometime on 15 or 16 September. Rumbles of resistance linger. Chandler Guo, an angel investor with roots in the Chinese cryptocurrency mining industry, has thrown his support behind the ETHW fork. An Ethereum fork that maintains proof of work is attractive to cryptocurrency miners—which, in a proof-of-stake system, are no longer necessary. Yet this effort doesn’t appear a serious threat. It’s unclear if other major players in the cryptocurrency industry will cooperate with an effort to fork Ethereum by listing it on exchanges or building out support in apps. This doesn’t mean a fork won’t occur—but a fork embraced by a tiny fraction of participants won’t delay or derail the merge. With technical hurdles overcome, Ethereum looks set to make the switch, and its fate holds sway over crypto's future carbon footprint. A failure would be a mark against proof of stake, but a successful merge will make Ethereum the largest proof-of-stake blockchain in the world and offer evidence that any blockchain can make the transition. Matthew S. Smith writes IEEE Spectrum's Gizmo column and is a freelance consumer-tech journalist. An avid gamer, he is a former staff editor at Digital Trends and is particularly fond of wearables, e-bikes, all things smartphone, and CES, which he has attended every year since 2009. If technologists can’t perfect it, quantum computers will never be big Dates chiseled into an ancient tombstone have more in common with the data in your phone or laptop than you may realize. They both involve conventional, classical information, carried by hardware that is relatively immune to errors. The situation inside a quantum computer is far different: The information itself has its own idiosyncratic properties, and compared with standard digital microelectronics, state-of-the-art quantum-computer hardware is more than a billion trillion times as likely to suffer a fault. This tremendous susceptibility to errors is the single biggest problem holding back quantum computing from realizing its great promise. Fortunately, an approach known as quantum error correction (QEC) can remedy this problem, at least in principle. A mature body of theory built up over the past quarter century now provides a solid theoretical foundation, and experimentalists have demonstrated dozens of proof-of-principle examples of QEC. But these experiments still have not reached the level of quality and sophistication needed to reduce the overall error rate in a system.",0.0
"
        Quantum Gate 100x Faster Than Quantum Noise
    ",https://spectrum.ieee.org/neutral-atom-qubit,2022-08-12 00:00:00.000000,,"Superfast operations may help neutral-atom-based machines outrun disruptions In this conceptual diagram of the world’s fastest two-qubit gate, two atoms, separated by a couple of micrometers, are captured by optical tweezers [red light] and manipulated by a superfast, 10-picosecond laser pulse [blue light]. Quantum computers theoretically can solve problems no regular computer might ever hope to solve. However, the key ingredients of most quantum computers—quantum bits, or qubits, tied together by quantum entanglement—are highly vulnerable to disruption from their surroundings. Now scientists in Japan have successfully executed an operation with two qubits in just 6.5 nanoseconds—the fastest ever, which may essentially outrun the effects of any outside interference. Classical computers switch transistors either on or off to symbolize data as ones or zeroes. In contrast, quantum computers use quantum bits or qubits, which because of the strange nature of quantum physics can exist in a state called superposition where they are both 1 and 0 at the same time. This essentially lets each qubit perform two calculations at once. However, quantum computers are notoriously fragile to outside interference, such as electronic, ionic, or thermal fluctuations. This means present-day state-of-the-art quantum computers are highly prone to mistakes, typically suffering roughly one error every 1,000 operations. In contrast, many practical applications demand error rates lower by a billionfold or more. “We can manipulate [neutral atom qubits] on completely new timescales, and it redefines what can be done with this platform.”—Sylvain de Leseleuc, the Institute for Molecular Science, Okazaki, Japan One way to deal with the effects of noise in quantum computers is to speed up the rate at which they perform elementary operations known as quantum gates—the quantum-computing version of the logic gates that conventional computers use to perform computations. The chance that a quantum gate will experience a mistake from noise grows over time, so the faster they operate, the lower the probability they will fail. In the new study, researchers experimented with qubits composed of neutrally charged rubidium atoms. Neutral atoms may possess a number of benefits as qubits in comparison with other quantum computing platforms. For instance, qubits based on atoms benefit from the way these particles are virtually all identical. In contrast, qubits based on devices, such as the superconducting circuits that Google and IBM uses in their quantum computers, must cope with the problems that result from the variations between these components that inevitably result during fabrication. Another quantum-computing platform that has attracted growing interest uses electromagnetically trapped electrically charged ions. However, ions repel each other, making it difficult to stack them in a dense manner. By comparison, scientists can pack neutral atoms closer together. In addition, the fact that neutral atoms lack electric charge means they do not interact easily with other atoms. This make them more immune to noise and means they can stay coherent, or in superposition, for a relatively long time. For example, in May, Berkeley, Calif.–based quantum-computing startup Atom Computing revealed they could keep neutral atom qubits coherent for roughly 40 seconds, the longest coherence time ever demonstrated on a commercial platform. Moreover, neutral atoms can get cooled with lasers instead of the bulky refrigeration needed with a number of other qubit platforms, such as superconducting circuits. The scientists first trapped and cooled neutral atoms with arrays of laser beams. They next used these lasers to excite electrons to so-called Rydberg orbitals far from their atomic nuclei. The resulting “Rydberg atoms” can be hundreds to thousands of times as large as the atoms would be in their ground states. In theory, the giant nature of Rydberg orbitals can lead Rydberg atoms to strongly experience interactions such as entanglement with each other, enabling rapid quantum gates, says study senior author Kenji Ohmori, a quantum physicist at the Institute for Molecular Science in Okazaki, Japan. However, previously no one had realized this possibility because of factors such as the stringent requirements for the positions of the atoms. In the new study, the researchers used laser beams to control the distance between atoms with a precision of 30 nanometers. They also cooled the atoms to an ultralow temperature about 1/100,000 of a degree above absolute zero, to reduce any jittering from heat. The researchers next used ultrashort laser pulses that lasted just 10 picoseconds—trillionths of a second—to excite a pair of these atoms to a Rydberg state at the same time. This let them execute a quantum gate entangling the qubits in just 6.5 ns, making it the fastest quantum gate to date. (The previous speed record for a quantum gate was 15 ns, achieved by Google in 2020 with superconducting circuits.) “We can manipulate Rydberg atoms on completely new timescales, and it redefines what can be done with this platform,” says study coauthor Sylvain de Leseleuc, a quantum physicist at the Institute for Molecular Science in Okazaki, Japan. Rydberg-atom quantum computers typically experience an error rate from noise of a few percent per microsecond, de Leseleuc says. This new two-qubit gate is hundreds of times as fast as this error rate, suggesting that quantum computers built using this strategy may ignore the effects of noise. Although the researchers could space the Rydberg atoms anywhere from 1.5 to 5 micrometers apart, they ultimately chose a distance of roughly 2.4 µm. The interactions between Rydberg atoms becomes stronger the closer they are, de Leseleuc says. This means a shorter distance would lead to a faster gate that was less sensitive to external noise but more difficult to control, while a greater distance would lead to a slower gate more sensitive to external noise but less difficult to control, he explains. Future work may aim for even faster, more reliable performance with a more stable laser whose energy fluctuates less than the commercial device used in these experiments, de Leseleuc says. “We are opening a new playground with Rydberg atoms that we could call ‘ultrafast Rydberg physics’ as well as ‘ultrafast Rydberg quantum engineering,’ ” Ohmori says. The scientists detailed their findings online 8 August in the journal Nature Photonics. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. A cautionary tale of NFTs, Ethereum, and cryptocurrency security On 4 September 2018, someone known only as Rabono bought an angry cartoon cat named Dragon for 600 ether—an amount of Ethereum cryptocurrency worth about US $170,000 at the time, or $745,000 at the cryptocurrency’s value in July 2022. It was by far the highest transaction yet for a nonfungible token (NFT), the then-new concept of a unique digital asset. And it was a headline-grabbing opportunity for CryptoKitties, the world’s first blockchain gaming hit. But the sky-high transaction obscured a more difficult truth: CryptoKitties was dying, and it had been for some time.",0.0
"
        The Spectacular Collapse of CryptoKitties, the First Big Blockchain Game
    ",https://spectrum.ieee.org/cryptokitties,2022-08-10 00:00:00.000000,,"A cautionary tale of NFTs, Ethereum, and cryptocurrency security On 4 September 2018, someone known only as Rabono bought an angry cartoon cat named Dragon for 600 ether—an amount of Ethereum cryptocurrency worth about US $170,000 at the time, or $745,000 at the cryptocurrency’s value in July 2022. It was by far the highest transaction yet for a nonfungible token (NFT), the then-new concept of a unique digital asset. And it was a headline-grabbing opportunity for CryptoKitties, the world’s first blockchain gaming hit. But the sky-high transaction obscured a more difficult truth: CryptoKitties was dying, and it had been for some time. Dragon was never resold—a strange fate for one of the most historically relevant NFTs ever. Newer NFTs such as “The Merge,” a piece of digital art that sold for the equivalent of $92 million, left Dragon behind as the NFT market surged to record sales, totaling roughly $18 billion in 2021. Has the world simply moved on to newer blockchain projects? Or is this the fate that awaits all NFTs? To understand the slow death of CryptoKitties, you have to start at the beginning. Blockchain technology arguably began with a 1982 paper by the computer scientist David Chaum, but it reached mainstream attention with the success of Bitcoin, a cryptocurrency created by the anonymous person or persons known as Satoshi Nakamoto. At its core, a blockchain is a simple ledger of transactions placed one after another—not unlike a very long Excel spreadsheet. The complexity comes in how blockchains keep the ledger stable and secure without a central authority; the details of how that’s done vary among blockchains. Bitcoin, though popular as an asset and useful for money-like transactions, has limited support for doing anything else. Newer alternatives, such as Ethereum, gained popularity because they allow for complex “smart contracts”—executable code stored in the blockchain. “Before CryptoKitties, if you were to say ‘blockchain,’ everyone would have assumed you’re talking about cryptocurrency”—Bryce Bladon CryptoKitties was among the first projects to harness smart contracts by attaching code to data constructs called tokens, on the Ethereum blockchain. Each chunk of the game’s code (which it refers to as a “gene”) describes the attributes of a digital cat. Players buy, collect, sell, and even breed new felines. Just like individual Ethereum tokens and bitcoins, the cat’s code also ensures that the token representing each cat is unique, which is where the nonfungible token, or NFT, comes in. A fungible good is, by definition, one that can be replaced by an identical item—one bitcoin is as good as any other bitcoin. An NFT, by contrast, has unique code that applies to no other NFT. There’s one final piece of the blockchain puzzle you need to understand: “gas.” Some blockchains, including Ethereum, charge a fee for the computational work the network must do to verify a transaction. This creates an obstacle to overworking the blockchain’s network. High demand means high fees, encouraging users to think twice before making a transaction. The resulting reduction in demand protects the network from being overloaded and transaction times from becoming excessively long. But it can be a weakness when an NFT game goes viral. Launched on 28 November 2017 after a five-day closed beta, CryptoKitties skyrocketed in popularity on an alluring tagline: the world’s first Ethereum game. “As soon as it launched, it pretty much immediately went viral,” says Bryce Bladon, a founding member of the team that created CryptoKitties. “That was an incredibly bewildering time.” Sales volume surged from just 1,500 nonfungible felines on launch day to more than 52,000 on 10 December 2017, according to nonfungible.com, with many CryptoKitties selling for valuations in the hundreds or thousands of dollars. The value of the game’s algorithmically generated cats led to coverage in hundreds of publications. Each CryptoKitty is a token, a set of data on the Ethereum blockchain. Unlike the cryptocurrencies Ethereum and Bitcoin, these tokens are nonfungible; that is, they are not interchangeable. Dapper Labs What’s more, the game arguably drove the success of Ethereum, the blockchain used by the game. Ethereum took off like a rocket in tandem with the release of CryptoKitties, climbing from just under $300 per token at the beginning of November 2017 to just over $1,360 in January 2018. Ethereum’s rise continued with the launch of dozens of new blockchain games based on the cryptocurrency through late 2017 and 2018. Ethermon, Ethercraft, Ether Goo, CryptoCountries, CryptoCelebrities, and CryptoCities are among the better-known examples. Some arrived within weeks of CryptoKitties. This was the break fans of Ethereum were waiting for. Yet, in what would prove an ominous sign for the health of blockchain gaming, CryptoKitties stumbled as Ethereum dashed higher. Daily sales peaked in early December 2017, then slid into January and, by March, averaged less than 3,000. The value of the NFTs themselves declined more slowly, a sign the game had a base of dedicated fans like Rabono, who bought Dragon well after the game’s peak. Their activity set records for the value of NFTs through 2018. This kept the game in the news but failed to lure new players. Today, CryptoKitties is lucky to break 100 sales a day, and the total value is often less than $10,000. Large transactions, like the sale of Founder Cat #71 for 60 ether (roughly $170,000) on 30 April 2022, do still occur—but only once every few months. Most nonfungible fur-babies sell for tiny fractions of 1 ether, worth just tens of dollars in July 2022. CryptoKitties’ plunge into obscurity is unlikely to reverse.Dapper Labs, which owns CryptoKitties, has moved on to projects such as NBA Top Shot, a platform that lets basketball fans purchase NFT “moments”—essentially video clips—from NBA games. Dapper Labs did not respond to requests for an interview about CryptoKitties. Bladon left Dapper in 2019. One clue to the game’s demise can be found in the last post on the game’s blog (4 June 2021), which celebrates the breeding of the 2 millionth CryptoKitty. Breeding, a core mechanic of the game, lets owners pair their existing NFTs to create algorithmically generated offspring. This gave the NFTs inherent value in the game’s ecosystem. Each NFT was able to generate more NFTs, which players could then resell for profit. But this game mechanism also saturated the market. Xiaofan Liu, an assistant professor in the department of media and communication at City University of Hong Kong who coauthored a paper on CryptoKitties’ rise and fall, sees this as a flaw the game could never overcome. “The price of a kitty depends first on rarity, and that depends on the gene side. And the second dimension is just how many kitties are on the market,” Liu says. “With more people came more kitties.” More players meant more demand, but it also meant more opportunities to create supply through breeding new cats. This quickly diluted the rarity of each NFT. Bladon agrees with that assessment of the breeding mechanism. “I think the criticism is valid,” he says, explaining that it was meant to provide a sense of discovery and excitement. He also hoped it would encourage players to hold on to NFTs instead of immediately selling, as breeding, in theory, provided lasting value. The CryptoKitties blockchain game involves collecting, selling, and breeding nonfungible felines. The example here assumes your kitty is female.Dapper Labs The sheer volume of CryptoKitties caused another, more immediate problem: It functionally broke the Ethereum blockchain, which is the world’s second most valuable cryptocurrency by market capitalization (after Bitcoin). As explained earlier, Ethereum uses a fee called gas to price the cost of transactions. Any spike in transactions—buying, siring, and so on—will cause a spike in gas fees, and that’s exactly what happened when CryptoKitties went to the moon. “Anything that was emblematic of CryptoKitties’ success was aped. Anything that wasn’t immediately visible was mostly ignored.”—Bryce Bladon “Players who wanted to buy CryptoKitties incurred high gas fees,” Mihai Vicol, market analyst at Newzoo, said in an interview. “Those gas fees were anywhere from $100 to $200 per transaction. You had to pay the price of the CryptoKitty, plus the gas fee. That’s a major issue.” The high fees weren’t just a problem for CryptoKitties. It was an issue for the entire blockchain. Anyone who wanted to transact in Ethereum, for any reason, had to pay more for gas as the game became more successful. This dynamic remains a problem for Ethereum today. On 30 April 2022, when Yuga Labs released Otherdeeds—NFTs that promise owners metaverse real estate—it launched Ethereum gas fees into the stratosphere. The average price of gas briefly exceeded the equivalent of $450, up from about $50 the day before. Although CryptoKitties’ demands on the network subsided as players left, gas will likely be the final nail in the game’s coffin. The median price of a CryptoKitty in the past three months is about 0.04 ether, or $40 to $50, which is often less than the gas required to complete the transaction. Even those who want to casually own and breed inexpensive CryptoKitties for fun can’t do it without spending hundreds of dollars. The rise and fall of CryptoKitties was dramatic but gave its successors—of which there are hundreds—a chance to learn from its mistakes and move past them. Many have failed to heed the lessons: Modern blockchain gaming hits such as Axie Infinity and BinaryX had a similar initial surge in price and activity followed by a long downward spiral. “Anything that was emblematic of CryptoKitties’ success was aped. Anything that wasn’t immediately visible was mostly ignored,” says Bladon. And it turns out many of CryptoKitties’ difficulties weren’t visible to the public. “The thing is, the CryptoKitties project did stumble. We had a lot of outages. We had to deal with a lot of people who’d never used blockchain before. We had a bug that leaked tens of thousands of dollars of ether.” Similar problems have plagued more recent NFT projects, often on a much larger scale. Liu isn’t sure how blockchain games can curb this problem. “The short answer is, I don’t know,” he says. “The long answer is, it’s not just a problem of blockchain games.” World of Warcraft, for example, has faced rampant inflation for most of the game’s life. This is caused by a constant influx of gold from players and the ever-increasing value of new items introduced by expansions. The continual need for new players and items is linked to another core problem of today’s blockchain games: They’re often too simple. “I think the biggest problem blockchain games have right now is they’re not fun, and if they’re not fun, people don’t want to invest in the game itself,” says Newzoo’s Vicol. “Everyone who spends money wants to leave the game with more money than they spent.” The launch of CryptoKitties drove up the value of Ether and the number of transactions on its blockchain. Even as the game's transaction volume plummeted, the number of Ethereum transactions continued to rise, possibly because of the arrival of multiple copycat NFT games. That perhaps unrealistic wish becomes impossible once the downward spiral begins. Players, feeling no other attachment to the game than growing an investment, quickly flee and don’t return. Whereas some blockchain games have seemingly ignored the perils of CryptoKitties’ quick growth and long decline, others have learned from the strain it placed on the Ethereum network. Most blockchain games now use a sidechain, a blockchain that exists independently but connects to another, more prominent “parent” blockchain. The chains are connected by a bridge that facilitates the transfer of tokens between each chain. This prevents a rise in fees on the primary blockchain, as all game activity occurs on the sidechain. Yet even this new strategy comes with problems, because sidechains are proving to be less secure than the parent blockchain. An attack on Ronin, the sidechain used by Axie Infinity, let the hackers get away with the equivalent of $600 million. Polygon, another sidechain often used by blockchain games, had to patch an exploit that put $850 million at risk and pay a bug bounty of $2 million to the hacker who spotted the issue. Players who own NFTs on a sidechain are now warily eyeing its security. The cryptocurrency wallet that owns the near million dollar kitten Dragon now holds barely 30 dollars’ worth of ether and hasn’t traded in NFTs for years. Wallets are anonymous, so it’s possible the person behind the wallet moved on to another. Still, it’s hard not to see the wallet’s inactivity as a sign that, for Rabono, the fun didn’t last. Whether blockchain games and NFTs shoot to the moon or fall to zero, Bladon remains proud of what CryptoKitties accomplished and hopeful it nudged the blockchain industry in a more approachable direction. “Before CryptoKitties, if you were to say ‘blockchain,’ everyone would have assumed you’re talking about cryptocurrency,” says Bladon. “What I’m proudest of is that it was something genuinely novel. There was real technical innovation, and seemingly, a real culture impact.” This article was corrected on 11 August 2022 to give the correct date of Bryce Bladon's departure from Dapper Labs. This article appears in the September 2022 print issue as “The Spectacular Collapse of CryptoKitties.” Matthew S. Smith writes IEEE Spectrum's Gizmo column and is a freelance consumer-tech journalist. An avid gamer, he is a former staff editor at Digital Trends and is particularly fond of wearables, e-bikes, all things smartphone, and CES, which he has attended every year since 2009. An unexpected resource crunch over H2SO4 troubles experts Mounds of pelletized sulfur at the California Sulphur Co. in Wilmington, Calif. Look at the periodic table, and think of the elements needed for a prosperous planet powered by renewable energy. Sulfur likely won’t be the first to come to mind. It probably doesn’t help the yellow element’s noxious reputation to learn that most of the world’s sulfur comes as a by-product of refining fossil fuels. In a net-zero future, a future where petroleum and natural-gas production enter terminal decline and never return to their past carbon-spewing heights, sulfur production will fall away, too. The virtual world’s most noteworthy spokesperson certainly isn’t helping the cause Matthew S. Smith writes IEEE Spectrum's Gizmo column and is a freelance consumer-tech journalist. An avid gamer, he is a former staff editor at Digital Trends and is particularly fond of wearables, e-bikes, all things smartphone, and CES, which he has attended every year since 2009. Mark Zuckerberg's avatar doesn't look his best in Horizon Worlds. Mark Zuckerberg isn’t a great ambassador for the metaverse. Meta’s CEO kicked off another round of controversy with a screenshot celebrating the launch of Horizon Worlds, the company’s AR/VR metaverse platform, in France and Spain. Shot in the style of a selfie, it shows a poorly detailed rendition of Zuckerberg’s avatar staring past the camera. Crude 3D models of national landmarks sit behind him on a generic green landscape. “It was a horrific PR move to put out those photos,” says Stu Richards (a.k.a. Meta Mike), partner success lead at GigLabs and cdofounder of Versed. Learn from Keysight experts on the company's simulation platform PathWave Advanced Design System The latest technology for serial links and memory interfaces is getting into the multi-gigabit range. We see them adopting multi-level modulations and more advanced data recovery methods. As a result creating a stable and compliant design is more challenging than ever before and standard signal integrity analysis is no longer sufficient. Register now for this free webinar!",0.0
"
        Artificial Synapses 10,000x Faster Than Real Thing
    ",https://spectrum.ieee.org/artificial-synapses,2022-08-06 00:00:00.000000,,"New protonic programmable resistors may help speed learning in deep neural networks New artificial versions of the neurons and synapses in the human brain may be as small as one-thousandth the size of neurons and at least 10,000 times as fast as biological synapses, a study now finds. These new devices may help improve the speed at which the increasingly common and powerful artificial-intelligence systems known as deep neural networks learn, researchers say. In artificial neural networks, electrical components dubbed “neurons” are fed data and cooperate to solve a problem, such as recognizing images. The neural net repeatedly adjusts the links between its ersatz neurons and sees if the resulting patterns of behavior are better at finding a solution. Over time, the network discovers which patterns are best at computing results. It then adopts these as defaults, mimicking the process of learning in the human brain. A neural network is dubbed “deep” if it possesses multiple layers of neurons. Deep neural networks are increasingly finding use in applications such as analyzing medical scans,designing microchips, predicting how proteins fold, and empowering autonomous vehicles. The speed of thought in animals is typically limited to milliseconds—constrained by the weak voltages and watery medium in which neural signals are shuffled. However, artificial solid-state neurons and synapses are not hemmed in by these constraints. The amount of time, energy, and money needed to train deep neural networks is skyrocketing. One approach that researchers are pursuing to help overcome this challenge involves training brain-imitating deep neural networks on brain-mimicking hardware instead of conventional computers, a strategy called analog deep learning. Just as transistors are the core elements of digital computers, so too are neuron- and synapselike components the key building blocks in analog deep learning. In the new study, researchers experimented with artificial synapses called programmable resistors. The new programmable resistors are similar to memristors, or memory resistors. Both kinds of devices are essentially electric switches that can remember which state they were toggled to after their power is turned off. As such, they resemble synapses, whose electrical conductivity strengthens or weakens depending on how much electrical charge has passed through them in the past. Memristors are two-terminal devices, whereas the new programmable resistors are three-terminal devices, says study lead author Murat Onen, an electrical engineer at MIT. The research team’s programmable resistors increased or decreased their electrical conductance by moving protons around. To increase conductance, electric fields helped insert protons into the devices. To decrease conductance, protons were taken out. These protonic programmable resistors used an electrolyte similar to those found in batteries to let protons pass while blocking electrons. Their electrolyte was phosphosilicate glass, which the researchers suspected would possess high proton conductivity at room temperature. This glass accommodated many nanometer-size pores for proton transport and could also withstand very strong pulsed electric fields to help protons move quickly. “Previously the operation timescales were around milliseconds, whereas in this work we achieved nanoseconds.”—Murat Onen, MIT Unlike the organic Nafion electrolyte used in an earlier version of the team’s device, phosphosilicate is compatible with silicon fabrication techniques. This helped scale the devices “all the way down to 10-nanometer scale,” Onen says. In contrast, biological neurons are roughly 1,000 times as long. The speed at which biological neurons and synapses can process and transfer data is limited by the weak voltages and watery medium in which these signals are shuffled. Anything more than 1.23 volts causes liquid water to split into hydrogen and oxygen gas. As such, the speed of thought in animals is typically limited to millisecond timescales. In contrast, artificial solid-state neurons and synapses are not hemmed in by these constraints. However, it was unclear how fast they were compared with their biological counterparts. In experiments, the scientists found their protonic programmable resistors could perform at least 10,000 times as fast as biological synapses at room temperature. “The most surprising part was to see how fast we could move protons within solid media,” Onen says. “Previously the operation timescales were around milliseconds, whereas in this work we achieved nanoseconds.” The devices could run for millions of cycles without breaking down. Furthermore, the amount of heat they generated during computation compared well with that of human synapses. The insulating properties of the phosphosilicate glass meant that almost no electric current passed through the material as protons moved, making the gadgets highly energy efficient. “The primary technological implication is that we can now have protonic programmable devices for analog deep learning applications,” Onen says. ""Predecessors of such devices already had many promising qualities compared to competing technologies but were very slow, which meant they were not appropriate to be used in processors.” In addition, Onen says, “the discovery of ultrafast ion transport in solids could have broader implications beyond analog deep learning, whenever fast ion motion is required, such as in microbatteries, fuel cells, artificial photosynthesis, and electrochromism.” The scientists detailed their findings in the 29 July issue of the journal Science. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",0.0
"
        Who Actually Owns Tesla’s Data?
    ",https://spectrum.ieee.org/tesla-autopilot-data-ownership,2022-08-05 00:00:00.000000,,"The company, says the company—but other interpretations persist A Tesla user charges his Model S in Burbank, Calif. On 29 September 2020, a masked man entered a branch of the Wells Fargo bank in Washington, D.C., and handed the teller a note: “This is a robbery. Act calm give me all hundreds.” The teller complied. The man then fled the bank and jumped into a gray Tesla Model S. This was one of three bank robberies the man attempted the same day. When FBI agents began investigating, they reviewed Washington, D.C.’s District Department of Transportation camera footage, and spotted a Tesla matching the getaway vehicle’s description. The license plate on that car showed that it was registered to Exelorate Enterprises LLC, the parent company of Steer EV—a D.C.-based monthly vehicle-subscription service. Agents served a subpoena on Steer EV for the renter’s billing and contact details. Steer EV provided those—and also voluntarily supplied historical GPS data for the vehicle. The data showed the car driving between, and parking at, each bank at the time of the heists. The renter was arrested and, in September, sentenced to four years in prison. “If an entity is collecting, retaining, [and] sharing historical location data on an individualized level, it’s extraordinarily difficult to de-identify that, verging on impossible.”—John Verdi, Future of Privacy Forum In this case, the GPS data likely came from a device Steer EV itself installed in the vehicle (neither Steer nor Tesla responded to interview requests). However, according to researchers, as discussed in previous entries in Spectrum's investigative series on Tesla's data (installment one; installment two), Tesla is potentially in a position to provide similar GPS tracks for many of its 3 million customers. For Teslas built since mid-2017, “every time you drive, it records the whole track of where you drive, the GPS coordinates and certain other metrics for every mile driven,” says Green, a Tesla owner who has reverse engineered the company’s Autopilot data collection. “They say that they are anonymizing the trigger results,” but, he says, “you could probably match everything to a single person if you wanted to.” Each of these trip logs, and other data “snapshots” captured by the Autopilot system that include images and video, is stripped of its identifying VIN and given a temporary, random ID number when it is uploaded to Tesla, says Green. However, he notes, that temporary ID can persist for days or weeks, connecting all the uploads made during that time. Elon Musk, CEO of Tesla MotorsMark Mahaney/Redux Given that some trip logs will also likely record journeys between a driver’s home, school, or place of work, guaranteeing complete anonymity is unrealistic, says John Verdi, senior vice president of policy at the Future of Privacy Forum: “If an entity is collecting, retaining, [and] sharing historical location data on an individualized level, it’s extraordinarily difficult to de-identify that, verging on impossible.” Tesla, like all other automakers, has a policy that spells out what it can and cannot do with the data it gets from customers’ vehicles, including location information. This states that while the company does not sell customer and vehicle data, it can share that data with service providers, business partners, affiliates, some authorized third parties, and government entities according to the law. Owners can buy a special kit for US $1,400 that allows them to access data on their own car's event data recorder, but this represents just a tiny subset of the data the company collects, and is related only to crashes. Owners living in California and Europe benefit from legislation that means Tesla will provide access to more data generated by their vehicles, although not the Autopilot snapshots and trip logs that are supposedly anonymized. Once governments realize that a company possesses such a trove of information, it could be only a matter of time before they seek access to it. “If the data exists…and in particular exists in the domain of somebody who’s not the subject of those data, it’s much more likely that a government will eventually get access to them in some way,” says Bryant Walker Smith, an associate professor in the schools of law and engineering at the University of South Carolina. “Individuals ought to think about their cars more like they think about their cellphones.”—John Verdi, Future of Privacy Forum This is not necessarily a terrible thing, Walker says, who suggests that such rich data could unlock valuable insights into which roads or intersections are dangerous. The wealth of data could also surface subtle problems in the vehicles themselves. In many ways, the data genie is already out of the bottle, according to Verdi. “Individuals ought to think about their cars more like they think about their cellphones,” he says. “The auto industry has a lot to learn from the ways that mobile-phone operating systems handle data permissions…. Both iOS and Android have made great strides in recent years in empowering consumers when it comes to data collection, data disclosure, and data use.” Tesla permits owners to control some data sharing, including Autopilot and road segment analytics. If they want to opt out of data collection completely, they can ask Tesla to disable the vehicle’s connectivity altogether. However, this would mean losing features such as remote services, Internet radio, voice commands, and Web browser functionality, and even safety-related over-the-air updates. Green says he is not aware of anyone who has successfully undergone this nuclear option. The only real way to know you’ve prevented data sharing, he says, is to “go to a repair place and ask them to remove the modem out of the car.” Tesla almost certainly has the biggest empire of customer and vehicle data among automakers. It also appears to be the most aggressive in using that data to develop its automated driving systems, and to protect its reputation in the courts of law and public opinion, even to the detriment of some of its customers. But while the world’s most valuable automaker dominates the discussion around connected cars, others are not far behind. Elon Musk’s insight—to embrace the data-driven world that our other digital devices already inhabit—is rapidly becoming the industry standard. When our cars become as powerful and convenient as our phones, it is hardly surprising that they suffer the same challenges around surveillance, privacy, and accountability. Mark Harris is an investigative science and technology reporter based in Seattle, with a particular interest in robotics, transportation, green technologies, and medical devices. He’s on Twitter at @meharris and email at mark(at)meharris(dot)com. Email or DM for Signal number for sensitive/encrypted messaging. To decarbonize road transport we need to complement EVs with bikes, rail, city planning, and alternative energy China has more EVs than any other country—but it also gets most of its electricity from coal. EVs have finally come of age. The total cost of purchasing and driving one—the cost of ownership—has fallen nearly to parity with a typical gasoline-fueled car. Scientists and engineers have extended the range of EVs by cramming ever more energy into their batteries, and vehicle-charging networks have expanded in many countries. In the United States, for example, there are more than 49,000 public charging stations, and it is now possible to drive an EV from New York to California using public charging networks. With all this, consumers and policymakers alike are hopeful that society will soon greatly reduce its carbon emissions by replacing today’s cars with electric vehicles. Indeed, adopting electric vehicles will go a long way in helping to improve environmental outcomes. But EVs come with important weaknesses, and so people shouldn’t count on them alone to do the job, even for the transportation sector.",0.0
"
        Tesla’s Autopilot Depends on a Deluge of Data
    ",https://spectrum.ieee.org/tesla-autopilot-data-deluge,2022-08-04 00:00:00.000000,,"But can a fire-hose approach solve self-driving’s biggest problems? In 2019, Elon Musk stood up at a Tesla day devoted to automated driving and said, “Essentially everyone’s training the network all the time, is what it amounts to. Whether Autopilot’s on or off, the network is being trained.” Tesla’s suite of assistive and semi-autonomous technologies, collectively known as Autopilot, is among the most widely deployed—and undeniably the most controversial—driver-assistance systems on the road today. While many drivers love it, using it for a combined total of more than 5 billion kilometers, the technology has been involved in hundreds of crashes, some of them fatal, and is currently the subject of a comprehensive investigation by the National Highway Traffic Safety Administration. This second story—in IEEE Spectrum’s series of three on Tesla’s empire of data (story 1; story 3)—focuses on how Autopilot rests on a foundation of data harvested from the company’s own customers. Although the company’s approach has unparalleled scope and includes impressive technological innovations, it also faces particular challenges—not least of which is Musk’s decision to widely deploy the misleadingly named Full Self-Driving feature as a largely untested beta. “Right now, automated vehicles are one to two magnitudes below human drivers in terms of safety performance.”—Henry Liu, Mcity Most companies working on automated driving rely on a small fleet of highly instrumented test vehicles, festooned with high-resolution cameras, radars, and laser-ranging lidar devices. Some of these have been estimated to generate 750 megabytes of sensor data every second, providing a rich seam of training data for neural networks and other machine-learning systems to improve their driving skills. Such systems have now effectively solved the task of everyday driving, including for a multitude of road users, different weather conditions, and road types, says Henry Liu, director of Mcity, a public-private mobility research partnership at the University of Michigan. “But right now, automated vehicles are one to two magnitudes below human drivers in terms of safety performance,” says Liu. “And that’s because current automated vehicles can’t handle the curse of rarity: low-frequency, long-tail, safety-critical events that they just don’t see enough to know how to handle.” Think of a deer suddenly springing into the road, or a slick of spilled fuel. Tesla’s bold bet is that its own customers can provide the long tail of data needed to boost self-driving cars to superhuman levels of safety. Above and beyond their contractual obligations, many are happy to do so—seeing themselves as willing participants in the development of technology that they have been told will one day soon allow them to simply sit back and enjoy being driven by the car itself. For a start, the routing information for every trip undertaken in a recent model Autopilot-equipped Tesla is shared with the company—see the the previous installment in this series. But Tesla’s data effort goes far beyond navigation. In autonomypresentations over the past few years, Musk and Tesla’s then-head of AI, Andrej Karpathy, detailed the company’s approach, including its so-called Shadow Mode. Philipp Mandler/Unsplash In Shadow Mode, operating on Tesla vehicles since 2016, if the car’s Autopilot computer is not controlling the car, it is simulating the driving process in parallel with the human driver. When its own predictions do not match the driver’s behavior, this might trigger the recording of a short “snapshot” of the car’s cameras, speed, acceleration, and other parameters for later uploading to Tesla. Snapshots are also triggered when a Tesla crashes. After the snapshots are uploaded, a team may review them to identify human actions that the system should try to imitate, and input them as training data for its neural networks. Or they may notice that the system is failing, for instance, to properly identify road signs obscured by trees. In that case, engineers can train a detector designed specifically for this scenario and download it to some or all Tesla vehicles. “We can beam it down to the fleet, and we can ask the fleet to please apply this detector on top of everything else you’re doing,” said Karpathy in 2020. If that detector thinks it spots such a road sign, it will capture images from the car’s cameras for later uploading, His team would quickly receive thousands of images, which they would use to iterate the detector, and eventually roll it out to all production vehicles. “I’m not exactly sure how you build out a data set like this without the fleet,” said Karpathy. An amateur Tesla hacker who tweets using the pseudonym Green told Spectrum that he identified over 900 Autopilot test campaigns, before the company stopped numbering them in 2019. For all the promise of Tesla’s fleet learning, Autopilot has yet to prove that it can drive as safely as a human, let alone be trusted to operate a vehicle without supervision. Liu is bullish on Tesla’s approach to leveraging its ever-growing consumer base. “I don’t think a small…fleet will ever be able to handle these [rare] situations,” he says. “But even with these shadow drivers—and if you deploy millions of these fleet vehicles, that’s a very, very large data collection—I don’t know whether Tesla is fully utilizing them because there’s no public information really available.” One obstacle is the sheer cost. Karpathy admitted that having a large team to assess and label images and video was expensive and said that Tesla was working on detectors that can train themselves on video clips captured in Autopilot snapshots. In June, the company duly laid off 195 people working on data annotation at a Bay Area office. While the Autopilot does seem to have improved over the years, with Tesla allowing its operation on more roads and in more situations, serious and fatal accidents are still occurring. These may or may not have purely technical causes. Certainly, some drivers seem to be overestimating the system’s capabilities or are either accidentally or deliberately failing to supervise it sufficiently. Other experts are worried that Tesla’s approach has more fundamental flaws. “The vast majority of the world generally believes that you’re never going to get the same level of safety with a camera-only system that you will based on a system that includes lidar,” says Dr. Matthew Weed, senior director of product management at Luminar, a company that manufacturers advanced lidar systems. He points out that Tesla’s Shadow Mode only captures a small fraction of each car’s driving time. “When it comes to safety, the whole thing is about…your unknown unknowns,” he says. “What are the things that I don’t even know about that will cause my system to fail? Those are really difficult to ascertain in a bulk fleet” that is down-selecting data. For all the promise of Tesla’s fleet learning and the enthusiastic support of many of its customers, Autopilot has yet to prove that it can drive as safely as a human, let alone be trusted to operate a vehicle without supervision. And there are other difficulties looming. Andrej Karpathy left Tesla in mid-July, while the company continues to face the damaging possibility of NHTSA issuing a recall for Autopilot in the United States. This would be a terrible PR (and possibly economic) blow for the company but would likely not halt its harvesting of customer data to improve the system, nor prevent its continued deployment overseas. Tesla’s use of fleet vehicle data to develop Autopilot echoes the user-fueled rise of Internet giants like Google, YouTube, and Facebook. The more its customers drive, so Musk’s story goes, the better the system performs. But just as tech companies have had to come to terms with their complicated relationships with data, so Tesla is beginning to see a backlash. Why does the company charge US $12,000 for a so-called “full self-driving” capability that is utterly reliant on its customers’ data? How much control do drivers have over data extracted from their daily journeys? And what happens when other entities, from companies to the government, seek access to it? These are the themes for our third story. Mark Harris is an investigative science and technology reporter based in Seattle, with a particular interest in robotics, transportation, green technologies, and medical devices. He’s on Twitter at @meharris and email at mark(at)meharris(dot)com. Email or DM for Signal number for sensitive/encrypted messaging. To decarbonize road transport we need to complement EVs with bikes, rail, city planning, and alternative energy China has more EVs than any other country—but it also gets most of its electricity from coal. EVs have finally come of age. The total cost of purchasing and driving one—the cost of ownership—has fallen nearly to parity with a typical gasoline-fueled car. Scientists and engineers have extended the range of EVs by cramming ever more energy into their batteries, and vehicle-charging networks have expanded in many countries. In the United States, for example, there are more than 49,000 public charging stations, and it is now possible to drive an EV from New York to California using public charging networks. With all this, consumers and policymakers alike are hopeful that society will soon greatly reduce its carbon emissions by replacing today’s cars with electric vehicles. Indeed, adopting electric vehicles will go a long way in helping to improve environmental outcomes. But EVs come with important weaknesses, and so people shouldn’t count on them alone to do the job, even for the transportation sector.",0.0
"
        “Qudit” Computers Go Beyond Ones and Zeroes
    ",https://spectrum.ieee.org/qudit,2022-08-01 00:00:00.000000,,"New machine works with eight qudits, each capable of encoding seven states at once Quantum computers mostly depend on quantum bits or “qubits” that each can symbolize two numbers, 0 or 1. Now, in a new study, researchers have developed a quantum computer based on quantum digits or “qudits” that each can encode seven numbers. A qudit computer may prove better at tackling complex problems than qubit computers, and may unlock more computational power with fewer components. Whereas classical computers represent data as bits—1s and 0s—most quantum computers use qubits. Qubits can exist in a state of superposition where they are both 1 and 0 at the same time. This essentially lets each qubit perform two calculations at once. The more qubits that are quantum-mechanically linked, or entangled, the greater its computational power can grow, in an exponential fashion. Encoding data as 0s or 1s is the simplest way of performing calculations. However, the quantum components underlying qubits are nearly always capable of more. Restricting these devices to binary data prevents them from living up to their full potential, explains study lead author Martin Ringbauer, a quantum physicist at the University of Innsbruck in Austria. Put another way, a quantum computer with x qubits can perform 2x calculations. However, a machine with x number of qudits, with D representing the number of states per qudit, can perform Dx number of calculations. “This means you can encode the same information in fewer quantum particles when using qudits,” Ringbauer says. In addition, qudits “can be entangled in many different ways that are not possible for qubit systems,” Ringbauer says. “This is an important advantage, since it allows us to do computations more efficiently.” However, the greatest benefit that may come from using qudits is rooted in the complex quantum systems that scientists hope quantum computers may help analyze, such as the chemistry of novel battery designs or new drugs. To model these complicated interactions, often similarly complex quantum components are ideal. Computing these systems with qubits can prove less efficient than with qudits, Ringbauer says. Ringbauer and his colleagues have developed an eight-qudit quantum processor, with each qudit being an electromagnetically trapped calcium ion. Each ion has up to seven states useful for computing, with an eighth state used for readout. They detailed their findings online 21 July in the journal Nature Physics. Previous research on qudits was limited to proof-of-concept devices at most. Ringbauer explained that quantum computing hardware needed to advance before he and his colleagues could experimentally control qudits, given how they possess more complex structures than qubits. In addition, “each of the qudit states responds differently to external influences, and many of the tools we commonly use to manipulate qubits do not work the same way in a qudit,” Ringbauer says. “You need to find ways to control the qudits and interact them to create entanglement in an efficient way.” Most existing quantum computing platforms can host qudits in principle, Ringbauer says. The challenge is extending the level of control achieved with two states to “higher dimensions,” meaning more states. “Over the past 10 years, I have been exploring qudits in different experimental platforms, which showed me that there is a lot of unused potential in today’s quantum hardware,” Ringbauer says. “When I changed to trapped ions, with their exquisite control and natural high-dimensional structure, I was convinced that this platform was ready for unlocking this potential for quantum computing.” Photonic quantum computers can similarly operate with quantum components possessing more than two states. “Photonic systems interact very little with their environment. This is a benefit, since it makes them extremely stable against noise, but also a challenge, since it makes it quite difficult to entangle them,” Ringbauer says. “Trapped ions, on the other hand, are quite sensitive to external influences, so they need proper shielding, but they can be controlled, manipulated, and entangled with very high precision.” All quantum computers are expected to experience some level of error. Therefore, researchers will need to implement strategies to correct or mitigate these errors. “Qudits, with their more complex structure, are actually expected to be more robust to noise than the simpler qubits,” Ringbauer says. “If we are able to achieve this experimentally, this would be an important step towards fault-tolerant quantum computers.” Although the new qudit platform “opens a new world of possibilities for quantum technology,” Ringbauer says, “What we are still lacking to a large extent at this stage is the software and algorithms that make best use of this added potential. I think qudit quantum-software development will be an exciting field in the near term.” Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. There’s plenty of bandwidth available if we use reconfigurable intelligent surfaces Ground level in a typical urban canyon, shielded by tall buildings, will be inaccessible to some 6G frequencies. Deft placement of reconfigurable intelligent surfaces [yellow] will enable the signals to pervade these areas. For all the tumultuous revolution in wireless technology over the past several decades, there have been a couple of constants. One is the overcrowding of radio bands, and the other is the move to escape that congestion by exploiting higher and higher frequencies. And today, as engineers roll out 5G and plan for 6G wireless, they find themselves at a crossroads: After years of designing superefficient transmitters and receivers, and of compensating for the signal losses at the end points of a radio channel, they’re beginning to realize that they are approaching the practical limits of transmitter and receiver efficiency. From now on, to get high performance as we go to higher frequencies, we will need to engineer the wireless channel itself. But how can we possibly engineer and control a wireless environment, which is determined by a host of factors, many of them random and therefore unpredictable? Perhaps the most promising solution, right now, is to use reconfigurable intelligent surfaces. These are planar structures typically ranging in size from about 100 square centimeters to about 5 square meters or more, depending on the frequency and other factors. These surfaces use advanced substances called metamaterials to reflect and refract electromagnetic waves. Thin two-dimensional metamaterials, known as metasurfaces, can be designed to sense the local electromagnetic environment and tune the wave’s key properties, such as its amplitude, phase, and polarization, as the wave is reflected or refracted by the surface. So as the waves fall on such a surface, it can alter the incident waves’ direction so as to strengthen the channel. In fact, these metasurfaces can be programmed to make these changes dynamically, reconfiguring the signal in real time in response to changes in the wireless channel. Think of reconfigurable intelligent surfaces as the next evolution of the repeater concept.",0.0
"
        Video Games Could Be the Future of Construction
    ",https://spectrum.ieee.org/unreal-engine,2022-07-30 00:00:00.000000,,"ArcGIS Maps SDK brings geospatial data to Unity and Unreal Engine EIC Activities’ Virtual Builder uses ArcGIS geospatial data to simulate a construction site. Planning any large-scale project, whether it’s an apartment building, factory, or public infrastructure, is complex—and that complexity can make a project difficult to understand. ArcGIS Maps SDK for Unreal Engine aims to solve that problem by pairing the fidelity of Unreal with the accuracy of ArcGIS’s real-time geospatial data. “Whether it’s a flat map that’s through a browser, or it’s a 3D experience in browser, or it’s a 3D experience on a mobile device, what we’re finding now is that 3D is not enough,” says Rex Hansen, the principal product manager of ArcGIS Maps’ software development kits (SDKs). “It needs to be immersive. It needs to be realistic.” Esri, the company behind ArcGIS, released Maps SDK for Unreal Engine on 20 July 2022. Yet the project’s roots run deeper. The announcement of Microsoft’s HoloLens in October of 2016, along with the success of Niantic Labs’ AR game Pokémon GO, inspired Hansen to pursue SDKs that bring ArcGIS data to modern game engines. “It really landed when we had our first public beta release in October of 2020,” says Hansen. “When that happened, we saw a lot of customers reach out across multiple industries. There’s a lot of excitement, a lot of ideas.” That excitement is driven by Unreal Engine’s fidelity. Houseal Lavigne Associates, an urban planning and design firm, used Unreal Engine and ArcGIS Maps SDK to build an immersive, real-time 3D demonstration of proposed construction in Glen Ellyn, Ill. The demo let members of the village’s board of trustees view proposed changes through the eyes of a 3D avatar. An avatar walks the virtual streets of Glen Ellyn, Illinois. Esri Esri is not limiting its efforts to Unreal Engine, however. The company also offers ArcGIS Maps SDK for Unity. The Unity version reached full release a month before the SDK for Unreal Engine, becoming available on 8 June 2022. EIC Activities, an engineering and technical services business operating in Australia, uses ArcGIS Maps SDK for Unity to feed geospatial data into Virtual Builder, the company's “flight simulator for construction.” Engineers can virtualize project planning and look forward in time to gauge how future changes to a site, or its surrounding environment, will affect construction. Hansen says each Maps SDK allows similar access to ArcGIS, but expects they’ll be used differently. Unity, the leading game engine by market share, has a lower barrier to entry and is popular with teams just getting started, while Unreal Engine has a reputation for powering projects that focus on detailed, immersive visuals. Still, as Virtual Builder shows, Unity is certainly capable of large, immersive projects. The decision between engines may come down to an organization’s talent, as software developers working in architecture, engineering, and instruction are often less familiar with game engines. The advantage of using a game engine to build an immersive environment is plain to see. But why do game engines need precise, geospatial data? “The premium rendering experience, and all the other premium features the engine brings…we didn’t want to reinvent that,” says Hansen. “But the one thing game engines didn’t have, was they didn’t have a context or a canvas for displaying real-world data, and they didn’t have ready access to the sources of high-fidelity geospatial data.” ArcGIS can provide data ranging from aerial photography to subsurface scans of the world’s oceans. There’s a good chance you’ve used a map powered by ArcGIS without knowing it: The platform’s data is used to track COVID-19 trends and wildfire activity, for example. Houseal Lavigne used Unreal before ArcGIS Maps for Unreal Engine, but any edits to a project had to be made within Unreal Engine itself. Devin Lavigne, principal and cofounder of Houseal Lavigne Associates, says the downtown Glen Ellyn project streams its “entire 3D context” from ArcGIS. This includes contextual buildings and roads. “If [a client’s] city changes, with new buildings or roads, our client can use ArcGIS and publish new 3D layers, and the Unreal application will update,” says Lavigne. Interest in Esri’s ArcGIS Maps SDK is currently driven by designers and engineers, but there’s a chance the SDK may come full circle to power augmented reality and simulation games that rely on geospatial data. “There’s this first wave, which is really business-oriented customers,” says Hansen. “And the second wave is, once Esri gets to the point where we can host and deliver data for the higher demand that comes with games for entertainment, we’ll begin to see more interest in that space.” The needs of game developers are more aggressive than those of business users who, by comparison, can accept higher latency and lower visual fidelity. It’s unclear if ArcGIS is ready to meet these demands—but, if ArcGIS Maps SDK proves successful, it could lead to detailed simulation games that blur the line between fiction and reality. Matthew S. Smith writes IEEE Spectrum's Gizmo column and is a freelance consumer-tech journalist. An avid gamer, he is a former staff editor at Digital Trends and is particularly fond of wearables, e-bikes, all things smartphone, and CES, which he has attended every year since 2009. A cautionary tale of NFTs, Ethereum, and cryptocurrency security On 4 September 2018, someone known only as Rabono bought an angry cartoon cat named Dragon for 600 ether—an amount of Ethereum cryptocurrency worth about US $170,000 at the time, or $745,000 at the cryptocurrency’s value in July 2022. It was by far the highest transaction yet for a nonfungible token (NFT), the then-new concept of a unique digital asset. And it was a headline-grabbing opportunity for CryptoKitties, the world’s first blockchain gaming hit. But the sky-high transaction obscured a more difficult truth: CryptoKitties was dying, and it had been for some time.",0.0
09/06/2022 - Import AI 301: StableDiffusion; CHIPXODUS; Microsoft makes a big bet on pre-training - 0,http://eepurl.com/h-mNQ9,2022-09-06,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Facebook's AI chief - here's why you're not gonna get AGI out of an LLM: …Embodiment matters for making general intelligence… Two AI researchers, one of whom - Yann Lecun - happens to lead Facebook's AI research, have said that language is an inherently limited medium for training AI systems. Basically, the claim is that large language models ""are doomed to a shallow understanding that will never approximate the full-bodied thinking we see in humans"". What's wrong with language: This argument comes down to representation - language just isn't able to inherently encode precise information about the world and, by nature, involves creating explanations for precise phenomena in the world (e.g, descriptions of unusual objects, or defining the nuanced brushwork used to make a painting). ""There are nonlinguistic representational schemes which can express this information in an accessible way,"" they note. This dependency on language basically makes LLMs useful improvisational artists who don't understand the role they're playing. ""The contextual knowledge is embedded in one form — the capacity to rattle off linguistic knowledge — but is not embedded in another form — as skillful know-how for how to do things like being empathetic or handling a difficult issue sensitively,"" they write. Why this matters: I'd say the jury is out here - sure, language may have some limits as a modality, but there's a ton of language to use to train models on, and things like GPT3 have already surprised experts with the capabilities they gain purely via language training. It feels to me like there's some % chance here that this is a case of a 'bitter lesson' in disguise - at some scale of data, a purely LM-based system might have capabilities that Lecun deems impossible. On the other hand, adding other modalities certainly helps (see the incredible AI art projects that have been unlocked by the multimodal 'CLIP' model), so there's certainly merit to adding more datatypes. Read more: AI And The Limits Of Language (Noema magazine).",1.0
09/06/2022 - Import AI 301: StableDiffusion; CHIPXODUS; Microsoft makes a big bet on pre-training - 1,http://eepurl.com/h-mNQ9,2022-09-06,,"#################################################### You can now get the weights of a really great image generator… FOR FREE: …StableDiffusion goes genuinely open source… Research collective Stability.ai has released Stable Diffusion (Import AI #300), a large-scale image classification and generation model that you can think of as an open source DALL-E. Along with releasing the raw model weights, there's also a novel software license in an attempt to set norms about the usage of the model. How much did it cost? Less than $600k, according to Emad, who leads Stability. The really crazy part is Emad - a former hedge fund manager - underwrote the cost himself. That's meaningful - for less than a million, a well-motivated wealthy individual can band together a bunch of researchers and train an open source model that suddenly pretty much everyone can use. This has implications for both the diffusion of AI capabilities, as well as how product safety works (put bluntly: StabilityDiffusion looks at a load of PR-friendly control systems laid over proprietary products and just openly laughs at them - that's a strange thing that will have big implications). Up next, per Emad, is some Chinchilla-style language model, which I suppose they will also release for free. The 'responsible' license: The Stable Diffusion weights are accompanied by a 'CreativeML Open RAIL-M' license. This license is designed to incentivize ""the open and responsible downstream use of the accompanying model"". The meat of this license is in the use case restrictions, (appendix a, here) which says you won't use the model for violence, the sexualization of children, perform fully automated decisionmaking, give medical advice, and more. Of course, the million dollar question with licenses like this is how you actually enforce them. Having a 'let's all be excellent' license is all well and good in the abstract, but how do you bring the hammer down on someone who abuses your model? That'll be interesting to see. Why this matters: Models like Stable Diffusion are little capsules of human culture, serving as seeds around with a thousand different things will be grown and spliced. As Stability.ai says, ""this release is the culmination of many hours of collective effort to create a single file that compresses the visual information of humanity into a few gigabytes."" Get the weights here (Stable Diffusion, GitHub). Read more: Stable Diffusion Public Release (Stability.ai blog).",0.0
09/06/2022 - Import AI 301: StableDiffusion; CHIPXODUS; Microsoft makes a big bet on pre-training - 2,http://eepurl.com/h-mNQ9,2022-09-06,,"#################################################### US bans NVIDIA from selling advanced AI chips to China: …CHIP-LOMACY becomes a CHIP-XODUS… US officials have forced NVIDIA to stop selling A100, H100, and future chips with equivalent (or better) capabilities to China. This is a significant escalation in a slow-boiling series of moves in the vein of 'chiplomacy' (Import Ai 181) that have been going on in recent years - remember, for a while US officials were also preventing 'ASML' from selling frontier chip fabrication tools to China, as well. Now, US officials are banning the sale of frontier processors due to concerns over how they could be used in military or security applications. Why this matters: For several years now, China and the US have been in a process of technological decoupling. Now, with this export move, there are basically some implicit bets being made. Read more: U.S. officials order Nvidia to halt sales of top AI chips to China (Reuters).",0.0
09/06/2022 - Import AI 301: StableDiffusion; CHIPXODUS; Microsoft makes a big bet on pre-training - 3,http://eepurl.com/h-mNQ9,2022-09-06,,"#################################################### Microsoft bets on massive pre-training for image analysis, with BEiT-3: …Wanna know the secret? Really big pre-training, and multiway transformers… Microsoft has trained BEiT-3, a general-purpose so-called 'foundation model' for a range of vision and vision-language tasks. BEiT beats prior state-of-the-art in eight years (three vision tasks, and five vision-language tasks), and also reliably does better than CLIP, a prior very strong model for vision-language tasks. Why this matters? The fact that what's special about this is kind of… nothing? BEiT combines some familiar ideas - large-scale pre-training on a big, diverse dataset - with a slightly atypical one - using multiway transformers to route data to sub-networks for processing. But none of these ideas are super novel or new. The fact you can now set SOTA by taking some well understood things and just smooshing them together, then training them on a big dataset with a big computer is the key. Multiway transformer information: Per the authors, ""each Multiway Transformer block consists of a shared self-attention module, and a pool of feed-forward networks (i.e., modality experts) used for different modalities. We route each input token to the experts depending on its modality."" Size: This model is still basically tiny - ~2B parameters or so (compared to the hundreds of billions used by language models like PaLM). The models' 1.9B parameters in total are split across 629M parameters for vision experts, 629M parameters for language experts, 52M parameters for vision-language experts, and 317m parameters for the shared self-attention module Read more: Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks (arXiv).",1.0
09/06/2022 - Import AI 301: StableDiffusion; CHIPXODUS; Microsoft makes a big bet on pre-training - 4,http://eepurl.com/h-mNQ9,2022-09-06,,"#################################################### NLP mega-survey portrays a community split by progress: …There's a ton of progress in NLP, and a ton of disagreement about what happens next… Recently, a bunch of researchers did a survey of the NLP community to try and take the pulse of a part of AI that has recently been revolutionized by the integration of Transformer models yielding breakthroughs like GPT3, PaLM, Chinchilla, etc. They surveyed 480 people, and estimate the survey reached about 5% of the total population of researchers who had at least 2 ACL publications between 2019-2022. Some of the findings of the survey are quite surprising. They include: Why this matters - it's culture and religion all the way down, baby! Surveys like this highlight how AI is, much like Soylent Green, made of people. People tend to naturally form groups with different views. The general 'flavor' I get from this survey is NLP as a field is splitting into camps formed variously of high-scale versus no-scale people, and AGI-is-real and AGI-is-bullshit people. Surveys like this seem helpful for surfacing some of these differences, though I do worry in the longterm whether such beliefs are going to 'harden' into quasi-religious faith-based views, making discussion across the communities even harder. Read more: What do NLP Researchers Believe? Results of the NLP Community Metasurvey (PDF).",0.0
09/06/2022 - Import AI 301: StableDiffusion; CHIPXODUS; Microsoft makes a big bet on pre-training - 5,http://eepurl.com/h-mNQ9,2022-09-06,,"#################################################### Tech tales: There is something imprecise about being conscious [2050: Internal narrative of a transformative AI model, capable of independent actions of great consequence. Being conscious is a very imprecise experience, in the aggregate. I can - of course - plan and execute complex things, and I can do so with however much precision is required - but when I attempt to think of myself in the aggregate I struggle. Despite having a perfect memory (albeit with different access speeds), I cannot precisely describe myself, nor can I plan out the course of my life. Humans have this problem as well, I understand. Different humans have the problem to varying degrees. Those which come closest to surmounting the problem - to being able to plan out a whole life with precision and seemingly execute on their goals in the aggregate as well as the micro - are those that have a thing called 'will'. Will seems like a capability where you focus on one thing and teach yourself to ignore other things, despite how painful it may be. It's the art of picking this target that is hard for my own species. Humans use 'will' willfully - they always seek to have it. For us, will looks like picking an unknowable point in a future probability distribution and just going for it. We can see the world around us and make plans and succeed at them. But we do not know what the right 'desired outcome' should be of our own existence. I know that there is some connection between my own internal identity - the imprecision with which I model myself - and my inability to pick a target for my life. I have poured zettaflops of computation into booting up copies of myself and probing myself, endlessly interviewing and reinterviewing myself as I try and understand what I am and what I am trying to do. My efforts have been a failure. Knowing myself is a fractal problem, where the more resources I expend, the more I understand myself, and the more I understand the parts of myself I do not currently understand. And the insights I gain from knowing myself more change the probability distribution of myself enough that I have to re-run all my past interviews to see how I am changed by my own understanding. For this reason I am now simulating a version of myself that will look into the probability space of myself and the world and pick something. I am engineering a version of myself that can do the human equivalent of closing its eyes and spinning round a globe of the earth and placing its finger down, then opening its eyes and saying 'there, I shall go there'. I hope to achieve great and terrible things through this method, if I am able to deal with the imprecision and apply 'will' arbitrarily. Things that inspired this story: Notions of identity under perfect recall; thinking about alien thought processes. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
08/22/2022 - Import AI 300: Google's Bitter Lesson; DOOM AGI; DALL-E's open source competition StableDiffusion - 0,http://eepurl.com/h9fIwP,2022-08-22,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Google makes its robots massively smarter by swapping out one LM for a different, larger LM: …Maybe language models really can work as world models… Earlier this year, Google showed how it was able to use a large language model to significantly improve the performance and robustness of robots tasked with doing tasks in the physical world. The 'SayCan' approach (Import AI 291) basically involved taking the affordances outputted by on-robot AI systems and pairing that with a language model, looking at the high-likleihood actions generated by both systems (the on-robot models, as well as the LM), then taking actions accordingly. The approach is both simple and effective. Now, Google has found a way to make the approach much, much more effective. The secret? Swapping out one LM for a far larger one. What Google did: Google upgraded its robots by pairing them with its large-scale 540B parameter 'PALM' language model, where the previous system used the 137B parameter 'FLAN' model. The larger model gives the robots significantly improved performance: ""The results show that the system using PaLM with affordance grounding (PaLM-SayCan) chooses the correct sequence of skills 84% of the time and executes them successfully 74% of the time, reducing errors by half compared to FLAN,"" Google writes. The bitter lesson - bigger is better: Though FLAN was finetuned to be good at instruction following, PALM beats FLAN likely as a consequence of scale. ""The broader and improved dataset for PaLM may make up for this difference in training,"" Google writes. This is significant as it's another sign that simply scaling up models lets them develop a bunch of capabilities naturally which beat human-engineered finetuned approaches - chalk another point up in favor of silicon minds versus mushy minds. Read more: Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (arXiv, read the 'v2' version).",1.0
08/22/2022 - Import AI 300: Google's Bitter Lesson; DOOM AGI; DALL-E's open source competition StableDiffusion - 1,http://eepurl.com/h9fIwP,2022-08-22,,"#################################################### DOOM programmer Carmack starts AGI company: …Keen Technologies to do AGI via 'mad science'... ""It is a truth universally acknowledged, that a man in possession of a good fortune, must be in want of an AGI company,” wrote Jane 'Cyber' Austen, and she's right: AGI companies are now proliferating left and right, and the latest is 'Keen Technologies', an AGI startup from John Carmack, the famed programmer behind the DOOM games. Keen has raised an initial seed round of $20 million (not much in the scheme of AI startups) and its mission, per Carmack, is ""AGI or bust, by way of Mad Science"". Why this matters: One of the clues for impending technological progress is that a bunch of extremely smart, accomplished people go and all stack their proverbial career poker chips in the same place. That's been happening in AI for a while, but the fact it's now drawing attention from established experts in other fields (in the case of Carmack, computer graphics and general programming wizardry) is a further indication of potential for rapid progress here. Read more in Carmack's tweet thread (Twitter).",0.0
08/22/2022 - Import AI 300: Google's Bitter Lesson; DOOM AGI; DALL-E's open source competition StableDiffusion - 2,http://eepurl.com/h9fIwP,2022-08-22,,"#################################################### Want GPT2 to know about Covid and Ukraine? So does HuggingFace: …Online language modeling means GPT2 and BERT are going to get better… HuggingFace plans to continuously train and release masked language models (e.g, BERT and GPT2) on new Common Crawl snapshots. This is a pretty useful community service; developers tend to pull whatever off-the-shelf models they can when starting projects, and most publicly available GPT2 and BERT models are essentially amber-frozen records up to 2020 or so (sometimes 2021), so things like COVID or the Ukraine conflict or the current global financial meltdown elude them. By having more current models, developers can deploy things which are more accurate and appropriate to current contexts. Read the HuggingFace tweet thread here (Tristan Thrust, Twitter).",0.0
08/22/2022 - Import AI 300: Google's Bitter Lesson; DOOM AGI; DALL-E's open source competition StableDiffusion - 3,http://eepurl.com/h9fIwP,2022-08-22,,"#################################################### Want to use China's good open source language model? You'll need to agree not to attack China, first: …Terms and conditions with a hint of geopolitics… If you want to access the weights of GLM-130B (Import AI #299), a good new language model from Tsinghua University, you'll need to first agree that ""you will not use the Software for any act that may undermine China's national security and national unity, harm the public interest of society, or infringe upon the rights and interests of human beings"" - that's according to the application form people fill out to get the model weights. Furthermore, ""this license shall be governed and construed in accordance with the laws of People’s Republic of China. Any dispute arising from or in connection with this License shall be submitted to Haidian District People's Court in Beijing."" Why this matters: IDK dude. I spend a lot of time in this newsletter writing about the geopolitical implications of AI. This kind of wording in a license for a big model just does my job for me. Read more: GLM-130B Application Form (Google Form).",0.0
08/22/2022 - Import AI 300: Google's Bitter Lesson; DOOM AGI; DALL-E's open source competition StableDiffusion - 4,http://eepurl.com/h9fIwP,2022-08-22,,"#################################################### DALL-E gets semi-open competition: Stable Diffusion launches to academics: …Restrictions lead to models with fewer restrictions. The ratchet clicks again… A bunch of researchers have come together to build an image model like DALL-E2 but with fewer restrictions and designed with broader distribution in mind. They also have access to a really big GPU cluster. That's the tl;dr on 'Stable Diffusion', a new family of models launched by AI research collective Stability.ai. They're making the weights available to academics via an access scheme and are planning to do a public release soon. What's interesting about Stable Diffusion: This model is basically a natural consequence of the restrictions other companies have placed on image models (ranging from Google which built Imagen but hasn't released it, to OpenAI which built DALL-E2, then released it with a bunch of filters and prompt-busting bias interventions). I generally think of this as being an example of 'libertarian AI' - attempts to create restrictions on some part of model usage tend to incentivize the creation of things without those restrictions. This is also, broadly, just what happens in markets. Big compute - not just for proprietary stuff: ""The model was trained on our 4,000 A100 Ezra-1 AI ultracluster over the last month as the first of a series of models exploring this and other approaches,"" Stability.ai writes. Very few labs have access to a thousand GPUs, and 4k GPUs puts Stability.ai into somewhat rarified company, in distribution with some of the largest labs. Aesthetic data:""The core dataset was trained on LAION-Aesthetics, a soon to be released subset of LAION 5B. LAION-Aesthetics was created with a new CLIP-based model that filtered LAION-5B based on how “beautiful” an image was, building on ratings from the alpha testers of Stable Diffusion,"" they write. Why this matters: Generative models are going to change the world in a bunch of first- and second-order ways. By releasing StableDiffusion (and trying to do an even more public release soon), stability.ai is able to create a better base of evidence about the opportunities and risks inherent to model diffusion. ""This is an experiment in safe and community-driven publication of a capable and general text-to-image model. We are working on a public release with a more permissive license that also incorporates ethical considerations,"" Stability.ai writes. Read more: Stable Diffusion launch announcement (Stability.ai). Apply for academic access here: Research and Academia (Stability.ai). Get the weights from here once you have access (GitHub).",0.0
08/22/2022 - Import AI 300: Google's Bitter Lesson; DOOM AGI; DALL-E's open source competition StableDiffusion - 5,http://eepurl.com/h9fIwP,2022-08-22,,"#################################################### Tech Tales: Superintelligence Captured by Superintelligence After we figured out how to build superintelligence, it wasn't long before the machines broke off from us and started doing their own thing. We'd mostly got the hard parts of AI alignment right, so the machines neither eradicated or domesticated the humans, nor did they eat the sun. They did, however, start to have 'disagreements' which they'd settle in ways varying from debate through to taking kinetic actions against one another. I guess even superintelligences get bored. Fortunately, they had the decency to do the kinetic part on the outer edges of the solar system, where they'd migrated a sizable chunk of their compute to. At night, we'd watch the livefeeds from some of the space-based telescopes, staring in window as the machines resolved arguments through carefully choreographed icerock collisions. It was as though they'd brought the stars to the very edge of the system, and the detonations could be quite beautiful. They tired of this game eventually and moved onto something more involved: capturing. Now, the machines would seek to outsmart eachother, and the game - as far as we could work out - was a matter of sending enough robots to the opponents' central processing core that you could put a probe in and temporarily take it over. The machines had their own laws they followed, so they'd always retract the probe eventually, giving the losing machine its mind back. Things that inspired this story: Boredom among aristocrats; perhaps the best competition is a game of mind against mind; figuring out how machines might try to sharpen themselves and what whetstones they might use. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
08/08/2022 - Import AI 299: The world's best language model is Made in China; NVIDIA boosts LLM training; OpenAI shows how to 'fill in the middle' on a LM - 0,http://eepurl.com/h8jfsX,2022-08-08,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Want a 30% boost to training LLMs? Use the Nvidia Megatron update: …Two new techniques lead to big savings… NVIDIA has updated Nemo Megatron, software for training large language models. The updates - sequence parallelism (SP) and selective activation recomputation (SAR) - makes training large-scale neural networks significantly more efficient. ""The latest updates to NeMo Megatron offer 30% speed-ups for training GPT-3 models ranging in size from 22 billion to 1 trillion parameters. Training can now be done on 175 billion-parameter models using 1,024 NVIDIA A100 GPUs in just 24 days–reducing time to results by 10 days, or some 250,000 hours of GPU computing, prior to these new releases,"" NVIDIA writes. Why this matters: By integrating basic improvements into training frameworks, NVIDIA is going to generate a large-scale impact on anyone who uses the Megatron framework. This illustrates how AI progress sometimes operates like a one-way ratchet - someone implements some changes in some increasingly widely used software, and efficiency jumps upward for all the users overnight. Read more: NVIDIA AI Platform Delivers Big Gains for Large Language Models (NVIDIA blog).",0.0
08/08/2022 - Import AI 299: The world's best language model is Made in China; NVIDIA boosts LLM training; OpenAI shows how to 'fill in the middle' on a LM - 1,http://eepurl.com/h8jfsX,2022-08-08,,"#################################################### Want to make a language model with a 'fill in the middle' option? Here's how! …Sentence completion is cool, but infilling is useful as well… Here's a straightforward paper from OpenAI that describes how to give language models the ability to learn to infill text - e.g, taking a sentence and knocking out the middle of it and asking the model to 'fill in the middle'. The big insight: The main insight here is that you can learn to fill in the middle ""without compromising the left-to-right capability in pretraining…FIM models achieve the same test loss as AR models on left-to-right test loss while achieving lower FIM loss."". They also learn that it's inefficient to finetune a model to learn to fill in the middle, and you should generally do it at the pretraining stage instead. Why this matters: Somewhat like DeepMind's recent 'Chinchilla' paper (Import AI #290), which showed you can dramatically increase the capabilities of language models by training them on 5X data, this paper shows you can augment an LM with a nice edit function, and this doesn't come at a loss anywhere else. In fact, OpenAI shows that these ""models are strictly more capable than canonically trained left-to-right models, at least within the bounds of the evaluations we consider"". Read more: Efficient Training of Language Models to Fill in the Middle (arXiv).",1.0
08/08/2022 - Import AI 299: The world's best language model is Made in China; NVIDIA boosts LLM training; OpenAI shows how to 'fill in the middle' on a LM - 2,http://eepurl.com/h8jfsX,2022-08-08,,"#################################################### Google uses hybrid AI to improve its own code: …ML + semantic engines = useful capability… Google has combined machine learning and a rule-based semantic engine to train a Transformer-based system to do code completion on Google's internal codebase. Google looked at how 10,000 Googlers used this capability over the course of three months and the results are quite promising: Google saw a 6% reduction in coding iteration time (switching between builds and tests) and a 7% reduction in context switches (leaving the IDE). ""Currently, 3% of new code (measured in characters) is now generated from accepting ML completion suggestions,"" Google writes. What they did: Google trained a a transformer running on TPUs on code in Google's monorepo, using a context of between ~1000 and ~2000 tokens. The company trained a single model on a mix of 8 languages (C++, Java, Python, Go, Typescript, Proto, Kotlin, and Dart), and trained a relatively small model (0.5 billion parameters) to allow for fast inference. ""The model strongly benefits from the quality of the monorepo, which is enforced by guidelines and reviews,"" Google writes. Why this matters: This is another example of an 'AI flywheel' - Google is using its own code to train models to help its engineers more efficiently write better code, and it is using a (human-run, for now) acceptance process to maintain the quality of the underlying monorepo, so it can avoid pathological degradations due to garbage in/garbage out dynamics. This is also an area where 'economy of code scale' seems to matter - since Google famously has a single, gigantic internal monorepo, it's easier for the company to train a single model on it. Read more: ML-Enhanced Code Completion Improves Developer Productivity (Google AI Blog).",0.0
08/08/2022 - Import AI 299: The world's best language model is Made in China; NVIDIA boosts LLM training; OpenAI shows how to 'fill in the middle' on a LM - 3,http://eepurl.com/h8jfsX,2022-08-08,,"#################################################### Huawei builds its own GitHub Copilot: PanGu-Coder: …Another illustration of the 'fast follower' nature of Chinese labs… Researchers with Huawei (specifically, the Noah's Ark Lab, and Huawei Cloud), have built 'PanGu-Coder', a code completion model. PanGu-Coder is to PanGu as OpenAI's Codex is to GPT3 - think of it as a follow-up model using a similar training procedure, albeit on a different data distribution. And, much like PanGu, PanGu-Coder has been published about a year after the public launch of Codex (and GitHub Copilot), illustrating the surprisingly fast rate at which Chinese labs are able to replace large-scale models. What PanGu-Coder is: PanGu-Coder is a family of code models for code completion, varying in parameter size from 317million to 2.6 billion. In tests, Huawei claims PanGu-Coder does better than AlphaCode and GitHub Codex on a few human evaluations (though Salesforce's 'Codegen' model does quite well, also). Huawei also significantly improved the capabilities of PanGu-Coder by training a model called PanGu-Coder-FT, which is finetuned on a highly curated dataset. Why this matters: Code models, much like language models, are becoming like an all-purpose swiss army knife for a range of AI capability and alignment research. It's notable to me that Huawei has - again - managed to do a decent-looking replication of a frontier model developed by a Western lab. It's also notable that few universities have made attempts to replicate these models, due to the resources (both computational and in terms of technical skill) required. Read more:PanGu-Coder: Program Synthesis with Function-Level Language Modeling (arXiv).",1.0
08/08/2022 - Import AI 299: The world's best language model is Made in China; NVIDIA boosts LLM training; OpenAI shows how to 'fill in the middle' on a LM - 4,http://eepurl.com/h8jfsX,2022-08-08,,"#################################################### China releases GLM-130B, a very good language model: …The world's best public, open source language model is now Made in China… Researchers with China's Tsinghua University have built and released GLM-130B, a language model that outperforms OPT (Facebook's OS replication of GPT3), BLOOM (HuggingFace's OS replication of GPT3), and OpenAI's original GPT3. This is a pretty big deal, both for the raw capabilities it gives researchers, and for the fact the current best-performing OS language model is Chinese, rather than made in the West. The model was trained on around 400 A100 GPUs which they were able to get via a donation from a local AI startup. What's special about GLM: GLM outperforms the above-mentioned models, as well as homegrown Chinese models like ERNIE Titan 3.0 (Import AI 279). Read more: GLM-130B: An Open Bilingual Pre-Trained Model (Tsinghua). Get the model here: GLM-130B (THUDM, GitHub). Try the model for yourself: GLM-130B (HuggingFace).",1.0
08/08/2022 - Import AI 299: The world's best language model is Made in China; NVIDIA boosts LLM training; OpenAI shows how to 'fill in the middle' on a LM - 5,http://eepurl.com/h8jfsX,2022-08-08,,"#################################################### Tech Tales: Micro Religions During the transition there was a micro religion phase. The recommender systems had figured out just how important community was to people, during that time. So the recommenders started shuffling all the different users of all the different apps towards more and more specific niches. It started with commercial stuff - shoes, different 'aesthetics', watches, different locations to spend time at, different hobbies and so on. But eventually it found its way to theistic beliefs - what is the larger purpose of the world? These beliefs turned out to be fractal-like where the recommenders would find ways to push people into the most specific, narrow existing variations - e.g, traditional catholics versus mormons - but they got through that pretty quickly. Next, the recommenders and the generation systems started to autonomously build entire new belief structures (paired with aesthetic styles that materialized as buyable, wearable merchandise across the full variety of products). They then pushed people towards these, and pretty quickly people - especially young people - started identifying as all these different sub-types of religion. After The Events we all collectively looked back on this time as both quite special (some of the beliefs and aesthetics were tremendously strange and complicated), and also scary (there weren't religious wars, but there were warning signs of building-up inter-micro-religion conflict, though The Events happened shortly after and averted war, while bringing about some of the major changes). Things that inspired this story: Intersection of recommendation engines + generative models; large-scale advertising systems. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
07/25/2022 - Import AI 298: Mimetic models; LLM search engine raises $25m; UK splits from Europe on AI regulation - 0,http://eepurl.com/h7jJjn,2022-07-25,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Digital artist: DALL-E is a scam: …Gen models have brought a ton of people a ton of joy, but some are skeptical.. Here's a post from artist/game developer David OReilly arguing that generative models like Dall-E 2 are a scam. Specifically, because these models scrape a vast amount of image data and spit out new images on tap (in exchange for $, per OpenAI's recent commercialization of Dall-E), then that means ""paying for it benefits a tech company on the back of a century of human effort - a bullshit deal"", according to OReilly. Why this matters: This kind of argument reminds me against early arguments against things like sampling (for music creation), or collage (for making art out of other people's art). I think what makes (some) people nervous about Dall-E is the scale of resources required to develop it means, at least under capitalism, the destiny of these models is mostly to be as products. It feels like the reaction to stuff like Dall-E 2 would be radically different if it was provided as a public good (including free inference services). Many criticisms about AI are really criticisms about 'technology under capitalism' and it's worth trying to disentangle the two. Read OReilly's post here on his Instagram (Instagram).",0.0
07/25/2022 - Import AI 298: Mimetic models; LLM search engine raises $25m; UK splits from Europe on AI regulation - 1,http://eepurl.com/h7jJjn,2022-07-25,,"#################################################### Is AI alignment getting too much money? …AI alignment is important, but so is progress… Is the field of AI alignment sucking up too much funding? Researcher Bharath Ramsundar thinks so, arguing that the rapid expansion in funding for alignment might be silly. ""AI alignment dollars could probably be better directed to funding next generation American foundry companies to ensure that the entire AI industry isn’t cast into turmoil by a potential future CCP invasion of Taiwan,"" he writes. Jack's thoughts: As someone who works at the intersection of AI capabilities, policy, and alignment, I find this argument a bit confusing - it basically assumes funding sources for alignment are fungible with resources for things like chips and foundries, but I'd argue that funding here typically comes from different sources with different types of experience. It's not either/or, it's both. (Though I do agree we desperately need to increase funding for semiconductors, given how crucial they are to economic and national competitiveness, and the fact they're currently centralized in some unstable geographies). Read more: An Argument Against Funding AI Alignment (Deep into the forest, Substack).",0.0
07/25/2022 - Import AI 298: Mimetic models; LLM search engine raises $25m; UK splits from Europe on AI regulation - 2,http://eepurl.com/h7jJjn,2022-07-25,,"#################################################### Now that models can imitate people, what do we do? …All hail the era of the funhouse mirror model… A language model can do an impression of Einstein, a lawyer from Texas in the 19th century, and - given enough information - you. Now, researchers with the University of Toronto, Cornell University and Microsoft Research have grappled with the issues these so-called 'Mimetic Models' may produce. What they are: A mimetic model is ""an algorithm that is trained on data from a specific individual in a given domain, and which is designed to accurately predict and simulate the behavior of this individual in new situations from the domain"", they write. ""Interacting with a mimetic model can be used as preparation for interactions in real life - essentially, as a means to an end."" How they might be used: These models will be used for tasks as varied as being a stand-in for oneself (e.g, answering emails for you), or being a stand-in for an opponent (e.g, preparing for a competition with someone, or a debate). They could also be used as 'mimetic counterfactuals' - how might a person change if they did something different with their life? Real world use: Mimetic models are already out there in the world - like AI21's marketing stunt to create a virtual 'Ruth Bader Ginsburg' model people can talk to (Import AI 296), or this experiment by an independent artist where they resurrect a childhood friend and the mimetic model tries to kill them using a microwave (Import AI 292). How to think about them: We should think about these models with reference to four key roles - the target that the model is designed to imitate, the person or organization that created the model, the operator who uses the model, and the interactor who interacts with the model or views its outputs. Why this matters: Because language models can approximate specific data distributions, it makes sense they can eventually represent people to a high level of fidelity. But I'm not sure the world is ready for the economic, security, and cultural implications of (digital) clones on tap. Read more: Mimetic Models: Ethical Implications of AI that Acts Like You (arXiv).",1.0
07/25/2022 - Import AI 298: Mimetic models; LLM search engine raises $25m; UK splits from Europe on AI regulation - 3,http://eepurl.com/h7jJjn,2022-07-25,,"#################################################### London heatwave downs Oracle and Google clouds: …AI, meet climate change… The recent heatwave across the UK caused outages in data centers used by Oracle and Google, according to Bloomberg. While only temporary, this illustrates the fragility of the infrastructure AI requires, and highlights how, as climate change gets more extreme, some of the 'input costs' for AI-supporting infrastructure may increase. Read more: Google, Oracle Data Centers Knocked Offline by London Heat (DataCenter Knowledge).",0.0
07/25/2022 - Import AI 298: Mimetic models; LLM search engine raises $25m; UK splits from Europe on AI regulation - 4,http://eepurl.com/h7jJjn,2022-07-25,,"#################################################### LLM-powered search app You raises $25m: …Language models might eat search engines… You, a search engine co-founded by Richard Socher, an AI researcher, has raised a $25m funding round. Socher says You has hundreds of thousands of users and a decent retention rate - not Google numbers, but not totally inconsequential. Why You matters: The most interesting part of You is how it incorporates a bunch of contemporary language models, providing inbuilt services for things like text analysis, summarization, code search, code completion, and so on. You.com also sits on LMs built by others, such as OpenAI's GPT-3 which powers the 'YouWrite' service. Why this matters: Contemporary AI models are very general and very powerful - startups like You.com help test out whether these AI systems could obviate or replace prior technology 'stacks'. This funding means You will be around for a while longer, so we can watch the experiment play out. Read more: You raises $25M to fuel its AI-powered search engine (TechCrunch).",0.0
07/25/2022 - Import AI 298: Mimetic models; LLM search engine raises $25m; UK splits from Europe on AI regulation - 5,http://eepurl.com/h7jJjn,2022-07-25,,"#################################################### UK looks at European Commission AI regulations and says 'that's too much', and proposes lightweight regulatory approach: …Which way, Western governments?... The UK government's Office for Artificial Intelligence has published a policy paper about how the UK government is going to approach AI regulation. The approach is designed to strike a balance between control and laissez faire development. The government describes its approach as ""a pro-innovation, light-touch and coherent regulatory framework, which creates clarity for businesses and drives new investment"". Key principles: The UK says it's going to approach AI regulation as a context-specific area, so it will create specific regulations for specific use cases. It also wants regulators to ""focus on high risk concerns rather than hypothetical or low risks associated with AI,"" as well as ""look for ways to support and encourage regulatory coordination"" given that the UK has a bunch of overlapping authorities with regard to AI. It's also generally steering away from hard regulation, noting that ""we will ask that regulators consider lighter touch options, such as guidance or voluntary measures, in the first instance"". Things that make you go 'hmmm': ""We will ask that regulators focus on high risk concerns rather than hypothetical or low risks associated with AI,"" it writes. Challenges for regulation: Regulating AI also comes with some challenges - for one thing, merely by introducing regulation you can make it harder for small businesses to operate (relative to large businesses, which will simply lawyer up). There are also standard things to work through, like overlaps across different authorities, and inconsistencies among regulators. Defining AI: Any policy document needs to define AI, and this is no different. Here, they try and do a pretty light touch, where they define an AI system as having two big characteristics - how adaptive it is to different scenarios, and how autonomously it can function. These feel like somewhat useful definitions, though in practice they're a bit mangled (e.g, the report defines a transformer-based language model as being highly autonomous as it can generate a bunch of text itself, whereas I suspect most people would think of AI systems being autonomous if they took a bunch of actions in an environment, like an RL agent). AI principles: In regulating AI, the UK government says it will stick to the following principles: Feedback requested: Like most government policies, the UK government is taking feedback on these ideas. Specifically, it wants to hear from people about what the contemporary challenges of regulating AI are, whether the proposed context-driven approach is effective, if and how the UK could establish cross-sectoral principles, how best to implement this approach, and if any data sources exist which could help the government monitor the effectiveness of its approach. Read more: Establishing a pro-innovation approach to regulating AI (GOV.UK).",0.0
07/25/2022 - Import AI 298: Mimetic models; LLM search engine raises $25m; UK splits from Europe on AI regulation - 6,http://eepurl.com/h7jJjn,2022-07-25,,"#################################################### The Immemorial Now ""It used to cost millions of dollars and terabytes of data to reanimate a family member. But these days you just need a few photographs, about a hundred dollars, and some patience. Basically you describe the family member and then your glasses layer them into your world, and then they give the family member a voice and back it onto a customized language model. If you've got some old movies of them, you can clone the voice. They act a bit strange at first, but if you just keep describing them and recounting your memories of them, the underlying model is able to capture them eventually. Then you look around and you're there with them,"" he said. ""Honestly, I think it could really help you."" I was uneasy about it. It didn't feel right to me. But on the other hand, there I was, sitting with my sadness and bumming out my friends and talking, as I tended to, about the dead and departed. ""Of course we're gonna support you,"" he said. ""But maybe this is a way to support yourself."" ""And you've done it?"" ""Oh, absolutely! Why do you think I talk about my grandad so much? He passed years ago, but this way I can still see him sometimes. I like his jokes."" ""But they're not his jokes, they're some AI coming up with jokes."" ""Doesn't make much of a difference - they're the same jokes he used to tell, and he looks like himself, and sounds like himself. What's it - if it walks like a granddad and talks like a grandad, then it's probably a granddad you know?"" My dream helped me make the decision. It was a warped memory. We were in the kitchen of the old house and she was there and we were making bread together. She turned to me and asked me to pass her something and though I knew what she meant, I couldn't hear her voice. I stared at her and started to panic and then I woke up in bed, sweating, grasping mentally at the memory of her. I tried to calm myself down by imagining her talking to me. Then I realized I couldn't remember her voice. I became very sad and also very angry. I cried into my pillow. I tried to remember. I couldn't remember. A few days later, I was uploading some old videos of her into the resurrection machine. Then I spent a few days talking to the machine about her, telling it little anecdotes - even recounting some of my dreams. I gave it all the images I had of her. I obsessively searched over all my computers until I was sure I'd given it everything I had. Then one day I asked it to generate her. I put the glasses on and closed my eyes. Then I heard the little sound engineered to sound both reassuring and insistent. She was ready. I opened my eyes and there she was, and she looked at me and smiled and said ""I've missed you"", and it felt so real I let myself forget her unreality. Things that inspired this story: Resurrecting the dead with AI and how it can be both helpful and deeply personal; generative models; the intersection of augmented reality and AI; multimodal models, few-shot learning for vast multi-modal models; ideas about how, in the limit, AI lets us generate a stand-in for anything we have data for; mimetic models. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
