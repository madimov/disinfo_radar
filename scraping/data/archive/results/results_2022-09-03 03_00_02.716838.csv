title,url,date,summary,cleaning,category
"Plan, Edit, Explain and Repeat: The PEER Collaborative Language Model Brings a Humanlike Process to Text Generation",https://syncedreview.com/2022/09/01/plan-edit-explain-and-repeat-the-peer-collaborative-language-model-brings-a-humanlike-process-to-text-generation/,2022-09-01,"
In the new paper PEER: A Collaborative Language Model, a research team from Meta AI, Carnegie Mellon University, PSL University, and University College London presents PEER, a collaborative language model that performs a humanlike writing process — composing drafts, adding suggestions, proposing edits and providing explanations for its actions.

 ","While contemporary large-scale language models have excelled in text generation, these models are designed to generate only a final text and lack capabilities — such as the modifying and refining drafts — that characterize real-world collaborative writing workflows and are crucial for developing accurate and high-quality final texts. A research team from Meta AI, Carnegie Mellon University, PSL University, and University College London addresses this limitation in the new paper PEER: A Collaborative Language Model. Their proposed PEER (Plan, Edit, Explain, Repeat) collaborative language model produces texts following a humanlike process — composing drafts, adding suggestions, proposing edits and providing explanations for its actions. The team summarizes their main contributions as follows: The PEER model comprises four main steps: Plan, Edit, Explain, and Repeat. Given an input text, the user or the PEER model can first specify a plan with regard to actions to be applied. This plan is then realized via edits that the model explains using textual comments and reference citing. PEER repeats this process until it generates the desired output. For their empirical study, the team initialized all instances of PEER from an LM-Adapted T5 (Text-to-Text Transfer Transformer, Raffel et al., 2020). They compared it with baselines (OPT, GPT3, etc.) to evaluate its ability to follow plans and perform meaningful edits in domains with no available edit histories; and to examine how the PEER-Undo, PEER-Explain, and PEER-Document encoder-decoder models can boost performance. The results show that PEER can continuously improve output quality during the iterative process and achieves impressive performance across various domains and editing tasks. Overall, this work demonstrates that PEER can serve as a helpful and humanlike writing assistant that widens the scope and advances the performance of intelligent agents in producing high-quality textual outputs. The paper PEER: A Collaborative Language Model is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Dynamic Memory-based Curiosity: A Bootstrap Approach for Exploration,"[{'href': 'http://arxiv.org/abs/2208.11349v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2208.11349v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-08-24 07:56:12,,"2 2 0 2 g u A 2 1 ] C H . s c [ 1 v 3 1 2 6 0 . 8 0 2 2 : v i X r a What is it like to program with artiﬁcial intelligence? Advait Sarkar Microsoft Research University of Cambridge advait@microsoft.com Andrew D. Gordon Microsoft Research University of Edinburgh adg@microsoft.com Carina Negreanu Microsoft Research cnegreanu@microsoft.com Christian Poelitz Microsoft Research cpoelitz@microsoft.com Sruti Srinivasa Ragavan Microsoft Research a-srutis@microsoft.com Ben Zorn Microsoft Research ben.zorn@microsoft.com Figure 1 – Code generation using the GitHub Copilot editor extension. The portion highlighted in blue has been generated by the model. Left: a function body is generated based on a textual description in a comment. Right: a set of test cases is generated. Source: copilot.github .com Abstract Large language models, such as OpenAI’s codex and Deepmind’s AlphaCode, can generate code to solve a variety of problems expressed in natural language. This technology has already been commercialised in at least one widely-used programming editor extension: GitHub Copilot. In this paper, we explore how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance. We draw upon pub- licly available experience reports of LLM-assisted programming, as well as prior usability and design studies. We ﬁnd that while LLM-assisted programming shares some properties of compilation, pair programming, and programming via search and reuse, there are fundamental differences both in the technical possibilities as well as the practical experience. Thus, LLM-assisted programming ought to be viewed as a new way of programming with its own distinct properties and challenges. Finally, we draw upon observations from a user study in which non-expert end user programmers use LLM-assisted tools for solving data tasks in spreadsheets. We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise. 1. Introduction Inferential assistance for programmers has manifested in various forms, such as programming by demon- stration, declarative programming languages, and program synthesis (Section 2). Large language models such as GPT mark a quantitative and qualitative step-change in the automatic generation of code and nat- ural language text. This can be attributed to cumulative innovations of vector-space word embeddings, the transformer architecture, large text corpora, and pre-trained language models (Section 3). 1 These models have been commercialised in the form of APIs such as OpenAI Codex, or as programmer- facing tools such as GitHub Copilot and Tabnine. These tools function as a sort of advanced autocom- plete, able to synthesize multiple lines of code based on a prompt within the code editor, which may be natural language (e.g., a comment), code (e.g., a function signature) or an ad-hoc mixture. The capa- bilities of such tools go well beyond traditional syntax-directed autocomplete, and include the ability to synthesize entire function bodies, write test cases, and complete repetitive patterns (Section 4). These tools have reliability, safety, and security implications (Section 5). Prior lab-based and telemetric research on the usability of such tools ﬁnds that developers generally appreciate the capabilities of these tools and ﬁnd them to be a positive asset to the development expe- rience, despite no strong effects on task completion times or correctness. Core usability issues include the challenge of correctly framing prompts as well as the effort required to check and debug generated code (Section 6). Longitudinal experience reports of developers support some of the lab-based ﬁndings, while contradict- ing others. The challenges of correctly framing prompts and the efforts of debugging also appear here. However, there are many reports that these tools do in fact strongly reduce task time (i.e., speed up the development process) (Section 7). Programming with large language models invites comparison to related ways of programming, such as search, compilation, and pair programming. While there are indeed similarities with each of these, the empirical reports of the experience of such tools also show crucial differences. Search, compila- tion, and pair programming are thus found to be inadequate metaphors for the nature of LLM-assisted programming; it is a distinct way of programming with its own unique blend of properties (Section 8). While LLM-assisted programming is currently geared towards expert programmers, arguably the great- est beneﬁciaries of their abilities will be non-expert end-user programmers. Nonetheless, there are is- sues with their direct application in end-user programming scenarios. Through a study of LLM-assisted end-user programming in spreadsheets, we uncover issues in intent speciﬁcation, code correctness, com- prehension, LLM tuning, and end-user behaviour, and motivate the need for further study in this area (Section 9). 2. Prior conceptualisations of intelligent assistance for programmers What counts as ‘intelligent assistance’ can be the subject of some debate. Do we select only features that are driven by technologies that the artiﬁcial intelligence research community (itself undeﬁned) would recognise as artiﬁcial intelligence? Do we include those that use expert-coded heuristics? Systems that make inferences a human might disagree with, or those with the potential for error? Mixed-initiative systems (Horvitz, 1999)? Or those that make the user feel intelligent, assisted, or empowered? While this debate is beyond the scope of this paper, we feel that to properly contextualise the qualitative difference made by large language models, a broad and inclusive approach to the term ‘intelligence’ is required. End-user programming has long been home to inferential, or intelligent assistance. The strategy of direct manipulation (Shneiderman & Norwood, 1993) is highly successful for certain types of limited, albeit useful, computational tasks, where the interface being used (“what you see”, e.g., a text editor or an image editor) to develop an information artefact can represent closely the artefact being developed (“what you get”, e.g., a text document or an image). However, this strategy cannot be straightforwardly applied to programs. Programs notate multiple possible paths of execution simultaneously, and they deﬁne “behaviour to occur at some future time” (Blackwell, 2002b). Rendering multiple futures in the present is a core problem of live programming research (Tanimoto, 2013), which aims to externalise programs as they are edited (Basman et al., 2016). The need to bridge the abstraction gap between direct manipulation and multiple paths of execution led to the invention of programming by demonstration (PBD) (Kurlander et al., 1993; Lieberman, 2001; Myers, 1992). A form of inferential assistance, PBD allows end-user programmers to make concrete demon- strations of desired behaviour that are generalised into executable programs. Despite their promise, PBD 2 systems have not achieved widespread success as end-user programming tools, although their idea sur- vives in vestigial form as various “macro recording” tools, and the approach is seeing a resurgence with the growing commercialisation of “robotic process automation”. Programming language design has long been concerned with shifting the burden of intelligence be- tween programmer, program, compiler, and user. Programming language compilers, in translating be- tween high-level languages and machine code, are a kind of intelligent assistance for programmers. The declarative language Prolog aspired to bring a kind of intelligence, where the programmer would only be responsible for specifying (“declaring”) what to compute, but not how to compute it; that responsibility was left to the interpreter. At the same time, the language was designed with intelligent applications in mind. Indeed, it found widespread use within artiﬁcial intelligence and computational linguistics research (Colmerauer & Roussel, 1996; Rouchy, 2006). Formal veriﬁcation tools use a speciﬁcation language, such as Hoare triples (Hoare, 1969), and writing such speciﬁcations can be considered programming at a ‘higher’ level of abstraction. Program synthesis, in particular synthesis through reﬁnement, aims at intelligently transforming these rules into executable and correct code. However, the term “program synthesis” is also used more broadly, and programs can be synthesised from other sources than higher-level speciﬁcations. Concretely, program synthesis by example, or simply programming by example (PBE), facilitates the generation of executable code from input-output examples. An example of successfully commercialised PBE is Excel’s Flash Fill (Gulwani, 2011), which synthesises string transformations in spreadsheets from a small number of examples. The Cognitive Dimensions framework (T. R. Green, 1989; T. Green & Blackwell, 1998) identiﬁes three categories of programming activity: authoring, transcription, and modiﬁcation. Modern programmer as- sistance encompasses each of these. For example, program synthesis tools transform the direct authoring of code into the (arguably easier) authoring of examples. Intelligent code completions (Marasoiu et al., 2015) support the direct authoring of code. Intelligent support for reuse, such as smart code copy/paste (Allamanis & Brockschmidt, 2017) support transcription, and refactoring tools (Hermans et al., 2015) support modiﬁcation. Researchers have investigated inferential support for navigating source code (Hen- ley & Fleming, 2014), debugging (J. Williams et al., 2020), and selectively undoing code changes (Yoon & Myers, 2015). Additionally, intelligent tools can also support learning (Cao et al., 2015). Allamanis et al. (2018) assemble a literature review of research—at the intersection of machine learning, programming languages, and software engineering—that seeks to adapt methods ﬁrst developed for natural language, such as language models, to source code. The emergence of large bodies of open source code, sometimes called “big code”, enabled this research area. Language models are sensitive to lexical features like names, code formatting, and order of methods, while traditional tools like compilers or code veriﬁers are not. The authors hypothesise that sensitivity to lexical features matters for software engineering: The naturalness hypothesis. Software is a form of human communication; software corpora have similar statistical properties to natural language corpora; and these properties can be exploited to build better software engineering tools. The earliest evidence for this hypothesis goes back to research that used n-gram models to build a code completion engine for Java that outperformed Eclipse’s completion feature (Hindle et al., 2012, 2016). The survey covers the methods and the applications they enable: recommender systems (such as code autocompletion), debuggers, code analysers (such as type checkers (Raychev et al., 2015)), and code synthesizers. These applications constitute intelligence assistance to programmers, but limited by the capabilities of the underlying language models. We can expect the recent dramatic expansion in capability of language models, which we discuss next, to magnify the effectiveness of these applications. 3. A brief overview of large language models for code generation 3.1. The transformer architecture and big datasets enable large pre-trained models In the past decade, natural language processing has evolved both in the development of language models (LMs) as well as tasks and evaluation. Mikolov et al. (2013) introduced Word2Vec, where vectors are as- 3 signed to words such that similar words are grouped together. This is done by looking at co-occurrences in free text (like Wikipedia articles) and ignores the fact that words have multiple meanings depend- ing on context. Long short-term memory (LSTM) neural networks (Hochreiter & Schmidhuber, 1997; Sutskever et al., 2014) and later encoder-decoder networks, account for order in an input sequence. Self- attention (Vaswani et al., 2017) signiﬁcantly simpliﬁed the prior networks by replacing each element in the input by a weighted average of the rest of the input. Transformers combined the advantages of (multi-head) attention and word embeddings, enriched with positional encodings (they add the order information to the word embeddings) into one architecture. While there are many alternatives to trans- formers for language modelling, in this paper when we mention a language model (LM) we will usually imply a transformer-based language model. There are large collections of unlabelled text data for the most common natural languages. For example, the Common Crawl project1 produces around 20 TB of text data (from web pages) monthly, but labelled task-speciﬁc data is less prevalent. This makes unsupervised training appealing which leads to the concept of pre-trained LMs (J. Li et al., 2021). Pre-trained LMs are commonly trained to perform next- word prediction (such as GPT models, e.g. (Brown et al., 2020)) where the model is trained to predict the next word in a sequence or masked (such as Bert, e.g. (Devlin et al., 2019)) where the model is trained to ﬁll a gap in a sequence. Ideally, pre-trained LMs learn general-purpose abilities and knowledge by seeing large amounts of text, which can then be transferred to downstream language tasks (where we have less labelled data) such as question answering, ﬁction generation, text summarisation, etc. Fine-tuning is the process of adapting a given pre-trained LM to different downstream tasks by introducing additional parameters and training them using task-speciﬁc objective functions. In certain cases the pre-training objective also gets adjusted to better suit the downstream task. Instead of (or on top of) ﬁne-tuning, the downstream task can be reformulated to be similar to the original LLM training. In practice, this means expressing the task as a set of instructions to the LLM via a prompt. So the goal, rather than deﬁning a learning objective for a given task, is to ﬁnd a way to query the LLM to directly predict for the downstream task. This is sometimes referred to as Pre-train, Prompt, Predict.2 3.2. Language models tuned for source code generation The downstream task of interest to us in this paper is code generation, where we provide snippets of code (including comments) and we want new code to be generated. Unlike other downstream tasks, a large corpus of data is available from public code repositories such as GitHub. Code generation can be divided into many sub-tasks, such as type decorators (variable type generation, e.g. (J. Wei et al., 2020)), code summarization (comment generation, e.g. (Liu et al., 2021)), clone detection (duplicate detection, e.g (Mou et al., 2016)), code translation (code migration from one language to another e.g. (Nguyen et al., 2015)) etc. A recent benchmark that covers many tasks is CodeXGLUE (Lu et al., 2021). LLM technology has brought us within reach of full-solution generation. Codex (Chen, Tworek, Jun, Yuan, Ponde, et al., 2021), a version of GPT-3 ﬁne-tuned for code generation, can solve in one generation on average 47/164 problems in the HumanEval code generation benchmark. HumanEval is a set of 164 hand-written programming problems, which include a function signature, docstring, body, and several unit tests, with an average of 7.7 tests per problem. Smaller models have followed Codex, like GPT-J3 (ﬁne-tuned on top of GPT-2), CodeParrot4 (also ﬁne-tuned on top of GPT-2, targets Python generations), PolyCoder (Xu, Alon, et al., 2022)(GPT-2 style but trained directly on code). LLMs comparable in size to Codex include AlphaCode (Y. Li et al., 2022a) and PaLM-Coder (Chowd- hery et al., 2022). AlphaCode is trained directly on GitHub data and ﬁne-tuned on coding competition problems. It introduces a method to reduce from a large number of potential solutions (up to millions) 1https://commoncrawl.org/ 2http://pretrain.nlpedia.ai/ 3https://huggingface.co/docs/transformers/main/model_doc/gptj 4https://huggingface.co/blog/codeparrot 4 Figure 2 – Code generation using the GitHub Copilot editor extension. The portion highlighted in blue has been generated by the model. Above: a repetitive time computation is extrapolated based on two examples. Below: function body is generated from the signature and the ﬁrst line. Source: copilot.github.com to a handful of candidates (competitions permit a maximum of 10). On a dataset of 10000 programming problems, if given 5 attempts Codex manages to solve around 3% of the problems versus AlphaCode which manages around 4-7%. In competitions for which it was ﬁne-tuned (CodeContests) AlphaCode manages a 34% success rate, on par with the average human competitor. Despite promising results there are known shortcomings. Models can directly copy code (full solutions or key parts of the solutions) from the training data, rather than generating new code. Though developers make efforts to clean and retain only high-quality code, there are no guarantees of correctness and errors can be directly propagated through generations. Codex can also produce syntactically incorrect or undeﬁned code, and can invoke functions, variables, and attributes that are undeﬁned or outside the scope of the codebase. Moreover, Codex struggles to parse through increasingly long and higher-level or system-level speciﬁcations which can lead to mis- takes in binding operations to variables (especially when the number of operations and variables in the docstring is large). Various approaches have been explored to ﬁlter out bad generations or repair them (especially for syntax errors). Consistency is another issue and there is a trade-off between non-determinism and generation diversity. Some parameter settings can control the diversity of generation (i.e., how diverse the different gener- ations for a single prompt might be), but there is no guarantee that we will get the same generation if we run the system at different times under the same settings. To alleviate this issue in measurements, metrics such as pass@k (have a solution that passes the tests within k tries) have been modiﬁed to be probabilistic. 4. Commercial programming tools that use large language models OpenAI Codex is a version of GPT that is ﬁne-tuned on publicly available source code (Chen, Tworek, Jun, Yuan, de Oliveira Pinto, et al., 2021). While Codex itself is not a programmer-facing tool, OpenAI has commercialised it in the form of an API that can be built upon. 5 Figure 3 – Code generation using the Tabnine editor extension. The grey text after the cursor is being suggested by the model based on the comment on the preceding line. Source: tabnine.com Figure 4 – API suggestion using the Visual Studio IntelliCode feature. Source: Silver (2018) The principal commercial implementation of Codex thus far has been in Github Copilot.5 Copilot is an extension that can be installed into code editors such as Neovim, Jetbrains, and Visual Studio Code. Copilot uses Codex, drawing upon the contents of the ﬁle being edited, related ﬁles in the project, and ﬁle paths or URLs of repositories. When triggered, it generates code at the cursor location, in much the same way as autocomplete. To help expand developer expectations for the capabilities of Copilot beyond the previous standard uses of autocomplete, suggested usage idioms for Copilot include: writing a comment explaining what a function does, and the function signature, and allowing Copilot to complete the function body; complet- ing boilerplate code; and deﬁning test cases (Figures 1 and 5). Programmers can cycle between different generations from the model, and once a particular completion has been accepted it can be edited like any other code. As of 23 June 2022, Amazon has announced a Copilot-like feature called CodeWhisperer,6 which also applies a large language model trained on a corpus of source code to generate autocompletions based on comments and code. The marketing material describes a set of safety features, such as: detecting when generated code is similar to code in the training set, detecting known security vulnerabilities in the generated code, and “removing code that may be considered biased and unfair” (although this latter claim induces skepticism). At present CodeWhisperer is not widely available and thus little is known of its use in practice. Other commercial implementations of AI-assisted autocompletion features include Visual Studio Intel- 5https://copilot.github.com/ 6https://aws.amazon.com/codewhisperer/features/ 6 licode (Silver, 2018) (Figure 4) and Tabnine (Figure 3)7. These are more limited in scope than Copilot and their user experience is commensurable to that of using ‘traditional’ autocomplete, i.e., autocom- plete that is driven by static analysis, syntax, and heuristics.8.The structure of the machine learning model used by these implementations is not publicly disclosed; however, both rely on models that have been trained on large corpora of publicly available source code. It is interesting to note, that despite the wide variety of types of intelligent programmer assistance we have discussed in Section 2 for several aspects of programming (authoring, transcription, modiﬁcation, debugging, and learning), commercial implementations of assistance based on large language models thus far are aimed primarily at authoring. Authoring can be viewed as the ﬁrst natural application of a generative language model, but the programming knowledge in these models can of course be used for assisting programmers in other activities, too. 5. Reliability, safety, and security implications of code-generating AI models AI models that generate code present signiﬁcant challenges to issues related to reliability, safety, and security. Since the output of the model can be a complex software artifact, determining if the out- put is “correct” needs a much more nuanced evaluation than simple classiﬁcation tasks. Humans have trouble evaluating the quality of software, and practices such as code review, applying static and dy- namic analysis techniques, etc., have proven necessary to ensure good quality of human-written code. Current methods for evaluating the quality of AI-generated code, as embodied in benchmarks such as HumanEval (Chen, Tworek, Jun, Yuan, de Oliveira Pinto, et al., 2021), MBPP (Austin et al., 2021), and CodeContests (Y. Li et al., 2022b), determine functional correctness of entire functions based on a set of unit tests. Such evaluation approaches fail to consider issues of code readability, completeness, or the presence of potential errors that software developers constantly struggle to overcome. Previous work (Chen, Tworek, Jun, Yuan, de Oliveira Pinto, et al., 2021) explores numerous implications of AI models that generate code, including issues of over-reliance, misalignment (the mismatch between what the user prompt requests and what the user really wants), bias, economic impact, and security implications. While these topics each are extensive and important, due to space limitations we only brieﬂy mention them here and point to additional related work when possible. Over-reliance occurs when individuals make optimistic assumptions about the correctness of the output of an AI model, leading to harm. For code generating models, users may assume the code is correct, has no security vulnerabilities, etc. and those assumptions may lead to lower quality or insecure code being written and deployed. Existing deployments of AI models for code, such as GitHub Copilot (Ziegler, 2021), have documentation that stresses the need to carefully review, test, and vet generated code just as a developer would vet code from any external source. It remains to be seen if over-reliance issues related to AI code generation will result in new software quality challenges. Since AI that generates code is trained on large public repositories, there is potential for low-quality training data to inﬂuence models to suggest low-quality code or code that contains security vulnera- bilities. One early study of GitHub Copilot (Pearce et al., 2021) examines whether code suggestions may contain known security vulnerabilities in a range of scenarios and ﬁnds cases where insecure code is generated. Beyond carefully screening new code using existing static and dynamic tools that detect security vulnerabilities in human-generated code, there are also possible mitigations that can reduce the likelihood that the model will make such suggestions. These include improving the overall quality of the training data by removing low-quality repositories, and ﬁne-tuning the large-language model speciﬁcally to reduce the output of known insecure patterns. 6. Usability and design studies of AI-assisted programming Vaithilingam et al. (2022) conducted a within-subjects comparative study (n=24) of Github Copilot, 7https://www.tabnine.com/ 8As of 15 June 2022, Tabnine has announced a shift to language model-driven autocompletion that more closely resembles the abilities of Copilot (Weiss, 2022). 7 comparing its user experience to that of traditional autocomplete (speciﬁcally, the Intellisense plugin, not the same as the Intellicode feature mentioned previously). Participants failed to complete the tasks more often with Copilot than with Intellisense, and there was no signiﬁcant effect on task completion time. Perhaps unsurprisingly, the authors ﬁnd that assessing the correctness of generated code is difﬁcult and an efﬁciency bottleneck, particularly when the code generated has a fundamental ﬂaw or inefﬁciency that leads the programmer on an ultimately unsuccessful ‘wild goose chase’ of repair or debugging. However, the overwhelming majority (19 of 24) of participants reported a strong preference for Copilot in a post-task survey. While participants were less conﬁdent about the code generated by Copilot, they almost universally (23 of 24) perceived it as more helpful, because it had the potential for generating useful starting points and saving the programmer the effort of searching online for documented solutions that could be the basis for reuse. Ziegler et al. (2022) conducted a survey (n=2,047) of the perceived productivity of Copilot users in the USA. They matched these to telemetric usage measurements of the Copilot add-in, which included metrics such as how often an auto-completion was shown, how often it was accepted, how often it per- sisted unchanged in the document for a certain time period, how often it persisted with minor variations (e.g., measured by Levenshtein distance) and so on. They ﬁnd that the acceptance rate (the ratio of ac- cepted suggestions to shown suggestions) is the strongest predictor of users’ perceived productivity due to Copilot. Fascinatingly, they ﬁnd that the pattern of acceptance rates for all users in aggregate follows a daily and weekly “circadian” rhythm, such that users are more likely to accept Copilot completions out of working-hours and on weekends. However, for any given user, the acceptance rate depends on that user’s normal working hours; suggestions outside of normal working hours are less likely to be accepted. Future work is needed to see whether this ﬁnding replicates, and if so to establish how and why acceptance rates are so signiﬁcantly affected by working hours. Xu, Vasilescu, & Neubig (2022) conducted a within-subjects study (n=31) comparing the programming experience with and without a code generation plugin. Their experimental plugin takes the form of a text ﬁeld in which the user enters a natural language prompt, the system responds with a list of code snippets, and when clicked the desired snippet is inserted at the cursor. This workﬂow differs from Copilot’s, where the ‘prompt’ is text within the source ﬁle, and can contain a mix of natural language comments and code. The plugin supported both code generation (using a tree-based neural network) and code snippet retrieval (searching the programming forum Stack Overﬂow). Results from both generation and retrieval are shown in the same list, but visually demarcated. The authors found no signiﬁcant effect of the plugin on task completion time or program correctness. They found that simple queries were more likely to be answered correctly through generation, and more complex queries requiring multiple steps were more likely to be answered correctly though retrieval, and that it was possible to predict which approach would succeed based on the word content of the queries. Further, they found that most (60%) natural language queries that participants wrote in their experiment were not sufﬁciently well-speciﬁed for a human expert to write code implementing those intents. Retrieved snippets were edited more often than generated snippets, mostly to rename identiﬁers and choose different parameters. In a post- experiment survey, participants reported mostly feeling neutral or somewhat positive (30 of 31). These participants felt that the plugin was helpful for ﬁnding snippets they were aware of but cannot recall, and less disruptive than using a browser, but the interaction worked better when the developer had a pre-existing knowledge of the target APIs and frameworks, and it took experimentation to understand the “correct way” to formulate queries. There was no clear indication of preference between retrieval and generation. Jiang et al. (2022) developed an LLM-based tool for converting natural language statements to code. As in Xu, Vasilescu, & Neubig (2022), prompts are entered in a pop-up dialog invoked at the cursor from within a code editor, rather than as comments. In a study (n = 14), participants were given a week to complete two website-building tasks with the tool, while recording the screen, and were interviewed afterwards. As in other studies, participants saw utility in the tool for facilitating quick API lookups and for writing boilerplate code. They found that novice programmers’ queries were mainly natural 8 for Figure 5 – Searching for code snippets using Bing Developer Assistant. Stack Overﬂow is shown. Note how the query “generate md5 hash from string @line” contains a hint about the identiﬁer line, which is used to rewrite the retrieved snippet. Source: https://www.microsoft.com/en-us/research/publication/ building-bing-developer-assistant/ A result language, whereas experts were more likely to mix code into their requests. While some queries were abstract, and expressed high-level goals, most had low granularity, being “roughly equivalent to a line of code”. To cope with model failures, participants used a variety of strategies to reword their query, such as reducing the scope of the request or replacing words with alternatives, but no particular strategy was observed to be more effective than any other. Participants struggled with forming a mental model of what the model can understand and the “syntax” of the language it required – this is precisely the fuzzy abstraction matching problem we described earlier, which the authors call an “uncanny valley”. The authors suggest possible solutions such as automated rewording of prompts, suggesting simpler tasks, suggesting task breakdowns, and better onboarding and tutorials. Barke et al. (2022) studied how programmers (n = 20) use GitHub Copilot to complete short program- ming tasks in Python, Rust, Haskell, and Java. Through analysis of screen recordings, the authors identifed two primary modes of interaction with Copilot: acceleration, where the programmer has a well-formed intent and Copilot speeds up code authoring in “small logical units”, and exploration, where Copilot suggestions are used to assist the planning process, “help them get started, suggest potentially useful structure and API calls, or explore alternative solutions”. In acceleration, long code suggestions, which take time to read and evaluate, can break the programmer’s ﬂow. Participants developed heuristics for quickly scanning suggestions, such as looking for the presence of certain keywords. In exploration, participants were more likely to prompt using purely natural language comments, rather than a mix of comments and code. Moreover, these prompt comments were often ‘cleaned’ subsequent to accepting a suggestion, which implies a form of ‘instruction language’ that is separate from ‘explanation language’. The Bing Developer Assistant (Y. Wei et al., 2015; Zhang et al., 2016) (also referred to as Bing Code Search) was an experimental extension for Visual Studio initially released in 2015. It enabled an in-IDE, identiﬁer-aware search for code snippets from forums such as Stack Overﬂow. It had the ability to rewrite retrieved code to use identiﬁers from the programmer’s current ﬁle. A user study (n=14) comparing task time in performing 45 short programming tasks with the extension versus regular web search found on average 28% of time was saved with the extension. Morever telemetry data gathered over three weeks (representing around 20,000 users and around 3,000 queries per day) showed that several programmers used the feature frequently. Some used it repeatedly for related problems in quick succession, showing its use in multi-step problems. Others issued the same query multiple times on separate days, suggesting 9 that the speed of auto-completion was useful even if the programmer knew the solution. 7. Experience reports At present, there is not a lot of research on the user experience of programming with large language models beyond the studies we have summarised in Section 6. However, as the availability of such tools increases, professional programmers will gain long-term experience in their use. Many such program- mers write about their experiences on personal blogs, which are then discussed in online communities such as Hacker News. Inspired by the potential for these sources to provide rich qualitative data, as pointed out by Barik (Barik et al., 2015; Sarkar et al., 2022), we draw upon a few such experience reports. A full list of sources is provided Appendix A; below we summarise their key points. 7.1. Writing effective prompts is hard As with several other applications of generative models, a key issue is the writing of prompts that in- crease the likelihood of successful code generation. The mapping that these models learn between natural language and code is very poorly understood. Through experimentation, some have developed heuristics for prompts that improve the quality of the code generated by the model. One developer, after building several applications and games with OpenAI’s code-davinci model (the second generation Codex model), advises to “number your instructions” and creating “logic ﬁrst” before UI elements. Another, in using Copilot to build a classiﬁer for natural language statements, suggests to provide “more detail” in response to a failure to generate correct code. For example, when asking Copilot to “bina- rize” an array fails, they re-write the prompt to “turn it into an array where [the ﬁrst value] is 1 and [the second value] is 0” – effectively pseudocode – which generates a correct result. Commenters on Hacker News are divided on the merits of efforts invested in developing techniques for prompting. While some see it as a new level of abstraction for programming, others see it as indirectly approaching more fundamental issues that ought to be solved with better tooling, documentation, and language design: “You’re not coding directly in the language, but now you’re coding in an implicit language provided by Copilot. [...] all it really points out is that code documentation and discovery is terrible. But I’m not for sure writing implicit code in comments is really a better approach than seeking ways to make discovery of language and library features more discoverable.” “[...] the comments used to generate the code via GitHub Copilot are just another very inefﬁcient programming language.” “[Responding to above] There is nonetheless something extremely valuable about being able to write at different levels of abstraction when developing code. Copilot lets you do that in a way that is way beyond what a normal programming language would let you do, which of course has its own, very rigid, abstractions. For some parts of the code you’ll want to dive in and write every single line in painstaking detail. For others [...] [Copilot] is maybe enough for your purposes. And being able to have that ability, even if you think of it as just another programming language in itself, is huge.” Being indiscriminately trained on a corpus containing code of varying ages and (subjective) quality has drawbacks; developers encounter generated code which is technically correct, but contains practices considered poor such as unrolled loops and hardcoded constants. One Copilot user found that: “Copilot [...] has made my code more verbose. Lines of code can be liabilities. Longer ﬁles to parse, and more instances to refactor. Before, where I might have tried to consolidate an API surface, I ﬁnd myself maintaining [multiple instances].” Another Copilot user reﬂected on their experience of trying to generate code that uses the fastai API, which frequently changes: “[...] since the latest version of fastai was only released in August 2020, GitHub Copilot was not able to provide any relevant suggestions and instead provided code for using older versions of fastai. [...] To me, this is a major concern [...] If we are using cutting edge tools [...] Copilot has no knowledge of this and cannot provide useful suggestions.” On the other hand, developers can also be exposed to better practices and APIs through these models. The developer that found Copilot to make their code more verbose also observed that: 10 “Copilot gives structure to Go errors . [...] A common idiom is to wrap your errors with a context string [which can be written in an inconsistent, ad-hoc style] [...] Since using Copilot, I haven’t written a single one of these error handling lines manually. On top of that, the suggestions follow a reasonable structure where I didn’t know structure had existed before. Copilot showed me how to add structure in my code in unlikely places. For writing SQL, it helped me write those annoying foreign key names in a consistent format [...] [Additionally,] One of the more surprising features has been [that] [...] I ﬁnd myself discovering new API methods, either higher-level ones or ones that are better for my use case.” In order to discover new APIs, of course, the APIs themselves need to be well-designed. Indeed, in some cases the spectacular utility of large language models can be largely attributed to the fact that API designers have already done the hard work of creating an abstraction that is a good ﬁt for real use cases (Myers & Stylos, 2016; Piccioni et al., 2013; Macvean et al., 2016). As a developer who used Copilot to develop a sentiment classiﬁer for Twitter posts matching certain keywords remarks, “These kinds of things are possible not just because of co pilot [sic] but also because we have awesome libraries which have abstracted a lot of tough stuff.” This suggests that API design, not just for human developers but also as a target for large language models, will be important in the near and mid-term future. Moreover, breaking down a prompt at the ‘correct’ level of detail is also emerging as an important developer skill. This requires at least some familiarity, or a good intuition, for the APIs available. Breaking down prompts into steps so detailed that the programmer is effectively writing pseudocode, can be viewed as an anti-pattern, and can give rise to the objections cited earlier that programming via large language models is simply a “very inefﬁcient programming language”. We term this the problem of fuzzy abstraction matching. The problem of ﬁguring out what the system can and can’t do, and matching one’s intent and instructions with the capabilities of the system, is not new – it has been well-documented in natural language interaction (Mu & Sarkar, 2019; Luger & Sellen, 2016). It is also observed in programming notation design as the ‘match-mismatch’ hypothesis (T. R. Green & Petre, 1992; Chalhoub & Sarkar, 2022). In the broadest sense, these can be seen as special cases of Norman’s “gulf of execution” (Hutchins et al., 1985), perhaps the central disciplinary problem of ﬁrst and second- wave (Bødker, 2015) human-computer interaction research: ‘how do I get the computer to do what I want it to do?’. What distinguishes fuzzy abstraction matching from previous incarnations of this problem is the re- silience to, and accommodation of, various levels of abstraction afforded by large language models. In previous natural language interfaces, or programming languages, the user needed to form an extremely speciﬁc mental model before they could express their ideas in machine terms. In contrast, large lan- guage models can generate plausible and correct results for statements at an extremely wide range of abstraction. In the context of programming assistance, this can range from asking the model to write programs based on vague and underspeciﬁed statements, requiring domain knowledge to solve, through to extremely speciﬁc and detailed instructions that are effectively pseudocode. This ﬂexibility is ulti- mately a double-edged sword: it has a lower ﬂoor for users to start getting usable results, but a higher ceiling for getting users to maximum productivity. In the context of programming activities, exploratory programming, where the goal is unknown or ill- deﬁned (Kery & Myers, 2017; Sarkar, 2016), does not ﬁt the framing of fuzzy abstraction matching (or indeed any of the variations of the gulf of execution problem). When the very notion of a crystallised user intent is questioned, or when the design objective is for the system to inﬂuence the intent of the user (as with much designerly and third-wave HCI work), the fundamental interaction questions change. One obvious role the system can play in these scenarios is to help users reﬁne their own concepts (Kulesza et al., 2014) and decide what avenues to explore. Beyond noting that such activities exist, and fall outside the framework we have proposed here, we will not explore them in greater detail in this paper. 7.2. The activity of programming shifts towards checking and unfamiliar debugging When code can be generated quickly, as observed with the studies in Section 6, checking the correctness of generating code becomes a major bottleneck. This shift, or tradeoff, of faster authoring at the expense 11 of greater time spent checking code, is not without criticism. For some it is the wrong balance of priorities between system and programmer. Correspondingly, some users have developed heuristics for when the cost of evaluating the correctness of the code is greater than the time or effort saved by code generation, such as to focus on very short (e.g., single line) completions and ignore longer completions. Furthermore, some users have found that rather than having suggestions show all the time, which can be distracting and time consuming, more intentional use can be made of Copilot by switching off auto- suggestion and only triggering code completion manually using a keyboard shortcut. However, this requires users to form a mental model of when Copilot is likely to help them in their workﬂow. This mental model takes time and intentionality to build, and may be incorrect. Moreover, it introduces a new cognitive burden of constantly evaluating whether the current situation would beneﬁt from LLM assistance. Commenters on Hacker News raise these issues: “I ﬁnd I spend my time reviewing Copilot suggestions (which are mostly wrong) rather than thinking about code and actually doing the work.” “[...] It’s much quicker to read code than to write it. In addition, 95% of Copilots suggestions are a single line and they’re almost always right (and also totally optional).[...] I admit that I’m paranoid every time it suggests more than 2 lines so I usually avoid it. [...] I’ve run into Copilot induced headaches twice. Once was in the ﬁrst week or so of using it. I sweared off [sic] of using it for anything more than a line then. Eventually I started to ease up since it was accurate so often and then I learned my second lesson with another mistake. [...]” “[...] writing code is not the bottleneck in need of optimization. Conceiving the solution is. Any time “saved” through Copilot and it’s ilk is immediately nulliﬁed by having to check it’s correctness. [...]” “What I want is a copilot that ﬁnds errors [...] Invert the relationship. I don’t need some boilerplate generator, I need a nitpicker that’s smarter than a linter. I’m the smart thinker with a biological brain that is inattentive at times. Why is the computer trying to code and leaving mistake catching to me? It’s backwards.” “I turned off auto-suggest and that made a huge difference. Now I’ll use it when I know I’m doing something repetitive that it’ll get easily, or if I’m not 100% sure what I want to do and I’m curious what it suggests. This way I get the help without having it interrupt my thoughts with its suggestions.” Another frequent experience is that language models can introduce subtle, difﬁcult to detect bugs, which are not the kind that would be introduced by a human programmer writing code manually. Thus, existing developer intuitions around the sources of errors in programs can be less useful, or even misleading, when checking the correctness of generated code. One developer reported their experience of having an incorrect, but plausible-sounding ﬁeld name sug- gested by Copilot (accessTokenSecret instead of accessSecret) and the consequent wild goose chase of debugging before discovering the problem. As sources of error, these tools are new, and developers need to learn new craft practices for debugging. “There are zero places that can teach you those things. You must experience them and unlock that kind of knowledge.”, the developer con- cludes, “Don’t let code completion AI tools rule your work. [...] I don’t blame [Copilot] for this. I blame myself. But whatever. At least I got some experience.”. Commenters on Hacker News report similar experiences: “[...] The biggest problem I’ve had is not that it doesn’t write correctly, it’s that it think it knows how and then produce good looking code at a glance but with wrong logic. [...]” “[...] it has proved to be very good at producing superﬁcially appealing output that can stand up not only to a quick scan, but to a moderately deep reading, but still falls apart on a more careful reading. [...] it’s an uncanny valley type effect. [...] it’s almost the most dangerous possible iteration of it, where it’s good enough to fool a human functioning at anything other than the highest level of attentiveness but not good enough to be correct all the time. See also, the dangers of almost self-driving cars; either be self-driving or don’t but don’t expect halfway in between to work well.” 12 “[...] The code it generates _looks_ right but is usually wrong in really difﬁcult to spot ways but things you’d never write yourself.” Many developers reported concerns around such tools repeating private information, or repeating copy- righted code verbatim, which might have implications for the licenses in their own projects. Notions of the dangers of such “stochastic parrots” (Bender et al., 2021) are not new and have been well-explored, and are not as directly connected to the user experience of programming assistance as some of the other concerns we have listed here. As such, we will not enter that discussion in depth here, except to mention that these concerns were present in several blog articles and online discussions. Thus, in practice, programmers describe the challenges of writing effective prompts, misinterpreted intent, code that includes subtle bugs or poor programming practices, the burden of inspecting and checking that generated code is correct, and worries about private information, plagiarism and copyright. 7.3. These tools are useful for boilerplate and code reuse Despite the challenges we have described so far in this section, the utility of these tools in certain contexts is undeniable, and some programmers report having developed workﬂows, in certain contexts, that are heavily dependent on AI assistance. Particularly for simple tasks that require a lot of “boilerplate” code, or common tasks for which there are likely to be snippets of code online which prior to these AI assistants would have required a web search to retrieve. Hacker News commenters write: “These days not having Copilot is a pretty big productivity hit to me. The other day Copilot somehow stopped offering completions for maybe an hour, and I was pretty shocked to realize how much I’ve grown to rely on just hitting tab to complete the whole line. (I was writing Go at the time which is on the boilerplatey side among the mainstream languages, so Copilot is particularly effective [...]” “I use GTP-3 codex [sic] daily when working. It saves me time, helps me explore unfamiliar lan- guages and APIs and generates approaches to solve problems. It can be shockingly good at coding in narrow contexts. It would be a mistake to miss the developments happening in this area” “[...] for a lot of quick programming questions, I’m ﬁnding I don’t even need a search engine. I just use Github Copilot. For example, if I wanted to remember how to throw an exception I’d just write that as a comment and let Copilot ﬁll in the syntax. Between that and ofﬁcial docs, don’t need a ton else.” “[...] It’s changing the way I write code in a way that I can already tell is allowing me to be much lazier than I’ve previously been about learning various details of languages and libraries. [...]” “[...] Github Copilot [...] pretty much replaced almost my entire usage of Stack Overﬂow.[...]” “[...] GitHub Copilot really shines in rote work: when it can correctly infer what you are about to do, it can and will assist you correctly. It’s not able to make big decisions, but in a pinch, it might be able to give hints. [...] If used right, Copilot can give developers a signiﬁcant velocity boost, especially in greenﬁeld projects where there is lots and lots of boilerplate to write. [...]” 8. The inadequacy of existing metaphors for AI-assisted programming 8.1. AI assistance as search In research studies, as well as in reports of developer experiences, comparisons have been drawn between the nature of AI programming assistance and programming by searching and reusing code from the Internet (or from institutional repositories, or from the same project, or from a developer’s previous projects). The comparison between AI programming assistance and search is a natural one, and there are many similarities. Superﬁcially, both have a similar starting point: a prompt or query that is predominantly natural language (but which may also contain code snippets). From the user perspective, both have an information asymmetry: the user does not know precisely what form the result will take. With both search and AI assistance, for any given query, there will be several results, and the user will need to invest time evaluating and comparing them. In both cases, the user may only get an inexact solution, or indeed nothing like what they want, and the user may need to invest time adapting and repairing what they get. 13 However, there are differences. When searching the web, programmers encounter not just code, but a variety of types of results intermingled and enmeshed. These include code snippets interspersed with human commentary, perhaps discussions on forums such as Stack Overﬂow, videos, and images. A search may return new APIs or libraries related to the query, thus showing results at different levels of abstraction. Search has signals of provenance: it is often (though not always) possible to determine the source of a code snippet on the web. There is a lot of information scent priming to assist with the information foraging task (Srinivasa Ragavan et al., 2016). In this way, programming with search is a mixed media experience. In contrast, programming with large language models can be said to be a ﬁxed media experience. The only output is tokens (code, comments, and data) that can be represented within the context of the code editor. This has some advantages: the increased speed of code insertion (which is the immediate aim) often came up in experience reports. However, the learning, exploration, and discovery, and access to a wide variety of sources and media types that occurs in web search is lost. Provenance, too is lost: it is difﬁcult to determine whether the generation is original to the model, or a stochastic parroting (Bender et al., 2021; Ziegler, 2021). Moreover, due to privacy, security, and intellectual property concerns, the provenance of code generated by large language models may be withheld or even destroyed (Sarkar, 2022). This suggests that in future assistance experiences, mixed-media search might be integrated into programmer assistance tools, or the models themselves might be made capable of generating more types of results than the simple code autocomplete paradigm of current tools. 8.2. AI assistance as compilation An alternative perspective is that AI assistance is more like a compiler. In this view, programming through natural language prompts and queries is a form of higher-level speciﬁcation, that is ‘compiled’ via the model to the source code in the target language, which is lower level. Let us (crudely) assume that as programming notations travel along the abstraction continuum from ‘lower’ to ‘higher’ levels, the programmer becomes, ﬁrstly, less concerned with the mechanistic details of program execution, and secondly, more and more declarative, specifying what computation is required rather than how to compute it. In general, these are desirable properties of programming notations, but they do not always make the activity of programming easier or more accessible. As people who write code in declarative languages or formal veriﬁcation tools will tell you, it’s often much more difﬁcult to specify the what than the how. The much more broadly adopted practice of test-driven development is adjacent; while tests are not necessarily written in a higher-level language than the code, they aim to capture a higher-level notion of correctness, the what of the problem being solved. Learning to be a test engineer takes time and experience, and the entire distinct career path of “software engineer in test” attests to the specialised requirements of programming at higher levels of abstraction. Some would draw a distinction between programming in a speciﬁcation language and a compiled pro- gramming language. Tony Hoare himself considers these different, on the grounds that while a compiler only aims to map a program from the source language into a ﬁnite set of valid programs in the target language, a speciﬁcation might be satisﬁed by an inﬁnite number of valid programs (pers comm., ﬁrst author, ca. 2014). Thus the technical and interaction design problems of programming through speciﬁ- cation reﬁnement encompasses, but is much broader than, the technical and interaction design problems of compilers. While we acknowledge this distinction, there is insufﬁcient empirical evidence from the experience reports summarised in Section 7 that working programmers themselves consistently make a meaningful distinction between these concepts. Programming with large language models, like in a higher-level notation, also allows the programmer to be less concerned with details of the target language. For example, developers in our experience reports relied on AI assistance to ﬁll in the correct syntax, or to discover and correctly use the appropriate API call, thus allowing them to focus on higher-level aspects of the problem being solved. However, there are fundamental differences between this experience and the experience of using a compiler. First, the abstraction is not complete, i.e., a programmer cannot completely be unaware of the target language, they 14 must still be able to understand and evaluate the generated code in order to use such tools effectively. With compilers, although knowledge of the target language can help experienced developers in certain circumstances, it is far from a prerequisite for effective usage. Moreover, compilers can be relied on almost universally to generate a correct and complete translation from source to target language, whereas programming with AI assistance involves the active checking and adaptation of translated code. Next, compilers are (comparatively) deterministic, in that they consistently produce the same output for the same input, but this is not the case for current AI programming tools (although this is not a fundamental limitation, and consistency can be enforced). Finally, though they are often criticised for being cryptic and unhelpful (Barik et al., 2018), compilers do offer levels of interaction and feedback through warnings and error messages, which help the programmer improve the code in the source language; there is currently no such facility with AI programming tools and this strikes us as an area with potential for innovation. Perhaps more profoundly, while natural language can be used to express concepts at a higher abstraction level, the range of abstraction expressible in natural language is much wider than with other forms of programming notation. Traditional programming notations with ad-hoc abstraction capabilities (subrou- tines, classes, etc.) allow programmers to manually raise the level of abstraction of their own code and APIs. But with code generated by language models, as we have seen from the reports in Section 7, a prompt can span the gamut from describing an entire application in a few sentences, to painstakingly describing an algorithm in step-by-step pseudocode. Thus it would be a mistake to view programming with AI assistance as another rung on the abstraction ladder. Rather, it can be viewed as a device that can teleport the programmer to arbitrary rungs of the ladder as desired. We close the discussion on AI assistance as a compiler with a few miscellaneous notes. The idea of using natural language as a programming notation has a long history (e.g., (Miller, 1981; Lieberman & Liu, 2006)), which we will not cover here. However, it is notable that there are many ways that natural language has been integrated with programming, such as debugging (Ko & Myers, 2004). With large language models, there are better capabilities for inference of intent and translation to code, but therefore also the potential to open up new strategies for inspecting and explaining code. There are also new failure modes for this paradigm of programming. 8.3. AI assistance as pair programming The third common perspective is that AI-assisted programming is like pair programming. GitHub Copi- lot’s commercial tagline describes it as “your AI pair programmer”. As opposed to search and compi- lation, which are both relatively impersonal tools, the analogy with pair programming is evocative of a more bespoke experience; assistance from a partner that understands more about your speciﬁc context and what you’re trying to achieve. AI-assisted programming does have the potential to be more person- alised, to the extent that it can take into consideration your speciﬁc source code and project ﬁles. As Hacker News commenters write: “[...] at one point it wrote an ENTIRE function by itself and it was correct. [...] it wasn’t some dumb boilerplate initialization either, it was actual logic with some loops. The context awareness with it is off the charts sometimes.[...]” “[...] It’s like having the stereotypical “intern” as an associate built-in to your editor. [...] It’s also ridiculously ﬂexible. When I start writing graphs in ASCII (cause I’m just quickly writing something down in a scratch ﬁle) it’ll actually understand what I’m doing and start autocompleting textual nodes in that ASCII graph.” Besides personalisation, the analogy also recalls the conventional role-division of pair programming between “driver” and “navigator”. When programming, one needs to form mental models of the program at many layers: from the speciﬁc statement being worked on, to its context in a subroutine, to the role that subroutine plays in a module, to the module within the program. However, code must be written at the statement level, which forces developers to keep this lowest level constantly at the forefront of their working memory. Experienced developers spend more time mapping out their code so that they can spend less time writing it. Research into code display and navigation has explored how different ways 15 of presenting lines of code can help programmers better keep these different layers of mental models in mind (Henley & Fleming, 2014). Pair programming, the argument goes, allows two partners to share the burden of the mental model. The driver codes at the statement and subroutine level while the navigator maps out the approach at the module and program level. By analogy to pair programming, the AI assistant taking the role of the driver, a solo programmer can now take the place of the navigator. But as we have seen, the experience of programming with AI assistance does not consistently absolve the human programmer of the responsibility for understanding the code at the statement and subroutine level. The programmer may be able to become “lazier [...] about learning various details of syntax and libraries”, but the experience still involves much greater statement-level checking. While a pair programming session requires a conscious, negotiated decision to swap roles, a solo pro- grammer with an AI assistant might ﬁnd themselves ﬂuidly traversing the spectrum from driving to navigation, from one moment to the next. This may partially explain why, in a preliminary experiment (n=21) comparing the experience of “pair programming” with GitHub Copilot to programming in a hu- man pair either as driver or navigator, Imai (2022) ﬁnds that programmers write more lines of code with Copilot than in a human pair, but these lines are of lower quality (more are subsequently deleted). Moreover, meta-analyses of pair programming have shown mixed efﬁcacy of human pair programming on task time, code quality and correctness (Salge & Berente, 2016; Hannay et al., 2009), suggesting that emulating the pair programming experience is not necessarily a good target to aim for. Multiple studies have concluded that the apparent successes of pair programming can be attributed, not to the role division into driver and navigator, but rather the high degree of verbalisation that occurs when pair programmers are forced to rationalise their decisions to each other (Hannay et al., 2009). Others have found that programming in pairs induces greater focus out of a respect for shared time; pair programmers are less likely to read emails, surf the web, or take long phone calls (L. A. Williams & Kessler, 2000). These particular beneﬁts of pair programming are not captured at all by AI assistance tools. The comparison to pair programming is thus relatively superﬁcial, and today’s experience of AI-assisted programming is not comparable with pair programming to the same extent as it is with search or compi- lation. 8.4. A distinct way of programming LLM-assisted programming assistance bears similarities to search: both begin with a prompt, both have an information asymmetry, there are several results, with inexact solutions. But there are differences: search is mixed-media, whereas LLM assistance is ﬁxed. Search (often) has provenance, and language models do not. It also bears similarities to compilation and programming by speciﬁcation. Both enable programming at a ‘higher’ level of abstraction (for some deﬁnition of higher). Yet unlike with compilers, a programmer using AI assistance must still have a working knowledge of the target language, they must actively check the output for correctness, and they get very little feedback for improving their ‘source’ code. It also bears a superﬁcial similarity to pair programming, in that it promises to let the programmer take the role of ‘navigator’, forming high-level mental models of the program while delegating the role of ‘driver’ to the language model. But unlike with pair programming, the human navigator must often hop into the driver’s seat. And unlike with pair programming, LLM-assisted programming does not require verbalisation, nor does it coerce greater focus out of a respect for shared time. Thus existing metaphors do not completely capture the experience of LLM-assisted programming. It is emerging as a distinct way of programming. It does not quite strike us as a distinct practice of pro- gramming, as that term has been applied to communities of programmers united by similar ethos and aims, such as enterprise software engineers, bricoleurs, live coders, and code benders; but as Bergström & Blackwell (2016) note, there are no clear criteria by which we can deﬁne the boundaries of a prac- tice. Nor does it strike us as being a new activity of programming as per the cognitive dimensions 16 Figure 6 – GridBook interface showing natural language formula in the spreadsheet grid. framework, since AI assistance is clearly orthogonal to authoring, transcription, and modiﬁcation, being applicable to each of these activities and others besides. Yet as a way of programming it seems to affect programmer’s experience more profoundly than a feature such as autocomplete, having far-reaching im- pact on their attitudes and practices of authoring, information foraging, debugging, refactoring, testing, documentation, code maintenance, learning, and more. 9. Issues with application to end-user programming The beneﬁts and challenges of programming with LLMs discussed so far concern the professional pro- grammer, or a novice programmer in training. They have formal training in programming and, often, some understanding of the imperfect nature of AI-generated code. But the majority of people who pro- gram do not fall into this category. Instead, they are ordinary end users of computers who program to an end. Such end-user programmers often lack knowledge of programming, or the workings of AI. They also lack the inclination to acquire those skills. It is reasonable to say that such end-user programmers (e.g., accountants, journalists, scientists, business owners) stand to beneﬁt the most from AI assistance, such as LLMs. In an ideal world, an end-user wanting to accomplish a task could do so by simply specifying their intent in familiar natural language without prior knowledge of the underlying programming model, or its syntax and semantics. The code will get generated and even automatically run to produce the desired output. However, as we have seen so far, the world is not ideal and even trained programmers face various chal- lenges when programming with AI. These challenges are only exacerbated for end-user programmers, as a study by Srinivasa Ragavan et al. (2022) observes. Participants in the study were data analysts (n=20) conducting exploratory data analysis in GridBook, a natural-language augmented spreadsheet system. In GridBook (Figure 6, adopted from Srinivasa Ra- gavan et al. (2022)) users can write spreadsheet formulas using the natural language (Figure 6: a-f); a formal formula is then synthesized from the natural language utterance. GridBook also infers the con- text of an utterance; for example, in Figure 6, the query in label 4 is a follow-up from label 3. Both the natural language utterance and the synthesized formula are persisted for users to edit and manipulate. 17 9.1. Issue 1: Intent speciﬁcation, problem decomposition and computational thinking When attempting to accomplish data analysis tasks using natural language, participants had to reﬁne their speciﬁcation of intent in the natural language several times, before they arrived at the desired result (if they did). The NL utterances were often underspeciﬁed, ambiguous, too complex, or contained domain phrases not speciﬁed in the context (e.g., in the data being analyzed). Thus, the ﬁrst issue is to communicate the capabilities of the system, and make it interpretable so users can see how their prompt is being interpreted. End-user programmers often also lack the key computational thinking skills (Wing, 2011), such as the ability to decompose problems into subproblems, reformulate problems in ways that can be computed by a system, etc. However, effective use of LLMs such as Codex requires such skills. For example, if these models are most accurate when solutions to a problem are single line, then the user should be able to break their problem into smaller sub-problems each of which can be solved in one or two lines. Moreover, they might also lack the ability to frame a problem as generic computational problems, rather than domain-speciﬁc problems. For example, a realtor is more likely to ask “which is the largest house” (declaratively), instead of “which is the house with maximum constructed area” (procedurally). Therefore, end-user computing environments powered by AI should help end-user programmers think “computationally”: they must aid users in breaking down their problems to smaller steps, or guiding users towards alternative strategies to specify or solve a problem (e.g., providing examples, offering alternatives) or even seek procedural prompts where needed (e.g., for disambiguation). 9.2. Issue 2: Code correctness, quality and (over)conﬁdence The second challenge is in verifying whether the code generated by the model is correct. In GridBook, users were able to see the natural language utterance, synthesized formula and the result of the for- mula. Of these, participants heavily relied on ‘eyeballing’ the ﬁnal output as a means of evaluating the correctness of the code, rather than, for example, reading code or testing rigorously. While this lack of rigorous testing by end-user programmers is unsurprising, some users, particularly those with low computer self-efﬁcacy, might overestimate the accuracy of the AI, deepening the overcon- ﬁdence end-user programmers are known to have in their programs’ accuracy Panko (2008). Moreover, end-user programmers might not be able to discern the quality of non-functional aspects of the generated code, such as security, robustness or performance issues. 9.3. Issue 3: Code comprehension and maintenance A third challenge with AI-driven programming is the issue of code comprehension. During GridBook’s user evaluation, participants mentioned that the generated formulas are hard to understand, even when users were familiar with the target language. This has potentially severe consequences: from evaluating the accuracy of the program by verifying logic, to the ability to customize code, to future debugging and reuse. As we discussed earlier, this problem also exists for trained developers. One approach to address this issue is for the AI system to include some notion of code readability or comprehensibility as a factor in code synthesis, such as during the learning phase, or when ranking suggestions, or even take it as input to the model (similar to the ‘temperature’ parameter in Codex). This approach is useful more broadly to synthesize high quality code, such as optimizing for performance or robustness. A second solution to tackle the comprehension problem is to explain the generated code to their users in a manner that is less ‘programmerese’ and more centered around the user’s current task and context. Initial evidence suggests that participants were open to these ideas; thus, these areas are ripe for future exploration. 9.4. Issue 4: Consequences of automation in end-user programming In any AI system, we need to consider the consequences of automation. End-user programmers are known to turn to local experts or gardeners (end-user programmers with interest and expertise in pro- gramming who serve as gurus in the end-user programming environment) when they are unable to solve a part of the problem (Nardi, 1993; Sarkar & Gordon, 2018). Task-orientation tendencies combined with 18 challenges of completing their tasks easily also leaves end-user programmers with limited attention for testing, or carefully learning what is going on with their programs. Assuming that LLMs and associated user experiences will improve in the coming years, making end-user programming faster with LLMs than without, it is tempting to wonder whether the programmer can be persuaded to invest the saved time and attention to aspects such as learning or testing their programs; if so, what would it take to inﬂuence behaviour changes? Another question is in the role of such experts. We conjecture that LLMs or similar AI capabilities will soon be able to answer a sizeable fraction of questions that end-user programmers will go to local experts for. An open question therefore is how the ecosystem of end-user programmers in organizations will change in their roles, importance and specialities. For example, will gardeners take on the role of educating users on better taking advantage of AI? If so, how can we communicate the working of such AI systems to technophile users and early adopters, so they can enable others in the organization? 9.5. Issue 5: No code, and the dilemma of the direct answer Finally, it is not a foregone conclusion that users are even interested in code. As Blackwell’s model of attention investment notes, in many cases the user may be content to perform an action manually, rather than invest in creating a reusable automation (Blackwell, 2002a; J. Williams et al., 2020). Spreadsheet users, in particular, are often not sensitive to the level of automation or automatability of a given work- ﬂow, using a mix of manual, automated, and semi-automated techniques to achieve the goal at hand (Pandita et al., 2018). Spreadsheet users often need ad-hoc transformations of their data that they will, in all likelihood, never need again. It may be that we can express this transformation as a program, but if the user is interested in the output and not the program, is it important, or even necessary, to communicate this fact to the user? One can argue that increasing the user’s awareness of the ﬂexibility and fallibility of the process of delivering an inferred result (i.e., enabling them to critically evaluate the output (Sarkar et al., 2015)) can build agency, conﬁdence, trust, and resilience. This issue is related to information retrieval’s “dilemma of the direct answer” (Potthast et al., 2021), raised in response to the increased phenomenon of search engines directly answering queries in addition to simply listing retrieved results. However, if the programming language used is not related to the languages familiar to the end-user, or the user is a complete novice, it is exceedingly difﬁcult for them to make any sense of it, as was shown by Lau et al. (2021) in their study of Excel users encountering Python code. Yet, there are socio-technical motivations for using an unfamiliar target language: long-term testing of LLM assistance shows that it shines when paired with high-level APIs that capture use cases well (Section 7). One advantage of the Python ecosystem is that it has an unparalleled set of libraries and APIs for data wrangling. An LLM-assisted tool that emits Excel formulas is therefore less likely to solve user problems than Python statements. In the longer term, this might be mitigated by developing a rich set of data manipulation libraries in the Excel formula language. 10. Conclusion Large language models have initiated a signiﬁcant change in the scope and quality of program code that can be automatically generated, compared to previous approaches. Experience with commercially available tools built on these models suggests that a they represent a new way of programming. LLM assistance transforms almost every aspect of the experience of programming, including planning, au- thoring, reuse, modiﬁcation, comprehension, and debugging. In some aspects, LLM assistance resembles a highly intelligent and ﬂexible compiler, or a partner in pair programming, or a seamless search-and-reuse feature. Yet in other aspects, LLM-assisted programming has a ﬂavour all of its own, which presents new challenges and opportunities for human-centric pro- gramming research. Moreover, there are even greater challenges in helping non-expert end users beneﬁt from such tools. 19 A. Experience report sources This appendix contains a list of sources we draw upon for the quotes and analysis in Section 7. While all sources were included in our analysis, we did not draw direct quotes from every source in this list. A.1. Blog posts and corresponding Hacker News discussions 1. Andrew Mayne, March 17 2022, “Building games and apps entirely through natural language using OpenAI’s code-davinci model”. URL: https://andrewmayneblog.wordpress .com/2022/03/17/building-games-and-apps-entirely-through-natural -language-using-openais-davinci-code-model/. Hacker News discussion: https://news.ycombinator.com/item?id=30717773 2. Andrew Mouboussin, March 24 2022, “Building a No-Code Machine Learning Model by Chat- ting with GitHub Copilot”. URL: https://www.surgehq.ai/blog/building-a-no -code-toxicity-classifier-by-talking-to-copilot. Hacker News discussion: https://news.ycombinator.com/item?id=30797381 3. Matt Rickard, August 17 2021, “One Month of Using GitHub Copilot”. URL: https://matt -rickard.com/github-copilot-a-month-in/. 4. Nutanc, November 15 2021, “Using Github copilot to get the tweets for a keyword and ﬁnd the sentiment of each tweet in 2 mins”. URL: https://nutanc.medium.com/ using-github-copilot-to-get-the-tweets-for-a-keyword-and-find -the-sentiment-of-each-tweet-in-2-mins-9a531abedc84. 5. Tanishq Abraham, July 14 2021, “Coding with GitHub Copilot”. URL: https://tmabraham .github.io/blog/github_copilot. 6. Aleksej Komnenovic, January 17 2022, “Don’t fully trust AI in dev work! https://akom.me/dont-fully-trust-ai-in-dev-work-yet. /yet”. URL: A.2. Miscellaneous Hacker News discussions 1. https://news.ycombinator.com/item?id=30747211 2. https://news.ycombinator.com/item?id=31390371 3. https://news.ycombinator.com/item?id=31020229&p=2 4. https://news.ycombinator.com/item?id=29760171 5. https://news.ycombinator.com/item?id=31325154 6. https://news.ycombinator.com/item?id=31734110 7. https://news.ycombinator.com/item?id=31652939 8. https://news.ycombinator.com/item?id=30682841 9. https://news.ycombinator.com/item?id=31515938 10. https://news.ycombinator.com/item?id=31825742 20 References Allamanis, M., Barr, E. T., Devanbu, P. T., & Sutton, C. (2018). A survey of machine learning for big code and naturalness. ACM Comput. Surv., 51(4), 81:1–81:37. Retrieved from https://doi .org/10.1145/3212695 doi: 10.1145/3212695 Allamanis, M., & Brockschmidt, M. (2017). Smartpaste: Learning to adapt source code. arXiv preprint arXiv:1705.07867. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., . . . Sutton, C. (2021). Program synthesis with large language models. arXiv. Retrieved from https://arxiv.org/abs/2108 .07732 doi: 10.48550/ARXIV.2108.07732 Barik, T., Ford, D., Murphy-Hill, E., & Parnin, C. (2018). How should compilers explain problems to developers? In Proceedings of the 2018 26th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering (pp. 633–643). Barik, T., Johnson, B., & Murphy-Hill, E. (2015). I heart hacker news: expanding qualitative research ﬁndings by analyzing social news websites. In Proceedings of the 2015 10th joint meeting on foun- dations of software engineering (pp. 882–885). Barke, S., James, M. B., & Polikarpova, N. (2022). Grounded copilot: How programmers interact with code-generating models. arXiv. Retrieved from https://arxiv.org/abs/2206.15000 doi: 10.48550/ARXIV.2206.15000 Basman, A., Church, L., Klokmose, C. N., & Clark, C. B. (2016). Software and how it lives on- embedding live programs in the world around them. In Ppig (p. 19). Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? In M. C. Elish, W. Isaac, & R. S. Zemel (Eds.), Facct ’21: 2021 ACM conference on fairness, accountability, and transparency, virtual event / toronto, canada, march 3-10, 2021 (pp. 610–623). ACM. Retrieved from https://doi.org/10.1145/ 3442188.3445922 doi: 10.1145/3442188.3445922 Bergström, I., & Blackwell, A. F. (2016). The practices of programming. In 2016 ieee symposium on visual languages and human-centric computing (vl/hcc) (pp. 190–198). Blackwell, A. F. (2002a). First steps in programming: A rationale for attention investment models. In Proceedings ieee 2002 symposia on human centric computing languages and environments (pp. 2–10). Blackwell, A. F. (2002b). What is programming? In Ppig (p. 20). Bødker, S. (2015). Third-wave hci, 10 years later - participation and sharing. Interactions, 22(5), 24–31. Retrieved from https://doi.org/10.1145/2804405 doi: 10.1145/2804405 Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., . . . Amodei, D. (2020). Language models are few-shot learners. Cao, J., Fleming, S. D., Burnett, M., & Scafﬁdi, C. (2015). Idea garden: Situated support for problem solving by end-user programmers. Interacting with Computers, 27(6), 640–660. Chalhoub, G., & Sarkar, A. (2022). “It’s Freedom to Put Things Where My Mind Wants”: Understanding In CHI Conference on and Improving the User Experience of Structuring Data in Spreadsheets. Human Factors in Computing Systems. New York, NY, USA: Association for Computing Machinery. Retrieved from https://doi.org/10.1145/3491102.3501833 doi: 10.1145/3491102 .3501833 21 Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., . . . Zaremba, W. (2021). Evaluating large language models trained on code. CoRR, abs/2107.03374. Retrieved from https://arxiv.org/abs/2107.03374 Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., . . . Zaremba, W. (2021). Evaluating large language models trained on code. ArXiv, abs/2107.03374. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., . . . Fiedel, N. (2022). Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311. Colmerauer, A., & Roussel, P. (1996). The birth of prolog. In History of programming languages—ii (pp. 331–367). Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019, June). BERT: Pre-training of deep bidi- In Proceedings of the 2019 conference of the rectional transformers for language understanding. north American chapter of the association for computational linguistics: Human language technolo- gies, volume 1 (long and short papers) (pp. 4171–4186). Minneapolis, Minnesota: Association for Computational Linguistics. Retrieved from https://aclanthology.org/N19-1423 doi: 10.18653/v1/N19-1423 Green, T., & Blackwell, A. (1998). Cognitive dimensions of information artefacts: a tutorial. In Bcs hci conference (Vol. 98, pp. 1–75). Green, T. R. (1989). Cognitive dimensions of notations. People and computers V, 443–460. Green, T. R., & Petre, M. (1992). When visual programs are harder to read than textual programs. In Human-computer interaction: Tasks and organisation, proceedings of ecce-6 (6th european confer- ence on cognitive ergonomics). gc van der veer, mj tauber, s. bagnarola and m. antavolits. rome, cud (pp. 167–180). Gulwani, S. (2011). Automating string processing in spreadsheets using input-output examples. In T. Ball & M. Sagiv (Eds.), Proceedings of the 38th ACM SIGPLAN-SIGACT symposium on principles of programming languages, POPL 2011, austin, tx, usa, january 26-28, 2011 (pp. 317–330). ACM. Retrieved from https://doi.org/10.1145/1926385.1926423 doi: 10.1145/1926385 .1926423 Hannay, J. E., Dybå, T., Arisholm, E., & Sjøberg, D. I. (2009). The effectiveness of pair programming: A meta-analysis. Information and software technology, 51(7), 1110–1122. Henley, A. Z., & Fleming, S. D. (2014). The patchworks code editor: Toward faster navigation with less code arranging and fewer navigation mistakes. In Proceedings of the sigchi conference on human factors in computing systems (pp. 2511–2520). Hermans, F., Pinzger, M., & van Deursen, A. (2015). Detecting and refactoring code smells in spread- sheet formulas. Empirical Software Engineering, 20(2), 549–575. Hindle, A., Barr, E. T., Gabel, M., Su, Z., & Devanbu, P. T. (2016). On the naturalness of software. Commun. ACM, 59(5), 122–131. Retrieved from https://doi.org/10.1145/2902362 doi: 10.1145/2902362 Hindle, A., Barr, E. T., Su, Z., Gabel, M., & Devanbu, P. T. (2012). On the naturalness of soft- In M. Glinz, G. C. Murphy, & M. Pezzè (Eds.), 34th international conference on soft- ware. ware engineering, ICSE 2012, june 2-9, 2012, zurich, switzerland (pp. 837–847). IEEE Com- puter Society. Retrieved from https://doi.org/10.1109/ICSE.2012.6227135 doi: 10.1109/ICSE.2012.6227135 22 Hoare, C. A. R. (1969). An axiomatic basis for computer programming. Commun. ACM, 12(10), 576– 580. Retrieved from https://doi.org/10.1145/363235.363259 doi: 10.1145/363235 .363259 Hochreiter, S., & Schmidhuber, J. (1997, nov). Long short-term memory. Neural Comput., 9(8), 1735–1780. Retrieved from https://doi.org/10.1162/neco.1997.9.8.1735 doi: 10 .1162/neco.1997.9.8.1735 Horvitz, E. (1999). Principles of mixed-initiative user interfaces. In Proceedings of the sigchi conference on human factors in computing systems (pp. 159–166). Hutchins, E. L., Hollan, J. D., & Norman, D. A. (1985). Direct manipulation interfaces. Hum. Comput. Interact., 1(4), 311–338. Retrieved from https://doi.org/10.1207/s15327051hci0104 _2 doi: 10.1207/s15327051hci0104\_2 Imai, S. (2022). Is github copilot a substitute for human pair-programming? an empirical study. In 2022 ieee/acm 44th international conference on software engineering: Companion proceedings (icse- companion) (pp. 319–321). Jiang, E., Toh, E., Molina, A., Olson, K., Kayacik, C., Donsbach, A., . . . Terry, M. (2022). Discovering the syntax and strategies of natural language programming with generative language models. In Chi conference on human factors in computing systems (pp. 1–19). Kery, M. B., & Myers, B. A. (2017). Exploring exploratory programming. In 2017 ieee symposium on visual languages and human-centric computing (vl/hcc) (pp. 25–29). Ko, A. J., & Myers, B. A. (2004). Designing the whyline: a debugging interface for asking questions In Proceedings of the sigchi conference on human factors in computing about program behavior. systems (pp. 151–158). Kulesza, T., Amershi, S., Caruana, R., Fisher, D., & Charles, D. (2014). Structured labeling for facilitat- ing concept evolution in machine learning. In Proceedings of the sigchi conference on human factors in computing systems (pp. 3075–3084). Kurlander, D., Cypher, A., & Halbert, D. C. (1993). Watch what i do: programming by demonstration. MIT press. Lau, S., Srinivasa Ragavan, S. S., Milne, K., Barik, T., & Sarkar, A. (2021). Tweakit: Supporting end- user programmers who transmogrify code. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (pp. 1–12). Li, J., Tang, T., Zhao, W. X., & Wen, J.-R. (2021, 8). Pretrained language model for text generation: A survey. In Z.-H. Zhou (Ed.), Proceedings of the thirtieth international joint conference on artiﬁcial intelligence, IJCAI-21 (pp. 4492–4499). International Joint Conferences on Artiﬁcial Intelligence Organization. Retrieved from https://doi.org/10.24963/ijcai.2021/612 (Survey Track) doi: 10.24963/ijcai.2021/612 Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., . . . Vinyals, O. (2022b). Competition-level code generation with alphacode. arXiv. Retrieved from https://arxiv.org/ abs/2203.07814 doi: 10.48550/ARXIV.2203.07814 Li, Y., Choi, D. H., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., . . . Vinyals, O. (2022a). Competition-level code generation with alphacode. ArXiv, abs/2203.07814. Lieberman, H. (2001). Your wish is my command: Programming by example. Morgan Kaufmann. 23 Lieberman, H., & Liu, H. (2006). Feasibility studies for programming in natural language. In End user development (pp. 459–473). Springer. Liu, S., Chen, Y., Xie, X., Siow, J. K., & Liu, Y. (2021). Retrieval-augmented generation for code summarization via hybrid GNN. In International conference on learning representations. Retrieved from https://openreview.net/forum?id=zv-typ1gPxA Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., . . . Liu, S. (2021). Codexglue: A machine learning benchmark dataset for code understanding and generation. ArXiv, abs/2102.04664. Luger, E., & Sellen, A. (2016). ""like having a really bad pa"" the gulf between user expectation and experience of conversational agents. In Proceedings of the 2016 chi conference on human factors in computing systems (pp. 5286–5297). Macvean, A., Church, L., Daughtry, J., & Citro, C. (2016). Api usability at scale. In Ppig (p. 26). Marasoiu, M., Church, L., & Blackwell, A. (2015). An empirical investigation of code completion usage by professional software developers. In PPIG. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, & K. Weinberger (Eds.), Advances in neural information processing systems (Vol. 26). Curran As- sociates, Inc. Retrieved from https://proceedings.neurips.cc/paper/2013/file/ 9aa42b31882ec039965f3c4923ce901b-Paper.pdf Miller, L. A. (1981). Natural language programming: Styles, strategies, and contrasts. IBM Systems Journal, 20(2), 184–215. Mou, L., Li, G., Zhang, L., Wang, T., & Jin, Z. (2016). Convolutional neural networks over tree structures for programming language processing. In Aaai. Mu, J., & Sarkar, A. (2019). Do we need natural language? Exploring restricted language interfaces In Extended Abstracts of the 2019 CHI Conference on Human Factors in for complex domains. Computing Systems (pp. 1–6). Myers, B. A. (1992). Demonstrational interfaces: A step beyond direct manipulation. Computer, 25(8), 61–73. Myers, B. A., & Stylos, J. (2016). Improving api usability. Communications of the ACM, 59(6), 62–69. Nardi, B. A. (1993). A small matter of programming: perspectives on end user computing. MIT press. Nguyen, A. T., Nguyen, T. T., & Nguyen, T. N. (2015). Divide-and-conquer approach for multi-phase statistical migration for source code (t). 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), 585-596. Pandita, R., Parnin, C., Hermans, F., & Murphy-Hill, E. (2018). No half-measures: A study of manual and tool-assisted end-user programming tasks in excel. In 2018 ieee symposium on visual languages and human-centric computing (vl/hcc) (pp. 95–103). Panko, R. R. (2008). Reducing overconﬁdence in spreadsheet development. arXiv preprint arXiv:0804.0941. Pearce, H., Ahmad, B., Tan, B., Dolan-Gavitt, B., & Karri, R. (2021). Asleep at the keyboard? assessing the security of github copilot’s code contributions. arXiv. Retrieved from https://arxiv.org/ abs/2108.09293 doi: 10.48550/ARXIV.2108.09293 24 Piccioni, M., Furia, C. A., & Meyer, B. (2013). An empirical study of api usability. In 2013 acm/ieee international symposium on empirical software engineering and measurement (pp. 5–14). Potthast, M., Hagen, M., & Stein, B. (2021). The dilemma of the direct answer. In Acm sigir forum (Vol. 54, pp. 1–12). Raychev, V., Vechev, M. T., & Krause, A. (2015). Predicting program properties from ""big code"". In S. K. Rajamani & D. Walker (Eds.), Proceedings of the 42nd annual ACM SIGPLAN-SIGACT sympo- sium on principles of programming languages, POPL 2015, mumbai, india, january 15-17, 2015 (pp. 111–124). ACM. Retrieved from https://doi.org/10.1145/2676726.2677009 doi: 10.1145/2676726.2677009 Rouchy, P. (2006). Aspects of prolog history: Logic programming and professional dynamics. Blekinge Institute of Technology, Sweden).(English). TeamEthno-Online(2), 85–100. Salge, C. A. D. L., & Berente, N. (2016). Pair programming vs. solo programming: What do we know after 15 years of research? In 2016 49th hawaii international conference on system sciences (hicss) (pp. 5398–5406). Sarkar, A. (2016). Interactive analytical modelling (Tech. Rep. No. UCAM-CL-TR-920). Uni- versity of Cambridge, Computer Laboratory. Retrieved from https://www.cl.cam.ac.uk/ techreports/UCAM-CL-TR-920.pdf doi: 10.48456/tr-920 Sarkar, A. (2022, March). In Workshop on Transparency and Explanations in Smart Systems (TeXSS), in conjunction with ACM Intelligent User Interfaces (IUI 2022) (pp. 192–199). Retrieved from http://ceur-ws.org/Vol-3124/ paper22.pdf Is explainable AI a race against model complexity? Sarkar, A., & Gordon, A. D. (2018, September). How do people learn to use spreadsheets? (work in progress). In Proceedings of the 29th Annual Conference of the Psychology of Programming Interest Group (PPIG 2018) (pp. 28–35). Sarkar, A., Jamnik, M., Blackwell, A. F., & Spott, M. Interactive visual machine learning (2015). In 2015 IEEE Symposium on Visual Languages and Human-Centric Computing in spreadsheets. (VL/HCC) (pp. 159–163). Sarkar, A., Srinivasa Ragavan, S., Williams, J., & Gordon, A. D. (2022). End-user encounters with lambda abstraction in spreadsheets: Apollo’s bow or Achilles’ heel? In 2022 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC). Shneiderman, B., & Norwood, N. (1993). 1.1 direct manipulation: a step beyond programming. Sparks of innovation in human-computer interaction, 17. Silver, A. (2018, May). Introducing visual studio intellicode. Microsoft. Retrieved from https://devblogs.microsoft.com/visualstudio/introducing-visual -studio-intellicode/ Srinivasa Ragavan, S., Hou, Z., Wang, Y., Gordon, A. D., Zhang, H., & Zhang, D. (2022). Gridbook: Natural language formulas for the spreadsheet grid. In 27th international conference on intelligent user interfaces (p. 345–368). New York, NY, USA: Association for Computing Machinery. Retrieved from https://doi.org/10.1145/3490099.3511161 doi: 10.1145/3490099.3511161 Srinivasa Ragavan, S., Kuttal, S. K., Hill, C., Sarma, A., Piorkowski, D., & Burnett, M. (2016). Foraging among an overabundance of similar variants. In Proceedings of the 2016 chi conference on human factors in computing systems (pp. 3509–3521). 25 Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Proceedings of the 27th international conference on neural information processing systems - volume 2 (p. 3104–3112). Cambridge, MA, USA: MIT Press. Tanimoto, S. L. (2013). A perspective on the evolution of live programming. In 2013 1st international workshop on live programming (live) (pp. 31–34). Vaithilingam, P., Zhang, T., & Glassman, E. L. (2022). Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In Chi conference on human factors in computing systems extended abstracts (pp. 1–7). Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., . . . Polosukhin, I. (2017). Attention is all you need. In Proceedings of the 31st international conference on neural information processing systems (p. 6000–6010). Red Hook, NY, USA: Curran Associates Inc. Wei, J., Goyal, M., Durrett, G., & Dillig, I. (2020). Lambdanet: Probabilistic type inference using graph neural networks. ArXiv, abs/2005.02161. Wei, Y., Chandrasekaran, N., Gulwani, S., & Hamadi, Y. (2015, May). Building bing developer assistant (Tech. Rep. No. MSR-TR-2015-36). Retrieved from https://www.microsoft.com/en-us/ research/publication/building-bing-developer-assistant/ Weiss, D. (2022, Jun). Blog / tabnine announcements / announcing our next-generation ai models. Tab- nine. Retrieved from https://www.tabnine.com/blog/announcing-tabnine-next -generation/ Williams, J., Negreanu, C., Gordon, A. D., & Sarkar, A. (2020). Understanding and inferring units In 2020 IEEE Symposium on Visual Languages and Human-Centric Computing in spreadsheets. (VL/HCC) (pp. 1–9). Williams, L. A., & Kessler, R. R. (2000). All i really need to know about pair programming i learned in kindergarten. Communications of the ACM, 43(5), 108–114. Wing, J. (2011). Research notebook: Computational thinking—what and why. The link magazine, 6, 20–23. Xu, F. F., Alon, U., Neubig, G., & Hellendoorn, V. J. (2022). A systematic evaluation of large lan- guage models of code. Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. Xu, F. F., Vasilescu, B., & Neubig, G. In-IDE Code Generation from Natural Language: Promise and Challenges. ACM Transactions on Software Engineering and Methodology (TOSEM), 31(2), 1–47. (2022). Yoon, Y., & Myers, B. A. (2015). Supporting selective undo in a code editor. In 2015 ieee/acm 37th ieee international conference on software engineering (Vol. 1, pp. 223–233). Zhang, H., Jain, A., Khandelwal, G., Kaushik, C., Ge, S., & Hu, W. (2016). Bing developer assistant: improving developer productivity by recommending sample code. In Proceedings of the 2016 24th acm sigsoft international symposium on foundations of software engineering (pp. 956–961). Ziegler, A. (2021, Jun). Github copilot research recitation. Microsoft. Retrieved from https:// github.blog/2021-06-30-github-copilot-research-recitation/ Ziegler, A., Kalliamvakou, E., Simister, S., Sittampalam, G., Li, A., Rice, A., . . . Aftandilian, E. (2022). Productivity assessment of neural code completion. arXiv preprint arXiv:2205.06537. 26",1.0
What is it like to program with artificial intelligence?,"[{'href': 'http://arxiv.org/abs/2208.06213v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2208.06213v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-08-12 10:48:46,,"Deep Learning and Health Informatics for Smart Monitoring and Diagnosis Amin Gasmi 30 Octobre 2020 Abstract The connection between the design and delivery of health care services using information technology is known as health informatics. It involves data usage, validation, and transfer of an integrated medical analysis using neural networks of multi-layer deep learning techniques to analyze complex data. For instance, Google incorporated “DeepMind” health mobile tool that integrates & leverage medical data needed to enhance professional healthcare delivery to patients. Moorfield Eye Hospital London introduced DeepMind Research Algorithms with dozens of retinal scans attributes while DeepMind UCL handled the identification of cancerous tissues using CT & MRI Scan tools. Atomise analyzed drugs and chemicals with Deep Learning Neural Networks to identify accurate pre-clinical prescriptions. Health informatics makes medical care intelligent, interactive, cost-effective, and accessible; especially with DL application tools for detecting the actual cause of diseases. The extensive use of neural network tools leads to the expansion of different medical disciplines which mitigates data complexity and enhances 3-4D overlap images using target point label data detectors that support data augmentation, un-semi-supervised learning, multi-modality and transfer learning architecture. Health science over the years focused on artificial intelligence tools for care delivery, chronic care management, prevention/wellness, clinical supports, and diagnosis. The outcome of their research leads to cardiac arrest diagnosis through Heart Signal Computer-Aided Diagnostic tool (CADX) and other multi- functional deep learning techniques that offer care, diagnosis & treatment. Health informatics provides monitored outcomes of human body organs through medical images that classify interstitial lung disease, detects image nodules for reconstruction & tumor segmentation. The emergent medical research applications gave rise to clinical-pathological human-level performing tools for handling Radiological, Ophthalmological, and Dental diagnosis. This research will evaluate methodologies, Deep learning architectures, approaches, bio-informatics, specified function requirements, monitoring tools, ANN (artificial neural network), data labeling & annotation algorithms that control data validation, modeling, and diagnosis of different diseases using smart monitoring health informatics applications. Keywords: Health Informatics Diagnosis, DL Smart Monitoring App, DL/ML Health Informatics, Deep Learning Algorithms, Health Informatics Devices. 1 Introduction The fundamental use of deep learning in neural networks commenced as a result of experts study of complex neurons, layers, and its architectural paradigms. This study allowed to monitor data, its extended layer pipeline, and non-linear outputs generated from low- dimensional input space projection. Health informatics involve the generation of an automatic character set of human cells with expert intrusions. Medical imaging in health informatics can be elaborated to determine implicit internal organs like fibroids and polyps tissue irregularities. It can also be used to study morphological tumors (Fakoor et al.2013)[1] Biologically, health informatics have anticipated translational features utilized for nucleotide DNA & RNA sequential protein strands. Convolutional neural nets (CNNs) as a deep learning approach in health informatics have architectural layers and filters for reducing, rectifying, and modifying poohing layers. The layers help to originate abstract features found in the visual cortex and receptive fields while other architectures like restructured Boltzmann machines, deep belief networks DBNs, stacked autoencoders, extended network and recurrent neural nets (RNNs); assist the advancement of graphical process units (GPUs) that impact the growth of deep learning applications. Experts previously proposed pre-GPU and CNNs as parallel algebraic operations with matrixes needed for experiments in clinics. To integrate deep learning architectures in health informatics, its essential to label data and implement activation functions known as “transfer functions and weights”. The transfer function must classify linear patterns to adjust the weights. McClelland et a. 1987 [3] proposed neural networks with hidden layers of perceptrons, stages, and epochs for new data input samples & weights with neurons adjustable based learning process, named “Delta Rule”. The rule aids neural network training, exploitation, and backpropagation routines. Rumelhart et al.1988 [4] observed the random values given to the network weights to determine it’s iterative training processes and minimize the difference between network outputs and desired outcomes. Rumelhart et al.1988 [4] also furthered the study of iterative training using gradient descent techniques to reduce surface errors in the 2 experiment. Deep learning accelerates supervised and un-supervised labeled data, whereby supervised labeled data train deep neural network to understand its weight, minimize errors, and predict the targeted value for classifying the unsupervised labeled data. Meanwhile, unsupervised deep learning data are utilized for clustering, reduction of dimension and featured extraction (Ngiam et al.2011)[2]. Artificial Neural network and its variants Four deep learning architectures such as Auto-encoder, RBM, CNN, and RNN are mentioned to assist health informatics prediction and diagnosis. The autoencoder is referred to as feed-forwarder two-phase network that handles encoder and decoder tasks using input X and hidden H which represents non-linear equations stated below; H - stand for non-linear activation functions that decodes maps hidden in the representation. Thus, the original hidden representation can also be calculated with Z -If the model parameters optimize and minimize errors in the auto-encoder variants, the reconstructing error collection data = N, the sample square error optimization = (Xi= f (xi))2 Xi represents the ith sample of the unsupervised data, h stands for hidden representation and X for data sample (Bengio et al. 2007)[61]. The image below demonstrates the difference between physical based, conventional data- driven, and deep learning auto-encoder algorithm. The conventional auto-encoder data-model requires handcrafting features for each individual trained module and cannot handle large datasets. Deep learning autoencoder methods provide end-to-end envisioned data structure without handcrafting features and train jointly large datasets; 3 Fig 1. Autoencoder schematic illustration [78] According to the diagram above, the learned transformation in the autoencoder must be sparse with constraints that implores the hidden unit with an optimizing function written below; (Xi= f (xi))2 M= hidden layer size Ki=divergence hidden units and jth hidden neuron. The conventional auto-encoder has an additional denoising network that corrects corrupt version of the data input, reconstruct, clean and train sample data X. it also has staking structure which puts together output lth layers as input (L+1)t-th layer to represent higher level and provide solutions to deep neural network model (Vincent et al.2008)[62]. 4 RBM variants Restricted Boltzmann machine has two-layers (NN) bipartite graph consisting of two visible groups known as units V and hidden unit h with an asymmetric link between both, but doesn’t connect their nodes. RBM model parameters = (w,b,a) energy functions. Where = Wij –Vi- hj. Wij -connecting weight between units Vi- total number 1 and hidden unit Hi -the total number of Ji bi and ai, which shows the joint RBM distribution over the unit based energy function equated as p(v, hj, Z -partitioned function/normalization factors). The conditional probabilities of the hidden & visible units h and V will be P (hj =I/Vj ) P (Vi=I/Vj = logistic function The w-learning approach is achieved using a contrastive divergence tool (CD). Deep belief network variants DBN is made up of stack multiple RBM with output ith layer (hidden unit and input (L+1)- th visible layer. DBN has a common similarity with SDA due to its large layer unsupervised pattern in handling pre-trained data parameters. Deep Boltzmann machine learning approach contains hidden units grouped into layers of single connectivity constraints with its full connection found between subsequent and non- neighboring layers. 5 Convolutional neural network and its variants CNN was utilized to perform image spatial processing via weights & pooling properties. CNN serves the purpose of authenticating natural language & speech recognition. It helps to learn about the alternating abstract features, stack convolutions & pool operations. The two dimensional CNN can be compared to a one-dimensional model using input sequence data X=(Xi………………….Xt)where t represents lengths of sequence, Vi-d respectively. The convolutional dot production can be filtered with vector U Where Ci = Where XT stands for matrix x, b and the output Ci is seen as the activated filter U that correspond with Xi; I + m-1. The slide filtering window features a map vector of Cj =(C1, C2, ……………………(l-M+1) The J represents the index J-th filter that corresponds with multi-windows (Xi; m, X2: m+1…………….X1-m + 1:1). CNN has a max-pooling layer that is capable of reducing the length of the featured map and minimized the numbers of modeled parameters. It has a hypermeter pooling layer denotation of MAX operation with consecutive value S and feature map Cj. The compressed feature h= (h, h2, --------- +1) Hj= max (c(j-1)s, C (J-1) S+1, …………….C, S-1). To predict data possibilities, the alternating CNN max-pooling layers can fully be connected to the softmax layer. Recurrent neural network and its variants Schmidhuber (2015)[63] highlights the arbitrary length sequence of pattern input, which builds the connection between direct cycle and multi-layer perceptron. RNN trains back- propagated supervised data with subsequent input and targeted datasets (Jaeger, 6 2002)[64]. It’s functional transition step T shows time information (Xt) moves from prior hidden output ht-1 to update the current hidden output ht = H (t,ht-1). H - non-linear & differential transforming function. Ht- learned representation showing input data and length T. Also, the conventional multi- layer perceptron mapped the obtainable ht representation to make the prediction successfully. The simple function “Vanilla RNN”, can be equated as-ht W and H that represent transformation matrixes while b =bias vector. Vanilla RNN suffers vanishing gradient problem due to back-propagation, but can easily be restored with LSTM and gate recurrent neural networks (GRU) to prevent errors and explosion. (Chung et al.2014)[65]. The advanced version of LSTM & GRUs has a multi-layer recurrent bi-directional model capable of offering structural flexibility. Fig 2. One-layer CNN, pooling layers, fully connected one Softmax Layer [79] 7 Health monitoring applications and wearables Rose et al. 2010 [5] implemented hierarchical clustering methods to detect mammographic image data. The image below shows health monitoring applications necessary for capturing arrays of pervasive sensors worn or implanted in the body to capture ambient inertia motion, ECG patches, smart watches, EEG, and prosthetics (Johnson et al.2016) [6]. Pervasive sensors are wearables implanted as ambient sensors that monitor human health and accurately estimate food intake, energy expenditure, tackles obesity, chronic diseases, and care for patients with disabilities. Patients undergoing rehabilitation and critical care situation are often implanted with assisting devices to check vital signs (Pouladzadeh et al. 2016)[7]. During epidemics, the escalation of health-related issues like obesity, and cardiovascular diseases are controlled with energy expenditure/activity recognition tool, since it controls the amount of diet by monitoring calorie intake. CNN can alternatively be used to recognize and monitor accurately food intake by adopting cloud computing, size-calibrating, and distance estimation tools. Pouladzadeh et al. 2016 [7] combined DL method with invariant hierarchical representation of video using two-layer 3D convolution & max-pooling large inputs to recognize human daily activities. Yalcin, 2016[9] used RGB D-video sequence to classify human activities and mount surveillance on elderly and child care clinics. CNN has furthered the detection of baby’s fall & crawls while alerting caregivers by raising an alarm. The RBMs work together on smartphones & watches (Assisting devices), with audio & tactile feedback application, specifically used to detect visual impairment. Some assistive device consists of CNN DL algorithms that recognize hand gestures, sign languages, sterile surroundings, and permits touch-less human-computer-interaction (HRI). Huang et al.2015[8] introduced a deep neural network (DNN) for sign language recognition using real-sense data that coordinates finger joints inputs without handcraft features. DL machine ensures that health monitoring is achieved with discrete targeted values applied through Softmax later (prognosis and linear regression layer), that limits human labor and expert’s skills. 8 Sun et al. (2016)[41] used a one-layer auto-encoder neural network to classify health- machine motor fault recognition and repair while Lu et al. (2017)[42] diagnose rotary health machine faults with components of stacked denoising AE in the three hidden notable layers. Most devices for vital signs like ECG, BCG (ballistocardiogram) are built with DL applications. Technology advancement brought about wearable photos or videos attached to outfits, and embedded sensors placed on chairs, car seats, and mattresses (e.g. Fitbit wrist band tracker controlled with mobile apps and add-ons). Experts studied the use of a genetic triaxle algorithm known as accelerometer bracelets to trace walking patterns (e.g. falls and seizure inpatients). Research declared that prolonged sedentary lifestyle can cause adverse health outcomes which made clinicians adopt the use of wearable devices for monitoring patients' health and advice physical activity when necessary. Choo et al. 2017[76] used wearable devices to trace language patterns of mother-child connection and childhood psychological development. Choi et al. (2017)[75] monitored stress patterns using mentally equipped sensors powered by Ml/DL to understand stress in children and adults by following their heartbeat, blood pressure, and temperature. According to our research, the electrodermal activity tool (EDA) known as the “emotion Board” has helped to measure skin reaction to stress. However, SVM and LDA help to classify stress and show up to 82.8% result. Chen et al. (2018) monitored heat stroke using a fuzzy logic technique to display an inferential signal on smart devices. The design of wireless communication profoundly changed patient’s management through point-to-care diagnostic devices like EMS (emergency medical service) used in emergency rooms & ICU. Some ICU systems revealed beyond vital signs to the extent of detecting patients' posture/position, toxic gases, and heat flux. Winokur et al. 2013[28] found wearable devices valuable for monitoring heart rate, recording ECG, 3D representation of Sternal Seismocardiogram (SCG). Da He et al. 2012 [27] introduced ear device and wearable cardio-meter defibrillator WCD) to prevent arrhythmic sudden death of patients. Fryar et al. 2017(12) detect hypertension 9 with physiological signals from cerebral blood flow-meter (CBF) that estimates hemodynamic cephalic symptoms in patients. Deep learning architectures, descriptions, and key points Deep learning networks are frameworks for classifying or regressing data which have either hidden or visible output layers. Some have more than two hidden layers that allow the expression of complex or non-linear hypothesis. Deep neural networks have successfully been used in bio-informatics but lack training authentication due to its back- propagated layers and slow learning process. Hinton and Salakhutdinov, 2006 [10] discussed deep auto-encoder designed to extract features and later dimensional reduction with its input, hidden, and output layers. Deep auto-encoder consists of similar input & output nodes, which help to recreate input vectors and handles unsupervised learning techniques. The deep auto-encoder is vital for labeling data, and robust representation (Sparse- AutEU). However, it does need pre-training to undergo the full training process. The deep belief network RBM composition has each sub-network hidden layers & visible layers with undirected connections. The two layers permit unsupervised & supervised training networks to initialize network commands, inference tracing for handling sampling process, and training procedures. (Hinton et al. 2016)[10]. Salakhutdinov and Hinton, 2009 [11] states the difference between deep belief network and deep Boltzmann machine network. According to his laid emphasis, Boltzmann has conditional independent layers that are undirected but uses stochastic algorithms to maximize lower bounds, incorporate robust inference and ambiguous inputs. Boltzman DL can also handle complex time inference that is higher than DBN; while optimizing large dataset parameters. The continual progression in neural networks led to the proposition of recurrent neural networks with the capacity to analyze huge data streams, memorize sequential events, model time dependencies, and process natural languages. The recurrent neural network faces a challenge of vanishing exploding gradients. (Williams and Zipser, 1989)[13]. The 10 convolutional neural network is communicably used since its quite compatible with 2D data and transforms filtered input to 3D output for neuron activations (LeCun et al. 1998)[14]. CNN supports Neuro-biological modeling, which performs visual cortex using flow neuron connections and many varied applications like Google Net & Clarifa. The main challenge in CNN is the hierarchical visual feature used to input large labeled datasets. The parallel GPU acceleration offers hardware capacity need to compute DNN on clouds and multi-core processors. The recurrent neural network comes with a hidden capacity to analyze several data, which made Bengio et al. (1994)[61] discuss RNN variation called Long-Short term memory unit (LSTM). LSTM solves gradient vanishing problems using long-input sequences. LSTM can be used to exploit stored information, write and read information without errors during training. It’s compatible with RNN and shares the same weight whilst aiding natural language processing like modeling, speech recognition, and image description. Ackley et al. 1986 [16] emphasized on the variant Boltzmann machine (RBM) type of stochastic neural networks with Gaussian learning procedures called GIBBS sampling. GIBBS sampling adjusts weights, minimize errors, and model the variable relationship probability. Wang et al . 2016 [17] reviewed the graphical probability model with stochastic units and characterize the conditional independence between variables and directed acyclic graph. Carreira and Hinton, 2005 [18] mentioned the contrastive divergence algorithm (CD) used in conjunction with RBM to handle unsupervised learning algorithms. It has positive and negative phases, whereby the positive phase encourages network configuration and the negative phase recreates current network configurations. CNN has regular correlated local data with multi-dimensional input that can be significant in back-propagation, adjusting of number parameters that support the neuro-biological visual cortex model. The visual cortex in CNN has receptive local field maps that move granularity anterior image inputs to a convolved sub-sampled output through small filters (Hubel and Wiesel, 1962)[19]. 11 DNN learning architectures Deep neural network architecture known as input-output deep architecture (IODA) can resolve different image labeling issues by assigning labels to each image pixel. DNN services both hyperspectral images, whereas spectral & spatial features come together to form hierarchical models that characterize human body issues. Kondo et al. 2014[20] employed a group method of data handling (GMDH) with a hybrid multi-layer neural process to authenticate polynomial activated functions. The essence of GMDH is to recognize liver and spleen data while performing principal component regression analysis. The same technique can be used to identify Cancer Mcyarduim, right, and left kidney issues. Application of deep neural network to translational bioinformatics Table 1 demonstrates software explored with CUBA/NVIDIA to aid GPU acceleration which Wolfram Mathetica and Nervana (2020)[74] used to provide cloud training process systems in combination with neuromorphic electronic system hardware. Most computational neuroscience simulations are conducted with neurons & synapses chips, integrated into hardware like (IBM) true north, (Spinnaker), and Curie (intel). The main purpose of bioinformatics as a discipline is to explore, investigate, and understand the biological molecular level and its processes. Past human genome project (HGP) researched raw data to develop new hypotheses of genes, and environmental factors related to the creation of human genetic proteins. To diagnose diseases using biotechnology, the first human genome motivating principle is “P4” (personalized, preventive, participatory, and predictive medicine) (Hood et al.2011)[22]. The predictive health informatics hold attributes for encoding DNA of the living beings while analyzes the alleles, environmental factors leading to diseases like cancer, and design targeted therapeutic procedures as a remedy. (Leung et al.2016)[23]. The concept of pharmacogenomics is focused on evaluating varieties of drugs & its response to gene-related treatment for aliments especially personalized diagnosis with fewer side effects. Epigenomics investigates interactive proteins and its higher-level 12 processes, and response while transcriptome (mRNA), Proteome and Metabolome modify gene’s response to its environments. Genetic variants are uniquely designed with slicing codes that predict the differences in human tissues, especially how it changes as a result of genetic variation. The alternate of slicing code helps to technically generate gene prediction of slicing patterns meant to comprehend gene phenotypes and its drug effects on autism, spinal muscular atrophy, and hereditary cancer. (Leung et al.2016)[23]. A quantitative activity structure relationship was meant to predict protein-protein coordinations which are usually structured with molecular information, compound interactive protein; for predicting proteins used for drug discovery. Compound interactive protein virtual analysis influenced the discovery of new compounds, toxic substances, and the interpretation of drugs related to targeted cells. In health informatics, deep learning models are utilized to enhance DNA methylation for providing visible outlooks of human chromosomes. It can be used to identify unstable chromosomes, error translation, cell transcription, differentiation, and cancer progression. (Angermueller et al.2016)[24]. Pastur-Romay et al. 2016 [25] named Chembl database in pharmacogenomics that detects millions of compounds descriptions used to develop & target drug evolutions, since the mentioned database encrypts molecular fingerprints, and understand traditional Ml approaches. Chembl database also helps to reduce data complexity in molecular RNA by binding predictive proteins together using RNA structural tertiary profiled outcome (Zhang et al. 2016)[49]. Fakoor et al. (2013)[1] utilized the autoencoder model to explore genetic data from diverse cancer patients to identify similar microarrays in the datasets. Ibrahim et al.2014 [26] detailed the effect of active learning methods using DBN to feature MicroRNA for classifying the performance of different cancer diseases like hepatocellular carcinoma. Deep learning approaches were adopted by Khadem et al. (2015)[28] to beat breast cancer disease through an attribute & noise combination of BDN & Bayesian network that helps 13 the extraction of micro-array data. Experts considered deep learning more effective than SVM in detecting slicing code of different genetic variants, which according to Angermueller et al. (2016)[24] DNN predicts DNA methylation from an incomplete sequence of methylated data. It also predicts embryonic stem cells and baseline comparison to show genome downstream demonstration. Deep learning was mentioned to have outpace conventional techniques, as Kernes et al. (2016) used graph convolutions to encrypt molecular features, physical properties, and assay activities that permit potential collaboration of molecular encoded information. Deep learning used for medical imaging procedure Experts found DL relevant in diagnosing illnesses and interpreting medical images. The processes are conversantly enabled by CAD (computer-aided diagnosis) for assimilating the cause of diseases. CAD model helps to identify causes of neurological Alzheimers, sclerosis, and stroke progression through brain scans, multi-modal mapping of the infected region. Over the years, convolutional neural network aids computer vision, especially the ability to personalize GPU to show parallel brain pathology (Havaei et al.2016)[29]. It further demonstrates CAD segmentation and shapes the analysis of the human brain. CAD has helped to overcome the challenges of difference in intensity & shapes of tumors and lesions using image protocols. Though, issues associated with CAD may include pathological tissue overlapping with healthy samples, RICIAN-Noise, non-Isotropic issues, and bias field effects evident in magnetic resonance images (MRI). Sometimes, the MRI cannot be handled automatically by CAD but requires a similar ML approach to decrypt data complexity and extract features through conventional approaches (Greenspan et al.2016)[30]. CNN as a deep learning approach works better than CAD in terms of data manipulation, operating patch images of abnormal tissues (e.g. CNNs medical imaging for lung diseases coordinated with computed tomography image). Experts used CNN and CT imaging applications to classify the manifestation of tuberculosis, the cell of the neural progenitor from somatic source, and hemorrhage color Fundus image detection. Yan et al (2016)[31] classified different anatomies with CT to understand human organ recognition using 14 multistaged frameworks to extract patches of pre-trained stages. Jamaluddin et al.2016 seem it essential to use CNN for the segmentation of Isotense brain tissue and brain extraction through a multi-modality image tool. Avendi et al. 2016[32] described how DL algorithms encode deformable model parameters and facilitate left ventricle segmentation necessary for short-axis cardiac imaging. CNN tools have 2D image components for segmenting MRI & CT in 3D format, to eradicate issues found in anisotropic voxel sizing. CNN was adopted in orthogonal patch extraction to segment axis, sagittal, and corona view which reduces time and overfitting problems. (Fritscher et al.2016)[33]. Common limitations of CNN include its non-spatial dependencies and the need for pre- processing to bring conditional random fields. These issues can be altered by substituting with conventional ML approaches to solve problems of incomplete data training, limited annotated data, cost/time, and manual medical image annotations. Previously, manual annotation was accepted to help the detection of medical images, but crowdsourcing according to (Greenspan et al.2016)[30] is a viable alternative due to its affordability, error-free medical image analysis. Havaei et al. 2016[29] used a transfer learning and fine-tuning approach to alleviate incomplete training issues on CNN, allowing the pre-trained data to be labeled manually. Tajbakhsh et al. (2016)[53] described the similarity between natural images and medical images by using the fine-tuning process to repeat the same experiment. Shin et al. (2016)[34] applied transfer learning to natural images of a thorax-lymph node to detect lung disease and the result shows consistency in performance without losses. Chen et al (2015)[35] identified fetal abdominal standards with a transfer learning approach to display low-layer CNN pre-training effects on natural images while implementing multi-tasking to handle the CAD image imbalance. Cheng et al.2016[36] utilized denoising autoencoder to diagnose breast legions and pulmonary nodules in CT scans. Shan et al. (2016) [37]tried Stack Sparse Autoencoder on Microaneurysms Fundus images to detect diabetic retinopathy whilst uses Softmax Output Layer to show Alzheimer's disease prediction with functional magnetic resonance images (fMRI). Li et al. (2015)[38] used the RBM method to effect biomarkers from MRI and position emission 15 tomography (PET SCAN) and the result of the scan shows 6% accuracy. Kuang et al. (2014)[39] discriminate attention deficit hyperactivity disorder with the same FMRI application. Hence, experts extracted RBM latent hierarchical 3D patch features from the brain using image segmented tools and Brosch et al. (2013)[40] implored manifold learning method to study 3D brain images, its full automated shape, and cranial nerves. Deep learning methods are known to outpace conventional approach through low-contact optic tracts and other human pathological anatomies. Beaulieu-Jones et al. 2018 [60] found pipeline models relevant in detecting & segmenting objects to achieve an automatic volumetric image process called marginal space DL. MSDL handled hierarchical marginal spacing with automatic features to detect deep learning datasets. Literature review Unsupervised learning techniques are characterized by an unlabeled dataset using metrics of low-high dimensional subspace anomalies for detecting clusters of data. (E.g. Prediction of heart & hepatitis diseases). Collins and Yao, 2018 [43] defined prognosis as the method of predicting disease with clinical practice settings whilst showing multi-modal data. Wang et al. 2012 [44] use prognosis to diagnose diabetics registered in electronic health data records. Medical image analysis follows enhancement, detection, classification, and segmentation procedure to reconstruct and store data. Chen et al. 2017 [45] implemented the reconstruction of MRI and CT image datasets using generative adversarial networks (GANS). The generative adversarial network (GAN) offers MRI reconstruction by cleaning motion pictures, artifacts, and handling image fusion & registration. El-Gamal et al. (2016)[46] noted the significance of image registration in surgical spine implant, tumor removal, and neuro-surgical process. 56 developed an image registration framework called “Quick- Silver”, for large deformation mapping and diffeomorphic metric prediction. Before image registration, data retrieval enables physicians to check the large images of patients' repeated visits to the clinic. Zech et al. 2018 [47] shared natural language processing methods for annotating retrieved images from clinical radiological reports. 16 To achieve real-time health monitoring with DL wearables, IOT sensors, and smart devices, DL clouds must be integrated into smart devices to attain the required results. DL had found its place in clinical workflows for predicting & diagnosing diabetes, dengue, heart & liver diseases, whereby IBM advanced CAD system to CADX that displays automatically fatty liver in Kurtosis image (MA et al. 2016)[48]. Zhang, 2019 [49] utilized clinical reinforcement learning to study the optimal diagnosis and treat patients by characterizing its performance evaluation with different methods (Q value iteration, tabular learning, Q-iteration, and deep Q-learning). The RL method helps to treat sepsis in intensive care units. The same clinical time-series data were studied to provide medical intervention to patients in intensive care units by using CNN and LSTM to predict traumatic brain damage, estimate the mean-variance of arterial blood pressure and intracranial pressure monitoring (Rau, 2018) [50]. Recently, experts have adopted Attention Models to forecast ICU activities, integrate multivariance time-series measurement, and controlling unexpected respiratory issues. To control NIP challenges, Neveol, 2018[52] used the CLAMP Toolkit to monitor different states of clinical text analysis of language acronyms, disparity, and quality variance. Several doctors studied clinical documentation, especially on how to use clinical speech and audio processing to minimize time spent on administrative tasks and medical reports. (Wallace, 2019)[51]. Speech and audio processing serve the purpose of identifying disorders using vocal hyper-functional tools to review patients with dementia and Alzheimers. Limitations Privacy and security challenges are primary limitations discovered in deep learning algorithms. The issue of data collection vulnerabilities is experienced during DL adoption of large datasets which also consumes time and human efforts. The problem of incorrect or altered datasets can lead to wrong diagnoses. Latif et al. 2018 [54] disclosed that instrumental and environmental noise from smart machines can cause an unnecessary disturbance, especially MRI multi-shots, high sensitive modal-motion, and an increased risk of mis-diagnosis due to mistakes in artifacts; can be detrimental to human health. 17 Unqualified physicians without knowledge of data analytics can commit unforgivable errors in medical diagnosis. Caruana et al. 2015 [55] explained the difficulties in data labeling and annotation while 86 lamented about ambiguous medical image classification which may lead to confusion and disagreement between physicians. Xia et al. 2012 [56] indicate that the use of inappropriate algorithms can be detrimental and life-threatening; if improper annotation happens while suggesting the use of meticulous approaches in the labeling of datasets to limit inefficiencies. Due to limited or imbalance datasets, wrong diagnoses can cause death of millions, and missing data sparsity values can lead to unmeasured or repetition in taking samples. Biggio et al. 2012 [57] viewed model training vulnerabilities, as improper training or incomplete breach of privacy causing model poisoning and data theft. To corrupt an already collected data is known as “data poisoning” which requires security, especially during digital forensics & bio-metrics. In case of compromise during deep learning deployment, realistic healthcare settings will experience distribution shifts, leading to incomplete data vulnerability in the testing phase. Security and recommendations Numerous security threats, influence & violations are associated with DL algorithms, constituting to integrity attack, and other vulnerabilities. Such an act can destroy the progress of DL in health and other fields of science. Usama et al. 2019[58] mentioned adversarial machine learning vulnerabilities inserted in input samples to evade privacy and data integrity. A data breach can cause modal poisoning & privacy evasion; whereby clinical deep learning applications are constantly under an attack. However, safety, privacy, ethical regulations & policy are being reinstalled to ensure the quality of data exchange standards. David et al. (2015)[59] recommends hyper-plane commodity data cryptography to control data breach in naïve Bayes classifiers. Zhu et al. (2018)[66] suggest the use of polynomial aggregation and random masking protect SVM with non-linear kernel algorithms. Jagieiski et al. (2018) indicate that data privacy can be reassured with a TRIM tool to protect linear creations. Lui et al. Ascertained that DL frameworks can be secured with 18 XMPP serve while Malalhi et al (2019[68] named Paillier homomorphic encryption for security Naïve Bayes, SVM neural network and FKnn-CBR used to rescue liver patients in India hospitals. Takabi et al. (2016)[69] suggest homomorphic encryption for deep neural networks that control more than 15 datasets in repositories. To guarantee the safety of logistic regression, Kim et al. (2018) [70] recommends homomorphic encryption to secure medical binary datasets. To update healthcare infrastructure, Finlayson et al. 2019 [71] suggest the use of the international classification of disease system which helps to minimize dataset vulnerabilities. However, privacy can be preserved with a cryptographic approach, homomorphic encryption, garbled circuiting, and secured processors. The Intel SGX processor offers confidentiality and authorized access to systems like K-mean, decision trees, and SVM (Ohrimenko et al. 2016)[72]. Google Inc added federated learning method to distribute data, decentralize scheme and predict heart-related diseases. McMahan et al. 2017 [73]. To control adversarial attacks, it's important to modify models using defensive distillation, network verification, gradient regularization, and classifier robustification. Conclusion Deep learning and health informatics algorithms will continually expand to other branches of science, as wearable smart monitoring devices are presently used to track and diagnose Parkinson's diseases. Google Glass conducted prototype child therapeutic analysis to monitor and diagnose autism spectrum disorder. To preserve mental health, psychiatric hospitals are screened, diagnosing, and monitoring depression with a system-on-chip solution that accelerates filters, and reveals heart rates on ECG. Smart monitoring devices are unique, compatible, embedded with DL, and simple to operate. However, aging adults may find it challenging. The future of DL and health informatics depends on the recent 5G wireless network, proposed to bring about new devices for testing red protein (Hemoglobin); especially for transporting oxygen to the blood. The accuracy of clinical results can be validated with a cross-validation approach to unravel the results. 19 Reference 1. Fakoor, R., Ladhak, F., Nazi, A., Huber, M. : Using deep learning to enhance cancer diagnosis and classiﬁcation,” in Proc. ICML, (2013). 2. Ngiam, J., Coates, A., Lahiri, A., Prochnow, B. Q., Le, V., Ng, A. Y. :On optimization methods for deep learning,” in Proc. ICML, (2011),pp. 265–272. 3. McClelland, J. L., Rumelhart, D. E. :Parallel distributed processing. MIT Press Cambridge, MA, (1987), vol. 2. 4. Rumelhart, D. E., Hinton, G. E., Williams, R. J. :Neurocomputing: Foundations of research,” J. A. Anderson and E. Rosenfeld, Eds.Cambridge, MA, USA: MIT Press, (1988), ch. Learning Representations by Back-propagating Errors, pp. 696–699. 5. Rose, D. C., Arel, I., Karnowski, T. P., Paquet, V. C.: Applying deep-layered clustering to mammography image analytics,” in BSEC, (2010), pp. 1–4. 6. Johnson, A. E. W., Ghassemi, M. M., Nemati, S., Niehaus, K. E., Clifton, D. A., Clifford, G. D. :Machine learning and decision support in critical care,” Proceedings of the IEEE, vol. 104, no. 2, pp. 444–466, Feb (2016). 7. Pouladzadeh, P., Kuhad, S. V. B., Peddi, A., Shir-Mohammadi, S. :Food calorie measurement using deep learning neural network,” in I2MTC, (2016), pp. 1–6. 8. Huang, J., Zhou, W., Li, H., Li, W.: Sign language recognition using real-sense,” in IEEE ChinaSIP, (2015), pp. 166–170. 9. Yalcin¸ H. :Human activity recognition using deep belief networks,” in, (2016), pp. 1649–1652. 10. Hinton, G. E., Salakhutdinov, R. R.: Reducing the dimensionality of data with neural networks,” Science, vol. 313, no. 5786, pp. 504–507,(2006). 11. Hinton, G. E., Osindero, S., Teh, Y.W.: A fast learning algorithm for deep belief nets,” Neural Comput., vol. 18, no. 7, pp. 1527–1554 (2006). 12. Fryar, C. D., Ostchega, Y., Hales, C. M., Zhang, G., & Kruszon-Moran, D. (2017). Hypertension Prevalence and Control Among Adults: United States, 2015-2016. NCHS Data Brief (289), 1-8. 13. Williams, R. J., Zipser, D.: A learning algorithm for continually running fully recurrent neural networks,” Neural Comput., vol. 1, no. 2, pp. 270–280, (1989). 20 14. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P. :Gradient-based learning applied to document recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, (1998). 15. Bengio, Y., Simard, P., Frasconi, P.: Learning long-term dependencies with gradient descent are difﬁcult,” IEEE Trans. Neural Netw., vol. 5, no. 2, pp. 157–166, (1994). 16. Ackley, D., Hinton, G., Sejnowski, T.: Learning and relearning in Boltzmann machines,” Parallel Distributed Processing: Explorations microstructure of Cognition, (1986). 17. Wang, H., Yeung, D.Y.: Towards Bayesian Deep Learning: A Survey,” ArXiv e-prints, Apr. 2016. 18. Carreira-Perpignan, M. A., Hinton, G. :On contrastive divergence learning.” in AISTATS, vol. 10, (2005), pp. 33–40. 19. Hubel D. H., Wiesel, T. N.: Receptive ﬁelds, binocular interaction and functional architecture in the cat’s visual cortex,” The Journal of physiology, vol. 160, no. 1, pp. 106–154, (1962). 20. Kondo, T., Ueno, J., Takao, S.: Medical image recognition of abdominal multi-organ by hybrid multi-layered gmdh-type neural network using principal component- regression analysis,” in CALENDAR,(2014), pp. 157–163. 21. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Good fellow, I. J., Fergus, R. :Intriguing properties of neural networks.”CoRR, vol. abs/1312.6199, (2013). 22. Hood, L., Friend, S. H.: Predictive, personalized, preventive, participatory (p4) cancer medicine,” Nature Reviews Clinical Oncology, vol. 8, no. 3, pp. 184–187, (2011). 23. Leung, M. K., Delong, A., Alipanahi, B., Frey, B. J.: Machine learning in genomic medicine: A review of computational problems and data sets,” Proceedings of the IEEE, vol. 104, no. 1, pp. 176–197,(2016). 24. Angermueller, C., Parnamaa, T., Parts, L., Stegle, O.: Deep learning for computational biology,” Molecular Systems Biology, vol. 12, no. 7, p. 878, (2016). 25. Pastur-Romay, L. A., Cedar, F. A., Pazos, A., Porto-Pazos, B. :Deep artiﬁcial neural networks and neuro morphic chips for big data analysis: Pharmaceutical and bioinformatics applications,” International Journal of Molecular Sciences, vol. 17, no. 8, p. 1313, (2016). 26. Ibrahim, R., Yousri, N. A., Ismail, M. A., El-Makky, N. M. :Multi-level gene/miRNA feature selection using deep belief nets and active learning,” in EMBC, (2014), pp. 3957–3960. 21 27. Da He, D., Winokur, E. S., & Sodini, C. G. (2012). An ear-worn continuous ballistocardiogram (BCG) sensor for cardiovascular monitoring. Paper presented at the Engineering in Medicine and Biology Society (EMBC), 2012 Annual International Conference of the IEEE. 28. Winokur, E. S., Delano, M. K., & Sodini, C. G. (2013). A wearable cardiac monitor for long-term data acquisition and analysis. IEEE Transactions on Biomedical Engineering, 60(1), 189-192. 29. Havaei, M., Guizard, N., Larochelle, H., Jodoin, P.: Deep learning trends for focal brain pathology segmentation in MRI,” CoRR, vol.abs/1607.05258, (2016). 30. Greenspan, H.,Van Ginneken, B., Summers, R. M. :Guest editorial deep learning in medical imaging: Overview and future promise of an exciting new technique,” IEEE Trans. Med. Imag., vol. 35, no. 5, pp.1153–1159, May (2016). 31. Yan, Z., Zhan, Y., Peng, Z., Liao, S., Shinagawa, Y., Zhang, S., Metaxas, D. N., Zhou, X. S. :Multi-instance deep learning: Discover discriminative local anatomies for body part recognition,” IEEE Trans.Med. Image, vol. 35, no. 5, pp. 1332–1343, (2016). 32. Avendi, M., Kheradvar, A., Jafarkhani, H.: A combined deep-learning and deformable- model approach to fully automatic segmentation of the left ventricle in cardiac MRI,” Medical image analysis, vol. 30, pp. 108–119, (2016). 33. Fritscher, K., Raudaschl, P., Zafﬁno, P., Spadea, M. F., Sharp, G. C., Schubert, R. :Deep neural networks for fast segmentation of 3dmedical images,” in MICCAI, (2016), pp. 158–165. 34. Shin, H.C., Roth, H. R., Gao, M., Lu, L., Xu, I., Nogues, J., Yao, D., Mollura, R., Summers, M. :Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics, and transfer learning,” IEEE Trans. Med. Imag., vol. 35, no. 5, pp.1285–1298, (2016). 35. Chen, H., Ni, D., Qin, J., Li, S., Wang, X. T., Heng, P. A.: Standard plane localization in fetal ultrasound via domain transferred deep neural networks,” IEEE J. Biomed. Health Inform. vol. 19, no. 5, pp. 1627–1636, (2015). 36. Cheng, Z., Ni, D., Chou, Y.H., Qin, J. C., Tiu, M., Chang, Y.C., Huang, C. S., Shen, D., Chen, C. M.: Computer-aided diagnosis with deep learning architecture: Applications to breast lesions in our images and pulmonary nodules in ct scans,” Scientiﬁc reports, vol. 6, (2016). 37. Shan, J., Li, L.: A deep learning method for micro-aneurysm detection in fundus images,” in IEEE CHASE, (2016), pp. 357–358. 22 38. Li, F., Tran, L., Thung, K. H., Ji, S., Shen, D., Li, J.: A robust deep model for improved classiﬁcation of ad/MCI patients,” IEEE J. Biomed. Health Inform. vol. 19, no. 5, pp. 1610–1616, Sept (2015). 39. Kuang, D., He, L.: Classiﬁcation on ADHD with deep learning,” in CCBD, Nov 2014, pp. 27–32. 40. Brosch, T., Tam, R., Initiative et al., A. D. N.: Manifold learning of brain MRIs by deep learning,” in MICCAI, 2013, pp. 633–640. 41. Sun, W., Shao, S., Zhao, R., Yan, R., Zhang, X., Chen, X.: A sparse auto-encoder-based faults classification,” induction motor deep neural network approach Measurement, vol. 89, pp. 171–178, (2016). for 42. Lu, C., Wang, Z. Y., Qin, W.L., Ma, J.: Fault diagnosis of rotary machinery components using a stacked denoising auto encoder-based health state identification,” Signal Processing, vol. 130, pp. 377–388, (2017). 43. Collins, A., Yao, Y.: Machine learning approaches: Data integration for disease prediction and prognosis,” in Applied Computational Genomics. Springer, (2018), pp. 137–141. 44. Wang, Z., Shah, A. D., Tate, A. R., Denaxas, S., Shawe-Taylor, J., Hemingway, H. :Extracting diagnoses and investigation results from unstructured text in electronic health records by semi-sup (2012) 45. Chen, H., Zhang, Y., Kalra, M. K., Lin, F., Chen, Y., Liao, P., Zhou, J., Wang, G. : Low- dose ct with a residual encoder-decoder convolutional neural network,” IEEE transactions on medical imaging, vol. 36no. 12, pp. 2524–2535, (2017). 46. El-Gamal, F. E., Elmogy, A. M., Atwan, A.: Current trends in medical image registration and fusion,” Egyptian Informatics Journal vol. 17, no. 1, pp. 99–124, (2016). 47. Zech, J., Pain, M., Titano, J., Badgeley, M., Schefflein, J., Su, A., Costa, A., Bederson, J., Lehar, J., Oermann, E. K. :Natural language-based machine learning models for the annotation of clinical radiology reports,” Radiology, vol. 287, no. 2, pp. 570–580, (2018). 48. Ma, H.Y., Zhou, Z., Wu, S., Wan, Y.L., Tsui, P.H. :A computer-aided diagnosis scheme for detection of fatty liver in vivo based on ultrasound kurtosis imaging,” Journal of medical systems, vol. 40, no. 1,p. 33, (2016). 23 49. Zhang, Z., et al. : Reinforcement learning in clinical medicine: a method to optimize dynamic treatment regime over time,” Annals of translational medicine, vol. 7, no. 14, 2019. 50. Rau, C.S., Kuo, P. J., Chien, P.C., Huang, C.Y., Hsieh, H.Y., Hsieh, C.H. :Mortality prediction in patients with isolated moderate and severe traumatic brain injury using machine learning models,” PloS one, vol. 13, no. 11, p. e0207192, (2018). 51. Wallace, D. S.: The role of speech recognition in clinical documentation,” Nuance Available: on Communications, https://www.hisa.org.au/slides/hic18/wed/SimonWallace.pdf 14-Dec2019. [Online]. access 2018, 52. Neveol, A., Dalianis, H., Velupillai, S., Savova, G., Zweigenbaum, P. :Clinical natural language processing in languages other than English: opportunities and challenges,” Journal of biomedical semantics, vol. 9, no. 1, p. 12, (2018). 53. Tajbakhsh, N., Shin, J. Y., Gurudu, S. R., Hurst, R. T., Kendall, C. B., Gotway, M. B., Liang, J.: Convolutional neural networks for medical image analysis: Full training or ﬁne tuning?” IEEE Trans.Med. Imag., vol. 35, no. 5, pp. 1299–1312, 2016. 54. Latif, S., Asim, M., Usman, M., Qadir, J., Rana, R. :Automating motion correction in multishot MRI using generative adversarial networks,” Published as Workshop Paper at 32nd Conference on Neural Information Processing Systems (NIPS 2018). 55. Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M., Elhadad, N. :Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission,” in Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, (2015), pp. 1721–1730. 56. Xia, F., Yetisgen-Yildiz, M. :Clinical corpus annotation: challenges and strategies,” in Proceedings of the Third Workshop on Building and Evaluating Resources for Biomedical Text Mining (BioTxtM’2012) in conjunction with the International Conference on Language Resources and Evaluation (LREC), Istanbul, Turkey, (2012). 57. Biggio, B., Nelson, B., Laskov, P. :Poisoning attacks against support vector machines,” in 29th International Conference on Machine Learning, (2012), pp. 1807–1814. 58. Usama, M., Qadir, J., Al-Fuqaha, A., Hamdi, M. :The adversarial machine learning conundrum: Can the insecurity of ml become the Achilles’ heel of cognitive networks?” arXiv preprint arXiv:1906.00679, (2019). 59. David, B., Dowsley, R., Katti, R., Nascimento, A. C. :Efficient unconditionally secure comparison and privacy-preserving machine learning classification protocols,” in International Conference on Provable Security. Springer, (2015), pp. 354–367. 24 60. Beaulieu-Jones, B. K., Yuan, W., Finlayson, S. G., Wu, Z. S. :Privacy-preserving distributed deep learning for clinical data,” Machine Learning for Health (ML4H) Workshop at NeurIPS, (2018). 61. Bengio, Y., Lamblin, P., Popovici, D., Larochelle et al., H. :Greedy layer-wise training of deep networks,” Advances in neural information processing systems, vol. 19, p. 153, (2007). 62. Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.A. :Extracting and composing robust features with denoising autoencoders,” in Proceedings of the 25th international conference on Machine learning. ACM, (2008), pp. 1096–1103. 63. Schmidhuber, J. :Deep learning in neural networks: An overview,” Neural Networks, vol. 61, pp. 85–117, (2015), published online 2014; based on TR arXiv:1404.7828 [cs.NE]. 64. Jaeger, H. :Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the” echo state network” approach. GMD For schungszentrum Informationstechnik, 2002. 65. Chung, J., Gulcehre, C., Cho, K., Bengio,Y. :Empirical evaluation of gated recurrent neural networks on sequence modeling,” arXiv preprint ar X iv:1412.3555, (2014). 66. Zhu, W., Liu, C., Fan, W., Xie, X. :Deep 3d dual-path nets for automated pulmonary in 2018 IEEE Winter Conference on nodule detection and classification,” Applications of Computer Vision (WACV). IEEE, (2018), pp. 673–681. 67. Jagielski, M., Oprea, A., Biggio, B., Liu, C., Nita-Rotaru, C., Li, B. :Manipulating machine learning: Poisoning Attacks and countermeasures for regression learning,” in 2018 IEEE Symposium on Security and Privacy (SP). IEEE, (2018), pp. 19–35. 68. Malathi, D., Logesh, R., Subramaniyaswamy, V., Vijayakumar, V., Sangaiah, A. :Hybrid reasoning-based privacy-aware disease prediction support system,” Computers & Electrical Engineering, vol. 73, pp. 114–127, (2019). 69. Takabi, H., Hesamifard, E., Ghasemi, M. :Privacy-preserving multiparty machine learning with homomorphic encryption,” in 29th Annual Conference on Neural Information Processing Systems (NIPS), 2016. 70. Kim, M., Song, Y., Wang, S., Xia, Y., Jiang, X. :Secure logistic regression based on homomorphic encryption: Design and evaluation,” JMIR medical informatics, vol. 6, no. 2, p. e19, (2018). 71. Finlayson, S. G., Bowers, J. D., Ito, J. J., Zittrain, L., Beam, A. L., Kohane, I. S. :Adversarial attacks on medical machine learning,” Science, vol. 363, no. 6433, pp. 1287–1289, (2019). 25 72. Ohrimenko, O., Schuster, F., Fournet, C., Mehta, A., Nowozin, S., Vaswani, K, Costa, M. :Oblivious multi-party machine learning on trusted processors,” in 25th USENIX Security Symposium (USENIX Security 16), (2016), pp. 619–636. 73. McMahan, H. B., Moore, E., Ramage, D., Hampson et al., S. :Communication-efficient learning of deep networks from decentralized data,” Proceedings of the 20 the International Conference on Artificial Intelligence and Statistics (AISTATS) JMLR: W&CP volume54, (2017). 74. Nervana Systems, Available:https://github.com/NervanaSystems/neon (2020). “Neon,” [Online]. 75. Choi, Y., Jeon, Y.-M., Wang, L., & Kim, K. (2017). A Biological Signal-Based Stress Monitoring Framework for Children Using Wearable Devices. Sensors, 17(9), 1936. 76. Choo, D., Dettman, S., Dowell, R., & Cowan, R. (2017). Talking to Toddlers: Drawing on Mothers' Perceptions of Using Wearable and Mobile Technology in the Home. Studies in health technology and informatics, 239, 21-27. 77. Chen, S.-T., Lin, S.-S., Lan, C.-W., & Hsu, H.-Y. (2018). Design and Development of a Wearable Device for Heat Stroke Detection. Sensors, 18(1), 17. 78. Ravì, D., Wong, C., Deligianni, F., Berthelot, M., Andreu-Perez, J., Lo, B., Yang, G. :Deep Learning for Health Informatics. IEEE journal of biomedical and health informatics. PP. 10.1109/JBHI.2016.2636665 (2016). 79. Rui, Z., Ruqiang, Y., Zhenghua, C., Kezhi M., Peng, W., Robert,G. :Deep Learning and Its Applications to Machine; Health Monitoring: A Survey JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST (2015) 26",0.0
Deep Learning and Health Informatics for Smart Monitoring and Diagnosis,"[{'href': 'http://arxiv.org/abs/2208.03143v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2208.03143v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-08-05 13:07:59,,"2 2 0 2 g u A 6 2 ] C D . s c [ 2 v 5 9 1 2 1 . 8 0 2 2 : v i X r a ExpoCloud: a Framework for Time and Budget-Eﬀective Parameter Space Explorations Using a Cloud Compute Engine Meir Goldenberg1 Jerusalem College of Technology Abstract Large parameter space explorations are among the most time consuming yet critically important tasks in many ﬁelds of modern research. ExpoCloud enables the researcher to harness cloud compute resources to achieve time and budget- eﬀective large-scale concurrent parameter space explorations. ExpoCloud enables maximal possible levels of concurrency by creating com- pute instances on-the-ﬂy, saves money by terminating unneeded instances, pro- vides a mechanism for saving both time and money by avoiding the exploration of parameter settings that are as hard or harder than the parameter settings whose exploration timed out. Eﬀective fault tolerance mechanisms make Ex- poCloud suitable for large experiments. ExpoCloud provides an interface that allows its use under various cloud envi- ronments. As a proof of concept, we implemented a class supporting the Google Compute Engine (GCE). We also implemented a class that simulates a cloud environment on the local machine, thereby facilitating further development of ExpoCloud. The article describes ExpoCloud’s features and provides a usage example. The software is well documented and is available under the MIT license [1, 2]. Keywords: parameter space exploration, distributed computing, cloud compute engine, large-scale. 1Email address: mgoldenb@g.jct.ac.il Preprint submitted to Journal of Parallel and Distributed Computing August 29, 2022 Introduction Large parameter space explorations are among the most time consuming yet critically important tasks in many ﬁelds of modern research. For example, com- puter science research is often concerned with the study of algorithms for solving computational problems, whereby the algorithm’s behavior and the computa- tion time for solving the problem are controlled by a number of parameters. The possible settings of these parameters form a large parameter space, whose thorough exploration requires that the algorithm be run to solve a number of problem instances for each parameter setting of both the algorithm and the problem. It is our assumption in this work that individual parameter settings can be explored concurrently and independently of each other. In the absence of a tool that makes large-scale parameter explorations easy to accomplish, researchers resort to running ad hoc scripts either on a local machine or on a cluster. Most recently, cloud-based compute engines became a budget-friendly option. The amount of computational power available through such services is usually much greater than that available in the research clusters. However, the amount of technical expertise and scripting required to set up an experiment that harnesses these resources may prove to be a stumbling block. As a result, the researchers adopt simplifying limitations, such as using multiple threads on a single compute instance [3]. The vision We envisioned a framework that would let the researcher deﬁne his/her work- load and achieve maximal concurrency while economizing on his/her time and money, allowing ﬂexibility in choosing the cloud platform and providing fault tolerance. ExpoCloud [1] is our implementation of the above vision. It realizes the words that appear above in italics as follows: 2 • The workload is a list of tasks, each deﬁned by a setting of parameters. It is computed at the commencement of the experiment and passed to the framework for automated execution. • Maximal concurrency is achieved by creating a new compute instance as often as is allowed by the cloud platform, for as long as there are tasks to assign. • Economizing on time is achieved by letting the user specify a deadline and a hardness (deﬁned below) for each task. When a task takes more time to execute than the time speciﬁed by the deadline, we say that the task has timed out. ExpoCloud terminates timed out tasks automatically. A task’s hardness is a tuple of parameter values that correlate with the time required to execute the task. The researcher speciﬁes which subset of parameters determines a task’s hardness and provides a method that compares hardnesses of two tasks. When a task times out, the framework terminates all currently running tasks that are as hard or harder than the timed out task. It also avoids running such tasks in the future. The framework executes the tasks in the order from the easiest to the hardest, so as to maximize the number of tasks that do not have to be executed. • Economizing on money is achieved by deleting a compute instance as soon as it is done with the tasks assigned to it and there are no more tasks to be assigned. • ExpoCloud provides great ﬂexibility for choosing the cloud platform. To adapt to a given cloud platform, one needs to merely provide an extension class with methods to create, terminate and list compute instances. In addition, the researcher is in full control of the properties of the compute instances, since all of them are created based on machine images speciﬁed by the researcher. • Fault tolerance means that the computation would proceed even if one or more compute instances fail for any reason. 3 Before moving on to the main part of the article, we introduce an example that we will use to demonstrate the framework’s design and usage. The example parameter exploration Consider the agent assignment problem, as follows. A team of n agents needs to complete a project consisting of m tasks, where n ≥ m. The tasks have to be performed sequentially. For each agent i and task j, we are given tij, the amount of time, in seconds, that the agent i requires to complete the task j. The problem is to assign an agent to each task, such that no agent is assigned to more than one task and the total time of completing the project is minimized. Suppose we use the classical branch and bound (B&B) search algorithm for solving this problem, as follows. The algorithm is recursive. It starts with an empty assignment, whereby no agent is assigned to a task. At each recursive call, it extends the current partial assignment by assigning an agent to the next task. When all tasks have been assigned an agent, we say that the assignment is full. At this base case of the recursion, the algorithm updates the currently best full assignment and the corresponding time for completing the project. The advantage of B&B search over the brute-force search is the so called B&B cutoﬀ. Namely, whenever the time corresponding to the current partial assignment is greater than that of the best full assignment, the current assign- ment can be discarded without losing the solution optimality. A more eﬃcient version of this algorithm uses a heuristic. Given a partial assignment, the heuristic is a lower bound on the time needed to complete the remaining tasks. This bound is computed by assigning the best of the unused agents to each of the remaining tasks, while allowing the assignment of the same remaining agent to more than one remaining task. Whenever the sum of the time corresponding to the current partial assignment and the heuristic is greater than that of the best full assignment, the current assignment can be discarded. Thus, we have three algorithmic variants - the brute-force search, the classi- cal B&B search and the B&B search with a heuristic. To thoroughly understand the properties of the agent assignment problem and the B&B search’s perfor- 4 mance for solving it, we need to run each algorithmic variant to solve a number of generated problem instances for many possible settings of the number of agents n and the number of tasks m. What range of values should we consider for the number of agents n and the number of tasks m? Without a framework like ExpoCloud, this question is not easily answered. First, the range will depend on the algorithmic variant. The brute-force search will only be able to handle small problems, while B&B with a heuristic might be able so solve much larger instances. Knowing his/her budget of time for the whole experiment, the researcher might decide on a deadline per problem instance. He/she might then perform several test runs to get a feeling for how much time each algorithmic variant takes to solve problem instances of various sizes. Even after this laborious tuning stage, the researcher will still run the risk of some instances taking disproportionately long time, possibly stumbling the whole experiment. With ExpoCloud, the question is really easy. First the researcher writes a short class that deﬁnes a task as running one algorithmic variant to solve a single problem instance for one particular setting of n and m. After deciding on a deadline, the researcher picks a large range of values for n and m, with the upper bounds that for sure cannot be solved by the best algorithmic variant. He/she writes a single nested loop to generate all the tasks. All of this is shown in the next section. Next, the researcher notices that larger values of n correspond to harder problem instances and so do larger values of m. It is also clear that the same instance is likely to be solved faster by the B&B search with a heuristic than with the classical B&B, which is in turn faster than the brute-force algorithm. The researcher deﬁnes several short methods informing the framework of these observations and oﬀ the experiment goes. ExpoCloud will care both for stopping a problem instance as soon as it times out and for not attempting exploring parameter settings that are as hard or harder. The researcher does not need to worry about deciding on the number of compute instances and creating those instances. Neither does he need to worry 5 about stopping compute instances when the experiment is done. The results are easily obtained in a nice tabular format, which is again speciﬁed with a few short methods. If the researcher wants to run the experiment locally, e.g. on his/her laptop, he/she can do that as well. ExpoCloud makes it easy to use as many CPUs of the researcher’s machine as desired. Furthermore, this run is actually a simulation of performing the experiment on the cloud. It is thus a powerful tool to facilitate further development of the framework. ExpoCloud is written in Python and is available on GitHub under the MIT license [1]. The GitHub page contains the user documentation and links to the developer’s documentation, where the source code is described [2]. The next section details the features of the framework and shows in full how the above example experiment is setup and run. Material and methods The overall architecture The overall architecture of ExpoCloud is shown in Figure 1. It is a server- client architecture that uses a pull model to assign jobs to clients. Previous research [4] has shown suitability of such an architecture for distributed scientiﬁc computations. A distinguishing attribute of ExpoCloud is that it creates compute instances on-the-ﬂy. Creating clients on-the-ﬂy enables ExpoCloud to harness the great potential for large-scale concurrency provided by cloud-based compute engines. Creating replacement servers on-the-ﬂy enables ExpoCloud to achieve eﬀective fault tolerance. Two features of the architecture are not shown in Figure 1. First, there are two-way communication channels between the clients and the backup server. We detail the need for these channels in the section on fault tolerance below. Second, a client creates and manages worker processes. Each worker is responsible for executing a single task and communicating the results to the client. 6 Figure 1: The overall architecture of ExpoCloud The next section demonstrates how one can specify the example experiment described in the introduction. We will then show how the individual components of the architecture are implemented to provide time and budget eﬃciency as described in the introduction. The example experiment To set up an experiment, one needs to write a short Python script that creates the primary server object, while providing it with the description of the tasks to be executed, the conﬁguration of the compute engine and other optional arguments. We now show a possible script for exploring the parameter space when solv- ing the agent assignment problem using B&B. Here is the part of the script that constructs the list of tasks: t a s k s = [ ] m a x n t a s k s = 50 n i n s t a n c e s p e r s e t t i n g = 20 f o r o p t i o n s in [ { Option .NO CUTOFFS} , { } , { Option . HEURISTIC } ] : f o r n t a s k s in range ( 2 , m a x n t a s k s + 1 ) : 7 f o r n a g e n t s in range ( n t a s k s , 2 ∗ n t a s k s ) : i n s t a n c e s = g e n e r a t e i n s t a n c e s ( n t a s k s , n a g e n t s , f i r s t i d = 0 , l a s t i d = n i n s t a n c e s p e r s e t t i n g − 1 ) f o r i n s t a n c e in i n s t a n c e s : t a s k s . append ( Task ( Algorithm ( o p t i o n s , i n s t a n c e ) ) ) The outer loop iterates over the three variants of the algorithm: the brute- force search, the classical B&B search and the B&B search with a heuristic. The following two nested loops iterate over the possible values of n and m. For each parameter setting, a task is formed for each of the 20 generated problem instances. This task is added to the list tasks. The key component here is the Task class, which the researcher needs to provide. For our experiment, this class may look as follows: c l a s s Task ( AbstractTask ) : def i n i t ( s e l f , a l g o r i t h m , t i m e o u t = 6 0 ) : super ( Task , s e l f ) . i n i t ( a l g o r i t h m , t i m e o u t ) def p a r a m e t e r t i t l e s ( s e l f ) : return s e l f . i n s t a n c e . p a r a m e t e r t i t l e s ( ) + ( ” Options ” , ) def p a r a m e t e r s ( s e l f ) : return s e l f . i n s t a n c e . p a r a m e t e r s ( ) + ( s e t 2 s t r ( s e l f . o p t i o n s ) , ) def h a r d n e s s p a r a m e t e r s ( s e l f ) : def o p t i o n s 2 h a r d n e s s ( o p t i o n s ) : i f Option . HEURISTIC in o p t i o n s : return 0 i f Option .NO CUTOFFS in o p t i o n s : return 2 return 1 return ( 8 o p t i o n s 2 h a r d n e s s ( s e l f . o p t i o n s ) , s e l f . i n s t a n c e . n t a s k s , s e l f . i n s t a n c e . n a g e n t s ) def r e s u l t t i t l e s ( s e l f ) : return s e l f . a l g o r i t h m . r e s u l t t i t l e s ( ) def run ( s e l f ) : return s e l f . a l g o r i t h m . s e a r c h ( ) def g r o u p p a r a m e t e r t i t l e s ( s e l f ) : return f i l t e r o u t ( s e l f . p a r a m e t e r t i t l e s ( ) , ( ’ i d ’ , ) ) A brief description of each method follows: • parameter_titles - returns the tuple of parameter names, which would appear as column titles in the formatted output. In the example imple- mentation, these consist of the parameters of the problem instance, such as the number of agents and the number of tasks, appended by the pa- rameters of the search algorithm being used. • parameters - returns the tuple of parameter values describing the task. • hardness_parameters - returns the subset of parameters used to deter- mine the task’s hardness. The default implementation in AbstractTask says that task T1 is as hard or harder than task T2 if all the hardness parameters of the former are greater than or equal to the corresponding parameters of the latter. Note how the shown code converts the param- eters of the search algorithm into a number, so as to adapt this default implementation. Internally, the hardness of a task is stored as an instance of the Hardness class deﬁned inside AbstractTask. The Task class derives from AbstractTask and may provide its own deﬁnition of Hardness, thereby 9 gaining full control over the way in which the hardnesses of two tasks are compared. • result_titles - returns the tuple of names of output quantities, such as the optimal time for executing the project and the time taken by the search algorithm. The actual tuple of output quantities is returned by the run method described below. • run - executes the task by running the search algorithm to solve the prob- lem instance. If the algorithm is implemented in Python, as in our exam- ple, the suitable method of the algorithm object is run. Otherwise, the algorithm can be run as a shell command. • group_parameter_titles - returns the tuple of parameter names that determine groups of tasks, as we now explain. Consider a state of the experiment, whereby results for three out of twenty problem instances for a particular setting of parameters have been computed. Suppose that a task timed out at this point, which disqualiﬁed the remaining sixteen tasks as being too hard. It stands to reason that the results for the three executed tasks should be discarded, since the average of the output quantities over only three tasks would have low statistical signiﬁcance. On the other hand, had we obtained results for eighteen instances before a particularly hard task timed out, we may want to keep the results for this setting of parameters. ExpoCloud makes the decision of whether to keep a parameter setting on a per-group basis. A group consists of all the tasks with the same values of the parameters returned by the group_parameter_titles method.2 A group is kept when the number of solved tasks in the group is at least as large as the optional min_group_size argument to the constructor of the server object. In the shown implementation, a group is deﬁned by all 2This is somewhat similar to the idea of the GROUP BY clause in SQL. 10 the parameters besides the id of the problem instance within a particular setting of parameters. The default value of the min_group_size argument is zero, which means that all the results are kept. The next section of the script speciﬁes the conﬁguration for the compute engine and passes this conﬁguration to the constructor of the engine object: c o n f i g = { ’ p r e f i x ’ : ’ agent−a s s i g n m e n t ’ , ’ p r o j e c t ’ : ’ bnb−agent−a s s i g n m e n t ’ , ’ zone ’ : ’ us−c e n t r a l 1 −a ’ , ’ s e r v e r i m a g e ’ : ’ s e r v e r −t e m p l a t e ’ , ’ c l i e n t i m a g e ’ : ’ c l i e n t −t e m p l a t e ’ , ’ r o o t f o l d e r ’ : ’ ˜/ ExpoCloud ’ , ’ p r o j e c t f o l d e r ’ : ’ examples . a g e n t a s s i g n m e n t ’ } e n g i n e = GCE( c o n f i g ) The conﬁguration is a dictionary with the following keys: • prefix - the preﬁx used for the automatically generated names of compute instances. Several experiments with diﬀerent preﬁxes may be conducted simultaneously. • project - the name identifying the project on the cloud platform. • zone - the zone to which the allocated compute instances will pertain. The current implementation of the GCE engine is limited to use a single zone. This limitation may be lifted in the future to enable an even larger scalability. • server_image and client_image - the names of the machine images stor- ing the conﬁguration (such as the CPU family, the number of CPUs, the amount of RAM, etc) of all future server and client instances, respectively. An inexpensive conﬁguration with one or two CPUs may be used for a 11 server, while one may opt for 64 or more CPUs per instance for a client. ExpoCloud’s clients make use of all the available CPUs automatically. • root_folder - the folder in which ExpoCloud resides on all the compute instances. • project_folder - the folder in which the user-provided scripts reside. The folder must be speciﬁed in the dotted format as shown in the listing.3 In our case, the engine being used is the Google Compute Engine (GCE). Some dictionary keys for other engines may diﬀer. For example, zone is a GCE concept and a more suitable key name may be used in the extension class for another platform. Lastly, we construct the primary server object and call its run method: S e r v e r ( t a s k s , e n g i n e ) . run ( ) Once the experiment completes, the main ExpoCloud folder at the primary server will have an output folder containing a results ﬁle and a folder for each client instance. Such a client folder will contain ﬁles with the events sent by the client. ExpoCloud provides a script for convenient viewing of both the results and the client events related to the execution of the tasks. ExpoCloud provides a local machine engine for running an experiment lo- cally. The only change in the above script concerns the construction of the engine object: e n g i n e=L o c a l E n g i n e ( ’ examples . a g e n t a s s i g n m e n t ’ ) Once the experiment completes, the main ExpoCloud folder will have an output folder for each of the servers, as well as a ﬁle for stdout and stderr for each client. Running the experiment locally is useful both for small initial explorations. It also enables rapid development, since it makes it unnecessary 3This is the format in which the path must be speciﬁed when using the -m command-line argument to python. 12 to copy each updated version to the cloud and avoids the latencies associated with using the cloud. We now describe in detail how the primary server operates. The primary server We ﬁrst describe how the primary server stores the tasks, then outline the workings of the run method at a high level, and lastly zoom in on the message- handling part of the primary server. a. The tasks-related lists There are three tasks-related lists - the actual list of tasks and two auxiliary lists used for performance and fault tolerance. We describe them in turn. The list of tasks, called tasks, is sorted in the order of non-decreasing hard- ness of tasks. This order maximizes the number of tasks that are not attempted as a result of a previous task timing out. The original order of tasks is restored prior to the printing of results. The list tasks_from_failed consists of indices of the tasks that have been assigned to a client, but not completed due to a failure of the client instance. When a client requests tasks, the tasks in tasks_from_failed are assigned ﬁrst. The next task from tasks is assigned only if tasks_from_failed is empty. Lastly, the list min_hard consists of hardnesses of tasks that have timed out. Whenever the server is about to assign a task, it ﬁrst checks whether the hardness of the task is equal or greater than any of the elements in min_hard. min_hard is kept small by only storing the minimal elements. b. The run method The primary server object’s run method executes an inﬁnite loop. An iter- ation of this loop performs the following actions: 1. Informs the backup server that the primary server is continuing to function properly. We refer to such a message as a health update. 13 2. Handles handshake requests from newly started instances. The instance can be either a backup server or a client. We refer to the instance that has shaken hands with the primary server as an active instance. In response to a handshake request, two-way communication channel with the instance is established.4 In contrast to this channel, the queue for ac- cepting handshakes is created by the primary server’s constructor. When an instance is started, it gets the IP address of the primary server and the port number for handshake requests as command line arguments. In addition, if the instance is a client, a folder for storing the client events is created. 3. Handles messages from clients. We outline the messages and how they are handled in the next section. 4. Creates either the backup server or a new client instance. The creation of the backup server takes precedence. If the backup server is already running or the researcher opted to not use a backup server for the experiment, then a new client is created. Cloud compute engines do not let users to create instances in quick succession. Therefore, ExpoCloud uses exponentially increasing delays between attempts at creating cloud instances. 5. Terminates unhealthy instances. An active instance is unhealthy if it failed to send health updates to the server for the period of time speciﬁed by the HEALTH_UPDATE_LIMIT constant. A non-active instance is unhealthy if it failed to shake hands with the primary server for the period of time speciﬁed by the INSTANCE_MAX_NON_ACTIVE_TIME constant. 6. Outputs the results once there are no tasks that have not been assigned to clients and all clients completed the tasks assigned to them. The servers do not stop once the results are output. Thus, the fault tolerance 4Namely, the instance owns two queues registered with a SyncManager object. The pri- mary server creates the two corresponding queues at its end. SyncManager is part of the multiprocessing module of the Python standard library. It provides for low-latency commu- nication, which makes the distributed approach eﬀective even for ﬁne-grained tasks. 14 mechanisms continue to protect the results against a possible primary server instance failure. c. The handling of messages The following is an outline of messages that may arrive to the primary server from the backup server and the client instances: • HEALTH_UPDATE - the health update coming from either the backup server or a client. The primary server simply saves the timestamp of the last health update for each instance. • REQUEST_TASKS - the request for tasks by a client. The body of the message speciﬁes the number of tasks requested. If there are remaining unassigned tasks, the GRANT_TASKS message is sent in response. The body of this message contains the tasks assigned to the requesting client, including both the parameters and the full representation of the problem instances to be solved. If there are no unassigned tasks, the response is the NO_- FURTHER_TASKS message. • RESULT - the result of executing a task. The primary server stores the result with the task object. • REPORT_HARD_TASK - the report about a timed out task. The primary server updates the min_hard list and sends the APPLY_DOMINO_EFFECT message to all the clients, so they can terminate any task that is as hard or harder than the task just timed out. • LOG and EXCEPTION - the report about an event related to executing a task or to an exception, respectively, sent by a client. The primary server stores the event in the ﬁtting ﬁle corresponding to the client. • BYE - the client is done, which means that it had sent to the primary server the results for all the tasks assigned to it and had previously received the NO_FURTHER_TASKS message. The primary server terminates the client instance, so the researcher will not incur any further charges. 15 The primary server forwards a copy of each message from a client to the backup server. This keeps the backup server up-to-date and ready to take over should the primary server instance fail. This is further detailed in the section on fault tolerance below. We will now describe the operation of the clients. The clients We ﬁrst describe the main loop of the client object’s run method, then detail how the workers are managed and lastly zoom in on the message-handling part of the client. a. The main loop In contrast to the primary server, the client’s main loop is not inﬁnite – it stops when there are no tasks assigned to the client, and no more tasks that can be assigned to it by the primary server (i.e. the NO_FURTHER_TASKS message has been received). Each iteration of the main loop performs the following actions: 1. Sends the health update to the servers. 2. Processes workers as detailed in the next section. 3. Requests tasks from the primary server, subject to availability of idle CPUs and the NO_FURTHER_TASKS message not having been received. Note that the tasks requested previously, but not yet granted are taken into account when determining how many idle CPUs there are. 4. Processes messages from the primary server as detailed in a separate sec- tion below. 5. If new tasks have been granted by the primary server, starts the worker processes to execute them. Foe each message sent to the primary server, the client sends a copy of the message to the backup server. The need for this is explained in the below section on fault tolerance. That section also details what the client does with the incoming messages from the backup server. 16 Once the main loop is exited, the client sends the BYE message and completes. b. The management of workers Each task is performed by a separate worker process. The client performs three action to manage the workers: • Processes messages arriving from the workers. A worker can send two messages - WORKER_STARTED and WORKER_DONE. In response to either mes- sage, the client sends the LOG message with the corresponding body to the servers. The WORKER_DONE message results in sending the RESULT message as well. • Takes accounting of the worker processes that are no longer alive (i.e. either done or terminated), so as to be able to assign the released CPUs to other tasks. • Terminates processes whose tasks timed out. The client sends the REPORT_HARD_TASK message to the servers for each timed out task. c. The handling of messages from the primary server The following is an outline of messages that may arrive to the client from the primary server: • GRANT_TASKS - one or more tasks have been assigned to the client. The task is added to tasks list and a LOG message is sent to the servers to record the event of the receipt. • APPLY_DOMINO_EFFECT - the hardness of a task that timed out is reported by the primary server. The client terminates all workers currently per- forming tasks of equal or greater hardness. • NO_FURTHER_TASKS - the primary server informs that there are no more tasks to be assigned. The client stores this information, so as to avoid requesting tasks and exit the main loop once all the worker processes are completed. 17 In addition to the above messages, there are the STOP, RESUME and SWAP_- QUEUES messages, used to achieve fault tolerance. These are detailed in the next section. Fault tolerance One standard technique for achieving fault tolerance in a distributed system is by using redundancy [5]. This is the approach we follow by employing a backup server that mirrors the primary one and substitutes for it in the case of a failure. A backup server is not used when the computation is performed using the local machine engine. As mentioned above, the researcher may choose to disable the use of the backup server. This may be desired for a short experiment. We distinguish between three kinds of failure: client instance failure, backup server failure and primary server failure. Client failure does not require any special action besides registering the failure and re-assigning the tasks previ- ously assigned to the failed client. The latter is achieved by maintaining the tasks_from_failed list, as described above. In contrast, care needs to be taken to achieve correctness of recovery after a server failure. The following sections detail how this is achieved. a. Creation of the backup server The primary server creates the backup server in the same way as it creates clients. When a backup server instance does not yet exist, its creation takes precedence over the creation of a new client. Note that the backup server maybe created either at the beginning of the computation or after a server failure.5 Therefore, we need to create the backup server under the assumption that the distributed computation is in progress. To make sure that the newly created backup server is fully synchronized with the primary server, the primary server freezes its state prior to creating 5We will see below how the case of primary server failure is reduced to the case of the backup server failure. 18 the backup server. First, it stops accepting handshake requests from new client instances. Second, it sends the STOP message to the active clients, which causes them to refrain from actions that may result in messages to the server. An exception is made for the health reports, which the clients continue to send. Next, the primary server serializes its full state into a ﬁle in the output folder, creates a new instance on the compute engine, and copies the output folder to it. It then starts the backup server script on the new instance. This script unserializes the server object and runs its assume_backup_role method. As the name suggests, this method converts the primary server object into a backup server one. First, it disconnects the server object from the clients’ channels for communicating with the primary server and connects it to the channels for communicating with the backup server. Second, it creates a two-way channel for communicating with the primary server. Lastly, it shakes hands with the primary server, whereby two-way communication between them is established. Upon this handshake, the primary server resumes accepting handshake requests from new client instances and sends the RESUME message to the clients, so they can resume normal operation. Finally, the backup server script starts the main event loop of the server object. b. Primary and backup server coordination Whenever a new client shakes hands with the primary server, the latter sends the NEW_CLIENT message to the backup server with the information about the client. In response to this message the backup server creates the client object and establishes communication with it. Similarly, whenever the primary server detects a client failure, it sends the CLIENT_TERMINATED message to the backup server, which destroys the corresponding client object. When a client sends a message to the primary server, it sends a copy of the message to the backup server. Thus, the backup server receives two copies of each message sent by the client. The copy received directly from the client is needed for the case when the primary server fails before forwarding the message to the backup server. The copy received from the primary server is needed to 19 keep the two servers synchronized, as described below. The backup server takes actions based on the copy of the message received from the primary server. It simply pops the corresponding message received directly from the client oﬀ the queue. When the backup server registers the pri- mary server failure, it will be ready to take over, with all the messages received directly from the clients after the last message forwarded by the primary server. The backup server processes messages from clients in the same exact way as the primary server. It also sends messages to the clients that mirror the messages sent from the primary server. The mechanism of the backup server taking actions based on the copy of the message received from the primary server takes care of two possible causes of desynchronization. First, a client may fail after sending a message to the primary server, but before sending a copy to the backup server. Second, due to race conditions, it is possible for the two servers to handle messages from diﬀerent clients, such as requests for tasks, in diﬀerent order and end up in diﬀerent states. Similarly to how the backup server processes two copies of each message from a client, the clients processes two copies of each message from the servers - one received from the primary server and the other received from the backup server. A client performs actions only based on the messages received from the primary server and pops oﬀ the queue the corresponding messages received from the backup server. When the primary server fails, the remaining messages received from the backup server are treated as if they were from the primary server, as detailed in the next section. c. Handling server failure In response to the backup server failure, the primary server simply creates the new backup server as outlined in the last section. In the case of the primary server failure, the backup server changes its own role to being the primary server. It then proceeds to create a temporary connec- tion to each client’s inbound queue for communication with the primary server 20 and sends a SWAP message. In response to this message, the client swaps the queues for communicating with the primary server with those for communicat- ing with the backup server. After this, the client is ready to proceed normally. Thus, the case of the primary server failure is now reduced to the case of the backup server failure discussed above. One special case is when the primary server fails after creating a client instance, but before the new client shakes hands and the backup server is up- dated. In this case, there is a dangling client instance incurring charges for the researcher. To avoid this, as part of the backup server assuming the role of the primary server, it requests from the engine the list of all compute instances and deletes all client instances that are not represented by an existing client object. Discussion and conclusions We have presented the ExpoCloud framework for distributed computing us- ing cloud compute engines. Unlike the existing tools geared towards business workloads [6], ExpoCloud is speciﬁcally designed to make it easy to execute large parameter-space explorations. It addresses the four main concerns of the researcher: ease of deﬁning the workload, harnessing as much compute power as can be used to speed up the experiment, eliminating computations that do not or are unlikely to complete in a reasonable amount of time, and avoid- ing unnecessary charges. Combined with mechanisms for fault tolerance, these properties make ExpoCloud a ﬁtting tool for many research projects requiring large-scale parameter-space explorations. Future work may consider executing workloads with task dependencies, integrating ExpoCloud with existing tools, and addressing security concerns. Acknowledgements Access to the Google Compute Engine was provided through the Israel Data Science Initiative. 21 References [1] M. Goldenberg, ExpoCloud’s page on GitHub. URL https://github.com/mgoldenbe/ExpoCloud [2] M. Goldenberg, ExpoCloud developer’s documentation. URL https://expocloud.netlify.app [3] A. Pollack, Tutorial: parallelize your python code and run it on Google Cloud. URL https://youtu.be/i4aFiIB5urA [4] C. Pinchak, P. Lu, J. Schaeﬀer, M. Goldenberg, The canadian internet- worked scientiﬁc supercomputer, 17th International Symposium on High Performance Computing Systems and Applications (HPCS) (2003) 193–199. [5] C. Storm, Fault Tolerance in Distributed Computing, Vieweg+Teubner Ver- lag, Wiesbaden, 2012, pp. 13–79. doi:10.1007/978-3-8348-2381-6_2. URL https://doi.org/10.1007/978-3-8348-2381-6_2 [6] B. Burns, B. Grant, D. Oppenheimer, E. Brewer, J. Wilkes, Borg, omega, and kubernetes, Communications of the ACM 59 (5) (2016) 50–57. 22",1.0
COOKIEGRAPH: Measuring and Countering First-Party Tracking Cookies,"[{'href': 'http://arxiv.org/abs/2208.12370v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2208.12370v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-08-25 22:56:31,,"TMIC: App Inventor Extension for the Deployment of Image Classification Models Exported from Teachable Machine Fabiano Pereira de Oliveira Department of Informatics and Statistics, Federal University of Santa Catarina Florianópolis/SC, Brazil fabiano.pereira.oliveira@grad.ufsc.br Christiane Gresse von Wangenheim Department of Informatics and Statistics, Federal University of Santa Catarina Florianópolis/SC, Brazil c.wangenheim@ufsc.br Jean C. R. Hauck Department of Informatics and Statistics, Federal University of Santa Catarina Florianópolis/SC, Brazil jean.hauck@ufsc.br Summary TMIC is an App Inventor extension for the deployment of ML models for image classification developed with Google Teachable Machine in educational settings. Google Teachable Machine, is an intuitive visual tool that provides workflow-oriented support for the development of ML models for image classification. Aiming at the usage of models developed with Google Teachable Machine, the extension TMIC enables the deployment of the trained models exported as TensorFlow.js to Google Cloud as part of App Inventor, one of the most popular block-based programming environments for teaching computing in K-12. The extension was created with the App Inventor extension framework based on the extension PIC and is available under the BSD 3 license. It can be used for teaching ML in K-12, in introductory courses in higher education or by anyone interested in creating intelligent apps with image classification. The extension TMIC is being developed by the initiative Computação na Escola of the Department of Informatics and Statistics at the Federal University of Santa Catarina/Brazil as part of a research effort aiming at introducing AI education in K-12. Keywords: Machine Learning, Image Classification, Google Teachable Machine, App Inventor, Extension Statement of need in our daily lives, e.g., as spam filters, recommendation Nowadays, Machine Learning (ML) is present mechanisms, chatbots, digital assistants, etc. Considering its inevitable impact that people understand Machine Learning not just as a consumer, but also as a creator of this type of innovation (Touretzky et al., 2019). Therefore, it is important to start teaching ML concepts in K-12 following a trend emerging during the last few years (Marques et al., 2020). According to the K-12 Guidelines for Artificial Intelligence by AI4K12 (Touretsky et al., 2019), AI education encompasses 5 big ideas, including Machine Learning (ML). This includes an understanding of basic ML concepts as well as the application of these concepts, developing ML applications typically focusing on the task of image classification. Image classification is the process of taking an input (like a photo or videostream) and outputting a class (like “plastic garbage”) or a probability that the input is a particular class (“there’s a 90% probability that this image shows a plastic garbage”). Teaching the application of ML following a human-centric is essential it interactive ML process (Amershi et al., 2019, Gresse von Wangenheim and von Wangenheim, 2021) includes the development, from requirements analysis to exporting the developed model and its deployment (Figure 1). Figure 1. Human-centric interactive ML process The development of ML models is typically taught in K-12 adopting visual tools, such as Google Teachable Machine. Google Teachable Machine (teachablemachine.withgoogle.com) is a free web-based GUI tool for creating custom machine learning classification models without specialized technical expertise (Carney et al., 2020). Google Teachable Machine uses TensorFlow.js, to train and run the users’ models online. It runs within the browser entirely on the user’s device, maintaining any input data locally, protecting data privacy. Google Teachable Machine provides an intuitive workflow-oriented interface supporting the upload of the dataset, model training and evaluation, as well as the prediction of the classes of new data and the export of the trained model (Figure 2). Figure 2. Example of ML model development with Google Teachable Machine Furthermore, it is possible to download a trained model as a TensorFlow.js model and host it on Google Cloud, so it can be deployed easily into any website or app. In this case Google Teachable Machine generates a URL where the model is hosted for free and this link can be shared to use the created model. It can also be converted into a TensorFlow or TensorFlow Lite model and downloaded for local use (Figure 3). Figure 3. Example of exporting trained model as TensorFlow.js is important Metadata, a JSON file indicating the versions of the used libraries, metadata on the user and model Model, a JSON file specifying the model topology Weights, a BIN file specifying the weights of the trained model The exported Tensorflow.js is a zip file including: ● name, as well as a list of the label names and the size of the images used to train the model ● ● The exported trained ML model can then be deployed in software systems, such as mobile applications. The deployment of the trained model to illustrate the usefulness of ML, not only teaching the development of ML models, but also the creating of “intelligent” solutions. Such a deployment as part of IA/ML education in K-12 is typically done within block-based programming environments such as App Inventor (Gresse von Wangenheim et al., 2021). MIT App Inventor (appinventor.mit.edu) is a free web platform that allows users to create mobile applications. Users can design their own applications using drag and drop components and program its behavior using a blocks-based programming language. The App Inventor core already provides a comprehensive set of components, methods and commands for diverse kinds of functionality, including sensors, communication, data storage, etc. It is also possible to further extend App Inventor by providing more components (Patton et al., 2019). Extensions can also be used to incorporate ML features into App Inventor using the App Inventor extension framework (http://ai2.appinventor.mit.edu/reference/other/extensions.html). An example of such an App Inventor extension for integrating custom-trained image classification models is the Personal Image Classifier (PIC) extension (Tang et al., 2019)(Tang, 2019). Support provided by PIC consists of a web application supporting the model development and of the extension with new components for running the trained model in App Inventor apps. However, certain shortcomings regarding the specific web application for the development of the model, such as a lack of evaluation support in V2.0, performance problems of the trained models, and a lack of flexibility allowing the deployment of ML models developed on other environments such as Teachable Machine, indicate a need for further extensions. TMIC Teachable Machine - Image Classifier Extension TMIC is an App Inventor extension for the deployment of image classification models developed in Google Teachable Machine. The extension is based on the PIC extension (Tang et al., 2019)(Tang, 2019), adapting the PIC extension in order to enable the import of TensorFlow.js models created with Google Teachable Machine and exported by uploading them on Google Cloud. The TMIC extension includes the following properties: URL_Model is the property responsible for containing the URL of the model trained with the Google Teachable Machine that has been exported as Tensorflow.js on Google Cloud. WebViewer is the property that allows the user to assign a Web Browser component so that the extension can be used. The Web Browser component is used in order to visualize the functionality of the extension. The TMIC extension provides the following blocks: Blocks of the TMIC extension Functionality The ClassifierReady event block is executed when the extension finishes loading the ML model from the GTM cloud. The GotClassification event block is executed when the extension finishes classifying an image. This event the occurs the of ClassifyVideoData predictions for each category in the model. execution returning of list block, right after the The ClassifyVideoData block starts the classification of the image captured by the smartphone’s rear-facing camera video stream, using the WebViewer component. When the classification is finished, the result is returned via the GotClassification event block. The StopWebcam block stops the webcam when leaving the screen in which the image classification is done. The URL_Model adjustment block allows the user to adjust the ML model URL to another link of the exported GTM model in Google Cloud. TeachableMachineImageClassifier The returns a specific instance of the extension. get block It in which the first is divided into backend and frontend, The TMIC extension was developed using as a base the PIC extension code inside of the App Inventor framework refers to the for creating extensions. TeachableMachineImageClassifier.java file, which initializes the extension and its blocks. Each extension block is defined in the TeachableMachineImageClassifier.java class as a method responsible for executing the action of this block. The front end of the TMIC extension is made up of an HTML file along with four other Javascript files. The extension needs to be rendered in an HTML page to open the camera (asking the user for permission), and display it to the user. Javascript files are needed to load the model and perform the classification of the image when the user requests it. The teachable_machine_image_classifier.js file is mainly responsible for performing the task of communicating with the backend, receiving requests for loading the model, opening the cell phone camera and classifying the image. This file also notifies the backend when the extension is ready and returns the ranking results. The other Javascript files refer to the GTM and TensorFlow.js frameworks, which work together to perform image classification on the mobile device. Its functions are called within the teachable_machine_image_classifier.js code and internally between the files. In total, the TMIC extension is made up of six files, one of the Java extension, one of the HTML and four Javascript extension files. The main methods of the Java class (TeachableMachineImageClassifier.java) are those that will define the behavior of the blocks. Most of them need to communicate with the front-end, calling functions and passing parameters to the file teachable_machine_image_classifier.js, which will process the submitted request. The teachable_machine_image_classifier.js Javascript file functions are responsible for receive requests from the TeachableMachineImageClassifier.java class, process them and return the result in cases where it is necessary to notify that the extension is ready for the usage or prediction results are ready. The classifyVideoData() function is responsible for loading the Teachable Machine cloud hosted template from a user-defined URL in the property URL_Model. The classifyVideoData() function captures the URL of the two JSON files, one containing the model trained and the other with the model metadata, from the URL defined earlier in the preparation of the extension when it is initialized. After assigning the URL of the JSON files to two variables, they are passed as parameters to the function tmImage.load(modelURL, metadataURL), belonging to the Teachable Machine framework file teachablemachine-image.min.js, which will return the model prepared to be used in image classification. After the model is prepared, the model.getTotalClasses() function is called, which returns the number of model classes for the maxPredictions variable. Finally, the function is called model.predict(webcamPredict.canvas), which calls the classifier passing as a parameter the image captured at the exact moment the classifyVideoData() function was executed, returning the prediction result in the prediction variable. Finally, after the prediction variable is already with the classification result, an array with the results is filled in and returned to the Java class, calling the method reportResult(String result), which will prepare the result and notify the event block GotClassification. The TMIC extension is provided with the BSD 3 license (https://opensource.org/licenses/BSD-3-Clause), included in a LICENSE file. The license is available from the TMIC source code in the GitLab repository hosted at the Federal University of Santa Catarina. Also along with the code sources, a NOTICE file is available recognizing that the TMIC was developed by adjusting the PIC code. Currently the TMIC extension supports only Tensorflow.js models exported to Google Cloud and only allows capturing images with the rear-facing camera of the smartphone. We are planning to improve the extension as part of future work. Usage example The extension can be used for teaching ML in K-12, in introductory courses in higher education or by anyone who wants to create “intelligent” apps for image classification. As with any App Inventor extension it can be imported into App Inventor and then used in order to run trained models as part of intelligent apps. In order to support its usage the following material is available (currently in Brazilian Portuguese only): ● TMIC extension .aix ● Example app for the classification of recycling trash .aia (wireframe and final version) ● Online tutorial explaining the use of the extension The extension and material is available online: https://computacaonaescola.ufsc.br/en/tmic/ Acknowledgments This work is supported by CNPq (National Council for Scientific and Technological Development), a Brazilian government entity focused on scientific and technological development. References Armeshi, S. et al. Software Engineering for Machine Learning: A Case Study. In: Proc. of the 41st International Conference on Software Engineering: Software Engineering in Practice, IEEE Press, 2019, 291–300. Carney, M. et al. Teachable Machine: Approachable Web-Based Tool for Exploring Machine Learning Classification. In: Proc. of Conference on Human Factors in Computing Systems, ACM, 2020. C. Gresse von Wangenheim, C. Gresse von Wangenheim. Overview on a human-centric interactive ML process for teaching ML in K-12. Working Paper WP_GQS_01_2021_v10, GQS/INCoD/UFSC, 2021. Gresse von Wangenheim, C.; Hauck, J. C. R.; Pacheco, F. S.; Bertonceli Bueno, M. F. Visual Tools for Teaching Machine Learning in K-12: A Ten-Year Systematic Mapping. Education and Information Technologies, 2021. Marques, L. S., Gresse von Wangenheim, C., Hauck, J. C. R. Teaching Machine Learning in School: A Systematic Mapping of the State of the Art. Informatics in Education, 19(2), 2020. Patton E.W., Tissenbaum M., Harunani F. MIT App Inventor: Objectives, Design, and Development. In: Kong SC., Abelson H. (eds) Computational Thinking Education. Springer, Singapore, 2019. Tang, D., Utsumi, Y., Lao, N. (2019). PIC: A Personal Image Classification Webtool for High School Students. In: Proc.of the IJCAI EduAI Workshop, Macao, China, 2019. Tang, D. (2019). Empowering Novices to Understand and Use Machine Learning With Personalized Image Classification Models, Intuitive Analysis Tools, and MIT App Inventor, M.Eng thesis, MIT, Cambridge, USA. Touretzky, D., Gardner-McCune, C., Martin, F., Seehorn, D. Envisioning AI for k-12: What should every child know about AI? In Proc. of AAAI Conference on Artificial Intelligence, Honolulu, HI, USA, 2019, 9795–9799.",1.0
"ExpoCloud: a Framework for Time and Budget-Effective Parameter Space
  Explorations Using a Cloud Compute Engine","[{'href': 'http://arxiv.org/abs/2208.12195v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2208.12195v2', 'rel': 'related', 'type': 'application/pdf'}]",2022-08-25 16:32:44,,"2 2 0 2 g u A 5 2 ] R C . s c [ 1 v 0 7 3 2 1 . 8 0 2 2 : v i X r a COOKIEGRAPH: Measuring and Countering First-Party Tracking Cookies Shaoor Munir UC Davis smunir@ucdavis.edu Steven Englehardt Independent Researcher se@senglehardt.com Sandra Siby EPFL sandra.siby@epﬂ.ch Umar Iqbal University of Washington umar@cs.washington.edu Zubair Shaﬁq UC Davis zubair@ucdavis.edu Carmela Troncoso EPFL carmela.troncoso@epﬂ.ch Abstract—Recent privacy protections by browser vendors aim to limit the abuse of third-party cookies for cross-site tracking. While these countermeasures against third-party cookies are widely welcome, there are concerns that they will result in advertisers and trackers abusing ﬁrst-party cookies instead. We provide the ﬁrst empirical evidence of how ﬁrst-party cookies are abused by advertisers and trackers by conducting a differential measurement study on 10K websites with third- party cookies allowed and blocked. We ﬁnd that advertisers and trackers implement cross-site tracking despite third-party cookie blocking by storing identiﬁers, based on probabilistic and deterministic attributes, in ﬁrst-party cookies. As opposed to third-party cookies, outright ﬁrst-party cookie blocking is not practical because it would result in major breakage of legitimate website functionality. We propose COOKIEGRAPH, a machine learning approach that can accurately and robustly detect ﬁrst-party tracking cookies. COOKIEGRAPH detects ﬁrst-party tracking cook- ies with 91.06% accuracy, outperforming the state-of-the-art CookieBlock approach by 10.28%. We show that COOKIE- GRAPH is fully robust against cookie name manipulation while CookieBlock’s accuracy drops by 15.68%. We also show that COOKIEGRAPH does not cause any major breakage while CookieBlock causes major breakage on 8% of the websites with SSO logins. Our deployment of COOKIEGRAPH shows that ﬁrst-party tracking cookies are used on 93.43% of the 10K websites. We also ﬁnd that the most prevalent ﬁrst-party tracking cookies are set by major advertising entities such as Google as well as many specialized entities such as Criteo. 1. Introduction Browser vendors and trackers are engaged in an arms race. As soon as browser vendors deploy privacy protec- tions, e.g., third-party cookie blocking [85], [30], trackers quickly adapt to evade them, e.g., CNAME cloaking [50], bounce tracking [94] etc. In response, browser vendors have developed targeted countermeasures against such evasions [92], [81]. To gain advantage over browser vendors, trackers have started exploiting browser features that are typically used for functional purposes, and thus cannot be trivially blocked. i.e., The abuse of JavaScript APIs to build identiﬁers, browser ﬁngerprinting [39], and the abuse of ﬁrst-party context to store tracking cookies [75] stand out as two prominent techniques. Browser vendors have largely strug- gled against these tracking techniques because preventing them requires compromising functionality [58], [85]. While these new tracking techniques are difﬁcult to counter, they do not offer the same ﬂexibility as third-party cookies for cross-site tracking. Browser ﬁngerprints enable cross-site tracking but are not stable over time [83]. On the other hand, ﬁrst-party cookies are stable but not linkable across different sites. When combined, however, browser ﬁngerprints and ﬁrst-party cookies complement each oth- ers’ shortcomings and enable reliable cross-site tracking. Speciﬁcally, trackers are able to leverage non-deterministic ﬁngerprints in the ﬁrst-party context to set deterministic ﬁrst-party tracking cookies [57]. Prior literature has shown how ﬁrst-party cookies set by third party scripts are exﬁltrated to tracking endpoints [75], [44], [56] and how trackers use browser ﬁngerprinting to respawn ﬁrst-party cookies [57]. While prior work has demonstrated that ﬁrst-party cookies are indeed abused by advertisers and trackers, no countermeasure has been pro- posed to speciﬁcally block ﬁrst-party tracking cookies. In this paper, we investigate how ﬁrst-party cookies are abused for cross-site tracking and use our ﬁndings to develop a machine learning based countermeasure, COOKIEGRAPH, to block ﬁrst-party tracking cookies. To this end, we ﬁrst perform a differential measure- ment study where we compare the ﬁrst- and third-party cookie usage from two crawls of 10K websites when third- party cookies are enabled and blocked. We show that third- party-cookie blocking does not signiﬁcantly impact sharing of identiﬁers to tracking endpoints because trackers use (or reactively shift to) ﬁrst-party cookies when third-party cookies are blocked. Speciﬁcally, we ﬁnd entities such as Criteo, Lotame, and ID5 that show an increased presence and reactively move to ﬁrst-party cookies when third-party cookies are blocked. Our further analysis reveals that they store identiﬁers in ﬁrst-party cookies, based on probabilistic and deterministic attributes, which can be then used for cross-site tracking. Unlike third-party cookies, blocking all ﬁrst-party cook- ies is not practical because it would lead to major breakage of legitimate website functionality. Privacy-enhancing con- tent blocking tools, which use crowdsourced ﬁlter lists or machine learning [65], [78], [77], could be an alternative since blocking requests would also block all cookies set by the requests (or the requested scripts). However, as we also ﬁnd in our evaluation, blocking requests would also lead to breakage since it is likely that many of the blocked cookies are needed for legitimate website functionality. Re- searchers have recently started to develop approaches to detect and block tracking cookies (both ﬁrst and third-party) [63], [41]. However, these approaches rely on content-based features such as cookie names and values, which can lead to high number of false positives (and consequently higher major website breakage) while also being susceptible to evasion [77]. Keeping these limitations in mind, we design and im- plement COOKIEGRAPH, a machine learning approach to detect ﬁrst-party tracking cookies. Instead of using content- based features, COOKIEGRAPH captures fundamental track- ing behaviors exhibited by ﬁrst-party cookies that we discover in our differential measurement study. COOKIE- GRAPH is able to detect ﬁrst-party tracking cookies with 91.06% accuracy, outperforming the state-of-the-art Cook- ieBlock [41] approach by 10.28%. We also show that COOKIEGRAPH does not cause any major website breakage, where CookieBlock causes major breakage on 8% of the websites with SSO logins. Moreover, COOKIEGRAPH is robust to evasion through cookie name manipulation while CookieBlock’s accuracy degrades by 15.68%. Our deployment of COOKIEGRAPH on 10K websites shows that ﬁrst-party tracking cookies are used on 93.43% of the websites. While ﬁrst-party tracking cookies are set by third-party scripts served from a total of 1,588 unique domains, we show that the most prevalent ﬁrst-party tracking cookies are set by major advertising entities such as Google, as well as many specialized entities such as Criteo. We also show that 41.45% of all ﬁrst-party tracking cookies are set by scripts served by domains involved in ﬁngerprinting. In summary, our key contributions are as follows: 1) We conduct a large-scale differential measure- ment study to understand the effectiveness of third- party cookie blocking and whether ﬁrst-party cook- ies are used in lieu of third-party cookies. 2) We design and implement COOKIEGRAPH, a ma- chine learning based countermeasure to detect and block ﬁrst-party tracking cookies. COOKIE- GRAPH captures fundamental tracking behaviors of ﬁrst-party cookies that we discovered in our mea- surement study, and outperforms the state-of-the- art in terms of accuracy, robustness, and breakage minimization. 3) We deploy COOKIEGRAPH on 10K websites sampled from the Alexa’s top-100K list to measure the prevalence of ﬁrst-party tracking cookies. We detect a total of 1,588 distinct domains that set ﬁrst- party tracking cookies, including major advertising entities such as Google, and show that 45 (2.83%) of these domains are known ﬁngerprinters which set 41.45% of all ﬁrst-party tracking cookies. Paper Organization: The rest of this paper is organized as follows: Section 2 provides an overview of the recent developments in third-party and ﬁrst-party cookie based tracking and countermeasures. Section 3 evaluates effec- tiveness of third-party cookie blocking in reducing tracking activity and measures the extent of ﬁrst-party cookie abuse by advertisers and trackers. Section 4 describes the design and evaluation of COOKIEGRAPH. We discuss limitations of COOKIEGRAPH in Section 5 and conclude in Section 6. 2. Background & Related Work 2.1. Adoption of third-party cookies for tracking While cookies were originally designed to recognize returning users, e.g., to maintain virtual shopping carts [70], they were quickly adopted by third-parties to track users across websites, e.g., to serve targeted ads [27]. Early stan- dardization efforts mostly focused on limiting unintended cookie sharing across domains [47] and, despite well-known privacy concerns [21], largely ignored the intentional misuse of cookies by third-parties for cross-site tracking. Over the years, the use of third-party cookies for cross-site tracking has become increasingly prevalent [74], [43], [75], [48]. Prior research has found that the vast majority of third- party cookies are set by advertising and tracking services [48] and that the third-party cookies outnumber ﬁrst-party cookies by a factor of two [43] and up to four when they contain identiﬁers [75]. 2.2. Countermeasures against third-party cookies 2.2.1. Safari. Since its inception in 2003, Safari has blocked third-party cookies from domains that have not been visited by the user as full-ﬂedged websites [85]. To strengthen its cookie blocking, Safari introduced Intelligent Tracking Prevention (ITP) in 2017. ITP used machine learning to automatically detect third-party trackers and revoked storage access from classiﬁed domains if users did not interact with them on a daily basis (i.e., a 24 hour period) [86]. Since 2017, ITP went through several iterations, i.e., ITP 1.1 [87], ITP 2.0 [88], ITP 2.1 [89], ITP 2.2 [90] and ITP 2.3 [91], eventually leading to full third-party cookie blocking [93]. 2.2.2. Firefox. Firefox experimented with third-party cookie blocking in 2013 [52], [53], but did not ship default- on third-party cookie blocking until the release of Enhanced Tracking Protection (ETP) in 2018 [71]. ETP blocks third- party cookies based on a blocklist of trackers provided by 2 Disconnect [26]. As of 2022, Firefox has launched Total Cookie Protection (TPC) which partitions all third-party cookie access [6]. Partitioning ensures that cookies set by a third-party on one site are distinct from those set by the same third-party on other websites, eliminating the third- party’s ability to track users across those websites. 2.2.3. Internet Explorer and Microsoft Edge. Amongst the mainstream browsers that have deployed countermea- third-party cookies, Internet Explorer (IE) sures against and Microsoft Edge have the most permissive protections. IE blocked third-party cookies from domains that did not specify their cookie usage policy with P3P response header [22]. However, website owners often misrepresented their cookie usage polices, which rendered P3P ineffective [69]. Since 2019, Microsoft Edge blocks access to cookies and storage in a third-party context from some trackers, based on Disconnect’s tracking protection list [82], [35], [26]. 2.2.4. Chrome. Google Chrome is the only mainstream browser that does not restrict third-party cookies in any way in its default mode. In 2020, Google announced plans to phase out third-party cookies in Chrome by 2022 [76]. However, the plan has been postponed several times and the latest timeline suggests the phasing out of cookies by late 2024 [59]. Google has also announced plans to implement privacy-preserving versions of advertising use cases that cur- rently depend on third-party cookies—including behavioral ad targeting and ad attribution/measurement [59]. 2.3. Adoption of ﬁrst-party cookies for tracking While third-party cookies are widely considered as the main mechanism for cross-site tracking, trackers have also relied on ﬁrst-party cookies for tracking. As early as 2012, Roesner et al. [74], noted that third-party tracking scripts, embedded on the main webpage (i.e., in ﬁrst-party context), set ﬁrst-party cookies. More recently, in 2020 Fouad et al. [56] found that trackers sync ﬁrst-party cookies to several third-parties on as many as 67.96% of the websites. In 2021, Chen et al. [44] found that more than 90% of the websites contain at least one ﬁrst-party cookie that is set by a third-party script. Similar to Fouad et al., they also found that at least one ﬁrst-party cookie is exﬁltrated to a third-party domain on more than half of the tested web- sites, raising concerns that these cookies might be used for tracking. These concerns were also echoed by Sanchez et al. [75], who uncovered several instances where different third-parties interacted with the same ﬁrst-party cookies. They conclude, through a large scale measurement study of top websites and a number of case studies, that even after blocking third-party cookies, users are still at risk of tracking through ﬁrst-party cookies. While prior studies have identiﬁed the use of ﬁrst-party cookies by trackers, they were not solely focused on study- ing ﬁrst-party tracking cookies. In fact, their measurement infrastructure was not designed to capture tracking through they did not conﬁgure ﬁrst-party cookies. For example, their browsers to block third-party cookies, which might not instigate trackers to use ﬁrst-party cookies for tracking. Techniques used to set ﬁrst-party cookies. It is non- trivial to generate ﬁrst-party identiﬁers that are accessible across websites. Prior research has found that trackers often leverage browser ﬁngerprinting to generate ﬁrst-party track- ing cookies [57]. Browser ﬁngerprinting provides unique identiﬁers that are accessible across websites but drift over time [67]. However, identiﬁers generated through browser ﬁngerprinting can be stored in cookies that persist even after ﬁngerprints change. In addition to browser ﬁngerprinting, several advertising and tracking services, such as Google Ad Manager [79], [1] and ID5 [10], specify in their docu- mentation that they also use publisher provided identiﬁers (PPIDs), such as email addresses, to set ﬁrst-party cookies. CNAME cloaking also allows advertisers or trackers to use ﬁrst-party cookies. In this paper, we do not focus on CNAME cloaking because ﬁrst-party cookie leaks due to CNAME cloaking is already extensively studied by prior work [49], [50]. 2.4. Countermeasures against ﬁrst-party cookies 2.4.1. Deployed countermeasures. Safari is the only main- stream browser that has deployed protections against ﬁrst- party tracking cookies. Safari’s ITP expires ﬁrst-party cook- ies and storage set by scripts in 7 days if users do not interact with the website [85]. For ﬁrst-party cookies, this limit is lowered to 24 hours if ITP detects link decoration being used for tracking [85]. However, ﬁrst-party cookie tracking does not require link decoration to be effective. In cases where link decoration isn’t used, trackers can still track users within the 7-day window and beyond if users interact with the website within the 7-day window. 2.4.2. Countermeasures proposed by prior research. Recently, researchers have proposed machine learning based approaches to detect ﬁrst-party and third-party tracking cookies. Hu et al. [63] developed a machine learning based approach that uses sub-strings in cookie names (e.g., track, GDPR) as features to detect ﬁrst-party and third-party track- ing cookies. Bollinger et al. [41] also developed a machine learning approach, CookieBlock, that uses several cookie attributes such as the domain name of the setter, cookie name, path, value, expiration, etc, as features to detect ﬁrst-party and third-party tracking cookies. However, rely- ing on hard-coded content features make these approaches susceptible to adversarial evasions (as we show later in Section 4.4.3). Moreover, these approaches mainly rely on self-disclosed cookie labels as ground truth which are known to be unreliable [84]. 2.4.3. Request blocking approaches. Request blocking through browser extensions, such as Adblock Plus [23], and machine learning based tracker detection approaches proposed by prior research, e.g., [77], can potentially block ﬁrst-party tracking cookies. However, request blocking is inherently prone to cause breakage (as we later show in 3 Section 4.4.3) because it blocks access to content or cookies that might be essential for website functionality. Focus of this paper. In conclusion, prior work has only incidentally measured the usage of ﬁrst-party tracking cook- ies and existing approaches to detect ﬁrst-party cookies are lacking. In this paper, we ﬁll this void by conducting a large- scale study to measure the prevalence of ﬁrst-party tracking cookies and develop an accurate and robust machine learn- ing approach, called COOKIEGRAPH, that is purpose-built to detect ﬁrst-party cookies. 3. Measurements In this section, we present a measurement study to understand the usage of ﬁrst-party cookies by advertising and tracking services (ATS) when third-party cookies are blocked. To this end, we conduct two web crawls (with and without third-party cookies) and analyze the differences in the tracking activity (i.e., sharing of identiﬁers to known adverting and tracking services) observed across these two crawls to understand the effectiveness of third-party cookie blocking and whether ﬁrst-party cookies are used in lieu of third-party cookies. 3.1. Data Collection and Methodology Data collection. We use OpenWPM [54] to crawl sites from Alexa’s top-100K list. To ensure that our crawls contain representative sites of different popularity, we crawl the top 1K sites, and randomly sample another 9K sites from the long tail of sites ranked 1K-100K. To ensure intra-page diversity (landing and internal pages [38]) we perform an interactive crawl. Speciﬁcally, for each site, we crawl its landing page, and then sample 5-10 anchor tags in this landing page uniformly at random, and crawl them to get a sample of internal pages. We conduct two crawls: one with third-party cookies enabled (3P-Allowed), and one with third-party cookies blocked (3P-Blocked). We conduct these crawls simultaneously to minimize temporal variations in sites across the two crawls1. Deﬁnition of ﬁrst- and third-party cookies. Cookies are set in the browser in two ways. They can either be set by the Set-Cookie HTTP response header or by using docu- ment.cookie() in JavaScript. Cookies are further classiﬁed as ﬁrst- or third-party. Cookies set via response header from the same (or different) domain as the ﬁrst-party are ﬁrst-party (or third-party) cookies. Classiﬁcation of cookies set by a script depends on whether the script is embedded in a ﬁrst- or third-party execution context. The cookies set by third- party scripts running in a ﬁrst-party context are ﬁrst-party cookies. The cookies set by third-party scripts running in a third-party context (e.g., third-party iframes) are third-party cookies. Labeling tracking activity. We use EasyList [28] and EasyPrivacy [29] to label requests as tracking (ATS) or not Figure 1. Average number of requests per site in 3P-Allowed and 3P-Blocked conﬁgurations: ) Non-ATS requests ( ) ATS requests without identiﬁers ( ) ATS requests with identiﬁers ( tracking (Non-ATS).2 Since the basic premise of tracking is to identify users, we are particularly interested in sharing of identiﬁers in these tracking requests. To this end, in line with prior work [66], [55], we deﬁne identiﬁers as a string that is longer than 8 characters and matches the regex [a − zA − Z0 − 9 = −]. Using this deﬁnition, we look for identiﬁers in URL query parameters [73] and cookie values [75], [51], [44], [43]. 3.2. Tracking after Blocking Third-Party Cookies We ﬁrst study whether blocking third-party cookies ef- fectively eliminates ATS requests. To this end, we compare the number of requests with and without third-party cookies. Figure 1 plots the number of requests with and without third-party cookies. It can be seen from the Figure 1 that when third-party cookies are blocked, there is only a modest reduction in the overall number of ATS requests, with just an 18.4% reduction in the number of ATS requests containing identiﬁers. This is surprising because cookie syncing, which is widely used for cross-site tracking [56], [72], entails sharing third-party identiﬁer cookies in query parameters [51], [44], [43]. With third-party cookies blocked, cookie syncing between third-parties cannot occur and we would expect to see a larger drop in identiﬁers shared in ATS requests. We address this surprising observation in Section 3.3. Next, we analyze whether third-party cookie blocking disparately impacts different ATS domains (eTLD+1). Fig- ure 2 plots the percentage of sites with at least one ATS request with identiﬁers for the top-10 most prevalent ATS domains across both crawls. We note that six of the top-10 ATS domains, all owned by Google, show only a negligible 1. The success rate of our crawl is 83.98%. Form the 10K sites visited, 8,398 were successfully crawled. 2. We label a request as tracking (ATS) if its URL matches the rules in either one of the lists. Otherwise, we label it as not tracking (Non-ATS). 4 s t s e u q e R f o r e b m u N e g a r e v A 300 250 200 150 100 50 0 18.4% Decrease 8.18% Decrease 1.59% Decrease 3P Cookies Enabled 3P Cookies Blocked Figure 2. Presence of top-10 tracking domains. The plot shows percentage of sites where at least one request is sent to a tracking domain. We include criteo.net due to its peculiar increased presence after blocking third- party cookies. ( ( ) third-party cookies allowed ) third-party cookies blocked Figure 4. Comparison of percentage of sites on which ﬁrst party and third- party identiﬁer cookies are set by ATS domains. ( ( ( ) ﬁrst-party cookies set when third-party cookies are allowed ) ﬁrst-party cookies set when third-party cookies are blocked ) third-party cookies set when third-party cookies are allowed 3.3. Tracking through First-Party Cookies Figure 1 showed that 82.6% of ATS requests contain identiﬁers even after third-party cookies are blocked. It is clear that the identiﬁers in these ATS requests are then likely originating from some storage mechanism other than third- party cookies. Since recent prior work has shown that ATSes are increasingly using ﬁrst-party cookies [75], [44], we next investigate whether ﬁrst-party cookies are being used in lieu of third-party cookies. We ﬁrst compare the average number of ﬁrst-party cook- ies in 3P-Allowed and 3P-Blocked crawls in Figure 3. We observe only a minor difference in the average number of ﬁrst-party cookies set by ATS scripts (or Non-ATS for that matter).3 However, it is noteworthy that 81% of the ﬁrst-party cookies are set by ATS scripts and further 82% of them are identiﬁer cookies. This demonstrates that an overwhelming number of ﬁrst-party cookies are in fact set by ATSes. Next, we compare the setting of ﬁrst- and third-party identiﬁer cookies by ATS domains (eTLD+1 of the setting script URL) to understand if ﬁrst-party cookie usage is equally prevalent across different ATSes. Figure 4 plots the percentage of sites where at least one ﬁrst-party and/or third-party identiﬁer cookie is set by top-10 ATS domains (with Criteo divided into criteo.com and criteo.net). First, we observe that for the six Google-owned ATS domains, which showed negligible difference in requests containing identi- ﬁers after blocking third-party cookies, there is also little to no change in use of ﬁrst-party identiﬁer cookies across 3. We label scripts as ATS or Non-ATS based on their src URL as in Section 3.1. Figure 3. Breakdown of average number of ﬁrst-party cookies per site set before and after blocking third-party cookies. ( ( ( ) Cookies set by non-tracking sources ) Non-identiﬁer cookies set by tracking sources ) Identiﬁer cookies set by tracking sources. reduction in the number ATS requests with identiﬁers when third-party cookies are blocked. In contrast, three other ATS domains, owned by Pubmatic, Rubicon, and OpenX, show an almost 50% reduction. Criteo exhibits interesting behav- ior, where the requests sent to Criteo are divided between two domains: criteo.com and criteo.net. While criteo.com shows negligible change across two crawls, criteo.net in- creases by about one-third. We attempt to better understand the reason behind this disparate impact across different ATS domains. 5 70 60 50 40 30 20 10 s e t i S f o % 0 google.co m pub m atic.co m doubleclick.net googlesyndication.co m rubiconproject.co m googletag m anager.co m google-analytics.co m googleadservices.co m openx.net criteo.co m criteo.net s e i k o o C P 1 f o r e b m u N e g a r e v A 50 40 30 20 10 0 0.44% Increase 1.89% Decrease 1.15% Increase 3P Cookies Enabled 3P Cookies Blocked s e t i S f o % 70 60 50 40 30 20 10 0 google.co m pub m atic.co m doubleclick.net googlesyndication.co m rubiconproject.co m googletag m anager.co m google-analytics.co m googleadservices.co m openx.net criteo.co m criteo.net Table 1. COOKIES SHOWING THE HIGHEST RATIO OF NEW APPEARANCES AFTER BLOCKING THIRD-PARTY COOKIES. Cookie Name cto bundle id5id pbjs-uniﬁedid pbjs-id5id s sq stripe mid stripe sid panoramaId cc id Script Domain New Appearances Total Appearances Ratio criteo.com pubmatic.com pubmatic.com pubmatic.com adobedtm.com stripe.com stripe.com crwdcntrl.net crwdcntrl.net 132 25 17 24 17 13 13 24 24 632 139 103 164 122 95 95 184 196 0.21 0.18 0.16 0.15 0.14 0.14 0.14 0.13 0.12 are blocked. We can quantify this shift as a ratio be- tween the number sites where ﬁrst-party cookie is present in 3P-Allowed crawl and number of new sites where ﬁrst-party cookie is present in 3P-Blocked crawl. Ta- ble 1 shows top-10 identiﬁer cookies based on this ra- is dominated by three well- tio. We note that known ad-tech organizations: Criteo (cto_bundle), ID5 (id5id, pbjs-unifiedid, pbjs-id5id), and Lotame ((panoramaID, _cc_id). We further investigate the be- havior of these cookies using their publicly available docu- mentation [14], [13], [9], [32], [4] in Appendix A. the list 3.4. Cross-site Tracking via First-party Cookies Our analysis of Criteo, Lotame, and ID5 in Appendix A reveals a common approach to using ﬁrst-party cookies for cross-site tracking. They build an “identity graph” to Figure 6. The ﬁgure shows the ﬂow of information and identiﬁers through an identity Graph for cross-site attribution. Initially, the user visits sites 1, 2, and 3. Trackers on sites 1, 2, and 3 collect and send ﬁngerprints F 1, F 2, and F 3 to their identity graph. The identity graph returns a U ID−1 for all the site visits, using a probabilistic matching of ﬁngerprints F 1, F 2, and F 3 sent on each respective website. A publisher provided ID, P P ID − 1, is also sent alongside F 3 when visiting site 3. When the user visits site 4, it sends ﬁngerprint F 4. Because the ﬁngerprint F 4 is different from F 1, F 2, and F 3, the identity graph cannot create a probabilistic match with the other sites. On site 4, the website obtains and sends a publisher provided ID which matches P P ID − 1 provided on site 3. As a result, the identity graph matches and returns the existing user’s U ID − 1 for site 4 using deterministic matching. All of these IDs are stored in ﬁrst-party cookies on the user’s device. 6 Figure 5. Percentage of sites ﬁrst-party cookies show up on before and after blocking ﬁrst-party cookies. ( ( ) third-party cookies are allowed ) third-party cookies are blocked. both crawls. These domains do not set a large number of third-party identiﬁer cookies, which likely explains why they were not impacted by third-party cookie blocking. Second, the other set of ATS domains (i.e., Pubmatic, Rubicon, and OpenX) disproportionately use more third-party identiﬁer cookies than ﬁrst-party identiﬁer cookies. This observation explains the drastic drop in number of requests containing identiﬁers to these other ATS domains after blocking third- party cookies in Figure 2. to set cookies: Finally, we further investigate Criteo which showed a peculiar behavior in Figure 2. Recall that Criteo uses criteo.com and criteo.net the ﬁrst-party identiﬁer cookies set by the former showed an increase after blocking third-party cookies while the latter does not change. In addition to this, criteo.com is also used to set third-party identiﬁer cookies, while criteo.net sets only ﬁrst- party identiﬁer cookies. Both of these domains set the same cto_bundle cookie. To compare cto_bundle with identiﬁer cookies set by other ATSes, we plot the percentage of sites where a cookie with the same name appears. Figure 5 plots the prevalence of ﬁrst-party cookies for top-20 cookies. We note that while other cookies witness a slight drop in their prevalence after blocking third-party cookies, it does not hold true for cto_bundle. In fact, cto_bundle’s prevalence increased after blocking third- party cookies, in accordance with the unexpected increase in total number of ﬁrst-party identiﬁer cookies set by scripts belonging to criteo.com. The aforementioned increased use of ﬁrst-party cookies represents an interesting scenario where trackers are reac- tively shifting to ﬁrst-party cookies if third-party cookies 80 60 40 20 s e t i S f o % 0 ga gid gads fbp uetvid gclau uetsid bundle hjT L D Test P H PS E SSID cto cf utm a b m utm c uid d y m y m utm b utm z utm v O ptanonC onsent consentdata userid pbjs Site 1 Site 2 Site 3 Site 4 F1 UI D-1 F2 UI D-1 F3+PPI D-1 UI D-1 F4+PPI D-1 UI D-1 F1 F2 F3 + PPI D-1 F4 + PPI D-1 identiﬁers to a particular user using ﬁrst- link different party information collected from different sites. A node can represent a user (or device such as web browser) based on different attributes and edges between the nodes are formed based on “deterministic” or “probabilistic” match- ing between attributes of a pair of nodes. For cross-site tracking, they need to establish edges between different nodes (that actually represent the same user/device) of the identity graph. Note that trackers cannot simply use third- party identiﬁer cookies if they are blocked. Trackers typically use two types of information to build their identity graph. They gather information provided by publishers including both deterministic attributes (e.g., email, phone, username, or any other publisher-provided ID [PPID] that can be directly used for identiﬁcation) and probabilistic attributes (e.g., zip code, city, age, etc. that can be used together for non-deterministic identiﬁcation). They themselves typically also gather probabilistic information such as IP address and ﬁngerprinting attributes such as browser and operating system information (e.g., name and speciﬁc version), device properties (e.g., display resolution, screen orientation), etc. To link different nodes in their identity graph (e.g., to link the same user across different sites or to link different devices of the same user, an example showing how different user devices are linked is shown in Appendix B), they use probabilistic or deterministic matching as shown in more detail in Figure 6. In probabilistic matching, they measure the similarity between probabilistic attributes and determine a match if the similarity is reasonably high (represented as gray edges in Figure 6). In deterministic matching, they can exactly match deterministic attributes (represented as black edges in Figure 6). Once these links are established, trackers store an identiﬁer in a ﬁrst-party cookie which uniquely represents that user across different sites (or devices). 3.5. Takeaway Our differential measurement study reveals that blocking third-party cookies is insufﬁcient in preventing tracking; as there is a minimal decrease in the number of ATS requests sharing identiﬁers when third-party cookies are blocked. However, the impact of third-party cookie blocking is not uniform across different ATSes– some ATS domains such as google-analytics.com and doubleclick.net show no change in their tracking requests, while others such as pubmatic.com and rubiconproject.com show a decrease, and yet others such as criteo.net show an increase. We ﬁnd that ﬁrst-party cookies are predominantly used by ATSes in lieu of third- party cookies to perform tracking. Some ATS domains, such as those owned by Google, only use ﬁrst-party cookies and are hence not impacted by third-party cookie blocking. Some other ATS domains that do use third-party cookies reactively shift to using ﬁrst-party cookies when third-party cookies are blocked. We ﬁnd that these ATSes rely on a combination of deterministic and probabilistic attributes to build an identity graph. Then, they use ﬁrst-party cookies to store these identiﬁers that are used for cross-site tracking. Next, we present our approach to accurately and robustly detect these ﬁrst-party ATS cookies. 4. COOKIEGRAPH: Detecting Tracking Cookies First-Party In this section, we describe COOKIEGRAPH, a graph- based machine learning approach to detect ﬁrst-party ATS cookies. COOKIEGRAPH creates a graph representation of a webpage’s execution based on HTML, network, JavaScript, and storage information collected by an instru- mented browser, in which ﬁrst-party cookies are represented as storage nodes. COOKIEGRAPH extracts distinguishing features of these cookies and uses a random forest classiﬁer to detect ﬁrst-party ATS cookies. Figure 7 provides an overview of COOKIEGRAPH’s pipeline. 4.1. Design and Implementation Browser instrumentation. COOKIEGRAPH relies on our extended version of OpenWPM [54] to capture webpage ex- ecution information across HTML, network, JavaScript, and the storage4 layers of the web stack. Speciﬁcally, COOKIE- GRAPH captures HTML elements created by scripts, net- work requests sent by HTML elements (as they are parsed) and scripts, responses received by the browser, exﬁltra- tion/inﬁltration of identiﬁers in network requests/responses, and read/write operations on browser’s storage mechanisms. Graph construction. The nodes in COOKIEGRAPH’s graph represent HTML elements, network requests, scripts, and storage elements. When localStorage and ﬁrst-party cookie nodes share the exact same name, COOKIEGRAPH considers them as one storage node. The edges represent a wide range of interactions among different types of nodes e.g., scripts sending HTTP requests, scripts setting cookies etc. In addition to interactions considered by prior work [77], COOKIEGRAPH incorporates edges that capture the tracking behavior of ﬁrst-party cookies. Informed by our ﬁndings in Section 3: cookies are typically set with the values inﬁltrated with HTTP responses and are exﬁltrated via URL parame- ters and request headers or bodies; COOKIEGRAPH captures inﬁltrations and exﬁltrations by linking the script-read/write cookies in the ﬁrst-party execution context to the requests of reader/writer script that contains those cookie values. In addition to plain text cookie values, COOKIEGRAPH also monitors Base64-, MD5-, SHA-1-, and SHA-256- encoded cookie values in URLs, headers, request and response bod- ies. As in our measurement study, because of the focus on identiﬁers, COOKIEGRAPH only captures cookie values that are at least 8 characters long. We illustrate the difference between COOKIEGRAPH’s graph representation and prior work, i.e., WebGraph [77] 4. Our measurements in Section 3 found a signiﬁcant use of localStorage in addition to cookies. Thus, we use the term “storage” to refer to both cookies and localStorage. In most cases, the description for cookies is also applicable to localStorage and vice versa. 7 Figure 7. Overview of COOKIEGRAPH pipeline: (1) Webpage crawl using an instrumented browser; (2) Construction of a graph representation to represent the instrumented webpage execution information; (3) Feature extraction for graph nodes that represent ﬁrst-party cookies; and (4) Classiﬁer training to detect ﬁrst-party ATS cookies. (a) Graph representation of Code 1 in WebGraph (b) Graph representation of Code 1 in COOKIEGRAPH Figure 8. Graph representation of Code 1 in WebGraph and COOKIEGRAPH. represents storage nodes. Node numbers correspond to the lines in Code 1. In Figure 8(a), dashed (- - -) and dotted (. . .) lines represent the additional edges that are captured by COOKIEGRAPH and missed by WebGraph. The grey dashed line shows different representations of the same event by both systems. represents script nodes, and represents network nodes, using an example script that involves ﬁrst-party ATS cook- ies. Code 1 shows a third-party script from tracker1.com executing in a ﬁrst party context on a webpage. The script ﬁrst reads infoCookie, which stores tracking information such as the publisher ID and a user signature. Then, it sends the content of the cookie to an endpoint via an HTTP POST request. The endpoint returns a user ID (UID) in the response body, which is stored in both a ﬁrst-party cookie and localStorage named IDStore. At a later point, the script exﬁltrates UID to two other tracking endpoints: to tracker2.com via a URL parameter and to tracker3.com via an HTTP header. The HTTP requests and responses that result from Code 1 are listed in Listing 1. Figure 8 shows the differences between the graph rep- resentations of this script created by prior work, WebGraph (left), and COOKIEGRAPH (right). WebGraph does not cap- ture the inﬁltration of the UID to the cookie from the response body and also does not consider inﬁltration and ex- ﬁltration via localStorage. In contrast, the dotted and dashed lines in Figure 8(b) show that COOKIEGRAPH captures both the inﬁltration and the exﬁltration in subsequent network requests. Moreover, while WebGraph captures exﬁltrations via URL parameters (shown by grey dashed lines) via edges from the setting script to the endpoint, COOKIEGRAPH is able to precisely link this exﬁltration to the ﬁrst-party cookie via an edge from the cookie node to the endpoint. Feature extraction. We use COOKIEGRAPH’s representa- tion to extract structural and information ﬂow features. Structural features represent relationships between nodes in the graph, such as ancestry information and connectivity. These features capture the relationships between the ﬁrst- party cookie nodes and scripts on the page. For example, how many scripts interacted with a cookie or whether a script that interacted with a cookie also interacted with other cookies. Flow features represent ﬁrst-party ATS cookie behavior. We extract three types of ﬂow features. First, we count the number of times a cookie was read or written. Second, the number of times a cookie was inﬁltrated we count or exﬁltrated via the methods explained in the previous section. Third, we calculate some features with respect to the setter of the cookie. Concretely, whether the setter’s domain also acted as an end-point for other cookie exﬁltrations, and whether the setter’s domain was involved in redirect chains (since redirects are commonly used in tracking). The intuition behind the third category of features is that domains 8 Webpage cr aw l u si n g Open W PM i n st r u m en t ed Fi r ef ox br ow ser (t h i r d-par t y cook i es bl ock ed) Cook i e f eat u r e ex t r act i on an d l abel i n g u si n g f i l t er l i st s an d Cook i epedi a Pr ocess cr aw l dat a t o bu i l d a gr aph r epr esen t at i on of page ex ecu t i on Fi r st -par t y cook i e cl assf i i cat i on ATS Non -ATS Script from tracker1.com Storage accesses 2 2 4 Cookie 9, 13 Cookie 10 Local Storage 14 18 Request to tracker1.com/sync Request to tracker2.com Request to tracker3.com Script from tracker1.com Storage accesses 2 2 4 Cookie 9, 13 Cookie 10 Local Storage ID exfiltrations to other trackers ID infiltration in response body 14 18 Request to tracker1.com/sync Request to tracker2.com Request to tracker3.com Table 2. COOKIEGRAPH FEATURES COMPARISON WITH WEBGRAPH. INDICATES THAT A FEATURE IS PRESENT. INDICATES THAT FEATURE WAS EXTENDED IN COOKIEGRAPH. COOKIEGRAPH CALCULATES GRAPH SIZE, DEGREE AND CENTRALITY FEATURES USING BOTH NORMAL AND SHARED INFORMATION EDGES. THE FORMER COMES UNDER STRUCTURAL FEATURES WHILE THE LATTER COMES UNDER FLOW FEATURES. Feature Type COOKIEGRAPH WebGraph Graph size (# of nodes, # of edges, and nodes/edge ratio) Degree (in, out, in+out, and average degree connectivity) Centrality (closeness centrality, eccentricity) Ascendant’s attributes Descendant of a script Ascendant’s script properties Parent is an eval script Local storage access (# of sets, # of gets) Cookie access (# of sets, # of gets) Storage access on local storage with same name (# of sets, # of gets) Requests (sent, received) Redirects (sent, received, depth in chain) Common access to the same storage node Cookie exﬁltration Cookie inﬁltration Cookie Setter (# of exﬁltration, # redirects) Graph size (# of nodes, # of edges, and nodes/edge ratio) Degree (in, out, in+out, and average degree connectivity) Centrality (closeness centrality, eccentricity) Structure Structure Structure Structure Structure Structure Structure Flow Flow Flow Flow Flow Flow Flow Flow Flow Flow Flow Flow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 <html> --------------------------------------------------- <script src=’tracker1.com/track.js’> ... infoCookie = document.cookie; var idReq = new XMLHTTPRequest(); idReq.open(""POST"", ""tracker1.com/sync"", true) idReq.send(infoCookie) var response = newReq.response document.cookie = ""IDStore="" + response; localStorage.setItem(IDStore, response); ... var exfilReq1 = new XMLHTTPRequest(); idCookie = document.cookie exfilReq1.open(""GET"", ""tracker2.com? user_id="" + idCookie); ... var exfilReq2 = new XMLHTTPRequest(); exfilReq2.setRequestHeader(""ID-header"", idCookie); exfilReq2.open(""GET"", ""tracker3.com""); ... Request 1 URL: tracker1.com/sync POST data: publisherID=704; signature=xyz Response 1 Status: 200 Content: UID=abcd --------------------------------------------------- Request 2 URL: tracker2.com?user_id=abcd Response 2 Status: 200 --------------------------------------------------- Request 3 Header: ID-header = abcd URL: tracker3.com Response 3 Status: 200 </script> ... </html> Code 1. Script from third party tracker1.com executing in a ﬁrst party context. The script obtains a UID from a sync point, stores it, and exﬁltrates it to tracker2.com and tracker3.com. involved in setting ﬁrst-party ATS cookies are also involved in sharing information with other ATSes. Table 2 shows the differences in features between COOKIEGRAPH and WebGraph. COOKIEGRAPH adds im- proved cookie exﬁltration features and also introduces a new complete new set of inﬁltration and setter features. Unlike WebGraph, COOKIEGRAPH also considers cases where a localStorage shares the same name as a cookie (a behavior Listing 1. HTTP requests and responses initiated from Code 1. observed in ﬁrst-party ATS cookies). COOKIEGRAPH does not use content features, e.g., based on cookie name, as they can be trivially used in detection evasion tactics [77], [66]. COOKIEGRAPH also removes three features because they are related to classiﬁcation of request nodes in WebGraph whereas COOKIEGRAPH classiﬁes storage nodes. 4.2. Evaluation Similar to previous work [65], [77], we use a random forest classiﬁer to distinguish between ATS and Non-ATS cookies. We ﬁrst train and test the accuracy of this classiﬁer 9 on a carefully labeled dataset. Then, we deploy it on our 10K website dataset. 4.2.1. Ground truth labeling. We use two complementary approaches to construct our ground truth for ﬁrst-party ATS cookies. We represent each ﬁrst-party cookie as a cookie- domain pair, since the same cookie name can occur on multiple sites. Filter lists. We rely on ﬁlter lists [28], [29] as previous work has found them to be reasonably reliable in detecting ATS endpoints [65], [77]. However, ﬁlter lists are designed to label resource URLs, rather than cookies. We adapt ﬁlter lists to label cookies by assigning the label of a particular resource to all the cookies set by that resource. Since both ATS and Non-ATS cookies can be set by the same resource, this labeling procedure could result in a non-trivial number of false positives. To limit the number of false positives in our ground truth, we only label Non-ATS cookies based on ﬁlter lists: i.e., if a script that sets a cookie is not marked by any of the ﬁlter lists, we label these cookies as Non- ATS. Conservatively, if any one of the ﬁlter lists mark the cookie’s setter as ATS, we label the cookie as Unknown. Cookiepedia. Inspired by prior work [41], we use Cook- iepedia [34] as an additional source of cookie labels. Cook- iepedia is a database of cookies maintained by a well-known Consent Management Platform (CMP) called OneTrust [62], [42]. For each cookie and domain pair, Cookiepedia pro- vides its purpose, deﬁned primarily through the cookie integration with OneTrust. Each cookie is assigned one of four labels: strictly necessary, functional, analytics, and advertising/tracking. As Cookiepedia-reported purposes are self-declared, we adopt a conservative approach: we only label a cookie-domain pair as ATS if a cookie’s purpose is declared as advertising/tracking or analytics in a particular domain. If the cookie’s declared purpose is strictly necessary or functional, we label the cookie as Unknown, as the cookie might have been, mistakenly or intentionally, mislabeled. We combine the results of the labeling approaches to obtain a ﬁnal label for ﬁrst-party cookies. If both approaches label a cookie as Unknown, its ﬁnal label is Unknown. If only one of the approaches has a known label, this is the ﬁnal label. When Cookiepedia marks a cookie as ATS and ﬁlter lists mark it as Non-ATS, we give precedence to the Cookiepedia label and assign the ﬁnal label as ATS because websites are unlikely to self-declare their Non-ATS cookies as ATS. Using this labeling process, 20,927 out of 78,560 ﬁrst- party cookies (26.64%) have a known (ATS or Non-ATS) label and the rest are labeled as Unknown. We then observe that cookies set by the same script across two different sites are often labeled ATS in one instance and Unknown in other instance because Cookiepedia does not have data for the latter. As it is unlikely that an ATS script changes purpose across sites, we propagate the ATS label to all instances set by the same script. After this label propagation, 51.76% of the data is now labeled, with 21,875 (53.79%) ATS and 18,786 (46.20%) Non-ATS labels. 10 Figure 9. Feature distribution of cookie exﬁltrations (top) and storage sets (bottom) for ATS and Non-ATS cookies. ATS cookies are exﬁltrated and set more than Non-ATS cookies, resulting in ﬂow features based on exﬁltrations and sets being helpful for the classiﬁer. 4.2.2. Classiﬁcation. We train and test the classiﬁer on the labeled dataset using standard 10-fold cross validation. We ensure that there is no overlap in the websites used for training and test in each fold. Similar to Section 4.1, we limit the classiﬁer to cookies whose value is at least 8 characters long. The classiﬁer has 91.87% precision and 90.59% recall, with an overall accuracy of 91.06%, indicating that the classiﬁer is successful in detecting ATS cookies. Feature analysis. We conduct feature analysis to understand the most inﬂuential features for the classiﬁer. We ﬁnd that the most inﬂuential features are the ﬂow features, which capture cookie exﬁltrations, set operations, and redirections by cookie setters. Figure 9 shows the distributions for the number of cookie exﬁltrations (top) and the number of times a cookie is set (bottom), for ATS and Non-ATS cookies. ATS cookies are much more likely to be exﬁltrated than Non- ATS cookies: ATS have a median number of 6 exﬁltrations (mean/std is 11.11/15.95) as compared to a median of 0 for Non-ATS (mean/std is 0.62/5.29). Also, ATS cookies tend to be set much more frequently by scripts, with a median of 3 set operations (mean and standard deviation is 4.86±6.99) as compared to 1 for Non-ATS cookies (mean and standard deviation is 2.17±6.08). These ﬁndings conﬁrm our conclu- sions in Section 3: ﬁrst-party ATS cookies are used to store identiﬁers which are then exﬁltrated to multiple endpoints. the cookies Error analysis. We conduct manual analysis of COOKIE- GRAPH’s false positives and false negatives to understand why the approach fails. We ﬁnd that that were most mis- classiﬁed as ATS are those whose publicly available descriptions indicate they are used to track visitors on a page (e.g., __attentive_id, messagesUtk, omnisendAnonymousID) [24], [33], [31]. We also ﬁnd a few instances of well-known Google Analytics cookies _ga and _gid that are labeled in ground truth as Non-ATS, but are classiﬁed by COOKIEGRAPH as ATS. Overall, we ﬁnd that the false positives are typically not caused by COOK- IEGRAPH misclassifying non-tracking cookies, but mostly that the tracking cookies ﬂagged by COOKIEGRAPH were mislabeled as Non-ATS in the ground truth. In other words, COOKIEGRAPH has likely correctly classiﬁed these tracking cookies. We note that even after our procedures to improve ground truth labels, there may be cookies that did not have self-disclosed labels or were served from slightly different scripts (thereby missing our hash-based script matching) leading to some mistakes in the ground truth. We leave investigation of further methods of improving the ground truth labeling to future work. For false negatives, a representative case is the _pin_unauth cookie. Its value is double-base64-encoded, that is not included in the list of potential encoding schemes used by COOKIEGRAPH to detect exﬁltration. These false negatives can be averted by using a more comprehensive list of encoding schemes or by performing full-blown infor- mation ﬂow tracking instead of approximating exﬁltration ﬂows; however, the latter would come at a performance cost as we discuss further in Section 4.4. Other false negatives are because COOKIEGRAPH does not capture sufﬁcient activity during webpage execution. We further discuss these cases of false negatives in Section 5.1 Table 3. LIST OF TOP-25 ATS COOKIES DETECTED BY COOKIEGRAPH Cookie Name gid ga fbp gcl au gpi ga gads gads uetsid uetvid gpi clck hjTLDTest clsk cto bundle ym d ym uid pin unauth utma utmb utmz qca utmc ttp hubspotutk Script Domain Org. Percentage of Sites google-analytics.com google-analytics.com facebook.net googletagmanager.com googlesyndication.com googletagmanager.com googlesyndication.com doubleclick.net bing.com bing.com doubleclick.net clarity.ms hotjar.com clarity.ms criteo.net yandex.ru yandex.ru pinimg.com google-analytics.com google-analytics.com google-analytics.com quantserve.com google-analytics.com tiktok.com hs-analytics.net Google Google Facebook Google Google Google Google Google Microsoft Microsoft Google Microsoft Hotjar Microsoft Criteo Yandex Yandex Pinterest Google Google Google Quantcast Google TikTok HubSpot 77.11% 68.88% 33.22% 14.22% 14.02% 12.79% 12.35% 11.68% 10.22% 10.22% 10.11% 8.81% 8.05% 7.88% 5.98% 4.85% 4.85% 4.57% 4.32% 4.32% 4.32% 4.19% 4.17% 3.75% 3.29% 4.3. Deployment We deploy COOKIEGRAPH to classify all cookies, in- cluding Unknown cookies, in our crawl of 10K sites. Prevalence of ﬁrst-party ATS cookies. Overall, COOKIE- GRAPH classiﬁes 62.48% of the 74,003 ﬁrst-party cookies in our dataset as ATS. We ﬁnd that 93.43% of sites deploy at least one ﬁrst-party ATS cookie. Of these sites, the average number of ﬁrst-party ATS cookies on a site is 6.29. Who sets ﬁrst-party ATS cookies? The vast majority (98.39%) of the ﬁrst-party ATS cookies are in fact set by third-party embedded scripts served from a total of 1,588 unique domains. This demonstrates that ﬁrst-party ATS cookies are actually set and used by third-party trackers. Because this is only possible if the ﬁrst-party allows the third-party trackers to embed a script in ﬁrst-party con- text, this suggests that there is intentional or unintentional collusion between the ﬁrst-party and third-party tracker. These third-party-set ﬁrst-party cookies enable third-parties to circumvent blocking-based countermeasures implemented by browsers. Next, we analyze the most prevalent ﬁrst-party cookies and the third-party entities that actually set them. Table 3 lists top-25 out of 5,019 ﬁrst-party ATS cookies5 based on their prevalence. Two major advertising entities (Google and Facebook) set ﬁrst-party ATS cookies on approximately a third of all sites in our dataset. COOKIEGRAPH detects _gid and _ga cookies by Google Analytics as ATS on 77.11% and 68.88% of the sites. The public documentation 5. We report distinct tuples of cookie name and the setter script’s URL. acknowledges using these two ﬁrst-party cookies to store user identiﬁers for tracking [8]. COOKIEGRAPH detects _fbp cookie by Facebook as ATS on 33.22% of the sites. Their public documentation acknowledges that Facebook tracking pixel stores unique identiﬁer in the ﬁrst-party _fbp cookie [5]. In fact, Facebook made a recent change to include ﬁrst-party cookie support in its tracking pixel to avoid third-party cookie countermeasures [20]. TikTok, an emerging social media app that is known to aggressively harvest sensitive user information [11], also recently added support for setting ﬁrst-party tracking cook- ies using TikTok Pixel [19], [17]. TikTok’s ﬁrst-party _ttp tracking cookie is present on 3.75% percent of sites, which is considerably lower than Facebook and Google but com- parable to more specialized entities such as Criteo. Criteo’s cto_bundle cookie is amongst the most prevalent ﬁrst-party ATS cookies. Recall from Section 3.3 that cto_bundle is sometimes purposefully set when third-party cookies are blocked. Our deployment of COOK- IEGRAPH shows that Criteo sets this ﬁrst-party ATS cookie on 5.98% of sites in our dataset. Note that ﬁrst-party ATS cookies from Lotame, ID5, and Adobe listed in Table 1 are also detected by COOKIEGRAPH but they do not make the top-25 list. Despite not being as prevalent as the other ﬁrst- party ATS cookies, their behavior analysis in Section 3.3 was crucial in discovering prevalent examples discussed in this section. Browser ﬁngerprinting. As discussed in Section 3.4, track- ers that use ﬁrst-party ATS cookies may employ other invasive tracking techniques such as browser ﬁngerprinting to implement cross-site tracking. We analyze the ﬁrst-party 11 cookies that are set by the scripts from entities that are known to engage in browser ﬁngerprinting. We use Dis- connect’s sublist of ﬁngerprinters [15], [7] from its tracking protection list [26]. We ﬁnd that Google’s and Facebook’s ﬁrst-party ATS cookies are predominately set by scripts served from domains involved in ﬁngerprinting. Lotame’s cookies (_cc_id, _cc_aud, _cc_cc) are also found to be set by such scripts. Overall, we ﬁnd that 45 (2.83%) distinct domains that set ﬁrst-party cookies are also known ﬁngerprinters. However, these handful of domains are responsible for setting 41.45% of all ﬁrst-party ATS cookies. This disproportionately be- tween domains and number of cookies set is not surprising. Effective cross-site tracking would require a tracker to be present on and collect data from a large number of sites. This presence will allow the tracker to collect extensive deterministic and probabilistic attributes about the user from a varied number of source, enhancing its ability to track users across sites in absence of third-party cookies. Our case studies in Appendix A and our analysis in Section 3.4 elaborate on how ﬁrst-party ATS cookies are combined with ﬁngerprinting for cross-site tracking. 4.4. Comparison with Existing Countermeasures Next, we compare COOKIEGRAPH with state-of-the- art countermeasures against ATS, CookieBlock [41] and WebGraph [77], in terms of detection accuracy, website breakage, and robustness. CookieBlock is a state-of-the-art approach to classify cook- ies, including advertising/tracking and analytics. It makes use of both manually curated allow lists and a machine learning classiﬁer, which mainly relies on features based on cookie attributes (cookie names and values). WebGraph is the state-of-the-art graph-based approach to classify ATS requests. Since WebGraph is not de- signed to directly classify cookies, we adapt it to this end by identifying ATS resources identiﬁed by WebGraph in 3P-Blocked and generating a block list of cookies for each domain set by those resources. This list is meant to mimic the effect of blocking these resources on ﬁrst-party ATS cookies. 4.4.1. Detection Accuracy. Table 4 compares the detec- tion accuracy of COOKIEGRAPH with CookieBlock and WebGraph. COOKIEGRAPH outperforms both approaches in all metrics. The superiority in precision indicates that existing countermeasures result on many more false posi- tives than COOKIEGRAPH. These additional false positives means that previous approaches would block functional ﬁrst- party cookies potentially affecting user experience. Next, we investigate the impact of these false positives on website breakage. 4.4.2. Website Breakage. We manually analyze the break- age caused by COOKIEGRAPH, CookieBlock and Web- Graph’s on 50 sites that are sampled from the 10K sites Table 4. CLASSIFICATION ACCURACY OF COOKIEGRAPH, WEBGRAPH, AND COOKIEBLOCK Classiﬁer Accuracy Precision Recall COOKIEGRAPH WebGraph CookieBlock 91.06% 78.74% 80.78% 91.87% 71.59% 69.95% 90.59% 85.49% 72.45% used in Section 3 (25 sites chosen randomly from top 100 and other 25 from the rest). We divide our breakage analysis in four categories of typical website usage: navigation (from one page to an- other), SSO (initiating and maintaining login state), appear- ance (visual consistency), and miscellaneous functionality (chats, search, shopping cart, etc.). We label breakage as major or minor for each category: major breakage – when it is not possible to use a functionality on the site included in either of the aforementioned categories, and minor breakage – when it is difﬁcult, but not impossible, for the user to make use of a functionality. To assess website breakage, we com- pare a vanilla Chrome browser (with no countermeasures against ﬁrst-party cookies) with browsers enhanced with an extension which blocks all ﬁrst-party cookies classiﬁed as ATS by COOKIEGRAPH, enhanced with an extension which blocks all cookies set by resources labeled as ATS by WebGraph, and enhanced with the ofﬁcial CookieBlock extension [3]. We use two reviewers to perform the breakage analysis to mitigate the impact of biases or subjectivity. Any disagreements between the reviewers were resolved after careful discussion. Out of the 50 sites, COOKIEGRAPH only had minor breakage on one site where an offer popup kept reappearing due to deletion of a cookie which stores user preferences. In contrast, both WebGraph and CookieBlock cause major breakage in at least one of the four categories on 10% of the sites. For example, WebGraph causes issues with cart functionality on darsoo.com, complete website breakage on espncricinfo.com, and SSO issues on other sites. Most of the breakage issues of CookieBlock relate to SSO logins and additional login-dependent functionality (e.g., missing proﬁle picture). Our results, that CookieBlock causes break- age on 8% of the sites with SSO logins, are inline with the 7-8% breakage reported by the authors [42]. We also ﬁnd that WebGraph blocks some additional ﬁrst- party cookies that are important for server-side functionality, but not directly related to user experience and therefore not immediately perceptible. For example, WebGraph blocks essential cookies such as Bm_sz cookie used by Akamai for bot detection, XSRF-TOKEN cookie used to prevent CSRF on different sites, and AWSALB cookies used by Amazon for load balancing. COOKIEGRAPH correctly classiﬁed these cookies at Non-ATS, and thus does not prevent these mea- sures from being deployed. 4.4.3. Robustness. We compare the robustness of COOKIE- GRAPH, CookieBlock, and WebGraph to evasion, i.e., mod- iﬁcations to cause the misclassiﬁcation of ATS resources as Non-ATS. Since advertisers and trackers are known to 12 Table 5. WEBSITE BREAKAGE COMPARISON OF ALL THREE COUNTERMEASURES.( BREAKAGE, AND ( ) SIGNIFIES NO BREAKAGE, ( ) MINOR ) MAJOR BREAKAGE. EACH CELL REPRESENTS THE PERCENTAGE OF SITES ON WHICH BREAKAGE WAS OBSERVED. Classiﬁer Navigation Miscellaneous Minor Major Minor Major Minor Major Minor Major Appearance SSO COOKIEGRAPH 0% WebGraph 10% CookieBlock 2% Table 6. ROBUSTNESS (DIFFERENCE IN CLASSIFICATION ACCURACY) 2% 4% 0% 0% 6% 8% 0% 0% 0% 0% 4% 0% 0% 2% 0% 0% 2% 2% 0% 0% 0% Classiﬁer ∆ Accuracy ∆ Precision ∆ Recall COOKIEGRAPH WebGraph CookieBlock 0.00% 0.00% -15.68% 0.00% 0.00% -15.08% 0.00% 0.00% -16.54% engage in the arms race with privacy-enhancing tools [37], [64], [61], to test whether the detection of ﬁrst-party ATS cookies is brittle in the face of trivial manipulation attempts such as changing cookie names. is important it We evaluate robustness on a test set of 2,000 sites from our dataset which also have the required CMP needed by CookieBlock for data collection and training. This translates to a total test set of 7,726 ﬁrst-party cookies. We change the names of the cookies in our test set to randomly generated strings of lengths between 2 and 15 characters. Table 6 shows the results. We note that both COOKIEGRAPH and WebGraph are fully robust to manipulation of cookies names while CookieBlock’s accuracy degrades by more than 15%. COOKIEGRAPH and WebGraph are robust because they do not use any content features (features related to the cookie characteristics, such as cookie name or domain) since these can be somewhat easily manipulated by an adversary aiming to evade classiﬁcation [77]. On the contrary, the most important feature of CookieBlock in fact depends on the cookie name, i.e., whether the name belongs to the top-500 most common cookie names [40]. Thus, CookieBlock can be easily bypassed with trivial cookie name modiﬁcations. COOKIEGRAPH’s implementation of ﬂow features can be manipulated by an adversary by using a different encod- ing than it currently considers or by changing the domains of exﬁltration endpoints. COOKIEGRAPH’s robustness to these attacks can be improved by more comprehensive informa- tion ﬂow tracking. However, full-blown information ﬂow tracking would incur prohibitively high run-time overheads (up to 100X-1000X [60]) and implementation complexity in the browser [46], [45], [80], [68]. 5. Limitations 5.1. Completeness COOKIEGRAPH relies on a graph representation of in- teractions between different elements during webpage exe- cution. The completeness of the interactions captured in the graph depends on the intensity and variety of user activity on a webpage (e.g., scrolling activity, number of internal pages clicked). In other words, it is possible that COOKIEGRAPH 13 may not detect certain ATS cookies if its graph represen- tation has not captured the interactions between different elements due to insufﬁcient user activity. To study the impact of user activity on COOKIEGRAPH, we recrawl sites performing two to three times more in- ternal page clicks than in the original crawl. We speciﬁ- cally recrawl 238 sites where Criteo’s cto_bundle cookie was originally classiﬁed as Non-ATS by COOKIEGRAPH. COOKIEGRAPH’s deployment on the recrawled sites results in successful detection of Criteo’s cto_bundle cookie as ATS on 121 of the 238 recrawled sites. We ﬁnd that the average number of inﬁltrations (exﬁltrations) increase from 1.54 to 2.95 (1.13 to 4.01) across the original and recrawled sites. We surmise that while there are cases where COOKIEGRAPH incorrectly classiﬁes ATS as Non- ATS due to incompleteness of the graph representation, its decision reﬂects the behavior of the cookie at the time of classiﬁcation. As more interaction is captured in the graph, COOKIEGRAPH is able to correctly switch the label to ATS. Moreover, COOKIEGRAPH never switch labels from ATS to Non-ATS due to increased interaction. We observed a similar trend for other prevalent ﬁrst-party ATS cookies in our dataset. 5.2. Deployment COOKIEGRAPH’s implementation is not suitable for run- time deployment due to the performance overheads asso- ciated to the browser instrumentation and machine learn- ing pipeline. We envision COOKIEGRAPH to be used in an ofﬂine setting: (1) ﬁrst-party ATS cookie-domain pairs are detected using machine learning classiﬁer and (2) the detected cookie-domain pairs are added to a cookie ﬁlter list such as those already supported in privacy-enhancing browser extensions such as uBlock Origin [18] for run- time blocking. We argue that a reasonably frequent (e.g., once a week) deployment of COOKIEGRAPH on a large scale would be sufﬁcient in generating and keeping the ﬁlter list up-to-date. While advertisers and trackers can in theory change cookie names at a rate faster than COOK- IEGRAPH’s periodic deployment, updating cookie names frequently is challenging in practice because setting these ﬁrst-party ATS cookies across many different sites requires tight coordination between different entities. To illustrate the practical issues associated with changing cookie names, consider the legacy demdex cookie set by Adobe’s em- bedded script is then exﬁltrated to the demdex.net domain. Adobe’s documentation explains that it is difﬁcult to change the legacy name because “... it is entwined deeply with Audience Manager, the Adobe Experience Cloud ID Service, and our installed user base” [25], [36]. If advertisers or trackers are somehow able to overcome these practi- cal challenges and change cookie names at a much faster pace, COOKIEGRAPH’s online implementation for run-time cookie classiﬁcation would be necessary. Further research is needed for efﬁcient and effective online implementation of COOKIEGRAPH. that 6. Conclusion We conducted a large scale differential measurement study to investigate how trackers abuse ﬁrst-party cookies to circumvent third-party cookie blocking. Our proposed COOKIEGRAPH was able to accurately and robustly block ﬁrst-party tracking cookies, and signiﬁcantly outperforming the state-of-the-art. Using COOKIEGRAPH, we found evi- dence of widespread abuse of ﬁrst-party cookies on more than 93% of the tested websites by 1500+ distinct tracking domains, which included major advertising entities such as Google as well as many specialized entities such as Criteo. References [18] “uBlock Origin: Resources Library,” https://github.com/gorhill/uBloc k/wiki/Resources-Library#cookie-removerjs-. [19] “Using cookies with tiktok pixel,” https://web.archive.org/web/ 20220610074648/https://ads.tiktok.com/help/article?aid=10007540. [20] “What facebook’s ﬁrst-party cookie means for adtech,” https://web.archive.org/web/20220729210450/https://clearcode.cc /blog/facebook-first-party-cookie-adtech/. [21] “This bug in your pc is a smart cookie,” https://archive.org/details/Fi nancialTimes1996UKEnglish, 1996. [22] “Internet privacy with ie6 and p3p: A summary of ﬁnd- ings,” http://web.archive.org/web/20200731061208/http://www.spyw arewarrior.com/uiuc/ie6-p3p.htm, 2001. [23] “Adblock plus,” https://adblockplus.org/, 2022. [Online]. Available: https://adblockplus.org/ [24] “Attentive cookie,” https://docs.attentivemobile.com/p [1] [2] [3] [4] [5] [6] [7] [8] [9] “About publisher provided identiﬁers,” https://web.archive.org/we b/20220614165742/https://support.google.com/admanager/answer/ 2880055?hl=en. “Cartographer 20220526085916/https://www.lotame.com/solutions/cartographe r-identity-graph/. https://web.archive.org/web/ identity graph,” “Cookieblock,” https://github.com/dibollinger/CookieBlock. Online “Criteo 20220819071808/https://ﬁlecache.investorroom.com/mr5ir crite o/977/download/Criteo Online Identification May2020.pdf/. Identiﬁcation),” https://web.archive.org/web/ and “fbp Parameters,” 20220722220344/https://developers.facebook.com/docs/marketi ng-api/conversions-api/parameters/fbp-and-fbc/. https://web.archive.org/web/ fbc “Firefox rolls out total cookie protection by default to all users world- wide,” https://blog.mozilla.org/en/products/firefox/firefox-rolls-out-t otal-cookie-protection-by-default-to-all-users-worldwide/. “Firefox’s protection against ﬁngerprinting,” https://support.mozilla. org/en-US/kb/firefox-protection-against-fingerprinting. “Google Analytics Cookie Usage //web.archive.org/web/20220812222800/https://developers.googl e.com/analytics/devguides/collection/gtagjs/cookie-usage. on Websites),” https: “Id5 identity cloud,” https://web.archive.org/web/20220727094611/ht tps://www.id5.io/identity-cloud/. [10] “Identity Guide,” https://web.archive.org/web/20220115155115/https: //yieldbird.com/identity-guide/. [11] “It’s their word against their https://internet2-0.com/whitepaper/its-their-word-against-their-s ource-code-tiktok-report/. source code - tiktok report,” [12] “Lotame – Data Collection Guide,” https://web.archive.org/web/ 20210730071853/https://my.lotame.com/t/p8hxvnd/data-collection-g uide. [13] “Lotame identity resolution,” 20220530231410/https://www.lotame.com/solutions/identity-res olution/. https://web.archive.org/web/ [14] “Lotame lightning tag,” 20220307010702/https://my.lotame.com/t/m1hxv7l/lotame-light ning-tag. https://web.archive.org/web/ [15] “Our new approach to address the rise of ﬁngerprinting,” https://blog.disconnect.me/our-new-approach-to-address-the-ris e-of-fingerprinting/. [16] “Panorama id,” https://web.archive.org/web/20220327180718/https:// www.lotame.com/panorama/id/. ages/developer-guides/third-party-integrations/referral-mar keting-platforms/talkable/, https://docs.attentivemobile.com/pages/developer-guides/third-p arty-integrations/referral-marketing-platforms/talkable/ [Online]. 2022. Available: [25] “Cookies the and ser- experience vice,” https://experienceleague.adobe.com/docs/id-service/using/intro /cookies.html?lang=en, 2022. [Online]. Available: https://experience league.adobe.com/docs/id-service/using/intro/cookies.html?lang=en identity cloud [26] “Disconnect tracking protection lists,” https://disconnect.me/trackerp rotection, 2022. [Online]. Available: https://disconnect.me/trackerpro tection [27] “Doubleclick,” https://web.archive.org/web/19970405225532/http: //www.doubleclick.com/, 2022. [28] “Easylist,” https://easylist.to/easylist/easylist.txt, 2022. [29] “Easyprivacy,” https://easylist.to/easylist/easyprivacy.txt, 2022. [30] “Enhanced tracking protection in ﬁrefox for desktop,” https://support. mozilla.org/en-US/kb/enhanced-tracking-protection-firefox-desktop, 2022. [31] “Hubspot cookie,” https://knowledge.hubspot.com/reports/what-coo kies-does-hubspot-set-in-a-visitor-s-browser, 2022. [Online]. Avail- able: https://knowledge.hubspot.com/reports/what-cookies-does-hub spot-set-in-a-visitor-s-browser [32] “Id5 - ﬁrst resolution methods party explained,” https://web.archive.org/web/20220408035339/https: //id5.io/news/index.php/2022/03/24/ﬁrst-party-ids-and-identity-resol ution-methods-explained/, 2022. identity and ids [33] “Omnisend cookie,” https://support.omnisend.com/en/articles [On- /1933402-explaining-and-managing-tracking-cookies, line]. Available: https://support.omnisend.com/en/articles/1933402-e xplaining-and-managing-tracking-cookies 2022. [34] “One trust. cookiepedia,” https://cookiepedia.co.uk, 2022. [35] “Tracking prevention in microsoft edge,” https://docs.microsoft.com /en-us/microsoft-edge/web-platform/tracking-prevention, 2022. [36] “Understanding calls do- https://experienceleague.adobe.com/docs/audience-manager main,” /user-guide/reference/demdex-calls.html?lang=en, 2022. [Online]. Available: https://experienceleague.adobe.com/docs/audience-manag er/user-guide/reference/demdex-calls.html?lang=en demdex the to [37] M. Alrizah, S. Zhu, X. Xing, and G. Wang, “Errors, misunder- standings, and attacks: Analyzing the crowdsourcing process of ad- blocking systems,” in Proceedings of the 2019 Internet Measurement Conference (IMC), 2019. [17] “Tiktok adds third-party cookies to its pixel – and tries to eat face- book’s lunch,” https://web.archive.org/web/20220623232016/https: //www.adexchanger.com/online-advertising/tiktok-adds-third-party-c ookies-to-its-pixel-and-tries-to-eat-facebooks-lunch/. [38] W. Aqeel, B. Chandrasekaran, A. Feldmann, and B. M. Maggs, “On landing and internal web pages: The strange case of jekyll and hyde in web performance measurement,” in Proceedings of the ACM Internet Measurement Conference, 2020. 14 [39] P. N. Bahrami, U. Iqbal, and Z. Shaﬁq, “Fp-radar: Longitudinal measurement and early detection of browser ﬁngerprinting,” in Pro- ceedings on Privacy Enhancing Technologies (PETS), 2022. [40] D. Bollinger, “Analyzing Cookies Compliance with the GDPR,” https: //www.research-collection.ethz.ch/handle/20.500.11850/477333. The- sis, ETH Zurich. [41] D. Bollinger, K. Kubicek, C. Cotrini, and D. Basin, “Automating cookie consent and GDPR violation detection,” in 31st USENIX Security Symposium (USENIX Security 22). USENIX Association, 2022. [42] ——, “Automating cookie consent and gdpr violation detection,” in 31st USENIX Security Symposium (USENIX Security 22), 2022. [43] A. Cahn, S. Alfeld, P. Barford, and S. Muthukrishnan, “An empirical study of web cookies,” in Proceedings of the 25th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2016, p. 891–901. [44] Q. Chen, P. Ilia, M. Polychronakis, and A. Kapravelos, “Cookie swap party: Abusing ﬁrst-party cookies for web tracking,” in Proceedings of the Web Conference, 2021. [45] Q. Chen and A. Kapravelos, “Mystique: Uncovering information leakage from browser extensions,” in Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, 2018, pp. 1687–1700. [46] A. Chudnov and D. A. Naumann, “Inlined information ﬂow mon- itoring for javascript,” in Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, 2015, pp. 629–643. [47] L. M. D. Kristol, “Http state management mechanism,” https://datatr acker.ietf.org/doc/html/rfc2109, 1997. [48] S. Dambra, I. Sanchez-Rola, L. Bilge, and D. Balzarotti, “When sally met trackers: Web tracking from the users’ perspective,” in USENIX Security Symposium, 2022. [49] H. Dao, J. Mazel, and K. Fukuda, “Cname cloaking-based tracking on the web: Characterization, detection, and protection,” IEEE Trans- actions on Network and Service Management, 2021. [60] D. Hedin, A. Birgisson, L. Bello, and A. Sabelfeld, “Jsﬂow: Tracking information ﬂow in javascript and its apis,” in Proceedings of the 29th Annual ACM Symposium on Applied Computing, 2014, pp. 1663– 1671. [61] L. Hieu, M. Athina, and S. Zubair, “Cv-inspector: Towards automat- ing detection of adblock circumvention,” in Network and Distributed System Security Symposium (NDSS), 2021. [62] M. Hils, D. W. Woods, and R. B¨ohme, “Measuring the emergence of consent management on the web,” in Proceedings of the ACM Internet Measurement Conference, 2020. [63] X. Hu, N. Sastry, and M. Mondal, “Cccc: Corralling cookies into categories with cookiemonster,” in 13th ACM Web Science Conference 2021. Association for Computing Machinery, 2021, p. 234–242. [64] U. Iqbal, Z. Shaﬁq, and Z. Qian, “The ad wars: Retrospective mea- surement and analysis of anti-adblock ﬁlter lists,” in IMC, 2017. [65] U. Iqbal, P. Snyder, S. Zhu, B. Livshits, Z. Qian, and Z. Shaﬁq, “Adgraph: A graph-based approach to ad and tracker blocking,” in IEEE Symposium on Security and Privacy (S&P). IEEE, 2020. [66] U. Iqbal, C. Wolfe, C. Nguyen, S. Englehardt, and Z. Shaﬁq, “Khaleesi: Breaker of advertising and tracking request chains,” in USENIX Security Symposium (USENIX), 2022. [67] P. Laperdrix, W. Rudametkin, and B. Baudry, “Beauty and the beast: Diverting modern web browsers to build unique browser ﬁngerprints,” in 2016 IEEE Symposium on Security and Privacy (SP), 2016. [68] S. Lekies, B. Stock, and M. Johns, “25 million ﬂows later: Large- scale detection of dom-based xss,” in Proceedings of the 2013 ACM SIGSAC conference on Computer and Communications Security, 2013, pp. 1193–1204. [69] P. G. Leon, L. F. Cranor, A. M. McDonald, and R. McGuire, “Token attempt: the misrepresentation of website privacy policies through the misuse of p3p compact policy tokens,” in Proceedings of the 9th Annual ACM Workshop on Privacy in the Electronic Society, 2010. [70] L. Montulli, “The reasoning behind web cookies,” http://montulli.blo gspot.com/2013/05/the-reasoning-behind-web-cookies.html, 2013. [50] Y. Dimova, G. Acar, L. Olejnik, W. Joosen, and T. Van Goethem, “The CNAME of the Game: Large-scale Analysis of DNS-based Tracking Evasion,” PETS, 2021. [71] N. Nguyen, “Latest ﬁrefox rolls out enhanced tracking pro- tection,” https://blog.mozilla.org/en/products/firefox/latest-firefox-rol ls-out-enhanced-tracking-protection/, 2018. [51] D´ıaz-Morales and Roberto, “Cross-device tracking: Matching devices and cookies,” in 2015 IEEE International Conference on Data Mining Workshop (ICDMW), 2015, pp. 1699–1704. [52] B. Eich, “C is for cookie,” https://brendaneich.com/2013/05/c-is-for -cookie/, 2013. [53] ——, “The cookie clearinghouse,” https://brendaneich.com/2013/06/ the-cookie-clearinghouse/, 2013. [54] S. Englehardt and A. Narayanan, “Online tracking: A 1-million-site measurement and analysis,” in Proceedings of ACM CCS 2016, 2016. [55] S. Englehardt, D. Reisman, C. Eubank, P. Zimmerman, J. Mayer, A. Narayanan, and E. W. Felten, “Cookies that give you away: The surveillance implications of web tracking,” in Proceedings of the 24th International Conference on World Wide Web, 2015. [56] I. Fouad, N. Bielova, A. Legout, and N. Saraﬁjanovic-Djukic, “Missed by ﬁlter lists: Detecting unknown third-party trackers with invisible pixels,” Proceedings on Privacy Enhancing Technologies, vol. 2020, pp. 499–518, 04 2020. [57] I. Fouad, C. Santos, A. Legout, and N. Bielova, “My cookie is a phoenix: detection, measurement, and lawfulness of cookie respawn- ing with browser ﬁngerprinting,” in Privacy Enhancing Technologies Symposium (PETS), 2022. [58] B. Fulgham, “Protecting against hsts abuse,” https://webkit.org/blog/ 8146/protecting-against-hsts-abuse/, 2018. [59] Google, “The privacy sandbox,” https://developer.chrome.com/docs/ privacy-sandbox/. [72] P. Papadopoulos, N. Kourtellis, and E. P. Markatos, “Cookie syn- chronization: Everything you always wanted to know but were afraid to ask,” in Proceedings of the World Wide Web (WWW) Conference, 2019. [73] A. Randall, P. Snyder, A. Ukani, A. Snoeren, G. Voelker, S. Savage, and A. Schulman, “Trackers bounce back: Measuring evasion of partitioned storage in the wild,” 2022. [74] F. Roesner, T. Kohno, and D. Wetherall, “Detecting and defending against third-party tracking on the web,” in 9th USENIX Symposium on Networked Systems Design and Implementation (NSDI 12), 2012, pp. 155–168. [75] I. Sanchez-Rola, M. Dell’Amico, , D. Balzarotti, P.-A. Vervier, and L. Bilge, “Journey to the center of the cookie ecosystem: Unraveling actors’; roles and relationships,” in S&P 2021, 42nd IEEE Symposium on Security & Privacy, 23-27 May 2021, San Francisco, CA, USA, 2021. [76] J. Schuh, “Building a more private web: A path towards making third party cookies obsolete,” https://blog.chromium.org/2020/01/building -more-private-web-path-towards.html, 2020. [77] S. Siby, U. Iqbal, S. Englehardt, Z. Shaﬁq, and C. Troncoso, “Web- graph: Capturing advertising and tracking information ﬂows for robust blocking,” in 31st USENIX Security Symposium (USENIX Security 22). USENIX Association, 2022. [78] A. Sj¨osten, P. Snyder, A. Pastor, P. Papadopoulos, and B. Livshits, “Filter list generation for underserved regions,” in WWW, 2020. 15 [79] S. Sluis, “Google is building integrations for publisher-speciﬁc identi- ﬁers,” https://www.adexchanger.com/platforms/google-is-building-int egrations-for-publisher-speciﬁc-identifiers/, 2021. [80] B. Stock, S. Lekies, T. Mueller, P. Spiegel, and M. Johns, “Precise client-side protection against dom-based cross-site scripting,” in 23rd USENIX Security Symposium (USENIX Security 14), San Diego, CA, 2014, pp. 655–670. [81] B. P. Team, “Fighting CNAME Trickery,” https://brave.com/privacy-u pdates/6-cname-trickery/, 2020. [82] M. E. Team, “Introducing tracking prevention, now available in microsoft edge preview builds,” https://blogs.windows.com/msedge dev/2019/06/27/tracking-prevention-microsoft-edge-preview/, 2022. [Online]. Available: https://blogs.windows.com/msedgedev/2019/06/ 27/tracking-prevention-microsoft-edge-preview/ [83] A. Vastel, P. Laperdrix, W. Rudametkin, and R. Rouvoy, “Fp-stalker: Tracking browser ﬁngerprint evolutions,” in IEEE Symposium on Security and Privacy (SP), 2018. [84] A. V. Veen and A. de Vries, “Cookie compliance of dutch hospital websites,” 2021. [85] WebKit, “Tracking prevention in webkit,” https://webkit.org/trackin g-prevention/, 2022. [Online]. Available: https://webkit.org/trackin g-prevention/ [86] J. Wilander, “Intelligent tracking prevention,” https://webkit.org/blog/ 7675/intelligent-tracking-prevention/, 2017. [87] ——, “Intelligent tracking prevention 1.1,” https://webkit.org/blog/ 8142/intelligent-tracking-prevention-1-1//, 2018. [88] ——, “Intelligent tracking prevention 2.0,” https://webkit.org/blog/ 8311/intelligent-tracking-prevention-2-0/, 2018. [89] ——, “Intelligent tracking prevention 2.1,” https://webkit.org/blog/ 8613/intelligent-tracking-prevention-2-1/, 2019. [90] ——, “Intelligent tracking prevention 2.2,” https://webkit.org/blog/ 8828/intelligent-tracking-prevention-2-2/, 2019. [91] ——, “Intelligent tracking prevention 2.3,” https://webkit.org/blog/ 9521/intelligent-tracking-prevention-2-3/, 2019. [92] ——, “Cname cloaking and bounce tracking defense,” https://we bkit.org/blog/11338/cname-cloaking-and-bounce-tracking-defense/, 2020. [93] ——, “Full third-party cookie blocking and more,” https://webkit.org /blog/10218/full-third-party-cookie-blocking-and-more/, 2020. [94] ——, “Bounce tracking protection,” https://github.com/privacycg/pro posals/issues/6, 2022. Appendix 1. Case Studies In this section, we look at case studies of ATSes identi- ﬁed in Section 3.3 which are found to be extensively using ﬁrst-party cookies for tracking purposes. We analyze the behavior of these ATS in our crawls, compare the observed behavior with their documentation, and create a generic model which all ﬁrst-party-cookie-based ATSes follow in Section 3.4. We present case studies of three ATSes here: Lotame, ID5, and Criteo. 1.1. Lotame. Lotame is a data and identity management solution which claims to provide a single ID to users across multiple browsers, devices, and platforms. Lotame’s Light- ning Tag [14] packages the user visit data in a JSON object and sends it to its servers. Code 2 shows an example payload sent to Lotame. The payload includes IDs assigned by the website, third-party identiﬁers present on the site, certain user behaviors (conﬁgured through collaboration between the publisher and Lotame), and other custom rules deﬁned per website [12]. Lotame processes the payload and matches the data with its Cartographer Identity Graph [2], and sends back an ID, called panoramaID [16], which is stored as a ﬁrst-party cookie or in localStorage. 1.2. ID5 Universal ID. ID5 provides identity resolution for publishers and advertisers through its Identity Cloud [9]. ID5’s script packages a payload that contains several de- terministic identiﬁers, such as email, usernames, and phone numbers (if available) and as well as probabilistic identiﬁers include, such as IP address, user agent, and location of the user [32]. ID5 then processes the payload and matches the data with its Identity Cloud and send back an ID, called universal id, which is stored as a ﬁrst-party cookie and as well as in local storage. An example payload from ID5 is shown in Figure 3. We note that ID5 also provides Partner Graph, a service that enables information sharing among its partners [9]. Partner Graph allows different identify providers to exchange information with each other. data: { behaviorIds: [1,2,3], behaviors: { int: [’behaviorName’, ’behaviorName2’], act: [’behaviorName’] }, ruleBuilder: { key1: [’value 1a’, ’value 1b’] }, thirdParty: { namespace: ’NAMESPACE’, value: ’TPID_VALUE’ } } Code 2. Example of data sent structure sent to Lotame during a user’s ﬁrst visit. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 16 party cookies and localStorage for storing cto_bundle cookie. We consider this to be one of the fundamental behaviors of ﬁrst-party ATS cookies. As described in Sec- tion 4.1, COOKIEGRAPH’s graph representation abstracts storage to refer to both Cookies and localStorage. We also include a count of localStorage accesses in the feature set computed from the graph representation. Inclusion of these features help COOKIEGRAPH effectively model ﬁrst-party ATS cookies behavior. 2. Cross-Device User Attribution The methodology for cross-site attribution can also be extended to cross-device attribution. In this slightly more complex scenario, the user is not only visiting different sites but also using different devices with different ﬁngerprints. We show an example of cross-device attribution in Fig- ure 10. Instead of visiting the sites on the same device, the user now visits sites 1-3 on device 1, and then visits sites 4-6 on device 2. Fingerprints F1, F2 and F3 are collected on sites 1-3 while using device 1, and F4, F5 and F6 are collected while using device 2. Sites 3 and 6 ask the user for an additional P P ID − 1. There is no similarity in ﬁngerprints between the two devices. However, as sites 3 and 6 collect the additional P P ID − 1, the tracker is able to identify and populate its ﬁrst-party cookie on each of those sites with the correct user ID (U ID1). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 { ""created_at"":""2022-02-09T11:42:40.817811Z"", ""id5_consent"":true, ""original_uid"":""ID5* FnFOGLkYzdJjuoK3KvAecVW2oFpZ7OrZiW7h-M0H ACAHYuWkxQrEGcpWuOkQUXbHB2OO8Rj0wt94jllT WHQ6wdkqOwSbnYea8cesuONCF4HZeIoDaB_TwBsy lKrs3tHB2Y87ZwP0DrpYlGz1OG1Fgdn0YdgqoSGU SGxzS1gUzsHaMIUBVqf2I08es6aUULEB2n48oyL0 nGnRtstVqtcQQdquS3Aay4Hhgbzh9gIZyYHa_nLT d5rjbbR0ZXwkXDzB2yU1XUC2dukip1J_clVAgdtt xC_xaRRBOLi0fnvp9cHbqr_pWihTtaUMS_R6eLuB 2_AMExt1UdhJZBe2mcXZAdwm9lcbeMMvlpg3MBrC oHcUgzNypi-5xLUqBD8GC4B3KsefcNkiDvI4n9ZL 7OjAdzqB9PD-KczAx63Ck0gEIHdNPfQeEi-f5VaO OEhf6B3VUqDoL11hqVoIuDhKJbgd2kp0mgXabhwJ tPO7sgWwHd_vz_uIYYmqQBTbH-JFVB3h1-kI9GQv dby2PyftDawd596ho3tuOsKtoDOk4S4Her2Uw-_u BYRxrt6YzVYqB3tRwTVI3Fxm8cGJyjdmYAd88lom BIpkOeg2Ok4VTNc"", ""universal_uid"":""ID5* HGH7W7iMpMu3-EPZCXUuqNBB7fFHUUVcbSddSSG Fu5UHYucsBxMz2jncvKS7rkwlB2MWiiPupapPxa 79eieMAdkTyMQz82s1vIekPr28DEHZbqTCrapj9 Fb9K0x4zjlB2YHOKNDwQY6mZwxk_1mwAdna3wWna hrpMEUrPxJSnAHaPYB-InS5DXGpQgqbqirB2nHFI D4j9i9BgCP3k0VygdqdtFHsT7eeDfFYuB8EQ0Ha4 -yV9Ifvbvi5oxmtH7HB2xg-mmmOeyVOPBYGi2tfw dtREZnUE83cfn_LHvHvu4HbvkLkwEFJiddOEp4PT ZbB2-de_VPyKHax5JtpO46xwdwZ_0UMgANOsZygV 0SrrMHcZ37qQB-LkCO4tWoTbv_B3KMGCMrebcfLE TeCn0AEgdzIR1utDJzM6AaiL9KVkAHdPtrAtTv73 ZyDg92Rq-_B3XeRNOOc7b2CEBsilXOlQd2sfmR36 NyW-dsK9CUmd4Hd3vcrlAWzfYEfw01Q5J1B3ibAF UYrA0XWMl-D9jSlAd5iX1tGA4vPu0wdZkXVOEHek q2xibOm9XwN2nSdZjbB3v8nOyzGuF9QgwI67pMGQ d85BszRCJDUkiiu-tv5BQ"", ""signature"":"" ID5_Ab6tnGgmCcjKo-qFGVKszuNpNePqkOHZT rbCmpuktLLOlNOCALhmY_91AHP8LU0BvfJT2Q JQWlsUEfynB1hBGZc"", ""link_type"":1, ""cascade_needed"":true, ""privacy"":{ ""jurisdiction"":""other"", ""id5_consent"":true } } Code 3. Example of data structure received from ID5 during a user’s ﬁrst visit. 1.3. Criteo. Criteo provides Criteo Identity Graph for iden- tity resolution [4]. Criteo Identity Graph is built from four different sources: (i) data contributed by advertisers, (ii) data collected from publisher websites by Criteo itself, (iii) data provided by Criteo partners such as LiveRamp and Oracle, (iv) and predictions on existing data by Criteo’s machine learning models. Criteo claims that its identity graph is able to stitch together identiﬁers from more than 2 billion users across the world, and that it contains persistent deterministic identiﬁers for 96% of the users [4]. Similar to other identity resolution services, Criteo generates an ID, based on identi- ﬁers, such as hashed emails, mobile device IDs, cookie IDs, and stores it in ﬁrst-party storage as cto_bundle. Their documentation shows that Criteo makes use of both ﬁrst- 17 Figure 10. This ﬁgure shows the ﬂow of information and identiﬁers through an identity graph for a cross-device attribution. The user visits sites 1, 2, and 3 via Device 1. The identity graph returns a U ID − 1 for all the site visits, using a probabilistic matching of ﬁngerprints F 1, F 2, and F 3 sent on each respective website. A Publisher Provided ID P P ID − 1 is also sent alongside F 3 when visiting site 3. The user visits sites 4, 5, and 6 through a new Device 2. Because the ﬁngerprints F 4 and F 5 are different from F 1, F 2, and F 3, the identity graph returns a new U ID − 2 for these site visits. On site 6, the website obtains and sends a Publisher Provided ID which matches P P ID − 1 provided on site 3. As a result, the identity graph matches and returns the existing user’s U ID − 1 for site 6. 18 Site 1 Site 2 Site 3 UID-1 UID-1 UID-1 Device 1 Cookie Stor e Site 1 Site 2 Site 3 F1 UI D-1 F2 UI D-1 F3+PPI D-1 UI D-1 F1 F4 F2 F3 + PPID-1 F6 + PPID-1 F5 Site 4 Site 5 Site 6 UID-2 UID-2 UID-1 Cookie Stor e Device 2 F4 UI D-2 F5 UI D-2 F6 + PPI D-1 UI D-1 Site 4 Site 5 Site 6",0.0
"TMIC: App Inventor Extension for the Deployment of Image Classification
  Models Exported from Teachable Machine","[{'href': 'http://arxiv.org/abs/2208.12637v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2208.12637v2', 'rel': 'related', 'type': 'application/pdf'}]",2022-08-24 17:34:47,,"Dynamic Memory-based Curiosity: A Bootstrap Approach for Exploration Zijian Gao1, Kele Xu1*, YiYing Li2, Yuanzhao Zhai1, Dawei Feng1, Bo Ding1, XinJun Mao1, Huaimin Wang1 1 National University of Defense Technology, Changsha, China 2 Artiﬁcial Intelligence Research Center, DII, Beijing, China kelele.xu@gmail.com 2 2 0 2 g u A 4 2 ] I A . s c [ 1 v 9 4 3 1 1 . 8 0 2 2 : v i X r a Abstract The sparsity of extrinsic rewards poses a serious chal- lenge for reinforcement learning (RL). Currently, many efforts have been made on curiosity which can pro- vide a representative intrinsic reward for effective ex- ploration. However, the challenge is still far from be- ing solved. In this paper, we present a novel curiosity for RL, named DyMeCu, which stands for Dynamic Memory-based Curiosity. Inspired by human curiosity and information theory, DyMeCu consists of a dynamic memory and dual online learners. The curiosity arouses if memorized information can not deal with the current state, and the information gap between dual learners can be formulated as the intrinsic reward for agents, and then such state information can be consolidated into the dynamic memory. Compared with previous curiosity methods, DyMeCu can better mimic human curiosity with dynamic memory, and the memory module can be dynamically grown based on a bootstrap paradigm with dual learners. On multiple benchmarks including Deep- Mind Control Suite and Atari Suite, large-scale empir- ical experiments are conducted and the results demon- strate that DyMeCu outperforms competitive curiosity- based methods with or without extrinsic rewards. We will release the code to enhance reproducibility. Introduction Despite the success of reinforcement learning (RL) on sequential decision-making tasks (Bellemare et al. 2013; Tesauro et al. 1995; Mnih et al. 2015), many current methods struggle with sparse extrinsic rewards. To cope with the spar- sity, curiosity provides a representative intrinsic reward that can encourage agents to explore new states. Designing algo- rithms to efﬁciently construct curiosity can be a key compo- nent in RL systems. Previous research has shown that intrin- sic rewards can help alleviate the issues resulting from the lacking of dense extrinsic rewards (Liu and Abbeel 2021b; Tao, Franc¸ois-Lavet, and Pineau 2020; Yang et al. 2021). For human learning, curiosity motivates people to seek and retain more information through exploration in the envi- ronment (Burda et al. 2018; Ryan and Deci 2000; Smith and Gasser 2005). The process of arousing and satisfying curios- ity can be summed up as one cycle: when a person encoun- ters a problem, he/she will ﬁrst try to solve it by retrieving information from memory. If retrieval from memory fails, he/she realizes that the current memorized information is in- sufﬁcient solve the problem. A conscious awareness of in- formation discrepancy then sparks curiosity about the prob- lem, and curiosity stimulates the search for new informa- tion. Once the information discrepancy is eliminated, people may have no further curiosity to learn more about the cur- rent problem until another problem is encountered (Rotgans and Schmidt 2017; Silvia 2017). Human curiosity is con- stantly consolidated based on the dynamic memory, which consists of the encoding, storing, and retrieving information stage (Hayes et al. 2021). As the curiosity fades, additional information is consolidated into the memory. The consolida- tion results in the forming of new dynamic memories, which depends on the hippocampus (O’Reilly and Rudy 2001). Many attempts have been made to build curiosity in RL, which fall into two main categories: count-based and prediction-based. However, such curiosity is very different from human curiosity, and the problem is far from solved. Taking the Random Network Distillation (RND) (Burda et al. 2019) method as an example, RND initializes a ran- dom ﬁxed target network with state embeddings, and trains another prediction network to ﬁt the output of the target net- work. A random ﬁxed target network can be regarded as a random ﬁxed memory, so that RND cannot retain contextual knowledge about the environment (Yang et al. 2021). With- out dynamically incorporating contextual information into memory, random features may not be sufﬁcient to interpret dynamic environments. Therefore, this kind of curiosity is evaluated in a non-developmental way, which severely lim- its the performance of curiosity in RL. In this work, to mimic human curiosity, we formalize and investigate a Dynamic Memory-based Curiosity mechanism, named DyMeCu. Inspired by the bootstrap paradigm (Guo et al. 2020; Grill et al. 2020; Flennerhag et al. 2021), we construct dual online learners to learn the latent state to formulate dynamic memory model (Figure 1). On the one hand, state information can be consolidated to the memory via the exponential moving average (EMA) (Haynes, Corns, and Venayagamoorthy 2012; Klinker 2011; Grebenkov and Serror 2014) of dual learners’ parameters. The bootstrap The term bootstrap is used in this text in its colloquial meaning *Corresponding author: Kele Xu rather than its statistical connotation. Figure 1: DyMeCu employs a novel learning framework to build the intrinsic reward for RL, which consists of a dynamic memory and dual online learners. The information discrepancy of the current state compared with the retrieved information from the memory makes curiosity arouse. We get the curiosity-based intrinsic reward for agent learning by calculating the information gap between dual online learners. Then the state information can be dynamically consolidated into the memory in the bootstrap paradigm for curiosity fading. paradigm, on the other hand, utilizes supervised signals from memory to improve dual learners’ encoding ability. Further- more, the curiosity is measured by the information gap be- tween the dual learners, which is essentially an uncertainty estimation of given state based on dynamic memory (Mai, Mani, and Paull 2022; Liu et al. 2020; Abdar et al. 2021). In brief, our contribution in this paper is: • We ﬁrstly analyze the shortcomings of previous curiosity-based intrinsic reward methods, and suggest to mimic human curiosity leveraging a dynamic memory in- stead of a ﬁxed one, based on the information theory. • We propose a novel and practicable intrinsic reward method for RL agents, named DyMeCu (Dynamic Memory-based Curiosity), which consists of a dynamic memory and dual online learners, and thus can measure the curiosity and consolidate the information in a feasi- ble way. Meanwhile, different strategies are explored to further improve the performance of DyMeCu. • On multiple benchmarks including DeepMind Con- trol Suite (DMC) (Tunyasuvunakool et al. 2020) and Atari Suite (Bellemare et al. 2013), large-scale empiri- cal experiments demonstrate that DyMeCu outperforms the other competitive curiosity-based methods and pre- training strategies. Related Work Curiosity-Based Intrinsic Reward In RL, the exploration issue is a long standing challenge. Previous attempts suggest that: if there is no additional re- ward, exploration can be regarded as a hunt for informa- tion theoretically, which also can be viewed as the curios- ity (Berlyne 1950; Schmidhuber 1991; Kidd and Hayden 2015; de Abril and Kanai 2018; Jaegle, Mehrpour, and Rust 2019; Friston et al. 2016; Peterson and Verstynen 2021). One intuitive formulation of curiosity is the count-based meth- ods, where the less visited state has more state novelty for exploration. But it can not scale to large-scale or continu- ous state spaces (Kearns and Singh 2002; Charikar 2002). Inspired by count-based methods, RND calculates the state novelty by distilling a random ﬁxed network (target net- work) into another prediction network (predictor network). The predictor network is trained to minimize the prediction error for each state and take the prediction error as the in- trinsic reward. Apart from count-based methods, prediction- based methods also show competitive or better performance by modeling the environment dynamics (Pathak et al. 2017; Pathak, Gandhi, and Gupta 2019; Kim et al. 2020; Burda et al. 2018). With the assumption that more visited state- action pairs will result in more accurate prediction, the in- trinsic reward can be applied as the variance of predic- tions of ensembles or the distance between prediction states and true states, such as the Disagreement method (Pathak, Gandhi, and Gupta 2019) and ICM (Pathak et al. 2017) method. There have been few attempts to design a curios- ity that contains memory and effectively uses information consolidated in memory, which however is the main goal of this paper. Uncertainty Estimation Our work is also related to the uncertainty estimation, as uncertainty is crucial which allows an agent to discern when to exploit and when to explore its environment in RL (Szepesv´ari 2009). Previous intrinsic rewards can also be interpreted from the perspective of uncertainty estima- tion, which can evaluate curiosity by estimating the deep learning model’s uncertainty (conﬁdence). Take Disagree- Curiosity Arouses Curiosity-based Learning & Curiosity Fades Novel State 𝑆𝑡 Encode Latent State Retrieve Information State 𝑠𝑡 Information Discrepancy Memory Model 𝜔 Output Retrieved Information Online Learner 𝜃1 EMA Memory Model 𝜔 EMA Online Learner 𝜃2 𝜃1 𝑧𝑡 loss 𝜔 𝑧𝑡 loss 𝜃2 𝑧𝑡 Stop- gradient 𝑖𝑛𝑡 𝑟𝑡 𝑠𝑡 State 𝜋 Policy 𝑎𝑡 Action 𝑠𝑡+1 Next State Algorithm 1: Dynamic Memory-based Curiosity Initialization: policy network πφ; dual online learner net- works fθ1, fθ2; memory network Mω; coefﬁcients of intrin- sic and extrinsic reward ζ, β. 1: while Training do 2: 3: 4: 5: Receive state st from environment at ← πφ(a|s) based on policy network πφ Take action at, receive state st+1 and extrinsic re- ext from environment ward rt Collect step data into replay buffer st ← st+1 for t = 1, · · · , T do end for Sample batch data as {(si, ai, ri replay buffer for each i = 1, · · · , N do ext, si+1)}N i=1from Generate latent state vectors zθ1 fθ2 (si), zω i = Mω(si) Calculate intrinsic reward rint Calculate total reward ri total = ζri i = (cid:107)zθ1 i − zθ2 i (cid:107)2 int + βrext i i = fθ1 (si), zθ2 i = end for Update θ1 and θ2 with sampled data by minimizing loss with equation (3) Update ω with equation (7) Update φ with sampled data by maximizing rtotal us- ing RL algorithm 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end while ment as an example, instead of comparing the prediction to the ground-truth, they suggest to evaluate the uncertainty of multiple prediction models using the deep ensemble (Diet- terich 2000), despite incurring additional computation. RND also claims that the distillation error can be viewed as a quantiﬁcation of the uncertainty. Unlike RND, in our work, we evaluate the uncertainty of given states though measur- ing the information gap between dual learners which rely on dynamic memory instead of a random ﬁxed network. Methodology In general, if an agent encounters a state with the informa- tion value E compared to its memory, then this state is worth exploring and such information value is worth consolidat- ing to its memory dynamically (Rotgans and Schmidt 2017; Silvia 2017). In detail, the concept of information value E necessitates the formation of the dynamic memory M and a way g to consolidate information to the memory. For deep neural networks, the memory M can be embedded in the la- tent space and g can by the function that maps state s into memory (Peterson and Verstynen 2021). This kind of con- solidating information is denoted by: g(s; M ) → M (cid:48). (1) With the memory M which has been learned by g over historical states, we can measure the information value E of the next state st+1. According to the information the- ory (Ishii, Yoshida, and Yoshimoto 2002; Reddy, Celani, and Vergassola 2016; Gray 2011) and the concepts proposed in Peterson and Verstynen (2021), the information value E of a state should (1) only depend on the memory and what can be immediately learned (i.e., M , s and g); (2) be non-negative because E is for exploring the environment; (3) decelerate in the ﬁnite time for the same state. Thus we deﬁne E as: E = (cid:107)g(st+1; M ) − M (cid:107) . (2) We can get from the deﬁnition of E as : (1) If one state has been completely explored, or cannot be learned, then no more information gain can be added into the current memory, and E = 0. Such state is no longer worth exploring. (2) If E > 0, then the larger the value of E, the more information gain can be consolidated into the memory. In other words, the larger the value of E, the memory M is less aware of the current state, such state is more worth explor- ing. It is such information deﬁciency of memory that sparks the curiosity of agents. In this paper, we will focus on how to obtain and lever- age the information value E for agents exploration, and the information consolidation method g in details. Dynamic Memory-based Curiosity In our framework, if the information in current memory cannot handle the encountered state, then the curiosity is aroused. We model the memory as a learnable neural net- work, but there is a dilemma that we do not have a “bench- mark” encoded network in the parameter space to encode the encountered state accurately, since not enough supervised signal provided here. Thus it is difﬁcult and not sound to di- rectly deﬁne the curiosity by comparing a random encoded state with the output latent state from a dynamic memory network. We instead introduce dual online learners for state encode representations. These dual learners have the same network architecture as the memory network but with each own different parameters. In their network parameter space, the dual networks are supervised by the memory encoding ability. And then our curiosity can be deﬁned by the gap of the encodings of the same state output by dual learners’ networks. The intuition of our dual learners is: if the state information has been squeezed out by the memory, then the memory can completely know and resolve the state, and the dual learners which can be seen as the two imitators to the memory are easier to get the similar encodings to the current state. In other words, if one state is little known by the mem- ory, then the dual learners may produce quite different en- codings to it, which represents the larger information value E and thus stimulates the agent to explore this state. Here, for the uniform description, we refer the encoded states and encodings to the latent states, which reﬂect the cognition of states by the memory and learner networks. In our implementation, such kind of latent gap E will spark the curiosity as the intrinsic reward for agents explo- ration. After the RL agents learning, such information will be consolidated to the memory for memory better growing. In terms of the consolidation method g, it is externalized as updating the memory parameters via the exponential mov- ing average (EMA) (Haynes, Corns, and Venayagamoorthy Figure 2: Performance of different methods in the ﬁne-tuning phase on DeepMind Control Suite. 2012; Klinker 2011; Grebenkov and Serror 2014) of the dual learners’ parameters. From such analysis, we see the dual learners ﬁrst learn based on the memory network for measure the information value for exploration, and then the memory network consol- idate information gain based on dual learners in the EMA way. The memory is actually seeking for the appropriate po- sition in the parameter space dynamically, in order that its network can better characterize the memory and cognition ability of the seen states in environments. In a word, our dy- namic memory is updated in a bootstrap (Grill et al. 2020) way. Figure 1 and algorithm 1 present the whole framework and pseudo-code of DyMeCu. • Learning of Dual Learners: Dual online learner models fθ1 and fθ2 are deﬁned by a set of weights θ1 and θ2 with the same architecture as the memory network Mω. The memory provides the regression targets for the learning of dual learners fθ1 and fθ2. Given a current state st, the learners transform it into the latent (cid:44) fθ1 (st) and zθ2 states zθ1 (cid:44) fθ2 (st) respectively, and the t t (cid:44) Mω (st). The mean squared memory network outputs zω t error (MSE) between them is: (cid:13) (cid:13)zθ1 (cid:13) (cid:13) (cid:13)zθ2 (cid:13) Based on Lθ1 and Lθ2 , the dual learners are updated as : (cid:26)θ1 ← optim (θ1, ∇θ1Lθ1 , η) , θ2 ← optim (θ2, ∇θ2Lθ1 , η) , where optim and η represent the optimizer and learning rate. (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) t − zω t t − zω t   Lθ2 Lθ1 (3) (4)  (cid:44) (cid:44) , . • Intrinsic Reward based on Curiosity: The curiosity relies on the information value of current state. In our method, such information value can be mea- sured by the information gap between dual learners. This information gap can also be considered following the δ- Progress (Achiam and Sastry 2017; Graves et al. 2017) to form the curiosity. We obtain the intrinsic reward to agents based on the curiosity from the information value: (5) t (cid:107)2. t − zω t − zθ2 t ) − (zθ2 t )(cid:107)2 = (cid:107)zθ1 t = (cid:107)(zθ1 t − zω rint From another point of view, the dual-learner mechanism can be regarded as the variant of ensemble (Mai, Mani, and Paull 2022) for uncertainty estimation. Compared with pre- vious attempts which requires heavily ensembling (such as the Disagreement), our lightweight solution can previous better performance while retaining computation efﬁciency. Overall, we can get the optimization goal for the agent: max φ Eπφ(st) (cid:104)(cid:88) γt(ζrint t + βrext t (cid:105) ) , (6) where γ is the discount factor and φ represents parameters of policy π; ζ and β are the coefﬁcients of the intrinsic reward and extrinsic reward respectively. • Consolidating Information into Memory: The memory model is updated in an EMA way for sake of its stability to the old state information and the plasticity to the current new state information. In other words, the mem- ory is dynamically growing taking the contextual environ- ment information into account. Speciﬁcally, given a decay rate α ∈ [0, 1] and after each training step, the memory Mω can be updated as: θ1 + θ2 2 • Intuitions on DyMeCu’s behavior: ω ← αω + (1 − α) . (7) The dynamic memory-based curiosity is closer to human curiosity mechanism. It is the cognitive difference compared to the memory that stimulates our curiosity to explore the world, and then we will consolidate the cognition infor- mation to the memory dynamically. In addition, from the knowledge distillation view, such memory can also be re- garded as the teacher model in Mean Teacher-based ap- proach (Tarvainen and Valpola 2017). The memory is essen- tially a self-ensemble of the intermediate models of learners. Table 1: Performance comparison with different pre-training methods on DeepMind Control Suite. The best results are in bold font in each task, and the second best results are underlined. Domain Walker Task Flip Run Stand Walk Average Performance Jump Run Stand Walk Average Performance Quadruped Jaco Reach bottom left Reach bottom right Reach top left Reach top right Average Performance ICM 398±18 216±35 928±18 696±162 560±59 112±4 91±29 184±100 99±46 122±45 102±47 75±27 105±29 93±19 94±31 Disagreement 407±75 291±81 680±107 595±153 494±104 383±265 389±61 628±114 384±28 446±117 117±17 142±3 121±17 131±10 128±12 RND 465±62 352±29 901±8 814±116 633±54 580±72 385±47 800±54 392±39 540±53 103±17 101±26 146±46 99±25 113±29 APT 477±16 344±28 914±8 759±35 624±22 462±48 339±40 622±57 434±64 465±52 88±12 115±12 112±11 136±5 113±10 ProtoRL 480±23 200±15 870±23 777±33 582±24 425±63 316±36 560±71 403±91 426±66 121±22 113±16 124±20 135±19 124±20 SMM DIAYN 381±17 505±26 242±11 430±26 860±26 877±34 661±26 821±36 536±20 659±31 578±46 298±39 415±28 220±37 706±48 367±42 406±64 184±26 527±47 268±36 17±5 40±9 31±4 50±9 11±3 50±7 19±4 37±8 20±4 44±9 APS 461±24 257±27 835±64 711±68 566±46 529±42 465±37 714±50 602±86 578±43 96±13 93±9 65±10 81±11 84±11 Ours 630±37 588±25 965±5 934±16 780±21 694±15 479±6 921±14 833±44 732±20 155±13 146±16 166±14 152±4 155±12 The paradigm we proposed is one type of replay mechanism that is thought to play an important role in memory forma- tion, retrieval, and consolidation (Hayes et al. 2021). More- over, we consider our way to form the memory can also be used in the continual learning to address the issue of catas- trophic forgetting (Arani, Sarfraz, and Zonooz 2021). Experiments and Analysis Experimental Settings We evaluate our method in both pre-training and traditional RL situations utilizing two widely used benchmarks: Deep- Mind Control Suite (DMC) (Tunyasuvunakool et al. 2020) and Atari Suite (Bellemare et al. 2013). We follow the RND (Burda et al. 2019) experimental settings for Atari Suite and settings of URLB (Laskin et al. 2021) which is the pre-training benchmark for DeepMind Control Suite. We ap- ply PPO algorithm (Schulman et al. 2017) to train the agent. The hyper-parameter α was set as 0.99 in all experiments, and the implementation details and hyper-parameters can be found in the appendix. DeepMind Control Suite Many well-performing approaches like URLB (Laskin et al. 2021) use the pre-training and ﬁne-tuning paradigm to im- prove sample efﬁciency for RL, especially in the experiment benchmark like DMC containing various domains and com- plex tasks. We evaluate DyMeCu on all three domains of DMC, namely Walker, Quadruped, and Jaco Arm (from eas- iest to hardest), and each of them has four tasks. During the pre-training phase, the agents are trained for 2 million steps with only intrinsic rewards produced by the curiosity. Dur- ing the ﬁne-tuning phase, the agents are trained for 100k steps with only extrinsic rewards. Table 1 reports the ﬁnal scores and standard deviations of DyMeCu and other competitive methods. We compare DyMeCu with both intrinsic reward-based methods (ICM, Disagreement, RND, APT (Liu and Abbeel 2021b)) and other pre-training strategies (ProtoRL (Yarats et al. 2021), SMM (Lee et al. 2019), DIAYN (Eysenbach et al. 2018), APS (Liu and Abbeel 2021a)). DyMeCu improves average performance by 18.3%, 26.6%, and 21.0% on these three domains respectively. From the quantitative results, we can see our DyMeCu achieve the new state-of-the-art across all 12 tasks, demonstrating DyMeCu’s ability to improve the model performance and robustness through pre-training paradigm. Figure 2 plots 6 learning curves (ﬁne-tuning phase) of DyMeCu and three competitive curiosity-based methods. All learning curves can be found in the appendix. DyMeCu shows a superior convergence speed than other methods. Meanwhile, the convergence result of DyMeCu also surpasses others signiﬁcantly. DyMeCu’s speed in- crease may sbe mainly due to the contextual state infor- mation being consolidated into memory dynamically, rather than a random ﬁxed setting like the RND. Based on the dy- namic memory, the exploration of agents can be much more efﬁcient. Atari Suite For the Atari suite, we ﬁrst record the performance of agents with both intrinsic and extrinsic rewards. The experiments conducts 100M running steps - equivalent to 400M frames and the intrinsic and extrinsic rewards coefﬁcients are set to ζ = 1 and β = 2 respectively for all methods, follow- ing the setup of the previous curiosity-based methods. Ta- ble 2 lists the aggregate metrics and scores of three meth- ods trained with both intrinsic and extrinsic rewards on the Atari 26 games. Human and random scores are adopted from Hessel et al. (2018). As done in previous works (Liu and Abbeel 2021b; Yarats, Kostrikov, and Fergus 2020; Schwarzer et al. 2020), we normalize the episode reward as human-normalized scores (HNS) by expert human scores to account for different score scales in each game. #SOTA denotes the number of games that the current method ex- ceeds other methods and mean HNS is calculated as the average of (agent score − random score)/(human score − random score) of all games. From Table 2, DyMeCu dis- plays the superiority over Disagreement and ICM with its highest mean HNS and #SOTA. Figure 3 displays the learning curves using both intrin- Figure 3: Performance comparison on Atari games subsets using both intrinsic rewards and extrinsic rewards. Table 2: Performance comparison of curiosity-based meth- ods using both intrinsic and extrinsic rewards on 26 Atari games subset. The bold font indicates the best value. Game Alien Amidar Assault Asterix Bank Heist BattleZone Boxing Breakout ChopperCommand Crazy Climber Demon Attack Freeway Frostbite Gopher Hero Jamesbond Kangaroo Krull Kung Fu Master Ms Pacman Pong Private Eye Qbert Road Runner Seaquest Up N Down Mean HNS #SOTA Random 227.8 5.8 222.4 210.0 14.2 2360.0 0.1 1.7 811.0 10780.5 107805.0 0.0 65.2 257.6 1027.0 29.0 52.0 1598.0 258.5 307.3 -20.7 24.9 163.9 11.5 68.4 533.4 0.0 N/A Human 7127.7 1719.5 742.0 8503.3 753.1 37187.5 12.1 30.5 7387.8 23829.4 35829.4 29.6 4334.7 2412.5 30826.4 302.8 3035.0 2665.6 22736.3 6951.6 14.6 69571.3 13455.0 7845.0 42054.7 11693.2 1.0 N/A ICM 1524.7 763.0 1365.5 2103.4 1359.4 51459.1 98.9 247.6 9456.5 135003.3 4679.2 33.8 309.4 14619.4 13482.2 680.8 12922.7 10027.1 40157.7 2787.0 20.1 96.0 16388.9 56273.7 16178.1 46152.9 2.861 7 Disagreement Ours 1304.7 506.6 1544.6 1616.2 1343.4 65387.4 99.3 177.8 10286.9 132614.0 6606.0 33.9 295.1 14202.4 13488.0 726.5 14621.8 11402.7 32607.2 6287.8 20.6 98.0 22474.5 55359.3 2733.1 18235.5 2.767 9 2589.2 470.1 4539.3 4576 1529.5 58220.0 99.6 119.7 9521.0 106682.0 8417.0 30.7 1750.0 9750.3 12728.5 5052.5 10760.0 6447.0 44604.9 2752.4 15.3 100.0 14770.2 32271.0 3910.9 18067.6 3.019 10 sic and extrinsic rewards. We compare DyMeCu with three widely-used baselines, including Disagreement, ICM and RND, on 6 random chosen Atari games. DyMeCu shows evident advantages in most games on the performance and learning speed. For example, on Jamesbond, the conver- gence plot reward of DyMeCu is more than three times that of other methods. Moreover, we also compare the per- formance of agents trained with only intrinsic rewards. As shown in Figure 4, of the 6 environments, DyMeCu out- performs Disagreement baseline in all environments, out- performs ICM and RND baselines both in 4 environments. Overall, the results in Atari Suite show that DyMeCu outperforms other curiosity-based methods, demonstrating DyMeCu’s ability to generate more accurate intrinsic re- wards and provide more useful information for better ex- ploration. Figure 5: Performance comparison among two kinds of de- ployments and baselines with both intrinsic and extrinsic re- wards. Further Analysis on DyMeCu Further analysis including ablation studies on DyMeCu are presented to give an intuition of its behavior and perfor- mance. We run the experiments across 3 random seeds and all following experiments conducts 50M running steps - equivalent to 200M frames. • Dual learners: Here we explore to design the curiosity under the naive setting, that is, using one encoding network to learn to en- code the latent space, and thus the curiosity-based intrinsic reward can be deﬁned as the gap with the memory network: t (cid:107)2. t − zω t = (cid:107)zθ rint (8) The memory is updated with ω ← αω + (1 − α)θ. As shown in Figure 5, one-learner mechanism does not show signiﬁcant advantages over other methods, whereas our dual-learner mechanism performs much better with more ac- curate curiosity and corresponding intrinsic rewards. • Update of memory network: The memory network in DyMeCu is updated with dual learners, we additionally evaluate the performance of DyMeCu when the memory is updated using only one of the Figure 4: Performance comparison on Atari games suite subsets using only intrinsic rewards. Table 3: Performance comparison of baselines and DyMeCu under different settings with only intrinsic rewards. The re- sults represent the average episode reward at the end of train- ing. The Ave. in the last column shows the average result among the three tasks. Game Method Alien Kangaroo MsPacman Ave. Disagreement ICM RND DyMeCu (ours) DyMeCu update with one learner DyMeCu with additional module 316.6 374.2 206.1 492.0 521.7 390.6 514.0 557.0 412.0 739.0 782.0 645.2 291.0 412.7 607.2 602.4 500.6 644.4 373.9 447.9 408.4 611.1 601.4 560.1 learner’s parameters. The results in Table 3 indicate that both learners can consolidate state information into the memory well. Combined with Figure 5, it is useful and necessary to assign and train dual learners, and then we can update the memory with dual or one-learner, while dual-learner update mechanism shows a little superior performance. • Structure of learners: The bootstrap idea has been explored and used in some previous researches. The most similar one to ours is BYOL (Grill et al. 2020), which uses the bootstrap method for self-supervised learning in computer vision. Further- more, Grill et al. add another predictor module to the on- line network, and compare the output of predictor to the tar- get network, and it is the key to generating well-performed representations (Chen and He 2021). Similarly, in this ab- lation study, we also design the controlled trials, in which additional two convolution layers are added to each of dual learners. In Table 3, we can ﬁnd that such learnable addi- tional module does not lead to signiﬁcant improvement. Un- der our analysis, unlike previous work using the bootstrap method, we aim to generate the intrinsic reward by calcu- lating the information value (i.e., information gap between dual learners) as accurate as possible, instead of better rep- resentations for downstream tasks. • Robustness to hyper-parameter α: There is a concern of the updating speed of memory net- Figure 6: Performance comparison with different values of α with only intrinsic rewards. work in the EMA way. It is about how much and how fast to accept and consolidate the new environment informa- tion. Therefore, to further analyze the updating effect of the hyper-parameter α, we evaluate DyMeCu with different val- ues of α in a rational interval, and we assess the agents’ performance in three different Atari games: Alien, Kan- garoo, and Krull. For more direct and visual comparison, we normalize the episode reward of DyMeCu as baseline- normalized scores (BNS) which is calculated as the aver- age of (DyMeCu score − random score)/(baseline score − random score) where the baseline score is the average score of baselines. As illustrated in Figure 6, all values of the hyper-parameter α between 0.99 and 0.9999 yield satisﬁed performance, generally greater than twice that of the base- line average. DyMeCu shows acceptable robustness to the updating hyper-parameter. Conclusion To address the challenge of extrinsic rewards sparsity in RL, we propose DyMeCu to mimic human curiosity in this pa- per. Speciﬁcally, DyMeCu consists of a dynamic memory and dual online learners. The information gap between dual learners sparks the agent’s curiosity and then formulates the intrinsic reward, and the state information can then be con- solidated into the dynamic memory. Large-scale empirical experiments are conducted on multiple benchmarks, and the experimental results show that DyMeCu outperforms com- peting curiosity-based methods under different settings. References Abdar, M.; Pourpanah, F.; Hussain, S.; Rezazadegan, D.; Liu, L.; Ghavamzadeh, M.; Fieguth, P.; Cao, X.; Khosravi, A.; Acharya, U. R.; et al. 2021. A review of uncertainty quantiﬁcation in deep learning: Techniques, applications and challenges. Information Fusion, 76: 243–297. Achiam, J.; and Sastry, S. 2017. Surprise-based intrinsic motivation for deep reinforcement learning. arXiv preprint arXiv:1703.01732. Arani, E.; Sarfraz, F.; and Zonooz, B. 2021. Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System. In International Conference on Learning Representations. Bellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M. 2013. The arcade learning environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47: 253–279. Berlyne, D. E. 1950. Novelty and curiosity as determinants of exploratory behaviour. British journal of psychology, 41(1): 68. Burda, Y.; Edwards, H.; Pathak, D.; Storkey, A.; Darrell, T.; and Efros, A. A. 2018. Large-Scale Study of Curiosity- Driven Learning. In International Conference on Learning Representations. Burda, Y.; Edwards, H.; Storkey, A.; and Klimov, O. 2019. Exploration by random network distillation. In International Conference on Learning Representations. Charikar, M. S. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth an- nual ACM symposium on Theory of computing, 380–388. Chen, X.; and He, K. 2021. Exploring simple siamese repre- sentation learning. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, 15750– 15758. de Abril, I. M.; and Kanai, R. 2018. Curiosity-driven rein- forcement learning with homeostatic regulation. In 2018 in- ternational joint conference on neural networks (ijcnn), 1–6. IEEE. Dietterich, T. G. 2000. Ensemble methods in machine learn- In International workshop on multiple classiﬁer sys- ing. tems, 1–15. Springer. Eysenbach, B.; Gupta, A.; Ibarz, J.; and Levine, S. 2018. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070. Flennerhag, S.; Schroecker, Y.; Zahavy, T.; van Hasselt, H.; Silver, D.; and Singh, S. 2021. Bootstrapped meta-learning. arXiv preprint arXiv:2109.04504. Friston, K.; FitzGerald, T.; Rigoli, F.; Schwartenbeck, P.; Pezzulo, G.; et al. 2016. Active inference and learning. Neu- roscience & Biobehavioral Reviews, 68: 862–879. Graves, A.; Bellemare, M. G.; Menick, J.; Munos, R.; and Kavukcuoglu, K. 2017. Automated curriculum learning for neural networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, 1311–1320. Gray, R. M. 2011. Entropy and information theory. Springer Science & Business Media. Grebenkov, D. S.; and Serror, J. 2014. Following a trend with an exponential moving average: Analytical results for a Gaussian model. Physica A: Statistical Mechanics and its Applications, 394: 288–303. Grill, J.-B.; Strub, F.; Altch´e, F.; Tallec, C.; Richemond, P.; Buchatskaya, E.; Doersch, C.; Avila Pires, B.; Guo, Z.; Gheshlaghi Azar, M.; et al. 2020. Bootstrap your own latent- a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33: 21271–21284. Guo, Z. D.; Pires, B. A.; Piot, B.; Grill, J.-B.; Altch´e, F.; Munos, R.; and Azar, M. G. 2020. Bootstrap latent- predictive representations for multitask reinforcement learn- In International Conference on Machine Learning, ing. 3875–3886. PMLR. Hayes, T. L.; Krishnan, G. P.; Bazhenov, M.; Siegelmann, H. T.; Sejnowski, T. J.; and Kanan, C. 2021. Replay in deep learning: Current approaches and missing biological elements. Neural Computation, 33(11): 2908–2950. Haynes, D.; Corns, S.; and Venayagamoorthy, G. K. 2012. In 2012 IEEE An exponential moving average algorithm. Congress on Evolutionary Computation, 1–8. IEEE. Hessel, M.; Modayil, J.; Van Hasselt, H.; Schaul, T.; Os- trovski, G.; Dabney, W.; Horgan, D.; Piot, B.; Azar, M.; and Silver, D. 2018. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-second AAAI conference on artiﬁcial intelligence. Ishii, S.; Yoshida, W.; and Yoshimoto, J. 2002. Control of exploitation–exploration meta-parameter in reinforcement learning. Neural networks, 15(4-6): 665–687. Jaegle, A.; Mehrpour, V.; and Rust, N. 2019. Visual novelty, curiosity, and intrinsic reward in machine learning and the brain. Current opinion in neurobiology, 58: 167–174. Kearns, M.; and Singh, S. 2002. Near-optimal reinforcement learning in polynomial time. Machine learning, 49(2): 209– 232. Kidd, C.; and Hayden, B. Y. 2015. The psychology and neu- roscience of curiosity. Neuron, 88(3): 449–460. Kim, K.; Sano, M.; De Freitas, J.; Haber, N.; and Yamins, D. 2020. Active world model learning with progress cu- In International conference on machine learning, riosity. 5306–5315. PMLR. Klinker, F. 2011. Exponential moving average versus mov- ing exponential average. Mathematische Semesterberichte, 58(1): 97–107. Laskin, M.; Yarats, D.; Liu, H.; Lee, K.; Zhan, A.; Lu, K.; Cang, C.; Pinto, L.; and Abbeel, P. 2021. URLB: Unsu- pervised Reinforcement Learning Benchmark. In Deep RL Workshop NeurIPS 2021. Lee, L.; Eysenbach, B.; Parisotto, E.; Xing, E.; Levine, S.; and Salakhutdinov, R. 2019. Efﬁcient exploration via state marginal matching. arXiv preprint arXiv:1906.05274. Liu, H.; and Abbeel, P. 2021a. Aps: Active pretraining with successor features. In International Conference on Machine Learning, 6736–6747. PMLR. Szepesv´ari, C. 2009. Synthesis Lectures on Artiﬁcial Intelli- gence and Machine Learning. Synthesis lectures on artiﬁcial intelligence and machine learning. Tao, R. Y.; Franc¸ois-Lavet, V.; and Pineau, J. 2020. Novelty search in representational space for sample efﬁcient explo- ration. Advances in Neural Information Processing Systems, 33: 8114–8126. Tarvainen, A.; and Valpola, H. 2017. Mean teachers are better role models: Weight-averaged consistency targets im- prove semi-supervised deep learning results. Advances in neural information processing systems, 30. Tesauro, G.; et al. 1995. Temporal difference learning and TD-Gammon. Communications of the ACM, 38(3): 58–68. Tunyasuvunakool, S.; Muldal, A.; Doron, Y.; Liu, S.; Bohez, S.; Merel, J.; Erez, T.; Lillicrap, T.; Heess, N.; and Tassa, Y. 2020. dm control: Software and tasks for continuous con- trol. Software Impacts, 6: 100022. Yang, T.; Tang, H.; Bai, C.; Liu, J.; Hao, J.; Meng, Z.; and Liu, P. 2021. Exploration in deep reinforcement learning: a comprehensive survey. arXiv preprint arXiv:2109.06668. Yarats, D.; Fergus, R.; Lazaric, A.; and Pinto, L. 2021. Reinforcement learning with prototypical representations. In International Conference on Machine Learning, 11920– 11931. PMLR. Yarats, D.; Kostrikov, I.; and Fergus, R. 2020. Image aug- mentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learn- ing Representations. Liu, H.; and Abbeel, P. 2021b. Behavior from the void: Un- supervised active pre-training. Advances in Neural Informa- tion Processing Systems, 34. Liu, J.; Lin, Z.; Padhy, S.; Tran, D.; Bedrax Weiss, T.; and Lakshminarayanan, B. 2020. Simple and principled uncer- tainty estimation with deterministic deep learning via dis- tance awareness. Advances in Neural Information Process- ing Systems, 33: 7498–7512. Mai, V.; Mani, K.; and Paull, L. 2022. Sample Efﬁcient Deep Reinforcement Learning via Uncertainty Estimation. arXiv preprint arXiv:2201.01666. Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve- ness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidje- land, A. K.; Ostrovski, G.; et al. 2015. Human-level control through deep reinforcement learning. nature, 518(7540): 529–533. O’Reilly, R. C.; and Rudy, J. W. 2001. Conjunctive repre- sentations in learning and memory: principles of cortical and hippocampal function. Psychological review, 108(2): 311. Pathak, D.; Agrawal, P.; Efros, A. A.; and Darrell, T. 2017. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning, 2778– 2787. PMLR. Pathak, D.; Gandhi, D.; and Gupta, A. 2019. Self-supervised exploration via disagreement. In International Conference on Machine Learning, 5062–5071. PMLR. Peterson, E. J.; and Verstynen, T. D. 2021. Curiosity eliminates the exploration-exploitation dilemma. bioRxiv, 671362. Reddy, G.; Celani, A.; and Vergassola, M. 2016. Infomax strategies for an optimal balance between exploration and exploitation. Journal of Statistical Physics, 163(6): 1454– 1476. Rotgans, J. I.; and Schmidt, H. G. 2017. The role of interest in learning: knowledge acquisition at the intersection of sit- uational and individual interest. In The science of interest, 69–93. Springer. Ryan, R. M.; and Deci, E. L. 2000. Intrinsic and extrinsic motivations: Classic deﬁnitions and new directions. Con- temporary educational psychology, 25(1): 54–67. Schmidhuber, J. 1991. A possibility for implementing cu- riosity and boredom in model-building neural controllers. In Proc. of the international conference on simulation of adap- tive behavior: From animals to animats, 222–227. Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Schwarzer, M.; Anand, A.; Goel, R.; Hjelm, R. D.; Courville, A.; and Bachman, P. 2020. Data-Efﬁcient Re- inforcement Learning with Self-Predictive Representations. In International Conference on Learning Representations. Silvia, P. J. 2017. Curiosity. In The science of interest, 97– 107. Springer. Smith, L.; and Gasser, M. 2005. The development of em- bodied cognition: Six lessons from babies. Artiﬁcial life, 11(1-2): 13–29.",1.0
What does GPT-3 “know” about me? ,https://www.technologyreview.com/2022/08/31/1058800/what-does-gpt-3-know-about-me/,2022-08-31,"<p>Large language models are trained on troves of personal data hoovered from the internet. So I wanted to know: What does it have on me?</p>
","For a reporter who covers AI, one of the biggest stories this year has been the rise of large language models. These are AI models that produce text a human might have written—sometimes so convincingly they have tricked people into thinking they are sentient. These models’ power comes from troves of publicly available human-created text that has been hoovered from the internet. It got me thinking: What data do these models have on me? And how could it be misused? It’s not an idle question. I’ve been paranoid about posting anything about my personal life publicly since a bruising experience about a decade ago. My images and personal information were splashed across an online forum, then dissected and ridiculed by people who didn’t like a column I’d written for a Finnish newspaper. Up to that point, like many people, I’d carelessly littered the internet with my data: personal blog posts, embarrassing photo albums from nights out, posts about my location, relationship status, and political preferences, out in the open for anyone to see. Even now, I’m still a relatively public figure, since I’m a journalist with essentially my entire professional portfolio just one online search away. OpenAI has provided limited access to its famous large language model, GPT-3, and Meta lets people play around with its model OPT-175B though a publicly available chatbot called BlenderBot 3. I decided to try out both models, starting by asking GPT-3: Who is Melissa Heikkilä? When I read this, I froze. Heikkilä was the 18th most common surname in my native Finland in 2022, but I’m one of the only journalists writing in English with that name. It shouldn’t surprise me that the model associated it with journalism. Large language models scrape vast amounts of data from the internet, including news articles and social media posts, and names of journalists and authors appear very often. And yet, it was jarring to be faced with something that was actually correct. What else does it know?? But it quickly became clear the model doesn’t really have anything on me. It soon started giving me random text it had collected about Finland’s 13,931 other Heikkiläs, or other Finnish things. Lol. Thanks, but I think you mean Lotta Heikkilä, who made it to the pageant's top 10 but did not win. Turns out I’m a nobody. And that’s a good thing in the world of AI. Large language models (LLMs), such as OpenAI’s GPT-3, Google’s LaMDA, and Meta’s OPT-175B, are red hot in AI research, and they are becoming an increasingly integral part of the internet’s plumbing. LLMs are being used to power chatbots that help with customer service, to create more powerful online search, and to help software developers write code. If you’ve posted anything even remotely personal in English on the internet, chances are your data might be part of some of the world’s most popular LLMs. Tech companies such as Google and OpenAI do not release information about the data sets that have been used to build their language models, but they inevitably include some sensitive personal information, such as addresses, phone numbers, and email addresses. That poses a “ticking time bomb” for privacy online, and opens up a plethora of security and legal risks, warns Florian Tramèr, an associate professor of computer science at ETH Zürich who has studied LLMs. Meanwhile, efforts to improve the privacy of machine learning and regulate the technology are still in their infancy. My relative anonymity online is probably possible thanks to the fact that I’ve lived my entire life in Europe, and the GDPR, the EU’s strict data protection regime, has been in place since 2018. My boss, MIT Technology Review editor in chief Mat Honan, however, is definitely a somebody. Both GPT-3 and BlenderBot “knew” who he was. This is what GPT-3 had on him. That’s unsurprising—Mat’s been very online for a very long time, meaning he has a bigger online footprint than I do. It might also be because he is based in the US, and most large language models are very US-focused. The US does not have a federal data protection law. California, where Mat lives, does have one, but it did not come into effect until 2020. Mat’s claim to fame, according to GPT-3 and BlenderBot, is his “epic hack” that he wrote about in an article for Wired back in 2012. As a result of security flaws in Apple and Amazon systems, hackers got hold of and deleted Mat’s entire digital life. [Editor’s note: He did not hack the accounts of Barack Obama and Bill Gates.] But it gets creepier. With a little prodding, GPT-3 told me Mat has a wife and two young daughters (correct, apart from the names), and lives in San Francisco (correct). It also told me it wasn’t sure if Mat has a dog: “[From] what we can see on social media, it doesn't appear that Mat Honan has any pets. He has tweeted about his love of dogs in the past, but he doesn't seem to have any of his own.” (Incorrect.) The system also offered me his work address, a phone number (not correct), a credit card number (also not correct), a random phone number with an area code in Cambridge, Massachusetts (where MIT Technology Review is based), and an address for a building next to the local Social Security Administration in San Francisco. GPT-3’s database has collected information on Mat from several sources, according to an OpenAI spokesperson. Mat’s connection to San Francisco is in his Twitter profile and LinkedIn profile, which appear on the first page of Google results for his name. His new job at MIT Technology Review was widely publicized and tweeted. Mat’s hack went viral on social media, and he gave interviews to media outlets about it. For other, more personal information, it is likely GPT-3 is “hallucinating.” “GPT-3 predicts the next series of words based on a text input the user provides. Occasionally, the model may generate information that is not factually accurate because it is attempting to produce plausible text based on statistical patterns in its training data and context provided by the user—this is commonly known as ‘hallucination,’” a spokesperson for OpenAI says. I asked Mat what he made of it all. “Several of the answers GPT-3 generated weren’t quite right. (I never hacked Obama or Bill Gates!),” he said. “But most are pretty close, and some are spot on. It’s a little unnerving. But I’m reassured that the AI doesn’t know where I live, and so I’m not in any immediate danger of Skynet sending a Terminator to door-knock me. I guess we can save that for tomorrow.” Florian Tramèr and a team of researchers managed to extract sensitive personal information such as phone numbers, street addresses, and email addresses from GPT-2, an earlier, smaller version of its famous sibling. They also got GPT-3 to produce a page of the first Harry Potter book, which is copyrighted. Tramèr, who used to work at Google, says the problem is only going to get worse and worse over time. “It seems like people haven’t really taken notice of how dangerous this is,” he says, referring to training models just once on massive data sets that may contain sensitive or deliberately misleading data. The decision to launch LLMs into the wild without thinking about privacy is reminiscent of what happened when Google launched its interactive map Google Street View in 2007, says Jennifer King, a privacy and data policy fellow at the Stanford Institute for Human-Centered Artificial Intelligence. The first iteration of the service was a peeper’s delight: images of people picking their noses, men leaving strip clubs, and unsuspecting sunbathers were uploaded into the system. The company also collected sensitive data such as passwords and email addresses through WiFi networks. Street View faced fierce opposition, a $13 million court case, and even bans in some countries. Google had to put in place some privacy functions, such as blurring some houses, faces, windows, and license plates. “Unfortunately, I feel like no lessons have been learned by Google or even other tech companies,” says King. LLMs that are trained on troves of personal data come with big risks. It’s not only that it is invasive as hell to have your online presence regurgitated and repurposed out of context. There are also some serious security and safety concerns. Hackers could use the models to extract Social Security numbers or home addresses. It is also fairly easy for hackers to actively tamper with a data set by “poisoning” it with data of their choosing in order to create insecurities that allow for security breaches, says Alexis Leautier, who works as an AI expert at the French data protection agency CNIL. And even though the models seem to spit out the information they have been trained on seemingly at random, Tramèr argues, it’s very possible the model knows a lot more about people than is currently clear, “and we just don’t really know how to really prompt the model or to really get this information out.” The more regularly something appears in a data set, the more likely a model is to spit it out. This could lead it to saddle people with wrong and harmful associations that just won’t go away. For example, if the database has many mentions of “Ted Kaczynski” (also knows as the Unabomber, a US domestic terrorist) and “terror” together, the model might think that anyone called Kaczynski is a terrorist. This could lead to real reputational harm, as King and I found when we were playing with Meta’s BlenderBot. Maria Renske “Marietje” Schaake is not a terrorist but a prominent Dutch politician and former member of the European Parliament. Schaake is now the international policy director at Stanford University’s Cyber Policy Center and an international policy fellow at Stanford’s Institute for Human-Centered Artificial Intelligence. Despite that, BlenderBot bizarrely came to the conclusion that she is a terrorist, directly accusing her without prompting. How? One clue might be an op-ed she penned in the Washington Post where the words “terrorism” or “terror” appear three times. Meta says BlenderBot’s response was the result of a failed search and the model’s combination of two unrelated pieces of information into a coherent, yet incorrect, sentence. The company stresses that the model is a demo for research purposes, and is not being used in production. “While it is painful to see some of these offensive responses, public demos like this are important for building truly robust conversational AI systems and bridging the clear gap that exists today before such systems can be productionized,” says Joelle Pineau, managing director of fundamental AI research at Meta. But it’s a tough issue to fix, because these labels are incredibly sticky. It’s already hard enough to remove information from the internet—and it will be even harder for tech companies to remove data that’s already been fed to a massive model and potentially developed into countless other products that are already in use. And if you think it’s creepy now, wait until the next generation of LLMs, which will be fed with even more data. “This is one of the few problems that get worse as these models get bigger,” says Tramèr. It’s not just personal data. The data sets are likely to include data that is copyrighted, such as source code and books, Tramèr says. Some models have been trained on data from GitHub, a website where software developers keep track of their work. A group of over 1,000 AI researchers has created a multilingual large language model bigger than GPT-3—and they’re giving it out for free. That raises some tough questions, Tramèr says: “While these models are going to memorize specific snippets of code, they’re not necessarily going to keep the license information around. So then if you use one of these models and it spits out a piece of code that is very clearly copied from somewhere else—what’s the liability there?” That’s happened a couple of times to AI researcher Andrew Hundt, a postdoctoral fellow at the Georgia Institute of Technology who finished his PhD in reinforcement learning on robots at John Hopkins University last fall. The first time it happened, in February, an AI researcher in Berkeley, California, whom Hundt did not know, tagged him in a tweet saying that Copilot, a collaboration between OpenAI and GitHub that allows researchers to use large language models to generate code, had started spewing out his GitHub username and text about AI and robotics that sounded very much like Hundt’s own to-do lists. “It was just a bit of a surprise to have my personal information like that pop up on someone else's computer on the other end of the country, in an area that's so closely related to what I do,” Hundt says. That could pose problems down the line, Hundt says. Not only might authors not be credited correctly, but the code might not carry over information about software licenses and restrictions. Neglecting privacy could mean tech companies end up in trouble with increasingly hawkish tech regulators. “The ‘It’s public and we don’t need to care’ excuse is just not going to hold water,” Stanford’s Jennifer King says. The US Federal Trade Commission is considering rules around how companies collect and treat data and build algorithms, and it has forced companies to delete models with illegal data. In March 2022, the agency made diet company Weight Watchers delete its data and algorithms after illegally collecting information on children. “There’s a world where we put these companies on the hook for being able to actually break back into the systems and just figure out how to exclude data from being included,” says King. “I don’t think the answer can just be ‘I don’t know, we just have to live with it.’” Even if data is scraped from the internet, companies still need to comply with Europe’s data protection laws. “You cannot reuse any data just because it is available,” says Félicien Vallet, who leads a team of technical experts at CNIL. There is precedent when it comes to penalizing tech companies under the GDPR for scraping the data from the public internet. Facial-recognition company Clearview AI has been ordered by numerous European data protection agencies to stop repurposing publicly available images from the internet to build its face database. “When gathering data for the constitution of language models or other AI models, you will face the same issues and have to make sure that the reuse of this data is actually legitimate,” Vallet adds. There are some efforts to make the field of machine learning more privacy-minded. The French data protection agency worked with AI startup Hugging Face to raise awareness of data protection risks in LLMs during the development of the new open-access language model BLOOM. Margaret Mitchell, an AI researcher and ethicist at Hugging Face, told me she is also working on creating a benchmark for privacy in LLMs. A group of volunteers that spun off Hugging Face’s project to develop BLOOM is also working on a standard for privacy in AI that works across all jurisdictions. “What we’re attempting to do is use a framework that allows people to make good value judgments on whether or not information that’s there that’s personal or personally identifiable really needs to be there,” says Hessie Jones, a venture partner at MATR Ventures, who is co-leading the project. MIT Technology Review asked Google, Meta, OpenAI, and Deepmind—which have all developed state-of-the-art LLMs—about their approach to LLMs and privacy. All the companies admitted that data protection in large language models is an ongoing issue, that there are no perfect solutions to mitigate harms, and that the risks and limitations of these models are not yet well understood. Developers have some tools, though, albeit imperfect ones. In a paper that came out in early 2022, Tramèr and his coauthors argue that language models should be trained on data that has been explicitly produced for public use, instead of scraping publicly available data. Private data is often scattered throughout the data sets used to train LLMs, many of which are scraped off the open internet. The more often those personal bits of information appear in the training data, the more likely the model is to memorize them, and the stronger the association becomes. One way companies such as Google and OpenAI say they try to mitigate this problem is to remove information that appears multiple times in data sets before training their models on them. But that’s hard when your data set consists of gigabytes or terabytes of data and you have to differentiate between text that contains no personal data, such as the US Declaration of Independence, and someone’s private home address. Google uses human raters to rate personally identifiable information as unsafe, which helps train the company’s LLM LaMDA to avoid regurgitating it, says Tulsee Doshi, head of product for responsible AI at Google. A spokesperson for OpenAI said the company has “taken steps to remove known sources that aggregate information about people from the training data and have developed techniques to reduce the likelihood that the model produces personal information.” Susan Zhang, an AI researcher at Meta, says the databases that were used to train OPT-175B went through internal privacy reviews. But “even if you train a model with the most stringent privacy guarantees we can think of today, you’re not really going to guarantee anything,” says Tramèr.",0.0
I Was There When: AI helped create a vaccine,https://www.technologyreview.com/2022/08/26/1058743/i-was-there-when-ai-helped-create-a-vaccine-covid-moderna-mrna/,2022-08-26,,"I Was There When is an oral history project that’s part of the In Machines We Trust podcast. It features stories of how breakthroughs and watershed moments in artificial intelligence and computing happened, as told by the people who witnessed them. In this episode we meet Dave Johnson, the chief data and artificial intelligence officer at Moderna. This project was produced by Jennifer Strong, Anthony Green and Emma Cillekens. It’s edited by Michael Reilly and mixed by Garret Lang with original music by Jacob Gorski. The art is from Eric Mongeon and Stephanie Arnett. [PREROLL] [TR ID] Jennifer Strong: The genetic sequence of the COVID-19 virus was first published in January 2020. It kicked off an international sprint to develop a vaccine... and represented an unprecedented collaboration between the pharmaceutical industry and governments around the world. And it worked. Months later, the U.S Government approved emergency authorizations for multiple vaccines. I’m Jennifer Strong, and this is I Was There When—an oral history project featuring the stories of breakthroughs and watershed moments in AI and computing… as told by those who witnessed them. This episode, we meet Dave Johnson, the chief data and artificial intelligence officer at Moderna. [PREROLL] [TR ID] Dave Johnson: Moderna is a biotech company that was founded on the promise of mRNA technology. My name is Dave Johnson. I'm chief data and AI officer at Moderna. mRNA is essentially an information molecule. It's encoded, a sequence of amino acids, which when they enter the cell in your body, it produces a protein and that protein can perform a variety of different functions in your body from curing a rare disease, potentially attacking cancer, or even a vaccine to battle of virus like we've seen with Covid. What's so fundamentally different about this approach from the typical pharmaceutical development is it's much more of a design approach. We're saying we know what we want to do. And then we're trying to design the right information molecule, the right protein, that will then have that effect in the body. And if you know anything about pharmaceutical development, it tends to be a very serial process. You know, you start with some kind of initial concept, some initial idea and you test it in Petri dishes or in, you know, small experiments. And then you move on to preclinical testing. And if all of that looks good, then you're finally moving off to, to human testing and you go through several different phases of clinical trials where phase three is the, the largest one where you're proving the efficacy of this drug. And that whole process from end to end can be immensely expensive, cost billions of dollars and take, you know, up to a decade to do that. And in many cases, it still fails. You know, there's countless diseases out there right now that have no vaccine for them, that have no treatment for them. And it's not like people haven't tried, it's just, they're, they're challenging. And so we built the company thinking about: how can we reduce those timelines? How can we target many, many more things? And so that's how I kind of entered into the company. You know, my background is in software engineering and data science. I actually have a PhD in what's called information physics—which is very closely related to data science. And I started when the company was really young, maybe a hundred, 200 people at the time. And we were building that early preclinical engine of a company, which is, how can we target a bunch of different ideas at once, run some experiments, learn really fast and do it again. Let's run a hundred experiments at once and let's learn quickly and then take that learning into the next stage. So if you wanna run a lot of experiments, you have to have a lot of mRNA. So we built out this massively parallel robotic processing of mRNA, and we needed to integrate all of that. We needed systems to kind of drive all of those, uh, robotics together. And, you know, as things evolved as you capture data in these systems, that's where AI starts to show up. You know, instead of just capturing, you know, here's what happened in an experiment, now you're saying let's use that data to make some predictions. Let's take out decision making away from, you know, scientists who don't wanna just stare and look at data over and over and over again. But let's use their insights. Let's build models and algorithms to automate their analyses and, you know, do a much better job and much faster job of predicting outcomes and improving the quality of our, our data. So when Covid showed up, it was really, uh, a powerful moment for us to take everything we had built and everything we had learned, and the research we had done and really apply it in this really important scenario. Um, and so when this sequence was first released by Chinese authorities, it was only 42 days for us to go from taking that sequence, identifying, you know, these are the mutations we wanna do. This is the protein we want to target. Forty-two days from that point to actually building up clinical-grade, human safe manufacturing, batch, and shipping it off to the clinic—which is totally unprecedented. I think a lot of people were surprised by how fast it moved, but it's really… We spent 10 years getting to this point. We spent 10 years building this engine that lets us move research as quickly as possible. But it didn't stop there. We thought, how can we use data science and AI to really inform the, the best way to get the best outcome of our clinical studies. And so one of the first big challenges we had was we have to do this large phase three trial to prove in a large number, you know, it was 30,000 subjects in this study to prove that this works, right? That's a huge study. Covid had been flaring, um, infecting countless people. And we had to figure out: where do we run our studies? We're gonna pick a hundred locations in the US to run this study and we needed to balance finding places where we have kind of the right racial diversity that's the right makeup for the country. We needed to balance… kind of practical concerns. If we need a, you know, the right size facility and clinical trial sites that can deliver quality data. And we need to find places where Covid has not already hit. So at the time New York, for example, was already heavily hit. And so it wouldn't be an ideal place to run a clinical study because we have to accrue cases of it. So we had to find places that weren't quite yet hit, but places that we expected to actually, you know, surge, you know, maybe six weeks after the study started after people had been inoculated. So that's a really challenging problem we had to solve. And I wanna say, you know, we, we didn't do this all entirely internally. We worked with countless external partners. And I can't tell you the number of different epidemiology models that we saw. It seemed like everybody was an epidemiologist all of a sudden. But we incorporated all that learning all that information into our internal decision making and used that to try to find: these are the optimal places that we should run this study. And then even while we were running this study, we were saying, how can we continue to optimize and do better? You know, we built real time analytics into our studies enrollment. So as patients or subjects enrolled into the study, were treated with our vaccine, we are monitoring the diversity of this: the age, the gender, and racial diversity to ensure that the final makeup of this study, when all said and done was representative of the US. We got, I wanna say, maybe 80% of the way through the study. And we realized, look, we are not gonna meet our, our objectives because the level of volunteers aren't quite what we wanted. And so we made the, the really difficult decision to say, look, we need a throttle, some areas of the country and focus on outreach in different areas to get the right makeup so that the study was representative. All told, it was about a year from when we, you know, started this journey on Covid to when we got the emergency use authorization for the vaccine—which again is really unprecedented for something that usually takes many years. And I'll say for myself personally, it was just such an amazing kind of emotional moment of, you know, I joined the company almost eight years earlier, not thinking necessarily I would ever use one of our own medicines because we weren't even doing vaccines at the time. But to have that injected in my arm and for my family to get it for my friends and everyone else to, to see that benefit and for so many other people in the world, was just an amazing moment for us. Jennifer Strong: I Was There When... features stories from people who witnessed or created breakthroughs in artificial intelligence and computing. Do you have a story to tell? Know someone who does? Drop us an email at podcasts at technology review dot com. [MIDROLL] Jennifer Strong: This project was produced by me with Anthony Green and Emma Cillekens. We’re edited by Michael Reilly and our mix engineer is Garret Lang. Thanks for listening, I’m Jennifer Strong. [TR ID]",0.0
The outgoing White House AI director explains the policy challenges ahead,https://www.technologyreview.com/2022/08/23/1058412/white-house-ai-director-challenges/,2022-08-23,"<p>Algorithmic bias, a small AI workforce, and a lack of AI education may hold the US back from leading the world in the technology, says Lynne Parker.</p>
","The first director of the White House’s National Artificial Intelligence Initiative Office, Lynne Parker, has just stepped down. The NAIIO launched in January 2021 to coordinate the different federal agencies that work on artificial-intelligence initiatives, with the goal of advancing US development of AI. Its goals are to ensure that the US is a leader in AI research and development, particularly in the development and use of trustworthy AI, and to prepare the US workforce with better education and training. As its first director, Parker oversaw the creation of a national AI R&D strategic plan, a national AI research institute, and an AI portal to help researchers apply for funding. and conducted research in ways to measure and evaluate AI. She is now returning to her role in academia as the director of AI initiatives at the University of Tennessee, Knoxville. We spoke to her about the office’s accomplishments and the major challenges ahead for AI in the US. The conversation has been condensed and lightly edited for clarity. What has been the NAIIO’s biggest accomplishment so far? The National AI Initiative covers so much territory: R&D, governance aspects of the use of AI, education and workforce training, international collaboration, and the use of AI within the federal government. That’s a lot of activities. The NAIIO has helped to better structure that, and it's been able to put in place a number of communication channels, and ways to prioritize and coordinate what we’re doing in all of those areas, so that we can make efficient and effective progress. What’s the most important challenge it needs to tackle in the future? In the R&D space, I think the challenge will be to make sure that we’re continuing to invest in high-quality, long-term research that has impactful outcomes, so that we can build up the next generation of AI that will give us benefit down the road. For the development and use of trustworthy AI, the challenge is how we actually implement many of the fundamental principles. For education and workforce—AI, in some sense, is becoming the new math. But not everyone needs advanced calculus, for instance; many just need to know algebra. And so it’s the same in the AI space. Many need to know the basic concepts and capabilities of AI at just a conceptual level, and others need to be able to be experts and program and develop new machine-learning algorithms. Coming up with education and training opportunities for different people through all walks of life and in all types of jobs is a challenge. Which aspects of the NAIIO’s remit have been easier to make progress on? Which have been harder? Part of this may reflect my own background, but I think R&D has been easier because it’s more structured … At the end of the day, funding is often what it boils down to in R&D, and I think we have done a very good job of prioritizing and funding AI R&D. In terms of a pillar that’s more challenging, I’ll come back to education and the workforce, because there’s so many different kinds of needs. And because K-12 education is managed by the states—it’s not a single approach for the entirety of the country—there’s a long-standing challenge there of how do we build up that capacity? How do we create curricula that people across the country can use? The lack of sufficient talent in the AI sphere, or just sufficient understanding of what AI is by all of our people, is a long-standing challenge. We’ve recognized that for many years as it relates to STEM areas in general. But we do have a bit of a cultural challenge in terms of people thinking that the field is hard, or it’s geeky, or something like that. And so not as many people will enter the field. We don’t currently have enough people to teach these fields. Many experts are leaving academia and going to industry. And it’s great that we have a thriving industry in this country in this space, but when we don’t have enough educators that can train the next generation, then that exacerbates the problem. So this is a very tough pillar in my mind, but it’s one that we really have to prioritize and continue to make progress in. The EU is working on legislation to regulate AI. Should the US adopt any of the same measures? One area of clear commonality is understanding AI implications and the need for regulation through the lens of risk. Taking a sector-based approach of evaluating risk is something that we at a high level agree on. The National Institute of Standards and Technology (NIST) is making important contributions in this space, in the development of the AI risk management framework. The European Union is planning new legislation aimed at curbing the worst harms associated with artificial intelligence. They’re making good progress on this and anticipate having that framework out by the beginning of 2023. There are some nuances here—different people interpret risk differently, so it’s important to come to a common understanding of what risk is and what appropriate approaches to risk mitigation might be, and what potential harms might be. You’ve talked about the issue of bias in AI. Are there ways that the government can use regulation to help solve that problem? There are both regulatory and nonregulatory ways to help. There are a lot of existing laws that already prohibit the use of any kind of system that’s discriminatory, and that would include AI. A good approach is to see how existing law already applies, and then clarify it specifically for AI and determine where the gaps are. NIST came out with a report earlier this year on bias in AI. They mentioned a number of approaches that should be considered as it relates to governing in these areas, but a lot of it has to do with best practices. So it’s things like making sure that we’re constantly monitoring the systems, or that we provide opportunities for recourse if people believe that they’ve been harmed. It’s making sure that we’re documenting the ways that these systems are trained, and on what data, so that we can make sure that we understand where bias could be creeping in. It’s also about accountability, and making sure that the developers and the users, the implementers of these systems, are accountable when these systems are not developed or used appropriately. What do you think is the right balance between public and private development of AI? The private sector is investing significantly more than the federal government into AI R&D. But the nature of that investment is quite different. The investment that’s happening in the private sector is very much into products or services, whereas the federal government is investing in long-term, cutting-edge research that doesn’t necessarily have a market driver for investment but does potentially open the door to brand-new ways of doing AI. So on the R&D side, it’s very important for the federal government to invest in those areas that don’t have that industry-driving reason to invest. Industry can partner with the federal government to help identify what some of those real-world challenges are. That would be fruitful for US federal investment. There is so much that the government and industry can learn from each other. The government can learn about best practices or lessons learned that industry has developed for their own companies, and the government can focus on the appropriate guardrails that are needed for AI.",0.0
Deep learning can almost perfectly predict how ice forms,https://www.technologyreview.com/2022/08/11/1057623/deep-learning-predicts-ice-formation/,2022-08-11,"<p>It’s a development that could significantly increase the accuracy of weather and climate forecasting.</p>
","Researchers have used deep learning to model more precisely than ever before how ice crystals form in the atmosphere. Their paper, published this week in PNAS, hints at the potential to significantly increase the accuracy of weather and climate forecasting. The researchers used deep learning to predict how atoms and molecules behave. First, models were trained on small-scale simulations of 64 water molecules to help them predict how electrons in atoms interact. The models then replicated those interactions on a larger scale, with more atoms and molecules. It’s this ability to precisely simulate electron interactions that allowed the team to accurately predict physical and chemical behavior. “The properties of matter emerge from how electrons behave,” says Pablo Piaggi, a research fellow at Princeton University and the lead author on the study. “Simulating explicitly what happens at that level is a way to capture much more rich physical phenomena.” It’s the first time this method has been used to model something as complex as the formation of ice crystals, also known as ice nucleation. This is one of the first steps in the formation of clouds, which is where all precipitation comes from. Xiaohong Liu, a professor of atmospheric sciences at Texas A&M University who was not involved in the study, says half of all precipitation events—whether snow or rain or sleet—begin as ice crystals, which then grow larger and result in precipitation. If researchers could model ice nucleation more accurately, it could give a big boost to weather prediction overall. Ice nucleation is currently predicted on the basis of laboratory experiments. Researchers collect data on ice formation under different laboratory conditions, and that data is fed into weather prediction models under similar real-world conditions. This method works well enough sometimes, but often it ends up being inaccurate because of the sheer number of variables involved in actual weather conditions. If even a few factors vary between the lab and the real world, the results can be quite different. “Your data is only valid for a certain region, temperature, or kind of laboratory setting,” Liu says. The firm worked with UK weather forecasters to create a model that was better at making short term predictions than existing systems. Predicting ice nucleation from the way electrons interact is much more precise, but it’s also very computationally expensive. It requires researchers to model at least 4,000 to 100,000 water molecules, and even on supercomputers, such a simulation could take years to run. Even that would only be able to model the interactions for 100 picoseconds, or 10-10 seconds—not long enough to observe the ice nucleation process. Using deep learning, however, researchers were able to run the calculations in just 10 days. The time duration was also 1,000 times longer—still a fraction of a second, but just enough to see nucleation. Of course, more accurate models of ice nucleation alone won’t make forecasting perfect, says Liu, since it is only a small though critical component of weather modeling. Other aspects are also important—understanding how water droplets and ice crystals grow, for example, and how they move and interact together under different conditions. Still, the ability to more accurately model how ice crystals form in the atmosphere would significantly improve weather predictions, especially those involving whether and how much it’s likely to rain or snow. It could also aid climate forecasting by improving the ability to model clouds, which affect the planet’s temperature in complex ways. Piaggi says future research could model ice nucleation when there are substances like smoke in the air, potentially improving the accuracy of models even more. Because of deep-learning techniques, it’s now possible to use electron interactions to model larger systems for longer periods of time. “That has opened essentially a new field,” Piaggi says. “It’s already having and will have an even greater role in simulations in chemistry and in our simulations of materials.”",0.0
How to craft effective AI policy,https://www.technologyreview.com/2022/08/11/1057611/emtech-digital-how-to-craft-effective-ai-policy/,2022-08-11,"<p>A conversation about equity and what it takes to make effective AI policy.</p>
","This episode was taped before a live audience at MIT Technology Review’s annual AI conference, EmTech Digital. This episode was created by Jennifer Strong, Anthony Green, Erin Underwood and Emma Cillekens. It was edited by Michael Reilly, directed by Laird Nolan and mixed by Garret Lang. Episode art by Stephanie Arnett. Cover art by Eric Mongeon. Special thanks this week to Amy Lammers and Brian Bryson. [PREROLL] [TR ID] Jennifer Strong: The applications of artificial intelligence are so embedded in our everyday lives it’s easy to forget it's there… But these systems, like ones powering Instagram filters or the price of a car ride home… can rely on pre-existing datasets that fail to paint a complete picture of consumers. It means people become outliers in that data – often the same people who’ve historically been marginalized. It’s why face recognition technologies are least accurate on women of color, and why ride-share services can actually be more expensive in low-income neighborhoods. So, how do we stop this from happening? Well would you believe a quote from Harry Potter and his wizarding world… might create a good starting point for this conversation? I’m Jennifer Strong and this episode, our producer Anthony Green brings you a conversation about equity from MIT Technology Review’s A-I conference, EmTech Digital. We’ll hear from Nicol Turner Lee—the director of the center for technology at the Brookings Institution—about what it takes to make effective AI policy. [EPISODE IN: Anthony Green: There's a quote from Harry Potter of all places. Nicol Turner Lee: Oh Lord, I, I, I haven't seen the Harry Potter episodes since my kids were little so I'll try. [Laughter] Anthony Green: Oh man. Uh, it's a pretty good one. No, it's, it's just kind of stuck with me over the years. I'm honestly not even otherwise a big fan, but, um, the quote goes, there will be a time where we must choose between what is right and what is easy and it feels like that applies pretty squarely to how companies design these systems. Right. So I guess my question is how can policy makers, right, start to push the needle in the right direction when it comes to favorable outcomes for AI in decision making? Nicol Turner Lee: Well, that's a great question. And again, thank you for having me. You may be wondering why I'm sitting here. I'm a sociologist. I've had the privilege of being on this stage for a couple of conferences here at MIT. But I got into this… And before I answer your question, because I think the quote that you're referencing points to much of what my colleagues have talked about, which are the sociotechnical implications of these systems. Anthony Green: Mm-hmm. Nicol Turner Lee: So I've been doing this for about 30 years. And part of the challenge that we've had is that we've not seen equitable access to technology. And as we think about these emerging sophisticated systems, to your point, we have to think about the extent to which they have effects on regular everyday people, particularly people who are already marginalized. Already vulnerable in our society. So that quote has a lot of meaning because if we're not careful, the technology in and of itself will sort of accelerate, I think, some of the progress that we've made when it comes to equity and civil rights. Anthony Green: Yeah. Nicol Turner Lee: Um, I'm gonna date myself for just a moment. I know I look a lot younger. When I was growing up I used to run home and watch the Jetsons, right. There were two cartoons. I watched Fred Flinstone, which if you all remember, he rode around on a car with rocks and I watched the Jetsons.. Anthony Green: Powered with his feet. Nicol Turner Lee: I know, right! You're too young to know about Fred Flinstone. Anthony Green: Oh, Boomerang. But, but if you notice. You know, Fred Flinstone is archaic. Right? Anthony Green: Right. Nicol Turner Lee: The, the rocks as wheels doesn't work. Anthony Green: Yeah. Nicol Turner Lee: The Jetsons is actually realized. And part of the challenge and the reason that I have interest in this work outside of my, you know, PhD in sociology and my interest in technology is that these systems now are so much more generally purposed that they impact people when they are contextualized in environments. And that's where I think we have to have more conversations that point to your question. So roundabout way. But I think it's really important that we have these conversations now, before the technology accelerates itself. Anthony Green: Hundred percent. And I mean, you know, all of that said, right, policy making alone isn't going to be the only solution needed to resolve these issues. So I would love it if you can speak to how accountability, specifically on the part of industry, comes into play as well. Nicol Turner Lee: Well, the problem with policy makers is that we're not necessarily technologists. And so we can see a problem and we actually sort of see that problem in its outcomes. Anthony Green: Yeah. Nicol Turner Lee: So I don't think there's any policy maker, or very few outside of people like Ro Khanna and others, right, who actually know what it's like to be in, in the tech space. Anthony Green: Sure. Nicol Turner Lee: That understands how these outcomes occur. They don't understand what's underneath the hood. Or as people say, I'm trying to move away from this language. It's not really a black box. Right. It's just a box. Anthony Green: Right. Nicol Turner Lee: Because there's some, uh, judgments that come with calling it a black box. But when you think about policy and those outcomes you have to say to yourself, how do policy makers sort of take an organic iterative model and then legislate or regulate it? And that's where people like me who are in the social sciences, I think, come in and have much more conversation on what they should be looking for. Um, so the accountability there is hard. Anthony Green: Yeah. Nicol Turner Lee: Because no one is talking the same language as many of you in this room, right. The technologists are sort of rushing to market. I call it permissionless forgiveness. Uh, as my colleagues at the center for technology innovation, Tom Wheeler has that great phrase, “build it and then break it and then come back and fix it.” Well, guess what happens? That's permissionless forgiveness. Cuz what happens? We say we're sorry when people have foreclosed, uh, mortgage rates, are in criminal justice systems where they're detained longer because these models dictate those predictions. Anthony Green: Right. Nicol Turner Lee: So policy makers have not quite, Anthony, caught up to the speed of innovation. And we've said that for decades, but it's actually true. Anthony Green: Absolutely. I mean, you've referred to this issue in the past as a civil and human rights issue. Nicol Turner Lee: It is. It is. Anthony Green: Right. So, I mean, can you kind of like expand on that and how that's kind of shaped your conversations about policy? Nicol Turner Lee: You know, it's shaped my conversations from the standpoint of this. I, I, you know, shameless plug, I have a book coming out on the US digital divide so I've been very interested. I call it, uh, Digitally Invisible, how the internet is creating the new underclass. And it's really about the digital divide going past the binary construction of who's online, who's not, to really thinking about what are the impacts when you are not connected. Anthony Green: Right. Nicol Turner Lee: And how do these emerging technologies impact you? So to your point, I call it a civil rights issue because what the pandemic demonstrated is that without internet access, you were actually not able to get the same opportunities as everybody else. You could not register for your vaccine. You could not communicate with your friends and family. Fifty-million school aged kids sent home, 15 to 16 million of them could not learn. And now we're seeing the effects of that. Anthony Green: Yeah. Nicol Turner Lee: And so when we think about artificial intelligence systems that now have replaced what I call the death of analog. Replace, uh, you know, how we used to do things in person we're now seeing, in a civil rights age, laws that are being violated. And that.. in ways that I, I don’t necessarily attribute to the malfeasance of technologists. But what they're doing is they’re foreclosing on opportunities that people have fought hard for. Anthony Green: Sure. Nicol Turner Lee: 2016 election. When we had foreign operatives come in and manipulate the content that was available to voters. That was a form of voter suppression. Anthony Green: Right. Nicol Turner Lee: And there was no place that those folks could go to like the Supreme Court or Congress to say my vote was just taken away based on the deep neural networks that were associated with what they were seeing. Anthony Green: Yeah. Nicol Turner Lee: Or the misinformation around polling. We're now at a state… when you are in a city like Boston and an Uber driver doesn't pick you up because he sees your face in the profile. Where do you go for the type of, um, you know, the benefits of, of the civil rights regime that we have that was not based on a digital atmosphere? So part of my work at Brookings has been how do we look at the flexibility and agility of these systems to apply to emerging technologies. And we have no simple answer because these rules were not necessarily developed , you know, in the 21st century. Anthony Green: Right. Nicol Turner Lee: They were developed when my grandfather told me how he walked to school with the same pair of shoes, right. Where the bottom was out because he wanted an education. We don't have that today. And I think it's worth a conversation as these technologies become more ubiquitous. How are we developing not just inclusive and equitable AI but legally compliant AI? AI that makes sense that people feel that they have some retribution for that malfeasance. So I'll talk a little bit about some of the work we're doing on there, but I think, you know, there's a cadre of individuals like myself, some of them here at MIT, that are really trying to figure out how do we go back and make people accountable to the civil and human liberties of folks and not allow the technology to be the fall person when it comes to, you know, why things wreck havoc or go wrong. Anthony Green: Don't blame the robots. Nicol Turner Lee: You know! I tell people robots do not discriminate. I'm sorry. You know, we do and, and it's something to be said about that. We start looking at civil rights. Anthony Green: I'm gonna go to the audience. Anyone got a question? Rene, audience member: Thank you so much, Renee from Sao Paulo, Brazil. Nicol Turner Lee: Hey! Rene, audience member: There is a common theme on these last presentations. It's about invisibility. Nicol Turner Lee: Yes! Rene, audience member: There are so many ways to be invisible. If, if you have the wrong badge you are invisible, like Harry Potter. If you are too old, if you have the wrong kind of skin. And there's one very interesting thing. When we talk, we talk about data and AI. AI is proposing things about data that are available. Nicol Turner Lee: Yeah. Rene, audience member: But there are data that are completely invisible about people who are invisible. So what kind of solutions are we building if you are basing on data.. based on data about all, always the same people. How do we bring visibility to everybody? Yes! Rene, audience member: So, thank you so much. Nicol Turner Lee: No, I love that question. Can I jump right in on this one? Anthony Green: Go for it. Nicol Turner Lee: You know, uh, my colleague and friend Renee Cummings, who is the AI, uh, scientist in residence at University of Virginia. She introduced me to, a few months ago and we did a podcast where she was featured, this concept of what's called data trauma. Anthony Green: Mmmm. Nicol Turner Lee: And I wanna sort of walk you through this because it blew me away when I began to think about the implications and it goes to Renee's question. What does it mean, you know, when we talk about AI, we often talk about the problem development, the data that we're training it on, the way that we're interpreting the outcomes or explaining them, but we never talk about the quality of the data and the fact that the data in and of itself holds within it, the, the wounds of our society. I don't care what people say. If you are training AI on criminal justice, um, issues, and you're trying to make a fair and equitable AI that recognizes who should be detained or who should be released. And we all know that particular algorithm I'm talking about. If it is trained on US data, it is disproportionately going to overrepresent people of color. So even though my friends, and I tell everybody this, just so you know, like she's not coming in here, you know, being angry. I tell everybody you need a social scientist as a friend. I don't care who you are. If you are a scientist, an engineer, a data scientist and you don't have one social scientist as your friend, you're not being honest to this problem. Right? Because what happens with that data? It comes with all of that noise. And despite our ability as scientists to sort of tease out that noise or diffuse the noise, you still have the basis and the foundation for the inequality. And so one of the things I've tried to tell people, it's probably okay for us to recognize the trauma of the data that we're using. It's okay for us to realize that our models will be normative in the extent to which there will be bias. Technical bias, societal bias, outcome bias and prediction bias, but we should disclose what those things are. Anthony Green: Yeah. Nicol Turner Lee: And that's where my work in particular has become really interesting to me as a person who is looking at this as, you know, the use of proxies and the use of data. For me, it becomes what part of the model is much more injurious to respondents and to outcomes. And what part should we disclose that we just don't have the right data to predict accurately without some type of, you know, risk… Anthony Green: Sure. Nicol Turner Lee: …to that population. Anthony Green: Yeah. Nicol Turner Lee: So to your question, I think if we acknowledge that, you know, I think then we can get to a point where we can have these honest conversations on how we bring interdisciplinary context to certain situations. Anthony Green: We've got another question. Kyle, audience member: Hi Nicol. Nicol Turner Lee: Hey. Kyle, audience member: I'm grateful for your perspective. Um, my name is Kyle. I run… I'm a data scientist by training and I run a team of AI and ML designers and developers. And so, you know, it scares me how fast the industry's evolving. You mentioned GPT-3. We're already talking about GPT-4 is in the works and the exponential leap and capabilities that's gonna present. Something that you mentioned that really struck me is that legislators don't understand what we're doing. And I don't believe that us as data scientists should be the ones making decisions about how to tie our hands behind our backs. Nicol Turner Lee: Yeah. Kyle, audience member: And how to protect our work from having unintended consequences. Nicol Turner Lee: Yes. Kyle, audience member: So how do we engage and how do we help legislators understand the real risks and not the hype that is sometimes heard or perceived in the media? Nicol Turner Lee: Yeah, no, I love that question. I'm actually gonna flip it. And I'm gonna talk about it in two ways in which I actually talk about it. So I do think that legislators who work in this space, particularly in those sensitive use cases. So I tell people, I give this example all the time. I love shopping for boots and I'm okay with the algorithm that tells me as a consumer that I love boots, but as Latonya Sweeney's work has indicated if you associate other things with me. Uh, what other, uh, attributes does this particular person have? When does she buy boots? How many boots does she have? Does she check her credit when she's buying boots? What kind of computer is she using when she's buying her boots? If you become to make that accumulative picture around me, then we run into what Dr. Sweeney has talked about—these associations that create that type of risk. So to your first question, I think you're right. That policy makers should actually define the guardrails, but I don't think they need to do it for everything. I think we need to pick those areas that are most sensitive. The EU has called them high risk. And maybe we might take from that, some models that help us think about what's high risk and where should we spend more time and potentially policy makers, where should we spend time together? I'm a huge fan of regulatory sandboxes when it comes to co-design and co-evolution of feedback. Uh, I have an article coming out in an Oxford University press book on an incentive-based rating system that I could talk about in just a moment. But I also think on the flip side that all of you have to take account for your reputational risk. As we move into a much more digitally advanced society, it is incumbent upon developers to do their due diligence too. You can't afford as a company to go out and put an algorithm that you think, or an autonomous system that you think is the best idea, and then land up on the first page of the newspaper. Because what that does is it degrades the trustworthiness by your consumers of your product. And so what I tell, you know, both sides is that I think it's worth a conversation where we have certain guardrails when it comes to facial recognition technology, because we don't have the technical accuracy when it applies to all populations. When it comes to disparate impact on financial products and services.There are great models that I've found in my work, in the banking industry, where they actually have triggers because they have regulatory bodies that help them understand what proxies actually deliver disparate impact. There are areas that we just saw this right in the housing and appraisal market, where AI is being used to sort of, um, replace a subjective decision making, but contributing more to the type of discrimination and predatory appraisals that we see. There are certain cases that we actually need policy makers to impose guardrails, but more so be proactive. I tell policymakers all the time, you can't blame data scientists. If the data is horrible. Anthony Green: Right. Nicol Turner Lee: Put more money in R and D. Help us create better data sets that are overrepresented in certain areas or underrepresented in terms of minority populations. The key thing is, it has to work together. I don't think that we'll have a good winning solution if policy makers actually, you know, lead this or data scientists lead it by itself in certain areas. I think you really need people working together and collaborating on what those principles are. We create these models. Computers don't. We know what we're doing with these models when we're creating algorithms or autonomous systems or ad targeting. We know! We in this room, we cannot sit back and say, we don't understand why we use these technologies. We know because they actually have a precedent for how they've been expanded in our society, but we need some accountability. And that's really what I'm trying to get at. Who's making us accountable for these systems that we're creating? It's so interesting, Anthony, these last few, uh, weeks, as many of us have watched the, uh, conflict in Ukraine. My daughter, because I have a 15 year old, has come to me with a variety of TikToks and other things that she's seen to sort of say, “Hey mom, did you know that this is happening?” And I've had to sort of pull myself back cause I've gotten really involved in the conversation, not knowing that in some ways, once I go down that path with her. I'm going deeper and deeper and deeper into that well. Anthony Green: Yeah. Nicol Turner Lee: And I think for us as scientists, it kind of goes back to this. I Have a Dream speech. We have to determine which side of history we wanna be on with this technology folks. And how far down the rabbit hole do we wanna go to contribute? I think what the greatness of AI is our ability to have human cognition wrapped up in these repetitive processes that go way beyond our wildest imagination of the Jetsons. And that allows us to do things that none of us have been able to do in our lifetime. Where do we want to sit on the right side of history? And how do we want to handle these technologies so that we create better scientists? Anthony Green: Sure. Nicol Turner Lee: Not ones that are worse. And I think that's a valid question to ask of this group. And it's a valid question to ask of yourself. Anthony Green: I don't know if we can end on anything better and we're out of time! Nicol, we can go all day but.. Nicol Turner Lee: I know. I always feel like a Baptist preacher, you know, so if I have energy about it… Anthony Green: Choir, can you sing it? Nicol Turner Lee: I know, right. I can't sing it, but you can do that I Have A Dream speech, Anthony. [Laughter] Anthony Green: Oh man. You're putting me on the stand and I'm already on stage. Nicol Turner Lee: Yeah, right haha. Anthony Green: Nicol, thank you so much. Nicol Turner Lee: Thank you so much as well. Appreciate it. Anthony Green: Absolutely. Nicol Turner Lee: Thank you everybody here. [MIDROLL AD] Jennifer Strong: This episode was produced by Anthony Green, Erin Underwood, and Emma Cillekens. It’s edited by Michael Reilly, directed by Laird Nolan and mixed by Garret Lang. It was recorded in front of a live audience at the MIT Media Lab in Cambridge Massachusetts, with special thanks to Amy Lammers and Brian Bryson. Jennifer Strong: Thanks for listening. I’m Jennifer Strong.",0.0
Automated techniques could make it easier to develop AI,https://www.technologyreview.com/2022/08/05/1056814/automation-ai-machine-learning-automl/,2022-08-05,"<p>Automated machine learning promises to speed up the process of developing AI models and make the technology more accessible.</p>
","Machine-learning researchers make many decisions when designing new models. They decide how many layers to include in neural networks and what weights to give inputs at each node. The result of all this human decision-making is that complex models end up being “designed by intuition” rather than systematically, says Frank Hutter, head of the machine-learning lab at the University of Freiburg in Germany. A growing field called automated machine learning, or autoML, aims to eliminate the guesswork. The idea is to have algorithms take over the decisions that researchers currently have to make when designing models. Ultimately, these techniques could make machine learning more accessible. Although automated machine learning has been around for almost a decade, researchers are still working to refine it. Last week, a new conference in Baltimore—which organizers described as the first international conference on the subject—showcased efforts to improve autoML’s accuracy and streamline its performance. There’s been a swell of interest in autoML’s potential to simplify machine learning. Companies like Amazon and Google already offer low-code machine-learning tools that take advantage of autoML techniques. If these techniques become more efficient, it could accelerate research and allow more people to use machine learning. The idea is to get to a point where people can choose a question they want to ask, point an autoML tool at it, and receive the result they are looking for. That vision is the “holy grail of computer science,” says Lars Kotthoff, a conference organizer and assistant professor of computer science at the University of Wyoming. “You specify the problem, and the computer figures out how to solve it—and that’s all you do.” But first, researchers will have to figure out how to make these techniques more time and energy efficient. At first glance, the concept of autoML might seem redundant—after all, machine learning is already about automating the process of gaining insights from data. But because autoML algorithms operate at a level of abstraction above the underlying machine-learning models, relying only on the outputs of those models as guides, they can save time and computation. Researchers can apply autoML techniques to pre-trained models to gain fresh insights without wasting computation power repeating existing research. For example, research scientist Mehdi Bahrami and his coauthors at Fujitsu Research of America presented recent work on how to use a BERT-sort algorithm with different pre-trained models to adapt them for new purposes. BERT-sort is an algorithm that can figure out what is called “semantic order” when trained on data sets—given data on movie reviews, for example, it knows that “great” movies rank higher than “good” and “bad” movies. With autoML techniques, the learned semantic order can also be extrapolated to classifying things like cancer diagnoses or even text in the Korean language, cutting down on time and computation. Humans have struggled to make truly intelligent machines. Maybe we need to let them get on with it themselves. “BERT takes months of computation and is very expensive—like, a million dollars to generate that model and repeat those processes,” Bahrami says. “So if everyone wants to do the same thing, then it’s expensive—it’s not energy efficient, not good for the world.” Although the field shows promise, researchers are still searching for ways to make autoML techniques more computationally efficient. For example, methods like neural architecture search currently build and test many different models to find the best fit, and the energy it takes to complete all those iterations can be significant. AutoML techniques can also be applied to machine-learning algorithms that don’t involve neural networks, like creating random decision forests or support-vector machines to classify data. Research in those areas is further along, with many coding libraries already available for people who want to incorporate autoML techniques into their projects. The next step is to use autoML to quantify uncertainty and address questions of trustworthiness and fairness in the algorithms, says Hutter, a conference organizer. In that vision, standards around trustworthiness and fairness would be akin to any other machine-learning constraints, like accuracy. And autoML could capture and automatically correct biases found in those algorithms before they’re released. But for something like deep learning, autoML still has a long way to go. Data used to train deep-learning models, like images, documents, and recorded speech, is usually dense and complicated. It takes immense computational power to handle. The cost and time for training these models can be prohibitive for anyone other than researchers working at deep-pocketed private companies. One of the competitions at the conference asked participants to develop energy-efficient alternative algorithms for neural architecture search. It’s a considerable challenge because this technique has infamous computational demands. It automatically cycles through countless deep-learning models to help researchers pick the right one for their application, but the process can take months and cost over a million dollars. The goal of these alternative algorithms, called zero-cost neural architecture search proxies, is to make neural architecture search more accessible and environmentally friendly by significantly cutting down on its appetite for computation. The result takes only a few seconds to run, instead of months. These techniques are still in the early stages of development and are often unreliable, but machine-learning researchers predict that they have the potential to make the model selection process much more efficient.",0.0
DeepMind has predicted the structure of almost every protein known to science,https://www.technologyreview.com/2022/07/28/1056510/deepmind-predicted-the-structure-of-almost-every-protein-known-to-science/,2022-07-28,"<p>And it’s giving the data away for free, which could spur new scientific discoveries.</p>
","DeepMind says its AlphaFold tool has successfully predicted the structure of nearly all proteins known to science. From today, the Alphabet-owned AI lab is offering its database of over 200 million proteins to anyone for free. When DeepMind introduced AlphaFold in 2020, it took the science community by surprise. Scientists had spent decades trying to understand how proteins, which are essential to life, are structured; it was considered one of the “grand challenges” of biology. Understanding how they are shaped is crucial to understanding how they function. Last year, DeepMind released the source code of AlphaFold and made the structures of 1 million proteins, including nearly every protein in the human body, available in its AlphaFold Protein Structure Database. The database was built together with the European Molecular Biology Laboratory, an international public research institute that already hosts a large database of protein information. The latest data release gives the database a massive boost. The update includes structures for “plants, bacteria, animals, and many, many other organisms, opening up huge opportunities for AlphaFold to have impact on important issues such as sustainability, fuel, food insecurity, and neglected diseases,” Demis Hassabis, DeepMind’s founder and CEO, told reporters on a call this week. A year after it took biologists by surprise, AlphaFold has changed how researchers work and set DeepMind on a new course. The expanded database could act as an important resource for scientists, helping them to better understand diseases. It could also speed innovation in drug discovery and biology. “AlphaFold is probably the most major contribution from the AI community to the scientific community,” said Jian Peng, a computer science professor at the University of Illinois Urbana-Champaign who specialises in computational biology. Since its release in 2020, researchers have already used AlphaFold to understand proteins that affect the health of honeybees and to develop an effective malaria vaccine. The database allows researchers to look up 3D structures of proteins “almost as easily as doing a keyword Google search,” said Hassabis. Predicting the structures of proteins is very time consuming, and having a tool with 200 million readily available protein structures will save researchers a lot of time, said Mohammed AlQuraishi, a systems biologist at Columbia University, who is not involved in DeepMind’s research. AlphaFold could also help scientists to reassess previous research to better understand how diseases happen, Peng said. However, for many proteins “we’re interested in understanding how their structure is altered by mutations and natural allelic variation, and that won’t be addressed by this database,” said AlQuraishi. “But of course the field is developing fast, and so I expect tools to accurately model protein variants will begin to appear soon,” he added. The quality of AlphaFold’s predictions may also not be as accurate for rarer proteins with less available evolutionary information, says Peng. The move is the latest development in DeepMind’s push into “digital biology,” where “AI and computational methods can help to understand and model important biological processes,” said Hassabis. Hassabis also leads a new venture, also owned by Alphabet, called Isomorphic Labs, which is developing AI for drug discovery. Pushmeet Kohli, head of AI for science at DeepMind, said the company has plenty of challenges in the life sciences it still wants to tackle, such as how proteins behave and interact with other proteins. Hassabis said his dream is that AI could not just help figure out the structure of proteins, but become a “significant part of the discovery process for new drugs and cures.”",0.0
Podcast: Can AI keep guns out of schools?,https://www.technologyreview.com/2022/07/27/1057112/podcast-can-ai-keep-guns-out-of-schools/,2022-07-27,,"Amid a growing epidemic of gun violence, can AI be part of the solution? In this episode we look at some of the weapons detection technologies schools are using in an effort to try to keep students safe. This episode was produced by Anthony Green and Emma Cillekens with reporting from Mark Keierleber. It was edited by Jennifer Strong, Rachel Courtland and Mat Honan, mixed by Garret Lang, with original music from Jacob Gorski. [PREROLL] [TR ID] Gary Hough: You think about what that feeling must have been like in, in the hearts and minds of, of, of those parents and grandparents and uncles and aunts and relatives and the other children in that school. Jennifer: He’s talking about school shootings… and the growing epidemic of gun violence in the US. Gary Hough: I think we have to reach out in a national movement to do our best to protect our young people. My name is Gary Hough. I am the superintendent of Fayette County schools in Fayette County, West Virginia. Jennifer: His district has been installing AI-enabled systems that are meant to detect guns and other weapons at school entrances. And so far, he’s finding them useful. Gary Hough: We don't have to have that long line that you deal with with the old metal detector scanners. They are able to roll straight through the process. Jennifer: Students don’t have to slow down to pass through… stopping only if an alarm goes off. He says this is important… because long lines present a safety hazard. Gary Hough: You know, when they're getting off those school buses and coming in, or getting out of the cars and coming in, that period of time that they're delayed coming through that front door where, you know, you're enclosed in a controlled area. They're in a vulnerable setting. This permits us to do the scanning without putting our students in that situation. Jennifer: These machines sometimes catch things that shouldn't be there… including brass knuckles. But he also says, false positives are a daily occurrence… and school issued laptops are often mistaken for weapons… So students now take their Chromebooks out of their bags and hold them up as they pass through the entrance. Gary Hough: You know, those are the kind of things that we had to train our people to do so that it doesn't stop them, but it's not like what you have when you have the belts and the scanners, you have to take all the change out of your pocket. Change and keys, do not set it off. You know what I mean? It already is programmed into there that that's not a security risk. Jennifer: He’s talking about a system from a company called Evolv… and we’ll meet one of the founders in just a bit. The product looks a lot like those plastic towers inside stores that detect shoplifting. It’s there in plain sight, but you don’t really notice it unless it goes off. I know, because my microphone set one off recently in New York City. But, unlike a standard metal detector… this one uses AI to keep it from going off over things like phones and keys. Gary Hough: I think our parents were very pleased and continue to be pleased because of the level of technology that we have. Is this a kind of a fix-all for the problem? No, it's not, but it's a good tool in your tool chest as a school administrator to know you've done everything you can. Jennifer: I’m Jennifer Strong and this episode we explore some of the AI technologies that schools are looking to, to try to help keep students safe. [SHOW ID] Jennifer: The conversation on school safety has long been about preventing mass shootings. But the pandemic caused a shift in that conversation… towards stopping the spread of the virus… with products like temperature sensors and cameras that detect masks. For many school districts, this shift also marked the beginning of security protocols driven by AI. Mark Keierleber: So now some of these folks who implemented these cameras… you know I talked to the security chief in Fulton county, Georgia, which is Metro Atlanta and they installed a system that detects masks and he specifically talked about how, you know, well, in a post pandemic environment, we may not be checking for masks, but this is a precursor to other forms of technology, specifically weapons detection. My name's Mark Keierleber. I'm an investigative reporter at The 74, a national K-12 education news website. Jennifer: He covers security and surveillance technology in schools. Mark Keierleber: That can range from metal detectors to locks inside classrooms to bulletproof blankets. Jennifer: You heard that right… he said bulletproof blankets… which are among a whole host of bulletproof school supplies. Mark Keierleber: There are bulletproof backpacks. There are bulletproof whiteboards. There are bulletproof shields that are designed to hang in a classroom next to the fire extinguisher. There's basically anything that you can think about inside of a school… has been redesigned in the last few years to be bulletproof. Jennifer: But he says the real focus in school security is on using AI and other tech to find weapons. Schools that use the kinds of technologies he reports on to try to prevent gun violence… could benefit from a bill introduced by Congress in June. It would pump hundreds of millions of dollars into a program started in response to previous school shootings. Mark Keierleber: So the idea is they've programmed the cameras to identify what, you know, a gun looks like in terms of its shape. There's one company that was marketing a gun detection system a few years ago where it was also equipped with speakers in the hallway. And it would say, like it would detect the gun and the speakers would, like, a robot would come across the speakers and say “warning, we have detected a gun please be advised that law enforcement is on the way.” Jennifer: That product comes from a company called Athena Security… which makes a variety of weapons detection products. Instructor: Okay. What you see in the bottom left hand corner is the live video feed. Jennifer: This is taken from a demo the company posted on YouTube. Instructor: Upper left hand corner is our alerting platform and in the right you're gonna see the person entering with a gun. Of course, this can also work with our fever detection platform. And if a person has the fever walking in front of the camera, it'll send the same exact alert. Jennifer: Fever detection technology took off during covid, and though it’s far from foolproof… it’s probably here to stay. Instructor: So what you see here is a person entering with a weapon. Soon as the camera sees the weapon on the lower left hand corner.. wait for it.. There it is. Boom. It shows the alert on the right hand side, it pops into Avigilon. You'll see the image and then you can pull open the video if you like. Jennifer: After the Parkland shooting, Reporter Mark Keierleber says he was getting all kinds of pitches from tech companies about everything from metal detectors to those bulletproof backpacks… and he says someone from Athena reached out with a quote that really stayed with him. Mark Keierleber: Well, I'm gonna read you the quote. // Artificial intelligence to help law enforcement stop crime before it starts or escalates, like Minority Report but in real life, is becoming a reality. // And that's pretty interesting. Right. Um, so if you've ever seen the movie Minority Report, the idea here is that, in Minority Report, you're, you're being constantly followed by different forms of surveillance. It's able to recognize, um, everything around you and basically predict crime before it happens. Announcer: [00:21] Within just one month under the pre-crime program, the murder rate in the District of Columbia was reduced 90 percent. Victim 1: They were gonna be waiting for me in the car. Victim 2: He was gonna rape me. Victim 3: I was going to be stabbed. Victim 4: Right here. Announcer: Within a year, pre-crime effectively stopped murder in our nation's capital. Jennifer: Up until recently, security cameras have been passive. Meaning, after a crime, people watch the tape to see what happened. But we’re moving to a world where cameras are active… with AI that makes real time assessments and predictions. Mark Keierleber: Certainly there are all kinds of other forms of weapons detection on the market. I mean, Shot Spotter is a big one and it exists in communities across the United States, especially major cities. Jennifer: Basically, that product is a microphone on a light pole that’s meant to detect the sound of a gunshot. Then, there’s products like Evolv. Mark Keierleber: It's these new forms of high tech, what appear to be basically metal detectors… and metal detectors play a role in the school security conversation. They certainly are discussed a lot at length in the wake of school shootings. We saw major growth in different forms of security and surveillance technology installed in schools, in the wake of the Columbine shooting and in the wake of Parkland and in the wake of Sandy Hook tragedy. And certainly nothing is going to be different with Uvalde. [Chapter Change Music] Mike Ellenbogen: Metal detectors are 90 years old, nine zero, right? They were first deployed in Germany in the 1920s, 1930s to look for castings being stolen out of factories. And they haven't fundamentally changed. And they were great for, you know, if, if you want, in a prison environment, if you've gotta make sure that nobody's coming in with a handcuff key or half a razor blade or something like that Mike Ellenbogen: Sure. Uh, Mike Ellenbogen. Founder, and chief innovation officer for Evolv Technologies. In an environment where you don't expect them to have any other metal objects on them, metal detectors are great. They're really good at finding metal. The problem is today, we're all carrying metal, right? Everybody's got a cell phone, everybody's got pockets full of stuff. Jennifer: His goal is to automatically detect threats being carried by people. Mike Ellenbogen: I've grown up if you will, in doing physical security technology. So Evolv’s actually my third company in this space. My last company was called Reveal Imaging Technologies. We made the systems that TSA uses in about 250 airports around the US to automatically look for explosives in suitcases before they get loaded into the belly of the aircraft. Jennifer: And after that company was acquired… he was looking for his next project. Mike Ellenbogen: And that's when the Sandy Hook shooting happened. So it was obvious at that point in time that what the world really needed was an answer to this active shooter problem. How do we keep threats like guns, bombs, you know, large threats to the crowd, tactical knives, things like that, out of venues that just want to create a safe environment. How do we enable security professionals to create a safe environment, but recognize that they still need to move a lot of people in and out quickly? So we set up Evolv to do exactly that. That's why we exist. Jennifer: He says the system uses a combination of sensors and machine learning to discern between everyday items and those that pose a threat. Mike Ellenbogen: So rather than that old school analog metal detector, the Evolv system's a digital platform at your front door. It's got the core weapons detection capability, but there's also cameras that we can use for different applications. The other sensors that we can add and other capabilities that we can provide, including the data that come from all that, that helps the venue optimize their operations. Jennifer: And he says they’re working on ways to integrate it with other systems. Mike Ellenbogen: We think there's a great opportunity to combine credentialing. So ticketing is a form of credentialing, right? An ID, you know, that it takes to get into your building is credentialing. So bringing those two things together using, whether it's face based recognition, maybe phone based. There’s a number of different ways in which we can do that. We spend a lot of time trying to understand the lay of the land and figure out which of the best, either, technologies that are either existing or that we might want to develop to address some of these needs. [music] Jennifer: You can find links to our reporting in the show notes... and you can support our journalism by going to tech review dot com, slash subscribe. We’ll be back… right after this. [MIDROLL] Donald Maye: Certainly with a product like Evolv, you know, they've raised hundreds of millions of dollars with aspirations to be part of everyday life in the United States and the world. And, you know, that is lofty expectations and we felt and feel that the public should really understand what this technology can and cannot do. I'm Donald Maye. I'm the head of operations of IPVM and IPVM is an organization that specializes in reporting on the video surveillance and security industry. We're a team of researchers and reporters that really try to understand the underlying technology that's being sold in the marketplace. And really what we try to do is be an objective resource that isn't influenced by the companies or the sellers of this technology. Jennifer: His team spent eight months trying to study how Evolv’s systems work. Donald Maye: What stands out is they bill themselves as a weapons detector and, and they go so far as to say on their website what they're not, and what they're not is a metal detector. And like, for us, that's wildly deceptive because when you look at the underlying technology, it is in fact a metal detector. One that's able to better distinguish between certain metal objects and make a determination of whether something is benign or an actual object of threat. Jennifer: But his tests found that strollers, umbrellas and eyeglass cases were often mislabeled as weapons. Donald Maye: What each of these items have as a challenge for the Evolv system to differentiate between those in a weapon is that they have properties that are similar to weapons and so the Evolv technology struggles to differentiate between those two. If you have a high volume area, you know, where thousands of people are walking through in an hour, you're gonna have many, many, many secondary alerts, particularly in environments where it's not expected for people to not be carrying metal objects and an environment like that would be the subway. You know, people, especially in New York City, take all kinds of metal objects onto the subway, which is much different than, say, going to a sports venue or going to a museum. Subways have people going about their day to day lives and so you can think very clearly now if you, if you understand this underlying limitation of the system, that it's gonna be a challenge logistically to deploy it in all the places that Evolv is seeking to deploy it. Jennifer: It also presents a problem for schools where Evolv systems are already deployed. Because school-issued laptops are another common culprit of those false positives. Donald Maye: There's metal hinges in certain Chromebooks that have properties similar to a gun. There's some cylinder shaped componentry to it and it's curved and looks like the outline of a gun. Jennifer: And while he says Evolv’s systems have something of an internal workaround for that detection issue… it comes with key tradeoffs. Donald Maye: One of the more enlightening pieces of information that we learned was from a school board meeting where an Evolv employee was describing the trade off of settings as it relates to Chromebooks and guns. And what he stated was, he goes, if you go on setting C, which is a lower sensitivity setting, he goes, you won't alert on Chromebooks. However, you might miss certain guns. Then he says it's a difficult give and take. And he specifically noted subcompact handguns. And so for me, I heard it. I go, well, that's, that is a difficult give and take. If you're a school you're having to decide whether or not I wanna do the work around where, where I'm asking people and students to hold up their Chromebook or go to a lower sensitivity setting and run the risk of someone, you know, not alerting on a gun. Jennifer: And Donald Maye says it’s not clear what variables the algorithm weighs when that sensitivity is adjusted. And this decision between efficiency and accuracy… one with potentially devastating consequences… is left up to the people using the device, which in this case, might fall to a mix of volunteers from the teaching staff. Donald Maye: Being presented information and not having the ability to reliably dissect it and understand it. And I think that disconnect in information creates an imbalance of power between the buyers and end users and the people who are selling it. And that can lead to a lot of problems. Jennifer: And what he’s describing… extends well beyond this topic… because the world is becoming ever more technical, and with it, that information imbalance just keeps on getting wider. It’s a bit like looking under the hood of a car. In the past drivers could understand the basic mechanics of how it all worked. Not so much anymore. And back to that laptop problem… making the system less sensitive could miss some weapons… but on the other hand, alarms going off all day are likely to be ignored. Mike Ellenbogen: Yeah. It's a, it's a real challenge within the security domain, right? Whether it's physical or cyber, you know, we call it alert fatigue. Jennifer: Mike Ellenbogen… is the cofounder of Evolv Mike Ellenbogen: If you remember, when car alarms were first introduced, right? If a car alarm went off, you know, in the street, outside your apartment, you know, you'd run to the window to see what's going on. And then they just became annoying and you would ignore them because they were happening all the time. And then people kind of just turned them off. And that's what happens with a lot of these technologies that aren't quite ready for primetime. That those false alarms create. That they undermine people's faith in the technology. And then it just becomes a nuisance. [CREDITS] Jennifer: This episode was produced by Anthony Green and Emma Cillekens with reporting from Mark Keierleber. It was edited by me, Rachel Courtland and Mat Honan… and mixed by Garret Lang… with original music from Jacob Gorski. If you have an idea for a story or something you’d like to hear, please drop a note to podcasts at technology review dot com. Thanks for listening… I’m Jennifer Strong. [TR ID]",0.0
A digital human could be your next favorite celebrity—or financial advisor,https://www.technologyreview.com/2022/07/21/1056335/a-digital-human-could-be-your-next-favorite-celebrity-or-financial-advisor/,2022-07-21,"When one of China’s biggest celebrities, Simon Gong—also known as Gong Jun—released a new music video in June 2022, it quickly attracted 15 million views on the country’s Twitter-like microblogging site Weibo. But the event also stood out for a different reason—one that only eagle-eyed fans might have noticed. The singer in the video was not…","Provided byBaidu When one of China’s biggest celebrities, Simon Gong—also known as Gong Jun—released a new music video in June 2022, it quickly attracted 15 million views on the country’s Twitter-like microblogging site Weibo. But the event also stood out for a different reason—one that only eagle-eyed fans might have noticed. The singer in the video was not Gong himself, but a digital replica created by Baidu, a “digital human” powered by artificial intelligence (AI). Likewise, the lyrics and melody were generated by AI, marking the recording as China’s first AI-generated content music video. Deloitte defines digital humans as AI-powered virtual beings that can produce a whole range of human body language. In recent years, businesses focused on providing round-the-clock services, as well as the media and entertainment industry, are increasingly adopting this nascent technology, aiming to capture a growing market. And as digital humans increasingly populate other sectors like retail, health care, and finance, Emergen Research forecasts that the global market for digital humans will jump to about $530 billion in 2030, from $10 billion in 2020. “Rising demand is driving the boom of digital humans,” says Shiyan Li, head of the digital human and robotics business at Baidu, which created the digital model-actor, Gong. “In China alone, there are over 400 million ACGN (animation, comics, games, and novel) fans, and an enterprise market worth hundreds of billions of dollars centered on digital humans.” And according to a company that tracks business registrations, Qichacha, China now has more than 280,000 enterprises that engage in digital human-related activities. The debut of Baidu’s digital celebrity may not seem like much at first, as the concept of “virtual idols” has been around for years. For example, US virtual influencer Lil Miquela has been appearing alongside real human celebrities in online advertisements and TV commercials since 2016, gaining over three million Instagram followers. However, there is something different about the virtual Chinese star: a digital human with the ability to listen, speak, and interact with real humans at a level never seen before. And Gong’s digital duties are not limited to singing. On the latest update of Baidu App, China’s leading search-plus-feed app, Gong appears on users’ phones, helping with searches and queries using the model-actor’s real voice. Since this interactive search experience was launched in 2021, it has boosted the number of voice search queries on Baidu App by 18.2%. Baidu AI Cloud first began developing a digital employee in 2019 in collaboration with Shanghai Pudong Development (SPD) Bank. Subsequently, they focused their efforts on building a digital financial advisor to provide a service equivalent to that of a human bank representative when real-life employees were unavailable. Today, SPD Bank says more than 460,000 customers rely on digital humans for banking services and portfolio management each month. “Access to digital humans outside of regular business hours allows SPD Bank to offer 24/7 customer service at low cost and high efficiency,” says a bank representative. More recently, a Baidu-created virtual anchor provided live commentary in sign language at the 2022 Beijing Winter Games for hearing-impaired viewers. In addition to looking like a real person, the avatar was empowered with speech recognition and sign-language interpretation abilities to ensure rapid and highly accurate input and output. With approximately 430 million people around the world experiencing “disabling” hearing loss, according to the World Health Organization, there is strong potential for this technology to be used to increase their ability to access a wide range of content. From entertainment to public services, digital humans are set to play a greater role in our daily lives. But behind their natural and effortless appearance is a complex web of new and emerging technologies pushing the boundaries of AI innovation. Baidu AI Cloud’s digital celebrity and virtual sign-language anchors were created through XiLing, a new digital platform launched in 2021. At the Baidu World 2022 event held on July 21, the company announced a new capability on XiLing, which supports the creation of digital humans that can be livestream hosts who can sing, dance, and respond to comments in real-time—without ever needing a single break. XiLing is unique in its ability to support the entire process of creating a digital human from crafting a realistic persona to endowing it with conversational and content-generation skills. One of its most striking attributes is speed. The platform can generate a 3D avatar based on a real person in one to two weeks, while a 2D avatar can be made in just a matter of minutes. In addition, using XiLing’s intelligent dialogue tools, creators can quickly customize a digital human’s conversational ability, letting it adapt and learn over time. This capability is powered by Baidu’s PLATO, a hundred-billion-parameter dialogue model that enables digital humans to participate in open-domain conversations—that is, to understand any topic and provide relevant responses. Highly accurate speech recognition and lip-syncing with above-98.5% accuracy allows the digital human to have smoother, more human-like interactions. “Use of advanced AI technologies will keep bringing down the cost of building digital humans and significantly improve their interactions with real humans,” says Li. Just as every real human has their own set of skills and talents, so too does the new generation of digital humans. This can even include giving digital humans the ability to be creative themselves, thanks to the recent progress made by large AI models like Baidu’s ERNIE, which can generate texts and create realistic images when prompted. Digital humans designed to serve as brand spokespersons, for example, can independently create and post on social media, design posters, and perform in videos. Digital humans and their virtual world do not merely represent reproductions of real humans and our physical world, but they could also create an entirely new medium of expression in next-generation social media worlds. “Web3 and the metaverse have sparked a wave of speculations in the tech field today about what the future may bring,” says Li. “A digital replica of self will be core to the metaverse. Digital people for customer service will continue to serve the metaverse with a better experience than a pure graphic interface.” Yanxia Lu, assistant research director at IDC China, says that the benefits are already clear. ""Digital humans are already demonstrating clear business value in numerous fields today,” says Lu. “In the future, there will definitely be a large team of digital humans coexisting side-by-side with us in life and work."" In other words, virtual financial advisors and avatar translators are just the beginning. Today, digital humans with the capability to adapt and learn are already poised to dramatically expand access to essential services and more. As society transitions to a more digital world, digital humans are set to play a key role in accompanying us on this journey. This content was produced by Baidu. It was not written by MIT Technology Review's editorial staff.",0.0
An edit button won’t fix Twitter’s problems,https://www.technologyreview.com/2022/09/01/1058931/edit-button-fix-twitter-problems/,2022-09-01,"<p>Twitter’s users have asked for an edit button for years. Giving them one isn’t going to change their behavior for the better.</p>
","After years of requests, Twitter is finally introducing an edit button, giving its users the ability to change their tweets up to 30 minutes after they’ve been sent. But the feature is unlikely to solve any of the biggest problems facing the company—and in some cases, it could worsen them. The feature will initially be restricted to Twitter staff for testing, before rolling out to subscribers to Twitter’s $5 monthly subscription service Blue later this month. Given that Twitter Blue is a test bed for the company’s new features, it’s highly likely that an edit button will be eventually made available for all users. Twitter has resisted adding the ability to edit tweets for years, even though this has been the most requested feature from its users, including would-be owner Elon Musk. Former chief executive Jack Dorsey said in 2020 that the company would probably never introduce an edit button, explaining that doing so would ruin the “vibe” from Twitter’s early days as an SMS messaging service. Experts have repeatedly pointed out that the ability to edit tweets could allow bad actors to rewrite history and spread misinformation, even if a full history of tweets is available. For example, harmless tweets that go viral could easily be edited to later display disinformation or hate speech, and even if the tweet’s previous versions are visible, that doesn’t necessarily mean people will look at them. An edit button would also, in theory, make high-profile users whose tweets garner mass attention even bigger targets for hacking, if bad actors know the tweets are guaranteed a mass audience. Users will be alerted to the fact that tweets have been edited by an icon, time stamp, and label, which Twitter said is designed to make it clear that the original message has been modified within half an hour of being sent. Tweets can be edited “a few times” within that time frame, and a log of how a tweet has been changed will be displayed when someone taps the label. ""Our goal with testing is to better understand how Edited Tweets will impact the way people use Twitter as well as plan for and anticipate what might happen if we bring it to everyone,"" a Twitter spokesperson said. The company has acknowledged that people might misuse the feature and says it is testing for that potential. It’s likely an attempt to downplay the significance, says Konstantinos Komaitis, an internet policy expert. “Depending on how Twitter decides to design this, it can either help people with typos and there’s nothing more to it, or it can actually shift, I believe, the whole public discourse and the way we interact and share an understanding,” he says. Giving users an edit button could also be interpreted as a handy distraction from the deeper problems the platform is dealing with: its forthcoming legal tussle with Musk, the glaring privacy and security issues laid bare by former security head turned whistleblower Peiter “Mudge” Zatko, and ongoing concerns about its deep-seated inability to curb trolling, hate speech, and other toxic behaviors. An edit button does nothing to solve these issues. Notifying users that a tweet has been edited will be essential to minimizing the possibility for abuse, Komaitis pointed out, using the example of someone tweeting a picture of a cute dog to generate positive responses and then swapping it for a picture of Hitler. “We know these situations can happen, and it’s not because of Twitter or the internet, but because that’s how society currently functions,” he said, adding that the potential benefits of an edit button are unlikely to outweigh the possibilities for abuse. “Twitter needs to come up with as many safeguards as possible in order to ensure that it is purely for small mistakes, or a regrettable choice of words, rather than completely changing the way the conversations are taking place.” By limiting the feature to paying subscribers at least for now, Twitter could dramatically shrink, although not fully eradicate, the pool of users who are likely to use it maliciously. But it also raises questions over whether an edited tweet will count as a real tweet. This could warp the number of daily active users. And it’s also debatable whether using paying subscribers to test the feature is really equivalent to handing it over to the platform’s most toxic user base. Ultimately, the popularity of the idea of an edit button speaks to our perfectionism, says Charles Arthur, author of Social Warming: How Social Media Polarizes Us All. “We seem bizarrely desirous of this ability to edit our lives—of using technologies to roll back time, which points to a sort of societal anxiety of ‘Oh no, did I say the wrong thing?’” he says. “We don’t have the confidence in what we’ve said, even if we’ve got it a bit wrong. The trouble is, anything that can be used maliciously will be used maliciously, and Twitter is the absolute hotbed of people doing malicious things.” Update: a response from a Twitter spokesperson has been added.",0.0
New 6G challenges inspire cross-disciplinary innovation,https://www.technologyreview.com/2022/08/29/1058452/new-6g-challenges-inspire-cross-disciplinary-innovation/,2022-08-29,"Roger Nichols remembers sending his first e-mail using wireless networks in the early 1990s, from the back of a bus during his daily commute. That was 30 years ago, on a 1G network—at a data rate about fifteen thousand times slower than today. Now the 6G program manager at Keysight Technologies, Nichols sees the rapid…","In association withKeysight Roger Nichols remembers sending his first e-mail using wireless networks in the early 1990s, from the back of a bus during his daily commute. That was 30 years ago, on a 1G network—at a data rate about fifteen thousand times slower than today. Now the 6G program manager at Keysight Technologies, Nichols sees the rapid growth of the current mobile wireless standard and knows that there is much more to come. As of June 2022, the Global Mobile Suppliers Association counted nearly 500 operators in 145 countries that had deployed—or planned to deploy—5G capabilities, up from 412 at the end of 2020. Omdia and 5G Americas project that by the end of 2022, more than 1.3 billion connections will be made to global 5G networks. “The intervening decades have seen an explosion in the use of wireless data connections,” he says. “5G systems will not only help with increasing demands for speed, latency, and reliability, but also with the flexibility required to make the most of the costly resources of modern networks: spectrum and energy.” But 5G is just the current stage in the evolution of mobile wireless networks. Today’s dramatic growth in 5G availability is the continuation of a march toward advanced wireless capabilities, one that began more than four decades ago when Japan’s NTT deployed the first automated mobile network. And even while this current generation of wireless connectivity is deployed, engineers and technologists are aiming to push the technology into its next generation, 6G. It’s clear that tomorrow’s innovative applications will require still better connectivity. Augmented reality will allow workers to enhance their surroundings with detailed information or will connect consumers through virtual worlds. Networked devices that collect data from physical objects—from airplanes to tires to infrastructure—promise to deliver more intelligence to management systems. Connected vehicles will communicate with one another, improving driving efficiency and safety. “How 6G will be used is speculative,” Nichols says; however, “the list of 6G use cases varies from ‘5G-on-steroids’ to what looks like science fiction.” To create 6G infrastructure, devices, and software, however, engineers and researchers will have to solve a plethora of problems. Aiming to improve mobile wireless connectivity by an order of magnitude presents challenges that can’t be addressed by merely scaling existing technology. These include mastering the physics of high-frequency signals, managing space requirements within devices for multiple wireless chips and hardware, and developing the software needed to automate the management of distributed and programmable networks. The coming 6G systems will add additional non-contiguous spectrum to an already complex spectrum map. They will also bring more sophisticated active-antenna systems, further integration into networks using other Radio Access Technologies (such as WLAN, Bluetooth, UWB, and Satellite), and joint communications and sensing technology. Integrating all of this into a single device, such as a smartphone, will demand a huge and complex variety of radio transceiver technology. This will require very creative electrical and computer engineering as well was disruptive industrial engineering and power management. Computing power and data storage will be ever increasing challenges for the new high-speed and programmable communication networks. To accommodate exponentially growing populations of devices that need to access the 6G network, new chips will need to process signals more quickly, requiring more power and faster storage. Managing the sensors required to create detailed digital twins—simulations based on the collection of real-world data—also requires a great deal more processing power and fast storage space than is available on current networks. Providing low latency (minimal delay) is already a familiar challenge for mobile networks, and one that will continue as next-generation applications increasingly allow interactive manipulation of data, responsive virtual environments, or real-time monitoring and management of remote systems. New 6G applications will layer on a need for extremely precise timing—absolute predictability of when data packets will be sent or received. Addressing these physical and technical limitations will require leaps of innovation, but the promise of applications powered by advanced 6G connectivity is motivating creative solutions. Adaptive technology solutions are a key area of research. Rather than focus on optimizing the bandwidth for a single device, for example, the 6G network will use nearby devices to help deliver the necessary bandwidth and reduce latency. This 3D signal shaping focuses on combining and processing wireless signals from multiple sources, based on their proximity to the end user. New semiconductor materials will help manage device space requirements as well as handle wider frequency bands. Though it requires complex engineering, one promising approach combines traditional silicon circuits with those made from more exotic compound semiconductors, such as indium phosphide. In addition, researchers are looking at ways of changing the environment with reconfigurable intelligent surfaces (“smart surfaces”) that can optimize signal propagation to modify signals in real time to deliver better bandwidth and lower latency. Another avenue of research relies on artificial intelligence to manage networks and optimize communications. Different types of network usage (texting, gaming, and streaming, for example) create different types of network demand. AI solutions enable a system to predict this demand based on behavioral patterns, instead of requiring engineers to always design for the highest demand levels. Nichols sees great potential for networks from improvements in artificial intelligence. “Today’s systems are so complex, with so many levers to pull to address the diverse demands,” says Nichols, “that most decisions on optimizing are limited to first-order adjustments like more sites, updated radios, better backhaul, more efficient data gateways, and throttling certain users.” By contrast, employing artificial intelligence to handle the optimization, he says, presents “a significant opportunity for a move to autonomous, self-optimized, and self-organized networks.” Virtual simulations and digital-twin technology are promising tools that will not only will assist in 6G innovation but will be further enabled by 6G once established. These emerging technologies can help companies test their products and systems in a sandbox that simulates real-world conditions, allowing equipment makers and application developers to test concepts in complex environments and create early product prototypes for 6G networks. While engineers and researchers have proposed innovative solutions, Nichols notes that building 6G networks will also require consensus between technology providers, operators, and carriers. While the rollout of 5G networks continues, industry players should create a cohesive vision for what applications the next-generation network will support and how their technologies will work together. It is this collaboration and complexity, however, that may generate the most exciting and enduring outcomes. Nichols notes that the breadth of engineering specialties required to build 6G, and the industry collaboration necessary to launch it, will drive exciting cross-disciplinary innovation. Because of the resulting demand for new solutions, the path to 6G will be paved, in Nichols’ words, with “a tremendous amount of technical research, development, and innovation from electronics to semiconductors to antennas to radio network systems to internet protocols to artificial intelligence to cybersecurity.” This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.",0.0
A pioneer of reproductive rights,https://www.technologyreview.com/2022/08/24/1058195/katharine-dexter-mccormick-repoductive-rights/,2022-08-24,"<p>Katharine Dexter McCormick changed the lives of millions of people.</p>
",,0.0
Meet the designer behind gender-neutral emoji,https://www.technologyreview.com/2022/08/23/1057142/gender-neutral-emoji-designer/,2022-08-23,"<p>Paul D. Hunt pushed the Unicode Consortium away from stereotypical gender iconography.</p>
","Last year the Unicode Consortium—the group responsible for the selection and design of emoji—released a new series that reflected the multiplicity of gender identities. That’s thanks to Paul D. Hunt, who since 2016 has been a key advocate for making emoji more inclusive, less sexist, and a better reflection of the human experience. Fighting to dismantle the gender stereotypes we see in emoji may seem unimportant. But consider that since their invention in 1999 by Shigetaka Kurita, emoji have expanded from 176 simple, pixelated icons to (as of September 2021) 3,633 increasingly detailed images. Every day, more and more people around the world have access to mobile phones and to emoji that add expressiveness to their text-based communications. The fight for gender inclusivity in emoji is personal for Hunt, who is nonbinary and transgender. Hunt is also a trained typographer and designer rooted in linguistics and art. There may be no better person on the planet to think about what it means to produce and consume emoji that reflect the multiplicity of gender identities. Hunt’s interest in “language and alphabets and design and culture” was rooted in their small-town childhood in a Mormon community embedded within the Navajo Nation in Arizona. They went to college intending to study for an international business degree but switched to design. Hunt was active in an online community of typographers, Typophile, while interning at a type foundry in Buffalo, New York, and spent time drawing letters and designing fonts before earning a master’s in typeface design at the University of Reading in the UK and becoming a leading typeface designer at Adobe, specializing in fonts that don’t use the Latin alphabet. But it is their participation in the emoji subcommittee of the Unicode Consortium that has garnered Hunt the most acclaim. And their thinking on gender and emoji had a surprising source: RuPaul’s Drag Race. At first, Hunt rolled their eyes at the flamboyance of the Drag Race contestants. “I used to think RuPaul was too camp, and I didn’t really understand this whole drag queen phenomenon,” they say. But Hunt’s husband was a fan, so Hunt began watching the show and was increasingly drawn into it, moved by the contestants’ refusal to fall into conventional gender roles and stereotypes. This led to an epiphany: gender was a performance. Every day we make choices “to skew our appearance one way or another, whether that’s masculine or feminine,” they explain, “and it made me ask what it means to be masculine or feminine.” As the new head of the Emoji Subcommittee for the Unicode Consortium, Jennifer Daniel has a vision for how to make these symbols work for everyone. Emoji tended to codify gender with traditional signs of masculinity (beard, mustache, short hair) and femininity (painted nails, longer hair, skirts). Hunt found this limiting, even disturbing: Why was a nurse a woman and a police officer a man? Why were “frivolous” activities like getting your nails painted or dancing depicted as feminine, while “serious” activities like construction were always depicted as masculine? Why were these images so staunchly gendered anyway? Hunt decided to do something about this. They were already part of the Emoji Subcommittee, a group of designers and industry experts within the nonprofit Unicode Consortium, which works with hardware and software companies to make emoji readable and universal across all devices. So in 2016, Hunt submitted a proposal to push for gender-inclusive emoji, which they defined as “a humanized appearance that employs visual cues that are common to all genders by excluding stereotypes that are either explicitly masculine or feminine.” It was revolutionary. To many, emoji were cutesy, simplistic additions to text, not humanistic and certainly not political. Hunt acknowledges as much, diplomatically saying there was a bit of skepticism from those running the committee. Some designers pointed to Google, which had tried to skirt gender and race with its yellow blobs in Gchat. On some level this worked, but Hunt found the accommodation a bit odd: Why couldn’t emoji express more of the nuances of human experience without resorting to abstraction? Hunt’s proposal found an audience in Jennifer Daniel, who now leads the Unicode Emoji Subcommittee and has been instrumental in redefining the linguistics of emoji by ushering in an era that celebrates inclusivity and creative use of the symbols as a means of expression. Daniel told me that when she joined the subcommittee, in 2018, “none of them [the gender-inclusive emoji Hunt had proposed] were properly supported.” She pushed for implementation of Hunt’s proposal, releasing guidelines for the creation of a gender-neutral class of emoji as well. For Hunt, emoji are powerful means of expression precisely because words sometimes fail us. They recall meeting their future husband, an Australian, while living in San Francisco: “When you get to know someone, you build a common story together and develop your own little language.” That language for Hunt and their spouse included the heart emoji with sprinkles, which became a “logo” for the budding relationship. “That emoji meant a lot to me,” they say. “It still does.”",0.0
"Exposing the messy, technologized, and undervalued nature of reproductive labor",https://www.technologyreview.com/2022/08/23/1057617/ani-liu-art-reproductive-labor/,2022-08-23,"<p>Artist Ani Liu’s work stands out for the precision with which it zeroes in on the frustrations of 21st-century motherhood.</p>
","Messy coils of plastic tubing sprawl across the gallery’s concrete floor. The liquid inside—opaque, white with a yellowish tinge—pulses once, twice, and the eye tracks its progress thanks to the air bubbles cycling through the loops. Could that be … milk? Follow the tubing back to an unassuming rectangular box. If it is milk, a panicked brain might ask, where is the mother? Having a child is an inherently optimistic act. People in the US are doing less and less of it. At this moment the mother, artist Ani Liu, is standing by the door of the pocket-­size Cuchifritos Gallery + Project Space in Lower Manhattan, wrapped in a tie-dyed T-shirt dress for tonight’s opening of her solo exhibition, “Ecologies of Care.” But she has also sat, pumping milk, in the broom closet next to her classroom at the University of Pennsylvania; in her basement studio in Queens; on trains and in cars. The volume of milk circulating through Untitled (pumping) and Untitled (feeding through space and time) represents a week of such sessions, or 5.85 gallons—some of the invisible labor of motherhood. It also represents modern breastfeeding technology—specifically, the Spectra pump that allowed Liu the alleged freedom to return to the workplace just weeks after having her first child. After headlines about a national formula shortage earlier this year, the liquid seems even more precious. The milk in this exhibit is not real. After much experimenting, Liu ended up filling the pump with “magician’s milk,” a proprietary formula, purchased from a magic shop, that requires no refrigeration and comes with the warning “Not a food product. Do not drink!” Nevertheless, it looks convincing. When installing the piece, Liu originally considered having the milky coils take over the whole gallery floor, even more aggressively immersing visitors in the visual and aural landscape of newborn care. Liu, who holds graduate degrees from the Harvard Graduate School of Design and the MIT Media Lab, resumed teaching just five days after she gave birth in 2021. She had just signed a new contract as an associate professor of practice at Penn, and the university offers maternity leave only to employees of more than a year. Though she was initially allowed to teach over Zoom from her home in Queens, she ended up pumping more than nursing. “I developed this really intense relationship with my pump, where just hearing the sound of it made me let down, rather than my baby’s cry. It was just such a weird Donna Haraway cyborg moment,” she says, referring to the feminist science and technology scholar who wrote, of the cyborg, that it “does not dream of community on the model of the organic family.” It was also a moment that led Liu to new research. Her discoveries emerge in the works on display in the Cuchifritos Gallery, which include a series of three-dimensional meditations on technology, motherhood, and childhood in our algorithm-­enabled world. Pumping is also on displayas part of the second edition of the exhibition “Designing Motherhood,” now at the MassArt Art Museum in Boston. Michelle Millar Fisher, part of the curatorial team, wrote that the work “cuts right to the heart of the ways in which reproductive labor is hidden, romanticized, socially taboo, and undervalued.” Liu’s work has additional urgency and resonance after the overturning of Roe v. Wade. Who controls, who supports, and who performs reproductive labor are not just bedroom or broom closet questions (and never should have been); they are playing out on the streets, in state houses, and in the Supreme Court. Millar Fisher has drawn parallels between Liu’s pumping installation and the work of the artist Hiromi Marissa Ozaki, known as Sputniko!, whose 2010 Menstruation Machine simulates the experience of menstruation; the video part of the piece shows a fictional day in the life of a young man who builds a device to experience life as a person with a uterus. Liu has long been fascinated by this sort of simulated experience. In 2019, after watching YouTube videos of men sampling simulated labor pains in order to understand their wives’ experience, and finding them wanting on multiple levels, she decided to create her own apparatuses, including a garment called Untitled (woman pains), fitted with a belly and electrodes, that would allow any non-pregnant person to experience the weight and discomforts of pregnancy. Another in the series, Untitled (small inconveniences), simulates incontinence. Made in collaboration with fabricator Randi Shandroski, the garments look like lingerie and simulate one result of sex, but these are not experiences generally considered sexy. Her pieces demonstrate a mischievous humor, embedded in the everyday indignities of modern life. Consumer culture might seem to celebrate pregnancy, but the products pushed to pregnant people focus on all the things that are “wrong” with the pregnant body: mood swings, stretch marks, incontinence. In response, Liu created Consumerist Pregnancy, which includes a series of creams, masks, and medications, designed in high millennial style (monochrome packaging, sans serif fonts) but honestly labeled “Fatigue,” “Shortness of Breath,” “Swelling.” If you saw them on a pharmacy shelf you would be initially attracted, but once you read the description, even as a person who has been pregnant, it would be hard not to say No, thank you. The Surrogacy features a 3D-printed model of a multichambered pig’s uterus which, upon approach, reveals human fetuses in each chamber, a commentary on the ethics of assisted reproduction and the exploitation of human surrogates. Hung on the wall of the gallery is a spreadsheet that, upon inspection, reveals itself to be a minute-by-minute accounting of all the touchpoints of the first month of Liu’s daughter’s life (she’s now two): every feeding, every pee, every poop. One inspiration for the spreadsheet was Post-Partum Document, a 1973–’79 work by conceptual artist Mary Kelly, in which Kelly displayed the liners from her son’s cloth diapers as monochromes in white frames. Untitled (Consumerist Pregnancy Reports): A set of ritual devices and consumable products made to simulate the biological experience of pregnancy. Liu’s work stands out not just for its topicality, or the precision with which it zeroes in on the frustrations of 21st-­century motherhood, but for its range. She is the kind of artist of whom you might say that she “works at the intersection of art and technology.” But that should really be “technologies,” plural. Different pieces have required her to delve into the intricacies of pumps, circuitry, machine learning, microscopy, and 3D printing, developing enough understanding of each field to identify the necessary expertise of her collaborators. When she was at the Media Lab she joined a biohacking club, which she found to be the ideal educational experience. No matter the question, she says, “someone would sit down with a beer and explain it.” Even her thesis project at the Media Lab, Mind Controlled Spermatozoa (2016–2017), recently wiggled back into the news as six Supreme Court justices, five male and one female, declared jurisdiction over child-bearing bodies. For that project Liu researched galvanotaxis, the directed movement of an organism or cell in response to an electric field or current. In the accompanying video, she dons an EEG machine, which measures electrical activity in the brain. She then applies those signals to a magnified sample of her now-husband’s sperm and is actually able to direct the movement of spermatozoa to the left and right with her thoughts. (The science is real, if dramatized for the video.) As she writes, “I seek to challenge this status quo by engineering a system by which I (a woman) can control something inherently and symbolical male: spermatozoa (sperm).” The subtitle of the piece is “Women of STEAM Grab Back.” A week after the leak of the draft Supreme Court decision overturning Roe v. Wade, Liu put the piece up on her Instagram with a new caption. “In the few times I’ve shown this work, men have often expressed to me how violating and unnatural it is to control sperm—sperm that is not even theirs, or in their body,” she wrote. “[T]hink of the plight of female bodies, that are constantly under threat of being controlled, regulated, censored.” Liu’s work attempts to shed light on the constant policing of cisgender women’s bodies, using the very machines and marketing techniques that typically oppress. Design critic Alexandra Lange is the author of Meet Me by the Fountain: An Inside History of the Mall.",0.0
We may never fully know how video games affect our well-being,https://www.technologyreview.com/2022/08/20/1058328/we-may-never-fully-know-how-video-games-affect-our-well-being/,2022-08-20,"<p>But researchers have pinpointed the data might help to provide more clues.</p>
","For decades, lawmakers, researchers, journalists, and parents have worried that video games are bad for us: that they encourage violent behavior or harm mental health. These fears have spilled into policy decisions affecting millions of people. The World Health Organization added “gaming disorder” to its International Classification of Diseases (ICD) in 2019, while China restricts people under 18 from playing games for more than three hours a week in a bid to prevent minors from becoming addicted. However, in recent years a growing body of research has argued that video games are in fact good for us, improving cognition, relieving stress, and bolstering communication skills. The reality, a new study suggests, is that we simply don’t have a good grip on how games affect our well-being, if at all. The research, described in the Royal Society Open Science journal last month, found little to no evidence for a causal connection between game play and well-being, meaning that time spent playing video games had neither a negative nor positive effect on players’ emotional health. Researchers from the Oxford Internet Institute (OII) at the University of Oxford analyzed how long 38,935 players spent playing seven different games: Animal Crossing: New Horizons, Apex Legends, Eve Online, Forza Horizon 4, Gran Turismo Sport, Outriders, and The Crew 2. This data was provided directly by the games’ publishers—a rarity, as the vast majority of studies on video games rely on players’ self-reports of how long they spend gaming. The Oxford team says such data is biased and rarely accurate. The gamers’ well-being was assessed through three surveys taken every two weeks over a six-week period. People ranked how often they experienced feelings including “pleasant” and “unpleasant,” and measured their general life satisfaction using the Cantril self-anchoring scale, picturing an imaginary ladder with the top rung representing their best possible life. Additionally, they answered questions about their experiences and motivations. The researchers say that examining players’ emotional well-being through their moods and emotional experiences is a stepping-stone to assessing mental health. Although the amount of time the participants spent playing games showed limited if any impact on their well-being, and the way they felt didn’t affect how long they spent gaming, their motivations did have an impact on their emotional state. Participants who played because they wanted to, rather than feeling compelled to play to beat a high score, for example, reported higher levels of well-being, although the relationship was small. Gamers would need to clock up an additional 10 hours a day on top of their average play for any noticeable effect to be observed. The research builds on the findings of a smaller study the same team published in 2020, which found a small positive relation between game play and well-being. This new study is the largest of its kind based on real player behavior collected from real games, which its authors say is a first step toward explicitly determining the real-world causal effects of playing video games on well-being over time.The findings demonstrate the complexity of making definite conclusions about how and why playing video games affects us. The science of researching games is relatively new, and studying them is tough because of how varied they are: a simple puzzle app on a smartphone is very different from a sprawling massively multiplayer online game, and modern games contain vast amounts of data. Another factor is that the industry’s technology evolves more quickly than researchers can conduct studies, meaning their methodologies for studying effects on mental health or aggression can be contentious. Arguments over whether game addiction is real have led to feuds between government departments and a national debate over policy. The evidence base that the WHO and Chinese authorities are drawing from is “trash” and seriously mismatched to the scale of the decisions being based on them, says Andrew Przybylski, a senior research fellow at the Oxford Internet Institute and coauthor of the report. “That’s not to say that countries, parents, and regulators don’t have a very serious role to play in making sure that games are safe and a rewarding part of people’s lives,” he says. “It just means that if we’re going to regulate them, and give parents advice, it has to be vaguely evidence based.” The moral panic around video games has stuck in a way that previous entertainment-fueled panics such as those around rock music and TV haven’t. But the evidence isn’t there. Media reports that the perpetrators of mass shootings from the mid-1990s onwards were avid gamers, coupled with a slew of studies starting in the early 2000s, fueled concerns that violent games made people more aggressive. These reports found that participants “punished” opponents for longer, gave taste testers larger doses of hot sauce, and were more likely to guess aggressive words such as “explode” in a word completion task after playing violent games. But other researchers have since questioned how effective these studies really were at measuring violent behavior. A 2020 meta-analysis in Royal Society Open Science, which reexamined 28 studies from previous years, found no evidence for a long-term link between aggressive video games and youth aggression. Lower-quality studies that didn’t use standardized or well-validated measures, it found, were more likely to exaggerate the effects of games on player aggression, while higher-quality studies tended to find negligible effects. The same pattern has repeated with respect to studies linking video games to poor mental health, which tend to report smaller effects once they use objective data on game duration (as the OII study did) rather than relying on subjective self-reporting from participants, says Peter Etchells, a professor of psychology and science communication at Bath Spa University, who thinks the past 20 to 30 years of gaming studies haven’t had a consistent handle on what they were trying to measure or how to do it. “New studies like this one can help to draw a line under this whole ‘Are video games good or bad for us?’ line, because it is and always has been the wrong question to ask,” he says. “It’s like asking ‘Is food bad for our waistline?’ It’s a stupid question.” “My hope is that we can get better at not thinking about it in terms of ‘Are video games, are video games bad?’ but thinking about that gray area in between,” he adds. “Because that’s where all the interesting stuff is.” Przybylski was among a group of academics who wrote to the WHO in 2016 arguing against the “premature” inclusion of gaming disorder in its ICD guidelines, citing the low quality of the research base and the fact that scholars had failed to reach a consensus. Six years on, not much has changed, and researchers are still divided over the extent to which being addicted to games could differ from addiction to substances or gambling, for example. An interesting next step would be to focus on any participants demonstrating problematic behavior in the OII’s study to see how they can be coached or supported, says Tony van Rooij, a senior researcher at the Trimbos Institute in the Netherlands who focuses on gaming, gambling, and digital balance. Another worthwhile area of study, he says, is the predatory business models that game makers use to exert pressure on players’ behavior, including encouraging them to make microtransactions to skip frustrating levels, play at fixed times, or log in daily to avoid missing out on something. “In our research and experience, we tend to find that there is a large group of ‘healthy’ gamers, who benefit from their gaming,” he says. “But there also is a minority of gamers with unhealthy playing habits—often accompanied by various other issues in life. Gaming is not necessarily the cause of these issues, but obviously extreme participation in gaming needs to be taken into account to restore balance. The study is very rigorous and well done, but I hope it will be a starting point, not a final destination.” Przybylski hopes game companies will make it easier for players to share data from their play with independent researchers, though he concedes that the industry has no financial incentive to turn over that data and runs the risk that the studies will return undesirable results. “I think it’s totally crazy that people who are already signing over their genetics and health information for study can’t go in, eyes open, and donate their play data,” he says. “It’s theirs legally. It’s about making the tools available for something more creative than selling ads or figuring out new ways to monetize players.” Ultimately, despite researchers’ best efforts, academics studying games are unlikely to reach a solid conclusion on how they affect us, says Yemaya Halbrook, a psychology researcher for the Lero Esport Science Research Lab at the University of Limerick in Ireland. “While we have been moving away from that slowly over the last decade or so, I don’t think there’s ever going to be a general consensus that video games have no positive or negative effect, or only a positive effect. There’s always going to be those people that say that video games are bad for you, and cite biased research,” she says. “We might be able to move them in a direction that says games aren’t entirely bad, but I don’t think we’ll ever get everybody to agree on a singular point, even if it’s a complete, total fact. People are not like that.”",0.0
Ring’s new TV show is a brilliant but ominous viral marketing ploy,https://www.technologyreview.com/2022/08/19/1058259/ring-nation-new-tv-show-viral-marketing/,2022-08-19,"<p><em>Ring Nation</em> wants to lure you in with funny content—and push you to buy a Ring camera to make your own.</p>
","There’s a genre of video floating around TikTok, Facebook, Nextdoor, and countless other social apps that you’ve probably seen, or at least scrolled past. They’re characterized by their brevity, their fish-eye framing, and often their prominent logo placement—which typically reads “Ring.com.” This footage comes from Ring customers, who install the company’s camera devices to protect their homes, keep an eye on deliveries, and see or interact with who’s at the door. The #RingDoorbell hashtag has 2.5 billion views on TikTok alone. Ring captures have become their own category of viral fame, and with the premiere next month of Ring Nation, videos like these will soon have the potential to reach even larger audiences. At the same time, the show will launder the image of Ring—a company that, over the past almost-decade, has been continuously criticized for its often lax approach to customer data, and especially for allowing law enforcement to access user videos without consent. Announced last week, Ring Nation will craft a careful image, featuring funny animals, marriage proposals, and heartwarming neighborhood interactions, according to a press release announcing the show. And, perhaps to distinguish the viewing experience from that of just watching any random playlist of Ring videos online, it will be narrated by comedian Wanda Sykes. It comes from MGM studios—which, along with Ring itself, is owned by Amazon—and is produced by Big Fish, the studio behind On Patrol: Live (previously known as LivePD, a controversial reality show that has also blurred the line between police action, and sometimes violence, and entertainment). “Our customers share videos with us all the time demonstrating how their Ring products and services help connect them with their communities, and protect what matters most to them,” Ring representative Emma Daniels told MIT Technology Review in an email. These videos will likely beget more videos, says Matthew Guariglia, a policy analyst at the Electronic Frontier Foundation who focuses on privacy. The TV show will highlight for viewers how “there are so many funny little moments that will be priceless—when they’ve been caught on camera,” he explains. But that will “only spread the ultimate objective,” he adds, which is “being under surveillance …everywhere, all the time.” And of course, to catch those moments on a Ring camera with its recognizable aesthetics—well, you’ll first need a Ring camera. Once something becomes a genre proven to get views, others will imitate the success of what they’re seeing online or (after September 26) on TV. That’s how internet virality works, and it essentially makes Ring Nation an extended viral marketing campaign that will produce more videos in an endless loop—with each video serving as low-cost marketing for the product itself. Ring Nation is far from the first time the company, based in Santa Monica, California, has experimented with turning surveillance videos into entertainment. The first such video made an appearance on the company’s YouTube channel in 2015, just a year after that channel was created. In the short 19-second clip, a mother holds a four-month-old infant in front of a Ring camera and guides the baby’s hand so that the baby appears to give a fistbump. It’s titled, “Bye Daddy! 4-Month-Old Gets to Say Bye Before Daycare Every Morning.” (Admittedly, it’s very cute.) Then, in 2018, the year that Amazon acquired the company, Ring added a new section to its website called Ring TV, a showcase of carefully curated videos from customers’ cameras. In many ways, Ring TV’s content was a predecessor to Ring Nation. Like the upcoming TV show, Ring TV features primarily funny animals, cute kids, and heartwarming neighborhood interactions. Individual Ring owners also create their own viral media without Amazon’s direct involvement, uploading it to social media platforms where these sorts of videos get a ton of views. In one video with 3.3 million views, for instance, a teenager makes funny faces into the Ring camera outside her dad’s house. Another popular video (3 million views) shows an Amazon driver tripping in a yard as he tries to avoid stepping on the homeowner’s flowers. Many more show people that camera owners consider to be suspicious—often people of color. One example of this—a video with 4 million views—shows a white homeowner answering the door to find two men who speak limited English and who are asking for help putting out a car fire down the street. (At first, the Ring owner doesn't believe them, but he then changes his mind, keeping his viewers in the loop via caption.) Partnerships with law enforcement give smart cameras to the survivors of domestic violence. But who does it really help? Meanwhile, earlier this year, some Ring users started a TikTok challenge, leaving notes for Amazon delivery drivers, requesting that they do trending dances in front of the camera. The footage, which rarely identifies the person performing a dance, was then uploaded to social media, growing the accounts of Ring owners and creating an even bigger demand for more dancing delivery drivers. While these moments may appear fun and benign, experts like media psychologist Pamela Rutledge call their use a “sleight of hand,” designed to strip videos of the contexts in which they exist: the normalization of surveillance cameras and easy law enforcement access to Ring videos. This, she says, “subverts civil liberties ... by reframing the activity as normal and fun.” One of the main liberties threatened by Ring, and the rise of surveillance technologies generally, is privacy and consent. It’s the Ring camera owner who agrees to Ring’s privacy policies and chooses how and whether to share the video by uploading it onto the accompanying Neighbors app or other social networks, making it available to the police, or sending it to the news media. But it’s not just camera owners that the devices capture. Many cameras point at public spaces, meaning any passerby could potentially be recorded, and recent reporting has shown how Ring devices can pick up audio from up to 20 feet away. According to Daniels, the Ring representative, this type of footage captured without consent will not make it onto the show. “Like with everything we do, privacy is foundational to the show, and we secure permissions for each video from the owner and anyone identifiable in the video or from companies that hold the rights to the clips,” she told MIT Technology Review. While this is certainly a step toward privacy on the show, and in this particular slice of content, it does little for the rest of us who inadvertently walk by Ring cameras in our daily lives. In 2021, Ring sold 1.7 million devices, roughly the same number as its next four competitors combined, according to business intelligence firm Strategy Analytics. In other words, it has successfully dominated the market that it created—even while the results regarding safety have been questionable. Previous MIT Technology Review reporting shows that evidence on whether Ring cameras actually reduce crime in a neighborhood is flimsy. Its market domination came, in no small part, as a result of Ring’s efforts, starting in 2016, to partner with law enforcement agencies. At various points, the company offered free cameras to individual officers, as well as entire departments, often in exchange for promoting Ring in the officers’ jurisdictions. For a time, they also offered police partners a special portal to access community videos—stopping only after multiple media outlets reported on the process, which was followed by public outcry. Yet that didn’t stop Ring’s policing problem; earlier this summer, in a response to a 2019 request for information from Senator Ed Markey, the company admitted to handing over video content to law enforcement without the video owner’s consent at least 11 times this year. “Everything Amazon does prioritizes growth, expansion, and reach,” says Chris Gilliard, a visiting scholar at the Harvard Kennedy School Shorenstein Center and a vocal critic of surveillance technologies. In that sense, “Ring Nation is best located along a continuum … this new initiative looks like an attempt to cement societal acceptance of Ring,” he adds. So now, Gilliard explains, it’s not surprising that the company is turning to a new strategy to further normalize surveillance. These darker sides of surveillance technology will not form part of Ring Nation’s narrative. After all, they don’t exactly fit in with the show’s mission to give “friends and family a fun new way to enjoy time with one another,” as Ring founder Jamie Siminoff put it in a press statement. Instead, in a self-reinforcing cycle, the show will significantly expand the audience for Ring videos, the pool of potential Ring video creators, and then (and most important) the number of Ring cameras out in the wild. And many of these new customers likely won’t think twice about what their new Ring camera is really doing. “Ring prides itself on being incredibly accessible, [but] it's still kind of a techie thing,” explains Guariglia of the Electronic Frontier Foundation. “But if you park your very non-techie relatives in front of the television all day and they see the Funniest Home Videos from Ring Cameras, Ring might spread to an audience that perhaps Amazon has had a slower time getting on board.” In other words, if the company has its way, Ring Nation, the television show, will bring us one step closer to a Ring nation, IRL.",0.0
The fight for “Instagram face”,https://www.technologyreview.com/2022/08/19/1057133/fight-for-instagram-face/,2022-08-19,"<p>Meta banned filters that “encourage plastic surgery,” but a massive demand for beauty augmentation on social media is complicating matters.</p>
","In October 2021, Facebook announced a massive pivot, changing its name to Meta and going all in on augmented and virtual reality through a futuristic vision of the internet called the metaverse. In fact, the strategy had been taking shape gradually for years, with help from a seemingly frivolous product feature on Instagram. Face filters that add puppy ears to your hairline or make your lips appear bigger sit on a sophisticated technical infrastructure for AR and VR that the company, which owns Instagram as well as WhatsApp, has built to support such effects. Thousands of creators have contributed filters free of charge, and the millions of people around the world who use the feature each day have provided Meta with troves of data. The little research that exists about digital beauty culture has found that visual platforms like Instagram, which rely on AI recommendation algorithms, are narrowing beauty standards at a stunningly rapid pace. Through filters, they’re also helping users achieve those ideals—though only in the digital world. There is evidence that excessive use of these filters online has harmful effects on mental health, especially for young girls. “Instagram face” is a recognized aesthetic template: ethnically ambiguous and featuring the flawless skin, big eyes, full lips, small nose, and perfectly contoured curves made accessible in large part by filters. Some people are excited to see realistic avatars that look like them. Others worry it might make body image issues even worse. But behind every filter is a person dragging lines and shifting shapes on a computer screen to achieve the desired look. Beauty may be subjective, and yet society continues to promote stringent, unattainable ideals that—for women and girls—are disproportionately white, slender, and feminine. Instagram publishes very little data about filters, especially beauty filters. In September of 2020, Meta announced that over 600 million people had tried at least one of its AR features. The metaverse is a concept much bigger than Meta and other companies investing in AR and VR products. Snap and TikTok capture huge numbers of filter users, though Snap is also investing in place-based AR. Meta’s product suite includes the Oculus headset and Ray-Ban smart glasses, but it’s focused on what made Facebook popular—the face. Beauty filters, especially those that dramatically alter the shape of a face and its features, are particularly popular—and contested. Instagram banned these so-called deformation effects from October 2019 until August 2020 because of concerns about the impact they have on mental health. The policy has since been updated to outlaw only filters that encourage plastic surgery. The policy states that “content must not promote the use or depict the sale of a potentially dangerous cosmetic procedure, as per the Facebook Community Standards. This includes effects that depict such procedures through surgery lines.” According to a statement to MIT Technology Review in April 2021, this policy is enforced by “a combination of human and automated systems to review effects as they are submitted for publishing.” Creators told me, however, that deformation filters often get flagged inconsistently, and it’s not clear what exactly encourages the use of cosmetic surgery. Though many people use beauty filters merely for fun and entertainment, those puppy ears are actually a big technical feat. First they require face detection, in which an algorithm interprets the various shades of pixels picked up by a camera to identify a face and its features. A digital mask of some standard face is then applied to the image of the real face and adjusts to its shape, aligning the mask’s virtual jawline and nose to the person’s. On that mask, graphics developed by coders create the effects seen on the screen. Computer vision technology of just the past few years has allowed this to happen in real time and in motion. Spark AR is Instagram’s software developer kit, or SDK, and it allows creators of augmented-reality effects to more easily make and share the face filters that cover the Instagram feed. It is in this deep rabbit hole of filter demonstration videos on YouTube that I first came across Florencia Solari, a creative AR technologist and a well-known creator of filters on Instagram. She showed me how to make a face filter that promised to plump and lift my cheeks and fill out my lips for that Kardashianesque, surgically enhanced face shape. “Thirty-two percent of teen girls said that when they felt bad about their bodies, Instagram made them feel worse.” “I have this inflate tool that I am going to apply with symmetry,” Solari said, “because any modifications that I do to this face, I want to be symmetrical.” I tried to keep up by dragging the outline of my digital mannequin’s cheekbone up and out with my cursor. Next, I right-clicked on the map of her bottom lip and selected “Increase” several times, playing God. Soon, with Solari as my guide, I had a filter that, while sloppy and simple, I could upload to Instagram and unleash to the world. Solari is part of a new class of AR and VR creators who have made a career by mastering this technology. She started coding when she was around nine years old and was drawn to the creativity of virtual-­world development. Making her own filters on Instagram was a hobby at first. But in 2020, Solari left a full-time job as an AR developer at Ulta Beauty to pursue online AR full time as an independent consultant. She’s recently worked with Meta and several other big brands (which she says she can’t disclose) to create branded AR web experiences, including filters. Solari’s very first filter, called “vedette++,” went viral back in September 2019. “I tried to make an interpretation of what the superstar of the future would be,” Solari says. The filter applies an iridescent, slightly green shine to the skin, which is smoothed all over and inflated under each eye to the point that it looks as if half a clementine has been shoved inside each cheek. Lips double in size, and face shape is adjusted so that a distinct jawline tapers into a small chin. “It was kind of a mix of an alien, but with a face that looked like it was full of Botox,” says Solari. “It really became, like, sensational.” The most widespread use of augmented reality isn’t in gaming: it’s the face filters on social media. The result? A mass experiment on girls and young women. Though Meta doesn’t make its filter data public, it does provide creators with some metrics, and I asked Solari and others to share the data with me. The numbers are stunning; vedette++ was viewed 130 million times and used over 1.2 million times in 3.5 months. Solari says the filter was one of the first ever to go viral. It helped that vedette++ was used by model and influencer Bella Hadid. “Influencers have a huge impact on how this spreads … You will get an influencer or a celebrity to use them, and then it will go more viral organically,” she says. According to Solari’s statistics from Meta, vedette++’s impressions spiked exponentially in the days after Hadid used the filter. Creators say that deformation effects and influencer shares are the keys to virality where filters are concerned. Several creators said the demand for deformation beauty filters is so consistent that they can essentially gamify virality by making a certain kind of effect that fits the “Instagram face” aesthetic. “This is something we don’t speak about—that deformation can make your filter go viral. If you don’t use deformation, your filter won’t succeed as much as the other ones, even if the others are more technically complicated,” says Lucie Bouchet, a popular filter creator. Bouchet notes that there are exceptions to this pattern, and filters that are especially fun, trendy, or unique also see massive success. Bouchet has stopped using deformation effects in many of her filters and now builds in a feature that enables the deformation effects only if users choose. But the statistics are hard to ignore. Bouchet’s most popular deformation filter, called “Golden Hair,” amassed almost 300 million impressions, while a similar one without deformation effects garnered a measly 7.2 million. Around 70% of the people using her filters are between 13 and 24. Bouchet’s concerns about the harmful effects of deformation filters, especially on girls, are shared by many creators who make them. I spoke with researcher Claire Pescott in the spring of 2021, when I first wrote about the effects of beauty filters on social media. Pescott studies the behavior of preteens on social media and has observed gender differences in filter use. She found that boys use filters primarily for fun and experimentation, and girls use them to enhance their appearance. Though Meta declined to speak with me on the record for this story, the company has taken some steps to address recent criticism surrounding the negative impact that Instagram can have on the mental health of users, particularly teenage girls. When whistleblower Frances Haugen came forward with internal company documents, some showed that its leaders had known about these problems for years. According to reporting by the Wall Street Journal, a March 2020 slide presentation by Facebook researchers read, “Thirty-two percent of teen girls said that when they felt bad about their bodies, Instagram made them feel worse.” Another slide said, “We make body image issues worse for one in three teen girls,” and acknowledged that “comparisons on Instagram can change how young women view and describe themselves.” Filters on platforms like Meta, TikTok, and Snapchat are not the only technology working to narrow beauty standards. Photo-editing tools have also exploded over the past 10 years with the rise of social media, and the results can have similar effects on aesthetics and mental health online. Recommendation algorithms and social preferences for certain looks can also create harm on visual platforms. But Solari thinks technology itself is not to blame in the first place. “It is not the filters that are making this [problem], but society is like this,” she says. “These are the values that society has and sees as beautiful. And that’s why it goes viral.” Creators observe a consistent and shockingly high demand for deformation beauty filters that fit a particular aesthetic. In December 2019, Instagram banned vedette++ as part of its clampdown on deformation effects. Solari says she wasn’t trying to encourage plastic surgery and believes that most people using her filter wanted to “perform with a face that just looked kind of out of this world.” She responded to the deformation ban with a scathing Medium post that was widely shared among filter creator communities. It reads: “This isn’t about plastic surgery. This is about FREEDOM. It’s about preserving the most valuable and unique thing we own: Who we are. Our individuality … The internet was our free space. It was a mask, yes, indeed. A mask that served us to be able to BE TRUE to ourselves. Express ourselves beyond our bodies, beyond our physical realities, explore the trans-human and the fantasy.” It’s this fantasy—this opportunity for escapism and expression—that many creators point to when they defend filters, saying that AR and VR offer the ability to test out certain personas and play. Pescott, the researcher, told me that trying on different identities and demonstrating them socially is an essential and healthy part of adolescence. For many people, filters offer a new way to do that. Solari has thought a lot about the tension between censorship and safety since vedette++’s viral success and subsequent ban. “I don’t believe in censorship of that kind of content, because I believe that people should be able to choose what they want to adopt or not,” she says. But Solari also believes that strict beauty standards do make it hard for people to accept themselves fully. “If we actually want to address this,” she says, “we have to look for a way to help people to really build the strength to say ‘I like myself as I am. I want to show myself as I am.’” “I find beauty in authenticity, in freedom, and in what I find to be the perfect balance between order and chaos,” she says.",0.0
Using technology to power the future of banking,https://www.technologyreview.com/2022/08/15/1055276/using-technology-to-power-the-future-of-banking/,2022-08-15,"A heritage financial services institution isn’t necessarily the first place a technologist looks to grow their career. But that hasn’t been a problem for JPMorgan Chase, which has made itself an appealing career destination for technologists. “Technology is not an afterthought,” says Gill Haus, chief information officer of consumer and community banking at JPMorgan Chase.…","In association withJPMorgan Chase & Co. A heritage financial services institution isn’t necessarily the first place a technologist looks to grow their career. But that hasn’t been a problem for JPMorgan Chase, which has made itself an appealing career destination for technologists. “Technology is not an afterthought,” says Gill Haus, chief information officer of consumer and community banking at JPMorgan Chase. “It is in everything we do, from our offices to our branches to our contact centers to our web and mobile applications.” Haus explains that there’s more to being a technology company than using technology to solve problems. “What really makes a technology company is how you think about the way you hire teams, the way you groom teams, the way you build software, the way you deploy that software,” says Haus. “It's how you organize around products, not around your business units.” Like a technology startup, the technical teams at JPMorgan Chase solve real-world problems. “Every single day, we are launching new features and products that make it easier for everyone. It's incredibly exciting,” says Haus. However, unlike a startup, JPMorgan Chase has scale. The financial services institution supports 44 million mobile active customers and 59 million digitally active customers in general. The company spends $12 billion on technology annually. “When we launch a solution or bring a new product to market, we don't bring it to market for 1,000 people. We don't bring it to market for a 100,000 people. We bring it to market for millions of customers immediately, and that is incredible,” says Haus. “There's incredible talent here, as you can imagine, because we have such a technical need. Talent begets talent. It's exciting to go to a place where you know you're going to be challenged.” Laurel Ruma: From MIT Technology Review, I'm Laurel Ruma and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace. Our topic today is innovation at scale. Established institutions in every industry have found themselves competing against cloud-born startups that run lean and mean. Legacy companies must not only innovate, but do so on a significantly larger scale than the current generation and much faster to maintain that scale in a rapidly changing marketplace. But success is possible. Two words for you. Scaling innovation. My guest today is Gill Haus, who is the chief information officer of consumer and community banking at JPMorgan Chase. Gill heads the Chase technology team, and is a member of both the consumer and community banking leadership team, and the firm's global technology leadership team. This podcast is produced in association with JPMorgan Chase. Welcome, Gill. Gill Haus: Thank you for having me, Laurel. Laurel: You've worked at a number of different companies, including PayPal and Curb, which is formerly known as RideCharge. What's it like working at a large financial institution after being at several fintechs? And why did you join JPMorgan Chase? Gill: I'll answer it in reverse, which is what led me to come to a company like a JPMorgan Chase. There is a size and a scale that is incredibly attractive, particularly given the technology challenges that we face. To put it in perspective, we have 44 million digitally active mobile customers and 59 million digitally active customers in general. Getting the experience right for them, making it load quickly, showing all the information their customer needs in a secure way, that's a technical masterpiece, and that's exciting for me and that's why I joined a firm like this. That's just the Chase portion of the business, not to mention our corporate and investment bank and other parts of our organization. There's also incredible talent here, as you can imagine, because of the fact that we have such a technical need. Talent begets talent. It's exciting to go to a place where you know that you're going to be challenged. You're going to learn from other people. But also, there's a culture. This one's important and it goes to the first part of your question. There's a culture here of, “We can tackle these problems. We are a startup on our own.” We look at any of the challenges facing us, whether it is in our consumer business, whether it's in our asset wealth management business, whether it's in investment international, and we look at each of those problems the way that a startup would. So, it feels very much like a startup here, like when I was at PayPal or Curb or other companies. The difference is we have the scale, so we have the resources at our disposal should we need them. We have the money to actually be able to make a difference. I do love startups having been there, but unlike some startups, when we launch a solution or bring some new product to market, we don't bring it to market for 1,000 people. We don't bring it to market for 100,000 people. We bring that to market for millions of customers immediately, and that is incredible. The other piece of this, which is really great about a startup, is purpose. A lot of the places that I joined when I was at a startup was because we're going to change the world. We're going to change ride share, which is why I joined Curb. At PayPal we were acquired, but it was about changing how money is used. The purpose that we have here at JPMorgan Chase is incredible because finance is at the center of everyone's life. There's nothing more personal than finance. Whether it is an individual customer, a small business, or a corporation or a government, money is so incredibly important. Every single day, we are launching new features and products that make it easier for everyone. It's incredibly exciting. The technical challenge is huge and that's why it feels a lot like a startup almost every single day, but also why I joined the organization. Laurel: You started as the head of digital technology and probably much like a startup, you moved along quite quickly to lead Chase's technology organization as CIO. How has your role changed? And what advice would you give to technologists who might actually follow that career path? Gill: If you had asked me years ago, “Do you want to be a CIO, whether it's here or anywhere else?” I don't know Laurel, if I would've said yes. And now that I know, I might go like, ""What, really? I don't know."" But what really kept me moving in my career were a few things. The first was the technology itself. I've always been passionate about technology. The roles that I've had have been on solving technical problems and having a curiosity and being technical. Now, I'm not as hands-on as I once was, but I do read constantly about technology. I get into the architecture, and I've done that throughout my career. That's an important thing too, because I believe that anyone that wants to be a CIO or a CTO, particularly in the way that the industry is progressing, you need to understand technology. So, staying close to the technology and curious and wanting to solve those problems has helped me. But there's another part to it, too. In every one of my roles, there have been times when I've seen something that wasn't necessarily working and I had ideas and wanted to help, but it might’ve been outside of my responsibility. I've always leaned in to help, even though I knew that it was going to help someone else in the organization, because it was the right thing to do and it helped the company, it helped other people. So, it ended up building stronger relationships, but also building my skillset. I think that's been a part of my rise too, and it's something that's just incredibly powerful from a cultural perspective. That’s something that I love here. Everybody is in it together to work that way. But I also think that it just speaks volumes about an individual, and people gravitate to want to work with people that operate that way. The reality is we all need help and there have been so many people that have done that for me in my career, and I either hire them or they've hired me and we stay close. So, that's been something else has been important in my career path. The last piece that I would say is, it's important to have two things. There is one, the excitement about the work. And I have that here. I had it in my last role here too, which is there's an excitement about the changes you're making every day, about the problem set and the technologies that we can use and the opportunity. That's really important because that ensures that you're always going to be excited, even if there's a bad day. The other thing that I make sure of is, am I a bit scared? Not scared like something bad is going to happen, but more scared like, can I do it? Do I understand how the technology works? Do I know how to navigate this situation? Having that means you're going to grow and you're going to learn skills you didn't learn before. So you won't get bored because you can get excited, and after a while the excitement wanes, but you still are really jazzed about the things that you're learning. I urge people to find roles where they have both of those challenges. If you have a cultural view of, I'm going to try to solve the problems and it's bigger than me, it's about the company, and I'm going to put myself in a situation that I'm having fun, you'll have energy. And I'm also a little bit scared, you're going to learn. That's the best way, I think, to grow someone's career. And what's so exciting here is we have so many awesome problems to solve, so many incredible people that are looking for assistance, that you just have an environment where people can grow their careers really quickly. I was surprised that I became CIO so quickly, but you can point to so many examples in this company where people that bring the attitude, the aptitude, the skill and the passion, they continue to grow because we're doing some incredible things, and we respect and we value that in our leaders. Laurel: Speaking of problem solving, for a lot of organizations, the pandemic accelerated digital transformation initiatives out of necessity. Perhaps some organizations had a five-year plan to roll out video collaboration solutions for remote work, but suddenly they had to make that a priority when everyone was home for the pandemic. Where was JPMorgan Chase in its digital transformation journey in early 2020, and did you feel the need to accelerate any initiatives to get through that pandemic? Gill: I'm smiling because I joined JPMC on March second of 2020, so it was right before we went into lockdown. I got to see the before and the after really quickly. One of the things that was incredibly apparent then and continues to be apparent is that our customer is our number one focus. That has always been the case and COVID didn't change that. In fact, it was a rallying call for the organization, that we need to make sure even more so, that we're doing right for our customers because our customers were unable to visit branches or perform the services they were used to. And they were also impacted by the pandemic themselves on their finances, etc. So, it was super clear about the mission we had here, even more so, we want to make sure that we're helping those that really depend on us. You could see it in the way that everyone rallied around solving problems. There were some things that the pandemic also drove. There was an increase in digital engagement. That would've probably happened anywhere to your point about a five-year plan, but that was fast-tracked because people were at home, and they needed to transact, and they used their digital devices because there was no other way. I believe that's here to stay and that digital adoption, that acceleration, we're seeing it because we are up 6% year over year. If you think about our digitally active customers, so 58.8 million customers. I mentioned the mobile number, we're up 11% year over year. That's something that we're continuing to see happen. Once you're using our mobile, or you're using web, it's hard to necessarily go back, but you still can. It required us to roll out video conferencing globally to our employees in a span of a weekend, which is not for the faint of heart. When you think about the 200,000 employees that we have, we were able to roll this out at that pace, which speaks not just to the technical powers we have more broadly in the firm, but to how adaptable we are when these sorts of things occur. That was all because we wanted to make sure that our people could service our customers the best way that we could. Remember, we have people that work in our call centers, and they were impacted, and we have people that work in branches, and they're impacted, etc. One of the things that was really clear when the pandemic occurred was how quickly our teams could deploy new software. Many talk about being able to build quickly and being agile. There's the Paycheck Protection Program, and this was the ability to offer small businesses who were not having as much traffic, etc., loans through the government. We had about a week to put this in place, and we were able to stand up that portal in about a week. We had it fully automated in a matter of two-ish weeks, and we were able to provide more funding than any other lender in both 2020 and 2021, which was just incredible. The fact that we were able to build that because of the technology we've invested in over the past years, build that so quickly and scale that to such a large volume for our customers was huge. But we also were able to make some fundamental changes in mobile. We were able to enhance things which might seem simple. We have a product inside of our mobile application called QuickDeposit, and this is where you're able to deposit a check. But as many know, sometimes checks are large numbers. Traditionally we asked people to go into branches to help prevent fraud. Because of the technology that we have, we were able to raise limits in a way that ensured that we were able to manage through fraud appropriately and allow customers that would have formerly had to come into a branch or ATM, make the deposits electronically. Those are the kinds of things that we've seen change, but the pace that we moved, that's not just limited to the Chase part of the business, but we saw this across all of J.P. Morgan. There’s one piece that I think is important on this Laurel. I was in a meeting and here I am a new person in the organization working on the Paycheck Protection Program. I recall there being somebody on Zoom. We were having a conversation and I assumed because I was new and they were in the meeting that they were on my team, and the person said, ""Oh no, I'm not on your team, but I know you're new and you needed assistance. And so here I am to help, and I just figured I'd navigate."" And that has stuck with me about the culture of this organization and how we focus on the customer both externally and internally, to really make sure that we are providing the best service that we possibly can. Laurel: That certainly requires an agile mindset. So, how is JPMorgan Chase transforming into an agile organization? You've laid a couple examples. Clearly you would not have been able to respond to the US government's Payroll Protection Act that quickly if you hadn't been already working on a number of these opportunities and abilities to be more agile. So, what lessons have you learned along the way and how have your teams and customers benefited from this shift? Gill: Oh, yes. An agile transformation is a really hard thing to do. Many people are making agile transformations, so it sounds like it should be easy. You have your scrums, and you have your various ceremonies and retrospectives, and you use a tool to manage your backlog and you're golden. One of the big challenges that we as a company had faced in JPMorgan, was we were organized more around our software and platforms than around our customer and the experiences back. That made it really frustrating for teams because it meant that you likely needed 10, maybe 12 different organizations to agree on building something. It wasn't clear who the owner was. The architectures sometimes would be a bit more frail because you were working through multiple teams. If you want to move quickly or you want to innovate, that's not a model in which you're able to actually operate. You can force it, but it requires many more meetings. It's difficult to know who the decision makers are. You can move more slowly and sometimes an application or a solution looks like many teams built it. There's Conway's Law, and you may have probably mentioned this before on other podcasts, but Dr. Conway said that your software will reflect how you're organized. That's really what we had seen. So, as opposed to us just trying to find a way to navigate around it, we said as an organization, “We're truly going to become agile, and we're going to accept Conway's Law, and we're going to organize around our products back.” In the community and consumer bank, we organized around 100 products, so we have a thousand teams that are aligned around these products. A product, for example, is something like account opening. So, I want to open an account on mobile or web. There is one product for this. There is one product leader, one design leader, one data leader, and one technology leader that are accountable for that product. Now we know who can manage the backlog. Now we know who can work through any kind of architectural decisions. Now we understand who is accountable for ensuring that we have innovation and understanding that customer needs. That has allowed us to pivot quickly, because if I need to move, I can work with the account opening team, they can make the decisions, they can manage a backlog, and they're able to adapt when we have things like the Paycheck Protection Program or other types of efforts that are out there. But it also gives more purpose to the individual teams because they set their destiny, they have more autonomy, and they're working together between tech and product design and data, so we can build the right solutions that we need. This creates a great experience for people in the organization. By the way, the whole of JPMC is moving to operate this way. This lets us not just move more quickly, it gives better work life balance for our employees and less frustration, because it's easier to know where you are. You have that purpose and you accept being part of a particular team. I mentioned we can respond more quickly when there is a challenge, but it's not just those challenges like PPP or a pandemic that we have to address, Laurel. There are places where our customer's needs are changing every single day. And by organizing around products this way, we can understand the data from our customers, and we can experiment, and we can adapt in a truly agile fashion for what our customers really need, versus what we think they might need and building something that doesn't really resonate with them. It allows us to operate in a truly agile fashion, which we were not able to do before and it's quite incredible being able to make a change like this at such scale. Laurel: With those hundreds of products and millions of customers, what are some of the challenges and opportunities that your teams face when operating on this scale? Gill: We are a company, so it's a job. And there are going to be the normal things that you would see in a company, which is we have to prioritize. We're like any other company in that respect. However, working here, in my opinion, is incredible. When you write code in this organization, you are deploying it at scale. The problems that we are solving that could be frustrating because it could be a day where this code isn't working or I'm running into this blocker, I can't figure it out or, I have to support some other change that's occurred — when we make those changes for our engineers, for our product, our design, our data leaders, we are deploying this at scale. The other part is we include the feedback from our millions upon millions of customers to improve an experience and improve the life of those customers. That's something that keeps me going every single day. Yeah, I might feel like this project's been a bit harder than it should have been. However, the difference that I'm making for a customer on every single day is tangible. Somebody buying groceries for their family, somebody buying their first car, somebody refinancing their house, you name it -- the list goes on. Because the work that's being done every single day is making that positive impact, I just feel incredible about it and I think everyone else in the organization does, too. This is the kind of thing that makes me excited about being here. Even if there is a challenge on a particular day. Now, there are things we want to do. We want to deliver software more quickly, and we're constantly working on improving in that space. We have a ton of products that we continue to improve, but that's the opportunity, which is we have thousands of engineers. We hire thousands of designers. And every day we're making it better, but we're also learning. And that's the career opportunity for people because what you join the company to do today, could be something different that you do in a year. That's exciting because what we learn to do in one of our other divisions, we go, ""That's wonderful, bring that over."" Career opportunity, mobility opportunity. And it's a career mindset every single day for people that's exciting and that's the opportunity I think, for our technologists here and the scale, that's really the gravy because most engineers, that's what they want, from an engineering perspective. I want to ship code at scale. Laurel: It's possible some people may be surprised how big a team is, so you're talking about data, design, customer service, as well as product managers, as well as your actual software developers. So with this, you actually have to invest in a collaborative and purpose-driven culture. And that seems to be certainly something that draws you to this organization, but it also creates an attractive workplace that can develop and retain talent. As you said, that is building that purpose-driven company for the customers themselves. So, why are you passionate about this idea of purpose-driven culture and what are the benefits that you're seeing every day? Gill: Yeah, as a purpose-driven company, it's more than just the services that we provide when you think about technology. We're making differences in communities. Because money is central to people's lives, when we make a difference in our mobile app, that's huge. But also we have a large branch network and many people still use us through branches, and if we can be your community bank and we're able to help a family or help a small business, that makes a difference. If we're able to help a large company that is working on investing in some new electric vehicle type technology, we're the ones that are helping change the world in a positive way, and the list goes on. That sense of purpose is what really attracts people to the organization. But we also recognize and are passionate about diversity in general. We know that diverse teams are more innovative, more productive, more effective. You name the thing that teams' ought to be, and diverse teams are that way. We spend a good deal of time ensuring that we are focused on diversity, whether it is internally and how we ensure that you can be your whole self and bring your whole self to work, to the communities that we help. These are the things that really are meaningful and drive a difference for the people that work here. So, that's why people want to come and work here because you make a difference every single day. Laurel: There's obviously an incredible demand for developers and other software and IT professionals across every industry, including banking. So, how are you specifically addressing this challenge and what is JPMorgan Chase doing that is different from competitors? Gill: Tech is not an afterthought. It is in everything that we do. This is from our offices, to our branches, to our contact centers, to our web and mobile applications. It is a core in our purpose. When people look at our organization, they should see that is what we do on a daily basis. In 2021, we hired thousands of software engineers with skill sets that range across full stack development, cloud, data science, machine learning, and engineering. You name the technology, we've invested in it. Now, the market is incredibly hot, and one of the things that we focus on is making sure that when we bring people in, that you are coming here for a career. You're not coming here just for a job. There's training. We do bring people in at entry level, and we put you through training, so you learn how to work in our environment. We also provide additional mobility and opportunity within the organization, so that you are learning, and you are growing. That is another attractive point. Our push for diversity. It's a welcoming culture. You work with people from different walks of life and backgrounds. All of that just make it a really enjoyable place to be. We recognize that attitude and aptitude make the difference. So that's what we are looking for in the talent that we bring in. We're looking for that really incredible talent that wants to change the world, because we're at the center of what everyone does on a daily basis. We're hiring across the country. We're hiring across the world, honestly, Laurel. We have our career page that people can visit, at careers.jpmorganchase.com. The thing that is just really passionate to us is we are focused on being a technology company in the sense of how we build software. We're moving to building our software in a truly DevOps fashion. You build your software, you test your software, you deploy your software, you manage your software in your agile team. We're operating in a way where we can release software more and more all the time. If you name the cloud provider, we're there, and if you name the technology, we are thinking about that technology or using that technology. Because we also deploy it at scale, it's one thing to say, ""I'm using the cloud,"" or, ""I built a mobile application."" It's another to use those technologies for 60 million customers on a regular basis. That is not just incredibly challenging from the user experience, but even the back-end of the plumbing and how we rethink or reimagine how we provide the foundational services to make that happen. Those sorts of things are really attractive to engineers. It's what gets me and the people here excited, too. That's the thing that we believe is really drawing people to the organization. Laurel: Delivering services at scale clearly is important and also, as you said, one of those attractive features. But what is it about the banking industry that makes it so appealing to technologists? Gill: When I was first approached to join a bank, Laurel, with full transparency, I said what I thought most technologists typically are thinking: ""A bank. I don't want to join a bank. I want to join a technology company."" I know I gave a bit about what a technology company is and how I think we are a technology company. When you think about banking and you think about the technology that we have, we employ technology at great scale. So, the first part is, in order to deliver the services we have, there's no way we can't be using technology. But what really makes a technology company is how you think about the way you hire teams, the way that you groom teams, the way that you build your software, the way that you deploy that software, as I mentioned. It's the way that you manage that software production. It's how you organize around products, not around your business units. It's how you obsess over the customer experience. That is what we are doing at JPMorgan Chase. And when you think about the technology problems we're solving, the way that we are solving them, there's no better place to be because we are moving from many heritage technologies into modern, cloud-based technologies. We're rebuilding, re-plumbing our entire deposit platform in a cloud-native manner. That is something that drives trillions of dollars of our deposits. And we'll be moving that to a modern ecosystem that we deploy on a regular basis, in a controlled, well-managed way. This is incredibly exciting. It's exhilarating actually, when I talk about it. We make the difference in people's lives. People ask if we are really committed to this sort of change. We spend for JPMorgan, $12 billion on technology annually. In my division, $4 billion alone. So, there's one thing for all the words that I've used on what we're doing, but also we're putting our money where our mouth is, which is incredible. Laurel: And you've mentioned some of those technologies that you're investing in, like machine learning, artificial intelligence, microservices and all of these technologies drive that product development and digital engagement for customers. Looking forward, what are you thinking about for the future of banking and the banking technology? Gill: There are a few areas where I think the future is. One is collaboration in general across our various business units. One of the reasons we're doing this transformation and we're really modernizing how we operate, is to move—and I'll use API as an example—but to enable our teams to be able to innovate at pace beyond just their local division. Organizing around our product teams makes it easier for us to build those products. By also ensuring that we have APIs and the data that is accessible across those teams, teams can innovate and explore in a truly agile fashion, which is empowering our engineers and designers, data, etc., to be able to do things they once couldn't do. I think you're going to see more of this happening, where I can use elements of data or a service from other portions of the bank within my mobile app that make for a more compelling experience for our customers. But I think the other part of the future of banking really is personalization. We know that you just booked a trip. We can make other recommendations while you're making that booking. If we see a flight is delayed, we can act immediately. If we identify that you have a big inflow of money, we can help you when it comes to, in real time, tell you where you could be spending that money more effectively, what you should be saving. The list goes on with the kinds of experiences we can create. And they're not just limited to our mobile or web app, but it's across our whole ecosystem. When you're in a branch, that kind of data can tell us, I know that you've called us about other types of products, or I saw you perform some experience on our mobile application, and we could tie it all together to a truly personalized experience where you're a customer of one, and that's a really incredible future, I think. There’s also just a little bit of the road less traveled. We know there are going to be things that we need to do or technologies that we haven't really tapped yet. You can see that happening on a regular basis. That's why we are always on the search for the best technologists in the industry and what we have our technologists doing here, which is what have we not unlocked yet. Because there are definitely things that we know we're going to need to do in the future. We're not looking for people that know how to do everything today. We're looking for those who are curious and want to know and are willing to learn, ""Okay, how do I need to do this in the future?"" Because all the things I just recommended, I'm sure there will be something even well beyond that in the coming years. The other part of the future is diversity. I mentioned before that diverse and inclusive teams are just better teams. We know that by science, not just anecdote, and we want this to be that sort of place. We want our teams to be that way. We want an environment where everybody brings their whole self to work. I think you're going to see that more and more, not just here, but even more broadly. It's important, because I do think that it is a secret sauce to how we develop products, because remember, the millions of customers that we have, they are also diverse. It doesn't matter who you are and so our people should reflect those people we serve so we can make better products, can understand our customers more. And that I think is extremely exciting to me on what I think the future of banking holds in store. Laurel: Excellent. Thank you Gill, for joining us today on the Business Lab. Gill: My pleasure, thank you for having me. Laurel: That was Gill Haus, who is a chief information officer for consumer and community banking at JPMorgan Chase, who I spoke to from Cambridge, Massachusetts, the home of MIT and MIT Technology Review, overlooking the Charles River. That's it for this episode of Business Lab. I'm your host Laurel Ruma. I'm the director of Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print, on the web, and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com. This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review. This episode was produced by Collective Text. Thanks for listening. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.",0.0
Google examines how different generations handle misinformation,https://www.technologyreview.com/2022/08/11/1057552/gen-z-misinformation/,2022-08-11,"<p>A new survey by Google shows Gen Z is better than millennials or boomers at fact-checking—but previous research tells a different story.</p>
","A habit called “lateral reading” is a core part of any good fact-checking routine. It means opening up a bunch of tabs and doing multiple searches to verify the facts, source, or claims made in a piece of online information. So it seemed like great news when a new study from Poynter, YouGov, and Google indicated that Generation Z is adopting this technique more than any previous generation. The study, released today by Google as the search engine team there rolls out several changes to how it handles misinformation, asked more than 8,000 people ranging in age from Generation Z (defined for this study as those 18 to 25) to the Silent Generation (68+), across seven countries, about misinformation and how they research questionable content online. Essentially, the study concludes that younger people are more likely to think they may have unintentionally shared false or misleading information—often driven by the pressure to share emotional content quickly. However, they are also more adept at using advanced fact-checking techniques. One-third of Gen Z respondents said they practice lateral reading always or most of the time when verifying information—more than double the percentage of boomers. About a third of younger people also said they run searches on multiple search engines to compare results, and go past the first page of search results. Portions of the survey provide an interesting snapshot of how people of different ages, and in different locations, experience misinformation and think about their own role in stopping or spreading it: 62% of all respondents believe they see misinformation online every week, for instance. Gen Z, millennial, and Gen X readers are more confident in their ability to spot misinformation and more concerned that their close family and friends might believe something misleading online. However, the study relies on participants to accurately report their own beliefs and habits. And the optimistic figures about Gen Z’s actual habits contrast pretty starkly with other findings on how people verify information online. Sam Wineburg, a Stanford University professor who studies fact-checking practices, thinks he knows why that might be: when you’re trying to understand how people actually behave on the internet, “self-report,” he says, “is bullshit.” After boosting unproven covid drugs and campaigning against vaccines, Steve Kirsch was abandoned by his team of scientific advisers—and left out of a job. “What people say they do versus what they do do?” he adds. “That discrepancy goes back to the earliest days of social psychology.” His own research has found that without intervention, younger people seldom use lateral reading or other advanced fact-checking techniques on their own. In one recent study led by Wineburg and his team at Stanford, researchers wanted to learn whether an online course in fact-checking techniques could improve how college students verify information. Before the course, just three of the 87 students they tested engaged in lateral reading, meaning in this case that they left the website they were asked to evaluate to consult an outside source. “If people spontaneously did [lateral reading], we’d all be in a lot better shape,” Wineburg said. In a larger study, more than 3,000 high school students were asked to investigate a series of online claims. The results were pretty bleak: more than half the students tested believed that an anonymous Facebook video filmed in Russia contained “strong evidence” of US voter fraud. (Full disclosure: I was a participant in an earlier study from Wineburg’s team that observed the methods of fact checkers and compared them with those used by historians and Stanford undergraduates.) Gen Z clearly uses the internet differently from previous generations. But young people are also susceptible to the same traps, weaponized misinformation tactics, and pressure to share that have fueled bad online practices for years.",0.0
"Software can do better than ‘male,’ ‘female,’ and ‘other’",https://www.technologyreview.com/2022/08/22/1057853/software-coding-more-gender-options/,2022-08-22,"<p>Providing more-expansive gender options is not a difficult coding problem.</p>
","For transgender and nonbinary people like me, a society organized into only “male” and “female” makes us feel excluded. And it’s something that happens frequently, especially online. Take Gmail. There are three gender options when you register. If you choose “other,” you can write in any gender identity. But first you must choose how you’d like Google to refer to you—as “male,” “female,” or “other.” Why something as dehumanizing as “other”? Even a choice of the three most popular—“he,” “she,” and “they”—would be reasonable. From a coding perspective, it would be quite simple to update the dropdown language. It should not be difficult for companies to improve gender inclusivity on existing forms. Providing additional options often requires just changing or adding a few lines of code. Here’s what it would look like to add a third gender category in PHP, which is used to program many web forms: Of course, not all software is easy to update. For one thing, databases that limit what information can be stored in the gender field may need extensive overhauls to accommodate more choices. That’s why it’s important for developers to create an inclusive program during the design stage, so that gender-diverse users can feel welcome at launch. Though a minority, trans people are still a large demographic for software. According to estimates, there are over 1.4 million transgender adults in the US—around twice the population of Alaska. How is it that we accept one of the least populated states as the second option in an alphabetized menu of dozens of options, but find it inconvenient to add a few more genders? “Female” and “male” should be at the top of the list; 99.5% of the population shouldn’t have to scroll excessively to find their gender. As a trans person, I’m simply asking that developers include options for everyone who uses their software. As a developer, I know that’s not too big an ask. Everett Franchuk is a web developer and writer based in Winnipeg, Manitoba. This article has been updated to acknowledge that some databases may better support changes to gender options than others.",0.0
Erik Prince wants to sell you a “secure” smartphone that’s too good to be true,https://www.technologyreview.com/2022/08/19/1058243/erik-prince-wants-to-sell-you-a-secure-smartphone-thats-too-good-to-be-true/,2022-08-19,"<p>MIT Technology Review obtained Prince’s investor presentation for the “RedPill Phone,” which promises more than it could possibly deliver.</p>
","Erik Prince’s pitch to investors was simple—but certainly ambitious: pay just €5 million and cure the biggest cybersecurity and privacy plagues of our day. The American billionaire—best known for founding the notorious private military firm Blackwater, which became globally infamous for killing Iraqi civilians and threatening US government investigators—was pushing Unplugged, a smartphone startup promising “free speech, privacy, and security” untethered from dominant tech giants like Apple and Google. In June, Prince publicly revealed the new phone, priced at $850. But before that, beginning in 2021, he was privately hawking the device to investors—using a previously unreported pitch deck that has been obtained by MIT Technology Review. It boldly claims that the phone and its operating system are “impenetrable” to surveillance, interception, and tampering, and its messenger service is marketed as “impossible to intercept or decrypt.” Open-source code runs on every computer on the planet—and keeps America’s critical infrastructure going. DARPA is worried about how well it can be trusted Boasting falsely that Unplugged has built “the first operating system free of big tech monetization and analytics,” Prince bragged that the device is protected by “government-grade encryption.” Better yet, the pitch added, Unplugged is to be hosted on a global array of server farms so that it “can never be taken offline.” One option is said to be a server farm “on a vessel” located in an “undisclosed location on international waters, connected via satellite to Elon Musk’s StarLink.” An Unplugged spokesperson explained that ""they benefit in having servers not be subject to any governmental law."" The Unplugged investor pitch deck is a messy mix of these impossible claims, meaningless buzzwords, and outright fiction. The product is the latest example in a decade-long tradition of privacy- and security-focused smartphones that promise to do far more than your Android or iPhone can to protect you and your data. Ever since Edward Snowden’s 2013 revelations about American spying, a new phone has popped up in this market at least once per year. The trend was already so prominent by 2014 that MIT Technology Review called “ultraprivate phones” one of the year’s technology breakthroughs. Well, mea culpa. Almost every attempt to build this kind of phone has failed. While none of the experts I spoke with had yet been able to test the phone or read its code, because the company hasn't provided access, the evidence available suggests Unplugged will fall wildly short of what's promised. “No device is impenetrable—that’s been proven over time,” says David Richardson, vice president at the mobile security firm Lookout. A suspected North Korean heist of cryptocurrency only adds to an already long and growing list of big crypto hacks. The selling points of Unplugged’s device, known as the UP Phone, are built on enormous promises of security and privacy that go beyond what any phone can accomplish. Buzzwords like “government-grade encryption” imply some kind of heightened protection, but—as the company never mentions—governments use the same standard encryption as the rest of us. When asked about the phrase by MIT Technology Review, Unplugged acknowledged ""this messaging doesn't resonate well with our community"" and said they won't use it moving forward. “There are two things happening here,” says Allan Liska, a cyberintelligence analyst at the cybersecurity firm Recorded Future. “There are the actual attempts to make real secure phones, and then there is the marketing BS. Distinguishing between those two can be really hard.” Prince told investors the UP Phone is built by “engineers with deep experience in lawful interception, surveillance, and spoofing capabilities.” While taking various privacy and security enhancements from open source projects, Unplugged president Ryan Paterson told MIT Technology Review via email, Unplugged's proprietary operating system developed their own ""enhancements"" including ""based on knowledge not available to the public (zero-days) and others."" A zero-day vulnerability is an unknown security weakness that can be attacked via exploit that can sell for millions of dollars. Unplugged’s day-to-day technology operations are run by Eran Karpen, a former employee of CommuniTake, the Israeli startup that gave rise to the now infamous hacker-for-hire firm NSO Group. There, Karpen built the IntactPhone, which the company called a “military-grade mobile device.” He’s also a veteran of Israel’s Unit 8200, an agency that conducts cyber espionage and is the country’s equivalent of the NSA. But anyone with that experience should be able to see through Prince’s claim that the UP Phone is impossible to surveil. “When I worked in US intelligence, we [penetrated] a number of phone companies overseas,” says Liska. “We were inside those phone companies. We could easily track people based on where they connected to the towers. So when you talk about being impenetrable, that’s wrong.” “This is a phone, and the way that phones work is they triangulate to cell towers, and there is always latitude and longitude for exactly where you’re sitting,” he adds. “Nothing you do to the phone is going to change that.” The UP Phone’s operating system, called LibertOS, is a proprietary version of Google’s Android, according to an Unplugged spokesperson. It's running on an unclear mix of hardware that a company spokesperson says they've designed on their own. Even just maintaining a unique Android “fork”—a version of the operating system that departs from the original, like a fork in the road—is a difficult endeavor that can cost massive money and resources, experts warn. For a small startup, that can be an insurmountable challenge. “There's such a high volume of vulnerabilities that Android is disclosing and patching on an ongoing basis that you really do need to stay on top of all of those,” says Richardson. Keeping all the software and hardware compatible with every new version of Android is something that very few companies other than tech giants can effectively do. To deal with that, some niche phones simply don’t adopt new Android versions—a cheaper but more dangerous road. Another key issue is life span. Apple’s iPhones are considered the most secure consumer device on the market due in part to the fact that the company offers security updates to some of its older phones for six years, longer than virtually all competitors. When support for a phone ends, security vulnerabilities go unaddressed, and the phone is no longer secure. There is no information available on how long UP Phones will receive security support. Some other privacy phones are serious if imperfect products. The Librem 5, for example, is built by Purism, an American “social purpose corporation” specializing in privacy-oriented products. The phone is fully transparent and publishes source code and hardware details for anyone to see—unlike Unplugged, which has released precious few details next to its big promises. Librem is based on Linux, a free and open-source operating system that gives the lie to Prince’s false claim of being first to create an operating system outside Big Tech. Numerous commercially available phones have done this already. The Librem’s critical reception has been nuanced: reviewers have praised the phone’s ambition and details, as well as the relatively straightforward and honest marketing, a respectable feat in and of itself. (But, like so many Linux devices, it will appeal mostly to tech experts and people who can tolerate a significant learning curve.) GrapheneOS is another sober and credible project that has set out to deliver a secure, open-source, auditable operating system for Android phones. Unplugged sits on the opposite end of this spectrum. The company’s claim that the phone is “impenetrable” recalls the “hack-proof” phone that John McAfee, known for being accused of running a multimillion-dollar cryptocurrency fraud just before his death, tried to sell in 2017. Since it was publicly unveiled in June 2022, the Unplugged phone has become an object of skepticism and scorn among cybersecurity experts. “Words and phrases like ‘government-grade’ and ‘impenetrable’ are rightly mocked online by the computer security community because we know that they’re used to fool people,” says Nicholas Weaver, a cybersecurity researcher at the International Computer Science Institute. Weaver believes the UP phone is not so much about the technology as it is about the perceived sales opportunity. “This is right-wing affinity fraud,” he argues. In fact, the phone was originally called the “RedPill Phone,” a name based on a meme adored by the American far right. Prince is a vocal supporter of former president Donald Trump, and he debuted the phone on “War Room,” a podcast hosted by former Trump strategist Steve Bannon. Bannon and his fans got a discount code from the show. It’s initially surprising, then, to see Prince pitch investors on the idea that the phone will appeal to “right wing and left wing alike.” But this offers a clue as to why Unplugged dropped the RedPill name. Still, Prince may find a receptive audience in Bannon’s followers—which could matter greatly to the success of the phone. Its future will likely come down to how much customers believe in Prince and his claims. “I think for the layperson, it comes down to trust,” says Kyle Rankin, president of Purism. “Does this vendor that’s selling you a phone require you to trust them to be secure? And then if so, are they worthy of that trust? It boils down to that.” The question of trust has long been a tricky one for many security and privacy phones. For example, the security firm DarkMatter, an incognito intelligence agency for the United Arab Emirates that has reportedly been busted hacking dissidents and journalists, marketed its own “ultrasecure” phone called the Katim beginning in 2018. The same year, a sleek black phone dubbed Anom was marketed specifically to people involved in organized crime, promising an “ultrasecure” device “hardened against targeted surveillance and intrusion.” In fact, however, the phone company was secretly run by the FBI. Often the reasons for failure are simpler. The Blackphone, a security-first device that came out almost immediately after the Snowden leaks, is reported to have quickly fallen millions of dollars into debt because of low sales. And that device was more than $200 less expensive than the UP Phone. In short, the market is littered with failure. Ambitions to build a more secure smartphone are noble. Claims that your phone is impenetrable are misleading at best and dangerous at worst. The UP Phone is due out in November 2022. Updates: The story has been updated to include details provided by an Unplugged spokesperson. It was later updated to clarify the name of the Unplugged spokesperson.",0.0
Inside the software that will become the next battle front in US-China chip war,https://www.technologyreview.com/2022/08/18/1058116/eda-software-us-china-chip-war/,2022-08-18,"<p>The US has moved to restrict export of EDA software. What is it, and how will the move affect China?</p>
","Tech Review Explains: where our writers untangle the complex, messy world of technology to help you better understand the world we live in—and what comes next. The days when computer chips were designed by hand are long gone. Where chips contained thousands of transistors in the 1970s, they have more than a hundred billion today, and it’s impossible to create these designs manually. That’s where electronic design automation (EDA) software comes in. It’s a category of tools that help electrical engineers design and develop ever more complex chips. This software now forms the latest battle front in the tech trade war between China and the United States. On August 12, the US Commerce Department announced a multilateral export control on certain EDA tools, blocking China and over 150 other countries—essentially any country that isn’t a traditional US ally—from accessing them without specially granted licenses. EDA software is a small but mighty part of the semiconductor supply chain, and it’s mostly controlled by three Western companies. That gives the US a powerful point of leverage, similar to the way it wanted to restrict access to lithography machines—another crucial tool for chipmaking—last month. So how has the industry become so American-centric, and why can’t China just develop its own alternative software? Electronic design automation (also known as electronic computer-aided design, or ECAD) is the specialized software used in chipmaking. It’s like the CAD software that architects use, except it’s more sophisticated, since it deals with billions of minuscule transistors on an integrated circuit. There’s no single dominant software program that represents the best in the industry. Instead, a series of software modules are often used throughout the whole design flow: logic design, debugging, component placement, wire routing, optimization of time and power consumption, verification, and more. Because modern-day chips are so complex, each step requires a different software tool. Although the global EDA market was valued at only around $10 billion in 2021, making it a small fraction of the $595 billion semiconductor market, it’s of unique importance to the entire supply chain. The semiconductor ecosystem today can be seen as a triangle, says Mike Demler, a consultant who has been in the chip design and EDA industry for over 40 years. On one corner are the foundries, or chip manufacturers like TSMC; on another corner are intellectual-property companies like ARM, which make and sell reusable design units or layouts; and on the third corner are the EDA tools. All three together make sure the supply chain moves smoothly. From the name, it may sound as if EDA tools are only important to chip design firms, but they are also used by chip manufacturers to verify that a design is feasible before production. There’s no way for a foundry to make a single chip as a prototype; it has to invest in months of time and production, and each time, hundreds of chips are fabricated on the same semiconductor base. It would be an enormous waste if they were found to have design flaws. Therefore, manufacturers rely on a special type of EDA tool to do their own validation. There are only a few companies that sell software for each step of the chipmaking process, and they have dominated this market for decades. The top three companies—Cadence (American), Synopsys (American), and Mentor Graphics (American but acquired by the German company Siemens in 2017)—control about 70% of the global EDA market. Their dominance is so strong that many EDA startups specialize in one niche use and then sell themselves to one of these three companies, further cementing the oligopoly. US companies’ outsize influence on the EDA industry makes it easy for the US government to squeeze China’s access. In its latest announcement, it pledged to add certain EDA tools to its list of technologies banned from export. The US will coordinate with 41 other countries, including Germany, to implement these restrictions. The restricted tools are those that can be used for GAAFET (gate-all-around field-effect transistor) architecture, the most advanced circuit structure today, which is critical to making the latest chips and more advanced ones in the future. The Commerce Department is still seeking public comment to identify which EDA software is most helpful in achieving this specific structure and therefore should be added to the list. Chinese hardware giant Huawei was similarly restricted in 2019, losing access to all US EDA tools, and the Biden administration has continued the Trump administration’s preference for export controls as a trade war weapon. “Their previous round of restrictions on Huawei was so successful [that] they found this to be the track to pursue,” says Xiaomeng Lu, an analyst at the consultancy firm Eurasia Group. In the short term China won’t be that badly affected, because Chinese foundries are not advanced enough to make the state-of-the-art chips that need the GAAFET structure. But the blockade means Chinese chip design firms won’t be able to access the most advanced tools, and as time passes, they’ll most likely fall behind. However, blocking the export of software is very different from blocking the export of bulky hardware like lithography machines, which are impossible to smuggle into China because they are so traceable. EDA software tools are distributed online, so they can be pirated. Chinese companies could either hold on to the EDA software they have already purchased or resort to hacking licenses or acquiring them through shadow entities. That makes it harder to predict how effective this latest round of restrictions will be. China has slowly realized that it needs to develop domestic alternatives. In its latest five-year plan, which is the country’s top economic blueprint, EDA was listed as the first cutting-edge technology within the semiconductor industry where China needs to make breakthroughs. This means more government resources will be put into R&D in this area, including China’s government-backed semiconductor investment fund, which invested in the Chinese EDA company Huada Empyrean in 2018. Huada Empyrean, whose founder has worked in EDA design since the 1980s, is the country’s current leader in domestic EDA software, but it only accounts for 6% of the domestic EDA market. It is also far from developing a whole design flow, meaning its product can only replace a small part of what American companies offer. There are more startups emerging to fill the gap, most of them led by former Chinese employees of Cadence or Synopsys, like Nanjing-based startup X-Epic and Shanghai-based Hejian Industrial Software. With more international experience and less historical burden, these startups may be better positioned to challenge the status quo, says Douglas Fuller, an associate professor at Copenhagen Business School. “In the longer term, if they cobbled them together, they could be a better alternative to Huada,” he says. China has a vibrant software industry, which has produced some world-famous consumer tech apps like Tencent’s WeChat and Alibaba’s Alipay. “But in terms of industrial-use, corporate-use software? That’s China’s drawback,” says Lu. “For the longest time, Chinese industrial policy makers didn't realize [EDA software] is the real bottleneck.” It’s an area where it takes decades and billions of dollars of investment to make significant research advancements, so even though Chinese companies want to catch up now, it will take a long time before they make much progress. One big issue is talent. EDA tool development is such a niche field, and Chinese companies have traditionally struggled to attract many of the small numbers of engineers trained in making EDA tools. The latest EDA export control is targeted at a very specialized and advanced section of the industry. “This high-end software is not yet widely used by Chinese firms, so the restriction's immediate commercial impact on US suppliers is limited,” says Lu. But export controls are generally not welcomed by chip companies, as such policies can reduce the demand for American products and therefore their income. This is true not just for EDA companies, but also the foundries, IP companies, equipment manufacturers, and everyone else on the supply chain. “You would have a lot of angry companies,” Fuller says. “Not only in the US—not just the EDA companies.”",0.0
Hackers linked to China have been targeting human rights groups for years,https://www.technologyreview.com/2022/08/16/1057894/hackers-linked-to-china-have-been-targeting-human-rights-groups-for-years/,2022-08-16,"<p>In a new report shared exclusively with MIT Technology Review, researchers expose a cyber-espionage campaign on “a tight budget” that proves simple can still be effective.</p>
","A hacking group linked to China has spent the last three years targeting human rights organizations, think tanks, news media, and agencies of multiple foreign governments, according to a revealing new report from the cybersecurity firm Recorded Future. The report, shared exclusively with MIT Technology Review, offers new clues about how private contractors and front companies operating with relatively few resources can run long-standing hacking operations and succeed against high-value targets with crude but effective tactics. By using private-sector hackers, experts say, the Chinese government gains the ability to hit more espionage targets—and frees up resources within intelligence and military agencies to carry out more advanced hacking. The operation also hints at a widespread and persistent failure among vulnerable institutions to implement even basic cybersecurity defenses. A decade-long quest to become a cyber superpower is paying off for China. The hackers, known as RedAlpha, have taken aim at organizations including Amnesty International, the International Federation for Human Rights, Radio Free Asia, the Mercator Institute for China Studies, and other think tanks and government and humanitarian groups around the world. The hackers’ impact remains unclear, but judging from the sheer length of the campaign, analysts expect that the digital espionage has, broadly speaking, seen success. Recorded Future researchers have “high” confidence that RedAlpha is sponsored by the Chinese government as all of the targets “fall within [its] strategic interests,” says Jon Condra, director of the organization’s strategic threats team. Perhaps unsurprisingly, the hacking group has over the past few years been particularly interested in organizations in Taiwan, including the Democratic Progressive Party and the American Institute in Taiwan, which is the de facto United States embassy in the small island democracy. The government in Beijing claims Taiwan as part of Chinese territory. RedAlpha has been active since at least 2015, though it wasn’t publicly identified until 2018, in a report by Citizen Lab. It has consistently targeted groups that the Chinese Communist Party calls the “five poisons”: Tibetans, Uyghurs, Taiwanese, democracy activists, and the Falun Gong. All of these include domestic dissidents who, for various reasons, criticize and challenge the Communist Party’s grip on China. They also share international visibility and support. Citizen Lab’s work first uncovered RedAlpha’s campaign against the Tibetan community, government agencies, and a media group. In the years since, Recorded Future has identified additional cyber campaigns against Tibetans, and last year a report from PricewaterhouseCoopers indicated that the group is expanding its focus to include individuals, vulnerable ethnic groups, civil society organizations, and a rising number of government agencies. What’s particularly interesting about these new findings is that RedAlpha is still operating with the same simple and inexpensive playbook that it used years ago. In fact, this latest slate of espionage was linked to previous campaigns because the group reused many of the same domains, IP addresses, tactics, malware, and even domain registration information that has been publicly identified by cybersecurity experts for years. The Justice Department’s effort to prosecute cases of economic espionage had drifted from its stated mission and drawn fierce criticism for appearing to target researchers because of their ethnicity. “If it’s not broken, don’t change it,” Condra says. RedAlpha’s tactics are so simple and straightforward that Condra describes its work as espionage likely conducted on “a tight budget”—but in this case at least, simple can be pretty effective. “This is probably not the most well-resourced group,” he says. “They may want to cut corners and save some money when they register domains or acquire hosting. If there are campaigns they do with tactics that seem to work regardless of public exposure, there is no reason for them to change. It works and it’s cost effective.” More specifically, RedAlpha has created and weaponized hundreds of fake, malicious domains disguised as their targets in an effort to steal usernames and passwords. “I’m willing to bet this is a pretty effective tactic for them,” Condra says. Researchers say this is likely due to poor adoption of basic security safeguards by organizations in their crosshairs, which creates a low bar to entry for the hackers. “There are a lot of organizations that have not implemented multifactor authentication,” Condra adds. “That’s even more true on the government side in countries that move slower, have tighter budgets, and have more institutional resistance to change. We wouldn’t see RedAlpha doing this over the course of three years if they weren’t getting something out of it from their targets.” (Multifactor authentication is a cybersecurity technology that prevents hackers from taking over an account even if they have stolen a password; it is widely recommended and relatively easy to implement, but is often pushed aside for other priorities.) As tensions continue to increase between the United States and China over Taiwan, analysts say, the hackers were likely conducting espionage with the goal of producing political intelligence. The group also impersonated government agencies from India, Brazil, Vietnam, and Portugal. China is widely considered to be one of the world’s most active and highly capable cyber powers, alongside the United States. While it has hackers in its intelligence and military agencies, China has also reportedly used private contractors like RedAlpha to conduct cyber-espionage operations, according to multiple American indictments. Significant clues point to RedAlpha’s connections to important state groups. Shared details on registration of malicious domains connect the group to an individual who once said he was a member of the Green Army, China’s first underground hacking group, dating back to 1997. The Green Army, in fact, is one of the most important groups in the history of Chinese hacking; an alliance of several thousand Chinese nationalist hackers who targeted foreign websites, the organization gave rise to some of the country’s most prominent hackers, and parts of the faction evolved into major private sector cybersecurity firms still active today. What’s more, an email address used to register several of RedAlpha’s malicious domains across multiple espionage campaigns has been connected to a Chinese company that works with numerous government-owned companies, as well as the People’s Liberation Army University of Science and Technology, an elite state-run institution focused on researching high-tech Chinese military capabilities. Now known as Jiangsu Cimer Information Security Technology Co., the company provides defensive and offensive cybersecurity products. Jiangsu Cimer did not respond to a request for comment. “This strategy allows [the Chinese government] to outsource some of the lower-hanging fruit, the simple stuff that still needs to get done,” Condra says. “But this doesn’t necessarily need to be done by the most professional operators in China. They don’t need to burn the most valuable, advanced tools on low-level campaigns.” When reached for comment, a Chinese government spokesperson said the country opposes cyber attacks and “will never encourage, support, or connive at” them.",0.0
Modern security demands an empathy-first approach to insiders,https://www.technologyreview.com/2022/08/16/1057500/modern-security-demands-an-empathy-first-approach-to-insiders/,2022-08-16,"Ransomware, hackers, and nation-state threat actors have long dominated security teams’ focus. These external threats often feel more urgent and more dangerous, carrying greater potential consequences for the business. There’s no question about intent—it’s malicious. From a psychological perspective, it’s straightforward for security teams to identify the enemy and prepare for a fight. But what…","Provided byCode42 Ransomware, hackers, and nation-state threat actors have long dominated security teams’ focus. These external threats often feel more urgent and more dangerous, carrying greater potential consequences for the business. There’s no question about intent—it’s malicious. From a psychological perspective, it’s straightforward for security teams to identify the enemy and prepare for a fight. But what happens when the threat is internal? What if it’s coming from a teammate, a colleague you eat lunch with, or even the executive you report to? Security teams can’t take the same approach as with external actors, mainly because insiders aren’t faceless enemies to take down, even if their impact can be just as damaging. Insider risk occurs when sensitive corporate data—intellectual property, digital assets, trade secrets, crown jewels—move to untrusted places like personal devices, email, or cloud destinations. Such data movement presents considerable competitive, financial, privacy, and compliance risk. The scope of the problem is significant. According to Aberdeen Research, one in three reported data breaches involve an insider. The cost of a data breach from an insider can account for up to 20% of a company’s annual revenue. Insider risk is not a new problem, but it has become increasingly urgent due to drivers like digital transformation, hybrid-remote work, and the “great resignation,” not to mention a noticeable uptick in the use of contractors and recent layoffs. Insider risk can occur anywhere within a company, by anyone. It can come from former disgruntled employees stealing artificial intelligence trade secrets or someone poached by a competitor taking mobile chip design secrets on their way out the door. It can even come from the C-suite, as one company learned recently when its CFO accidentally shared a document to the entire company titled “Restructuring.” Unintentional data exposure can cause employee unrest, or even trigger US Securities and Exchange Commission (SEC) Regulation Fair Disclosure (Reg FD) filing requirements for public companies, if the leaked data could affect shareholders. For the security team, it may be inappropriate to take a combative approach—intended for outside threats—with a CFO over an unintentional data share. There is a better way. The way we should approach an external risk—like malware, for example—versus that from insiders is vastly different. There are many factors to consider when managing insider risk, especially as they relate to the desired business outcome. Insider investigations should not fall solely within the purview of the security team and often require the collaboration of security, HR, and legal. According to Gartner, “Survey data…indicates that over 50% of insider incidents are non-malicious,” which means that, more often than not, the employee at the root of the incident was simply trying to get their work done, making a mistake, or taking a shortcut. Treating them as though their actions were intentionally malicious is the wrong approach and could backfire. Those involved in the investigation must take an empathetic approach devoid of judgment. Otherwise, the risk of that employee making the same mistake again or becoming disgruntled and disenfranchised rises significantly. Approaching insider investigations with empathy requires a psychological shift. It is the first step to building trust, so the best outcome for the organization can be reached. Here are five important elements of an empathetic approach to insider investigations: Approaching insider investigations with empathy helps build a culture of trust, open communication, and respect. It builds and perpetuates a positive security culture—and best of all, it will help keep your organization’s most valuable data safe and secure. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.",0.0
Corruption is sending shock waves through China’s chipmaking industry,https://www.technologyreview.com/2022/08/05/1056975/corruption-chinas-chipmaking-industry/,2022-08-05,"<p>The arrests of several top semiconductor fund executives could force the government to rethink how it invests in the sector.</p>
","China’s chipmaking industry descended into chaos last week, with at least four top executives associated with a state-owned semiconductor fund arrested on corruption charges. It’s an explosive turn of events that could force the country to fundamentally rethink how it invests in chip development, according to analysts and experts. On July 30, China’s top anticorruption institution announced that Ding Wenwu, the chief executive of the China Integrated Circuit Industry Investment Fund, nicknamed the “Big Fund,” had been arrested for “suspected serious violations of the law.” Ding is not the only person in trouble. Two weeks ago, Lu Jun, a former executive at the Big Fund’s management institution, was also taken into custody, along with two other fund managers, according to the Chinese news outlet Caixin. Established in 2014, the Big Fund was intended to use government money to build a supply chain of chips made in China, thus reducing reliance on the US and its allies. The fund epitomizes the way the Chinese government can throw its weight behind a strategic industry—in this case, semiconductors. Eight years later, a total of $30 billion poured into the industry—with $20 billion more on the way—has yielded a complicated mix of successes and failures. The fact that the fund was driven by a political mission and not financial interests made it ripe for corruption, and analysts say the latest investigations may push China to manage semiconductor funding with more precision and professional knowledge. The idea of the Big Fund was to pour money into industries not getting funding from traditional routes like venture capital. Instead of startups, its first $20 billion funding round, in 2014, went after publicly listed companies and their subsidiaries, often in semiconductor materials and manufacturing, according to Rui Ma, a tech analyst and host of the podcast Tech Buzz China. These companies find it harder to make money because any progress in chipmaking requires a long period of time and significant investment in research. Therefore they’re less attractive to venture capitalists, Ma says. The Big Fund was arguably ahead of its time. In 2014, China’s central government decided it could use public funding to address the capacity gap in chip production while several local governments started experimenting with smaller funds. But it wasn’t until 2019, when the US cut off Huawei from accessing chips made with US technologies, that the situation became urgent. The semiconductor industry traditionally relies on global supplies, and Chinese tech companies are dependent on overseas suppliers like Taiwan’s TSMC, Korea’s Samsung, or the Netherlands’ ASML. All those countries are US allies. The urgency has only intensified in recent months and years: the US is increasingly squeezing China’s ability to access advanced chip technologies, even asking ASML to stop exporting older lithography machines to China. That makes the Big Fund, and the associated self-sufficiency drive, ever more important. The Chinese government has yet to reveal the exact reason why Ding and other people are being investigated. But most media outlets and analysts have associated the case with a cluster of corruption investigations around Tsinghua Unigroup, a semiconductor company invested in by the Big Fund that failed spectacularly in recent years. Founded in 1988, Tsinghua Unigroup is one of the oldest chipmakers in China. It made headlines in 2015 when its plan to acquire the American company Micron Technologies was blocked by the US government. Many of its ambitious acquisitions were backed by the Big Fund, which invested at least $2 billion in Unigroup and its subsidiaries to develop wafer manufacturing, flash memory chips, and 5G chips. But the juggernaut eventually faced bankruptcy in 2021. In July 2022, three former or current executives of Unigroup, including its chairman of 13 years, were put under investigation over corruption allegations, although no public charges have been posted so far. It remains unclear whether the failure of Unigroup directly triggered the anticorruption earthquake within Big Fund. However, the strategy that the latter has taken—throwing massive investments against the wall and seeing what sticks—can fail miserably. According to longtime observers, that strategy is also the perfect breeding ground for corruption. “This is the least surprising corruption investigation I’ve heard of for a while,” says Matt Sheehan, a fellow at the US think tank the Carnegie Endowment for International Peace. “Not because I know Ding Wenwu is personally corrupt, but when you have that amount of money sloshing around in an industry, it’d be way more surprising if there isn’t a major corruption scandal.” A significant part of the problem was a lack of precision, says Sheehan. China knew it needed to invest in semiconductors but didn’t know what exact sub-industry or company to prioritize. The country has been forced to learn by trial and error, feeling its way through issues like the bankruptcy of Unigroup and the expanding technology blockade by the US. The next step should be more targeted investments into specific companies, Sheehan says. That might mean a new boss for the Big Fund—someone who’s better versed in getting financial returns, says Paul Triolo, a senior VP at the business strategy firm Albright Stonebridge, which advises companies operating in China. Many of the Big Fund’s managers came from government backgrounds and may simply have lacked the relevant experience. Ding, who’s under investigation now, used to be a department director at China’s Ministry of Industry and Information Technology. “You need competent people to run this [Big Fund] that understand the industry, finance, and are not going to fund projects that don’t have a sound commercial basis,” Triolo says. Ultimately, these investigations may end up being positive for China’s semiconductor industry because they highlight the limitation of politically driven funding and may push the Big Fund to be managed on a more market-based basis. Beijing’s appetite for experiments is waning as its worries about self-sufficiency intensify. “They can’t afford to squander $5 billion on fabs that aren’t going to be viable,” says Triolo.",0.0
How governments seize millions in stolen cryptocurrency,https://www.technologyreview.com/2022/07/26/1056447/how-governments-seize-millions-in-cryptocurrency/,2022-07-26,"<p>Cryptocurrency hacks are increasing. Here’s how the government tries to track, freeze, and seize the stolen money before it disappears out of reach.</p>
","Tech Review Explains: where our writers untangle the complex, messy world of technology to help you better understand the world we live in—and what comes next. There have been so many recent multimillion-dollar cryptocurrency thefts that it’s easy to lose track. Organized crime, bad cybersecurity, financially motivated spies, and colorful criminals of all kinds have made so many headlines that even huge heists can go mostly unnoticed by the public. A suspected North Korean heist of cryptocurrency only adds to an already long and growing list of big crypto hacks. But sometimes the government is able to get it back. Last week, the United States seized $500,000 in cryptocurrency from alleged North Korean hackers who got that money by extorting American medical organizations. That’s just a drop in the bucket considering the grand total: the IRS alone seized $3.5 billion in cryptocurrency in 2021. But how exactly does seizing cryptocurrency work? Skilled criminals know they need to make dirty money clean. Money laundering is the age-old act of making the capital gained from illegal activity look as if it has no connection to the crime itself, so that the money can then be used freely. “I’d say the laundering is more sophisticated than the hacks themselves,” Christopher Janczewski, who was a lead case agent at the IRS specializing in cryptocurrency cases, told MIT Technology Review previously. More than $8.6 billion was successfully laundered through cryptocurrency in 2021. Unique among nations, North Korea has used theft of cryptocurrency as a means to fund its financially isolated regime. Pyongyang uses cryptocurrency to get around the restrictions imposed upon it and pay for anything from weapons to luxuries. The tactics are always evolving. A “peel chain” moves cryptocurrency through thousands of transactions to obfuscate the source and destination. “Chain hopping” crosses blockchains and currencies. “Cryptocurrency mixers” take transactions from anyone and then pay out in different wallets or even different currencies in an effort to disconnect the deposits and withdrawals. All of this is meant to throw off investigators. The US government has invested significantly in blockchain surveillance and analysis tools. Between rising electricity rates and soaring climate costs, cryptomining is taking its toll on communities. Companies like Chainalysis, TRM Labs, and Elliptic sell software to track and analyze the cryptocurrency ecosystem. Governments have heavily bought into this nascent industry as a way to unmask hackers stealing, laundering, and cashing out of illicit cryptocurrency. For example, TRM Forensics is a product designed to trace cryptocurrency transactions across 26 different blockchains, graph the flow of funds, and identify the wallets where the coins ended up. Similarly, Chainalysis Reactor provides ongoing surveillance of different cryptocurrency assets so a customer, like a US government agency, can know if a specific wallet belongs to a darknet market, a high-risk cryptocurrency exchange, or an online casino. The output will include neat sets of data visualizations ready for government investigations and, eventually, court prosecutions. But no amount of tracing by software will actually get the money back. “Tracing is just one tool in the toolbox,” says Ari Redbord, a former federal prosecutor and currently the head of government affairs at TRM Labs. “Then they have to use police work for the end of the rainbow. Some of it is just great investigative work.” There are three basic ways the US government can lawfully access and seize funds. The largest single seizure in US history came just this year, when the Justice Department took hold of $3.6 billion in cryptocurrency allegedly stolen during the 2016 hack of Bitfinex, a virtual currency exchange. This case was, in key ways, much simpler for American police because two arrests of US residents were made in Manhattan. Blockchain analysis found that the stolen currency was moved, after a long but unsuccessful attempt to launder the cash, to accounts controlled by a suspect. Police got a search warrant for the suspect’s cloud storage account, which contained an encrypted file. The file was decrypted and found to contain 2,000 cryptocurrency addresses and private keys. Almost every wallet was linked directly to the Bitfinex hack. Law enforcement received a seizure warrant and took the money into the government’s possession—and arrested two suspects. The cryptocurrency ecosystem has a reputation in the popular imagination as a Wild West. But the truth is that, in a bid to do business and make money in wealthy nations, exchanges and other cryptocurrency businesses have become vastly more compliant with Western law enforcement over the years. After meeting probable-cause and burden-of-proof requirements, law enforcement can get seizure warrants for any illicit funds that eventually land on compliant exchanges—and many funds eventually do. Law enforcement will then work with the crypto business to move the funds to a government-controlled wallet or freeze them. “Another method is that the adversary or a member of their conspiracy cooperates and provides private keys to the government as part of a plea negotiation or cooperation to benefit them in some way,” says Gurvais Grigg, who was assistant director of the FBI before becoming an executive at Chainalysis. The third possibility is to compromise the target’s security—which can happen in numerous ways. “When you're talking about a country like North Korea or Russian cyber criminal organizations, it can take years of building out networks of confidential informants and working with other governments, even those that aren't always friendly to us,” Redbord says. “One piece is potentially hacking into a server or machine or, frankly more likely, just great police work.” For hackers outside the United States, the task is trickier. An arrest can be impossible if the suspect is in a country that doesn’t cooperate with Washington, so prosecutors focus elsewhere. “Good prosecutors understand that a criminal prosecution is only one part of the larger investigation and results in these types of cases,” says Redbord, who was a prosecutor for 11 years. Instead, the focus is the money. The other aspects are regulation, politics, and diplomacy. There are several notable “rogue areas” around the world that don’t comply with international anti-money-laundering rules, Grigg says, including North Korea and Iran, “but those parts of the world are becoming smaller and smaller islands.” There are two reasons for that. If you’re a business, compliance means you have a chance to access the world’s richest markets; if you’re a nation, it means your own lawful orders can be honored in return. As governments become better at surveilling and seizing cryptocurrency, hackers and criminal tactics continue to evolve. Mixers offer a popular tactic these days. Mixers take in funds from various origins, pool them together, and then send funds back out at random as a way to obfuscate their source and ultimate destination. Although there are numerous reasons one could use mixers, their chief customers have always been criminals and hackers. According to a recent report from Chainalysis, mixers have moved over $50 million monthly on average this year, twice as much as last year. Blockchain analysis firms are hustling to tackle the problem and reliably “demix” the funds, but for now, mixers remain a go-to tool for criminals. The US Treasury Department has opted for another, more immediate approach: in May 2022, the US issued the first sanctions against a cryptocurrency mixer. This one was allegedly used to launder cryptocurrency following a $600 million theft by North Korean hackers. “The last thing we’ve seen is the increase in the multiplicity of attacks,” Griggs says. “Think of thousands of wildebeests crossing a river at once so that crocodiles can only get a few. Attackers have flooded the zone with an increased number of attacks, potentially in the hopes of making it difficult for authorities to catch an individual actor. “The problem is that investigators can link what appear to be disparate attacks back to a central command, and in some cases this might make it easier for the government to prove a large conspiracy.”The efforts to track, freeze, and seize the funds will only become more important. And it’s just as certain that billions will continue to slip through the cracks. Just before news of the US seizure against North Korean hackers made headlines, another group from North Korea launched an international ransomware hacking campaign.",0.0
"The China AI and Autonomy Report: Issue 21, August 25, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-21,2022-08-25 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. We begin this issue with coverage of the PRC exercises around Taiwan that followed Speaker of the House Nancy Pelosi's trip there on August 2-3. According to Taiwan media, PRC UAVs overflew Jinmen and other islands belonging to Taiwan, leading the Taiwan military to fire flares as warning shots. We also discuss three indicators that PRC commercial unmanned systems are being used for military purposes: (1) The Russian embassy in Beijing deleted a Weibo post praising the military utility of drones manufactured by the PRC company DJI. (2) A “robot dog” carrying an anti-tank weapon demonstrated at a Russian arms exhibition appeared to be of PRC origin. (3) The US-based website The War Zone reported extensively on strikes against the Russian Black Sea Fleet headquarters and a Russian oil refinery that were conducted using UAVs similar in appearance to a PRC commercial drone. In non-military news, the Cyberspace Administration of China (CAC) has published a list of 30 recommendation algorithms, along with short descriptions of their use. The normally tightly held algorithms were submitted to CAC by many of China's top tech companies in accordance with a regulation that came into effect on March 1. Meanwhile, the Beijing city government has announced a plan to develop the market for virtual humans—digital avatars often used in advertising and entertainment. PRC Exercises Around Taiwan",0.0
"The China AI and Autonomy Report: Issue 20, August 11, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-20,2022-08-11 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. We begin with PRC media discussion of the Russia-Ukraine war and counter UAV operations. The Hong Kong-based Phoenix News reports that the Russian military has been successful in downing Ukrainian UAVs with the use of an integrated air defense system combining long-, medium-, and short-range missiles and artillery. An article in the PLA Daily, however, discusses some of the difficulties in countering UAVs and concludes that challenges will remain for some time. As for future warfare, a PLA Daily article urges its readers to move beyond a mentality focused on mechanized warfare and to focus instead on intellingentized warfare. The PLA Daily also carried a lengthy article on the use of “digital twin” technologies to improve the PLA’s education, training, command and control, and research and development (R&D). Meanwhile, a student team from the PLA’s National University of Defense Technology took two first-place finishes in an international robotics competition held in Bangkok. Moving away from military news, Guangming Daily discusses how technology can be used to address grassroots social governance issues. In industry news, both Fortune and the Financial Times report that venture capital firm Sequoia China will invest a likely record-breaking US$9 billion in PRC tech startups, focusing on PRC government priorities such as AI. Russia-Ukraine War",0.0
"The China AI and Autonomy Report: Issue 19, July 28, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-19,2022-07-28 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. We begin this issue with a discussion of two PRC media reports commenting on the successes and challenges that the Russian military has experienced in its UAV and counter-UAV operations in Ukraine. Some of the challenges discussed include the inability of Russian radar systems to detect Ukraine’s UAVs, large numbers of Ukraine’s UAVs overwhelming Russia’s air defense systems, and the Russian military’s inability to obtain a sufficient supply of UAVs for the war effort. We also cover several PLA Daily articles examining intelligent warfare, discussing such topics as countering unmanned systems, cognitive warfare in a “post-truth” era, the importance of high-quality data in command decision-making, and hybrid warfare involving the metaverse. Recognizing the critical role of information fusion technologies in collecting data from various types of sources in an intelligentized warfare environment, the Chinese Institute of Command and Control has established the Information Fusion Professional Committee. PLA researchers are trying to use AI-enabled cubesats (satellites weighing less than 2 kg) to defend their satellites, envisioning that AI could be used to determine exactly when and where to release the cubesats to fend off enemy satellites. A journal article reveals that China’s smart court SoS (system of systems) “now connects to the desk of every working judge across the country.” The smart court SoS trains on 100,000 cases from across the country every day, and judges must provide an explanation if they disagree with the system’s recommended case decisions. Meanwhile, the PRC government has fined ride-hailing company Didi $1.2 billion for violating its laws on data security and personal information protection. Russia-Ukraine War",0.0
"Artificial Intelligence and Autonomy in Russia: Issue 43, August 8, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/ai-and-autonomy-in-russia/issue-43,2022-08-08,,Intro (no tag) GOVERNANCE AND LEGISLATION,0.0
