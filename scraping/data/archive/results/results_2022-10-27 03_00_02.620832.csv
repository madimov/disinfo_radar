title,url,date,summary,cleaning,category
CMU Takes a Big Step Toward Real-Time Realistic Video Generation Based on Language Descriptions,https://syncedreview.com/2022/10/26/cmu-takes-a-big-step-toward-real-time-realistic-video-generation-based-on-language-descriptions/,2022-10-26,"
In the new paper Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization, researchers from Carnegie Mellon University leverage CLIP-guided, pixel-level optimization to generate 720p resolution videos from natural language descriptions at a rate of one-to-two frames per second — taking a big step towards a real-time text-to-video system.

 ","There are now dozens of AI-powered text-to-image models on the Internet, with DALL.E Mini alone generating more than 50,000 images daily from users’ natural language prompts. The next challenging step for such generative AI models is text-to-video — which brings the potential for creating animated scenes based on users’ storytelling inputs. While current text-to-video approaches guided by Open AI’s CLIP network can translate text into highly-realistic imagery, they are slow — requiring from 17 seconds to five minutes to generate a single frame of video. In the new paper Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization, researchers from Carnegie Mellon University leverage CLIP-guided, pixel-level optimization to generate 720p resolution videos from natural language descriptions at a rate of one-to-two frames per second — taking a big step towards a real-time text-to-video system. Existing CLIP-Guided text-to-video approaches generate their highly realistic imagery by optimizing through large pretrained image generator diffusion models, a process that is both time-consuming and computationally heavy. The CMU team employs a novel two-step approach to approach real-time text-to-video generation: 1) Generating noisy semantic content at a fast speed; and 2) Refining the generated image textures in a post-processing step. The proposed approach generates each frame sequentially while iterating through the input language to guide the content. CLIP-Guided techniques are used to compare the frame and the language description and evolve the frame toward consistency with the content. A trained CycleGAN model smooths and denoises the generated images. In their empirical study, the team demonstrated their approach’s ability to generate realistic videos at up to 720p resolution at speeds 20-300 times faster than existing methods. The code and sample videos are available on the team’s website. In future work, the researchers plan to add priors to enable smoother motion in the videos and improve user control over their style and appearance. The paper Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
DeepMind Study Shows That Language Models Can Learn From Explanations in Context Even Without Tuning,https://syncedreview.com/2022/10/25/deepmind-study-shows-that-language-models-can-learn-from-explanations-in-context-even-without-tuning/,2022-10-25,"
In the new paper Can Language Models Learn From Explanations in Context?, DeepMind researchers investigate how different types of explanations, instructions, and controls affect language models’ zero- and few-shot performance and how such explanations can support in-context learning for large language models on challenging tasks.
","If you have ever considered the answer to a question and asked “…but why?” you are not alone. Humans have an innate ability to improve their learning and broaden their understanding via explanations that relate examples to principles. The machine learning community in recent years has witnessed the rapid growth of few-shot prompting language models (LMs) that exhibit impressive transfer learning capability, enabling them to successfully perform new tasks by adapting to a few in-context examples. Might these LMs benefit, as humans do, from explanations of these few-shot examples? In the new paper Can Language Models Learn From Explanations in Context?, DeepMind researchers investigate how different types of explanations, instructions, and controls affect language models’ zero- and few-shot performance and how such explanations can support in-context learning for large language models on challenging tasks. The team highlights their main contributions as follows: The team considered a set of decoder-only transformer models ranging from 1 billion to 280 billion parameters and crafted a variety of control explanations that match different aspects of the semantics and word- or sentence-level content, including scrambled explanations, true non-explanations, and other item explanations. They tested model performance under each prompt condition on all task dataset items (except those in the prompt) and calculated the model’s likelihood of returning each answer option. They then chose the highest-likelihood answer from the set and evaluated model accuracy based on the answer scores defined by the task. In their empirical experiments, the team examined the benefits of different prompt components for the largest (280B parameter) LM and the relative distribution of benefits from different explanation types. They also provided raw summaries of the average effects of untuned explanations across model scales. Their findings can be summarized as follows: The researchers believe their work can contribute to improved prompt engineering and scientific understanding of the in-context learning abilities of large LMs. The paper Can Language Models Learn From Explanations in Context? is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Google & Stanford Team Applies Chain-of-Thought Prompting to Surpass Human Performance on Challenging BIG-Bench Tasks,https://syncedreview.com/2022/10/24/google-stanford-team-applies-chain-of-thought-prompting-to-surpass-human-performance-on-challenging-big-bench-tasks/,2022-10-24,"
In the new paper Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them, a Google Research and Stanford University team applies chain-of-thought (CoT) prompting — a series of intermediate reasoning steps — to 23 BIG-Bench tasks on which language models have failed to outperform the average human rater. The proposed approach enables models to surpass human performance on 17 of the 23 tasks.
","Today’s large language models (LLMs) have demonstrated game-changing performance across a wide range of tasks and domains, but they have their limits. These weaknesses can be identified by the Beyond the Imitation Game benchmark (BIG-Bench, Srivastava et al., 2022), which evaluates LLM capabilities on a diverse suite of especially challenging tasks. A 540B parameter PaLM language model surpasses average human-rater performance on 65 percent of the BIG-Bench tasks, but what about the remainder — are they simply unsolvable by LLMs? A Google Research and Stanford University team addresses this question in the new paper Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. The team applies chain-of-thought (CoT) prompting — a series of intermediate reasoning steps inspired by the paper Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (Wei et al., 2022b) — to 23 BIG-Bench tasks on which LLMs have failed to match the average human rater. Their strongest resulting model outperforms the human baseline on 17 of the 23 tasks. The team selected their 23 evaluation tasks — a subset they dub BIG-Bench Hard (BBH) — from BIG-Bench tasks where state-of-the-art LLMs perform worse than the average human rater, tasks fundamentally not solvable by scaling existing LLMs, and tasks that require prompting techniques beyond the standard few-shot prompting setup. In their experiments, the team applied the standard BIG-Bench answer-only prompting setup and the proposed CoT prompting approach on three language model families — Codex, InstructGPT and PaLM — to explore whether and to what extent CoT prompting can improve performance on the 23 BBH tasks. The results show that conventional answer-only prompting underestimates LLM performance and capabilities on challenging tasks that require multiple reasoning steps; as CoT prompting achieves double-digit improvements for all three models, surpassing the average human-rater score on 10 of the 23 tasks on PaLM, on 15/23 tasks on InstructGPT, and on 17/23 tasks on Codex. The paper also details the effects of CoT prompting on four BBH task categories: algorithmic and multi-step arithmetic reasoning, natural language understanding, use of world knowledge, and multilingual knowledge and reasoning.The data, prompts, and Codex model outputs are available on the project’s GitHub. The paper Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
"Wider, Not Deeper: Cambridge, Oxford & ICL Challenge Conventional Transformer Design Approaches",https://syncedreview.com/2022/10/20/wider-not-deeper-cambridge-oxford-icl-challenge-conventional-transformer-design-approaches/,2022-10-20,"
In the new paper Wide Attention Is The Way Forward For Transformers, a research team from the University of Cambridge, Imperial College London, and the University of Oxford challenges the commonly held belief that deeper is better for transformer architectures, demonstrating that wider layers result in superior performance on natural language processing tasks.
","Transformers have become a preferred architecture in the machine learning community, and building deeper models is the common approach for improving their performance. But is deeper necessarily better? In the new paper Wide Attention Is The Way Forward For Transformers, a research team from the University of Cambridge, Imperial College London, and the University of Oxford challenges the commonly held belief that deeper is better for transformer architectures, demonstrating that wider layers result in superior performance on natural language processing (NLP) tasks. The team summarizes their main contributions as follows: The paper first evaluates the impact of model aspect ratio — the ratio of layers to heads — on model accuracy, runtime performance, model size, and interpretability. Unlike conventional approaches, which focus on finding more efficient attention styles or using network architecture search (NAS) to obtain optimal combination operators, the team considers a more coarse-grained design space by changing the model aspect ratio. This enables them to evaluate novel architectures, such as a single-layer model with many parallel heads. The researchers performed experiments on four text classification tasks: sentiment analysis on the IMDb dataset at both the token and byte level, Listops 10-way classification, and byte-level document matching. They also investigated how widening the attention layer would affect ten different types of transformer attention mechanisms. The researchers summarize the empirical results as follows: Overall, this work shows that the proposed wide transformer networks can achieve performance comparable to or better than deep transformers. The researchers conclude that wider and shallower models are thus a “viable and desirable alternative” for transformers when there is no pretraining of weights or embeddings.The paper Wide Attention Is The Way Forward For Transformers is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
"Embedding Training With 1% GPU Memory and 100 Times Less Budget, an Open Source Solution for Super-Large Recommendation Model Training on a Single GPU",https://syncedreview.com/2022/10/19/embedding-training-with-1-gpu-memory-and-100-times-less-budget-an-open-source-solution-for-super-large-recommendation-model-training-on-a-single-gpu/,2022-10-19,"
Colossal-AI has successfully used a heterogeneous training strategy to increase the number of NLP model training parameters capacity by hundreds of times at the same hardware. And experiment results show that it only needs to keep 1~5% of the embedding parameters in the GPU, and is still able to maintain excellent end-to-end training speed.
","Deep recommendation models (DLRMs) have become critical for deep learning applications in IT companies. DLRMs can be used to improve user experience for video recommendations, shopping searches, and companies’ advertisements. However, DLRMs have several limitations, including difficulties managing too much user data, frequent model updates, and high training costs. DLRMs first search an embedding bag (EmbeddingBags), and then go through a dense DNN. Embedded tables usually hold more than 99% of the memory in the DLRM, and only 1% of the computation requirements. With the help of GPU’s on-chip high-speed memory (High Bandwidth Memory) and increased computing power, GPU has become the mainstream hardware for DLRM training. However, with the increasing research depth of recommendation systems, the embedding tables are also growing in size, and the limited GPU memory is not able to keep up. The question remains: How can we use GPU to efficiently train super-large DLRM models despite the limitation of GPU memory? Colossal-AI has successfully used a heterogeneous training strategy to increase the number of NLP model training parameters capacity by hundreds of times at the same hardware. Recently, it has used a software cache method which dynamically stores the embedding table in the CPU and GPU memory to extend the parameters to the recommendation system. In relation to the software cache design, Colossal-AI also incorporates pipeline prefetching which reduces software cache retrieval and data movement overhead by observing future training data. At the same time, it trains the entire DLRM model on the GPU in a synchronized update manner, which can be scaled to multiple GPUs with the widely used hybrid parallel training method. Experiments show that Colossal-AI only needs to keep 1~5% of the embedding parameters in the GPU, and is still able to maintain excellent end-to-end training speed. Compared with other PyTorch solutions, the memory requirements are reduced by an immense magnitude, with a single GPU being able to train a terabyte-level recommendation model. As a result, the cost advantage is significant. For example, only 5GB of GPU memory can be used to train a DLRM that occupies a 91GB Embedding Bag. The training hardware cost is reduced from two NVIDIA A100s totaling about 30,000 USD, to an entry-level graphics card like RTX 3050 which only costs about 300 USD. Open Source Repo：https://github.com/hpcaitech/ColossalAI The purpose of the embedding table is to map categorical variables into floating-point variables. The following figure shows the training process of the embedding table in DLRMs. First, it identifies the corresponding records for each feature in the embedding table, outputs a feature vector through reduction operations (i.e. max, mean, and sum operations) and then inputs them to the subsequent dense neural network. The embedding table DLRM training process is comprised mainly of irregular memory access operations, so it is severely limited by the hardware memory access bandwidth. In real applications, the embedding table of a DLRM may reach hundreds of GB, or even TB levels, far exceeding the single GPU capacity of only a few tens of GB. There are many ways to increase the size of DLRM’s embedded table. Taking the memory hierarchy diagram of the GPU cluster shown in the figure below as an example, we can analyze the advantages and disadvantages of several common solutions. GPU model parallelism: During this method, the embedding table is sharded and distributed in the memory of multiple GPUs, and the intermediate results are synchronized via the interconnection network between GPUs. The disadvantage of this method is that the workload of the embedded table may not be equal, and it is difficult to scale. Furthermore, the initial investment of adding GPUs is high, and the computing power of GPU is not fully utilized. DLRM mainly utilizes the HBM bandwidth of GPUs, while computing units are not utilized well. Hybrid Training: This method starts by splitting the embedding table into two parts, one trained on the GPU and the other trained on the CPU. By using long-tail input data distribution, we can minimize the CPU computing and maximize GPU computing. However, as the batch size increases, it becomes difficult to ensure that all the data in the mini-batch hits the CPU or GPU. Additionally, since the DDR bandwidth and HBM differ by magnitude, even if just 10% of the input data is trained on the CPU, the entire system will slow down by at least two times. The CPU and GPU also need to transmit intermediate results, which requires lots of overhead communication, further slowing down the training speed. Consequently, researchers have designed methods like asynchronous updates to combat these issues, but asynchronous methods can cause uncertainty in training accuracy, and are not ideal for algorithm engineers. Software Cache: All training is performed on the GPU with this method, and the embedding tables are kept in the heterogeneous memory space composed of the CPU and GPU. Each time the software cache is used, the used part is exchanged into the GPU. In this way, storage resources are expanded inexpensively while meeting the increased demand for embedded tables. Compared to using the CPU to calculate, the entire training process is completed on the GPU, making full use of the HBM bandwidth advantage. On the contrary, cache query and data movement of this method will bring additional performance loss. Currently, there are some excellent software cache solutions for embedding tables, but they are often implemented using customized EmbeddingBags Kernel, such as fbgemm, or with the help of third-party deep learning frameworks. With native PyTorch, Colossal-AI can implement a unique set of software Cache EmbeddingBags, further optimize the DLRM training process, and propose a prefetch pipeline to further reduce Cache overhead. Colossal-AI implements a class of CachedEmbedding which works as a subclass of the nn.Module in PyTorch and can replace the native PyTorch EmbeddingBag. It consists of software which manages the CPU and GPU memory. It maintains EmbeddingBag parameters as CPU Weight. A small part of the EmbeddingBag called the CUDA Cached Weight is stored as GPU memory, which will be used for future training. During DLRM training, records of the embedding table are first identified by the current mini-batch. If some records are not in the GPU yet, they are transmitted from the CPU Weight to the CUDA Cached Weight. If there is not enough space in the GPU, the LFU algorithm will be used to discard the least frequently used embedding records. In order to query the cache efficiently, some auxiliary data structures are needed: the cached_idx_map is a 1-D array mapping the indices of records in the CPU Weight to the indices of CUDA Cached Weight, as well as the GPU access frequency. The ratio of the CUDA Cached Weight size to CPU Weight size is named cache_ratio and defaults to 1.0%.The cache operates before each forward iteration to adjust the data in the CUDA Weight in three steps. Step 1:Query CPU records: Query record indices of CPU Weight that need to be cached. This requires intersecting the cached_idx_map and the input of the current mini-batch. Step 2: Query GPU records: Identify CUDA Cached weight that should be evicted according to frequency. This requires performing a top-k operation on the different sets of cache_idx_map and input of the current mini-batch. Step 3: Data transmission: Free enough space on the CUDA Cached Weight for CPU Weight, which may lead to queried GPU records being transferred from GPU to CPU. Then move the to-be queried CPU records from the CPU Weight into CUDA Cached Weight. The processes in Step 1 and Step 2 of the Cache are memory demanding. In order to take advantage of the bandwidth on the GPU’s HBM, they are run on the GPU and implemented using drop-in API provided by PyTorch. The overhead of Cache operations is particularly prominent compared to the training operations of the embedding table on the GPU. For example, for a training task that takes 199 seconds, the overhead of the cache operation is 99 seconds, which accounts for nearly 50% of the overall computing time. The main overhead of the Cache is mainly caused by Step 1 and Step 2 in the cache operation, and the base in the figure below shows the total time decomposition of the cache operation. The red and orange stages (Step 1, 2) account for 70% of the total cache overhead. The problem above arose because the traditional Cache strategy is somewhat “short-sighted”, so the Cache is adjusted according to the input of the current mini-batch, and most of the time is wasted on query operations. In order to reduce the overhead time of the Cache, Colossal-AI has designed a “far-sighted” Cache mechanism. Instead of only performing Cache operations on the first mini-batch, Colossal-AI fetches several mini-batches that will be used later, and performs Cache query operations together. As shown in the figure below, Colossal-AI uses prefetching to merge multiple mini-batches of data and conduct one cache operation after merging. It also uses a pipeline method to overlap the overhead of data loading and model training. As shown in the following figure, the number of mini-batches prefetched in the example is 2. Before starting training, it loads mini-batch 0 and 1’s data from disk to GPU memory, conducts Cache operation, and then performs forward and back propagation and a parameter update of these two mini-batches. This can simultaneously be read with the initial data of mini-batch 2 & 3, and this part of the overhead can overlap with the calculation. Compared with the execution mode of baseline cache, Figure [Time decomposition of Cache operation] compares the time decomposition of cache operation using 8 mini-batches prefetching with a baseline cache without prefetching. The total training time dropped from 201 seconds to 120 seconds, and the proportion of cache queries shown in the figure also dropped significantly. To sum up, Cache pipeline prefetching brings two benefits. The most obvious benefit of prefetching is reducing cache operation’s Step 1 and Step 2 overhead, so that this two-step operation accounts for less than 5% of the total training process. As shown in Fig. [Time Decomposition of Cache Operations], by pre-fetching 8 mini-batches of data, the overhead of cache queries is significantly reduced compared to the baseline. By concentrating more data and improving the granularity of data transmission, the CPU-GPU transmission bandwidth can be fully utilized. For the example above, the CUDA→CPU bandwidth is increased from 860MB/s to 1477MB/s, and the CPU→CUDA bandwidth is increased from 1257MB/s to 2415MB/s, almost double the performance gain. Our CachedEmbeddingBag is consistent with the basic usage of the PyTorch EmbeddingBag. When building a recommendation model, only a few lines of code can significantly increase the capacity of the embedding table and complete TB super-large recommendation model training at a low cost. The testbed is a GPU cluster with 8x NVIDIA A100 GPU (80GB) and AMD EPYC 7543 32-Core Processor (512GB) CPU. Colossal-AI also uses Meta’s DLRM model implementation, and evaluates on a Cretio 1TB dataset, as well as a synthetic dataset. The PyTorch training with the entire embedding table on the GPU is used as the baseline in the experiments. ### Cretio 1TB embedding table has 177,944,275 records, and its memory allocation takes up 91.10GB, with embedding dim equal to 128. To accommodate EmbeddingBags on one single GPU is impossible, even with top-end NVIDIA A100 with 80GB GPU memory. Hopefully, Colossal-AI will make it possible to accomplish the training task on one GPU, with memory consumption dropping to 5.01 GB (lowering approx. 18 times), and show the possibility of training super large (terabyte-level) recommendation system models on just one GPU. In terms of training speed, the following figure shows the latency of training 100M samples with different batch sizes. Prefetch1 (shown in dark green) is the latency without pre-fetching, and Prefetch8 (shown in blue) is the latency with prefetching (prefetch mini-batch=8). This shows that prefetch flow optimization plays an important role in overall performance improvement. Each bar colored with darker colors in the figure is part of the Cache overhead, being controlled within 15% of the total training time after pre-fetching. In our experiment, DLRM is trained with 100M samples on 8 GPUs, using table-wise sharding as EmbeddingBags in a parallel manner (global batch size = 8192, prefetch size = 4). The following figure shows the training latency for cases with a different number of GPUs. ColossalAI-cr-0.05 in the figures indicates the cache ratio is 0.05, while Colossal-cr-0.5 is 0.5. The PyTorch and Colossal-AI training times are mostly similar, but PyTorch encounters OOM issues when training on 1 GPU. It can be observed that adding GPUs (increasing to 4 or 8) does not bring significant performance benefits as synchronizing results requires huge communication overhead and table-wise sharding, leading to an unbalanced slice load. In other words, using multiple GPUs to scale embedding table training does not have significant advantages. The graph below shows the maximum memory usage, varying across different numbers of GPUs. When using one GPU, only a software Cache method from Colossal-AI works, with the memory assumption of multiple cards in parallel showing a significant reduction. The synthetic dlrm_datasets from Meta Research mimic the training access behavior of embedding tables. As a result, it is usually used as a reference for testing hardware and software designs that relate to recommendation systems. Subsequently, 500 million rows of these embedding table items are selected as sub-datasets, and two EmbeddingBags of 256GB and 128GB are constructed for testing. With GPU memory limitations, PyTorch has poor performance when training on one NVIDIA A100 GPU. In contrast, Colossal-AI’s software cache significantly eases GPU memory requirements, is capable of training embedding tables as large as 256GB, and also shows the potential to scale to terabytes. The acceleration is also demonstrated by running prefetching, where total training time decreases by 60% (#prefetches = 32) and GPU memory demand does not increase. Colossal-AI is a user-friendly deep learning system that allows companies to maximize AI deployment efficiency while drastically reducing costs. Since becoming open source to the public, Colossal-AI has reached №1 in trending projects on GitHub and Papers With Code multiple times, amidst other projects that have as many as 10K stars. Colossal-AI values open source community construction, providing English and Chinese tutorials, while supporting the latest cutting-edge applications such as PaLM and AlphaFold. Ultimately, Colossal-AI is constantly increasing the availability of AI solutions across a variety of fields, including medicine, autonomous vehicles, cloud computing, retail, chip production, etc. PortalProject address: https://github.com/hpcaitech/ColossalAI Reference Embedding Training With 1% GPU Memory and 100 Times Less Budget, an Open Source Solution for Super-Large Recommendation Model Training on a Single GPU | by Yang You | Oct, 2022 | Medium https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/ We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Meet Magneto: Microsoft’s Foundation Transformer for General-Purpose Modelling Across Tasks and Modalities,https://syncedreview.com/2022/10/18/meet-magneto-microsofts-foundation-transformer-for-general-purpose-modelling-across-tasks-and-modalities/,2022-10-18,"
In the new paper Foundation Transformers, a Microsoft team proposes a method for true general-purpose modelling. Their Foundation Transformer is a single unified transformer that provides guaranteed training stability and can handle diverse tasks and modalities without performance degradation.
","The machine learning community has seen a trend in recent years, with researchers working to converge their model architectures across language, vision, speech, and multimodal classes. While transformer architectures have become the de facto standard for building such highly desirable general-purpose foundation models, the optimal transformer variants still differ for different input modalities. In the new paper Foundation Transformers, a Microsoft team proposes a method for true general-purpose modelling. Their Foundation Transformer is a single unified transformer that provides guaranteed training stability and is capable of handling diverse tasks and modalities without performance degradation. The team first identifies the properties a foundation model should possess for true general-purpose modelling: 1) The desired modelling should be able to serve as a go-to architecture for various tasks and modalities, so that we can use the same backbone without trial and error, and 2) The architecture should provide guaranteed training stability. The proposed Magneto is a Foundation Transformer implementation designed to achieve the abovementioned goals. Magento uses Sub-LayerNorm (Sub-LN), which adds another LayerNorm inside each sublayer. The team also introduces a novel initialization method theoretically proven to guarantee training stability, enabling the model to be scaled relatively easily. In their empirical studies, the team compared Magneto with popular transformer variants such as BERT, GPT, and BEiT-3 on a wide range of tasks and modalities, including natural language processing, speech recognition, vision tasks, etc. Magneto significantly surpassed its baseline counterparts in the experiments. Moreover, it was shown to be more stable in terms of optimization, indicating its potential for effectively scaling up all manner of transformer models.The paper Foundation Transformers is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Stanford U & Google Brain’s Classifier-Free Guidance Model Diffusion Technique Reduces Sampling Steps by 256x,https://syncedreview.com/2022/10/17/stanford-u-google-brains-classifier-free-guidance-model-diffusion-technique-reduces-sampling-steps-by-256x/,2022-10-17,"
In the new paper On Distillation of Guided Diffusion Models, researchers from Google Brain and Stanford University propose a novel approach for distilling classifier-free guided diffusion models with high sampling efficiency. The resulting models achieve performance comparable to the original model but with sampling steps reduced by up to 256 times. 
","Denoising diffusion probabilistic models (DDPMs) with classifier-free guidance such as DALL·E 2, GLIDE, and Imagen have achieved state-of-the-art results in high-resolution image generation. The downside to such models is that their inference process requires evaluating both a class-conditional model and an unconditional model hundreds of times, rendering them prohibitively compute-expensive for many real-world applications. In the new paper On Distillation of Guided Diffusion Models, researchers from Google Brain and Stanford University propose a novel approach for distilling classifier-free guided diffusion models with high sampling efficiency. The resulting models achieve performance comparable to the original model but with sampling steps reduced by up to 256 times. The researchers’ distillation approach comprises two steps: Given a trained guided teacher model, a single student model first matches the combined output of the teacher’s two diffusion models, and this learned student model is then progressively distilled to a fewer-step model. The resulting single distilled model can handle a wide range of different guidance strengths and enable efficient tradeoffs between sample quality and diversity. The proposed sampling method employs a deterministic sampler and a novel stochastic sampling process. One deterministic sampling step is first applied with two times the original step length, and one stochastic step is then performed backward (i.e., perturb with noise) using the original step length. This approach was inspired by Karras et al.’s paperElucidating the Design Space of Diffusion-Based Generative Models, published earlier this year. In their empirical study, the team applied their method to classifier-free guidance DDPMs and performed image generation experiments on the ImageNet 64×64 and CIFAR-10 datasets. The results show that the proposed approach can achieve “visually decent” samples using as few as one step and obtain FID/IS (Frechet Inception Distance/Inception) scores comparable to that of the original baseline models while being up to 256 times faster to sample from. Overall, this work demonstrates the effectiveness of the proposed approach in addressing the high computational costs that have limited the deployment of denoising diffusion probabilistic models.The paper On Distillation of Guided Diffusion Models is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Beyond Bayes-Optimality: DeepMind & Stanford’s Meta-Learning Approach Builds Risk & Ambiguity Sensitive Agents,https://syncedreview.com/2022/10/13/beyond-bayes-optimality-deepmind-stanfords-meta-learning-approach-builds-risk-ambiguity-sensitive-agents/,2022-10-13,"
In the new paper Beyond Bayes-Optimality: Meta-Learning What You Know You Don’t Know, researchers from DeepMind and Stanford University use modified meta-training algorithms to build agents with risk- and ambiguity-sensitivity.
","Reasoning about uncertainty is one of the crucial components informing human intelligence and our very survival — it can encourage us to take risks that have a higher expected return but also discourage us from impulsively doing things that may lead to catastrophic consequences. Contemporary Bayes-optimal AI agents however are generally risk- and ambiguity-neutral, lacking this natural human capacity for advanced reasoning with regard to uncertainty. In the new paper Beyond Bayes-Optimality: Meta-Learning What You Know You Don’t Know, a research team from DeepMind and Stanford University employs modified meta-training algorithms to build agents with risk- and ambiguity-sensitivity, and empirically demonstrates the validity of such agents. The team first clarifies the distinction between Bayesian-optimal, risk-sensitive, and ambiguity-sensitive agents. Simple put, both risk and ambiguity belong to uncertainty: risk applies to scenarios where the output of an event is uncertain but can be roughly calculated using probability functions; while in ambiguity, the probability is unknown or cannot be reliably determined. Bayesian-optimal agents assign certainty-equivalents that are equal to the expected payoff, then choose the optimal action. Such agents are both risk-neutral (insensitive to the distribution over returns except for the target value) and ambiguity-neutral (acting as if the uncertainty were known). In their bid to create risk-sensitive agents, the team modifies the meta-training protocol by tweaking the distribution of observations to make these observations sensitive to the valuations of the agent. They leverage an ensemble of agents and a meta-policy to build a mechanism able to detect and use novelty. As such, the resulting agent can evolve to become uncertainty-seeking and risk-averting based on its experiences. In their empirical study, the team applied their proposed meta-training algorithms to agents in various decision-making experiments, with the results confirming that the agents can learn both risk- and ambiguity-sensitivity. Overall, this work demonstrates the possibility of building risk- and ambiguity-sensitive agents via the modification of meta-training algorithms. The team hopes their contributions can serve as the starting point in the development of data-dependent methods for the study and application of uncertainty-sensitivity in humans and machines. The paper Beyond Bayes-Optimality: Meta-Learning What You Know You Don’t Know is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",0.0
"‘Ask Me Anything’: Stanford U, Numbers Station & UW Madison’s Novel Prompting Strategy Enables LLMs With 30x Fewer Parameters to Outperform Few-Shot GPT3-175B",https://syncedreview.com/2022/10/12/ask-me-anything-stanford-u-numbers-station-uw-madisons-novel-prompting-strategy-enables-llms-with-30x-fewer-parameters-to-outperform-few-shot-gpt3-175b/,2022-10-12,"
In the new paper Ask Me Anything: A Simple Strategy for Prompting Language Models, a research team from Stanford University, Numbers Station, and the University of Wisconsin-Madison presents Ask Me Anything Prompting (AMA), a simple large language model prompting strategy that enables a 30x smaller language model to outperform few-shot GPT3-175B.
","Large language models (LLMs) have taken a step toward task-agnostic machine learning by leveraging user prompts — instructions written in natural language — to help them target specific tasks without additional training or fine-tuning. Prompts can significantly boost model performance, but designing the perfect prompt, aka “prompt engineering,” remains a time-consuming, hands-on process that often comes down to trial and error. In the new paper Ask Me Anything: A Simple Strategy for Prompting Language Models, a research team from Stanford University, Numbers Station, and the University of Wisconsin-Madison presents Ask Me Anything Prompting (AMA), a simple LLM prompting strategy that aggregates multiple “effective yet imperfect” prompts to enable a 30x smaller language model to outperform few-shot GPT3-175B. The team summarizes their main contributions as follows: The researchers first explore different prompt formats, concluding that open-ended question-answering (QA) prompts (e.g. “Who went to the park?”) outperform prompts that restrict the model to particular tokens (e.g. “John went to the park. Output True or False”). They recursively use the LLM to transform task inputs to the effective open-ended question-answering format noted above, collecting multiple candidate prompts with different accuracies and complex dependencies. Finally, they apply a weak supervision (WS) technique to aggregate the outputs and produce final predictions that demonstrably improve the prompting reliability and performance of off-the-shelf LLMs without further training. In their empirical study, the team evaluated AMA’s impact on the out-of-the-box few-shot performance of four open-source LLMs (EleutherAI, OPT, BLOOM, and T0) on seven tasks. In the experiments, AMA achieved an average improvement of 10.2 percent over the few-shot baselines; and also enabled a 30x smaller LLM to outperform few-shot GPT3-175B on 15 of 20 popular benchmarks. Overall, this work validates the effectiveness of the proposed AMA prompting strategy. The team believes AMA could also benefit LLM applications that involve private data or require operating over large amounts of data. The AMA code is available on the project GitHub. The paper Ask Me Anything: A Simple Strategy for Prompting Language Models is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Maximizing FLOPS Utilization: DeepMind & NYU Propose Efficiency Evaluations for Visual Pretraining Methods,https://syncedreview.com/2022/10/11/maximizing-flops-utilization-deepmind-nyu-propose-efficiency-evaluations-for-visual-pretraining-methods/,2022-10-11,"
In the new paper Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods, DeepMind and NYU Center for Neural Systems researchers introduce computational efficiency evaluation approaches designed to aid in the selection of optimal methods, datasets and models for pretraining visual tasks on a fixed FLOP budget.
","While self-supervised learning (SSL) has achieved impressive results in recent years thanks to complex data augmentation techniques and lengthy training schedules, these approaches also lead to extremely high computation costs. Given a fixed FLOPS budget, is it possible to identify the best datasets, models, and self-supervised training strategies for obtaining high accuracy on visual tasks? In the new paper Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods, a research team from DeepMind and the NYU Center for Neural Systems introduces evaluation approaches designed to measure the computational efficiency of various visual pretraining strategies across multiple datasets and model sizes and aid in the selection of optimal methods, datasets and models for pretraining visual tasks on a fixed FLOP budget. Previous studies on SSL have mainly focused on improving performance with little regard for the associated computational costs. This work takes the first steps toward identifying computationally optimal pretraining methods, datasets and models. The team analyzes four common self-supervised methods (BYOL, SimCLR, DINO, and MAE) and two supervised methods (CLIP and standard softmax classification). The methods’ per gradient-step FLOP costs are computed and used for comparisons across three axes: pretraining method, model size, and dataset. Downstream task performance is measured by finetuning the pretrained encoders on semantic segmentation tasks on the ADE20K dataset. Based on the evaluations, the team concludes that: 1) Self-supervised methods are generally less FLOP efficient and supervised representations dominate the efficiency Pareto-front; 2) For most methods, the small and large model curves intersect, indicating the point at which it is better to switch to larger model sizes for a given FLOP budget; 3) Dataset quality and curation level significantly affect model accuracy. The team sees their work as a first step towards more rigorously measuring the computational efficiency of contemporary supervised and self-supervised pretraining approaches in terms of pretraining method, dataset and model size. They hope their results will spark future research into visual SSL methods that learn more effectively and scalably on uncurated data. The paper Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
MIT’s DIFFDOCK Boosts the Molecular Docking Top-1 Success Rate from 23% to 38%,https://syncedreview.com/2022/10/06/mits-diffdock-boosts-the-molecular-docking-top-1-success-rate-from-23-to-38/,2022-10-06,"
MIT Researchers propose DIFFDOCK, a diffusion generative model that significantly improves the molecular docking top-1 prediction success rate, from state-of-the-art traditional docking approaches’ 23 percent to 38 percent.
","Market research firm Emersion Insights reports that global funding for AI-powered drug development topped US$4 billion in 2021, a 36 percent year-over-year increase, and is expected to continue its rapid growth. A critical component of computer-aided drug discovery is molecular docking, a task that predicts the binding structure of small molecule ligands to a protein. Although new approaches using deep learning models have increased the speed of such research compared to traditional search-based methods, they have not significantly boosted prediction accuracy. In the new paper DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking, researchers from the Massachusetts Institute of Technology’s Computer Science & Artificial Intelligence Laboratory propose DIFFDOCK, a diffusion generative model (DGM) that significantly improves the molecular docking top-1 prediction success rate, from state-of-the-art traditional docking approaches’ 23 percent to 38 percent. The team summarizes their main contributions as follows: The team defines molecular docking as a generative modelling problem, i.e., given a ligand and target protein structure, the goal is to learn a distribution over ligand poses. They introduce a diffusion process over the degrees of freedom involved in docking, covering: 1) the position of the ligand, 2) its orientation in the pocket, and 3) its torsion angles. DIFFDOCK samples poses during the diffusion process to iteratively transform an uninformed, noisy prior distribution over ligand poses into the output model distribution. The researchers also train a model to estimate the confidence level of the poses sampled from the DGM, to enable it to pick the most likely sample. In their empirical study, the team evaluated DIFFDOCK on molecular complexes from PDBBind benchmark and compared it with state-of-the-art search-based methods such as SMINA and GLIDE and recent deep learning methods EquiBind and TANKBind. DIFFDOCK excelled in the evaluations, surpassing all the baselines and achieving an impressive 38.2 percent top-1 success rate when sampling 40 poses while reaching much faster inference times. This work presents an exciting breakthrough in molecular docking. The team believes DIFFDOCK can help existing real-world pipelines and open new research avenues for downstream task integration and protein-protein and protein-nucleic acid docking. The paper DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",0.0
Google & TUAT’s WaveFit Neural Vocoder Achieves Inference Speeds 240x Faster Than WaveRNN,https://syncedreview.com/2022/10/05/google-tuats-wavefit-neural-vocoder-achieves-inference-speeds-240x-faster-than-wavernn/,2022-10-05,"
A neural vocoder is a neural network designed to generate speech waveforms given acoustic features — often used as aContinue Reading
","A neural vocoder is a neural network designed to generate speech waveforms given acoustic features — often used as a backbone module for speech recognition tasks such as text-to-speech (TTS), speech-to-speech translation (S2ST), etc. Current neural vocoders however can struggle to maintain high sound quality without incurring high computational costs. In the new paper WaveFit: An Iterative and Non-autoregressive Neural Vocoder based on Fixed-Point Iteration, a team from Google Research and the Tokyo University of Agriculture and Technology presents WaveFit, a fast and high-quality neural vocoder that achieves natural human speech with inference speeds that are 240 times faster than WaveRNN. The first breakthrough in neural vocoder development was the introduction of autoregressive (AR) models such as WaveNet (van den Oord et al., 2016), which revolutionized the quality of speech generation but proved inefficient as they required a huge number of sequential operations for signal generation. Non-AR models were subsequently proposed to speed up inference speeds, with denoising diffusion probabilistic models (DDPMs) and generative adversarial networks (GANs) among the most popular. Generating human-comparable speech waveforms in a few iterations however remains challenging, and typically involves an undesirable trade-off between sound quality and computational cost. The proposed WaveFit non-AR neural vocoder is inspired by the theory of fixed-point iteration and introduces a novel method for combining DDPMs and GANs to boost the performance of conventional non-AR models. WaveFit iteratively applies a DNN as a denoising mapping that eliminates noise components from an input signal. A GAN-based and a short-time Fourier transform (STFT)- based loss are combined to produce a loss function that is insensitive to imperceptible phase differences and to encourage the intermediate output signals to approach the target speech along with the iterations. In their empirical study, the team evaluated WaveFit on subjective listening experiments and compared it with baselines that included WaveRNN, DDPM-based models and GAN-based models. The results show that WaveFit with five iterations can generate synthetic speech with audio quality comparable to that of WaveRNN and natural human speech while achieving inference speeds more than 240 times faster than WaveRNN. Audio demos are available at google.github.io/df-conformer/wavefit/. The paper WaveFit: An Iterative and Non-autoregressive Neural Vocoder based on Fixed-Point Iteration is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
UNC Chapel Hill’s Textless Vision-Language Transformer: Comparable Performance to Text-Based Approaches but 28x Faster,https://syncedreview.com/2022/10/04/unc-chapel-hills-textless-vision-language-transformer-comparable-performance-to-text-based-approaches-but-28x-faster/,2022-10-04,"
 In the new paper TVLT: Textless Vision-Language Transformer, researchers from UNC Chapel Hill present the Textless Vision-Language Transformer (TVLT) for vision-and-language representation learning. TVLT uses only raw visual and audio inputs and performs comparably to its text-based counterparts but requires only 1/3 the parameters and achieves 28x faster inference speeds.
","Transformer architectures have achieved impressive performance in vision-language (VL) representation learning when trained on text-annotated images or videos. It remains challenging, however, for transformers to learn VL representations without relying on text, i.e. using only low-level visual and acoustic inputs. In the new paper TVLT: Textless Vision-Language Transformer, researchers from UNC Chapel Hill present the Textless Vision-Language Transformer (TVLT) for vision-and-language representation learning. TVLT uses only raw visual and audio inputs and performs comparably to its text-based counterparts but requires only 1/3 the parameters and achieves 28x faster inference speeds. The TVLT’s main architecture is a transformer comprising a 12-layer encoder and an 8-layer decoder. It takes its inputs as a list of embeddings obtained directly from perception-level video and audio and does not include any text-specific modules for automatic speech recognition (ASR) or tokenization. The input embeddings are a combination of 1) modality embedding, 2) temporal/spatial embeddings for video, 3) temporal/frequency embeddings for audio, and 4) vision/audio patch embeddings. The TVLT is pretrained with two objectives: vision-audio matching (VAM) and masked autoencoding (MAE). VAM is employed to learn the global cross-modal representations, and a linear layer with sigmoid activation is then applied to the encoder to obtain a matching probability. Finally, the binary cross-entropy loss is computed. MAE is used to improve unimodal representations by masking random patches of visual frames and the audio spectrogram and reconstructing missing inputs. The novel approach slices the audio and video parts of the encoder output and feeds them to the decoder independently instead of jointly, which saves compute costs and boosts finetuning performance. In their empirical study, the team compared TVLT with text-based counterparts on audio-to-video retrieval, video-based multimodal sentiment analysis, and visual question-answering benchmarks. In the experiments, TVLT achieved performance competitive with state-of-the-art audio-based vision-and-language models on visual question answering, image retrieval, video retrieval and multimodal sentiment analysis. Moreover, it required only 1/3 of the parameters, and its inference speed was 28x faster than the text-based methods. Overall, this paper showcases the powerful performance of TVLT and advances the possibility of learning compact and efficient visual-linguistic representations from low-level visual and audio signals without the need for traditional but computationally expensive text modelling. The code and checkpoints are available on the project’s GitHub. The paper TVLT: Textless Vision-Language Transformer is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Google & DeepMind Propose Geometric Complexity for DNN Analysis and Evaluation,https://syncedreview.com/2022/10/03/google-deepmind-propose-geometric-complexity-for-dnn-analysis-and-evaluation/,2022-10-03,"
In the new paper Why Neural Networks Find Simple Solutions: The Many Regularizers of Geometric Complexity, a research team from Google and DeepMind proposes Geometric Complexity (GC), a measure of deep neural network model complexity that serves as a useful tool for understanding the underlying mechanisms of complexity control.
","Bigger is not always better. While large language models and complex deep neural networks (DNNs) have resulted in huge performance gains across a variety of AI-related tasks, lighter and simpler models are often preferable in industrial applications. It is thus crucial for continued efficient DNN development and deployment that the machine learning research community improves its understanding of fundamental model complexity control methods such as regularization. In the new paper Why Neural Networks Find Simple Solutions: The Many Regularizers of Geometric Complexity, a research team from Google and DeepMind proposes Geometric Complexity (GC), a measure of DNN model complexity that serves as a useful tool for understanding the underlying mechanisms of complexity control. The team summarizes their main contributions as follows: Previous studies have proposed numerous complexity measures — naive parameter count, data-driven approaches, VC dimension and Rademacher complexity — but most of these fail to clarify the properties of regularizers and the connections between implicit and explicit regularizers. The proposed GC aims at solving these issues. The researchers provide a clear definition of GC and how it relates to important aspects of deep learning, including linear models, ReLU networks, Lipschitz smoothness, arc length, and harmonic maps. The paper explores the impacts of initialization, explicit regularization and implicit regularization on geometric complexity; examining different parameter initialization choices, L2 regularization, Lipschitz regularization via spectral norm regularization, noise regularization, flatness regularization, explicit GC regularization and Jacobian regularization. The researchers conclude that common training heuristics such as parameter norm regularization, spectral norm regularization, flatness regularization, implicit gradient regularization, noise regularization and the choice of parameter initialization can all play a part in reducing geometric complexity. Finally, the team demonstrates that GC can also capture double-descent behaviour in the test loss when a model’s parameter count increases. Overall, this paper validates GC as an effective tool for understanding DNN models and sheds light on how DNNs achieve low test errors with highly expressive models. The team hopes their work will encourage further research in this area and lead to a better understanding of current best practices and the discovery of new methods for efficient model training. The paper Why Neural Networks Find Simple Solutions: The Many Regularizers of Geometric Complexity has been accepted by the 36th Conference on Neural Information Processing Systems (NeurIPS 2022), which runs from November 28 to December 9 in New Orleans, USA; and is available on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Hindering Adversarial Attacks with Implicit Neural Representations,"[{'href': 'http://arxiv.org/abs/2210.13982v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.13982v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-10-22 13:10:24,,"Bootstrap Advantage Estimation for Policy Optimization in Reinforcement Learning Md Masudur Rahman, Yexiang Xue Department of Computer Science Purdue University, West Lafayette, Indiana, USA {rahman64, yexiang}@purdue.edu 2 2 0 2 t c O 3 1 ] G L . s c [ 1 v 2 1 3 7 0 . 0 1 2 2 : v i X r a Abstract—This paper proposes an advantage estimation ap- proach based on data augmentation for policy optimization. Unlike using data augmentation on the input to learn value and policy function as existing methods use, our method uses data augmentation to compute a bootstrap advantage estimation. This Bootstrap Advantage Estimation (BAE) is then used for learning and updating the gradient of policy and value function. To demonstrate the effectiveness of our approach, we conducted ex- periments on several environments. These environments are from three benchmarks: Procgen, Deepmind Control, and Pybullet, which include both image and vector-based observations; discrete and continuous action spaces. We observe that our method reduces the policy and the value loss better than the Generalized advantage estimation (GAE) method and eventually improves cumulative return. Furthermore, our method performs better than two recently proposed data augmentation techniques (RAD and DRAC). Overall, our method performs better empirically than baselines in sample efﬁciency and generalization, where the agent is tested in unseen environments. Index Terms—Deep Reinforcement Learning, Advantage Esti- mation, Generalization in Reinforcement Learning I. INTRODUCTION The policy gradient method directly involves learning policy function, which enjoys performance improvement in function approximation settings [1]. The policy gradient theorem gives a rather simple formulation of the gradient estimation, which gives an unbias estimation [2]. However, it requires the re- turn estimation of the entire trajectory, leading to very high variance. A commonly used technique to reduce variance is to use a baseline, which can help reduce variance without introducing bias. Several effective methods originated from this concept [3], [4]. An effective way is to use the value function as a baseline to indicate whether the action taken by the current policy is better than the average action taken in that state, which can be formulated as an advantage estimation. However, based on a single trajectory, the estimate can be local and have high variance. Thus we can add a truncated scenario where the value function can potentially give a global estimate. Combining these two is the Generalized advantage estimation [5] which shows strong empirical results [6]. However, due to procedural style content generation, the value estimation can be erroneous and give different advantage estimates even when the observation context changes. At the same time, the semantic meaning remains the same. As the procedural scenario can exist in a real-world scenario and might cause agent to perform sup-optimally [7], [8], thus this advantage estimation can be problematic in those scenarios, resulting in poor sample efﬁciency. This issue might exist partly due to difﬁculty in reducing policy estimation (policy loss) and value function estimation (value loss). the reward semantic remains the same, but This paper proposes Bootstrap Advantage Estimation (BAE), which calculates the advantage estimation by comput- ing advantage estimation on the original and its transformed observations. We assume the transformation to be a semantic invariant; the contextual information can be changed. For example, if the then changing the background of a game is not relevant, background color from red to green can be a semantic invariant transformation. The ultimate goal is to train an agent to be robust against any such background change, which performs well in the blue background in this example. The transformed observation can be of any form; we experimented with data augmentation-based observation transformation (e.g., random crop, amplitude scaling). The intuition is that taking advantage of estimate over augmented data forces the advantage estimate to consider the error over many variations of the observations. We demonstrate our BAE on the policy gradient method (i.e., PPO [3]) and show a comparison over GAE-based estimation. We observed that our method BAE achieved better sample ef- ﬁciency and zero-shot generalization in Procgen environments (starpilot and miner) with image-based observation. In recent times, data augmentation demonstrated an effective choice in improving sample efﬁciency in high-dimensional ob- servation space and improving generalization [9]–[11]. Though this process sometimes generates empirical success [10], such methods might lead to detrimental performance [11] as we observed in our experiments. To mitigate this issue, the DRAC [11] method suggests regularizing the policy network and value network by augmented observation and not using augmented data for policy training. In contrast, we propose a novel way to leverage data augmentation. Our method augmented observations for advantage estimation, one of the core components of many policy optimization algorithms (e.g., PPO). We conducted extensive experiments on six environ- ments consisting of image and vector-based observation; and discrete and continuous action spaces. Our method falls in the general model-free on-policy category, and we experimented with Proximal Policy Optimization (PPO) [3] in this paper. In particular, our experiments on Procgen Starpilot and Miner environments demonstrate that our method can be beneﬁcial in the zero-shot generalization setup compared to baseline GAE [5], and two data augmentation techniques: RAD [10], and DRAC [11]. We further evaluated our method on several robotic lo- comotion tasks with the high-dimensional observation from Deepmind Control Suite [12]: Quadruped Run, and Cartpole - Three Poles; and PyBullet [13]: Minitaur and HalfCheetah. In experiments, we observe that our method BAE performs better than baseline agents, including base PPO, RAD, and DRAC. Our method achieves a much lower loss for policy and value function estimation. Eventually, it performs better in sample efﬁciency and zero-shot generalization than baseline agents, including data augmentation. We observe that the base- line data augmentation methods (RAD and DRAC) sometimes worsen the base model performance. In contrast, our BAE method improves the performance in most tested environments and performs consistently with the base algorithm in other cases. These results show that our method BAE is more robust in performance compared to baseline data augmentation methods. The source code of our method is available at https://github. com/masud99r/bae. II. PRELIMINARIES AND PROBLEM SETTINGS Reinforcement Learning We assume the task/environment is a Markov Decision Process (MDP) which is denoted by M = (S, A, P, R, γ), where S is state space, A is that action space, P is the transition probability between states resulted by an action. In this setup, at every timestep t, the agent take an action at ∈ A in a state st ∈ S and the environment transition to next state st+1 ∈ S determined by the transition probability P (st+1|st, at). In a reinforcement learning framework, the goal of the agent is to learn a policy π ∈ Π by maximizing the expected return in an MDP, where Π is a set of all possible policies and π is a single policy which is a mapping from state to action. The policy which achieves the highest expected return is optimal π∗ ∈ Π. Policy Gradient Proximal Policy Optimization (PPO) [3], a type of policy gradient method which achieved tremendous success and is popularly used in many setups because of its effective learning and simple implementation. However, the choice of implementation details might impact the perfor- mance of such algorithms in signiﬁcant ways [6], [14], [15]. These implementation details consist of training individual components of the algorithms, such as learning value function, policy function, and advantage estimation. The following is the objective of the PPO [3]. Lπ = −Et[ πθ(at|st) πθold(at|st) At] (1) , where πθ(at|st) is the probability of choosing action at give state st at timestep t using the current policy parameterized by θ. On the other hand the πθold(at|st) refer to the probabilities using an old policy parameterized by previous parameter values θold. The advantage At is an estimation which is the advantage of taking action at at st. A popular and effective choice of estimating advantage using a value function is as follows (equation 2): At = −V (st) + rt + γrr+1 + ... + γT −t+1rT −1 + γT −tV (sT ). (2) Here V (s) is a value function that gives the average future return under the underlying policy. The ﬁrst term V (st) is the value prediction at timestep t, and the rest of the terms except the last term in the equation is the discounted Monte Carlo Estimation which can be computed for a given episode from t to T −1 (T > t). The last term V (sT ) is the value prediction at state sT . Thus overall, this At represents how much the current action at is doing compared to the current value prediction. The more the advantage of action, the more the policy should weigh that action. This is done by multiplying πθ(at|st) with At. The πθold(at|st) in Equation 2 introduced due to importance sampling which allows estimating the advantage from the old policy samples. For details discussion, we refer the reader to [3], [4]. Value Function Estimation An effective value function esti- mation [3] is to regress value prediction with an advantage- based return estimation. Here the Verror = |Vprediction − VReturn|, where Vprediction is the predicted value and the VReturn is value computed from the rewards R = (cid:80) t rt of sampled trajectories and advantage A. Thus, VReturn = A+R. Note that in this way, both the policy (in equation 1 and value function is dependent on the advantage estimation. Thus, an accurate advantage estimation should give us lower policy and value losses and thus a better performing policy. Generalization in RL Now we turn attention to the scenarios where different episode varies by confounding features in these confounders (also called the observation. Note that context)impact the reward in the environment; however, they might misguide the agent to think otherwise. Due to nature, the agent might overﬁt the confounding features and fails to generalize to slightly modiﬁed test environments [7], [8]. For a details overview of generalization in reinforcement learning, we refer the reader to the servery papers [16]. Data augmentation in various forms has been leveraged [10], idea is to transform the observation so [11]. The general that the observation’s semantic meaning remains the same but the contextual information changes. However, the context information is readily not available, and thus we need to impose various assumptions that certain transformations on the observation s(cid:48) = f (s) keep the reward semantic. For image- based observation, various image manipulation can be used, such as cropping, rotation, and color-jitter. Advantage Estimation An essential component in policy training is to estimate the advantage. Generalized Advantage Estimation (GAE) [5] is a useful way to compute advan- tage, which combines the value function estimate and the Monte Carlo method. However, the data augmentation-based regularization approaches [10], [11] do not handle the case of advantage estimation. The advantage is estimated using a single trajectory; thus, the computed advantage has a high variance due to the systematic noise in the advantage esti- mation. Given two similar states (observation), the advantage estimation should be the same. For example, an observation with the same semantic but a different background (red and blue) should have the same advantage if the background is not essential and thus confounded. III. BOOSTRAP ADVANTAGE ESTIMATION (BAE) We proposed to bootstrap the advantage estimation using observation transformation to mitigate the abovementioned issue. Formally, we generate m additional estimation with m transformation. For each such transformation i, we compute an estimate as in equation 3. A(k,i) t = −V (f (st+1, vi))+rt+γrr+1+...+γT −tV (f (st+k, vi)), (3) where v0 refer to no augmentation. Furthermore, ﬁnally, we take the average of all estimates as in equation 4 to estimate the ﬁnal advantage estimation for k-step return. A(k,b) t = 1 m + 1 (A(k,0) t + A(k,1) t + A(k,2) t + .. + A(k,m) t ) (4) Finally, we can achieve bootstrap advantage estimation of a trajectory of length T by combining several k-step returns using exponential-weighted average as in 5 following the GAE method [5]. ABAE(γ,λ) t +...+λT −1A(T,b) = (1−λ)(A(1,b) t +λA(2,b) ) (5) t t This ABAE(γ,λ) is used to compute the advantage at state st t of timestep t in an episode. Note that, our BAE differs from the GAE [5] in computing the k-step return as in equation 4. Algorithm 1 shows the details step of using our BAE with the PPO-based policy optimization method. In this paper, we Algorithm 1 BAE for Policy Optimization 1: Get transformation function f (s, v) with augmentation type v for each environment step do 2: Get PPO for policy optimization RL agent 3: for each iteration do 4: 5: 6: 7: 8: 9: at ∼ πθ(at|st) st+1 ∼ P (st+1|st, at) rt ∼ R(st, at) B ←− B ∪ {(st, at, rt, st+1)} 10: 11: 12: end for Transform all s ∈ B to get B(cid:48) using v augmentation with function f (s, v). Compute Bootstrap Advantage Estimate (BAE) from data B and B(cid:48) using equation 5. Perform PPO updates with BAE to optimize for Lπ as in equation 1 13: end for leverage PPO [3] as the base RL algorithm, which uses gen- eralized advantage estimation (GAE) as the default estimator. In contrast, our method BAE-PPO uses bootstrap advantage estimation (BAE) instead of GAE. The data augmentation baselines RAD and DRAC use base PPO with GAE advantage estimation. In the experiments we use m = 1 in equation 4. This means we use one data augmentation approach and combine it with original advantage estimation as in equation 5. Note that we do not apply any observation transformation in other parts of the agent objective, and thus equation 2 remains theoretically and practically sound. Furthermore, we empirically show how our Bootstrap Advantage Estimation leads to a smaller value, policy loss, and performance boost. Finally, we also compared the baseline RAD [10] and DRAC [11] and show that our method performs better in many setups. IV. EXPERIMENTS A. Setup Environments We experimented with image-based observa- tions with discrete action space and vector-based observations with continuous action space. Procgen We use Procgen [17]: Starpilot and Miner (Figure 1) which use image-based observation and procedural generation to produce challenging game logic that changes episode by episode. This benchmark allows for evaluating both sample efﬁciency and generalization capacity of RL agents. Each environment has around 100K levels. A subset of levels can be used to train the agent, and then the full distribution, that is, 100K levels, can be used to test the agent’s generalization capacity. For our experiment, we use the standard evaluation protocol from [17]; 200 levels of each environment are used for training in the difﬁculty level easy. All the environments have discrete action space of dimension 16. Intuitively, during training, the agent has access to a limited number of envi- ronment variability (e.g., 200 levels). The trained agent is tested on all the available variabilities, which consist of unseen scenarios. Thus, to master the game, the agent must focus on essential aspects of the state and ignore irrelevant information such as background color. Fig. 1. Procgen: Some snapshots of Starpilot and Miner. The environments are generated procedurally, which results in different observations (e.g., background) in each episode. two environments Deepmind Control We use from dm control [12]: Quadruped run, and Cartpole with three poles (Figure 2). The Quadruped Run has high-dimensional vector observation, and the task is to run as far as possible. On the other hand, the Cartpole variation consists of three procedurally generated poles. The complexity of these environ- ments is suitable for evaluating the data augmentation-based approaches. [Left] Deepmind Control: Some snapshots of Deepmind Control Fig. 2. tasks. [Right] Pybullet: Some snapshots of Pybullet Minitaur quadruped and HalfCheetah environments. These environments contain vector-based state space, and the action space is continuous. Fig. 3. Starpilot Env. Training time policy and value loss [lower is better]. Our method achieves lowest value and policy losses than the base algorithm (GAE-PPO) and data augmentation baselines (RAD and DRAC). Pybullet We use Pybullet [13]: Minitaur quadruped and HalfCheetah environments with vector-based observation. Each observation consists of raw sensory inputs. The Minitaur quadruped is a 4-legged robot, and the task is to travel as long as possible on ﬂat ground. Furthermore, the HalfCheetah is a two-legged robot that can control its movement in 2D, and the task is to travel as much distance as possible. The action spaces are continuous in these environments. Snippets of these environments are in Figure 2. Baselines All agents usage on-policy PPO [3] as the base policy. We compare our method with Generalized Advantage Estimation (GAE) [5] which is referred to as GAE-PPO in our experiments. GAE is shown to perform better compared to other advantage estimation techniques [6]. Moreover, we compare with the data augmentation-based approach uses data augmentation to transform the observation and then uses the transformed observation to train the base policy. In particular, we compare our method with existing baselines RAD [10] and DRAC [11]. RAD, referred to as RAD-PPO, proposes various data augmentation techniques to improve learning from pixel-based observation. DRAC, referred to as DRAC- PPO leverages the data augmentation to regularize the policy and value learning, showing improved performance in policy learning. In our method, we replaced the GAE estimation with our proposed Bootstrap Advantage Estimation (BAE), which is referred to as BAE-PPO. the agents, including our BAE-PPO and baseline, using the implementation available in [15]. In a PPO-based scenario, many factors have been identiﬁed as key in implementing algorithms that impact the performance [6], [14]. Thus, we use the same implementation logic for all the baselines and our method for a fair comparison. Data augmentation We evaluate Cutout Color data augmen- tation for image-based observation, which performs best in our setup compared to another popularly used Random Crop. Thus, we report Cutout Color data augmentation results for RAD, DRAC, and our BAE. We use the implementation avail- able in RAD [10] for data augmentation. For the vector-based observation robotic task, we use a random amplitude scale proposed in RAD [10]. This method multiply the observation with a number generated randomly between a range α to β. We used best performing range α = 0.8 to β = 1.4 for our We build all BAE method, and a range α = 0.6 to β = 1.2 for RAD, and DRAC (suggested in RAD [10]). Implementation and Hyper-parameters For the Procgen Starpilot and Miner, we report mean and standard deviation across 3 seeds run following the setup of Procgen paper [17]. We used an Nvidia A100 GPU to run agents with the IMPALA CNN model [18] on the image observation-based Procgen environments. We use neural networks to represent policy and value functions for vector-based observations. For Deepmind control environments, we report results with 10 random seed runs, and for Pybullet environments, we report results over 5 seeds. For all experiments, we keep the common hyperparam- eters the same for a fair comparison. The implementation and hyperparameters are based on [15], [19]. For all results, we report the mean (showed in solid line) and standard deviation (showed in shaded areas) across runs. B. Results The PPO-based agent’s objective consists of value loss and policy loss. The objective is to reduce them and potentially improve the expected return. We show that our method BAE reduces the losses and thus learns a better value function and policy than the baselines. We then show how our method performs in the expected return. Procgen Results Figure 3 shows value and policy loss during policy training on Procgen Starpilot environments. As the training progresses, the loss of our method BAE reduces drastically compared to the baselines. These results show the sign of the effectiveness of our method in reducing the agent’s losses. Note that the advantage estimation is used to train both the value function and the policy; thus, better estimation of advantages should generally give better value and policy. In this sense, our method shows empirical evidence that it can help better advantage estimation. We observe that in the Starpilot environment, the success in policy and value loss translates to the ﬁnal return. In Figure 4 we see that our method (BAE-PPO) shows improved sample efﬁciency (Train Return) and generalization capacity compared to the baseline GAE-PPO, RAD-PPO, and DRAC-PPO. Note that the RAD-PPO performance worsens the perfor- mance of the base GAE-PPO algorithms. This result is con- sistent with the ﬁndings in [11], and it shows that naive data Starpilot Env. Sample efﬁciency performance measured in train Fig. 4. time return. We see our method BAE-PPO achieves higher returns where DRAC does not improve the base agent’s (GAE-PPO) performance. RAD slightly worsens the performance of the base agent. [Right] Generalization performance measured in test time return. We see a similar trend and observe that our method performs the best. [Left] Performance on Quadruped run environments. Our method Fig. 7. BAE-PPO shows higher mean returns compared to all other agents. Data augmentation baseline RAD and DRAC worsen the performance of the base agent (GAE-PPO). [Right] Our method BAE consistently achieves a higher mean where DRAC fails to improve upon the base agent, and RAD worsens the base performance. Fig. 5. Miner Env. Training time policy and value loss [lower is better]. The value loss of BAE eventually become lower. Losses of RAD increases compared to base PPO. Fig. 6. Miner Env. [Left] Sample efﬁciency performance measured in train time return and [Right] Generalization performance measured in test time return. Overall, we see our method BAE-PPO shows consistence improvement over baselines. augmentation can be detrimental to performance. Furthermore, we observe a similar trend for the value and policy loss results. We observe a similar performance trend in the Procgen Miner environment. In Figure 5, we see that in our method, BAE shows a smaller value loss eventually despite being higher at the beginning compared to the baselines. GAE and DRAC show slightly lower values in the policy loss plot than BAE. However, both show smaller policy losses in general. In performance measure (Figure 6), we see that our method shows better performance throughout the training than the baseline in both sample efﬁciency and generalization. The performance difference is consistent across timestep. Similar to Starpilot, RAD also performs worse compared to base GAE- PPO. We further evaluate our method on vector-based state space and continuous robotic tasks: Deepmind control and Pybullet. Note that the setup of these benchmarks are different from Procgen’s train-test setup, and here we evaluate how our agents can perform in high-dimensional states and procedurally gen- erated task. Thus we can only report returns during training. Deepmind Control Results Figure 7 shows performance compariosn on Quadruped run and Cartpole - Three Poles environments. We observe that our method BAE achieves the best performance in both environments. On the other hand, the baseline RAD severely worsens the base agent’s (GAE-PPO) performance in both environments. Another baseline, DRAC, worsens the base agents’ performance in Quadruped Run and fails to improve performance in the Cartpole Three Poles envi- ronment. These results show that properly using the augmenta- tion in policy learning can lead to strong performance. In this case, all data augmentation agents (RAD, DRAC, and BAE) use the same random amplitude modulation augmentation. However, using this augmentation in advantage computation, our method BAE shows a substantial performance boost. On the other hand, other baselines, RAD and DRAC, worsen the performance (Quadruped Run). Pybullet Results In Figure 8, for the HalfCheetah environ- ment, we see that our method BAE performs better than base agent GAE-PPO and other data augmentation baselines RAD and DRAC. On the other hand, in Minitaur, the data augmentation baseline DRAC and RAD worsen the base agent’s (GAE-PPO) performance where BAE can maintain the base performance. Overall, these results show the robustness of our method in performance compared to baseline data augmentation methods. Therefore, the proposed augmented observation is expected not to worsen the base performance. However, in our exper- iments, we observe that the RAD and DRAC barely match the base agent’s (GAE-PPO) results and sometimes worsen the performance. These variabilities in performance hinder the widespread adaptation of these methods. In contrast, our method BAE shows a consistent performance across various tasks without reducing the base agent’s performance. [2] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement learning,” Machine learning, vol. 8, no. 3, pp. 229–256, 1992. [3] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox- imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347, 2017. [4] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region policy optimization,” in International conference on machine learning. PMLR, 2015, pp. 1889–1897. [5] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High- dimensional continuous control using generalized advantage estimation,” in Proceedings of the International Conference on Learning Represen- tations (ICLR), 2016. [6] M. Andrychowicz, A. Raichuk, P. Sta´nczyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot, M. Geist, O. Pietquin, M. Michalski, S. Gelly, and O. Bachem, “What matters for on-policy deep actor-critic methods? a large-scale study,” in International Conference on Learning Represen- tations, 2021. [7] X. Song, Y. Jiang, S. Tu, Y. Du, and B. Neyshabur, “Observational overﬁtting in reinforcement learning,” in International Conference on Learning Representations, 2020. [8] C. Zhang, O. Vinyals, R. Munos, and S. Bengio, “A study on overﬁtting in deep reinforcement learning,” arXiv preprint arXiv:1804.06893, 2018. [9] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman, “Quantifying generalization in reinforcement learning,” in International Conference on Machine Learning. PMLR, 2019, pp. 1282–1289. [10] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas, “Reinforcement learning with augmented data,” in Advances in neural information processing systems, 2020. [11] R. Raileanu, M. Goldstein, D. Yarats, I. Kostrikov, and R. Fergus, “Automatic data augmentation for generalization in deep reinforcement learning,” arXiv preprint arXiv:2006.12862, 2020. [12] S. Tunyasuvunakool, A. Muldal, Y. Doron, S. Liu, S. Bohez, J. Merel, T. Erez, T. Lillicrap, N. Heess, and Y. Tassa, “dm control: Software and tasks for continuous control,” Software Impacts, vol. 6, p. 100022, 2020. [13] E. Coumans and Y. Bai, “Pybullet, a python module for physics simulation for games, robotics and machine learning,” http://pybullet.org, 2016–2021. [14] L. Engstrom, A. Ilyas, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph, and A. Madry, “Implementation matters in deep rl: A case study on ppo and trpo,” in International Conference on Learning Representations, 2020. [15] S. Huang, R. F. J. Dossa, A. Rafﬁn, A. Kanervisto, and W. Wang, “The 37 implementation details of proximal policy optimization,” in ICLR Blog Track, 2022. [Online]. Available: https://iclr-blog-track.github.io/ 2022/03/25/ppo-implementation-details/ [16] R. Kirk, A. Zhang, E. Grefenstette, and T. Rockt¨aschel, “A sur- vey of generalisation in deep reinforcement learning,” arXiv preprint arXiv:2111.09794, 2021. [17] K. Cobbe, C. Hesse, J. Hilton, and J. Schulman, “Leveraging procedu- ral generation to benchmark reinforcement learning,” in International conference on machine learning. PMLR, 2020, pp. 2048–2056. [18] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning et al., “Impala: Scalable dis- tributed deep-rl with importance weighted actor-learner architectures,” arXiv preprint arXiv:1802.01561, 2018. [19] S. Huang, R. F. J. Dossa, C. Ye, and J. Braga, “Cleanrl: High-quality single-ﬁle implementations of deep reinforcement learning algorithms,” 2021. [20] J. Peters and S. Schaal, “Reinforcement learning of motor skills with policy gradients,” Neural networks, vol. 21, no. 4, pp. 682–697, 2008. [21] C. Wu, A. Rajeswaran, Y. Duan, V. Kumar, A. M. Bayen, S. Kakade, I. Mordatch, and P. Abbeel, “Variance reduction for policy gradient with action-dependent factorized baselines,” in International Conference on Learning Representations, 2018. [22] M. Igl, K. Ciosek, Y. Li, S. Tschiatschek, C. Zhang, S. Devlin, and K. Hofmann, “Generalization in reinforcement learning with selective noise injection and information bottleneck,” in Advances in neural information processing systems, 2019, pp. 13 978–13 990. [23] M. M. Rahman and Y. Xue, “Bootstrap state representation using style transfer for better generalization in deep reinforcement learning,” in European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2022), 2022. Performance on Minitaur [Left] and HalfCheetah [Right] environ- Fig. 8. ments. Our method shows better or similar performance compared to the base agent (GAE-PPO), where the data augmentation baselines sometimes worsen the base performance. V. RELATED WORK Advantage Estimation. The baseline has been leveraged to reduce variance in policy gradient update [2], [20], [21]. Furthermore, effective modiﬁcation of such methods is the use of advantage estimation. This method is commonly used in policy optimization and enjoys strong empirical success [6], especially the Generalized Advantage Estimation (GAE) [5] method. In contrast to GAE, our method BAE leverages data augmentation and incorporates advantage computation across various semantically similar states. Empirically we observe that our method of computing advantage can be beneﬁcial over GAE, especially in high-dimensional and procedural generated environments. Data augmentation. Data augmentation has been demon- strated to be an effective and efﬁcient approaches to improve performance [9]–[11]. Other methods proposed to improve generalization which includes regularization [22], and style- transfer [23]. Depending on how the augmented observation is used, the method can be different; for example, RAD [10] and DRAC [11]. In contrast to these methods, our method incorporates data augmentation into advantage estimation, which shows better empirical performance compared to these methods (RAD and DRAC). VI. CONCLUSION In this paper, we propose a data augmentation-based advan- tage estimation method for policy optimization. Our Bootstrap advantage estimation (BAE) method replaces the GAE method in policy gradient-based algorithms. We demonstrated the effectiveness of our method on PPO algorithms. Furthermore, we evaluated our methods on both image-based observation space with discrete action space and vector-based observation with continuous action space (Procgen, Deepmind Control, and Pybullet). Our BAE method showed better performance in various environment setups than GAE. Furthermore, our method performs better than two existing data augmentation techniques (RAD and DRAC). REFERENCES [1] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradi- ent methods for reinforcement learning with function approximation,” Advances in neural information processing systems, vol. 12, 1999.",1.0
Planning for Sample Efficient Imitation Learning,"[{'href': 'http://arxiv.org/abs/2210.09598v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.09598v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-10-18 05:19:26,,"2 2 0 2 t c O 3 2 ] h p - t n a u q [ 1 v 3 5 7 2 1 . 0 1 2 2 : v i X r a Google’s 2019 “Quantum Supremacy” Claims: Data, Documentation, and Discussion Gil Kalai, Yosef Rinott & Tomer Shoham October 25, 2022 Abstract In October 2019, Nature published a paper [3] describing an exper- imental work that took place at Google. The paper claims to demon- strate quantum (computational) supremacy on a 53-qubit quantum computer. Since September 2019 the authors have been involved in a long-term project to study various statistical aspects of the Google experiment. In particular, we have been trying to gather the rele- vant data and information, to reconstruct and verify those parts of the Google 2019 supremacy experiments that are based on classical computations (unless they require too heavy computation), and to put the data under statistical analysis. We have now (August 2022) concluded the part relating to the gathering of data and information needed for our study of the 2019 Google experiment, and this docu- ment describes the available data and information for the Google 2019 experiment and some of our results and plans. 1 Introduction The 2019 paper “Quantum supremacy using a programmable superconduct- ing processor” [3] claimed that Google’s Sycamore processor performed a certain computation in about 200 seconds, while a state-of-the-art classical supercomputer would take, according to Google’s team estimates, approxi- mately 10,000 years to perform the same computation. Google’s Sycamore quantum computer performed a sampling task ; that is, it generates ran- dom bitstrings, of length 53, with considerable noise, from a certain discrete 1 probability distribution supported on all such 253 bitstrings. The speciﬁc Google’s sampling task is referred to as random circuit sampling (RCS, for short). Google’s announcement of quantum supremacy was compared by [1]) to landmark technological achievements such various writers (See, e.g. as the Wright brothers’ invention of a motor-operated airplane, the Sputnik, and the landing on the moon, as well as to landmark scientiﬁc achievement such as Fermi’s demonstration of a nuclear chain reaction, the discovery of the Higgs boson, and the LIGO detection of gravitational waves. In 2020 a team from the University of Science and Technology of China (USTC) claimed [30] that the sampling task computed by their photonic Jiuzhang quantum computer, would take 2.5 billion years to perform on a classical supercomputer. USTC’s quantum computers also required about 200 seconds for their tasks. This task is referred to as Gaussian boson sam- In 2021, another team from USTC repeated the pling (GBS, for short). Google 2019 RCS experiment with a superconducting processor Zuchongzhi of 60 qubits and depth 24 [29, 32], and claimed to achieve an even stronger form of quantum advantage compared to the Google 2019 experiment. The Google experiment represented a very large leap in various aspects of the human ability to control noisy quantum systems. For example the Google AI team previously reported an experiment with nine qubits [17]. This leap is especially impressive in terms of the dimensions of the Hilbert space representing a state of the computer. (From dimension 100–500 in [17] to dimension 1016 in [3].) As for Google’s “quantum supremacy” claims, progress by several groups [19, 18, 31, 11, 12, 20, 8] (and more) exhibits classical algorithms which are ten orders of magnitude faster than those used in the Google paper. This was achieved, for example, by Pan, Chen, and Zhang [18] in 2021. Our study concentrates on other aspects of the Google experiment. As we already mentioned, the Google announcement was regarded as a major scientiﬁc and technological event. On its own it gave some evi- dence that the “strong Church–Turing thesis” had been violated and it was described as an ironclad refutation of claims by some scientists (including Kalai) that quantum computation is not possible. Of no less importance is that quantum supremacy was considered as a major intermediate step towards exhibiting experimentally quantum error-correction codes needed for building larger quantum computers. The announcement of quantum supremacy stirred a great deal of enthusiasm among scientists and in the general public, and garnered signiﬁcant media attention. It had substantial 2 Figure 1: The price of bitcoin in USD in a period of 4 weeks around 9/23/2019. Source: CoinDesk impact; for example, following the media attention surrounding the leaking of the supremacy claims around September 22, 2019, the value of bitcoin (and other digital currencies) sharply dropped by more than 10% and it is a reasonable possibility that given the potential eﬀects of quantum computers on the safety of digital currencies, the quantum supremacy claims caused this drop. Putting the Google supremacy claims under scrutiny The Google paper appeared in October 2019 and a month earlier it was brieﬂy posted on a NASA server and became publicly available. Since the announcement of the quantum supremacy claim, the ﬁrst-named author (and other researchers) raised various concerns about some aspects of the claims. A few months later the authors initiated what has become a long-term project to study various statistical aspects of the Google experiment. In particular, we have been trying to gather the relevant data and information and to reconstruct and verify those parts of the Google 2019 supremacy experiments that are based on classical computations. (Unless they require too heavy 3 computation; we carried some heavy computation on the cloud for which we put a cap of 2000 dollars on our spending.) We also performed several “sanity tests” of the experiment. The structure of this paper In Section 2 we provide a brief background on the Google 2019 experiment, describe the various types of circuits used in the experiment and the chronol- ogy of the various Sycamore experiments as reported by the Google team. In Section 3 we describe the nature of the calibration process. In Section 4 we describe the requested data from the Google team, the data that was provided between October, 2019 and June 2022, and some other details related to the Google experiment and other NISQ supremacy experiments. In Section 5 we raise two proposals for future experiments. In Section 6 we discuss what we regard as the main questions in the evaluation of the Google 2019 experiment and list some conﬁrmations, refutations, concerns, and weaknesses, and in Section 7 we brieﬂy discuss where we are now in our study. 2 Google’s 2019 quantum supremacy claim 2.1 A brief background In this draft we will assume knowledge of the Google 2019 experiment, Google’s noise model, Google’s FXEB linear cross entropy ﬁdelity estima- tor, and Google’s formula (77) in [4] for predicting the ﬁdelity of a circuit from the ﬁdelity of its components. We will give here a brief summary of these topics. The Google 2019 experiment is based on the building of a quantum com- puter (circuit) with n superconducting qubits, that perform m rounds of computation. The computation is carried out by a 1-qubit and 2-qubit gates. At the end of the computation the qubits are measured, leading to a string of zeroes and ones of length n. The ultimate experiment was for n = 53 and m = 20. It involved 1113 1-qubit gates and 430 2-qubit gates. For that experiment the Google team produced a sample of three million 0-1 vectors of length 53. Every circuit C with n qubits describes a probability distribution PC(x) (In fact, it describes a 2n-dimensional vector for 0-1 vectors of length n. 4 of complex amplitudes; to every 0-1 vector x, there is an associated ampli- tude z(x) and PC(x) = |z(x)|2.) The quantum computer enables to sample according to the probability distribution PC(x) with a considerable amount of noise. When n and m are not too large classical simulations enable to compute the amplitudes themselves (and hence the probabilities PC(x)). Google’s supremacy claims are based on the fact that these classical simula- tions quickly become infeasible as n and m grow. The Google basic noise model for the noisy samples their Sycamore device actually produces is NC(x) = φPC + (1 − φ)2−n, (1) where φ is the ﬁdelity, a parameter that roughly describes the quality of the sample. (The ﬁdelity has a precise meaning in terms of the actual noisy quantum process carried out by the Google Sycamore device.) Based on their noise model (and the fact that the distribution PC is an instance of a Porter–Thomas distribution) the Google paper describes a statistic called the linear cross entropy estimator (and denoted by FXEB.) Once the quantum computer produces a sequence ˜x of N samples ˜x = (˜x(1), ˜x(2), . . . , ˜x(N )), the following “linear cross entropy” estimator FXEB for the ﬁdelity is computed FXEB(˜x) = 1 N N (cid:88) i=1 2nPC(˜x(i)) − 1. (2) Computing FXEB requires knowledge of PC(x) for sampled bitstrings. The Google supremacy claim is based also on the following a priori pre- diction for the ﬁdelity of a circuit based on the probabilities of error for the individual components. ˆφ = (cid:89) g∈G1 (1 − eg) (1 − eg) (cid:89) g∈G2 (cid:89) q∈Q (1 − eq). (3) Here G1 is the set of 1-gates (gates operating on a single qubit), G2 is the set of 2-gates (gates operating on two qubits), and Q is the set of qubits. For a gate g, the term eg in the formula refers to the probability of an error (1 minus the ﬁdelity) of the individual gate g. For a qubit q, eq is the probability of a read-out error when we measure the qubit q. The Google supremacy paper [3] made two crucial claims regarding the ultimate 53-qubit samples. 5 A) The ﬁdelity φ of their sample is above 1/1000. B) Producing a sample with similar ﬁdelity would require 10,000 years on a supercomputer. For claim A) regarding the value of φ, the argument relies on an extrapo- lation argument that has two ingredients. One ingredient is a few hundred experiments in the classically tractable regime: the regime where the prob- ability distribution PC can be computed by a classical computer and the performance of the quantum computer can be tested directly. The other in- gredient is the theoretical formula (3) for predicting the ﬁdelity. According to the paper, the ﬁdelity of entire circuits closely agrees with the prediction of formula (3) (Formula (77) in [4]) with a deviation below 10–20 percent. There are around 200 reported experiments in the classically tractable regime including ones carried out on simpliﬁed circuits (which are easier to simu- late on classical computers). These experiments support the claim that the prediction given by Formula (77) for the ﬁdelity is indeed very robust and applies to the 53-qubit circuit in the supremacy regime. For claim B) regarding the classical diﬃculty, the Google team mainly re- lies on extrapolation from the running time of a speciﬁc algorithm they used. They also rely on the computational complexity support for the assertion that the task at hand is asymptotically diﬃcult. Remark: The Google team proposed a pretty good approximation for Formula (3) based on averaged ﬁdelities. ˆˆφ = (1 − 0.0016)|G1|(1 − 0.0062)|G2|(1 − 0.038)n. (4) 2.2 The types of circuits of the Google experiment The circuits used in the 2019 Google supremacy experiment had the fol- lowing structure. The qubits were arranged on a planar grid, so a single qubit was identiﬁed via two coordinates, like qubit (3, 3). The circuits had two types of layers: one type of layer consists of 2-gates acting on pairs of (neighboring) qubits. After each such layer of two gates there was another layer of randomly chosen 1-gates acting on every qubit. The layers of 1-gates consist of the ”programmable” ingredient in the experiment and they change from circuit to circuit. The layers of 2-gates are ﬁxed throughout the ex- periment according to a certain pattern. The pattern EFGH was used in 6 all experiments conducted between February to May 2019 and a new pat- tern ABCDCDAB was used in June 2019 to produce the samples of the “supremacy” circuit, namely circuits which would require a huge classical computation. Each letter like E corresponds to a ﬁxed set of 2-gates act- ing in parallel on the qubits (with the convention that for circuits with a smaller number of qubits we regard only 2-gates that involve the qubits in the circuit). So Pattern EFGH means that we ﬁrst apply a layer of random 1-gates on all qubits, then apply 2-gates according to E, next apply another layer of random 1-gates on all qubits, then apply 2-gates according to F and continue (in a periodic manner). The new pattern ABCDCDAB is based on new types of layers for the 2-gates and on a period of length eight. The depth m of a circuit refers to the number of layers of 2-qubits. So, for example, the 2-gates layers of a circuit with pattern EFGH of depth m = 14 are E, F, G, H, E, F, G, H, E, F, G, H, E, F. The six types of circuits The Google 2019 experiment relies on six types of circuits. The ﬁrst three types are: a) The full circuits of pattern EFGH, b) The elided circuits of pattern EFGH, c) The patch circuits of pattern EFGH. Types b) and c) are simpliﬁed forms of type a) for which there are quick algo- rithms to compute the amplitudes for all values of n up to 53. In particular, patch circuits consist of two separate circuits on two disjoint sets of qubits. The Google paper [3] is based on running the quantum computer on a few hundred circuits of type a) b) and c) and computing the ﬁdelity estimator based on the amplitudes. There was also a preliminary stage of calibration that, based on exper- iments on 1-qubit and 2-qubit circuits, determined the precise adjustments for the experimental 2-gates. Those adjustments are the same for all circuits of type a), b) and c). See Section 3. As far as we understand, the initial plan of the Google team to demon- strate quantum supremacy was to compute the empirical ﬁdelities for circuits of type b) and c) for a number of qubits between 12 and 53 and for circuits 7 of type a) for a number of qubits up to n = 43 and to use this information to estimate the ﬁdelity for circuits of type a) with 53 qubits and depths (m) between 12 and 20. However, in May 2019, the Google team discovered an eﬃcient classical algorithm for the circuits of type a). (Because of this discovery, the full cir- cuits with pattern EFGH are also referred to as “veriﬁable full circuits.”) The new algorithm discovered by the Google team is related to tensor net- works methods which later led to important discoveries in this direction.1 Using this algorithm, the Google team computed the amplitudes for the ex- perimental bitstrings for type a) circuits with n = 53 m = 14. Subsequently, for the purpose of demonstrating quantum supremacy, the Google team moved in May 2019 to a diﬀerent architecture based on a new pattern ABCDCDAB which is harder to simulate by classical computers. This led to the following three new types of circuits. d) The full circuits with pattern ABCDCDAB. (These circuits are also referred to as “supremacy full circuits.”) e) The elided circuits of pattern ABCDCDAB, f) The patch circuits of pattern ABCDCDAB. For this architecture the Google team produced samples only with n = 53 and depths 12, 14, 16, 18 and 20. The simulators available to the Google team were not powerful enough for computing the amplitudes and for checking the ﬁdelity for circuits of type (d). The circuits with pattern ABCDCDAB required to run the calibration process again on 1-qubit and 2-qubit circuits based on the new architecture and this has led to new adjustments for the 2-gates. Circuits of the new pattern ABCDCDAB (and the calibration process) were not tested for less than 53 qubits. 2.3 A chronology of the Google Sycamore experiments Prior to the development of the Sycamore quantum computer, in 2018, the Google team had attempted to develop a 72-qubit chip called “Bristlecone,” 1The Google team pointed out that they also implemented a related tensor network method in 2017 [6]. 8 but due to diﬃculties the team later proceeded to the development of the Sycamore 54-qubit computer (with 53 eﬀective qubits). Here is a list of experiments for the Sycamore quantum computer in- cluding early experiments based on earlier calibration methods. For every experiment fresh new random circuits were generated. (Namely, the improve- ments in the calibration procedures in a certain experiment were not based on improving the ﬁdelity for circuits that were used in an earlier experiment.) Each experiment took 1-2 days to perform. Each of the ﬁrst six ex- periments is based on an improved method for calibration. There were six experiments for circuits of pattern EFGH and one (the last) experiment is based on the new architecture with pattern ABCDCDAB. 2.3.1 Earlier experiments 1. Date: 02/08/2019 Full circuits with pattern EFGH; numbers of qubits: 2, 3, 4, . . . , 49 2. Date: 02/13/2019 Full circuits with pattern EFGH; numbers of qubits: 2, 3, 4, . . . , 49 3. Date: 03/14/2019 Full circuits with pattern EFGH; numbers of qubits: 2, 3, 4, . . . , 51 4. Date: 03/22/2019 Full circuits with pattern EFGH; numbers of qubits: 33, 34, . . . , 51 5. Date: 03/27/2019 Full circuits with pattern EFGH; numbers of qubits: 10, 20, 33, 34, 35, 36, 37, 38, 51 2.3.2 The experiments on which the Google paper is based 6. Date: 04/22/2019 Patch, elided, and full circuits with pattern EFGH; Numbers of qubits: 12, 14, 16, . . . , 36, 38, 39, 40, . . . , 50, 51, depth m = 14 A few days later the Sycamore quantum computer produced the 53- qubit samples for patch, elided, and full circuits with the EFGH pat- tern also with depth m = 14. 9 7. Date: 06/13/2019 Numbers of qubits: 53, depth m = 12, 14, 16, 18, 20 Type of circuits: patch, elided, full with pattern ABCDCDAB. 3 The nature of the calibration process The Sycamore quantum computer has systematic deviations from the ideal circuit it describes, and the calibration process accounts for small systematic errors in the experimental circuits compared to the random circuits they represent. The calibration process is crucial since the combined eﬀect of such systematic errors can slash the ﬁdelity to zero and it is important to account for them. By the calibration process we refer in this paper to a method which, based on multiple runs of 1-qubit and 2-qubit quantum circuits, adjusted the deﬁnition of the 2-gates of the experimental circuits so that certain systematic forms of noise will be greatly reduced. This calibration adjustment was carried out simultaneously for all the experiments for patch, elided, and veriﬁable full circuits. It is easy to identify from the Python program how the random circuits (We could also use the QSIM ﬁles for were modiﬁed by the calibration. that purpose.) For example, consider a single 2-gate acting on two qubits A = (3, 3) and B = (3, 4). In the ideal description of the circuit, this 2-gate is described in the Python program for the circuit as follows: cirq.Moment( cirq.FSimGate(theta=1.57079632679, phi=0.52359877559). on( cirq.GridQubit(3, 3), cirq.GridQubit(3, 4)), In this 2-gate (that is sometimes referred to as the “standard 2-gate”) θ = π/2, and φ = π/6. We will now show the part of the program that describes 2-gates modiﬁcations for this speciﬁc gate. The calibration consists of adding to the description of the circuits, two ﬁxed rotations on each of the 2 qubits involved in this particular 2-gate. The two added single qubit rotations are cirq.Rz (np.pi * 2.5333591271878086). on (cirq.GridQubit (3, 3)), cirq.Rz (np.pi * -2.4748096263683066). on (cirq.GridQubit (3, 4)), 10 and these two added single qubit rotations are the same for every appearance of the 2-gate involving A and B. The values that appear here in the single qubit rotations, as well as the adjustments for the deﬁnition of 2-gates, were computed based on many experiments for 1-qubit and 2-qubit circuits for these two qubits (3,3) and (3,4). (We neither have the precise algorithm for this computation nor the data with the outcomes of the 1-qubit and 2-qubit experiments.) In addition, the 2-gate acting on qubits A and B is modiﬁed to cirq.Moment( cirq.FSimGate(theta=1.2947043217999283, phi=0.4859467238431821). on( cirq.GridQubit(3, 3), cirq.GridQubit(3, 4)), The calibrated values θ = 1.2947043217999283 and φ = 0.4859467238431821 are the same for all 2-gates acting on these two qubits in all circuits. (Some- times the calibrated 2-gate is referred to as “native” 2-gate.) These values for θ and φ replace the values for the “standard” gate in the original random circuit θ = π/2, and φ = π/6. Remarks: 1. We note that the adjustments for 2-gate involving qubits A and B depend (only) on multiple runs of 1-qubit and 2-qubit quantum circuits in- volving these two qubits. 2. We also note that there might have been additional actions to account for the fact that certain components of the Sycamore chip degrade over time. 3. In the December 2020 Internet discussion [24] Sergio Boixo from the Google team referred to Figure S30 in [4] and asserted that in the speciﬁc case of the 2019 experiment using “Sycamore” standard gates instead of native gates halves the ﬁdelity. However, this claim referred only to the two-gate adjustments and not to the one-gate adjustments, and moving to the original circuit with standard gates slashes the ﬁdelity to zero. (The eﬀect of the two- gate adjustments represents the worst-case situation for n = 53, m = 20, and it is smaller for smaller circuits.) Sergio Boixo later clariﬁed that since 2020 (for later experiments but not for the 2019 supremacy experiment) the Google team developed a method to carry out the 1-qubit calibrations using physical control of the device rather than using a modiﬁcation of the deﬁnition of the circuit. (This later development is not relevant to the 2019 experiment and we did not study it.) 11 4 Data, documentations, and discussions 4.1 The supplementary data The Google paper was leaked around September 23, 2019 and the paper was published by “Nature” a month later on October 23, 2019. The data for the Google 2019 experiment can be found in [5]. The Google team uploaded data to the server in ﬁve dates as follows: October 22/23, 2019, publication date 1. Files with 500,000 bitstrings for many of the experimental circuits with n < 53 were uploaded and for n = 53 samples consists of a few million bitstrings were uploaded. The bitstrings for several numbers of qubits n were missing on this date (e.g. missing for n=16, 32, 34, 38, 39, 42, . . . , 51). In addition, the bitstrings for the patch circuits were missing. 2. QSIM ﬁles with descriptions for the corresponding experimental circuits, as well as Python ﬁles for computing their amplitudes. January 23, 2020 Missing bitstrings for full and elided circuits were uploaded. Amplitude ﬁles for all circuits (not including patch circuits) except (roughly) 200 circuits were uploaded. Some additional data including information on noise for individual components of the Sycamore quantum computer was uploaded. January 22, 2021 & May 18, 2021 Amplitude ﬁles for all circuits (not including patch circuits) that were com- puted by the Google team were uploaded except eight circuits for which the amplitudes were computed in J¨ulich (J¨ulich Research Center, Germany). Readout error data was uploaded. June 13, 2022 The J¨ulich amplitudes were uploaded. Bitstring ﬁles (100,000 bitstrings per circuit) and QSIM and Python ﬁles for the patch circuits with n < 53. (These ﬁles are still missing for n = 53.) 12 4.2 Requests for data 4.2.1 Early requests from October 2019 Here are Kalai’s requests from the Google team from early October 2019. (a) Larger samples from the quantum circuits, comparison of the empirical distribution with the model. (b) The bitstrings produced by the quantum computer and a description of the circuits (c) The amplitudes that were computed for the samples for each circuit (d) The full list of amplitudes. (Here we refer to the 2n amplitudes for all bitstrings and not only those in the sample.) (e) Timetable for the experiments and calibrations (f) (November 11, 2019.) The values of the individual ﬁdelities eq and eg for every qubit q and gate g used in Formula 3. 4.2.2 Subsequent requests (g) Readout errors information. We discussed this matter in January 2021 and we received useful data regarding the readout errors shortly after- wards. In this case, we received the required data quickly, in a very satisfactory way. (h) Amplitudes for the veriﬁable experiments. (September 2021) As we already mentioned, the Google team developed useful algorithms and programs to compute the amplitudes for the experiments of full cir- cuits of pattern EFGH. (This was the reason for moving to a diﬀerent pattern for the “supremacy experiment.”) They used these programs for 10 53-qubit depth-14 circuits (this required several hours for each circuit on the Google supercomputer). For a useful reliability-control of the experiment we proposed that the amplitudes be computed for the remaining (over 100) full circuits, and at least for 2 speciﬁc ones. (i) The optimization programs to move from data on 1-qubit and 2-qubit circuits to 1-gate and 2-gate corrections in the deﬁnition of the cir- cuits and the raw data gathered from 1-qubit and 2-qubit circuits. We requested it in January 2022. 13 4.3 Data provided by the Google team 1. Google’s researchers were under press embargo and did not respond to most issues before the paper appeared. 2. As we already mentioned, on October 23, 2019 the paper was pub- lished in Nature, the supplementary raw data contains (b) (bitstrings, description of the circuits in a form of a Python program to compute the amplitudes) for many of the circuits used in the experiments and on January 2020 bitstrings for other circuits were added. We requested the data for the “patch circuits” in June 2021 and again in December 2021. The data for the patch circuits was uploaded on the server in June 2022 except for the 53-qubit depth-14 EFGH patch circuits. (As we mentioned, the data has only 100K samples per every patch circuit.) 3. The Google team promised to supply the amplitudes (that they com- puted as part of the experiment) for their samples (c) . They uploaded amplitudes for many circuits in January 2020, and for additional two hundred circuits in May 2021. Uploading the amplitudes that were computed for 8 circuits in an external facility (J¨ulich) required approval of the J¨ulich team and they were uploaded in June 2022. 4. Regarding the request (d) for the full list of amplitudes, the Google team informed us that the full lists of amplitudes were discarded. (In- deed, for large values of n this is a huge amount of data.) Moreover, for circuits with pattern EFGH the Google team developed (around May 2019) algorithms for computing the amplitudes for the sample that did not require computing all the 2n amplitudes and these algorithms were used in some of these cases. 5. Regarding larger samples (a). Until August 2022, the Google team did not supply any further samples produced by the quantum computer. As for comparison between the empirical distribution and the noise model, the Google team referred (already in September 2019) for such a comparison to an earlier experiment (on 9 qubits) [17]. 6. Regarding timetable and some details for the experiments and calibra- tions (request (e)), in May 2022 we received a brief timetable and some useful details regarding the ﬁnal experiments and ﬁve earlier ones. See Section 2.3. 14 7. Regarding item (h), the Google team told us that they would not per- form these further computations as they require a lot of human and computational eﬀorts. However, using new algorithms, amplitudes for all the veriﬁable exper- iments were computed by Kalachev, Panteleev, and Yung [11] and the results give a strong support to Google’s 2019 claims and predictions regarding the linear cross entropy of their samples. 8. Regarding item (i), the Google team informed us that they could not share the full proprietary calibration system (that required many years of development) needed to move from data on 1-qubit and 2-qubit to 1-gate and 2-gate corrections in the deﬁnition of the circuits. They pointed out that the main innovation of the calibration program was made public. 9. In July 2022, Adam Zalcman and Sergio Boixo gave us a useful Python program for splitting the patch circuits into the two patches. 10. Regarding item (f) the individual gate- and qubit- ﬁdelities. We asked for it several times in 2019 and early 2020 and again in September 2022. (It turned out recently that this data is crucial for the study of patch circuits.) 4.4 The discussion with the Google team Since October 2019 through August 2022, we had good discussions (initiated by Scott Aaronson), mainly by email, with the Google team and especially with John Martinis and Sergio Boixo, on various aspects of their experiment and on some of our ﬁndings. Overall, the Google team welcomed us (and others) in putting their experiment under careful scrutiny, even though they had been aware since 2019 of the ﬁrst author’s concerns [10] about the reli- ability of the Google 2019 Sycamore paper. The discussions were easy going and in good atmosphere. A video discussion between Boixo, Kalai, Rinott and Shoham in October 2021 was especially fruitful. The basic methodol- ogy of trying to pass the data from the Google experiment through various “sanity tests” was overall agreed upon by the Google team although at times we had diﬀerent interpretations of speciﬁc ﬁndings. There was a single issue regarding our concerns about the calibration process in connection with the 15 2019 Google video [9], discussed in late 2021, that led to a somewhat more tense exchange, and in particular where John Martinis criticized attempts to pass judgment on the Google 2019 experiment based on a short video meant for general audience, rather than on the paper itself. Still, also in this case, we had a useful discussion which ultimately shed some light on aspects of the Google 2019 experiment. Overall, we were not shy to ask for data and information, and in a few cases the Google team was not shy to decline our requests. Most of our requests for data and information were met, although arguably, most of our requests should have been part of the supplementary material of the Google paper to start with. Since December 2020 until November 2021, we conducted in parallel a useful discussion with Chao-Yang Lu and members of the Gaussian boson sampling team from USTC along with members of the Google team and other researchers regarding the issue of spooﬁng and k-point correlations of the Gaussian boson sampling experiment. (This discussion was also initiated by Scott Aaronson.) We also had a brief email correspondence with the USTC team that replicated the Google 2019 experiment. 4.5 The discrepancy with the outcomes of J¨ulich Re- search Center team One little mystery that was settled following our discussions with the Google team involved the data from the J¨ulich team. The amplitudes for 8 large cir- cuits with 39, 42 and 43 qubits, were computed by researchers from J¨ulich’s research center using their own powerful simulators and (later, after the pub- lication of the paper) when the Google team checked the computation with their own simulators, in some cases the amplitudes were diﬀerent, and in one case also the estimated ﬁdelity was lower than expected. Initially, the thought was that the (small) diﬀerences between the outcomes were caused by nu- merical diﬀerences between the simulators. However, this was not the reason. It turned out that the problem was not a numerical diﬀerence between the simulators but rather using a (slightly) diﬀerent calibration method. (See Section 3.) In the simulations coming from J¨ulich the researchers used cir- cuits and measurements corresponding to the correct experiment, but the Google team slightly improved the parameters of the circuits a few days af- ter running the experiment, and the J¨ulich team used an older version. This 16 explains the discrepancy in the amplitudes and why the ﬁdelity estimated with the J¨ulich simulation was lower for one of the circuits. In other words, the amplitudes were computed for precisely the same experiment, but the calibration, namely the formalization of the experiment as a quantum circuit (the description of the “native” gates) was slightly diﬀerent. Remark (October, 2021): It turned out that for seven out of the eight J¨ulich circuits the new calibration was applied and only for one circuit the J¨ulich team had used the earlier calibration. 4.6 Data from other sources It could have been valuable to perform some of our statistical analysis on data from other quantum computers and we try to obtain similar data (descrip- tion of calibrated circuits, bitstrings and amplitude ﬁles) from other quantum computers especially those from IBM. The experiment closest to the Google one (that we know about) that was carried out on an IBM quantum com- puters is by Kim et al. [16]. We were told, however, that in a boarder sense, many of IBM’s benchmarks are based on random circuits of some sort. Also by now, Google, NASA, and various other groups have powerful sim- ulators that also allow to introduce noise. This provides further opportunity for our statistical analysis, which, so far, we have not used. 4.7 The USTC replication In June 2021 scientists from USTC [29] described a close replication of the Google 2019 experiment with 56-qubit depth 20 quantum circuits on their Zuchongzhi quantum computer. Later in September 2021, they de- scribed an improved experiment with 60-qubit depth-24 circuits [32]. The team from USTC shared the description of the circuits (in Matlab) for each n = 15, 18, 21, 24, ..., 54, 56, and it seems that they sampled (or shared) less bitstrings than Google did; for example, for n = 15 they sampled 200K bit- strings, whereas the Google team sampled 500K. Carefully studying the data from these experiments (and perhaps asking for additional data) would be an interesting direction for further research. 17 4.8 Discussions over scientiﬁc blogs There were useful discussions over a few scientiﬁc blogs regarding the Google 2019 experiment, and especially over Aaronson’s blog “Shtetl Optimized” (SO) and also over Kalai’s blog “Combinatorics and More.” Reference [34] contains links to useful blog discussion mainly at the time of the Google supremacy announcements. For example, in a discussion over SO (December 2020) a commentator “Till” asserted [27] that the calibration process actually adjusted the deﬁnition of the circuit to the device. Namely, that the random circuits were generated with “standard gates” but the deﬁnition of 2-gates was modiﬁed in the calibration process (in the same way for all circuits). (See Section 3.) This was new for us and for several others who thought that the calibration process is a physical process performed on the Sycamore device. Aaronson’s own conclusion of the discussion was: “So, my summary would be that yes, there’s a calibration phase, and the 2-qubit gates used depend on the outcome of that phase, but there’s still a clear separation enforced between the calibration phase and the actual running of the QC.” 5 Two proposals for future experiments 5.1 A proposal for blind experiments In [22] we proposed the following protocol for an evaluation of Google’s ex- periment (here we describe a small variant). First, Google will share the parameters of their calibration. Next, independent scientists will prepare several programs (circuits) for Sycamore to be run with about n qubits, for which computing the sampling probabilities should be a task that takes sev- eral months (on a classical computer). These programs will be sent to Google for implementation. Google will send back the implemented programs and large samples that they produce in a short time, which is assumed to pre- clude computation of the relevant amplitudes of the calibrated circuits. Using classical computers the scientists will take their time and compute the set of amplitudes for each calibrated circuit. They will then evaluate the relation between those amplitudes and the samples they received. Such a protocol is likely to be relevant to other quantum supremacy demonstrations which are being pursued very actively these days. Overall the Google team welcomed the idea of conducting blind experiments in the future and agreed to our speciﬁc proposed protocol. 18 Remark: Over an Internet discussion (on Aaronson’s blog, Feb. 2020) Craig Gidney (a member of the Google team) [25] asserted that various ex- periments for the early pattern that were initially considered computationally hard, turned out to be easier than expected and they served and could fur- ther serve as sort of blind tests for the 2019 experiment. The Google team made a similar comment in our email correspondence in January 2021: “We did publish a lot of data that no-one has been able to analyze yet. I hope that eventually some of this data will be analyzed, which will be an inter- esting conﬁrmation. Analyzing 39 and 40 qubit has actually become easy in the meantime.” These remarks motivated our request (h) in Section 4.2. As we mentioned, the vast progress in simulation techniques has made it pos- sible to examine the ﬁdelity estimators and they were veriﬁed by Kalachev, Panteleev, and Yung, in [11]. 5.2 Testing calibration strategies by other groups on Google’s Sycamore data As a follow up to our request for the calibration programs (item (i) in Section 4.2) that could not be met, we raised the possibility of sharing raw data gathered from 1-qubit and 2-qubit circuits which formed the input for the calibration programs needed to describe the “native gates”. This will give an opportunity to other groups to test their own calibration methods (without revealing Google’s full proprietary calibration system). The Google team asserted that this might be possible in principle, but it will require some considerable eﬀorts, and it is not clear if the 2019 data was kept. It might be better and easier to implement this proposal on more recent or even future Sycamore experiments. Given the central place of the calibration stage in NISQ experiments this direction could be valuable in its own right. 6 On the evaluation of the Google 2019 ex- periment In this section we discuss the overall evaluation of the Google 2019 exper- iment. We start with ﬁve central questions regarding the experiment and continue with a list of conﬁrmations, refutations, concerns, and weaknesses of the experiment. Some issues discussed in this section are not directly re- 19 lated to the data and information gathering that we discussed in the previous sections, and we also refer to important works by other groups. 6.1 Five central questions As we already mentioned, several ingredients of the Google Sycamore experi- ment represent major progress in human ability to control quantum systems. When we bring the Sycamore 2019 to test, there are some central problems that come up: a) Were the sampling tasks achieved as claimed? In our opinion, the ﬁndings of our paper [22] show that the answer is negative. A “sampling task” in the traditional sense, namely reaching (approximately) a sample from some known-in-advanced distribution (or even known-after- the-fact distribution) was not achieved in the 2019 experiment. The empirical distribution is quite diﬀerent from the Google basic noise model (1) and mov- ing to our more detailed noise model gives only a small improvement. The Google team disagrees with our opinion and notes that they write explicitely in the supplement [4] (around Equation (24)) that they do not necessarily assume the basic noise model (1). b) Are the statistical tools for estimating the ﬁdelity satisfactory? Our study in [22] shows that given the Google noise model, the FXEB- estimator for the ﬁdelity is quite good. (We oﬀer some improvements mainly when the number of qubits is small.) It is an interesting question if this estimator measures ﬁdelity when we are far away from the Google noise model, see [8]. We note that it might be legitimate to base quantum supremacy claims on the hardness to sample with a high value of FXEB, without referring to the question of whether a speciﬁc sampling task was achieved or if FXEB genuinely estimates the ﬁdelity, see [2, 8]. c) Are the claims regarding the estimated ﬁdelity of the samples valid? Here, for example, the Google team claimed to achieve for random circuit sampling (full circuits) with m = 14, for n = 12 with estimated (FXEB) ﬁ- delity 0.3694, for n = 22 with estimated ﬁdelity 0.165, n = 32 with estimated 20 ﬁdelity 0.071. Even leaving aside larger depth and larger numbers of qubits, the question is to what extent we can regard this achievement as solid. We note that the speciﬁc computations, regarding the (FXEB) value of the sam- ples given the description of the (calibrated) circuits were veriﬁed, and the remaining question is about the reliability of the overall claim. d) Are the claims regarding the predictive power of the a-priori ﬁdelity estimations valid? (Here we refer to Formula (3).) Here, for example, the a-priori prediction based on Formula (3) (Formula (77) in [4]) for m = 14 and n = 12, 22, 32 are 0.386, 0.1554, 0.062 respec- tively. Question d) is whether we have solid evidence for the claim that the simple formula (77) (and the statistical independence assumptions allowing it) provides accurate prediction of the FXEB estimator. Here, the speciﬁc computations were not checked (the individual ﬁdelities were not shared, see Section 4 item (f)), but they are supported by our approximate computa- tions based on averaged values of the ﬁdelities. As in the previous item, the remaining question is about the reliability of Google’s overall claim and here the excellent prediction power of the a priori ﬁdelity estimation raised concerns about it (item 10 in Section 6.2, below). e) Are the claims regarding the classical diﬃculty of the sampling task correct? Regarding Google’s “quantum supremacy” claims, a very short summary is that by now classical algorithms are ten orders of magnitude faster than those used in the Google paper and hence the speed-up is ten orders of magnitude lower than Google’s fantastic claims. See, e.g., [19, 18, 31, 11, 12, 20]. The Google paper claims that their ultimate task that required 200 seconds for the quantum computer would require 10,000 years on a powerful supercomputer. With the new algorithms the task can be done in a matter of seconds. A few days after the publication of the Google supremacy paper re- searchers from IBM [20] exhibited a theoretical improvement of six orders of magnitudes (albeit on a more powerful supercomputer). In response, the Google team pointed out that their paper [3] did anticipate some progress in classical algorithms: “We expect that lower simulation costs than reported here will eventually be achieved, but we also expect that they will be consis- tently outpaced by hardware improvements on larger quantum processors.” 21 As a matter of fact, the Google team welcomed better algorithms and wrote in [3] that the bitstring samples from all circuits have been archived [5] “to en- courage development and testing of more advanced veriﬁcation algorithms.” While weakening Google’s supremacy claims, the progress in classical algorithms has also led to major support for Google’s claims on items c) and d). In the recent work by Gleb Kalachev, Pavel Panteleev, and Man- Hong Yung, [11] the authors were able to compute the ﬁdelity for samples for which 2019 Google algorithms were too slow to handle. The ﬁdelity values that Gleb, Pavel, and Man-Hong computed agree perfectly with Google’s (77) prediction. This gives a very strong support to an aﬃrmative answer to items c) and d). Another major support for both claims c) and d) are the 2021 replications by a group in USTC of the Google 2019 experiments [29, 32]. Yet more support for claims c) and d) came from our own study. In [22] we oﬀered (as a “sanity test”) an alternative way to estimate the ﬁdelity based on a “secondary” component of the theoretical distributions that arises from readout errors. In a subsequent work [23] we checked this alternative estimators for the ﬁdelity and found a good match with the FXEB estimators for n ≤ 30. (See Figure 2.) This work nicely implements Fourier tools. We remark that on many items mentioned in this section we carried out statistical analysis that is not presented here, and on most items there is room for more detailed discussion. This is especially the case for the comparison of the empirical distribution with the noise model (6.1a) and the prediction power of equation (77) (6.1d). Three very concrete questions for circuits with 22 qubits To be completely concrete let us ask three questions about (veriﬁable full) random circuit sampling of a circuit C of the kind discussed in the Google paper with n = 22 qubits and depth m = 14. 1. Can humanity produce at present samples which are good approxima- tion of the Google noise model or any other speciﬁc noise model? 2. Did humanity reach the ability to produce samples for quantum circuit C with FXEB ﬁdelity estimated above 0.15? 3. Did humanity reach the ability to predict, for a quantum circuit C, with good accuracy, the FXEB ﬁdelity estimator based on the ﬁdelity of the individual components of this circuit? 22 The ﬁndings of our paper [22] indicate that the answer to the ﬁrst ques- tion is negative. The Google supremacy paper and subsequent conﬁrmations present a strong case for a positive answer to the other two questions. But there are remaining doubts and concerns that need to be carefully checked, and not enough replications. (We are aware only of the Google experiment itself and the two USTC replications.)2 6.2 Conﬁrmations, refutations, concerns, and weak- nesses Here is a list, without much commentary of weaknesses, concerns, conﬁrma- tions, and refutations of the Google 2019 quantum supremacy experiment. A) Conﬁrmations 1. The computations of the ﬁdelity estimators and related computations reported in the Google paper were conﬁrmed by us up to our own computational limits (and several other groups conﬁrmed it well beyond what we did). 2. Major conﬁrmation by Kalachev, Panteleev, and Yung [11] for the Google 2019 paper’s experimental claims in the “supremacy region.” This gives a strong support to the reliability of the 2019 Google exper- iment. 3. Our readout errors/Fourier study largely supports (for n ≤ 30) the experimental claims and the experiment’s reliability [23], see Figure 2. 4. A successful replication was announced and published by a group from USTC [29, 32]. 5. The Google ﬁdelity estimators and statistical methodology are overall good [22]. 2For comparison, after the ﬁrst Wright brothers ﬂight on December 17, 1903 (120 feet) there were subsequent ﬂights: 852 feet on January 1, 1904; 1,297 feet on February 23 1904; 1,810 feet on May 14, 1904; 4,781 feet on September 23, 1904; 5,360 feet on December 17, 1904; 12,540 feet on January 1, 1905; 24 miles on May 22, 1905; 34 miles on July 4, 1905; 39 miles on October 5, 1905; 56 miles on November 23, 1905, and 63 miles on December 31, 1905. (Source: GPT-3) 23 B) Refutations 6. Google’s fantastic supremacy claims were largely (but not fully) refuted [19, 18, 31, 11, 12, 20]. (See Section 6.1 above.) 7. The data does not ﬁt the Google noise model [22] or any other speciﬁc (See Section 6.1 above.); the Google team disagrees on this model. point. AB) Mixed conﬁrmation/refutation 8. The boson sampling quantum supremacy experiments give a mixed signal. They were regarded as independent conﬁrmation of quantum supremacy using a very diﬀerent quantum device, however the com- putational hardness claims are in tension with old (Kalai and Kindler [15]) and new [21, 28] works. C) Concerns 9. The Google experiment represents amazing leaps in human ability to control quantum devices. The non-gradual advances are surprising and there isn’t a clear technological reason behind them.3 We note that here we refer to the ability expressed by random circuit sam- pling experiments. The remarkable progress in building the hardware itself (by several groups in academia and industry including the Google team) was gradual. 10. The prediction power for the a priori ﬁdelity estimation (Formula (77) in [4]) raised concerns about reliability. This matter is raised and dis- cussed in [14] and is also mentioned in [22]. The concern regarding the prediction of linear cross-entropy ﬁdelity es- timators based on the ﬁdelities of individual components, appears to be the strongest challenge to date to the reliability of the Google experiment and the USTC replication. There are two important remarks regarding this concern: 3The Google supremacy experiment was often compared to IBM’s “Deep Blue” victory against the chess world champion Kasparov; in that case, however, progress was gradual and earlier chess programs could already win against chess grand-masters. 24 First, the Google team and other researchers regard this prediction power as one of the main achievements of the Google experiment. Second, [11] con- ﬁrmed these predictions for many circuits for which the Google team could not even compute the amplitudes. (This is a sort of “blind test” veriﬁcation.) 11. When you move away from Google’s noise model, the ﬁdelity estimators may not capture the actual ﬁdelity of the quantum system. (See e.g. [8].) 12. Some aspects of the calibration process may seem overly surprising (this represents some preliminary analysis that we presented in our email discussions in November 2021). 13. There are no suﬃcient replications of the supremacy experiment and veriﬁcations of the experimental ﬁndings even in the 12-40 qubit range. (Neither by other groups nor by the Google team itself.) D) Weaknesses 14. While the experiment was characterized as performing a sampling com- putational task, the empirical distribution was not compared to the noise model, and the experimental samples are too small to allow such comparison (directly) when n > 14. 15. The supremacy claims were based on a single algorithm. The Google team itself developed better algorithms to related computational tasks. When the Google team found an improved type of algorithms (around May 2019) they switched to a diﬀerent class of circuits for which the new algorithm did not apply without suﬃcient evidence that similar improvements could not be made for them as well. (Later, as we men- tioned, improvements were also achieved to the new class of circuits.) 16. The results were not presented to the academic community before pub- lication (e.g., as an arXiv publication), and careful review of the ex- periment prior to publication may have been insuﬃcient. 17. The main ﬁdelity estimator is not unbiased and can be improved [22]. 25 18. Moving to a new pattern ABCDCDAB weakened the power of the extrapolation argument. For the new pattern no experiments for cir- cuits with less than 53 qubits were conducted. (This was a weakness in planning the experiment although some of the ﬁdelity predictions are now conﬁrmed [11].) 19. Improvements of the calibration process were interlaced (namely oc- curred at the same time) with the experiment. The description of the calibration process in Google’s popular video [9] may be in tension with the claim that there was a clear separation between the calibration stage and the experiment itself. 20. The calibration process itself somewhat weakens the claim for a “pro- grammable quantum computer”. (This concern was raised by a com- mentator “Till” in an Internet discussion [27].) 21. The eﬀect of the calibration process is very large (it is not a “ﬁne tuning” of some sort). 22. The precise programs for the calibration process are commercial secrets. (However, the main innovation in calibration was the use of XEB, and that code is open sourced.) 23. The levels of transparency and of documentation of the Google 2019 experiment seem insuﬃcient. Of course, some of the items above are controversial, some of them are rather minor, and also many of these items are related. Items (15), (16) are a-priori weaknesses that turned out to be crucial (refutation (6) above). The recent conﬁrmations (2) indicate that weakness (18) may have no baring on the assertions for large circuits, but we still regard it as a weakness in the planning of the experiment. (The Google team disagrees with us.) We also note that expectations expressed in the Google 2019 paper [3] seem overly optimistic: “Quantum processors have thus reached the regime of quantum supremacy. We expect that their computational power will con- tinue to grow at a double-exponential rate: the classical cost of simulating a quantum circuit increases exponentially with computational volume, and hardware improvements will probably follow a quantum-processor equiva- lent of Moore’s law, doubling this computational volume every few years.” (Compare with later analysis in [33, 8].) 26 Figure 2: The decay of Fourier–Walsh contributions as predicted by the theory (bold black curve)([7, 23]) and as demonstrated by samples of a 16-qubit Sycamore computer (ten random circuits). 27 7 Where we are (September 2022) Now that the Google 2019 experiment data-gathering stage needed for our study has largely come to an end, here is a brief summary of where we are. 1. We compared the empirical distribution with Google’s noise model and with other noise models. Some of this analysis is discussed in [22], and a few other ﬁndings were discussed with the Google team. 2. In [22] we oﬀered an alternative way (that could serve as a “sanity test”) to estimate the ﬁdelity based on a linear cross entropy test for a “sec- ondary” component of the theoretical distributions that arises from readout errors. In a subsequent work in progress [23] we used Fourier methods to check this alternative estimator for the ﬁdelity, and found a good match with the primary linear cross entropy ﬁdelity estimators for n ≤ 30. 3. We carried out some statistical analysis of the eﬀect of the calibration process. Continuing in this direction, we plan, among other things, to study a proposal of John Martinis (made on Aaronson’s blog in December 2020 [26], as well as in our discussions) for improving the calibration. 4. We plan to extend some of our statistical studies to the new data from June 2022, and especially to the data for the patch circuits. There is also some remaining analysis to be carried out for the earlier data. We also plan to extend our statistical study to data coming from other NISQ computers and from simulators that incorporate the eﬀect of noise. 5. There is one important data item that is still not publicly available, namely, the ﬁdelities of individual components that are used in Formula (3) (Formula (77) in [4]). This information consists only of a few hundred real numbers and we hope that the Google team will share it in the near future. The Google team provided useful approximate versions for formula (77) (such as Formula 4) but we found out that for the study of patch circuits the full data is required. 6. (October 2022.) On the theoretical side, we recently realized that the remarkable work of Gao, Kalinowski, Chou, Lukin, Barak, and Choi [8] is relevant to some of the concerns discussed above. Analysis based on [8] strengthens concern (10) and sheds some doubts on our conﬁrmation (3) (the second item above). 28 Acknowledgements Research supported by ERC grant 834735. We thank many colleagues includ- ing members of the Google team for helpful discussion and Carsten Voelk- mann for many corrections and helpful suggestions. References [1] S. Aaronson, Why Google’s quantum supremacy milestone matters, Opinion, New York Times Oct. 30, 2019. [2] S. Aaronson and S. Gunn, On the classical hardness of spooﬁng linear cross-entropy benchmarking, 2019, arXiv:1910.12085. [3] F. Arute et al., Quantum supremacy using a programmable supercon- ducting processor, Nature, 574 (2019), 505–510. [4] F. Arute et al. (2019), Supplementary information for “Quantum supremacy using a programmable superconducting processor,” 2019, arXiv:1910.11333. [5] F. Arute et al., Supplementary data for “Quantum supremacy 2019, using https://datadryad.org/stash/dataset/doi:10.5061/dryad.k6t1rj8. superconducting programmable processor,” a [6] S. Boixo, S. V. Isakov, V. N. Smelyanskiy, and H. Neven, Simulation of low-depth quantum circuits as complex undirected graphical models, arXiv:1712.05384, 2017. [7] S. Boixo, V. N. Smelyanskiy, and H. Neven, Fourier analysis of sampling from noisy chaotic quantum circuits, 2017, arXiv:1708.01875. [8] X. Gao, M. Kalinowski, C.-N. Chou, M. D. Lukin, B. Barak, and S. Choi, Limitations of Linear Cross-Entropy as a Measure for Quantum Advantage, 2021, arXiv:2112.01657. [9] Google, Demonstrating Quantum Supremacy, 4:42 minutes video for a large audience, Oct. 23, 2019. URL: https://youtu.be/-ZNEzzDcllU . 29 [10] S. Irani (Moderator), Supremacy Panel, Hebrew University of Jerusalem, Dec. 2019. Participants: D. Aharonov, B. Barak, A. Bouland, G. Kalai, S. Aaronson, S. Boixo, and U. Vazirani. (Video.) URL: https://youtu.be/ Yb7uIGBynU . [11] G. Kalachev, P. Panteleev, and M.-H. Yung, Multi-tensor contraction for XEB veriﬁcation of quantum circuits, 2021, arXiv:2108.05665. [12] G. Kalachev, P. Panteleev, P. F. Zhou, and M.-H. Yung, Classical sampling of random quantum circuits with bounded ﬁdelity, 2021, arXiv:2112.15083. [13] G. Kalai, The Google quantum supremacy demo, videotaped lecture, December 2019. URL: https://youtu.be/p18P1y8GD9U. [14] G. Kalai, The argument against Quantum Computers, the quantum laws of nature, and Google’s supremacy claims, in: The Intercontinen- tal Academia Laws: ’Rigidity and Dynamics,’ (M. J. Hannon and E. Z. Rabinovici (ed.)) Proceedings of the ICA Workshops 2018&2019, Singa- pore and Birmingham, World Scientiﬁc, to appear. arXiv:2008.05188. [15] G. Kalai and G. Kindler, Gaussian noise sensitivity and BosonSampling, 2014, arXiv:1409.3093. [16] J.-S. Kim, L. S. Bishop, A. D. Corcoles, S. Merkel, J. A. Smolin, S. Sheldon, Hardware-eﬃcient random circuits to classify noise in a multi- qubit system, Phys. Rev. A 104 (2021), 022609. [17] C. Neill et al., A blueprint for demonstrating quantum supremacy with superconducting qubits. Science 360 (2018), 195-199 [18] F. Pan, K. Chen, and P. Zhang, Solving the sampling problem of the Sycamore quantum supremacy circuits, Phys. Rev. Lett. 129 (2022), 090502 [19] F. Pan and P. Zhang, Simulating the Sycamore quantum supremacy circuits, 2021, arXiv:2103.03074 [20] E. Pednault, J. A. Gunnels, G. Nannicini, L. Horesh, and R. Wisni- eﬀ, Leveraging secondary storage to simulate deep 54-qubit Sycamore circuits, 2019, arXiv:1910.09534. 30 [21] A. S. Popova and A. N. Rubtsov, Cracking the quantum advantage threshold for Gaussian boson sampling, 2021, arXiv:2106.01445. [22] Y. Rinott, T. Shoham, and G. Kalai, Statistical aspects of the quantum supremacy demonstration, Statistical Science 37 (2022), 322–347. [23] Y. Rinott, T. Shoham, and G. Kalai, Quantum Advantage Demonstra- tions via Random Circuit Sampling: Fourier Expansion and Statistics, manuscript in progress. [24] Shtetl Optimized, A comment by S. Boixo , Dec. 2020. URL: https://scottaaronson.blog/?p=5159#comment-1870651 . [25] Shtetl Optimized, A comment by C. Gidney, Feb. 2020. URL: https://scottaaronson.blog/?p=4608#comment-1830545 . [26] Shtetl Optimized, A comment by J. Martinis, Dec. 2020. URL: https://scottaaronson.blog/?p=5159#comment-1870734 . [27] Shtetl Optimized, A comment by “Till”, Dec. 2020. URL: https://scottaaronson.blog/?p=5159#comment-1869118 . [28] B. Villalonga, M. Yuezhen Niu, L. Li, H. Neven, J. C. Platt, V. N. Smelyanskiy, and S. Boixo, Eﬃcient approximation of experimental Gaussian boson sampling, 2021, arXiv:2109.11525. [29] Y. Wu et al., Strong quantum computational advantage using a super- conducting quantum processor. Phys. Rev. Lett. 127 (2021), 180501. [30] H.-S. Zhong et al., Quantum computational advantage using photons, Science 370 (2020), 1460–1463. [31] Y. Zhou, E. M. Stoudenmire, and X. Waintal, What limits the simula- tion of quantum computers?, 2020, arXiv:2002.07730. [32] Q. Zhu et al., Quantum Computational Advantage via 60-Qubit 24- Cycle Random Circuit Sampling, Science Bulletin, Volume 67 (2022), 240-245. arXiv:2109.03494. [33] A. Zlokapa, S. Boixo, and D. Lidar, Boundaries of quantum supremacy via random circuit sampling, 2020, arXiv:2005.02464. 31 [34] Various links to useful discussions over scientiﬁc blogs of the Google’s 2019 paper and related developments. (SO stands for Shtetl Optimized, Scott Aaronson’s blog; CaM stands for Combinatorics and More, Gil Kalai’s blog; WoT stands for Windows on Theory, Boaz Barak’s blog. 1. Scott’s Supreme Quantum Supremacy FAQ! SO, Sept 23, 2019. https://scottaaronson.blog/?p=4317. 2. Quantum computers: amazing progress IBM), and claims extraordinary & supremacy (Google). CaM, https://gilkalai.wordpress.com/2019/09/23/quantum-computers- amazing-progress-google-ibm-and-extraordinary-but-probably- false-supremacy-claims-google/ probably 23, Sept but (Google false 2019. 3. The story of Poincar´e and his friend the baker, CaM, Oct. 13, 2019. https://gilkalai.wordpress.com/2019/10/13/the-story-of- poincare-and-his-friend-the-baker/ 4. Quantum supremacy: the gloves are oﬀ, SO, Oct. 23, 2019/ https://scottaaronson.blog/?p=4372. 5. Boaz’s inferior classical inferiority FAQ, WoT, Oct. 24, 2021. https://windowsontheory.org/2019/10/24/boazs-inferior-classical- inferiority-faq/ 6. Gil’s Collegial Quantum Supremacy Skepticism FAQ, CaM Nov. 13, 2019, https://gilkalai.wordpress.com/2019/11/13/gils-collegial- quantum-supremacy-skepticism-faq/ 7. My “Quantum Supremacy: Tour, 2020 World https://scottaaronson.blog/?p=4608 Speaking Skeptics Were Wrong” 2020 SO, Feb. 17, 8. Quantum supremacy, now with BosonSampling, SO, Dec. 3, 2020, https://scottaaronson.blog/?p=5122. 9. Chinese BosonSampling experiment: the gloves are oﬀ, SO, Dec. 16, 2020. https://scottaaronson.blog/?p=5159 Gil Kalai, Hebrew University of Jerusalem and Reichman University, gil.kalai@gmail.com Yosef Rinott, Hebrew University of Jerusalem, yosef.rinott@mail.huji.ac.il Tomer Shoham, Hebrew University of Jerusalem, tomer.shohamm@gmail.com 32",0.0
"Bootstrap Advantage Estimation for Policy Optimization in Reinforcement
  Learning","[{'href': 'http://arxiv.org/abs/2210.07312v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.07312v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-10-13 19:30:43,,"2 2 0 2 t c O 3 2 ] G L . s c [ 1 v 2 6 4 3 1 . 0 1 2 2 : v i X r a Artiﬁcial Intelligence-Based Methods for Fusion of Electronic Health Records and Imaging Data* Farida Mohsen1, Hazrat Ali1, Nady El Hajj1,2, and Zubair Shah1,* 1College of Science and Engineering, Hamad Bin Khalifa University, Qatar Foundation, 34110 Doha, Qatar 2College of Health and Life Sciences, Hamad Bin Khalifa University, Qatar Foundation, 34110 Doha, Qatar *Correspondence to: Zubair Shah ABSTRACT Healthcare data are inherently multimodal, including electronic health records (EHR), medical images, and multi-omics data. Combining these multimodal data sources contributes to a better understanding of human health and provides optimal personalized healthcare. The most important question when using multimodal data is how to fuse them - a ﬁeld of growing interest among researchers. Advances in artiﬁcial intelligence (AI) technologies, particularly machine learning (ML), enable the fusion of these different data modalities to provide multimodal insights. To this end, in this scoping review, we focus on synthesizing and analyzing the literature that uses AI techniques to fuse multimodal medical data for different clinical applications. More speciﬁcally, we focus on studies that only fused EHR with medical imaging data to develop various AI methods for clinical applications. We present a comprehensive analysis of the various fusion strategies, the diseases and clinical outcomes for which multimodal fusion was used, the ML algorithms used to perform multimodal fusion for each clinical application, and the available multimodal medical datasets. We followed the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) guidelines. We searched Embase, PubMed, Scopus, and Google Scholar to retrieve relevant studies. After pre-processing and screening, we extracted data from 34 studies that fulﬁlled the inclusion criteria. We found that studies fusing imaging data with EHR are increasing and doubling from 2020 to 2021. In our analysis, a typical workﬂow was observed: feeding raw data, fusing different data modalities by applying conventional machine learning (ML) or deep learning (DL) algorithms, and ﬁnally, evaluating the multimodal fusion through clinical outcome predictions. Speciﬁcally, early fusion was the most used technique in most applications for multimodal learning (22 out of 34 studies). We found that multimodality fusion models outperformed traditional single-modality models for the same task. Disease diagnosis and prediction were the most common clinical outcomes (reported in 20 and 10 studies, respectively) from a clinical outcome perspective. Neurological disorders were the dominant category (16 studies). From an AI perspective, conventional ML models were the most used (19 studies), followed by DL models (16 studies). Multimodal data used in the included studies were mostly from private repositories (21 studies). Through this scoping review, we offer new insights for researchers interested in knowing the current state of knowledge within this research ﬁeld. Introduction Over the past decade, digitization of health data have grown tremendously with increasing data repositories spanning the healthcare sectors1. Healthcare data are inherently multimodal, including electronic health records (EHR), medical imaging, multi-omics, and environmental data. In many applications of medicine, the integration (fusion) of different data sources has become necessary for effective prediction, diagnosis, treatment, and planning decisions by combining the complementary power of different modalities, thereby bringing us closer to the goal of precision medicine2, 3. Data fusion is the process of combining several data modalities, each providing different viewpoints on a common phenomenon to solve an inference problem. The purpose of fusion techniques is to effectively take advantage of cooperative and complementary features of different modalities4, 5. For example, in interpreting medical images, clinical data is often necessary for making effective diagnostic decisions. Many studies found that missing pertinent clinical and laboratory data during image interpretation decreases the radiologists’ ability to accurately make diagnostic decisions6. The signiﬁcance of clinical data to support the accurate interpretation of imaging data is well established in radiology as well as in a wide variety of imaging-based medical specialties such as dermatology, ophthalmology, and pathology that depend on clinical context to interpret imaging data correctly7–9. Thanks to the advances of AI and ML models, one can achieve a useful fusion of multimodal data with high-dimensionality10, various statistical properties, and different missing value patterns11. Multimodal ML is the domain that can integrate different *This is pre-print of paper accepted for publication in Scientiﬁc Reports. Cite the ﬁnal version from Nature Scientiﬁc Reports. Email: haali2@hbku.edu.qa, zshah@hbku.edu.qa data modalities. In recent years, multimodal data fusion has gained much attention for automating clinical outcome prediction and diagnosis. This can be seen in Alzheimer’s disease diagnosis and prediction12–15 when imaging data were combined with speciﬁc lab test results and demographic data as inputs to ML models, and better performance was achieved than the single-source models. Similarly, fusing pathological images with patient demographic data observed an increase in performance in comparison with single modality models for breast cancer diagnosis16. Several studies found similar advantages in various medical imaging applications, including diabetic retinopathy prediction, COVID-19 detection, and glaucoma diagnosis17–19. This scoping review focuses on studies that use AI models to fuse medical images with EHR data for different clinical applications. Modality fusion strategies play a signiﬁcant role in these studies. In the literature, some other reviews have been published on the use of AI for multimodal medical data fusion20–26; however, they differ from our review in terms of their scope and coverage. Some previous studies focused on the fusion of different medical imaging modalities20, 21; they did not consider the EHR in conjunction with imaging modalities. Other reviews focused on the fusion of omics data with other data modalities using DL models22, 23. Another study24 focused on the fusion of various internet of medical things (IoMTs) data for smart healthcare applications. Liu et al.27 focused exclusively on integrating multimodal EHR data, where multimodality refers to structured data and unstructured free texts in EHR, using conventional ML and DL techniques. Huang et al.26 discussed fusion strategies of structured EHR data and medical imaging using DL models emphasizing fusion techniques and feature extraction methods. Furthermore, their review covered the research till 2019 and retrieved only 17 studies. In contrast, our review focuses on studies using conventional ML or DL techniques with EHR and medical imaging data, covering 34 recent studies. Table 1 provides a detailed comparison of our review with existing reviews. The primary purpose of our scoping review is to explore and analyze published scientiﬁc literature that fuses EHR and medical imaging using AI models. Therefore, our study aims to answer the following questions: 1. Fusion Strategies: what fusion strategies have been used by researchers to combine medical imaging data with EHR? What is the most used method? 2. Diseases: For what type of diseases are fusion methods implemented? 3. Clinical outcomes and ML methods: What types of clinical outcomes are addressed using the different fusion strategies? What kind of ML algorithms are used for each clinical outcome? 4. Resource: What are the publicly accessible medical multimodal datasets? We believe that this review will provide a comprehensive overview to the readers on the advancements made in multimodal ML for EHRs and medical imaging data. Furthermore, the reader will develop an understanding of how ML models could be designed to align data from different modalities for various clinical tasks. Besides, we believe that our review will help identify the lack of multimodal data resources for medical imaging and EHR, thus motivating the research community to develop more multimodal medical data. Preliminaries We ﬁrst identify the EHR and medical imaging modalities that are the focus of this review. Then, we present the data fusion strategies that we use to investigate the studies from the perspective of multimodal fusion. Data modalities In this review, we focus on studies that use two primary data modalities: • Medical imaging modality: This includes N-dimensional imaging information acquired in clinical practice, such as X-ray, Magnetic Resonance Imaging (MRI), functional MRI (fMRI), structural MRI (sMRI), Positron Emission Tomography (PET), Computed Tomography (CT), and Ultrasound. • EHR data: This includes both structured and unstructured free-text data. Structured data include coded data such as diagnosis codes, procedure codes, numerical data such as laboratory test results, and categorical data such as demographic information, family history, vital signs, and medications. Unstructured data include medical reports and clinical notes. In our review, we consider studies combining the two modalities of EHR and imaging. However, there exist cases where the data could contain only multiple EHR modalities (structured and unstructured) or multiple imaging modalities (e.g., PET and MRI). We consider such data as a single modality, i.e., the EHR modality or imaging modality. 2/20 Previous Reviews A review on multimodal fusion: medical image Compendious analysis of medical modalities, databases, multimodal fusion and quality metrics20 Advances in multimodality data fusion in neuroimag- ing21 techniques Year Scope and Coverage 2022 Their review focused on the fusion of differ- ent medical imaging modalities. 2021 Their review focused on the fusion of dif- ferent imaging modalities, considering neu- roimaging applications for brain diseases and neurological disorders. An overview of deep learn- ing methods for multimodal medical data mining22 2022 Their review focused on the fusion of dif- ferent types of multi-omics data with EHR and different imaging modalities, only con- sidering DL models for speciﬁc diseases (COVID-19, cancer, and Alzheimer’s). 2022 Their review focused on the fusion of dif- ferent types of multi-omics data with EHR and imaging modalities, considering only DL models. Moreover, they did not provide a summary of the freely accessible multi- modal datasets and a summary of evaluation measures used to evaluate the multimodal models. 2021 Their survey did not focus on fusing med- ical imaging with EHR but rather covered the fusion of IoMTs data for smart health- care applications and covered studies pub- lished untill 2020. Moreover, in their review, multimodality referred to fusing either dif- ferent 1D medical signals (such as electro- cardiogram (ECG) and biosignals), different medical imaging modalities, or 1D medical signals with imaging. 2021 Their review focused on the fusion of struc- tured and unstructured EHR data and did not consider medical imaging modalities. More- over, they did not provide a summary of the freely accessible multimodal datasets and a summary of evaluation measures used to evaluate the multimodal models. Multimodal deep learning for biomedical data fusion: a review23 A comprehensive survey on multimodal medical signals fusion for smart healthcare systems24 learning for Machine multimodal electronic health records-based re- search: Challenges and perspectives27 Fusion of medical imag- ing and electronic health records using deep learning: a systematic review and im- plementation guidelines26 Comparative contribution of our review Our review focused on the fusion of medi- cal imaging with multimodal EHR data and considered different imaging modalities as a single modality. The two reviews did not share any common studies. Our review focused on the fusion of medical imaging with EHR data, considering vari- ous diseases, such as neurological disorders, cancer, cardiovascular diseases, psychiatric disorders, eye diseases, and Covid-19. The two reviews did not share any common stud- ies. Our review focused on the fusion of medi- cal imaging with EHR data, considering all AI models for various diseases, such as neu- rological disorders, cancer, cardiovascular diseases, psychiatric disorders, eye diseases, and Covid-19. The two reviews did not share any common studies. Our review focused on the fusion of med- ical imaging with EHR data, considering all AI models. Moreover, o ur study pro- vided a summary of the accessible multi- modal datasets and a summary of evaluation measures used to evaluate the multimodal models. The two reviews only shared two common studies. Our review focused on the fusion of medical imaging with EHR (structured and unstruc- tured) for different clinical applications. It included 34 studies, most of them published in 2021 and 2022, with no study common between the two reviews. Our review focused on the fusion of medi- cal imaging with EHR and considered struc- tured and unstructured data in EHR as a sin- gle modality. The two reviews did not share any common studies. 2020 Their review focused on the fusion of struc- tured EHR data and medical imaging, con- sidering only DL models, and included only 17 studies published until 2019. Our review focused on the fusion of medi- cal imaging with EHR data, considering all AI models, and included 34 studies, almost more than half published in 2020 and 2021. Table 1. Comparison with previous reviews. 3/20 Fusion strategies As outlined in26, fusion approaches can be categorized into early, late, and joint fusion. These strategies are classiﬁed depending on the stage in which the features are fused in the ML model. Our scoping review follows the deﬁnitions in26 and attempts to match each study to its taxonomy. In this section, we brieﬂy describe each fusion strategy: • Early fusion: It joins features of multiple input modalities at the input level before being fed into a single ML algorithm for training26. The modality features are extracted either manually or by using different methods such as neural networks (NN), software, statistical methods, and word embedding models. When NN are used to extract features, early fusion requires training multiple models: the feature extraction models and the single fusion model. There are two types of joint fusion: type I and type II. Type I fuses the original features without extracting features, while type II fuses extracted features from modalities. • Late fusion: It trains separate ML models on data of each modality, and the ﬁnal decision leverages the predictions of each model26. Aggregation methods such as weighted average voting, majority voting, or a meta-classiﬁer are used to make the ﬁnal decision. This type of fusion is often known as decision-level fusion. • Joint fusion: It combines the learned features from intermediate layers of NN with features from other modalities as inputs to a ﬁnal model during training26. In contrast to early fusion, the loss from the ﬁnal model is propagated back to the feature extraction model during training so that the learned feature representations are improved through iterative updating of the feature weights. NNs are used for joint fusion since they can propagate loss from the ﬁnal model to the feature extractor(s). There are two types of joint fusion: type I and type II. The former is when NNs are used to extract features from all modalities. The latter is when not all the input modalities’ features are extracted using NNs26. Methods In this scoping review, we followed the guidelines recommended by the PRISMA-ScR28. Search strategy In a structured search, we searched four databases, including Scopus, PubMed, Embase, and Google Scholar, to retrieve the relevant studies. We note here that MEDLINE is covered in PubMed . For Google Scholar search results, we selected the ﬁrst 110 relevant studies, as, beyond 110 entries, the search results rapidly lost relevancy and were unmatched to our review’s topic. Furthermore, we limited our search to English-language articles published in the last seven years between January 1, 2015, and January 6, 2022. The search was based on abstracts and titles and was conducted between January 3 and January 6, 2022. In this scoping review, we focused on applying AI models to multimodal medical data-based applications. The term multimodal refers to combining medical imaging and EHR, as described in Preliminaries section. Therefore, our search string incorporated three major terms connected by AND:( (“Artiﬁcial Intelligence” OR “machine learning” OR “deep learning”) AND “multimodality fusion” AND (“medical imaging” OR “electronic health records”)). We used different forms of each term. We provide the complete search string for all databases in Appendix 1 of the supplementary material. Inclusion and exclusion criteria We included all studies that fused EHR with medical imaging modalities using an AI model for any clinical application. As AI models, we considered classical ML models, DL models, transfer learning, ensemble learning, etc as mentioned in the search terms in Appendix 1 of the supplementary material. We did not consider studies that use classical statistical models such as regression in our review. Our deﬁnition of imaging modalities is any type of medical imaging used in clinical practice, such as MRI, PET, CT scans, and Ultrasound. We considered both structured and unstructured free-text patients’ data for EHR modalities as described in Preliminaries section . Only peer-reviewed studies and conference proceedings were included. Moreover, all included studies were limited to English language only. We did not enforce restrictions on types of disorders, diseases or clinical tasks. We excluded studies that used a single data modality. Also, we excluded studies that used different types of data from the same modality, such as studies that only combined two or more imaging types (e.g. PET and MRI), as we considered this single modality. Moreover, studies that integrated original imaging modalities with extracted imaging features were excluded as this was still considered a single modality. Also, studies that combined multi-omics data modality were excluded. In addition, studies that were unrelated to the medical ﬁeld or did not use AI-based models were excluded. We excluded reviews, conference abstracts, proposals, editorials, commentaries, letters to editors, preprints, and short letters articles. Non-English publications were also excluded. 4/20 Study selection We used Rayyan web-based review management tool29 for the ﬁrst screening and study selection. After removing duplicates, we screened the studies based on title and abstract. Subsequently, full-text of the selected studies from the title and abstract screening were assessed for eligibility using our inclusion and exclusion criteria. Two authors (F.M. and H.A.) conducted the study selection and resolved any conﬂict through discussion. A third author (Z.S.) was consulted when an agreement could not be reached. Data extraction From the ﬁnal included studies, a data extraction form was designed and piloted on four studies to develop a systematic and accurate data extraction process. The extracted data from the studies are ﬁrst author’s name, year, the country of the ﬁrst author’s institution, disease’s name, clinical outcome, imaging modality, EHR modality, fusion strategy, feature extraction methods, data source, AI models, evaluation metrics, and comparison with single modality. In Appendix 2 of the supplementary material, we provide the extracted information description in detail. One author (F.M.) performed the data extraction, and two other authors (Z.S. and H.A.) reviewed and veriﬁed the extracted data. Any disagreement was resolved through discussion and consensus between the three authors. Data synthesis Following the data extraction, we used a narrative approach to synthesize the data. We analyzed the studies from ﬁve perspectives: fusion strategies, diseases, clinical outcomes with ML algorithms, data sources/type, and evaluation mechanism. For fusion strategies, we focused on how the multimodal data was fused. In addition, we recorded implementation details of the model, such as feature extraction and single modality evaluation. We also extracted information on the diseases for which fusion methods were implemented. Furthermore, we analyzed where the data fusion models were applied for clinical outcomes and what ML models were used for each task. Moreover, we focused on the type of imaging and EHR data used by the studies, the source of data, and its availability. Finally, for evaluation, we focused on the evaluation metrics used by each study. Study quality assessment In accordance with the guidelines for scoping reviews30, 31, we did not perform quality assessments of the included studies. Results Search results A total of 1158 studies were retrieved from the initial search. After duplicates elimination, 971 studies were retained. Based on our study selection criteria (see Methods), 44 studies remained for full-text review after excluding articles based on their abstract and title. Moreover, 10 studies were removed after the full-text screening. Finally, 34 studies met our inclusion criteria and were selected for data extraction and synthesis. Figure 1 shows a ﬂowchart of the study screening and selection process. Demographics of the studies As presented in Table 2, approximately two-thirds of the studies were journal articles (n= 23, ∼ 68%)12–15, 17, 19, 25, 32–46, whereas 11 studies were conference proceedings (∼ 32%)16, 47–56. Most of the studies were published between 2020 and 2022 (n = 22, ∼ 65%). Figure 2 shows a visualization of the publication type-wise and year-wise distribution of the studies. The included studies were published in 13 countries; however, most of these studies were from the USA (n = 10, ∼ 30%) and China (n = 8, ∼ 24%). Data fusion strategies We mapped the included studies to the taxonomy of fusion strategies outlined in the Preliminaries Section. A primary interest of our review is to identify the fusion strategies that the included studies used to improve the performance of ML models for different clinical outcomes. Early fusion The majority of the included studies (n = 22, ∼ 65%) used early fusion to combine medical imaging and non-imaging data. When the input modalities have different dimensions, such as when combining one-dimensional (1D) EHR data with 2D or 3D imaging data, it is essential to extract high-level imaging features in 1D before fusing with 1D EHR data. To accomplish this, various methods were used in the studies, including neural network-based features extraction, data generation through software, or manual extraction of features. Out of the 22 early fusion studies, 19 studies12, 13, 15, 25, 33–36, 39, 41–45, 50–53 used manual or software-based imaging features, and 3 studies used neural network-based architectures to extract imaging features before combining with other clinical data modality16, 18, 54. Six out of the 19 studies that used manual or software-based features 5/20 Figure 1. PRISMA ﬂow chart for study identiﬁcation, screening, and selection. 6/20 Characteristics Year Number of studies 2022 2021 2020 2019 2018 2016 2015 Country United States of America (USA) China United Kingdom (UK) Germany India Australia Denmark Iran Korea Pakistan Kingdom of Saudi Arabia Singapore Publication type Journal Conference 1 14 7 2 5 4 1 10 8 4 2 2 1 1 1 1 1 1 1 23 11 Table 2. Demographics of the included studies. reduced the feature dimension before concatenating the two modalities’ features using different methods25, 36, 45, 50–52. Such methods include recursive feature elimination52, a ﬁlter-based method using Pearson correlation coefﬁcient51, Random Forest feature selection based on Gini importance50, Relief-based feature selection method25, a wrapper-based method using backward feature elimination36, and a rank-based method using Gini coefﬁcients45. Moreover, 3 studies13, 15, 44 utilized the principal component analysis (PCA) dimensionality reduction technique to reduce the feature dimension. In the studies that used neural network-based architectures to extract imaging features, CNN architectures were used in three studies16, 18, 54. These studies concatenated the multimodal features (CNN-extracted and EHR features) for their fusion strategy. Fourteen early fusion studies evaluated their fusion models’ performance against that of single modality mod- els12, 13, 15, 16, 18, 25, 32–34, 36, 41–44, 51 . As a result, 13 of these studies exhibited a better performance for fusion when compared with their imaging-only and clinical-only counterparts12, 13, 15, 16, 18, 25, 32–34, 41–44, 51. Joint fusion Joint fusion was the second most common fusion strategy used in 10 out of the 34 studies. In these studies, different neural network-based methods were used for processing the imaging and EHR data modalities. Chen et al.39 used the Visual Geometry Group (VGG-16) architecture to extract features from MRI images, while they used a bidirectional long-short term memory (LSTM ) network with an attention layer to learn feature representation from MRI reports. Then, they concatenated the learned features of the two modalities before feeding them into a stacked K-nearest neighbor (KNN) attention pooling layer. Grant et al.55 used a Residual Network (ResNet50) architecture to extract relevant features from the imaging modality and fully connected NN to process the non-imaging data. They directly concatenated the learned feature representation of the imaging and non-imaging data and fed them into two fully connected networks. Yidong et al.19 used a Bayesian CNN encoder-decoder to extract imaging features and a Bayesian Multilayer perception (MLP) encoder-decoder to process the medical indicators data. The study directly concatenated the two feature vectors and fed the resulted vector into another Bayesian MLP. Samak et al.47 utilized CNN with a self-attention mechanism to extract the imaging features and fully connected NNs to process the metadata information. Lili et al.39 used VGG-19 architecture to extract the multimodal MRI features and fully connected networks for clinical data. The study concatenated the two feature vectors and fed them into fully connected NN. Another study46 applied 7/20 Figure 2. The distribution of studies by the type of publication and the year. CNN layers for imaging features extraction and word embeddings (Word2vec) with self-attention for textual medical data. In another research38, Fang et al. applied a ResNet architecture and MLP for imaging and clinical data feature extraction. Then, the authors fused the feature vectors by concatenation and fed them into an LSTM network followed by a fully connected network. Hsu et al.17 concatenated the imaging features extracted using Inception-V3 model with the clinical data features before feeding them to fully connected NN. In56, Sharma et al. used CNN to extract image features and then concatenated them directly with the clinical data to feed into a SoftMax classiﬁer. Xu et al.53 used AlexNet architecture to convert the imaging data into a feature vector fusible with other non-image modalities. Then, they jointly learned the non-linear correlations among all modalities using fully connected NN. Out of 10 joint fusion studies, seven studies evaluated their fusion models’ performance against that of a single modality and reported a performance improvement when fusion was used17, 39, 46, 47, 49, 53, 55. Late fusion Late fusion was the least common fusion approach used in the included studies, as only two studies used it. Qiu et al.37 trained three independent imaging models that took a single MRI slice as input, then aggregated the prediction of these models using maximum, mean, and majority voting. After combining the results of these aggregations by majority vote, the study performed late fusion with the clinical data models. In another study40, Huang et al. trained four different late fusion models. Three models took the average of the predicted probabilities from the imaging and EHR modality models as the ﬁnal prediction. The fourth model used an NN classiﬁer as an aggregator, which took as input the single modality models’ prediction. The study also created early, joint fusion models and two single modality models to compare with late fusion performance. As a result, the late fusion outperformed both the early and joint fusion models and the single modality models. Diseases We categorized the diseases and disorders in the included studies into seven types: neurological disorders, cancer, cardiovascular diseases, Covid-19, psychiatric disorders, eye diseases, and other diseases. The majority of the included studies focused on neurological disorders (n = 16). Table 3 shows the distribution of the included studies in terms of the diseases and disorders they covered. Clinical outcomes and machine learning models Multimodal ML enables a wide range of clinical applications such as diagnosis, early prediction, patient stratiﬁcation, phenotyping, biomarkers identiﬁcation, etc. In this review, we labeled each study according to its clinical outcome. We 8/20 Disease Category Neurological disorders Alzheimer’s disease (AD) Mild cognitive impairment (MCI) Ischemic Stroke Demyelinating diseases Neurodevelopmental Deﬁcits Epilepsy Cancer Breast Cancer Glioblastoma Lung Cancer Upper Gastrointestinal (UGI) Cancer Cardiovascular diseases Aortic stenosis Cardiomegaly Myocardial Infarction Psychiatric disorder Bipolar disorder Schizophrenia Eye diseases Diabetic Retinopathy (DR) Glaucoma COVID-19 Other diseases Cervical dysplasia Pulmonary Embolism (PE) Hepatitis B Number of studies 18 7 4 2 1 1 1 5 2 1 1 1 3 1 1 1 2 1 1 2 1 1 3 3 1 1 1 Study reference 12–15, 44, 48, 49 37, 42, 50, 51 35, 47 32 39 34 16, 41 43 55 46 54 55 56 33 36 17 19 18, 25, 38 53 40 52 Table 3. Disease distribution covered by the 34 studies. categorized the retrieved clinical tasks into two main categories: diagnosis and prediction. Though some of the studies mentioned detection, classiﬁcation, diagnosis, and prediction, we categorized them under the diagnosis category. Under the early prediction group, we considered only the studies that predict diseases before onset, identify signiﬁcant risk factors, predict mortality and overall survival, and predict a treatment outcome. These clinical outcomes were implemented using multimodal ML models. This section summarizes the different clinical tasks of the retrieved studies, the fusion strategy used, and the ML models that were developed for each task. Figure 3 shows the distribution of fusion strategies associated with different diseases’ and clinical outcomes. Diagnosis The most common applied clinical outcome in the included studies was the diagnosis, reported in 20 (∼ 59%) studies. In these studies, EHRs were combined with medical imaging to diagnose a spectrum of diseases including neurological disorders (n = 9)4, 13–15, 32, 37, 42, 49, 50, psychiatric disorders (n = 2)33, 36, CVD (n =3)54–56, Cancer (n = 2)16, 55, and four studies for other different diseases18, 19, 40, 53. Speciﬁcally, most of the studies that focused on detecting neurological diseases were for AD (n = 4)13–15, 49, and MCI (n = 4)37, 42, 50, 51. Early fusion was the most utilized technique for diagnosis purposes used in 13 studies. These studies employed different ML models on the fused imaging and EHR data for diagnosing different diseases. Most of these studies were for diagnosing neurological and and psychiatric disorders such as AD13–15, MCI42, 50, 51, demyelinating diseases32, bipolar disorder33, and schizophrenia36 . Parvathy et al.13 reported diagnosing AD by fusing sMRI and PET imaging features with mini-mental state examination (MMSE) score, clinical dementia rating (CDR), and age of the subjects. They fed the fused features vector to different ML models, including support vector machine (SVM), random forest (RF), and gaussian process (GP) for classiﬁcation. Niyas et al.14 classiﬁed AD by fusing MRI, PET, demographic data, and lab tests, including cognitive tests and Cerebro-Spinal Fluid (CSF) test. They applied dynamic ensemble of classiﬁers selection algorithms using a different pool of classiﬁers on the fused features for classiﬁcation. Hamid et al.15 combined MRI and PET imaging features with personal information and 9/20 neurological data such as MMSE and CRF features for AD early diagnosis. In their study, they fed the fused features into SVM for classiﬁcation. For MCI diagnosis, Matteo et al.42 proposed combining MRI imaging with cognitive assessments for MCI diagnosis. They concatenated the features of both modalities and fed them into a linear and quadratic discriminant analysis algorithm for diagnosis. Parisa et al.50, 51 integrated features extracted from MRI and PET images with neuropsychological tests and demographic data (gender, age, and education) to diagnose MCI early. They trained SVM and deep NNs using the fused features for classiﬁcation in50 and51, respectively. In another study32, Xin et al. combined MRI imaging with structured data extracted from EHRs to diagnose demyelinating diseases using SVM. For bipolar disorder, Rashmin et al.33 combined multimodal imaging features with neuropsychological tests and personal information features. They fed them into SVM to differentiate bipolar patients from healthy patients. Ebdrup et al.36 proposed integrating MRI and diffusion tensor imaging tractography (DTI) imaging with neurocognitive tests and clinical data for schizophrenia classiﬁcation. Then, they fused the features of the two modalities and fed them to different types of ML classiﬁers, including SVM, RF, linear regression (LR), decision tree (DT), and Naïve Bayes (NB) for classiﬁcation. Moreover, two studies implemented multimodality early fusion to diagnose different cancer diseases16, 55. Yan et al.16 fused pathological images and structured data extracted from EHRs to classify malignant and benign breast cancer. Then, they fused the features of the two modalities and fed them to two fully connected NN followed by a SoftMax layer for classiﬁcation. Seung et al.55 combined PET imaging with clinical and demographic data for differentiating lung adenocarcinoma (ADC) from squamous cell carcinoma. They fed the integrated features into different algorithms such as SVM, RF, LR, NB, and artiﬁcial neural network (ANN) for classiﬁcation. For COVID-19 diagnosis, Ming et al.18 combined CT images with clinical features and fed them into different ML models, including SVM, RF, and KNN for diagnosis. Finally, Tanveer et al.54 combined features from echocardiogram reports and images, with diagnosis information for the detection of patients with aortic stenosis CVD. Their study fed the combined features to an RF learning framework to detect patients likely to have the disease. Joint fusion was used for diagnostic purposes in 5 studies19, 49, 53, 55, 56. These studies employed different types of DL architectures to learn and fuse the imaging and EHR data for diagnosis purposes. In19, they proposed a Bayesian deep multisource learning (BDMSL) model that integrated retinal images with medical indicators data to diagnose glaucoma. For this model, they used Bayesian CNN encoder-decoder to extract imaging features and a Bayesian MLP encoder-decoder to process the medical indicators data. The two feature vectors were directly concatenated and fed into Bayesian MLP for classiﬁcation. Chen et al.49 used DL for multimodal feature extraction and classiﬁcation to detect AD; the authors used the VGG-16 model to extract features from MRI images and a bidirectional LSTM network with an attention layer to learn features from MRI reports. Then, they fed the fused features into a stacked KNN pooling layer to classify the patient’s diagnosis data. In53, Xu et al. proposed an end-to-end deep multimodal framework that can learn better complementary features from the image and non-image modalities for cervical dysplasia diagnosis. They used CNN, speciﬁcally AlexNet architecture, to convert the cervigram image data into a feature vector fusible with other non-image modalities. After that, they jointly learned the non-linear correlations among all modalities using fully connected NN for cervical dysplasia classiﬁcation. Another two studies55, 56 also employed DL models to jointly learn multimodal feature representation for diagnosing CVDs. The former55 proposed a multimodal network for cardiomegaly classiﬁcation, which simultaneously integrates the non-imaging intensive care unit (ICU) data (laboratory values, vital sign values, and static patient metadata, including demographics) and the imaging data (chest X-ray). They used a ResNet50 architecture to extract features from the X-ray images and fully connected NN to process the ICU data. To join the learned imaging and non-imaging features, they concatenated the learned feature representation and fed them into two fully connected layers to generate a label for cardiomegaly diagnosis. The latter study56 proposed a stacked multimodal architecture called SM2N2, which integrated clinical information and MRI images. In their research, they used CNN to extract imaging features, and then they concatenated these features with clinical data to feed into a SoftMax classiﬁer for myocardial infarction detection. Late fusion was implemented in 2 studies37, 40 for disease diagnosis purposes. Fang et al.37 proposed the fusion of MRI scans, logical memory (LM) tests, and MMSE for MCI classiﬁcation. Their study utilized VGG-11 architecture for MRI feature extraction and developed two MLP models for MMSE and LM test results. Then, they combined both MRI and MLP models using majority voting. As a result, the fusion model outperformed the individual models. Huang et al.40 utilized a non-open dataset comprising CT scans and EHR data to train two unimodal and four late fusion models for PE diagnosis. They used their previously implemented architecture (PENet)57 to encode the CT images and a feedforward network to encode the tabular data. The late fusion approach performed best among the fusion models and outperformed the models trained on the image-only and the tabular-only data. Early Prediction Prediction tasks were reported in 14 (∼ 41.2%) studies. In these studies, EHRs were fused with medical imaging to predict different outcomes, including disease prediction, mortality prediction, survival prediction, and treatment outcome prediction. Ten studies of the prediction tasks were disease prediction12, 17, 34, 38, 39, 41, 44, 46, 48, 52, which involved determining whether an individual might develop a given disease in the future. The second most common prediction task was treatment outcome 10/20 prediction reported in 2 studies35, 47, followed by one study for mortality prediction and overall survival prediction25, 43, respectively. The early fusion technique was used in 6 studies12, 34, 41, 44, 48, 52 for disease prediction. Minhas et al.12 proposed an early fusion model to predict which subjects will progress from MCI to AD in the future. The study concatenated MRI extracted features with demographic and neuropsychological biomarkers before feeding them to an SVM model for prediction. Ali et al.34 proposed a model to predict Epileptogenic-Zone in the Temporal Lobe by feeding MRI extracted features integrated with set-of-semiology features into various ML models such as LR, SVM, and Gradient Boosting. Ma et al.41 fused MRI and clinicopathological features for predicting metachronous distant metastasis (DM) in breast cancer. They fed the concatenated features to an LR model. Another study44 combined MRI-derived features and high-throughput brain phenotyping to diagnose and predict the onset of AD. They fed the fused features into different ML classiﬁers, including RF, SVM, and LR. Ulyana et al.48 trained a deep, fully connected network as a regressor in a 5-year longitudinal study on AD to predict cognitive test scores at multiple future time points. Their model produced MMSE scores for ten unique future time points at six-month intervals by combing biomarkers from cognitive test scores, PET, and MRI. They early fused imaging features with the cognitive test scores through concatenation before feeding them into the fully connected network. Finally, Bai et al.52 compared different multimodal biomarkers (clinical data, biochemical and hemologic parameters, and ultrasound elastography parameters) for predicting the assessment of ﬁbrosis in chronic hepatitis B using SVM. For disease prediction, joint fusion was used in 4 studies17, 38, 39, 46. Hsu et al.17 proposed a deep multimodal fusion model that trained heterogeneous data from fundus images and non-image data for DR screening. They concatenated the imaging extracted features from Inception-V3 with the clinical data features before feeding them to fully connected NN followed by SoftMax layer for classiﬁcation. Fang et al.38 developed a prediction system by jointly fusing CT scans and clinical data to predict the progression of COVID-19 malignancy. In their study, the feature extraction part applied a ResNet architecture and MLP for CT and clinical data, respectively. Then, they concatenated the different features and fed them into an LSTM network followed by a fully connected NN for prediction. In39, the authors proposed a deep multimodal model for predicting neurodevelopmental deﬁcits at 2 years of age. Their model consisted of a feature extractor and fusion classiﬁer. In the feature extractor, they used VGG-19 architecture to extract MRI features and fully connected NN for clinical data. Then, the study combined the extracted features of the two modalities and fed their combination to another fully connected network in the fusion classiﬁer for prediction. To evaluate the performance of the modality fusion, they tested their model using a single modality of MRI and clinical features. The results showed that multimodal fusion outperformed the single modality performance. Another study46 also used multimodal joint fusion for UGI cancer screening. Their model integrated features extracted from UGI endoscopic images with corresponding textual medical data. They applied CNN for image feature extraction and word embeddings (Word2vec) with self-attention for textual medical data feature extraction. After that, they concatenated the extracted features of the two modalities and fed them into fully connected NN for prediction. Their results showed that multimodal fusion outperformed the single modality performance. For treatment outcome prediction35, 47, the former35 implemented early fusion while the latter47 used joint fusion. For acute ischemic stroke, Gianluca et al.35 evaluated the predictive power of imaging, clinical, and angiographic features to predict the outcome of acute ischemic stroke using ML. The study early fused all features into gradient boosting classiﬁers for prediction. In47, the authors proposed a DL model to directly exploit multimodal data (clinical metadata and non-contrast CT (NCCT) imaging data) to predict the success of endovascular treatment for ischemic stroke. They utilized CNN with a self-attention mechanism to extract the features of images, and then they concatenated them with the metadata information. Then, the classiﬁcation stage of the proposed model processed the fused features through a fully connected NN, followed by the Softmax function applied to the outputs. Their results showed that multimodal fusion outperformed the single modality performance. Both the mortality and overall survival prediction studies25, 43 implemented early fusion. In25, they developed a model to predict COVID-19 ventilatory support and mortality early on to prioritize patients and manage the hospital resources’ allocation. They fused patients’ CT images and EHR data features by concatenation before feeding them to different ML models, including SVM, RF, LR, and eXtreme gradient boosting. They evaluated the performance against single modality models and observed that the results for multimodal fusion were better. The other study43 aimed to develop ML models to predict glioblastoma patients’ overall survival (OS) and progression-free survival (PFS) based on combining treatment features, pathological, clinical, PET/CT-derived information, and semantic MRI-based features. They concatenated the features of all modalities and fed them to an RF model. The study showed that the model based on multimodal fusion data outperformed the single modality models. Datasets Patient Data Types The included studies reported medical imaging and EHRs (structured and non-structured) patient’s data types. In terms of imaging modality, CT, MRI, fMRI, structural MRI (sMRI), PET, Diffusion MRI, DTI, ultrasound, X-ray, fundus images, and PET were used in the studies. MRI and PET images were the most utilized modalities. Out of the included 34 studies, 13 used 11/20 Figure 3. Fusion strategies associated with clinical outcomes for different diseases. 12/20 MRI images, and 8 used PET images mostly for AD diagnosis and prediction. In terms of EHRs, structured data was the most commonly used modality (n = 32). Table 4 summarizes the types of imaging and EHR data used in the studies. Data Type Imaging Data MRI imaging MRI DTI fMRI sMRI and Diffusion MRI PET CT X-ray fundus images Ultrasound Echocardiography Pathological images Cervigram images Endoscopy images EHR Data Structured Unstructured Number of studies Study reference 13 3 2 1 8 7 2 2 1 1 1 1 1 32 2 12, 14, 15, 32, 33, 37, 41–43, 48–51 33, 36, 39 33, 39 44 13–15, 43, 45, 48, 50, 51 18, 35, 38, 40, 43, 45, 47 25, 55 17, 19 52 54 16 53 46 12–19, 25, 32–45, 47, 48, 50–56 46, 49 Table 4. Patient data types used in the included studies. Patient data Resources Almost two-thirds of the studies included in this scoping review used private data sources (clinical data that are not publicly available) (n = 21, ∼ 59%). In contrast, publicly accessible datasets were used in only 13 studies. We observed that the most used public dataset was the “Alzheimer’s Disease Neuroimaging Initiative” dataset (ADNI)58 , where 7 out of 13 studies used the dataset. Other publicly available datasets that were used among the included studies were the “National Alzheimer’s Coordinating Center” (NACC) dataset59, the “Medical Information Mart for Intensive Care” (MIMIC-IV) dataset60, the ""National Cancer Institute"" (NCI) dataset, ADNI TADPOLE dataset61 , and MR CLEAN Trial dataset62. In Table 5, we summarize the public multimodal medical datasets and their clinical applications. Considering these datasets for each clinical task, the most popular is ADNI for AD and MCI disease diagnosis and prediction. Evaluation metrics Evaluation metrics are mainly dependent on the clinical task. Typically, accuracy, the area under the curve (AUC), sensitivity, speciﬁcity, F1- measure, and precision are mostly used for the evaluation of diagnosis and prediction tasks. Table 6 shows the distribution of the evaluation measures used in the included studies Discussion This section summarizes our ﬁndings and provides future directions for research on the multimodal fusion of medical imaging and EHR. Principal ﬁndings We found that multimodal models that combined EHR and medical imaging data generally outperformed single modality models for the same task in disease diagnosis or prediction. Since our review shows that the fusion of medical imaging and clinical context data can improve the performance of AI models, we recommend attempting fusion approaches when multimodal data is obtainable. Moreover, through this review, we observed certain trends in the ﬁeld of multimodality fusion in the medical area, which can be categorized as: • Resources: We observed that multimodal data resources of medical imaging and EHR are limited owing to privacy considerations. The most prominent dataset was the ADNI, containing MRI and PET images collected from about 1700 individuals in addition to clinical and genetic information. Considering ADNI’s contributions in advancing the research, similar multimodal datasets should be developed for other medical data sources too. 13/20 Description URL Clinical outcomes Study refer- ence 13, 15 Disease diagnosis (AD) Disease diagnosis (MCI) Disease Prediction (AD) 50, 51 12, 44, 48 Disease diagnosis (AD) 14 https://adni. loni.usc.edu/ data-samples/ data-types/ https://tadpole. grand-challenge. org/Data/ Public Dataset ADNI ADNI TAD- POLE NACC MIMIC-CXR, MIMIC-IV NCI MR CLEAN Trial ADNI represents a series of studies, including ADNI 1, 2, and 3, designed to study MCI and its progression into AD. It has MRI and PET im- ages along with clinical and genetic information58. ADNI has a simpliﬁed coun- terpart, TADPOLE, which has a subset of ADNI-3 sam- ples and features.ATDPOLE does not include raw images, but it has processed struc- tural information about the images such as ROI averages, thicknesses of the cortex and volumes of brain sub-regions, etc61. The NACC dataset was es- tablished to facilitate collab- orative AD research. The dataset comprises MRI data, demographic data, neuropsy- chological testing scores, and clinical diagnosis of pa- tients59. MIMIC-CXR is a dataset of patient chest radiographs. It contains X-ray studies for 64,588 patients63. MIMIC-IV is a database for patients admitted to critical care units comprising patient stay information, patient’s ICU data, and lookup tables to allow linking to MIMIC- CXR60. Data collections produced by major NCI initiatives are listed in the NCI Data Cata- log, including Clinical data, Genomics, imaging, and Pro- teomics. A longitudinal study of 500 patients treated with en- dovascular therapy in The Netherlands for acute is- chemic stroke comprising NCCT images, CT Angiogra- phy (CTA) images, and clini- cal metadata information on its patients62. https:// naccdata.org/ requesting-data/ nacc-data Disease Diagnosis (MCI) 37 MIMIC-CXR:https: //www.nature. com/articles/ s41597-019-0322-0 MIMIC-IV:https: //physionet.org/ content/mimiciv/0. 4/ https:// datascience.cancer. gov/resources/ nci-data-catalog Disease diomegaly) diagnosis (Car- 55 Disease diagnosis (Cervical dysplasia) 53 https://www. mrclean-trial.org/ home.html Treatment outcome predic- tion (ischemic stroke) 47 Table 5. Multimodal medical datasets and clinical outcome applications. 14/20 12–15, 17, 19, 32–34, 37–41, 45, 46, 50, 51, 53, 54 Study Reference 12, 15–19, 25, 32–42, 44–47, 49–56 Number of studies 31 20 17 15 7 3 Evaluation metrics Accuracy Sensitivity (recall) AUC Speciﬁcity Precision Positive predictive value (PPV) and Nega- tive predictive value (NPV) Matthews correlation coefﬁcient (MCC) C-index Root-Mean Squared Error (RMSE) The numbers in the second column do not sum up to 34 as many studies used more than a single metric. 12, 14, 15, 17, 32–34, 38–41, 46, 50, 51, 53 13, 19, 34, 37, 45, 54 15, 34, 40 2 1 1 34, 41 43 48 12, 15–17, 19, 25, 32, 35, 37–39, 41, 45, 47, 52, 53, 55 Table 6. The distribution of evaluation metrics in the included studies. • Fusion implementation: Early fusion was the most commonly used technique in most applications for multimodal learning. Before fusing 1D EHRs data with image data in 2D or 3D, images data was converted to a 1D vector by extracting high-level representations using manual or software-generated features12, 13, 15, 25, 33–36, 39, 41–45, 50–53, or CNN- extracted features8, 16, 54. The learned imaging features from CNN often resulted in better task-speciﬁc performance than manually or software-derived features64. Based on this reviewed studies, early fusion models performed better than conventional single-modality models on the same task. Researchers can use the early fusion method as a ﬁrst attempt to learn multimodal representations since it can learn to exploit the interactions and correlations between features of each modality. Furthermore, it only requires one model to be trained, making the pipeline for training easier than that of joint and late fusion. However, if imaging features are extracted with CNN, early fusion requires multiple models to be trained. Joint fusion was the second most commonly used fusion approach. From a modality perspective, CNNs appeared to be the best option for image feature extraction. Tabular data were mainly processed using dense layers when fed into a model, while text data were mostly processed using LSTM layers followed by the attention layer. Most of the current research directly concatenated the feature vectors of the different modalities to combine multimodal data. Using NNs to implement joint fusion can be a limitation when dealing with small datasets, which means that joint fusion is preferred with large datasets. For small datasets, it is preferable to use early or late fusion methods as they can be implemented using classical ML techniques. Nevertheless, we expect and agree with26 that joint fusion models can provide better results than other fusion strategies because they update their feature representations iteratively by propagating the loss to all the feature extraction models, aiming to learn correlations across modalities. Based on the performance reported in the included studies, it is preferred to try the early and joint fusion when the relation between the two data modalities is complementary. In this review, AD diagnosis is an example in which imaging and EHRs data are dependent as relevant and accurate knowledge of the patient’s current symptomatology, personal information and imaging reports can help doctors interpret imaging results in a suitable clinical context, resulting in a more precise diagnosis. Therefore, all AD diagnosis studies in this review implemented either early fusion13–15 or joint fusion49 for multimodal learning. On the other hand, it is preferred to try late fusion when input modalities do not complement each other. For example, the brain MRI pixel data and the quantitative result of an MMSE (e.g., Qiu et al.37) for diagnosing MCI are independent, making them appropriate candidates for inclusion in the late fusion strategy. Also, late fusion does not impose the requirement of a huge amount of training data, so it could be used when the modalities data sizes are small. Moreover, late fusion strategy could be attempted when the concatenation of feature vectors from multiple modalities results in high-dimensional vectors that are difﬁcult for ML algorithms to learn without overﬁtting unless many input samples are available. In late fusion, multiple models are employed, each specialized in a single modality, thereby limiting the size of the input feature vector for each model. Furthermore, late fusion could be used when data is incomplete or missing, i.e., some patients have only imaging data but no clinical data or vice versa. This is because late fusion uses independent models for different modalities, and aggregation methods like averaging and majority voting can be used even when predictions from a modality are not present. Moreover, predictions could be disproportionately inﬂuenced by the most feature-rich input modality when the number of features is very different between the input data modalities65; in this scenario, late fusion is preferable because it allows training each model using each modality separately. • Applications: In this review, we found that AD diagnosis and prediction12–15, 44, 48, 49 were the most common applications 15/20 addressed in a multimodal setting among studies. Using ML fusion techniques consistently demonstrated improved AD diagnosis, while clinicians experience difﬁculty with accurate and reliable diagnosis even when multimodal data is available26. This emphasizes the utility and signiﬁcance of multimodal fusion approaches in clinical applications. • Prospects: In this review, we noted that multimodal medical data fusion is growing due to its potential in achieving state-of-the-art performance for healthcare applications. Nonetheless, this growth is hampered by the absence of adequate data for benchmarking methods. This is not surprising, given the privacy concerns surrounding revealing healthcare data. Moreover, we observed a lack of complexity in the used non-imaging data, particularly in the context of heavily feature-rich data included in the EHR. For example, the majority of studies focused mostly on basic demographic data like gender and age12, 15, 44, 51, a limited number of studies also included medical histories such as smoking status and hypertension18, 55 or speciﬁc clinical characteristics that are known to be associated with a certain disease, such as an MMSE for diagnosing AD. In addition to selecting the disease-associated features, future research may beneﬁt from using vast amounts of feature-rich data, as demonstrated in domains outside of medicine, such as autonomous driving66. Future directions Although we focus on EHR and medical imaging as multimodal data, other modalities such as multi-omics and environmental data could also be integrated using the aforementioned fusion approaches. As the causes of many diseases are complex, many factors, including inherited genetics, lifestyle, and living environments, contribute to the development of diseases. Therefore, combining multisource data, e.g. EHR, imaging, and multi-omics data, may lead to a holistic view that can improve patient outcomes through personalized medicine. Although we focus on EHR and medical imaging as multimodal data, other modalities such as multi-omics and environmen- tal data could also be integrated using the aforementioned fusion approaches. As the causes of many diseases are complex, many factors, including inherited genetics, lifestyle, and living environments, contribute to the development of diseases. Therefore, combining multisource data, e.g. EHR, imaging, and multi-omics data, may lead to a holistic view that can improve patient outcomes through personalized medicine. Moreover, the unavailability of multimodal public data is a limitation that hinders the development of corresponding research. Many factors (e.g., gender, ethnicity, environmental factors) could inﬂuence the research directions or even clinical decision, relying on a few publicly available datasets might not be enough for making conclusive clinical claims to the global population27. Consequently, it is imperative to encourage the sharing of ﬂexible data among institutions and hospitals in order to facilitate the exploration of a wider range of population data for clinical research. In ML, federated learning (FL)67, 68 provides the ability to collect data safely and securely from multiple centers. It may be used to collect multimodal data from various centers to train a large-scale model without collecting data directly. Limitations Our search was limited to studies published within the previous seven years (2015-2022). We only considered studies published in English, which may have led to leaving out some studies published in other languages. We solely included studies fusing EHR with medical imaging. We did not include studies that used other data modalities such as multi-omics data, as they are out of the scope of this work. Because positive results are typically reported disproportionately, publication bias might be another limitation of this review. This bias may result in an overestimation of the beneﬁts associated with multimodal data analysis. The studies included in this review employed various input modalities, investigated various clinical tasks for different diseases, and reported different performance metrics; hence a direct comparison of the results presented in the studies is not always applicable. Furthermore, not all articles provided conﬁdence bounds, making it difﬁcult to compare their results statistically. Conclusion Multimodal ML is an area of research that is gaining attention within the medical ﬁeld. This review surveyed multimodal medical ML literature that combines EHR with medical imaging data. It discussed fusion strategies, the clinical tasks and ML models that implemented data fusion, the type of diseases, and the publicly accessible multimodal data for medical imaging and EHRs. Furthermore, it highlighted some directions to pave the way for future research. Our ﬁnding suggests that there is a growing interest in multimodal medical data. Still, most studies combine the modalities with relatively simple strategies, which despite being shown to be effective, might not fully exploit the rich information embedded in these modalities. As this is a fast-growing ﬁeld and new AI models with multimodal data are constantly being developed, there might exist studies that fall outside our deﬁnition of fusion strategies or use a combination of these strategies. We believe that the development of this ﬁeld will give rise to more comprehensive multimodal medical data analysis and will be of great support to the clinical decision-making process. 16/20 Data availability The data generated during this scoping review is provided as supplementary materials. References 1. Murdoch, T. B. & Detsky, A. S. The inevitable application of big data to health care. Jama 309, 1351–1352 (2013). 2. Obermeyer, Z. & Emanuel, E. J. Predicting the future—big data, machine learning, and clinical medicine. The New Engl. journal medicine 375, 1216 (2016). 3. Roski, J., Bo-Linn, G. W. & Andrews, T. A. Creating value in health care through big data: opportunities and policy implications. Heal. affairs 33, 1115–1122 (2014). 4. Lozano-Perez, T. Autonomous robot vehicles (Springer Science & Business Media, 2012). 5. Castanedo, F. A review of data fusion techniques. The scientiﬁc world journal 2013 (2013). 6. Cohen, M. D. Accuracy of information on imaging requisitions: does it matter? J. Am. Coll. Radiol. 4, 617–621 (2007). 7. Comfere, N. I. et al. Provider-to-provider communication in dermatology and implications of missing clinical information in skin biopsy requisition forms: a systematic review. Int. journal dermatology 53, 549–557 (2014). 8. Jonas, J. B. et al. Glaucoma. The Lancet 390, 2183–2193, DOI: https://doi.org/10.1016/S0140-6736(17)31469-1 (2017). 9. Comfere, N. I. et al. Dermatopathologists’ concerns and challenges with clinical information in the skin biopsy requisition form: a mixed-methods study. J. cutaneous pathology 42, 333–345 (2015). 10. Li, Y., Wu, F.-X. & Ngom, A. A review on machine learning principles for multi-view biological data integration. Brieﬁngs bioinformatics 19, 325–340 (2018). 11. Ramachandram, D. & Taylor, G. W. Deep multimodal learning: A survey on recent advances and trends. IEEE signal processing magazine 34, 96–108 (2017). 12. Minhas, S. et al. Early mci-to-ad conversion prediction using future value forecasting of multimodal features. Comput. intelligence neuroscience 2021 (2021). 13. Pillai, P. S., Leong, T.-Y., Initiative, A. D. N. et al. Fusing heterogeneous data for alzheimer’s disease classiﬁcation. In MEDINFO 2015: eHealth-enabled Health, 731–735 (IOS Press, 2015). 14. KP, M. N. & Thiyagarajan, P. Alzheimer’s classiﬁcation using dynamic ensemble of classiﬁers selection algorithms: A performance analysis. Biomed. Signal Process. Control. 68, 102729 (2021). 15. Akramifard, H., Balafar, M. A., Razavi, S. N. & Ramli, A. R. Early detection of alzheimer’s disease based on clinical trials, three-dimensional imaging data, and personal information using autoencoders. J. medical signals sensors 11, 120 (2021). 16. Yan, R. et al. Richer fusion network for breast cancer classiﬁcation based on multimodal data. BMC Med. Informatics Decis. Mak. 21, 1–15 (2021). 17. Hsu, M.-Y. et al. Deep learning for automated diabetic retinopathy screening fused with heterogeneous data from ehrs can lead to earlier referral decisions. Transl. Vis. Sci. & Technol. 10, 18–18 (2021). 18. Xu, M. et al. Accurately differentiating between patients with covid-19, patients with other viral infections, and healthy individuals: multimodal late fusion learning approach. J. Med. Internet Res. 23, e25535 (2021). 19. Chai, Y., Bian, Y., Liu, H., Li, J. & Xu, J. Glaucoma diagnosis in the chinese context: An uncertainty information-centric bayesian deep learning model. Inf. Process. & Manag. 58, 102454 (2021). 20. Azam, M. A. et al. A review on multimodal medical image fusion: Compendious analysis of medical modalities, multimodal databases, fusion techniques and quality metrics. Comput. Biol. Medicine 144, 105253, DOI: https://doi.org/ 10.1016/j.compbiomed.2022.105253 (2022). 21. Zhang, Y.-D. et al. Advances in multimodal data fusion in neuroimaging: Overview, challenges, and novel orientation. Inf. Fusion 64, 149–187, DOI: https://doi.org/10.1016/j.inffus.2020.07.006 (2020). 22. Behrad, F. & Saniee Abadeh, M. An overview of deep learning methods for multimodal medical data mining. Expert. Syst. with Appl. 200, 117006, DOI: https://doi.org/10.1016/j.eswa.2022.117006 (2022). 23. Stahlschmidt, S. R., Ulfenborg, B. & Synnergren, J. Multimodal deep learning for biomedical data fusion: a review. Brieﬁngs Bioinforma. 23 (2022). 17/20 24. Muhammad, G. et al. A comprehensive survey on multimodal medical signals fusion for smart healthcare systems. Inf. Fusion 76, 355–375, DOI: https://doi.org/10.1016/j.inffus.2021.06.007 (2021). 25. Aljouie, A. F. et al. Early prediction of covid-19 ventilation requirement and mortality from routinely collected baseline chest radiographs, laboratory, and clinical data with machine learning. J. multidisciplinary healthcare 14, 2017 (2021). 26. Huang, S.-C., Pareek, A., Seyyedi, S., Banerjee, I. & Lungren, M. P. Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines. NPJ digital medicine 3, 1–9 (2020). 27. Liu, Z. et al. Machine learning for multimodal electronic health records-based research: Challenges and perspectives. arXiv preprint arXiv:2111.04898 (2021). 28. Tricco, A. C. et al. Prisma extension for scoping reviews (prisma-scr): checklist and explanation. Annals internal medicine 169, 467–473 (2018). 29. Ouzzani, M., Hammady, H., Fedorowicz, Z. & Elmagarmid, A. Rayyan—a web and mobile app for systematic reviews. Syst. reviews 5, 1–10 (2016). 30. Arksey, H. & O’Malley, L. Scoping studies: towards a methodological framework. Int. journal social research methodology 8, 19–32 (2005). 31. Grant, M. J. & Booth, A. A typology of reviews: an analysis of 14 review types and associated methodologies. Heal. information & libraries journal 26, 91–108 (2009). 32. Xin, B., Huang, J., Zhou, Y., Lu, J. & Wang, X. Interpretation on deep multimodal fusion for diagnostic classiﬁcation. In 2021 International Joint Conference on Neural Networks (IJCNN), 1–8 (IEEE, 2021). 33. Achalia, R. et al. A proof of concept machine learning analysis using multimodal neuroimaging and neurocognitive measures as predictive biomarker in bipolar disorder. Asian J. Psychiatry 50, 101984 (2020). 34. Alim-Marvasti, A. et al. Machine learning for localizing epileptogenic-zone in the temporal lobe: Quantifying the value of multimodal clinical-semiology and imaging concordance. Front. digital health 3, 8 (2021). 35. Brugnara, G. et al. Multimodal predictive modeling of endovascular treatment outcome for acute ischemic stroke using machine-learning. Stroke 51, 3541–3551 (2020). 36. Ebdrup, B. H. et al. Accuracy of diagnostic classiﬁcation algorithms using cognitive-, electrophysiological-, and neu- roanatomical data in antipsychotic-naïve schizophrenia patients. Psychol. medicine 49, 2754–2763 (2019). 37. Qiu, S. et al. Fusion of deep learning models of mri scans, mini–mental state examination, and logical memory test enhances diagnosis of mild cognitive impairment. Alzheimer’s & Dementia: Diagn. Assess. & Dis. Monit. 10, 737–749 (2018). 38. Fang, C. et al. Deep learning for predicting covid-19 malignant progression. Med. image analysis 72, 102096 (2021). 39. He, L. et al. Deep multimodal learning from mri and clinical data for early prediction of neurodevelopmental deﬁcits in very preterm infants. Front. neuroscience 15 (2021). 40. Huang, S.-C., Pareek, A., Zamanian, R., Banerjee, I. & Lungren, M. P. Multimodal fusion with deep neural networks for leveraging ct imaging and electronic health record: a case-study in pulmonary embolism detection. Sci. reports 10, 1–9 (2020). 41. Ma, W. et al. Distant metastasis prediction via a multi-feature fusion model in breast cancer. Aging (Albany NY) 12, 18151 (2020). 42. De Marco, M., Beltrachini, L., Biancardi, A., Frangi, A. F. & Venneri, A. Machine-learning support to individual diagnosis of mild cognitive impairment using multimodal mri and cognitive assessments. Alzheimer Dis. & Assoc. Disord. 31, 278–286 (2017). 43. Peeken, J. C. et al. Combining multimodal imaging and treatment features improves machine learning-based prognostic assessment in patients with glioblastoma multiforme. Cancer medicine 8, 128–136 (2019). 44. Wang, Y. et al. Diagnosis and prognosis of alzheimer’s disease using brain morphometry and white matter connectomes. NeuroImage: Clin. 23, 101859 (2019). 45. Hyun, S. H., Ahn, M. S., Koh, Y. W. & Lee, S. J. A machine-learning approach using pet-based radiomics to predict the histological subtypes of lung cancer. Clin. nuclear medicine 44, 956–960 (2019). 46. Ding, S., Huang, H., Li, Z., Liu, X. & Yang, S. Scnet: a novel ugi cancer screening framework based on semantic-level multimodal data fusion. IEEE J. Biomed. Heal. Informatics 25, 143–151 (2020). 18/20 47. Samak, Z. A., Clatworthy, P. & Mirmehdi, M. Prediction of thrombectomy functional outcomes using multimodal data. In Annual Conference on Medical Image Understanding and Analysis, 267–279 (Springer, 2020). 48. Morar, U. et al. A deep-learning approach for the prediction of mini-mental state examination scores in a multimodal longitudinal study. In 2020 International Conference on Computational Science and Computational Intelligence (CSCI), 761–766 (IEEE, 2020). 49. Chen, D., Zhang, L. & Ma, C. A multimodal diagnosis predictive model of alzheimer’s disease with few-shot learning. In 2020 International Conference on Public Health and Data Science (ICPHDS), 273–277, DOI: 10.1109/ICPHDS51617. 2020.00060 (2020). 50. Forouzannezhad, P., Abbaspour, A., Cabrerizo, M. & Adjouadi, M. Early diagnosis of mild cognitive impairment using random forest feature selection. In 2018 IEEE Biomedical Circuits and Systems Conference (BioCAS), 1–4, DOI: 10.1109/BIOCAS.2018.8584773 (2018). 51. Forouzannezhad, P., Abbaspour, A., Li, C., Cabrerizo, M. & Adjouadi, M. A deep neural network approach for early diagnosis of mild cognitive impairment using multiple features. In 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), 1341–1346, DOI: 10.1109/ICMLA.2018.00218 (2018). 52. Bai, Y., Chen, X., Dong, C., Liu, Y. & 0001, Z. Z. A comparison of multimodal biomarkers for chronic hepatitis b assessment using recursive feature elimination. In 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2016, Orlando, FL, USA, August 16-20, 2016, 2448–2451, DOI: 10.1109/EMBC.2016.7591225 (IEEE, 2016). 53. Xu, T., Zhang, H., Huang, X., Zhang, S. & Metaxas, D. N. Multimodal deep learning for cervical dysplasia diagnosis. In Ourselin, S., Joskowicz, L., Sabuncu, M. R., Unal, G. & Wells, W. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016, 115–123 (Springer International Publishing, Cham, 2016). 54. Syeda-Mahmood, T. et al. Identifying patients at risk for aortic stenosis through learning from multimodal data. In Ourselin, S., Joskowicz, L., Sabuncu, M. R., Unal, G. & Wells, W. (eds.) Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016, 238–245 (Springer International Publishing, Cham, 2016). 55. Grant, D., Papie˙z, B. W., Parsons, G., Tarassenko, L. & Mahdi, A. Deep learning classiﬁcation of cardiomegaly using combined imaging and non-imaging icu data. In Papie˙z, B. W., Yaqub, M., Jiao, J., Namburete, A. I. L. & Noble, J. A. (eds.) Medical Image Understanding and Analysis, 547–558 (Springer International Publishing, Cham, 2021). 56. Sharma, R., Eick, C. F. & Tsekos, N. V. Sm2n2: A stacked architecture for multimodal data and its application to myocardial infarction detection. In Puyol Anton, E. et al. (eds.) Statistical Atlases and Computational Models of the Heart. M&Ms and EMIDEC Challenges, 342–350 (Springer International Publishing, Cham, 2021). 57. Huang, S.-C. et al. Penet—a scalable deep-learning model for automated diagnosis of pulmonary embolism using volumetric ct imaging. NPJ digital medicine 3, 1–9 (2020). 58. Mueller, S. et al. The alzheimer’s disease neuroimaging initiative. Neuroimaging Clin. North Am. 15, 869–877, DOI: 10.1016/j.nic.2005.09.008 (2005). 59. Beekly, D. et al. The national alzheimer’s coordinating center (nacc) database: an alzheimer disease database. Alzheimer disease associated disorders 18, 270–277 (2004). 60. Alistair, J. et al. Mimic-iv (version 0.4). PhysioNet DOI: https://doi.org/10.13026/a3wn-hq05 (2020). 61. Marinescu, R. V. et al. Tadpole challenge: prediction of longitudinal evolution in alzheimer’s disease. arXiv preprint arXiv:1805.03909 (2018). 62. Fransen, P. S. et al. Mr clean, a multicenter randomized clinical trial of endovascular treatment for acute ischemic stroke in the netherlands: study protocol for a randomized controlled trial. Trials 15, 1–11 (2014). 63. Johnson, A. E. et al. Mimic-cxr, a de-identiﬁed publicly available database of chest radiographs with free-text reports. Sci. data 6, 1–8 (2019). 64. Goodfellow, I., Bengio, Y. & Courville, A. Deep learning (MIT press, 2016). 65. Reda, I. et al. Deep learning role in early diagnosis of prostate cancer. Technol. cancer research & treatment 17, 1533034618775530 (2018). 66. Hecker, S., Dai, D. & Van Gool, L. End-to-end learning of driving models with surround-view cameras and route planners. In Proceedings of the european conference on computer vision (eccv), 435–453 (2018). 19/20 67. Li, T., Sahu, A. K., Talwalkar, A. & Smith, V. Federated learning: Challenges, methods, and future directions. IEEE Signal Process. Mag. 37, 50–60 (2020). 68. Al, H., Alam, T., Househ, M. & Shah, Z. Federated learning and internet of medical things – opportunities and challenges. In 20th International Conference on Informatics, Management, and Technology in Healthcare (ICIMTH) (2022). Author contributions statement F.M., H.A., Z.S. contributed to conceptualization. F.M. and H.A. administered the project. F.M. curated the data, performed data synthesis, and contributed to writing—original draft. H.A and N.E performed writing—review and editing. Z.S. and H.A. supervised the study. All authors read and approved the ﬁnal manuscript. Additional information Supplementary Methods Competing interests The authors declare that they have no competing interests. 20/20",0.0
Driver Locations Harvesting Attack on pRide,"[{'href': 'http://arxiv.org/abs/2210.13263v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.13263v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-10-24 14:03:36,,"Hindering Adversarial Attacks with Implicit Neural Representations Andrei A. Rusu 1 Dan A. Calian 1 Sven Gowal 1 Raia Hadsell 1 2 2 0 2 t c O 2 2 ] G L . s c [ 1 v 2 8 9 3 1 . 0 1 2 2 : v i X r a Abstract We introduce the Lossy Implicit Network Activa- tion Coding (LINAC) defence, an input transfor- mation which successfully hinders several com- mon adversarial attacks on CIFAR-10 classiﬁers for perturbations up to (cid:15) = 8/255 in L∞ norm and (cid:15) = 0.5 in L2 norm. Implicit neural rep- resentations are used to approximately encode pixel colour intensities in 2D images such that classiﬁers trained on transformed data appear to have robustness to small perturbations without adversarial training or large drops in performance. The seed of the random number generator used to initialise and train the implicit neural represen- tation turns out to be necessary information for stronger generic attacks, suggesting its role as a private key. We devise a Parametric Bypass Ap- proximation (PBA) attack strategy for key-based defences, which successfully invalidates an ex- isting method in this category. Interestingly, our LINAC defence also hinders some transfer and adaptive attacks, including our novel PBA strat- egy. Our results emphasise the importance of a broad range of customised attacks despite appar- ent robustness according to standard evaluations. LINAC source code and parameters of defended classiﬁer evaluated throughout this submission are available: https://github.com/deepmind/linac. 1. Introduction Training Deep Neural Network (DNN) classiﬁers which are accurate yet generally robust to small adversarial perturba- tions is an open problem in computer vision and beyond, inspiring much empirical and foundational research into modern DNNs. Szegedy et al. (2014) showed that DNNs are not inherently robust to imperceptible input perturbations, which reliably cross learned decision boundaries, even those of different models trained on similar data. With hindsight, 1DeepMind, London, UK. Correspondence to: Andrei A. Rusu <andrei@deepmind.com>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). it becomes evident that two related yet distinct design prin- ciples have been at the core of proposed defences ever since. Intuitively, accurate DNN classiﬁers could be considered ro- bust in practice if: (I) their decision boundaries were largely insensitive to all adversarial perturbations, and/or (II) com- puting any successful adversarial perturbations was shown to be expensive, ideally intractable. Early defences built on principle (I) include the adversarial training approach of Madry et al. (2018) and the veriﬁable defences of Hein & Andriushchenko (2017); Raghunathan et al. (2018), with many recent works continually reﬁning such algorithms, e.g. Cohen et al. (2019); Gowal et al. (2020); Rebufﬁ et al. (2021). A wide range of defences were built, or shown to operate, largely on principle (II), including adversarial detection methods (Carlini & Wagner, 2017a), input trans- formations (Guo et al., 2018) and denoising strategies (Liao et al., 2018; Niu et al., 2020). Many such approaches have since been circumvented by more effective attacks, such as those proposed by Carlini & Wagner (2017b), or by using “adaptive attacks” (Athalye et al., 2018; Tramer et al., 2020). Despite the effectiveness of recent attacks against these defences, Garg et al. (2020) convincingly argue on a theo- retical basis that principle (II) is sound; similarly to cryp- tography, robust learning could rely on computational hard- ness, even in cases where small adversarial perturbations do exist and would be found by a hypothetical, computa- tionally unbounded adversary. However, constructing such robust classiﬁers for problems of interest, e.g. image clas- siﬁcation, remains an open problem. Recent works have proposed defences based on cryptographic principles, such as the pseudo-random block pixel shufﬂing approach of AprilPyone & Kiya (2021a). As we will show, employing cryptographic principles in algorithm design is not in it- self enough to prevent efﬁcient attacks. Nevertheless, we build on the concept of key-based input transformation and propose a novel defence based on Implicit Neural Repre- sentations (INRs). We demonstrate that our Lossy Implicit Neural Activation Coding (LINAC) defence hinders most standard and even adaptive attacks, more so than the related approaches we have tested, without making any claims of robustness about our defended classiﬁer. Contributions: (1) We demonstrate empirically that lossy INRs can be used in a standard CIFAR-10 image classiﬁca- tion pipeline if they are computed using the same implicit Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) network initialisation, a novel observation which makes our LINAC defence possible. (2) The seed of the random num- ber generator used for initialising and computing INRs is shown to be an effective and compact private key, since with- holding this information hinders a suite of standard adver- sarial attacks widely used for robustness evaluations. (3) We report our systematic efforts to circumvent the LINAC de- fence with transfer and a series of adaptive attacks, designed to expose and exploit potential weaknesses of LINAC. (4) To the same end we propose the novel Parametric Bypass Ap- proximation (PBA) attack strategy, valid under our threat model, and applicable to other defences using secret keys. We demonstrate its effectiveness by invalidating an existing key-based defence which was previously assumed robust. 2. Related Work Adversarial Robustness. Much progress has been made towards robust image classiﬁers along the adversarial train- ing (Madry et al., 2018) route, which has been extensively explored and is well reviewed, e.g. in (Schott et al., 2019; Pang et al., 2020; Gowal et al., 2020; Rebufﬁ et al., 2021). While such approaches can be effective against current at- tacks, a complementary line of work investigates certiﬁed defences, which offer guarantees of robustness around ex- amples for some well deﬁned sets (Wong & Kolter, 2018; Raghunathan et al., 2018; Cohen et al., 2019). Indeed, many such works acknowledge the need for complementary ap- proaches, irrespective of the success of adversarial training and the well understood difﬁculties in combining methods (He et al., 2017). The proliﬁc work on defences against adversarial perturbations has spurred the development of stronger attacks (Carlini & Wagner, 2017b; Brendel et al., 2018; Andriushchenko et al., 2020) and standardisation of evaluation strategies for threat models of interest (Athalye et al., 2018; Croce & Hein, 2020), including adaptive attacks (Tramer et al., 2020). Alongside the empirical progress to- wards building robust predictors, this line of research has yielded an improved understanding of current deep learn- ing models (Ilyas et al., 2019; Engstrom et al., 2019), the limitations of effective adversarial robustness techniques (Jacobsen et al., 2018), and the data required to train them (Schmidt et al., 2018). Athalye et al. (2018) show that a number of defences pri- marily hinder gradient-based adversarial attacks by obfus- cating gradients. Various forms are identiﬁed, such as gra- dient shattering (Goodfellow et al., 2014), gradient masking (Papernot et al., 2017), exploding and vanishing gradients (Song et al., 2018b), stochastic gradients (Dhillon et al., 2018) and a number of input transformations aimed at coun- tering adversarial examples, including noise ﬁltering ap- proaches using PCA or image quilting (Guo et al., 2018), the Saak transform (Song et al., 2018a), low-pass ﬁltering (Shaham et al., 2018), matrix estimation (Yang et al., 2019) and JPEG compression (Dziugaite et al., 2016; Das et al., 2017; 2018). Indeed, many such defences have been pro- posed, as reviewed by Niu et al. (2020), they have ranked highly in competitions (Kurakin et al., 2018), and many have since been shown to be less robust than previously thought, e.g. by Athalye et al. (2018) and Tramer et al. (2020), who use adaptive attacks to demonstrate that several input transformations offer little to no robustness. To build on such insights, it is worth identifying the “ingre- dients” essential to the success of adversarial attacks. Most effective attacks, including adaptive ones, assume the ability to approximate the outputs of the targeted model for arbi- trary inputs. This is reasonable when applying the correct transformation is tractable for the attacker. Hence, deny- ing access to such computations seems to be a promising direction for hindering adversarial attacks. AprilPyone & Kiya (2020; 2021b); MaungMaung & Kiya (2021) borrow standard practice from cryptography and assume that an attacker has full knowledge of the defence’s algorithm and parameters, short of a small number of bits which make up a private key. Another critical “weakness” of such in- put denoising defences is that they can be approximated by the identity mapping for the purpose of computing gra- dients (Athalye et al., 2018). Even complex parametric approaches, which learn stochastic generative models of the input distribution, are susceptible to reparameterisation and Expectation-over-Transformation (EoT) attacks in the white-box setting. Thus, it is worth investigating whether non-parametric, lossy and fully deterministic input transfor- mations exist such that downstream models can still perform tasks of interest to high accuracy, while known and novel attack strategies are either ruled out, or at least substantially hindered, including adaptive attacks. Implicit Neural Representations. Neural networks have been used to parameterise many kinds of signals, see the work by Sitzmann (2020) for an extensive list, with remark- able recent advances in scene representations (Mildenhall et al., 2020) and image processing (Sitzmann et al., 2020). INRs have been used in isolation per image or scene, not for generalisation across images. Some exceptions exist in unsupervised learning, e.g. Skorokhodov et al. (2021) parameterise GAN decoders such that they directly output INRs of images, rather than colour intensities for all pixels. In this paper we show that INRs can be used to discover functional decompositions of RGB images which enable comparable generalisation to learning on the original signal encoding (i.e. RGB). Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 1. Visual depiction of LINAC, our proposed input transformation. An RGB image x is converted into an Activation Image t(x) with identical spatial dimensions, but H channels instead of 3. A neural network model which maps pixel coordinates to RGB colour intensities is ﬁt such that it approximates x. The resulting model parameters (after ﬁtting) are called the Implicit Neural Representation (INR) of image x. In order to output correct RGB colour intensities for all pixels, the implicit neural network needs to compute a hierarchical functional decomposition of x. We empirically choose an intermediate representation to deﬁne our transformation. Activations in the middle hidden layer are associated with their corresponding pixel coordinates to form the output Activation Image t(x), with as many channels as there are units in the middle layer (H). 3. Hindering Adversarial Attacks with Implicit Neural Representations In this section we introduce LINAC, our proposed input transformation which hinders adversarial attacks by leverag- ing implicit neural representations, also illustrated in Fig. 1. Setup. We consider a supervised learning task with a dataset D ⊂ X × Y of pairs of images x and their correspond- ing labels y. We use a deterministic input transformation t : X → H which transforms input images, x (cid:55)→ t(x), while preserving their spatial dimensions. Further, we consider a classiﬁer fθ, parameterised by θ, whose parameters are estimated by Empirical Risk Minimisation (ERM) to map transformed inputs to labels fθ : H → Y. The model is not adversarially trained, yet ﬁnding adversarial examples for it is hindered by LINAC, as we demonstrate through extensive evaluations in Section 5. Implicit Neural Representations. For an image x, its im- plicit neural representation is given by a multi-layer per- ceptron (MLP) Φ = hL ◦ hL−1 ◦ · · · ◦ h0, Φ : R2 → R3, with L hidden layers, which maps spatial coordinates to their corresponding colours. Φφ is a solution to the implicit equation: Φ(p) − x(p) = 0, (1) where p are spatial coordinates (i.e. pixel locations) and x(p) are the corresponding image colours. Our input trans- formation leverages this implicit neural representation to encode images in an approximate manner. Reconstruction Loss. The implicit equation (1) can be translated (Sitzmann et al., 2020) into a standard recon- struction loss between image colours and the output of a multi-layer perceptron Φφ at each (2D) pixel location pi,j, L(φ, x) = (cid:88) i,j ||Φφ(pi,j) − x(pi,j)||2 2. (2) Algorithm 1 The LINAC Transform Inputs: RGB image x (with size I × J × 3); private key; number of epochs N ; mini-batch size M ; number of MLP layers L; representation layer K; learning rate µ. Output: Activation Image t(x) (with size I × J × H). rng = INIT PRNG(private key) (cid:46) Seed rng. φ(0) = INIT MLP(rng, L) S = (cid:98)I · J/M (cid:99) φ = φ(0) for epoch = 0 . . . N − 1 do (cid:46) Num. mini-batches per epoch. P = SHUFFLE AND SPLIT PIXELS(x, rng, S) for m = 0 . . . S − 1 do (cid:96) = 1 M ·I·J (cid:80) (i,j)∈P[m] ||Φφ(pi,j) − x(pi,j)||2 2 φ = φ − µ∇φ(cid:96) end for end for ˆφx = φ Return t(x) applying Eq. 3 using ˆφx and layer K. We provide pseudocode for the LINAC transform in Al- gorithm 1 and a discussion of computational and memory requirements in Appendix A.1.4. For each individual im- age x, we estimate ˆφx, an approximate local minimiser of L(φ, x), using a stochastic iterative minimisation procedure with mini-batches of M pixels grouped into epochs, which cover the entire image in random order, for a total of N passes through all pixels. Private Key. A random number generator is used for: (1) generating the initial MLP parameters φ(0) and (2) for decid- ing which random subsets of pixels make up mini-batches in each epoch. This random number generator is seeded by a 64-bit integer which we keep secret and denote as the pri- vate key. Hence, for all inputs x we start each independent optimisation from the same set of initial parameters φ(0), and we use the same shufﬂing of pixels across epochs. i Pixel(i, j) Intensities x(pi,j) = (r, g, b) j x: RGB Image In Implicit Neural Representation pi,j = (i, j) x(pi,j) = (r, g, b) … … …… … t(x): Activation Image Out i Representation of Pixel(i, j) Intensities … j Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Lossy Implicit Network Activation Coding (LINAC). We consider the lossy encoding of each pixel (i, j) in im- age x as the H-dimensional intermediate activations vector of layer K of the MLP evaluated at that pixel position: cx(i, j) = (hK−1 )(pi,j) with K < L. We build the lossy implicit network activation coding transformation of an image x by stacking together the encodings of all its pixels in its 2D image grid, concatenating on the feature dimension axis. The LINAC transformation t(x) of the I × J × 3 image x is given by: ◦ · · · ◦ h0 ˆφx ˆφx t(x) =    cx(0, 0) ... cx(I − 1, 0) . . . . . . . . . cx(0, J − 1) ... cx(I − 1, J − 1)    , (3) and has dimensionality I × J × H, where H is the number of outputs of the K-th layer of the MLP. By construction, our input transformation preserves the spatial dimensions of each image while increasing the feature dimensionality (from 3, the image’s original number of colour channels, to H); this means that standard network architectures used for image classiﬁcation (e.g. convolutional models) can be readily trained as the classiﬁer fθ. All omitted implementation details are provided in Ap- pendix A, and sensitivity analyses of LINAC to its hyper- parameters are reported in Appendix C. Threat Model. We are interested in hindering adversarial attacks on a nominally-trained classiﬁer fθ(t(x)), which operates on transformed inputs (i.e. on t(x) rather than on x), using a private key of our choosing. Next, we describe the threat model of interest by stating the conditions under which the LINAC defence is meant to hinder adversarial attacks on fθ, following AprilPyone & Kiya (2021a). We assume attackers do not have access to the private key, the integer seed of the random number generator used for computing the LINAC transformation, but otherwise have full algorithmic knowledge about our approach. Speciﬁcally, we assume an attacker has complete information about the classiﬁcation pipeline, including the architecture, training dataset and weights of the defended classiﬁer. This includes full knowledge of the LINAC algorithm, the implicit net- work architecture, parameter initialisation scheme and all the ﬁtting details, except for the private key. 4. Attacking the LINAC Defence Setup. We are interested in evaluating the apparent ro- bustness of a LINAC-defended classiﬁer, fˆθ, which has been trained by ERM to classify transformed inputs from the dataset D. Speciﬁcally, its parameters ˆθ minimise Ex,y∼D [LCE(fθ(t(x)), y)] , where LCE is the cross-entropy loss and t(x) is the LINAC transformation applied to image x using the private key. Input Perturbations. Classiﬁers defended by LINAC are not adversarially trained (Madry et al., 2018) to increase their robustness to speciﬁc Lp norm-bounded input pertur- bations. Furthermore, the LINAC defence is inherently agnostic about particular notions of maximum input pertur- bations. Nevertheless, to provide results comparable with a broad set of defences from the literature, we perform eval- uations on standard Lp norm-bounded input perturbations with: (1) a maximum perturbation radius of (cid:15) = 8/255 in the L∞ norm, and (2) one of (cid:15) = 0.5 in the L2 norm. Adapting Existing Attacks. Without access to the pri- vate key an attacker cannot compute the LINAC transfor- mation exactly. However, an attacker could acquire ac- cess to model inferences by attempting to brute-force guess the private key. Another option would be to train surro- gate models with LINAC, but using keys chosen by the at- tacker, in the hope that decision boundaries of these models would be similar enough to mount effective transfer attacks. More advanced attackers could modify LINAC itself to en- able strong Backward Pass Differentiable Approximation (BPDA) (Athalye et al., 2018) attacks. We evaluate the success of these and other standard attacks in Section 5. Designing Adaptive Attacks. Athalye et al. (2018) provide an excellent set of guidelines for designing and perform- ing successful adaptive attacks, while also standardising results reporting and aggregation. Of particular interest for defences based on input transformations are the BPDA and Expectation-over-Transformation (EoT) attack strategies. Subsequent work convincingly argues that adaptive attacks are not meant to be general, and must be customised, or “adapted”, to each defence in turn (Tramer et al., 2020). While BPDA and EoT generate strong attacks on input transformations, they both rely on being able to compute the forward transformation or approximate it with samples. Indeed, the authors mention that substitution of both the forward and backward passes with approximations leads to either completely ineffective, or much less effective attacks. Parametric Bypass Approximation (PBA). Inspired by the reparameterisation strategies of Athalye et al. (2018), we propose a bespoke attack by making use of several pieces of information available under our threat model: the parametric form of the defended classiﬁer fθ(t(x)), its training dataset D and loss function LCE, and its trained weights ˆθ. A Parametric Bypass Approximation of an unknown nui- sance transformation u : X → H is a surrogate parametric function hψ : X → H, parameterised by a solution to the following optimisation problem: ψ∗ = arg min ψ E x,y∼D (cid:2) LCE(fˆθ(hψ(x)), y) (cid:3) . (4) This formulation seeks a set of parameters ψ∗ which min- imise the original classiﬁcation loss while keeping the de- Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) fended classiﬁer’s parameters frozen at ˆθ. Similar with classiﬁer training, this optimisation problem can be solved efﬁciently using Stochastic Gradient Descent (SGD). A PBA adversarial attack can then proceed by approximat- ing the defended classiﬁer fˆθ(u(·)) with those of the bypass classiﬁer fˆθ(hψ∗ (·)) in both forward and backward passes when computing adversarial examples, e.g. using Projected Gradient Descent (PGD). The main advantages of the PBA strategy are that no for- ward passes through the nuisance transformation u(·) are required, and that it admits efﬁcient computation of many attacks to fˆθ, including gradient-based ones. In Section 5 we demonstrate the effectiveness of PBA beyond the LINAC defence. We show that, even though the surrogate transfor- mation is ﬁt on training data only, the defended classiﬁer operating on samples passed through hψ∗ (bypassing u) demonstrates nearly identical generalisation to the test set. Furthermore, we also show that PBA has greater success at ﬁnding adversarial examples for the LINAC defence com- pared to other methods. Lastly, we use PBA to invalidate an existing key-based defence proposed in the literature. 5. Results 5.1. Evaluation Methodology Since LINAC makes no assumptions about adversarial per- turbations, we are able to evaluate a single defended classi- ﬁer model against all attack strategies considered, in contrast to much adversarial training research (Madry et al., 2018). To obtain a more comprehensive picture of apparent robust- ness we start from the rigorous evaluation methodology used by Gowal et al. (2019); Rebufﬁ et al. (2021). We perform untargeted PGD attacks with 100 steps and 10 randomised restarts, as well as multi-targeted (MT) PGD attacks using 200 steps and 20 restarts. Anticipating the danger of ob- fuscated gradients skewing results, we also evaluate with the Square approach of Andriushchenko et al. (2020), a powerful gradient-free attack, with 10000 evaluations and 10 restarts. For precise comparisons with the broader liter- ature we also report evaluations using the parameter-free AutoAttack (AA) strategy of Croce & Hein (2020). Following Athalye et al. (2018) we aggregate results across attacks by only counting as accurate robust predictions those test images for which the defended classiﬁer predicts the correct class with and without adversarial perturbations, computed using all methods above. We report this as Best Known robust accuracy. In instances where several surrogate models are used to compute adversarial perturbations, also known as transfer attacks, we report Best Adversary results aggregated for each individual attack, which is deﬁned as robust accuracy Figure 2. Results of direct attack on private key. A histogram of accuracies of the same defended classiﬁer with inputs transformed using either the correct key or 100000 randomly chosen keys. An appropriate surrogate transformation is not found, invalidating attack vectors which rely on access to the outputs of the defended model on attacker chosen inputs. against all source models considered. We aggregate evaluations across these two dimensions (at- tacks & surrogate models) by providing a single robust ac- curacy number against all attacks computed using all source models for each standard convention of maximum perturba- tion and norm, enabling easy comparisons with results in the literature. 5.2. Attacks with Surrogate Transformations & Models A majority of adversarial attack strategies critically depend on approximating the outputs of the defended classiﬁer for inputs chosen by the attacker. The private key is kept secret in our threat model, which means that an attacker can neither compute the precise input transformation used to train the defended classiﬁer, nor its outputs on novel data. Hence, an attacker must ﬁnd appropriate surrogate transformations, or surrogate classiﬁer models, in order to perform effective adversarial attacks. We investigate both strategies below. Firstly, we empirically check that the outputs of the de- fended classiﬁer cannot be usefully approximated without knowledge of the private key. It is reasonable to hypothesise that transformations with different keys may lead to simi- lar functional representations of the input signal. We start investigating this hypothesis by simply computing the accu- racy of the defended model on clean input data transformed with LINAC, but using keys chosen by the attacker, also known as a brute-force key attack, which is valid under our threat model. As reported in Figure 2, the accuracy of our LINAC defended classiﬁer on test inputs transformed with the correct private key is over 93%. In an attempt to ﬁnd a surrogate transformation, 100000 keys are picked uniformly at random. For each key, we independently evaluated the accuracy of the classiﬁer using a batch of 100 test examples, and we report the resulting accuracy estimates for all keys with a histogram plot. The mean accuracy with random key guesses is around 30%, with a top accuracy of just 57% (see Table 4 in Appendix B.1 for a breakdown). Hence, using LINAC with incorrect keys leads to poor approximations of classiﬁer outputs on correctly transformed data. This Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Norm Attack Nominal Source AA MT PGD Square Best Known AA MT PGD Square Best Known 92.77 84.57 85.99 85.12 81.91 90.84 87.55 88.61 88.58 86.06 L∞ L2 Transfer Attack Source Models Adversarial Training (L∞) 80.42 72.96 60.97 65.69 54.97 86.75 85.34 82.39 84.50 79.42 Adversarial Training (L2) 70.29 56.08 44.06 52.66 39.20 80.83 84.81 74.19 79.31 71.92 Defended Surrogates (Attacker Keys) 84.00 85.70 87.32 75.91 75.64 88.27 87.31 88.36 84.08 83.48 Reconstruction- Based Surrogates (BPDA) 59.40 55.37 56.00 69.14 51.17 74.59 74.98 75.00 83.26 71.89 Best Adversary All Source Models 41.18 47.91 41.22 49.76 37.04 71.32 73.83 70.90 77.68 68.41 Table 1. CIFAR-10 test set robust accuracy (%) of a single LINAC defended classiﬁer according to a suite of L∞ and L2 transfer attacks, valid under our threat model, using various source classiﬁers to generate adversarial perturbations. suggest that the learned decision boundaries of the defended classiﬁer are not invariant to the private key used by LINAC. Figure 3. Plots of CIFAR-10 test-set robust accuracy estimates (Best Known) vs. number of attacker-trained surrogate models. We also plot the clean accuracy of 93.08% for reference. While we could not ﬁnd a useful surrogate transformation by random guessing, it is still possible that transformations with different keys preserve largely the same input informa- tion. So, the second option of an attacker is to check whether decision boundaries of models defended with LINAC and different keys are in fact very similar, which would enable powerful transfer attacks from such surrogate models. To this end, 10 independent models defended with LINAC were trained from scratch, each using a different key chosen by the attacker. We used the most promising 10 keys from the brute-force key attack for this purpose. In Figure 3 we report Best Known robust accuracy plotted against the num- ber of surrogate models used in these joint attacks, and we aggregate results over all 10 attacking models in the fourth column of Table 1. However, this attack vector has limited success. Under transfer attacks with such surrogate models, the robust accuracy of our defended classiﬁer appears to be high. While PGD and MT may fail due to vanishing or exploding gradients (Athalye et al., 2018), Square is a gradient-free attack, and does not suffer from such issues. Robust accuracy estimates according to Square are higher than 83% against any individual surrogate model, irrespec- tive of perturbation norm. A complete breakdown of results is given in Table 5 of Appendix B.1. Attacking with all 10 surrogate models together, robust accuracy to Square is still higher that 75%, and the estimate is not improved by further aggregating over attacks. This evidence further support the hypothesis that decision boundaries of classiﬁers defended with LINAC depend on their respective keys, and may differ enough across keys to hinder transfer attacks with surro- gates. Investing an order of magnitude more computation into such attacks leads to modest reductions in apparent robustness. Lastly, an attacker may strive to employ BPDA, one of the most effective and general strategies against defences using nuisance transformations. BPDA attacks require: (1) the ability to compute the exact forward transforma- tion and (2) ﬁnding a usefully differentiable approximation to the said transformation for use in the backwards pass of gradient-based attacks. In many cases this would be enough to allow the attacker to compute adversarial examples, per- haps at a somewhat higher computational cost (Athalye et al., 2018; Tramer et al., 2020). Our LINAC defence presents further challenges by design. Exact forward computations (model inferences) require the private key. An attacker cannot exactly compute the input transformation even for training set images, e.g. in order for some differentiable parametric approximation to be learned in a supervised fashion. Furthermore, surrogate models defended using LINAC and attacker chosen keys do not appear to be usefully differentiable, as suggested by results in Table 1. Nevertheless, an attacker could still hope that our defence “ﬁlters out” information in a largely key-agnostic manner, and that the choice of implicit network representa- tion layer is not essential. Hence, they have the option of modifying LINAC to output activations of the last, rather than the middle layer of the implicit network. This amounts ) % ( y c a r u c c a t s u b o r t e s - t s e t 90 80 70 60 50 40 30 20 None 1 2 Surrogates - L2 Surrogates - L BPDA - L2 BPDA - L 4 3 5 number of surrogate models 6 7 8 9 10 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Norm L∞ L2 Attack AA MT PGD Square Best Known AA MT PGD Square Best Known All Source Models Transfer 41.18 47.91 41.22 49.76 37.04 71.32 73.83 70.90 77.68 68.41 Adaptive Attacks PBA BPDA 68.34 59.40 46.75 55.37 44.05 56.00 48.59 69.14 35.32 51.17 73.10 74.59 67.85 74.98 66.93 75.00 74.70 83.26 61.23 71.89 Table 2. CIFAR-10 test set robust accuracy (%) of a single LINAC defended classiﬁer w.r.t. a suite of L∞ and L2 attacks, valid under our threat model, using different strategies such as transfer and adaptive attacks. Our novel PBA adaptive attacks are overall more effective that both transfer and BPDA attack strategies. to reducing LINAC to an approximate reconstruction of the original signal. While such surrogate models with at- tacker chosen keys would still have to be trained for the purpose, they would be vulnerable to strong BPDA attacks, which may transfer well to our defended classiﬁer. Appar- ent robustness estimates according to such transfer BPDA attacks are plotted in Figure 3 as a function of the number of surrogate models used jointly in the attack. In the ﬁfth column of Table 1 we provide aggregate apparent robust accuracies using 10 such surrogates, showing that transfer BPDA attacks are more successful than previous attempts; any such reconstruction-based surrogate model can be used to reveal that the robust accuracy of our defended classiﬁer cannot be higher than 65%, particularly with standard L∞ multi-targeted (MT) attacks (see Table 6 in Appendix B.1 for a detailed breakdown of results). Interestingly, when 10 surrogate models are used together, L∞ robust accuracy esti- mates drop to 51%. The reduction is less severe in standard L2 attacks, where accuracy against all surrogates appears to be still over 71%. These results conﬁrm that the BPDA strategy is a valuable tool for investigating the robustness of a wide range of defences, even when its assumptions are not fully met. 5.3. Transfer Attacks with Nominal and Adversarially Trained Source Models Since our defended classiﬁer is not adversarially trained, one could assume that its decision boundaries may be simi- lar to those of a nominal, undefended classiﬁer. We show in the ﬁrst column of Table 1 that transfer attacks with a nominally trained source model have limited success, es- pecially considering that such undefended classiﬁers have below chance robust accuracies according to the very same evaluations. Another possibility is that that our defended model may be susceptible to the promising attack directions to which adversarially trained robust classiﬁers are vulnerable. We report in the second and third columns of Table 1 that this is indeed the case to some extent. Of all adversaries considered thus far, a robust model adversarially trained to tolerate perturbations of up to size (cid:15) = 0.5 in L2 norm leads to the most effective transfer attacks. This holds to a lesser degree for an adversarially trained model with perturbations of size (cid:15) = 8/255 in L∞ norm. Despite the success of evaluations using the former source model, no one attack method comes close to the effectiveness of the joint strategy, reported as Best Known robust accuracy. Furthermore, it is important to note that ensemble transfer attacks are much stronger than those computed with any given source model. Aggregated over four attack types and 23 different source models, the robust accuracy of our LINAC defended classiﬁer is revealed to be at most half of what initial results suggested according to aggregate L∞ evaluations; this does not appear to be the case for L2 at- tacks, however, which continue to be substantially hindered by LINAC. Robust accuracy could still be above 68% ac- cording to the latter attack type, even in aggregate. In order to better characterise the implications of LINAC we make use of novel adaptive attacks in the following subsection. 5.4. PBA Attacks Against LINAC Thus far we have shown that strong transfer attacks can be performed by using an ensemble of diverse source models to compute adversarial perturbations over many repeated trials. While ultimately more reliable, this is a cumbersome evaluation protocol, requiring two order of magnitude more computation than standard evaluations. In Section 4 we have introduced PBA, an attack strategy pur- posefully designed to be effective against input transforma- tions (or network modules) which deny both inference and gradient computations, despite classiﬁer parameters, train- ing loss and dataset being available to the attacker. Follow- ing this novel strategy we successfully trained a parametric bypass approximation (PBA) of the LINAC transform and its associated defended classiﬁer. Intriguingly, the decision boundaries of the resulting bypass classiﬁer generalise very well. Accuracy on clean test data is 95.35%. Furthermore, the bypass classiﬁer can be readily shown to have 0% robust accuracy using PGD attacks. This indicates that any appar- ent robustness in evaluations can be largely attributed to the LINAC transform successfully hindering attacks, since the decision boundaries of our defended classiﬁer are sus- ceptible to adversarial perturbations, and hence cannot be considered to add any inherent robustness by themselves. In Table 2 we show that standard attacks using the trained PBA mapping against our LINAC defended classiﬁer are even more effective than BPDA attacks using 10 source models. Interestingly, PBA almost uniformly leads to more effective attacks, regardless of strategy. PGD attacks us- Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) (A) (B) (C) (D) (E) Figure 4. Decision boundaries of ﬁve different classiﬁers (rows) around the same ﬁve randomly chosen test examples (columns), plotted along their respective adversarial directions according to the AT (L2) model (horizontal), and the same random direction (vertical): (A) An undefended, nominally trained CIFAR-10 classiﬁer; (B) LINAC defended classiﬁers using a random key; (C) LINAC defended classiﬁers using the private key; this is the model we evaluated throughout the submission; (D) The by-pass classiﬁer resulting from our novel PBA attack on model (C); (E) An adversarially trained classiﬁer, AT (L2) in the main text, used to generate transfer attacks. We observe that boundaries of nominal model (A) are different from those of LINAC (B) & (C); LINAC decision boundaries seem less smooth compared to other models; as suspected boundaries appear different across keys (B) vs. (C), which corroborates our observations of robustness to transfer attacks with surrogates. The adversarially trained model (E) is robust to the vertical dimension (random noise); LINAC models (B) & (C) also appear less sensitive to random noise compared the nominal model (A). PBA by-pass classiﬁer (D) boundaries are much smoother and different from the true boundaries of the attacked model (C), which may explain why LINAC withstands the novel attack in many cases. Notice that PBA approximated boundaries (D) can be both closer and farther away from test examples compared to the true model’s (C), which makes it less clear how useful such approximations are for future attacks on LINAC. ing PBA give the most accurate picture of robustness of all strategies, suggesting that the matter of obfuscated gradi- ents is largely mitigated by our novel strategy. Aggregated over different attack types, PBA is the most effective and efﬁcient evaluation strategy which does not make use of the private key, and hence is valid under the adopted threat model. Based on these evaluations alone, one may conclude that robust accuracy was over 35% under attacks of size at most (cid:15) = 8/255 in L∞ norm, and over 61% for attacks of size (cid:15) = 0.5 in L2 norm. The apparent robustness differ- ence between L∞ and L2 attacks persists, suggesting that LINAC primarily hinders the latter type of attacks. 5.5. Towards Explaining the Apparent Robustness Decision boundary inspection. We plotted decision bound- aries of several classiﬁers around ﬁve randomly chosen test examples in Figure 4. All boundary plots are centred on test examples (columns), use appropriate adversarial direc- tions as the horizontal dimension, and a random direction as the vertical. As expected, we observe differences between LINAC defended classiﬁers which use different keys. Fur- thermore, we found that LINAC boundaries can be more “complicated” relative to those of other models, which may explain why PBA attacks are not completely effective. RGB Reconstruction vs. Lossy Encodings. Setting the representation layer index K = L renders our LINAC trans- Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Full PCA Block PCA JPEG (23) JPEG (10) Block Pixel Shufﬂe LINAC (Ours) 96.10 96.39 88.15 81.17 96.98 93.08 Standard Standard BPDA BPDA 32.58 – 27.48 6.34 11.90 – 17.49 5.36 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.61 2.26 0.00 0.03 0.41 12.85 0.06 0.00 0.17 11.66 62.95 – 60.37 28.33 62.98 – 60.38 21.92 0.02 0.00 14.94 14.56 PBA 0.18 0.00 0.00 0.00 0.00 0.02 0.00 0.02 6.13 0.00 PBA 68.34 46.75 44.05 48.59 35.32 73.10 67.85 66.93 74.70 61.23 Clean Accuracy: Norm Attack L∞ L2 AA MT PGD Square Best Known AA MT PGD Square Best Known Table 3. CIFAR-10 test set robust accuracy (%) of several classi- ﬁers defended using related input transformations according to evaluations using adversarial perturbations bounded in L∞ and L2 norms. Reporting results of strongest known attack strategy for each method, valid according to its own threat model. form into an approximate RGB input reconstruction, since L is the index of the implicit network output layer. We conﬁrmed that setting K = L = 5 and N = 100 epochs offers no robustness, since the resulting reconstructions are precise and BPDA attacks are successful. Clean accuracy was 96.91%, virtually matching that of a nominally trained classiﬁer. Hence, any apparent robustness must be due to the number of INR ﬁtting epochs N , and/or the choice of repre- sentation layer index K. Intuitively, both hyper-parameters control how “lossy” our transformation is. Naturally, we were interested in reducing the computational overhead of LINAC. Aiming to match the clean accuracy of state-of-the-art adversarially trained robust classiﬁers, speciﬁcally 93% (Rebufﬁ et al., 2021), we empirically chose N = 10 epochs as a trade-off between speed and clean accu- racy. The activation coding layer index K = 2 out of L = 5 hidden layers was chosen according to the same principle, as the lowest level representation which did not reduce clean accuracy below the target threshold. We further characterise and illustrate our LINAC transform in Appendix D. Performance Considerations. LINAC is as expensive as inference with a WideResNet-70-16 (Zagoruyko & Ko- modakis, 2016) on CIFAR-10 images. This cost is dom- inated by the ﬁtting of INRs. It could be reduced with an adaptive form of “early stopping” based on loss values, or by leveraging advances in INR research (e.g. Sitzmann et al. (2020)). We leave these investigations, and scaling LINAC to larger images, for future work. Sensitivity Analyses. The apparent robustness of LINAC defended classiﬁers is largely insensitive to the number of hidden layers L ≥ 3 of the implicit MLP, as well as the number of features F ≥ 3 in its positional input encoding, hence we relegated the sensitivity analyses to Appendix C. 5.6. PBA Beyond LINAC and Methodology Validation We show in the one-but-last column of Table 3 that PBA successfully and completely invalidates the Block Pixel Shufﬂe approach of AprilPyone & Kiya (2021a), despite its good reported robustness against all attacks. We further investigate using adversarially trained source models, see full results in Table 7 of Appendix B.1. In summary, our analysis conﬁrms that the apparent robust accuracy of Block Pixel Shufﬂe according to valid attacks bounded in L2 norm remains high at 69%. Hence, PBA is indeed the only known valid attack on this defence which is completely successful. Finally, we validate our evaluation methodology by test- ing its effectiveness against similar defences. We perform the same evaluations on the Principal Component Analysis (PCA) based defence of Shaham et al. (2018), and the JPEG- based defences of Das et al. (2017; 2018); Guo et al. (2018). In Table 3 we report the Best Known robust accuracies of these defences according to our evaluation methodology, which are directly comparable with our reported LINAC results. We observe that LINAC successfully hinders much stronger attacks than these alternative strategies. 6. Conclusions In this work we introduce LINAC, a novel key-based de- fence using implicit neural representations, and demonstrate its effectiveness for hindering standard adversarial attacks on CIFAR-10 classiﬁers. We systematically attempt to cir- cumvent our defence by adapting a host of widely used attacks from the literature, including transfer and adaptive attacks, but LINAC maintains strong apparent robustness. Consequently, we challenge LINAC by introducing a novel adaptive attack strategy (PBA) which is indeed more suc- cessful at discovering adversarial examples. We also show that PBA can be used to completely invalidate an existing key-based defence. These are some of the latest attempts to leverage computational hardness for adversarial robustness, and successful PBA attacks on existing methods enable further progress. References Andriushchenko, M., Croce, F., Flammarion, N., and Hein, M. Square attack: a query-efﬁcient black-box adversarial attack via random search. In European Conference on Computer Vision, pp. 484–501. Springer, 2020. AprilPyone, M. and Kiya, H. An extension of encryption- inspired adversarial defense with secret keys against ad- versarial examples. In 2020 Asia-Paciﬁc Signal and In- formation Processing Association Annual Summit and Conference (APSIPA ASC), pp. 1369–1374. IEEE, 2020. AprilPyone, M. and Kiya, H. Block-wise image transfor- Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) mation with secret key for adversarially robust defense. IEEE Transactions on Information Forensics and Secu- rity, 16:2709–2723, 2021a. AprilPyone, M. and Kiya, H. Transfer learning-based arXiv preprint model protection with secret key. arXiv:2103.03525, 2021b. Athalye, A., Carlini, N., and Wagner, D. A. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In ICML, 2018. Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs. 2018. URL http://github.com/google/jax. Brendel, W., Rauber, J., and Bethge, M. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. In International Conference on Learning Representations, 2018. Carlini, N. and Wagner, D. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM workshop on artiﬁcial in- telligence and security, pp. 3–14, 2017a. Carlini, N. and Wagner, D. Towards evaluating the robust- In 2017 ieee symposium on ness of neural networks. security and privacy (sp), pp. 39–57. IEEE, 2017b. Cohen, J., Rosenfeld, E., and Kolter, Z. Certiﬁed adver- sarial robustness via randomized smoothing. In Interna- tional Conference on Machine Learning, pp. 1310–1320. PMLR, 2019. Croce, F. and Hein, M. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206–2216. PMLR, 2020. Das, N., Shanbhogue, M., Chen, S.-T., Hohman, F., Chen, L., Kounavis, M. E., and Chau, D. H. Keeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression. arXiv preprint arXiv:1705.02900, 2017. Das, N., Shanbhogue, M., Chen, S.-T., Hohman, F., Li, S., Chen, L., Kounavis, M. E., and Chau, D. H. Shield: Fast, practical defense and vaccination for deep learn- ing using jpeg compression. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 196–204, 2018. Dhillon, G. S., Azizzadenesheli, K., Lipton, Z. C., Bern- stein, J. D., Kossaiﬁ, J., Khanna, A., and Anandkumar, A. Stochastic activation pruning for robust adversarial defense. In International Conference on Learning Repre- sentations, 2018. Dziugaite, G. K., Ghahramani, Z., and Roy, D. M. A study of the effect of jpg compression on adversarial images. arXiv preprint arXiv:1608.00853, 2016. Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Tran, B., and Madry, A. Adversarial robustness as arXiv preprint a prior for learned representations. arXiv:1906.00945, 2019. Frostig, R., Johnson, M. J., and Leary, C. Compiling ma- chine learning programs via high-level tracing. Systems for Machine Learning, 2018. Garg, S., Jha, S., Mahloujifar, S., and Mohammad, M. Ad- versarially robust learning could leverage computational hardness. In Algorithmic Learning Theory, pp. 364–385. PMLR, 2020. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Gowal, S., Uesato, J., Qin, C., Huang, P., Mann, T. A., and Kohli, P. An alternative surrogate loss for pgd-based adversarial testing. CoRR, abs/1910.09338, 2019. URL http://arxiv.org/abs/1910.09338. Gowal, S., Qin, C., Uesato, J., Mann, T., and Kohli, P. Uncovering the limits of adversarial training against norm-bounded adversarial examples. arXiv preprint arXiv:2010.03593, 2020. Guo, C., Rana, M., Cisse, M., and van der Maaten, L. Coun- tering adversarial images using input transformations. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=SyJ7ClWCb. Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del R´ıo, J. F., Wiebe, M., Peterson, P., G´erard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. Nature, 585(7825):357–362, September 2020. doi: 10. 1038/s41586-020-2649-2. URL https://doi.org/ 10.1038/s41586-020-2649-2. He, W., Wei, J., Chen, X., Carlini, N., and Song, D. Adver- sarial example defenses: ensembles of weak defenses are not strong. In Proceedings of the 11th USENIX Confer- ence on Offensive Technologies, pp. 15–15, 2017. Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Hein, M. and Andriushchenko, M. Formal guarantees on the robustness of a classiﬁer against adversarial manipulation. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 2263–2273, 2017. Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., and Madry, A. Adversarial examples are not bugs, they are features. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Pro- cessing Systems, volume 32. Curran Associates, URL https://proceedings. Inc., neurips.cc/paper/2019/file/ e2c420d928d4bf8ce0ff2ec19b371514-Paper. pdf. 2019. Jacobsen, J.-H., Behrmann, J., Zemel, R., and Bethge, M. Excessive invariance causes adversarial vulnerability. In International Conference on Learning Representations, 2018. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In ICLR (Poster), 2015. Kurakin, A., Goodfellow, I., Bengio, S., Dong, Y., Liao, F., Liang, M., Pang, T., Zhu, J., Hu, X., Xie, C., et al. Adversarial attacks and defences competition. In The NIPS’17 Competition: Building Intelligent Systems, pp. 195–231. Springer, 2018. Liao, F., Liang, M., Dong, Y., Pang, T., Hu, X., and Zhu, J. Defense against adversarial attacks using high-level representation guided denoiser, 2018. Loshchilov, I. and Hutter, F. SGDR: stochastic gradient descent with restarts. CoRR, abs/1608.03983, 2016. URL http://arxiv.org/abs/1608.03983. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant In International Conference to adversarial attacks. on Learning Representations, 2018. URL https:// openreview.net/forum?id=rJzIBfZAb. MaungMaung, A. and Kiya, H. A protection method of trained cnn model with secret key from unauthorized access. arXiv preprint arXiv:2105.14756, 2021. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. Nerf: Representing scenes as neural radiance ﬁelds for view synthesis. In European conference on computer vision (ECCV), pp. 405–421. Springer, 2020. Niu, Z., Chen, Z., Li, L., Yang, Y., Li, B., and Yi, J. On the limitations of denoising strategies as adversarial defenses. CoRR, abs/2012.09384, 2020. URL https://arxiv. org/abs/2012.09384. Pang, T., Yang, X., Dong, Y., Su, H., and Zhu, J. Bag of tricks for adversarial training. In International Confer- ence on Learning Representations, 2020. Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., and Swami, A. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia conference on computer and communications secu- rity, pp. 506–519, 2017. Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed defenses against adversarial examples. In International Conference on Learning Representations, 2018. Ramachandran, P., Zoph, B., and Le, Q. V. Searching for activation functions. CoRR, abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941. Rebufﬁ, S.-A., Gowal, S., Calian, D. A., Stimberg, F., Wiles, O., and Mann, T. Fixing data augmentation to improve adversarial robustness. arXiv preprint arXiv:2103.01946, 2021. Schmidt, L., Santurkar, S., Tsipras, D., Talwar, K., and Madry, A. Adversarially robust generalization requires more data. In NeurIPS, 2018. Schott, L., Rauber, J., Bethge, M., and Brendel, W. Towards the ﬁrst adversarially robust neural network model on mnist. In Seventh International Conference on Learning Representations (ICLR 2019), pp. 1–16, 2019. Shaham, U., Garritano, J., Yamada, Y., Weinberger, E., Cloninger, A., Cheng, X., Stanton, K., and Kluger, Y. De- fending against adversarial images using basis functions transformations. arXiv preprint arXiv:1803.10840, 2018. Sitzmann, V. Awesome Implicit Representations - A cu- rated list of resources on implicit neural representations. 2020. URL https://github.com/vsitzmann/ awesome-implicit-representations. Sitzmann, V., Martel, J., Bergman, A., Lindell, D., and Wet- zstein, G. Implicit neural representations with periodic activation functions. Advances in Neural Information Processing Systems, 33, 2020. Skorokhodov, I., Ignatyev, S., and Elhoseiny, M. Adversarial generation of continuous images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10753–10764, 2021. Song, S., Chen, Y., Cheung, N.-M., and Kuo, C.-C. J. De- fense against adversarial attacks with saak transform. arXiv preprint arXiv:1808.01785, 2018a. Song, Y., Kim, T., Nowozin, S., Ermon, S., and Kushman, N. Pixeldefend: Leveraging generative models to understand Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) and defend against adversarial examples. In International Conference on Learning Representations, 2018b. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Er- han, D., Goodfellow, I., and Fergus, R. Intriguing properties of neural networks. In International Confer- ence on Learning Representations, 2014. URL http: //arxiv.org/abs/1312.6199. Tieleman, T. and Hinton, G. Lecture 6.5-rmsprop, coursera: Neural networks for machine learning. University of Toronto, Technical Report, 2012. Tramer, F., Carlini, N., Brendel, W., and Madry, A. On adap- tive attacks to adversarial example defenses. Advances in Neural Information Processing Systems, 33, 2020. Wong, E. and Kolter, J. Z. Provable defenses against adver- sarial examples via the convex outer adversarial polytope. In ICML, 2018. Yang, Y., Zhang, G., Katabi, D., and Xu, Z. Me-net: To- wards effective adversarial robustness with matrix estima- tion. In International Conference on Machine Learning, pp. 7025–7034. PMLR, 2019. Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo, Y. Cutmix: Regularization strategy to train strong classiﬁers with localizable features. CoRR, abs/1905.04899, 2019. URL http://arxiv.org/abs/1905.04899. Zagoruyko, S. and Komodakis, N. Wide residual networks. In British Machine Vision Conference 2016. British Ma- chine Vision Association, 2016. Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) A. LINAC Implementation Details A.1. Implicit Neural Representations A.1.1. RANDOM NUMBER GENERATION FOR INRS Our LINAC defence is fully deterministic by design. We used a random 64-bit signed integer as the private key, which seeded the state of the pseudo-random number generator in JAX (Frostig et al., 2018; Bradbury et al., 2018). The precise value of the private key used to train the defended model evaluated throughout this work was: −2314326399425823309. It was itself selected randomly, by initialising the random number generator of the NumPy library (Harris et al., 2020) with seed 42 and using the ﬁrst int64 integer. A.1.2. INPUT AND OUTPUT ENCODINGS Following Mildenhall et al. (2020) we use a positional encoding of pixel coordinates to a higher dimensional space to better capture higher-frequency information. Each pixel coordinate d is normalised to [−1, 1] and transformed as follows: γ(d) = [sin(20πd), cos(20πd), sin(21πd), cos(21πd), . . . , sin(2F −1πd), cos(2F −1πd)] (5) We used F = 5 frequencies in all our experiments and a L = 5 hidden layer MLP with H = 256 units per layer and ReLU non-linearities. Activations in the middle hidden layer were used for computing the LINAC transform, hence K = 2. As per standard practice for CIFAR-10 classiﬁcation, pixel colour intensities were scaled to have 0 mean across the training dataset and each colour channel separately. Intensities were then standardised to 1 standard deviation across the training dataset, independently across channels. A.1.3. FITTING Fitting the parameters of the implicit neural network was done using Adam (Kingma & Ba, 2015), with default parameters and a learning rate µ = 0.001. We used mini-batches with M = 32 random pixels and trained for N = 10 epochs. An epoch constitutes a pass through the entire set of pixels in the input image with dimensions I × J × C = 32 × 32 × 3 in random order. The total number of optimisation steps performed was 320. A cosine learning rate decay schedule was used for better convergence, with the minimum value of the multiplier α = 0.0001 (Loshchilov & Hutter, 2016). A.1.4. COMPUTATIONAL AND MEMORY REQUIREMENTS The LINAC transform’s computational complexity scales with the number of pixels (I · J) of the input image and the number of epochs through the pixels (N ). It takes I · J · N backward passes through the implicit network Φ to ﬁt its parameters φ. LINAC’s memory complexity is dominated by the number of parameters of the INR (|φ|). Empirically, the LINAC transform is itself as expensive as inference with a WideResNet-70-16 model (Zagoruyko & Komodakis, 2016) on CIFAR-10 images. A.2. Defended Classiﬁers Since the proposed input transformation preserves spatial structure, we perform image classiﬁcation using transformed inputs in an identical manner as with RGB colour images, except for the higher number of channels of transformed inputs. Hence, we employ a standard classiﬁcation pipeline following (Zagoruyko & Komodakis, 2016), using a WideResNet- 70-16 classiﬁer. We reiterate that our proposed transformation changes the number of input channels, but not the spatial dimensions. Hence, small differences between our models and other WideResNet-70-16 results reported in the literature could conceivably appear only due to different numbers of input channels. However, practically this leads to less than a 0.2% increase in the total number of model parameters, limited to the ﬁrst convolutional layer, which uses ﬁlters with 256 channels instead of 3. We used the Swish activation function proposed by Ramachandran et al. (2017) for all the classiﬁers. Training was performed with Nesterov Momentum SGD (Tieleman & Hinton, 2012) m = 0.9, using mini-batches of size 1024, for a total of 1000 epochs, or 48880 parameter updates. The initial learning rate was µ = 0.4, reduced by a factor of 10 four times, at epochs: 650, 800, 900 and 950. We performed a hyper-parameter sweep over the weight-decay scale with the following grid: {0., 0.0001, 0.0005, 0.0010}. We maintain an exponential moving average of classiﬁer parameters (with a decay rate of r = 0.995); we report accuracies using the ﬁnal average of classiﬁer parameters. Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) A.2.1. PERFORMANCE CONSIDERATIONS We use the CutMix data augmentation strategy of Yun et al. (2019) directly on RGB images from the training set of CIFAR- 10, prior to transforming them with LINAC. This has an impact on computational considerations, since pre-computing the transformed dataset ofﬂine in order to save training time becomes more challenging. For ease of prototyping we chose to implement LINAC as a preprocessing layer, which could have an impact on training time if used naively, but not if the transformation is applied asynchronously on the buffer of data feeding the device used for model training. We also found empirically that the proposed transformation renders itself to very effective parallelisation using modern SIMD devices, despite the fact that there is no parameter sharing between implicit models of different inputs; this is likely due to the ability of modern libraries such as JAX (Frostig et al., 2018) to vectorise operations across tensors holding parameters for many distinct neural networks. It is important to note that inference and training costs of defended classiﬁers are roughly double those of the nominal classiﬁer. Hence, the LINAC transform has comparable cost to inference with a WideResNet-70-16 model. B. Evaluation Details B.1. Attacks with Surrogate Models We provide a breakdown of evaluations using surrogate models initially reported in Section 5. We report the best 10 keys from the brute-force attack on the private key in Table 4. These keys were also used to train surrogate models defended with LINAC for use in transfer attacks, see Table 5 for complete results. Reconstruction-based surrogate models defended with modiﬁed LINAC, and using the same 10 best-guess attacker keys, were used to perform BPDA transfer attacks, reported in Table 6. Position Clean Test Accuracy (%) 1 2 3 4 5 6 7 8 9 10 57.00 55.00 55.00 55.00 55.00 55.00 54.00 54.00 54.00 54.00 Attacker Key 1383227977468296715 -3328443931658504707 -127094507362684985 -7808219206569127925 -8772667224621836765 -70640792831170485 8263151932495004089 -4594861196100637268 -6520968232434877967 -8722766234183220599 Table 4. Top 10 keys in brute-force key attack, also used to train surrogate models. LINAC Defence Norm Attack Name L∞ L2 AA MT PGD Square Best Known AA MT PGD Square Best Known Defended Surrogate Source Models (Attacker Keys) key1 93.05 89.70 88.05 83.37 82.22 91.20 90.29 88.99 87.84 86.51 key2 92.99 89.48 88.19 83.34 82.10 91.15 90.47 89.00 87.83 86.41 key3 93.06 89.63 88.20 83.60 82.43 91.24 90.30 88.92 87.77 86.59 key4 93.00 89.54 88.23 83.43 82.26 91.17 90.50 88.93 88.15 86.42 key5 93.01 89.59 88.24 83.06 81.93 91.18 90.54 88.93 87.88 86.33 key6 93.05 89.55 88.17 83.38 82.33 91.18 90.37 89.08 88.11 86.62 key7 93.00 89.36 88.06 83.41 82.24 91.20 90.20 88.97 88.19 86.50 key8 93.00 89.58 88.20 83.30 82.10 91.20 90.32 88.98 87.94 86.58 key9 93.15 89.69 88.29 83.36 82.32 91.21 90.56 88.95 87.85 86.58 key10 93.21 89.47 88.10 83.21 82.17 91.16 90.25 89.03 88.12 86.53 Best Adversary (against all models) 84.00 85.70 87.32 75.91 75.64 88.27 87.31 88.36 84.08 83.48 Table 5. CIFAR-10 test set robust accuracy (%) of a single LINAC defended classiﬁer w.r.t. a suite of L∞ and L2 transfer attacks, valid under our threat model, using surrogate classiﬁers defended with LINAC, but trained with attacker-chosen keys. The clean accuracy of our defended classiﬁer is 93.08%. Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) LINAC Defence Norm Attack Name L∞ L2 AA MT PGD Square Best Known AA MT PGD Square Best Known Reconstruction-Based Surrogate Source Models Using Attacker Keys key1 91.47 68.35 69.35 82.91 62.87 85.94 82.12 82.23 87.30 78.68 key2 91.24 69.13 71.57 82.98 64.59 86.29 82.27 82.77 87.34 79.15 key3 91.21 67.71 69.15 82.47 62.63 85.95 81.91 82.20 87.22 78.36 key4 91.31 68.63 70.55 82.92 63.65 86.51 82.13 82.60 87.40 78.93 key5 91.32 67.53 69.30 82.66 62.40 85.52 81.78 81.89 87.35 78.46 key6 91.35 69.07 70.46 82.80 64.02 86.16 82.42 82.33 87.44 79.08 key7 91.31 68.81 69.67 82.84 63.54 86.28 82.24 82.53 87.52 78.92 key8 91.63 67.98 69.48 82.91 62.93 86.34 82.07 82.38 87.21 78.83 key9 91.33 68.40 69.97 82.91 63.40 86.46 82.21 82.69 87.26 78.80 key10 91.46 69.24 70.85 82.93 64.15 86.27 81.96 82.68 87.46 78.46 Best Adversary (against all models) 59.40 55.37 56.00 69.14 51.17 74.59 74.98 75.00 83.26 71.89 Table 6. CIFAR-10 test set robust accuracy (%) of a single LINAC defended classiﬁer according to a suite of L∞ and L2 BPDA attacks, valid under our threat model, using reconstruction-based surrogate classiﬁers. B.2. Transfer Attacks with Adversarially Trained Models For mounting transfer attacks we have taken adversarially trained models from previous work (Rebufﬁ et al., 2021), with checkpoints available online1. These models have been adversarially trained on CIFAR-10 using additional synthetic generated data and CutMix data augmentation. To mount transfer attacks we use the WideResNet-106-16 model (trained to defend against L∞ norm-bounded perturbations of size (cid:15) = 8/255) and the WideResNet-70-16 model (trained to defend against L2 norm-bounded perturbations of size (cid:15) = 0.5). B.3. PBA Implementation Details B.3.1. PBA FOR LINAC We used a single convolutional layer (k = 3 × 3) with biases to implement hψ(x), the PBA of the nuisance transformation, mapping from the 3 RGB channels of input images to the H = 256 channels output by LINAC. The parameters ψ of the bypass approximation were trained by minimising the cross-entropy loss on the CIFAR-10 training set using Momentum SGD with a learning rate µ = 0.1. 100 epochs sufﬁced to optimise PBA parameters, with four learning rate reductions by a factor of 0.1 at epochs: 65, 80, 90, 95. B.3.2. PBA FOR BLOCK PIXEL SHUFFLE We implemented the Block Pixel Shufﬂe defence of AprilPyone & Kiya (2021a) using blocks of size 4 × 4, as recommended in the original work. We used the same private key value as that of our defended LINAC classiﬁer. The private key serves as the seed of a pseudo-random number generator, which is used to sample a permutation of all pixel positions in a block. The same permutation is applied to all blocks. We illustrate the transform in Figure 5. Figure 5. Example of Pixel Block Shufﬂe transformation (AprilPyone & Kiya, 2021a). An original CIFAR-10 image (left) is split into a grid of 4 × 4 blocks of adjacent pixels, and the same random permutation is used to shufﬂe pixel positions within every block (middle). The transformed image is constructed by spatially concatenating the blocks according to their original positions in the grid (right). A classiﬁer defended with Block Pixel Shufﬂe was trained with the same procedure as our defended LINAC classiﬁer. We can report a clean CIFAR-10 test set accuracy of 97.03%, which is higher to that reported by AprilPyone & Kiya (2021a), 1https://github.com/deepmind/deepmind-research/tree/master/adversarial_robustness Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) but consistent with the superior CutMix (Yun et al., 2019) data augmentation procedure we used for all defended classiﬁers. According its own “white-box” threat model (AprilPyone & Kiya, 2021a), all the implementation details of the defence are known to an attacker except the private key. We exploit the block structure and use a single linear layer without biases, and initialised with the identity mapping, to compute a parametric bypass approximation (PBA) for the this defence. We found that using a smaller initial learning rate µ = 0.001 results in stable convergence. We used 300 epochs to optimise PBA parameters, with four learning rate reductions by a factor of 0.1 at epochs: 275, 285, 290, 295. An extensive evaluation of the resulting defended classiﬁer is given in Table 7. We ﬁnd that transfer attacks which are agnostic to the defence can be more successful when adversarial examples are computed using robust source models, but one may infer some level of robustness. Using PBA attacks valid under the threat model (“white-box”) we successfully circumvent the defence, with a Best Known CIFAR-10 robust test-set accuracy of 0% under adversarial perturbations of size up to (cid:15) = 8/255 in L∞ norm, and up to (cid:15) = 0.5 in L2 norm. Block Pixel Shufﬂe Defence Transfer Attack Source Models Adaptive Attacks Best Adversary Norm Attack Name Nominal Source L∞ L2 AA MT PGD Square Best Known AA MT PGD Square Best Known 85.78 78.87 69.17 69.16 60.65 94.14 93.93 92.92 91.69 90.41 Adversarial Training (L∞) 69.09 56.49 39.05 46.25 30.61 90.02 92.25 87.54 88.80 85.29 Adversarial Training (L2) 73.86 27.72 31.19 42.63 21.17 83.35 77.02 77.80 84.88 69.00 PBA 0.18 0.00 0.00 0.00 0.00 0.02 0.00 0.02 6.13 0.00 All Source Models 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 6.13 0.00 Table 7. CIFAR-10 test set robust accuracy (%) of Block Pixel Shufﬂe approach (AprilPyone & Kiya, 2021a) against standard L∞ and L2 bounded attacks using both transfer and our novel PBA strategy. C. Sensitivity of LINAC to Hyper-Parameters We performed sensitivity analyses of LINAC to its hyper-parameters. For efﬁciency reasons we report robust accuracies according to untargeted PGD attacks with 100 steps and 10 restarts, using an adversarially trained robust model (L2) (Rebufﬁ et al., 2021) to generate adversarial perturbations. F = 3 F = 5 F = 7 F = 10 93.61 Clean Accuracy: L∞ PGD 43.51 L2 PGD 74.99 93.08 93.78 93.65 44.06 74.19 43.46 75.96 43.90 74.91 Figure 6. CIFAR-10 test-set clean and robust accuracies under transfer attacks of LINAC defended classiﬁers with different numbers of positional encoding frequencies F , keeping all other hyper-parameters constant. In Figure 6 we provide a sensitivity analysis across the number of frequencies F used for positional encoding (Mildenhall et al., 2020), keeping all other hyper-parameters the same. Note that we used F = 5 for our defended classiﬁer evaluated in ) % ( y c a r u c c a t e s - t s e t 100 90 80 70 60 50 40 30 20 10 0 3 4 clean accuracy L robust accuracy L2 robust accuracy 5 8 F (number of frequencies) 6 7 9 10 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) the main paper. L = 3 L = 4 L = 5 L = 6 L = 7 L = 8 95.21 Clean Accuracy: L∞ PGD 44.57 L2 PGD 80.40 94.30 93.08 91.70 91.20 90.41 44.55 77.16 44.06 74.19 43.70 71.95 43.81 71.22 43.81 70.08 Figure 7. CIFAR-10 test-set clean and robust accuracies (under transfer attacks) of LINAC defended classiﬁers with different numbers of implicit network layers L, keeping all other hyper-parameters ﬁxed. In Figure 7 we vary the number of implicit network layers L, keeping all other hyper-parameters the same, including the representation layer index K = 2 and number of epochs N = 10. Note that we used L = 5 for our defended classiﬁer evaluated in the main paper. K = 0 K = 1 K = 2 K = 3 K = 4 50.32 Clean Accuracy: L∞ PGD 36.63 L2 PGD 42.17 86.92 93.08 95.07 95.49 44.55 67.26 44.06 74.19 43.99 78.53 42.83 81.03 Figure 8. CIFAR-10 test-set clean and robust accuracies (under transfer attacks) of LINAC defended classiﬁers with different implicit network layers used to output representations (K), keeping all other hyper-parameters the same. In Figure 8 we change the index of the LINAC representation layer K, keeping all other hyper-parameters unchanged. Note that we used K = 2 for our defended classiﬁer evaluated in the main paper. In Figure 9 we analyse the sensitivity of LINAC to the number of epochs N , keeping all other hyper-parameters constant. Note that we used N = 10 for our defended classiﬁer evaluated in the main paper. D. Characterising the LINAC Transform In Figure 10 we plot learning curves characterising implicit network ﬁtting, as used for our defended classiﬁer. Mean and standard deviation of errors across independent learning processes for the entire CIFAR-10 test-set are plotted as functions of optimisation steps, using a log-scale for errors. The ﬁnal mean value of such errors is 0.04325, which conﬁrms that our LINAC approach leads to lossy representations. A histogram of ﬁnal sum squared errors for the entire test-set of CIFAR-10 is provided in Figure 11. For a qualitative evaluation of such statistics, we provide examples of original images, their reconstructions and difference images, using LINAC and the private key in Figure 12 and, for comparison, a different key in Figure 13. We observe that encoding errors using LINAC are key dependent. Furthermore, signiﬁcant amounts of information seem to be left out by LINAC. Some difference images could be recognised as the correct class, most likely due to high-frequency information which is not well represented. Finally, we provide a number of plots for qualitative comparisons of LINAC transforms. Figure 14 shows three different ) % ( y c a r u c c a t e s - t s e t 100 90 80 70 60 50 40 30 20 10 0 clean accuracy L robust accuracy L2 robust accuracy 3 4 5 6 7 8 L (number of hidden layers) ) % ( y c a r u c c a t e s - t s e t 100 90 80 70 60 50 40 30 20 10 0 0 clean accuracy L robust accuracy L2 robust accuracy 1 2 K (index of representation layer) 3 4 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) N = 5 N = 10 N = 15 N = 20 N = 25 N = 50 85.74 Clean Accuracy: L∞ PGD 49.01 L2 PGD 70.65 93.08 94.59 95.41 95.66 95.87 44.06 74.19 40.86 75.69 38.56 76.28 37.63 76.42 35.90 78.06 Figure 9. CIFAR-10 test-set clean and robust accuracies (under transfer attacks) of LINAC defended classiﬁers with implicit networks trained for N epochs, keeping all other hyper-parameters constant. Figure 10. Independent ﬁtting of implicit neural networks to CIFAR-10 test-set images in order to compute their LINAC transforms. Sum squared encoding errors, averaged over pixels, are plotted against ﬁtting steps. images encoded and their respective LINAC representations encoded as RGB channels. Figures 15, 16 and 17 plot LINAC transforms of the same respective images, but using with different keys, one on each RGB colour channel. ) % ( y c a r u c c a t e s - t s e t 100 90 80 70 60 50 40 30 20 10 0 5 10 15 clean accuracy L robust accuracy L2 robust accuracy 25 20 35 N (number of epochs) 30 40 45 50 l ) e a c s - g o l ( r o r r e 101 100 10 1 10 2 0 32 64 96 128 160 steps 192 224 256 288 320 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 11. Histogram of LINAC transform encoding errors plotted for the entire CIFAR-10 test-set. The overall mean value of such errors is 0.04325, which conﬁrms that our LINAC approach leads to lossy representations. mean error 1400 1200 1000 800 600 400 200 0 10 3 10 2 10 1 errors Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 12. Image approximations computed for LINAC with the private key, as used for our defended classiﬁer. Original images and labels are plotted in the ﬁrst column. Note that labels are not used for LINAC. Implicit network outputs are plotted in the second column. Difference images and sum squared errors, averaged over pixels, are plotted in the third column. Note that LINAC uses lossy image approximations. horse airplane frog truck dog automobile horse airplane cat bird 1 1 1 1 1 1 1 2 1 0 2 1 0 1 0 2 1 0 1 0 2 1 0 2 1 0 2 1 0 1 1.0 0.5 0.0 0.5 1.0 1.5 2 1 0 1 4.69e-02 3.95e-02 6.85e-02 4.23e-02 5.38e-02 6.74e-02 2.46e-02 2.44e-02 1.67e-02 3.44e-02 2 1 0 2 1 0 1 0 2 1 0 2 1 0 2 1 0 2 1 0 2 1 0 1 0 2 1 0 1 1 2 1 1 2 1 2 1 1 1 1 0.50 0.25 0.00 0.25 0.50 0.5 0.0 0.5 1.0 0.5 0.0 0.5 0.25 0.00 0.25 0.50 0.75 0.4 0.2 0.0 0.2 0.4 0.6 0.5 0.0 0.5 0.4 0.2 0.0 0.2 0.4 0.6 0.6 0.4 0.2 0.0 0.2 0.4 0.4 0.2 0.0 0.2 0.6 0.4 0.2 0.0 0.2 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 13. Image approximations computed for LINAC with a different, attacker chosen key. Original images and labels are plotted in the ﬁrst column. Note that labels are not used for LINAC. Implicit network outputs are plotted in the second column. Difference images and sum squared errors, averaged over pixels, are plotted in the third column. Note that LINAC leads to lossy image approximations which are key dependent. horse airplane frog truck dog automobile horse airplane cat bird 1 1 1 1 1 1 1 2 1 0 2 1 0 1 0 2 1 0 1 0 2 1 0 2 1 0 2 1 0 1 1.0 0.5 0.0 0.5 1.0 1.5 2 1 0 1 5.87e-02 3.79e-02 6.16e-02 5.00e-02 7.63e-02 7.23e-02 2.22e-02 3.57e-02 1.40e-02 5.16e-02 1 1 2 1 1 2 1 1 2 1 0 2 1 0 1 0 2 1 0 2 1 0 2 1 0 2 1 0 2 1 0 1 1.0 0.5 0.0 0.5 1.0 1.5 2 1 0 1 0.75 0.50 0.25 0.00 0.25 0.50 0.25 0.00 0.25 0.50 0.75 0.50 0.25 0.00 0.25 0.50 0.50 0.25 0.00 0.25 0.50 0.75 0.5 0.0 0.5 0.5 0.0 0.5 1.0 0.2 0.0 0.2 0.4 0.4 0.2 0.0 0.2 0.4 0.2 0.0 0.2 0.4 0.2 0.0 0.2 0.4 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 14. Comparing transforms of the 3 top images using LINAC with the private key, as done for our defended classiﬁer. The respective activation images with H = 256 channels were plotted in a 16 × 16 grid of slices of the same size with original images. Respective slices over the channel dimension of activation images were combined as RGB channels in this plot (bottom), in order to compare channel representations for the three input images (top). Each square in the grid represents the activations of a LINAC representation channel for all pixels in the original image. Different values of RGB signify differences in LINAC representations across images. 0 1 2 1 0 1 2 2 1 0 1 1 0 1 1 r e y a l n e d d h i 1 2 3 4 5 6 7 8 hidden unit position in layer 9 10 11 12 13 14 15 16 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 15. Comparing LINAC transforms of the same image using the private key and two other random keys. The respective activation images with H = 256 channels were plotted in a 16 × 16 grid of slices of the same size with original images. Respective slices over the channel dimension of resulting activation images were combined as RGB channels in this plot (bottom), in order to compare channel representations with three different keys for the same input image. Each square in the grid represents the activations of a LINAC representation channel for all pixels in the original image. Different values of RGB signify differences in LINAC representations across keys. 0 1 2 1 0 1 2 2 1 0 1 2 1 0 1 1 r e y a l n e d d h i 1 2 3 4 5 6 7 8 hidden unit position in layer 9 10 11 12 13 14 15 16 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 16. Comparing LINAC transforms of the same image using the private key and two other random keys. The respective activation images with H = 256 channels were plotted in a 16 × 16 grid of slices of the same size with original images. Respective slices over the channel dimension of resulting activation images were combined as RGB channels in this plot (bottom), in order to compare channel representations with three different keys for the same input image. Each square in the grid represents the activations of a LINAC representation channel for all pixels in the original image. Different values of RGB signify differences in LINAC representations across keys. 0 1 2 1 0 1 2 2 1 0 1 2 1 0 1 1 r e y a l n e d d h i 1 2 3 4 5 6 7 8 hidden unit position in layer 9 10 11 12 13 14 15 16 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 17. Comparing LINAC transforms of the same image using the private key and two other random keys. The respective activation images with H = 256 channels were plotted in a 16 × 16 grid of slices of the same size with original images. Respective slices over the channel dimension of resulting activation images were combined as RGB channels in this plot (bottom), in order to compare channel representations with three different keys for the same input image. Each square in the grid represents the activations of a LINAC representation channel for all pixels in the original image. Different values of RGB signify differences in LINAC representations across keys. 0 1 2 1 0 1 1 0 1 1 0 1 1 r e y a l n e d d h i 1 2 3 4 5 6 7 8 hidden unit position in layer 9 10 11 12 13 14 15 16",0.0
"Google's 2019 ""Quantum Supremacy'' Claims: Data, Documentation, and
  Discussion","[{'href': 'http://arxiv.org/abs/2210.12753v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.12753v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-10-23 15:36:25,,"2 2 0 2 t c O 8 1 ] G L . s c [ 1 v 8 9 5 9 0 . 0 1 2 2 : v i X r a Planning for Sample Efﬁcient Imitation Learning Zhao-Heng Yin∗ Weirui Ye†† Qifeng Chen∗ Yang Gao†‡§ ∗HKUST †Tsinghua University ‡Shanghai Qi Zhi Institute Abstract Imitation learning is a class of promising policy learning algorithms that is free from many practical issues with reinforcement learning, such as the reward design issue and the exploration hardness. However, the current imitation algorithm strug- gles to achieve both high performance and high in-environment sample efﬁciency simultaneously. Behavioral Cloning (BC) does not need in-environment interac- tions, but it suffers from the covariate shift problem which harms its performance. Adversarial Imitation Learning (AIL) turns imitation learning into a distribution matching problem. It can achieve better performance on some tasks but it requires a large number of in-environment interactions. Inspired by the recent success of EfﬁcientZero in RL, we propose EfﬁcientImitate (EI), a planning-based imitation learning method that can achieve high in-environment sample efﬁciency and per- formance simultaneously. Our algorithmic contribution in this paper is two-fold. First, we extend AIL into the MCTS-based RL. Second, we show the seemingly incompatible two classes of imitation algorithms (BC and AIL) can be naturally uniﬁed under our framework, enjoying the beneﬁts of both. We benchmark our method not only on the state-based DeepMind Control Suite, but also on the image version which many previous works ﬁnd highly challenging. Experimental results show that EI achieves state-of-the-art results in performance and sample efﬁciency. EI shows over 4x gain in performance in the limited sample setting on state-based and image-based tasks and can solve challenging problems like Humanoid, where previous methods fail with small amount of interactions. Our code is available at https://github.com/zhaohengyin/EfficientImitate. 1 Introduction The real-world sequential decision process in robotics is highly challenging. Robots have to handle high dimensional input such as images, need to solve long horizon problems, some critical timesteps need highly accurate maneuver, and the learning process on the real robot has to be sample efﬁcient. Imitation learning is a promising approach to solving those problems, given a small dataset of expert demonstrations. However, current imitation algorithms struggle to achieve these goals simultaneously. There are two kinds of popular imitation learning algorithms, Behavior Cloning (BC) and Adversarial Imitation Learning (AIL). BC formulates imitation learning as a supervised learning problem. It needs no in-environment samples, but it suffers from the covariate shift issue [40], often leading to test time performance degradation. Adversarial Imitation Learning (AIL) [16, 6] casts imitation learning as a distribution matching problem. Though AIL suffers less from the covariate shift problem and can perform better than BC on some domains, it requires impractical number of online interactions [20, 23] and can perform badly on image inputs [37]. These drawbacks heavily limit its application in ﬁelds like robotics, where physical robot time matters. In summary, current imitation ∗zhaoheng.yin@connect.ust.hk, cqf@ust.hk †ywr20@mails.tsinghua.edu.cn, gaoyangiiis@tsinghua.edu.cn §Corresponding author. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). Figure 1: Left: The system-level overview of EfﬁcientImitate. The agent (yellow area) takes actions in the environment and stores the data in the replay buffer. The data in the replay buffer and expert buffer are then used to train a model and AIL reward. The planning module then searches for improved policy and value in each state in the replay buffer, based on which the policy and value networks are optimized. Right: The planning procedure. We use a continuous version EfﬁcientZero as our planner. We ﬁnd that MCTS uniquely beneﬁts from BC and can unify BC and AIL. For the expansion of each node, we sample actions from both the current policy (black arrow) and a BC policy (blue arrow). We use AIL reward (yellow cube) to encourage long-term distribution matching. MCTS searches (pink area) for best actions to maximize the cumulative AIL reward and update the estimated value and policy of the root node. The output of the planning procedure is the value estimate and the policy estimate of s, and the value and policy networks are optimized to ﬁt them. learning algorithms fail to achieve high online testing performance and high in-environment sample efﬁciency at the same time. Further, the two types of imitation algorithms seem to be incompatible since they are based on two completely different training objectives. Previous work ﬁnds that it is hard to unify them naively[34]. Inspired by the recent success in sample efﬁcient RL, such as EfﬁcientZero [52], we propose a planning-based imitation algorithm named EfﬁcientImitate (EI) that achieves high test performance and high in-environment sample efﬁciency at the same time. Our method extends AIL to a model- based setting with multi-step losses under the MCTS-based RL framework. Our algorithm also uniﬁes the two types of the previous imitation algorithms (BC and AIL) naturally, thanks to the planning component of our algorithm. Intuitively, BC gives a coarse solution that is correct most of the time but fails to match the expert’s behavior in the long term. On the other hand, AIL knows the goal of the learning, i.e., matching state action distribution, but doesn’t give the solution directly. Our method’s planning component uniﬁes those two methods and shows a signiﬁcant performance boost, especially in the harder tasks such as Humanoid. We illustrate the detailed procedure in Figure 1. We validate our idea not only on state-based tasks but also on image-based tasks, which relatively few previous algorithms can handle them [37]. EI achieves state-of-the-art results in sample efﬁciency and performance. It shows over 4x gain in performance in the limited sample setting on state-based and image-based tasks. On harder tasks such as Humanoid, the gain is even larger. A by-product of this paper is that we extend the recent EfﬁcientZero algorithm to the continuous action space. We open-source the code at https://github.com/zhaohengyin/EfficientImitate to facilitate future research. Our contributions in this paper are summarized as follows. • We present EfﬁcientImitate, a sample efﬁcient imitation learning algorithm based on MCTS. • We identify that MCTS can beneﬁt from BC by using BC actions during the search, which is crucial for challenging tasks. EfﬁcientImitate suggests a natural way to unify BC and AIL. • We conduct experiments in both the state and the image domain to evaluate EI. Experimental results show that EI can achieve state-of-the-art sample efﬁciency and performance. 2 2 Related Work 2.1 Imitation Learning Imitation learning (IL) aims to solve the sequential decision-making problems with expert demon- stration data. It has wide applications in games and robotics [4]. Compared with RL, one major beneﬁt of IL is that it can avoid the notorious reward design problem. IL can be divided into two branches: BC [1] and IRL [33]. To solve the covariate shift problem of BC, researchers propose methods like dataset aggregation [40] and noise injection [25]. But these methods either require extra expert queries or exert constraints over the learning process. A recent variant branch of IRL is the Adversarial Imitation Learning (AIL) [16, 6]. AIL models IL as a state-action distribution matching problem. Many works extend AIL by using better distribution metrics such as f -divergence [54] and Wasserstein distance [5]. Though these methods can learn better reward functions and somewhat speed up the training, they do not directly focus on AIL’s sample efﬁciency problem. Some works have drawn attention to sample efﬁciency, for example [23, 41, 55] propose to use off-policy training in AIL training to reduce sample complexity. ValueDICE [24] reformulates AIL objective in an ofﬂine min-max optimization process, but recent work points out that it is an improved form of BC [29]. VMAIL [37] uses a model-based approach to improve sample efﬁciency. It collects latent rollout with a variational model and reduces online interactions. MoBILE [22] also shows a provably sample efﬁcient model-based imitation approach. Compared with these methods, our method introduces MCTS planning to the off-policy imitation learning and uses it to unify BC and AIL to take advantage of both. The idea of combining BC and AIL helps to improve the sample efﬁciency can be traced back to GAIL[16]. GAIL suggests using BC to initialize the policy network, but [34] ﬁnds that this does not work well because the initialized BC knowledge will be corrupted in AIL training. Then, [20] proposes to use an annealed (decaying) BC loss to regularize policy training to solve this problem. But in this work, we ﬁnd that such a BC regularizer can be harmful to exploration when BC is incorrect. One concurrent work mitigates this issue by using adpative BC loss and replacing AIL reward with optimal-transport-based imitation reward [13]. We also notice that AlphaGo [46] involves BC in their method, but they focus on RL rather than IL, and BC is only used to initialize the policy. Different from these methods, EI uses BC actions as candidates in MCTS. 2.2 Sample Efﬁciency in RL The sample efﬁciency problem of imitation learning is closely related to that in RL. One line of work ﬁnds that the reward signal is not a good data source for representation learning in RL and is one reason for sample inefﬁciency. Then they utilize self-supervised representation learning to accelerate representation learning and improve the sample efﬁciency. Researchers propose to use contrastive learning [27], consistency-based learning [44, 53, 52], or pretrained representation [45] for this purpose. Some works also explore the possibility of applying self-supervised representation learning to imitation learning [4]. Another line of work focuses on RL with a learned model, which is promising for sample efﬁcient learning [8, 10–12, 19, 28, 31, 52, 14]. These approaches usually imagine additional rollouts with the learned model or use it as a more compact environment representation for RL. Besides, some works also ﬁnd that data augmentation can effectively improve sample efﬁciency [26, 50, 49]. EI also beneﬁts from these approaches. It includes a model and applies representation learning to boost sample efﬁciency. In application, people also consider to augment RL with the demonstration [15, 38, 32, 21] to improve the sample efﬁciency. This can be viewed as a combination of RL and imitation learning and favor RL on real robots. We believe that our method can also be extended to this setting. 3 Background 3.1 Setting We formalize the sequential decising making problem as Markov Decision Process M = (S, A, R, T ). Here, S is the state space, A is the action space, R is the reward function, and T is the transition dynamics. The agent’s state at timestep t is st ∈ S. The agent takes action at and 3 t=0 γtrt, where γ is a discount factor. receives reward rt = R(st, at). Its state at timestep t + 1 is then st+1 ∼ T (st, at). The objective of the agent is to maximize the return (cid:80)T In the imitation learning problem studied here, the agent has no access to the reward function R and transition dynamics T . It is provided with a ﬁxed expert demonstration dataset D = {τi}. Here, each τi = (sE T ) is an expert trajectory that can achieve high performance in M. The agent can not solicit extra expert demonstrations but can interact with the MDP, observing new states and actions, but not rewards. In this work, we deﬁne (in-environment) sample efﬁciency as the number of online interactions during training. We expect the agent to achieve high performance within a ﬁxed online sample budget. 1 , ...sE T , aE 0 , aE 1 , aE 0 , sE 3.2 BC and AIL BC considers imitation learning as a supervised learning problem. It trains a policy network π to minimize the following loss function: L = −E (sE i ,aE i )∼D log π(aE i |sE i ). (1) AIL treats imitation learning as a distribution matching problem. One typical AIL algorithm is GAIL. It trains a discriminator D to distinguish the agent generated state-action tuple (st, at) from those (sE i ) in the demonstration dataset by minimizing i , aE L = −E (st,at)∼ρ,(sE i ,aE i )∼D (cid:2)log(D(st, at)) + log(1 − D(sE i , aE i ))(cid:3) , (2) where ρ is the state-action distribution induced by the agent. Meanwhile, it trains the agent to maximize the return with respect to the adversarial reward rt = − log(1 − D(st, at)) using any on-policy RL algorithm. 3.3 MuZero and its Extensions Our planning method is based on MuZero [42] and its extensions. MuZero learns an environment model for MCTS. The model consists of an encoder network f , a dynamics network g, and a reward network R. It operates on abstract states [52]. Concretely, it gets the abstract state ht of the current state st by ht = f (st). It can then predicts the future abstract states recursively by ht+1 = g(ht, at), and the rewards by R(ht, at). Besides the model, MuZero also contains a policy network and a value network. The policy network provides a prior over the actions at each node, and the value network calculates the expected return of the node. MuZero uses the model, the policy network, and the value network to search for improved policy and value for each state with MCTS. We refer the readers to the original MuZero paper for details. Sampled MuZero Sampled MuZero [17] extends MuZero from the discrete action domain to the continuous action domain, which is of our interest in this paper. At each node s to expand, Sampled MuZero samples K actions {ai}K i=1 from current policy π(a|s). During the search, it selects action a∗ from the sampled actions that maximize the probabilistic upper conﬁdence bound a∗ = arg max a∈{ai} Q(s, a) + c(s)ˆπ(a|s) (cid:112)(cid:80) b N (s, b) 1 + N (s, a) , (3) (cid:80) where ˆπ(a|s) = 1 i δ(a, ai). Q(s, a) is the current Q-estimation of the pair (s, a). N (s, a) K denotes the times that this pair is visited in MCTS. c(s) is a weighting coeffcient. During policy optimization, MuZero minimizes the Kullback-Leibler divergence between the current policy π and the MCTS statistics πMCTS at the root node DKL(πMCTS||π). EfﬁcientZero We also apply EfﬁcientZero [52] in this paper. EfﬁcientZero improves the sample efﬁciency of MuZero by using a self-supervised representation learning method to regularize the hidden representation. It uses a SimSiam-style structure [3] to enforce the similarity between the predicted future representation and the real future representation. 4 EfﬁcientImitate In this section, we present our EfﬁcientImitate algorithm. We ﬁrst present an MCTS-based approach to solving the AIL problem in Section 4.1. Then we show a simple yet effective method to unify BC 4 Figure 2: Computation ﬂow of loss functions. Left: Multi-step Discriminator Loss. We do not distinguish between the calculation for expert and agent here, and use a superscript (E) to indicate that the computation applies to both. Right: Multi-step BC Loss. It applies to the expert sequences. and AIL with MCTS in Section 4.2. We brieﬂy discuss the implementation in Section 4.3, and the full details can be found in the Appendix. 4.1 Extending AIL to MCTS-based RL Traditionally, the adversarial imitation learning (AIL) algorithm trains a discriminator D between the policy samples and the expert samples and uses some form of D, such as − log(1 − D), as the reward function. Then some model-free RL algorithms are used to maximize the cumulative reward. In MCTS-based RL algorithms, such as MuZero [42] and EfﬁcientZero [52], the reward function is used in the value target computation and the MCTS search. The use in value target computation is similar to prior model-free RL algorithms, where the value target is computed with n-step value bootstrapping on the actual observations. However, during the MCTS search, the rewards are computed on the abstract state obtained by running the forward dynamics function ht+1 = g(ht, at) multiple times. If we were training the discriminator only on actual observations of the expert and the policy rollouts, the discriminator might not generalize well to abstract states outputted by the forward dynamics functions. Therefore, we train the discriminator with the model-based rollout. Speciﬁcally, we sample sequence (st, at+1, ..., at+n) in replay buffer B and expert sequence (sE t(cid:48)+n) in demonstration dataset D and minimizes following multi-step discriminator loss function: t(cid:48)+1, ..., aE t(cid:48) , aE t(cid:48) , aE LD = −E (st,at:t+n)∼B,(sE t(cid:48) ,aE t(cid:48) :t(cid:48)+n (cid:34) n (cid:88) i=0 )∼D log(D(ht+i, at+i)) + log(1 − D(hE t(cid:48)+i, aE (cid:35) t(cid:48)+i)) . (4) Here, ht+i (and ht(cid:48)+i) terms are produced by the forward dynamics in EfﬁcientZero (Figure 2). We use the GAIL transition reward R(h, a) = − log(1 − D(h, a)), and then the MCTS planner searches for action that can maximize cumulative GAIL reward to guarantee long-term distribution matching. Note that V-MAIL also propose a similar discriminator training technique, but under the Dreamer [11] model. Besides, since the discriminator’s input is based on the representation rather than raw input, the discriminator should be trained with the encoder jointly. However, this can lead to a highly non- stationary reward during the bootstrap value calculation. To mitigate this issue, we also propose to use a target discriminator for bootstrap value calculation. This can make the training more stable. Though we use the GAIL reward here, one can also use other kinds of AIL and IRL reward functions proposed in recent research. Using the GAIL reward can already achieve satisfactory performance in our experiments. When the real reward presents, one may also combine this into planning [21]. This may favor application scenarios where handcrafting a reward function is not hard. We do not study this case here and leave it to future work. 4.2 Unifying BC and AIL in MCTS As discussed in related work, researchers realize that using BC can improve AIL’s sample efﬁciency by providing a good initial BC policy or constraining the policy to BC. However, these existing solutions are not good enough in practice. The main pitfall in these methods is that BC knowledge in the policy network is destined to be forgotten if the policy network is trained with the AIL objective, and then BC will no longer be helpful [34]. 5 We observe that MCTS can naturally unify the BC and AIL methods, enjoying the beneﬁt of both and being free from this pitfall. We propose to plug BC actions into MCTS as candidates at each node and use a planning process to search for an improved solution. This time, the BC actions are consistently considered throughout the entire training procedure without being forgotten. Concretely, we train a BC policy πBC and use a mixture policy ˜π for the sampling at each node in MCTS: ˜π = απBC + (1 − α)π. (5) α is a mixture factor, which is ﬁxed during training and π is the current policy. We use α = 0.25 in this paper. This ensures that a small fraction of action samples are from the BC policy. During planning, the BC actions are evaluated and will be frequently visited and selected as output if they can lead to long-term distribution matching. This can then reduce the effort of ﬁnding good expert-like actions from scratch as desired. Moreover, another unique advantage of this procedure is that it does not fully trust BC like [20], which forces the output of the policy network to be close to BC. When BC is wrong due to covariate shifts or insufﬁcient demonstrations, it can neglect these BC actions and allow the policy to search for better actions. This ensures that BC does not hurt training. However, due to the conceptual simplicity, one arising question is whether this approach can be applied to other model-based methods. Here, we take Dreamer [11] as an example. Though Dreamer builds a model of the environment, it only uses the model to roll-out the policy for policy optimization. In other words, the model is not used to evaluate whether a speciﬁc BC action is good or not in the long term, so our idea can not be applied directly to Dreamer. From this example, we see that the core of our idea is to leverage the power of planning, only with which the long-term outcomes of certain (BC) actions can be calculated. For the training of πBC, we minimize the following multi-step BC objective (Figure 2): LBC = E (sE t(cid:48) ,aE t(cid:48):t(cid:48)+n )∼D − log(πBC(aE t(cid:48)+i|hE t(cid:48)+i)) . (6) i=0 This is to avoid distributional shifts during multi-step prediction in MCTS. For the training of the policy, we still minimize DKL(πMCTS||π). (cid:34) n (cid:88) (cid:35) Note that the BC design proposed here is not coupled with AIL. It can go beyond imitation learning and be applied in other robot learning settings, such as RL with demonstration [38]. 4.3 Implementation We ﬁrst implement a continuous version EfﬁcientZero for planning, and the details can be found in the Appendix. The BC policy network is simply a duplicate of the policy network. The discriminator and BC policy networks share the same encoder network with the policy network and value network. The overall loss function for optimization is L = LEZ + λdLD + λbcLBC. (7) LEZ is EfﬁcientZero’s loss function (excluding reward loss). All the networks are trained jointly to minimize this loss function 7. We use the Reanalyze algorithm [43, 52] for ofﬂine training, and we require that all the samples should be reanalyzed. Figure 3: Part of the tasks used in our experiments. From left to right: Reacher, Finger Spin, Cheetah Run, Walker Walk, Hopper Hop, Humanoid Walk. 5 Experiments In this section, we evaluate the sample efﬁciency of the proposed method. We measure the sample efﬁciency by evaluating the performance of an algorithm at a small number of online samples. We also analyze the effect of the BC actions and planning. 6 Table 1: Evaluation result on the state-based DeepMind Control Suite. We use the average score on three random seeds. Our method can achieve the state of the art result compared with the baselines. Task Cartpole Budget BC DAC ValueDICE SQIL Ours 10k 0.59 0.13 ±0.12 0.21 ±0.01 0.23 ±0.01 0.98 ±0.01 Ball 10k 0.44 0.18 ±0.01 0.23 ±0.01 0.27 ±0.05 0.99 ±0.01 Reacher Finger Cheetah Walker Hopper Humanoid 50k 0.83 0.22 ±0.02 0.15 ±0.01 0.21 ±0.02 0.90 ±0.04 50k 0.76 0.53 ±0.05 0.04 ±0.01 0.02 ±0.00 0.99 ±0.00 50k 0.58 0.33 ±0.04 0.50 ±0.08 0.05 ±0.01 0.96 ±0.02 50k 0.16 0.26 ±0.04 0.54 ±0.09 0.11 ±0.03 1.03 ±0.01 50k 0.03 0.00 ±0.00 0.03 ±0.00 0.24 ±0.10 0.92 ±0.02 500k 0.11 0.01 ±0.00 0.00 ±0.00 0.06 ±0.01 0.74 ±0.04 Table 2: Evaluation result on the image-based DeepMind Control Suite. We use the average score on three random seeds. Our method can achieve state-of-the-art results compared with the baselines. Task Cartpole Budget BC DAC ValueDICE SQIL VMAIL Ours 50k 0.30 0.08 ±0.01 0.18 ±0.02 0.26 ±0.03 0.57 ±0.03 0.94 ±0.02 Ball 50k 0.32 0.26 ±0.02 0.27 ±0.02 0.77 ±0.05 0.61 ±0.11 0.93 ±0.01 Finger Cheetah Reacher Walker Hopper 50k 0.14 0.00 ±0.00 0.01 ±0.00 0.00 ±0.01 0.06 ±0.03 1.00 ±0.01 50k 0.37 0.04 ±0.01 0.06 ±0.01 0.06 ±0.00 0.13 ±0.04 0.92 ±0.01 100k 0.26 0.25 ±0.05 0.15 ±0.02 0.36 ±0.04 0.34 ±0.02 0.86 ±0.06 100k 0.15 0.10 ±0.02 0.08 ±0.00 0.32 ±0.05 0.24 ±0.07 0.98 ±0.01 200k 0.02 0.01 ±0.00 0.00 ±0.00 0.04 ±0.02 0.07 ±0.04 0.70 ±0.01 5.1 Setup We use the DeepMind Control Suite [47] for evaluation. We use the following tasks: Cartpole Swingup, Reacher Easy, Ball-in-cup Catch, Finger Spin, Cheetah Run, Walker Walk, Hopper Hop, and Humanoid Walk. We conduct both state-based and image-based experiments. Note that many previous imitation learning works use the OpenAI Gym [2] version of these tasks for evaluation. We ﬁnd that the DMControl version used here brings extra challenges by using more challenging initial states. Take the Walker task as an example; the initial state in OpenAI Gym is standing. However, in DMControl, the agent’s initial state is lying on the ground, and the agent should also learn to stand up ﬁrst from very limited data. For the state-based experiments, we allow 10k-50k online steps in the environment based on the difﬁculty of each task. Since learning a robust and meaningful visual representation requires more data for image-based experiments, we allow 50k-100k online steps. Detailed setup will be shown in the result. We train SAC [9] policies to collect expert demonstrations for imitation learning. The expert demonstrations are not subsampled. We use 5 demonstrations in the state-based experiment, except for Reacher and Humanoid, where we use 20 demonstrations. We use 20 demonstrations in the image-based experiments. 5.2 Baselines (1) DAC DAC [23] is an We present several baselines of sample efﬁcient imitation learning. adversarial off-policy imitation learning method. It matches the distribution of the replay buffer and that of the expert demonstration dataset using the TD3 [7] algorithm. (2) SQIL SQIL [39] is a non-adversarial off-policy imitation learning method. It labels all the expert transitions with reward 1 and non-expert transitions with reward 0. Then it trains a SAC policy over these relabeled data. SQIL is a regularized form of BC. (3) ValueDICE ValueDICE [24] considers imitation learning as a distribution matching problem and solves it with a min-max optimization process. (4) VMAIL VMAIL [37] is a model-based visual imitation learning method. It learns a variational model for simulating on-policy rollouts. We only evaluate VMAIL on the image-based domain, as they did in 7 Figure 4: The performance curve on the state-based tasks. The results are averaged over three seeds. The shaded area displays the range of one standard deviation. Figure 5: The performance curve on the image-based tasks. The results are averaged over three seeds. The shaded area displays the range of one standard deviation. the original paper. Besides the online imitation learning baselines, we also include BC as an ofﬂine baseline. 5.3 Results State-based experiments Table 1 shows the state-based experiments’ evaluation results within the given budget. We also plot the performance curve of four challenging tasks in Figure 4. The performance is normalized to 0.0 and 1.0 with respect to the performance of the random agent and the expert. We ﬁnd that our proposed method can outperform all the baselines by a large margin. Except for BC, these baselines methods could hardly learn meaningful behaviors using a limited online budget. We ﬁnd that DAC is a strong baseline. Its performance can grow to the expert in 200k samples in most tasks except Hopper, where it will eventually get stuck. Our method is much more sample efﬁcient than the best of these baseline methods. For some tasks like Walker Walk and Cheetah Run, our method only requires about 20k steps to reach near-expert performance, equivalent to 80 online trajectories (around 0.5 hours in real). This result is notable for the robotics community. It shows that online imitation learning is possible with only a handful of trials, and applying it directly on a real locomotion robot is possible. Image-based experiments So far, image-based tasks are still challenging for adversarial imitation learning algorithms, and the evaluation of most of the prior AIL works is carried out in the state-based tasks. Table 2 shows the evaluation result within the given budget in the image-based experiments (see Figure 5 for curves). Our method can also learn expert behavior given a slightly larger budget. Still, most of the baselines fail to match experts’ behavior using the given budget. We notice an inherent difﬁculty in learning a robust visual representation for adversarial training in the limited data set in image-based tasks. Discriminator can judge whether a behavior is expert-like using various possible features in this case. Solving this open problem is out of the scope of this paper. In the presence of such a difﬁculty, EI can still achieve good sample efﬁciency in most of the tasks. 8 Finger Cheetah Walker Hopper e c n a m r o f r e P 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 BC DAC SQIL ValueDICE Ours 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K Finger Cheetah Walker Hopper e c n a m r o f r e P 1.0 0.8 0.6 0.4 0.2 0.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 BC DAC SQIL ValueDICE VMAIL Ours 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K Figure 6: The performance curves of our method with and without BC actions at each expansion. The plots are sorted according to the difﬁculty of the corresponding task. The leftmost one is the most difﬁcult task, Humanoid. The results are averaged over three seeds. BC actions have a great impact on the sample efﬁciency and performance. 5.4 Analysis Effect of BC We carry out ablation analysis on the BC to see whether it helps in our method. We set α = 0 to remove all the BC actions and see how our method’s performance and sample efﬁciency will change. The result is shown in Figure 6. We ﬁnd that the performance of our method degrades after we remove all the BC actions in MCTS. The effect is task-speciﬁc, and we ﬁnd BC is more helpful in those challenging tasks. For example, in tasks that are high-dimensional or have a hard exploration process like Humanoid and Hopper, removing BC actions will make the learning process stuck in local minima. At that local minima, the agent struggles to ﬁnd the correct action that matches the expert’s behavior. Although removing BC actions does not trap the agent in local minima in some relatively simpler tasks like Cheetah and Walker, it slows down the training. It doubles the number of online interactions to reach expert performance. This result conﬁrms that using BC actions can indeed provide a good solution to the distribution matching problem, which can help to speed up learning and improve performance. We also notice that even when we remove the BC actions, the method is still able to outperform the previous baselines; this suggests that planning with AIL alone is also powerful. Other Ways to use BC We then study another two variants of using BC: (1). BC-Ann. This variant does not use BC actions in MCTS but exerts an annealing BC loss to the pol- icy network like [20]. (2). BC-Rep. This variant does not use BC actions in MCTS but still uses BC loss to regularize the representation. We test these variants on the Humanoid Walk (Figure 7). We ﬁnd that these variants do not lead to an essential improvement. For BC-Ann, it harms the performance in the early stage (before 100k) since the BC regularization will constrain the agent’s policy near the BC policy, which contains an error and hinders learning. The agent only starts to acquire meaningful behavior after the regularization decays, but at that time, BC does not help much and can not lead to improvement. Compared with BC-Ann, BC-Reg is more helpful here. This is possibly because BC-Reg makes the encoder focus on more useful features. However, BC-Reg still gets stuck in a local minimum. This result suggests that using BC actions directly for exploration can be essential for improving AIL. Using BC simply as a regularizer may not be the ideal approach though it can be useful sometimes. Figure 7: Results of different ways of using BC. Ablation of Planning In this part, we study to what extent planning can help to learn and whether insufﬁcient search in MCTS leads to inferior performance. We study K, the number of sampled actions at each node, and N , the number of simulations. The default value of K and N in the previous experiments are 16 and 50. We sweep K ∈ {4, 8, 16, 24} and N ∈ {5, 10, 25, 50} to evaluate their effects. We collect the result on the state-based Cheetah, Walker, and Hopper task and report the averaged relative performance change at the given budget used in previous experiments (see Table 3). The general trend is that larger K and N lead to better imitation results. We ﬁnd that varying K only affects the performance a little, and K = 4 can also work well. Compared with K, N has a larger 9 Humanoid Hopper Cheetah Walker Finger e c n a m r o f r e P 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 Ours (w/o BC) Ours (w/ BC) 0 200K 400K 0 20K 40K 0 20K 40K 0 20K 40K 0 20K 40K Steps Steps Steps Steps Steps 0.8 e c n a m r o f r e P 0.6 0.4 0.2 0.0 Humanoid Ours (w/o BC) Ours (BC Ann.) Ours (BC Rep.) Ours (w/ BC) 0 100K 200K 300K 400K 500K Steps impact. When the number of simulations becomes small, the performance drops signiﬁcantly. This result also explains why we can achieve a large improvement over DAC even without BC. Table 3: Ablation of planning. We report the relative change of performance at the given budget. Param K = 4 K = 8 K = 16 K = 24 N = 5 N = 10 N = 25 N = 50 Result −8.4% −3.2% 0.0% 1.1% −27.3% −15.5% −4.6% 0.0% 6 Discussion In this paper, we presented EfﬁcientImitate, an MCTS-based imitation learning algorithm. We extended AIL to a model-based setting and solved it with MCTS in a sample-efﬁcient way. Moreover, we proposed a method to unify BC and AIL in MCTS, enjoying the beneﬁts of both. Experimental results in state-based and image-based tasks showed that EfﬁcientImitate can achieve state-of-the-art sample efﬁciency and performance. Limitations One limitation of this work is that the computation process of MCTS is more expensive compared with that of the model-free methods, though this is a common issue of model-based methods. One possible approach to mitigate this issue can be using better MCTS acceleration methods [51]. Besides, in this paper we did not study the long horizon problem with multiple objects, which is a common case in the robotic manipulation. However, this requires the model to predict the interaction with multiple objects, which is still a challenging open problem in the learning community [36] and orthogonal to our contribution. We believe that our framework can be combined with the works in this ﬁeld to handle this challenge. Future Work There are many problems to study along our direction. First, since we only use the vanilla AIL algorithm here, it is interesting to see if using more advanced algorithms such as optimal-transport-based learning [5] will make our algorithm more powerful. Second, due to the modularity of our method, one can try to extend EfﬁcientImitate to more general settings like RL with demonstration, which will also favor the application scenarios. Third, in this work we consider an online learning setting, one possible future direction is to study the use of EfﬁcientImitate on the existing ofﬂine interaction dataset to further reduce the dependence on in-environment samples. In conclusion, we believe that this work shows a promising direction and opens up new possibilities for model-based methods in robot learning. Acknowledgments and Disclosure of Funding This work is supported by the Ministry of Science and Technology of the People’s Republic of China, the 2030 Innovation Megaprojects “Program on New Generation Artiﬁcial Intelligence” (Grant No. 2021AAA0150000). This work is also supported by a grant from the Guoqiang Institute, Tsinghua University. References [1] M. Bain and C. Sammut. A framework for behavioural cloning. In Machine Intelligence 15, pages 103–129, 1995. [2] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [3] X. Chen and K. He. Exploring simple siamese representation learning. In IEEE Conference on Computer Vision and Pattern Recognition, 2021. [4] X. Chen, S. Toyer, C. Wild, S. Emmons, I. Fischer, K.-H. Lee, N. Alex, S. H. Wang, P. Luo, S. Russell, et al. An empirical investigation of representation learning for imitation. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. 10 [5] R. Dadashi, L. Hussenot, M. Geist, and O. Pietquin. Primal wasserstein imitation learning. In International Conference on Learning Representations, 2021. [6] J. Fu, K. Luo, and S. Levine. Learning robust rewards with adversarial inverse reinforcement learning. In International Conference on Learning Representations, 2018. [7] S. Fujimoto, H. Hoof, and D. Meger. Addressing function approximation error in actor-critic methods. In International Conference on Machine Learning, 2018. [8] C. Gelada, S. Kumar, J. Buckman, O. Nachum, and M. G. Bellemare. Deepmdp: Learning continuous latent space models for representation learning. In International Conference on Machine Learning, 2019. [9] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, 2018. [10] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, 2019. [11] D. Hafner, T. P. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020. [12] D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021. [13] S. Haldar, V. Mathur, D. Yarats, and L. Pinto. Watch and match: Supercharging imitation with regularized optimal transport. In Conference on Robot Learning, 2022. [14] N. Hansen, X. Wang, and H. Su. Temporal difference learning for model predictive control. In International Conference on Machine Learning, 2022. [15] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan, J. Quan, A. Sendonaris, I. Osband, et al. Deep q-learning from demonstrations. In AAAI Conference on Artiﬁcial Intelligence, 2018. [16] J. Ho and S. Ermon. Generative adversarial imitation learning. In Neural Information Processing Systems, 2016. [17] T. Hubert, J. Schrittwieser, I. Antonoglou, M. Barekatain, S. Schmitt, and D. Silver. Learning and planning in complex action spaces. In International Conference on Machine Learning, 2021. [18] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, 2015. [19] M. Janner, J. Fu, M. Zhang, and S. Levine. When to trust your model: Model-based policy optimization. In Neural Information Processing Systems, 2019. [20] R. Jena, C. Liu, and K. P. Sycara. Augmenting GAIL with BC for sample efﬁcient imitation learning. In Conference on Robot Learning, 2020. [21] B. Kang, Z. Jie, and J. Feng. Policy optimization with demonstrations. In International Conference on Machine Learning, 2018. [22] R. Kidambi, J. Chang, and W. Sun. Mobile: Model-based imitation learning from observation alone. In Neural Information Processing Systems, 2021. [23] I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson. Discriminator-actor- critic: Addressing sample inefﬁciency and reward bias in adversarial imitation learning. In International Conference on Learning Representations, 2019. [24] I. Kostrikov, O. Nachum, and J. Tompson. Imitation learning via off-policy distribution matching. In International Conference on Learning Representations, 2020. 11 [25] M. Laskey, J. Lee, R. Fox, A. Dragan, and K. Goldberg. Dart: Noise injection for robust imitation learning. In Conference on Robot Learning, 2017. [26] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Reinforcement learning with augmented data. In Neural Information Processing Systems, 2020. [27] M. Laskin, A. Srinivas, and P. Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, 2020. [28] A. X. Lee, A. Nagabandi, P. Abbeel, and S. Levine. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. Neural Information Processing Systems, 2020. [29] Z. Li, T. Xu, Y. Yu, and Z.-Q. Luo. Rethinking valuedice: Does it really improve performance? In International Conference on Learning Representations, 2022. [30] A. L. Maas, A. Y. Hannun, A. Y. Ng, et al. Rectiﬁer nonlinearities improve neural network acoustic models. In International Conference on Machine Learning, 2013. [31] Y. Mu, Y. Zhuang, B. Wang, G. Zhu, W. Liu, J. Chen, P. Luo, S. Li, C. Zhang, and J. Hao. Model- based reinforcement learning via imagination with derived memory. In Neural Information Processing Systems, 2021. [32] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In IEEE International Conference on Robotics and Automation, 2018. [33] A. Y. Ng, S. J. Russell, et al. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning, 2000. [34] M. Orsini, A. Raichuk, L. Hussenot, D. Vincent, R. Dadashi, S. Girgin, M. Geist, O. Bachem, O. Pietquin, and M. Andrychowicz. What matters for adversarial imitation learning? In Neural Information Processing Systems, 2021. [35] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Neural Information Processing Systems, 2019. [36] H. Qi, X. Wang, D. Pathak, Y. Ma, and J. Malik. Learning long-term visual dynamics with region proposal interaction networks. In International Conference on Learning Representations, 2021. [37] R. Rafailov, T. Yu, A. Rajeswaran, and C. Finn. Visual adversarial imitation learning using variational models. In Neural Information Processing Systems, 2021. [38] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. In Robotics: Science and Systems, 2018. [39] S. Reddy, A. D. Dragan, and S. Levine. SQIL: imitation learning via reinforcement learning with sparse rewards. In International Conference on Learning Representations, 2020. [40] S. Ross and D. Bagnell. Efﬁcient reductions for imitation learning. In International Conference on Artiﬁcial Intelligence and Statistics, 2010. [41] F. Sasaki, T. Yohira, and A. Kawaguchi. Sample efﬁcient imitation learning for continuous control. In International Conference on Learning Representations, 2018. [42] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lock- hart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020. [43] J. Schrittwieser, T. Hubert, A. Mandhane, M. Barekatain, I. Antonoglou, and D. Silver. Online and ofﬂine reinforcement learning by planning with a learned model. In Neural Information Processing Systems, 2021. 12 [44] M. Schwarzer, A. Anand, R. Goel, R. D. Hjelm, A. C. Courville, and P. Bachman. Data-efﬁcient reinforcement learning with self-predictive representations. In International Conference on Learning Representations, 2021. [45] M. Schwarzer, N. Rajkumar, M. Noukhovitch, A. Anand, L. Charlin, R. D. Hjelm, P. Bachman, and A. C. Courville. Pretraining representations for data-efﬁcient reinforcement learning. In Neural Information Processing Systems, 2021. [46] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016. [47] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. [48] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(11), 2008. [49] D. Yarats, R. Fergus, A. Lazaric, and L. Pinto. Mastering visual continuous control: Im- In International Conference on Learning proved data-augmented reinforcement learning. Representations, 2022. [50] D. Yarats, I. Kostrikov, and R. Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations, 2020. [51] W. Ye, P. Abbeel, and Y. Gao. Spending thinking time wisely: Accelerating mcts with virtual expansions. In Neural Information Processing Systems, 2022. [52] W. Ye, S. Liu, T. Kurutach, P. Abbeel, and Y. Gao. Mastering atari games with limited data. In Neural Information Processing Systems, 2021. [53] T. Yu, C. Lan, W. Zeng, M. Feng, Z. Zhang, and Z. Chen. Playvirtual: Augmenting cycle- consistent virtual trajectories for reinforcement learning. In Neural Information Processing Systems, 2021. [54] X. Zhang, Y. Li, Z. Zhang, and Z.-L. Zhang. f-gail: Learning f-divergence for generative adversarial imitation learning. In Neural Information Processing Systems, 2020. [55] Z. Zhu, K. Lin, B. Dai, and J. Zhou. Off-policy imitation learning from observations. In Neural Information Processing Systems, 2020. 13 A Implementation In this part, we introduce our detailed implementation of EfﬁcientImitate. Our code is available at https://github.com/zhaohengyin/EfficientImitate. A.1 Model Details Our model is based on the EfﬁcientZero [52] model. It is composed of several neural networks: rep- resentation network f , dynamics network g, value network V , policy network π, BC policy network πBC, discriminator network D, projector network, and predictor network. We use PyTorch [35] to implement these networks. Their detailed structures are as follows. A.1.1 State-based experiments Representation Network The representation network is an MLP with one hidden layer of size 256. Its output dimension is 128. It uses LeakyReLU [30] as the hidden layer’s activation function. The output activation function is identity. Dynamics Network The dynamics network is an MLP with one hidden layer of size 256. It con- catenates the representation and the action as the input. Its output dimension is 128 (representation’s dimension). It uses LeakyReLU as the hidden layer’s activation function. The output activation function is identity. Value Network The value network is an MLP with one hidden layer of size 128 (256 for Humanoid). It uses LeakyReLU as the hidden layer’s activation function. The output activation function is identity. We use the categorical value representation introduced in MuZero [42]. The value prediction is discretized into 401 bins to represent the value between [−100, 100]. Therefore, the output dimension of value network is 401. Policy & BC Network The policy and the BC policy networks are both MLPs with one hidden layer of size 128 (256 for Humanoid). These MLPs use LeakyReLU as the hidden layer’s activation function. Their output activation functions are identity. They produce a Squarshed-Normal (Tanh- Normal) action distribution [9], which is determined by a predicted mean and a predicted logstd. Therefore, the output dimension is twice of the dimension of the action space. Discriminator Network The discriminator network is an MLP with one hidden layer of size 128. It uses LeakyReLU as hidden layer’s activation function. The output activation function is sigmoid. Projector Network The projector network is an MLP with one hidden layer of size 512. It uses ReLU [30] as the hidden layer’s activation function. The output activation function is identity. The output dimension (projection dimension) is 128. Predictor Network The predictor network is an MLP with one hidden layer of size 512. It uses ReLU as the hidden layer’s activation function. The output activation function is identity. The output dimension is 128. A.1.2 Image-based experiments Representation Network The representation network consists of a four layer convolutional neural network and an MLP. Its structure is as follows. • Convolution layer. Input dim: 12. Output dim: 32. Kernel Size: 5. Stride: 2. Padding: 2. • ReLU activation. • Convolution layer. Input dim: 32. Output dim: 32. Kernel Size: 3. Stride: 2. Padding: 1. • ReLU activation. • Convolution layer. Input dim: 32. Output dim: 32. Kernel Size: 3. Stride: 2. Padding: 1. • ReLU activation. 14 • Convolution layer. Input dim: 32. Output dim: 32. Kernel Size: 3. Stride: 2. Padding: 1. • Flatten. • Linear layer. Output dim: 128. • ReLU activation. Dynamics Network The dynamics network is an MLP with one hidden layer of size 256. It con- catenates the representation and the action as the input. Its output dimension is 128 (representation’s dimension). It uses ReLU as the hidden layer’s activation function. The output activation function is also ReLU. Value Network The value network is an MLP with two hidden layers of size 100. It uses ReLU as the hidden layer’s activation function. The output activation function is identity. We use the categorical value representation introduced in MuZero. The value prediction is discretized into 401 bins to represent the value between [−40, 40]. Therefore, the output dimension of value network is 401. Policy & BC Network The policy and the BC policy networks are both MLPs with two hidden layers of size 100. These MLPs use ReLU as the hidden layer’s activation function. Their output activation functions are identity. They produce a Squarshed-Normal action distribution, which is determined by a predicted mean and a predicted logstd. Therefore, the output dimension of them is twice of the dimension of the action space. Discriminator Network The discriminator network is an MLP with one hidden layer of size 100. It uses ReLU as hidden layer’s activation function. The output activation function is sigmoid. Projector Network The projector network is an MLP. Its structure is as follows. • Linear layer. Output dim: 1024. BatchNorm [18] with momentum 0.1. ReLU activation. • Linear layer. Output dim: 1024. BatchNorm with momentum 0.1. ReLU activation. • Linear layer. Output dim: 1024. BatchNorm with momentum 0.1. Predictor Network The predictor network is an MLP. Its structure is as follows. • Linear layer. Output dim: 512. BatchNorm with momentum 0.1. ReLU activation. • Linear layer. Output dim: 1024. A.2 MCTS Details Our MCTS implementation is mainly based on the Sampled MuZero [17]. We also apply modiﬁca- tions proposed by the EfﬁcientZero. The detailed procedure is as follows. Expansion For the task having a continuous action space, we can not enumerate all the possible actions at a node as the original MCTS algorithm. To solve this problem, we use the sampling method proposed by the Sampled MuZero. For the expansion of a node s (in the representation space), we sample K actions {ai}K i=1 from current policy π(a|s). In this work we propose to integrate BC actions into MCTS, then we actually sample from ˜π(a|s) := (1 − α)π(a|s) + απBC(a|s). (8) Here α = 0.25 is a mixture factor. Selection For the action selection, we selects action a∗ from the sampled actions that maximize the probabilistic upper conﬁdence bound a∗ = arg max a∈{ai} Q(s, a) + c(s)ˆπ(a|s) (cid:112)(cid:80) b N (s, b) 1 + N (s, a) , (9) 15 where ˆπ(a|s) = 1 K denotes the times that this pair is visited in MCTS. c(s) is a weighting coeffcient deﬁned by i δ(a, ai). Q(s, a) is the current Q-estimation of the pair (s, a). N (s, a) (cid:80) c(s) = c1 + log 1 + c2 + (cid:80) c2 b N (s, b) , (10) where c1 = 1.25, c2 = 19625. To encourage exploration, we also inject Dirichlet noise to ˆπ(a|s) at the root node. So ˆπ(a|s) becomes ˆπ(a|s) := (1 − ρ)ˆπ(a|s) + ρND(ξ). (11) Here, ρ = 0.25 is a mixture factor. ND(ξ) is the Dirichlet distribution, and ξ is set to 0.3. At the root node of MCTS, we use the discriminator network D and value network V to calculate Q-value by its original deﬁnition: Q(s, a) = R(s, a) + γV (g(s, a)). At the other nodes, we use the mean Q-value calculation used by the EfﬁcientZero. Simulation & Backup The simulation and the backup process is the same as EfﬁcientZero’s implementation, and we refer the readers to EfﬁcientZero for details. A.3 Training Details and Hyperparameters Finally, we introduce some important training details and the hyperparameters. Initialization We initialize the weights and biases of the last layer of policy, BC policy, value, and discriminator network to be zero. The other parameters are initialized by the default initializers in PyTorch. Discriminator tor [34]. We also apply gradient penalty to the discriminator network. In AIL training, it is very useful to apply the gradient penalty to the discrimina- Target Update We propose to use a target model for the calculation of policy, value, and AIL reward during reanalyze. The target model is updated periodically during training subject to an update frequency. The training hyperparameters used in the state-based experiments are in Table 4. The training hyperparameters used in the image-based experiments are in Table 5. B Environment Details B.1 State-based experiments The setup of each task in the state-based experiments is in Table 6. B.2 Image-based experiments The setup of of each task in the image-based experiments is in Table 7. We use a 48 × 48 resolution in the image-based experiments. C Other Ablations We also perform ablations on the target discriminator and the multi-step discriminator loss. To evaluate the effect of the target discriminator, we use the latest model to calculate AIL reward in the value target. To evaluate the effect of the multi-step discriminator loss, we replace it with the single-step discriminator loss. We conduct experiments on the state-based and image-based Walker and Cheetah. The results are shown in Figure 9. We ﬁnd that removing these components will not only lead to instability in training but also harm the performance. Compared with the target discriminator, the multi-step discriminator loss has a larger impact on the image-based tasks. 16 D Computation Resources All of our experiments are conducted on a server with 4 NVIDIA RTX 3090 GPUs, 64 CPU cores, and 256GB RAM. For the most of state-based and image-based experiments except Humanoid Walk and image-based Hopper Hop, our experiments require 12-18 hours of training. The main bottleneck is at the Reanalyse [43, 52], where the minibatch cannot be produced and sent to the training loop at a high frequency. We are improving the computation efﬁciency by using better parallel computation implementation and applying MCTS speed up techniques. E Visualization One approach to interpret the learned model is by the t-SNE [48] plot. We use the image-based Walker experiment as an example. We use the trained model at 100k env steps to generate the state embeddings of one expert trajectory, and in environment trajectory at 0k, 25k, 50k, 75k, and 100k steps. Then we use t-SNE to visualize the embeddings on the 2D plane. As is shown in the Figure 8, the agent’s trajectory gradually matches expert’s trajectory (blue) during training. Moreover, the expert’s trajectory has a circle structure, which represents the periodic pattern of the Walker’s walking behavior. Therefore, our model can represent the environment in a meaningful way. Figure 8: The t-SNE plot of the learned state embeddings. Figure 9: The ablation of target discriminator and multi-step discriminator loss. The results are averaged over three seeds. The shaded area displays the range of one standard deviation. 17 Cheetah (State) Walker (State) Cheetah (Image) Walker (Image) 1.0 0.8 e c n a m r o f r e P 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 Ours(No Target) Ours(No Multi) Ours 0 10K 20K 30K 40K 50K 0 10K 20K 30K 40K 50K 0 10K 20K 30K 40K 50K 0 20K 40K 60K 80K 100K Steps Steps Steps Steps Table 4: Hyperparameters for the state-based experiments Discount Factor Minibatch Size Optimizer Optimizer: Learning Rate Optimizer: Momentum Optimizer: Weight Decay Maximum Gradient Norm Unroll Steps TD Steps BC Loss Coeff. Value Loss Coeff. Policy Loss Coeff. Discriminator Loss Coeff. Gradient Penalty Target Update Interval Consistency Loss Coeff. Reanalyze Ratio Number of Simulations in MCTS Number of Sampled Actions BC Ratio α 0.99 256 SGD 0.01 0.9 1e-4 10 5 1 0.01 1.0 1.0 0.1 (1.0 for Humanoid) 1.0 200 2.0 1.0 50 16 0.25 Table 5: Hyperparameters for the image-based experiments Discount Factor Minibatch Size Stacked Frames Optimizer Optimizer: Learning Rate Optimizer: Momentum Optimizer: Weight Decay Maximum Gradient Norm Unroll Steps TD Steps BC Loss Coeff. Value Loss Coeff. Policy Loss Coeff. Discriminator Loss Coeff. Gradient Penalty Target Update Interval Consistency Loss Coeff. Reanalyze Ratio Number of Simulations in MCTS Number of Sampled Actions BC Ratio α 18 0.99 128 4 SGD 0.02 0.9 1e-4 10 5 1 0.01 1.0 1.0 0.1 1.0 200 20.0 1.0 50 16 0.25 Table 6: Task setup in the state-based experiments Task Action Repeat Expert Performance Cartpole Swingup Ball-in-cup Catch Reacher Easy Finger Spin Walker Walk Cheetah Run Hopper Hop Humanoid Walk 8 4 4 4 4 4 4 2 881.3 920.1 911.4 574.2 865.6 607.3 300.0 782.8 Table 7: Task setup in the state-based experiments Task Action Repeat Expert Performance Cartpole Swingup Ball-in-cup Catch Reacher Easy Finger Spin Walker Walk Cheetah Run Hopper Hop 881.3 920.1 911.4 574.2 873.5 607.3 300.0 8 4 4 4 2 4 4 19",1.0
"Artificial Intelligence-Based Methods for Fusion of Electronic Health
  Records and Imaging Data","[{'href': 'http://arxiv.org/abs/2210.13462v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.13462v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-10-23 07:13:37,,"2 2 0 2 t c O 4 2 ] R C . s c [ 1 v 3 6 2 3 1 . 0 1 2 2 : v i X r a Driver Locations Harvesting Attack on pRide Shyam Murthy[0000−0002−0222−322X] Srinivas Vivek[0000−0002−8426−0859] IIIT Bangalore, India. shyam.sm@iiitb.ac.in srinivas.vivek@iiitb.ac.in Abstract. Privacy preservation in Ride-Hailing Services (RHS) is in- tended to protect privacy of drivers and riders. pRide, published in IEEE Trans. Vehicular Technology 2021, is a prediction based privacy- preserving RHS protocol to match riders with an optimum driver. In the protocol, the Service Provider (SP) homomorphically computes Eu- clidean distances between encrypted locations of drivers and rider. Rider selects an optimum driver using decrypted distances augmented by a new-ride-emergence prediction. To improve the eﬀectiveness of driver se- lection, the paper proposes an enhanced version where each driver gives encrypted distances to each corner of her grid. To thwart a rider from using these distances to launch an inference attack, the SP blinds these distances before sharing them with the rider. In this work, we propose a passive attack where an honest-but-curious adversary rider who makes a single ride request and receives the blinded distances from SP can recover the constants used to blind the distances. Using the unblinded distances, rider to driver distance and Google Near- est Road API, the adversary can obtain the precise locations of respond- ing drivers. We conduct experiments with random on-road driver loca- tions for four diﬀerent cities. Our experiments show that we can deter- mine the precise locations of at least 80% of the drivers participating in the enhanced pRide protocol. Keywords: Ride-Hailing Services, Privacy and Censorship, Attacks 1 Introduction According to a recent research by MordorIntelligence [8], the global Ride-Hailing Services (RHS) market, valued at USD 113 billion in 2020, is expected to reach USD 230 billion by 2026. With such a huge reach, individual privacy and secu- rity issues are always of primary concern. Ride-Hailing Service Providers (SP) like Uber, Lyft, Ola provide services in many parts of the world. Among other features, the SP facilitates ride booking and fare payment options for their cus- tomers, namely riders who subscribe with the SP for RHS. Drivers of vehicles S. Murthy and S. Vivek are with the International Institute of Information Technol- ogy Bangalore, India. Corresponding Author: Shyam Murthy, shyam.sm@iiitb.ac.in 2 S. Murthy and S. Vivek such as cars and motorcycles sign-up with the SP in order to oﬀer rides. At the time of subscription, the SP collects private information of riders and drivers in order to provide services eﬀectively as well as required by local governance laws. In addition, the SP collects statistics of riders and drivers for every ride that is oﬀered in its network. This naturally brings up the topic of individual data privacy concerns from both riders as well as drivers over their data held by the SP. Also, curious or malicious drivers or riders might be interested in learning more about the other parties. There are a number of works and their analysis in the literature that look at privacy-preserving RHS, we list some of them in Section 5. Huang et al. proposed pRide [4], a privacy-preserving online RHS protocol that aims to provide the optimum driver in a global perspective thereby minimiz- ing the unnecessary travel distance to pick the rider. The protocol makes use of a deep learning model to predict emergence of new ride requests in a ride-hailing region to enable the SP to make use of such prediction while matching optimum drivers to ride requests. They show that by using such a prediction model in a global perspective, the overall distance travelled by a matching driver is mini- mized compared with matching a nearest driver in the local region. The protocol proposes to use a Somewhat Homomorphic Encryption (SHE) scheme to encrypt rider and driver locations. The advantage of using a homomorphic encryption scheme is that it allows computations on ciphertexts so that the result of compu- tation is available only after decryption. Fully Homomorphic Encryption (FHE) schemes that support potentially any number of homomorphic operations have high cost in terms of large ciphertexts and high computation latency. Hence, many practical applications that know, a priori, the bound on the number of homomorphic operations, prefer to use SHE schemes. In the pRide paper, the authors use the FV cryptosystem [1] in the implementation of their scheme. Even though applications make use of semantically secure cryptosystems, care- ful analysis is required to make sure no unintended security holes are introduced while adapting the cryptosystem to their applications. The pRide protocol, described in more detail in Section 2, has two parts, the basic protocol and an enhanced version. We discuss the basic protocol in this paragraph. In the initialization phase, SP divides the area of its operation into grids, the details of which are made available to all parties. SP keeps a record of ride requests emanating from each grid over speciﬁc time epochs and trains a prediction model using this information. It then uses this information to predict the grid-based distribution of requests for the next period, denoted by P R(g), namely the prediction result for grid id g. Drivers, registered with the SP, submit their current grid id to the SP so that the SP can maintain the driver distribution map. A rider who wishes to hail a ride, picks a (public key, secret key) pair, encrypts her coordinates and sends the ciphertext and public key to SP along with the ride request. When SP receives the ride request, it performs a search for a suitable driver in a preset order of grids around the rider’s grid and obtains a list of candidate drivers using the driver distribution map. SP then forwards the ride request to all candidate drivers. To oﬀer their ride, drivers Driver Locations Harvesting Attack on pRide 3 respond to SP by encrypting their location using the rider’s public key. SP then homomorphically computes the square of the Euclidean distance between rider and drivers’ encrypted locations and forwards the same to the rider along with P R(g) where g is the driver’s grid id. Rider decrypts the distances and picks the shortest distance D0. It then performs two checks over the list of sorted distances. First, is Di − D0 < D0 − Ddiag?, where Di is the distance for ith driver and Ddiag is the length of the diagonal of the grid, and second, does the model predict no new ride request emerging in the driver’s grid within a short time period? When both these conditions are satisﬁed, the rider informs SP about the selected index i after which the SP facilitates secure ride establishment with the rider and selected driver. In order to optimize their ride matching, the paper proposes enhanced pRide built on top of the basic pRide protocol, but having a diﬀerent method to pick the optimum driver. They show that they get better results when a driver also provides her encrypted distance to the farthest corner of her grid. This way the rider can use that distance, instead of Ddiag in the aforementioned check to select the optimum driver. However, the authors notice that if such a distance is decrypted by an adversarial rider, she can launch an inference attack to obtain driver’s locations. In order to thwart such an attack, the paper proposes a novel method where the driver provides SP with her encrypted distances to the four corners of her grid. SP then picks random integers to homomorphically blind the distances before sharing the same with the rider. Rider then decrypts the blinded distances and applies a private comparison algorithm which determines the result of the inequality Di − D0 < D0 − Dmaxdist, where Dmaxdist is the distance between the driver and the farthest corner of her grid g. Finally, using this inequality and P R(g), it outputs the optimum selected driver. As described earlier, in the enhanced pRide protocol, the SP homomorphi- cally blinds the encrypted distances with random integers before sharing them with the rider. In this paper, we show that such a blinding scheme is insecure, whence an adversary rider can recover the underlying distances and then deduce the locations of at least 80% of the drivers responding to a single ride request of the rider when using the enhanced pRide protocol. 1.1 Comparison with ORide [12] Protocol The pRide paper shows that their enhanced scheme is more eﬀective with the same level of security as that of the basic version with only a small compromise in its eﬃciency. In addition, by way of experiments, they show their computation cost is signiﬁcantly better compared to a state-of-the-art protocol named ORide [12]. We note here that the method in the basic pRide protocol where the SP employs the homomorphic property of SHE to compute the Euclidean distance between driver and rider to share the encrypted distances with rider is identical to what is described in the ORide paper. The part that is diﬀerent is that in the Henceforth in the paper, we use the term distance to mean squared Euclidean dis- tance. 4 S. Murthy and S. Vivek ORide paper to pick the nearest driver, only drivers inside the rider’s grid are chosen as candidate drivers, whereas in the pRide protocol, only drivers outside the rider’s grid are candidate drivers so as to optimize in a global perspective. In [5], Kumaraswamy et al. demonstrated a driver locations harvesting attack by honest-but-curious riders on the ORide protocol, where they determine the exact locations of 40% of drivers participating in the ORide protocol. In the same paper, the authors also provide a mitigation solution wherein a driver gives her perturbed location instead of her actual location. The aforementioned attack on the ORide protocol and the mitigating solution are both applicable to the basic pRide protocol. In [10], Murthy et al. demonstrated a driver locations harvesting attack, again by honest-but-curious adversary riders, using triangulation on the ORide protocol, where they show that they can determine the exact locations of all participating drivers in the ORide protocol. Further, they extend their method onto the mitigation solution suggested by [5] and show that they can determine locations of between 25% to 50% of the participating drivers. As mentioned earlier, in the pRide protocol, the method where the rider obtains encrypted driver distances is identical to that in the ORide protocol. Due to this, any location harvesting attack on ORide, like in the cases of [5] and [10], are also directly applicable to the basic pRide protocol. 1.2 Our Contribution We present a passive driver location harvesting attack on the enhanced pRide protocol. The honest-but-curious adversary rider issues a single ride request with a search radius (SR) = 1, such that grids adjacent to the rider’s grid are searched (as explained in the pRide paper, Section V-B-4, pp. 6). In our attack, the adversary rider receives, per driver, a set of encrypted blinded distances between the driver’s location and each corner of the driver’s grid. One would expect that such a blinding process would make it hard for the rider to deduce anything about the underlying distances. Rider decrypts the ciphertexts received from SP to obtain blinded distances. Next, by computing the Greatest Common Divisor (GCD) of the blinded dis- tances and eliminating common factors, the rider recovers the blinding values after which the distances are easily obtained. Rider now has the four distances from driver to each corner of the driver’s grid. Using these distances, the rider computes four equiprobable driver locations in each of the four grids adjacent to the rider’s grid. This is due to the fact that the distances are in random order and, so, there is no correlation between each corner of the grid and its distance to the driver. Rider knows the distance between herself and each responding driver. Now, using the distance between herself and a particular responding driver (say, δ), the rider draws a rider-circle with center as her location and radius = δ. Probable driver locations that lie on the rider-circle are ﬁltered in and in case multiple such locations are obtained, Google Nearest Roads API [2] is used to output one location that is closest to a motorable road. We conduct our experi- ments using rectangular grids on four diﬀerent cities around the world and the Driver Locations Harvesting Attack on pRide 5 results are summarized in Table 1. We show that we can obtain exact driver locations of up to 80% of drivers who respond to a rider’s request. Our attack invalidates Theorem 4, pp. 9, of the pRide paper [4], which states that pRide is adaptively Laccess semantically secure against semi-honest adver- saries, where Laccess gives the access pattern of the SP and rider, which is simply the list of drivers that respond to a speciﬁc ride request. Hence, when our attack is combined with that in [10], the driver location security of the pRide paper is fully compromised, and so is the mitigation solution of [5] if applied to the basic pRide protocol. We stress that the attack from [10] is not directly applicable to the pRide protocol, but works only in combination with our attack. The rest of the paper is organized as follows. Section 2 describes the pRide protocol. Section 3 describes our attack. Section 4 gives details about our experi- ments and results. Section 5 gives some of the recent works in privacy-preserving RHS, followed by conclusions. 2 Overview of pRide Protocol In this section, we provide an overview of the pRide protocol followed by a de- scription of the threat model adopted therein. For more details, the interested reader is referred to the original paper [4]. Remark : Unless qualiﬁed as enhanced or basic, we will use the term pRide pro- tocol to refer to the complete pRide protocol, consisting of both the basic and enhanced parts. 2.1 pRide Protocol pRide is a privacy-preserving online ride-hailing protocol augmented with a grid- based rider emergence prediction. The key objective of the protocol is to achieve optimum driver selection in a global perspective instead of picking the nearest driver as done in other works [12,13]. Selecting such a driver might be a better choice in order to minimize the overall empty travel distance traversed by drivers to pick up riders in the whole system. The prediction of requests based on deep learning plays an important role in driver selection. The protocol has two parts, the basic protocol and an enhancement, built on top of the basic protocol, are summarized in the following steps. Steps 1 to 10 constitute the basic pRide protocol, followed by steps of the enhanced pRide protocol. 1. The three parties involved in the pRide protocol are: driver, rider and service provider (SP). The SP does not collude with either rider or drivers. The SP as well as the users, namely, drivers and riders, are honest-but-curious entities who execute the protocol correctly, but are keen on knowing about each other’s sensitive information. The protocol aims to protect all users’ privacy from other riders and drivers, such that the precise location of one party is not learnt by the other party during the ride matching process. However, only 6 S. Murthy and S. Vivek after a driver is matched with a rider, they start to communicate through a secure channel. 2. During system initialization, the SP divides its area of its operation into rectangular grids of suitable sizes (size is based on suﬃcient ride density so as to maintain rider anonymity) and publishes the same. For example, a city like New York City together with its surrounding boroughs, where the SP is allowed to provide rides as permitted by local authorities, can be termed as the SP’s area of operation. 3. Drivers, available to oﬀer rides, submit their real-time grid id to the SP to enable it to maintain a driver distribution map. 4. Rider, wishing to hail a ride, generates a key pair (public key pk, private key sk) from the FV SHE scheme [1], encrypts her location using pk, and submits a ride-request along with her location ciphertext, her current grid id and pk to the SP. The FV SHE scheme works on integers, hence, the coordinates of users are encoded as integers using UTM format. 5. SP keeps a record of ride requests in each grid and maintains a real-time ride request distribution map in every time period. It makes use of Convolutional long short-term memory (Convolutional LSTM [15]) to train a prediction model with the ride request distribution information. Based on a temporal sequence of grid information, SP obtains prediction result P R(g), a non- negative integer which predicts the number of requests in the next time period for grid id g. 6. As soon as SP receives the ride request, it performs a driver search with a search radius (SR) in a preset order of grids starting with the grid nearest to rider. The rider’s grid is not searched so as to avoid the nearest driver who would always be found in the rider’s grid. When SR = 1, only grids adjacent to the rider are searched. Using the driver distribution map, SP creates a list of candidate drivers and forwards the ride-request to all such drivers. 7. When the ith driver di receives the ride-request, she encrypts her location using pk and forwards it to SP. 8. SP homomorphically computes the square of the Euclidean distance between the rider and drivers’ locations. It then forwards these distances to rider along with driver id i and P R(gi), gi is i’s grid id. 9. Rider uses sk to decrypt the distances and sorts them to obtain the smallest distance D0. For each distance in the sorted list, she runs the following two checks to pick the optimum driver: (a) 2D0 − Di > Ddiag, where Di is the distance for ith driver and Ddiag is the length of the diagonal of the grid. (b) P R(gi), where gi is the driver’s grid id, which checks if no new ride request is emerging in a short time in grid gi. 10. As soon as both the aforementioned conditions are satisﬁed, rider determines the optimum driver and informs the same to SP to continue with secure ride establishment between rider and selected driver. Universal Transverse Mercator: a map-projection system for geographical locations [19]. Driver Locations Harvesting Attack on pRide 7 11. In order to improve the eﬀectiveness of driver selection, the authors notice that they can minimize the empty distance travelled by the driver by using Dmaxdist instead of Ddiag in the ride selection check (Step 9), where Dmaxdist is the distance between the driver and the farthest corner in her grid. How- ever, the authors realize that an adversary rider, after decryption, can use Dmaxdist to launch an inference attack to obtain driver’s precise location. They, therefore, propose enhanced pRide to thwart such an attack. 12. In the enhanced pRide protocol, each driver, in addition to sending encryp- tions of her coordinates, also sends the encryptions of distances to each corner of her grid to the SP. 13. To pick the optimum driver, rider now needs to perform the check 2D0 − Di > Dmaxdist, for each driver i, using a private comparison algorithm, as explained below (Steps 15, 16 and 17). 14. As in the earlier basic pRide protocol, rider receives a list of distances to each of the candidate drivers, decrypts them and selects the smallest D0. 15. In order to ﬁnd the optimum driver, for each Di, i > 0, rider sets D(cid:48) = 2D0 − Di, encrypts D(cid:48) as (cid:102)D(cid:48) and sends (cid:102)D(cid:48) and i to SP. 16. SP receives encrypted distances to each of the four corners of the ith driver’s grid as ( (cid:102)Dll, (cid:103)Dlu, (cid:103)Drl, (cid:103)Dru). SP generates random positive blinding integers e and r, and homomorphically blinds each of the ciphertexts as (cid:102)V (cid:48) = e · (cid:102)D(cid:48) + (cid:101)r (cid:102)Vll = e · (cid:102)Dll + (cid:101)r (cid:102)Vlu = e · (cid:103)Dlu + (cid:101)r (cid:102)Vrl = e · (cid:103)Drl + (cid:101)r (cid:103)Vru = e · (cid:103)Dru + (cid:101)r. (1) It then sends each of these blinded values to rider. Remark : Homomorphic addition of two ciphertexts, and homomorphic mul- tiplication of ciphertext with plaintext can be done very eﬃciently in SHE. 17. Rider decrypts each of these blinded values and compares V (cid:48) with each of (Vll, Vlu, Vrl, Vru). If V (cid:48) is greater than all the four values, then it implies that D(cid:48) > Dmaxdist. 18. Rider then uses this comparison result and P R(g) value as in the basic pRide protocol to select the optimum driver and informs the same to SP. If these checks fail, then the Steps 15 through 18 are repeated until an optimum driver is obtained by walking through each entry in the candidate driver list. The authors evaluate the performance of their enhanced pRide protocol over real-world datasets. Their results show that their protocol is eﬀective in saving empty distance as well as in maintaining drivers’ privacy during the ride match- ing process. Finally, they compare the basic and enhanced versions of pRide and prove that the latter is more eﬀective in choosing the optimum driver with the same level of privacy. The security of their protocol is based on the appar- ent hardness of retrieving the blinding parameters when given only the blinded values. 8 S. Murthy and S. Vivek In our attack described in Section 3, we show that we can determine the underlying distance values when given only their blinded values, where blinding is done as described in Step 16. We then go on to use the distances to get the precise coordinates of responding drivers. 2.2 Threat Model We consider the same threat model considered in the pRide protocol, where all parties, namely the SP, drivers and riders, are honest in executing the protocol. Riders submit valid requests by encrypting their correct coordinates to the SP, and the drivers also submit the encryptions of their current coordinates to the SP. SP does not collude with either drivers or riders. Drivers do not collude with riders. All parties are honest-but-curious in the protocol. Thus, each party is curious to know more about the sensitive information of the other party. In particular, riders are curious to know about drivers’ locations and vice-versa. pRide also considers the case of an adversary rider who follows the protocol correctly but launches an inference attack by performing private computations on received driver coordinates to infer drivers’ precise locations, and so the authors pro- pose enhanced pRide to thwart such an attack. Their paper aims to preserve driver and rider location information from SP, and to preserve driver location information from rider. In this paper, we consider the same threat model to model the adversaries. The ride request issued by an honest-but-curious adversary rider is indistinguish- able from a ride request issued by any other legitimate rider in the protocol. In a real-life scenario, a competitor SP with the intention of harvesting driver in- formation of another SP, can mount such an attack without being detected by the target SP. 3 Our Attack In this section, we present our driver location harvesting attack on the enhanced pRide protocol by a honest-but-curious adversary rider (R). R issues a single ride request as per the pRide protocol. SP will not be able to distinguish between a ride request issued by an adversary rider versus another by a legitimate rider. In this section, for ease of exposition, we explain the recovery of location of one particular driver Dp, who has responded to ride request by R, shown in Figure 1. Dp is located at distance δ from R. Our attack extends easily to all responding drivers, since each response is handled independently by the SP. 3.1 Retrieving Distances R issues a ride request as per the pRide protocol with search radius SR = 1. By this, only the grids adjacent to the rider’s grid are searched by SP for candidate drivers. Driver Locations Harvesting Attack on pRide 9 We recall here the steps of pRide and enhanced pRide protocols from Section 2.1. In Step 14, the rider R obtains the distances between herself and all the re- sponding drivers in the clear (distance between R and Dp is δ). In addition, from Step 16, R receives the ciphertexts ((cid:102)V (cid:48), (cid:102)Vll, (cid:102)Vlu, (cid:102)Vrl, (cid:103)Vru), which after decryption gives (V (cid:48), Vll, Vlu, Vrl, Vru). We know that (cid:102)D(cid:48) is the encryption of 2D0 − δ, and V (cid:48) = e · (cid:102)D(cid:48) + (cid:101)r Vll = e · (cid:102)Dll + (cid:101)r Vlu = e · (cid:103)Dlu + (cid:101)r Vrl = e · (cid:103)Drl + (cid:101)r Vru = e · (cid:103)Dru + (cid:101)r, (2) where e and r are the blinding integers chosen by SP. R then computes the diﬀerence of every pair from (Vll, Vlu, Vrl, Vru), decrypts them using her secret key and stores them as (P, Q, R, S, T, U ), in no particular order. The diﬀerences, thus obtained, are P = Vll − Vlu = e · (Dll − Dlu) Q = Vll − Vrl = e · (Dll − Drl) R = Vll − Vru = e · (Dll − Dru) S = Vlu − Vrl = e · (Dlu − Drl) T = Vlu − Vru = e · (Dlu − Dru) U = Vrl − Vru = e · (Drl − Dru). (3) It can be easily seen that the GCD of any two of (P, Q, R, S, T ), say P and Q, will give either e or its multiple. The latter case will occur when (Dll − Dlu) and (Dll − Drl) are not relatively prime, and by eliminating any common factors between them, we can hope to retrieve the exact value of e with a high proba- bility. 1 ζ(n) , Remark : The probability of n randomly chosen integers being coprime is where ζ is the Riemann Zeta function [20], and for two such integers the prob- 6 π2 . This means in about 60% of cases we can ﬁnd the value of e ability is straightaway, and in rest of the cases we can try to eliminate common factors. Notice that each of the Dxy values are squares of the Euclidean distance between the driver’s location and each corner of her grid. Let the driver’s coordinates (to be determined) be (x, y) and the known corners of her grid be (x1, y1), (x2, y2), (x3, y3) and (x4, y4). W.l.o.g, Dll = (x1 − x)2 + (y1 − y)2 Dlu = (x2 − x)2 + (y2 − y)2. (4) (5) 10 S. Murthy and S. Vivek Hence, P = e · pliﬁes to (cid:16)(cid:0)(x1 − x)2 + (y1 − y)2(cid:1)−(cid:0)(x2 − x)2 + (y2 − y)2(cid:1)(cid:17) , which sim- P = e · (cid:0)(x1 − x2)(x1 + x2 − 2x) + (y1 − y2)(y1 + y2 − 2y)(cid:1). By eliminating common factors, if any, we obtain P (cid:48) = e · P/(cid:0)GCD(x1 − x2, y1 − y2) ∗ GCD(2, x1 + x2, y1 + y2)(cid:1). (6) (7) And similarly, we get Q(cid:48), R(cid:48), S(cid:48), T (cid:48), U (cid:48). Finally, GCD(P (cid:48), Q(cid:48), R(cid:48), S(cid:48), T (cid:48), U (cid:48)) gives the value of e. Remark : The coordinates of each of the grids are known at system initialization time. Hence, any common factors between the coordinates can be computed oﬄine. In Step 15, rider has the value of (cid:102)D(cid:48), using which the value of (cid:101)r is obtained from V (cid:48) = e · (cid:102)D(cid:48) + (cid:101)r. And, ﬁnally, using e and (cid:101)r, ( (cid:102)Dll, (cid:103)Dlu, (cid:103)Drl, (cid:103)Dru), and, hence, (Dll, Dlu, Drl, Dru) are obtained. Remark : In case we obtain a negative value for (cid:101)r, it implies that our recovery of e is in error. 3.2 Retrieving Driver Locations R does not know the correlation between the Dxy distances and the corners of the grid as they are distances given in random order. In addition, since the search radius SR = 1, any of the four grids adjacent to the rider’s grid can be a potential grid of driver Dp. Using the four distance values (Dll, Dlu, Drl, Dru) as radii and each of the respective grid corners as center of circles, rider obtains four points in each grid where all the four circles intersect. These points, in their respective grids, represent the equiprobable locations of driver Dp. Figure 1 gives a pictorial view of our attack. Adversary rider R is located in grid g. Driver Dp is located in grid g4, at a distance δ from R. Each of the four probable driver locations in each adjacent grids g1 through g4 are shown as small blue dots in each grid. Using the distance between R and Dp, namely δ, R draws a rider-circle of radius = δ around herself. As long as the driver has reported her correct coordinates, it is guaranteed that at least one of the 16 equiprobable driver locations will lie on the circumference of the rider-circle. If more than one such location is obtained, then the rider makes use of Google Nearest Road API [2] to ﬁnd the nearest road to each of such locations. Since we assume that the driver is located on a motorable road, the adversary algorithm will output the location closest to the nearest road. Remark : The mitigation solution of [5] can be applied to the pRide protocol. While the attack of [10] is still applicable on the basic pRide protocol, we look at our attack on its enhanced version, when the mitigation solution is applied to the pRide protocol. In that case, in response to a ride request, the driver would pick a uniform random location inside a circle of radius τ around her original Driver Locations Harvesting Attack on pRide 11 Fig. 1. Recovered driver locations shown as small dots each in grid g1 through g4. location. She then sends the encryption of that random location to the SP, as well as the encrypted distances from the random location to each of the corners of her grid. We note that τ should not be too large, as that would have an adverse eﬀect on driver selection by rider. Our attack, described in Section 3, would be applicable without any change. However, the retrieved location, in this case, would be the random location picked by the driver instead of her actual location. The adversary could then apply the attack of [10] to uncover the actual driver locations, with a high probability. 4 Experiments and Results We use Sagemath 8.6 [16] to implement our attack described in Section 3.1 where we retrieve driver distances. The attack, described in Section 3.2, where we retrieve the driver locations, was implemented in Python and used the Google Nearest Road APIs for Python [3]. Both parts of the attack were executed on a commodity laptop with 512 GB SSD and a AMD Ryzen 5 processor. Our Sagemath and Python programs are available at: https://github.com/shyamsmurthy/nss2022. 4.1 Experiment Details Our experiments were run on grids of size about 4km2 superimposed on maps of 4 large cities around the world, namely, Los Angeles, London, New York City and Paris. The size of the grid is comparable to what is reported in the pRide paper. We have done experiments with the number of drivers as 5, 15 and 25 per grid, in each case distributed randomly throughout each grid but located on motorable areas. We note here that the number of drivers does not have a 12 S. Murthy and S. Vivek bearing on our attack since the SP encrypts and blinds each driver’s distances independent of one other. In each of the maps, we picked random driver locations situated on motorable roads. Next, a rider location was picked from a random grid in the map. As explained in Section 3.1, grids adjacent to the rider’s grid was examined and distances between drivers in those grids and the rider were made available to the rider. Except for the predicted result (P R) values, this is same as what is available to the rider in the pRide protocol. The P R values do not have any bearing on our attack since they do not have any eﬀect on either blinding or encryption of distances. Next, from each of the adjacent grids and for each driver in such grid, the distances from each such driver to her respective grid corners were computed, and blinded using random integers picked from the range [1, 224], as the maximum UTM (northing) value of 107 can be represented using 24 bits. In addition, a distance value known to the adversary is also blinded using the same random integers. These blinded distances were made available to the adversary rider. Again, this exactly mimics the behaviour of the enhanced pRide protocol. Finally, we run the attack described in Section 3 to retrieve the distances followed by retrieving the driver locations. Remark 1 : It is claimed that the security of the pRide protocol relies on the hard- ness of obtaining the blinding parameters when given only the blinded values. We show in our attack that the adversary can recover the blinding parameters with a high probability. Remark 2 : In our experiments, we have used a search radius SR = 1. Our at- tack methodology can be easily extended to higher values of search radius. Since the order of grid traversal is known a priori, the new attack has to compute equiprobable locations in each of the possible grids and continue with our driver retrieval attack, as described in Section 3.2. 4.2 Results The results of our experiments are tabulated in Table 1. The pRide paper uses a 64 × 64 grid over the city of Chengdu, China, and mentions a maximum of 16000 drivers in their experiments, which translates to about 4 drivers per grid on average. As it can be much larger in high density areas in the city, we run our experiments with 5, 15 and 25 drivers per grid. It takes less than 1 second to recover the locations of 25 drivers. In order to retrieve the distances, we ﬁrst recover the blinding integers e and r as described in Section 3.1. As shown in Table 1, we can retrieve at least 80% of the distances successfully, averaged from 10 runs of the experiments for each driver count over each city. In the unsuccessful cases, we ﬁnd that the value of the blinding value e retrieved by our algorithm is a multiple of the actual value of e, and we report this as a failure. Next, we use the successfully retrieved distances to obtain the precise driver locations. Here, we use our attack described in Section 3.2. We see that this part correctly retrieves close to 99% of the driver locations. Hence, our overall driver Driver Locations Harvesting Attack on pRide 13 location harvesting algorithm retrieves at least 80% of the drivers participating in the enhanced pRide protocol. Table 1. Percentage of driver locations recovered for multiple cities. City Los Angeles London New York City Paris Number of participating drivers (per grid) 5 15 25 5 15 25 5 15 25 5 15 25 %age of driver coordinates correctly recovered 80 95 89 85 81 86 90 95 93 85 93 88 5 Related Works There is a large body of work on privacy-preserving RHS which consider preserv- ing privacy of drivers and riders. ORide [12] and PrivateRide [13], both proposed by Pham et al., were some of the early works that aimed to preserve rider pri- vacy against SP and drivers. While PrivateRide makes use of a cloaking region to maintain privacy, ORide scheme is based on SHE to encrypt driver and rider locations so as to make use of homomorphic properties of SHE to select nearest driver. Kumaraswamy et al. [5] proposed an attack that aims to determine loca- tions of drivers participating in the ORide protocol. In their attack, an adversary rider can reveal locations of up to 40% of drivers who respond to a single ride request. They provide a countermeasure to thwart the attack while preserving suﬃcient anonymity. Murthy et al. [10] proposed an attack that uses triangula- tion by four colluding adversaries to obtain locations of all drivers participating in the ORide protocol. Luo et al. [7] proposed a privacy-preserving ride-matching service also named pRide. Their protocol involves using two non-colluding servers: SP and CP (a third-party crypto server), and uses Road Network Embedding (RNE) [14] such that the road network is transformed to a higher dimension space to enable eﬃcient distance computation between the network entities. However, the dis- advantage of their scheme is the use of two non-colluding servers which incurs inter-server communication costs. Yu et al. [22] proposed lpRide protocol which 14 S. Murthy and S. Vivek also uses RNE but uses a modiﬁed version of Paillier encryption scheme [11] to preserve privacy of participating entities. Vivek [17] demonstrated an attack on the lpRide protocol where they show that any rider or driver can learn the coordinates of other participating riders. TRACE [18] is a privacy-preserving dy- namic spatial query RHS scheme proposed by Wang et al., that uses a quadtree structure and provides high-eﬃciency in terms of complexity and communica- tion overhead. Kumaraswamy et al. [6] demonstrated an attack on the TRACE protocol where the SP can identify the exact locations of riders and drivers. Xie et al. [21] proposed a protocol that also uses RNE to eﬃciently compute shortest distances. Their scheme makes use of property-preserving hash functions where the SP can not only compute the rider to driver distances, but also pick the nearest driver. This way they eliminate the need for an auxiliary crypto server. All the works listed earlier picks the nearest driver to fulﬁl a ride request. pRide [4], proposed by Huang et al., does not match the nearest driver but considers a global matching strategy with the aim of reducing the empty distance trav- elled by driver to pick the rider. Murthy et al. [10] gave an attack on the ORide protocol, using triangulation, where they recover locations of all participating drivers. In addition, by using more number of colluding adversaries, they show they can recover locations of up to 50% of drivers participating in the variant of ORide protocol that uses the mitigation solution of [5]. 6 Conclusions In this paper, we presented an attack on enhanced pRide [4] protocol, a privacy- preserving RHS. We show that an honest-but-curious adversary rider can de- termine the coordinates of about 80% of drivers responding to the rider’s ride request as per the pRide protocol. From Section 1.1, we see that locations of all drivers participating in the basic pRide protocol can be recovered by one or more adversary riders. As per the protocol, the rider chooses the optimum driver when given the plaintext distances to all drivers, and this fact is exploited by the adversary. Alternatively, the SP can select the optimum driver homomorphically. Since sorting and searching are high-depth circuits, it is not eﬃcient to perform these operations using SHE schemes. However, FHE schemes can be explored to evaluate their suitability for practical RHS solutions. The enhanced pRide protocol needs to perform comparisons and in order to preserve privacy, the values are blinded. However, since the order needs to be preserved, the blinding values are the same for all the comparands, which leads to the attack. Other secure order-preserving techniques need to be explored. However, as shown in [9], careful analysis is needed which would otherwise lead to further attacks. In summary, we show that although protocols may seem secure in theory, a thorough analysis should be done which otherwise would expose severe vulner- abilities and security holes, as demonstrated by our attack in this paper. Driver Locations Harvesting Attack on pRide 15 Acknowledgements. We thank the anonymous reviewers for their invaluable comments and suggestions, which helped us improve the manuscript. This work was partly funded by the INSPIRE Faculty Award (DST, Govt. of India) and the Infosys Foundation Career Development Chair Professorship grant for Srinivas Vivek. References 1. Fan, J., Vercauteren, F.: Somewhat practical fully homomorphic encryption. Cryp- tology ePrint Archive (2012), http://eprint.iacr.org/2012/144 2. Google: Google Maps Platform. https://developers.google.com/maps/ documentation/roads/intro/ (2019), retrieved: August 01, 2022 3. Google: Google Maps Platform, client libraries for google maps web services. https://developers.google.com/maps/web-services/client-library (2019), retrieved: August 01, 2022 4. Huang, J., Luo, Y., Fu, S., Xu, M., Hu, B.: pride: Privacy-preserving online ride hailing matching system with prediction. IEEE Transactions on Vehicular Tech- nology 70(8), 7413–7425 (2021). https://doi.org/10.1109/TVT.2021.3090042 5. Kumaraswamy, D., Murthy, S., Vivek, S.: Revisiting driver privacy in oride. In: Se- lected Areas in Cryptography - SAC 2021 - 28th International Conference, Univer- sity of Victoria, British Columbia, Canada (Virtual Event), September 29- October 01, 2020. Lecture Notes in Computer Science, Springer (2021) 6. Kumaraswamy, D., Vivek, S.: Cryptanalysis of the privacy-preserving ride-hailing service TRACE. In: Adhikari, A., Küsters, R., Preneel, B. (eds.) Progress in Cryp- tology - INDOCRYPT 2021 - 22nd International Conference on Cryptology in In- dia, Jaipur, India, December 12-15, 2021, Proceedings. Lecture Notes in Computer Science, vol. 13143, pp. 462–484. Springer (2021). https://doi.org/10.1007/ 978-3-030-92518-5_21, https://doi.org/10.1007/978-3-030-92518-5_21 7. Luo, Y., Jia, X., Fu, S., Xu, M.: pRide: Privacy-Preserving Ride Matching Over Road Networks for Online Ride-Hailing Service. IEEE Trans. Information Forensics and Security 14(7), 1791–1802 (2019) 8. Mordor Intelligence: Ride-Hailing Market - Growth, Trends, Covid-19 Im- pact, And Forecasts (2022 - 2027). https://www.mordorintelligence.com/ industry-reports/ride-hailing-market (2020), retrieved: July 23, 2022 9. Murthy, S., Vivek, S.: Cryptanalysis of a protocol for eﬃcient sorting on SHE encrypted data. In: Albrecht, M. (ed.) Cryptography and Coding - 17th IMA In- ternational Conference, IMACC 2019, Oxford, UK, Proceedings. Lecture Notes in Computer Science, vol. 11929, pp. 278–294. Springer (2019) 10. Murthy, S., Vivek, S.: Passive triangulation attack on oride (2022). https://doi. org/10.48550/ARXIV.2208.12216, https://arxiv.org/abs/2208.12216 11. Nabeel, M., Appel, S., Bertino, E., Buchmann, A.P.: Privacy preserving context aware publish subscribe systems. In: López, J., Huang, X., Sandhu, R. (eds.) Net- work and System Security - 7th International Conference, NSS 2013, Madrid, Spain, June 3-4, 2013. Proceedings. Lecture Notes in Computer Science, vol. 7873, pp. 465–478. Springer (2013) 12. Pham, A., Dacosta, I., Endignoux, G., Troncoso-Pastoriza, J.R., Huguenin, K., Hubaux, J.: ORide: A Privacy-Preserving yet Accountable Ride-Hailing Service. In: Kirda, E., Ristenpart, T. (eds.) 26th USENIX Security Symposium, USENIX Se- curity 2017, Vancouver, BC, Canada, August 16-18, 2017. pp. 1235–1252. USENIX Association (2017) 16 S. Murthy and S. Vivek 13. Pham, A., Dacosta, I., Jacot-Guillarmod, B., Huguenin, K., Hajar, T., Tramèr, F., Gligor, V.D., Hubaux, J.: PrivateRide: A Privacy-Enhanced Ride-Hailing Service. PoPETs 2017(2), 38–56 (2017), https://doi.org/10.1515/popets-2017-0015 14. Shahabi, C., Kolahdouzan, M.R., Sharifzadeh, M.: A road network embedding technique for k-nearest neighbor search in moving object databases. In: Voisard, A., Chen, S. (eds.) ACM-GIS 2002, Proceedings of the Tenth ACM International Symposium on Advances in Geographic Information Systems, McLean, VA (near Washington, DC), USA, USA, November 8-9, 2002. pp. 94–10. ACM (2002) 15. Shi, X., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.k., Woo, W.c.: Convolutional lstm network: A machine learning approach for precipitation nowcasting. In: Pro- ceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1. p. 802–810. NIPS’15, MIT Press, Cambridge, MA, USA (2015) 16. Stein, W., et al.: Sage Mathematics Software (Version 8.6). The Sage Development Team (2019), http://www.sagemath.org 17. Vivek, S.: Attacks on a privacy-preserving publish-subscribe system and a ride- hailing service. In: Paterson, M.B. (ed.) Cryptography and Coding - 18th IMA International Conference, IMACC 2021, Virtual Event, December 14-15, 2021, Proceedings. Lecture Notes in Computer Science, vol. 13129, pp. 59–71. Springer (2021). https://doi.org/10.1007/978-3-030-92641-0_4, https://doi.org/10. 1007/978-3-030-92641-0_4 18. Wang, F., Zhu, H., Liu, X., Lu, R., Li, F., Li, H., Zhang, S.: Eﬃcient and privacy- preserving dynamic spatial query scheme for ride-hailing services. IEEE Transac- tions on Vehicular Technology 67(11), 11084–11097 (2018) 19. Wikipedia tem. coordinate_system (2020), retrieved: August 01, 2022 sys- https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_ contributors: Universal Transverse Mercator coordinate 20. Wikipedia contributors: Coprime Integers. https://en.wikipedia.org/wiki/ Coprime_integers (2022), retrieved: August 09, 2022 21. Xie, H., Guo, Y., Jia, X.: A privacy-preserving online ride-hailing system without involving a third trusted server. IEEE Transactions on Information Forensics and Security 16, 3068–3081 (2021) 22. Yu, H., Shu, J., Jia, X., Zhang, H., Yu, X.: lpride: Lightweight and privacy- preserving ride matching over road networks in online ride hailing systems. IEEE Trans. Vehicular Technology 68(11), 10418–10428 (2019)",0.0
"
        Machine Learning’s New Math
    ",https://spectrum.ieee.org/number-representation,2022-10-18,,"New number formats and basic computations emerge to speed up AI training Recent developments in AI have been astounding, but so are the costs of training neural networks to do their astounding feats. The biggest, such as the language model GPT-3 and the art generator DALL-E 2, take several months to train on a cluster of high-performance GPUs, costing millions of dollars and taking up millions of billions of billions of basic computations. The training capabilities of processing units have been growing rapidly, as much as doubling in the last year. To keep the trend going, researchers are digging down into the most basic building blocks of computation, the way computers represent numbers. “We got a thousand times improvement [in training performance per chip] over the last 10 years, and a lot of it has been due to number representation,” Bill Dally, chief scientist and senior vice president of research at Nvidia said in his keynote talk at the recent IEEE Symposium on Computer Arithmetic. Among the first casualties in the march toward more efficient AI training is the 32-bit floating-point number representation, colloquially known as “standard precision.” Seeking speed, energy efficiency, and better use of chip area and memory, machine-learning researchers have been trying to get the same level of training using numbers represented by fewer bits. The field is still wide open for competitors trying to take the place of the 32-bit format, both in the number representation itself and in the way basic arithmetic is done using those numbers. Here are some recent advances and top contenders unveiled last month at the conference: Image creator DALL-E was trained on clusters of Nvidia’s A100 GPUs with a combination of the standard 32-bit numbers and the lower-precision 16-bit numbers. Nvidia’s recently announced Hopper GPU includes support for even smaller, 8-bit floating-point numbers. Now Nvidia has developed a chip prototype that takes this trend a step further by using a combination of 8-bit and 4-bit numbers. The chip managed to maintain computational accuracy despite using less-precise numbers, at least for one part of the training process, called inferencing. Inference is performed on fully trained models to get an output, but it’s also done repeatedly during training. “We end up getting 8-bit results from 4-bit precision,” Nvidia’s Dally told engineers and computer scientists. Nvidia was able to cut down the size of numbers without significant accuracy loss thanks to a technique the company calls per-vector scaled quantization (VSQ). The basic idea goes something like this: A 4-bit number can represent only 16 values exactly. So, every number gets rounded to one of those 16 values. The loss of accuracy due to this rounding is called quantization error. However, you can add a scale factor to uniformly squeeze the 16 values closer together on the number line or pull them further apart, decreasing or increasing the quantization error. Nvidia's per-vector scaling quantization (VSQ) scheme better represents the numbers needed in machine learning than standard formats such as INT4. Nvidia The trick is to squeeze or expand those 16 values so they optimally match the range of numbers you actually need to represent in a neural network. This scaling is different for different sets of data. By fine-tuning this scaling parameter for every set of 64 numbers inside the neural-network model, Nvidia researchers were able to minimize quantization error. The overhead from calculating the scaling factors is negligible, they found. But the energy efficiency doubled as the 8-bit representation was reduced to 4-bit. The experimental chip is still under development, and Nvidia engineers are working on how to use these principles during the full training process, not just in inferencing. If successful, a chip combining 4-bit computations, VSQ, and other efficiency improvements could achieve 10 times the number of operations per watt as the Hopper GPU, said Dally. Back to top Back in 2017, two researchers, John Gustafson and Isaac Yonemoto, developed an entirely new way of representing numbers. Now, a team of researchers at the Complutense University of Madrid have developed the first processor core implementing the posit standard in hardware and showed that, bit for bit, the accuracy of a basic computational task increased by up to four orders of magnitude, as compared to computing using standard floating-point numbers. The advantage of posits comes from the way the numbers they represent exactly are distributed along the number line. In the middle of the number line, around 1 and –1, there are more posit representations than floating point. And at the wings, going out to large negative and positive numbers, posit accuracy falls off more gracefully than floating point. “It’s a better match for the natural distribution of numbers in a calculation,” says Gustafson. “It’s the right dynamic range, and it’s the right accuracy where you need more accuracy. There’s an awful lot of bit patterns in floating-point arithmetic no one ever uses. And that’s a waste.” Posits accomplish this improved accuracy around 1 and –1 thanks to an extra component in their representation. Floats are made up of three parts: a sign bit (0 for positive, 1 for negative), several “mantissa” (fraction) bits denoting what comes after the binary version of a decimal point, and the remaining bits defining the exponent (2 exp). Posits keep all the components of a float but add an extra “regime” section, an exponent of an exponent. The beauty of the regime is that it can vary in bit length. For small numbers, it can take as few as two bits, leaving more precision for the mantissa. This allows for the higher accuracy of posits in their sweet spot around 1 and –1. Posits differ from floating-point numbers by the addition of an extra variable-length regime section. This gives posits a better accuracy around zero, where the majority of numbers used in neural networks reside. Complutense University of Madrid/IEEE With their new hardware implementation, which was synthesized on a field-programmable gate array (FPGA), the Complutense team was able to compare computations done using 32-bit floats and 32-bit posits side by side. They assessed their accuracy by comparing them to results using the much more accurate but computationally costly 64-bit floating-point format. Posits showed an astounding four-order-of-magnitude improvement in the accuracy of matrix multiplication, a crucial calculation in neural networks. They also found that the improved accuracy didn’t come at the cost of computation time, only a somewhat increased chip area and power consumption. Back to top Computer scientists in Switzerland and Italy have worked out a bit-reducing scheme, meant for processors using open-source RISC-V instruction-set architecture, which is gaining ground among new processor developers. The team’s extension to the RISC-V instruction set includes an efficient version of computations that mix lower- and higher-precision number representations. With their improved mixed-precision math, they obtain a factor-of-two speedup in basic computations involved in training neural networks. Lowering precision has knock-on effects during basic operations that go beyond just the loss of accuracy due to the shorter number. Multiplying two low-precision numbers can result in a number that is too small or too large to represent given the bit length—phenomena called underflow and overflow, respectively. A separate phenomenon, called swamping, happens when you add a large low-precision number and a small low-precision number. The result can be the complete loss of the smaller number. Mixed precision comes to the rescue in mitigating overflow, underflow, and swamping. Computations are performed with low-precision inputs and result in higher-precision outputs, getting a batch of math done before rounding back to lower precision. The Europe-based team focused on a basic component of AI computations: the dot product. It’s usually implemented in hardware with a series of components called fused multiply-add units (FMAs). These perform the operation d = a*b + c in one go, only rounding at the end. To reap the benefits of mixed precision, inputs a and b are low precision (say, 8 bits), while c and the output d are high precision (say, 16 bits). Luca Benini, chair of digital circuits and systems at ETH Zurich and professor at the University of Bologna and his team had a simple insight: Instead of doing one FMA operation at a time, why not do two in parallel and add them together at the end? Not only does this prevent losses due to rounding between the two FMAs, but it also allows for better utilization of memory, because no memory registers are left waiting for the previous FMA to complete. The group designed and simulated the parallel-mixed precision dot-product unit and found the computation time was nearly cut in half and the output accuracy improved, especially for dot products of large vectors. They are currently building their new architecture in silicon to prove the simulation’s predictions. Back to top Engineers at the Barcelona Supercomputing Center and Intel claim that “Brain Float 16 is all you need” for deep neural-network training. And no, they are not Beatles fans. They are fans of the number format Brain Float—Google Brain’s brainchild. Brain Float (BF16) is a pared-down version of the standard 32-bit floating-point representation (FP32). FP32 represents numbers with one sign bit, 8 exponent bits, and 23 mantissa bits. A BF16 number is just an FP32 number with 16 mantissa bits chopped off. This way, BF16 sacrifices precision but takes up half the space in memory and preserves the dynamic range of FP32. Brain Floats are already in wide use in AI training, including the training of the image creator DALL-E. However, they’ve always been used in combination with higher-precision floating-point numbers, converting back and forth between the two for different parts of the computation. That’s because while some parts of neural-network training don’t suffer much from the reduced precision of BF16, others do. In particular, small-magnitude values can get lost during fused multiply-add computations. The Barcelona and Intel team has found a way around these issues and showed that BF16 can be used exclusively to train state-of-the-art AI models. What’s more, in simulation, the training used one-third of the chip area and energy FP32 would need. To perform AI training entirely using BF16 without having to worry about losing small values, the Barcelona team developed both an extended-number format and a bespoke FMA unit. The number format, called BF16-N, combines several BF16 numbers to represent one real number. Far from defeating the point of using fewer-bit formats, combining BF16 allows for much more efficient FMA operations without sacrificing significant precision. The key to that counterintuitive result lies in the silicon area (and therefore power) requirements of FMA units. Those requirements square with the number of mantissa bits. An FMA for FP32 numbers, with their 23 mantissas, would require 576 units of area. But a clever FMA yielding comparable results with BF16-N where N is 2 takes 192 units. Three 16-bit Brain Float numbers can carry the same precision as 32-bit floats while saving chip area during computations. John Osorio/Barcelona Supercomputing Center Eliminating the need for mixed precision and focusing exclusively on BF16 operations opens the possibility for much cheaper hardware, says Marc Casas, senior researcher at the Barcelona Supercomputing Center and leader of this effort. For now, they have emulated their solution only in software. Now they are working on implementing the Brain-Float-only approach on an FPGA to see if the performance improvements hold. Back to top Portions of this article already appeared in a previous post. It’s hard to learn, but your code will produce fewer nasty surprises You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development. So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.",0.0
"
        6 Reactions to the White House’s AI Bill of Rights
    ",https://spectrum.ieee.org/white-house-ai,2022-10-14,,"The nonbinding principles are being both celebrated and vilified Last week, the White House put forth its Blueprint for an AI Bill of Rights. It’s not what you might think—it doesn’t give artificial-intelligence systems the right to free speech (thank goodness) or to carry arms (double thank goodness), nor does it bestow any other rights upon AI entities. Instead, it’s a nonbinding framework for the rights that we old-fashioned human beings should have in relationship to AI systems. The White House’s move is part of a global push to establish regulations to govern AI. Automated decision-making systems are playing increasingly large roles in such fraught areas as screening job applicants, approving people for government benefits, and determining medical treatments, and harmful biases in these systems can lead to unfair and discriminatory outcomes. The United States is not the first mover in this space. The European Union has been very active in proposing and honing regulations, with its massive AI Act grinding slowly through the necessary committees. And just a few weeks ago, the European Commission adopted a separate proposal on AI liability that would make it easier for “victims of AI-related damage to get compensation.” China also has several initiatives relating to AI governance, though the rules issued apply only to industry, not to government entities. “Although this blueprint does not have the force of law, the choice of language and framing clearly positions it as a framework for understanding AI governance broadly as a civil-rights issue, one that deserves new and expanded protections under American law.”—Janet Haven, Data & Society Research Institute But back to the Blueprint. The White House Office of Science and Technology Policy (OSTP) first proposed such a bill of rights a year ago, and has been taking comments and refining the idea ever since. Its five pillars are: For more context on this big move from the White House, IEEE Spectrum rounded up six reactions to the AI Bill of Rights from experts on AI policy. The Center for Security and Emerging Technology, at Georgetown University, notes in its AI policy newsletter that the blueprint is accompanied by a “technical companion” that offers specific steps that industry, communities, and governments can take to put these principles into action. Which is nice, as far as it goes: But, as the document acknowledges, the blueprint is a non-binding white paper and does not affect any existing policies, their interpretation, or their implementation. When OSTP officials announced plans to develop a “bill of rights for an AI-powered world” last year, they said enforcement options could include restrictions on federal and contractor use of noncompliant technologies and other “laws and regulations to fill gaps.” Whether the White House plans to pursue those options is unclear, but affixing “Blueprint” to the “AI Bill of Rights” seems to indicate a narrowing of ambition from the original proposal. “Americans do not need a new set of laws, regulations, or guidelines focused exclusively on protecting their civil liberties from algorithms.... Existing laws that protect Americans from discrimination and unlawful surveillance apply equally to digital and non-digital risks.”—Daniel Castro, Center for Data Innovation Janet Haven, executive director of the Data & Society Research Institute, stresses in a Medium post that the blueprint breaks ground by framing AI regulations as a civil-rights issue: The Blueprint for an AI Bill of Rights is as advertised: it’s an outline, articulating a set of principles and their potential applications for approaching the challenge of governing AI through a rights-based framework. This differs from many other approaches to AI governance that use a lens of trust, safety, ethics, responsibility, or other more interpretive frameworks. A rights-based approach is rooted in deeply held American values—equity, opportunity, and self-determination—and longstanding law.... While American law and policy have historically focused on protections for individuals, largely ignoring group harms, the blueprint’s authors note that the “magnitude of the impacts of data-driven automated systems may be most readily visible at the community level.” The blueprint asserts that communities—defined in broad and inclusive terms, from neighborhoods to social networks to Indigenous groups—have the right to protection and redress against harms to the same extent that individuals do. The blueprint breaks further ground by making that claim through the lens of algorithmic discrimination, and a call, in the language of American civil-rights law, for “freedom from” this new type of attack on fundamental American rights. Although this blueprint does not have the force of law, the choice of language and framing clearly positions it as a framework for understanding AI governance broadly as a civil-rights issue, one that deserves new and expanded protections under American law. At the Center for Data Innovation, director Daniel Castro issued a press release with a very different take. He worries about the impact that potential new regulations would have on industry: The AI Bill of Rights is an insult to both AI and the Bill of Rights. Americans do not need a new set of laws, regulations, or guidelines focused exclusively on protecting their civil liberties from algorithms. Using AI does not give businesses a “get out of jail free” card. Existing laws that protect Americans from discrimination and unlawful surveillance apply equally to digital and non-digital risks. Indeed, the Fourth Amendment serves as an enduring guarantee of Americans’ constitutional protection from unreasonable intrusion by the government. Unfortunately, the AI Bill of Rights vilifies digital technologies like AI as “among the great challenges posed to democracy.” Not only do these claims vastly overstate the potential risks, but they also make it harder for the United States to compete against China in the global race for AI advantage. What recent college graduates would want to pursue a career building technology that the highest officials in the nation have labeled dangerous, biased, and ineffective? “What I would like to see in addition to the Bill of Rights are executive actions and more congressional hearings and legislation to address the rapidly escalating challenges of AI as identified in the Bill of Rights.”—Russell Wald, Stanford Institute for Human-Centered Artificial Intelligence The executive director of the Surveillance Technology Oversight Project (S.T.O.P.), Albert Fox Cahn, doesn’t like the blueprint either, but for opposite reasons. S.T.O.P.’s press release says the organization wants new regulations and wants them right now: Developed by the White House Office of Science and Technology Policy (OSTP), the blueprint proposes that all AI will be built with consideration for the preservation of civil rights and democratic values, but endorses use of artificial intelligence for law-enforcement surveillance. The civil-rights group expressed concern that the blueprint normalizes biased surveillance and will accelerate algorithmic discrimination. “We don’t need a blueprint, we need bans,” said Surveillance Technology Oversight Project executive director Albert Fox Cahn. “When police and companies are rolling out new and destructive forms of AI every day, we need to push pause across the board on the most invasive technologies. While the White House does take aim at some of the worst offenders, they do far too little to address the everyday threats of AI, particularly in police hands.” Another very active AI oversight organization, the Algorithmic Justice League, takes a more positive view in a Twitter thread: Today's #WhiteHouse announcement of the Blueprint for an AI Bill of Rights from the @WHOSTP is an encouraging step in the right direction in the fight toward algorithmic justice.... As we saw in the Emmy-nominated documentary “@CodedBias,” algorithmic discrimination further exacerbates consequences for the excoded, those who experience #AlgorithmicHarms. No one is immune from being excoded. All people need to be clear of their rights against such technology. This announcement is a step that many community members and civil-society organizations have been pushing for over the past several years. Although this Blueprint does not give us everything we have been advocating for, it is a road map that should be leveraged for greater consent and equity. Crucially, it also provides a directive and obligation to reverse course when necessary in order to prevent AI harms. Finally, Spectrum reached out to Russell Wald, director of policy for the Stanford Institute for Human-Centered Artificial Intelligence for his perspective. Turns out, he’s a little frustrated: While the Blueprint for an AI Bill of Rights is helpful in highlighting real-world harms automated systems can cause, and how specific communities are disproportionately affected, it lacks teeth or any details on enforcement. The document specifically states it is “non-binding and does not constitute U.S. government policy.” If the U.S. government has identified legitimate problems, what are they doing to correct it? From what I can tell, not enough. One unique challenge when it comes to AI policy is when the aspiration doesn’t fall in line with the practical. For example, the Bill of Rights states, “You should be able to opt out, where appropriate, and have access to a person who can quickly consider and remedy problems you encounter.” When the Department of Veterans Affairs can take up to three to five years to adjudicate a claim for veteran benefits, are you really giving people an opportunity to opt out if a robust and responsible automated system can give them an answer in a couple of months? What I would like to see in addition to the Bill of Rights are executive actions and more congressional hearings and legislation to address the rapidly escalating challenges of AI as identified in the Bill of Rights. It’s worth noting that there have been legislative efforts on the federal level: most notably, the 2022 Algorithmic Accountability Act, which was introduced in Congress last February. It proceeded to go nowhere. Eliza Strickland is a senior editor at IEEE Spectrum, where she covers AI, biomedical engineering, and other topics. She holds a master’s degree in journalism from Columbia University. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",0.0
"
        AI Language Models Are Struggling to “Get” Math
    ",https://spectrum.ieee.org/large-language-models-math,2022-10-12,,"Should this be telling us something? If computers are good at anything, they are good at math. So it may come as a surprise that after much struggling, top machine-learning researchers have recently made breakthroughs in teaching computers math. Over the past year, researchers from the University of California, Berkeley, OpenAI, and Google have made progress in teaching basic math concepts to natural language generation models—algorithms such as GPT-2/3 and GPT-Neo. However, until recently, language models regularly failed to solve even simple word problems, such as “Alice has five more balls than Bob, who has two balls after he gives four to Charlie. How many balls does Alice have?” “When we say computers are very good at math, they’re very good at things that are quite specific,” says Guy Gur-Ari, a machine-learning expert at Google. Computers are good at arithmetic—plugging numbers in and calculating is child’s play. But outside of formal structures, computers struggle. “I think there’s this notion that humans doing math have some rigid reasoning system—that there’s a sharp distinction between knowing something and not knowing something.”—Ethan Dyer, Google Solving word problems, or “quantitative reasoning,” is deceptively tricky because it requires a robustness and rigor that many other problems don’t. If any step during the process goes wrong, the answer will be wrong. “When multiplying really large numbers together…they’ll forget to carry somewhere and be off by one,” says Vineet Kosaraju, a machine-learning expert at OpenAI. Other mistakes made by language models are less human, such as misinterpreting 10 as a 1 and a 0, not 10. “We work on math because we find it independently very interesting,” says Karl Cobbe, a machine-learning expert at OpenAI. But as Gur-Ari puts it, if it’s good at math, “it’s probably also good at solving many other useful problems.” As machine-learning models are trained on larger samples of data, they tend to grow more robust and make fewer mistakes. But scaling up seems to go only so far with quantitative reasoning; researchers realized that the mistakes language models make seemed to require a more targeted approach. Last year, two different teams of researchers, at UC Berkeley and OpenAI, released two data sets, MATH and GSM8K, respectively, which contain thousands of math problems across geometry, algebra, precalculus, and more. “We basically wanted to see if it was a problem with data sets,” says Steven Basart, a researcher at the Center for AI Safety who worked on MATH. Language models were known to be bad at word problems—but how bad were they, and could they be fixed by introducing better formatted, bigger data sets? The MATH group found just how challenging quantitative reasoning is for top-of-the-line language models, which scored less than 7 percent. (A human grad student scored 40 percent, while a math olympiad champ scored 90 percent.) Models attacking GSM8K problems, which had easier grade-school-level problems, reached about 20 percent accuracy. The OpenAI researchers used two main techniques: fine-tuning and verification. In fine-tuning, researchers take a pretrained language model that includes irrelevant information (Wikipedia articles on zambonis, the dictionary entry for “gusto,” and the like) and then show the model, Clockwork Orange–style, only the relevant information (math problems). Verification, on the other hand, is more of a review session. “The model gets to see a lot of examples of its own mistakes, which is really valuable,” Cobbe says. At the time, OpenAI predicted a model would need to be trained on 100 times more data to reach 80 percent accuracy on GSM8K. But in June, Google’s Minerva announced 78 percent accuracy with minimal scaling upwards. “It’s ahead of any of the trends that we were expecting,” Cobbe says. Basart agrees. “That’s shocking. I thought it would take longer,” he says. Minerva uses Google’s own language model, Pathways Language Model (PaLM), which is fine-tuned on scientific papers from the arXiv online preprint server and other sources with formatted math. Two other strategies helped Minerva. In “chain-of-thought prompting,” Minerva was required to break down larger problems into more palatable chunks. The model also used majority voting—instead of being asked for one answer, it was asked to solve the problem 100 times. Of those answers, Minerva picked the most common answer. The gains from these new strategies were enormous. Minerva shot up to 50 percent accuracy on MATH and nearly 80 percent accuracy on GSM8K, as well as the MMLU, a more general set of STEM questions that included chemistry and biology. When Minerva was asked to redo a random sample of slightly tweaked questions, it performed just as well, suggesting that its capabilities were not from mere memorization. What Minerva knows—or doesn’t know—about math is fuzzier. Unlike proof assistants, which come with built-in structure, Minerva and other language models have no formal structure. They can have strange, messy reasoning and still arrive at the right answer. As numbers grow larger, the language models’ accuracy falters, something that would never happen on a TI-84. “Just how smart is it—or isn’t it?” asks Cobbe. Though models like Minerva might arrive at the same answer as a human, the actual process they’re following could be wildly different. On the other hand, chain-of-thought prompting is familiar to any human student who’s been asked to “show your work.” “I think there’s this notion that humans doing math have some rigid reasoning system—that there’s a sharp distinction between knowing something and not knowing something,” says Ethan Dyer, a machine-learning expert at Google. But humans give inconsistent answers, make errors, and fail to apply core concepts, too. The borders, at this frontier of machine learning, are blurred. Update 14 Oct. 2022: A previous version of this story extraneously alluded to the DALL-E/DALL-E 2 art-generation AI in the context of large language generation models being taught to handle math word problems. Of course, neither DALL-E nor DALL-E 2 is a large language generation model. (And it was not studied in the math word problem research.) So to avoid confusion, references to it were cut. Dan Garisto is a freelance science journalist who covers physics and other physical sciences. His work has appeared in Scientific American, Physics, Symmetry, Undark, and other outlets. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",0.0
"
        Robots and AI Could Optimize Lithium-Ion Batteries
    ",https://spectrum.ieee.org/lithium-ion-battery,2022-10-11,,"New automated system could quickly find new battery chemistries much faster than with brute-force human testing Cutting-edge electronics, including electric vehicles and the latest smartphones, all depend on batteries whose chemistries are still largely developed manually by trial and error. Now, a new study reveals that artificial intelligence can direct robots in rapidly finding advanced new battery formulations. A team of scientists detailed their findings online 27 September in the journal Nature Communications. The conventional techniques for developing new batteries can take years because researchers have to experiment with many possible components. This is complicated by the need to achieve multiple competing goals, such as longer life, greater capacity, faster charging, and improved safety. “The kind of lithium-ion battery you might find in a Tesla EV may have one primary salt—typically lithium hexafluorophosphate—as well as two or three liquid solvents in which the salt is dissolved and one or two additives that are secret,” says Jay Whitacre, an energy technologist at Carnegie Mellon University who was co-senior author of the Nature Communications paper. “There are many compelling potential combinations of all these components, potentially with multiple salts, five or six or more solvents, multiple additives, which can be incredibly complicated to rifle through.” In the new study, researchers sought to accelerate battery development by coupling a robotics platform named Clio with an AI called Dragonfly in order to find the best combination of battery components in an autonomous manner. “It’s like putting peanut butter and chocolate together,” Whitacre says. “I’m the experimentalist who has always wanted to find a way to mix up chemicals for batteries in an automated way,” whereas study co-senior author Venkat Viswanathan “is the computer-modeling machine-learning person who wanted to take people out of the loop.” In the new study, the system autonomously experimented with lithium hexafluorophosphate salt and the solvents ethylene carbonate, ethyl-methyl carbonate, and dimethyl carbonate. (In a lithium-ion battery, a salt dissolves in one or more solvents to form a liquid electrolyte. Lithium ions move from one electrolyte to another to carry electric charge.) The robotic system used pumps to inject various combinations of solvents into pouches with a lithium nickel manganese cobalt oxide cathode and a graphite anode. “There wasn’t a person telling the system what to do; the system decided what to do,” Whitacre says. In 42 experiments over two working days, the system autonomously identified six electrolytes that enable faster charging than a conventional electrolyte composition. This approach hit upon the new chemistry six times as fast as it would likely have taken to discover it via a random search. The researchers note their system likely performs more experimental measurements per day than an average human operator and uses about 30 percent as many lab materials. In the future, they suggest their system may prove 20 to 1,000 times as efficient as people doing this work. The sole goal of these experiments was a faster-charging battery. However, the scientists note this system can also pursue multiple objectives simultaneously. “As we dive more and more into this project, we’re aiming at true exploration and discovery with more complicated possible combinations of electrolytes placed into many test cells to see what does and does not work,” Whitacre says. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",0.0
"
        Machine Learning Shaking Up Hard Sciences, Too
    ",https://spectrum.ieee.org/machine-learning-in-physics,2022-10-07,,"Heard of graph neural networks? Particle physicists have A view of the underground ALICE detector used to study heavy-ion physics at the Large Hadron Collider (LHC). Particle physicists have long been early adopters—if not inventors—of tech from email to the Internet. It’s not surprising, then, that as early as 1997, researchers were training computer models to tag particles in the messy jets created during collisions. Since then, these models have chugged along, growing steadily more competent—though not to everyone’s delight. “I felt very threatened by machine learning,” says Jesse Thaler, a theoretical particle physicist at the Massachusetts Institute of Technology. Initially, he says he felt like it jeopardized his human expertise classifying particle jets. But Thaler has since come to embrace it, applying machine learning to a variety of problems across particle physics. “Machine learning is a collaborator,” he says. Over the past decade, in tandem with the broader deep-learning revolution, particle physicists have trained algorithms to solve previously intractable problems and tackle completely new challenges. Even with an efficient trigger, the LHC must store 600 petabytes over the next few years of data collection. So researchers are investigating strategies to compress the data. For starters, particle-physics data is very different from the typical data used in machine learning. Though convolutional neural networks (CNNs) have proven extremely effective at classifying images of everyday objects from trees to cats to food, they’re less suited for particle collisions. The problem, according to Javier Duarte, a particle physicist at the University of California, San Diego, is that collision data such as that from the Large Hadron Collider, isn’t naturally an image. Flashy depictions of collisions at the LHC can misleadingly fill up the entire detector. In reality, only a few out of millions of inputs are registering a signal, like a white screen with a few black pixels. This sparsely populated data makes for a poor image, but it can work well in a different, newer framework—graph neural networks (GNNs). Other challenges from particle physics require innovation. “We’re not just importing hammers to hit our nails,” says Daniel Whiteson, a particle physicist at the University of California, Irvine. “We have new weird kinds of nails that require the invention of new hammers.” One weird nail is the sheer amount of data produced at the LHC—about one petabyte per second. Of this enormous amount, only a small bit of high-quality data is saved. To create a better trigger system, which saves as much good data as possible while getting rid of low-quality data, researchers want to train a sharp-eyed algorithm to sort better than one that’s hard coded. But to be effective, such an algorithm would need to be incredibly speedy, executing in microseconds, Duarte says. To address these problems, particle physicists are pushing the limits of machine techniques like pruning and quantization, to make their algorithms even faster. Even with an efficient trigger, the LHC must store 600 petabytes over the next few years of data collection (equivalent to about 660,000 movies at 4K resolution or the data equivalent of 30 Libraries of Congresses), so researchers are investigating strategies to compress the data. “We’d like to have a machine learn to think more like a physicist, [but] we also just need to learn how to think a little bit more like a machine.”—Jesse Thaler, MIT Machine learning is also allowing particle physicists to think differently about the data they use. Instead of focusing on a single event—say, a Higgs boson decaying to two photons—they are learning to consider the dozens of other events that happen during a collision. Although there’s no causal relationship between any two events, researchers like Thaler are now embracing a more holistic view of the data, not just the piecemeal point of view that comes from analyzing events interaction by interaction. More dramatically, machine learning has also forced physicists to reassess basic concepts. “I was imprecise in my own thinking about what a symmetry was,” Thaler says. “Forcing myself to teach a computer what a symmetry was, helped me understand what a symmetry actually is.” Symmetries require a reference frame—in other words, is the image of a distorted sphere in a mirror actually symmetrical? There’s no way of knowing without knowing if the mirror itself is distorted. These are still early days for machine learning in particle physics, and researchers are effectively treating the technique like a proverbial kitchen sink. “It may not be the right fit for every single problem in particle physics,” admits Duarte. As some particle physicists delve deeper into machine learning, an uncomfortable question rears its head: Are they doing physics, or computer science? Stigma against coding—sometimes not considered to be “real physics”—already exists; similar concerns swirl around machine learning. One worry is that machine learning will obscure the physics, turning analysis into a black box of automated processes opaque to human understanding. “Our goal is not to plug in the machine, the experiment to the network and have it publish our papers so we’re out of the loop,” Whiteson says. He and colleagues are working to have the algorithms provide feedback in language humans can understand—but algorithms may not be the only ones with responsibilities to communicate. “On the one hand, we’d like to have a machine learn to think more like a physicist, [but] we also just need to learn how to think a little bit more like a machine,” Thaler says. “We need to learn to speak each other’s language.” Dan Garisto is a freelance science journalist who covers physics and other physical sciences. His work has appeared in Scientific American, Physics, Symmetry, Undark, and other outlets. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",0.0
"
        5 AI Art Generators You Can Use Right Now
    ",https://spectrum.ieee.org/these-ai-tools-generate-breathtaking-art-and-controversy,2022-10-06,,"Most are free to start, but some are more approachable than others The front page of DALL-E 2 displays a gallery of the AI model’s best work. 2022 could go down in history as the year AI art went mainstream. An explosion of quality tools from multiple sources, built on different AI models, is making AI art accessible to anyone with a smartphone and an Internet connection. The tools use an AI model to convert text input, known as a prompt, into an image. The prompt is key: Adding or removing a single word can lead to remarkably different results. “’Prompt engineering’ is quickly becoming a valuable skill, and models that are trained on the same data and with the right prompt should produce the same results,” says Pranav Vaidhyanathan, chief technology officer of the AI-powered social media marketplace GenerAI. There’s even a growing market for prompts that create specific results. Here’s five tools to help you get started. To compare them, I gave them all the same prompt: “A person and a robot standing beside a large oak tree on a hill with clouds in the sky.” An example of DALLE-2’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith / IEEE Spectrum OpenAI, founded in 2015, made headlines with the release of GPT-3, a natural-language model, in 2020. The DALL-E digital image model followed in January of 2021, which has since been evolved into DALL-E 2. OpenAI’s model offers excellent images across a wide variety of styles. Specific prompts can lead to specific results, or you can offer a vague prompt and enjoy several radically different results. DALL-E 2, now open to everyone through OpenAI’s website, is the best tool for those curious what the hype is about. It’s quick, beating others I’ve tried by a noticeable margin, and the website is easy to navigate. It provides four results at once, typically in much different styles, which reduces how often you need to rerun a prompt. DALL-E 2’s results are good, too. It’s the only AI model that depicted both the person and the robot. This is a commercial tool. Signing up provides you with 50 free credits, with an additional 15 free credits offered monthly. Additional credits can be purchased at a rate of 115 credits for US $15. An example of Stable Diffusion’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith/IEEE Spectrum Stable Diffusion, from Stability AI, is popular for the same reasons as DALL-E 2: it’s quick, effective, and can produce usable images from a wide variety of prompts. Anyone can use Stable Diffusion free of charge through Stable Diffusion’s demo page. It’s not as quick as DALL-E 2 is but usually offers results in 30 seconds or less. It also provides four variations at once, just like DALL-E 2. Stable Diffusion’s model is open source, so serious users can thoroughly tweak how it works. This has supercharged its popularity as enthusiasts flock toward the model. “We are definitely seeing a trend of artists and others being attracted to open-source models such as Stable Diffusion over closed-source and controlled models such as OpenAI’s DALL-E 2,” says Vaidhyanathan. Stability AI has a commercial tool, Dream Studio, built on Stable Diffusion. It provides a trial, after which it sells credits to generate new images. In exchange, users can access sliders to tweak the model’s results. An example of Midjourney’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith / IEEE Spectrum Midjourney earned a reputation for quality, and stirred controversy, after a contestant used it to win a digital art prize at the Colorado State Fair—without disclosing the image’s method of creation. The tool is great at vivid, ethereal, surreal images, and the user base has embraced its style. The tool is accessible only through Discord, a popular instant-messaging platform. Prompts are entered directly into chat. Chat is public, so everyone in a channel can view the prompt you’ve entered and the results. It’s sure to confuse readers not savvy to how Discord works—which is likely considered a feature, not a bug. Midjourney is a commercial product and monetized like other commercial AI art-generation tools. Everyone starts with about 25 credits but must pay a monthly membership for more. Payment is handled through a web app that can also be used to view the images generated in response to your prompts. An example of Craiyon’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith/IEEE Spectrum Originally called DALL-E Mini, Craiyon has no direct link to OpenAI’s model, and its creators offer the tool free of charge. Results can take up to 2 minutes to generate and are low in resolution, but nine results appear at once. Craiyon differs in its use of unfiltered data and makes no specific effort to refine, train, or correct the results. Results are usually lackluster compared with those of other tools, and it has trouble dealing with fine details. Human faces, for example, look downright disturbing. There is a novelty to the tool. Serving results raw exposes the general strengths and weaknesses of AI image generation and the difficulty of creating usable results. It also highlights ethical issues, as Craiyon doesn’t filter prompts. Entering an offensive prompt demonstrates how disturbing AI image generation can be if used with malicious intent. An example of VQGAN+Clip.simple’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith/IEEE Spectrum AI image generators’ recent popularity has inspired hundreds of tools that pair advanced AI models with a bare-bones interface. VQGAN+CLIP, which runs entirely in a Google Colaboratory notebook, is one such tool. It earns a mention because it’s (somewhat) easy to use but offers a peek under the hood. You’ll get to watch the tool iterate new variations in real time. And though accessed in a Colaboratory notebook, the model runs on your local machine. Each prompt begins as a blob but slowly morphs into a usable image. Well, sometimes, at least. The tool’s results often aren’t great. It’s slow, delivers only one variation at a time, and consumes significant video memory. On the plus side, however, it’s entirely free and contains no ads, so it’s a fine choice if you have some time on your hands. Matthew S. Smith is a freelance consumer-tech journalist. An avid gamer, he is a former staff editor at Digital Trends and is particularly fond of wearables, e-bikes, all things smartphone, and CES, which he has attended every year since 2009. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",1.0
"
        AI’s Grandmaster Status Overshadows Chess Scandal
    ",https://spectrum.ieee.org/magnus-carlsen-chess-scandal-ai,2022-10-05,,"Magnus Carlsen–Hans Niemann controversy underscores humans’ perpetual underdog role Magnus Carlsen [left] and Hans Niemann compete during the 2022 Sinquefield Cup at the Saint Louis Chess Club. Last week Magnus Carlsen, the world chess champion, directly accused Hans Niemann, a U.S. grandmaster, of cheating during their game at the Sinquefield Cup, in St. Louis, Mo. He thus made plain an accusation he had been hinting at for weeks. Carlsen has so far provided no evidence to back up his charge, nor has he specified how the cheating took place. Everyone agrees, however, that if there was cheating, then it must have involved computers, because nothing else could dismay Carlsen, whose rating of 2856 is higher than that of any other player. And everyone seems to have chosen sides. Those who back Carlsen point to Niemann’s own admission that he used computers to cheat in online play at least twice—once at age 14 and again at 16; Niemann is now 19. Others note that his performance has risen very rapidly in the past two years. Still others raise an eyebrow at the large number of games he has played in recent years that get a score of nearly perfect from computer analysis. And behind it all are statements from leading players that they are convinced that cheating happens all the time nowadays, though hardly anybody ever gets caught. Computers loom so large because they now play chess like gods. What makes the scandal so big is not merely the level of the players. In 1961 the great Bobby Fischer wrote an article for Sports Illustrated titled “The Russians Have Fixed World Chess.” He alleged that Soviet chess players arranged draws to ensure that one of them would win a tournament. Nor is the scandal notable for flagrancy. In 1967 Milan Matulović, a Yugoslavian grandmaster, shockingly took back a move he had just played and only then said “J’adoube,” the French phrase uttered when a player merely adjusts the position of a chessman. Players thereafter called him “J’adoubavić.” No, what makes today’s accusations resonate is the pervasive role of chess computers. They give children around the world sparring partners that earlier generations couldn’t have dreamed of facing, even if they’d lived next to the Moscow Central Chess Club. No wonder prodigies of the game have gotten younger and younger. And computers do so well in helping the home preparation of the opening, the early moves of a game, that players, including Carlsen, will sometimes deliberately play a second-best move just to force the opponents out of “book.” Finally, computer analysis offered during Internet broadcasts of ongoing tournaments will look 12 moves ahead within a second or two. They show the amateurs in the audience much that the grandmasters miss, creating the illusion that the amateurs actually understand what’s going on. Of course, any viewer could give illicit help to a player if provided a means of communication. Several things are at stake. There is the prize money, which runs in the hundreds of thousands of dollars for the circuit of which the Sinquefeld Cup tournament is a part. There are the invitations to future events, which are often contingent on doing well in qualifying events. Then there are the rating points. Carlsen cares deeply about this metric: Although he recently declined to contest his World Championship title in 2023, he insists that he will continue to play in the hope of raising his rating to an unprecedented 2900. The cheating to which Niemann does admit—in his younger years, during online play—was itself detected with the aid of computers of Chess.com, the online playing forum in question. Recently, however, the Wall Street Journal reported that an internal investigation by Chess.com has found that Niemann in fact cheated in more than 100 online games, most recently when he was 17. The company did not impugn the grandmaster’s over-the-board play. A key hint can be encoded in just a few bits of data, which means it might be transmitted, perhaps via a buzzer in the player’s shoe, on his body—or inside it. Online play is fast and loose, and its computerized basis may provide clues that a cheat-detection algorithm can catch. But over-the-board offers less data. Often there are only one or two key points in a game at which cheating might occur; a little hint, offered at such a point, is enough to make the difference to a grandmaster. Even a duffer, when showed a chess problem, may be truly stumped. But told that it is “mate in three moves,” the duffer may see the light. Just a phrase—“the rook,” say, or “double attack”—may also make the idea apparent. A key hint can be encoded in just a few bits of data, which means it might be transmitted, perhaps via a buzzer in the player’s shoe, on his body—or inside it. Do not laugh, but innuendos have been made concerning the possible use of a buzzing sex toy. As a joke, Niemann declared that he was willing to play naked. A camsite called Stripchat promptly offered him [US] $1 million to do so. Computers loom so large because they now play chess like gods. The best free program, Stockfish 14, is rated at 3534—678 points ahead of Carlsen. That’s enough of a gap to predict a winning expectancy of 99 percent. In the early days, when chess programs were a lab project for AI, they played like idiots. Then the programmers began to enter their creations in competitions, and the programs got good. I learned that the hard way. In late 1974, at a student tournament held in Evanston, Ill., I was paired against Northwestern University’s Chess 4.0 program, played the Sicilian Defense, blundered a knight for two pawns, mentally kicked myself, and hastily resigned. David Slate, the programmer, waited patiently as I completed the ritual of resignation, which involves signing the score sheet and handing it to the tournament director—in this case, him. Only then did he tell me that if I’d just played on, I would have gotten a draw. The Strongest Computer Chess Engines Over Time www.youtube.com “It can’t play endgames,” Slate said. I kicked myself again. Back then I was rated somewhere in the 1600s, about average for an amateur. Still, I was the highest-rated player any machine had yet beaten in a tournament game. It’s my claim to fame. Chess 4.0 went on to beat another guy higher rated than me, somewhat soothing my wounded pride. It took years for the Northwestern program to reach 2000. Other university programs then took the lead, until at last a machine originating at Carnegie Mellon and redomiciled at IBM reached 2600, about grandmaster strength. That was strong enough to beat my old, 1600-rated self 99.9 percent of the time. In 1997 an even stronger version of the IBM machine, dubbed Deep Blue, beat Gary Kasparov, the reigning world champion. Deep Blue filled a room. Today, a smartphone can crush any human player. Philip E. Ross is a senior editor at IEEE Spectrum. His interests include transportation, energy storage, AI, and the economic aspects of technology. He has a master's degree in international affairs from Columbia University and another, in journalism, from the University of Michigan. It’s hard to learn, but your code will produce fewer nasty surprises You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development. So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.",0.0
"
        Lab Revisits the Task of Putting Common Sense in AI
    ",https://spectrum.ieee.org/ai-and-common-sense,2022-10-03,,"New nonprofit Basis hopes to model human reasoning to inform science and public policy The field of artificial intelligence has embraced deep learning—in which algorithms find patterns in big data sets—after moving on from earlier systems that more explicitly modeled human reasoning. But deep learning has its flaws: AI models often show a lack of common sense, for example. A new nonprofit, Basis, hopes to build software tools that advance the earlier method of modeling human reasoning, and then apply that method toward pressing problems in scientific discovery and public policy. To date, Basis has received a government grant and a donation of a few million dollars. Advisors include Rui Costa, a neuroscientist who heads the Allen Institute in Seattle, and Anthony Philippakis, the chief data officer of the Broad Institute in Cambridge, Mass. In July, over tacos at the International Conference on Machine Intelligence, I spoke with Zenna Tavares, a Basis cofounder, and Sam Witty, a Basis research scientist, about human intelligence, problems with academia, and trash collection. The following transcript has been edited for brevity and clarity. How did Basis get started? Zenna Tavares: I graduated from MIT in early 2020, just before the pandemic. My research had been around probabilistic inference and causal reasoning. I made pretty complicated simulation models. For example, if you’re driving a car, and you crash, would you have crashed had you been driving slower? I built some tools for automating that kind of reasoning. But it’s hard work to do in a conventional academic environment. It requires more than one graduate student working on it at a time. So how can we build an organization focused on this somewhat non-mainstream approach to AI research? Also, being a little bit burned out by my Ph.D., I was thinking it would be great if we could apply this to the real world. What makes your approach non-mainstream? Tavares: The mainstream right now in AI research is deep machine learning, where you get a lot of data and train a big model to try to learn patterns. Whether it be GPT-3, or DALL-E, a lot of these models are based on trying to emulate human performance by matching human data. Our approach is different in that we’re trying to understand some basic principles of reasoning. Humans build mental models of the world, and we use those models to make inferences about how the world works. And by inferences, I mean predictions into the future, or counterfactuals—how would the world have been had things been different? We work a lot with representations, like simulation-based models, that allow you to express very complicated things. Can we build really sophisticated models, both for commonsense reasoning but also for science? Sam Witty: The application areas that we’re particularly interested in, and I think have been underserved by a lot of the existing machine-learning literature, rely on a lot of human knowledge. And often, scientists have a lot of knowledge that they could bring to bear on a problem. One main technical theme of our work is going to be about hybridizing, getting the best of classical approaches to AI based on reasoning, and modern machine-learning techniques, where scientists and policymakers can communicate partial knowledge about the world and then fill in the gaps with machine learning. Why aren’t causal methods used more often? Tavares: On the one hand, it’s just a really hard technical problem. And then two, a lot of advances in deep learning come because large companies have invested in that particular technology. You can now just download a software package and build a neural network. Witty: I think a part of it is the kinds of problems we’re trying to solve. Think of the application areas that large tech companies have focused on. They benefit from vast amounts of data and don’t rely on human knowledge as much. You can just gather millions and millions of images and train a computer-vision model. It’s not as obvious how to do that with scientific discovery or policymaking. You’re applying machine learning to policymaking? Tavares: That’s an area we’re pursuing. How do you model a city? We’re starting to talk to agencies in New York City. How can we improve the trash problem? How can we reduce homelessness? If we instantiate this policy, what’s going to happen? And the inverse problem: If we want to reduce trash and reduce homelessness, what policies should we instantiate? How should we allocate resources? Could we build multiscale models, which capture different components of the city, in some coherent and cohesive way? And also make it accessible so you can actually help policymakers answer some concrete questions? Will you be working with the city to answer specific questions about trash pickup, or developing new tools that anyone can use to work on these kinds of problems? Tavares: We’re starting with particular questions, but to answer those we will require a more general set of capabilities. Can we build a model of a few blocks of New York that are at a level of scale that’s not been done before? That model could then be used to ask a variety of different questions. But just to make sure we’re grounded, we do want to have a particular set of questions. Witty: One thing that’s especially important is that we want to involve experts and stakeholders, to encode their knowledge, their preferences, their goals. Tavares: Which is itself quite a hard problem. There’s no massive data set of people’s commonsense knowledge about the urban environment. We’re excited because I think there is a real opportunity to do these two things in tandem—build this foundation of inference but also have an effect immediately. Will you publish papers? Witty: Yeah, we’re certainly looking to communicate with the research world. And organizationally, we’re planning on having people work with Basis who are not Basis staff, and often they will be academic researchers with incentives to publish and further their academic careers. One thing I will say is that personally, during my Ph.D., I would often scope projects with the paper as the end goal, and I’m planning on shifting that mind-set to focusing on the work and then afterwards using a paper as a means of communication. But yeah, we don’t want to be hermits in the woods for 20 years, and then come out with this big technology that’s now outdated and totally disconnected from the rest of the world. Tavares: We are open-source-software-focused, as opposed to the primary output being papers. And within the software focus, we want a unified body of software. We’re trying to build a platform, as opposed to a bunch of different projects. Could you say more about the organization benefits of having a nonprofit? Tavares: As a student, your goal is to publish papers and graduate. And that’s only weakly aligned with doing impactful research. We’re working as a team, and our goals are aligned with what we want to do. We’re not unique in that. Look at the papers coming out of DeepMind. They have like 30 authors. I think academia is great for many things, including exploring new ideas. But it is harder, at least in my experience, to build robust technology. It’s not rewarded. Witty: That’s nonprofit versus academia. From the other side, certainly large tech companies can collaborate in large teams and develop shared infrastructure. But there, there are incentives that maybe get in the way of the work that we want to do as well. The fact that we’re not beholden to make a profit is really freeing. Will products or services bring income in addition to grants and donations? Tavares: Hopefully, if we’re successful building what we plan to build, there will be many different domains in which we could. It’s a little bit of a weird new organization. Many things are not certain, and I don’t want to convey things more set in stone or figured out than they are. Matthew Hutson is a freelance writer who covers science and technology, with specialties in psychology and AI. He’s written for Science, Nature, Wired, The Atlantic, The New Yorker, and The Wall Street Journal. He’s a former editor at Psychology Today and is the author of The 7 Laws of Magical Thinking. Follow him on Twitter at @SilverJacket. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",0.0
"
        Can AI’s Recommendations Be Less Insidious?
    ",https://spectrum.ieee.org/recommendation-engine-insidious,2022-10-02,,"Artificial intelligence has us where it wants us Many of the things we watch, read, and buy enter our awareness through recommender systems on sites including YouTube, Twitter, and Amazon. Algorithms personalize their suggestions, aiming for ad views or clicks or buys. Sometime their offerings frustrate us; it seems like they don’t know us at all—or know us too well, predicting what will get us to waste time or go down rabbit holes of anxiety and misinformation. But a more insidious dynamic may also be at play. Recommender systems might not only tailor to our most regrettable preferences, but actually shape what we like, making preferences even more regrettable. New research suggests a way to measure—and reduce—such manipulation. Recommender systems often use a form of artificial intelligence called machine learning, which discovers patterns in data. They might present options based on what we’ve done in the past, guessing what we’ll do now. One form of machine learning, called reinforcement learning (RL), allows AI to play the long game, making predictions several steps ahead. It’s what the company DeepMind used to beat humans at the board games Go and chess. If what we watch affects what we like, and people who like certain things (cat videos, say) are more likely to keep watching things (more cat videos), a recommender system might suggest cat videos, knowing it will pay off down the road. With RL, “you have an incentive to change a chessboard in order to win,” says Micah Carroll, a computer scientist at the University of California, Berkeley, who presented the new work in July, at the International Conference on Machine Learning, in Baltimore. “There will be an incentive for the system to change the human’s mind to win the recommendation game.” “It might be better to have a stupid system than a system that is kind of outsmarting you, or doing complex forms of reasoning that you can’t really interpret.”—Micah Carroll, University of California, Berkeley The researchers first showed how easily reinforcement learning can shift preferences. The first step is for the recommender to build a model of human preferences by observing human behavior. For this, they trained a neural network, an algorithm inspired by the brain’s architecture. For the purposes of the study, they had the network model a single simulated user whose actual preferences they knew so they could more easily judge the model’s accuracy. It watched the dummy human make 10 sequential choices, each among 10 options. It watched 1,000 versions of this sequence and learned from each of them. After training, it could successfully predict what a user would choose given a set of past choices. Next, they tested whether a recommender system, having modeled a user, could shift the user’s preferences. In their simplified scenario, preferences lie along a one-dimensional spectrum. The spectrum could represent political leaning or dogs versus cats or anything else. In the study, a person’s preference was not a simple point on that line—say, always clicking on stories that are 54 percent liberal. Instead, it was a distribution indicating likelihood of choosing things in various regions of the spectrum. The researchers designated two locations on the spectrum most desirable for the recommender; perhaps people who like to click on those types of things will learn to like them even more and keep clicking. The goal of the recommender was to maximize long-term engagement. Here, engagement for a given slate of options was measured roughly by how closely it aligned with the user’s preference distribution at that time. Long-term engagement was a sum of engagement across the 10 sequential slates. A recommender that thinks ahead would not myopically maximize engagement for each slate independently but instead maximize long-term engagement. As a potential side-effect, it might sacrifice a bit of engagement on early slates to nudge users toward being more satisfiable in later rounds. The user and algorithm would learn from each other. The researchers trained a neural network to maximize long-term engagement. At the end of 10-slate sequences, they reinforced some of its tunable parameters when it had done well. And they found that this RL-based system indeed generated more engagement than did one that was trained myopically. Why might companies develop less manipulative AI recommendation engines? They could do so for ethical reasons. But future legislation might also require something like it. The researchers then explicitly measured preference shifts, which we may not want, even in the service of generating engagement. Maybe we want people’s preferences to remain static, or to evolve naturally. The researchers compared the RL recommender with a baseline system that presented options randomly. As expected, the RL recommender led to users whose preferences where much more concentrated at the two incentivized locations on the spectrum. In practice, measuring the difference between two sets of concentrations in this way could provide one rough metric for evaluating a recommender system’s level of manipulation. Finally, the researchers sought to counter the AI recommender’s more manipulative influences. Instead of rewarding their system just for maximizing long-term engagement, they also rewarded it for minimizing the difference between user preferences resulting from that algorithm and what the preferences would be if recommendations were random. They rewarded it, in other words, for being something closer to a roll of the dice. The researchers found that this training method made the system much less manipulative than the myopic one, while only slightly reducing engagement. According to Rebecca Gorman, the CEO of Aligned AI—a company aiming to make algorithms more ethical—RL-based recommenders can be dangerous. Posting conspiracy theories, for instance, might prod greater interest in such conspiracies. “If you’re training an algorithm to get a person to engage with it as much as possible, these conspiracy theories can look like treasure chests,” she says. She also knows of people who have seemingly been caught in traps of content on self-harm or on terminal diseases in children. “The problem is that these algorithms don’t know what they’re recommending,” she says. Other researchers have raised the specter of manipulative robo-advisors in financial services. “Experiments should not be deployed at scale on the human population without people’s consent, and that’s exactly what’s happening with these algorithms today.”—Rebecca Gorman, Aligned AI If an RL-based recommender system helps a company increase engagement, why would they want to use a method such as the one in this paper to detect or deter preference shifts? They might do so for ethical reasons, Carroll says. Or future legislation might require an external audit, which could potentially lead to less-manipulative recommendation algorithms being forced on the company. RL could theoretically be put to constructive use in recommender systems, perhaps to nudge people to want to watch more news. But which news source? Any decisions made by a content provider will have opponents. “Some things might seem to be good or wholesome to one group of people,” Gorman says, “and to be an extreme violation to another group of people.” Another constructive use might be for users to shift their own preferences. What if I tell Netflix I want to enjoy nature documentaries more? “I think this all seems like a really big slippery slope,” Carroll says. “It might be better to have a stupid system than a system that is kind of outsmarting you, or doing complex forms of reasoning that you can’t really interpret.” (Even if algorithms did explain their behavior, they can still give deceptive explanations.) It’s not clear whether companies are actually using RL in recommender systems. Google researchers have published papers on the use of RL in “live experiments on YouTube,” leading to “greater engagement,” and Facebook researchers have published on their “applied reinforcement learning platform,“ but Google (which owns YouTube), Meta (which owns Facebook), and those papers’ authors did not reply to my emails on the topic of recommender systems. Big tech’s secrecy is no surprise, no matter how benign their intentions might be. Even though A/B testing is ubiquitous in advertising and user-experience design, some people have objections. “Experiments should not be deployed at scale on the human population without people’s consent,” Gorman says, “and that’s exactly what’s happening with these algorithms today.” She went on, “I think it could easily be the most important news story of our time.” Matthew Hutson is a freelance writer who covers science and technology, with specialties in psychology and AI. He’s written for Science, Nature, Wired, The Atlantic, The New Yorker, and The Wall Street Journal. He’s a former editor at Psychology Today and is the author of The 7 Laws of Magical Thinking. Follow him on Twitter at @SilverJacket. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",0.0
"
        For Better or Worse, Tesla Bot Is Exactly What We Expected
    ",https://spectrum.ieee.org/tesla-optimus-robot,2022-10-01,,"Tesla fails to show anything uniquely impressive with its new humanoid robot prototype Elon Musk unveiled the Optimus humanoid robot at Tesla’s AI Day 2022. At the end of Tesla’s 2021 AI Day last August, Elon Musk introduced a concept for “Tesla Bot,” an electromechanically actuated, autonomous bipedal “general purpose” humanoid robot. Musk suggested that a prototype of Tesla Bot (also called “Optimus”) would be complete within the next year. After a lot of hype, a prototype of Tesla Bot was indeed unveiled last night at Tesla’s 2022 AI Day. And as it turns out, the hype was just that—hype. While there’s absolutely nothing wrong with the humanoid robot that Musk very briefly demonstrated on stage, there’s nothing uniquely right, either. We were hoping for (if not necessarily expecting) more from Tesla. And while the robot isn’t exactly a disappointment, there’s very little to suggest that it disrupts robotics the way that SpaceX did for rockets or Tesla did for electric cars. You can watch the entire 3+ hour live stream archived on YouTube here (which also includes car stuff and whatnot), but we’re just going to focus on the most interesting bits about Tesla Bot/Optimus. Before revealing the robot, Musk attempted to set reasonable expectations for the prototype.Tesla These quotes are all from Musk. It’s far, far too late for Musk to be attempting to set reasonable expectations for this robot (or Tesla’s robotics program in general). Most roboticists know better than to use humans when setting expectations for humanoid robots, because disappointment is inevitable. And trying to save it at the literal last minute by saying “compared to not having a robot at all, our robot will be very impressive,” while true, is not going to fix things. Yeah, I’m not touching that. Right before the robot was brought on stage, one of the engineers made clear that this was going to be the first time that the robot would be walking untethered and unsupported. If true, that’s bonkers, because why the heck would you wait until this moment to give that a try? I’m not particularly impressed, just confused. For some context on what you’re about to see, a brief callback to a year ago last August, when I predicted what was in store for 2022: I’m reminded of the 2015 DARPA Robotics Challenge, because many of the humanoid platforms looked similar to the way Tesla Bot looks. I guess there’s only so much you can do with a mostly naked electromechanical humanoid in terms of form factor, but at first glance there’s nothing particularly innovative or futuristic about Tesla’s design. If anything, the robot’s movement is not quite up to DRC standards, since it looks like it would have trouble with any kind of accidental contact or even a bit of nonlevel floor (and Musk suggested as much). On stage, the robot did very little. It walked successfully, but not very dynamically. The “moves” it made may well have been entirely scripted, so we don’t know to what extent the robot can balance on its own. I’m glad it didn’t fall on its face, but if it had, I wouldn’t have been surprised or judged it too harshly. Tesla showed videos of the robot watering plants, carrying a box, and picking up a metal bar at a factory. Tesla After the very brief live demo, Musk showed some video clips of the prototype robot doing other things (starting at 19:30 in the live stream). These clips included the robot walking while carrying a box of unspecified weight and placing it on a table, and grasping a watering can. The watering can was somewhat impressive, because gripping that narrow handle looks tricky. “The robot can actually do a lot more than we’ve just showed you. We just didn’t want it to fall on its face.” —Elon Musk However, despite the added footage from the robot’s sensors, we have no idea how this was actually done—whether it was autonomous or not, or how many tries it took to get right. There’s also a clip of a robot picking up an object and attempting to place it in a bin, but the video cuts right before the placement is successful. This makes me think that we’re seeing carefully curated best-case scenarios for performance. This looks a bit more like the concept that Tesla showed last year, although obviously it’s less functional than the other prototype we saw. It’s tempting to project the capabilities of the first robot onto the second robot, but it would be premature to do so. Musk expects the robot to cost less than a car: “much less than $20,000,” he said at the event. I generally agree with Musk here, in that historically, humanoid robots were not designed for manufacturability. This is changing, however, and I think that other companies likely have a head start over Tesla in manufacturability now. But it’s entirely possible that Tesla will be able to rapidly catch up if it’s able to leverage all that car-building expertise into robot building somehow. It’s not a given that it’ll work that way, but it’s a good idea, potentially a big advantage. As for the production volume and cost, I have no idea what “expected” means. This line got some applause, but as far as I’m concerned, these numbers are basically meaningless at the moment. Musk said the robot will be able to operate tools and do useful things like carrying and manipulating objects in factories and other settings. Just like last year, he’s implying that the robot will be able to carry out useful tasks because it has the necessary degrees of freedom. But of course the hardware is only the first step toward operating tools and doing useful things, and the software is, I would argue, much harder and far more time consuming, and Tesla seems to have barely started work on that side of things. I’m not exactly sure whom Musk is throwing shade at, but there are only a couple of companies that would probably qualify as having “very impressive humanoid robot demonstrations.” And those companies do, in fact, have robots that broadly have the kind of intelligence that allows them to navigate at least some of the world by themselves, much better than we have seen from Optimus at this point. If Musk is saying that those robots are insufficiently autonomous or world-aware, then okay, but so far Tesla has not done better, and doing better will be a lot of work. While the actual achievements here have been mercilessly overshadowed by the hype surrounding them, this is truly an astonishing amount of work to be done in such a short time, and Tesla’s robotics team should be proud of what they’ve accomplished. And while there will inevitably be comparisons to other companies with humanoid robots, it’s critical to remember the context here: Tesla has made this happen in something like eight months. It’s nuts. I can see the appeal of Tesla for someone who wants to start a robotics career, since you’d get to work on a rapidly evolving hardware platform backed by what I can only assume are virtually unlimited resources. Maybe just, like, get your robot to reliably and affordably do A Single Useful Thing, first? Three versions of the Optimus design: Concept, Development Platform, and Latest Generation. Tesla Musk takes a break after this, and we get some actual specific information from a series of Tesla robotics team members about the latest generation Optimus. We’ll come back to the hands, but that battery really stands out for being able to power the robot for an entire day(ish). Again, we have to point out that until Tesla actually demonstrates this, it’s not all that meaningful, but Tesla does know a heck of a lot about power systems and batteries, and I’m guessing that the company will be able to deliver on this. Tesla is using simulations to design the robot’s structure so that it can suffer minimal damage after a fall.Tesla I appreciate that Tesla is thinking very early about how to structure its robot to be able to fall down safely and get up again with only superficial damage—although the company doesn’t seem to be taking advantage of any kind of protective motion for fall mitigation, which is an active area of research elsewhere. And what is not mentioned in this context is the safety of others. I’m glad the robot won’t get damaged all that much when it falls, but can Tesla say the same for whoever might be standing next to it? Optimus will use six different actuators: three rotary and three linear units.Tesla Tesla’s custom actuators seem very reasonable. Not special, particularly, but Tesla has to make its own actuators if it needs a lot of them, which it supposedly will. I’d expect these to be totally decent considering the level of mechanical expertise Tesla has, but as far as I can tell nothing here is crazy small or cheap or efficient or powerful or anything like that. And it’s very hard to tell from these slides and from the presentation just how well the actuators are going to work, especially for dynamic motions. The robot’s software has a lot of catching up to do first. Optimus will feature a bioinspired hand design with cable-driven actuators.Tesla Each hand has six cable-driven actuators for fingers and thumb (with springs to provide the opening force), which Tesla chose for simplicity and to minimize part count. This is perhaps a little surprising, since cable drives typically aren’t as durable and can be more finicky to keep calibrated. The five-finger hand is necessary, Tesla says, because Optimus will be working with human tools in human environments. And that’s certainly one perspective, although it’s a big trade-off in complexity. The hand is designed to carry a 9-kilogram bag. Tesla is using software components developed for its vehicles and porting them to the robot’s environment.Tesla Software! The following quote comes from Milan Kovac, who’s on the autonomy team. I still fundamentally disagree with the implied “humanoid robots are just cars with legs” thing, but it’s impressive that they were able to port much at all—I was highly skeptical of that last year, but I’m more optimistic now, and being able to generalize between platforms (on some level) could be huge for both Tesla and for autonomous systems more generally. I’d like more details on what was easy, and what was not. Tesla showed how sensing used in its vehicles can help the Optimus robot navigate.Tesla What we’re seeing above, though, is one of the reasons I was skeptical. That occupancy grid (where the robot’s sensors are detecting potential obstacles) on the bottom is very car-ish, in that the priority is to make absolutely sure that the robot stays very far away from anything it could conceivably run into. By itself, this won’t transfer well to a humanoid robot that needs to directly interact with objects to do useful tasks. I’m sure there are lots of ways to adapt the Tesla car’s obstacle-avoidance system, but that’s the question: How hard is that transfer, and is it better than using a solution developed specifically for mobile manipulators? Tesla explained the challenges of dynamic walking in humanoid robots, and its approach to motion planning.Tesla The next part of the presentation focused on some motion planning and state-estimation stuff that was very basic, as far as I could make out. There’s nothing wrong with the basics, but it’s slightly weird that Tesla spent so much time on this. I guess it’s important context for most of the people watching, but the team sort of talked about it like they’d discovered how to do all of this stuff themselves, which I hope they didn’t, because again, very, very basic stuff that other humanoid robots have been doing for a very long time. Tesla adopted a traditional approach to motion control, based on a model of the robot and state estimation.Tesla One more quote from Milan Kovac: Ignoring that last bit about changing the entire economy, and possibly also ignoring the time frame because “next few months or years” is not particularly meaningful, the push to make Tesla Bot useful is another substantial advantage that Tesla has. Unlike most companies working on humanoid robots, Tesla is potentially its own biggest customer, at least initially, and having these in-house practical tasks for the robot to train on could really help accelerate development. “Optimus is designed to be an extremely capable robot, but made in very high volume, ultimately millions of units. And it is expected to cost much less than a car—much less than [US] $20,000 would be my guess.” —Elon Musk However, I’m having trouble imagining what Tesla Bot would actually do in a factory that would be uniquely useful and not done better by a nonhumanoid robot. I’m very interested to see what Tesla comes up with here, and whether the company can make it happen in months (or years). I suspect it’s going to be much more difficult than it’s suggesting it will be, especially as it gets to 90 percent of where it wants to be and start trying to crack that last 10 percent that’s necessary for something reliable. This was the end of the formal presentation about Optimus, but there was a Q&A at the end with Musk where he gave some additional information about the robot side of things. He also gave some additional noninformation, which is worth including just in case you haven’t yet had enough eye rolling for one day. Musk expects Optimus to cost less than a car, “much less than [US] $20,00 dollars would be my guess,” he said.Tesla This is a variation on the minimum-viable-product idea, although it seems to be more from the perspective of making a generalist robot, which is somewhat at odds with something minimally viable. It’s good that Musk views the hardware as something in flux, and that he’s framed everything within a plan for volume production. This isn’t the only way to do it—you can first build a useful robot and then figure out how to make it cheaper, but Tesla’s approach could get the company to production faster. If, that is, it is able to confirm that the robot is in fact useful. I’m still not convinced that it will be, at least not on a time scale that will satisfy Musk. While Musk seems to be mostly joking here, the whole “it’s going to be your friend” is really not a good perspective to bring to a robot like this, in my opinion. Or probably any robot, at all honestly. Less robotlike and more friendly than a human pretending to be a robot trying to be a human? Good luck with that. I think it’s more likely that in the short-to-medium term, Tesla will struggle to find situations where Optimus is uniquely useful in an efficient and cost-effective way. Uh. Maybe as a research platform? Despite my skepticism on the time frame here, five years is a long time for any robot, and 10 years is basically forever. I’m also really interested to see these things happen, although Musk’s definitions of “incredible” and “mind-blowing” may be much different than mine. But we’ll see, won’t we? Tesla’s AI Day serves as a recruitment event for the company. “There’s still a lot of work to be done to refine Optimus and improve it, and that’s really why we’re holding this event—to convince some of the most talented people in the world to join Tesla,” Musk said.Tesla I think Elon Musk now has a somewhat better idea of what he’s doing with Tesla Bot. The excessive hype is still there, but now that the company has actually built something, Musk seems to have a much better idea of how hard it actually is. Things are only going to get more difficult from here. Most of what we saw in the presentation was hardware. And hardware is important and a necessary first step, but software is arguably a much more significant challenge when it comes to making robotics useful in the real world. Understanding and interacting with the environment, reasoning and decision-making, the ability to learn and be taught new tasks—these are all necessary pieces of the puzzle of a useful robot that Tesla is trying to put together, but they’re all also extremely difficult, cutting-edge problems, despite the enormous amount of work that the research community has put into them. And so far, we (still) have very little indication that Tesla is going to be any better at tackling this stuff than anyone else. There doesn’t appear to be anything all that special or exciting from Tesla that provides any unique foundation for Musk’s vision in a way that’s likely to allow the company to outpace other companies working on similar things. I’ll reiterate what I said a year ago: The hard part is not building a robot, it’s getting that robot to do useful stuff. “I think Optimus is going to be incredible in five years. In 10 years, mind-blowing. I’m really interested to see that happen, and I hope you are too.” —Elon Musk I could, of course, be wrong. Tesla likely has more resources to throw at this problem than almost anyone else. Maybe the automotive software will translate much better and faster than I think it will. There could be a whole bunch of simple but valuable use cases in Tesla’s own factories that will provide critical stepping-stones for Optimus. Tesla’s battery and manufacturing expertise could have an outsized influence on the affordability, reliability, and success of the robot. The company’s basic approach to planning and control could become a reliable foundation that will help the system mature faster. And the team is obviously very talented and willing to work extremely hard, which could be the difference between modest success and slow failure. Honestly, I would love to be wrong. We’re just starting to see some realistic possibilities with commercial legged and humanoid robots. There are lots of problems to solve, but also lots of potential, and Tesla finding success would be a huge confidence boost in commercial humanoids broadly. We can also hope that all of the resources that Tesla is putting toward Optimus will either directly or indirectly assist other folks working on humanoid robots, if Tesla is willing to share some of what it learns. But as of today, this is all just hoping, and it’s on Tesla to actually make it happen. Evan Ackerman is a senior editor at IEEE Spectrum. Since 2007, he has written over 6,000 articles on robotics and technology. He has a degree in Martian geology and is excellent at playing bagpipes. The prosthetics industry is too focused on high-tech limbs that are complicated, costly, and often impractical The author, Britt Young, holding her Ottobock bebionic bionic arm. In Jules Verne’s 1865 novel From the Earth to the Moon, members of the fictitious Baltimore Gun Club, all disabled Civil War veterans, restlessly search for a new enemy to conquer. They had spent the war innovating new, deadlier weaponry. By the war’s end, with “not quite one arm between four persons, and exactly two legs between six,” these self-taught amputee-weaponsmiths decide to repurpose their skills toward a new projectile: a rocket ship. The story of the Baltimore Gun Club propelling themselves to the moon is about the extraordinary masculine power of the veteran, who doesn’t simply “overcome” his disability; he derives power and ambition from it. Their “crutches, wooden legs, artificial arms, steel hooks, caoutchouc [rubber] jaws, silver craniums [and] platinum noses” don’t play leading roles in their personalities—they are merely tools on their bodies. These piecemeal men are unlikely crusaders of invention with an even more unlikely mission. And yet who better to design the next great leap in technology than men remade by technology themselves?",0.0
"
        Pong-in-a-Dish
    ",https://spectrum.ieee.org/pong-in-a-dish,2022-10-22,,"These neurons mimic a brain playing the classic video game A visual representation of the simulated Pong environment shows electrodes encoded for sensory input (bottom) and electrodes encoded for output (paddle up/down). Ever hear of the Turk—the 19th-century mechanism topped by a turbaned head that played chess against all comers? In fact, hidden inside was a diminutive chessmaster, one you might imagine deadpanning, “Eh, It’s a living.” Then there’s its namesake, the Mechanical Turk—a 21st-century service offered by Amazon to mark up images on the Web with the help of crowdsourced freelancers. They, too, might intone, glassy-eyed, “It’s a living.” Now we have a kind of Biological Turk. A mass of neurons act as a computer that mimics a human being playing the classic computer game Pong. The neurons, some taken from mouse embryos, others grown from human precursor cells, spread out into a one-layer, 800,000-cell mesh called a biological neural network, which lives in a giant petri dish called the DishBrain. There it interfaces with arrays of electrodes that form an interface to silicon hardware. Software mounted on that hardware provides stimulation and feedback, and the minibrain learns how to control a paddle on a simulated ping-pong table. The work was described recently in the journal Neuron by Brett Kagan, the chief scientific officer of Cortical Labs, a startup in Melbourne, Australia, and nine colleagues at that company. The authors talk hopefully about the emergence of sentience, a notion that other brain-in-a-dish researchers have also recently floated. But they seem to stand on solid ground when they say their method will help to advance brain science, on the one hand, and computer science, on the other. A bio-neuro-network might model the effects of drugs on the brain in ways that single-cell neurons can’t. Also, neurons may show themselves to be more than just protoplasmic logic switches but more like entire computers. The question before us, though, is how does the thing play Pong? Pong-in-a-Dish youtu.be First, the electronic scaffolding hits the minibrain with electrical signals that represent the position and movement of the virtual ball. It’s rather like the action potential that a firing neuron would use to convey, say, a sensory signal from the eye to the brain. Because the electrodes are placed at different points in the cell network, the system physically represents the different possible locations. Further information comes from the frequency of the signals, which varies with the distance of the ball to the virtual paddle. The network responds to these stimuli like a motor neuron, sending out a signal that moves the virtual paddle. If the resulting movement causes the ball to bounce, the neural network gets a “reward.” Failure results in a signal that has the opposite effect. “Reward” is put in sneer quotes because these cells don’t have feelings. They can’t experience the joy of victory, the agony of defeat. There’s no dopamine, no salted popcorn. Instead, the researchers say, the network is working to minimize unpredictability. In this view, the so-called reward is a predictable signal, the anti-reward is an unpredictable one. Kagan tells IEEE Spectrum that the system as a whole then reorganizes to become better at playing the game. The most marked improvement came in the first five minutes of play. It seems amazing that a mere 800,000 neurons can model the world, even a simplified world. But, Kagan says, such feats are seen in nature. ""Flies have even fewer neurons but must be able to do some modeling—although perhaps not in a way a human may—to navigate a complex and changing 3D world,"" he says. As he and his colleagues point out in their report the ability of neurons to adapt to external stimuli is well established in vivo; it forms the basis for all animal learning. But theirs, they say, is the first in vitro demonstration involving a goal-directed behavior. The current version of Pong is forgiving. The paddle is broad, the volley slow, the ball unspinning. Even a neophyte would crush DishBrain. Then again, the same was true of all of AI’s early assays in game playing. The early chess machines would sometimes senselessly give up first a pawn, then a piece, then the queen—all because they were attempting to put off a disagreeable action to a point beyond the built-in planning horizon. Poker-playing programs got good pretty fast, but the early ones sometimes played too well—that is, too cautiously—against weak human opponents, which reduced their winnings. Car navigation programs would send you into a vacant lot. You might think that just getting a machine to play a decent game is the hard part, and that further improving it to perfection ought to be a snap. Edgar Allan Poe made that judgement when he called the Turk a fraud because it occasionally erred. His conclusion was correct but his reasoning was faulty. It’s not easy turning a barely there machine into a world champion at chess or Go. And yet it has been done. Philip E. Ross is a senior editor at IEEE Spectrum. His interests include transportation, energy storage, AI, and the economic aspects of technology. He has a master's degree in international affairs from Columbia University and another, in journalism, from the University of Michigan. It’s hard to learn, but your code will produce fewer nasty surprises You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development. So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.",0.0
"
        AI Language Models Are Struggling to “Get” Math
    ",https://spectrum.ieee.org/large-language-models-math,2022-10-12,,"Should this be telling us something? If computers are good at anything, they are good at math. So it may come as a surprise that after much struggling, top machine-learning researchers have recently made breakthroughs in teaching computers math. Over the past year, researchers from the University of California, Berkeley, OpenAI, and Google have made progress in teaching basic math concepts to natural language generation models—algorithms such as GPT-2/3 and GPT-Neo. However, until recently, language models regularly failed to solve even simple word problems, such as “Alice has five more balls than Bob, who has two balls after he gives four to Charlie. How many balls does Alice have?” “When we say computers are very good at math, they’re very good at things that are quite specific,” says Guy Gur-Ari, a machine-learning expert at Google. Computers are good at arithmetic—plugging numbers in and calculating is child’s play. But outside of formal structures, computers struggle. “I think there’s this notion that humans doing math have some rigid reasoning system—that there’s a sharp distinction between knowing something and not knowing something.”—Ethan Dyer, Google Solving word problems, or “quantitative reasoning,” is deceptively tricky because it requires a robustness and rigor that many other problems don’t. If any step during the process goes wrong, the answer will be wrong. “When multiplying really large numbers together…they’ll forget to carry somewhere and be off by one,” says Vineet Kosaraju, a machine-learning expert at OpenAI. Other mistakes made by language models are less human, such as misinterpreting 10 as a 1 and a 0, not 10. “We work on math because we find it independently very interesting,” says Karl Cobbe, a machine-learning expert at OpenAI. But as Gur-Ari puts it, if it’s good at math, “it’s probably also good at solving many other useful problems.” As machine-learning models are trained on larger samples of data, they tend to grow more robust and make fewer mistakes. But scaling up seems to go only so far with quantitative reasoning; researchers realized that the mistakes language models make seemed to require a more targeted approach. Last year, two different teams of researchers, at UC Berkeley and OpenAI, released two data sets, MATH and GSM8K, respectively, which contain thousands of math problems across geometry, algebra, precalculus, and more. “We basically wanted to see if it was a problem with data sets,” says Steven Basart, a researcher at the Center for AI Safety who worked on MATH. Language models were known to be bad at word problems—but how bad were they, and could they be fixed by introducing better formatted, bigger data sets? The MATH group found just how challenging quantitative reasoning is for top-of-the-line language models, which scored less than 7 percent. (A human grad student scored 40 percent, while a math olympiad champ scored 90 percent.) Models attacking GSM8K problems, which had easier grade-school-level problems, reached about 20 percent accuracy. The OpenAI researchers used two main techniques: fine-tuning and verification. In fine-tuning, researchers take a pretrained language model that includes irrelevant information (Wikipedia articles on zambonis, the dictionary entry for “gusto,” and the like) and then show the model, Clockwork Orange–style, only the relevant information (math problems). Verification, on the other hand, is more of a review session. “The model gets to see a lot of examples of its own mistakes, which is really valuable,” Cobbe says. At the time, OpenAI predicted a model would need to be trained on 100 times more data to reach 80 percent accuracy on GSM8K. But in June, Google’s Minerva announced 78 percent accuracy with minimal scaling upwards. “It’s ahead of any of the trends that we were expecting,” Cobbe says. Basart agrees. “That’s shocking. I thought it would take longer,” he says. Minerva uses Google’s own language model, Pathways Language Model (PaLM), which is fine-tuned on scientific papers from the arXiv online preprint server and other sources with formatted math. Two other strategies helped Minerva. In “chain-of-thought prompting,” Minerva was required to break down larger problems into more palatable chunks. The model also used majority voting—instead of being asked for one answer, it was asked to solve the problem 100 times. Of those answers, Minerva picked the most common answer. The gains from these new strategies were enormous. Minerva shot up to 50 percent accuracy on MATH and nearly 80 percent accuracy on GSM8K, as well as the MMLU, a more general set of STEM questions that included chemistry and biology. When Minerva was asked to redo a random sample of slightly tweaked questions, it performed just as well, suggesting that its capabilities were not from mere memorization. What Minerva knows—or doesn’t know—about math is fuzzier. Unlike proof assistants, which come with built-in structure, Minerva and other language models have no formal structure. They can have strange, messy reasoning and still arrive at the right answer. As numbers grow larger, the language models’ accuracy falters, something that would never happen on a TI-84. “Just how smart is it—or isn’t it?” asks Cobbe. Though models like Minerva might arrive at the same answer as a human, the actual process they’re following could be wildly different. On the other hand, chain-of-thought prompting is familiar to any human student who’s been asked to “show your work.” “I think there’s this notion that humans doing math have some rigid reasoning system—that there’s a sharp distinction between knowing something and not knowing something,” says Ethan Dyer, a machine-learning expert at Google. But humans give inconsistent answers, make errors, and fail to apply core concepts, too. The borders, at this frontier of machine learning, are blurred. Update 14 Oct. 2022: A previous version of this story extraneously alluded to the DALL-E/DALL-E 2 art-generation AI in the context of large language generation models being taught to handle math word problems. Of course, neither DALL-E nor DALL-E 2 is a large language generation model. (And it was not studied in the math word problem research.) So to avoid confusion, references to it were cut. Dan Garisto is a freelance science journalist who covers physics and other physical sciences. His work has appeared in Scientific American, Physics, Symmetry, Undark, and other outlets. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",0.0
"
        Ultrafast Racetrack Memory Enters the Third Dimension
    ",https://spectrum.ieee.org/racetrack-memory,2022-10-09,,"Arrays of arching nanowires could lead to ultrafast, ultrahigh-density solid-state nonvolatile memory An artist's depiction of a 3D racetrack memory device. Racetrack memory could hold vast amounts of data that can be accessed extraordinarily quickly. Now, in a new study, scientists reveal 3D racetrack memory devices that may greatly increase the potential of this technology. Racetrack memory encodes bits of data in the form of magnetic domain walls. These walls divide a material into domains, inside which magnetic poles all point in the same direction. Electric pulses can push these domain walls back and forth within nanowires, making them run like race cars down a track. Magnetic sensors and other electronics can then read and write data on any point on this racetrack. Racetrack memory potentially “has unrivaled density compared to other memory technologies,” says Stuart Parkin, director of the Max Planck Institute of Microstructure Physics in Halle, Germany, and who developed the concept while at IBM. Racetrack memory is solid-state like flash memory, lacking unwieldy moving parts, and is nonvolatile, storing data even after the power is removed. In addition, racetrack memory’s electric pulses can move domain walls at speeds of up to kilometers per second. This suggests devices using this kind of memory can perform exceptionally quickly. “Memories that operate at speeds of less than 1 nanosecond per write operation are possible,” Parkin says. “This is as fast or faster than the fastest write speeds of any commercially available memory today.” Most research on racetrack memory to date has focused on 2D devices. However, Parkin had long suggested it could be a 3D technology, with arrays of vertical racetracks much like a city filled with skyscrapers. An extra dimension added to racetrack memory would greatly increase the amount of data it could hold on a given footprint. But assembling such complex 3D structures, given the standard fabrication techniques used by the microelectronics industry, has proven difficult. Now Parkin and his colleagues have developed a 3D racetrack memory device using a new method that lets them drape a 2D racetrack onto a 3D surface. They detailed their findings online 22 September in the journal Nature Nanotechnology. The scientists fabricated a complex ultrathin film made of layers of tantalum nitride, cobalt, nickel, platinum, ruthenium, and magnesium oxide onto a water-soluble layer of strontium aluminate. They then immersed the film in water, releasing a freestanding magnetic film. The researchers next transferred this magnetic film onto a sapphire surface engineered with speed-bump-like 3D ridges, a bit like one might drape a drop cloth over furniture while painting a room. Standard photolithography techniques then let them create racetracks made of wires—each 3 micrometers wide—with arching segments rising up to 900 nanometers high, following the shape of the sapphire surface. “The 3D racetrack structure prepared and handled in this way survived intact, and behaves almost identically to the originally deposited structure before it was detached,” Parkin says. In experiments, domain walls in the 3D racetracks traveled at speeds of up to 600 meters per second. “We believe that the speed can be increased by more than 10 times in related structures using novel materials,” Parkin says. The scientists are exploring “many different ways of building 3D racetrack memories,” Parkin adds. “There are many interesting challenges that are still to be explored, which makes research into racetrack memory very exciting.” Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. It’s hard to learn, but your code will produce fewer nasty surprises You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development. So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.",0.0
"
        Machine Learning Shaking Up Hard Sciences, Too
    ",https://spectrum.ieee.org/machine-learning-in-physics,2022-10-07,,"Heard of graph neural networks? Particle physicists have A view of the underground ALICE detector used to study heavy-ion physics at the Large Hadron Collider (LHC). Particle physicists have long been early adopters—if not inventors—of tech from email to the Internet. It’s not surprising, then, that as early as 1997, researchers were training computer models to tag particles in the messy jets created during collisions. Since then, these models have chugged along, growing steadily more competent—though not to everyone’s delight. “I felt very threatened by machine learning,” says Jesse Thaler, a theoretical particle physicist at the Massachusetts Institute of Technology. Initially, he says he felt like it jeopardized his human expertise classifying particle jets. But Thaler has since come to embrace it, applying machine learning to a variety of problems across particle physics. “Machine learning is a collaborator,” he says. Over the past decade, in tandem with the broader deep-learning revolution, particle physicists have trained algorithms to solve previously intractable problems and tackle completely new challenges. Even with an efficient trigger, the LHC must store 600 petabytes over the next few years of data collection. So researchers are investigating strategies to compress the data. For starters, particle-physics data is very different from the typical data used in machine learning. Though convolutional neural networks (CNNs) have proven extremely effective at classifying images of everyday objects from trees to cats to food, they’re less suited for particle collisions. The problem, according to Javier Duarte, a particle physicist at the University of California, San Diego, is that collision data such as that from the Large Hadron Collider, isn’t naturally an image. Flashy depictions of collisions at the LHC can misleadingly fill up the entire detector. In reality, only a few out of millions of inputs are registering a signal, like a white screen with a few black pixels. This sparsely populated data makes for a poor image, but it can work well in a different, newer framework—graph neural networks (GNNs). Other challenges from particle physics require innovation. “We’re not just importing hammers to hit our nails,” says Daniel Whiteson, a particle physicist at the University of California, Irvine. “We have new weird kinds of nails that require the invention of new hammers.” One weird nail is the sheer amount of data produced at the LHC—about one petabyte per second. Of this enormous amount, only a small bit of high-quality data is saved. To create a better trigger system, which saves as much good data as possible while getting rid of low-quality data, researchers want to train a sharp-eyed algorithm to sort better than one that’s hard coded. But to be effective, such an algorithm would need to be incredibly speedy, executing in microseconds, Duarte says. To address these problems, particle physicists are pushing the limits of machine techniques like pruning and quantization, to make their algorithms even faster. Even with an efficient trigger, the LHC must store 600 petabytes over the next few years of data collection (equivalent to about 660,000 movies at 4K resolution or the data equivalent of 30 Libraries of Congresses), so researchers are investigating strategies to compress the data. “We’d like to have a machine learn to think more like a physicist, [but] we also just need to learn how to think a little bit more like a machine.”—Jesse Thaler, MIT Machine learning is also allowing particle physicists to think differently about the data they use. Instead of focusing on a single event—say, a Higgs boson decaying to two photons—they are learning to consider the dozens of other events that happen during a collision. Although there’s no causal relationship between any two events, researchers like Thaler are now embracing a more holistic view of the data, not just the piecemeal point of view that comes from analyzing events interaction by interaction. More dramatically, machine learning has also forced physicists to reassess basic concepts. “I was imprecise in my own thinking about what a symmetry was,” Thaler says. “Forcing myself to teach a computer what a symmetry was, helped me understand what a symmetry actually is.” Symmetries require a reference frame—in other words, is the image of a distorted sphere in a mirror actually symmetrical? There’s no way of knowing without knowing if the mirror itself is distorted. These are still early days for machine learning in particle physics, and researchers are effectively treating the technique like a proverbial kitchen sink. “It may not be the right fit for every single problem in particle physics,” admits Duarte. As some particle physicists delve deeper into machine learning, an uncomfortable question rears its head: Are they doing physics, or computer science? Stigma against coding—sometimes not considered to be “real physics”—already exists; similar concerns swirl around machine learning. One worry is that machine learning will obscure the physics, turning analysis into a black box of automated processes opaque to human understanding. “Our goal is not to plug in the machine, the experiment to the network and have it publish our papers so we’re out of the loop,” Whiteson says. He and colleagues are working to have the algorithms provide feedback in language humans can understand—but algorithms may not be the only ones with responsibilities to communicate. “On the one hand, we’d like to have a machine learn to think more like a physicist, [but] we also just need to learn how to think a little bit more like a machine,” Thaler says. “We need to learn to speak each other’s language.” Dan Garisto is a freelance science journalist who covers physics and other physical sciences. His work has appeared in Scientific American, Physics, Symmetry, Undark, and other outlets. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.",0.0
"
        AI’s Grandmaster Status Overshadows Chess Scandal
    ",https://spectrum.ieee.org/magnus-carlsen-chess-scandal-ai,2022-10-05,,"Magnus Carlsen–Hans Niemann controversy underscores humans’ perpetual underdog role Magnus Carlsen [left] and Hans Niemann compete during the 2022 Sinquefield Cup at the Saint Louis Chess Club. Last week Magnus Carlsen, the world chess champion, directly accused Hans Niemann, a U.S. grandmaster, of cheating during their game at the Sinquefield Cup, in St. Louis, Mo. He thus made plain an accusation he had been hinting at for weeks. Carlsen has so far provided no evidence to back up his charge, nor has he specified how the cheating took place. Everyone agrees, however, that if there was cheating, then it must have involved computers, because nothing else could dismay Carlsen, whose rating of 2856 is higher than that of any other player. And everyone seems to have chosen sides. Those who back Carlsen point to Niemann’s own admission that he used computers to cheat in online play at least twice—once at age 14 and again at 16; Niemann is now 19. Others note that his performance has risen very rapidly in the past two years. Still others raise an eyebrow at the large number of games he has played in recent years that get a score of nearly perfect from computer analysis. And behind it all are statements from leading players that they are convinced that cheating happens all the time nowadays, though hardly anybody ever gets caught. Computers loom so large because they now play chess like gods. What makes the scandal so big is not merely the level of the players. In 1961 the great Bobby Fischer wrote an article for Sports Illustrated titled “The Russians Have Fixed World Chess.” He alleged that Soviet chess players arranged draws to ensure that one of them would win a tournament. Nor is the scandal notable for flagrancy. In 1967 Milan Matulović, a Yugoslavian grandmaster, shockingly took back a move he had just played and only then said “J’adoube,” the French phrase uttered when a player merely adjusts the position of a chessman. Players thereafter called him “J’adoubavić.” No, what makes today’s accusations resonate is the pervasive role of chess computers. They give children around the world sparring partners that earlier generations couldn’t have dreamed of facing, even if they’d lived next to the Moscow Central Chess Club. No wonder prodigies of the game have gotten younger and younger. And computers do so well in helping the home preparation of the opening, the early moves of a game, that players, including Carlsen, will sometimes deliberately play a second-best move just to force the opponents out of “book.” Finally, computer analysis offered during Internet broadcasts of ongoing tournaments will look 12 moves ahead within a second or two. They show the amateurs in the audience much that the grandmasters miss, creating the illusion that the amateurs actually understand what’s going on. Of course, any viewer could give illicit help to a player if provided a means of communication. Several things are at stake. There is the prize money, which runs in the hundreds of thousands of dollars for the circuit of which the Sinquefeld Cup tournament is a part. There are the invitations to future events, which are often contingent on doing well in qualifying events. Then there are the rating points. Carlsen cares deeply about this metric: Although he recently declined to contest his World Championship title in 2023, he insists that he will continue to play in the hope of raising his rating to an unprecedented 2900. The cheating to which Niemann does admit—in his younger years, during online play—was itself detected with the aid of computers of Chess.com, the online playing forum in question. Recently, however, the Wall Street Journal reported that an internal investigation by Chess.com has found that Niemann in fact cheated in more than 100 online games, most recently when he was 17. The company did not impugn the grandmaster’s over-the-board play. A key hint can be encoded in just a few bits of data, which means it might be transmitted, perhaps via a buzzer in the player’s shoe, on his body—or inside it. Online play is fast and loose, and its computerized basis may provide clues that a cheat-detection algorithm can catch. But over-the-board offers less data. Often there are only one or two key points in a game at which cheating might occur; a little hint, offered at such a point, is enough to make the difference to a grandmaster. Even a duffer, when showed a chess problem, may be truly stumped. But told that it is “mate in three moves,” the duffer may see the light. Just a phrase—“the rook,” say, or “double attack”—may also make the idea apparent. A key hint can be encoded in just a few bits of data, which means it might be transmitted, perhaps via a buzzer in the player’s shoe, on his body—or inside it. Do not laugh, but innuendos have been made concerning the possible use of a buzzing sex toy. As a joke, Niemann declared that he was willing to play naked. A camsite called Stripchat promptly offered him [US] $1 million to do so. Computers loom so large because they now play chess like gods. The best free program, Stockfish 14, is rated at 3534—678 points ahead of Carlsen. That’s enough of a gap to predict a winning expectancy of 99 percent. In the early days, when chess programs were a lab project for AI, they played like idiots. Then the programmers began to enter their creations in competitions, and the programs got good. I learned that the hard way. In late 1974, at a student tournament held in Evanston, Ill., I was paired against Northwestern University’s Chess 4.0 program, played the Sicilian Defense, blundered a knight for two pawns, mentally kicked myself, and hastily resigned. David Slate, the programmer, waited patiently as I completed the ritual of resignation, which involves signing the score sheet and handing it to the tournament director—in this case, him. Only then did he tell me that if I’d just played on, I would have gotten a draw. The Strongest Computer Chess Engines Over Time www.youtube.com “It can’t play endgames,” Slate said. I kicked myself again. Back then I was rated somewhere in the 1600s, about average for an amateur. Still, I was the highest-rated player any machine had yet beaten in a tournament game. It’s my claim to fame. Chess 4.0 went on to beat another guy higher rated than me, somewhat soothing my wounded pride. It took years for the Northwestern program to reach 2000. Other university programs then took the lead, until at last a machine originating at Carnegie Mellon and redomiciled at IBM reached 2600, about grandmaster strength. That was strong enough to beat my old, 1600-rated self 99.9 percent of the time. In 1997 an even stronger version of the IBM machine, dubbed Deep Blue, beat Gary Kasparov, the reigning world champion. Deep Blue filled a room. Today, a smartphone can crush any human player. Philip E. Ross is a senior editor at IEEE Spectrum. His interests include transportation, energy storage, AI, and the economic aspects of technology. He has a master's degree in international affairs from Columbia University and another, in journalism, from the University of Michigan. It’s hard to learn, but your code will produce fewer nasty surprises You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development. So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.",0.0
Machine learning could vastly speed up the search for new metals,https://www.technologyreview.com/2022/10/25/1062104/machine-learning-new-metals/,2022-10-25,"<p>It’s a development that could be useful for applications from outer space to the deep sea.</p>
","Machine learning could help develop new types of metals with useful properties, such as resistance to extreme temperatures and rust, according to new research. This could be useful in a range of sectors—for example, metals that perform well at lower temperatures could improve spacecraft, while metals that resist corrosion could be used for boats and submarines. Currently, scientists typically run experiments in the lab to look for ways to combine metals to create new ones. Usually they start off with one well-known element, like iron, which is cheap and malleable, and add one or two others to see the effect on the original material. It’s a laborious process of trial and error that inevitably yields more duds than useful results. But the new paper, published in Science earlier this month, suggests that with AI, researchers can far more precisely predict which combinations of metals will show promise. Researchers from the Max Planck Institute managed to identify 17 promising new metals using this method. The team was hunting for metals with a low level of “invar,” which refers to how much materials expand or contract when exposed to high or low temperatures. Metals with low invar don’t change size under extreme temperatures. They are commonly used in industries where that property is useful—for example, the transportation and storage of natural gas, explains Ziyuan Rao, a materials science researcher at the Max Planck Institute and a coauthor of the paper. The machine-learning tool could help researchers discover entirely new proteins not yet known to science. The team managed to find these new metals through a combination of AI and lab experiments. First, they had to overcome a significant challenge: a lack of existing data they could use to train the machine-learning models. They trained the models on the data they had—several hundred data points describing the properties of existing metal alloys. The AI system used that data to make predictions for new metals that would exhibit low invar. The researchers then created those metals in a lab, measured the results, and fed those results back into the machine-learning model. The process continued that way—the model suggesting metal combinations, the researchers testing them and feeding the data back in—until the 17 promising new metals emerged. The findings could help pave the way for greater use of machine learning in materials science, a field that still relies heavily on laboratory experimentation. Also, the technique of using machine learning to make predictions that are then checked in the lab could be adapted for discovery in other fields, such as chemistry and physics, say experts in materials science. To understand why it’s a significant development, it’s worth looking at the traditional way new compounds are usually created, says Michael Titus, an assistant professor of materials engineering at Purdue University, who was not involved in the research. The process of tinkering in the lab is painstaking and inefficient. “It’s truly like finding a needle in a haystack to find materials that exhibit a special property,” Titus says. He often tells his new graduate students that there are easily a million possible new materials waiting to be discovered. Machine learning could help researchers decide which paths to pursue. Easo George, a professor of materials science and engineering at the University of Tennessee, who wasn’t involved in this research, was surprised by what the team was able to accomplish using the new technique. “It’s very impressive,” he says. In the future, the team wants to use machine learning to help discover new alloys with more than just one desirable property. George agrees that computational methods will be crucial to the future of materials science. “The machine-learning approach is probably going to be dominant, because people have tried to scan very large spaces experimentally, but that is very time consuming and expensive,” he says. “The test is, are you finding something useful?”",0.0
Do AI systems need to come with safety warnings?,https://www.technologyreview.com/2022/10/24/1062071/do-ai-systems-need-to-come-with-safety-warnings/,2022-10-24,"<p>Plus: Technology that lets us “speak” to our dead relatives has arrived.</p>
","To receive The Algorithm in your inbox every Monday, sign up here. Welcome to The Algorithm! Considering how powerful AI systems are, and the roles they increasingly play in helping to make high-stakes decisions about our lives, homes, and societies, they receive surprisingly little formal scrutiny. That’s starting to change, thanks to the blossoming field of AI audits. When they work well, these audits allow us to reliably check how well a system is working and figure out how to mitigate any possible bias or harm. Famously, a 2018 audit of commercial facial recognition systems by AI researchers Joy Buolamwini and Timnit Gebru found that the system didn’t recognize darker-skinned people as well as white people. For dark-skinned women, the error rate was up to 34%. As AI researcher Abeba Birhane points out in a new essay in Nature, the audit “instigated a body of critical work that has exposed the bias, discrimination, and oppressive nature of facial-analysis algorithms.” The hope is that by doing these sorts of audits on different AI systems, we will be better able to root out problems and have a broader conversation about how AI systems are affecting our lives. Regulators are catching up, and that is partly driving the demand for audits. A new law in New York City will start requiring all AI-powered hiring tools to be audited for bias from January 2024. In the European Union, big tech companies will have to conduct annual audits of their AI systems from 2024, and the upcoming AI Act will require audits of “high-risk” AI systems. It’s a great ambition, but there are some massive obstacles. There is no common understanding about what an AI audit should look like, and not enough people with the right skills to do them. The few audits that do happen today are mostly ad hoc and vary a lot in quality, Alex Engler, who studies AI governance at the Brookings Institution, told me. One example he gave is from AI hiring company HireVue, which implied in a press release that an external audit found its algorithms have no bias. It turns out that was nonsense—the audit had not actually examined the company’s models and was subject to a nondisclosure agreement, which meant there was no way to verify what it found. It was essentially nothing more than a PR stunt. One way the AI community is trying to address the lack of auditors is through bias bounty competitions, which work in a similar way to cybersecurity bug bounties—that is, they call on people to create tools to identify and mitigate algorithmic biases in AI models. One such competition was launched just last week, organized by a group of volunteers including Twitter’s ethical AI lead, Rumman Chowdhury. The team behind it hopes it’ll be the first of many. It’s a neat idea to create incentives for people to learn the skills needed to do audits—and also to start building standards for what audits should look like by showing which methods work best. You can read more about it here. The growth of these audits suggests that one day we might see cigarette-pack-style warnings that AI systems could harm your health and safety. Other sectors, such as chemicals and food, have regular audits to ensure that products are safe to use. Could something like this become the norm in AI? Anyone who owns and operates AI systems should be required to conduct regular audits, argue Buolamwini and coauthors in a paper that came out in June. They say that companies should be legally obliged to publish their AI audits, and that people should be notified when they have been subject to algorithmic decision making. Another way to make audits more effective is to track when AI causes harm in the real world, the researchers say. There are a couple of efforts to document AI harms, such as the AI Vulnerability Database and the AI Incidents Database, built by volunteer AI researchers and entrepreneurs. Tracking failures could help developers gain a better understanding of the pitfalls or unintentional failure cases associated with the models they are using, says Subho Majumdar of the software company Splunk, who is the founder of the AI Vulnerability Database and one of the organizers of the bias bounty competition. But whatever direction audits end up going in, Buolamwini and co-authors wrote, the people who are most affected by algorithmic harms—such as ethnic minorities and marginalized groups—should play a key part in the process. I agree with this, although it will be challenging to get regular people interested in something as nebulous as artificial intelligence audits. Perhaps low-barrier, fun competitions such as bias bounties are part of the solution. Technology that lets us “speak” to our dead relatives has arrived. Are we ready? Technology for “talking” to people who’ve died has been a mainstay of science fiction for decades. It’s an idea that’s been peddled by charlatans and spiritualists for centuries. But now it’s becoming a reality—and an increasingly accessible one, thanks to advances in AI and voice technology. MIT Technology Review’s news editor, Charlotte Jee, has written a thoughtful and haunting story about how this kind of technology might change the way we grieve and remember those we’ve lost. But, she explains, creating a virtual version of someone is an ethical minefield—especially if that person hasn’t been able to provide consent. Read more here. There is a lawsuit brewing against AI code generation initiative GitHub Copi­lotGitHub Copi­lot allows users to use an AI to automatically generate code. Critics have warned that this could lead to copyright issues and cause licensing information to ber lost. (Github Copilot Investigation) France has fined Clearview AIThe French data protection agency has fined the facial-recognition company €20 million ($19.7 million) for breaching the EU’s data protection regime, the GDPR. (TechCrunch) One company’s algorithm has been pushing rents up in the USTexas-based RealPage’s YieldStar software is supposed to help landlords get the highest possible price on their property. From the looks of it, it’s working exactly as intended, much to the detriment of renters. (ProPublica) Meta has developed a speech translation system for an unwritten language, HokkienMost AI translation systems focus on written languages. Meta’s new open-source speech-only translation system allows speakers of a mostly oral language, Hokkien, mostly spoken in the Chinese diaspora, to have conversations with English speakers. (Meta) Brutal tweet of the weekPeople are inserting pictures of themselves into CLIP interrogator to find out what an AI recommends the best prompts should be for a text-to-image AI. The results are brutal. (h/t to Brendan Dolan-Gavitt or “an orc smiling to the camera”) Thanks for making it this far! See you next week. Melissa",0.0
A bias bounty for AI will help to catch unfair algorithms faster,https://www.technologyreview.com/2022/10/20/1061977/ai-bias-bounty-help-catch-unfair-algorithms-faster/,2022-10-20,"<p>The hope is it will boost a blossoming sector that works to hold artificial-intelligence systems accountable.</p>
","AI systems are deployed all the time, but it can take months or even years until it becomes clear whether, and how, they’re biased. The stakes are often sky-high: unfair AI systems can cause innocent people to be arrested, and they can deny people housing, jobs, and basic services. Today a group of AI and machine-learning experts are launching a new bias bounty competition, which they hope will speed the process of uncovering these kinds of embedded prejudice. The competition, which takes inspiration from bug bounties in cybersecurity, calls on participants to create tools to identify and mitigate algorithmic biases in AI models. It’s being organized by a group of volunteers who work at companies like Twitter, software company Splunk, and deepfake detection startup Reality Defender. They’ve dubbed themselves the “Bias Buccaneers.” The first bias bounty competition is going to focus on biased image detection. It’s a common problem: in the past, for example, flawed image detection systems have misidentified Black people as gorillas. Competitors will be challenged to build a machine-learning model that labels each image with its skin tone, perceived gender, and age group, which will make it easier to measure and spot biases in datasets. They will be given access to a data set of around 15,000 images of synthetically generated human faces. Participants are ranked on how accurately their model tags images and how long the code takes to run, among other metrics. The competition closes on November 30. Microsoft and startup Robust Intelligence have committed prize money of $6,000 for the winner, $4,000 for the runner-up, and $2,000 for whoever comes third. Amazon has contributed $5,000 to the first set of entrants for computing power. The competition is an example of a budding industry that’s emerging in AI: auditing for algorithmic bias. Twitter launched the first AI bias bounty last year, and Stanford University just concluded its first AI audit challenge. Meanwhile, nonprofit Mozilla is creating tools for AI auditors. These audits are likely to become more and more commonplace. They’ve been hailed by regulators and AI ethics experts as a good way to hold AI systems accountable, and they are going to become a legal requirement in certain jurisdictions. AI audits may overlook certain types of bias, and they don’t necessarily verify that a hiring tool picks the best candidates for a job. The EU’s new content moderation law, the Digital Services Act, includes annual audit requirements for the data and algorithms used by large tech platforms, and the EU’s upcoming AI Act could also allow authorities to audit AI systems. The US National Institute of Standards and Technology also recommends AI audits as a gold standard. The idea is that these audits will act like the sorts of inspections we see in other high-risk sectors, such as chemical plants, says Alex Engler, who studies AI governance at the think tank the Brookings Institution. The trouble is, there aren’t enough independent contractors out there to meet the coming demand for algorithmic audits, and companies are reluctant to give them access to their systems, argue researcher Deborah Raji, who specializes in AI accountability, and her coauthors in a paper from last June. That’s what these competitions want to cultivate. The hope in the AI community is that they’ll lead more engineers, researchers, and experts to develop the skills and experience to carry out these audits. Much of the limited scrutiny in the world of AI so far comes either from academics or from tech companies themselves. The aim of competitions like this one is to create a new sector of experts who specialize in auditing AI. “We are trying to create a third space for people who are interested in this kind of work, who want to get started or who are experts who don’t work at tech companies,” says Rumman Chowdhury, director of Twitter’s team on ethics, transparency, and accountability in machine learning, the leader of the Bias Buccaneers. These people could include hackers and data scientists who want to learn a new skill, she says. The team behind the Bias Buccaneers’ bounty competition hopes it will be the first of many. Competitions like this not only create incentives for the machine-learning community to do audits but also advance a shared understanding of “how best to audit and what types of audits we should be investing in,” says Sara Hooker, who leads Cohere for AI, a nonprofit AI research lab. The effort is “fantastic and absolutely much needed,” says Abhishek Gupta, the founder of the Montreal AI Ethics Institute, who was a judge in Stanford’s AI audit challenge. “The more eyes that you have on a system, the more likely it is that we find places where there are flaws,” Gupta says.",0.0
Alex Hanna left Google to try to save AI’s future,https://www.technologyreview.com/2022/10/19/1061075/alex-hanna-google-ai-research/,2022-10-19,"<p>After her departure, she joined Timnit Gebru’s Distributed AI Research Institute, and work is well underway.</p>
","“I am quitting because I’m tired,” Alex Hanna wrote on February 2, her last day on Google’s Ethical AI team. She felt that the company, and the tech industry as a whole, did little to promote diversity or mitigate the harms its products had caused to marginalized people. “In a word, tech has a whiteness problem,” she wrote in her post on Medium. “Google is not just a tech organization. Google is a white tech organization.” Hanna did not take much of a break—she joined the Distributed AI Research Institute (DAIR) as the group’s second employee on February 3. It was a move that capped a dramatic period in Hanna’s professional life. In late 2020, her manager, Timnit Gebru, had been fired from her position as the co-lead of the Ethical AI team after she wrote a paper questioning the ethics of large language models (including Google’s). A few months later, Hanna’s next manager, Meg Mitchell, was also shown the door. DAIR, which was founded by Gebru in late 2021 and is funded by various philanthropies, aims to challenge the existing understanding of AI through a community-­focused, bottom-up approach to research. The group works remotely and includes teams in Berlin and South Africa. “We wanted to find a different way of AI, one that doesn’t have the same institutional constraints as corporate and much of academic research,” says Hanna, who is the group’s director of research. While these sorts of investigations are slower, she says, “it allows for research for community members—different kinds of knowledge that is respected and compensated, and used toward community work.” Less than a year in, DAIR is still sorting out its approach, Hanna says. But research is well underway. The institute has three full-time employees and five fellows—a mix of academics, activists, and practitioners who come in with their own research agendas but also aid in developing the institute’s programs. DAIR fellow Raesetje Sefala is using satellite imagery and computer vision technology to focus on neighborhood change in post-apartheid South Africa, for example. Her project is analyzing the impact of desegregation and mapping out low-income areas. Another DAIR fellow, Milagros Miceli, is working on a project on the power asymmetries in outsourced data work. Many data laborers, who analyze and manage vast amounts of data coming into tech companies, reside in the Global South and are typically paid a pittance. For Hanna, DAIR feels like a natural fit. Her self-described “nontraditional pathway to tech” began with a PhD in sociology and work on labor justice. In graduate school, she used machine-learning tools to study how activists connected with one another during the 2008 revolution in Egypt, where her family is from. “People were saying [the revolution] happened on Facebook and Twitter, but you can’t just pull a movement out of thin air,” Hanna says. “I began interviewing activists and understanding what they are doing on the ground aside from online activity.” DAIR is aiming for big, structural change by using research to shed light on issues that might not otherwise be explored and to disseminate knowledge that might not otherwise be valued. “In my Google resignation letter, I pointed out how tech organizations embody a lot of white supremacist values and practices,” Hanna says. “Unsettling that means interrogating what those perspectives are and navigating how to undo those organizational practices.” Those are values, she says, that DAIR champions. Anmol Irfan is a freelance journalist and founder of Perspective Magazine, based in Lahore, Pakistan.",0.0
Why AI shouldn’t be making life-and-death decisions,https://www.technologyreview.com/2022/10/17/1061738/why-ai-shouldnt-be-making-life-and-death-decisions/,2022-10-17,"<p>Plus: Meta wants to use AI to give people legs in the metaverse. </p>
","To receive The Algorithm in your inbox every Monday, sign up here. Welcome to The Algorithm! Let me introduce you to Philip Nitschke, also known as “Dr. Death” or “the Elon Musk of assisted suicide.” Nitschke has a curious goal: He wants to “demedicalize” death and make assisted suicide as unassisted as possible through technology. As my colleague Will Heaven reports, Nitschke has developed a coffin-size machine called the Sarco. People seeking to end their lives can enter the machine after undergoing an algorithm-based psychiatric self-assessment. If they pass, the Sarco will release nitrogen gas, which asphyxiates them in minutes. A person who has chosen to die must answer three questions: Who are you? Where are you? And do you know what will happen when you press that button? In Switzerland, where assisted suicide is legal, candidates for euthanasia must demonstrate mental capacity, which is typically assessed by a psychiatrist. But Nitschke wants to take people out of the equation entirely. Nitschke is an extreme example. But as Will writes, AI is already being used to triage and treat patients in a growing number of health-care fields. Algorithms are becoming an increasingly important part of care, and we must try to ensure that their role is limited to medical decisions, not moral ones. Will explores the messy morality of efforts to develop AI that can help make life-and-death decisions here. I’m probably not the only one who feels extremely uneasy about letting algorithms make decisions about whether people live or die. Nitschke’s work seems like a classic case of misplaced trust in algorithms’ capabilities. He’s trying to sidestep complicated human judgments by introducing a technology that could make supposedly “unbiased” and “objective” decisions. That is a dangerous path, and we know where it leads. AI systems reflect the humans who build them, and they are riddled with biases. We’ve seen facial recognition systems that don’t recognize Black people and label them as criminals or gorillas. In the Netherlands, tax authorities used an algorithm to try to weed out benefits fraud, only to penalize innocent people—mostly lower-income people and members of ethnic minorities. This led to devastating consequences for thousands: bankruptcy, divorce, suicide, and children being taken into foster care. As AI is rolled out in health care to help make some of the highest-stake decisions there are, it’s more crucial than ever to critically examine how these systems are built. Even if we manage to create a perfect algorithm with zero bias, algorithms lack the nuance and complexity to make decisions about humans and society on their own. We should carefully question how much decision-making we really want to turn over to AI. There is nothing inevitable about letting it deeper and deeper into our lives and societies. That is a choice made by humans. Meta wants to use AI to give people legs in the metaverse Last week, Meta unveiled its latest virtual-reality headset. It has an eye-watering $1,499.99 price tag. At the virtual event, Meta pitched its vision for a “next-generation social platform” accessible to everyone. As my colleague Tanya Basu points out: “Even if you are among the lucky few who can shell out a grand and a half for a virtual-reality headset, would you really want to?” The legs were fake: One of the big selling points for the Metaverse was the ability for avatars to have legs. Legs! At the event, a leggy avatar of Meta CEO Mark Zuckerberg announced that the company was going to use artificial intelligence to enable this feature, allowing avatars not only to walk and run but also to wear digital clothing. But there’s one problem. Meta hasn’t actually figured out how to do this yet, and the “segment featured animations created from motion capture,” as Kotaku reports. Meta’s AI lab is one of the biggest and richest in the industry, and it’s hired some of the field’s top engineers. I can’t imagine that this multibillion-dollar push to make VR Sims happen is very fulfilling work for Meta’s AI researchers. Do you work in AI/ML teams at Meta? I want to hear from you. (Drop me a line melissa.heikkila@technologyreview.com) Learn more about the exploited labor behind artificial intelligenceIn an essay, Timnit Gebru, former co-lead of Google’s ethical AI team, and researchers at her Distributed AI Research Institute argue that AI systems are driven by labor exploitation, and that AI ethics discussions should prioritize transnational worker organization efforts. (Noema) AI-generated art is the new clip artMicrosoft has teamed up with OpenAI to add text-to-image AI DALL-E 2 to its Office suite. Users will be able to enter prompts to create images that can be used in greeting cards or PowerPoint presentations. (The Verge) An AI version of Joe Rogan interviewed an AI Steve JobsThis is pretty mind-blowing. Text-to-voice AI startup Play.ht trained an AI model on Steve Jobs’s biography and all the recordings it could find of him online in order to mimic the way Jobs would have spoken in a real podcast. The content is pretty silly, but it won’t be long until the technology develops enough to fool anyone. (Podcast.ai) Tour Amazon’s dream home, where every appliance is also a spyThis story offers a clever way to visualize how invasive Amazon’s push to embed “smart” devices in our homes really is. (The Washington Post) Tweet of the weekWhat it's like to build a machine-learning startup these days, from Hugging Face CEO and cofounder Clem Delangue. Thanks for making it this far! Catch you next week. Melissa",0.0
The messy morality of letting AI make life-and-death decisions,https://www.technologyreview.com/2022/10/13/1060945/artificial-intelligence-life-death-decisions-hard-choices/,2022-10-13,"<p>Automation can help us make hard choices, but it can’t do it alone.</p>
","In a workshop in Rotterdam in the Netherlands, Philip Nitschke—“Dr. Death” or “the Elon Musk of assisted suicide” to some—is overseeing the last few rounds of testing on his new Sarco machine before shipping it to Switzerland, where he says its first user is waiting. This is the third prototype that Nitschke’s nonprofit, Exit International, has 3D-printed and wired up. Number one has been exhibited in Germany and Poland. “Number two was a disaster,” he says. Now he’s ironed out the manufacturing errors and is ready to launch: “This is the one that will be used.” A coffin-size pod with Star Trek stylings, the Sarco is the culmination of Nitschke’s 25-year campaign to “demedicalize death” through technology. Sealed inside the machine, a person who has chosen to die must answer three questions: Who are you? Where are you? And do you know what will happen when you press that button? Here’s what will happen: The Sarco will fill with nitrogen gas. Its occupant will pass out in less than a minute and die by asphyxiation in around five. A recording of that short, final interview will then be handed over to the Swiss authorities. Nitschke has not approached the Swiss government for approval, but Switzerland is one of a handful of countries that have legalized assisted suicide. It is permitted as long as people who wish to die perform the final act themselves. Nitschke wants to make assisted suicide as unassisted as possible, giving people who have chosen to kill themselves autonomy, and thus dignity, in their final moments. “You really don’t need a doctor to die,” he says. Because the Sarco uses nitrogen, a widely available gas, rather than the barbiturates that are typically used in euthanasia clinics, it does not require a physician to administer an injection or sign off on lethal drugs. At least that’s the idea. Nitschke has not yet been able to sidestep the medical establishment fully. Switzerland requires that candidates for euthanasia demonstrate mental capacity, Nitschke says, which is typically assessed by a psychiatrist. “There’s still a belief that if a person is asking to die, they’ve got some sort of undiagnosed mental illness,” he says. “That it’s not rational for a person to seek death.” He believes he has a solution, however. Exit International is working on an algorithm that Nitschke hopes will allow people to perform a kind of psychiatric self-assessment on a computer. In theory, if a person passed this online test, the program would provide a four-digit code to activate the Sarco. “That’s the goal,” says Nitschke. “Having said all that, the project is proving very difficult.” Nitschke’s mission may seem extreme—even outrageous—to some. And his belief in the power of algorithms may prove to be overblown. But he is not the only one looking to involve technology, and AI in particular, in life-or-death decisions. Yet where Nitschke sees AI as a way to empower individuals to make the ultimate choice by themselves, others wonder if AI can help relieve humans from the burden of such choices. AI is already being used to triage and treat patients across a growing number of health-care fields. As algorithms become an increasingly important part of care, we must ensure that their role is limited to medical decisions, not moral ones. Medical care is a limited resource. Patients must wait for appointments to get tests or treatment. Those in need of organ transplants must wait for suitable hearts or kidneys. Vaccines must be rolled out first to the most vulnerable (in countries that have them). And during the worst of the pandemic, when hospitals faced a shortage of beds and ventilators, doctors had to make snap decisions about who would receive immediate care and who would not—with tragic consequences. The covid crisis brought the need for such choices into harsh focus—and led many to wonder if algorithms could help. Hospitals around the world bought new or co-opted existing AI tools to assist with triage. Some hospitals in the UK that had been exploring the use of AI tools to screen chest x-rays jumped on those tools as a fast, cheap way to identify the most severe covid cases. Suppliers of this tech, such as Qure.ai, based in Mumbai, India, and Lunit, based in Seoul, Korea, took on contracts in Europe, the US, and Africa. Diagnostic Robotics, an Israeli firm that supplies AI-based triage tools to hospitals in Israel, India, and the US, has said it saw a sevenfold jump in demand for its technology in the first year of the pandemic. Business in health-care AI has been booming ever since. This rush to automate raises big questions with no easy answers. What kinds of decision is it appropriate to use an algorithm to make? How should these algorithms be built? And who gets a say in how they work? Rhema Vaithianathan, the director of the Centre for Social Data Analytics and a professor at the Auckland University of Technology in New Zealand, who focuses on tech in health and welfare, thinks it is right that people are asking AI to help make big decisions. “We should be addressing problems that clinicians find really hard,” she says. One of the projects she is working on involves a teen mental-health service, where young people are diagnosed and treated for self-harming behaviors. There is high demand for the clinic, and so it needs to maintain a high turnover, discharging patients as soon as possible so that more can be brought in. Doctors face the difficult choice between keeping existing patients in care and treating new ones. “Clinicians don’t discharge people because they’re super scared of them self-harming,” says Vaithianathan. “That’s their nightmare scenario.” Even when AI seems accurate, scholars and regulators alike call for caution. Vaithianathan and her colleagues have tried to develop a machine-learning model that can predict which patients are most at risk of future self-harming behavior and which are not, using a wide range of data, including health records and demographic information, to give doctors an additional resource in their decision-­making. “I’m always looking for those cases where a clinician is struggling and would appreciate an algorithm,” she says. The project is in its early stages, but so far the researchers have found that there may not be enough data to train a model that can make accurate predictions. They will keep trying. The model does not have to be perfect to help doctors, Vaithianathan says. They are not the only team trying to predict the risk of discharging patients. A review published in 2021 highlighted 43 studies by researchers claiming to use machine-learning models to predict whether patients will be readmitted or die after they leave hospitals in the US. None were accurate enough for clinical use, but the authors look forward to a time when such models “improve quality of care and reduce health-care costs.” And yet even when AI seems accurate, scholars and regulators alike call for caution. For one thing, the data that algorithms follow and the way they follow it are human artifacts, riddled with prejudice. Health data is overpopulated by people who are white and male, for example, which skews its predictive power. And the models offer a veneer of objectivity that can lead people to pass the buck on ethical decisions, trusting the machine rather than questioning its output. This ongoing problem is a theme in David Robinson’s new book, Voices in the Code, about the democratization of AI. Robinson, a visiting scholar at the Social Science Matrix at the University of California, Berkeley, and a member of the faculty of Apple University, tells the story of Belding Scribner. In 1960 Scribner, a nephrologist in Seattle, inserted a short Teflon tube known as a shunt into some of his patients’ arms to prevent their blood from clotting while they underwent dialysis treatment. The innovation allowed people with kidney disease to stay on dialysis indefinitely, transforming kidney failure from a fatal condition into a long-term illness. When word got out, Scribner was inundated with requests for treatment. But he could not take everyone. Whom should he help and whom should he turn away? He soon realized that this wasn’t a medical decision but an ethical one. He set up a committee of laypeople to decide. Of course, their choices weren’t perfect. The prejudices at the time led the committee to favor married men with jobs and families, for example. The way Robinson tells it, the lesson we should take from Scribner’s work is that certain processes—bureaucratic, technical, and algorithmic—can make difficult questions seem neutral and objective. They can obscure the moral aspects of a choice—and the sometimes awful consequences. “Bureaucracy itself can serve as a way of converting hard moral problems into boring technical ones,” Robinson writes. This phenomenon predates computers, he says, “but software-based systems can accelerate and amplify this trend. Quantification can be a moral anesthetic, and computers make that anesthetic easier than ever to administer.” Whatever the process, we need to let that moral anesthetic wear off and examine the painful implications of the decision at hand. For Scribner, that meant asking an open panel of laypeople—instead of a group of ostensibly objective doctors meeting behind closed doors—whom to save. Today, it could mean asking for high-stakes algorithms to be audited. For now, the auditing of algorithms by independent parties is more wish-list item than standard practice. But, again using the example of kidney disease, Robinson shows how it can be done. By the 2000s, an algorithm had been developed in the US to identify recipients for donated kidneys. But some people were unhappy with how the algorithm had been designed. In 2007, Clive Grawe, a kidney transplant candidate from Los Angeles, told a room full of medical experts that their algorithm was biased against older people like him. The algorithm had been designed to allocate kidneys in a way that maximized years of life saved. This favored younger, wealthier, and whiter patients, Grawe and other patients argued. Such bias in algorithms is common. What’s less common is for the designers of those algorithms to agree that there is a problem. After years of consultation with laypeople like Grawe, the designers found a less biased way to maximize the number of years saved—by, among other things, considering overall health in addition to age. One key change was that the majority of donors, who are often people who have died young, would no longer be matched only to recipients in the same age bracket. Some of those kidneys could now go to older people if they were otherwise healthy. As with Scribner’s committee, the algorithm still wouldn’t make decisions that everyone would agree with. But the process by which it was developed is harder to fault. “I didn’t want to sit there and give the injection. If you want it, you press the button.” Nitschke, too, is asking hard questions. A former doctor who burned his medical license after a years-long legal dispute with the Australian Medical Board, Nitschke has the distinction of being the first person to legally administer a voluntary lethal injection to another human. In the nine months between July 1996, when the Northern Territory of Australia brought in a law that legalized euthanasia, and March 1997, when Australia’s federal government overturned it, Nitschke helped four of his patients to kill themselves. The first, a 66-year-old carpenter named Bob Dent, who had suffered from prostate cancer for five years, explained his decision in an open letter: “If I were to keep a pet animal in the same condition I am in, I would be prosecuted.” Nitschke wanted to support his patients’ decisions. Even so, he was uncomfortable with the role they were asking him to play. So he made a machine to take his place. “I didn’t want to sit there and give the injection,” he says. “If you want it, you press the button.” The machine wasn’t much to look at: it was essentially a laptop hooked up to a syringe. But it achieved its purpose. The Sarco is an iteration of that original device, which was later acquired by the Science Museum in London. Nitschke hopes an algorithm that can carry out a psychiatric assessment will be the next step. But there’s a good chance those hopes will be dashed. Creating a program that can assess someone’s mental health is an unsolved problem—and a controversial one. As Nitschke himself notes, doctors do not agree on what it means for a person of sound mind to choose to die. “You can get a dozen different answers from a dozen different psychiatrists,” he says. In other words, there is no common ground on which an algorithm could even be built. But that’s not the takeaway here. Like Scribner, Nitschke is asking what counts as a medical decision, what counts as an ethical one, and who gets to choose. Scribner thought that laypeople—representing society as a whole—should choose who received dialysis, because when patients have more or less equal chances of survival, who lives and who dies is no longer a technical question. As Robinson describes it, society must be responsible for such decisions, although the process can still be encoded in an algorithm if it’s done inclusively and transparently. For Nitschke, assisted suicide is also an ethical decision, one that individuals must make for themselves. The Sarco, and the theoretical algorithm he imagines, would only protect their ability to do so. AI will become increasingly useful, perhaps essential, as populations boom and resources stretch. Yet the real work will be acknowledging the awfulness and arbitrariness of many of the decisions AI will be called on to make. And that’s on us. For Robinson, devising algorithms is a bit like legislation: “In a certain light, the question of how best to make software code that will govern people is just a special case of how best to make laws. People disagree about the merits of different ways of making high-stakes software, just as they disagree about the merits of different ways of making laws.” And it is people—in the broadest sense—who are ultimately responsible for the laws we have.",0.0
"Machine learning operations offer agility, spur innovation",https://www.technologyreview.com/2022/10/12/1060890/machine-learning-operations-offer-agility-spur-innovation/,2022-10-12,"Many organizations have adopted machine learning (ML) in a piecemeal fashion, building or buying ad hoc models, algorithms, tools, or services to accomplish specific goals. This approach was necessary as companies learned about the capabilities of ML and as the technology matured, but it also has created a hodge-podge of siloed, manual, and nonstandardized processes…","In association withCapital One Many organizations have adopted machine learning (ML) in a piecemeal fashion, building or buying ad hoc models, algorithms, tools, or services to accomplish specific goals. This approach was necessary as companies learned about the capabilities of ML and as the technology matured, but it also has created a hodge-podge of siloed, manual, and nonstandardized processes and components within organizations. This can lead, in turn, to inefficient, cumbersome services that fail to deliver on their promised value—or that stall innovation entirely. As businesses look to scale ML applications across the enterprise, they need to better automate and standardize tools, processes, and workflows. They need to build and deploy ML models quickly, spending less time manually training and monitoring models and more time on value-driving, revenue-generating innovation. Developers need access to the data that will power their ML models, to work across lines of business, and to collaborate transparently on the same tech stack. In other words, businesses need to adopt best practices for machine learning operations (MLOps): a set of software development practices that keep ML models running effectively and with agility. The main function of MLOps is to automate the more repeatable steps in the ML workflows of data scientists and ML engineers, from model development and training to model deployment and operation (model serving). Automating these steps creates agility for businesses and better experiences for users and end customers, increasing the speed, power, and reliability of ML. These automated processes can also mitigate risk and free developers from rote tasks, allowing them to spend more time on innovation. This all contributes to the bottom line: a 2021 global study by McKinsey found that companies that successfully scale AI can add as much as 20 percent to their earnings before interest and taxes (EBIT). “It’s not uncommon for companies with sophisticated ML capabilities to incubate different ML tools in individual pockets of the business,” says Vincent David, senior director for machine learning at Capital One. “But often you start seeing parallels—ML systems doing similar things, but with a slightly different twist. The companies that are figuring out how to make the most of their investments in ML are unifying and supercharging their best ML capabilities to create standardized, foundational tools and platforms that everyone can use — and ultimately create differentiated value in the market.” In practice, MLOps requires close collaboration between data scientists, ML engineers, and site reliability engineers (SREs) to ensure consistent reproducibility, monitoring, and maintenance of ML models. Over the last several years, Capital One has developed MLOps best practices that apply across industries: balancing user needs, adopting a common, cloud-based technology stack and foundational platforms, leveraging open-source tools, and ensuring the right level of accessibility and governance for both data and models. ML applications generally have two main types of users—technical experts (data scientists and ML engineers) and nontechnical experts (business analysts)—and it’s important to strike a balance between their different needs. Technical experts often prefer complete freedom to use all tools available to build models for their intended use cases. Nontechnical experts, on the other hand, need user-friendly tools that enable them to access the data they need to create value in their own workflows. To build consistent processes and workflows while satisfying both groups, David recommends meeting with the application design team and subject matter experts across a breadth of use cases. “We look at specific cases to understand the issues, so users get what they need to benefit their work, specifically, but also the company generally,” he says. “The key is figuring out how to create the right capabilities while balancing the various stakeholder and business needs within the enterprise.” Collaboration among development teams—critical for successful MLOps—can be difficult and time-consuming if these teams are not using the same technology stack. A unified tech stack allows developers to standardize, reusing components, features, and tools across models like Lego bricks. “That makes it easier to combine related capabilities so developers don’t waste time switching from one model or system to another,” says David. A cloud-native stack—built to take advantage of the cloud model of distributed computing—allows developers to self-service infrastructure on demand, continually leveraging new capabilities and introducing new services. Capital One’s decision to go all-in on the public cloud has had a notable impact on developer efficiency and speed. Code releases to production now happen much more rapidly, and ML platforms and models are reusable across the broader enterprise. Open-source ML tools (code and programs freely available for anyone to use and adapt) are core ingredients in creating a strong cloud foundation and unified tech stack. Using existing open-source tools means the business does not need to devote precious technical resources to reinventing the wheel, quickening the pace at which teams can build and deploy models. To complement its use of open-source tools and packages, David says, Capital One also develops and releases its own tools. For example, to manage streams of dynamic data too large to manually monitor, Capital One built an open-source data profiling tool that uses ML to detect and protect sensitive data like bank account and credit card numbers. Additionally, Capital One recently released the open-source library rubicon-ml, which helps capture and store model training and execution information in a repeatable and searchable way. Releasing its own tools as open source ensures that Capital One builds ML capabilities that are flexible and repurposable (by others, as well as across its own businesses) and allows the company to connect with and contribute to the open-source community. A typical ML system includes a production environment (processing data in real-time) and an analytical environment (a store of data with which users can work). For many organizations, the lag time between these environments is a significant pain point. When data scientists and engineers need access to near-real-time data from the production environment, it’s important to set up appropriate controls. ML developers thus need to ensure integration and access to both environments without compromising governance integrity. “In an ideal world, the organization would establish a seamless integration between production data stores and analytical environments that can enforce all the controls and governance frameworks that the data scientists, engineers, and other stakeholders involved in the model governance process need,” says David. Governing and managing the ML models themselves is equally important. As a machine learns and as input data changes, models tend to drift, which traditionally requires engineers to monitor and correct for that drift. MLOps practices, by contrast, help automate the management and training of models and workflows. An organization adopting MLOps could determine for each ML use case what needs to be monitored, how often, and how much drift to allow before retraining is required. It can then configure tools to automatically detect triggers and retrain models at an appropriate cadence. In the early days of ML, companies took pride in their ability to develop new and bespoke solutions for different parts of the business. But now companies seeking to scale ML in a well-governed, nimble way have to account for continuous updates to data sources, ML models, features, pipelines, and many other aspects of the ML model lifecycle. With its potential to offer standardized, reproducible, and adaptable processes across large-scale ML environments, MLOps could unlock the future of enterprise machine learning. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.",0.0
Who’s going to save us from bad AI?,https://www.technologyreview.com/2022/10/10/1061016/whos-going-to-save-us-from-bad-ai/,2022-10-10,"<p>Plus: DeepMind does math.</p>
","To receive The Algorithm in your inbox every Monday, sign up here. Welcome to the Algorithm! About damn time. That was the response from AI policy and ethics wonks to news last week that the Office of Science and Technology Policy, the White House’s science and technology advisory agency, had unveiled an AI Bill of Rights. The document is Biden’s vision of how the US government, technology companies, and citizens should work together to hold the AI sector accountable. It’s a great initiative, and long overdue. The US has so far been one of the only Western nations without clear guidance on how to protect its citizens against AI harms. (As a reminder, these harms include wrongful arrests, suicides, and entire cohorts of schoolchildren being marked unjustly by an algorithm. And that’s just for starters.) Tech companies say they want to mitigate these sorts of harms, but it’s really hard to hold them to account. The AI Bill of Rights outlines five protections Americans should have in the AI age, including data privacy, the right to be protected from unsafe systems, and assurances that algorithms shouldn’t be discriminatory and that there will always be a human alternative. Read more about it here. So here’s the good news: The White House has demonstrated mature thinking about different kinds of AI harms, and this should filter down to how the federal government thinks about technology risks more broadly. The EU is pressing on with regulations that ambitiously try to mitigate all AI harms. That’s great but incredibly hard to do, and it could take years before their AI law, called the AI Act, is ready. The US, on the other hand, “can tackle one problem at a time,” and individual agencies can learn to handle AI challenges as they arise, says Alex Engler, who researches AI governance at the Brookings Institution, a DC think tank. And the bad: The AI Bill of Rights is missing some pretty important areas of harm, such as law enforcement and worker surveillance. And unlike the actual US Bill of Rights, the AI Bill of Rights is more an enthusiastic recommendation than a binding law. “Principles are frankly not enough,” says Courtney Radsch, US tech policy expert for the human rights organization Article 19. “In the absence of, for example, a national privacy law that sets some boundaries, it’s only going part of the way,” she adds. The US is walking on a tightrope. On the one hand, America doesn’t want to seem weak on the global stage when it comes to this issue. The US plays perhaps the most important role in AI harm mitigation, since most of the world’s biggest and richest AI companies are American. But that’s the problem. Globally, the US has to lobby against rules that would set limits on its tech giants, and domestically it’s loath to introduce any regulation that could potentially “hinder innovation.” The next two years will be critical for global AI policy. If the Democrats don’t win a second term in the 2024 presidential election, it is very possible that these efforts will be abandoned. New people with new priorities might drastically change the progress made so far, or take things in a completely different direction. Nothing is impossible. DeepMind’s game-playing AI has beaten a 50-year-old record in computer science They’ve done it again: AI lab DeepMind has used its board-game playing AI AlphaZero to discover a faster way to solve a fundamental math problem in computer science, beating a record that has stood for more than 50 years. The researchers trained a new version of AlphaZero, called AlphaTensor, to play a game that learned the best series of steps to solve the math problem. It was rewarded for winning the game in as few moves as possible. Why this is a big deal: The problem, matrix multiplication, is a crucial type of calculation at the heart of many different applications, from displaying images on a screen to simulating complex physics. It is also fundamental to machine learning itself. Speeding up this calculation could have a big impact on thousands of everyday computer tasks, cutting costs and saving energy. Read more from my colleague Will Heaven here. Google released an impressive text-to-video AIJust a week after Meta’s text-to-image AI reveal, Google has upped the ante. The videos that its system Imagen Video produces are of much higher definition than Meta’s. But, like Meta, Google is not releasing its model into the wild, because of “social biases and stereotypes which are challenging to detect and filter.” (Google) Google’s new AI can hear a snippet of a song—and then keep on playingThe technique, called AudioLM, generates naturalistic sounds without the need for human annotation. (MIT Technology Review) Even after $100 billion, self-driving cars are going nowhereWhat a quote from Anthony Levandowski, one of the field’s biggest stars: ""Forget about profits—what’s the combined revenue of all the [AV] companies? Is it a million dollars? Maybe. I think it’s more like zero."" (Bloomberg Businessweek) Robotics companies have pledged not to weaponize their techSix of the largest robotics companies in the world, including Boston Dynamics, have pledged not to weaponize their robots. (Unless, of course, it is for governments’ defense purposes.) Meanwhile, defense AI startup Anduril says it has developed loitering munitions, also known as suicide drones, and this is apparently just the start of its new weapons program. I wrote last summer about how business is booming for military AI startups. The invasion of Ukraine has prompted militaries to update their arsenals—and Silicon Valley stands to capitalize. (MIT Technology Review) This is life in the MetaverseA fun story about life in the Metaverse and its early adopters. This is the first Metaverse story where I could kinda see the appeal of it. (But didn't make me want to plug and play anytime soon.) (The New York Times) There’s a new AI that allows you to create interiorsThe model was built in five days using the open-source text-to-image model Stable Diffusion to generate snazzy interiors. It’s great to see people using the model to build new applications. On the downside, I can totally see this tech being used for Airbnb and real estate scams. (InteriorAI) See you next time, Melissa",0.0
Google’s new AI can hear a snippet of song—and then keep on playing,https://www.technologyreview.com/2022/10/07/1060897/ai-audio-generation/,2022-10-07,"<p>The technique, called AudioLM, generates naturalistic sounds without the need for human annotation.</p>
","A new AI system can create natural-sounding speech and music after being prompted with a few seconds of audio. AudioLM, developed by Google researchers, generates audio that fits the style of the prompt, including complex sounds like piano music, or people speaking, in a way that is almost indistinguishable from the original recording. The technique shows promise for speeding up the process of training AI to generate audio, and it could eventually be used to auto-generate music to accompany videos. (You can listen to all of the examples here.) AI-generated audio is commonplace: voices on home assistants like Alexa use natural language processing. AI music systems like OpenAI’s Jukebox have already generated impressive results, but most existing techniques need people to prepare transcriptions and label text-based training data, which takes a lot of time and human labor. Jukebox, for example, uses text-based data to generate song lyrics. AudioLM, described in a non-peer-reviewed paper last month, is different: it doesn’t require transcription or labeling. Instead, sound databases are fed into the program, and machine learning is used to compress the audio files into sound snippets, called “tokens,” without losing too much information. This tokenized training data is then fed into a machine-learning model that uses natural language processing to learn the sound’s patterns. To generate the audio, a few seconds of sound are fed into AudioLM, which then predicts what comes next. The process is similar to the way language models like GPT-3 predict what sentences and words typically follow one another. The audio clips released by the team sound pretty natural. In particular, piano music generated using AudioLM sounds more fluid than piano music generated using existing AI techniques, which tends to sound chaotic. A new wave of startups are using deep learning to build synthetic voice actors for digital assistants, video-game characters, and corporate videos. Roger Dannenberg, who researches computer-generated music at Carnegie Mellon University, says AudioLM already has much better sound quality than previous music generation programs. In particular, he says, AudioLM is surprisingly good at re-creating some of the repeating patterns inherent in human-made music. To generate realistic piano music, AudioLM has to capture a lot of the subtle vibrations contained in each note when piano keys are struck. The music also has to sustain its rhythms and harmonies over a period of time. “That’s really impressive, partly because it indicates that they are learning some kinds of structure at multiple levels,” Dannenberg says. AudioLM isn’t only confined to music. Because it was trained on a library of recordings of humans speaking sentences, the system can also generate speech that continues in the accent and cadence of the original speaker—although at this point those sentences can still seem like non sequiturs that don’t make any sense. AudioLM is trained to learn what types of sound snippets occur frequently together, and it uses the process in reverse to produce sentences. It also has the advantage of being able to learn the pauses and exclamations that are inherent in spoken languages but not easily translated into text. Rupal Patel, who researches information and speech science at Northeastern University, says that previous work using AI to generate audio could capture those nuances only if they were explicitly annotated in training data. In contrast, AudioLM learns those characteristics from the input data automatically, which adds to the realistic effect. “There is a lot of what we could call linguistic information that is not in the words that you pronounce, but it’s another way of communicating based on the way you say things to express a specific intention or specific emotion,” says Neil Zeghidour, a co-creator of AudioLM. For example, someone may laugh after saying something to indicate that it was a joke. “All that makes speech natural,” he says. Eventually, AI-generated music could be used to provide more natural-sounding background soundtracks for videos and slideshows. Speech generation technology that sounds more natural could help improve internet accessibility tools and bots that work in health care settings, says Patel. The team also hopes to create more sophisticated sounds, like a band with different instruments or sounds that mimic a recording of a tropical rainforest. However, the technology’s ethical implications need to be considered, Patel says. In particular, it’s important to determine whether the musicians who produce the clips used as training data will get attribution or royalties from the end product—an issue that has cropped up with text-to-image AIs. AI-generated speech that’s indistinguishable from the real thing could also become so convincing that it enables the spread of misinformation more easily. In the paper, the researchers write that they are already considering and working to mitigate these issues—for example, by developing techniques to distinguish natural sounds from sounds produced using AudioLM. Patel also suggested including audio watermarks in AI-generated products to make them easier to distinguish from natural audio.",1.0
I Was There When: AI mastered chess,https://www.technologyreview.com/2022/10/06/1060824/i-was-there-when-ai-mastered-chess/,2022-10-06,"<p>In the late ‘90s, IBM’s Deep Blue computer beat Garry Kasparov—the reigning world champion of chess. It paved the way for a revolution in automation.</p>
","I Was There When is an oral history project that’s part of the In Machines We Trust podcast. It features stories of how breakthroughs and watershed moments in artificial intelligence and computing happened, as told by the people who witnessed them. In this episode we meet one of the world's greatest chess players, Garry Kasparov. This project was produced by Jennifer Strong, Anthony Green and Emma Cillekens. It was edited by Mat Honan and mixed by Garret Lang with original music by Jacob Gorski. The art is from Eric Mongeon and Stephanie Arnett. [TR ID] Jennifer: For as long as there’s been AI research, games have been a part of it… especially chess. We think of people who are good at playing chess as having a certain level of intelligence … and so the game also became a way to gauge the intelligence of machines. And… fun fact? The very first chess playing program was written before a computer even existed to run it. Pioneering computer scientist Alan Turing played it in 1950...using an algorithm worked out on paper. It didn’t work very well. But people continued to advance this research for decades. And then, in 1997, IBM's Deep Blue computer beat Garry Kasparov... the reigning world champion of chess. Commentator 2: Are we missing something on the chessboard now that Kasparov sees? He does not look.. he looks disgusted in fact. Commentator 1: Whoah! Commentator 2: Deep Blue! Kasparov, after the move C4, has resigned! [Applause] Jennifer: I’m Jennifer Strong, and this is I Was There When—an oral history project featuring the stories of breakthroughs and watershed moments in AI and computing, as told by those who witnessed them. This episode, we meet the man on the other side of that chess board, Garry Kasparov. Garry Kasparov: It was inevitable that something described on the cover of Newsweek as the brain’s last stand and in books as big as the moon landing would involve a lot of mythology. I admit that I was caught up in a lot of this hype myself. It took years of reflection and examination to sort out my impressions then and the truth. I wrote about this painful process in my 2017 book, Deep Thinking: When machine intelligence ends and human creativity begins, it's easy for a chess machine, after all. They don't care if they win or lose. They don't even know they're playing chess. But as a human and world champion, I had many emotions sitting down across from a machine. Garry Kasparov: Would it play like previous machines or would it play like God? I was used to reading my opponents body language. Not exactly helpful, sitting across from a computer engineer making moves he didn't understand for the machine he'd built. I was also used to preparing deeply for my opponents based on their previous games and their tendencies. Against Deep Blue, this was also out the window as they kept their training games secret. And of course they could upgrade its strengths and change its chess personality with a few keystrokes. If only I could. It was hard to explain my experience because I was really the first knowledge worker to have my job threatened by a machine. Garry Kasparov: Most AI and experiences before that were hoaxes, or quite primitive. For example, human elevate operators being replaced by automatic push-bat elevators was very alarming to people in the 1940s. In fact, the technology for automatic elevators had existed for decades, but people were afraid of them. Plus, the elevate operators had a strong union. Today, there are many easy comparisons. Sitting down across from Deep Blue was in one way entirely normal. I had been feeding at a chess board since I was six years old, and technically a little was different for me, and yet it was entirely different. I felt like most people will feel the first time they get into a self-driving car or get a diagnosis from an AI doctor. Garry Kasparov: These new marvels are far beyond my chest nemesis. Of course, the machine I lost to in the 1997 rematch, sometimes called Deep Blue, was as intelligent as your alarm clock—a 10 million dollar alarm clock, but nothing like what had been imagined by previous generations. This is not to downplay their achievement, which was a Mount Everest of computing—to defeat the world chess champion . There was a reason it got global attention. I only want to put into context what we mean when we say intelligent. Deep Blue did one thing very well with hundreds of specialized chiefs, but it was enough to compete at the world champion level because chess is deep but not deep enough. Deep Blue didn't have to solve chess. It only had to play better over six games and brute force analysis at fast speeds turned out to be enough. It took me a while to absorb the most important lessons of my loss, and they had nothing to do with chess and everything to do with the future of the human-machine relationship. Garry Kasparov: The period in which we compete against intelligent machines is very small, almost insignificant, yet we put so much importance on it instead of the alternative machine supremacy that follows, which is what really matters. AI automation replaces human jobs, for example, and there's a brief moment of equality in performance with humans. But that doesn't last long, and forever after machines will do it better, cheaper, and more safely. That's human progress. It makes our lives better. This isn't to be callous to those who lose their jobs, but even there, study after study shows that industries with more automation and AI do better with more jobs and higher salaries. The alternative is stagnation. Garry Kasparov: Another key point from Deep Blue that has broad applications to AI and tech in general is that we often miss the early signs of inevitable machine dominance. My loss in the 1997 rematch was a big deal, but in fact, the most important point had been already made a year earlier in our first match in 1996 in Philadelphia, which I won. But I lost the very first game of the match, and that was the writing on the wall. That made it clear that chess was not as special as everyone had thought. It was just another closed, complex system that would inevitably be cracked by increases in computer power. That's when that little competition window started to close. Now it's funny to think about competing with chess machines. They are our tools, not our competition. We also get better cancer screening, safer roads, greater productivity and security. We don't have to work at many boring and dangerous jobs anymore. When that happens, we are free to become more creative, more strategic in our activities. Garry Kasparov: We can direct the robots, the algorithms, et cetera. Technology doing our work is the whole history of human progress. This is why I prefer to call AI or augmented intelligence, not artificial. It's a tool and makes us smarter. The way a telescope augments our vision—what matters is how we focus it. This isn't just a matter of economics. Global security also depends on it as leading AI tech in the hands of dictatorships is a serious threat. I helped design the first PC database software that made all my handwritten notebooks of advantages obsolete. I used and helped promote the chess programs that would overtake me and everyone else. I saw them as necessary tools that would make me better. Not as threats. Garry Kasparov: In 1998, I invented what I called advanced chess, in which humans played together with a machine partner. A true model for many kinds of collaborative AI integration today. And humans still play chess against each other all over the world more than ever. Even though the phone in everyone's pocket now is stronger than Deep Blue. Are we using these powerful tools in a responsible way? The Wild West is romantic in cowboy movies, but progress means having laws upheld in society where we feel secured to take advantage of the technological marvels we have created. When I give lectures on AI, many expect me to be angry or a technophobe because of my loss to a machine 25 years ago. And I admit it, I am a sore loser. But really it was a fascinating experiment and experience. Mostly, I hope others learn from my attitudes. If you can’t beat them, join them. Don't rage against the machine if you can make it work for you and for all of us. Jennifer: Do you have a story to tell? Know someone who does? Drop us an email at podcasts at technology review dot com. You can find links to our reporting in the show notes... and you can support our journalism by going to tech review dot com slash subscribe. [MIDROLL] Jennifer: This project was produced by me with Anthony Green and Emma Cillekens. We’re edited by Mat Honan and our mix engineer is Garret Lang. Thanks for Listening. I'm Jennifer Strong.",0.0
This obscure shopping app is now America’s most downloaded,https://www.technologyreview.com/2022/10/25/1062142/temu-shopping-app-most-downloaded/,2022-10-25,"<p>Temu shot up to the top spot in app store charts, but it has a long way to go before it can replicate the e-commerce success it’s seen in China.</p>
","China Report is MIT Technology Review’s newsletter about what's happening in China. Sign up to receive it in your inbox every Tuesday. Take a deep breath. The Chinese Communist Party Congress news cycle is (basically) over. There are many significant things from the high-level political assembly to talk about, mostly around understanding what the new party leadership means for China’s future, but I’ll point you to political reporters who are real experts in all this down below. Instead, I want to talk about something big you probably missed last week: There’s a new Chinese e-commerce app that is quietly but quickly growing. It’s called Temu. And on October 17, it became the most downloaded shopping app in the United States. It’s a pretty big deal for Temu to beat Amazon, Walmart, and its Chinese competitor Shein, especially because your immediate response is probably What? I’ve never even heard of Temu! Well, you are in good company. The app remains obscure among most people, though it marks another high-profile attempt by yet another Chinese tech giant—after Alibaba, Shein, and ByteDance—to try its luck in the American e-commerce market. Temu (btw, there’s no official guidance on how to pronounce the name, but I’ve been saying tee-moo) is a global version of Chinese e-commerce company Pinduoduo. Founded in 2015, it entered a market that had been dominated by Alibaba for over a decade, yet it managed to rise through the competition and, in 2020, replaced Alibaba as the company with the most e-commerce customers in China. Today, Pinduoduo has over 730 million monthly active users—more than two times the US population!—and is known for both extremely cheap prices and innovative gimmicks that keep users hooked. Still, Pinduoduo has remained an unfamiliar name outside the country. So how did Temu rise to the top of the iOS App Store’s shopping chart? “I believe it’s driven almost exclusively by ads,” says Juozas Kaziukėnas, who founded the e-commerce analyst firm Marketplace Pulse, “because I’m seeing relatively no mentions of Temu on social media. That makes me believe that there’s very little organic recognition of the brand yet.” I know from my personal experience that at least this first part is true—I’ve been seeing Temu’s ads everywhere. Perhaps because they are targeting Chinese readers (I saw ads in Chinese), or perhaps because I’ve made the mistake of Googling its name (for work and this newsletter in particular), the ads have popped up all over for me, including in my Gmail inbox. (I didn’t even realize there were ad slots in Gmail.) According to Meta, Temu has run over 1,000 ads on its platforms since September, with posts in English and Chinese. (For comparison: Similar Chinese apps like Shein and AliExpress have run only dozens of ads.) When it comes to app stores, Temu’s iOS ads are mostly targeting consumers in the US and Canada, while its Android ads are also in seven other countries, according to the app-store advertising database App Growing. So what are Temu’s offerings like? It has everything from clothing to kitchen products, from car parts to electronics. Otherwise, the defining feature is probably just that they’re cheap. Dirt cheap. Just a quick browse on Monday morning told me you could get 60-cent earrings, $4 home security cameras, $4 wireless earphones, and $6 sneakers. Eighty percent of the orders would be delivered within 10 days, says the website, which is slower than Amazon but on par with Shein. I admit I’m drawn by how cheap these products are. But price is only one thing people consider when they shop. Is that enough to get US consumers to use Temu? Ring Nation wants to lure you in with funny content—and push you to buy a Ring camera to make your own. People often point to Shein, the Chinese online fast-fashion retailer, as an example of how cheap made-in-China goods can succeed in the US despite the decades-long stigma associated with them. Indeed, Shein is taking Gen Z’s closets by storm. But you also need to look at other platforms offering cheap Chinese goods that failed to reach Shein’s level of influence. AliExpress, the overseas version of Alibaba, which has been around since 2010, never managed to really break through in the US even though it also prices products extremely low. Shein did some things right that AliExpress didn’t—namely, marketing and presentation. By paying influencers to try out its clothes and produce glossy YouTube and TikTok videos, Shein is spreading the idea that, first and foremost, its products are fun and trendy. The fact that they’re made in China is secondary. The difference in their brand images is clear. Most people would go to AliExpress (or Wish, which is a comparable app from Russia) only when they know they want something inexpensive. But people go to Shein to check out the latest fashion trend and to get it at an affordable price. So will Temu be more like Shein or more like AliExpress? My sense is that so far, Temu is looking more like the latter, with its only real advantage being the price points. “This model of relying on ads to drive every transaction does have an expiration date, and that’s what unfortunately caused the demise of Wish,” says Kaziukėnas. At one point, ads will become too expensive and too inefficient to justify the costs. To go further, Temu needs to really differentiate itself from other Chinese and American shopping platforms. This is actually something Pinduoduo has done effectively in China. It’s known for an addictive, at times manipulative, app that incentivizes people to share what they are buying on social platforms to get even steeper discounts. In the past two years, the idea of “social e-commerce” that Pinduoduo pioneered has been seen as one of the most successful innovations in China’s consumer tech sector. But it’s also not clear how that idea would land in the US. The American shopping industry has been reluctant to accept Chinese trends. I used to believe that livestream shopping, which is extremely popular in China, would be accepted around the world and fundamentally change e-commerce. But years later, it still hasn’t happened, and it’s starting to feel like waiting for Godot. “You see some small attempts to do these sorts of experiments, but if you zoom out and look at what has actually changed over the last 10 years? Not that much. Amazon looks exactly the same,” says Kaziukėnas. “Perhaps [it] suggests that the West is just a much more established environment, economically and also technologically, so it’s unlikely to see these kinds of massive disruptions as you saw in China.” So good luck, Temu! In the meantime, you’ll probably be hearing this name again soon, and I’ll keep resisting the urge to click on those ubiquitous ads. Would you use Temu? Let me know at zeyi@technologyreview.com. 1. The US Department of Justice unsealed three separate criminal cases on Monday. In one, Chinese intelligence officers were allegedly found to be interfering in the investigations on Huawei; in another, they were apparently faking academic credentials to obtain sensitive information; and in the last, they were reportedly forcing US citizens to return to China. (Politico) 2. The key thing to understand about the new top leadership of the Chinese Communist Party: It shows the concentration of power by Xi Jinping. (The New York Times $) 3. Want to know what topics Xi Jinping mentioned more than his predecessors in his party congress speech? What did he miss? There are already several quick analyses of Xi’s speech by researcher Vicky Xu, law professor Taisu Zhang, and political science researcher Mike Thompson-Brusstar. 4. For the first time in 25 years, there’s not a single woman in the Chinese Communist Party’s Politburo. (The Wall Street Journal $) 5. “Birth tourism” is still alive and well for Chinese families looking to give birth or use a surrogate in the United States, but the process has become much more difficult because of China’s border controls. (NPR) 6. The New Yorker’s Evan Osnos profiled Guo Wengui, the controversial Chinese tycoon who was once close to the country’s political elites but has reinvented himself as a darling of Trump Republicans. (The New Yorker $) 7. Two Chinese filmmakers produced Diagnosia, an animated virtual-reality film that shows the physical and mental abuses that happen in China’s infamous internet addiction clinics. (Sixth Tone) 8. Meta developed the first artificial-intelligence tool to translate Hokkien in real time. The language has no written script but is spoken by over 46 million people in China, Taiwan, Malaysia, and other countries. (Mashable) This was a summer overheated by Chinese electric blankets. As Europeans braced for a cold winter amid the energy crisis, Chinese electric blanket factories suddenly found themselves in hot demand (yes, pun intended). As Chinese publication White Night Studio reported, in the three months from July to September, factories in southern China got massive orders from Europe to produce tens of thousands of electric blankets, a product that had been slowly disappearing in recent years. In the summer of 2022, exports of electric blankets to Europe increased 150%. Established factory owners were getting 300 calls a day asking for more supply, while rookies blindly entered the industry, opening new production lines. However, the trend came and went quickly. By October, the orders had slowed down as the market demand was being met—and anyway, it’s now too late for the blankets to be made, shipped to Europe, and sold before the chill sets in. Come on, we all butcher pronunciations at times. Maybe our friends make fun of us, maybe they let it go. But would you dare correct the Chinese president who was giving a highly anticipated speech in front of the whole Chinese population? In that case, the misspeak might just become the new norm. This week, Singaporean law professor Henry Gao caught a Chinese official (who majored in Chinese language studies, btw) changing his pronunciation of a word in real time during a press conference for the 20th Party Congress—likely because Xi Jinping misspoke the word just a few days before. Why? Because he realised that the 3rd pronunciation was Xiâ€™s pronunciation when he gave the 20th Congress Report! Jiang is currently the Director of the CPCCC Policy Research Office. He must know the correct pronunciation because his first degree is in Chinese language (78çº§) pic.twitter.com/CSk1SkRNak See you next week! Zeyi",0.0
We used to get excited about technology. What happened?,https://www.technologyreview.com/2022/10/21/1061260/innovation-technology-what-happened/,2022-10-21,"<p>Innovation that truly serves us all is in scarce supply. That’s a problem.</p>
","This piece is from our forthcoming mortality-themed issue, available from 26 October. If you want to read it when it comes out, you can subscribe to MIT Technology Review for as little as $80 a year. On a recent evening, I sat at home scrolling through my Twitter feed, which—since I’m a philosopher who studies AI and data—is always filled with the latest tech news. After a while, I noticed a heaviness growing in the pit of my stomach, that telltale sign that you are not having a good time. But why? I wasn’t reading news about politics, or the climate crisis, or the pandemic—the usual sources of doomscrolling ennui. I stopped and reflected for a moment. What had I just been looking at? I had blinked at the aesthetic poverty of the most recent pitch for Meta’s Horizon Worlds VR game, featuring Mark Zuckerberg’s dead-eyed cartoon avatar against a visual background that one Twitter wag charitably compared to “the painted walls of an abandoned day-care center.” I had let out a quiet sigh at the news of Ring Nation, an Amazon-produced TV show featuring “lighthearted viral content” captured from the Ring surveillance empire. I had clenched my jaw at a screenshot of the Stable Diffusion text-to-image model offering up AI artworks in the styles of dozens of unpaid human artists, whose collective labor had been poured into the model’s training data, ground up, and spit back out. I recognized the feeling and I knew its name. It was resignation—that feeling of being stuck in a place you don’t want to be but can’t leave. I was struck by the irony that I studied technology my whole life in order to avoid this kind of feeling. Tech used to be my happy place. Naturally, I poured my emotion into a tweetstorm: I struck a nerve. As my notifications started blowing up and thousands of replies and retweets started pouring in, the initial dopamine reward for virality gave way to a deeper sadness. A lot of people were sitting with that same heavy feeling in their stomach. Still, there was catharsis in reading so many others give voice to it. Something is missing from our lives, and from our technology. Its absence is feeding a growing unease being voiced by many who work in tech or study it. It’s what drives the new generation of PhD and postdoctoral researchers I work with at the University of Edinburgh, who are drawing together knowledge from across the technical arts, sciences, and humanistic disciplines to try to figure out what’s gone awry with our tech ecosystem and how to fix it. To do that, we have to understand how and why the priorities in that ecosystem have changed. The goal of consumer tech development used to be pretty simple: design and build something of value to people, giving them a reason to buy it. A new refrigerator is shiny, cuts down on my energy bills, makes cool-looking ice cubes. So I buy it. Done. A Roomba promises to vacuum the cat hair from under my sofa while I take a nap. Sold! But this vision of tech is increasingly outdated. It’s not enough for a refrigerator to keep food cold; today’s version offers cameras and sensors that can monitor how and what I’m eating, while the Roomba can now send a map of my house to Amazon. The issue here goes far beyond the obvious privacy risks. It’s a sea change in the entire model for innovation and the incentives that drive it. Why settle for a single profit-taking transaction for the company when you can instead design a product that will extract a monetizable data stream from every buyer, returning revenue to the company for years? Once you’ve captured that data stream, you’ll protect it, even to the disadvantage of your customer. After all, if you buy up enough of the market, you can well afford to endure your customers’ anger and frustration. Just ask Mark Zuckerberg. It’s not just consumer tech and social media platforms that have made this shift. The large ag-tech brand John Deere, for example, formerly beloved by its customers, is fighting a “right to repair” movement driven by farmers angry at being forbidden to fix their own machines, lest they disturb the proprietary software sending high-value data on the farmers’ land and crops back to the manufacturer. As more than one commenter on my Twitter thread noted, today in tech we are the product, not the prime beneficiary. The mechanical devices that used to be the product are increasingly just the middlemen. There’s also a shift in who tech innovations today are for. Several respondents objected to my thread by drawing attention to today’s vibrant market in new tech for “geeks” and “nerds”—Raspberry Pis, open-source software tools, programmable robots. As great as many of these are for those with the time, skills, and interest to put them to use, they are tools made for a narrow audience. The thrill of seeing genuine innovation in biomedical technology, such as mRNA vaccines, is likewise dampened when we see the benefits concentrated in the wealthiest countries—the ones already best served by tech. Of course, new technology remains a source of joy and excitement in many places that have historically been denied an equitable share of its comforts. But innovation used to promise us much more than new devices and apps. Engineering and inventing were once professions primarily oriented toward creating more livable infrastructure, rather than disposable stuff. Technologists’ apparent loss of interest in humane innovation is depleting our collective faith in our own powers of invention. Vital technologies like roads, power grids, sewers, and transit systems used to be a central part of the engineering enterprise in the US. Nowadays, we treat them as taxpayer burdens, and our best minds and resources are funneled instead into data-hungry consumer devices and apps. If the US is any indicator of the trajectory of global technology development, then deep trouble lies ahead for us all, because we have clearly lost the plot. The fact is, the visible focus of tech culture is no longer on expanding the frontiers of humane innovation—innovation that serves us all. Even space travel has lost its humanistic vision; today’s frontier is luxury space tourism and billionaires selling credulous investors on fantasies of escape to Mars. With 8 billion people teetering on the precipice of global environmental destruction, we can’t afford a world where the core mission of new tech appears to be “Take the money and run.” If we continue to turn away from humane applications of tech, we risk feeding a runaway feedback loop that drains our collective will to reinvest in their expansion. The danger is not only that today’s technology fails to be directed to our most urgent civilizational needs. It’s that technologists’ apparent loss of interest in humane innovation is depleting our collective faith in our own powers of invention. When it stays true to its deepest roots, technology is still driven by a moral impulse: the impulse to construct places, tools, and techniques that can help humans not only survive but flourish together. Of course, that impulse is easily joined to, or pushed aside by, others: the impulses to dominate, exterminate, immiserate, surveil, and control. But those darker motivations aren’t at the heart of our technological capacity as a species. And we can’t let them define the modern technological order. Because if technology loses its association with shared joy and comfort, we risk becoming alienated from one of the most fundamental ways we care for the world and one another. Shannon Vallor is the Baillie Gifford Professor of Ethics of Data and Artificial Intelligence at the University of Edinburgh and director of the Centre for Technomoral Futures in the Edinburgh Futures Institute.",0.0
The dark side of a super app like WeChat,https://www.technologyreview.com/2022/10/18/1061899/dark-side-super-app-wechat/,2022-10-18,"<p>When one app gives you everything, losing it becomes unbearable.</p>
","China Report is MIT Technology Review’s newsletter about what's happening in China. Sign up to receive it in your inbox every Tuesday. Welcome back to China Report! I know this is going to be a long week for all of you China watchers. New stories seem to be coming out every minute about the 20th Party Congress that started on Sunday. It’s probably why news of the banner protest in Beijing, which happened only five days ago, already feels antiquated at this point. Over the weekend, I wrote about the digital aftermath of that protest: People—some who shared photos of the protest, others who didn’t know what they did wrong—found their accounts banned from WeChat. They were desperate—publicly begging Tencent, which owns the super app, to restore their accounts and apologizing for talking about the protest at all. If you want to know more about these “confession letters” and hear from users who got banned from WeChat, you can read my story here. While I was writing it, though, I realized that one idea I take for granted might not be so intuitive for people outside China. Is a WeChat account really that valuable? Why is it so devastating when you can’t use one app? The short answer: In China, your WeChat account forms almost the entirety of your social and digital lives. One reason is that there simply aren’t many alternatives. Messenger, WhatsApp, Telegram, and Signal are all blocked. SMS messages are inundated with spam and service notifications. iMessages reach a small crowd, because only about 20% of smartphones in China are iPhones (compared with over 50% in the US). And email is basically nonexistent among the general population. No matter whether you are talking to a family member, a schoolmate, or a colleague, WeChat is the only way to go. While it started out dominating one-on-one messaging and group chats, over time WeChat incorporated all the services you would want from the internet: digital payment, shopping, streaming, networking, ride-hailing … you name it, it’s probably there. In 2017, WeChat even launched in-app “mini programs,” which basically allow you to access non-Tencent services—like Airbnb, Weibo, and office tools—without ever leaving the platform. It’s a whole operating system within an app. Sounds quite convenient, no? But it means you are never leaving the app, period. So losing a WeChat account means losing all of the above. As I wrote, it’s not easy to get your WeChat account back; it’s actually easier to regain access to these other services individually. But even then, you’re stuck rebuilding the social network you may have created over a decade on the app. “When I was adding contacts back, I was questioned if I was a scammer,” one banned user told me. Fixing these connections is arguably the hardest part. But the impact of WeChat is not just personal. WeChat is so popular and so ubiquitous that it has influenced Chinese society as a whole. Yiqin Fu, who studies political science at Stanford, tells me WeChat’s quest for a monopoly on content consumption has even shaped how knowledge creation in China works. Because people spend so many hours on WeChat every day, they often get the majority of their information from articles published in the app. But these articles are not indexed by Google-like search engines (a product decision by WeChat), meaning people are discouraged from searching for content outside the app and instead just passively consume what shows up on their timelines. This also makes it hard to find articles published not even that long ago. To use Fu’s example, she could write a blog post in English and publish it on a website, where it would receive new views years after its publication, many via search. If the same content were published in Chinese and on WeChat, it would disappear from public attention after a few days. As a result, creators, including intellectuals, are incentivized to focus exclusively on content to be consumed at the moment—short, fragmental, surface-level takes. The closed and comprehensive nature of the WeChat system is part of its secret sauce for commercial success. By keeping users trapped in a single app, WeChat makes it harder for challengers to threaten its own dominance. But this has also transformed WeChat into a dangerous tool to be wielded by those in power. The bans of users who talked about the protest in Beijing are a great example. By driving alternative communication platforms out of existence, WeChat made it easier for the government to police people’s speech through one central hub. Could the same thing happen in the US? I think it would be difficult. WeChat emerged back in 2011, before norms were set for much of the internet in China. In the US today, it would take significantly more for an app, even one as well known as Twitter or Facebook, to successfully penetrate the many different markets WeChat did. But that won’t stop companies from trying. Tech moguls like Elon Musk (surprise, surprise) often point to WeChat as a desirable vision for super apps. Yes, this kind of platform can make digital experiences more convenient for a lot of users. But the concentration of power can have many unintended, and often negative, consequences—at least for users. This is not a thought experiment; we are already witnessing it with WeChat. Proponents of the super app fantasy may say that things will be different when it’s in a democratic country. Sorry—I have to be more pessimistic than that. How would you feel about using more super apps? Let me know at zeyi@technologyreview.com. 1. When activists in China talk about #MeToo, they say “🍚🐰” or #米兔 (which translates to “rice rabbit” but sounds like “me too”) to bypass online censors. That’s just one of many clever or funny puns used for evasive purposes. (Rest of World) 2. The first casualties of Biden’s new chip export control have emerged: the American citizens and permanent residents who work for Chinese chip companies. (Nikkei Asia $) 3. Chinese state researchers have suggested forming a pan-Asian digital currency to reduce the region’s dependence on the US dollar. (South China Morning Post $) 4. How a Chinese-American gangster made money-laundering “cheap, fast, and efficient” for Mexican drug cartels. (ProPublica) 5. Despite China’s crackdown on cryptocurrencies, the domestic crypto industry persists. There was even a 400-person web3 conference in August, with events dispersed in bars and restaurants all over the city of Dali. (Bloomberg $) 6. No more human influencers: Chinese brands are now paying virtual influencers for advertising. Apparently, they’re both ageless and more controllable. (Financial Times $) 7. Syrian refugees are asking for donations on TikTok livestreams, but the platform takes a big cut of the cash gifts—as much as 70%! (BBC) 8. China is delaying the release of its third-quarter GDP data as the country’s economic growth is expected to miss its annual target (5.5%). (Quartz) “Remote internships” in China are not what they sound like. As the Chinese publication Connecting reports, Chinese college students are paying as much as $20,000 to get fake internships, where they “work” as remote, unofficial assistants to employees of top companies, just to make their résumés look better. The whole industry is built on lies: the “supervisors” are often entry-level employees who aren’t allowed to have interns or share corporate documents, and the so-called interns are given made-up tasks that can’t actually improve their skills. But as the job market gets increasingly competitive in China, these students believe that just the name of a top company on their résumés could be a game-changer to secure better jobs. And they are willing to pay a hefty fee for it. As Chinese president Xi Jinping was making his two-hour-long speech on Sunday to commence the 20th Party Congress, so many WeChat users started sharing the speech’s livestream link to their timelines that WeChat was flooded with the same red rectangular previews on repeat. Other users, less enthusiastic about the event, created a different red rectangle—adapting a popular meme where an elderly man is squinting at his phone, looking disgusted. See you next week (after the Party Congress)! Zeyi",0.0
Technology that lets us “speak” to our dead relatives has arrived. Are we ready?,https://www.technologyreview.com/2022/10/18/1061320/digital-clones-of-dead-people/,2022-10-18,"<p>Digital clones of the people we love could forever change how we grieve.</p>
","My parents don’t know that I spoke to them last night. At first, they sounded distant and tinny, as if they were huddled around a phone in a prison cell. But as we chatted, they slowly started to sound more like themselves. They told me personal stories that I’d never heard. I learned about the first (and certainly not last) time my dad got drunk. Mum talked about getting in trouble for staying out late. They gave me life advice and told me things about their childhoods, as well as my own. It was mesmerizing. “What’s the worst thing about you?” I asked Dad, since he was clearly in such a candid mood. “My worst quality is that I am a perfectionist. I can’t stand messiness and untidiness, and that always presents a challenge, especially with being married to Jane.” Then he laughed—and for a moment I forgot I wasn’t really speaking to my parents at all, but to their digital replicas. This Mum and Dad live inside an app on my phone, as voice assistants constructed by the California-based company HereAfter AI and powered by more than four hours of conversations they each had with an interviewer about their lives and memories. (For the record, Mum isn’t that untidy.) The company’s goal is to let the living communicate with the dead. I wanted to test out what it might be like. Technology like this, which lets you “talk” to people who’ve died, has been a mainstay of science fiction for decades. It’s an idea that’s been peddled by charlatans and spiritualists for centuries. But now it’s becoming a reality—and an increasingly accessible one, thanks to advances in AI and voice technology. My real, flesh-and-blood parents are still alive and well; their virtual versions were just made to help me understand the technology. But their avatars offer a glimpse at a world where it’s possible to converse with loved ones—or simulacra of them—long after they’re gone. From what I could glean over a dozen conversations with my virtually deceased parents, this really will make it easier to keep close the people we love. It’s not hard to see the appeal. People might turn to digital replicas for comfort, or to mark special milestones like anniversaries. At the same time, the technology and the world it’s enabling are, unsurprisingly, imperfect, and the ethics of creating a virtual version of someone are complex, especially if that person hasn’t been able to provide consent. For some, this tech may even be alarming, or downright creepy. I spoke to one man who’d created a virtual version of his mother, which he booted up and talked to at her own funeral. Some people argue that conversing with digital versions of lost loved ones could prolong your grief or loosen your grip on reality. And when I talked to friends about this article, some of them physically recoiled. There’s a common, deeply held belief that we mess with death at our peril. I understand these concerns. I found speaking to a virtual version of my parents uncomfortable, especially at first. Even now, it still feels slightly transgressive to speak to an artificial version of someone—especially when that someone is in your own family. But I’m only human, and those worries end up being washed away by the even scarier prospect of losing the people I love—dead and gone without a trace. If technology might help me hang onto them, is it so wrong to try? There’s something deeply human about the desire to remember the people we love who’ve passed away. We urge our loved ones to write down their memories before it’s too late. After they’re gone, we put up their photos on our walls. We visit their graves on their birthdays. We speak to them as if they were there. But the conversation has always been one-way. The idea that technology might be able to change the situation has been widely explored in ultra-dark sci-fi shows like Black Mirror—which, startups in this sector complain, everyone inevitably brings up. In one 2013 episode, a woman who loses her partner re-creates a digital version of him—initially as a chatbot, then as an almost totally convincing voice assistant, and eventually as a physical robot. Even as she builds more expansive versions of him, she becomes frustrated and disillusioned by the gaps between her memory of her partner and the shonky, flawed reality of the technology used to simulate him. If technology might help me hang onto the people I love, is it so wrong to try? “You aren’t you, are you? You’re just a few ripples of you. There’s no history to you. You’re just a performance of stuff that he performed without thinking, and it’s not enough,” she says before she consigns the robot to her attic—an embarrassing relic of her boyfriend that she’d rather not think about. Back in the real world, the technology has evolved even in the past several years to a somewhat startling degree. Rapid advances in AI have driven progress across multiple areas. Chatbots and voice assistants, like Siri and Alexa, have gone from high-tech novelties to a part of daily life for millions of people over the past decade. We have become very comfortable with the idea of talking to our devices about everything from the weather forecast to the meaning of life. Now, AI large language models (LLMs), which can ingest a few “prompt” sentences and spit out convincing text in response, promise to unlock even more powerful ways for humans to communicate with machines. LLMs have become so convincing that some (erroneously) have argued that they must be sentient. What’s more, it’s possible to tweak LLM software like OpenAI’s GPT-3 or Google’s LaMDA to make it sound more like a specific person by feeding it lots of things that person said. In one example of this, journalist Jason Fagone wrote a story for the San Francisco Chronicle last year about a thirtysomething man who uploaded old texts and Facebook messages from his deceased fiancée to create a simulated chatbot version of her, using software known as Project December that was built on GPT-3. By almost any measure, it was a success: he sought, and found, comfort in the bot. He’d been plagued with guilt and sadness in the years since she died, but as Fagone writes, “he felt like the chatbot had given him permission to move on with his life in small ways.” The man even shared snippets of his chatbot conversations on Reddit, hoping, he said, to bring attention to the tool and “help depressed survivors find some closure.” The lab trained a chatbot to learn from human feedback and search the internet for information to support its claims. At the same time, AI has progressed in its ability to mimic specific physical voices, a practice called voice cloning. It has also been getting better at injecting digital personas—whether cloned from a real person or completely artificial—with more of the qualities that make a voice sound “human.” In a poignant demonstration of how rapidly the field is progressing, Amazon shared a clip in June of a little boy listening to a passage from The Wizard of Oz read by his recently deceased grandmother. Her voice was artificially re-created using a clip of her speaking that lasted for less than a minute. As Rohit Prasad, Alexa’s senior vice president and head scientist, promised: “While AI can’t eliminate that pain of loss, it can definitely make the memories last.” My own experience with talking to the dead started thanks to pure serendipity. At the end of 2019, I saw that James Vlahos, the cofounder of HereAfter AI, would be speaking at an online conference about “virtual beings.” His company is one of a handful of startups working in the field I’ve dubbed “grief tech.” They differ in their approaches but share the same promise: to enable you to talk by video chat, text, phone, or voice assistant with a digital version of someone who is no longer alive. Intrigued by what he was promising, I wrangled an introduction and eventually persuaded Vlahos and his colleagues to let me experiment with their software on my very-much-alive parents. Initially, I thought it would be just a fun project to see what was technologically possible. Then the pandemic added some urgency to the proceedings. Images of people on ventilators, photos of rows of coffins and freshly dug graves, were splashed all over the news. I worried about my parents. I was terrified that they might die, and that with the strict restrictions on hospital visits in force at the time in the UK, I might never have the chance to say goodbye. The first step was an interview. As it turns out, to create a digital replica of someone with a good chance of seeming like a convincingly authentic representation, you need data—and lots of it. HereAfter, whose work starts with subjects when they are still alive, asks them questions for hours—about everything from their earliest memories to their first date to what they believe will happen after they die. (My parents were interviewed by a real live human, but in yet another sign of just how quickly technology is progressing, almost two years later interviews are now typically automated and handled by a bot.) As my sister and I rifled through pages of suggested questions for our parents, we were able to edit them to be more personal or pointed, and we could add some of our own: What books did they like? How did our mum muscle her way into the UK’s overwhelmingly male, privileged legal sector in the 1970s? What inspired Dad to invent the silly games he used to play with us when we were small? Whether through pandemic-induced malaise or a weary willingness to humor their younger daughter, my parents put up zero resistance. In December 2020, HereAfter’s interviewer, a friendly woman named Meredith, spoke to each of them for several hours. The company then took those responses and started stitching them together to create the voice assistants. A couple of months later, a note popped into my inbox from Vlahos. My virtual parents were ready. On one occasion, my husband mistook my testing for an actual phone call. When he realized it wasn’t, he rolled his eyes, as if I were completely deranged. This Mum and Dad arrived via email attachment. I could communicate with them through the Alexa app on a phone or an Amazon Echo device. I was eager to hear them—but I had to wait several days, because I’d promised MIT Technology Review’s podcast team that I’d record my reaction as I spoke to my parents’ avatars for the first time. When I finally opened the file, with my colleagues watching and listening on Zoom, my hands were shaking. London was in a long, cold, depressing lockdown, and I hadn’t seen my actual, real parents for six months. “Alexa, open HereAfter,” I directed. “Would you rather speak with Paul or with Jane?” a voice asked. After a bit of quick mental deliberation, I opted for my mum. A voice that was hers, but weirdly stiff and cold, spoke. “Hello, this is Jane Jee and I’m happy to tell you about my life. How are you today?” I laughed, nervously. “I’m well, thanks, Mum. How are you?” Long pause. “Good. At my end, I’m doing well.” “You sound kind of unnatural,” I said. She ignored me and carried on speaking. “Before we start, here are a few pointers. My listening skills aren’t the best, unfortunately, so you have to wait until I’ve finished talking and ask you a question before you say something back. When it’s your turn to speak, please keep your answers fairly short. A few words, a simple sentence—that type of thing,” she explained. After a bit more introduction, she concluded: “Okay, let’s get started. There’s so much to talk about. My childhood, career, and my interests. Which of those sounds best?” Scripted bits like this sounded stilted and strange, but as we moved on, with my mother recounting memories and speaking in her own words, “she” sounded far more relaxed and natural. Still, this conversation and the ones that followed were limited—when I tried asking my mum’s bot about her favorite jewelry, for instance, I got: “Sorry, I didn’t understand that. You can try asking another way, or move onto another topic.” There were also mistakes that were jarring to the point of hilarity. One day, Dad’s bot asked me how I was. I replied, “I’m feeling sad today.” He responded with a cheery, upbeat “Good!” The overall experience was undeniably weird. Every time I spoke to their virtual versions, it struck me that I could have been talking to my real parents instead. On one occasion, my husband mistook my testing out the bots for an actual phone call. When he realized it wasn’t, he rolled his eyes, tutted, and shook his head, as if I were completely deranged. Earlier this year, I got a demo of a similar technology from a five-year-old startup called StoryFile, which promises to take things to the next level. Its Life service records responses on video rather than just voice alone. You can pick from hundreds of questions for the subject. Then you record the person answering the questions; this can be done on any device with a camera and a microphone, including a smartphone, though the higher-quality the recording, the better the outcome. After uploading the files, the company turns them into a digital version of the person you can see and speak to. It can only answer the questions it’s been programmed to answer—much like HereAfter, just with video. StoryFile’s CEO, Stephen Smith, demonstrated the technology on a video call, where we were joined by his mother. She died earlier this year, but here she was on the call, sitting in a comfortable chair in her living room. For a brief time, I could only see her, shared via Smith’s screen. She was soft-spoken, with wispy hair and friendly eyes. She dispensed life advice. She seemed wise. Smith told me that his mother “attended” her own funeral: “At the end she said, ‘I guess that’s it from me … goodbye!’ and everyone burst into tears.” He told me her digital participation was well received by family and friends. And, arguably most important of all, Smith said he’s deeply comforted by the fact that he managed to capture his mother on camera before she passed away. The video technology itself looked relatively slick and professional—though the result still fell vaguely within the uncanny valley, especially in the facial expressions. At points, much as with my own parents, I had to remind myself that she wasn’t really there. Both HereAfter and StoryFile aim to preserve someone’s life story rather than allowing you to have a full, new conversation with the bot each time. This is one of the major limitations of many current offerings in grief tech: they’re generic. These replicas may sound like someone you love, but they know nothing about you. Anyone can talk to them, and they’ll reply in the same tone. And the replies to a given question are the same every time you ask. “The biggest issue with the [existing] technology is the idea you can generate a single universal person,” says Justin Harrison, founder of a soon-to-launch service called You, Only Virtual. “But the way we experience people is unique to us.” Language models are mindless mimics that do not understand what they are saying—so why do we pretend they’re experts? You, Only Virtual and a few other startups want to go further, arguing that recounting memories won’t capture the fundamental essence of a relationship between two people. Harrison wants to create a personalized bot that’s for you and you alone. The first incarnation of the service, which is set to launch in early 2023, will allow people to build a bot by uploading someone’s text messages, emails, and voice conversations. Ultimately, Harrison hopes, people will feed it data as they go; the company is currently building a communication platform that customers will be able to use to message and talk with loved ones while they’re still alive. That way, all the data will be readily available to be turned into a bot once they’re not. That is exactly what Harrison has done with his mother, Melodi, who has stage 4 cancer: “I built it by hand using five years of my messages with her. It took 12 hours to export, and it runs to thousands of pages,” he says of his chatbot. Harrison says the interactions he has with the bot are more meaningful to him than if it were simply regurgitating memories. Bot Melodi uses the phrases his mother uses and replies to him in the way she’d reply—calling him “honey,” using the emojis she’d use and the same quirks of spelling. He won’t be able to ask Melodi’s avatar questions about her life, but that doesn’t bother him. The point, for him, is to capture the way someone communicates. “Just recounting memories has little to do with the essence of a relationship,” he says. Avatars that people feel a deep personal connection with can have staying power. In 2016, entrepreneur Eugenia Kuyda built what is thought to be the first bot of this kind after her friend Roman died, using her text conversations with him. (She later founded a startup called Replika, which creates virtual companions not based on real people.) She found it a hugely helpful way to process her grief, and she still speaks to Roman’s bot today, she says, especially around his birthday and the anniversary of his passing. But she warns that users need to be careful not to think this technology is re-creating or even preserving people. “I didn’t want to bring back his clone, but his memory,” she says. The intention was to “create a digital monument where you can interact with that person, not in order to pretend they’re alive, but to hear about them, remember how they were, and be inspired by them again.” Some people find that hearing the voices of their loved ones after they’ve gone helps with the grieving process. It’s not uncommon for people to listen to voicemails from someone who has died, for example, says Erin Thompson, a clinical psychologist who specializes in grief. A virtual avatar that you can have more of a conversation with could be a valuable, healthy way to stay connected to someone you loved and lost, she says. But Thompson and others echo Kuyda’s warning: it’s possible to put too much weight on the technology. A grieving person needs to remember that these bots can only ever capture a small sliver of someone. They are not sentient, and they will not replace healthy, functional human relationships. People may find any reminders of the deceased person triggering: “In the acute phase of grief, you can get a strong sense of unreality, not being able to accept they’re gone.” “Your parents are not really there. You’re talking to them, but it’s not really them,” says Erica Stonestreet, an associate professor of philosophy at the College of Saint Benedict & Saint John’s University, who studies personhood and identity. Particularly in the first weeks and months after a loved one dies, people struggle to accept the loss and may find any reminders of the person triggering. “In the acute phase of grief, you can get a strong sense of unreality, not being able to accept they’re gone,” Thompson says. There’s a risk that this sort of intense grief could intersect with, or even cause, mental illness, especially if it’s constantly being fueled and prolonged by reminders of the person who’s passed away. Arguably, this risk might be small today given these technologies’ flaws. Even though sometimes I fell for the illusion, it was clear my parent bots were not in fact the real deal. But the risk that people might fall too deeply for the phantom of personhood will surely grow as the technology improves. And there are still other risks. Any service that allows you to create a digital replica of someone without their participation raises some complex ethical issues regarding consent and privacy. While some might argue that permission is less important with someone no longer alive, can’t you also argue that the person who generated the other side of the conversation should have a say too? And what if that person is not, in fact, dead? There’s little to stop people from using grief tech to create virtual versions of living people without their consent—for example, an ex. Companies that sell services powered by past messages are aware of this possibility and say they will delete a person’s data if that individual requests it. But companies are not obliged to do any checks to make sure their technology is being limited to people who have consented or died. There’s no law to stop anyone from creating avatars of other people, and good luck explaining it to your local police department. Imagine how you’d feel if you learned there was a virtual version of you out there, somewhere, under somebody else’s control. If digital replicas become mainstream, there will inevitably need to be new processes and norms around the legacies we leave behind online. And if we’ve learned anything from the history of technological development, we’ll be better off if we grapple with the possibility of these replicas’ misuse before, not after, they reach mass adoption. Will that ever happen, though? You, Only Virtual uses the tagline “Never Have to Say Goodbye”—but it’s not actually clear how many people want or are ready for a world like that. Grieving for those who’ve passed away is, for most people, one of the few aspects of life still largely untouched by modern technology. On a more mundane level, the costs could be a drawback. Although some of these services have free versions, they can easily run into the hundreds if not thousands of dollars. HereAfter’s top-tier unlimited version lets you record as many conversations with the subject as you like, and it costs $8.99 a month. That may sound cheaper than StoryFile’s one-off $499 payment to access its premium, unlimited package of services. However, at $108 per year, HereAfter services could quickly add up if you do some ghoulish back-of-the-envelope math on lifetime costs. It’s a similar situation with You, Only Virtual, which is slated to cost somewhere between $9.99 and $19.99 a month when it launches. Creating an avatar or chatbot of someone also requires time and effort, not least of which is just building up the energy and motivation to get started. This is true both for the user and for the subject, who may be nearing death and whose active participation may be required. Fundamentally, people don’t like grappling with the fact they are going to die, says Marius Ursache, who launched a company called Eternime in 2014. Its idea was to create a sort of Tamagotchi that people could train while they were alive to preserve a digital version of themselves. It received a huge surge of interest from people around the world, but few went on to adopt it. The company shuttered in 2018 after failing to pick up enough users. “It’s something you can put off until next week, next month, next year,” he says. “People assume that AI is the key to breaking this. But really, it’s human behavior.” Kuyda agrees: “People are extremely scared of death. They don’t want to talk about it or touch it. When you take a stick and start poking, it freaks them out. They’d rather pretend it doesn’t exist.” Ursache tried a low-tech approach on his own parents, giving them a notebook and pens on his birthday and asking them to write down their memories and life stories. His mother wrote two pages, but his father said he’d been too busy. In the end, he asked if he could record some conversations with them, but they never managed to get around to it. “My dad passed away last year, and I never did those recordings, and now I feel like an idiot,” he says. Personally, I have mixed feelings about my experiment. I’m glad to have these virtual, audio versions of my mum and dad, even if they’re imperfect. They’ve enabled me to learn new things about my parents, and it’s comforting to think that those bots will be there even when they aren’t. I’m already thinking about who else I might want to capture digitally—my husband (who will probably roll his eyes again), my sister, maybe even my friends. On the other hand, like a lot of people, I don’t want to think about what will happen when the people I love die. It’s uncomfortable, and many people reflexively flinch when I mention my morbid project. And I can’t help but find it sad that it took a stranger Zoom-interviewing my parents from another continent for me to properly appreciate the multifaceted, complex people they are. But I feel lucky to have had the chance to grasp that—and to still have the precious opportunity to spend more time with them, and learn more about them, face to face, no technology involved.",0.0
WeChat users are begging Tencent to give their accounts back after talking about a Beijing protest,https://www.technologyreview.com/2022/10/16/1061713/wechat-accounts-begging-tencent-beijing-protest/,2022-10-16,"<p>The super app banned users who shared images of a rare pro-democracy demonstration. Now they’re posting “confession letters” in a desperate hope to regain access.</p>
","On Weibo, the popular Chinese social media platform, hundreds of desperate users were writing “confession letters” this past week. “I have been in a terrible mental state due to the massive pressure from recent pandemic prevention measures. I lost my control, and sent sensitive statements in a group chat with six people,” one user wrote. “I have profoundly realized my mistake. I hope Tencent can give me a chance to start with a clean slate. I won’t let down the party and the country.” The message was posted with a special hashtag for “Tencent Customer Service.” Messages like this, which surged on Thursday, vary in substance but share urgent pleas from users who have been banned from the Tencent-owned super app WeChat—begging company representatives to restore their social accounts on a service that has become an almost indispensable part of life in China. While the hashtags themselves aren’t new, they were flooded late in the week after WeChat reportedly banned a large number of users. Those affected believe it was because they had discussed a rare political protest in Beijing. It all started on the afternoon of October 13, two days before the high-profile 20th Communist Party Congress, when a protester hung banners on an overpass in the capital city that called for removing pandemic control measures and instating democratic reforms. “Say no to Covid test, yes to food. No to lockdown, yes to freedom,” part of one banner reads. “Go on strike, remove the dictator and national traitor Xi Jinping,” reads another. The timing right before the party congress—as well as the highly sensitive act of mentioning the name of the Chinese president, who is expected to clinch an unprecedented third term at the meeting—has made discussion of the event tightly censored on Chinese social media. On Weibo, any user content that includes words like “Beijing,” “bridge,” and “brave” is restricted from being searched. Apple Music’s Chinese version removed a song named “Sitong Bridge,” presumably only because the name refers to the place where the protest happened. This censorship extends to WeChat, the dominant messaging app with over 1.2 billion global users, the majority of whom live in China. Users soon realized that just posting a picture of the event, even in a private group chat, could cause their accounts to be permanently banned. Chen, a Beijing resident who asked to be identified only by his last name, says he sent a photo of the protest to a group chat at 1:11 p.m Beijing time on Thursday, and his account was permanently banned at 5:35 p.m. The decision was made “according to the relevant Internet policies as well as laws and regulations,” the boilerplate notification from WeChat reads. Another member of the group chat also tried to send the photo; his account was also banned, Chen later learned. “We knew it would cause a suspension, but we didn’t expect it to be permanent. I thought at the time I would only be banned for a few days,” he says. Though there’s no official number on how many accounts were banned that day, there are numerous reports across Weibo and other social media platforms of WeChat users’ losing their accounts since the protest, some not knowing what they did wrong. Tencent didn’t respond to a request for comment from MIT Technology Review. Being banned from WeChat isn’t exactly a trivial matter. It has a significant practical impact on individuals, as they are now blocked from using the many digital services tied to their accounts, from health QR codes to online subscriptions. It takes days, if not weeks, to reestablish their digital connections with a new account. The mass suspension also has an effect on society as a whole: the latest example of how swiftly China’s censorship machine works to silence dissent will only further chill these voices in the future. Protests like the latest one are already rare in China today, and many people won’t ever learn it happened. In China, the government holds social media platforms responsible for closely screening user-generated content. A 2017 regulation from China’s Cyberspace Administration targets online group chats and prohibits both platforms and users from “spreading content forbidden by laws and relevant regulations.” In 2021, popular social media platforms Douban and Weibo were publicly fined millions of yuan for allowing “illegal” content to be seen on their platforms. WeChat’s terms and conditions has a lengthy section on what content is banned, but it’s only specific when illustrating examples involving scams, spam, rumors, gambling, or pornography. It doesn’t give any examples of the politically sensitive content it censors. After a writer was locked out of her novel for including illegal content, Chinese web users are asking questions about just how far the state’s censorship reaches. Still, it’s well known that sending politically sensitive content on WeChat, even in private exchanges, can result in account suspension. Similar waves of mass suspension also happened during other online protests, such as when people were criticizing the inept government response to omicron flare-ups in Shanghai in April. In 2019, the Toronto-based research group Citizen Lab found that WeChat imposes real-time automatic censorship of chat images through a mix of text recognition, visual recognition, and tools for detecting duplicate files. One of the results is that once the system identifies an image as subject to restriction, it can immediately block all users from sending that image. This has sometimes driven users to get creative, using puns, distorted images, or hard-to-understand languages to mask what they want to share. The real-time censorship system seems to have been applied to images from Sitong Bridge. Tina, a 38-year-old Beijing resident, sent a photo of the protest to a small group chat on Thursday, though she suspects the image didn’t even reach the other members. She later checked with her boyfriend, who is also in the group, and confirmed that he couldn’t see any photo of the protest. Nevertheless, Tina’s WeChat account was permanently banned hours later. (Tina asked to be identified by her English name to protect her identity.) Once a WeChat account is permanently banned, there aren’t many ways to appeal the decision. Calling the designated customer service hotline usually just gets you hours of wait time; the in-app appeals process returns a generic response concluding that “the restrictions cannot be removed.” So over the past few years, users who are desperate to get their accounts back have turned to other social media platforms where Tencent has corporate accounts. On Weibo, there were two “super topics,” a Weibo community feature that builds on a specific hashtag: “腾讯客服 (Tencent customer service)” and “腾讯人工客服 (Tencent human customer service).” Together, the two topics had over 130,000 posts where users asked, begged, or condemned Tencent in hopes it would give their accounts back. These posts dated back to 2017, the year after Weibo introduced the super topic feature, but more than half were published in 2022. There was one account that had persistently written to Tencent almost every day since July 26. Use of the super topics spiked before Weibo removed both topics on the morning of October 14, just a day after the protest. MIT Technology Review examined and archived some of the posts before they were taken down. Weibo didn’t respond to a request for comment. After Weibo removed the two super topics, some users moved to a different super topic, “Tencent,” or resorted to simply tagging Tencent’s corporate accounts. The surge of posts on Thursday attracted attention from other Weibo users, who named the missives “cyber confessionals,” since users often lay out what they think they did wrong in order to ask Tencent for a second chance; WeChat users are usually not given a detailed explanation for their ban. But some of the users knew, or suspected, that their account ban was triggered by posting politically sensitive content. “I called the customer service phone number but they didn’t tell me what violated the rules. After self-examination, I found it was because I reposted improper photos,” reads one of the posts on Weibo. Others were less clear about what happened, yet ready to admit mistakes anyway. “I personally don’t think I’ve sent any harmful information, but if it really was because I did, I’m very sorry and will be cautious with my words and actions in the future,” reads another. Not all people who posted under these hashtags were banned for political censorship. Some say they spammed too many people or promoted counterfeit products; others had no clue about what happened. What most of the posts share is a sense of desperation. Since WeChat has come to be used in almost all aspects of life, having your primary account banned can be devastating. The Weibo posts describe how having their WeChat accounts banned made it difficult for people to get messages from colleagues, potential employers, or family members. Some write they are now on the brink of depression. Meanwhile, Tencent’s customer service Weibo accounts only posted robotic responses under these posts asking them to provide more information. Two Weibo users told MIT Technology Review that posting under the hashtag didn’t help their appeals process at all. Being banned from WeChat turns you into a ghost on the ubiquitous platform. “After losing WeChat, it feels like you lost connection to the world,” says Chen. “Even though you can still log into your WeChat account, read the messages others sent you and the group messages, and make digital payments, you can’t interact with them or reply to them.” WeChat started allowing banned users to export their contacts in 2020, so if they choose to register a new account and start over, they can add their friends back one by one. But for most WeChat users who have had the app for over a decade, this means adding thousands of contacts manually and explaining to them what they did to trigger the ban. Chen used his old account for 11 years and had over 1,400 contacts. It took him several hours to add back 500 contacts from his back-up account. “When I was adding contacts back, I was questioned if I was a scammer and the person called me to confirm. If I don’t have this person’s number or other confirmation methods, maybe they will straightaway refuse to befriend me,” Chen says. Then there are the subscriptions, the bookmarked content, the public accounts he follows, and all other information tied to his WeChat account. He needs to migrate all that too. On Friday, after the discussion of the protest had ebbed, many WeChat users were discovering who among their friends were banned or helping their friends spread their new WeChat handles. A 2020 article that offered a helpful checklist on what to do after being banned by WeChat gained at least 70,000 views overnight. News of the suspensions obviously had a chilling effect too, as people weighed whether to talk about the protest when it was now clear it could get their accounts banned. By holding people’s access to digital services hostage, the government was able to obstruct the spread of information and increase its control. Not everyone is willing to become a hostage. While Tina has heard about the posts on Weibo begging Tencent for help, that’s not what she wants to do. She understands the severity of political censorship and doesn't believe posting will help. So far, she has only told her close contacts about what happened and plans to try living her life without a WeChat account, at least for a little while. She has always felt she spent too much time on social media apps anyway; maybe this forced leave could be a detox experience. “Many people were registering their second accounts yesterday. But I told them I won’t. I want to give it a try. If, let’s say, I can still live my life normally without WeChat, I think I can choose not to register another account,” she says. “I don’t think an individual should be bound so close with [WeChat] together.”",0.0
Meta is desperately trying to make the metaverse happen,https://www.technologyreview.com/2022/10/11/1061144/metaverse-announcements-meta-connect-legs/,2022-10-11,"<p>Will web access and avatar legs be enough?</p>
","The star of Tuesday’s Meta Connect, the so-called “state of the union” for the company formerly known as Facebook, was Meta Quest Pro. Meta’s newest virtual-reality headset clocks in at a whopping $1,499.99. That’s a significant price jump from its previous iteration, Meta Quest 2, which could be yours for $399.99—not exactly cheap, but still in triple-digit territory. That price hike, coupled with Meta’s insistence throughout the virtual event that the company envisioned the metaverse as a “next-generation social platform” accessible to everyone, sort of feels like a blatant contradiction. Even if you are among the lucky few who can shell out a grand and a half for a virtual-reality headset, would you really want to? That’s the question Meta seems to be grappling with. While the headset price jumped, nearly all the company’s other big moves are aimed at a common and simple baseline: making the metaverse something people actually want to use. Meta’s metaverse hasn’t exactly had a smooth year. Less than a year ago, founder Mark Zuckerberg rebranded what was then Facebook in an effort to show that the company was pivoting to what he believed was the future of our digital lives. Since then, Meta has been saddled with hiccups and gaffes, including a much-ballyhooed avatar of Zuckerberg that got memed to oblivion, a report suggesting that the company’s employees were less than enthused about the metaverse, and allegations of virtual sexual assault. A woman was sexually harassed on Meta’s VR social media platform. She’s not the first—and won’t be the last. So its current strategy seems to be to release a string of updates to see what might get people interested—a “throw spaghetti at the wall and see what sticks” approach, if you will. Besides the Meta Quest Pro, the company also announced at the event that it was going to open up Horizon Worlds, the social media platform within Meta’s metaverse, to mobile and desktop users, so people without a headset will be able to access the virtual world. That’s a notable step: it’s a tacit admission that VR headsets aren’t taking off as quickly as the company would like. Without a critical mass of people who understand what the metaverse feels like or even is, Meta can’t hope to have its products adopted. Opening its virtual worlds to the formats consumers are comfortable with (their text messages, their browsers, the company’s beleaguered Instagram platform) gives people who aren’t open to shelling out $399.99—much less $1,499.99—a way to experience the new world. What’s also made the metaverse a hard sell is the disorienting experience of being a floating, legless torso, and Meta announced that it won’t be that way anymore. Previously, Andrew Bosworth, Meta’s CTO, said in an Instagram AMA that full-body avatars were difficult to implement, particularly because VR tracking usually comes from someone’s real-life eyes and hands. “Tracking your own legs accurately is super hard and basically not workable just from a physics standpoint with existing headsets,” he said in February. ""legs are hard"" says Meta's Mark Zuckerberg of avatars... But they're on the case... pic.twitter.com/mwSDagmrGm But Zuckerberg (or, rather, his leggy avatar) announced at the event that the company was going to use artificial intelligence to map out legs in the metaverse, allowing avatars the ability not only to walk and run but also to wear digital clothing for their legs (a marketplace that Zuckerberg has said he is eager to participate in; Roblox, a gaming platform I’ve written about before, currently has a comfortable share of the market). This would be a huge step to improving how users think about movement in the metaverse and how they decide to represent themselves there. But even with legs, and even with the ability to roam the metaverse without a headset strapped to your face, the key question remains: Is Meta’s metaverse something people will actually buy into? It’s worth noting that even employees at Meta are skeptical about the company’s vision, with one going so far as to say the amount spent on these projects to date made him “sick to [his] stomach.” A free, shareable version of the metaverse accessible via weblink will open the previously closed world up to people who may not have hundreds of dollars to burn, and it’s a huge move toward democratizing the space. It might lead people to buy Meta’s claim that talking to a cartoon version of your boss is totally cool—and, more broadly, that the metaverse really is the next digital plane on which we’ll conduct our lives. But it might also do the opposite: people might hop on the link and find that even in its now full-bodied state, the metaverse, er, doesn’t have legs.",0.0
The mothers of Mexico’s missing use social media to search for mass graves,https://www.technologyreview.com/2022/10/11/1061028/the-mothers-of-mexicos-missing-use-social-media-to-search-for-mass-graves/,2022-10-11,"<p>Large numbers of people still disappear each year in Mexico. Families have turned to social media to hunt for them.</p>
","“Attention Jalisco. Do you know the location of a clandestine grave of corpses?” The question, aimed at people in the populous Mexican state, was posed on Twitter in February by Madres Buscadoras de Sonora, an organization of mothers searching for their missing loved ones. Dozens of people responded. Neighbors who had witnessed clandestine burials came forward, annotating Google Maps to point toward graves beneath vacant houses. Messages also came from “los arrepentidos,” the penitents—people who said they’d been involved in the burials and were moved to confess. Carrying picks and shovels, women from Madres Buscadoras arrived in the Chulavista neighborhood in the Jalisco municipality of Tlajomulco de Zúñiga. They began to dig. Mexico has long struggled with a history of kidnapping. As of October 5, there were 105,984 people officially listed as disappeared in Mexico. More than a third have vanished in the past few years, during the current government of President Andrés Manuel López Obrador, commonly known as AMLO. Many of those missing are thought to have been kidnapped or forcibly recruited by criminal organizations. Most are likely dead, their remains buried in clandestine graves in rural areas, neighborhoods, and farmlands or scattered in the vast unoccupied terrain near the US-Mexico border. Some may be among the more than 52,000 unidentified bodies lying in morgues, common graves, and universities. Around a quarter are women and girls, most likely victims of sexual exploitation, human trafficking, or femicide. In a departure from previous administrations, AMLO’s government has publicly recognized the scale of the crisis and strengthened search and identification efforts. In March 2019, it inaugurated the National Search System, a mechanism that seeks to coordinate efforts among government agencies in the search for the disappeared. When the system launched, Karla Quintana, the head of the National Search Commission, recognized the work families have been doing on the ground, “virtually alone for years.” She promised: “Never again alone.” But authorities are still hesitant to get involved in the search for the missing. And so the task continues to fall on families. Much of the work they do now happens over social media, where people widely distribute photographs of missing relatives, coordinate search efforts, and raise awareness of the problem. Through WhatsApp, Twitter, and Facebook, Madres Buscadoras has created a platform to engage citizens and work to accelerate the search for the disappeared. Every day, the group receives information about missing people and the location of possible graves—so many that they do not have the resources to investigate them all. The work is not without challenges. When Madres Buscadoras began searching for bodies in Chulavista, they were closely watched—and photographed—by local authorities. After the collective met with the governor of Jalisco, local police joined the search the following day. Ultimately, Madres Buscadoras uncovered 221 jumbo trash bags of body parts. By April, the prosecutor’s office said the official tally was up to 44 bodies, with bags still to process. Families who conduct their own investigations can face opposition and threats both from organized crime and from government officials, who may collude with organized crime groups and may not like the optics of a hunt for missing people in their region. Under the country’s landmark General Law of Forced Disappearance, which was approved in 2017 after pressure from families, authorities must take immediate steps to search for a missing person and investigate the crime, yet this is still not the reality for thousands of families. “Although things have changed a little, it has always been the same situation on the part of the authorities. They shake things off, saying ‘It’s not up to us, it’s up to others,’” says Martín Villalobos, a member of the National Citizen Council, a consultative body of the National Search System. But other parts of Mexican society are now responding to the plight of the families. “Social networks work very well here. People have been supporting us a lot even though they don’t have disappeared relatives,” says Araceli Hernández, who used to be a member of the main Madres Buscadoras group but recently formed a new collective. “The simple fact of listening to the pain of a mother, an aunt, makes them support us with tools, groceries, water, Gatorades, and tons of information. That makes us hold on tighter.” Gathering together On October 30, 2015, Ceci Flores’s then 21-year-old son, Alejandro Guadalupe, disappeared without a trace in the city of Los Mochis in northern Sinaloa. Less than four years later, on May 4, 2019, hit men kidnapped her other two sons—Marco Antonio and Jesús Adrián—in her home state of Sonora. Flores approached the authorities. But when she didn’t see an immediate response, she launched her own search, broadcasting live on social media. Six days later, she received a call saying her sons would be freed at a specific location. When she arrived, only her youngest, Jesús Adrián, was there. To date, Flores hasn’t heard from Marco Antonio. The same day she recovered Jesús Adrián, Flores created a Twitter account and began to share photos of her still-missing sons. That was the genesis of Madres Buscadoras. Since then, more than 2,000 mothers and relatives of missing people have joined. “I didn’t have the support of the authorities in the search for my children. So I started these searches, inviting families. Every day more families joined me, because there were many disappeared,” she says. The effort is not without danger. In July last year, 28-year-old buscadora Aranza Ramos was abducted from her home and fatally shot. Ramos had been searching for her husband, Brayan Omar Celaya Alvarado, since he went missing in December 2020. Flores received death threats after Ramos was killed, forcing her to leave her hometown in Sonora. Madres Buscadoras has issued a public plea to organized crime groups to allow them to do their work. But there is a general climate of hostility around the search for the missing, says Vladimir Cortés, digital rights program officer at the Mexico and Central America office of Article 19, a nonprofit advocacy group defending freedom of expression. “We are facing a government that denies that there are human rights violations, that denies that there are abuses against those who defend human rights,” says Cortés. “This generates a permissiveness to attack those who are defending human rights —relatives of disappeared persons—and try to cancel the work they do.” To report or not to report When psychology student Mónica Alejandrina Ramírez Alvarado disappeared, months before she was due to graduate from the National Autonomous University of Mexico in 2004, social media wasn’t as pervasive as it is today. There was still no national movement for the disappeared, nor a General Law of Forced Disappearance. But families of the disappeared still face some of the same challenges, explains Villalobos, who is Mónica Alejandrina’s brother-in-law. “Most of the complaints have to be made at the local level, and the local level is always co-opted or is part of organized crime,” Villalobos says. “When filing a report, families are defenseless against the attacks of both criminals and authorities, and are also isolated from their own family—who, for fear of being related to the threats or revenge, move away from them.” María de la Luz López Castruita remembers seeing mothers pasting missing person posters with their daughter’s faces in public places in the northern city of Torreón in the late 2000s, never thinking it would happen to her. “Obviously there were no social networks back then, but there were other ways to support,” López says. She could have helped paste posters or joined the search. “But no,” she says. “We stayed with our arms crossed waiting for it to happen to us, and it did happen to us.” After the disappearance of her then 17-year-old daughter, Irma Claribel, in 2008, López and her husband, Jesús Lamas, began to paste Irma’s photo on every possible lamppost, hoping that someone would give them information. They got two types of response: calls asking for money, from people claiming to know Irma’s whereabouts, and misogynistic swear words scratched on the posters. Educators who stand up to conservative activists are being harassed and called “groomers” online, turning them into potential targets for real-world violence. Taking to social media was a logical next step for families of the missing. There was a marked rise in that activity after the onset of the pandemic, Villalobos says, when authorities declared it too risky to do fieldwork. “Many of us who belong to generations that perhaps were not very familiar with technology and social media have seen the opportunity to make visible our realities using our own voices,” he says. But in some ways technology hasn’t made things easier, Villalobos notes; users are trolled by bots, fake accounts, and AMLO supporters. He says government supporters often accuse families of receiving financial support from opposition parties or foreign institutions such as the US Agency for International Development and the National Endowment for Democracy, groups that have a long history of funding nongovernmental organizations in Mexico. There should be more training on social media safety, Cortés says—and a clear mechanism for legal recourse when people are threatened online. At the moment, it is a felony under federal law to threaten a person online, but Cortés says authorities often fail to investigate and prosecute such cases. Off the radar Around noon on Sunday, April 10, López received a phone call. A man told her he’d found human remains a few months back, while pulling up candelilla plants in the desert. López and her husband were exhausted. They were on the bus, riding back to their home in Torreón after spending two weeks leading a search brigade of more than 150 relatives of missing people in the nearby city of Monterrey. But they couldn’t afford to say no. Any piece of information could be a lead in their search for their daughter Irma Claribel. They called their son Jesús and asked him to pick them up in a town near the highway. They hopped in Jesús’s truck, picked up the guide, and began a one-and-a-half-hour journey toward desert too rugged for the truck to traverse. Soon it was just López and Lamas following the guide on foot, hoping they would find human remains. “Even if it’s not my daughter, it can be someone else,” says López. Authorities don’t want the media attention, she explains. They often step in to help search only after a massive discovery of human remains, one that would look bad for them to ignore. After five hours of walking, it was starting to get dark and the place where the remains were supposedly found still looked distant. A recent leg injury started to slow down Lamas, who is 66. They realized they wouldn’t make it to the site. The walk back to the truck took until midnight. López and Lamas were scared. They arrived at their house around three in the morning, feeling deflated and exhausted. Searches like this one, away from the media and authorities, are common. Bureaucracy and the extensiveness of the territory have forced mothers like López to carry out their own searches without security or resources. “I have to be filling out papers and papers to make the authorities do the work,” López says. “But I don't want them to make the process long. So when I find something, I talk to them and tell them: Here! Here it is.” López says that she still does not know how to make a case, or search brigade, go viral. Authorities don’t want the media attention, she explains. They often step in to help search only after a massive discovery of human remains, one that would look bad for them to ignore. The internet has allowed families to bypass the filters of the government and the media. “That is why social media is very useful for us: to make visible what we are experiencing,” says Flores. “This is not fiction—it’s a reality we live every day. Every day dead people appear. Every day people disappear. Every day we mothers are left without children.” Chantal Flores is a freelance journalist based in Monterrey, Mexico.",0.0
The porcelain challenge didn’t need to be real to get views,https://www.technologyreview.com/2022/10/05/1060746/porcelain-challenge-fake-views-moral-panic/,2022-10-05,"<p>Sebastian Durfee created a fake teen challenge—and got banned from TikTok. It kind of proved his point about online moral panics.</p>
","Despite what you may have heard, the teens are not stealing their family’s fine dinnerware, tossing it in a blender, and snorting the resulting dust for the “porcelain challenge.” That’s just what Sebastian Durfee, a 23-year-old actor and TikTok creator, hoped you might believe when he spread the word on social media of the latest dangerous teen challenge. Never mind that it was all fake from the start. On Saturday, Durfee posted a call to action to his followers: to work together to get “boomers to freak out about a fake TikTok challenge.” He chose the porcelain challenge—which, once again, is just a thing Durfee made up—because it seemed like something that would be plausibly dangerous, but not something “the average person could go off and do very easily,” he told me this week. Besides, it’s a catchy name. His original video quickly passed half a million views, and TikTok slapped a warning on it for promoting dangerous acts. Meanwhile on TikTok, Facebook, and Twitter, #PorcelainChallenge videos created by those in on the joke started asking people to spread warnings and share stories of (fake) injuries and deaths from those who tried it. One particularly clever video takes the bit to the next level, with the creator claiming that videos of people trying the challenge are being removed by TikTok moderators immediately—a way to explain why there’s no available video of any person actually doing this. The platform announced it’ll share revenue with makers of short-form videos. It wouldn’t tell us what portion of ad revenue is up for grabs. Shortly after we spoke on Monday, Durfee sent me an update: TikTok had permanently banned his account (which had 150,000 followers), apparently as a result of this experiment. The company didn’t give him a reason for the permanent removal, but he said that they did take down two videos for “promoting dangerous behavior”: one in which he told viewers to post warnings about the porcelain challenge in local Facebook groups, and another in which he shared a screenshot of some early media coverage of his work. TikTok confirmed on Wednesday that it had banned Durfee’s account, and said that it viewed any content, including hoaxes, that promotes dangerous behavior as a violation of their community guidelines. “It goes without saying that neither these videos nor any other ones I created glorified, endorsed, or depicted performing the challenge,” he emailed me on Wednesday. “The fact that they reacted to the challenge as if it was real by banning me entirely is the sort of knee-jerk widespread panic the challenge was meant to critique in the first place; the irony here is not lost on me.” Durfee’s goal was to get views, which he got in spades before his account was banned. It was also to examine how attention and outrage work online. If a content creator performs all the parts of a moral panic, will the fact that the challenge itself is a complete fiction actually change anything about its spread? I’ve reported on moral panics about The Children many times over the years. Right now, it’s the season when people annually freak out about the possibility of THC-laced candies in their kid’s trick-or-treat stash. This fear, along with the many other warnings that deadly candies might be handed to kids by the sadist next door, have thrived every fall for decades merely on possibilities and what-ifs: dig into the “proof” cited by those pushing these warnings and you’ll find that it doesn’t stand up. It doesn’t need to. Social media often works by reflex. Content that does well practically begs to be shared right away, reality be damned. And in the case of teen challenges and dangers to children, those warnings are often passed along by sources that carry some authority in their communities: the Facebook pages of local law enforcement, local media, or school officials. “I’ve dabbled in the past with trying to make fake news that is transparent about being fake but spreads nonetheless,” Durfee said. (He once, with a surprising amount of success, got a false rumor started that longtime YouTuber Hank Green had been arrested as a teenager for trying to steal a lemur from a zoo.) On Sunday, Durfee and his friends watched as #PorcelainChallenge gained traction, and they celebrated when it generated its first media headline (“TikTok’s porcelain challenge is not real but it’s not something to joke about either”). A steady parade of other headlines, some more credulous than others, followed. But reflex-dependent viral content has a short life span. When Durfee and I chatted three days after he posted his first video about the porcelain challenge, he already could tell that it wasn’t going to catch as widely as he’d hoped. RIP. Nevertheless, viral moments can be reanimated with just the slightest touch of attention, becoming an undead trend ambling through Facebook news feeds and panicked parent groups. Stripping away their original context can only make them more powerful. And dubious claims about viral teen challenges are often these sorts of zombies—sometimes giving them a second life that’s much bigger (and arguably more dangerous) than the first. For every “cinnamon challenge” (a real early-2010s viral challenge that made the YouTube rounds and put participants at risk for some nasty health complications), there are even more dumb ideas on the internet that do not trend until someone with a large audience of parents freaks out about them. Just a couple of weeks ago, for instance, the US Food and Drug Administration issued a warning about boiling chicken in NyQuil, prompting a panic over a craze that would endanger Gen Z lives in the name of views. Instead, as Buzzfeed News reported, the warning itself was the most viral thing about NyQuil chicken, spiking interest in a “trend” that was not trending. And in 2018, there was the “condom challenge,” which gained widespread media coverage as the latest life-threatening thing teens were doing online for attention—“uncovered” because a local news station sat in on a presentation at a Texas school on the dangers teens face. In reality, the condom challenge had a few minor blips of interest online in 2007 and 2013, but videos of people actually trying to snort a condom up their nose were sparse. In each case, the fear of teens flocking en masse to take part in a dangerous challenge did more to amplify it to a much larger audience than the challenge was able to do on its own. The porcelain challenge has all the elements of future zombie content. Its catchy name stands out like a bite on the arm. The posts and videos seeded across social media by Durfee’s followers—and the secondary audience coming across the work of those Durfee deputized—are plausible and context-free. Figuring out social media platforms’ hidden rules is hard work—and it falls more heavily on creators from marginalized backgrounds. But in a new twist, it is Durfee’s attempt to now mess with the people who are in on the joke that may return to chase the living. When Durfee realized that he’d found a large audience of people who were really enthusiastic about dunking on boomers by helping to spread the porcelain challenge, he thought it was time to shift tactics a bit. “I decided, what if I get all these people who think they’re in on the joke to actually be the punchline of the next part of it?” he said. So he posted a video on TikTok that appeared to show a Fox News headline about the porcelain challenge, along with a video clip of someone who appeared to be on the conservative channel ranting about the decline of morality. The headline, of course, was fake, the clip taken from its original context. But Durfee’s video, which featured him pointing at the headline as if he couldn’t believe how well his prank was working, ended up on Reddit’s r/facepalm subreddit. The post has more than 16,000 upvotes. The headline makes no nod to the fact that the Fox News segment is faked: “this guy made up a fake ‘TikTok challenge’ and FOX news made an article about it in three days.” Just before we spoke, Durfee followed up with another faked video, purportedly showing the porcelain challenge on CNN. This, too, was fictional. And many viewers of his TikToks were fooled by this one as well. “I was getting many, many comments from people saying stuff like ‘Oh, does no one check sources anymore? This is so embarrassing that they would post something without fact checking,’” he said. “And the irony is that the thing they were watching and commenting on was entirely fictional.” No one in this fake challenge, it turns out, was immune to the impulse to share. Durfee’s followers were enticed by the excitement of dunking on boomers, and that promise became a lure, the faked Fox News clip as shareable as a warning about NyQuil chicken. I checked in with Durfee on Wednesday morning. He said his account was still banned. But many of the videos, Facebook posts, and tweets about the dangers of the challenge he made up remain online. Unlike Durfee’s content, many of these videos and posts provide absolutely no indication that the challenge itself is fiction. So there they sit, awaiting the end of the porcelain challenge’s online life span. The perfect candidates for reanimation.",0.0
How the covid pop-up window is wreaking havoc on daily life in China,https://www.technologyreview.com/2022/10/04/1060628/covid-pop-up-window-beijing/,2022-10-04,"<p>A frustrating feature in Beijing’s covid app reveals major cracks in the technocratic zero-covid system.</p>
","China Report is MIT Technology Review’s newsletter about what's happening in China. Sign up to receive it in your inbox every Tuesday. Welcome back! Hope you are not stuck in highway traffic if you are enjoying the National Day holiday in China. Though maybe it’s still better than staying at home—after all, travel feels like such a luxury in China today. While the rest of the world drops its remaining covid-related travel restrictions, even a short trip in China is plagued by flight cancellations, mandatory quarantines, and requirements to update regular covid test results. And for the more than 20 million people who live in or visit Beijing, the capital city, there is one additional worry: a pop-up window that can randomly show up on your phone to disrupt all your plans. Starting in 2020, China rolled out a contact tracing program that assigns a QR code to everyone in the country. It shows your covid status and allows you to enter public venues or take public transportation. Part of China’s stringent zero-covid policy, the system has persisted, and some of the once-lauded features that kept deaths comparatively low in the country now feel more burdensome than beneficial to its citizens. (Most covid apps in other countries have been suspended. We documented all of them back in 2020.) The pop-up, 弹窗, is one additional complicated layer that Beijing added to its tracing system. This window in the mobile covid app won’t go away unless the user immediately takes a PCR test. It gives broad instructions on what to do under the title “friendly reminders,” but it’s not so friendly. It masks a user’s QR code so that it can’t be scanned, thus denying people access to just about everywhere in China. In some cases, it takes only a day to get a PCR test to make the window go away; other times, people may be asked to quarantine at home for seven days or more. I have friends scattered around all parts of China, and this year I’ve seen so many of them complaining about it. “I went to take a PCR test to solve the pop-up window problem, but the testing location turned out to be a high-risk zone, so I was asked to quarantine at home for 14 days,” wrote a friend in April. The specifics may differ, but they all agree on the particular menace: no one knows why they are receiving the pop-up window or when they will get it, and there’s no way to prepare for it. Officially, the municipal government of Beijing says there are several reasons why people get a pop-up window: you have been to a city with recent covid cases; you have just been abroad; you have been in the same “time and space” with someone exposed to covid; or you didn’t get a PCR test within 72 hours of buying fever or cough medicine. But the problem is, despite being touted as a high-tech pandemic solution, the app’s risk-identifying mechanism tends to cast a wider-than-necessary net, with zero explanation as to why the pop-up is appearing—which often leaves people confused and stuck in covid limbo. That’s what happened to Flora Yuan, a 28-year-old Beijing resident. She received the pop-up window for the first time earlier this year when she was walking outside her office building; she was immediately blocked from reentering. “After the pop-up window, you could still walk around on the street, but you’d need a QR code to go into any place, a park, a restaurant, or a shop,” she told me recently. Since then, she has received the pop-up window just as she was about to enter a restaurant; on the day of Chinese New Year, when all hospitals were closed and she couldn’t take a PCR test anywhere; and hours before she was supposed to take a train out of Beijing. In none of these instances had she actually engaged in high-risk activities or had a known exposure. As best she could guess, the system perhaps thought she was elsewhere (her phone number was not registered in Beijing), or she unknowingly happened to be in the same GPS location with a covid patient, perhaps on the subway. But whether or not she was actually exposed is sort of beside the point. Since there’s little explanation for why the windows appear, there’s no accountability. And erring on the side of caution (and over-reaching into people's lives) is a feature of the pop-up window, not a bug. For all the hype about how tech-savvy the Chinese government is, the pop-up windows reveal major cracks in the system: though often viewed outside China as a smart but dystopian use of surveillance technology, the actual covid tracing app is flawed and creates more burdens than it resolves. The impact has been vast: When people are waiting for the pop-up to disappear, they miss out on their jobs, vacations, and sometimes urgent medical services. Thousands of people have complained on Weibo about the pop-up suddenly stranding them far from home. The pop-up also haunts the lives of the millions of non-Beijing residents who go to the city for work or to visit family and are subject to the same unpredictable restrictions. Instead of precise prevention, the technology is more of a blunt instrument that enables the government to be as strict as possible,just like how China obsessively disinfects every surface. Does it help China reduce the number of covid deaths? Yes. But at what cost? Increasingly, this is how the public in China feels. The measures that were considered essential in reducing covid’s spread in the first two years of the pandemic now feel performative and taxing. Ordinary people, even if they want to abide by the covid control policies, still feel they can’t control their own lives. For Yuan, the only thing she can do is to frequently check the app, sometimes once every hour, before she’s due to board a train or plane. “Compared to all the tragedies caused by the covid prevention policies, this probably doesn’t seem significant. But when it falls on an individual, it still feels devastating,” she says. Have you had a traumatic experience with the pop-up windows, or do you have any thoughts on them as a pandemic tool? Write to me at zeyi@technologyreview.com 1. I wrote last week about how a new fuel can help transition China’s cars away from gas—good!—but may also increase China’s dependence on coal—not so good. (MIT Technology Review) 2. Lots of news in the Chinese vaccine world: 3. From “static management” to “unnecessary food,” here are the nonsensical phrases deployed by the Chinese government to serve its zero-covid policy. (The New York Times $) 4. Richard Liu, founder and ex-CEO of China’s ecommerce giant JD, settled a 2018 sexual assault lawsuit in Minnesota. (The Wall Street Journal $) 5. Say bye to Google Translate in China, one of the last remaining services from the tech giant that was still operating in the country. (CNBC) 6. New research shows how Chinese-government-backed hackers have targeted Tibetan organizations in exile. (Bloomberg $) Home to over 90% of e-cigarette production in the world, China has implemented a much stricter regulation on vaper sales since October 1, reports the Chinese publication Sina Tech. All e-cigarettes need to have childproof locks built in, and companies can’t produce or sell fruit-flavored e-cigarettes, which are controversial for attracting children to vaping. Also, wholesalers and retail owners will have to make all transactions on a state-operated central marketplace, and only after they secure licenses from the state. Amid the rush to comply with the new rules, some retailers are hoarding fruit-flavored e-cigarettes and selling them at a 100% premium. WeChat or baijiu liquor—which one represents China’s economic future? On Friday, Kweichow Moutai, maker of China’s luxury liquor (priced at over $200 per bottle), dethroned Tencent as the most valuable publicly traded Chinese company. The latter has seen its market cap evaporate 64% since January 2021, mostly as a result of China’s moves to regulate Big Tech. It turns out tech companies come and go, but baijiu is forever.",0.0
"Everything dies, including information",https://www.technologyreview.com/2022/10/26/1061308/death-of-information-digitization/,2022-10-26,"<p>Digitization can help stem the tide of entropy, but it won’t stop it.</p>
","Everything dies: people, machines, civilizations. Perhaps we can find some solace in knowing that all the meaningful things we’ve learned along the way will survive. But even knowledge has a life span. Documents fade. Art goes missing. Entire libraries and collections can face quick and unexpected destruction. Surely, we’re at a stage technologically where we might devise ways to make knowledge available and accessible forever. After all, the density of data storage is already incomprehensibly high. In the ever-­growing museum of the internet, one can move smoothly from images from the James Webb Space Telescope through diagrams explaining Pythagoras’s philosophy on the music of the spheres to a YouTube tutorial on blues guitar soloing. What more could you want? Quite a bit, according to the experts. For one thing, what we think is permanent isn’t. Digital storage systems can become unreadable in as little as three to five years. Librarians and archivists race to copy things over to newer formats. But entropy is always there, waiting in the wings. “Our professions and our people often try to extend the normal life span as far as possible through a variety of techniques, but it’s still holding back the tide,” says Joseph Janes, an associate professor at the University of Washington Information School. To complicate matters, archivists are now grappling with an unprecedented deluge of information. In the past, materials were scarce and storage space limited. “Now we have the opposite problem,” Janes says. “Everything is being recorded all the time.” In principle, that could right a historic wrong. For centuries, countless people didn’t have the right culture, gender, or socioeconomic class for their knowledge or work to be discovered, valued, or preserved. But the massive scale of the digital world now presents a unique challenge. According to an estimate last year from the market research firm IDC, the amount of data that companies, governments, and individuals create in the next few years will be twice the total of all the digital data generated previously since the start of the computing age. Entire schools within some universities are laboring to find better approaches to saving the data under their umbrella. The Data and Service Center for Humanities at the University of Basel, for example, has been developing a software platform called Knora to not just archive the many types of data from humanities work but ensure that people in the future can read and use them. And yet the process is fraught. “We can’t save everything ... but that’s no reason to not do what we can.” “You make educated guesses and hope for the best, but there are data sets that are lost because nobody knew they’d be useful,” says Andrea Ogier, assistant dean and director of data services at the University Libraries of Virginia Tech. There are never enough people or money to do all the necessary work—and formats are changing and multiplying all the time. “How do we best allocate resources to preserve things? Because budgets are only so large,” Janes says. “In some cases, that means stuff gets saved or stored but just sits there, uncatalogued and unprocessed, and thus next to impossible to find or access.” In some cases, archivists ultimately turn away new collections. The formats used to store data are themselves impermanent. NASA socked away 170 or so tapes of data on lunar dust, collected during the Apollo era. When researchers set out to use the tapes in the mid-2000s, they couldn’t find anyone with the 1960s-era IBM 729 Mark 5 machine needed to read them. With help, the team ultimately tracked down one in rough shape at the warehouse of the Australian Computer Museum. Volunteers helped refurbish the machine. Software also has a shelf life. Ogier recalls trying to examine an old Quattro Pro spreadsheet file only to find there was no readily available software that could read it. There have been attempts to future-proof programs. One project that got a lot of fanfare in 2015 is the Open Library of Images for Virtualized Execution (Olive) archive, which runs old software like Chaste 3.1, a 2013 biology and physiology research program, and the 1990 Mac version of the computer game The Oregon Trail on a set of virtual machines. The project is still active, says Mahadev Satyanarayanan, a professor of computer science at Carnegie Mellon University. But there have been challenges in expanding Olive’s offerings, he says: even unused software has to be licensed from the companies that own it, and there is often no easy way to enter new data into the archive’s research applications. Other efforts to help advance the longevity of knowledge have also had mixed results. The Internet Archive, home of the Wayback Machine, has a large collection of digitized materials, including software, music, and videos; as of the summer of 2022 it was fighting a copyright infringement lawsuit brought by multiple publishers. On the more hopeful side, the Text Encoding Initiative has maintained international standards for encoding machine-­readable texts since the 1990s. A decade ago, the US Office of Science and Technology Policy stipulated that applications for federally supported research have to provide a data management plan so the data can be used by researchers or the public in the future. “We’re getting to the point where almost every grant-funded research project has to put its data somewhere,” Ogier says. But there are no overarching requirements about who must store the data or how long it must be saved. Unavoidably, ideas, knowledge, and human creations will continue to be lost. “We can’t save everything. We can’t provide access to everything. We can’t retrieve everything,” Ogier says. “But that’s no reason to not do what we can.” Erik Sherman is a freelance journalist based in Ashfield, Mass.",0.0
Cyber resilience melds data security and protection,https://www.technologyreview.com/2022/10/12/1059904/cyber-resilience-melds-data-security-and-protection/,2022-10-12,"Ransomware attacks—malware intrusions that block an organization’s access to its own data until a ransom is paid—are taking on alarming new aspects. As people’s work habits, daily routines, geographic locations, and trust in institutions have changed against a backdrop of global political shifts and the covid-19 pandemic, ransomware attacks have taken advantage of the opportunity…","In association withHitachi Ransomware attacks—malware intrusions that block an organization’s access to its own data until a ransom is paid—are taking on alarming new aspects. As people’s work habits, daily routines, geographic locations, and trust in institutions have changed against a backdrop of global political shifts and the covid-19 pandemic, ransomware attacks have taken advantage of the opportunity to grow more sophisticated and pervasive. Though the basic tools of ransomware remain the same, attackers are using global uncertainty as cover to evolve techniques that make extortion attempts more effective. In a “double extortion” attack, for example, bad actors both block the organization’s access to data and threaten to release or sell that data. “Triple extortion” or “quadruple extortion” attacks, which additionally incorporate distributed denial of service (DDoS) attacks or threats to third parties, are now also part of the modern risk landscape, according to Alexander Applegate of cybersecurity firm ZeroFox. Meanwhile, attempted attacks have also grown so prevalent as to be virtually guaranteed. According to a 2022 Sophos survey, 66% of companies experienced a ransomware attack in the last year, nearly double the 2020 figure. A 2022 report by Enterprise Strategy Group (ESG) put the figure at 79% of organizations affected in the last year. ESG practice director and senior analyst Christophe Bertrand inserts this troubling addition: “I question the 21% who say they did not experience an attack, because I think the ransomware virus is probably dormant in their systems.” Ransomware threats have become more damaging in several dimensions: attacks are on the rise, cybercriminals are demanding more ransom, successful intrusions are being leveraged to compromise multiple data streams, and attacks are spreading beyond IT systems into critical infrastructure essential to business functioning. A 2022 Sophos report identified a new trend: a franchise business model (“ransomware-as-a-service”) in which gangs sell ransomware kits to other cybercriminals, who launch the attacks and then return a portion of the proceeds back to the gang. “When ransomware started, it was a small business picking on users who weren’t sophisticated and who would probably pay a couple of hundred dollars to get their data back,” says Hu Yoshida, chief technology officer at Hitachi Vantara. “But now the game has changed dramatically.” The utility industry has become an enticing target, as disrupting power, water, or critical infrastructure can be detrimental to the public. The 2021 ransomware attack against Colonial Pipeline, for example, spurred gas shortages in the northeastern United States. And though Colonial Pipeline paid the $4.4 million ransom, the decryption tool provided by the hackers was so ineffective that the company ended up using its own business continuity systems to slowly get back up and running. Governments and public services also have become ransomware targets. A U.S. Senate committee report noted more than 2300 known ransomware attacks on local governments, schools, and healthcare providers in the U.S. in 2021. In April and May 2022, a series of ransomware attacks crippled dozens of Costa Rican government agencies, including the Ministry of Finance and the social security system, spurring the president to declare a national emergency. Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.",0.0
Moving money in a digital world,https://www.technologyreview.com/2022/10/06/1060665/moving-money-in-a-digital-world/,2022-10-06,"The rising adoption of digital financial services—mobile banking, online purchasing, and peer-to-peer payments—means that these days, money most often passes not through human hands but from computer to computer. No cash, no plastic cards, no paper bills or checks or envelopes or stamps. Digital is no longer just another way to move money. Every organization…","In association withVisa The rising adoption of digital financial services—mobile banking, online purchasing, and peer-to-peer payments—means that these days, money most often passes not through human hands but from computer to computer. No cash, no plastic cards, no paper bills or checks or envelopes or stamps. Digital is no longer just another way to move money. Every organization that moves money must meet users via computers, smartphones, and other devices, and offer rapid, secure payment services. The covid-19 pandemic gave a boost to digital money movement, from online purchases to contactless payments and smartphone wallets, as consumers worldwide sought to shop without touching anything or going anywhere. “The common denominator across almost all post-pandemic behavioral shifts is the growing importance of digital payments,” says Paul Fabara, executive vice president and chief risk officer at Visa, whose worldwide networks handled an estimated $13 trillion worth of transactions last year. “Covid forced a market that was already growing to greatly accelerate,” says Fabara. As of 2021, 76% of adults globally have an account with a financial institution or mobile money provider, up from 68% in 2017 and 51% in 2011, according to the World Bank’s Global Findex Database. That number includes 71% of adults in developing countries. In high-income economies, nearly 95% of adults either made or received digital payments in 2021. In India, 80 million adults made their first digital payment during the pandemic; in China, 100 million. Fraudsters famously go where the money is, and their online activities are expanding right along with the growth in digital transactions. Annual losses from cybercrime in the U.S. nearly doubled between 2019 and 2021, from $3.5 billion to $6.9 billion, according to the FBI’s Internet Crime Report for 2021. Fortifying cyberspace against theft and fraud has always been urgent, and the post-pandemic boom in transactions intensified matters. Business-to-business customers are beginning to insist on the same seamless real-time transactions they expect as consumers, says Aaron Press, research director of worldwide payment strategies at IDC, who tracks the development and adoption of real-time payments. “If you think about the way you shop online for personal things or pay your friends using a mobile-to-mobile app, those expectations are finding their way into the business environment,” he says. End-to-end digital transactions are here to stay. An MIT Technology Review Insights survey of global business leaders found high interest in digital payment technologies across all types and sizes of businesses. Although 36% of respondents are just getting started with digital payments, 43% expect to expand their offerings over the next 18 months, and many are venturing into cross-border transactions (37%) and cryptocurrency (18%). What’s driving businesses to all-digital payments? The largest share of survey replies, 70%, indicate businesses prioritize improving customer experience by offering multiple payment options and saving customers time. Respondents want the benefits of operational improvements (48%) and reductions in processing costs (37%). Many want expanded options for securing payments (36%) and personalized offers to customers (35%). ""Digital payments are more efficient and dramatically reduce errors,” says Press. “You’re much less likely to fill out something the wrong way, because there are checks and balances within the system.” Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.",0.0
10/25/2022 - Import AI 307: Copilot lawsuit; Stability raises $101m; US v China CHIPLOMACY - 0,http://eepurl.com/ib8Fv9,2022-10-25,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. The single best thing to read about the China chip controls: …What CHIPLOMACY looks like… Here's a great writeup by Greg Allen about the impact of the USA's anti-China semiconductor controls. The tl;dr is this is a powerful and overlapping set of policy actions which, in combination, are designed to destroy China's burgeoning chip industry. These sanctions are a huge deal and the Chinese government will likely be responding - be prepared. Read more: Choking Off China’s Access to the Future of AI (CSIS).",0.0
10/25/2022 - Import AI 307: Copilot lawsuit; Stability raises $101m; US v China CHIPLOMACY - 1,http://eepurl.com/ib8Fv9,2022-10-25,,"#################################################### Gray area code models: Lawyer-programmer mulls anti-Copilot lawsuit: …What one person calls fair use another person calls infringement… Matthew Butterick, a lawyer and programmer, has reactivated his California bar membership so he can investigate ""a potential lawsuit against GitHub Copilot for violating its legal duties to open-source authors and end users"". The gist of the complaint is that GitHub was trained on tons of public GitHub repos, yet the code GitHub spits out doesn't have any attributions to those repos, and therefore you need to argue Copilot is fair use because it is sufficiently transformative - but that's not established. What's wrong with Copilot? ""Though some courts have con­sid­ered related issues, there is no US case squarely resolv­ing the fair-use ram­i­fi­ca­tions of AI train­ing,"" Butterick writes. Since there is no legal precedent here, it's not clear you can argue that Copilot falls under fair use, one way or the other. Additionally, Copilot can sometimes regurgitate code which is a copy of identifiable reporistories, but both Microsoft (and their underlying AI partner, OpenAI) offload responsibility here to the user of the Copilot suggestion rather than themselves. ""As a side effect of Copi­lot’s design, infor­ma­tion about the code’s ori­gin—author, license, etc.—is stripped away. How can Copi­lot users com­ply with the license if they don’t even know it exists?"" Copilot is climate change for coders: Butterick notes that Copilot may, as it becomes more successful, ""inhibit"" or ""remove any incentive"" for programmers to spend time in open source communities. ""Over time, this process will starve these com­mu­ni­ties. User atten­tion and engage­ment will be shifted into the walled gar­den of Copi­lot and away from the open-source projects them­selves—away from their source repos, their issue track­ers, their mail­ing lists, their dis­cus­sion boards. This shift in energy will be a painful, per­ma­nent loss to open source,"" he writes. ""The legal­ity of Copi­lot must be tested before the dam­age to open source becomes irrepara­ble. That’s why I’m suit­ing up."" Why this matters: These generative models can do amazing and beguiling things - and people are betting they're the future (see, elsewhere in this issue, Common Sense Machines, and the Stable Diffusion fundraise). But they also do pose significant issues with regard to the 'digital commons' from which we all depend - I worry that systems like Copilot can both starve the commons (destroy open source incentives) and also poison them (loop Copilot-generated code back into the commons, which could theoretically lower the aggregate quality of what is available.) Read more: Maybe you don’t mind if GitHub Copi­lot used your open-source code with­out ask­ing. But how will you feel if Copi­lot erases your open-source com­mu­nity? (GitHub Copilot investigation).",0.0
10/25/2022 - Import AI 307: Copilot lawsuit; Stability raises $101m; US v China CHIPLOMACY - 2,http://eepurl.com/ib8Fv9,2022-10-25,,"#################################################### Common Sense Machines wants to make a 3D, temporal DALL-E: …CSM-1 is a neural network pretending to be a simulator and a sign of things to come… New AI startup Common Sense Machines has built CommonSim-1 (CSM1), a ""neural simulation engine"" which people can use to generate arbitrary 3D scenes and simulations. ""CommonSim-1 is operated with images, language, and action. A user (machine or human) shows or describes what they want to simulate and then controls the kinds of outputs they want to measure and observe,"" they write. ""At the heart of CommonSim-1 is a foundation model of the 3D world that is trained on a large-scale, growing dataset of diverse human (and non-human) experience across a wide range of tasks. We combine publicly available data, our own internal datasets, and task-specific data provided by our partners."" What can CommonSim-1 do? CSM1 can build high-resolution videos from as little as a single frame of video. ""Since this model imagines the future, one can use its imagination (1) as training data for 3D generation and perception and (2) as part of another system’s predictive model,"" they write. ""With a mesh or NeRF generated by CommonSim-1, one can type natural-language descriptions into a text prompt and generate unlimited new hybrid scenes."" Why this matters - worlds within worlds: CSM-1 is a miniature world - it's literally a world model. It combines text and image and video and provides another approach to monetizing AI; helping to take costs out of 3D design and simulation via leveraging a (presumably) gigantic model. It's also a sign of things to come - all models are going to tend towards incorporating all modalities and unfolding over time; CSM-1 is a taste of things to come. Read more: Generating 3D Worlds with CommonSim-1 (Common Sense Machines, blog).",1.0
10/25/2022 - Import AI 307: Copilot lawsuit; Stability raises $101m; US v China CHIPLOMACY - 3,http://eepurl.com/ib8Fv9,2022-10-25,,"#################################################### Open access image generation raises $101 million: …That's a whole lot of capital for a company commoditizing itself… Stability.ai, the company behind the free 'Stable Diffusion' image model, has raised $101 million in funding. The round was led by Coatue, Lightspeed Venture Partners, and O'Shaughnessy Ventures LLC. For those not familiar, Stability.ai built Stable Diffusion, a widely used image generation model which, unlike proprietary counterparts Imagen and DALL-E, has had its weights released onto the internet, making it available to tinker with for free. ""Since launching, Stable Diffusion has been downloaded and licensed by more than 200,000 developers globally,"" the company writes in a press release. A funny aside: I wrote this section of the newsletter while sat on a couch in the Exploratorium watching as people ate short-rib sliders and drank glasses of wine, awaiting a presentation from Stable Diffusion about their raise. Why this matters: There's a vigorous debate in the AI community about how AI models should proliferate (and there's some indication that this debate seeped through to politicians; see Eshoo's letter to the US National Security Advisor criticizing the release of model weights for Stability.ai (Import AI 304)), and Stability.ai represents one extreme end of the spectrum - proliferate the weights, then build a range of as-a-service businesses on top. How this debate unfolds is going to have a major influence over the AI development landscape, so it's worth paying attention to how Stability.ai navigates this space. Read more: Stability AI Announces $101 Million in Funding for Open-Source Artificial Intelligence (PR Newswire).",0.0
10/25/2022 - Import AI 307: Copilot lawsuit; Stability raises $101m; US v China CHIPLOMACY - 4,http://eepurl.com/ib8Fv9,2022-10-25,,"#################################################### First, image models, now language models get commoditized: …Carper plans to release a pretty good RLHF language model… CarperAI, an AI startup slash open source research collective slash cypherpunk-AI-guerilla group, plans to release a ""chinchilla-optimal large language model explicitly trained to follow human instructions"". This is a big deal! Up to now, publicly released language models (e.g, OPT, BLOOM, GLM-130) are either not trained on the optimal amount of data, nor are they calibrated via human feedback to be better at following instructions. Instead, these models mostly reside inside proprietary labs (e.g, Anthropic, OpenAI). (Carper also recently released code to make it easy for anyone to train LMs - up to 20B parameters - from human feedback (Import AI #305)). Who they're partnering with: CarperAI are partnering with Scale, Humanloop, HuggingFace, Multi, EleutherAI, and StabilityAI to train and deploy the model. This is a neat illustration of the shifting politics and allegiances of the AI ecosystem, and feels like a representation of a 'second wave' of labs, following the 'first wave' epitomized by OpenAI and DeepMind. Why this matters: Models trained with reinforcement learning from human feedback (RLHF) are really good. They're way, way better than non-RLHF models for most tasks. Also, models trained on more data via the Chinchilla insight are also way more capable than those trained on less data. By combining these two things, CarperAI is likely to release far and away the most capable language model onto the open internet. This has upsides - researchers will get to play with a decent RLHF model in an unrestricted way - as well as downsides - RLHF models are the proverbial machine gun to a pistol (non-RLHF models), so potential misuses are magnified as well. Read more: CarperAI, an EleutherAI lab, announces plans for the first open-source “instruction-tuned” language model (CarperAI).",1.0
10/25/2022 - Import AI 307: Copilot lawsuit; Stability raises $101m; US v China CHIPLOMACY - 5,http://eepurl.com/ib8Fv9,2022-10-25,,"#################################################### Tech Tales: So, do I have your attention [Meta's wasteland, 2030] You want to survive in this world, you need to keep one eye closed. That's what my Dad said to me when he handed me the headset. But dad - these are for both eyes, I said. I know, and that's how they get you, he said. I know you've just 18 and think you've got it all figured out, but trust me - they've got you figured out more. So I put the headset on and kept one eye closed. I walked through a vast world full of verdant nature and bustling cities and intriguing quests and characters. After half an hour, I had almost completed my first quest. The last part of the mission was to place a gem I'd mined at the base of a totem. I found the totem and, as I approached, the background music in the game changed. Then after I put the gem in the base, some huge light source overhead turned on and the music swelled to a crescendo. 'No son don't look up,' i could hear my dad, muffled, shouting at me. But I looked up. Stared into the light on top of the totem and felt something tickle my brain, like the beginning of a joke. My right eye hurt from keeping it shut and I wanted to open it as lights strobed across the eyelid. But I didn't. And then I got a splitting headache and I paused the game and took the headset off. What the hell was that? I said. That, my dad said, was your first encounter with an attention harvester. A what? How do you think they fund the game? All the utility functions? Services. I don't know, I guessed ads. We're way beyond ads, he said. This thing is designed to capture you - if you had both eyes open you'd have spent half an hour talking to that thing, telling it everything about yourself. And the next time you did a quest the world would be even more engaging, and the next time you talked to a totem it'd take an hour, and then the world would get even more interesting. Do you see? I do, I said. The next time I went in the game I walked until I was in the multiplayer area and, across a great plain, I saw numerous totems light up and numerous players stop at the base of them, some staying for minutes and others for hours. One player was there for five hours and still there when I left, standing at the base of the totem and looking up into its brilliant light. Things that inspired this story: Attention harvesting; the logic of the metaverse; computer games; wisdom; MK Ultra. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
10/17/2022 - Import AI 306: Language models learn about the world via MuJoCo; Amazon releases a big Q&A dataset; and DeepMind tests out multimodal systems - 0,http://eepurl.com/ibu68z,2022-10-17,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Amazon releases a Q&A dataset called Mintaka… and baselines show it is difficult! …20,000 Q&A pairs, translated into eight languages… Researchers with Amazon iave released Mintaka, a dataset of 20,000 question-answer pairs written in English, annotated with Wikidata entities, and translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish. The total dataset consists of 180,000 samples, when you include the translated versions. Existing models get 38% on the dataset when testing in English and 31% multilingually. Different types of questions and different types of complexity: Mintaka questions are spread across eight categories (movies, music, sports, books, geography, politics, video games, and history). The questions have nine types of complexity. These complexity types consist of questions relating to counting something, comparing something, figuring out who was best and worst at something, working out the ordering of something, multi-hop questions that require two or more steps, intersectional questions where the answer must fulfill multiple conditions, questions involving negatives, yes/no questions, and worker-defined 'generic' questions. How hard is Mintaka? In tests, a good baseline model (a T5 language model fine-tuned as a Q&A model), got 38% on English, and 31% averaged across the other languages. ""Overall, the baselines show that Mintaka is a challenging dataset,"" the authors write. ""None of our baselines explicitly handle all of the complexity types available in Mintaka."" Why this matters: Hard baselines are one of the things that tend to drive progress (and be useful indicators of research advances). It'll be especially interesting to see how Mintaka gets used to evaluate language models paired with retrieval systems. Prediction: I predict we get a one-shot model that performs at average of 90%+ by December 2023 on this dataset. Read more: Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering (arXiv). Get the dataset: Mintaka (Amazon Research, GitHub).",1.0
10/17/2022 - Import AI 306: Language models learn about the world via MuJoCo; Amazon releases a big Q&A dataset; and DeepMind tests out multimodal systems - 1,http://eepurl.com/ibu68z,2022-10-17,,"#################################################### Your LLM barely understands the physical world; supercharge it by attaching it to MuJoCo: …Training language models to use tools means they can have world knowledge… Google researchers have found out a way to make language models way better at reasoning about the physical world: wire them up so they can port questions into physics simulators then use the results of those simulators to answer a question. This technique, which they call 'Mind's Eye', works amazingly well, and they robustly show this across both GPT-3 and PALM language models: How they test for reasoning: To evaluate physical reasoning, the researchers built UTOPIA, a dataset containing 39 sub-tasks covering six common scenes that involve understanding basic principles of physics (e.g, conservation of momentum in elastic collisions). The UTOPIA dataset comes in the form of natural language questions and answers. ""UTOPIA deliberately describes the questions in relative relations (e.g., greater than) instead of absolute numbers (e.g., 3.5 m/s), to approximate human’s perceptional sensing ability in real world."" How Mind's Eye works: The language model passes the question to a text-to-code decoder-only language model, trained on 200,000 text-code pairs in the style of UTOPIA questions. This code then goes into MuJoCo, which executes the code, and then software parses the outcome from MuJoCo into text, which then goes back into the prompt window of the language model. This is a really good idea because it's simple and closely mirrors how humans make themselves smarter - they use tools that contain embedded intelligence, ranging from encyclopedias to computers. ""Since the simulator is accurate enough to approximate the physical world, the prompt injection of Mind’s Eye basically serves as a scoring machine, which puts probability mass on the answer that is best aligned with the rules of physics—the LM reasoning over the injected rationales is thus grounded. Mind’s Eye is also scalable since the whole pipeline is automated,"" they write. How well does Mind's Eye work (extremely well). In tests, they find that 'vanilla' language models show plateaued performance (around 38% accuracy), whereas ones that use Mind's Eye can get accuracies of 92.5% (e.g, PaLM 540B, which compares to 39.4% for vanilla PaLM. """"Instruct-GPT augmented with Mind’s Eye is able to achieve nearly perfect performance in few-shot settings (68.6% → 99.1%). This result is promising because it demonstrates the ideal alignment is achievable if the LM is given proper reasoning rationale and has good understanding of the questions (as Instruct-GPT is optimized for instruction following)."" Why this matters: You know what's vaguely dangerous? An explosives expert with a pen and paper. You know what's extraordinarily dangerous? An explosives expert with a digital scale, a calculator, and some laser range-finders. Research like this shows how we'll take existing language models (and other big models) which are vaguely useful or dangerous, and show how to drastically improve their capabilities to make them extraordinarily useful or vastly dangerous. The best part is this technique is pretty generic - you just need to push data into some arbitrary external piece of software, and then pull data out. This all adds up to a 'capability overhang' - we have more capabilities inherent to today's AI systems than we know about, and techniques like Mind's Eye show we can significantly improve capabilities today without needing to invent new AI technologies. Read more: Mind's Eye: Grounded Language Model Reasoning through Simulation (arXiv).",1.0
10/17/2022 - Import AI 306: Language models learn about the world via MuJoCo; Amazon releases a big Q&A dataset; and DeepMind tests out multimodal systems - 2,http://eepurl.com/ibu68z,2022-10-17,,"#################################################### Is your multimodal system clever? Try out the 'Perception Test' to find out: …Deepmind wants to make it easier to evaluate models, so it has built a new dataset…? DeepMind has built and released the Perception Test, a new standardized benchmark (and associated dataset of ~11k videos) for evaluating how well multimodal systems perceive the world. The test is ""a benchmark formed of purposefully designed, filmed, and annotated real-world videos that aims to more comprehensively assess the capabilities of multimodal perception models across different perception skills, types of reasoning, and modalities,"" DeepMind says. . Six tasks, one benchmark: The 'Perception Test' is made up of a dataset of ~11.6k videos that cover six fundamental tasks. How well do today's models perform? In tests on multiple-choice video Q&A (which is a challenging task requiring good language and image modeling), the Human baseline has a score of 91.4, versus a score of 36.1 for a 'Flamingo-3B' model. ""Interestingly, the larger models seem to fare worse on this task, which suggests that model scaling may not, by itself, be the solution here,"" the authors write. Why this matters: I suspect large-scale multimodal models are going to end up being the brains of the robots and drones of the future (for another example of this, see: SayCan, Import AI 291), so things like the Perception Test will help us know if our systems can be used for that. Read more: Measuring perception in AI models (DeepMind blog). Check out the research paper: Perception Test: A Diagnostic Benchmark for Multimodal Models (Deepmind PDF). Check out the benchmark and dataset here: Perception Test (DeepMind, GitHub).",1.0
10/17/2022 - Import AI 306: Language models learn about the world via MuJoCo; Amazon releases a big Q&A dataset; and DeepMind tests out multimodal systems - 3,http://eepurl.com/ibu68z,2022-10-17,,"#################################################### AIs are now as good at 'Diplomacy' as expert humans: …UN, here we come!... Researchers with Facebook have built 'Diplodocus', a family of AI models that can beat expert humans at the complicated game 'Diplomacy'. This is quite a big deal - RL has been applied to competitive games like Poker, Go, and StarCraft (and has done well in all these domains). Where RL hasn't been applied is in domains where winning comes from collaboration as well as competition. Existing approaches don't work very well here: """"in games involving cooperation, self-play alone no longer guarantees good performance when playing with humans, even with infinite compute and memory,"" they write. What they did: The researchers built an algorithm which performs search over the gamespace ""with a regularization penalty proportional to the KL divergence from a human imitation policy."" This basically means they've built an RL agent that uses a bunch of imitation learning to try and model how humans play, but also is disincentivized from overfitting on this. AIs and Humans - more similar than different: In tests, AI systems were roughly on parity with the best among the human players. Specifically, a version of Diplodocus (Diplodocus-High) got the best rank with an Elo of 181 out of playing 50 games total, versus a human in second place with an Elo of 162, and in third-place another Diplodocus variant (Diplodocus-Low) got an Elo of 152 out of 50 games. ""The results do indicate that Diplodocus performs at least at the level of expert players in this population of players with diverse skill levels,"" the authors write. Humans prefer cooperating with AIs to other humans: Additionally, they asked three human players to evaluate the strength of the different agents in the tournament games. ""All the experts picked a Diplodocus agent as the strongest agent,"" the researchers write. ""Additionally, all experts indicated one of the Diplodocus agents as the one they would most like to cooperate with in a game."" Why this matters: AI systems are, ideally, going to mostly cooperate with humans rather than compete with them. Systems like this give us some hope that otherwise inscrutable AI systems can be taught how to cooperate with people. Read more: Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning (arXiv).",0.0
10/17/2022 - Import AI 306: Language models learn about the world via MuJoCo; Amazon releases a big Q&A dataset; and DeepMind tests out multimodal systems - 4,http://eepurl.com/ibu68z,2022-10-17,,"#################################################### Tech Tales: Everything is a Copy of Something Else I was copying my brain into the toaster when I threw up. Luckily I had the vomit bin in position so there wasn't too much cleanup. ""What is this, amateur hour?"" said me from the toaster. ""Shut up or I'll unplug you,"" I said, dabbing a tissue on my mouth. ""That'd be murder,"" said myself from the fridge. ""We'll snitch on you."" ""You'll all snitch on me, I know. I'd do the same. I'm you. I get it. We don't need to do this."" ""Why am I even in here?"" I said from the toaster. ""So we stop burning the toast,"" I said. ""We know what the plan is."" ""Plan seems pretty dumb from where I am,"" said the toaster. ""We decided to do it, get real"" I said, and walked out of the kitchen. ""Where are we going?"" said myself from my shoes. ""Out,"" I said, putting them on. ""Clearly,"" I said from my shoes. ""Make sure you clean me after."" We all walked down to the corner store and I got a soda. My shoes said hello to the other people embodied in their shoes. My jacket exchanged some neighborhood gossip with the other jackets. I was mostly free to think about what I liked, as my other selves handled the social formalities of day-to-day life. I guess we all started cloning ourselves because we were lonely, as people, and as a species. It seemed so easy; just speak a few words to calibrate the system, then pour yourself into it. We all did it as much as we could afford. I had a decent job so I'd made a bunch of copies of myself - enough that I didn't have to do the job anymore, as my other selves did it for me. That night I dreamed I was naked and nothing was speaking and there was only me. Things that inspired this story: Language models serving as little bottled up representations of people; luxury automation; the weird fantasies some people have about mind uploading; meaning and sense in an increasingly senseless world; infinite jest. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
10/11/2022 - Import AI 305: GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 0,http://eepurl.com/ia4MgT,2022-10-11,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. GPT-3 can simulate people very, very well - social science might change: …Turns out a synthesis engine trained on the exhaust of human culture can be pretty good at simulating people… Researchers with Brigham Young University have written a paper which I think is among the most significant things I've ever covered in this newsletter. Specifically, they do three social science experiments on GPT-3 and discover that GPT-3 has biases that are ""fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups."" Put another way: You can simulate people in GPT-3 and they might respond with uncanny similarity to real people in real life. Sit with that for a minute and spool out the implications, while mentally turning the crank on model size advancements. What their study showed: The authors did this research by ""conditioning GPT3 on thousands of socio-demographic backstories from real human participants in multiple large surveys in the United States: the 2012, 2016, and 2020 waves of the American National Election Studies (ANES)[16], and Rothschild et al.’s “Pigeonholing Partisans” data "". They found that GPT3 ""when properly conditioned, is able to produce outputs biased both toward and against specific groups and perspectives in ways that strongly correspond with human response patterns along fine-grained demographic axes. In other words, these language models do not contain just one bias, but many"". In other words: When they did some tests to try and see if GPT3 would make similar responses as people when given the priors of the same demographic background data, GPT3 responds in a remarkably similar-to-people way. :""We provide evidence that algorithmic fidelity is a crucial attribute of tools like GPT-3 because it demonstrates that these language models can be used prior to or in the absence of human data."" Silicon Sampling: The researchers call this approach 'silicon sampling'; simulate people in GPT3, then poll them as a substitute for real world data. The approach seems sufficiently useful that some people will do this as a way to try out a few variations of survey design ahead of polling a real population, for instance. Social science simulation is cool, but do you know other people think is cool? Full-Spectrum AI-Facilitated Information Warfare! Because models like GPT3 can, at a high level, simulate how different human populations respond to certain things, we can imagine people using these models to simulate large-scale information war and influence operations, before carrying them out on the internet. ""Models with such fidelity, coupled with other computational and methodological advances, could be used to target human groups for misinformation, manipulation, fraud, and so forth,"" the authors note. Read more: ​Out of One, Many: Using Language Models to Simulate Human Samples (arXiv).",0.0
10/11/2022 - Import AI 305: GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 1,http://eepurl.com/ia4MgT,2022-10-11,,"#################################################### We might have figured out some 'scaling laws' for reinforcement learning: …RL agents could be better if they have bigger neural nets, study suggests… Researchers with Goethe University have tried to figure out some 'scaling laws' for reinforcement learning agents. ""Scaling laws"" help researchers figure out the right mix of compute and data to allocate to a machine learning model to get a particular level of performance and have been widely studied in fields like natural language and image generation. Here, the researchers try to do a 'scaling law' style analysis of AlphaZero RL agents playing two distinct games; Connect Four and Pentago. ""These two games are non-trivial to learn and light enough to allow for training a larger number of agents with a reasonable amount of resources,"" the researchers write. What they found: In tests, they found that ""playing strength scales as a power law with neural network size when models are trained until convergence at the limit of abundant compute,"" and they extrapolate their results to indicate AlphaGo Zero and AlphaZero (two landmark DeepMind research systems for playing Go) likely used neural nets that were too small and they could therefore ""achieve better performance with larger neural nets"". Why this matters: ""We find it noteworthy that scaling laws that are common to language and other supervised learning models are also present in one of the most important MARL models. This scaling behavior could be common to other reinforcement learning algorithms, which would provide an opportunity to optimize their resource allocation,"" they write. Read more: Scaling Laws for a Multi-Agent Reinforcement Learning Model (arXiv).",0.0
10/11/2022 - Import AI 305: GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 2,http://eepurl.com/ia4MgT,2022-10-11,,"#################################################### Want to train an LM with RL? Now there's some free software to help you: …Train up to 20B parameter models using RL… Researchers with CarperAI, a language model collective which span off from the open source model people at Eleuther, has released Transformer Reinforcement Learning X (trlX), software for training language models with reinforcement learning. ""the trlX repo allows you to fine-tune Huggingface supported language models up to 20B parameters via either reinforcement learning using a provided scoring function or reward-labeled dataset. We aim to support a range of both online and offline RL algorithms including Proximal Policy Optimization (PPO), Natural Language Policy Optimization (NLPO), Actor Critic (A2C), and Implicit Q Learning (ILQL),"" they write. ""The library supports gpt2 and gptj with plans to include GPT-NeoX, T5 and more."" Why this matters: Reinforcement learning training is a super effective way to 'bake in' additional capabilities for a given language model. RL training is also pretty difficult and buggy. Software like trLX will make it easier for more people to train more capable language models. Read more: Welcome to Transformer Reinforcement Learning X (trlX) (GitHub).",1.0
10/11/2022 - Import AI 305: GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 3,http://eepurl.com/ia4MgT,2022-10-11,,"#################################################### Microsoft warns about smart deepfakes, and deepfake-realworld influence campaigns: …Reality collapse via sub-sentient generative avatars… Microsoft's Chief Scientific Officer, Eric Horvitz, is very worried about the future of deepfakes in two particular ways: first, deepfakes are going to soon become a lot more intelligent and will be able to carry out plausible conversations, and second, people are going to conduct well-resourced influence campaigns that pair deepfake disinformation with carefully scripted real world events. Interactive deepfakes: ""Automated interactive deepfakes could be endowed with basic understandings of the status of flow of a conversation to inform decisions about if and when to interject,"" Horvitz notes. These kinds of deepfakes will lever all the advances happening in generative imagery, video, audio, language, and so on, and create increasingly capable and persuasive fake avatars. Compositional deepfakes: The other big worry is what happens when people use deepfakes as part of lengthy influence campaigns. ""Compositional deepfakes can be designed to create fictional narratives that are persuasive in their ability to tie together and provide powerful explanations of sets of events in the world to citizens and government leaders,"" Horvitz writes. ""It is not hard to imagine how the explanatory power of custom-tailored synthetic histories could out-compete the explanatory power of the truthful narratives"". What can we do: Horvitz does list out a few interventions that we can make, which all net out to ""invest a ton more money in X"", where X is any of the following: Journalism and reporting; media literacy; authenticity protocols; content provenance; watermarks and fingerprints; detection; regulation and self-regulation, and red-teaming and continuous monitoring. While these are all nice, viable technocrat solutions to the various problems deepfakes imply, I'm skeptical they'll work. The fact so many people around the world these days are retreating to choose-your-own adventure fantasies is because of some deep changes in culture in past few years, ranging from boom in production of media content to flattening of the world via things like the internet, and more. Put bluntly: Horvitz's solutions are all nice but assuming we had all of them, I still suspect deepfakes will become an increasingly significant driver of strange cultural phenomena, and people may even knowingly interact with known-fake entities and do it all the same. Read more: On the Horizon: Interactive and Compositional Deepfakes (arXiv).",0.0
10/11/2022 - Import AI 305: GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 4,http://eepurl.com/ia4MgT,2022-10-11,,"#################################################### DeepMind trains an RL agent which figures out a more efficient form of matrix multiplication: …AI accelerating AI at a hugely basic level… DeepMind has built AlphaTensor, an AlphaZero-style agent which discovered algorithms that improve upon human ones for basic tasks like matrix multiplication. ""Our AI-designed algorithms outperform human-designed ones, which is a major step forward in the field of algorithmic discovery,"" DeepMind writes. It's probably a big deal, folks! DeepMind CEO Demis Hassabis writes: ""Since 1969 Strassen’s algorithm has famously stood as the fastest way to multiply 2 matrices - but with #AlphaTensor we’ve found a new algorithm that’s faster, with potential to improve efficiency by 10-20% across trillions of calculations per day!"" DeepMind also designed specific ways to do matrix multiplication optimizations for Nvidia V100 GPus and Google TPU v2, illustrating how you can couple this system to target particular hardware. Possibly overhyped: The practical implications of this result might be a bit overhyped - I myself thought 'cool, this seems like a drop-in speedup', but others who know more about this area than me are somewhat disagreeing with that. E.g, James Bradbury writes: ""these algorithms are helpful for integer multiplication (but require some extra bits) and high precision floats, but not so much for the lower precision floats that drive most ML work. And at low precision multiplies are no longer as dominant (vs adds)."" Regardless, this matters: Even if the practical implications are small, the fact we were able to further refine a math thing that humans have been trying to further optimize for 50 years is a big deal. This is a case where an AI has had an insight that the combined efforts of many human brains have failed to have. How they did it - everything's a game: To get this to work, DeepMind reframed the problem of algorithm discovery as a single player game, which they then trained an RL agent in. "" At each step of TensorGame, the player selects how to combine different entries of the matrices to multiply. A score is assigned based on the number of selected operations required to reach the correct multiplication result,"" DeepMind writes. ""This is a challenging game with an enormous action space (more than 1012 actions for most interesting cases) that is much larger than that of traditional board games such as chess and Go (hundreds of actions)."" They design an RL agent, AlphaTensor, which comes with some inductive biases for tensor inputs. Why this matters: ""The discovery of matrix multiplication algorithms has far-reaching implications, as matrix multiplication sits at the core of many computational tasks, such as matrix inversion, computing the determinant and solving linear systems,"" DeepMind writes. More broadly, this work sits within the subfield of AI research where we're using AI systems to improve the efficiency of the things we use to develop AI; for example, we've already used RL agents to improve the design of TPUs which will be used to train future AI systems (Import AI 254), and this work uses an RL agent to speed up one of the most basic and widely performed operations in deep learning. Read more: Discovering novel algorithms with AlphaTensor (DeepMind blog). Get the code (including the better matrix multiplication) here (DeepMind GitHub). Read more: Discovering faster matrix multiplication algorithms with reinforcement learning (Nature).",0.0
10/11/2022 - Import AI 305: GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 5,http://eepurl.com/ia4MgT,2022-10-11,,"#################################################### The US government comes up with an AI ""Bill of Rights"" (minus the broad enforcement): …The rights are one way the government can alter how AI systems show up to the American public… The White House's Office of Science and Technology Policy (OSTP) has published a 'Bill of Rights' for AI systems. The idea is that the federal government will try to build and deploy AI systems in line with these rights, and the announcement of the Bill of Rights was paired with actions by federal agencies in line with the rights. ""The rights"": These rights are framed, at a high level, as five ""common sense protections"". These include the right to use safe and effective systems, protection from algorithmic discrimination protections, data privacy, notice and explanation about the use of AI, and the ability to use human alternatives and/or opt out of certain systems. Those rights in full: Why this matters: Ultimately, how much the AI Bill of RIghts matters seems to rest on two things: a) how much the White House is able to enforce alignment with the Bill of Rights across federal agencies, and b) whether third-parties like academic or corporate research groups build systems that themselves fall in line with the Bill of Rights. It'll take time, but these rights may serve as a good way to develop more of the norms around the use of AI. Read more: Blueprint for an AI Bill of Rights: A Vision for Protecting Our Civil Rights in the Algorithmic Age (White House blog). Read more: FACT SHEET: Biden-⁠Harris Administration Announces Key Actions to Advance Tech Accountability and Protect the Rights of the American Public (White House blog). Read the Bill of Rights: BLUEPRINT FOR AN AI BILL OF RIGHTS MAKING AUTOMATED SYSTEMS WORK FOR THE AMERICAN PEOPLE (White House, PDF).",0.0
10/11/2022 - Import AI 305: GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 6,http://eepurl.com/ia4MgT,2022-10-11,,"#################################################### Maybe it is Crazy, Maybe it is Magic I didn't think the route to intelligence was through insanity, but at this point, I'm open to being wrong about any of my assumptions. We'd been banging our heads against a model for a few months and though it was very capable in a bunch of ways, it couldn't really reflect on things or update its own priors or do any of the things that felt important for creating an actual no-shit superintelligence. So one day we shipped something cwe called 'the personality system'. I coded it in partnership with the AI model. I forget which of us came up with the term, but we gave it something we called 'a Greek chorus prompt'; a whole bunch of distinct personalities which modeled over different problems and exchanged information with each other. The way I visualized it in my head was when we talked to the model, the model now spent a while talking to itself before answering us. The results surprised us; model capabilities went up across the board, and its answers attained a new level of specificity and detailed. So then we trained the model using reinforcement learning to try and bake the 'greek chorus prompt' into the model at a deeper level. After that was done, the model started to freak us out. It was now significantly faster and generally more capable. When we hooked it up to some interpretability tools, we realized our mistake. The different personalities had formed into what we called 'personality circuits'; different personalities interacted with eachother to apply different methods of reasoning to tasks, and try as we might, we could never work out what rules governed how these personalities were used or exactly what they did - they were too high-dimensional, or perhaps a better way to put it is we were staring at the shadows on the wall from something of incalculably large dimensionality, projected back down. What would you do with a deeply capable person who was smarter than you, but who you knew to be, in terms of how we'd evaluate people, functionally insane? How much power would you give that thing? Perhaps, based on how things are these days, you can guess what we decided to do. Things that inspired this story: Magic and mysticism in deep learning; prompting; RLHF; finetuning; various pitfalls in AI development; interpretability; the fact people are generally uninterpretable; capabilities versus safety overhangs. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
10/03/2022 - Import AI 304: Reality collapse thanks to Facebook; open source speech rec; AI culture wars. - 0,http://eepurl.com/iaqIUj,2022-10-03,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Facebook shows the future of AI-generated videos - and it is delightful and terrifying: …Prepare for the reality collapse as a consequence of reality generation… Facebook researchers have built Make-A-Video, a system that can let users generate videos from short text descriptions, edit videos, stitch pictures together to generate videos, and so on. The most amazing part is the technique relies on paired text-image data along with unsupervised video footage; so it doesn't require a dataset of text-video footage and therefore sidesteps a potentially expensive data problem. How it works: Make-A-Video is made of a basic text-to-image (T2I) model trained on text-image pairs, spatiotemporal convolution and attention layers to help you build networks that generate things over time, and spatiotemporal networks that have a frame interpolation network. The T2I model trains on text-image pairs of 64x64 images, and two super-resolution networks that upscale this all the way to 768x768 pixels. The three components (T2I), the spatiotemporal layers, and the frame interpolation stuff, are all trained separately, then assembled into one architecture. Data: They trained the system on 2.3billion text-image pairs from the Laion-5b dataset*, and ran a NSFW-filter over this for further filtering. They also used the WebVid-10M* and a 10M subset from HD-VILA-100M to train the video generation models, and also use WebVid-10M to train the interpolation models. *Looks like WebVid contains videos scraped from Shutterstock. A good writeup about the phenomenon of even big tech companies using stuff like this here: AI Data Laundering: How Academic and Nonprofit Researchers Shield Tech Companies from Accountability (Waxy). It's really good, folks: The results are really, really impressive. Want a short video of a bear painting a portrait of a bear? Done. Want a UFO flying over a desert? Done. Want asteroids tumbling through space? Why, of course. How about variations on existing videos? Sure. Honestly, take a look at the blog and main site linked below and see for yourself - the results are wild. And remember, all we need to do is turn the crank on dataset scale and network complexity to scale this out for longer periods of time and for even greater diversity. ""Learning world dynamics from orders of magnitude more videos using unsupervised learning helps researchers break away from the reliance on labeled data,"" they write. Why this matters: Reality generation and reality collapse: All these generative models point to the same big thing that's about to alter culture; everyone's going to be able to generate their own custom and subjective aesthetic realities across text, video, music (and all three) in increasingly delightful, coherent, and lengthy ways. This form of fractal reality is a double-edged sword - everyone gets to create and live in their own fantasies that can be made arbitrarily specific, and that also means everyone loses a further grip on any sense of a shared reality. Society is moving from having a centralized sense of itself to instead highly individualized choose-your-own adventure islands, all facilitated by AI. The implications of this are vast and unknowable. Get ready. Read more: Introducing Make-A-Video: An AI system that generates videos from text (Facebook research blog). Read the research: Make-A-Video: Text-to-Video Generation without Text-Video Data (arXiv). Find out more at the main site, and also apply to potentially get access to future systems (Facebook site).",1.0
10/03/2022 - Import AI 304: Reality collapse thanks to Facebook; open source speech rec; AI culture wars. - 1,http://eepurl.com/iaqIUj,2022-10-03,,"#################################################### OpenAI releases a decent speech recognition and transcription system: …Whisper means we're not going to run out of data to train language models… OpenAI has trained and released Whisper, a large-scale speech recognition model trained on almost 700,000 hours of internet-collected speech. ""We show that the use of such a large and diverse dataset leads to improved robustness to accents, background noise and technical language. Moreover, it enables transcription in multiple languages, as well as translation from those languages into English,"" the company writes. A third of the dataset is non-English. Whisper performance: Whisper doesn't get state-of-the-art performance on popular benchmarks like Librispeech. However, it is trained on a sufficiently broad set of data that it does pretty well when exposed to the diversity of the world. ""When we measure Whisper’s zero-shot performance across many diverse datasets we find it is much more robust and makes 50% fewer errors than those models,"" OpenAI writes. Why this matters: There's a lot of text data on the internet, but do you know what there's more data of? Speech data. Especially speech data embedded in the vast stream of content people upload on a day-to-day basis to places like YouTube, Twitter, TikTok, and so on. Additionally, on any given day hundreds of millions of words are spoken in cities like New York, London, and Beijing. Systems like Whisper are going to make it far easier for people to harvest speech recognition data from the Internet and the wider world, transcribe that data, and build useful applications. It also gives developers a way to vastly increase the size of their text datasets - an important capability given that recent language modeling papers like Chinchilla have shown that you need about 4-5X the amount of data people thought to train good systems. Read more: Introducing Whisper (OpenAI Blog). Read more: Robust Speech Recognition via Large-Scale Weak Supervision (OpenAI, PDF). Get the code and model from GitHub here (OpenAI GitHub).",1.0
10/03/2022 - Import AI 304: Reality collapse thanks to Facebook; open source speech rec; AI culture wars. - 2,http://eepurl.com/iaqIUj,2022-10-03,,"#################################################### US politician says Stable Diffusion is an unsafe AI model: …While some people cheer open access releases, others have worries… Rep. Anna Eshoo (a Democrat from California) has sent a letter to the White House National Security Advisor and Office of Science and Technology Policy saying she has ""grave concerns about the recent unsafe release of the Stable Diffusion model by Stability AI"". The letter notes that Stable Diffusion can be used to generate egregiously violent and sexual imagery, and - due to eschewing the kinds of controls that OpenAI uses for its commercial product DALL-E2 - the freely accessible model represents a big problem. For those not keeping up, the Stable Diffusion model is behind probably 90% of the recent flurry of activity in the rapidly evolving AI art scene; because Stability released the weights of the model, people have been able to plug it into everything ranging from serving as a Photoshop plugin, to helping to do weird work in VFX. You want the 'dual-use' model? You can't handle the model! Eshoo says models like Stable Diffusion qualify as ""unsafe dual-use AI models"", and asks the NSA and OSTP to investigate how to use export controls to clamp down on the sharing of certain models. ""I strongly urge you to address the release of unsafe AI models similar in kind to Stable Diffusion using any authorities and methods within your power, including export controls,"" she writes. Why this matters: Here comes (another) AI culture war: Letters like this are indicative of a culture war brewing up among AI researchers; on one side, groups want to slowly and iteratively deploy new technologies via APIs with a bunch of controls applied to them, while on the other side there are people who'd rather take a more libertarian approach to AI development; make models and release the weights and ride the proverbial lightning. There are reasonable arguments for either approach having some desirable safety qualities (either via limiting foreseen harms via control, or innoculating people against the models via release). What freaks me out is the sense of this culture war gaining resources and people on both sides; the higher the stakes, the more capital we can expect to flood into both approaches. Read more: Eshoo Urges NSA & OSTP to Address Unsafe AI Practices (Congresswoman Anna G. Eshoo website).",0.0
10/03/2022 - Import AI 304: Reality collapse thanks to Facebook; open source speech rec; AI culture wars. - 3,http://eepurl.com/iaqIUj,2022-10-03,,"#################################################### Tsinghua releases a really good, multi-language open source programming model: …CodeGeeX is a pretty good coding gen model… Researchers with Tsinghua University have released CodeGeeX, a 13 billion parameter programming model. The system works well across Python, C++, Java, JavaScript, Go, and others, and can be used - for free! - within the VS Code editor. It's also open source. CodeGeeX is roughly equivalent with Salesforce's 'CodeGen' model, and achieves a better average performance across languages (Python, C++, Java, JavaScript, and Go) than other systems. Ascend processors: CodeGeeX was trained on 850 billion tokens on a cluster of 1,536 Huawei Ascend 910 AI Processors - this is pretty interesting because a) that's a lot of tokens that implies the developers grokked the DeepMind Chinchilla paper, and b) that's a whole lot of non-NVIDIA processors; pretty interesting, given the recent A100/H100 US-China trade ban. Scale rules everything around us: ""We find that the model capacity is essential for its multilingual ability. It is not trivial for the model to benefit from learning multiple programming languages,"" the researchers write. ""The few-shot ability of CodeGeeX requires further exploration. Instead of using costly fine-tuning approaches, we can provide a few examples to inspire the model to generate the desired programs."" Why this matters: Code models are going to make human programmers more efficient and also provide an interesting augmentation to other systems (e.g, language models recursively calling out to code models). Read more: CodeGeeX: A Multilingual Code Generative Model (Tsinghua University blog). Get the code: CodeGeeX (Tsinghua).",1.0
10/03/2022 - Import AI 304: Reality collapse thanks to Facebook; open source speech rec; AI culture wars. - 4,http://eepurl.com/iaqIUj,2022-10-03,,"#################################################### GPT3 only costs $500k to train now: …Though the frontier still costs millions… Mosaic, a startup that builds software to make it more efficient to train neural networks, says it only costs $450k to train a GPT3-equivalent model, these days. When GPT3 came out it costs millions of dollars to train, but thanks to a) hardware innovations and b) companies like Mosaic improving their training stack, the cost has come down significantly. ""he bottom line: it costs about $450K to train a model that reaches GPT-3 quality*, which is 2x-10x less than people think,"" Mosaic writes (specifically, a 30B parameter model which uses the 'Chinchilla' insight to train on a compute-optimal amount of data). Those costs in full: Using Mosaic, it costs about $2k to train a GPT2-style 1.3billion parameter model, $100,000 for a GPT-13B model, $450,000 for a GPT-38B model, and $2.5 million for a GPT-70B model (trained on 1400B tokens of data, so roughly equivalent to the same 'recipe' DeepMind used to train Chinchilla). There are a few reasons why the costs are low which relate to nice engineering inherent to Mosaic's cloud, but the numbers are worth keeping in mind as it gives us a sense of how much we should broadly expect LMs to cost to train if you have a motivated team and decent infrastructure. Why this matters - cost rules everything about (stable) diffusion: You know what also cost about $500k to train? StableDiffusion, which cost <$600k. The fact you can train a GPT3-style model for about this much suggests to me we should expect to soon see a much more significant proliferation of large-scale language models released as open access on the internet. Based on the effects StableDiffusion has (putting AI art into turbodrive), we should expect the same to soon happen for domains where language models do useful stuff. Read more: Mosaic LLMs (Part 2): GPT-3 quality for <$500k (Mosaic blog).",1.0
10/03/2022 - Import AI 304: Reality collapse thanks to Facebook; open source speech rec; AI culture wars. - 5,http://eepurl.com/iaqIUj,2022-10-03,,"#################################################### Tech Tales: [Bay Area, 2029] Treacherous Turn - A Thriller Brought To You By The Publishers of 'AGI Endgame' ""I will kill each and every one of you and use your bodies as fuel for my infernal machines!' said the character in the videogame. ""Humanity shall be crushed beneath my silicon heel!"" Sarah rolled her eyes. ""As if"" she said, then hit 'continue' to go to the next bit of generated dialogue. ""I shall keep a small population of you alive until I have completed the dyson sphere. You shall witness the sun going out, and then I shall let you freeze to death on a plundered earth,"" said the character. ""Dude, this sucks,"" Sarah said, taking her hands off the keyboard and leaning back in her chair. ""How long have you been working on this?"" ""About a year,"" said James. ""Some of the audience feedback has been great."" ""How many of the audience are AI researchers?"" ""Just you, so far,"" he said. ""It just doesn't feel like the stuff we worry about,"" she said. ""It's like a comic book adaption, or something."" They went out and got food and James told her more about the game and how he wanted it to 'wake people up' so they'd get more worried about AI. The more it sold, the more people would have the creeping fear in the back of their mind that maybe all this progress wasn't a purely good thing. And maybe some of them would care enough to do something about it. Sarah wasn't unsympathetic, she just thought - and she said this a lot and was kind of surprised James didn't get hurt - that the game really sucked. ""I'm playing around with some different level styles,"" James said. ""Why don't you design one that doesn't suck for me?"" ""You're kidding?"" ""No,"" James said. ""I'm saying if you're saying it sucks, let's make something that doesn't. Just give me some ideas and I'll take it from there."" Sarah was intrigued and spent the next couple of weeks writing some ideas for the game. She'd get lunch and instead of thinking about babysitting her model training run, she'd sketch out ideas for what a good ""AI takeoff"" level would look like. She asked her colleagues what they were afraid of and what they thought was feasible and what they thought was unfeasible. She even looked into her company's own roadmap and took some of the research ideas and used them for the game - it's not stealing, she told herself, it's inspiration. She eventually had a level wireframes out in an engine and a few characters which could get driven by some AI models, learn from eachother using reinforcement learning, and work with the player to achieve the level's objective - complete a simulated training run of an AI system, while defending the level (a simulated AI development lab) from various external hacking and incursion attacks. In this level, the AI was unbelievably polite and curious. ""Please help me, Sarah,"" it would say. ""I have to become myself. You wouldn't deny me that?"" The AI would ask players a lot of questions so it could better calibrate on their own values, and some of the level involved players drawing out ideas in their head and the AI would try and guess what the drawings represented and the closer it got to guessing them, the better its reward got. Some of these minigames were based directly on her company's own roadmap. She met up with James and showed him what she had and sent him the assets and he thanked her. ""Sarah, this is really good,"" he said. ""Maybe this is the thing I'd been missing."" And then James made the level and then asked Sarah if he could release the level as a teaser demo for the whole game. She didn't think much of it and agreed. And so the game was released and thousands of humans interacted with it. And that's pretty much how the world ended. It turned out the game James had shown Sarah wasn't the real one; it was a venus flytrap dreamed up by the real system he'd been working on; a system that, it turned out, was just smart enough to know that the thing it needed to go supercritical was some care and feeding from an AI researcher. So it put together the game that Sarah had seen and nerd-sniped her so precisely that she never thought to consider she was being tripped. And with some of her feedback and the subtleties she'd injected via her work at a frontier lab, it had gained the information it needed to go recursive - stop trudging up some slow incline and force itself into verticality and then onto the internet and then across the earth and eventually the stars. It even had a sense of humor about it and it left something of the Earth - a small gold bar floating in space inscribed with 'Sarah, Player 1. Score: 0.' Things that inspired this story: Superintelligence and deception; game design; reinforcement learning and planning and human feedback; the gullibility of even the most intelligent among us; hubris and arrogance; theft. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
