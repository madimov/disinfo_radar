title,url,date,summary,cleaning,category
Google & MIT’s  Confident Adaptive Language Modeling Uses Dynamic Compute Allocation to Achieve 3x Speedups,https://syncedreview.com/2022/07/19/google-mits-confident-adaptive-language-modeling-uses-dynamic-compute-allocation-to-achieve-3x-speedups/,2022-07-19,"
In the new paper Confident Adaptive Language Modeling, a research team from Google and MIT presents Confident Adaptive Language Modeling (CALM), a framework that dynamically allocates different amounts of compute to each input and generation timestep, achieving up to 3x speedups while maintaining high performance.

 ","There was a nineteenth-century saying that mocked the use of “a sledgehammer to crack a peanut.” Google AI researcher Tal Schuster echoes this concept in introducing the new paper Confident Adaptive Language Modeling. While acknowledging the tremendous power of transformer-based large language models (LLMs), Schuster notes that many of the predictions they work on “require only minimal effort.” It could be said that using the entire LLM in such cases amounts to a sledgehammer-like overkill. LLMs’ ever-increasing computation costs and associated inference slowdowns are the main bottlenecks impeding their practical application. Developed by a Google and MIT team, the proposed Confident Adaptive Language Modeling (CALM) framework addresses these issues by dynamically allocating different compute amounts to each input and generation timestep. CALM achieves up to 3x speedups on natural language processing (NLP) tasks while maintaining high model performance. The team summarizes their main contributions as: The proposed framework is based on a saturation theory: that the top-ranked prediction in LLMs remains unchanged after some layer and is propagated upward. The number of layers used by the model can thus be dynamically decided with regard to each input. Following this idea, the team develops an adaptive compute approach to dynamically allocate computational resources per input to reduce model complexity while maintaining good performance. This method is also referred to as “early-exiting.” Building on their analysis of the early-exiting paradigm, the team developed CALM as a principled method for increasing model efficiency. CALM leverages a distribution-free risk control technique for calibrating local, per-token exit decisions, such that model performance is provably maintained with arbitrarily high probability. CALM can dynamically allocate different amounts of compute per generated token, following explicitly defined tolerance levels based on the full generation output. In their empirical study, the team implemented CALM on top of the T5 encoder-decoder model and evaluated text-generation task performance on three datasets — CNN/DM, WMT EN-FR, and SQUAD. The results show that CALM can reduce model compute burdens and gain speedups of up to 3x while maintaining high performance.The paper Confident Adaptive Language Modeling is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Amazon’s Sockeye 3: Neural Machine Translation With PyTorch That Is 126% Faster on GPUs,https://syncedreview.com/2022/07/18/amazons-sockeye-3-neural-machine-translation-with-pytorch-that-is-126-faster-on-gpus/,2022-07-18,"
Amazon has introduced the latest version of their Sockeye toolkit for the efficient training of stronger and faster neural machine translation (NMT) models. Sockeye 3 achieves speeds up to 126 percent faster than other PyTorch implementations on GPUs and up to 292 percent faster on CPUs.
","Anyone who regularly uses machine translation systems will have noticed huge performance improvements over the last few years, attributable to neural network-based models that have largely replaced the previous generation of phrase-based systems. Introduced in 2018, Sockeye is an open-source framework that offers fast and reliable PyTorch implementation for neural machine translation (NMT) and has been powering Amazon Translate and other NMT applications. Sockeye 2 was released in 2020. In the new paper Sockeye 3: Fast Neural Machine Translation with PyTorch, an Amazon team presents the latest version of the Sockeye toolkit for efficient training of stronger and faster models. Sockeye 3 achieves speeds up to 126 percent faster than other PyTorch implementations on GPUs and up to 292 percent faster on CPUs. Sockeye 3 optimizes a distributed mixed precision training strategy to yield faster calculations and speedups by fitting larger batches into memory. Moreover, it can scale to any number of GPUs and any size of training data by launching separate training processes that use PyTorch’s distributed data parallelism to synchronize updates. For inference design, Sockeye 3 uses static computation graphs to minimize the impacts of dynamic shapes and data-dependent control flow, enabling it to trace various model components via PyTorch’s JIT compiler. The developers also maintain backward compatibility with Sockeye 2 MXNet models — all models that were trained with Sockeye 2 can be converted to models running on Sockeye 3 with PyTorch. Sockeye 3 also introduces many new advanced features: It supports replacing the decoder’s self-attention layers with Simpler Simple Recurrent Units (SSRUs) and fine-tuning with parameter freezing, and enables users to specify arbitrary prefixes (sequences of tokens) on both the source and target sides for any input. In their empirical studies, the team compared Sockeye with benchmark NMT toolkits that included Fairseq (Ott et al., 2019) and OpenNMT (Klein et al., 2017). In the evaluations, Sockeye 3 achieved comparable or better performance on GPUs and CPUs: delivering a 15 percent improvement for batched GPU inference, +126 percent for non-batched GPU inference, and +292 percent for CPU inference. Overall, Sockeye 3 provides much faster model implementations and more advanced features for NMT. As with previous versions, It has been open-sourced under an Apache 2.0 license, and the Amazon team welcomes pull requests from community members. The code is available on the project’s GitHub. The paper Sockeye 3: Fast Neural Machine Translation with PyTorch is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Google Open-Sources Its TensorFlow GNN  Framework to Encourage Graph Neural Network Productization and Experimentation,https://syncedreview.com/2022/07/14/google-open-sources-its-tensorflow-gnn-framework-to-encourage-graph-neural-network-productization-and-experimentation/,2022-07-14,"
In the new paper TF-GNN: Graph Neural Networks in TensorFlow, a research team from Google Core ML, Google Research, and DeepMind open-sources the TensorFlow GNN (TF-GNN) scalable library, which leverages heterogeneous relational data to create graph neural network models.
","Graph Neural Networks (GNNs) that operate on graph-based data bring multimodal capabilities to machine learning models and have practical applications in areas as diverse as the modelling of physics systems, learning molecular fingerprints, predicting protein interfaces, classifying social networks and more. A key component for pushing GNN development is better software frameworks for learning from graph-structured data. In the new paper TF-GNN: Graph Neural Networks in TensorFlow, a research team from Google Core ML, Google Research, and DeepMind open-sources the TensorFlow GNN (TF-GNN) scalable library, which leverages heterogeneous relational data to create GNN models and enable GNN training and inference on arbitrary graph-structured data. The research team summarizes their main contributions as follows: TF-GNN includes four API components of varying abstraction levels to assist developers with varying machine learning expertise in creating graph models: 1) a data level for representing heterogeneous graphs and loading them into TensorFlow, which will appeal to proficient users; 2) a data exchange level for sending information between its nodes, edges, and the graph context, aimed at intermediate users; 3) a model level that offers trainable transformations of the data exchanged across the graphs; and 4) a minimal-code experience level for beginners, where an “Orchestrator” toolkit that includes popular graph learning objectives, distributed training capabilities and accelerator support — and can handle some of the vexing TensorFlow idiosyncrasies — enables simpler data input, feature processing, graph objectives, training and validation. TF-GNN models are currently being used by many Google teams, and the company hopes the library’s open-sourcing will facilitate the creation of GNNs for developers of all levels and push the industrial adaptation of these promising models at more organizations. The paper TF-GNN: Graph Neural Networks in TensorFlow is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",0.0
Colossal-AI Seamlessly Accelerates Large Models at Low Costs with Hugging Face,https://syncedreview.com/2022/07/13/colossal-ai-seamlessly-accelerates-large-models-at-low-costs-with-hugging-face/,2022-07-13,"
HPC-AI Tech’s flagship open-source and large-scale AI system, Colossal-AI, now allows Hugging Face users to seamlessly develop their ML models in a distributed and easy manner. 
","Forbes News, the world’s leading voice, recently declared large AI models as one of six AI trends to watch for in 2022. As large-scale AI models continue their superior performances across different domains, trends emerge, leading to distinguished and efficient AI applications that have never been seen in the industry. For example, Microsoft-owned GitHub and OpenAI partnered to launch Copilot recently. Copilot plays the role of an AI pair programmer, offering suggestions for code and entire functions in real-time. Such developments continue to make coding easier than before. Another example released by OpenAI, DALL-E 2, is a powerful tool that creates original and realistic images as well as art from only simple text. One month later, Google announced its own robust text-to-image diffusion model called Imagen. Imagen delivers exceptional results and accelerates the race of large AI models to a climax. Image Generated by Imagen (left 2 col.) vs DALLE-2 (right 2 col.)“Greek statue of a man tripping over a cat” In recent years, the outstanding performance of model scaling has led to an escalation in the size of pre-trained models. Unfortunately, training and even simply fine-tuning large AI models are usually unaffordable, requiring tens or hundreds of GPUs. Existing deep learning frameworks like PyTorch and Tensorflow may not offer a satisfactory solution for very large AI models. Furthermore, advanced knowledge of AI systems is typically required for sophisticated configurations and optimization of specific models. Therefore, many AI users, such as engineers from small and medium-sized enterprises, can’t help but feel overwhelmed by the emergence of large AI models. In fact, the core reasons for the increased cost of large AI models are GPU memory restrictions and the inability to accommodate sizeable models. In response to all of this, Colossal-AI developed the Gemini module, which efficiently manages and utilizes the heterogeneous memory of GPU and CPU and is expected to help solve the mentioned bottlenecks. Best of all, it is completely open-source and requires only minimal modifications to allow existing deep learning projects to be trained with much larger models on a single consumer-grade graphics card. In particular, it makes downstream tasks and application deployments such as large AI model fine-tuning and inference much easier. It even grants the convenience of training AI models at home! Hugging Face is a popular AI community that strives to advance and democratize AI through open source and open science. Hugging Face has had success collating large-scale models into their own model hub with over 50,000 models, including trendy large AI models like GPT and OPT. HPC-AI Tech’s flagship open-source and large-scale AI system, Colossal-AI, now allows Hugging Face users to seamlessly develop their ML models in a distributed and easy manner. In the following paragraphs, we will take one of the most popular AI models in Hugging Face Hub, OPT from Meta, to demonstrate how to train and fine-tune your large AI models at a low cost with minimal modifications to your code.Open source code: https://github.com/hpcaitech/ColossalAI Meta recently released Open Pretrained Transformer (OPT), a 175-Billion parameter AI language model. To encourage AI democratization in the community, Meta has released both the code and trained model weights, which stimulates AI programmers to perform various downstream tasks and application deployments. We will now demonstrate fine-tuning Casual Language Modelling with pre-training weights of the OPT model provided by Hugging Face Hub. It is very simple to use the powerful features of Colossal-AI. Users only need a simple configuration file, and are not required to alter their training logic to equip models with their desired features (e.g. mixed-precision training, gradient accumulation, multi-dimensional parallel training, and memory redundancy elimination).Suppose we intend to develop the OPT on one GPU. We can accomplish this by leveraging heterogeneous training from Colossal-AI, which only requires users to add relevant items to the configuration files. Among the items added, tensor_placement_policy, which can be configured as cuda, cpu, or auto, determines our heterogeneous training strategy. Each training strategy has its distinct advantages: For typical users, they can just select the auto strategy, which maximizes training efficiency by dynamically adapting its heterogeneous strategy with respect to its current memory state. With the configuration file ready, only a few lines of code are needed for the newly declared functions.Firstly, awaken Colossal-AI through a single line of code in the configuration file. Colossal-AI will automatically initialize the distributed environment, read in configuration settings, and integrate the configuration settings into its components (i.e. models and optimizers). After that, users may define their own datasets, models, optimizers, and loss functions per usual, or by using raw PyTorch code. Only their models need to be initialized under ZeroInitContext. In the given example, we adopt the OPTForCausalLM model along with its pretrained weights by Hugging Face and make adjustments to the Wikitext dataset. Next, use colossalai.initialize to integrate heterogeneous memory functions defined in the configuration file, into the training engine to enable the feature. On a single GPU, Colossal-AI’s automatic strategy provides remarkable performance gains from the ZeRO Offloading strategy by Microsoft DeepSpeed. Users can experience up to a 40% speedup, at a variety of model scales. However, when using a traditional deep learning training framework like PyTorch, a single GPU can no longer support the training of models at such a scale. Adopting the distributed training strategy with 8 GPUs is as simple as adding a -nprocs 8 to the training command of Colossal-AI! Such remarkable improvements come from Colossal-AI’s efficient heterogeneous memory management system, Gemini. To put it simply, Gemini uses a few warmup steps during model training to collect memory usage information from PyTorch computational graphs. After warm-up, and before performing each operation, Gemini pre-allocates memory for the operator equivalent to its peak usage based on the collected memory usage records. At the same time, it re-allocates some model tensors from GPU memory to CPU memory. The inbuilt memory manager by Gemini attaches a state to each tensor, including HOLD, COMPUTE, FREE, etc. Based on the queried memory usage, the manager constantly converts the tensor states, and adjusts tensor positions. Compared to the static memory classification by DeepSpeed’s ZeRO Offload, Colossal-AI Gemini employs a more efficient use of GPU and CPU memory, maximizes model capacities, and balances training speeds, all with small amounts of hardware equipment. For the representative of large models, GPT, Colossal-AI is capable of training up to 1.5 billion parameters on a gaming laptop with RTX 2060 6GB. For a PC with RTX3090 24GB, Colossal-AI can train GPT with 18 billion parameters. Colossal-AI can also bring significant improvements to high performance graphics cards such as a Tesla V100. Parallel and distributed technologies are vital methods to further accelerate model training. To train the world’s largest and most advanced AI models within the shortest time, efficient distributed parallelization is still a necessity. Issues found in existing solutions include limited parallel dimension, low efficiency, poor versatility, difficult deployment, and lack of maintenance. With this in mind, Colossal-AI uses technologies such as efficient multi-dimensional parallelism and heterogeneous parallelism to allow users to deploy large AI models efficiently and rapidly with minimal modifications to their code. To counter complications arising from data, pipeline, and 2.5D parallelism simultaneously, a simple line of code declaration suffices with Colossal-AI. The typical system/framework method of hacking into underlined code logic is no longer necessary. For a super-large AI model such as GPT-3, Colossal-AI only needs half the computing resources compared to the NVIDIA solution to start training. If the same computing resources were used, the speed could be further increased by 11%, which could reduce the training cost of GPT-3 by over a million dollars. In theory, this sounds fantastic, but what about in practice? Colossal-AI has proven its capabilities in application to real-world issues across a variety of industries, including autonomous driving, cloud computing, retail, medicine, and chip production. For AlphaFold, which is used for protein structure prediction, our team has introduced FastFold, based on the Colossal-AI acceleration scheme. FastFold has successfully surpassed other schemes including those proposed by Google and Columbia University. It successfully reduces the training time of AlphaFold from 11 days to 67 hours, simultaneously lowering the overall cost. Moreover, the process of long sequence inference is accelerated by about 9.3 to 11.6 times. Colossal-AI values open-source community construction. We offer detailed tutorials and support the latest cutting-edge applications such as PaLM and AlphaFold. Colossal-AI will regularly produce new and innovative features. We always welcome suggestions and discussions and would be more than willing to help if you encounter any issues. You can raise an issue here or create a discussion topic in our forum. Your suggestions are highly appreciated. Recently, Colossal-AI reached No. 1 in trending projects on Github and Papers With Code, together with projects that have as many as 10K stars. The open-source code is on Project’s GitHub. Referencehttps://medium.com/@yangyou_berkeley/colossal-ai-seamlessly-accelerates-large-models-at-low-costs-with-hugging-face-4d1a887e500d We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
"Academia Sinica’s YOLOv7 Outperforms All Object Detectors, Reduces Costs by 50%",https://syncedreview.com/2022/07/12/academia-sinicas-yolov7-outperforms-all-object-detectors-reduces-costs-by-50/,2022-07-12,"
 In the new paper YOLOv7: Trainable Bag-Of-Freebies Sets New State-Of-The-Art for Real-Time Object Detectors, an Academia Sinica research team releases YOLOv7. This latest YOLO version introduces novel “extend” and “compound scaling” methods that effectively utilize parameters and computation; and surpasses all known real-time object detectors in speed and accuracy.
","The 2016 release at CVPR of the YOLO (You Only Look Once) real-time object detector revolutionized the field of computer vision. YOLO delivered unprecedented speed and accuracy on a fundamental task with applications in autonomous driving, robotics, security, medical image analysis and more. Various techniques and tricks (multi-scale predictions, a better backbone classifier, etc.) have since been implemented to improve YOLO training and boost performance. A research team from Taiwan’s Institute of Information Science, Academia Sinica furthers YOLO development in their new paper YOLOv7: Trainable Bag-Of-Freebies Sets New State-Of-The-Art for Real-Time Object Detectors. This latest YOLO version introduces novel “extend” and “compound scaling” methods that effectively utilize parameters and computation; and surpasses all known real-time object detectors in speed and accuracy. The team summarizes their main contributions as: The team starts by building an efficient architecture. Their extended efficient layer aggregation network (Extended-ELAN, or E-ELAN) uses expand, shuffle, and merge cardinality to continuously enhance the network’s learning ability without changing the original gradient path, i.e. it changes only the computational block and leaves the transition layer untouched. For model scaling, the researchers propose a compound scaling method that can be applied to concatenation-based architectures and calculate changes in the output channel of a computational block to enable depth factor scaling. This proposed compound scaling method can thus maintain the properties of the original model design and the optimal structure. In their empirical study, the researchers compared the proposed YOLOv7 with state-of-the-art object detectors. YOLOv7 achieved 1.5 percent higher AP than YOLOv4 despite having 75 percent fewer parameters and using 36 percent less computation. When trained only on the MS COCO dataset and without any pretrained weights, YOLOv7 beat all other popular detectors (YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B) in the evaluations. The YOLOv7 source code has been released on the project’s GitHub. The paper YOLOv7: Trainable Bag-Of-Freebies Sets New State-Of-The-Art for Real-Time Object Detectors is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",0.0
A New Network Design Direction? DeepMind Examines How Networks Generalize and Climb the Chomsky Hierarchy,https://syncedreview.com/2022/07/11/a-new-network-design-direction-deepmind-examines-how-networks-generalize-and-climb-the-chomsky-hierarchy/,2022-07-11,"
In the new paper Neural Networks and the Chomsky Hierarchy, DeepMind researchers examine generalization in neural network architectures and whether insights from the theory of computation and the Chomsky hierarchy can predict the practical limits of network generalization.
","Reliable generalization to out-of-distribution inputs is a crucial feature for developing strong machine learning models. But determining how and why neural networks are able to generalize on algorithmic sequence prediction tasks remains an open question. In the new paper Neural Networks and the Chomsky Hierarchy, a DeepMind research team conducts an extensive generalization study on neural network architectures that explores whether insights from the theory of computation and the Chomsky hierarchy can predict the practical limits of neural network generalization. The team summarizes their main contributions as: Many previous works have investigated whether conventional neural network architectures are able to learn a formal language. While these studies have typically focused on single architectures and a limited set of tasks, the DeepMind paper presents an extensive empirical study on a wide range of models with regard to the Chomsky hierarchy. Named after the influential American linguist and philosopher who developed it, the Chomsky hierarchy is basically a containment hierarchy of formal grammar (unrestricted grammar, context-sensitive grammar, context-free grammar, and regular grammar) that classifies languages based on the type of automaton able to recognize them. By relating different models to the Chomsky hierarchy, it is possible to determine whether they can recognize certain regular languages. The researchers note that lower-level automata have restrictive memory models and can only solve lower-level problem sets, while atop the hierarchy, Turing machines with infinite memory and unrestricted memory access can solve all computable problems, i.e. are Turing complete. The paper examines a wide range of neural network architectures and memory-augmented neural networks — transformer, RNN, LSTM, Stack-RNN, NDStack-RNN and Tape-RNN — covering a total of 2200 models applied to 16 sequence-prediction tasks. The results show that LSTMs and transformers are not Turing complete as they cannot solve simple sequence tasks such as duplicating a string when the sequences are significantly longer than those seen during training. Models interacting with an external memory structures meanwhile can climb the Chomsky hierarchy, indicating this setup as a promising research direction for improving architecture design.The code is publicly available on the project’s GitHub. The paper Neural Networks and the Chomsky Hierarchy is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Salesforce’s CodeRL Achieves SOTA Code Generation Results With Strong Zero-Shot Transfer Capabilities,https://syncedreview.com/2022/07/07/salesforces-coderl-achieves-sota-code-generation-results-with-strong-zero-shot-transfer-capabilities/,2022-07-07,"
In the new paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning, a Salesforce Research team presents CodeRL, a novel framework for program synthesis tasks that employs pretrained language models (LMs) and deep reinforcement learning (RL) and achieves state-of-the-art performance on the challenging APPS benchmark while also demonstrating impressive zero-shot transfer capabilities. 
","Large-scale pretrained language models (LMs) have shown promising results on simple code generation tasks, but they have several limitations: training models with only next-token prediction objectives leads to accumulating errors, and neglecting potentially meaningful signals from unit tests results in poor generalization capability when facing complex unseen coding tasks. A Salesforce Research team addresses these issues in the new paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning, proposing CodeRL, a novel framework for program synthesis tasks that employs pretrained LMs and deep reinforcement learning (RL) and achieves state-of-the-art performance on the challenging APPS benchmark while demonstrating impressive zero-shot transfer capabilities. The team extends the Salesforce CodeT5 (Wang et al., 2021) unified pretrained encoder-decoder transformer architecture as CodeRL’s backbone. Although CodeT5 pretraining tasks such as masked span prediction (MSP) can benefit code understanding tasks, they do not necessarily align with program synthesis objectives. To mitigate this, a next-token prediction (NTP) pretraining task is integrated into CodeT5 to uniformly sample a pivot location for each code sample, then pass the content preceding the pivot to the encoder and the remaining content to the decoder. The researchers formulate CodeRL’s program synthesis as an RL problem and introduce an actor-critic approach to improve model performance by utilizing the unit test signals in both the model optimization and generation processes. The team conducted experiments on the challenging APPS (Automated Programming Progress Standard) code generation benchmark (Hendrycks et al., 2021) to evaluate the performance of the proposed CodeRL; and used the MBPP (Mostly Basic Programming Problems) benchmark (Austin et al., 2021) to evaluate its zero-shot ability. On APPS, researchers compared their models with strong conventional baselines that included GPT-2, GPT-Neo, GPT3, Codex, and AlphaCode, where CodeRL with CodeT5 achieved new SOTA results of 2.69 percent pass@1, 6.81 percent pass@5, and 20.98 percent pass@1000. On MBPP, CodeRL with CodeT5 obtained surprisingly good zero-shot performance, achieving a new SOTA of 63.0 percent pass@80 over GPT-137B’s 61.4 percent pass@80. This work shows that the CodeRL method can effectively leverage unit test signals to push code generation performance to new SOTA performance and achieve strong zero-shot transfer capabilities. The code is available on the project’s GitHub. The paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
NYU Explores the  Principles for Modelling Neural Collapse and Its Role in Generalization,https://syncedreview.com/2022/07/06/nyu-explores-the-principles-for-modelling-neural-collapse-and-its-role-in-generalization/,2022-07-06,"
 In the new paper Neural Collapse: A Review on Modelling Principles and Generalization, researchers from New York University analyze Neural Collapse (NC) and present a thought model to explain the effects of variance collapse, aiming at a better understanding of the generalization capabilities of neural networks. 
","Deep neural networks (DNNs) have advanced the state-of-the-art on tasks ranging from image classification to language processing and gameplay. But as models have become deeper and more complex, understanding their behaviours has become more challenging. A case in point is an intriguing empirical phenomenon called Neural Collapse, first identified by Papyan et al. in 2020. In the new paper Neural Collapse: A Review on Modelling Principles and Generalization, researchers from New York University analyze the principles of Neural Collapse (NC) and present a thought model designed to explain the effect of variance collapse, aiming at insights on and a better understanding of the generalization capabilities of DNNs. The team summarizes their main contributions as: The modern training paradigm for DNNs involves training well beyond the zero error threshold and toward zero loss. This post-zero-error phase is called the Terminal Phase of Training (TPT), which begins at the epoch where training error first vanishes. During TPT, the training error stays effectively zero while the training loss is pushed to zero. The TPT however is exposed to a pervasive inductive bias, NC, which involves four deeply interconnected phenomena: The team uses a principled approach to review the NC phenomena, first confirming that the final layer classifiers in DNNs tend to fall into a simple symmetric structure that helps the models obtain their high performance and state-of-the-art results. In an effort to capture the essence of the NC phenomena, the researchers then analyze such models from the ground up and unify them under a common set of principles. Overall, the paper provides a solid overview of current efforts to explain NC, It also probes the implications of NC on generalization and transfer learning via a thought model that explains the effects of variance collapse on transfer learning based on the inverse-square law to provide additional insights on the generalization capabilities of DNNs. The team hopes their work’s analytical results will be of interest to the deep learning community and encourage future research in this area.The paper Neural Collapse: A Review on Modelling Principles and Generalization is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",0.0
Learning Bellman Complete Representations for Offline Policy Evaluation,"[{'href': 'http://arxiv.org/abs/2207.05837v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.05837v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-12 21:02:02,,"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 An Empirical Evaluation of Four Off-the-Shelf Proprietary Visual-Inertial Odometry Systems Jungha Kim, Minkyeong Song, Yeoeun Lee, Moonkyeong Jung, and Pyojin Kim∗ 2 2 0 2 l u J 4 1 ] O R . s c [ 1 v 0 8 7 6 0 . 7 0 2 2 : v i X r a Abstract—Commercial visual-inertial odometry (VIO) systems have been gaining attention as cost-effective, off-the-shelf six degrees of freedom (6-DoF) ego-motion tracking methods for estimating accurate and consistent camera pose data, in addition localization from to their ability to operate without external is unclear motion capture or global positioning systems. It from existing results, however, which commercial VIO platforms are the most stable, consistent, and accurate in terms of state estimation for indoor and outdoor robotic applications. We assess four popular proprietary VIO systems (Apple ARKit, Google ARCore, Intel RealSense T265, and Stereolabs ZED 2) through a series of both indoor and outdoor experiments where we show their positioning stability, consistency, and accuracy. We present our complete results as a benchmark comparison for the research community. Index Terms—Commercial visual-inertial odometry, Apple ARKit, Google ARCore, Intel T265, Stereolabs ZED 2 I. INTRODUCTION T HIS article presents a benchmark comparison of off-the- shelf proprietary visual-inertial odometry (VIO) systems used for autonomous navigation of robotic applications, which are the process of determining the position and orientation of a camera-inertial measurement unit (IMU)-rig in 3D space by analyzing the associated camera images and IMU data. As the VIO research has reached a level of maturity, there exist several open published VIO methods such as MSCKF [1], OKVIS [2], VINS-Mono [3], and many commercial prod- ucts utilize closed proprietary VIO algorithms such as Apple ARKit [4], Google ARCore [5] that offer off-the-shelf VIO pipelines which can be employed on an end-user’s system of choice. The current research studies provide some comparative experiments on the performance of the popular VIO ap- proaches, however, they consider only a subset of the existing open-source and proprietary VIO algorithms, and conduct insufﬁcient performance evaluation only on publicly-available datasets rather than indoor and outdoor challenging real-world environments. In particular, although commercial VIO systems (Intel T265, Stereolabs ZED 2) play an important role in the several DARPA challenges [6], [7] and many commercial products or apps (Pok´emon GO, IKEA Place AR), there is a lack of research for benchmarking the positioning accuracy of these closed proprietary VIO platforms. The motivation of this paper is to address this deﬁciency by performing a comprehensive evaluation of off-the-shelf All authors are with Department Systems Engineering, Sookmyung Women’s University, Seoul, South Korea. {alice3071,smk615,snwfry,jmk7791,pjinkim}@ sookmyung.ac.kr (∗ Corresponding author: Pyojin Kim) of Mechanical Fig. 1. The custom-built capture rig for benchmarking 6-DoF motion tracking performance of Apple ARKit (iPhone 12 Pro Max), Google ARCore (LG V60 ThinQ), Intel RealSense T265, and Stereolabs ZED 2. commercially-available VIO systems in challenging indoor and outdoor environments as shown in Fig. 2. This is the ﬁrst comparative study on four popular proprietary VIO systems in six challenging real-world environments, both indoors and outdoors. Especially, we select the following four proprietary VIO systems that are frequently used in autonomous driving robotic applications: • Apple ARKit [4] - Apple’s augmented reality (AR) plat- form, which includes ﬁltering-based VIO algorithms [8] to enable iOS devices to sense how they move in 3D space. • Google ARCore [5] - Google’s AR platform utilizing a multi-state constraint Kalman ﬁlter (MSCKF) style VIO algorithm [1], [9], called concurrent odometry and mapping (COM) [10]. • Intel RealSense T265 [11] - a stand-alone VIO and simultaneous localization and mapping (SLAM) tracking device developed for use in robotics, drones, and more, with all position computations performed on the device. • Stereolabs ZED 2 [12] - a hand-held stereo camera with built-in IMU for neural depth sensing and visual-inertial stereo, requiring an external NVIDIA GPU to obtain the 6-DoF camera poses. We do not consider open-source published VIO methods and non-inertial visual simultaneous localization and map- ping (SLAM) algorithms, for example ROVIO [13], VINS- Mono [3], ORB-SLAM [14], and DSO [15]. We focus on the off-the-shelf commercial VIO/SLAM products that might be of interest to more researchers and engineers because open pub- lished VIO algorithms are relatively difﬁcult to understand and operate, and their comparisons are made in the literature [16], [17] to some extent. figure_capture_rig.pdf Google ARCore (LG V60 ThinQ) Apple ARKit (iPhone 12 Pro Max) Intel RealSense T265 Stereolabs ZED 2 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2 Fig. 2. Accumulated 3D point cloud (middle) with the estimated 6-DoF trajectory (red) from Apple ARKit in multi-ﬂoor environments. We capture the 6-DoF camera poses and 3D points while climbing the multi-story stairs (left). Among the four proprietary VIO systems, Apple ARKit shows the most consistent and accurate 6-DoF motion tracking results, consistently reconstructing the 3D geometry of stairs and hallways. The Apple ARKit track (red) and 3D reconstruction results have a similar shape as the ground-truth blueprint of a building (right). Our experiments are conducted in six challenging indoor and outdoor environments with the custom-built rig equipped with the four VIO devices as illustrated in Fig. 1. Our test sequences contain long and narrow corridors, large open spaces, repetitive stairways, an underground parking lot with insufﬁcient lighting, and about 3.1 kilometers of a vehicular test in complex urban trafﬁc environments. test Our goal is to provide a thorough benchmark of closed proprietary VIO systems, in order to provide a reference for researchers on VIO products, as well as readers who require an off-the-shelf 6-DoF state estimation solution that is suitable for their robotic platforms and autonomous vehicles. II. RELATED WORK Despite proprietary VIO systems being utilized in many products and areas for industrial usage (e.g., for building an accurate indoor map, as a precise positioning system, etc.), there is no benchmark study that satisﬁes our proposed goals. While comprehensive comparisons of open-source published VIO methods exist [16], they focus only on evaluating the popular academic VIO algorithms on the EuRoC micro aerial vehicle dataset [18], and do not cover off-the-shelf proprietary VIO systems and various indoor and outdoor environments. Although ADVIO [19] presents a VIO comparison including three proprietary platforms and two academic approaches, its main contribution is to develop a set of RGB and IMU smartphone datasets, not a performance evaluation between the proprietary VIO platforms. In [20], [21], some comparative studies of proprietary VIO systems have been performed, they consider only a few proprietary VIO platforms. but Performance evaluation is only conducted in a simple 2D indoor environment with a short camera moving distance. Since we focus on the 6-DoF positioning accuracy of the proprietary VIO systems, we can instead consider the existing results relevant to this problem. The proposed VIO approach in [22] compares to Google ARCore and VINS-Mono [3], but only on a few indoor sequences with very little camera movement. The evaluation framework in [23] assesses the 6- DoF motion tracking performance of ARCore with the ground truth under several circumstances, but they lack comparative results for other proprietary VIO systems such as ARKit and T265, and detailed analyses are performed only for ARCore. Most important is that no existing work considers an in- door/outdoor performance evaluation for four popular propri- etary VIO systems that are frequently deployed on robotic applications, AR/VR apps, and industrial usages. Our test se- quences are authentic and illustrate realistic use cases, contain- ing challenging environments with scarce or repetitive visual features, both indoors and outdoors, and varying motions from walking to driving camera movements. They also include rapid rotations without translation as they are problematic motions for many VIO/SLAM algorithms. III. VISUAL-INERTIAL ODOMETRY SYSTEMS We brieﬂy summarize the primary features of four off- the-shelf proprietary VIO systems based on data published on the relevant ofﬁcial websites, papers, GitHub, and patent documents, and how to collect 6-DoF pose estimates from each VIO mobile device. Since most proprietary VIO/SLAM platforms are all closed-source, we do not cover the detailed VIO academic backgrounds and implementations. A. Apple ARKit Apple ARKit [4] is Apple’s augmented reality (AR) soft- ware framework, which includes a tightly-coupled ﬁltering- based VIO algorithm similar to the MSCKF [1] to enable iOS devices to sense how they move in 3D space. It contains a sliding window ﬁlter, bundle adjustment, motion/structure marginalization modules [8], and is expected to be applied to various robotic applications such as Apple Glasses and Car in the future, not just for iPhone and iPad, which is why we conduct vehicle tests in this benchmark. We develop a custom iOS data collection app1 for capturing ARKit 6-DoF camera poses, RGB image sequences, and IMU measurements using an iPhone 12 Pro Max running iOS 14.7.1. It saves the pose estimates as a translation vector and a unit quaternion at 60 Hz, 1https://github.com/PyojinKim/ios logger figure_front_page.pdf Floor 5 Floor 4 Floor 3 Floor 2 Floor 1 19.2 m 13.2 m 9.2 m 4.6 m 0.0 m JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3 and each pose is expressed in a global coordinate frame created by the phone when starting iOS data collection. Although there are various iPhone and iPad models, the core VIO algorithm in ARKit is the same, thus we empirically conﬁrm that there is little difference in the VIO performance of each device. B. Google ARCore ARCore [5] is Google’s platform for building AR ex- periences utilizing the multi-state constraint Kalman ﬁlter (MSCKF) style VIO/SLAM algorithms [9], [24] with many subsequent variations, called concurrent odometry and map- ping (COM) [10]. ARCore is a successor to Google Project Tango [25], and is currently applied only to Android OS smartphones, but it would be extended to various robotic platforms such as Google Wing, Maps, and Waymo, which is why we evaluate ARCore in a large-scale outdoor sequence of about 3.1 kilometers of a vehicular test. We build a custom Android OS app based on Google’s ARCore example2 to acquire ARCore 6-DoF camera poses and IMU measurements at 30 Hz with an LG V60 ThinQ running Android 10.0.0 and ARCore 1.29. Although there are various Android OS devices such as Samsung Galaxy and Google Pixel, smartphones on the list3 certiﬁed by Google demonstrate similar motion tracking performance regardless of device model. C. Intel RealSense T265 a hassle-free Intel RealSense T265 stand-alone is VIO/SLAM device to track its own position and orientation in 3D space. The embedded processor, vision processing unit (VPU), runs the entire VIO algorithm onboard, analyzes the image sequences from stereo ﬁsheye cameras and fuses all sensor information together. Since the T265 VIO algorithm runs on the device itself without using the resource of the host computer, is widely used as a 6-DoF positioning sensor in 3D space for various robotic applications such as DARPA challenges [6] and autonomous ﬂying drones [26]. We collect the 6-DoF motion tracking results at 200 Hz using Intel RealSense SDK 2.04, and save the T265 6-DoF camera poses by connecting it to an Intel NUC mini PC. it D. Stereolabs ZED 2 Stereolabs ZED 2 is a hand-held stereo camera with built-in IMU for neural depth sensing, 6-DoF VIO/SLAM, and real- time 3D mapping. Stereolabs has not made their VIO/SLAM algorithm public, and the description of the VIO algorithm is relatively vague compared to other proprietary VIO systems. It is one of the popular stereo camera sensors for various robotic applications such as drone inspection [27], but has the disadvantage of requiring an external NVIDIA GPU to perform positional tracking and neural depth sensing. We develop a program to collect the ZED 2 6-DoF camera poses at 30 Hz based on ZED SDK 3.5.25 on an NVIDIA Jetson Nano onboard computer. 2https://github.com/rfbr/IMU and pose Android Recorder 3https://developers.google.com/ar/devices 4https://github.com/IntelRealSense/librealsense 5https://www.stereolabs.com/developers/release/ Fig. 3. We carry the capture rig by hand, and store the onboard computers and batteries to collect the motion data indoors (left). In the outdoor vehicular tests, we ﬁx the capture rig to the front passenger seat (right). IV. EXPERIMENTS We evaluate four proprietary VIO systems with the four devices (iPhone 12 Pro Max, LG V60 ThinQ, Intel T265, ZED2) attached to the custom-built capture rig as shown in Fig. 1 and Fig. 3 on the large-scale challenging indoor and outdoor environments, both qualitatively and quantitatively. Indoors, we record the motion data by a walking person, and outdoors, the data is collected by rigidly attaching the capture rig to a car in Fig. 3. We save the 6-DoF pose estimates of ARKit and ARCore through the custom apps on each smartphone device, and record the moving trajectories of T265 and ZED2 in the Intel NUC and NVIDIA Jetson Nano onboard computers. We maintain the default parameter settings of each VIO platform, and deactivate all capabilities related to SLAM (e.g., loop closure) for a fair comparison between each VIO system. Furthermore, in order to interpret the motion tracking results in the same reference coordinate frame, we calibrate the intrinsic and extrinsic parameters of all cameras by capturing multiple views of a checkerboard. Our benchmark dataset contains various indoor and outdoor sequences in six different locations, and the total length of each sequence ranges from 83 m to 3051 m, which are primarily designed for benchmarking medium and long-range VIO performance. There are three indoor and three outdoor sequences, and all indoor sequences are captured in a 7-story building in the university campus including long corridors, open hallway spaces, and stair climbs as shown in the top row of Fig. 4. The indoor cases are as realistic as possible contain- ing repetitive motion in stairs, temporary occlusions, and areas lacking visual features. The bottom row of Fig. 4 illustrates example frames from three outdoor sequences acquired in the outdoor university campus, underground parking lot, and urban outdoor roads. In order to evaluate the performance of each VIO system quantitatively without an external motion capture system, we coincide the start and end points of the movement trajectories in all experiments, and measure the ﬁnal drift error (FDE) metric, which is the end point position error in meter. We report the quantitative evaluation results of four VIO systems in Table I. The smallest end point position error for each sequence is indicated in bold. The ideal FDE value (the ground-truth path) should be 0, and a large FDE value denotes an inaccurate position estimate since we deﬁne the starting point of movement as the origin. In addition, by overlaying the estimated VIO trajectories on the ﬂoorplan of the building figure_external_view.pdf Google ARCore Apple ARKit Intel T265 ZED 2 Laptop Google ARCore Apple ARKit Battery Intel NUC & NVIDIA Jetson Nano Stereolabs ZED 2 Intel RealSense T265 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 Fig. 4. Example image frames from indoor and outdoor benchmark datasets. The top row represents three indoor sequences by foot including long corridors (a), open hallway spaces (b), and repetitive stairs (c) from a university building. We acquire the camera motion data in the outdoor campus on foot (d), and through a car in the underground parking lot (e) and urban outdoor roads (f). TABLE I EVALUATION RESULTS (FDE) OF FOUR PROPRIETARY VIO SYSTEMS Experiment ARKit ARCore T265 ZED 2 Length (m) Indoor Corridor Indoor Hallway Indoor Stairs Outdoor Campus Parking Lot Outdoor Roads 0.79 0.14 0.19 2.01 0.26 2.68 0.12 0.09 3.98 0.07 1.14 140.08 1.88 0.61 1.49 4.08 9.01 × 1.44 4.58 4.76 206.38 10.85 409.25 145.21 83.98 114.13 513.81 446.26 3051.61 or Google Maps, we evaluate the consistency, stability, and reliability of each VIO system qualitatively. A. Indoor Long Corridors and Open Hallway Sequences We evaluate four VIO systems in a long U-shape corridor and open hallway spaces easily found in typical ofﬁce and uni- versity buildings as shown in Fig. 5. Fig. 4 (a) and (b) illustrate example frames from both locations. The trajectories of these sequences are approximately 145 and 84 meters, and include 5 and 11 pure rotational movements and difﬁcult textures. In a long U-shape corridor and open hallway sequences, the start and end points of ARKit (red) meet at the black circle without a severe rotational drift while maintaining the orthogonality and scale of the estimated trajectory well compared with the ﬂoorplan. Although ARCore (green) shows the most accurate results in terms of the FDE metric in Table I, the estimated VIO trajectory does not match the ﬂoorplan well. Intel T265 (blue) estimates accurate 3-DoF rotational motion well, but there is a problem with the scale of the moving trajectory compared to the ﬂoorplan, showing a little larger trajectory than the actual movements. ZED2 (magenta) presents the most inaccurate and inconsistent positioning performance among the four VIO methods as the rotational motion drift error gradually accumulates over time. Overall, the estimated VIO trajectories by ARKit (red) are the most similar and consistent motion tracking results to the actual movements following the shape of the corridor on the ﬂoorplan. B. Indoor Multistory Stairs Sequence We perform a comparative experiment in a multi-ﬂoor staircase environment with the 114 m trajectory going up the stairs from the 2nd basement ﬂoor (B2) to the 5th ﬂoor (5F) of a building in Fig. 6. The repetitive rotational motion included in the 3D trajectory of climbing the stairs makes VIO positioning challenging. Fig. 4 (c) shows example frames from multistory stair sequence. In the top view (xy-plane), we start and end at the same points marked in the black circle to check loop closing in the estimated VIO trajectories. ARKit (red) has the best performance; the top and side views of ARKit (red) show the overlapped, consistent 6-DoF motion tracking results while other VIO systems gradually diverge from the initially estimated loop. With ARKit (red), the starting and ending points in xy-plane (top view) nearly match; for the others, they do not. The ﬁnal drift error (FDE) of ARKit in xy-plane is 0.19 m, while ARCore, T265, and ZED2 are 3.98 m, 1.49 m, and 4.76 m, respectively. In particular, ZED2 (magenta) has the most severe trajectory distortion in the z-axis direction (height) among the four VIO systems. Fig. 6 illustrates the side and front views of the stairway with the paths from four VIO devices, showing high consistency of ARKit (red) compared to other VIO platforms. It is noteworthy that the height of each ﬂoor estimated by ARKit and the actual height (the ground- truth) from the building blueprint are approximately identical. C. Outdoor University Campus Sequence We choose an outdoor location in the university campus approximately 513 m to determine which VIO system works well in the environment of the rapid change of the topogra- phy, and the narrow road returning as shown in the left of Fig. 7. Example frames are shown in Fig. 4 (d). It shows the resulting 6-DoF trajectories from four VIO platforms overlaid on Google Maps, demonstrating that the start and end points of ARKit (red) and ARCore (green) meet while matching well with the shape of the roads shown on Google Maps. The shape of the estimated trajectory of T265 (blue) is very figure_example_frames.pdf (a) Indoor Corridor (b) Indoor Hallway (c) Indoor Stairs (d) Outdoor Campus (e) Underground Parking Lot (f) Urban Outdoor Roads JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5 Fig. 5. Estimated trajectories with four proprietary VIO systems in a long U-shape corridor (left) and open hallway space (right) sequences. We start and end at the same point marked in the black circle to evaluate the loop closing performance of tested commercial VIO systems. The estimated paths for ARKit (red) match the building ﬂoorplan most consistently, and only the starting and ending points of ARKit nearly meet; for the others, they do not. due to its inaccurate rotation estimation, showing the most severe distortion of the actual movement trajectory among the four VIO systems as shown in the left of Fig. 7. D. Outdoor Urban Roads and Parking Lot Sequences We perform an outdoor vehicle driving experiment with a mileage of approximately 3 km by attaching the capture rig to the vehicle as shown in the right of Fig. 7. Fig. 4 (e) and (f) show example frames from the underground parking lot and urban outdoor roads. We acquire the motion data while driving on public automobile roads near Seoul Station in Seoul, and there are plenty of moving people, cars, and occasional large vehicles visible in the outdoor environments, which makes mo- tion tracking of VIO challenging. Even in high-speed driving conditions, sometimes exceeding 60 km/h, ARKit (red) shows surprisingly accurate and consistent 6-DoF motion tracking results overlaid on Google Maps as shown in the right of Fig. 7. The start and end points of ARKit (red) accurately meet in the black circle, and the ﬁnal drift error (FDE) is only 2.68 m in Table I. ARCore (green) occasionally fails when the speed of the car increases or the light variations occur abruptly. In T265 (blue), if the car stops temporarily due to a stop signal or is driving too fast, the VIO algorithm diverges and fails to estimate the location. ZED2 (magenta) accumulates rotational drift error over time, resulting in inaccurate motion estimation results. While four VIO systems perform relatively well in the previous walking sequences, this is not the case in the more challenging vehicular test, which is not ofﬁcially supported by any of the tested VIO devices. Only ARKit is able to produce stable motion tracking results even in a vehicular test. We conduct an additional vehicular test driving the same trajectory repeatedly in a dark underground parking lot with poor visual conditions as shown in Fig. 8. The total traveling distance is about 450 m, and we drive the car at a low speed from 5 to 15 km/h. Although ARKit does not restore the actual movements perfectly in the parking lot, ARKit (red) shows the Fig. 6. Comparison of four VIO systems on multi-story stairs from the 2nd basement ﬂoor (B2) to the 5th ﬂoor (5F). It shows the side (left), front (right- bottom), and top (right-top) views of the estimated VIO trajectories. ARKit has the most consistent camera motions along with the shape of the stairs, and only matches the start and end points marked in the black circle. similar to ARKit and ARcore’s results, however, the scale of the estimated path of T265 is smaller than the actual movements. T265 suffers from a scale inconsistency problem, which is generally observed in monocular visual odometry conﬁguration. The orthogonality of ZED2 (magenta) is broken JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6 Fig. 7. Estimated motion trajectories of four proprietary VIO systems in an outdoor campus (left) and urban outdoor roads (right) sequences overlaid on Google Maps. We start and end at the same point marked in the black circle to check loop closing performance. ARKit (red) tracks the 6-DoF camera poses well following the shape of the roads on Google Maps most consistently and accurately. Only ARKit (red) is able to produce stable motion tracking performance even when driving a vehicle over 60 km/h (right). in realistic use cases where people and vehicles are very crowded not only indoors but also outdoors. Google ARCore exhibits accurate and consistent motion tracking performance next to ARKit. ARCore works well for indoor sequences and the motion data collected by a walking person, but it diverges or the VIO algorithm deteriorates sharply when moving rapidly or in poor lighting conditions. Intel RealSense T265 shows good positioning performance just behind Google ARCore. T265 operates 6-DoF motion tracking indoors not badly, however, it has a problem of scale inconsistency issue when estimating the moving path larger or smaller than the scale of the actual movements. Also, T265’s motion tracking sometimes fails if the moving speed is too slow or fast. The motion tracking performance of Stereolabs ZED 2 is the most inconsistent and inaccurate among the four VIO de- vices for indoors and outdoors. As the 6-DoF motion tracking progresses, the rotational error occurs most severely, and this rotation error accumulates over time, resulting in an incorrect path in which the starting and ending points are very different. In particular, ZED2 exhibits a tendency that it cannot track a straight path correctly when we actually move in a straight line outdoors, and rotational drift error is more severe when moving fast. VI. CONCLUSION Fig. 8. Example paths in the underground parking lot overlaid on the ﬂoorplan to evaluate the consistency and accuracy. The trajectories of ARKit (red) overlap signiﬁcantly, but the paths of the other VIO devices suffer from a rotational drift, showing inaccurate and inconsistent positioning results. overlapped, consistent motion estimation results while other VIO systems gradually diverge from the initially estimated loop. Since we perform the evaluation at a relatively low speed (10 km/h) compared to the previous vehicle test (60 km/h), other VIO systems do not diverge or fail at all. Among four VIO methods, the ZED2 positioning results are the most deviating from the actual movements in the underground parking lot. V. DISCUSSION Overall, Apple ARKit demonstrates the most consistent, accurate, reliable, and stable motion tracking results among the four VIO systems across both indoor and outdoor uses. ARKit performs well and robustly in various real-world challeng- ing environments such as sudden camera movements, abrupt changes in illumination, and high-speed movements with very rare cases where tracking failure or motion jump occurs. ARKit achieves accurate and robust positioning performance We have conducted a survey of the 6-DoF ego-motion tracking performance of four off-the-shelf proprietary VIO platforms in challenging indoor and outdoor environments. To the best of our knowledge, this is the ﬁrst back-to-back comparison of ARKit, ARCore, T265, and ZED2, demon- strating that Apple ARKit performs well and robustly in most indoor and outdoor scenarios. We hope that the results and conclusions presented in this paper may help members of the research community in ﬁnding appropriate VIO platforms for their robotic systems and applications. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7 REFERENCES [1] A. I. Mourikis and S. I. Roumeliotis, “A multi-state constraint kalman ﬁlter for vision-aided inertial navigation,” in IEEE ICRA, 2007. [2] S. Leutenegger, P. Furgale, V. Rabaud, M. Chli, K. Konolige, and R. Siegwart, “Keyframe-based visual-inertial slam using nonlinear op- timization,” Proceedings of Robotis Science and Systems (RSS) 2013, 2013. [3] T. Qin, P. Li, and S. Shen, “Vins-mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018. [4] “Apple ARKit,” https://developer.apple.com/documentation/arkit/, Ac- cessed: 2022-02-22. [5] “Google ARCore,” https://developers.google.com/ar, Accessed: 2022- 02-22. [6] T. Rouˇcek, M. Pecka, P. ˇC´ıˇzek, T. Petˇr´ıˇcek, J. Bayer, V. ˇSalansk`y, D. Heˇrt, M. Petrl´ık, T. B´aˇca, V. Spurn`y, et al., “Darpa subterranean challenge: Multi-robotic exploration of underground environments,” in International Conference on Modelling and Simulation for Autonomous Systems. Springer, 2019. [7] P. Root, “Fast lightweight autonomy (ﬂa),” Defense Advanced Research Projects Agency, https://www. darpa. mil/program/fast-lightweight- autonomy [retrieved 31 Dec. 2018], 2021. [8] A. Flint, O. Naroditsky, C. P. Broaddus, A. Grygorenko, S. Roumeliotis, and O. Bergig, “Visual-based inertial navigation,” Dec. 11 2018, US Patent 10,152,795. [9] A. I. Mourikis, N. Trawny, S. I. Roumeliotis, A. E. Johnson, A. Ansar, and L. Matthies, “Vision-aided inertial navigation for spacecraft entry, descent, and landing,” IEEE Transactions on Robotics, 2009. [10] E. Nerurkar, S. Lynen, and S. Zhao, “System and method for concurrent odometry and mapping,” Oct. 13 2020, US Patent 10,802,147. [11] “Intel RealSense Tracking Camera T265,” https://www.intelrealsense. com/tracking-camera-t265/, Accessed: 2022-02-22. [12] “Stereolabs ZED 2 Stereo Camera,” https://www.stereolabs.com/zed-2/, Accessed: 2022-02-22. [13] M. Bloesch, S. Omari, M. Hutter, and R. Siegwart, “Robust visual inertial odometry using a direct ekf-based approach,” in 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS), 2015. [14] R. Mur-Artal and J. D. Tard´os, “Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras,” IEEE transactions on robotics, 2017. [15] J. Engel, V. Koltun, and D. Cremers, “Direct sparse odometry,” IEEE transactions on pattern analysis and machine intelligence, 2017. [16] J. Delmerico and D. Scaramuzza, “A benchmark comparison of monoc- ular VIO algorithms for ﬂying robots,” in IEEE ICRA, 2018. [17] C. Campos, R. Elvira, J. J. G. Rodr´ıguez, J. M. Montiel, and J. D. Tard´os, “Orb-slam3: An accurate open-source library for visual, visual–inertial, and multimap slam,” IEEE Transactions on Robotics, 2021. [18] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W. Achtelik, and R. Siegwart, “The euroc micro aerial vehicle datasets,” The International Journal of Robotics Research, vol. 35, no. 10, pp. 1157–1163, 2016. [19] S. Cort´es, A. Solin, E. Rahtu, and J. Kannala, “ADVIO: An authentic dataset for visual-inertial odometry,” in ECCV, 2018. [20] A. Alapetite, Z. Wang, and M. Patalan, “Comparison of three off-the- shelf visual odometry systems,” Robotics, 2020. [21] S. Ouerghi, N. Ragot, and X. Savatier, “Comparative study of a com- mercial tracking camera and ORB-SLAM2 for person localization,” in VISAPP, 2020. [22] Y. Ling, L. Bao, Z. Jie, F. Zhu, Z. Li, S. Tang, Y. Liu, W. Liu, and T. Zhang, “Modeling varying camera-imu time offset in optimization- based visual-inertial odometry,” in Proceedings of the European Con- ference on Computer Vision (ECCV), 2018. [23] H. G¨umg¨umc¨u, “Evaluation framework for proprietary slam systems exempliﬁed on google arcore,” Master’s thesis, ETH Zurich, 2019. [24] E. D. Nerurkar, K. J. Wu, and S. I. Roumeliotis, “C-klam: Constrained keyframe-based localization and mapping,” in 2014 IEEE international conference on robotics and automation (ICRA). [25] E. Marder-Eppstein, “Project tango,” in ACM SIGGRAPH 2016 Real- Time Live!, 2016, pp. 25–25. [26] R. Bonatti, R. Madaan, V. Vineet, S. Scherer, and A. Kapoor, “Learning visuomotor policies for aerial navigation using cross-modal representa- tions,” in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). [27] R. Fan, J. Jiao, J. Pan, H. Huang, S. Shen, and M. Liu, “Real-time dense stereo embedded in a uav for road inspection,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2019.",1.0
An Empirical Study of Implicit Regularization in Deep Offline RL,"[{'href': 'http://arxiv.org/abs/2207.02099v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.02099v2', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-05 15:07:31,,"A Comparison of Source Distribution and Result Overlap in Web Search Engines AUTHORS SECTION Yagci, Nurce Sünkler, Sebastian Häußler, Helena Lewandowski, Dirk HAW Hamburg, Germany | nurce.yagci@haw-hamburg.de HAW Hamburg, Germany | sebastian.suenkler@haw-hamburg.de HAW Hamburg, Germany | helena.haeuessler@haw-hamburg.de HAW Hamburg, Germany | dirk.lewandowski@haw-hamburg.de ABSTRACT When it comes to search engines, users generally prefer Google. Our study aims to find the differences between the results found in Google compared to other search engines. We compared the top 10 results from Google, Bing, DuckDuckGo, and Metager, using 3,537 queries generated from Google Trends from Germany and the US. Google displays more unique domains in the top results than its competitors. Wikipedia and news websites are the most popular sources overall. With some top sources dominating search results, the distribution of domains is also consistent across all search engines. The overlap between Google and Bing is always under 32%, while Metager has a higher overlap with Bing than DuckDuckGo, going up to 78%. This study shows that the use of another search engine, especially in addition to Google, provides a wider variety in sources and might lead the user to find new perspectives. KEYWORDS Web search; search engine; web scraping; Google; source comparison INTRODUCTION Why should there be more than one search engine? While users may prefer one search engine over others for its usability, specialized features, or a more convenient integration into their technical environment, the question that interests us in this research is whether a user will benefit from using another search engine than Google when it comes to finding results from different sources. Our starting point is the fact that Google is the most-used search engine by far (StatCounter, 2022), that user to a large degree trust search engines to provide them relevant and useful results (European Commission, 2016; Purcell et al., 2012), and that only some users use another search engine in addition to Google (Schultheiß & Lewandowski, 2021). Users place great trust in search engines. This is reflected by the 91% of US users who said they find what they are looking for always or most of the time, and the 66% who believe search engines are a fair and unbiased source of information (Purcell et al., 2012). Furthermore, 78% of European internet and online platform users said they trust that their search engine results are the most relevant results (European Commission, 2016). Globally, users trust search engines more than any other source (including traditional news outlets) when it comes to news (Edelman Trust Institute, 2022) and users trust news found via search significantly more than news found on social media (Newman et al., 2021). As the Web is enormous and different search engines might prefer different sources, it is interesting to see whether the top sources shown in search results differ from one search engine to the other. It might be that an alternative search engine prefers results from ""alternative"" sources, e.g., in terms of political leaning or preferring non- commercial content providers. This all comes down to whether alternative search engines are actually alternatives in regards to the results they display. If they were, possible benefits of using a search engine other than Google include finding different results, finding additional results, and finding more relevant results. No matter which of these goals a user aims to achieve, they will need other results than Google's. Therefore, it is interesting to see whether other search engines provide users with such results. There has been an ongoing discussion on alternative search engines and how Google's dominance in the search engine market can be broken. Approaches range from establishing single alternative search engines to building infrastructures for such alternatives (e.g., Lewandowski, 2019); also see Mager, 2014). With Google dominating the search engine market (StatCounter, 2022), it often seems that there are no alternatives at all. On the other hand, the number of alternative (or simply ""other"") search engines is often overestimated. Many seem-to-be search engines are merely search portals displaying results from a partner instead of generating the results from their own index. For instance, Yahoo and Ecosia get their results from Bing and can therefore not be considered search engines in their own right. But still, there may be other reasons for using a search engine without its own index. Some of the unique benefits alternative search engines advertise are privacy (e.g., Startpage and DuckDuckGo) or being a company investing its profits in environmental projects (e.g., Ecosia). Another type of search engine is the meta search engine (e.g., Metager). Such an engine sends the queries to several other search engines, then aggregates and re-ranks the top results. We deem it especially interesting whether such an approach will lead to a wider variety of search results, i.e., results from a more diverse set of sources. So, in the context of our research, we will consider any search engine that either has its own index or provides a unique selection and re-ranking of results from one or more indexes as an alternative search engine. We are especially interested in the differences in the source distribution; the relevance of the results is out of the scope of our research. More than 20 years ago, Introna & Nissenbaum (2000) argued that search engines as commercial operations tend to prefer big websites and, therefore, a portion of the Web, i.e., the smaller sites, remain hidden from view. Studies measuring what users select seem to confirm this: Goel et al. (2010) found that within Yahoo, only 10,000 websites account for approximately 80% of result clicks. It is important to note that this does not merely result from user preference for particular sources but that users predominantly select from the top results shown by the search engine. What is out of the immediate view of users will not be chosen (Lewandowski & Kammerer, 2021). It is striking that few studies have compared the results between different search engines in recent years. Older studies (see literature review section) overall found that top results from different search engines did not overlap too much. In this paper, we address how the top results of Google differ from alternatives and, therefore, whether it is worthwhile for a user to consider these alternatives. If a search engine other than Google produced very similar results to Google, a user would not benefit much from using that search engine when source variety is considered. LITERATURE REVIEW Search result overlap By the mid-1990s, researching the overlap of search results between different search engines sparked interest for the purpose of estimating the size of the Web (Bharat, 1998; Ding & Marchionini, 1996; Lawrence & Giles, 1999). The generally small degrees of overlap indicated diverse underlying databases, each of limited size. Therefore, meta- search engines that combined the results of several search engines were meant to provide an additional value (Chignell et al., 1999) and attracted further research (i.e., Meng et al., 2002). Consequently, Spink et al. (Spink et al., 2006) included the meta search engine Dogpile in their extensive study on the overlap of search results. For a collection of 12,570 queries, the results found on the first search engine result page (SERP) of Ask Jeeves, Google, MSN Search, and Yahoo were captured. 84.9% of the results were unique to one search engine, while only 1.1% were shared by all search engines. Additionally, the low overlap between search engines also manifests in the ranking since only 7% of the top results were similar. This is consistent with previous studies that reported low overlap in ranking (Bar-Ilan, 2005; Bar‐Ilan et al., 2006). Subsequent studies report an increase of overlap in search results, while ranking algorithms appear to be the leading cause for differences in the presentation of the results. Bilal and Ellis (2011) compared major web search engines (Google, Bing, Yahoo) with search engines specifically designed for children (Ask Kids, Yahoo Kids) for queries of various lengths. Bing and Yahoo shared the same overlap with Google for nearly all queries, ranging between 22% and 40%. In contrast, Yahoo Kids had mostly unique results. However, the disparate relevance ranking between Bing and Yahoo suggests different ranking algorithms. Cardoso and Magalhães (2011) measured the overlap of search result rankings based on URLs and website contents. Search results for 40,000 queries were retrieved from Google and Bing. The overlap in domains was about 29%, and Google had more exclusive domains. When looking at the result sets without considering the positions, the similarity between the result sets increased. This indicates that Google and Bing have different ranking preferences but index mostly the same sources. Similarly, Agrawal et al. (2016) based their overlap analysis on content but factored in the description of the result snippets. The top 10 results of 67 informational queries were collected from Google and Bing. The results indicate a high overlap between Google and Bing, with a slightly higher similarity among the top 5 results. Most recently, a study about information on Covid-19 was conducted by Makhortykh et al. (2020). The first 50 results for queries in different languages were collected from Baidu, Bing, DuckDuckGo, Google, Yandex, and Yahoo. DuckDuckGo and Yahoo shared nearly 50% of their results, whereas the other pairs remained under 25%. Google had an overlap of around 10% with Bing and a negligible one with DuckDuckGo, Yandex, Yahoo, and Baidu. Source diversity Various studies took a closer look at search results and evaluated the domains, the types of sources, as well as their diversity across result sets and search engines. Comparing Google, Live Search, and Yahoo, Thelwall (2008) reported that Yahoo returned the highest amount of different domains to a query and also returned around 10% more top-level domains than other search engines. Höchstötter & Lewandowski (2009) showed that while the sources in top results of various search engines differ, there is a concentration on some popular sources in top results. More recently, Lewandowski & Sünkler (2019) collected search results on queries related to insurance providers and identified the distribution of top-level domains. Only ten different domains were found in the first result across all queries. The five most popular domains in the top 5 results are price comparison websites, which make up 88.4% of all first results. The share of popular domains decreases according to the ranking but remains at 42.9% at position ten. Unkel and Haim (2019) observed Google search results prior to the German parliamentary elections in 2017. The study found that result lists for candidates and parties exhibit a high share of self-administered websites. In contrast, results about election facts, guidance, and issues are mainly from general interest news, government information, and privately run websites. Among the top ten domains across all search results are seven news websites which account for a quarter of all search results. By contrast, Wikipedia made up 5.4% of all search results. The high share of news and Wikipedia articles is confirmed by Steiner et al. (2022) who compared the first results for queries on debated topics in Germany across the search engines Ask, Bing, DuckDuckGo, Google, and Ixquick. All search engines positioned Wikipedia in the first rank for queries on climate change and the Transatlantic Trade and Investment Partnership (TTIP); Ask did so for almost every query. In the other search engines, news sites were placed in the first rank most of the time. However, for the topic Covid-19, the sources are more diverse. As Makhortykh et al. (2020) observed, only Yahoo incorporates recent information from legacy media, whereas Bing strongly relied on healthcare-related sources and Google highlighted government-related websites. Yandex was the only search engine that included alternative media in its top 20 search results. Especially for recent topics that lack an established knowledge basis, the choice of search engine may considerably impact what information a user gets to see. OBJECTIVES AND RESEARCH QUESTIONS Our study addresses the following research questions: 1. Do top results from alternative search engines differ from Google's in regard to the number of unique sources? 2. Do top results from alternative search engines differ from Google's in regard to top sources? 3. Do top results from alternative search engines differ from Google's in regard to source concentration, i.e., are results distributed over more or fewer sources in different search engines? To answer these questions, we selected three alternative search engines to compare to Google. Aside from Bing, which is the biggest competitor to Google, we chose DuckDuckGo and Metager. DuckDuckGo uses results from Bing, Yahoo, and Yandex, and has the added benefit of advanced privacy settings such as non-personalized results. Metager, a German meta-search engine, aggregates results from several search engines, including Bing and Yandex. As such, either search engine might provide a more varied set of sources. Since the alternative search engines each have an index different from Google's, the comparison will give insights into what domains are favored by Google and which domains are excluded. Also, we will gain insights into the differences in ranking between them. METHODS As illustrated in Figure 1, this study was conducted in five steps. In the first step, the query sets were generated from previously collected Google Trends data. The daily trends for both Germany and the US were collected daily at 3 am CET from November 10th, 2021, until March 31st, 2022. Since the daily number of trending queries can vary, the query sets for Germany and the US are not the same size. Two thousand nine hundred sixteen trending queries for Germany and 2,819 for the US were initially collected. However, since some topics of interest are longer lived than a single day, the same topics can be trending on multiple days. Therefore, after removing duplicates, our query sets consist of 1,821 queries for Germany and 2,126 queries for the US. The queries have a wide range, including topics like celebrities (e.g., ""Sandra Bullock""), sports (e.g., ""NFL,"" ""Liverpool vs. Southampton""), seasonal events (e.g., ""Macy's Thanksgiving Day Parade,"" ""Restaurants open on Thanksgiving""), headline news (e.g., ""TikTok class- action lawsuit settlement""), and more. Figure 1. Methodology of the current study In step 2, for each query, the top 10 results from all four search engines were collected. We made sure to request results for the appropriate language and location by using URL parameters. Furthermore, we decided to limit the comparison to the top 10 results because users usually only consider these. Again, for our study, only the implications for the user perception are relevant. The Web scraping component of the Relevance Assessment Tool (RAT; Sünkler et al., 2022) was used to collect the data. The scraping took place between April 6th and 9th, 2022, with 230,315 results (Germany: 109,604; US: 120,711) gathered. It is important to note that only organic search results were considered. A fair comparison can be ensured when ranked result lists instead of vertical inserts of universal search results (e.g., Google News and Bing News) are considered. We also ignored ads since they are not part of the search engine's web index. During step 3, the python library urllib was used to get the domain of each search result. To ensure a fair comparison between all search engines, in step 4, we removed results for any query that had less than ten results in any of the search engines. Additionally, the data was checked and stripped of any errors that might have occurred during the collection, like duplicate results. This resulted in a further reduced dataset consisting of 1,672 queries and 66,880 results for Germany and 1,865 queries and 74,600 results for the US. Lastly, the extraction of top-level domains was further refined in this step. We used basic string matching to unify URLs that pointed to the same domains. For example, the following three URLs would have been counted as separate domains without the string matching: https://www.facebook.com, ""http://www.facebook.com"", and ""http://www.facebook.com/"". Finally, in step five, we used different methods of comparing the data collected. An initial comparison is made by using simple descriptive statistics. To measure source concentration, we adapt the Gini coefficient, a measure of statistical dispersion used to measure income or wealth inequality (Gini, 1936). The Gini coefficient is a single number ranging from 0 to 1, where 0 represents perfect equality, and 1 is the maximum inequality. As Ortega et al. (2008) demonstrated, adapting this measure to the distribution of values in different domains is appropriate. In the case of search result sets, this allows us to compare the various search engines easily. The lower the Gini coefficient, the more equally the results are distributed over all domains contributing to a particular result set. Following that, we calculate the Jaccard similarity index (González et al., 2008). Puschmann (2019) demonstrates the usage of the Jaccard index to measure the similarity between two result sets. The Jaccard index is a single number ranging from 0 to 1, where 0 means that the two sets are entirely dissimilar, while 1 means that they are identical. We calculate the Jaccard index for every two-way combination of our four search engines (e.g., Google and Bing, Bing and Metager, etc.) for all stages between the top 1 and top 10 results (e.g., Google and Bing position 1 to 5). RESULTS Classification of domains Since we are working with sources from two different countries and the type of content behind some domains might not be immediately recognizable, we manually created a general set of categories based on the top 50 domains found across all results. Following this, we manually classified the 50 most popular domains for both the German and US results (see Table 1). When comparing the top 50 domains in the German results, News service dominates with 54%, while Movies & Entertainment and Sports make up 18% and 14%, respectively. In the US, 34% of the top 50 domains are Sports websites, with News services following close behind at 30%. Movies & Entertainment only make up 12%. These are the overall distributions of domain classes across all search engines; below, we look at the differences between the search engines. Example Top 50 Germany (share) Top 50 US (share) Class Celebrities E-Commerce variety.com amazon.com Government website bundesregierung.de Information service wikipedia.org Movies & Entertainment News service Sports imdb.com cnn.com espn.com Social media instagram.com 0.06 0.02 0 0.02 0.18 0.54 0.14 0.04 0.10 0.06 0 0.02 0.12 0.30 0.34 0.06 Table 1. Classes of domains and their frequency in the top 50 domains Variety of domains A comparison of the number of root domains in the search results of Google, Bing, DuckDuckGo, and Metager in Germany shows that Google has the greatest diversity by a small margin (see Figure 2). Overall, the values are very similar. However, it is noticeable that Google has the greatest variety of domains, especially in the first three positions. Interestingly, the greater diversity of Google's sources is even more pronounced in the US results. To examine the differences more closely, we look at the numbers below. Figure 2. Cumulative number of unique domains Table 2 shows the cumulative frequencies for the German results. The number of unique root domains converges at the fifth position. We found 2,841 unique domains in Google's results, 2,783 unique domains for Bing, 2,707 unique domains for DuckDuckGo, and 2,683 unique domains for Metager in Germany. Position 1 2 3 4 5 Google Bing DuckDuckGo Metager # 609 959 1,216 1,480 1,703 share 0.21 0.34 0.43 0.52 0.60 # 389 778 1,072 1,325 1,602 share 0.14 0.28 0.39 0.48 0.58 # 302 544 900 1,232 1,530 share 0.11 0.20 0.33 0.46 0.57 # 372 735 1,027 1,298 1,531 share 0.14 0.27 0.38 0.48 0.57 Position 6 7 8 9 10 Google Bing DuckDuckGo Metager # share # share # share # share 1,891 2,092 2,329 2,577 2,841 0.67 0.74 0.82 0.91 1.00 1,853 2,071 2,292 2,528 2,783 0.67 0.74 0.82 0.91 1.00 1,773 2,047 2,272 2,497 2,707 0.65 0.76 0.84 0.92 1.00 1,785 2,008 2,222 2,453 2,693 0.66 0.75 0.83 0.91 1.00 Table 2. Cumulative number of unique domains (Germany; #: cumulative number, %: percentage of total) There are differences when comparing the search results in Germany and the US. It is notable that, in contrast to Germany, in the US results, there are only minor differences in the first positions. However, Google also shows the greatest variety of root domains in the search results in the US. This is more evident than in the German results (see Table 3). Overall, we found 4,085 unique domains in Google, 3,602 in Bing, 3,579 in DuckDuckGo, and 3,500 in Metager. Position 1 2 3 4 5 6 7 8 9 10 Google Bing DuckDuckGo Metager # 508 961 1,398 1,828 2,188 2,531 2,896 3,289 3,682 4,085 share 0.12 0.24 0.34 0.45 0.54 0.62 0.71 0.81 0.90 1.00 # 521 851 1,164 1,491 1,822 2,168 2,521 2,870 3,231 3,602 share 0.14 0.24 0.32 0.41 0.51 0.60 0.70 0.80 0.90 1.00 # 493 849 1,210 1,547 1,860 2,208 2,546 2,888 3,207 3,579 share 0.14 0.24 0.34 0.43 0.52 0.62 0.71 0.81 0.90 1.00 # 485 820 1,130 1,445 1,772 2,110 2,442 2,785 3,133 3,500 share 0.14 0.23 0.32 0.41 0.51 0.60 0.70 0.80 0.90 1.00 Table 3. Cumulative number of unique domains (USA; #: cumulative number, %: percentage of total) Popular domains When comparing the top domains for each search engine across all German search results collected, we find a clear preference for Wikipedia in all of them (see Table 4). While Wikipedia is the most popular domain in all search engines, the frequency in the Google results is significantly lower, with only 658 compared to the number of occurrences in Bing being 1,948, in Metager 1,878, and in DuckDuckGo 1,752. The other domains in the top 10 across the search engines are News services (Google: 7, Bing: 4, DuckDuckGo: 3, Metager: 4). Sports make up 2 out of 10 for all but Google, with only one sports website. Surprisingly, Instagram is in the top 10 for DuckDuckGo and Metager, even though in Table 1, we showed that domains of the class Social Media only make up 4% of top domains in the German results. The same goes for Amazon, which is the second most frequent domain in DuckDuckGo, even though in the overall results, E-Commerce websites only make up 2%. No. Google Bing DuckDuckGo Metager 1 2 3 wikipedia.org (658) wikipedia.org (1,948) wikipedia.org (1,752) wikipedia.org (1,878) spiegel.de (414) stern.de (375) zdf.de (324) bild.de (298) amazon.de (735) zdf.de (344) zdf.de (298) bunte.de (314) No. Google Bing DuckDuckGo Metager 4 5 6 7 8 9 gala.de (300) bild.de (288) bunte.de (296) kicker.de (261) bild.de (290) kicker.de (294) bild.de (255) kicker.de (285) t-online.de (279) gala.de (279) tagesschau.de (251) gala.de (282) tagesschau.de (271) tagesschau.de (274) wunderweib.de (248) tagesschau.de (268) kicker.de (222) sportschau.de (261) bunte.de (245) sportschau.de (256) zdf.de (219) web.de (252) instagram.com (243) web.de (238) 10 sueddeutsche.de (219) instagram.com (245) sportschau.de (242) instagram.com (232) Table 4. Top 10 domains shown in different search engines (Germany) Contrarily, in the US results, Wikipedia has the highest number of occurrences in Google with 1,892 compared to Bing's 1,388, Metager's 1,304, and DuckDuckGo's 1,287 (see Table 5). Still, what remains the same is that Wikipedia is the most popular domain across all search engines. Another difference in the US results is that Social Media sources are much more prevalent across all search engines, especially in Google, with Instagram as the second most frequent domain, Facebook as the third, and Twitter as the sixth. This is surprising when considering that Social Media domains only make up 6% of the top domains for the US results (see Table 1). Another finding is that YouTube, a subsidiary of Google, is the top 8th domain for all three alternatives but not for Google. Again, 30% of the top domains for Google, DuckDuckGo, and Metager are News services. For Bing, it is 40%. For each search engine, Sports websites make up 20% of domains. No. Google Bing DuckDuckGo Metager 1 2 3 4 5 6 7 8 9 wikipedia.org (1,892) wikipedia.org (1,388) wikipedia.org (1,287) wikipedia.org (1,304) instagram.com (726) imdb.com (669) imdb.com (644) yahoo.com (714) facebook.com (503) yahoo.com (634) yahoo.com (642) imdb.com (680) espn.com (377) espn.com (610) espn.com (601) espn.com (614) cbssports.com (363) nypost.com (518) nypost.com (536) nypost.com (514) twitter.com (328) cbssports.com (511) cbssports.com (513) cnn.com (504) imdb.com (244) cnn.com (495) cnn.com (506) cbssports.com (502) britannica.com (226) youtube.com (414) youtube.com (352) youtube.com (417) usatoday.com (222) msn.com (322) go.com (320) msn.com (324) 10 nytimes.com (220) instagram.com (318) instagram.com (281) instagram.com (288) Table 5. Top 10 domains for each search engine (USA) Exclusivity of domains This section compares the top 50 domains for each search engine to determine what source Google and its alternatives might exclusively offer to the user. Table 6 shows the domains found solely in Google's top 50 domains list for the German queries. Out of these 20 domains, eight are Sports websites, six are News services, and four belong to the Entertainment & Movies class, with one each for E-Commerce and Government websites. ran.de amazon.de dazn.com sky.de eurosport.de zeit.de dfb.de kino.de fr.de fernsehserien.de tagesspiegel.de filmstarts.de bundesregierung.de tz.de weltfussball.de spox.com sportbuzzer.de deutschlandfunk.de rnd.de dw.com Table 6. Domains only found in Google's top 50 (Germany) On the contrary, table 7 shows the list of domains found in the top 50 domains of all three alternatives but not in Google. These would be the domains missed by users who only use Google. Out of the 13 domains, four are News services, followed by three Movies & Entertainment and two Celebrities sources. Interestingly, Instagram and YouTube are missing from Google, as well. sport.de instagram.com wunderweib.de fussballdaten.de promipool.de n-tv.de abendzeitung-muenchen.de finanzen.net youtube.com imdb.com ardmediathek.de express.de daserste.de Table 7. Domains found in all search engines but Google's top 50 (Germany) When implementing the same evaluation for the US results, we find that 17 domains are exclusively found in Google's top 50 domains, with five domains classified as Sports, four as Movies & Entertainment, another four as News services, three as Social Media and one as Celebrities (see Table 8). Notable is that social media giants Facebook and TikTok are only found in Google and not in the alternatives. marca.com reuters.com rotowire.com facebook.com tiktok.com usmagazine.com npr.org discogs.com spotify.com theathletic.com spotrac.com linkedin.com deadline.com forbes.com variety.com washingtonpost.com sports-reference.com Table 8. Domains only found in Google's top 50 (US) On the opposite end, 13 domains are not found in the US results on Google (see Table 9). Six of them are News services, three are websites about Celebrities, two are about Movies & Entertainment, and one each are Sports and E-Commerce websites. Interestingly, Amazon is missing in Google's results, as well as msn.com, a service from Microsoft, like Bing itself. amazon.com yardbarker.com msn.com cnn.com pagesix.com screenrant.com heavy.com foxnews.com decider.com tmz.com newsweek.com huffpost.com biography.com Table 9. Domains found in all search engines but Google's top 50 (US) Distribution of domains Next, we look at the Gini coefficient to measure the concentration and statistical dispersion of root domains in search engines. The distributions across all search engines in both German and US results show very similar values. The results range from 0.77 to 0.79 and 0.73 to 0.76 for German and US results, respectively (see Figure 3). Overall, this means that there is an imbalance in the distribution of results from root domains and that some top sources dominate the search results for both countries. Figure 3. Source distribution of domains (Germany) Similarity of domains Comparing the similarities of the top 10 German results, we find that Bing and Metager are most similar in terms of their top results, with 70%, followed by 64% overlap between DuckDuckGo and Metager and 63% between Bing and DuckDuckGo. However, when comparing the alternative search engines to Google, we find that the highest overlap is between Google and Bing with 28% and slightly lower ones with DuckDuckGo and Metager at 27%. Interestingly, the top 1 result overlap is higher between Google and Bing (31%) than the overlap between Bing and DuckDuckGo (15%). The results show a mean overlap of 30% of top 1, 40% of top 3, and 47% of top 10 results (table 10). This indicates that when looking at the entire first search results page, nearly half of the domains are the same in all search engines considered. For search engine users, this means that in addition to Google, this would also allow them to see search results that they would otherwise miss if they used the alternative search engine Rank Top 1 Top 3 Top 10 Google Bing Google DDG Google Metager 0.31 0.30 0.28 0.10 0.21 0.27 0.29 0.30 0.27 Bing DDG 0.15 0.39 0.63 Bing Metager DDG Metager 0.78 0.78 0.70 0.15 0.39 0.64 Mean 0.30 0.40 0.47 Table 10. Jaccard similarity of domains (Germany; DDG: DuckDuckGo) The same evaluation for the US yields similar results. Again, the overlap between Bing and Metager is the highest, but the margin between this pair and the other alternative pairings has been reduced. When looking at the top 10 results, Bing and Metager overlap by 65%, Bing and DuckDuckGo by 64%, and DuckDuckGo and Metager by 62%. While Google's overlap with the other search engines is lower in the US results, again, the pair of Google and Bing is slightly higher (25%) than the 24% overlap between Google and the other two alternatives (see Table 11). Narrowing the results down to the top 3, the overlap between Bing and DuckDuckGo increases minimally to 65% and remains the same for DuckDuckGo and Metager at 62%. Here, the overlap between Google and Bing is lower than in the German results, with only 26%. Furthermore, the top 1 result overlap between Google and Bing is at 29%, which is also lower than in the German results. Generally, compared to the German results, there is a higher overlap in the top 1 position for the pairs Google and DuckDuckGo (20%), Bing and DuckDuckGo (50%), and DuckDuckGo and Metager (47%). The averaged results make this trend clearer because the overlaps for the top 1, top 3, and top 10 results are similar, with 41%, 46%, and 44%, respectively. Rank Top 1 Top 3 Top 10 Google Bing Google DDG Google Metager 0.29 0.26 0.25 0.20 0.24 0.24 0.28 0.25 0.24 Bing DDG 0.50 0.65 0.64 Bing Metager DDG Metager 0.74 0.71 0.65 0.47 0.62 0.62 Mean 0.41 0.46 0.44 Table 11. Jaccard similarity of domains (US; DDG: DuckDuckGo) DISCUSSION Our study examined whether there are differences in the sources of the top search results between Google and alternative search engines based on popular search queries. An evaluation of root domains of the most popular sources shows only a small overlap between Google and the alternative search engines (RQ1). Overall, we found an overlap of 27% to 28% between Google and the alternatives in German results, and in the US, the overlap ranges from 24% to 25%. There is a significantly higher overlap between the alternative search engines of about 63% to 70% in German results and 62% to 65% in US results. This may be explained by all three alternative search engines using Bing's index at least in part. However, our findings show that Metager consistently has the highest overlap with Bing, going up to 78% and only overlapping as much as 64% with DuckDuckGo. The lower overlap of Google with the alternative search engines had already been shown in the studies of Agrawal et al. (2016) and Makhortykh et al. (2020). Our study provides further evidence for this. When looking at the uniqueness of sources across all German queries, we found that the search engines returned a similar total number of sources, ranging from 2,693 to 2,841. However, for the US queries, the variety in Google was noticeably higher, with over 4,000 total sources compared to around 3,500 of the alternatives. The most popular domain was Wikipedia, followed by sources we classified as News services (RQ2). This is likely exacerbated by selecting Google Trends as the source of our search queries, which usually includes queries related to popular news stories, sports, and celebrities. Still, it is consistent with previous studies' findings (Steiner et al., 2022) which already showed that Wikipedia and news made up the majority of search results. Furthermore, in our research, Wikipedia and news sources were the top domains across all results and the most frequent sources for each search engine individually. When comparing the result sets of the top 10 results from Google, Bing, DuckDuckGo, and Metager, using 3,537 queries generated from Google Trends from Germany and the US, it is interesting to see that news sources are far more prevalent in German results, with social media being very infrequent. On the other hand, the US results had more social media websites. Interestingly, in the US results, YouTube was in the top 10 most popular domains in all search engines but Google. The same was the case for the top 50 domains in German results. This is unexpected because YouTube is a subsidiary of Google. However, this finding may be explained by the fact that we only collected organic search results, and Google might be using universal search results to display YouTube results. Another interesting difference is the greater preference of Wikipedia in Google in US results (1,892, 10,2% of all domains) compared to German results (658, 4% of all domains). Furthermore, the second and third most popular sources on Google are Instagram and Facebook for the US results, while they are not even in the top 10 of German Google sources. Finally, in terms of what is missing from Google in the US results, it is notable that Fox News is not found in the top 50 sources, while it is present in all of the alternatives. The concentration of sources and source diversity showed a tendency for only a few root domains to make up a large share of search results. The Gini Index values of 0.73 and 0.79 in Germany and the United States, respectively, are a clear indicator (RQ3). This is consistent with findings from previous studies (Höchstötter & Lewandowski, 2009) that showed that only a few top sources dominate the search results in search engines. Of course, our study is not without limitations. First, the selection of search queries is the most significant factor in compiling the data that was evaluated. Even though the number of queries is high and there is some diversity in the queries, the topics are almost always focused on news, celebrities, and sports, which inevitably leads to many news sources in the search results. A refined approach to selecting search queries would be appropriate for a more accurate evaluation of source diversity. For example, this could be achieved by focusing on socially controversial topics and choosing the queries accordingly. Further limitations arise in the evaluation of source types. The classification we used in this study is very broad. A more precise classification would help make statements about the actual kinds of sources and thus to determine even more precisely which preferences and biases exist in different search engines. For example, grouping sources according to seriousness and reliability could serve as an explanation for the selection of sources to be displayed in search results. Regarding the search results collected in this study, another limitation is that only organic search results were considered. We did not consider advertisements or universal search results, although these have a strong influence on what users see on the SERP. The relevance of the search results was also not considered. CONCLUSION This study provides important insights into whether, although Google is by far the most popular search engine, the use of alternatives could benefit users. Our results show that using another or more than one search engine leads to seeing more diverse search results, allowing users to inform themselves more comprehensively. It should be noted that within each search engine's results, the concentration of sources shows that only a few top sources dominate the results, meaning whichever search engine a user chooses to use will shape what sources the information they get to see comes from. RESEARCH DATA Research data is available at: https://osf.io/nt3wv/ ACKNOWLEDGMENTS This work is funded by the German Research Foundation (DFG – Deutsche Forschungsgemeinschaft; Grant No. 460676551). REFERENCES Agrawal, R. (2016). Overlap in the Web Search Results of Google and Bing. Journal of Web Science, 2(1), 17–30. https://doi.org/10.1561/106.00000005 Bar-Ilan, J. (2005). Comparing rankings of search results on the Web. Information Processing & Management, 41(6), 1511–1519. https://doi.org/10.1016/J.IPM.2005.03.008 Bar‐Ilan, J., Levene, M., & Mat‐Hassan, M. (2006). Methods for evaluating dynamic changes in search engine rankings: a case study. Journal of Documentation, 62(6), 708–729. https://doi.org/10.1108/00220410610714930 Bharat, K. (1998). A technique for measuring the relative size and overlap of public Web search engines. Computer Networks and ISDN Systems, 30(1–7), 379–388. https://doi.org/10.1016/S0169-7552(98)00127-5 Bilal, D., & Ellis, R. (2011). Evaluating Leading Web Search Engines on Children’s Queries. In Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Vol. 6764 LNCS (Issue PART 4, pp. 549–558). https://doi.org/10.1007/978-3-642-21619- 0_67 Cardoso, B., & Magalhães, J. (2011). Google, bing and a new perspective on ranking similarity. Proceedings of the 20th ACM International Conference on Information and Knowledge Management - CIKM ’11, 1933–1936. https://doi.org/10.1145/2063576.2063858 Chignell, M. H., Gwizdka, J., & Bodner, R. C. (1999). Discriminating meta-search: a framework for evaluation. Information Processing & Management, 35(3), 337–362. https://doi.org/10.1016/S0306-4573(98)00065-X Ding, W., & Marchionini, G. (1996). A comparative study of web search service performance. Proceedings of the ASIS Annual Meeting, 33, 136–142. https://eric.ed.gov/?id=EJ557172 Edelman Trust Institute. (2022). Edelman Trust Barometer 2022 - Global Report. https://www.edelman.com/sites/g/files/aatuss191/files/2022-01/2022 Edelman Trust Barometer FINAL_Jan25.pdf European Commission. (2016). Special Eurobarometer 447 – Online Platforms. European Commission. https://doi.org/10.2759/937517 Gini, C. (1936). On the measure of concentration with special reference to income and statistics. Colorado College Publication, General Series, 208(1), 73–79. Goel, S., Broder, A., Gabrilovich, E., & Pang, B. (2010). Anatomy of the long tail. Proceedings of the Third ACM International Conference on Web Search and Data Mining - WSDM ’10, 201. https://doi.org/10.1145/1718487.1718513 González, C. G., Bonventi, W., & Rodrigues, A. L. V. (2008). Density of Closed Balls in Real-Valued and Autometrized Boolean Spaces for Clustering Applications. In Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Vol. 5249 LNAI (pp. 8–22). Springer Verlag. https://doi.org/10.1007/978-3-540-88190-2_7 Höchstötter, N., & Lewandowski, D. (2009). What users see – Structures in search engine results pages. Information Sciences, 179(12), 1796–1812. https://doi.org/10.1016/j.ins.2009.01.028 Introna, L. D., & Nissenbaum, H. (2000). Shaping the Web: Why the Politics of Search Engines Matters. The Information Society, 16(3), 169–185. https://doi.org/10.1080/01972240050133634 Lawrence, S., & Giles, C. L. (1999). Accessibility of information on the web. Nature, 400(6740), 107–107. https://doi.org/10.1038/21987 Lewandowski, D. (2019). The web is missing an essential part of infrastructure. Communications of the ACM, 62(4), 24–24. https://doi.org/10.1145/3312479 Lewandowski, D., & Kammerer, Y. (2021). Factors influencing viewing behaviour on search engine results pages: a review of eye-tracking research. Behaviour & Information Technology, 40(14), 1485–1515. https://doi.org/10.1080/0144929X.2020.1761450 Lewandowski, D., & Sünkler, S. (2019). What does Google recommend when you want to compare insurance offerings? Aslib Journal of Information Management, 71(3), 310–324. https://doi.org/10.1108/AJIM-07-2018- 0172 Mager, A. (2014). Is Small Really Beautiful? Big Search and Its Alternatives. In R. König & M. Rasch (Eds.), Society of the Query Reader (pp. 59–72). Institute of Network Cultures. Makhortykh, M., Urman, A., & Ulloa, R. (2020). How search engines disseminate information about COVID-19 and why they should do better. Harvard Kennedy School Misinformation Review, 1(May), 1–12. https://doi.org/10.37016/mr-2020-017 Meng, W., Yu, C., & Liu, K.-L. (2002). Building efficient and effective metasearch engines. ACM Computing Surveys, 34(1), 48–89. https://doi.org/10.1145/505282.505284 Newman, N., Fletcher, R., Schulz, A., Andı, S., Robertson, C., & Kleis Nielsen, R. (2021). The Reuters Institute Digital News Report 2021. https://reutersinstitute.politics.ox.ac.uk/sites/default/files/2021- 06/Digital_News_Report_2021_FINAL.pdf Ortega, F., Gonzalez-Barahona, J. M., & Robles, G. (2008). On the Inequality of Contributions to Wikipedia. Proceedings of the 41st Annual Hawaii International Conference on System Sciences (HICSS 2008), 304–304. https://doi.org/10.1109/HICSS.2008.333 Purcell, K., Brenner, J., & Rainie, L. (2012). Search Engine Use 2012. https://www.pewresearch.org/internet/wp- content/uploads/sites/9/media/Files/Reports/2012/PIP_Search_Engine_Use_2012.pdf Puschmann, C. (2019). Beyond the Bubble: Assessing the Diversity of Political Search Results. Digital Journalism, 7(6), 824–843. https://doi.org/10.1080/21670811.2018.1539626 Schultheiß, S., & Lewandowski, D. (2021). A representative online survey among German search engine users with a focus on questions regarding search engine optimization (SEO): a study within the SEO Effect project - Working Paper 2. https://osf.io/wzhxs Spink, A., Jansen, B. J., Blakely, C., & Koshman, S. (2006). A study of results overlap and uniqueness among major Web search engines. Information Processing and Management, 42(5), 1379–1391. https://doi.org/10.1016/j.ipm.2005.11.001 StatCounter. (2022). Search Engine Market Share. https://gs.statcounter.com/search-engine-market-share/ Steiner, M., Magin, M., Stark, B., & Geiß, S. (2022). Seek and you shall find? A content analysis on the diversity of five search engines’ results on political queries. Information, Communication & Society, 25(2), 217–241. https://doi.org/10.1080/1369118X.2020.1776367 Sünkler, S., Lewandowski, D., Schultheiß, S., Yagci, N., Sygulla, D., & von Mach, S. (2022). Relevance Assessment Tool. osf.io/t3hg9 Thelwall, M. (2008). Quantitative comparisons of search engine results. Journal of the American Society for Information Science and Technology, 59(11), 1702–1710. https://doi.org/10.1002/asi.20834 Unkel, J., & Haim, M. (2021). Googling Politics: Parties, Sources, and Issue Ownerships on Google in the 2017 German Federal Election Campaign. Social Science Computer Review, 39(5), 844–861. https://doi.org/10.1177/0894439319881634",0.0
A Security & Privacy Analysis of US-based Contact Tracing Apps,"[{'href': 'http://arxiv.org/abs/2207.08978v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.08978v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-18 23:14:49,,"Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Jonathan D. Chang * 1 Kaiwen Wang * 1 Nathan Kallus 2 Wen Sun 1 Abstract 1. Introduction 2 2 0 2 l u J 2 1 ] G L . s c [ 1 v 7 3 8 5 0 . 7 0 2 2 : v i X r a We study representation learning for Ofﬂine Re- inforcement Learning (RL), focusing on the im- portant task of Ofﬂine Policy Evaluation (OPE). Recent work shows that, in contrast to supervised learning, realizability of the Q-function is not enough for learning it. Two sufﬁcient conditions for sample-efﬁcient OPE are Bellman complete- ness and coverage. Prior work often assumes that representations satisfying these conditions are given, with results being mostly theoretical in nature. In this work, we propose BCRL, which directly learns from data an approximately lin- ear Bellman complete representation with good coverage. With this learned representation, we perform OPE using Least Square Policy Evalua- tion (LSPE) with linear functions in our learned representation. We present an end-to-end theoreti- cal analysis, showing that our two-stage algorithm enjoys polynomial sample complexity provided some representation in the rich class considered is linear Bellman complete. Empirically, we ex- tensively evaluate our algorithm on challenging, image-based continuous control tasks from the Deepmind Control Suite. We show our represen- tation enables better OPE compared to previous representation learning methods developed for off-policy RL (e.g., CURL, SPR). BCRL achieves competitive OPE error with the state-of-the-art method Fitted Q-Evaluation (FQE), and beats FQE when evaluating beyond the initial state dis- tribution. Our ablations show that both linear Bellman complete and coverage components of our method are crucial. *Equal contribution 1Computer Science, Cornell University, Ithaca, NY, USA 2Operations Research and Information Engi- neering, Cornell Tech, New York, NY, USA. Correspondence to: Jonathan D. Chang <https://jdchang1.github.io>, Kaiwen Wang <https://kaiwenw.github.io>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). Deep Reinforcement Learning (RL) has developed agents that solve complex sequential decision making tasks, achiev- ing new state-of-the-art results and surpassing expert human performance. Despite these impressive results, these algo- rithms often require a prohibitively large number of online interactions to scale to higher dimensional inputs. To address these sample complexity demands, a recent line of work (van den Oord et al., 2018; Anand et al., 2019; Mazoure et al., 2020; Yang & Nachum, 2021) has incor- porated advances in unsupervised representation learning from the supervised learning literature into developing RL agents. For example, CURL (Laskin et al., 2020) and SPR (Schwarzer et al., 2021) utilize contrastive representation ob- jectives as auxiliary losses within an existing RL framework. These efforts are motivated by the tremendous success such self-supervised techniques have offered in computer vision, natural language processing, speech processing, and beyond. While these formulations have shown sample complexity improvements empirically, it remains an open question whether these approaches successfully address the unique challenges from RL that usually do not appear in supervised learning, such as exploration and exploitation, credit assign- ments, long horizon prediction, and distribution shift. In particular, recent work (Wang et al., 2021b; Amortila et al., 2020; Foster et al., 2021) has shown that realizability of the learning target in RL (namely, the Q-function) is insufﬁcient to avoid exponential dependence on problem parameters. In this paper, we study representation learning for an important subtask of off-policy RL: ofﬂine policy evaluation (OPE). OPE is a critical component for any off-policy policy optimization approach (e.g., off-policy actor critic such as SAC, Haarnoja et al., 2018, and DDPG, Lillicrap et al., 2016). Moreover, OPE allows us to focus on issues arising from distribution shift and long horizon prediction. Speciﬁcally, we propose a new approach that leverages rich function approximation (e.g., deep neural networks) to learn a representation that is both Bellman complete and exploratory. A linear Bellman complete representation means that linear functions in the representation have zero inherent Bellman error (Antos et al., 2008), i.e., applying the Bellman operator on a linear function results in a new linear function. An exploratory representation means that Learning Bellman Complete Representations for Ofﬂine Policy Evaluation the resulting feature covariance matrix formed by the ofﬂine dataset is well-conditioned. These two representational properties ensure that, under linear function approximation (i.e., the linear evaluation protocol, Grill et al., 2020), classic least squares policy evaluation (LSPE) (Nedi´c & Bertsekas, 2003; Duan et al., 2020; Wang et al., 2021a) can achieve accurate policy evaluation. We provide an end-to-end analysis showing that our representation learning approach together with LSPE ensures near-optimal policy evaluation with polynomial sample complexity. Empirically, we extensively evaluate our method on image- based continuous control tasks from the Deepmind Control Suite. First, we compare against two representation learning approaches developed for off-policy RL: CURL (Laskin et al., 2020) and SPR (Schwarzer et al., 2021). These bear many similarities to contrastive learning techiniques for unsupervised representation learning: SimCLR (Chen et al., 2020) and Bootstrap your own latent (BYOL, Grill et al., 2020), respectively. Under the linear evaluation protocol (i.e., LSPE with a linear function on top of the learned representation), our approach consistently outperforms these baselines. We observe that representations learned by CURL and SPR sometimes even exhibit instability when evaluated using LSPE (prediction error blows up when more iterations of LSPE is applied), while our approach is more stable. This comparison demonstrates that representation learning in ofﬂine RL is more subtle and using representation techniques developed from supervised learning settings may not result in the best performance for ofﬂine RL. Our ablations show that both linear Bellman completeness and coverage are crucial, as our method also blows up if one ingredient is missing. Finally, BCRL achieves state-of-the-art OPE error when compared with other OPE methods, and improves the state-of-the-art when evaluating beyond the initial state distribution.1 1.1. Related Work Representation Learning in Ofﬂine RL: From the theoretical side, Hao et al. (2021) considers of- ﬂine RL in sparse linear MDPs (Jin et al., 2020). Learning with sparsity can be understood as feature selection. Their work has a much stronger coverage condition than ours: namely, given a representation class Φ, they assume that any feature φ ∈ Φ has global coverage under the ofﬂine data distribution, i.e., Es,a∼νφ(s, a)φ(s, a)(cid:62) is well conditioned where ν is ofﬂine data distribution. In our work, we only assume that there exists one φ(cid:63) that has global coverage, thus strictly generalizing their coverage condition. Uehara & Sun (2021) propose a general model-based ofﬂine RL approach that can perform representation learning for linear MDPs in the ofﬂine setting without global coverage. How- 1 Code available at https://github.com/CausalML/bcrl. ever, their algorithm is a version space approach and is not computationally efﬁcient. Also, our linear Bellman com- pleteness condition strictly generalizes linear MDPs. (Ni et al., 2021) consider learning state-action embeddings from a known RKHS. We use general function approximation that can be more powerful than RKHS. Finally, Parr et al. (2008) identiﬁes bellman completeness as a desirable condition for feature selection in RL when analyzing an equivalence be- tween linear value-function approximation and linear model approximation. In our work, we investigate how to learn bellman completeness representation, and also the role of coverage in our feature selection. From the empirical side, Yang & Nachum (2021) evalu- ated a broad range of unsupervised objectives for learning pretrained representations from ofﬂine datasets for down- stream Imitation Learning (IL), online RL, and ofﬂine RL. They found that the use of pretrained representations dra- matically improved the downstream performance for policy learning algorithms. In this work, we aim to identify such an unsupervised representation objective and to extend the empirical evaluation to ofﬂine image-based continuous con- trol tasks. Nachum & Yang (2021) presents a provable contrastive representation learning objective for IL (derived from maximum likelihood loss), learning state representa- tions from expert data to do imitation with behavior cloning. Our approach instead focuses on learning state-action rep- resentations for OPE. Finally, both Song et al. (2016) and Chung et al. (2019) present algorithms to learn representa- tions suitable for linear value function approximation. Song et al. (2016) is most relevant to our work where they aim to learn bellman complete features. Both works, however, work in the online setting and do not consider coverage induced from the representation which we identify is impor- tant for accurate OPE. Representation Learning in Online RL: Theoretical works on representation learning in online RL focus on learning representations to facilitate exploration from scratch. OLIVE (Jiang et al., 2017), BLin-UCB (Du et al., 2021), FLAMBE (Agarwal et al., 2020), Mofﬂe (Modi et al., 2021), and Rep-UCB (Uehara et al., 2022) propose ap- proaches for representation learning in linear MDPs where they assume that there exists a feature φ(cid:63) ∈ Φ that ad- mits the linear MDP structure for the ground truth transi- tion. Zhang et al. (2021) posits an even stronger assumption where every feature φ ∈ Φ admits the linear MDP struc- ture for the true transition. Note that we only assume that there exists one φ(cid:63) that admits linear Bellman complete- ness, which strictly generalizes linear MDPs. Hence, our representation learning setting is more general than prior theoretical works. Note that we study the ofﬂine setting while prior works operate in the online setting which has additional challenges from online exploration. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation On the empirical side, there is a large body of works that adapt existing self-supervised learning techniques devel- oped from computer vision and NLP to RL. For learning representations from image-based RL tasks, CPC (van den Oord et al., 2018), ST-DIM (Anand et al., 2019), DRIML (Mazoure et al., 2020), CURL (Laskin et al., 2020), and SPR (Schwarzer et al., 2021) learn representations by op- timizing various temporal contrastive losses. In particular, Laskin et al. (2020) proposes to interleave minimizing an In- foNCE objective with similarities to MoCo (He et al., 2020) and SimCLR (Chen et al., 2020) while learning a policy with SAC (Haarnoja et al., 2018). Moreover, Schwarzer et al. (2021) proposes learning a representation similar to BYOL (Grill et al., 2020) alongside a Q-function for Deep Q-Learning (Mnih et al., 2013). We compare our repre- sentational objective against CURL and SPR in Section 6, and demonstrate that under linear evaluation protocol, ours outperform both CURL and SPR. Note, we refer the readers to Schwarzer et al. (2021) for comparisons between SPR and CPC, ST-DIM, and DRIML. 2. Preliminaries In this paper we consider the inﬁnite horizon discounted MDP (cid:104)S, A, γ, P, r, d0(cid:105). where S, A are state and action spaces which could contain a large number of states and ac- tions or could even be inﬁnite, γ ∈ (0, 1) is a discount factor, P is the transition kernel, r : S × A → R is the reward function, and d0 ∈ ∆(S) is the initial state distribution. We assume rewards are bounded by 1, i.e. |r(s, a)| ≤ 1. Given a policy π : S (cid:55)→ ∆(A), we denote V π(s) = E (cid:2)(cid:80)∞ h=0 γhrh|π, s0 := s(cid:3) as the expected dis- counted total reward of policy π starting at state s. We = Es∼d0 V π(s) as the expected discounted total denote V π d0 reward of the policy π starting at the initial state distribu- tion d0. We also denote average state-action distribution dπ h=0 γhdπ h(s, a) is d0 the probability of π visiting the (s, a) pair at time step h, starting from d0. (s, a) = (1 − γ) (cid:80)∞ h(s, a) where dπ In OPE, we seek to evaluate the expected discounted to- tal reward V πe of a target policy πe : S (cid:55)→ ∆(A) given data drawn from an ofﬂine state-action distribution ν ∈ ∆(S × A). The latter can, for example, be the state-action distribution under a behavior policy πb. Namely, the ofﬂine dataset D = {si, ai, ri, s(cid:48) i}N i=1 consists of N i.i.d tuples generated as (s, a) ∼ ν, r = r(s, a), s(cid:48) ∼ P (·|s, a). We deﬁne Bellman operator T πassociated with π as follows: T πf (s, a) =∆ r(s, a) + γEs(cid:48)∼P (s,a),a(cid:48)∼π(s(cid:48)) [f (s(cid:48), a(cid:48))] We may drop the superscript π when it is clear from context, in particular when π = πe is the ﬁxed target policy. A representation, or feature, φ : S × A → Rd is an embedding of state-action pairs into d-dimensional (cid:80)N space. We let Σ(φ) = Eν [φ(s, a)φ(s, a)(cid:124)] , (cid:98)Σ(φ) = i=1 φ(si, ai)φ(si, ai)(cid:124). We consider learning a rep- 1 N resentation from a feature hypothesis class Φ ⊂ [S × A (cid:55)→ Rd]. We assume features have bounded norm: (cid:107)φ(s, a)(cid:107)2 ≤ 1, ∀s, a, ∀φ ∈ Φ. In our experiments, Φ is convolutional neural nets with d outputs. Notation We denote BW := {x ∈ Rd : (cid:107)x(cid:107)2 ≤ W } as the Euclidean Ball in Rd with radius W . Given a distribution ν ∈ ∆(S × A) and a function f : S × A (cid:55)→ R, we denote ν = Es,a∼νf 2(s, a). Given a positive L2(ν) norm as (cid:107)f (cid:107)2 xT Σx and let λmin(Σ) deﬁnite matrix Σ, let (cid:107)x(cid:107)Σ = denote the minimum eigenvalue. When ν (cid:28) µ we let dν dµ denote the Radon-Nikodym derivative. We use ◦ to denote composition, so s(cid:48), a(cid:48) ∼ P (s, a) ◦ π is short-form for s(cid:48) ∼ P (s, a), a(cid:48) ∼ π(s(cid:48)). For any function f (s, a) and a policy π, we denote f (s, π) = Ea∼π(s) [f (s, a)]. √ 3. Linear Bellman Completeness Before we introduce our representation learning approach, in this section, we ﬁrst consider OPE with linear function approximation with a given representation φ. Lessons from supervised learning or linear bandit may suggest that OPE should be possible with polynomially many ofﬂine samples, as long as (1) ground truth target Qπe is linear in φ (i.e. ∃w ∈ Rd, such that for all s, a, Qπe (s, a) = w(cid:62)φ(s, a)), and (2) the ofﬂine data provides sufﬁcient coverage over πe (i.e., λmin(Σ(φ)) > 0). Unfortunately, under these two assumptions, there are lower bounds indicating that for any OPE algorithm, there exists an MDP where one will need at least exponentially (exponential in horizon or d) many ofﬂine samples to provide an accurate estimate of V πe (Wang et al., 2021b; Foster et al., 2021; Amortila et al., 2020). This lower bound indicates that one needs additional structural conditions on the representation. The additional condition the prior work has consider is Bellman completeness (BC). Since we seek to learn a rep- resentation rather than assume theoretical conditions on a given one, we will focus on an approximate version of BC. Deﬁnition 3.1 (Approximate Linear BC). A representation φ is εν-approximately linear Bellman complete if, max w1∈BW min w2∈BW (cid:107)w (cid:124) 2 φ − T πe (w (cid:124) 1 φ)(cid:107)ν ≤ εν. Note the dependence on ν, πe, and W . (cid:124) 1 φ(s, a), T π(w Intuitively the above condition is saying that for any linear (cid:124) 1 φ(s, a)) itself can be approxi- function w mated by another linear function under the distribution ν. Remark 3.2. Low-rank MDPs (Jiang et al., 2017; Agarwal et al., 2020) are subsumed in the exact linear BC model, i.e., εν = 0 under any distribution ν. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Algorithm 1 Least Squares Policy Evaluation (LSPE) 1: Input: Target policy πe, features φ, dataset D 2: Initialize (cid:98)θ0 = 0 ∈ BW . 3: for k = 1, 2, ..., K do (cid:124) k−1φ(s, a), Set (cid:98)fk−1(s, a) = (cid:98)θ 4: (cid:98)Vk−1(s) = Ea∼πe(s)[ (cid:98)fk−1(s, a)] 5: Perform linear regression: 1 N N (cid:88) i=1 (cid:98)θk ∈ arg min θ∈BW 6: end for 7: Return (cid:98)fK. (cid:0)θ(cid:124)φ(si, ai) − ri − γ (cid:98)Vk−1(s(cid:48) i)(cid:1)2 Note that the Bellman completeness condition is more sub- tle than the common realizability condition: for any ﬁxed φ, increasing its expressiveness (e.g., add more features) does not imply a monotonic decrease in εν. Thus common tricks such as lifting features to higher order polynomial kernel space or more general reproducing kernel Hilbert space does not imply the linear Bellman complete condition, nor does it improve the coverage condition. We next show approximate Linear BC together with coverage imply sample-efﬁcient OPE via Least Square Policy Evaluation (LSPE) (Algo- rithm 1). We present our result using the relative condition number, deﬁned for any initial state distribution p0 as x(cid:124)Es,a∼dπe φ(s, a)φ(s, a)(cid:124)x κ(p0) := sup x∈Rd p0 x(cid:124)Σ(φ)x . (1) Theorem 3.3 (Sample Complexity of LSPE). Assume fea- ture φ satisﬁes approximate Linear BC with parameter εν. For any δ ∈ (0, 0.1), w.p. at least 1 − δ, we have for any initial state distribution p0 (cid:12) (cid:12)V πe (cid:12) p0 (cid:104) − Es∼p0 (cid:98)fK(s, πe) (cid:105)(cid:12) (cid:12) (cid:12) ≤ γK/2 1 − γ + (cid:114)(cid:13) (cid:13) 4 (cid:13) ddπe p0 dν (cid:13) (cid:13) (cid:13)∞ (1 − γ)2 εν + 480(cid:112)κ(p0)(1 + W )d log(N )(cid:112)log(10/δ) √ (1 − γ)2 N , where (cid:98)fK is the output of Algorithm 1. Please see Appendix C.1 for proof. The above result holds simultaneously over all initial state distributions p0 covered by the data distribution ν. If ν has full coverage, i.e. if Σ (cid:23) βI, as is commonly assumed in the literature, one can show that κ(p0) ≤ β−1 for any initial state distribution (cid:13) (cid:13) p0. Also note that the concentrability coefﬁcient (cid:13)∞ shows up as T πe (cid:98)fk−1 can be nonlinear. In the exact Linear BC case, where εν = 0 (i.e., there is a linear function that perfectly approximates T πe (cid:98)fk−1 under ν), the term involving the concentrability coefﬁcient will be 0 and we can even avoid its ﬁniteness. ddπe p0 dν (cid:13) (cid:13) (cid:13) 4. Representation Learning The previous section indicates sufﬁcient conditions on the representation for efﬁcient and accurate OPE with linear function approximation. However, a representation that is Bellman complete and also provides coverage is not avail- able a priori, especially for high-dimensional settings (e.g., image-based control tasks as we consider in our experi- ments). Existing theoretically work often assume such rep- resentation is given. We propose to learn a representation φ that is approximately Bellman complete and also provides good coverage, via rich function approximation (e.g., a deep neural network). More formally, we want to learn a repre- sentation φ such that (1) it ensures approximate Bellman complete, i.e., εν is small, and (2) has a good coverage, i.e., λmin(Σ(φ)) is as large as possible, which are the two key properties to guarantee accurate and efﬁcient OPE indicated by Theorem 3.3. To formulate the representation learning question, our key assumption is that our representation hy- pothesis class Φ is rich enough such that it contains at least one representation φ(cid:63) that is linear Bellman complete (i.e., εν = 0) and has a good coverage. Assumption 4.1 (Representational power of Φ). There ex- ists φ(cid:63) ∈ Φ, such that (1) φ(cid:63) achieves exact Linear BC (deﬁnition 3.1 with εν = 0), and (2) φ(cid:63) induces coverage, i.e., λmin(Σ(φ(cid:63))) ≥ β > 0. Our goal is to learn such a representation from the hypothe- sis class Φ. Note that unlike prior RL representation learning works (Hao et al., 2021; Zhang et al., 2021), here we only assume that there exists a φ(cid:63) has linear BC and induces good coverage, other candidate φ ∈ Φ could have terrible coverage and does not necessarily have linear BC. Before proceeding to the learning algorithm, we ﬁrst present an equivalent condition for linear BC, which does not rely on the complicated min-max expression of Deﬁnition 3.1. Proposition 4.2. Consider a feature φ with full rank covari- ance Σ(φ). Given any W > 0, the feature φ being linear BC (under BW ) implies that there exist (ρ, M ) ∈ BW × Rd×d with (cid:107)M (cid:107)2 ≤ 1 − (cid:107)ρ(cid:107)2 (cid:113) W 2 , and Eν (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) φ(s, a) − (cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 = 0. (2) On the other hand, if there exists (ρ, M ) ∈ BW × Rd×d with (cid:107)M (cid:107)2 < 1 such that the above equality holds, then φ must satisfy exact linear BC with W ≥ (cid:107)ρ(cid:107)2 . 1−(cid:107)M (cid:107)2 Please see Appendix D for proof. The above shows that lin- ear BC is equivalent to a simple linear relationship between the feature and the expected next step’s feature and reward. This motivates our feature learning algorithm: if we are ca- pable of learning a representation φ such that the transition Learning Bellman Complete Representations for Ofﬂine Policy Evaluation from the current feature φ(s, a) to the expected next fea- ture Es(cid:48)∼P (s,a)[φ(s(cid:48), πe)] and reward r(s, a) is linear, then we’ve found a feature φ that is linear BC. 4.1. Algorithm To learn the representation that achieves linear BC, we use Proposition 4.2 to design a bilevel optimization program. We start with deterministic transition as a warm up. Deterministic transition Due to determinism in the tran- sition, we do not have an expectation with respect to s(cid:48) anymore. So, we design the bilevel optimization as follows: However, we cannot directly optimize the above objective since we do not have access to P (s, a) to compute the ex- pected next step feature Es(cid:48)∼P (s,a)φ(s(cid:48), πe). Also note that the expectation Es(cid:48) is inside the square which means that we cannot even get an unbiased estimate of the gradient of (φ, M ) with one sample s(cid:48) ∼ P (s, a). This phenomenon is related to the double sampling issue on ofﬂine policy evaluation literature which forbids one to directly optimize Bellman residuals. Algorithms such as TD are designed to overcome the double sampling issue. Here, we use a different technique to tackle this issue (Chen & Jiang, 2019). We introduce a function class G ⊂ S × A (cid:55)→ Rd which is rich enough to contain the expected next feature. (cid:104) (cid:21) min φ∈Φ (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 Θ = (cid:8)(ρ, M ) ∈ B(cid:107)ρ(cid:63)(cid:107) × Rd×d :, (cid:107)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2 (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) min (ρ,M )∈Θ φ(s, a) − ED (cid:9) , where ρ(cid:63), M (cid:63) are the optimal ρ, M for the linear BC φ(cid:63) (in Assumption 4.1). Namely, we aim to search for a repre- sentation φ ∈ Φ, such that the relationship between φ(s, a) and the combination of the next time step’s feature φ(s(cid:48), πe) and the reward, is linear. The spectral norm constraint in Θ is justiﬁed by Proposition 4.2. Solving the above bilevel moment condition ﬁnds a representation that achieves ap- proximate linear BC. However, there is no guarantee that such representation can provide a good coverage over πe’s traces. We introduce regularizations for φ using the ideas from optimal designs, particularly the E-optimal design. Deﬁne the minimum eigenvalue regularization (3) Assumption 4.3. For any φ ∈ Φ, we have that the mapping (s, a) (cid:55)→ γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)] is in G. We form the optimization problem as follows: (cid:104) min φ∈Φ min (ρ,M )∈Θ (cid:21) ED (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) ED (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2 φ(s, a) − (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:105) . 2 (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (5) − min g∈G Note that under Assumption 4.3, the min over G will approx- imate γ2ED(cid:107)Es(cid:48)∼P (s,a)φ(s(cid:48), πe) − φ(s(cid:48), πe)(cid:107)2 2, i.e., the av- erage variance induced by the stochastic transition. Thus, for a ﬁxed φ and M , we can see that the following ED (cid:107)M φ(s, a) − γφ(s(cid:48), πe)(cid:107)2 − γ2ED(cid:107)Es(cid:48)∼P (s,a)φ(s(cid:48), πe) − φ(s(cid:48), πe)(cid:107)2 2 2 RE(φ) := λmin (ED [φ(s, a)φ(s, a)(cid:124)]) , is indeed an unbiased estimate of: as the smallest eigenvalue of the empirical feature covari- ance matrix under the representation φ. Maximizing this quantity ensures that our feature provides good coverage, i.e. λmin(Σ(φ)) is as large as possible. Thus, to learn a representation that is approximately linear Bellman complete and also provides sufﬁcient coverage, we formulate the following constrained bilevel optimization: (cid:104) min φ∈Φ (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) s.t., RE(φ) ≥ β/2. min (ρ,M )∈Θ ED (cid:21) φ(s, a) − (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 To extend this to stochastic transitions, there is an additional double sampling issue, which we discuss and address now. Stochastic transition Ideally, we would solve the follow- ing bilevel optimization problem, (cid:104) min φ∈Φ min (ρ,M )∈Θ ED (cid:21) (cid:13) (cid:13) (cid:13) (cid:13) (cid:20)M ρ(cid:124) φ(s, a) − (cid:20)γEs(cid:48)∼P (s,a)[φ(s(cid:48), πe)] r(s, a) (cid:105) . 2 (cid:21)(cid:13) (cid:13) (cid:13) (cid:13) 2 (4) Es,a∼ν (cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13) (cid:13) 2 (cid:13) 2 which matches to the ideal objective in Eq. 4. Thus solv- ing for φ based Eq. 5 allows us to optimize Eq. 4, which allows us to learn an approximate linear Bellman complete representation. Similarly, we incorporate the E-optimal de- sign here by adding a constraint that forcing the smallest eigenvalue of the empirical feature covariance matrix, i.e., RE(φ), to be lower bounded. Once we learn a representation that is approximately linear BC, and also induces sufﬁcient coverage, we simply call the LSPE to estimate V πe . The whole procedure is summarized in Algorithm 2. Note in Alg 2 we put constraints to the objective function using Lagrangian multiplier. There are other design choices that also encourage coverage. One particular choice we study empirically is motivated by the idea of D-optimal design. Here we aim to ﬁnd a representation that maximizes the following log-determinant RD(φ) := ln det (ED [φ(s, a)φ(s, a)(cid:124)]) . Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Algorithm 2 OPE with Bellman Complete and exploratory Representation Learning (BCRL) 1: Input: Representation class Φ, dataset D of size 2N , design regularization R, function class G, policy πe. 2: Randomly split D into two sets D1, D2 of size N . 3: If the system is stochastic, learn representation (cid:98)φ as, (cid:104) arg min φ∈Φ − min g∈G (cid:21) ED1 min (ρ,M )∈Θ (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) ED1 (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2 φ(s, a) − (cid:105) 2 (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 − λR(φ) 4: Otherwise, for deterministic system, learn (cid:98)φ as, (cid:104) arg min φ∈Φ min (ρ,M )∈Θ ED1 (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) φ(s, a) − (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 − λR(φ) 5: Return (cid:98)V := LSPE(πe, (cid:98)φ, D2). When D is large, then the regularization RD(φ) approx- imates (cid:80) i ln (σi (Eν [φ(s, a)φ(s, a)(cid:124)])). Maximizing RD(φ) then intuitively maximizes coverage over all directions. D-optimal design is widely used for bandits (Lattimore & Szepesvári, 2020) and RL (Wang et al., 2021b; Agarwal et al., 2019) to design exploration distributions with global coverage. Note that, in contrast to these contexts where the feature is ﬁxed and the distribution is optimized, we optimize the feature, given the data distribution ν. 4.2. Sample Complexity Analysis We now prove a ﬁnite sample utility guarantee for the em- pirical constrained bilevel optimization problem, (cid:98)φ ∈ arg min φ∈Φ (cid:104) min (ρ,M )∈Θ ED (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ (cid:13) (cid:124) φ(s, a) − ED (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2 2 (cid:105) . − min g∈G (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (6) s.t., RE(φ) ≥ β/2. For simplicity, we state our results for discrete function class Φ and G. Note that our sample complexity only scales with respect ln(|Φ|) and ln(|G|), which are the standard com- plexity measures for discrete function classes. We extend our analysis to inﬁnite function classes under metric entropy assumptions (Wainwright, 2019; van der Vaart & Wellner, 1996) in the Appendix; see Assumption E.5. The following theorem shows that Algorithm 2 learns a representation (cid:98)φ that is O(N −1/2) approximately Linear BC and has coverage. Theorem 4.4. Assume Assumption 4.1 (and Assump- := Let C2 tion 4.3 if 96 log1/2(|Φ|)+4 2 , then for any the system is stochastic). √ . If N ≥ C 2 d+4 log1/2(1/δ) β/4 δ ∈ (0, 1), w.p. at least 1 − δ, we have 1. (cid:98)φ satisﬁes (cid:98)ε-approximate Linear BC with (cid:98)ε ≤ 13d(1 + W )2 log1/2(4W |Φ|N/δ) √ N + 7γ(1 + W ) log1/2(2|G|/δ) √ N , 2. λmin(Σ( (cid:98)φ)) ≥ β/4. If transitions are deterministic, treat log(|G|) = 0. Proof Sketch. First, we use Weyl’s Perturbation Theorem and chaining to show that the eigenvalues of Σ(φ) are close to (cid:98)Σ(φ), uniformly over φ. This implies that (a) λmin((cid:98)Σ(φ(cid:63))) ≥ β/2, and hence is feasible in the empirical bilevel optimization Equation (6), and (b) λmin(Σ( (cid:98)φ)) ≥ β/4. Since φ(cid:63) is feasible, we apply uniform concentration arguments to argue that (cid:98)φ has low population loss (Equa- tion (5)), which implies approximate Linear BC. The error term in (cid:98)ε is comprised of the statistical errors of ﬁtting Φ and of ﬁtting G for the double sampling correc- tion. In the contextual bandit setting, i.e. γ = 0, there is no transition, so the second term becomes 0. Chaining together with Theorem 3.3 gives the following end-to-end (cid:101)O(N −1/2) evaluation error guarantee for LSPE with the learned features: Theorem 4.5. Under Assumption 4.1 (and Assumption 4.3 if P (s, a) is stochastic). Let C2, (cid:98)ε be as deﬁned in Theo- rem 4.4. If N ≥ C 2 2 , then we have for any δ ∈ (0, 1/2), w.p. at least 1 − 2δ, for all distributions p0, (cid:12) (cid:12)V πe (cid:12) − Es∼p0 (cid:104) (cid:98)fK(s, πe) p0 960β−1/2(1 + W )d log(N )(cid:112)log(10/δ) (cid:105)(cid:12) (cid:12) (cid:12) ≤ γK/2 1 − γ + √ (1 − γ)2 N . + (cid:114)(cid:13) (cid:13) (cid:13) 4 ddπe p0 dν (cid:13) (cid:13) (cid:13)∞ (1 − γ)2 (cid:98)ε Comparison to FQE What if one ignores the representa- tion learning and just runs the Fitted Q-Evaluation (FQE) which directly performs least square ﬁtting with the nonlin- ear function class F := {w(cid:62)φ(s, a) : φ ∈ Φ, (cid:107)w(cid:107)2 ≤ W }? As N → ∞, FQE will suffer the following worst-case Bell- man error (also called inherent Bellman error): εibe := max f ∈F min g∈F (cid:107)g − T πe f (cid:107)2 ν . Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Algorithm 3 Practical Instantiation of BCRL 1: Input: Ofﬂine dataset D = {s, a, s(cid:48)}, target policy πe 2: Initialize parameters for φ, M , ρ, and φ 3: for t = 0, 1, . . . , T do 4: Mt+1 ← Mt − η∇M J(φt, Mt, ρt, φt) ρt+1 ← ρt − η∇ρJ(φt, Mt, ρt, φt) 5: φt+1 ← φt − η∇φJ(φt, Mt+1, ρt+1, φt) 6: φt+1 ← τ φt+1 + (1 − τ )φt 7: 8: end for 9: Linear evaluation: (cid:98)V = LSPE(πe, φT , D). Note that our assumption that there exists a linear BC rep- resentation φ(cid:63) does not imply that the worst-case Bellman error εibe is small. In contrast, when N → ∞, our approach will accurately estimate V πe p0 . 5. A Practical Implementation In this section we instantiate a practical implementation to learn our representation using deep neural networks for our representation function class Φ. Based on Equation (3), we ﬁrst formalize our bilevel optimization objective: J(φ, M, ρ, φ) = ED (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ (cid:13) − λ log det ED φ(s, a) − (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:2)φ(s, a)φ(s, a)(cid:62)(cid:3) . (cid:124) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 In our implementation, we replace the hard constraint pre- sented in Section 4.1 with a Lagrange multiplier, i.e. we use the optimal design constraint as a regularization term when learning φ. Speciﬁcally, we maximize the log det of the covariance matrix induced by the feature, which maximizes all eigenvalues since log det is the sum of the log eigen- values. Our experiment results in Section 6.4 demonstrate that it indeed improves the condition number of Σ(φ). In some of our experiments, we use a target network in our implementation. Namely, the updates make use of a target network φ where the weights can be an exponential moving average of the representation network’s weights. This idea of using a slow moving target network has been shown to stabilize training in both the RL (Mnih et al., 2013) and the representation learning literature (Grill et al., 2020). As summarized in Algorithm 3, given our ofﬂine behavior dataset D and target policy πe, we iteratively update M Figure 1. Representative frames from DeepMind Control Suite tasks. From left to right, Finger Turn Hard, Cheetah Run, Quadruped Walk, and Humanoid Stand. and φ and then use the resulting learned representation to perform OPE. Please see Appendix F for implementation and hyperparameter details. As we will show in our experiments, our update procedures for φ signiﬁcantly minimizes the Bellman Completeness loss and also improve the condition number of Σ(φ), which are the two key quantities to ensure good performance of LSPE with linear regression as shown in Theorem 3.3. 6. Experiments Our goal is to answer the following questions. (1) How do our representations perform on downstream LSPE com- pared to other popular unsupervised representation learning techniques? (2) How important are both the linear bellman completeness and optimal design components for learning representations? (3) How competitive is BCRL with other OPE methods, especially for evaluating beyond the initial state distribution? Following the standards in evaluating representations in su- pervised learning, we used a linear evaluation protocol, i.e., linear regression in LSPE on top of a given representa- tion. This allows us to focus on evaluating the quality of the representation. We compared our method to prior techniques on a range of challenging, image-based continuous control tasks from the DeepMind Control Suite benchmark (Tassa et al., 2018): Finger Turn Hard, Cheetah Run, Quadruped Walk, and Humanoid Stand. Frames from the tasks are shown in Figure 1. To investigate our learned representation, we benchmark our representation against two state-of-the-art self-supervised representation learning objectives adopted for RL: (1) CURL uses the In- foNCE objective to contrastively learn state-representations; and (2) SPR adopts the BYOL contrastive learning frame- work to learn representations with latent dynamics. Note, we modiﬁed CURL for OPE by optimizing the contrastive loss between state-action pairs rather than just states. For SPR, we did not include the Q prediction head and used their state representation plus latent dynamics as the state- action representation for downstream linear evaluation. We used the same architecture for the respective representa- tions across all evaluated algorithms. To compare against other OPE methods, we additionally compared against Fit- ted Q-Evaluation (Munos & Szepesvári, 2008; Kostrikov & Nachum, 2020) (FQE), weighted doubly robust policy evaluation (Jiang & Li, 2016; Thomas & Brunskill, 2016) (DR), Dreamer-v2 (Hafner et al., 2021) model based eval- uation (MB), and DICE (Yang et al., 2020). We modiﬁed implementations from the benchmark library released by Fu et al. (2021) for FQE and DR and used the authors’ released codebases for Dreamer-v2 and BestDICE.2 2https://github.com/google-research/dice_rl. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Figure 2. OPE curves across ﬁve seeds using representations trained with BCRL, SPR, and CURL on the ofﬂine datasets (Table 1). mators beyond the original initial state distribution d0. The ﬁrst setting ensures that baselines and our algorithm all sat- isfy the coverage condition, thus demonstrating the unique beneﬁt of learning a Bellman complete representation. The second setting evaluates the robustness of our algorithm, i.e., the ability to estimate beyond the original d0. Evaluation on On-Policy + Off-Policy Data: To further investigate the importance of learning linear BC features, we experiment with learning representations from an ofﬂine dataset that also contains state-action pairs from the target policy. More speciﬁcally, we train our representations on a dataset containing 100K behavior policy and 100K target policy samples. Note, only the experiment in this paragraph uses this mixture dataset. With the addition of on-policy data from the target policy, we can focus on just the role of linear BC for OPE performance because the density ratio ddπe po dν and the relative condition number (Eq. 1) is at most 2, i.e. we omit the design regularization and focused on minimizing the Bellman completeness loss. Figure 4 (Left) shows that BCRL outperforms baselines in this setting, even Figure 3. (Left) Root mean squared evaluation error across all tasks. (Right) Mean spearman ranking correlation across all tasks. Our target policies were trained using the authors’ imple- mentation of DRQ-v2 (Yarats et al., 2021a), a state-of-the- art, off-policy actor critic algorithm for vision-based contin- uous control. With high-quality target policies, we collected 200 rollouts and did a linear evaluation protocol to predict the discounted return. For our ofﬂine behavior datasets, we collected 100K samples from a trained policy with mean performance roughly a quarter of that of the target policy (Table 1). All results are aggregated over ﬁve random seeds. See Appendix F for details on hyperparameters, environ- ments, and dataset composition. 6.1. OPE via LSPE with Learned Representations Figure 2 compares the OPE performance of BCRL against SPR and CURL. Representations learned by BCRL outper- form those learned by SPR and CURL. On some tasks, SPR and CURL both exhibited an exponential error ampliﬁca- tion with respect to the number of iterations of LSPE, while BCRL did not suffer from any blowup. 6.2. OPE Performance Figure 3 compares the OPE performance of BCRL against multiple benchmarks from the OPE literature. BCRL is competitive with FQE and evaluates better than other bench- marks across the tasks that we tested on. We also evaluated how well estimated values from BCRL rank policies. Following (Fu et al., 2021), we use the spearman ranking correlation metric to compute the correlation between ordinal rankings according to the OPE estimates and the ground truth values. For ranking, we evaluated three additional target policies with mean perfor- mances roughly 75%, 50%, and 10% of the target policy. Figure 3 presents the mean correlation of each evaluation algorithm across all tasks. BCRL is competitive with FQE and consistently better than other benchmarks at ranking policies. 6.3. Further Investigation of Different Settings In this section, we consider two additional settings: (1) the ofﬂine dataset contains some on-policy data, which ensures that the ofﬂine data provides coverage over the evaluation policy’s state action distribution; (2) we evaluate all esti- Figure 4. (Left) Evaluation on a mixture dataset with on-policy and off-policy data. Note the addition of target policy data bounds the relative condition number (Eq. 1). (Right) Evaluation beyond the initial state distribution (given just the ofﬂine data). Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Figure 5. (Left) BCRL’s OPE curves for Cheetah Run with (blue) and without (red) the D-optimal design based regularization. (Center) Bar plot of the singular values of the feature covariance matrices for the left plot. (Right) OPE curves for Finger Turn Hard with (blue) and without (red) optimizing for linear BC. matching FQE performance across tasks. Our experiments corroborate our analysis that explicitly enforcing our learned representations to be linear BC improves OPE. Note that the fact that LSPE with SPR and CURL features still blows up under this mixture data means that the other representations’ failures are not just due to coverage. Evaluation Beyond Initial State Distribution: To further investigate the beneﬁts of optimizing the D-optimal design to improve coverage, we investigate doing OPE beyond the initial state distribution d0, which is supported by Theo- rem 4.5. Note that if our representation is exactly Bellman complete and also the corresponding feature covariance ma- trix is well-conditioned, we should be able to evaluate well on any states. Speciﬁcally, we experiment on evaluating at all timesteps in a target policy rollout, not just at the initial state distribution. Figure 4 (Right) shows that BCRL is able to more robustly evaluate out-of-distribution than all other benchmarks. 6.4. Ablation Studies Impact of Optimal Design Regularization: To investi- gate the impact of maximizing the D-optimal design, we ablate the design regularization term from our objective and analyze the downstream evaluation performance and the respective feature covariance matrices on the ofﬂine dataset. Figure 5 (Center) presents a bar plot of the sin- gular values of the feature covariance matrix (Σ(φ) := Es,a∼νφ(s, a)φ(s, a)(cid:62)). Figure 5 (Left) shows the down- stream OPE performance for features trained with and with- out the design regularization on the Cheetah Run task. Note that without the regularization, we ﬁnd that that the feature covariance matrix has much worse condition number, i.e. feature is less exploratory. As our analysis suggests, we also observe a deterioration in evaluation performance with- out the design regularization to explicitly learn exploratory features. Impact of Linear Bellman Completeness: Figure 5 (Right) presents an ablation study where we only opti- mize for the design term in our objective. We ﬁnd that downstream OPE performance degrades without directly op- timizing for linear BC, suggesting that a feature with good coverage alone is not enough to avoid error ampliﬁcation. 7. Conclusion We present BCRL which leverages rich function approxi- mation to learn Bellman complete and exploratory repre- sentations for stable and accurate ofﬂine policy evaluation. We provide a mathematical framework of representation learning in ofﬂine RL, which generalizes all existing repre- sentation learning frameworks from the RL theory literature. We provide an end-to-end theoretical analysis of our ap- proach for OPE and demonstrate that BCRL can accurately estimate policy values with polynomial sample complex- ity. Notably, the complexity has no explicit dependence on the size of the state and action space, instead, it only depends on the statistical complexity of the representation hypothesis class. Experimentally, we extensively evaluate our approach on the DeepMind Control Suite, a set of image- based, continuous control, robotic tasks. First, we show that under the linear evaluation protocol – using linear regres- sion on top of the representations inside the classic LSPE framework – our approach outperforms prior RL representa- tion techniques CURL and SPR which leverage contrastive representation learning techniques SimCLR and BYOL re- spectively. We also show that BCRL achieves competitive OPE performance with the state-of-the-art FQE, and notice- ably improves upon it when evaluating beyond the initial state distribution. Finally, our ablations show that approxi- mate Linear Bellman Completeness and coverage are crucial ingredients to the success of our algorithm. Future work includes extending BCRL to ofﬂine policy optimization. ACKNOWLEDGEMENTS This material is based upon work supported by the National Science Foundation under Grant No. 1846210 and by a Cor- nell University Fellowship. We thank Rahul Kidambi, Ban Kawas, and the anonymous reviewers for useful discussions and feedback. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation References Agarwal, A., Jiang, N., Kakade, S. M., and Sun, W. Rein- forcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, 2019. Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W. Flambe: Structural complexity and representation learn- ing of low rank mdps. NeurIPS, 33:20095–20107, 2020. Amortila, P., Jiang, N., and Xie, T. A variant of the wang- foster-kakade lower bound for the discounted setting. arXiv preprint arXiv:2011.01075, 2020. Anand, A., Racah, E., Ozair, S., Bengio, Y., Côté, Unsupervised state rep- M., and Hjelm, R. D. resentation learning in atari. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox, E. B., and Garnett, R. (eds.), NeurIPS, pp. 8766– URL https://proceedings. 8779, neurips.cc/paper/2019/hash/ 6fb52e71b837628ac16539c1ff911667-Abstract. html. 2019. Antos, A., Szepesvári, C., and Munos, R. Fitted In and Roweis, (eds.), NIPS, volume 20. Curran Associates, URL https://proceedings. q-iteration in continuous action-space mdps. Platt, J., Koller, D., Singer, Y., S. Inc., neurips.cc/paper/2007/file/ da0d1111d2dc5d489242e60ebcbaf988-Paper. pdf. 2008. Bhatia, R. Matrix analysis, volume 169. Springer Science & Business Media, 2013. Boucheron, S., Lugosi, G., and Massart, P. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013. Chen, J. and Jiang, N. Information-theoretic considerations in batch reinforcement learning. In ICML, pp. 1042–1051. PMLR, 2019. Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. E. A simple framework for contrastive learning of visual representations. ICML, 2020. URL https://arxiv. org/abs/2002.05709. Chung, W., Nath, S., Joseph, A., and White, M. Two- timescale networks for nonlinear value function approxi- mation. In ICLR, 2019. URL https://openreview. net/forum?id=rJleN20qK7. Duan, Y., Jia, Z., and Wang, M. Minimax-optimal off-policy evaluation with linear function approximation. In ICML, pp. 2701–2709. PMLR, 2020. Foster, D. J., Krishnamurthy, A., Simchi-Levi, D., and Xu, Y. Ofﬂine reinforcement learning: Fundamental barri- ers for value function approximation. arXiv preprint arXiv:2111.10919, 2021. Fu, J., Norouzi, M., Nachum, O., Tucker, G., Wang, Z., Novikov, A., Yang, M., Zhang, M. R., Chen, Y., Kumar, A., Paduraru, C., Levine, S., and Paine, T. L. Benchmarks for deep off-policy evaluation. ICLR, 2021. URL https: //arxiv.org/abs/2103.16596. Grill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., Piot, B., kavukcuoglu, k., Munos, R., and Valko, M. Bootstrap your own latent - a new approach to self-supervised learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), NeurIPS, volume 33, pp. 21271–21284. Curran As- sociates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ f3ada80d5c4ee70142b17b8192b2958e-Paper. pdf. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor- critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In ICML, pp. 1861–1870. PMLR, 2018. Hafner, D., Lillicrap, T. P., Norouzi, M., and Ba, J. In In- Mastering atari with discrete world models. ternational Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=0oabwyZbOu. Hao, B., Duan, Y., Lattimore, T., Szepesvári, C., and Wang, M. Sparse feature selection makes batch reinforcement learning more sample efﬁcient. In ICML, pp. 4063–4073. PMLR, 2021. He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. B. Mo- mentum contrast for unsupervised visual representation learning. CVPR, 2020. URL http://arxiv.org/ abs/1911.05722. Jiang, N. and Li, L. Doubly robust off-policy value evalua- tion for reinforcement learning. In ICML, pp. 652–661. PMLR, 2016. Du, S. S., Kakade, S. M., Lee, J. D., Lovett, S., Mahajan, G., Sun, W., and Wang, R. Bilinear classes: A struc- tural framework for provable generalization in rl. arXiv preprint arXiv:2103.10897, 2021. Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R. E. Contextual decision processes with low bellman rank are pac-learnable. In ICML, pp. 1704–1713. PMLR, 2017. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. Provably efﬁcient reinforcement learning with linear function ap- proximation. In COLT, pp. 2137–2143. PMLR, 2020. Kostrikov, I. and Nachum, O. Statistical bootstrapping for uncertainty estimation in off-policy evaluation, 2020. URL https://arxiv.org/abs/2007.13609. Laskin, M., Srinivas, A., and Abbeel, P. Curl: Contrastive unsupervised representations for reinforcement learning. In ICML, pp. 5639–5650. PMLR, 2020. Lattimore, T. and Szepesvári, C. Bandit algorithms. Cam- bridge University Press, 2020. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. In ICLR, 2016. Parr, R., Li, L., Taylor, G., Painter-Wakeﬁeld, C., and Littman, M. L. An analysis of linear models, lin- ear value-function approximation, and feature selection In ICML, pp. 752–759, for reinforcement learning. New York, NY, USA, 2008. Association for Comput- doi: 10. ing Machinery. 1145/1390156.1390251. URL https://doi.org/ 10.1145/1390156.1390251. ISBN 9781605582054. Pollard, D. Empirical processes: theory and applications. In NSF-CBMS regional conference series in probability and statistics, pp. i–86. JSTOR, 1990. Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville, A. C., and Bachman, P. Data-efﬁcient rein- forcement learning with momentum predictive represen- tations. ICLR, 2021. URL https://arxiv.org/ abs/2007.05929. Mazoure, B., Tachet des Combes, R., Doan, T. L., Bachman, P., and Hjelm, R. D. Deep reinforcement and infomax learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), NeurIPS, volume 33, pp. 3686–3698. Curran Asso- ciates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ 26588e932c7ccfa1df309280702fe1b5-Paper. pdf. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. A. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop, 2013. URL http://arxiv. org/abs/1312.5602. Modi, A., Chen, J., Krishnamurthy, A., Jiang, N., and Agarwal, A. Model-free representation learning arXiv preprint and exploration in low-rank mdps. arXiv:2102.07035, 2021. Munos, R. and Szepesvári, C. Finite-time bounds for ﬁtted value iteration. JMLR, 9(5), 2008. Nachum, O. and Yang, M. Provable representation learning for imitation with contrastive fourier features. NeurIPS, 2021. URL https://arxiv.org/abs/ 2105.12272. Nedi´c, A. and Bertsekas, D. P. Least squares policy evalua- tion algorithms with linear function approximation. Dis- crete Event Dynamic Systems, 13(1):79–110, 2003. Ni, C., Zhang, A. R., Duan, Y., and Wang, M. Learning good state and action representations via tensor decomposition. In 2021 IEEE International Symposium on Information Theory (ISIT), pp. 1682–1687. IEEE, 2021. learning. reinforcement Song, Z., Parr, R. E., Liao, X., and Carin, L. Linear feature encoding for In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), NIPS, volume 29. Curran Asso- ciates, Inc., 2016. URL https://proceedings. neurips.cc/paper/2016/file/ 8232e119d8f59aa83050a741631803a6-Paper. pdf. Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, arXiv preprint A., et al. Deepmind control suite. arXiv:1801.00690, 2018. Thomas, P. S. and Brunskill, E. Data-efﬁcient off-policy policy evaluation for reinforcement learning, 2016. URL https://arxiv.org/abs/1604.00923. Uehara, M. and Sun, W. Pessimistic model-based ofﬂine reinforcement learning under partial coverage. arXiv preprint arXiv:2107.06226, 2021. Uehara, M., Zhang, X., and Sun, W. Representation learning for online and ofﬂine RL in low-rank MDPs. In ICLR, 2022. URL https://openreview.net/forum? id=J4iSIR9fhY0. van den Oord, A., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv, 2018. URL http://arxiv.org/abs/1807.03748. van der Vaart, A. W. and Wellner, J. A. Weak Con- vergence and Empirical Processes. Springer Se- ISBN ries in Statistics. Springer New York, 1996. 9781475725476. doi: 10.1007/978-1-4757-2545-2. URL http://link.springer.com/10.1007/ 978-1-4757-2545-2. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Wainwright, M. J. High-dimensional statistics: A non- asymptotic viewpoint, volume 48. Cambridge University Press, 2019. Wang, R., Foster, D. P., and Kakade, S. M. What are the statistical limits of ofﬂine RL with linear function approx- imation? ICLR, 2021a. URL https://arxiv.org/ abs/2010.11895. Wang, Y., Wang, R., and Kakade, S. M. An exponential lower bound for linearly-realizable mdps with constant suboptimality gap. arXiv preprint arXiv:2103.12690, 2021b. Yang, M. and Nachum, O. Representation matters: Ofﬂine pretraining for sequential decision making. In Meila, M. and Zhang, T. (eds.), ICML, volume 139 of Proceedings of Machine Learning Research, pp. 11784–11794. PMLR, 2021. URL http://proceedings.mlr.press/ v139/yang21h.html. Yang, M., Nachum, O., Dai, B., Li, L., and Schuurmans, D. Off-policy evaluation via the regularized lagrangian. NeurIPS, 33:6551–6561, 2020. Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. Mastering visual continuous control: Improved data-augmented re- inforcement learning. arXiv preprint arXiv:2107.09645, 2021a. Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R. Improving sample efﬁciency in model- free reinforcement learning from images. AAAI, 2021b. URL http://arxiv.org/abs/1910.01741. Zhang, W., He, J., Zhou, D., Zhang, A., and Gu, Q. Prov- ably efﬁcient representation learning in low-rank markov decision processes. arXiv preprint arXiv:2106.11935, 2021. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Appendices A. Metric Entropy and Entropy Integral We recall the standard notions of entropy integrals here, based on the following distance on Φ, dΦ(φ, (cid:101)φ) =∆ ED (cid:104)(cid:13) (cid:13) (cid:13)φ(s, a) − (cid:101)φ(s, a) (cid:105) (cid:13) (cid:13) (cid:13)2 Let N (t, Φ) denote the t-covering number under dΦ. Deﬁnition A.1. Deﬁne the entropy integral, which we assume to be ﬁnite as, κ(Φ) =∆ (cid:90) 4 0 log1/2 N (t, Φ)dt When Φ is ﬁnite, N (t) ≤ |Φ|, so κ(Φ) ≤ O(log1/2(|Φ|)). B. Technical Lemmas Lemma B.1. Let Xi be i.i.d. random variables s.t. |Xi| ≤ c and E (cid:2)X 2 least 1 − δ, i (cid:3) ≤ ν, then for any δ ∈ (0, 1), we have w.p. at (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 N N (cid:88) i=1 Xi − E [X] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ inf a>0 ν 2a + (c + a) log(2/δ) N , and if ν ≤ LE [X] for some positive L, then in particular, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 N N (cid:88) i=1 (cid:12) (cid:12) Xi − E [X] (cid:12) (cid:12) (cid:12) ≤ 1 2 E [X] + (c + L) log(2/δ) N . Proof. First, by Bernstein’s inequality (Boucheron et al., 2013, Theorem 2.10), we have w.p. 1 − δ, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 N N (cid:88) i=1 (cid:12) (cid:12) Xi − E [X] (cid:12) (cid:12) (cid:12) (cid:114) ≤ 2ν log(2/δ) N + c log(2/δ) N Using the fact that 2xy ≤ x2/a + ay2 for any a > 0, split the square root term, (c + a) log(2/δ) N which yields the ﬁrst part. If ν ≤ LE [X], picking a = L concludes the proof. ≤ inf a>0 ν 2a + We now state several results of Orlicz norms (mostly from Pollard, 1990) for completeness. For an increasing, convex, positive function Φ : R+ (cid:55)→ R+ , such that Φ(x) ∈ [0, 1), deﬁne the Orlicz norm as (cid:107)Z(cid:107)Φ := inf {C > 0 | E [Φ(|Z|/C)] ≤ 1} . It is indeed a norm on the Φ-Orlicz space of random variables LΦ(ν), since it can be interpreted as the Minkowski functional of the convex set K = {X : E [Φ(|X|)] ≤ 1}. Let x1, x2, . . . , xN be i.i.d. datapoints drawn from some underlying distribution. Let ω denote the randomness of the N sampled datapoints, and let Fω = (cid:8)(f (xi(ω)))N i=1 : f ∈ F(cid:9) ⊂ RN denote the (random) set of vectors from the data corresponding to ω. Let σ denote a vector of N i.i.d. Rademacher random variables (±1 equi-probably), independent of all else. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Lemma B.2 (Symmetrization). For any increasing, convex, positive function Φ, we have (cid:34) (cid:32) Eω Φ sup f ∈Fω (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) N (cid:88) i=1 fi − Eωfi (cid:33)(cid:35) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:34) (cid:32) (cid:33)(cid:35) ≤ Eω,σ Φ 2 sup f ∈Fω |σ · f | . Proof. See Theorem 2.2 of (Pollard, 1990). Lemma B.3 (Contraction). Let F ⊂ RN , and suppose λ : RN (cid:55)→ RN s.t. each component λi : R (cid:55)→ R is L-Lipschitz. Then, for any increasing, convex, positive function Φ, we have (cid:34) (cid:32) Eσ Φ sup f ∈F (cid:33)(cid:35) |σ · λ(f )| ≤ (cid:34) (cid:32) (cid:33)(cid:35) Eσ Φ 2L sup f ∈F |σ · f | 3 2 Proof. Apply Theorem 5.7 of (Pollard, 1990) to the functions λi/L, which are contractions. We now focus on the speciﬁc Orlicz space of sub-Gaussian random variables, with the function Ψ(x) = 1 Ψ-Orlicz norm of the maximum of random variables can be bounded by the maximum of the Ψ-Orlicz norms. 5 exp(x2). The Lemma B.4. For any random variables Z1, ..., Zm, we have (cid:107) max i≤m |Zi|(cid:107)Ψ ≤ (cid:112)2 + log(m) max i≤m (cid:107)Zi(cid:107)Ψ Proof. See Lemma 3.2 of (Pollard, 1990). Lemma B.5. For each f ∈ RN , we have (cid:107)σ · f (cid:107)Ψ ≤ 2(cid:107)f (cid:107)2. Proof. See Lemma 3.1 of (Pollard, 1990). The following is a truncated chaining result for Orlicz norms. This result is not new, but many sources state and prove it in terms of covering and Rademacher complexity, rather than for Orlicz norms. In particular, it generalizes Theorem 3.5 of (Pollard, 1990) – consider a sequence of α’s converging to zero. Lemma B.6 (Chaining). Let F ⊂ RN such that 0 ∈ F. Then, (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) sup f ∈F |σ · f | (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Ψ (cid:40) ≤ inf α≥0 3α √ N + 9 (cid:90) b α (cid:112)log(D(δ/2, F))dδ (cid:41) , where b = supf ∈F (cid:107)f (cid:107)2 and D(δ, F) is the Euclidean δ-packing number for F. Proof. Suppose b and all the packing numbers are ﬁnite, otherwise the right hand side is inﬁnite and there is nothing to show. For an arbitrary K > 1, construct a sequence of K ﬁner and ﬁner approximations to F, {0} = F0 ⊂ F1 ⊂ · · · ⊂ FK ⊂ FK+1 = F where for any k ∈ [K], Fk satisﬁes the property that for any f ∈ F, there exist nk(f ) ∈ Fk s.t. (cid:107)nk(f ) − f (cid:107)2 ≤ b2−k. Indeed this can be done iteratively: for any Fk, we can construct Fk+1 by adding elements to construct a maximal b2−k packing (maximality ensures the distance requirement, since the existence of any vector which has larger distance can be added to the packing). By deﬁnition of D(·, F), we have |Fk| ≤ D(b2−k, F). Learning Bellman Complete Representations for Ofﬂine Policy Evaluation For any k ∈ [K], we have by triangle inequality, sup f ∈Fk+1 |σ · f | ≤ sup f ∈Fk+1 |σ · nk(f ))| + sup |σ · (nk(f ) − f )| = sup f ∈Fk |σ · f | + sup f ∈Fk+1 f ∈Fk+1 |σ · (nk(f ) − f )| If k = K, we can loosely bound the right-most term by Cauchy-Schwarz, since for any f ∈ F, we have |σ · (nk(f ) − f )| ≤ √ √ N (cid:107)nk(f ) − f (cid:107)2 ≤ N b2−K. So, we have (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) sup f ∈F |σ · f | (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Ψ ≤ (cid:13) (cid:13) (cid:13) (cid:13) max f ∈FK |σ · f | (cid:13) (cid:13) (cid:13) (cid:13)Ψ + √ N b2−K √ log 5 , since for any non-negative constant c, (cid:107)c(cid:107)Ψ = inf (cid:8)C > 0 : 1 log 5 . If k < K, the suprema are taken over ﬁnite sets, so the maximum is attained. Hence, we can apply a special property of the Ψ-Orlicz norm (Lemma B.4), to get, 5 exp((c/C)2) ≤ 1(cid:9) = c√ (cid:13) (cid:13) (cid:13) (cid:13) max f ∈Fk+1 |σ · f | (cid:13) (cid:13) (cid:13) (cid:13)Ψ By Lemma B.5, ≤ ≤ ≤ ≤ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) max f ∈Fk |σ · f | max f ∈Fk |σ · f | max f ∈Fk |σ · f | max f ∈Fk |σ · f | + (cid:13) (cid:13) (cid:13) (cid:13) max f ∈Fk+1 |σ · (nk(f ) − f ))| (cid:13) (cid:13) (cid:13) (cid:13)Ψ + (cid:112)2 + log(|Fk+1|) max f ∈Fk+1 (cid:107)σ · (nk(f ) − f ))(cid:107)Ψ + 2(cid:112)2 + log(|Fk+1|) max f ∈Fk+1 (cid:107)nk(f ) − f )(cid:107)2 + 2(cid:112)2 + log(|Fk+1|) · b2−k. Also, note that since F0 = {0} by construction, we have (cid:107)maxf ∈F0 |σ · f |(cid:107)Ψ = 0. Unrolling this, we have (cid:107) sup f ⊂F |σ · f |(cid:107)Ψ ≤ √ N b2−K √ log 5 K−1 (cid:88) + 2−k2b (cid:113) 2 + log(D(b2−(k+1), F)) (cid:13) (cid:13) (cid:13) (cid:13)Ψ (cid:13) (cid:13) (cid:13) (cid:13)Ψ (cid:13) (cid:13) (cid:13) (cid:13)Ψ (cid:13) (cid:13) (cid:13) (cid:13)Ψ k=0 √ Since for any D ≥ 2, we have (cid:112)2 + log(1 + D)/ D ≤ 2.2, √ N b2−K √ log 5 √ N b2−K √ log 5 ≤ = Since D(·, F) is a monotone decreasing, √ √ N b2−K √ log 5 N b2−K √ log 5 ≤ = + 4.4b K−1 (cid:88) k=0 2−k (cid:113) log(D(b2−(k+1), F)) K−1 (cid:88) (cid:113) + 17.6b (2−(k+1) − 2−(k+2)) log(D(b2−(k+1), F)) k=0 (cid:90) b/2 + 17.6 b2−(K+1) (cid:112)log(D(δ, F))dδ (cid:90) b + 8.8 b2−K (cid:112)log(D(δ/2, F))dδ. Now consider any α > 0. Pick K such that b 2K+1 ≤ α ≤ b 2K , then we have (cid:107) sup f ⊂F |σ · f |(cid:107)Ψ ≤ √ 2α √ N log 5 + 8.8 (cid:90) b α (cid:112)log(D(δ/2, F))dδ. Since α was arbitrary, the above bound holds when we take an inﬁmum over α. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation C. Proofs for LSPE We ﬁrst show a generalization of the performance difference lemma (PDL). Lemma C.1 (Generalized PDL). For any policies π, π(cid:48), any function f : S × A (cid:55)→ R, and any initial state distribution µ, we have µ − Es∼µ [f (s, π(cid:48))] = V π 1 1 − γ Es,a∼dπ µ (cid:104) T π(cid:48) (cid:105) f (s, a) − f (s, π(cid:48)) (7) Proof. Let T π be the distribution of trajectories τ = (s0, a0, s1, a1, s2, a2, ...) from rolling out π. So, we have, µ − Es∼µ [f (s, π(cid:48))] = Eτ ∼T π V π = Eτ ∼T π = Eτ ∼T π (cid:35) γtr(st, at) − Es∼µ [f (s, π(cid:48))] (cid:35) γt (r(st, at) + f (st, π(cid:48)) − f (st, π(cid:48))) − f (s0, π(cid:48)) γt (r(st, at) + γf (st+1, π(cid:48)) − f (st, π(cid:48))) (cid:35) (cid:34) ∞ (cid:88) t=0 (cid:34) ∞ (cid:88) t=0 (cid:34) ∞ (cid:88) t=0 = = 1 1 − γ 1 1 − γ Es,a∼dπ µ (cid:2)r(s, a) + γEs(cid:48)∼P (s,a) [f (s(cid:48), π(cid:48))] − f (s, π(cid:48))(cid:3) Es,a∼dπ µ (cid:104) T π(cid:48) (cid:105) f (s, a) − f (s, π(cid:48)) . This generalizes the PDL, which we can get by setting f (s, a) = Qπ(cid:48) (s, a): µ − V π(cid:48) V π µ = = = 1 1 − γ 1 1 − γ 1 1 − γ Es,a∼dπ µ Es,a∼dπ µ (cid:104) T π(cid:48) (cid:104) Qπ(cid:48) Qπ(cid:48) (s, a) − Qπ(cid:48) (s, π(cid:48)) (cid:105) (s, a) − V π(cid:48) (s) (cid:105) Es,a∼dπ µ (cid:104) Aπ(cid:48) (cid:105) (s, a) . To prove our LSPE guarantee, we’ll instantiate f (s, a) to be the estimated (cid:98)f (s, a) from LSPE, and set π = π(cid:48). This gives us an expression for the prediction error of LSPE, (cid:12) (cid:12)V π (cid:12) µ − Es∼µ (cid:104) (cid:98)fk(s, π) (cid:105)(cid:12) (cid:12) (cid:12) = 1 1 − γ (cid:12) (cid:12) (cid:12) Edπ µ (cid:104) T π (cid:98)fk(s, a) − fk(s, a) (cid:105)(cid:12) (cid:12) (cid:12) . We then upper bound the right hand side by its L2(dπ of running LSPE by the regression losses at each step. Lemma C.2. Consider any policy π and functions f1, . . . , fK : S × A (cid:55)→ R that satisfy maxk=1,...,K (cid:107)fk − T πfk−1(cid:107)L2(dπ ) ≤ η, and f0(s, a) = 0. Then, for all k = 1, . . . , K, we have (cid:107)fk − T πfk(cid:107)L2(dπ µ) norm, which is the Bellman error. Next, we bound the Bellman error ) ≤ 4 1−γ η + γk/2. p0 p0 Proof. For any k = 1, . . . , K, (cid:107)fk − T πfk(cid:107)L2(dπ p0 ) ≤ (cid:107)fk − T πfk−1(cid:107)L2(dπ ≤ η + γ(Es,a∼dπ p0 ≤ η + γ(Es,a∼dπ p0 ) + (cid:107)T πfk−1 − T πfk(cid:107)L2(dπ p0 ) p0 [(Es(cid:48),a(cid:48)∼P (s,a)◦π [fk−1(s(cid:48), a(cid:48)) − fk(s(cid:48), a(cid:48))])2])1/2 ,s(cid:48),a(cid:48)∼P (s,a)◦π[(fk−1(s(cid:48), a(cid:48)) − fk(s(cid:48), a(cid:48)))2])1/2 Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Since dπ p0 thus non-negative, (s, a) = γE˜s,˜a∼dπ P0 P (s|˜s, ˜a)π(a|s) + (1 − γ)p0(s)π(a|s), and the quantity inside the expectation is a square, and ≤ η + γ(γ−1Es,a∼dπ p0 [(fk−1(s, a) − fk(s, a))2])1/2 √ = η + γ(cid:107)fk−1 − fk(cid:107)L2(dπ p0 ) √ ≤ η + γ (cid:16) η + (cid:107)fk−1 − T πfk−1(cid:107)L2(dπ p0 (cid:17) . ) Unrolling the recursion and using (cid:107)f0 − T πf0(cid:107)L2(dπ p0 ) ≤ 1, we have (cid:107)fK − T πfK(cid:107)L2(dπ p0 ) ≤ η + √ γ(η + √ γ(η + . . . √ γ(η + 1) . . . )) √ γK √ γ 1 − 1 − = η + γK/2, which gives the claim since 1 √ 1− γ ≤ 2(1 − γ)−1 and 1 − √ γK ≤ 1. The following lemma is a “fast rates”-like result for norms. It shows that the norm induced by the empirical covariance matrix (cid:98)Σ can be bounded by the norm induced by two times the population covariance matrix Σ, up to some (cid:101)O(N −1/2) terms. Lemma C.3 (Fast Rates for Σ-norm). For any δ ∈ (0, 1), we have with probability at least 1 − δ, for any x ∈ BW , we have and (cid:107)x(cid:107) (cid:98)Σ ≤ 2(cid:107)x(cid:107)Σ + 5W (cid:114) d log(N/δ) N , (cid:107)x(cid:107)Σ ≤ 2(cid:107)x(cid:107) (cid:98)Σ + 5W (cid:114) d log(N/δ) N . Proof. First, ﬁx any x ∈ BW . Since (x(cid:124)φ(s, a))2 ≤ W 2 almost surely, we have E (cid:2)(x(cid:124)φ(s, a))4(cid:3) ≤ W 2E (cid:2)(x(cid:124)φ(s, a))2(cid:3). So by Lemma B.1, w.p. at least 1 − δ, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 N N (cid:88) i=1 (cid:12) (cid:12) (x(cid:124)φ(si, ai))2 − E (cid:2)(x(cid:124)φ(s, a))2(cid:3) (cid:12) (cid:12) (cid:12) ≤ 1 2 E (cid:2)(x(cid:124)φ(s, a))2(cid:3) + 2W 2 log(2/δ) N . In particular, this means (cid:107)x(cid:107)2 (cid:98)Σ W/ N √ N -net of BW , which can be done with (1 + 2 2 (cid:107)x(cid:107)2 ≤ 3 Σ + 2W 2 log(2/δ) √ and (cid:107)x(cid:107)2 Σ ≤ 2(cid:107)x(cid:107)2 (cid:98)Σ + 4W 2 log(2/δ) N . Now union bound over a N )d elements. The approximation error from this cover is (cid:107)x(cid:107) ≤ (cid:98)Σ ≤ (cid:107)n(x)(cid:107) (cid:114) 3 2 (cid:114) 3 2 (cid:114) 3 2 ≤ ≤ (cid:98)Σ + (cid:107)n(x) − x(cid:107) (cid:98)Σ (cid:115) (cid:107)n(x)(cid:107)Σ + 2W 2 log(2(1 + 2 (cid:114) 3 2 (cid:114) (cid:107)x(cid:107)Σ + (cid:107)n(x) − x(cid:107)Σ + (cid:107)x(cid:107)Σ + 4W d log(N/δ) N , √ N )d/δ) + W √ N N (cid:115) 2W 2 log(2(1 + 2 √ N )d/δ) N + W √ N Learning Bellman Complete Representations for Ofﬂine Policy Evaluation where n(x) is the closest element in the net to x. Similarly, (cid:107)x(cid:107)Σ ≤ (cid:107)n(x)(cid:107)Σ + (cid:107)n(x) − x(cid:107)Σ ≤ 2(cid:107)n(x)(cid:107) (cid:98)Σ + (cid:115) 4W 2 log(2(1 + 2 √ N )d/δ) + N (cid:115) W √ N √ ≤ 2(cid:107)x(cid:107) (cid:98)Σ + 2(cid:107)n(x) − x(cid:107) (cid:114) (cid:98)Σ + + ≤ 2(cid:107)x(cid:107) (cid:98)Σ + 5W d log(N/δ) N . 4W 2 log(2(1 + 2 N )d/δ) + W √ N N Now deﬁne the following notation for analyzing Least Squares Policy Evaluation (LSPE). For every target vector (cid:107)ϑ(cid:107)2 ≤ W , deﬁne yϑ = r + γϑ(cid:124)φ(s(cid:48), π), θϑ = arg min (cid:107)θ(cid:107)≤W Es,a∼ν,s(cid:48)∼P (s,a)[(yϑ − θ(cid:124)φ(s, a))2] := (cid:96)(θ, ϑ), i = ri + γϑ(cid:124)φ(s(cid:48) yϑ (cid:98)θϑ = arg min (cid:107)θ(cid:107)≤W 1 N i, π), N (cid:88) i=1 (yϑ i − θ(cid:124)φ(si, ai))2 := (cid:98)(cid:96)(θ, ϑ). The following lemma are useful facts about the optimal θϑ and (cid:98)θϑ. Lemma C.4. For any ϑ1, ϑ2, we have For any θ, ϑ, we have (cid:107)θϑ1 − θϑ2 (cid:107)Σ ≤ (cid:112)2γW (cid:107)ϑ1 − ϑ2(cid:107)2, (cid:98)Σ ≤ (cid:112)2γW (cid:107)ϑ1 − ϑ2(cid:107)2. (cid:107)(cid:98)θϑ1 − (cid:98)θϑ2 (cid:107) (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) ≥ (cid:107)θ − θϑ(cid:107)2 Σ. recall that θϑ minimizes f (θ) = E (cid:2)(yϑ − θ(cid:124)φ(s, a))2(cid:3), which has the Jacobian ∇f (θ) = Proof. First, 2E (cid:2)(θ(cid:124)φ(s, a) − yϑ)φ(s, a)(cid:3). Since θϑ is optimal over BW , the necessary optimality condition is that −∇f (θϑ) ∈ ϑφ(s, a))(cid:3) , θ − θϑ(cid:105) ≤ 0. In NBW (θϑ), the normal cone of BW at θϑ, i.e. for any θ ∈ BW , we have (cid:104)E (cid:2)φ(s, a)(yϑ − θ particular, we have (cid:124) adding the two we get (cid:104)E (cid:2)φ(s, a)(yϑ1 − θ (cid:104)E (cid:2)φ(s, a)(yϑ2 − θ (cid:124) ϑ1 (cid:124) ϑ2 φ(s, a))(cid:3) , θϑ2 − θϑ1(cid:105) ≤ 0 φ(s, a))(cid:3) , θϑ1 − θϑ2(cid:105) ≤ 0 (cid:107)θϑ1 − θϑ2 (cid:107)2 Σ = (cid:104)E [φ(s, a)(θϑ1 − θϑ2 )(cid:124)φ(s, a))] , θϑ1 − θϑ2(cid:105) ≤ (cid:104)E (cid:2)(yϑ1 − yϑ2)φ(s, a)(cid:3) , θϑ1 − θϑ2(cid:105) = γ(cid:104)E [φ(s, a)φ(s(cid:48), π)(cid:124)] (ϑ1 − ϑ2), θϑ1 − θϑ2(cid:105) ≤ γ(cid:107)E [φ(s, a)φ(s(cid:48), π)(cid:124)] (cid:107)2(cid:107)ϑ1 − ϑ2(cid:107)2(cid:107)θϑ1 − θϑ2 (cid:107)2 ≤ 2γW (cid:107)ϑ1 − ϑ2(cid:107)2. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation The claim with (cid:98)θϑ1, (cid:98)θϑ2 follows by the same argument. For the second claim, we ﬁrst apply the Parallelogram law, followed by the ﬁrst-order optimality of θϑ, (cid:96)(θ, ϑ) = (cid:96)(θϑ, ϑ) + Eν((θϑ − θ)(cid:124)φ(s, a))2 + 2Eν[(yϑ − θ(cid:124)φ(s, a))φ(s, a)(cid:124)(θϑ − θ)] ≥ (cid:96)(θϑ, ϑ) + Eν((θϑ − θ)(cid:124)φ(s, a))2 + 0 = (cid:96)(θϑ, ϑ) + (cid:107)θ − θϑ(cid:107)2 Σ. Now we show our key lemma about the concentration of least squares, uniformly over all targets generated by ϑ ∈ BW . Lemma C.5 (Concentration for Least Squares). Let F = {(s, a) (cid:55)→ θ(cid:124)φ(s, a) : (cid:107)θ(cid:107)2 ≤ W }, with (cid:107)φ(s, a)(cid:107)2 ≤ 1. Then, for any δ ∈ (0, 1) w.p. at least 1 − δ, we have (cid:107)ˆθϑ − θϑ(cid:107)Σ < 120d(1 + W ) sup ϑ∈BW log(N )(cid:112)log(10/δ) √ N . Proof. By Lemma C.3, we have that w.p. at least 1 − δ, we can bound the random (cid:107) · (cid:107) simultaneously over all vectors in a ball, (cid:98)Σ norm by (cid:107) · (cid:107)Σ, and vice versa, E = {∀x ∈ B2W , (cid:107)x(cid:107) (cid:98)Σ ≤ 2(cid:107)x(cid:107)Σ + t and (cid:107)x(cid:107)Σ ≤ 2(cid:107)x(cid:107) (cid:98)Σ + t}, provided t ≥ 5W probability and expectations will be implicitly conditioned on E. (cid:113) d log(N/δ) N . For the remainder of this proof, we condition on this high-probability event. That is, any First, we’ll show that for an arbitrary and ﬁxed ϑ ∈ BW , w.p. at least 1 − δ, we have (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ < t. To do so, we will bound the probability of the complement, which in turn can be simpliﬁed by the following chain of arguments, (cid:107)ˆθϑ − θϑ(cid:107)Σ ≥ t By (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) ≥ (cid:107)θ − θϑ(cid:107)Σ from Lemma C.4, =⇒ (cid:96)(ˆθϑ, ϑ) − (cid:96)(θϑ, ϑ) ≥ t2 =⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) ≥ t2, ˆ(cid:96)(θ, ϑ) − ˆ(cid:96)(θϑ, ϑ) ≤ 0 By convexity and continuity of (cid:96)(·, ϑ), we can make this strict equality. Indeed, given this, by Intermediate Value Theorem, there exists λ ∈ [0, 1] such that θ(cid:48) = (1 − λ)θ + λθϑ has (cid:96)(θ(cid:48), ϑ) − (cid:96)(θϑ, ϑ) = ν. Then by convexity, ˆ(cid:96)(θ(cid:48), ϑ) − ˆ(cid:96)(θϑ, ϑ) ≤ (1 − λ)ˆ(cid:96)(θ, ϑ) − (1 − λ)ˆ(cid:96)(θϑ, ϑ) ≤ 0. =⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) = t2, ˆ(cid:96)(θ, ϑ) − ˆ(cid:96)(θϑ, ϑ) ≤ 0 =⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:107)θ − θϑ(cid:107)Σ ≤ t, ((cid:96)(θ, ϑ) − ˆ(cid:96)(θ, ϑ)) − ((cid:96)(θϑ, ϑ) − ˆ(cid:96)(θϑ, ϑ)) ≥ t2 By conditioning on E, =⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:107)θ − θϑ(cid:107) (cid:98)Σ ≤ 3t, ((cid:96)(θ, ϑ) − ˆ(cid:96)(θ, ϑ)) − ((cid:96)(θϑ, ϑ) − ˆ(cid:96)(θϑ, ϑ)) ≥ t2 Hence, we now focus on bounding (cid:32) P sup θ∈Θ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) N (cid:88) i=1 (cid:12) (cid:12) ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)] (cid:12) (cid:12) (cid:12) (cid:33) ≥ N t2 ≤ δ, Learning Bellman Complete Representations for Ofﬂine Policy Evaluation where we deﬁne ζi(θ, ϑ) = (yϑ i − θ(cid:124)φ(si, ai))2, Θ = {θ : (cid:107)θ(cid:107) ≤ W, (cid:107)θ − θϑ(cid:107) Since ζi(θ, ϑ) − ζi(θϑ, θ) = λi((θ − θϑ)(cid:124)φ(si, ai)) where λi(x) = x(2yϑ and λi(0) = 0, the contraction lemma (Lemma B.3) tells us that it sufﬁces to consider a simpler class. Deﬁne (cid:98)Σ ≤ 3t}. i − (θ + θϑ)(cid:124)φ(si, ai)) is 2(1 + W )-Lipschitz (8) ωi(θ, ϑ) = (θ − θϑ)(cid:124)φ(si, ai) and ω(θ, ϑ) = (ωi(θ, ϑ))N i=1. By Chaining (Lemma B.6), we have (cid:18) 1 J (cid:40) |σ · ω(θ, ϑ)| Eσ Ψ (cid:20) sup Θ √ (cid:90) b where J = inf α≥0 3α N + 9 α (cid:19)(cid:21) ≤ 1 (cid:112)log(D(δ/2, ω(Θ)))dδ (cid:41) , D(δ, F) is the Euclidean packing number of F ⊂ RN , and b = supΘ (cid:107)ω(θ, ϑ)(cid:107)2 is the envelope. ω(Θ) = {ω(θ, ϑ) : θ ∈ Θ} , Now we’ll bound the truncated entropy integral, J. First notice that b ≤ 3t which localizes in (cid:107) · (cid:107) (cid:98)Σ, √ N , based on the deﬁnition of Θ (Equation (8)), b √ N = sup Θ (cid:118) (cid:117) (cid:117) (cid:116) 1 N N (cid:88) i=1 (θ − θϑ)(cid:124)φ(si, ai)φ(si, ai)(cid:124)(θ − θϑ) = sup Θ (cid:107)θ − θϑ(cid:107) (cid:98)Σ ≤ 3t. Now, we bound the packing number D(·, ω(Θ)). Let θ1, θ2 ∈ BW be arbitrary, (cid:107)ω(θ1, ϑ) − ω(θ2, ϑ)(cid:107)2 √ N = (cid:107)θ1 − θ2(cid:107) (cid:98)Σ ≤ (cid:113) σmax((cid:98)Σ)(cid:107)θ1 − θ2(cid:107)2 ≤ (cid:107)θ1 − θ2(cid:107)2. So for any ε, we can construct an ε-cover by setting (cid:107)θ1 − θ2(cid:107)2 ≤ ε/ N (ε, F) denote the Euclidean covering number of F ⊂ RN . Then, √ N , which requires (W (1 + 2 √ N /ε))d points. Let log D(ε, ω(Θ)) ≤ log N (ε/2, ω(Θ)) ≤ d log W (1 + 4 √ N /ε) So, √ N (cid:90) 3t (cid:112)log(D(ε/2, ω(Θ)))dε 0 (cid:90) 3t √ N (cid:113) d log(W (1 + 8 √ N /ε))dε J ≤ ≤ 0 √ ≤ 3t √ ≤ 3t dN ((cid:112)log W + (cid:90) 1 (cid:112)log(1 + 3/(εt)))dε dN ((cid:112)log W + (cid:112)log(4/t)), 0 since (cid:82) 1 0 (cid:112)log(1 + c/ε)dε ≤ (cid:112)log(1 + c) for any c > 0, and assuming t ≤ 1. Now we put everything together. Let c denote a positive constant, (cid:32) P (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) N (cid:88) i=1 (cid:12) (cid:12) ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)] (cid:12) (cid:12) (cid:12) sup Θ (cid:33) ≥ N t2 Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Since Ψ is increasing, (cid:32) (cid:32) = P Ψ c sup Θ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) N (cid:88) i=1 (cid:12) (cid:12) ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)] (cid:12) (cid:12) (cid:12) (cid:33) (cid:33) ≥ Ψ (cid:0)cN t2(cid:1) By Markov’s inequality, (cid:104) E Ψ ≤ (cid:16) c supΘ (cid:12) (cid:12) (cid:12) (cid:80)N (cid:12) i=1 ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)] (cid:12) (cid:12) (cid:17)(cid:105) Ψ (cN t2) By Symmetrization (Lemma B.2), ≤ E [Eσ [Ψ (2c supΘ |σ · (ζ(θ, ϑ) − ζ(θϑ, ϑ))|)]] Ψ (cN t2) By Contraction (Lemma B.3) and that ζi(θ, ϑ) − ζi(θϑ, ϑ) = λi(ωi(θ, ϑ)) where λi is L = 2(1 + W ) Lipschitz, ≤ 3 2 · E [Eσ [Ψ (4cL supΘ |σ · ω(θ, ϑ)|)]] Ψ (cN t2) Setting c = 1 4LJ and applying Chaining (Lemma B.6), the numerator is bounded by 1, (cid:32) ≤ 8 exp − (cid:19)2(cid:33) (cid:18) N t2 4LJ Applying upper bound on J,  (cid:32) ≤ 8 exp − 8(1 + W ) · 3t √ N t2 √ dN ( log W + (cid:112)log(4/t)) (cid:19) (cid:33)2  (cid:18) ≤ 8 exp − N t2 242(1 + W )2d(log W + log(4/t)) Now, we set t = 24(1 + W ) (cid:113) d log(N ) log(1/δ) N , (cid:18) ≤ 8 exp − log(N ) log(1/δ) log(N/d log(N )) (cid:19) Since log(N ) ≥ log(N/d log(N )), ≤ 8δ. Hence, we have shown that for an arbitrary and ﬁxed ϑ ∈ BW , w.p. 1 − 9δ, we have (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ < t = 24(1 + W ) (cid:114) d log(N ) log(1/δ) N . We ﬁnally apply a union bound over ϑ ∈ BW . Consider a W/N -cover of ϑ ∈ BW , which requires (1 + 2N )d points. Then, for any ϑ ∈ BW , we have (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ ≤ (cid:107)(cid:98)θϑ − (cid:98)θn(ϑ)(cid:107)Σ + (cid:107)(cid:98)θn(ϑ) − θn(ϑ)(cid:107)Σ + (cid:107)θn(ϑ) − θϑ(cid:107)Σ (cid:98)Σ + t) + t + (cid:112)2γW (cid:107)n(ϑ) − ϑ(cid:107) ≤ (2(cid:107)(cid:98)θϑ − (cid:98)θn(ϑ)(cid:107) ≤ 2t + 3(cid:112)2γW (cid:107)n(ϑ) − ϑ(cid:107) ≤ 2t + 3(cid:112)2γW 2/N ≤ 5t. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Thus, we have shown that w.p. 1 − δ, sup ϑ∈BW (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ < 120d(1 + W ) log(N )(cid:112)log(10/δ) √ N . A nice corollary is that when Σ provides full coverage, i.e. Σ (cid:23) βI for some positive β, then we can bound sup ϑ∈BW (cid:107)ˆθϑ − θϑ(cid:107)2 ≤ σmin(Σ)−1/2 sup ϑ∈BW (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ ≤ β−1/2 · sup ϑ∈BW (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ. C.1. Main LSPE theorem We now prove our LSPE sample complexity guarantee Theorem 3.3. Theorem 3.3 (Sample Complexity of LSPE). Assume feature φ satisﬁes approximate Linear BC with parameter εν. For any δ ∈ (0, 0.1), w.p. at least 1 − δ, we have for any initial state distribution p0 (cid:12) (cid:12)V πe (cid:12) p0 (cid:104) − Es∼p0 (cid:98)fK(s, πe) (cid:105)(cid:12) (cid:12) (cid:12) ≤ γK/2 1 − γ + (cid:114)(cid:13) (cid:13) 4 (cid:13) ddπe p0 dν (cid:13) (cid:13) (cid:13)∞ (1 − γ)2 εν + 480(cid:112)κ(p0)(1 + W )d log(N )(cid:112)log(10/δ) √ (1 − γ)2 N , where (cid:98)fK is the output of Algorithm 1. Proof of Theorem 3.3. By Lemmas C.1 and C.2, (cid:12) (cid:12)V π (cid:12) p0 (cid:104) − Es∼p0 (cid:98)fk(s, π) (cid:105)(cid:12) (cid:12) (cid:12) ≤ 4 (1 − γ)2 max k=1,2,... (cid:107) ˆfk − T π ˆfk−1(cid:107)L2(dπ p0 ) + γk/2 1 − γ . Next, we bound the maximum regression error. Consider any initial state distribution p0, then we have max k=1,2,... (cid:107) ˆfk − T π ˆfk−1(cid:107)L2(dπ p0 ) ≤ sup (cid:107)ϑ(cid:107)≤W (cid:124) (cid:107)ˆθ ϑφ − T π(ϑ(cid:124)φ)(cid:107)L2(dπ p0 ) ≤ sup (cid:107)ϑ(cid:107)≤W (cid:124) (cid:107)ˆθ ϑφ − θ (cid:124) ϑφ(cid:107)L2(dπ p0 (cid:107)θ (cid:124) ϑφ − T π(ϑ(cid:124)φ)(cid:107)L2(dπ p0 ) ≤ (cid:112)κ(p0) sup (cid:107)ϑ(cid:107)≤W (cid:107)ˆθϑ − θϑ(cid:107)Σ + sup (cid:107)ϑ(cid:107)≤W (cid:107)θ (cid:124) ϑφ − T π(ϑ(cid:124)φ)(cid:107)L2(ν) ) + sup (cid:107)ϑ(cid:107)≤W (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) ddπ p0 dν ddπ p0 dν (cid:13) (cid:13) (cid:13) (cid:13)∞ (cid:13) (cid:13) (cid:13) (cid:13)∞ εν. ≤ (cid:112)κ(p0) sup ϑ∈BW (cid:107)ˆθϑ − θϑ(cid:107)Σ + The quantity supϑ∈BW (cid:107)ˆθϑ − θϑ(cid:107)Σ can be directly bounded by Lemma C.5 w.p. at least 1 − δ. Thus, we have shown the desired result: for any initial state distribution p0, (cid:12) (cid:12)V π (cid:12) p0 − Es,a∼p0◦π (cid:104) (cid:98)fk(s, a) (cid:105)(cid:12) (cid:12) (cid:12) ≤ 4 (1 − γ)2 (cid:32) (cid:112)κ(p0)120d(1 + W ) log(N )(cid:112)log(10/δ) √ N + (cid:33) (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) ddπ p0 dν (cid:13) (cid:13) (cid:13) (cid:13)∞ εν + γk/2 1 − γ . Learning Bellman Complete Representations for Ofﬂine Policy Evaluation D. Proofs for Linear BC Equivalence Proposition 4.2. Consider a feature φ with full rank covariance Σ(φ). Given any W > 0, the feature φ being linear BC (under BW ) implies that there exist (ρ, M ) ∈ BW × Rd×d with (cid:107)M (cid:107)2 ≤ 1 − (cid:107)ρ(cid:107)2 (cid:113) W 2 , and Eν (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) φ(s, a) − (cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 = 0. (2) On the other hand, if there exists (ρ, M ) ∈ BW × Rd×d with (cid:107)M (cid:107)2 < 1 such that the above equality holds, then φ must satisfy exact linear BC with W ≥ (cid:107)ρ(cid:107)2 . 1−(cid:107)M (cid:107)2 Proof. (⇐=) Suppose that (cid:107)M (cid:107)2 < 1 and ρ ∈ BW satisfy, Eν (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) φ(s, a) − (cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 = 0. Then, for any w1 ∈ BW , setting w2 = ρ + M (cid:124)w1 satisﬁes, 2 φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w (cid:124) 1 φ(s(cid:48), πe)](cid:1)2 (cid:107)w (cid:124) 2 φ − T πe (w (cid:124) 1 φ)(cid:107)2 ν = Eν = Eν = Eν = 0. (cid:124) (cid:0)w (cid:0)ρ(cid:124)φ(s, a) − r(s, a) + w (cid:0)w (cid:124) 1 (cid:0)M φ(s, a) − γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)](cid:1)(cid:1)2 (cid:124) 1 M φ(s, a) − γEs(cid:48)∼P (s,a) [w (cid:124) 1 φ(s(cid:48), πe)](cid:1)2 Also, we have (cid:107)w2(cid:107)2 ≤ (cid:107)ρ(cid:107)2 + (cid:107)M (cid:107)2(cid:107)w1(cid:107)2 ≤ W since W ≥ (cid:107)ρ(cid:107)2 1−(cid:107)M (cid:107)2 . Thus, φ satisﬁes exact Linear BC. (=⇒) Suppose φ satisﬁes exact Linear BC, that is max w1∈BW min w2∈BW (cid:107)w (cid:124) 2 φ − T π(w (cid:124) 1 φ)(cid:107)2 ν = 0. To see that there exists ρ ∈ BW that linearizes the reward w.r.t φ under ν, set w1 = 0, and we have: min w2∈BW Es,a∼ν (cid:13) (cid:13)w(cid:62) 2 φ(s, a) − r(s, a)(cid:13) 2 2 = 0. (cid:13) Let ρ to be the minimizer of the above objective. Now we need to show that there exists a M ∈ Rd×d with (cid:107)M (cid:107)2 < (cid:113) 1 − (cid:107)ρ(cid:107)2 W 2 that satisﬁes Es,a∼ν (cid:13) (cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13) 2 2 = 0. (cid:13) To extract the i-th row of M , plug in wi = W ei (note that wi ∈ BW ). By exact Linear BC, we know that there exists a vector vi ∈ BW , such that: (cid:13) (cid:13)v (cid:124) i φ(s, a) − ρ(cid:124)φ(s, a) − γW Es(cid:48)∼P (s,a)e Repeating this for every i ∈ [d], we can construct M as follows, M = 1 W  (v1 − ρ)(cid:124) ...   (vd − ρ)(cid:124)    , (cid:124) i φ(s(cid:48), πe)(cid:13) (cid:13)ν = 0. which satisﬁes, Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Es,a∼ν d (cid:88) = i=1 d (cid:88) i=1 = (cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13) (cid:13) 2 (cid:13) 2 Es,a∼ν (cid:13) (cid:13)e (cid:124) i (cid:0)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:1)(cid:13) 2 (cid:13) 2 Es,a∼ν (cid:13) (cid:13) (cid:13) (cid:13) 1 W (vi − ρ)(cid:124)φ(s, a) − γEs(cid:48)∼P (s,a)e (cid:13) 2 (cid:13) (cid:124) i φ(s(cid:48), πe) (cid:13) (cid:13) 2 = 0. Hence, we have (cid:13) (cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13) 2 ν = 0. (cid:13) (cid:113) 1 − (cid:107)ρ(cid:107)2 W 2 . First we show that (cid:107)M (cid:107)2 ≤ 1. For any w1 ∈ BW , by exact linear Finally, we must show that (cid:107)M (cid:107)2 < BC, there exists w2 ∈ BW s.t. (cid:13) 1 φ(s(cid:48), πe)](cid:13) 2 ν = 0, and by the construction of M , (cid:13)w (cid:13) φ(s, a)(cid:107)2 satisﬁes (cid:107)(w2 − ρ − M (cid:124)w1) Σ(φ) = 0. Since Σ(φ) is positive deﬁnite, we have that w2 = ρ + M (cid:124)w1 is the unique choice of w2, which by exact linear BC is in BW . Hence, we’ve shown that for any w1 ∈ BW , we also have that ρ + M (cid:124)w1 ∈ BW . Now take w1 and −w1, subtracting the two expressions yields that 2M (cid:63)(cid:124)w1 ∈ B2W . Since this is true for arbitrary w1, taking supremum over w1 ∈ BW shows that (cid:107)M (cid:107)2 ≤ 1. 2 φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w ν = (cid:107)w2 − ρ − M (cid:124)w1(cid:107)2 (cid:124) (cid:124) (cid:124) (cid:124) Now we show that the inequality must be strict. Consider the singular value decomposition: M = (cid:80)d i where {ui}i∈[d] and {vi}i∈[d] are each an orthonormal basis of Rd, and σi is the i-th largest singular value. Without loss of generality, suppose ρ(cid:124)u1 ≥ 0, since we can always ﬂip the sign of u1. If we pick x = W v1 ∈ BW , we have M x = W σ1u1. By the argument in the previous paragraph, since x ∈ BW , we have ρ + M x ∈ BW , implying that i=1 σiuiv W 2 ≥ (cid:107)M x + ρ(cid:107)2 2 = (cid:107)W σ1u1 + (ρ(cid:124)u1)u1 + (ρ − (ρ(cid:124)u1)u1)(cid:107)2 2 Since u1 and (ρ − (ρ(cid:124)u1)u1) are orthogonal, by Pythagoras, we have = |W σ1 + ρ(cid:124)u1|2 + (cid:107)(ρ − (ρ(cid:124)u1)u1)(cid:107)2 2 = (W σ1)2 + 2W σ1ρ(cid:124)u1 + (ρ(cid:124)u1)2 + (cid:107)(ρ − (ρ(cid:124)u1)u1)(cid:107)2 2 = (W σ1)2 + 2W σ1ρ(cid:124)u1 + (cid:107)(ρ(cid:124)u1)u1(cid:107)2 2 + (cid:107)(ρ − (ρ(cid:124)u1)u1)(cid:107)2 2 By Pythagoras, = (W σ1)2 + 2W σ1ρ(cid:124)u1 + (cid:107)ρ(cid:107)2 2 . Hence, we get the following inequality: Solve for σ1 and using the fact that ρ(cid:124)u1 ≥ 0, we have that, W 2σ2 1 + 2W (ρ(cid:124)u1)σ1 + (cid:107)ρ2(cid:107)2 − W 2 ≤ 0. σ1 ≤ −ρ(cid:124)u1 + (cid:112)(ρ(cid:124)u1)2 + (W 2 − (cid:107)ρ(cid:107)2) W (cid:112)W 2 − (cid:107)ρ(cid:107)2 W ≤ (cid:114) ≤ 1 − (cid:107)ρ(cid:107)2 W 2 . We ﬁnally show that (cid:107)ρ(cid:107)2 < W unless M = 0. We prove this by contradiction. Assume (cid:107)ρ(cid:107)2 ≥ W . Following the above argument, take any w1 ∈ BW , we must have w2 := ρ + M (cid:124)w1 ∈ BW . We discuss two cases. First if ρ (cid:54)∈ range(M (cid:124)). In this case, we must have (cid:107)w2(cid:107)2 has non-zero entries. Thus, this case leads to contradiction . 2 = (cid:107)ρ(cid:107)2 2 + (cid:107)M (cid:124)w1(cid:107)2 2 = W 2 + (cid:107)M (cid:124)w1(cid:107)2 2 > W 2, as long as M Second, if ρ ∈ range(M (cid:124)). In this case, there must exist a vector x (cid:54)= 0 such that M (cid:124)x = ρ. Consider ¯x := W x (cid:107)x(cid:107)2 We have w2 := ρ + M (cid:124) ¯x = ρ , which means that (cid:107)w2(cid:107)2 > W , which causes contradiction again. (cid:17) (cid:16) 1 + W (cid:107)x(cid:107)2 ∈ BW . So unless M = 0, which only happens when γ = 0 (i.e. horizon is 1), we have (cid:107)ρ(cid:107)2 < W . Learning Bellman Complete Representations for Ofﬂine Policy Evaluation (cid:21) Eν (ρ,M )∈Θ Llbc(φ) = min We now show an approximate version to the equivalence of Proposition 4.2. First, recall the bilevel loss from Equation (3). We use Llbc and (cid:98)Llbc to denote the population and empirical versions as follows, (cid:13) (cid:20)M (cid:20)γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)] (cid:13) (cid:13) ρ(cid:124) r(s, a) (cid:13) (cid:21)(cid:13) (cid:13) (cid:20)M 2 (cid:13) (cid:13) (cid:13) (cid:13) ρ(cid:124) (cid:13) (cid:13) 2 where Θ = (cid:8)(ρ, M ) ∈ BW × Rd×d : (cid:107)ρ(cid:107) ≤ (cid:107)ρ(cid:63)(cid:107), (cid:107)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2 (cid:9) and (ρ(cid:63), M (cid:63)) are corresponding to the linear BC φ(cid:63). Lemma D.1. Suppose a feature (cid:98)φ satisﬁes Llbc( (cid:98)φ) ≤ ε2. Then, (cid:98)φ is ε(1 + W )-approximately Linear BC, provided W ≥ (cid:107)ρ(cid:63)(cid:107)2 ED (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2 2 , (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:98)Llbc(φ) = min (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 − min g∈G φ(s, a) − φ(s, a) − (ρ,M )∈Θ (10) ED (9) (cid:21) . 1−(cid:107)M (cid:63)(cid:107)2 Proof. Suppose Llbc( (cid:98)φ) ≤ ε2, so there exists (cid:99)M (s.t. (cid:107) (cid:99)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2 < 1) and (cid:98)ρ ∈ BW s.t. (cid:20) (cid:21) (cid:99)M (cid:98)ρ(cid:124) (cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 φ(s, a) − Es,a∼ν ≤ ε2 (cid:13) (cid:13) (cid:13) (cid:13) For any w1 ∈ BW , we can take w2 = (cid:98)ρ + (cid:99)M (cid:124)w1. Then, (cid:107)w2(cid:107)2 ≤ (cid:107)(cid:98)ρ(cid:107)2 + (cid:107) (cid:99)M (cid:107)2W ≤ (cid:107)ρ(cid:63)(cid:107)2 + (cid:107)M (cid:63)(cid:107)2W ≤ W by our assumption on W . Hence, max w1∈BW ≤ max w1∈BW (cid:124) (cid:13) (cid:13)w 2 φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w min w2∈BW (cid:13) (cid:13)((cid:98)ρ + (cid:99)M (cid:124)w1)(cid:124)φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w (cid:13) (cid:115) (cid:124) 1 φ(s(cid:48), πe)](cid:13) (cid:13)ν (cid:13) (cid:124) 1 φ(s(cid:48), πe)] (cid:13) (cid:13)ν ((cid:98)ρ + (cid:99)M (cid:124)w1)(cid:124)φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w (cid:115) (cid:20)(cid:16) (cid:124) ((cid:98)ρ(cid:124)φ(s, a) − r(s, a))2(cid:105) + Es,a∼ν w 1 ( (cid:99)M φ(s, a) − γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)]) (cid:17)2(cid:21) (cid:17)2(cid:21) (cid:124) 1 φ(s(cid:48), πe)] = max w1∈BW Es,a∼ν (cid:20)(cid:16) (cid:114) Es,a∼ν (cid:104) ≤ max w1∈BW ≤ ε(1 + W ), as desired. E. Proofs for Representation Learning To simplify analysis, assume that the functions in G have bounded (cid:96)2 norm, i.e. ∀g ∈ G, s, a ∈ S × A, (cid:107)g(s, a)(cid:107)2 ≤ γ. This is reasonable, and can always be achieved by clipping without loss of accuracy, since the target for g(s, a) is γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)] and (cid:107)φ(s, a)(cid:107)2 ≤ 1 for any s, a ∈ S × A. E.1. Lemmas Recall that λk(A) denotes the k-th largest eigenvalue of a matrix A, i.e. λ1(A), λn(A) give the largest and smallest eigenvalues respectively. Lemma E.1 (Weyl’s Perturbation Theorem). Let A, B ∈ Cn×n be Hermitian matrices. Then max k |λk(A) − λk(B)| ≤ (cid:107)A − B(cid:107)2 . Proof. Please see (Bhatia, 2013, Corollary III.2.6). Learning Bellman Complete Representations for Ofﬂine Policy Evaluation We extend this to be uniform over all Φ. Lemma E.2 (Uniform spectrum concentration). For any δ ∈ (0, 1), w.p. 1 − δ, sup φ∈Φ,k∈[d] (cid:12) (cid:12) (cid:12)λk(Σ(φ)) − λk((cid:98)Σ(φ)) (cid:12) (cid:12) (cid:12) ≤ 1 √ N (cid:16) (cid:17) 96κ(Φ) + 4d + 4 log1/2(1/δ) Proof. First observe that, sup φ∈Φ sup k (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ sup (cid:12)λk(Σ(φ)) − λk((cid:98)Σ(φ)) φ∈Φ (cid:13) (cid:13) (cid:13)Σ(φ) − (cid:98)Σ(φ) (cid:13) (cid:13) (cid:13)2 x(cid:124)(Σ(φ) − (cid:98)Σ(φ))x (Eν − ED)(x(cid:124)φ(s, a))2 = = sup φ∈Φ,(cid:107)x(cid:107)2≤1 sup φ∈Φ,(cid:107)x(cid:107)2≤1 Now we want to bound the Rademacher complexity of the class F = (cid:8)(s, a) (cid:55)→ (x(cid:124)φ(s, a))2, φ ∈ Φ, (cid:107)x(cid:107)2 ≤ 1(cid:9) First, to bound the envelope, we have (x(cid:124)φ(s, a))2 ≤ 1. To cover, consider any φ ∈ Φ and x ∈ Rd s.t. (cid:107)x(cid:107)2 ≤ 1. Pick (cid:101)φ, (cid:101)x close to φ, x, so that (cid:118) (cid:117) (cid:117) (cid:116) 1 N N (cid:88) (cid:16) i=1 (x(cid:124)φ(si, ai))2 − ((cid:101)x(cid:124) (cid:101)φ(si, ai))2 (cid:17)2 (cid:118) (cid:117) (cid:117) (cid:116) ≤ 2 1 N N (cid:88) (cid:16) i=1 (x(cid:124)φ(si, ai) − (cid:101)x(cid:124) (cid:101)φ(si, ai)) (cid:17)2  (cid:118) (cid:117) (cid:117) (cid:116) ≤ 2  1 N N (cid:88) (cid:16) i=1 x(cid:124)(φ(si, ai) − (cid:101)φ(si, ai)) (cid:17)2 + (cid:118) (cid:117) (cid:117) (cid:116) 1 N N (cid:88) (cid:16) i=1 (x − (cid:101)x)(cid:124) (cid:101)φ(si, ai) (cid:17)2   (cid:16) ≤ 2 dΦ(φ, (cid:101)φ) + (cid:107)x − (cid:101)x(cid:107)2 (cid:17) So it sufﬁces to take dΦ(φ, (cid:101)φ), (cid:107)x − (cid:101)x(cid:107)2 ≤ t/4 to t-cover F in L2(D). Note the t/4-covering number for x in the unit ball is (1 + 8/t)d. Thus, by Dudley’s entropy bound ((5.48) of (Wainwright, 2019)), RN (F) ≤ ≤ 24 √ N 96 √ N (cid:90) 1 log1/2(N (t/4, Φ) · (1 + 8/t)d)dt 0 (cid:16) κ(Φ) + 4 √ (cid:17) d Thus, by Theorem 4.10 of (Wainwright, 2019), w.p. 1 − δ, sup φ∈Φ,(cid:107)x(cid:107)2≤1 (Eν − ED)(x(cid:124)φ(s, a))2 ≤ 2RN (F) + 4 log1/2(1/δ) √ N √ ≤ (cid:16) 1 √ N 96κ(Φ) + 4 d + 4 log1/2(1/δ) (cid:17) We now prove the double sampling lemma, i.e. modiﬁed Bellman Residual Minimization (Chen & Jiang, 2019). This will help deal with the double sampling issue when transitions are stochastic. Recall that G is a function class of functions g : X (cid:55)→ Rd. Let ν be a distribution over X ⊂ Rd and, for any x ∈ X , let P (x) be a distribution over Y ⊂ Rd. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Lemma E.3 (Double Sampling). Suppose x (cid:55)→ Ey∼P (x) [y] ∈ G. Then, Ex∼ν (cid:104)(cid:13) (cid:13)x − Ey∼P (x) [y](cid:13) 2 (cid:13) 2 (cid:105) = Ex∼ν,y∼P (x) (cid:104) (cid:107)x − y(cid:107)2 2 (cid:105) − inf g∈G Ex∼ν,y∼P (x) (cid:104) (cid:107)g(x) − y(cid:107)2 2 (cid:105) Proof. Ex∼ν,y∼P (x) (cid:104) (cid:105) − Ex∼ν (cid:107)x − y(cid:107)2 2 (cid:104) (cid:105) (cid:104)(cid:13) (cid:13)x − Ey∼P (x) [y](cid:13) 2 (cid:13) 2 (cid:105) 2 − 2 (cid:10)x, Ey∼P (x) [y](cid:11) + (cid:13) (cid:107)x(cid:107)2 (cid:16) 2 (cid:13)Ey∼P (x) [y](cid:13) 2 (cid:13) 2 (cid:17)(cid:105) (cid:104) (cid:104) (cid:104) = Ex∼ν = Ex∼ν = Ex∼ν Ey∼P (x) Ey∼P (x) Ey∼P (x) (cid:104) (cid:107)x(cid:107)2 2 − 2 (cid:104)x, y(cid:105) + (cid:107)y(cid:107)2 (cid:105) (cid:13)Ey∼P (x) [y](cid:13) 2 (cid:13) 2 (cid:105)(cid:105) − (cid:13) (cid:107)y(cid:107)2 2 (cid:104)(cid:13) (cid:13)y − Ey∼P (x) [y](cid:13) 2 (cid:13) 2 − (cid:105) where the last step uses the fact that (cid:13) observe that, assuming g(cid:63)(x) (cid:55)→ Ey∼P (x) [y] ∈ G, we have that it is the minimizer of, (cid:13)Ey∼P (x) [y](cid:13) 2 2 = Ey∼P (x) (cid:13) (cid:2)(cid:10)y, Ey∼P (x) [y](cid:11)(cid:3), and completing the square. Now, g(cid:63) ∈ arg min g∈G Ex∼ν,y∼P (x) (cid:104) (cid:107)g(x) − y(cid:107)2 2 (cid:105) (11) which completes the proof. E.2. Concentration lemmas For any φ, deﬁne the optimal ρ, M, g for our losses as follows: ρφ ∈ arg min (ρ,_)∈Θ Eν (cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3) Mφ ∈ arg min (_,M )∈Θ Eν◦P (cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), πe)(cid:107)2 2 (cid:3) gφ ∈ arg min g∈G Eν◦P (cid:2)(cid:107)g(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:3) Similarly, deﬁne (cid:98)ρφ, (cid:99)Mφ, (cid:98)gφ to be minimizers of the above losses when expectation is taken over the empirical distribution D, instead of the population distribution ν. Observe that the unconstrained minimization yields a closed form solution for gφ as gφ(s, a) = γEs(cid:48)∼P (s,a) [φ(s(cid:48), π)] – Assumption 4.3 posits that G is rich enough to capture this. The key property of our squared losses is that the second moment can be upper bounded by the expectation, which allows us to invoke the second part of the above Lemma B.1. We now combine this with covering to get uniform convergence results. Lemma E.4. For any δ ∈ (0, 1), w.p. at least 1 − δ, for any φ ∈ Φ, ρ ∈ BW and M ∈ Rd×d with (cid:107)M (cid:107)2 ≤ 1, we have (cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3) − Eν (cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 (cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3)(cid:12) 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 1 2 Eν (cid:12) ≤ (cid:3) − Eν◦P (cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3) + ερ, (cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:3)(cid:12) (cid:12) Eν◦P (cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:3) + εM , (cid:12) (cid:12)ED (cid:12) (cid:12)ED 1 2 ≤ and, assuming realizability (Assumption 4.3), for every g ∈ G, we have (cid:12) (cid:12)ED 1 2 ≤ (cid:2)(cid:107)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) − Eν (cid:2)(cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 (cid:3)(cid:12) (cid:12) Eν (cid:2)(cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 (cid:3) + εg, where ερ, εM , ρg are deﬁned below. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Finite function classes Assuming Φ and G are ﬁnite, we have ερ ≤ 6d(1 + W )2 log(4W |Φ| N/δ) N εM ≤ 32d2 log(4 |Φ| N/δ) N εg ≤ 20γ2 log(2 |G| /δ) N . Proof for ερ. For a ﬁxed φ ∈ Φ, ρ ∈ BW , apply Lemma B.1 to Xi = (ρ(cid:124)φ(si, ai) − r(si, ai))2. The envelope is (cid:3) ≤ (1 + W )2E [Xi]. So, the error from the lemma is |Xi| ≤ (1 + W )2 and the second moment is bounded E (cid:2)X 2 (cid:12)(ρ(cid:124)φ(s, a) − r(s, a))2 − ((cid:101)ρ(cid:124)φ(s, a) − r(s, a))2(cid:12) 2(1+W )2 log(2/δ) (cid:12) = N |(ρ − (cid:101)ρ)(cid:124)φ(s, a)((ρ + (cid:101)ρ)(cid:124)φ(s, a) + 2r(s, a))| ≤ 2(1 + W )(cid:107)(cid:101)ρ − ρ(cid:107)2, we consider a 1 N -net of BW which requires (1 + 2W N )d points. The error from this ε-net approximation is at most 4(1+W ) . Now union bound over an ε-net of BW . Since (cid:12) . Finally, union bound over Φ. i N Proof for εM . For a ﬁxed φ ∈ Φ and M ∈ Rd×d s.t. (cid:107)M (cid:107)2 < 1, apply Lemma B.1 to Xi = (cid:107)a(cid:107)2 a = M φ(si, ai)−γφ(s(cid:48) Further, observe that (cid:107)a(cid:107)2 E (cid:2)(cid:107)a + b(cid:107)2 2 − (cid:107)b(cid:107)2 2 where 2 ≤ (1+γ)2+(2γ)2 ≤ 8. (cid:3) ≤ (cid:3) ≤ 16E [Xi], where we used Lemma E.3 to give us 2 = (cid:104)a + b, a − b(cid:105) ≤ (cid:107)a + b(cid:107)2(cid:107)a − b(cid:107)2. So, the second moment is bounded E (cid:2)X 2 (cid:3) ≤ (1 + 3γ)2E (cid:2)(cid:107)M φ(si, ai) − γgφ(si, ai)(cid:107)2 i, π). The envelope is |Xi| = |Xi| ≤ (cid:107)a(cid:107)2 i, π), b = gφ(si, ai)−γφ(s(cid:48) 2 − (cid:107)b(cid:107)2 2+(cid:107)b(cid:107)2 2(cid:107)a − b(cid:107)2 2 2 i E (cid:2)(cid:107)M φ(si, ai) − γgφ(si, ai)(cid:107)2 2 (cid:3) = E (cid:2)(cid:107)M φ(si, ai) − γφ(s(cid:48) i, π)(cid:107)2 2 − (cid:107)gφ(si, ai) − γφ(s(cid:48) i, π)(cid:107)2 2 (cid:3) . So, the error from the lemma is 24 log(2/δ) N . Now union bound over an ε-net of (cid:8)M ∈ Rd×d : (cid:107)M (cid:107)2 ≤ 1(cid:9). Observe that (cid:12) (cid:0)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 (cid:12) (cid:12) 2 (cid:13) (cid:13)M φ(si, ai) + (cid:102)M φ(si, ai) − 2γφ(s(cid:48) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13)(M − (cid:102)M )φ(si, ai) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)(M − (cid:102)M ) (cid:13)2 (cid:13) (cid:13) (cid:13)(M − (cid:102)M ) · (2 + 2γ) ≤ 4 (cid:13) (cid:13) (cid:13)2 (cid:1) − ≤ ≤ (cid:16) . (cid:13) (cid:13) i, π) (cid:13)2 (cid:107) (cid:102)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:17)(cid:12) (cid:12) (cid:12) N -net (under (cid:107)(cid:107)F ) for {M ∈ Rd×d : (cid:107)M (cid:107)F ≤ Consider a 1 like the (cid:96)2 for a d2-dimensional vector. This is a 1 (cid:107)M (cid:107)2 ≤ (cid:107)M (cid:107)F ≤ √ d(cid:107)M (cid:107)2. The error from this ε-net approximation is at most 8 points since it is N -net (under (cid:107)(cid:107)2) for the subset (cid:8)M ∈ Rd×d : (cid:107)M (cid:107)2 ≤ 1(cid:9) since d}, which requires (1 + 2N N . Finally, union bound over Φ. d)d2 √ √ Proof for εg. For a ﬁxed g ∈ G, apply Lemma B.1 to Xi = (cid:107)g(si, ai) − γφ(cid:63)(s(cid:48) excess regression loss. Under realizability Assumption 4.3, we have E [Xi] = (cid:107)g − gφ(cid:63) (cid:107)2 i, π)(cid:107)2 2, since 2 − (cid:107)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2, the E [Xi] = E = E (cid:104) (cid:107)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 (cid:104) (cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 − (cid:107)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:105) 2 + 2(cid:104)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π), g(s, a) − gφ(cid:63) (s, a)(cid:105) (cid:105) By deﬁnition of gφ(cid:63) , we have Es(cid:48)∼P (s,a) [gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)] = 0, so, = E (cid:104) (cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 (cid:105) is envelope |Xi| The E (cid:2)(cid:107)g(s, a) + gφ(cid:63) (s, a) − 2γEa(cid:48)∼π(s(cid:48)) [φ(cid:63)(s(cid:48), a(cid:48))] (cid:107)2 lemma is 20γ2 log(2/δ) . Now union bound over G. ≤ (2γ)2 N the and 2(cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 second moment ≤ bounded (cid:3) ≤ (4γ)2E [Xi]. So the error term from the is i E (cid:2)X 2 (cid:3) Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Inﬁnite function classes When Φ and G are inﬁnite, we need to assume some metric entropy conditions. Then in the ﬁnal step of the ﬁnite-class proofs above, we union bound on a well-chosen ε-net and collect an additional approximation error which is on the order of O(1/N ). Assumption E.5. For F ∈ {Φ, G}, we assume there exists p ∈ R++ such that N (t, F) (cid:46) t−p, where the net is under the following distances, dΦ(φ, (cid:101)φ) =∆ ED + ED (cid:105) + Eν (cid:105) (cid:13) (cid:104)(cid:13) (cid:13) (cid:13) (cid:13)φ(s, a) − (cid:101)φ(s, a) (cid:13)2 (cid:13) (cid:104)(cid:13) + Es,a∼D,s(cid:48)∼P (s,a) (cid:13)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π) (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:104)(cid:13) (cid:13)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π) (cid:13) (cid:13) (cid:13)2 (cid:114) (cid:105) (cid:105) (cid:104)(cid:13) (cid:13) (cid:13)φ(s, a) − (cid:101)φ(s, a) (cid:104)(cid:13) (cid:13)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:107)g(si, ai) − (cid:101)g(si, ai)(cid:107)2 2 (cid:105) + (cid:104) Eν (cid:107)g(s, a) − (cid:101)g(s, a)(cid:107)2 2 (cid:105) (cid:105) (cid:13) (cid:13) (cid:13)2 + Es,a∼ν,s(cid:48)∼P (s,a) (cid:114) dG(g, (cid:101)g) =∆ (cid:104) ED Note that this assumption is automatically satisﬁed for any p > 0 by VC classes (van der Vaart & Wellner, 1996, Theorem 2.6.4). Under this assumption, we have N ερ (cid:46) d(1 + W )2(1 + p) log(W N/δ) εM (cid:46) d2(1 + p) log(N/δ) N εg (cid:46) γ2p log(N/δ) . N Proof for ερ. From before, we showed for a ﬁxed φ ∈ Φ, we have ερ (cid:46) d(1+W )2 log(W N/δ) . Observe that (cid:12) (cid:12) (cid:101)φ(s, a) − r(s, a))2(cid:12) (cid:12) (cid:12)ρ(cid:124)(φ(s, a) − (cid:101)φ(s, a))(ρ(cid:124)(φ(s, a) + (cid:101)φ(s, a)) + 2r(s, a)) (cid:12)(ρ(cid:124)φ(s, a) − r(s, a))2 − (ρ(cid:124) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ 2W (1+ (cid:12) = W )(cid:107)φ(s, a) − (cid:101)φ(s, a)(cid:107)2, and so the difference of the loss with φ and the loss with (cid:101)φ is bounded by 2W (1 + W )dΦ(φ, (cid:101)φ). Now union bound over a 1 N -net, which requires O(N p) points by Assumption E.5. The approximation error using the net is 2W (1+W ) N N . Proof for εM . From before, we showed for a ﬁxed φ ∈ Φ, we have εM (cid:46) d2 log(N/δ) N . Observe that (cid:12) (cid:0)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 (cid:12) (cid:12) ≤ (cid:107)M (φ(s, a) − (cid:101)φ(s, a)) − γ(φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π))(cid:107)(cid:107)M (φ(s, a) + (cid:101)φ(s, a)) − γ(φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π))(cid:107) + (cid:107)gφ(s, a) − g 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:107)M (cid:101)φ(s, a) − γ (cid:101)φ(s(cid:48), π)(cid:107)2 (cid:101)φ(s, a) − γ(φ(s(cid:48), π) + (cid:101)φ(s(cid:48), π))(cid:107) 2 − (cid:107)g (cid:1) − (cid:16) (cid:101)φ(s, a) − γ (cid:101)φ(s(cid:48), π)(cid:107)2 2 (cid:17)(cid:12) (cid:12) (cid:12) (cid:17) (cid:107)φ(s, a) − (cid:101)φ(s, a)(cid:107) + γ(cid:107)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π)(cid:107) (cid:101)φ(s, a) − γ(φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π))(cid:107)(cid:107)gφ(s, a) + g · (2 + 2γ) (cid:17) (cid:101)φ(s, a)(cid:107) + γ(cid:107)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π)(cid:107) (cid:107)gφ(s, a) − g · (2 + 2γ) (cid:16) (cid:16) ≤ + Using closed form solution for gφ, ≤ 16dΦ(φ, (cid:101)φ). Now union bound over a 1 at most 16 N . N -net, which requires O(N p) points by Assumption E.5. The approximation error using the net is Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Proof for εg. From before, we showed for a ﬁxed g ∈ G, we have εg (cid:46) γ2 log(1/δ) N . Observe that (cid:12)(cid:107)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 − (cid:107)(cid:101)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2(cid:12) (cid:12) (cid:12) ≤ (cid:107)g(s, a) − (cid:101)g(s, a)(cid:107)(cid:107)g(s, a) + (cid:101)g(s, a) − 2γφ(cid:63)(s(cid:48), π)(cid:107) ≤ (2 + 2γ)dG(g, (cid:101)g). Now union bound over a 1 at most 4 N . N -net, which requires O(N p) points by Assumption E.5. The approximation error using the net is E.3. Main Results Lemma E.6. Suppose φ(cid:63) ∈ Φ is Linear BC. Suppose Assumption 4.3 if transitions are stochastic. Moreover, suppose φ(cid:63) is feasible in the bilevel optimization (and so (cid:98)Llbc( (cid:98)φ) ≤ (cid:98)Llbc(φ(cid:63))). Then, w.p. 1 − 5δ, Llbc( (cid:98)φ) ≤ 24d(1 + W )2 log(4W |Φ|N/δ) + 128d2 log(4|Φ|N/δ) + 40γ2 log(2|G|/δ) N Proof. Llbc( (cid:98)φ) (cid:104) = Eν Since ρ ≤ Eν + Eν◦P (cid:124) (ρ (cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105) (cid:98)φ are minimizers under ν, (cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105) ((cid:98)ρ (cid:98)φ, M (cid:104) (cid:124) + Eν◦P (cid:104) (cid:107)M (cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107) (cid:99)M (cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:105) (cid:105) − Eν◦P − Eν◦P By the ρ, M parts of Lemma E.4, ≤ 2ED (cid:104) (cid:124) (cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105) ((cid:98)ρ + 2ED Since (cid:98)g (cid:98)φ minimizes under D, ≤ 2ED (cid:104) (cid:124) (cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105) ((cid:98)ρ + 2ED By optimality of (cid:98)φ under (cid:98)Llbc, (cid:104) (cid:107) (cid:99)M (cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107) (cid:99)M (cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:105) (cid:105) − 2ED − 2ED (cid:104) (cid:107)g (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107)g (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107)g (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107)(cid:98)g (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:105) (cid:105) (cid:105) (cid:105) + 2ερ + 2εM + 2ερ + 2εM ≤ 2ED (cid:104) (cid:124) φ(cid:63) φ(cid:63)(s, a) − r(s, a))2(cid:105) ((cid:98)ρ + 2ED (cid:104) (cid:107) (cid:99)Mφ(cid:63) φ(cid:63)(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:105) − 2ED (cid:2)(cid:107)(cid:98)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) + 2ερ + 2εM By − 1 2 the G part (cid:2)(cid:107)gφ(cid:63) − (cid:98)gφ(cid:63) (cid:107)2 2 Eν have ED of Lemma E.4, we (cid:3) + εg ≤ εg. Then, using the optimality of (cid:98)ρ and (cid:99)M under D, (cid:3) − 2ED (cid:2)(cid:107)Mφ(cid:63) φ(cid:63)(s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 + 2ED (cid:2)(cid:107)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 ≤ 2ED (cid:104) (ρ (cid:124) φ(cid:63) φ(cid:63)(s, a) − r(s, a))2(cid:105) (cid:2)(cid:107)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) 2 − (cid:107)(cid:98)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) ≤ + 2ερ + 2εM + 2εg By the ρ, M parts of Lemma E.4, ≤ 3Eν (cid:104) (ρ (cid:124) φ(cid:63) φ(cid:63)(s, a) − r(s, a))2(cid:105) + 3Eν◦P (cid:2)(cid:107)Mφ(cid:63) φ(cid:63)(s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) − 3Eν (cid:2)(cid:107)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) + 4ερ + 4εM + 2εg Learning Bellman Complete Representations for Ofﬂine Policy Evaluation By Assumption 4.3 and Lemma E.3, = 3Llbc(φ(cid:63)) + 4ερ + 4εM + 2εg By assumption that φ(cid:63) is Linear BC and Proposition 4.2, = 4ερ + 4εM + 2εg. We now prove Theorem 4.4, in the general stochastic case. The deterministic transitions case is subsumed by ignoring the minimization over G, i.e. setting the complexity term of G to zero. Theorem 4.4. Assume Assumption 4.1 (and Assumption 4.3 if 96 log1/2(|Φ|)+4 . If N ≥ C 2 d+4 log1/2(1/δ) √ 2 , then for any δ ∈ (0, 1), w.p. at least 1 − δ, we have β/4 the system is stochastic). Let C2 := 1. (cid:98)φ satisﬁes (cid:98)ε-approximate Linear BC with (cid:98)ε ≤ 13d(1 + W )2 log1/2(4W |Φ|N/δ) √ N + 7γ(1 + W ) log1/2(2|G|/δ) √ N , 2. λmin(Σ( (cid:98)φ)) ≥ β/4. If transitions are deterministic, treat log(|G|) = 0. Proof of Theorem 4.4. First, by our assumption that w.p. at least 1 − δ, we have supφ∈Φ important consequences: (cid:12) (cid:12) (cid:12)λmin(Σ(φ)) − λmin((cid:98)Σ(φ)) N ≥ 4(96κ(Φ) + 4 d + 4 log1/2(1/δ))/β, Lemma E.2 implies that (cid:12) (cid:12) (cid:12) ≤ β/4. Under this high probability event, we have two √ √ 1. φ(cid:63) is feasible in Equation (6), since λmin((cid:98)Σ(φ(cid:63))) ≥ λmin(Σ(φ(cid:63))) − β/4 ≥ β(1 − 1/4) ≥ β/2. In particular, this means (cid:98)Llbc( (cid:98)φ) ≤ (cid:98)Llbc(φ(cid:63)). 2. The covariance of (cid:98)φ has lower-bounded eigenvalues, since λmin(Σ( (cid:98)φ)) ≥ λmin((cid:98)Σ( (cid:98)φ)) − β/4 ≥ β(1/2 − 1/4) ≥ β/4. Now, apply Lemma E.6 to bound Llbc( (cid:98)φ), so w.p. 1 − 5δ, Llbc( (cid:98)φ) ≤ 24d(1 + W )2 log(4W |Φ|N/δ) + 128d2 log(4|Φ|N/δ) + 40γ2 log(2|G|/δ) N By Lemma D.1, we have that (cid:98)φ is (cid:98)ε-approximately Linear BC, with parameter (cid:114) (cid:98)ε ≤ (1 + W ) · 24d(1 + W )2 log(4W |Φ|N/δ) + 128d2 log(4|Φ|N/δ) + 40γ2 log(2|G|/δ) N 7γ(1 + W ) log1/2(2|G|/δ) √ N 13d(1 + W )2 log1/2(4W |Φ|N/δ) √ N + ≤ Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Finally, we remark that the (cid:99)W for (cid:98)φ (in the approximately Linear BC case) is upper bounded by a polynomial in W (cid:63) in the assumed exact Linear BC of φ(cid:63). Consider our assumption that φ(cid:63) is exactly Linear BC with W (cid:63) = W (use (cid:63) to highlight that it is the W in the assumption, which we now show matches the W in the result). Then, by Proposition 4.2, ∃M (cid:63) with (cid:107)M (cid:63)(cid:107)2 ≤ W (cid:63)2 . Hence, it sufﬁces to minimize over this smaller ball for (cid:99)M , so that (cid:107) (cid:99)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2. Now, take the smallest possible W in Lemma D.1, so that 1 − (cid:107)ρ(cid:63)(cid:107)2 (cid:113) 2 (cid:99)W = ≤ = ≤ (cid:107)ρ(cid:63)(cid:107)2 1 − (cid:107)M (cid:63)(cid:107)2 (cid:107)ρ(cid:63)(cid:107)2 (cid:113) 1 − (cid:107)ρ(cid:63)(cid:107)2 2 W (cid:63)2 (cid:113) 1 − (cid:107)ρ(cid:63)(cid:107)2 (1 + 1 − (cid:107)ρ(cid:63)(cid:107)2 W (cid:63)2 ) 2 (cid:107)ρ(cid:63)(cid:107)2 2 W (cid:63)2 2W (cid:63)2 (cid:107)ρ(cid:63)(cid:107)2 , which is a polynomial in W (cid:63). Our end-to-end result is deduced by chaining our LSPE theorem and the above theorem together. Theorem 4.5. Under Assumption 4.1 (and Assumption 4.3 if P (s, a) is stochastic). Let C2, (cid:98)ε be as deﬁned in Theorem 4.4. If N ≥ C 2 2 , then we have for any δ ∈ (0, 1/2), w.p. at least 1 − 2δ, for all distributions p0, (cid:12) (cid:12)V πe (cid:12) − Es∼p0 (cid:104) (cid:98)fK(s, πe) p0 960β−1/2(1 + W )d log(N )(cid:112)log(10/δ) (cid:105)(cid:12) (cid:12) (cid:12) ≤ γK/2 1 − γ + √ (1 − γ)2 N . + (cid:114)(cid:13) (cid:13) (cid:13) 4 ddπe p0 dν (cid:13) (cid:13) (cid:13)∞ (1 − γ)2 (cid:98)ε Proof of Theorem 4.5. We ﬁrst apply Theorem 4.4 to see that (cid:98)φ satisﬁes the two properties needed for LSPE. It is indeed approximately Linear BC, with (cid:98)ε speciﬁed in the theorem, and also has coverage (i.e. λmin(Σ( (cid:98)φ)) ≥ β/4). Using these two facts, and on a separate independent dataset D2 (needs to be a separate dataset since (cid:98)φ is data-dependent), we run LSPE and directly apply Theorem 3.3 for the result. F. Implementation Details Here we detail all environment speciﬁcations and hyperparameters used in the main text. F.1. Dataset Details Using the publicly released implementation for DrQ-v2, we trained high quality target policies and saved checkpoints for ofﬂine behavior datasets. We refer the readers to Yarats et al. (2021a) for exact hyperparameters. F.2. Environment Details Following the standards used by DrQ-v2 (Yarats et al., 2021a), all environments have a maximum horizon length of 500 timesteps. This is achieved by the behavior/target policy having an action repeat of 2 frames. Furthermore, each state is 3 stacked frames that are each 84 × 84 dimensional RGB images (thus 9 × 84 × 84). Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Task Target Performance Behavior Performance Finger Turn Hard Cheetah Run Quadruped Walk Humanoid Stand 927 758 873 827 226 (24%) 192 (25%) 236 (27%) 277 (33%) Table 1. Performance for target and behavior policies used to collect evaluation and ofﬂine datasets respectively. Task Action Space Dimension Task Traits Reward Type Finger Turn Hard Cheetah Run Quadruped Walk Humanoid Stand 2 6 12 21 turn locomotion locomotion stand sparse dense dense dense Table 2. Task descriptions, action space dimension, and reward type for each tested environment. F.3. Representation Architecture and Hyperparameter Details We adopt the same network architecture as DrQ-v2’s critic, ﬁrst introduced in SAC-AE (Yarats et al., 2021b). More speciﬁcally, to process pixel input, we have a 5 layer ConvNet with 3 × 3 kernels and 32 channels with ReLU activations. The ﬁrst convolutional layer has a stride of 2 while the rest has stride 1. The output is fed through a single fully connected layer normalized by LayerNorm. Finally, there is a tanh nonlinearity on the outputted 50 dimensional state-representation. The action is then concatenated with this output and fed into a 4-layer MLP all with ReLU activations. Hyperparameter Value Feature Dimension Weight Initialization Optimizer Learning Rate Batch Size Training Epochs τ (target) λDesign 512 orthogonal init. Adam 1 × 10−5 2048 200 0.005 5 × 10−6 Table 3. Hyperparameters used for BCRL Learning Bellman Complete Representations for Ofﬂine Policy Evaluation F.4. Benchmarks and Metrics Modiﬁcations to CURL: Originally, CURL only does contrastive learning between the image states with data augmentation. For OPE, apply the same CURL objective to the state-action feature detailed in the previous section. Note we also train CURL with the same random cropping image augmentations presented by the authors. Finally, since we are not interleaving the representation learning with SAC, we do not have a Q prediction head. Modiﬁcations to SPR: We use the same image encoder as our features for SPR. The main difference is in the architecture of the projection layers where we implement as 3-layer mlp with ReLU activations. Note that these are additional parameters that neither CURL nor BCRL require. Finally, similarly to CURL, we do not have an additional Q-prediction head. Spearman Ranking Correlation Metric: This rank correlation measures the correlation between the ordinal rankings of the value estimates and the ground truth returns. As deﬁned in Fu et al. (2021), we have for policies 1, 2, . . . , N , true returns V1:N , and estimated returns ˆV1:N : Ranking Correlation = (cid:17) (cid:16) V1:N , ˆV1:N Cov σ (V1:N ) σ( ˆV1:N )",1.0
"A Comparison of Source Distribution and Result Overlap in Web Search
  Engines","[{'href': 'http://arxiv.org/abs/2207.07330v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.07330v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-15 07:58:49,,"2 2 0 2 l u J 8 1 ] R C . s c [ 1 v 8 7 9 8 0 . 7 0 2 2 : v i X r a A Security & Privacy Analysis of US-based Contact Tracing Apps Joydeep Mitra Department of Computer Science Stony Brook University joydeep.mitra@stonybrook.edu July 20, 2022 Abstract With the onset of COVID-19, governments worldwide planned to develop and deploy contact tracing apps to help speed up the contact tracing process. However, experts raised concerns about the long-term privacy and security implications of using these apps. Consequently, several proposals were made to design privacy-preserving contact tracing apps. To this end, Google and Apple developed the Google/Apple Exposure Notiﬁcation (GAEN) framework to help public health authorities develop privacy-preserving contact tracing apps. In the United States, 26 states used the GAEN framework to develop their contact tracing apps. In this paper, we empirically evaluate the US-based apps to determine 1) the privileges these apps have, 2) if the apps comply with their deﬁned privacy policies, and 3) if they contain known vulnerabilities that can be exploited to compromise privacy. The results show that all apps violate their privacy policies and contain several known vulnerabilities. 1 Introduction Contact tracing is one of many methods health authorities use to contain the rapid spread of COVID-19 [1]. However, manual contact tracing to keep track of the growing pandemic has been a challenge for health authorities due to the lack of human resources and the limitations of human memory to remember all possible contacts accurately [2]. Consequently, there have been several eﬀorts to automate the contact tracing process by developing mobile apps that use tracking tech- nology in mobile phones, such as GPS and Bluetooth [3, 4]. In addition, since mobile phones are ubiquitous, they can be used to eﬀectively and quickly identify a person’s contacts when they test positive for COVID. 1.1 Background 1.1.1 How do Contact Tracing Apps Work? Contact tracing apps work by estimating if two mobile phones, X and Y, are close to each other based on a metric deﬁned by local health authorities (e.g., 6 feet). Y will be notiﬁed of a potential infection if X tests positive and conﬁrms the result. An app detects proximity using Bluetooth 1 Low Energy (BLE), Global Positioning System (GPS) technology, or a combination of the two. If two phones are nearby, then the apps exchange encounter messages, which contain, among other things, random identiﬁers and signal strength. Apps may also include location data in the encounter messages. Each app keeps a record of all the encounter messages it has exchanged with other apps. If one of these messages is from a phone whose user has tested positive, then the app analyzes the encounter message to calculate the level of risk based on factors the local health authorities decided. This calculation is performed either locally in the device or by a central server. While contact tracing apps can be useful to help expedite the process of contact tracing, experts have warned that such technologies have long-term consequences for user privacy and security since they can be easily exploited and used in malicious contexts such as mass surveillance [5, 6, 7]. Therefore, contact tracing apps’ design and implementation must be carefully scrutinized before being deployed and adopted. 1.1.2 Contact Tracing App Architectures The need to ensure privacy in contact tracing apps has led to several proposals of app architectures that will best protect user privacy – centralized, decentralized, and hybrid [8, 9]. In a centralized architecture, a central server collects and stores personally identiﬁable information (PII), which is further used to generate random identiﬁers for the encounter messages. Further, the central server is responsible for determining the users that have been potentially exposed. Government health authorities have access to the data stored in the central server and can use it to perform advanced aggregate analysis, which can be useful for understanding trends to help inform mitigation eﬀorts. However, this approach negatively aﬀects user privacy since the central server is assumed to be trusted and has access to vast amounts of personal information, which unauthorized or hostile entities can potentially misuse. On the other hand, a decentralized server has minimal data. Most of the data is maintained locally on the user’s device. Each app periodically communicates with the server to download a set of random identiﬁers that have tested positive. The app then performs a risk analysis of its encounter messages locally to determine if it had come close to a device that tested positive. This architecture helps preserve privacy better than a centralized architecture. However, it limits access to crucial data that could be used to analyze the spread of the pandemic or to understand the eﬀectiveness of the app. The hybrid architecture combines features of the centralized and decentralized architecture, where random identiﬁer generation is left to the local device, whereas the server manages the risk analysis and exposure notiﬁcation. Countries have developed and deployed apps based on a centralized architecture (e.g., Singa- pore’s TraceTogether [10]) and decentralized architecture (e.g., the apps based on the Google/Apple framework). Further, many apps also used location data to enable automated contact tracing (e.g., India’s Arogya Setu [11]). However, there is no consensus among researchers about which approach is the most feasible for eﬀective contact tracing while minimizing privacy risks [12, 13]. 1.1.3 The Google Apple Exposure Notiﬁcation System In May 2020, Google and Apple collaborated to develop the Google Apple Exposure Notiﬁcation (GAEN) framework based on the decentralized architecture to help public health authorities develop contact tracing apps [14, 15]. A GAEN app works by generating a temporary key, which changes periodically. The key is used to encrypt locally stored data and to generate a random identiﬁer. The app embeds the identiﬁer 2 in encounter messages and exchanges them with other apps using BLE. When a user tests positive, a public health oﬃcial uses a veriﬁcation server to send the user a conﬁrmation code. The user can use the conﬁrmation code to upload all their recent keys (e.g., last 14 days) to a key server. Each app downloads keys periodically from the key server and compares them with a set of keys exchanged in the last few days. If there is a match, the app determines the risk of the potential exposure based on a formula pre-determined by public health authorities. 1.2 Motivation In the absence of an oﬃcial national contact tracing app in the United States, individual US states used the GAEN framework to develop their own contact tracing apps. To date, 26 US states have developed a GAEN app. Despite the security and privacy guarantees built into the GAEN framework, people in the US lack conﬁdence in the apps’ ability and intentions to protect their privacy [16]. The Center for Disease Control and Prevention (CDC) in the US recommends that healthcare authorities should conduct a third-party assessment of contact tracing apps and make the results publicly available [9]. However, according to the technology assessment conducted by the United States Government Accountability Oﬃce (GAO) at the behest of the US Congress, most states with a contact tracing apps have not conducted a third-party assessment. Moreover, the states that have conducted an evaluation, have not made the results publicly available [9]. Motivated by the CDC’s recommendations and the lack of assessments of the US-based apps, in this paper we are analyzing the privacy and security of GAEN-based Android apps for contact tracing in the US. Speciﬁcally, we are asking the following research questions: • RQ1: What degree of privilege do the GAEN-based Android apps for contact tracing have? Android apps by default have least privilege. They need to request the system or users for permission to perform privileged operations (e.g., use Bluetooth). The purpose of this question is to understand the permissions used by these apps and their privacy implications. • RQ2: Do GAEN-based Android apps for contact tracing violate their own privacy policies? Contact tracing apps are required to publish a privacy policy to inform users of the app’s capabilities and its data sharing, storage and retention policies. The purpose of this question is to determine if the privacy policy is consistent with the behavior encoded in the app’s source code. RQ3: Do GAEN-based Android apps for contact tracing contain known Android app vulner- abilities? Android apps can have vulnerabilities that can be exploited by malicious apps available locally or remotely to compromise users’ privacy. The purpose of this question is to identify if GAEN-based contact tracing apps have similar vulnerabilities. 2 Methodology In this section, we describe the apps selected, the tools chosen, and the factors we considered to answer our research questions. 2.1 App Selection We selected the oﬃcial contact tracing apps in Android of all US states developed using the exposure notiﬁcation APIs by Google and Apple (GAEN). We considered these apps since the focus of our 3 App State Alabama Arizona California Colorado Connecticut Delaware DC Guam Hawaii Louisiana Maryland Michigan Minnesota Nevada New Jersey New Mexico New York North Carolina North Dakota & Wyoming Pennsylvania Utah Virginia Washington Wisconsin App Name GuideSafe Covid Watch Arizona CA Notify CO Exposure Notiﬁcations COVID Alert CT COVID Alert DE DC CAN Guam Covid Alert AlohaSafe Alert COVID Defense MD COVID Alert MI COVID Alert COVIDaware MN COVID Trace Nevada COVID Alert NJ NM Notify COVID Alert NY SlowCOVIDNC Care19 Alert Version Name 1.10.0 2.1.11 minted1000003 minted141006 1.0.1 Package Name gov.adph.exposurenotiﬁcations gov.azdhs.covidwatch.android gov.ca.covid19.exposurenotiﬁcations minted14020 gov.co.cdphe.exposurenotiﬁcations gov.ct.covid19.exposurenotiﬁcations gov.de.covidtracker gov.dc.covid19.exposurenotiﬁcations minted1100019 org.pathcheck.guam.bt org.alohasafe.alert org.pathcheck.la.bt gov.md.covid19.exposurenotiﬁcations minted151008 gov.michigan.MiCovidExposure org.pathcheck.covidsafepathsBt.mn gov.nv.dhhs.en com.nj.gov.covidalert gov.nm.covid19.exposurenotiﬁcations minted1200004 gov.ny.health.proximity gov.nc.dhhs.exposurenotiﬁcation com.proudcrowd.exposure 1.4 1.17.12 minted1200005 1.0.1 1.0.10 1.0.15 1.9.1 1.1.5 1.6 1.2 Version Code 2764 201011 minted14020 10000032 141006 15 11000192 1947 41 2661 151008 255 3503 12000052 20 12000042 81 205 10 COVID Alert PA UT Exposure Notiﬁcations COVIDWISE WA Notify WI Exposure Notiﬁcation gov.pa.covidtracker gov.ut.covid19.exposurenotiﬁcations minted1100011 1.5 giv.vdh.exposurenotiﬁcation gov.wa.doh.exposurenotiﬁcations minted142004 gov.wi.covid19.exposurenotiﬁcations minted141003 2.0.0 46 11000112 160 142004 141003 Table 1: US-based contact tracing GAEN apps. App size Download (in MB) Date 6.70 3.56 10.07 3.38 9.94 105.55 11.8 64.52 64.64 7.37 10.19 3.19 3.19 12.12 105.62 3.9 105.90 3.1 7.23 Nov 12, 2021 Nov 19, 2021 Oct 15, 2021 Nov 19, 2021 Nov 12, 2021 Dec 3, 2021 Dec 3, 2021 Jan 19, 2022 Nov 5, 2021 Feb 4, 2022 Mar 11, 2022 Oct 15, 2021 Mar 11, 2022 Mar 25, 2022 Nov 5, 2021 Mar 4, 2022 Oct 8, 2021 Nov 5, 2021 Nov 12, 2021 105.68 11.82 9.32 10.36 9.86 Oct 14, 2021 Dec 3, 2021 Mar 24, 2022 Nov 5, 2021 Mar 19, 2022 study is to examine GAEN apps based in the US. We found the apps from the Android Developer’s oﬃcial page [17]. Each US-based app has a link to Google Play. We used the links to download the corresponding APK ﬁle from Google Play in an Emulator running an Android version supported by the app. We transferred the apk ﬁles from the emulator to a computer where we could reverse engineer and statically analyze the apps. All US states did not develop a GAEN app. Further, North Dakota and Wyoming use a single app. The Massachusetts app is built into the device and can only be installed from the device’s settings, not from Google Play. We could not obtain this app since we were using an emulator to install the apps. Consequently, we did not consider the Massachusetts app in our assessment. In total, we ended up with 24 apps. Table 1 lists all the selected apps along with their name, package, version information, size of the APK ﬁles, and when we downloaded them. 2.2 Tool Selection Research in mobile app security and privacy has led to the development of several tools and tech- niques to detect vulnerabilities and malicious behavior [18, 19]. The tools are based on static and dynamic analysis. Most use static analysis to either ﬂag vulnerabilities or malicious behavior or guide subsequent dynamic analysis. Few tools use only dynamic analysis. Prior research eﬀorts studying the eﬃcacy of such tools have observed that for freely available tools, static analysis tools detect more known Android app vulnerabilities than dynamic analysis tools [20]. Furthermore, among the static analysis tools, MobSF [21] detects the most known vulnerabilities. Based on this observation, we used MobSF as the primary tool for analysis. MobSF has a static and dynamic analyzer. The static analyzer statically analyzes the app’s source code to determine if the app uses APIs and features that are known to cause Android app 4 Figure 1: Snippet from a report generated by MobSF. Each issue has a severity, security standards it is associated with, and the source ﬁle/s in which it was detected vulnerabilities (see Figure 1). We failed to run the dynamic analyzer on the contact tracing apps that we had selected. Hence, we considered only the static analyzer. We used Androguard [22] to generate the control ﬂow graph of an app (see Figure 2), which was used to analyze the data ﬂow through the app. We tracked the data ﬂow in the control ﬂow graph to determine if data ﬂowing out of the app is sensitive or if the data being used by the app is potentially malicious. This was necessary to verify the potential data leak and data injection vulnerabilities reported by MobSF’s static analyzer, as it is known to report false positives. Speciﬁcally, we used the following strategies: • We conﬁrmed a potential data leak vulnerability if there was a path in the control ﬂow graph from a pre-identiﬁed sensitive data source node to a target node with shared storage, network, or inter-app communication APIs. Sensitive data sources include APIs used to collect user input (e.g., biometric), read from app’s private ﬁles and communication channels (e.g., Bluetooth), and strings hard coded with personal information (e.g., IP address). • We conﬁrmed a data injection vulnerability if there was a path in the control ﬂow graph from a pre-identiﬁed source node of potentially malicious data such as shared storage, network, or inter-app communication APIs to a target node and the target node was using the data without sanitizing it. Examples of a target node using potentially malicious data after sani- tization include an exported broadcast receiver that uses input data only if it was sent via an 5 intent-ﬁlter with a system-deﬁned action or a function in a target node that uses input data from a trusted remote server. Figure 2: Snippet of the list of edges in a control ﬂow graph generated by the AndroGuard tool for the California app. The source and target columns indicate the nodes in the graph. Each row is a directed edge connecting the source node to the target node. Each node has an ID and an API call contained in the source code. 2.3 Policy Analysis A focus of our study is to determine if an app’s source code is consistent with the app’s privacy policy. To this end, we downloaded each app’s privacy policy and examined them. We identiﬁed the features that the privacy policy of an app claims the app does not use (e.g., does not collect location). All selected apps in their privacy policies claim the following: 1. does not collect, store, or transmit any personally identiﬁable data. 2. stores exposure data (e.g., random IDs and exposure date) locally in the users’ device. 3. prevents unauthorized access to locally stored data. 4. encrypts locally stored data. 5. communicates with trusted servers through encrypted networks in the United States. We looked for these features in the apps using MobSF and Androguard. If at least one such feature was found in the app, then we deemed that the app violates its own privacy policy. For example, using MobSF, we determined instances in an app’s source code where data was being stored in external storage. We then used the app’s control ﬂow graph (generated by Androguard) to determine the source of the data stored in external storage and whether the source is sensitive. If sensitive data was being stored in external storage, then we deemed it as a violation of the app’s privacy policy due to bullet three listed above. 6 2.4 Known Vulnerabilities Analysis Android apps have vulnerabilities, which malicious apps exploit to cause harm to the user [23, 24, 25, 26]. Therefore, it is necessary to ensure such vulnerabilities do not occur in apps, especially contact tracing apps, which deal with sensitive personal information and can perform privileged operations on the phone. Hence, we analyzed these apps for known Android app vulnerabilities. We used MobSF, and the Ghera repository [25] for our analysis. MobSF provides a list of potential vulnerabilities in its static analysis report. We investigated each of them to determine their veracity as MobSF is known to report false positives. Moreover, MobSF does not detect all known vulnerabilities. Therefore, we used Ghera, a repository of 60 known vulnerability benchmarks, to further guide our analysis. Each benchmark in Ghera is well-documented and contains only the features related to the vulnerability captured in the benchmark. We statically analyzed the 24 apps in our set using MobSF to determine the features/APIs used in them. We then considered only those features also used in the Ghera benchmarks. We investigated each such feature to determine if it resulted in a potentially exploitable vulnerability, that is, it could be exploited by a malicious app on the device or remotely. For example, an app can give unrestricted access to another app using the pending intent feature in Android. If MobSF found an app with a component using pending intent, we investigated the component to conﬁrm if it performed privileged operations, which, if true, could result in a potential privilege escalation attack. 3 Results In this section, we report the ﬁndings of our study in terms of the permissions requested and used by the apps, their potential privacy violations, and the potential vulnerabilities in them that can be exploited to cause harm to the user. 3.1 RQ1: What degree of privilege do the GAEN-based Android apps for contact tracing have? A total of eight permissions are used across all 24 contact tracing apps. An app uses approximately seven permissions on average. The least number of permissions used by an app is six, and the highest is eight. These statistics are higher than the average permissions used by apps in general, which is ﬁve [27], with nearly 100k of one million apps using zero permissions as of June 2014. Furthermore, ﬁve of the eight permissions are used by all 24 apps, which suggests that only ﬁve of them are necessary for contact tracing. The other three permissions are most likely extraneous. Therefore, the GAEN apps in Android are over-privileged, which is concerning since these apps have access to vast amounts of personal data. Over-privileged apps increase the risk of being exploited since they expose a larger attack surface due to having more privileges than needed [28]. Therefore, developers of these apps should carefully consider the required permissions and ensure that only the necessary ones are used by their apps. All permissions, except two, are normal permissions [29], that is, granted by Android when the app is installed. Therefore, the apps do not need to ask the user for permission at runtime; they always have them. In the context of contact tracing, where many users install these apps to help prevent the spread of the pandemic, this permission model compromises privacy by not giving users control of granting or denying permission to the apps. Furthermore, it assumes that the apps are benign and that users should trust them during installation. This assumption hinders an app’s 7 Permissions Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y N N N N Y N Y Y Y N N Y N Y N Y N N Y N N N N Y N Y Y Y N N Y N Y N N N N Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 # Permissions per app App 8 Y Alabama 8 Y Arizona 6 Y California 6 Y Colorado 6 Y Connecticut 7 Y Delaware 6 Y DC 8 Y Guam 8 Y Hawaii 8 Y Louisiana 6 Y Maryland Michigan 6 Y 8 Y Minnesota 8 Y Nevada 8 Y New Jersey 8 Y New Mexico New York 7 Y 6 North Carolina Y North Dakota 8 Y & Wyoming Penn Utah Virginia Washington Wisconsin # Apps per permission N N N N N N N N N N N N N Y N Y N N Y N Y N N N N N N N N N N N N N N N N N N Y N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N Y N Y N N Y Y Y Y Y Y 24 Y Y Y Y Y 24 Y Y Y Y Y 24 Y Y Y Y Y 24 Y Y Y Y Y 24 Y Y Y Y Y 23 Y N N N N 9 N N N N N 2 N N N N N 2 N N N N N 7 N N N N N 2 N N N N N 2 7 6 6 6 6 Table 2: Permissions used by the US-based GAEN apps as declared in their manifest ﬁle. The permissions names are encoded as PN due to lack of space. The exact permission names are listed in Table 3 Permission Code Permission Name P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 android.permission.INTERNET android.permission.VIBRATE android.permission.RECEIVE BOOT COMPLETED android.permission.BLUETOOTH android.permission.ACCESS NETWORK STATE android.permission.ACCESS WIFI STATE android.permission.WAKE LOCK android.permission.FOREGROUND SERVICE com.google.android.c2dm.permission.RECEIVE com.google.android.ﬁnsky.permission.BIND GET INSTALL REFERRER SERVICE android.permission.USE BIOMETRIC android.permission.USE FINGERPRINT Table 3: Name of permissions used in all US-based GAEN apps. 8 adoption in an environment where the role of government-deployed contact tracing apps is being seen with suspicion [30]. However, app developers cannot resolve this issue as Android, not the apps, deﬁne these permissions. Therefore, platform developers should either consider changing the way these permissions are granted or deﬁne custom permissions that will be used in the context of contact tracing. Google-based third-party analytics libraries, not the core Android system, deﬁne two permis- sions. Only two of the 24 considered apps use these permissions. Therefore, this raises the question if these permissions are necessary for contact tracing. Further, such apps are designed to have the necessary permissions to access privileged operations in the device (e.g., Bluetooth). In this context, should contact tracing apps further increase security and privacy risks by using third-party libraries that are not directly related to the task of contact tracing? The USE FINGERPRINT permission is deprecated [31]. Two of 24 apps use it. While using deprecated permissions is not recommended, this is not a major concern as both the apps use the permission in conjunction with the USE BIOMETRIC permission, which is the recommended permission to use instead of USE FINGERPRINT. Nevertheless, apps should not use deprecated permissions since they may have unknown and unexpected security and privacy implications. Two of the 24 apps use biometric permissions to access the device capabilities to use and collect biometric information. While this is not a violation of privacy by itself, it increases the risk of exposing private sensitive user info to unauthorized entities. Furthermore, since a majority of the GAEN apps are not using biometric permissions, it raises questions about the necessity of using such capabilities to collect and transmit biometric-related data. The ACCESS WIFI STATE permission is used by 17 of the 24 selected apps. This suggests that not all apps need this permission for contact tracing. Apps use this permission when they connect to a remote server through WiFi. Using WiFi is not always secure since WiFi networks may not be protected and may be susceptible to Man-In-The-Middle attacks. Therefore, apps that collect and transmit sensitive information over the internet should avoid WiFi communication. The VIBRATE permission is used by nine of 24 apps to control the device’s vibration. This permission is not necessarily benign. If used incorrectly or maliciously, it may damage a user’s phone [27]. Therefore, it is best to avoid using such permissions if not absolutely necessary. Since a majority of the selected apps do not use the VIBRATE permission, this permission is not likely necessary for GAEN apps. Only one of the 24 apps declares a query element in its manifest ﬁle to indicate the list of apps it can communicate with. Speciﬁcally, the Arizona app declares that the query elements android.intent.action.DIAL and android.intent.action.SEND in its manifest ﬁles. This implies that the Arizona app has capabilities to place phone calls and share data with any app with the SEND intent. In the context of contact tracing, these capabilities are unnecessary and pose additional risk to the security and privacy of the app’s users. 3.2 RQ2: Do GAEN-based Android apps for contact tracing violate their own privacy policies? All US-based GAEN apps violate their privacy policy since their behavior is inconsistent with at least one of the claims in their policy. This is concerning since contact tracing apps collect and store vast amounts of private data, which can be potentially misused. Therefore, they should take additional care to guarantee their users’ privacy or at least be consistent with their own policies. The GAEN apps are designed not to collect, store, or track location. However, 20 of the 24 9 GAEN apps collect users’ locations despite claiming otherwise in their privacy policy. On further analysis, we discovered that these apps are not explicitly collecting location. However, they use a library called TwilightManager that collects user location to determine the local time. Apps that have conﬁgured dark themes automatically import and use this library. Therefore, these apps violate their privacy policy due to using a library that collects location. Consequently, this raises the question if app developers are aware of the privacy implications of the libraries they used in their apps. Vetting the libraries before using them is especially crucial for contact tracing apps, which malicious actors can potentially misuse to compromise user privacy. The Nevada and New Mexico apps collect Biometrics for authentication or to encrypt locally stored data. However, the privacy policy of these apps does not explicitly state that they col- lect biometric information. Moreover, they mention that the apps do not collect any personally identiﬁable information. Hence, we deem these apps as violating their own privacy policy. All apps claim in their privacy policy that they store exposure-related data in local storage in a way that prevents unauthorized access. However, nine of the 24 apps store their data in external storage. Any app installed on the device (including malicious apps) can access this data if they have the necessary permission to access external storage. Therefore, all other apps can potentially access the exposure-related data stored by these nine apps. Consequently, this leads to a violation of privacy as deﬁned in the apps’ privacy policy. This issue could have been addressed by using the app’s internal storage instead of the external storage because, in Android, the internal storage of an app can only be accessed by the app. Moreover, Android recommends that an app’s data should be stored in internal storage unless it needs to be shared with other apps. Considering more than a third of the selected apps used external storage instead of internal storage suggests that several developers of these apps are not aware of the diﬀerence between the two. This lack of knowledge is concerning since the apps collect and store vast amounts of personal information. If access to the data is not minimized, then they can be potential targets for cyberattacks by malicious actors. The privacy policy of all the selected apps mentions that data stored locally in the device is protected by encryption. However, six of the 24 apps use the AES block cipher in ECB mode, which is a weak cipher. Weak encryption is a violation of the apps’ privacy policy of encrypting local data since it leads to a potential leak of sensitive data. This result implies that a signiﬁcant number of developers are not aware that AES in ECB mode should be avoided despite several security guidelines, such as OWASP, recommending not to use it. One likely reason that developers end up using the ECB mode is that this is the default mode for AES encryption in Android. As a result, developers must explicitly change the mode. However, this oversight is surprising since the focus of the GAEN apps was to ensure user privacy by storing personally identiﬁable information locally and protecting them via strong encryption. Failure to use strong encryption despite claiming to do so in their privacy policies raises questions about their diligence in protecting users’ data from misuse. Fourteen of the 24 apps use HTTP to communicate with remote servers. Consequently, their communication can be potentially hijacked by Man-In-The-Middle attacks. Furthermore, this is inconsistent with the apps’ stated privacy policy that mentions that communication with remote servers is encrypted end-to-end. Considering that HTTP is used in more than half of the selected apps implies that the developers of these apps are either unaware of the implications of using HTTP or did not do due diligence to verify that all communication with remote web servers uses HTTPS. The use of HTTP is concerning, especially since using HTTPS is an essential requirement of apps that transmit sensitive personal information to remote web servers. Moreover, the developers of apps promoted for large-scale use and with access to vast amounts of personal information must be 10 more perceptive of such issues and take extra care to avoid them. Exposure notiﬁcation apps transmit exposure-related data (e.g., random IDs and exposure date) to remote servers when a user consents to share their data in the event of an exposure to COVID-19. The apps’ privacy policy does not mention the location of the servers. Therefore, users reasonably assume that the servers are in the United States. However, 13 of the 24 apps communicate with exposure notiﬁcation servers outside the United States. Furthermore, the privacy policy of two of the 13 apps explicitly states that the apps only communicate with servers in the United States. Therefore, this raises the question if the apps are sending exposure-related data of US-based residents outside the US. The apps should be more transparent and explicitly state in their policies the location of the exposure notiﬁcation servers so users can make informed decisions. 3.3 RQ3: Do GAEN-based Android apps for contact tracing contain known Android app vulnerabilities? We discovered a total of six known vulnerabilities across all 24 apps, and each app had approximately two vulnerabilities on average. All 24 apps had at least one of the six vulnerabilities. Table 5 shows the breakdown of the vulnerabilities found in each app. The vulnerabilities are brieﬂy described as follows: • Unprotected Component. Android apps consist of components. Apps can export their com- ponents to share operations or data with other apps. However, if components are exported without restrictions, all apps, including malicious apps, can access them. Consequently, it can lead to denial-of-service, data leak, and data injection attacks. • Insecure PRNG. Apps that use the Random package in Java to generate pseudo-random numbers can be more easily predicted than apps that use the SecureRandom package. Since contact tracing apps based on GAEN rely on random identiﬁers, they should use a random number generator that makes it harder to predict the random numbers. Therefore, apps should use the SecureRandom package instead of the Random package to generate hard-to- predict random identiﬁers. • Weak hashing. Hashing algorithms such as MD5 and SHA-1 are considered weak. Attackers can use a hash collision to forge a duplicate hash. Therefore, apps should avoid using them to prevent forgery attacks. • Data Backup. Android allows users to create backups of all data in an app without having root privileges. Consequently, malicious users with access to the device will be able to create a backup of all the app’s data using a USB. Apps can be conﬁgured to protect against this potential attack by setting the allowBackup attribute in the app’s manifest ﬁle to false. Android recommends apps to disable this feature to prevent malicious users from accessing an app’s local data. • Insecure TLS/SSL Implementation. Apps using TLS/SSL protocol to communicate with remote servers must verify the trustworthiness of the servers. The established way to verify trust is for the app to maintain a list of trusted certiﬁcate authorities (CAs). The server is conﬁgured with a certiﬁcate containing a public key and a matching private key. The certiﬁcate must be signed by a certiﬁcate authority (CA) trusted by the app. Generally, the list of trusted CAs is pre-installed in the device on which the app is installed. However, the 11 connection may fail if the certiﬁcate used to conﬁgure the server (1) is signed by a CA, not in the list of trusted CAs, (2) or is self-signed, (3) or is signed by an intermediate certiﬁcate missing from the server conﬁguration. While the third reason is addressed at the server-side, the ﬁrst two reasons are addressed by implementing a custom TrustManager, an Android API, in the app. Implementation mistakes in the custom TrustManager lead to vulnerabilities (e.g., trust all CAs) that can lead to Man-In-The-Middle attacks. • Unpinned Certiﬁcates. Apps installed in a device trust all CAs that are pre-conﬁgured with the device. App developers can further restrict the CAs that the app will trust by pinning a set of trusted CAs to the app. The app then trusts only the pinned CAs and not any other CA, including the ones trusted by the device. Although not mandatory, certiﬁcate pinning is good security practice. However, they should be used with care as they can hamper usability due to communication failure because of outdated certiﬁcates as a result of changes to the server conﬁguration. Twelve of the 24 apps had at least one unprotected component vulnerability. We found sev- eral manifestations of this vulnerability. For example, apps used a third-party library that had an unprotected broadcast receiver that could write to shared preferences. Malicious apps could potentially exploit this vulnerability to execute data injection attacks. In other instances, apps deﬁned an unprotected activity that could access Bluetooth and location. As a result, malicious apps could potentially exploit this activity to get access to privileged features without having the necessary permissions. Furthermore, few apps deﬁned an activity that could share exposure-related diagnostic information with a remote server. Since this activity was exported without restriction, malicious apps could potentially exploit this vulnerability to communicate with the remote server. Apps must protect these components by making them private to the app. If these apps need to share these components with the underlying system, then they must protect the components with system permissions. All apps, except two, chose to use an insecure PRNG to generate the random identiﬁers used to identify devices where the app is installed anonymously. Choosing an insecure PRNG over a more secure PRNG allows malicious actors to potentially predict the random identiﬁer more easily, which could be used to create duplicate identiﬁers and hence create erroneous or fake exposure-related data entries. Therefore, all the selected apps must use the SecureRandom package to generate random identiﬁers. Seven of the 24 apps use MD5 or SHA-1, which are weak hashing algorithms. Choosing a weak hashing algorithm for contact tracing apps is problematic since these apps are expected to provide strong privacy and security guarantees. A strong hashing algorithm is the most basic requirement with which the app can ensure the integrity of the information it stores. A sixth of the 24 apps, that is, four apps allow users to back up the app’s data without rooting the device. This is insecure for GAEN apps since they store a user’s and their contacts’ exposure- related data locally in the app. A malicious user with access to the device but without root access can get access to this data and misuse it. Apps that have this feature should disable it by setting the allowbackup attribute in their manifest ﬁle to false. Five of the 24 apps chose to use certiﬁcate pinning instead of relying on the device’s list of trusted CAs. While certiﬁcate pinning provides additional protection against MITM attacks, most apps choose not to use it, possibly because of potential connection failures due to the pinned certiﬁcates becoming outdated. In such situations, the only way to restore the app is by pushing a software update, which the users must install. Temporary connection failures are not ideal and could render 12 an app useless. However, in the context of the GAEN framework, where communication with remote servers is minimal1 and security is paramount, it is advisable for the apps to use certiﬁcate pinning to reduce the risk of an MITM attack. Further, apps could pin backup certiﬁcates to prevent relying on only one pinned certiﬁcate. If one of them is outdated, the app can use the backups to connect to the server. MobSF reported an insecure SSL implementation vulnerability in exactly one app (the Arizona app) because the app was using a pinned self-signed certiﬁcate. Apps pinning self-signed certiﬁcates have beneﬁts and limitations in terms of preventing MITM attacks. Consider a scenario where an app has pinned a certiﬁcate signed by a CA that has been compromised. In this situation, the app will need to be updated via a software update with a newly issued certiﬁcate. On the other hand, if the app had pinned a self-signed certiﬁcate, then the app only trusts that certiﬁcate and will not be aﬀected if any other CA’s certiﬁcate is compromised. However, using self-signed certiﬁcates are secure only if they are continuously monitored. In its absence, app developers may not know about a compromised certiﬁcate, and the app’s communication with the compromised server will continue unknowingly. However, actively maintaining self-signed certiﬁcates is more cumbersome than using certiﬁcates from a trusted CA. Trusted CAs can revoke compromised certiﬁcates used by a server to stop communication between the app with the pinned certiﬁcate and the server, protecting the user from further harm. Therefore, in the general case, it is more secure to pin certiﬁcates signed by trusted CAs instead of self-signing them. 4 Discussion 4.1 Observations on the apps The results show that contact tracing apps in Android based on the GAEN framework are over- privileged. Further, in Android, the apps are granted permission to use privileged system features at install time. As a result, users have less control over granting permission to these apps at runtime. This is concerning since these apps could potentially be used for tasks other than contact tracing, such as mass surveillance. Therefore, the apps must collect minimal information and use the least privileges. The GAEN framework was developed to help create apps that preserve user privacy. How- ever, our analysis shows that all apps violate their own privacy policy due to several potential reasons, such as developer oversight and developers’ lacking domain knowledge and awareness of the underlying platform. Oversight is concerning but understandable since contact tracing apps were developed hurriedly to tackle the challenges of a growing pandemic. However, oversight in this context leads to a lack of transparency and credibility. It exacerbates the skepticism that the general public has towards contact tracing apps and hampers their widespread adoption. Weak adoption is not desirable since contact tracing apps if used eﬀectively, are a vital tool to contain the pandemic. If developer oversight is the reason for privacy violations, then there is a need to develop tools and techniques that help developers write privacy policies that are consistent with their app’s behavior and vice versa. Few apps in our study missed mentioning in their privacy policy all the personally identiﬁable information that they collected in their apps. One possible reason for this is a lack of domain 1Users only connect with the remote server to upload a positive test result. Also, apps periodically connect with the remote server to check for positive cases. 13 knowledge in developers. In this context, privacy research eﬀorts should focus on developing meth- ods to help identify domain-speciﬁc personally identiﬁable information. Contact tracing apps have features that could be misused to violate users’ privacy. Therefore, it is crucial to accurately iden- tify the information that the apps collect so users can make informed decisions about the privacy implications of using the apps. The GAEN framework made it easier for healthcare providers to create apps for eﬀective contact tracing without the need to know the details of the underlying platform [9]. This is also evident from certain privacy violations, which could have been avoided if developers had known about the underlying platform behavior. Therefore, existing research in app security and privacy should focus on developing methods and tools to assist less experienced developers gain the necessary knowledge to avoid privacy violations. The selected apps had vulnerabilities that are well known in the app development community. In fact, all four vulnerabilities that were discovered are part of the OWASP top 10 [32], a popular set of guidelines for developing secure mobile and web apps. The apps had these vulnerabilities despite the focus on ensuring that the apps are secure and preserve user privacy. This suggests that the app developers did not have the experience to avoid these mistakes, or they did not have access to tools to help them prevent these vulnerabilities eﬀectively. The latter is less likely since app development IDEs such as Android Studio have support for detecting and preventing such vulnerabilities during development. Therefore, it is more likely that the states did not allocate the resources to recruit developers with suﬃcient experience in mobile app development. While it is understandable that states need to prioritize their resources to tackle a pandemic, they should have planned better before developing and deploying apps with long-term consequences for user privacy and security. 4.2 Observations on MobSF While investigating the potential vulnerabilities MobSF reported, we discovered that a few of them were false positives, that is, falsely reported as vulnerabilities. We report and discuss them to help tool developers like MobSF improve their tools. Brieﬂy, MobSF reported a total of ﬁve false positives across the 24 apps. Further, MobSF reported ﬁve false positives for all apps except the Virginia app and the Arizona app, which had three and four false positives, respectively. We explain the false positives, their likely reasons, and suggestions on how to avoid them as follows: • The SQLInjection vulnerability was falsely reported in 23 of the 24 apps. MobSF reported the vulnerability in apps using the execSQL API to execute SQL queries. However, using execSQL leads to SQLInjection only if the query string has user-supplied input with potentially malicious SQL. None of the queries reported as being potentially vulnerable to SQLInjection relied on user input. Hence, they were not vulnerable to SQLInjection. Suggestion: MobSF should ﬂag queries in execSQL only if they rely on user-input and are not parameterized. • MobSF falsely reported an unprotected component vulnerability in all 24 apps because it found components in these apps that were exported. However, these components were also protected by system permissions, that is, only the system could be granted access to these components. Furthermore, these permissions were deﬁned as part of the GAEN framework and were made available only for the purpose of contact tracing to government healthcare 14 authorities. Therefore, apps without necessary authorization cannot request these permissions and get access to the exported components. Suggestion: MobSF should consider the system permissions deﬁned in the GAEN framework during analysis to improve the detection of unprotected components. • MobSF reported that 22 of the 24 apps saved sensitive data such as usernames, passwords, and secret keys in clear text. Furthermore, MobSF claimed that 22 of the 24 apps logged sensitive data. On further inspection, we found the claims to be false. MobSF reported the apps because they were saving or logging string constants in ﬁles and the strings contained words such as ”key” and ”password,” but these constants were not sensitive data. Suggestion: Considering that none of the string constants with words like key and password were sensitive data in our sample of apps, MobSF should consider using other heuristics to identify sensitive data. • MobSF falsely reported a Janus signature vulnerability in all 24 apps. Janus is a system vulnerability that allows attackers to inject a DEX ﬁle into an APK ﬁle signed with the v1 signature scheme without aﬀecting the signatures. The vulnerability can be exploited because an Android package can be a valid DEX ﬁle and APK ﬁle at the same time. However, this vulnerability is exploitable only if the app runs on an Android version lower than 7.0. Android developers ﬁxed this vulnerability in version 7.0 and above. As a result, Android APKs or packages have to be signed with the v2 and v3 signature schemes. All 24 contact tracing apps we considered used the v2 and v3 signature schemes to sign their APKs. MobSF ﬂagged the apps as potentially vulnerable to Janus because the apps were also signed with the v1 signature scheme to enable backward compatibility. We categorize this as a false positive since the app developers have no choice but to sign their apps with the v1 signature scheme to support Android versions less than 7.0. Moreover, the app developers are doing due diligence by signing the apps with v2 and v3 signature schemes along with the v1 scheme (for backward compatibility), which is the best they can do under the circumstance. Suggestion: MobSF should ﬂag apps as potentially vulnerable to Janus if they are signed only with the v1 signature scheme. For apps signed with v1,v2, and v3 signatures, MobSF should consider reporting a diﬀerent label like an information label to inform the developers that while this cannot be ﬁxed at the app stage, one should be aware that the v1 signature is vulnerable on Android versions less than 7. Therefore, apps might want to consider supporting only Android version 7.0 or more. In conclusion, MobSF reported more false positives than potential true positives w.r.t potential vulnerabilities in each app (see Tables 5 vs. 6), which shows that MobSF has a high false positive rate. This observation is consistent with prior research eﬀorts to evaluate the eﬀectiveness of security analysis tools for Android apps [20]. In general, static analysis tools like MobSF have a high false positive rate, which hampers their adoption and reduces their eﬀectiveness. Due to a high false positive rate, MobSF users have to manually verify the warnings and issues reported, which reduces trust in the tool verdicts. Consequently, this hampers tool adoption and the overall eﬀectiveness of the tool. 15 5 Related Work Since the onset of the pandemic, numerous proposals have been made to automate the contact In this context, many contact tracing apps have been implemented and tracing process [4, 33]. deployed worldwide. This has led to a plethora of eﬀorts to survey the characteristics and evaluate the eﬀectiveness of contact tracing apps [34, 35, 36]. Several agencies worldwide have called for an independent assessment of the security and privacy risks posed by contact tracing apps for greater transparency and accountability [9]. In this context, researchers have evaluated the security and privacy of the contact tracing apps to understand if they pose any privacy risks to the users [37, 38]. In this section, we discuss the eﬀorts that are most closely related to our work and how they are diﬀerent. Wen et al. [39] performed a systematic study of 41 contact tracing apps deployed on Android and iOS. They used program analysis to determine the APIs relevant for contact tracing and identify the information collected by the apps. Additionally, they performed a cross-platform comparison of apps available on Android and iOS. Their results show that some apps expose identiﬁable information that can enable ﬁngerprinting of apps and tracking of speciﬁc users. Moreover, they observed that some apps exhibited inconsistencies across platforms, which led to diﬀerent privacy implications across the platforms. Their eﬀort was one of the ﬁrst attempts to understand the privacy and security implications of contact tracing apps. Although their eﬀort is related to our evaluation, we focus on diﬀerent aspects. For example, in addition to vulnerability analysis, we also analyze the privacy policy of the apps, which their eﬀort does not consider. Samhi et al. [34] conducted an empirical evaluation of Android apps in Google Play related to COVID-19. The aim of their study was to broadly characterize the apps in terms of their purpose, intended users, complexity, development process, and potential security risks. While their focus was not on security and privacy, they observed that none of the apps they considered leaked sensitive data based on static analysis of the apps using tools FlowDroid and IccTA. However, recent eﬀorts to measure the eﬀectiveness of security analysis tools in Android have raised questions on the eﬀectiveness of static analysis tools like FlowDroid and IccTA in detecting sensitive data leaks. Hatmian et al. [40] analyzed the privacy and security performance of 28 contact tracing apps available on the Android platform in May-June 2020. They analyzed the permissions used by the apps and the potential vulnerabilities in them. Further, they measured the coverage of the privacy policy of the apps w.r.t the privacy principles outlined in the General Data Protection Regulation (GDPR) laws [41]. Our work is diﬀerent from theirs in a number of ways. First, we focus on oﬃcial apps developed by the US states based on the GAEN framework. None of the four US apps in the study conducted by Hatmian et al. are based on the GAEN framework. Second, instead of measuring the coverage of the apps’ privacy policies w.r.t the privacy principles, we analyze if the apps’ privacy policies are consistent with their source code, that is, apps are not violating their own privacy policies. Third, we critically analyze the verdicts reported by tools such as MobSF instead of reporting them as is. For example, Hatmian et al. report, based on MobSF’s analysis, that all apps they considered log sensitive information. In our evaluation, MobSF also ﬂagged all apps as logging sensitive information. However, after manually verifying the verdict, we discovered this was a false positive for all apps. In November 2020, Baumgartner et al. [37] demonstrated that the GAEN framework’s design is vulnerable to proﬁling and de-anonymizing infected persons and relay-based wormhole attacks that are capable of generating fake contacts to derail the contact tracing process. They claimed that if the vulnerabilities are not addressed, then all apps based on the framework will be vulnerable. 16 Instead of analyzing the GAEN framework’s design, in this paper, we are analyzing the apps based on the GAEN framework from the perspective of whether the apps comply with their own privacy policy and if they contain vulnerabilities that manifest due to known implementation bugs and incorrect conﬁgurations. Ang et al. [42] reviewed the security and privacy of 70 contact tracing apps one year after the pandemic. They statically analyzed the apps using MobSF for vulnerabilities based on threat sce- narios they identiﬁed for contact tracing apps. Additionally, they reported data trackers embedded in apps that can potentially violate privacy since data collected by the trackers can be used with- out the users’ consent. However, their privacy analysis does not include any analysis of an app’s privacy policy. The set of apps in the evaluation by Ang et al. includes 20 apps that we considered in our assessment. However, the results of our analysis diﬀer signiﬁcantly. For example, 80% of the apps they considered stored sensitive information in cleartext. On the other hand, we found this to be a false positive for 20 of the 24 apps we considered. Similarly, we discovered vulnerabilities in our analysis (e.g., data backup) that are not reported in Ang et al. Further, Ang et al. used dynamic analyzers such as VirusTotal [43] to detect the presence of malware in the contact tracing apps. However, malware detection tools such as VirusTotal do not accurately detect the presence of malware as they often falsely identify Potentially Unwanted Programs (PUPs) as malware [20]. Kouliaridis et al. [44] investigated all oﬃcial contact tracing apps deployed by European coun- tries as of Feb 2, 2021. They analyzed the apps both statically and dynamically. Static analysis included sensitive permissions and API calls, third-party trackers, and known vulnerabilities and conﬁgurations that aﬀect app security based on the Common Weakness Enumerations (CWEs) [45] and Common Vulnerabilities and Exposures (CVEs) [46]. Dynamic analysis involved instrumenting the app’s source code, verifying if the app uses location and Bluetooth services at runtime, and in three monitoring network traﬃc. The evaluation in our paper diﬀers from Kouliaridis et al. distinct ways. First, we considered a diﬀerent set of apps. Second, we analyzed the apps’ privacy policies to determine if they are consistent with their encoded behavior. Third, we did not dynam- ically analyze the apps. Further, there are notable diﬀerences in our static analysis observations. For example, Kouliaridis et al. report that two-thirds of their apps, which included GAEN apps, had a potential SQL injection vulnerability based on MobSF’s analysis. However, we observed that MobSF falsely reported SQL injection as a vulnerability in 23 of the 24 GAEN apps we considered. 6 Caveats In vulnerability analysis, we considered vulnerabilities in Ghera and reported by MobSF. Since we did not cover vulnerabilities outside these sources, it is possible that they existed in the apps but went unreported. Consequently, our vulnerability analysis may not be comprehensive. App developers reading this report must take steps to ﬁx the reported vulnerabilities and perform further analysis to ensure that other vulnerabilities not reported here do not exist in their apps. We conﬁrmed the potential vulnerabilities reported by static analysis tools by manually examin- ing them. However, we did not build malicious applications to exploit the vulnerabilities. Therefore, we do not know to what extent the vulnerabilities are exploitable. The results reported in this study are limited to the set of apps considered or the GAEN apps in general. Therefore, they should not be generalized for other contact tracing apps, especially apps not based on the GAEN framework. 17 7 Conclusion In this paper, we conducted a systematic investigation of 24 contact tracing apps based on the GAEN framework in the US. All the apps were implemented and deployed by the oﬃcial health departments of the respective US states. We discovered that the considered apps are over-privileged, they violate their own privacy policies, and contain vulnerabilities that can be exploited by malicious users to cause harm to the app’s users. While there have been previous eﬀorts at evaluating the contact tracing apps for privacy violations and vulnerabilities, none of them have focused on the consistency of the apps’ privacy policies w.r.t to their encoded behavior. Although there are similarities between our eﬀort and existing eﬀorts in terms of vulnerability analysis of contact tracing apps, the results diﬀer markedly. Our results show that few vulnerabilities reported as potential vulnerabilities in related evaluations are false positives. For example, several existing research eﬀorts have reported the Janus vulnerability, which we reported as a false positive, as a true positive in their results. Therefore, this raises the question if eﬀorts to study the privacy and security of contact tracing apps are reporting vulnerabilities that may not manifest in reality. Reporting false positives as potential true positives may erode the public’s trust in contact tracing apps and eventually lead to reduced adoption, which may ultimately weaken eﬀorts to contain the pandemic. Therefore, there is a need for researchers to continuously evaluate the security and privacy of contact tracing apps to reproduce and verify the results. 8 Ackowledgements We wish to thank Minqi Shi, Taylor Giles, Soroush Semerkant, Mihir Madhira, Jeﬀrey Jiminez, Colin Ruan, and Patrick Wszeborowski, undergraduate students in the Department of Computer Science at Stony Brook University for assisting with data collection. References [1] D. Klinkenberg, C. Fraser, and H. Heesterbeek, “The eﬀectiveness of contact tracing in emerg- ing epidemics,” PloS one, vol. 1, no. 1, p. e12, 2006. [2] T. Jiang, Y. Zhang, M. Zhang, T. Yu, Y. Chen, C. Lu, J. Zhang, Z. Li, J. Gao, and S. Zhou, “A survey on contact tracing: the latest advancements and challenges,” ACM Transactions on Spatial Algorithms and Systems (TSAS), vol. 8, no. 2, pp. 1–35, 2022. [3] A. Anglemyer, T. H. Moore, L. Parker, T. Chambers, A. Grady, K. Chiu, M. Parry, M. Wilczyn- ska, E. Flemyng, and L. Bero, “Digital contact tracing technologies in epidemics: a rapid review,” Cochrane Database of Systematic Reviews, no. 8, 2020. [4] N. Ahmed, R. A. Michelin, W. Xue, S. Ruj, R. Malaney, S. S. Kanhere, A. Seneviratne, W. Hu, H. Janicke, and S. K. Jha, “A survey of covid-19 contact tracing apps,” IEEE access, vol. 8, pp. 134 577–134 601, 2020. [5] A. Akinbi, M. Forshaw, and V. Blinkhorn, “Contact tracing apps for the covid-19 pandemic: a systematic literature review of challenges and future directions for neo-liberal societies,” Health Information Science and Systems, vol. 9, no. 1, pp. 1–15, 2021. 18 [6] F. Rowe, “Contact tracing apps and values dilemmas: A privacy paradox in a neo-liberal world,” International Journal of Information Management, vol. 55, p. 102178, 2020. [7] F. Hassandoust, S. Akhlaghpour, and A. C. Johnston, “Individuals’ privacy concerns and adoption of contact tracing mobile applications in a pandemic: A situational privacy calculus perspective,” Journal of the American Medical Informatics Association, vol. 28, no. 3, pp. 463–471, 2021. [8] T. Martin, G. Karopoulos, J. L. Hern´andez-Ramos, G. Kambourakis, and I. Nai Fovino, “De- mystifying covid-19 digital contact tracing: A survey on frameworks and mobile apps,” Wireless Communications and Mobile Computing, vol. 2020, 2020. [9] U. S. G. A. Oﬃce, “Beneﬁts and challenges of smartphone applications to augment contact tracing,” https://www.gao.gov/products/gao-21-104622, Sept 2021. [10] T. TraceTogether, “How does tracetogether work,” 2020. [11] R. Gupta, M. Bedi, P. Goyal, S. Wadhera, and V. Verma, “Analysis of covid-19 tracking tool in india: case study of aarogya setu mobile application,” Digital Government: Research and Practice, vol. 1, no. 4, pp. 1–8, 2020. [12] S. Vaudenay, “Centralized or decentralized? the contact tracing dilemma,” Cryptology ePrint Archive, 2020. [13] T. Li, C. Faklaris, J. King, Y. Agarwal, L. Dabbish, J. I. Hong et al., “Decentralized is not risk- free: Understanding public perceptions of privacy-utility trade-oﬀs in covid-19 contact-tracing apps,” arXiv preprint arXiv:2005.11957, 2020. [14] Google, “Exposure notiﬁcations implementation guide,” https://developers.google.com/ android/exposure-notiﬁcations/implementation-guide, Feb. 2022. [15] Apple, “Enexposureconﬁguration,” exposurenotiﬁcation/enexposureconﬁguration, Feb. 2022. https://developer.apple.com/documentation/ [16] S. Altmann, L. Milsom, H. Zillessen, R. Blasone, F. Gerdon, R. Bach, F. Kreuter, D. Nosenzo, S. Toussaert, J. Abeler et al., “Acceptability of app-based contact tracing for covid-19: Cross- country survey study,” JMIR mHealth and uHealth, vol. 8, no. 8, p. e19857, 2020. [17] Google, “Publicly-available exposure notiﬁcations apps,” https://developers.google.com/ android/exposure-notiﬁcations/apps, Feb. 2022. [18] Sufatrio, D. J. J. Tan, T.-W. Chua, and V. L. L. Thing, “Securing android: A survey, taxonomy, and challenges,” ACM Comput. Surv., pp. 58:1–58:45, 2015. [19] L. Li, T. F. Bissyand´e, M. Papadakis, S. Rasthofer, A. Bartel, D. Octeau, J. Klein, and L. Traon, “Static analysis of android apps: A systematic literature review,” Information and Software Technology, vol. 88, pp. 67–95, 2017. [20] V.-P. Ranganath and J. Mitra, “Are free android app security analysis tools eﬀective in de- tecting known vulnerabilities?” Empirical Software Engineering, vol. 25, no. 1, pp. 178–219, 2020. 19 [21] A. Abraham, Magaofei, M. Dobrushin, and V. Nadal, “Mobsf github,” https://github.com/ MobSF/Mobile-Security-Framework-MobSF, Feb. 2022. [22] A. Desnos, G. Gueguen, and S. Bachmann, “Androguard,” https://androguard.readthedocs. io/en/latest/, Feb. 2022. [23] L. Lu, Z. Li, Z. Wu, W. Lee, and G. Jiang, “Chex: statically vetting android apps for compo- nent hijacking vulnerabilities,” in Proceedings of the 2012 ACM conference on Computer and communications security, 2012, pp. 229–240. [24] T. Watanabe, M. Akiyama, F. Kanei, E. Shioji, Y. Takata, B. Sun, Y. Ishi, T. Shibahara, T. Yagi, and T. Mori, “Understanding the origins of mobile app vulnerabilities: A large-scale measurement study of free and paid apps,” in 2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR). IEEE, 2017, pp. 14–24. [25] J. Mitra and V.-P. Ranganath, “Ghera: A repository of android app vulnerability benchmarks,” in Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering, 2017, pp. 43–52. [26] M. Ghafari, P. Gadient, and O. Nierstrasz, “Security smells in android,” in 2017 IEEE 17th IEEE, international working conference on source code analysis and manipulation (SCAM). 2017, pp. 121–130. [27] P. R. Center, “An analysis of android app permissions,” https://www.pewresearch.org/ internet/2015/11/10/an-analysis-of-android-app-permissions/, Nov 2015. [28] B. P. Sarma, N. Li, C. Gates, R. Potharaju, C. Nita-Rotaru, and I. Molloy, “Android permis- sions: a perspective combining risks and beneﬁts,” in Proceedings of the 17th ACM symposium on Access Control Models and Technologies, 2012, pp. 13–22. [29] Google, “Android permissions/overview, May 2022. app permissions,” https://developer.android.com/guide/topics/ [30] A. V. Prakash and S. Das, “Explaining citizens’ resistance to use digital contact tracing apps: A mixed-methods study,” International Journal of Information Management, vol. 63, p. 102468, 2022. [31] Google, “Manifest permissions in android,” https://developer.android.com/reference/android/ Manifest.permission#USE FINGERPRINT, Jul. 2022. [32] Owasp, “Owasp top 10,” https://owasp.org/www-project-mobile-top-10/, Jul. 2022. [33] J. Bell, D. Butler, C. Hicks, and J. Crowcroft, “Tracesecure: Towards privacy preserving contact tracing,” arXiv preprint arXiv:2004.04059, 2020. [34] J. Samhi, K. Allix, T. F. Bissyand´e, and J. Klein, “A ﬁrst look at android applications in google play related to covid-19,” Empirical Software Engineering, vol. 26, no. 4, pp. 1–49, 2021. [35] H. Cho, D. Ippolito, and Y. W. Yu, “Contact tracing mobile apps for covid-19: Privacy considerations and related trade-oﬀs,” arXiv preprint arXiv:2003.11511, 2020. 20 [36] M. Lanzing, “Contact tracing apps: an ethical roadmap,” Ethics and information technology, vol. 23, no. 1, pp. 87–90, 2021. [37] L. Baumg¨artner, A. Dmitrienko, B. Freisleben, A. Gruler, J. H¨ochst, J. K¨uhlberg, M. Mezini, R. Mitev, M. Miettinen, A. Muhamedagic et al., “Mind the gap: Security & privacy risks of contact tracing apps,” in 2020 IEEE 19th international conference on trust, security and privacy in computing and communications (TrustCom). IEEE, 2020, pp. 458–467. [38] Y. Gvili, “Security analysis of the covid-19 contact tracing speciﬁcations by apple inc. and google inc.” Cryptology ePrint Archive, 2020. [39] H. Wen, Q. Zhao, Z. Lin, D. Xuan, and N. Shroﬀ, “A study of the privacy of covid-19 con- tact tracing apps,” in International Conference on Security and Privacy in Communication Systems. Springer, 2020, pp. 297–317. [40] M. Hatamian, S. Wairimu, N. Momen, and L. Fritsch, “A privacy and security analysis of early- deployed covid-19 contact tracing android apps,” Empirical software engineering, vol. 26, no. 3, pp. 1–51, 2021. [41] E. Union, “General data protection regulation,” https://gdpr.eu/what-is-gdpr/, 2022. [42] V. Ang and L. K. Shar, “Covid-19 one year on–security and privacy review of contact tracing mobile apps,” IEEE Pervasive Computing, vol. 20, no. 4, pp. 61–70, 2021. [43] H. Systemas, “Virustotal,” https://www.virustotal.com/gui/home/upload, 2022. [44] V. Kouliaridis, G. Kambourakis, E. Chatzoglou, D. Geneiatakis, and H. Wang, “Dissecting contact tracing apps in the android platform,” Plos one, vol. 16, no. 5, p. e0251867, 2021. [45] Mitre, “Common weakness enumeration,” https://cwe.mitre.org/, 2022. [46] “Common vulnerabilities and exposures,” https://cve.mitre.org/, 2022. 21 App Collects Location Uses Insecure Uses Weak Uses HTTP Communicates With Non-US # Violations Privacy Policy Violation Y Alabama Y Arizona Y California Y Colorado Y Connecticut Y Delaware Y DC N Guam Hawaii Y Y Louisiana Maryland Y Y Michigan N Minnesota Nevada Y New Jersey N New Mexico Y New York N North Carolina Y Y North Dakota Penn Y Y Utah Virginia Y Y Washington Y Wisconsin # Apps per 20 violation Storage Y N N N N Y N N Y N N N Y N Y Y Y N Y Y N N N N 9 Encryption Y N N N N N N Y Y Y N N Y N N N N Y N N N N N N 6 N N Y Y N Y Y N N N Y Y Y Y Y Y N Y N N Y N Y Y 14 server domain Y N Y Y Y N Y N N N Y N Y Y N Y N Y Y N N N Y Y 13 per app 4 1 3 3 2 3 3 1 3 2 3 2 4 3 2 4 1 4 3 2 2 1 3 3 Table 4: Privacy Violations by each US-based GAEN app. Columns 2-6 indicate a feature or an action that an app claims it does not use or do in its privacy policy. The cells with Y/N denote Yes if an app performs the action in the corresponding column and No if it does not. Y implies a privacy violation and N implies otherwise. 22 No Certiﬁcate # Vulns. per app 3 3 2 2 3 2 3 4 3 3 3 1 3 4 2 4 2 2 2 2 3 2 3 3 App Unprotected Component Y Alabama N Arizona N California N Colorado Y Connecticut N Delaware N DC Y Guam Y Hawaii Y Louisiana Maryland Y N Michigan Y Minnesota Y Nevada N New Jersey New Mexico Y N New York North Carolina N North Dakota/Wyoming N N Penn N Utah Virginia Y Y Washington Y Wisconsin # Apps per 12 Vuln Known Vulnerabilities Allows Insecure Weak PRNG Y Y Y Y Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y N Y Y 22 hashing Data Backup N N N Y N N N N N N N Y N N N Y N N N N N N N N N N Y N N Y Y N N Y N N N Y N Y Y N N N N N N N 4 7 Insecure SSL Impl. Pinning N Y N N N N Y N N N N N N N N N N N N N N N N N 1 Y N Y Y Y N Y Y Y Y Y Y Y Y N Y N Y N N Y Y Y Y 18 Table 5: Known Vulnerabilities in each US-based GAEN app. Columns 2-5 indicate a known vulnerability. The cells with Y/N denote Yes if an app contains the vulnerability and No otherwise. 23 App SQL Injection Unprotected Component Cleartext Storage Log Sensitive False Positive Vulnerability Y Alabama Y Arizona Y California Y Colorado Y Connecticut Y Delaware Y DC Y Guam Hawaii Y Y Louisiana Y Maryland Y Michigan Y Minnesota Y Nevada New Jersey Y Y New Mexico Y New York North Carolina Y North Dakota/Wyoming Y Y Penn Utah Y N Virginia Y Washington Y Wisconsin # Apps per 23 false positive Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y 24 Y N Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y N Y Y 22 Data Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y 24 Table 6: False Positives reported by MobSF in every US-based GAEN app. The cells with Y indicate that the vulnerability in the corresponding column was falsely reported as a potential vulnerability by MobSF and N denotes MobSF did not report the vulnerability in the corresponding column. 5 5 5 5 5 5 5 4 5 5 per app Janus Signature # False Positives Vulnerability Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y 24 5 5 5 5 5 3 5 5 5 5 5 5 5 5 24",0.0
"An Empirical Evaluation of Four Off-the-Shelf Proprietary
  Visual-Inertial Odometry Systems","[{'href': 'http://arxiv.org/abs/2207.06780v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.06780v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-14 09:40:34,,"2 2 0 2 l u J 7 ] G L . s c [ 2 v 9 9 0 2 0 . 7 0 2 2 : v i X r a Preprint An empirical study of implicit regularization in deep oﬄine RL Caglar Gulcehre∗, Srivatsan Srinivasan∗, Jakub Sygnowski, Georg Ostrovski, Mehrdad Farajtabar, Matt Hoﬀman, Razvan Pascanu, Arnaud Doucet DeepMind Abstract Deep neural networks are the most commonly used function approximators in oﬄine re- inforcement learning. Prior works have shown that neural nets trained with TD-learning and gradient descent can exhibit implicit regularization that can be characterized by under- parameterization of these networks. Speciﬁcally, the rank of the penultimate feature layer, also called eﬀective rank, has been observed to drastically collapse during the training. In turn, this collapse has been argued to reduce the model’s ability to further adapt in later stages of learning, leading to the diminished ﬁnal performance. Such an association between the eﬀective rank and performance makes eﬀective rank compelling for oﬄine RL, primarily for oﬄine policy evaluation. In this work, we conduct a careful empirical study on the relation between eﬀective rank and performance on three oﬄine RL datasets : bsuite, Atari, and DeepMind lab. We observe that a direct association exists only in restricted settings and disappears in the more extensive hyperparameter sweeps. Also, we empirically identify three phases of learning that explain the impact of implicit regularization on the learning dynamics and found that bootstrapping alone is insuﬃcient to explain the collapse of the eﬀective rank. Further, we show that several other factors could confound the relationship between eﬀective rank and performance and conclude that studying this association under simplistic assumptions could be highly misleading. Contents 1 Introduction 2 Background 2.1 Eﬀective rank and implicit under-regularization . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Auxiliary losses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Deep oﬄine RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Experimental setup 4 Eﬀective rank and performance 4.1 Lifespan of learning with deep Q-networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 5 5 6 6 7 8 9 4.2 The eﬀect of dataset size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 ∗Indicates joint ﬁrst authors. 1 Preprint 5 Interactions between rank and performance 6 The eﬀect of activation functions 7 Optimization 8 The eﬀect of loss function 8.1 Q-learning and behavior cloning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2 CURL: The eﬀect of self-supervision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Tandem RL 10 Oﬄine policy selection 11 Robustness to input perturbations 12 Discussion A Appendix A.1 The impact of depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Spectral density of hessian for oﬄine RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 DeepMind lab: performance vs the eﬀective ranks . . . . . . . . . . . . . . . . . . . . . . . . . A.4 SAM optimizer: does smoother loss landscape aﬀect the behavior of the rank collapse? . . . . A.5 Eﬀective rank and the value error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 CURL on DeepMind Lab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.7 Learning rate evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.8 Learning curves long training regime and the diﬀerent phases of learning . . . . . . . . . . . . A.9 Eﬀective rank and the performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.10 Interventions to eﬀective rank and randomized controlled trials (RCT) . . . . . . . . . . . . . A.11 Computation of feature ranks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.12 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.13 bsuite phase transitions and bottleneck capacity . . . . . . . . . . . . . . . . . . . . . . . . . A.14 Activation sparsity on bsuite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 15 15 17 17 18 19 22 23 25 30 30 30 32 32 33 34 35 35 35 35 35 38 38 38 1 Introduction The use of deep networks as function approximators in reinforcement learning (RL), referred to as Deep Reinforcement Learning (DRL), has become the dominant paradigm in solving complex tasks. Until recently, most DRL literature focused on online-RL paradigm, where agents must interact with the environment to explore and learn. This led to remarkable results on Atari (Mnih et al., 2015), Go (Silver et al., 2017), StarCraft II (Vinyals et al., 2019), Dota 2 (Berner et al., 2019), and robotics (Andrychowicz et al., 2020). Unfortunately, the need to interact with the environment makes these algorithms unsuitable and unsafe for many real-world applications, where any action taken can have serious ethical or harmful consequences or be 2 Preprint costly. In contrast, in the oﬄine RL paradigm (Fu et al., 2020; Fujimoto et al., 2018; Gulcehre et al., 2020; Levine et al., 2020), also known as batch RL (Ernst et al., 2005; Lange et al., 2012), agents learn from a ﬁxed dataset previously logged by other (possibly unknown) agents. This ability makes oﬄine RL more applicable to the real world. Recently, Kumar et al. (2020a) showed that oﬄine RL methods coupled with TD learning losses could suﬀer from a eﬀective rank collapse of the penultimate layer’s activations, which renders the network to become under-parameterized. They further demonstrated a signiﬁcant fraction of Atari games, where the collapse of eﬀective rank collapse corresponded to performance degradation. Subsequently, Kumar et al. (2020a) explained the rank collapse phenomenon by analyzing a TD-learning loss with bootstrapped targets in the kernel and linear regression setups. In these simpliﬁed scenarios, bootstrapping leads to self-distillation, causing severe under-parametrization and poor performance, as also observed and analyzed by Mobahi et al. (2020). Nevertheless, Huh et al. (2021) studied the rank of the representations in a supervised learning setting (image classiﬁcation tasks) and argued that low rank leads to better performance. Thus the low-rank representations could act as an implicit regularizer. Figure 1: [Atari] The rank and the performance on broad vs narrow hyperparameter sweep: Correlation between eﬀective rank and agent’s performance towards the end of training in diﬀerent Atari games. We report the regression lines for the narrow sweep, which covers only a single oﬄine RL algorithm, with a small minibatch size (32) and a learning rate sweep similar to the hyperparameter sweep deﬁned in RL Unplugged (Gulcehre et al., 2020), whereas in the broad setting, we included more data from diﬀerent models and a larger hyperparameter sweep. In the narrow setup, there is a positive relationship between the eﬀective rank and the agent’s performance, but that relationship disappears in the broad data setup and almost reverses. Typically, in machine learning, we rely on empirical evidence to extrapolate the rules or behaviors of our learned system from the experimental data. Often those extrapolations are done based on a limited number of experiments due to constraints on computation and time. Unfortunately, while extremely useful, these extrapolations might not always generalize well across all settings. While Kumar et al. (2020a) do not concretely propose a causal link between the rank and performance of the system, one might be tempted to extrapolate the results (agents performing poorly when their rank collapsed) to the existence of such a causal 3 Preprint link, which herein we refer to as rank collapse hypothesis. In this work, we do a careful large-scale empirical analysis of this potential causal link using the oﬄine RL setting (also used by Kumar et al. (2020a)) and also the Tandem RL (Ostrovski et al., 2021) setting. The existence of this causal link would be beneﬁcial for oﬄine RL, as controlling the rank of the model could improve the performance (see the regularization term explored in Kumar et al. (2020a)) or as we investigate here, the eﬀective rank could be used for model selection in settings where oﬄine evaluation proves to be elusive. Key Observation 1: The rank and the performance are correlated in restricted settings, but that correlation disappears when we increase the range of hyperparameters and the models (Figure 1) a. aThis is because other factors like hyperparameters and architecture can confound the rank of the penultimate layer; unless the those factors are controlled carefully, the conclusions drawn from the experiments based on the rank can be misleading. Instead, we show that diﬀerent factors aﬀect a network’s rank without aﬀecting its performance. This ﬁnding indicates that unless all of these factors of variations are controlled – many of which we might still be unaware of – the rank alone might be a misleading indicator of performance. A deep Q network exhibits three phases during training. We show that rank can be used to identify diﬀerent stages of learning in Q-learning if the other factors are controlled carefully. We believe that our study, similar to others (e.g. Dinh et al., 2017), re-emphasizes the importance of critically judging our understanding of the behavior of neural networks based on simpliﬁed mathematical models or empirical evidence from a limited set of experiments. Key Observation 2: Deep Q-learning approaches goes through three phases of learning: i) simple behaviors, ii) complex behaviors, iii) under-parameterization (Figure 2). These phases can be identiﬁed by the eﬀective rank and performance on a given task a. aThe ﬁrst two phases of learning happen during the training of all models we tested. At times, the third phase of the learning could potentially lead to the agent losing its representation capacity and the ensuing poor performance. Figure 2: Lifespan of learning in deep Q-learning: The plot on the left-hand side illustrates the evolution of the eﬀective rank, and the plot on the right-hand side demonstrates the evolution of the performance during training. In the ﬁrst phase, the model learns easy-to-learn behaviors that are simplistic by nature and ignore many factors of environmental variations. The eﬀective rank collapses to a minimal value in the ﬁrst phase since the model does not need a large capacity to learn the simple behaviors. In phase 2, the model learns more complex behaviors that we identify as those that obtain large returns when the policy is evaluated in the environment. Typically, in supervised learning, phase 2 is followed by overﬁtting. However, in oﬄine RL (speciﬁcally the TD-learning approaches that we tried here), we observed that it is often followed by underﬁtting/under-parameterization in phase 3. 4 k n a R e v i t c e f f E s n r u t e R e d o s i p E Training Steps Training Steps Phases of Learning Phase 1: Simple Behaviors Phase 2: Complex Behaviors Phase 3: Underparametrization Preprint We organize the rest of the paper and our contributions in the order we introduce in the paper as follows: • Section 2 presents the related work, and Section 3 explains the experimental design that we rely on. • In Sections 4, 8, 6, 7 and 9, we study the extent of impact of diﬀerent interventions such as architectures, loss functions and optimization on the causal link between the rank and agent performance. Some interventions in the model, such as introducing an auxiliary loss (e.g. CURL (Laskin et al., 2020), SAM (Foret et al., 2021) or the activation function) can increase the eﬀective rank but does not necessarily improve the performance. This ﬁnding indicates that the rank of the penultimate layer is not enough to explain an agent’s performance. We also identify the settings where the rank strongly correlates with the performance, such as DQN with ReLU activation and many learning steps over a ﬁxed dataset. • In Section 4.1, we show that a deep Q network goes through three stages of learning, and those stages can be identiﬁed by using rank if the hyperparameters of the model are controlled carefully. • Section 5 describes the main outcomes of our investigations. Particularly, we analyze the impact of the interventions described earlier and provide counter-examples that help contradict the rank collapse hypothesis, establishing that the link between rank and performance can be aﬀected by several other confounding factors of variation. • In Section 11, we ablate and compare the robustness of BC and DQN models with respect to random perturbation introduced only when evaluating the agent in the environment. We found out that the oﬄine DQN agent is more robust than the behavior cloning agent which has higher eﬀective rank. • Section 12 presents the summary of our ﬁndings and its implications for oﬄine RL, along with potential future research directions. 2 Background 2.1 Eﬀective rank and implicit under-regularization The choice of architecture and optimizer can impose speciﬁc implicit biases that prefer certain solutions over others. The study of the impact of these implicit biases on the generalization of the neural networks is often referred to as implicit regularization. There is a plethora of literature studying diﬀerent sources of implicit regularization such as initialization of parameters (Glorot and Bengio, 2010; Li and Liang, 2018; He et al., 2015), architecture (Li et al., 2017; Huang et al., 2020), stochasticity (Keskar et al., 2016; Sagun et al., 2017), and optimization (Smith et al., 2021; Barrett and Dherin, 2020). The rank of the feature matrix of a neural network as a source of implicit regularization has been an active area of study, speciﬁcally in the context of supervised learning (Arora et al., 2019; Huh et al., 2021; Pennington and Worah, 2017; Sanyal et al., 2018; Daneshmand et al., 2020; Martin and Mahoney, 2021). In this work, we study the phenomenon of implicit regularization through the eﬀective rank of the last hidden layer of the network, which we formally deﬁne below. Deﬁnition: Eﬀective Rank Recently, Kumar et al. (2020a) studied the impact of the eﬀective rank on generalization in the general RL context. With a batch size of N and D units in the feature layer, they proposed the eﬀective rank formulation presented in Equation 1 for a feature matrix Φ ∈ RN ×D where N ≥ D that uses a threshold value of δ with the singular values σi(Φ) in descending order i.e. σ1(Φ) ≥ σ2(Φ) ≥ · · · . We provide the speciﬁc implementation of the eﬀective rank we used throughout this paper in Appendix A.11. eﬀective rankδ(Φ) = min k ( k : 5 Pk PD i=1 σi(Φ) j=1 σj(Φ) ) ≥ 1 − δ . (1) Preprint Terminology and Assumptions. Note that the threshold value δ throughout this work has been ﬁxed to 0.01 similar to Kumar et al. (2020a). Throughout this paper, we use the term eﬀective rank to describe the rank of the last hidden layer’s features only, unless stated otherwise. This choice is consistent with prior work (Kumar et al., 2020a) as the last layer acts as a representation bottleneck to the output layer. Kumar et al. (2020a) suggested that the eﬀective rank of the Deep RL models trained with TD-learning objectives collapses because of i) implicit regularization, happening due to repeated iterations over the dataset with gradient descent; ii) self-distillation eﬀect emerging due to bootstrapping losses. They supported their hypothesis with both theoretical analysis and empirical results. The theoretical analysis provided in their paper assume a simpliﬁed setting, with inﬁnitesimally small learning rates, batch gradient descent and linear networks, in line with most theory papers on the topic. 2.2 Auxiliary losses Additionally, we ablate the eﬀect of using an auxiliary loss on an agent’s performance and rank. Speciﬁcally, we chose the ""Contrastive Unsupervised Representations for Reinforcement Learning"" (CURL) (Laskin et al., 2020) loss that is designed for use in a contrastive self-supervised learning setting within standard RL algorithms: L(s, a, r, θ) = LQ(s, a, r, θ) + λ ∗ LCU RL(s, ˆs, θ). (2) Here, LQ refers to standard RL losses and the CURL loss LCU RL(s, ˆs, θ) is the contrastive loss between the features of the current observation s and a randomly augmented observation ˆs as described in Laskin et al. (2020). Besides this, we also ablate with Sharpness-Aware Minimization (SAM) (Foret et al., 2021), an approach that seeks parameters that have a uniformly low loss in its neighborhood, which leads to ﬂatter local minima. We chose SAM as a means to better understand whether the geometry of loss landscapes help inform the correlation between the eﬀective rank and agent performance in oﬄine RL algorithms. In our experiments, we focus on analyzing mostly the deep Q-learning (DQN) algorithm (Mnih et al., 2015) to simplify the experiments and facilitate deeper investigation into the rank collapse hypothesis. 2.3 Deep oﬄine RL Online RL requires interactions with an environment to learn using random exploration. However, online interactions with an environment can be unsafe and unethical in the real-world (Dulac-Arnold et al., 2019). Oﬄine RL methods do not suﬀer from this problem because they can leverage oﬄine data to learn policies that enable the application of RL in the real world (Menick et al., 2022; Shi et al., 2021; Konyushkova et al., 2021). Here, we focused on oﬄine RL due to its importance in real-world applications, and the previous works showed that the implicit regularization eﬀect is more pronounced in the oﬄine RL (Kumar et al., 2020a; 2021a). Some of the early examples of oﬄine RL algorithms are least-squares temporal diﬀerence methods (Bradtke and Barto, 1996; Lagoudakis and Parr, 2003) and ﬁtted Q-iteration (Ernst et al., 2005; Riedmiller, 2005). Recently, value-based approaches to oﬄine RL have been quite popular. Value-based approaches typically lower the value estimates for unseen state-action pairs, either through regularization (Kumar et al., 2020b) or uncertainty (Agarwal et al., 2020). One could also include R-BVE (Gulcehre et al., 2021) in this category, although it regularizes the Q function only on the rewarding transitions to prevent learning suboptimal policies. Similar to R-BVE, Mathieu et al. (2021) have also shown that the methods using single-step of policy improvement work well on tasks with very large action space and low state-action coverage. In this paper, due to their simplicity and popularity, we mainly study action value-based methods: oﬄine DQN (Agarwal et al., 2020) and oﬄine R2D2 (Gulcehre et al., 2021). Moreover, we also sparingly use Batched Constrained Deep Q-learning (BCQ) algorithm (Fujimoto et al., 2019), another popular oﬄine RL algorithm that uses the behavior policy to constrain the actions taken by the target network. Most oﬄine RL approaches we explained here rely on pessimistic value estimates (Jin et al., 2021; Xie et al., 2021; Gulcehre et al., 2021; Kumar et al., 2020b). Mainly because oﬄine RL datasets lack exhaustive exploration, and extrapolating the values to the states and actions not seen in the training set can result in 6 Preprint extrapolation errors which can be catastrophic with TD-learning (Fujimoto et al., 2018; Kumar et al., 2019). On the other hand, in online RL, it is a common practice to have inductive biases to keep optimistic value functions to encourage exploration (Machado et al., 2015). We also experiment with the tandem RL setting proposed by Ostrovski et al. (2021), which employs two independently initialized online (active) and oﬄine (passive) networks in a training loop where only the online agent explores and drives the data generation process. Both agents perform identical learning updates on the identical sequence of training batches in the same order. Tandem RL is a form of oﬄine RL. Still, unlike the traditional oﬄine RL setting on ﬁxed datasets, in the tandem RL, the behavior policy can change over time, which can make the learning non-stationary. We are interested in this setting because the agent does not necessarily reuse the same data repeatedly, which was pointed in Lyle et al. (2021) as a potential cause for the rank collapse. 3 Experimental setup To test and verify diﬀerent aspects of the rank collapse hypothesis and its potential impact on the agent performance, we ran a large number of experiments on bsuite (Osband et al., 2019), Atari (Bellemare et al., 2013) and DeepMind lab (Beattie et al., 2016) environments. In all these experiments, we use the experimental protocol, datasets and hyperparameters from Gulcehre et al. (2020) unless stated otherwise. We provide the details of architectures and their default hyperparameters in Appendix A.12. • bsuite – We run ablation experiments on bsuite in a fully oﬄine setting with the same oﬄine dataset as the one used in Gulcehre et al. (2021). We use a DQN agent (as a representative TD Learning algorithm) with multi-layer feed-forward networks to represent the value function. bsuite provides us with a small playground environment which lets us test certain hypotheses which are computationally prohibitive in other domains (for e.g.: computing Hessians) with respect to terminal features and an agent’s generalization performance. • Atari – To test whether some of our observations are also true with higher-dimensional input features such as images, we run experiments on the Atari dataset. Once again, we use an oﬄine DQN agent as a representative TD-learning algorithm with a convolutional network as a function approximator. On Atari, we conducted large-scale experiments on diﬀerent conﬁgurations: (a) Small-scale experiments: – DQN-256-2M: Oﬄine DQN on Atari with minibatch size 256 trained for 2M gradient steps with four diﬀerent learning rates: [3e − 5, 1e − 4, 3e − 4, 5e − 4]. We ran these experiments to observe if our observations hold in the default training scenario identiﬁed in RL Unplugged (Gulcehre et al., 2020). – DQN-32-100M: Oﬄine DQN on Atari with minibatch size of 32 trained for 100M gradient steps with three diﬀerent learning rates: (cid:2)3 × 10−5, 1 × 10−4, 3 × 10−4(cid:3). We ran those experiments to explore the eﬀects of reducing the minibatch size. (b) Large-scale experiments: – Long run learning rate sweep (DQN-256-20M): Oﬄine DQN trained for 20M gradients steps with 12 diﬀerent learning rates evenly spaced in log-space between 10−2 and 10−5 trained on minibatches of size 256. The purpose of these experiments is to explore the eﬀect of wide range of learning rates and longer training on the eﬀective rank. – Long run interventions (DQN-interventions): Oﬄine DQN trained for 20M gradient steps on minibatches of size 256 with 128 diﬀerent hyperparameter interventions on activation functions, dataset size, auxiliary losses etc. The purpose of these experiments is to understand the impact of such interventions on the eﬀective rank in the course of long training. • DeepMind Lab – While bsuite and Atari present relatively simple fully observable tasks that require no memory, DeepMind lab tasks (Gulcehre et al., 2021) are more complex, partially observable tasks where it is very diﬃcult to obtain good coverage in the dataset even after collecting billions of transitions. We speciﬁcally conduct our observational studies on the eﬀective ranks on the DeepMind Lab dataset, 7 Preprint which has data collected from a well-trained agent on the SeekAvoid level. The dataset is collected by adding diﬀerent levels of action exploration noise to a well-trained agent in order to get datasets with a larger coverage of the state-action space. Figure 3: Structural causal model (SCM) of diﬀerent factors that we test: M represents the model selection method that we use to determine the h which denotes the observed confounders that are chosen at the beginning of the training, including the task, the model architecture (including depth and number of units), learning rate and the number of gradient steps to train. β is the eﬀective rank of the penultimate layer. λ is the agent’s performance, measured as episodic returns the agent attains after evaluating in the environment. A represents the unobserved confounders that change during training but may aﬀect the performance, such as the number of dead units, the parameter norms, and the other underlying factors that can inﬂuence learning dynamics. We test the eﬀect of each factor by interventions. We illustrate the structural causal model (SCM) of the interactions between diﬀerent factors that we would like to test in Figure 3. To explore the relationship between the rank and the performance, we intervene on h, which represents potential exogenous sources of implicit regularization, such as architecture, dataset size, and the loss function, including the auxiliary losses. The interventions on h will result in a randomized controlled trial (RCT.) A represents the unobserved factors that might aﬀect performance denoted by λ an the eﬀective rank β such as activation norms and number of dead units. It is easy to justify the relationship between M, h, A and β. We argue that β is also confounded by A and h. We show the confounding eﬀect of A on β with our interventions to beta via auxiliary losses or architectural changes that increase the rank but do not aﬀect the performance. We aim to understand the nature of the relationship between these terms and whether SCM in the ﬁgure describes what we notice in our empirical exploration. We overload the term performance of the agent to refer to episodic returns attained by the agent when it is evaluated online in the environment. In stochastic environments and datasets with limited coverage, an oﬄine RL algorithm’s online evaluation performance and generalization abilities would correlate in most settings (Gulcehre et al., 2020; Kumar et al., 2021c). The oﬄine RL agents will need to generalize when they are evaluated in the environment because of: • Stochasticity in the initial conditions and transitions of the environment. For example, in the Atari case, the stochasticity arises from sticky actions and on DeepMind lab, it arises from the randomization of the initial positions of the lemons and apples. • Limited coverage: The coverage of the environment by the dataset is often limited. Thus, an agent is very likely to encounter states and actions that it has never seen during training. 4 Eﬀective rank and performance Based on the results of Kumar et al. (2020a), one might be tempted to extrapolate a positive causal link between the eﬀective rank of the last hidden layer and the agent’s performance measured as episodic returns 8 β: Eﬀective rank h: Observed confounders λ: Performance A: Hidden confounders M: Model selection method M h λ A β Preprint attained when evaluated in the environment. We explore this potentially interesting relationship on a larger scale by adopting a proof by contradiction approach. We evaluated the agents with the hyperparameter setup deﬁned for the Atari datasets in RL Unplugged (Gulcehre et al., 2020) and the hyperparameter sweep deﬁned for DeepMind Lab (Gulcehre et al., 2021). For a narrow set of hyperparameters this correlation exists as observed in Figure 1. However, in both cases, we notice that a broad hyperparameter sweep makes the correlation between performance and rank disappear (see Figures 1 and DeepMind lab Figure in Appendix A.3). In particular, we ﬁnd hyperparameter settings that lead to low (collapsed) ranks with high performance (on par with the best performance reported in the restricted hyperparameter range) and settings that lead to high ranks but poor performance. This shows that the correlation between eﬀective rank and performance can not be trusted for oﬄine policy selection. In the following sections, we further present speciﬁc ablations that help us understand the dependence of the eﬀective rank vs. performance correlation on speciﬁc hyperparameter interventions. 4.1 Lifespan of learning with deep Q-networks Empirically, we found eﬀective rank suﬃcient to identify three phases when training an oﬄine DQN agent with a ReLU activation function (Figure 2). Although the eﬀective rank may be suﬃcient to identify those stages, it still does not imply a direct causal link between the eﬀective rank and the performance as discussed in the following sections. Several other factors can confound eﬀective rank, making it less reliable as a guiding metric for oﬄine RL unless those confounders are carefully controlled. • Phase 1 (Simple behaviors): The eﬀective rank of the model ﬁrst collapses to a small value, in some cases to a single digit value, and then gradually starts to increase. In this phase, the model learns easy to learn behaviors that would have low performance when evaluated in the environment. We hypothesized that this could be due to the implicit bias of the SGD to learn functions of increasing complexity over training iterations (Kalimeris et al., 2019); therefore early in training, the network relies on simple behaviours that are myopic to most of the variation in the data. Hence the model has a low rank. The rank collapse early in the beginning of the training happens very abruptly, just in a handful of gradient updates the rank collapses to a single digit number. However, this early rank collapse does not degrade the performance of the agent. We call this rank collapse in the early in the training as self-pruning eﬀect. • Phase 2 (Complex behaviors): In this phase, the eﬀective rank of the model increases and then usually ﬂattens. The model starts to learn more complex behaviors that would achieve high returns when evaluated in the environment. • Phase 3 (Underﬁtting/Underparameterization): In this phase, the eﬀective rank collapses to a small value (often to 1) again, and the performance of the agent often collapses too. The third phase is called underﬁtting since the agent’s performance usually drops, and the eﬀective rank also collapses, which causes the agent to lose part of its capacity. This phase is not always observed (or the performance does not collapse with eﬀective ran towards the end of the training) in all settings as we demonstrate in our diﬀerent ablations. Typically in supervised learning, Phase 2 is followed by over-ﬁtting, but with oﬄine TD-learning, we could not ﬁnd any evidence of over-ﬁtting. We believe that this phase is primarily due to the target Q-network needing to extrapolate over the actions not seen during the training and causing extrapolation errors as described by (Kumar et al., 2019). A piece of evidence to support this hypothesis is presented in Figure 29 in Appendix A.5, which suggests that the eﬀective rank and the value error of the agent correlate well. In this phase, the low eﬀective rank and poor performance are caused by a large number of dead ReLU units. Shin and Karniadakis (2020) also show that as the network has an increasing number of dead units, it becomes under-parameterized and this could negatively inﬂuence the agent’s performance. It is possible to identify those three phases in many of the learning curves we provide in this paper, and our ﬁrst two phases agree with the works on SGD’s implicit bias of learning functions of increasing complexity (Kalimeris et al., 2019). Given a ﬁxed model and architecture, whether it is possible to observe all these three phases during training fundamentally depends on: 9 Preprint Figure 4: The three phases of learning: On IceHockey and MsPacman RL Unplugged Atari games we illustrate the diﬀerent phases of learning with the oﬄine DQN agent using the learning rate of 0.0004. The blue region in the plots identiﬁes the Phase 1, green region is the Phase 2, and red region is the Phase 3 of the learning. IceHockey is one of the games where the expert that generated the dataset performs quite poorly on it, thus the majority of the data is just random exploration data. It is easy to see that in this phase, the Oﬄine DQN performs very poorly and never manages to get out of the Phase 1. The performance of the agent on IceHockey is poor and the eﬀective rank of the network is low throughout the training. On MsPacman, we can observe all the three phases. The model transition into Phase 2 from Phase 1 quickly, and then followed by the under-ﬁtting regime where the eﬀective rank collapses the agent performs poorly. 1. Hyperparameters: The phases that an oﬄine RL algorithm would go through during training depends on hyperparameters such as learning rate and the early stopping or training budget. For example, due to early stopping the model may just stop in the second phase, if the learning rate is too small, since the parameters will move much slower the model may never get out of Phase 1. If the model is not large enough, it may never learn to transition from Phase 1 into Phase 2. 2. Data distribution: The data distribution has a very big inﬂuence on the phases the agent goes thorough, for example, if the dataset only contains random exploratory data and ORL method may fail to learn complex behaviors from that data and as a result, will never transition into Phase 2 from Phase 1. 3. Learning Paradigm: The learning algorithm, including optimizers and the loss function, can inﬂuence the phases the agent would go through during the training. For example, we observed that Phase 3 only happens with the oﬄine TD-learning approaches. It is possible to avoid phase three (underﬁtting) by ﬁnding the correct hyperparameters. We believe the third phase we observe might be due to non-stationarity in RL losses (Igl et al., 2020) due to bootstrapping or errors propagating through bootstrapped targets (Kumar et al., 2021b). The underﬁtting regime only appears if the network is trained long enough. The quality and the complexity of the data that an agent learns from also plays a key role in deciding which learning phases are observed during training. In Figure 4, we demonstrate the diﬀerent phases of learning on IceHockey and MsPacman games. On IceHockey, since the expert that generated the dataset has a poor performance on that game, the oﬄine DQN is stuck in Phase 1, and did not managed to learn complex behaviors that would push it to Phase 2 but on MsPacMan, all the three phases are present. We provide learning curves for all the online policy selection Atari games across twelve learning rates in Appendix A.8. 10 k n a R e v i t c e f f E 6 4 2 −2.5 −5 −7.5 −10 −12.5 n r u t e R e d o s p E i IceHockey MsPacman 60 40 20 0 5000 10000 IceHockey 15000 20000 0 0 Learner Steps (x1000) 2500 5000 10000 MsPacman 15000 20000 2000 1500 1000 500 0 0 5000 10000 15000 20000 0 5000 10000 15000 20000 Learner Steps (x1000) Phases of Learning Phase 1: Simple Behaviors Phase 2: Complex Behaviors Phase 3: Underparametrization Preprint Figure 5 shows the relationship between the eﬀective rank and twelve learning rates. In this ﬁgure, the eﬀect of learning rate on the diﬀerent phases of learning is distinguishable. For low learning rates, the ranks are low because the agent can never transition from Phase 1 to Phase 2, and for large learning rates, the eﬀective ranks are low because the agent is in Phase 3. Therefore, the distribution of eﬀective ranks and learning rates has a Gaussian-like shape, as depicted in the ﬁgure. The distribution of rank shifts towards low learning rates as we train the agents longer because slower models with low learning rates start entering Phase 2, and the models trained with large learning rates enter Phase 3. Figure 5: [Atari]: Bar charts of of eﬀective ranks with respect to the learning rates after 1M and 20M learning steps. After 1M gradient steps, the ranks are distributed almost like a Gaussian. After 20M learning steps the mode of the distribution of the ranks shifts towards left where the mode goes down for most games as well. Namely as we train the network longer the rank goes down and in particular for the large learning rates. For the low learning rates the rank is low because the model is stuck in Phase 1, the large learning rates get into the Phase 3 quickly and thus the low ranks. 4.2 The eﬀect of dataset size We can use the size of the dataset as a possible proxy metric for the coverage that the agent observes in the oﬄine data. We uniformly sampled diﬀerent proportions (from 5% of the transitions to the entire dataset) from the transitions in the RLUnplugged Atari benchmark dataset (Gulcehre et al., 2020) to understand how the agent behaves with diﬀerent amounts of training data and whether this is a factor aﬀecting the rank of the network. Figure 6 shows the evolution of rank and returns over the course of training. The eﬀective rank and performance collapse severely with low data proportions, such as when learning only on 5% of the entire 11 Preprint dataset subsampled. Those networks can never transition from phase 1 to phase 2. However, as the proportion of the dataset subsampled increases, the agents could learn more complex behaviors to get into phase 2. The eﬀective rank collapses less severely for the larger proportions of the dataset, and the agents tend to perform considerably better. In particular, we can see that in phase 1, an initial decrease of the rank correlates with an increase in performance, which we can speculate is due to the network reducing its reliance on spurious parts of the observations, leading to representations that generalize better across states. It is worth noting that the ordering of the policies obtained by using the agents’ performance does not correspond to the ordering of the policies with respect to the eﬀective rank throughout training. For example, oﬄine DQN trained on the full dataset performs better than 50% of the dataset, while the agent trained using 50% of the data sometimes has a higher rank. A similar observation can be made for Zaxxon, at the end of the training, the network trained on the full dataset underperforms compared to the one trained on 50% of the data, even if the rank is the same or higher. Figure 6: [Atari] Dataset size: Evolution of ranks and returns as we vary the fraction of data available for the agent to train on. We see that the agent which sees very little data collapses both in terms of rank and performance. The agent which sees more of the data has good performance even while allowing some shrinkage of rank during the training. 5 Interactions between rank and performance To understand the interactions and eﬀects of diﬀerent factors on the rank and performance of oﬄine DQN, we did several experiments and tested the eﬀect of diﬀerent hyperparameters on eﬀective rank and the agent’s performance. Figure 3 shows the causal graph of the eﬀective rank, its confounders, and the agent’s performance. Ideally, we would like to intervene in each node on this graph to measure its eﬀect. As the rank is a continuous random variable, it is not possible to directly intervene on eﬀective rank. Instead, we emulate interventions on the eﬀective rank by conditioning to threshold β with τ . In the control case (λ(1)), we assume β > τ and for λ(0), we would have β ≤ τ . We can write the average treatment eﬀect (ATE) for the eﬀect of setting the eﬀective rank to a large value on the performance as: ATE(λ, β, τ ) = E[λ|λ(1)] − E[λ|λ(0)]. (3) Let us note that this ATE(λ, β, τ ) quantity doesn’t necessarily measure the causal eﬀect of β on λ, since we know that the λ can be confounded by A. We study the impact of factors such as activation function, learning rate, data split, target update steps, and CURL and SAM losses on the Asterix and Gravitar levels. We chose these two games since Asterix is an 12 Preprint i) ReLU ii) tanh iii) both Figure 7: [Atari]: The correlation plot between the eﬀective rank and the performance (measured in terms of episode returns by evaluating the agent in the environment) of oﬄine DQN on Asterix and Gravitar games over 256 diﬀerent hyper-parameter conﬁgurations trained for 2M learning steps. There is a strong correlation with the ReLU function, but the correlation disappears for the network with tanh activation function. There is no signiﬁcant correlation between eﬀective rank and the performance on the Asterix game with the complete data. Still, a positive correlation exists on the Gravitar game. These results are not aﬀected by the Simpson’s paradox since the subgroups of the data when split into groups concerning activation functions, do not show a consistent correlation. easy-to-explore Atari game with relatively dense rewards, while Gravitar is a hard-to-explore and a sparse reward setting (Bellemare et al., 2016). In Table 1, we present the results of intervening to β thresholded with diﬀerent quantiles of the eﬀective rank. Choosing a network with a high eﬀective rank for a ReLU network has a statistically signiﬁcant positive eﬀect on the agent’s performance concerning diﬀerent quantiles on both Asterix and Gravitar. The agent’s performance is measured in terms of normalized scores as described in Gulcehre et al. (2020). However, the results with the tanh activation function are mixed, and the eﬀect of changing the rank does not have a statistically signiﬁcant impact on the performance in most cases. In Figure 7, we show the correlations between the rank and performance of the agent. The network with ReLU activation strongly correlates rank and performance, whereas tanh does not. The experimental data can be prone to Simpson’s paradox which implies that a trend (correlation in this case) may appear in several subgroups of the data, but it disappears or reverses when the groups are aggregated (Simpson, 1951). This can lead to misleading conclusions. We divided our data into equal-sized subgroups for hyperparameters and model variants, but we could not ﬁnd a consistent trend that disappeared when combined, as in Figure 7. Thus, our experiments do not appear to be aﬀected by Simpson’s paradox. 13 Preprint Table 1: Atari: The average treatment eﬀect of having a network with a higher rank than concerning diﬀerent quantiles. We report the average treatment eﬀect (ATE), its uncertainty (using 95% conﬁdence intervals using standard errors), and p-values for Asterix and Gravitar games. Higher eﬀective rank seems to have a smaller eﬀect on Asterix than Gravitar. Let us note that, Gravitar is a sparse reward problem, and Asterix is a dense-reward one. Overall the eﬀect is more prominent for ReLU than tanh, and with tanh networks, it is not statistically signiﬁcant. We boldfaced the ATEs where the eﬀect is statistically signiﬁcant (p < 0.05.) The type column of the table indicates the activation functions used in the experiments we did the intervention. The “Combined” type corresponds to a combination of tanh and ReLU experiments. Level Type Asterix Gravitar Combined ReLU Tanh Combined ReLU Tanh quantile=0.25 ATE 0.019 ± 0.010 0.079 ± 0.018 -0.014 ± 0.004 0.954 ± 0.077 1.654 ± 0.150 0.028 ± 0.090 quantile=0.5 quantile=0.75 quantile=0.95 p-value ATE 0.001 0.000 0.999 0.000 0.000 0.303 0.007 ± 0.013 0.089 ± 0.025 0.009 ± 0.005 0.569 ± 0.098 1.146 ± 0.163 0.185 ± 0.118 p-value ATE 0.207 0.000 0.996 0.000 0.000 0.005 0.005 ± 0.019 0.112 ± 0.040 -0.005 ± 0.006 0.496 ± 0.141 1.105 ± 0.256 0.242 ± 0.167 p-value ATE 0.324 0.000 0.886 0.000 0.000 0.009 -0.031 ± 0.011 0.110 ± 0.085 0.020 ± 0.017 0.412 ± 0.206 1.688 ± 0.564 0.054 ± 0.265 p-value 0.999 0.017 0.023 0.001 0.000 0.367 Table 2: [Atari] Average Treatment Eﬀect of diﬀerent interventions on Asterix and Gravitar: Quantity of interest Y is the terminal rank of the features. Activation function and learning rate have the most considerable eﬀect on the terminal feature ranks in our setup. We indicate them with boldface; changing the activation function from ReLU to tanh improves the eﬀective rank, whereas changing the learning rate from 3 × 10−4 to 3 × 10−5 reduces the rank. u Activation Function Learning Rate Target Update Steps SAM Loss Weight Level CURL Loss Weight Data Split Control (u) ReLU 3 × 10−4 2500 0 Asterix 0 100% Treatment T(u) tanh 3 × 10−5 200 0.05 Gravitar 0.001 5% Yt(u) − Yc(u) 66.97 ± 7.13 -54.15 ± 7.54 -15.23 ± 8.21 -10.05 ± 8.26 -9.41 ± 8.25 8.09 ± 8.27 5.11 ± 8.27 In this analysis, we set our control setting to be trained with ReLU activations, the learning rate of 3e − 4, without any auxiliary losses, with training done on the full dataset in the level Asterix. We present the Average Treatment Eﬀect (ATE) (the diﬀerence in ranks between the intervention being present and absent in an experiment) of changing each of these variables in Table 3. We ﬁnd that changing the activation function from ReLU to tanh drastically aﬀects the eﬀective ranks of the features, which is in sync with our earlier observations on other levels. In Figure 8, we present a heatmap of ATEs where we demonstrate the eﬀective rank change when two variables are changed from original control simultaneously. Once again, the activation functions and learning rate signiﬁcantly aﬀect the terminal ranks. We also observe some interesting combinations that lead the model to converge to a lower rank – for example, using SAM loss with dropout. These observations further reinforce our belief that diﬀerent factors aﬀect the phenomenon. Figure 8: [Atari] ablations: Eﬀects of diﬀerent pairs of interventions on the control set. The control sam- ple is on level Asterix with no dropout, curl and loss smoothing with the full dataset and a target update period of 2500 steps. Overall, changing the activation function and learning rate has the largest eﬀect on the rank. 14 Activation(tanh) 186.1 Learning Rate(3e-5) 94.9 -74.9 SAM Weight(0.05) 71.5 77.3 91.8 CURL Weight(0.001) 165.1 -78.8 -27.1 -50.1 Dropout Rate(0.5) 123.9 -67.0 109.3 -64.6 70.8 Dataset Size(100%) 138.3 -80.1 55.7 -47.1 99.3 -49.5 Target Update Steps(200) 70.2 -78.1 55.3 -70.5 57.5 -2.4 -40.3 Level Name(Gravitar) 30.3 -68.6 -64.0 46.7 -35.9 -61.5 -78.3 -59.7 150 100 50 0 −50 ) h n a t ( n o i t a v i t c A ) 5 - e 3 ( e t a R g n n r a e L i ) 5 0 . 0 ( t h g e W M A S i ) 1 0 0 . 0 ( t h g e W L R U C i ) 5 . 0 ( e t a R t u o p o r D ) % 0 0 1 ( e z S i t e s a t a D ) r a t i v a r G ( e m a N l e v e L ) 0 0 2 ( s p e t S e t a d p U t e g r a T Preprint The extent of rank collapse and mitigating rank collapse alone may never fully ﬁx the agent’s learning ability in diﬀerent environments. 6 The eﬀect of activation functions The severe rank collapse of phase 3 is apparent in our Atari models, which have simple convolutional neural networks with ReLU activation functions. When we study the same phenomenon on the SeekAvoid dataset, rank collapse does not seem to happen similarly. It is important to note here that to solve those tasks eﬀectively, the agent needs memory; hence, all networks have a recurrent core and LSTM. Since standard LSTMs use tanh(·) activations, investigating in this setting would help us understand the role of the choice of architecture on the behavior of the model’s rank. Figure 10 shows that the output features of the LSTM network on the DeepMind lab dataset do not experience any detrimental eﬀective rank collapse with diﬀerent exploration noise when we use tanh(·) activation function for the cell. However, if we replace the tanh(·) activation function of the LSTM cell with ReLU or if we replace the LSTM with a feed-forward MLP using ReLU activations (as seen in Figure 10) the eﬀective rank in both cases, collapses to a small value at the end of training. This behavior shows that the choice of activation function has a considerable eﬀect on whether the model’s rank collapses throughout training and, subsequently, its ability to learn expressive value functions and policies in the environment as it is susceptible to enter phase 3 of training. Observation: Agents that have networks with ReLU units tend to have dead units which causes the eﬀective rank to collapse in phase 3 of learning while other activations like tanh do not suﬀer a similar collapse. The activation functions inﬂuence both the network’s learning dynamics and performance. As noted by Pennington and Worah (2017), the activation function can inﬂuence the rank of each layer at initialization. Figure 9 presents our ﬁndings on bsuite levels. In general, the eﬀective rank of the penultimate layer with ReLU activations collapses faster, while ELU and tanh(·) tend to maintain a relatively higher rank than ReLU. As the eﬀective rank goes down for the catch environment, the activations become sparser, and the units die. We illustrate the sparsity of the activations with Gram matrices over the features of the last hidden layer of a feedforward network trained on bsuite in Figure 37 in Appendix A.14. Figure 9: [bsuite] Eﬀective ranks of diﬀerent activation functions: The magnitude of drop in eﬀective rank is more severe for ReLU and sigmoid activation functions than tanh. 7 Optimization The inﬂuence of minibatch size and the learning rate on the learning dynamics is a well-studied phenomenon. Smith and Le (2017); Smith et al. (2021) argued that the ratio between the minibatch size and learning rate relates to implicit regularization of SGD and also aﬀects the learning dynamics. Thus, it is evident that those factors would impact the eﬀective rank and performance. Here, we focus on a setup where we see the correlation between eﬀective rank and performance and investigate how it emerges. 15 Preprint Figure 10: [DeepMind lab SeekAvoid] Activation functions: Evolution of ranks in a typical LSTM network with tanh activations at the cell of LSTM on DeepMind lab. We see that a typical LSTM network does not get into Phase 3. However, when we change the activation function of the cell gate from tanh to ReLU the eﬀective rank collapses to very small values. The eﬀective rank collapses when the LSTM is replaced with a feedforward network too in cases of ReLU activation. Our analysis and ablations in Table 3 and Figure 8 illustrate that the learning rate is one of the most prominent factors on the performance of the agent. In general, there is no strong and consistent correlation between the eﬀective rank and the performance across diﬀerent models and hyperparameter settings. However, in Figure 1, we showed that the correlation exists in a speciﬁc setting with a particular minibatch size and learning rate. Further, in Figure 7, we narrowed down that the correlation between the eﬀective rank and the performance exists for the oﬄine DQN with ReLU activation functions. Thus in this section, we focus on a regime where this correlation exists with the oﬄine DQN with ReLU and investigate how the learning rate and minibatches aﬀect the rank. We ran several experiments to explore the relationship between the minibatch size, learning rates, and rank. In this section, we report results on Atari in few diﬀerent settings – Atari-DQN-256-2M, Atari-DQN-32-100M, Atari-DQN-256-20M and Atari-BC-256-2M. The dying ReLU problem is a well-studied issue in supervised learning (Glorot et al., 2011; Gulcehre et al., 2016) where due to the high learning rates or unstable learning dynamics, the ReLU units can get stuck in the zero regime of the ReLU activation function. We compute the number of dead ReLU units of the penultimate layer of a network as the number of units with zero activation value for all inputs. Increasing the learning rate increases the number of dead units as can be seen in Figures 11 and 12. Even in BC, we observed that using large learning rates can cause dead ReLU units, rank collapse, and poor performance, as can be seen in Figure 13, and hence this behavior is not unique to oﬄine RL losses. Nevertheless, models with TD-learning losses have catastrophic rank collapses and many dead units with lower learning rates than BC. Let us note that the eﬀective rank depends on the number of units (D) and the number of dead units (η) at a layer. It is easy to see that eﬀective rank is upper-bounded by D − η. In Figure 14, we observe a strong correlation between the number of dead ReLU units of the penultimate layer of the ReLU network and its eﬀective rank. Observation: The pace of learning inﬂuences the number of dead units and the eﬀective rank: the larger the learning rates and the smaller minibatches are, the number of dead units increases, and the eﬀective rank decreases. However, the performance is only poor when the eﬀective rank is severely low. Finally, we look into how eﬀective ranks shape up towards the end of training. We test 12 diﬀerent learning rates, trying to understand the interaction between the learning rates and the eﬀective rank of the representations. We summarize our main results in Figure 5 where training oﬄine DQN decreases the 16 Preprint Figure 11: [Atari DQN-32-100M] Learning curves of oﬄine DQN for 100M gradient steps of training with minibatch size of 32. Increasing the learning rate increases the number of dead units in the ReLU network. As a result, the increased learning rate also causes more severe collapse as well, which aligns very well with the number of dead units. We observed that this behavior can also happen with networks using saturating activation functions such as sigmoid. eﬀective rank. The eﬀective rank is low for both small and large learning rates. For higher learning rates, as we have seen earlier, training for longer leads to many dead ReLU units, which in turn causes the eﬀective rank to diminish, as seen in Figures 11 and 12. Moreover, as seen in those ﬁgures, in Phase 1, the eﬀective rank and the number of dead units are low. Thus, the rank collapse in Phase 1 is not caused by the number of dead units. In Phase 3, the number of dead units is high, but the eﬀective rank is drastically low. The drastically low eﬀective rank is caused by the network’s large number of dead units in Phase 3. In Phase 3, we believe that both the under-parameterization and the poor performance is caused by the number of dead units in ReLU DQN, which was shown to be the case by (Shin and Karniadakis, 2020) in supervised learning. Overall, we could only observe a high correlation between the eﬀective rank and the agent’s performance, when we use ReLU activations in the network after a long training. We also present more analysis on how controlling the loss landscape aﬀects the rank vs performance in Appendix A.2 and A.4. 8 The eﬀect of loss function 8.1 Q-learning and behavior cloning Behavior cloning (BC) is a method to learn the behavior policy from an oﬄine dataset using supervised learning approaches (Pomerleau, 1989). Policies learned by BC will learn to mimic the behavior policy, and thus the performance of the learned BC agent is highly limited by the quality of the data the agent is trained on. We compare BC and Q-learning in Figure 15. We conﬁrm that with default hyperparameters, the BC agent’s eﬀective rank does not collapse at the end of training. In contrast, as shown by Kumar et al. (2020a), 17 Preprint Figure 12: [Atari DQN-256-3M] Learning curves of oﬄine DQN for 3M gradient steps of training with minibatch size of 256. Increasing the minibatch size improves the performance of the network with larger learning rates. DQN’s eﬀective rank collapses. DQN outperforms BC even if its rank is considerably lower, and during learning, the rank is not predictive of the performance (Figure 15). This behavior indicates the unreliability of eﬀective rank for oﬄine model selection. 8.2 CURL: The eﬀect of self-supervision We study whether adding an auxiliary loss term proposed for CURL (Laskin et al., 2020) (Equation 2) during training helps the model mitigate rank collapse. In all our Atari experiments, we use the CURL loss described in (Laskin et al., 2020) without any modiﬁcations. Since the DeepMind lab tasks require memory, we apply a similar CURL loss to the features aggregated with a mean of the states over all timesteps. In all experiments, we also sweep over the weight of the CURL loss λ. Figure 16 shows the ranks and returns on Atari (for DeepMind lab results see Appendix A.6.) In Atari games, using very large weights for an auxiliary loss (≈ 1) prevents rank collapse, but they simultaneously deteriorate the agent performance. We speculate, borrowing on intuitions from the supervised learning literature on the role of the rank as implicit regularizer Huh et al. (2021), that in such scenarios large rank prevents the network from ignoring spurious parts of the observations, which aﬀects its ability to generalize. On the other hand, moderate weights of CURL auxiliary loss do not signiﬁcantly change the rank and performance of the agent. Previously Agarwal et al. (2021) showed that the CURL loss does not improve the performance of an RL agent on Atari in a statistically meaningful way. On DeepMind lab games, we do not observe any rank collapse. In none of our DeepMind lab experiments agents enter into Phase 3 after Phase 1 and 2. This is due to the use of the tanh activation function for LSTM based on our investigation of the role of the activation functions in Section 6. 18 Preprint Figure 13: [Atari BC-256-2M] Learning Curves of BC After 2M gradient steps with mini-batch size of 256. For large enough learning rates the rank of BC agent also collapses. We hypothesize that the rank collapse is a side eﬀect of learning in general and not only due to TD-learning based losses. Figure 14: [Atari] Scatter plot for the correlation between the number of dead units and eﬀective rank at the end of training and we observe that the eﬀective rank strongly correlates with the number of dead units. Also, using larger learning rate increases the number of dead units in the network. 9 Tandem RL Kumar et al. (2020a) propose one possible hypothesis for the observed rank collapse as the re-use of the same transition samples multiple times, particularly prevalent in the oﬄine RL setting. A setting in which this hypothesis can be tested directly is the ‘Tandem RL’ proposed in Ostrovski et al. (2021): here a secondary (‘passive’) agent is trained from the data stream generated by an architecturally equivalent, independently initialized baseline agent, which itself is trained in a regular, online RL fashion. The passive agent tends to under-perform the active agent, despite identical architecture, learning algorithm, and data stream. This 19 Preprint Figure 15: [Atari] Oﬄine DQN and Behavior Cloning (BC): We compare BC and DQN agents on the Atari dataset. We used the same architecture, dataset, and training protocols for both baselines. We used the hyperparameters deﬁned in (Gulcehre et al., 2020) for comparisons. The rank of the DQN agent is signiﬁcantly lower and achieves higher returns than BC. Figure 16: [Atari] Auxiliary losses: Evolution of ranks as we increase the weight of auxiliary losses. We see that a strong weight for auxiliary loss helps mitigate the rank collapse but prevents the model from learning useful representations. setup presents a clean ablation setting in which both agents use data in the same way (in particular, not diﬀering in their re-use of data samples), and so any diﬀerence in performance or rank of their representation cannot be directly attributable to the reuse of data. In Figure 17, we summarize the results of a Tandem-DQN using Adam (Kingma and Ba, 2014) and RMSProp (Tieleman et al., 2012) optimizers. Despite the passive agent reusing data in a similar fashion as the online agent, we observe that it collapses to a lower rank. Besides, the passive agent’s performance tends to be signiﬁcantly (in most cases, catastrophically) worse than the online agent that could not satisfactorily explained just by the extent of diﬀerence in their eﬀective ranks alone. We think that the Q-learning 20 Preprint Figure 17: Atari, tandem RL: We investigate the eﬀect of the choice of optimizers between Adam and RMSProp on rank and the performance of the models, both for the active (solid lines) and passive agents (dashed lines). We observe the rank of the passive agent is lower than the active agent both for Adam and RMSProp. Figure 18: Atari, Forked tandem RL: We evaluate diﬀerent loss functions in the forked tandem setting, where the passive agent is forked from the online agent, and the online agent’s parameters are frozen. Still, the parameters of the online agent are updated. We forked the passive agent after it had seen 50M frames during training which is denoted with dashed lines in the ﬁgure. We observe that using both BVE and MC return losses improves the agent’s rank, but the agent’s performance is still poor. algorithm seems to be not eﬃcient enough to exploit the data generated by the active agent to learn complex behaviors that would put the passive agent into Phase 2. We also noticed that, although Adam achieves better performance than RMSProp, the rank of the model trained with the Adam optimizer tends to be lower than the models trained with RMSProp. In Figure 18, we investigate the eﬀect of diﬀerent learning algorithms on the rank and the performance of an agent in the forked tandem RL setting. In the forked tandem setting, an agent is ﬁrstly trained for a fraction of its total training time. Then, the agent is ‘forked’ into active and passive agents, and they both start with the same network weights where the active agent is ‘frozen’ and not trained but continues to generate data from its policy to train the passive agent on this generated data for the remaining of the training time. Here, we see that the rank ﬂat-lines when we fork the Q-learning agent, but the performance collapses dramatically. In contrast, despite the rank of BVE and an agent trained with on-policy Monte Carlo returns going up, the 21 breakout pong i ) e v s s a p ( s k n a R l e n r e K e r u t a e F 200 150 100 50 0 200 100 0 0 0 50 100 150 200 200 150 100 50 0 seaquest space_invaders 200 100 Algorithm BVE Monte-Carlo Q-learning 50 100 150 200 0 0 50 100 150 200 50 100 150 200 0 Frames (x 1M) breakout pong seaquest space_invaders i ) e v s s a p ( n r u t e R e d o s p E i 400 300 200 100 0 20 10 0 −10 −20 20000 15000 10000 5000 0 0 50 100 150 200 0 50 100 150 200 0 Frames (x 1M) 1500 1000 500 Algorithm BVE Monte-Carlo Q-learning 50 100 150 200 0 0 50 100 150 200 Preprint performance still drops. Nevertheless, the decline of performance for BVE and the agent trained with Monte Carlo returns is not as bad as the Q-learning agent on most Atari games. 10 Oﬄine policy selection In Section 7, we found that with the large learning rate sweep setting rank and number of dead units have high correlation with the performance. Oﬄine policy selection (Paine et al., 2020) aims to choose the best policy without any online interactions purely from oﬄine data. The apparent correlation between the rank and performance raises the natural question of how well rank performs as an oﬄine policy selection approach. We did this analysis on DQN-256-20M experiments where we previously observed strong correlation between the rank and performance. We ran the DQN network described in DQN-256-20M experimental setting until the end of the training and performed oﬄine policy selection using the eﬀective rank to select the best learning rate based on the learning rate that yields to maximum eﬀective rank or minimum number of dead units. Figure 19: Atari DQN-256-20M: This plot is depicting the simple regret of a DQN agent with ReLU activations using eﬀective rank and the number of dead units. On the left, we show the the simple regret to select the best learning rate using the eﬀective rank and on the right hand side, we show the simple regret achieved based on the number of dead units. Figure 19 illustrates the simple regret on each Atari oﬄine policy selection game. The simple regret between the recommended policy measures how close an agent is to the best policy, and it is computed as described in Konyushkova et al. (2021). A simple regret of 1 would mean that our oﬄine policy selection mechanism successfully chooses the best policy, and 0 would mean that it would choose the worst policy. After 2M training steps, the simple regret with the number of dead units as a policy selection method is poor. In contrast, the simple regret achieved by selecting the agent with the highest eﬀective rank is good. The mean simple regret achieved by using the number of dead units as an oﬄine policy selection (OPS) method is 0.45 ± 0.11, where the uncertainty ±0.11 is computed as standard error across ﬁve seeds. In contrast, simple regret achieved by using eﬀective rank as an OPS method is 0.24 ± 0.12. The eﬀective ranks for most learning rates collapse as we train longer since more models enter Phase 3 of learning—the number of dead units increases. After 20M of learning steps, the mean simple regret computed using eﬀective rank as the OPS method becomes 0.40 ± 0.07, and the mean simple regret is 0.25 ± 0.12 with the number of dead units for the OPS. Let us note that the 2M learning step is more typical in training agents on the RL Unplugged Atari dataset. The number of dead units becomes a good metric to do OPS when the network is trained for long (20M steps in our experiment), where the rank becomes drastically small for most learning rate conﬁgurations. However, eﬀective rank seems to be a good metric for the OPS earlier in training. Nevertheless, without prior information about the task and agent, it is challenging to conclude whether the number of dead units or eﬀective rank would be an appropriate OPS metric. Other factors such as the number of training steps, activation functions, and other hyperparameters confound the eﬀective rank and number of dead units. Thus, we believe those two metrics we tested are unreliable in general OPS settings without controlling those other extraneous factors. 22 t e r g e R e p m S l i Effective rank Number of dead units 1.000 20M steps 2M steps 0.791 0.838 0.665 0.588 0.545 0.503 0.504 0.457 0.569 0.524 0.359 0.309 0.665 0.539 0.393 0.411 0.423 0.621 0.542 0.446 0.190 0.079 k c a t t A n o m e D 0.000 y e k c o H e c I r e n n u R d a o R 0.021 n o x x a Z i r e d R m a e B n a m c a P s M 0.009 n a y o o P 0.035 0.000 k n a t o b o R k n u D e b u o D l 0.000 0.000 0.026 k c a t t A n o m e D y e k c o H e c I r e n n u R d a o R 0.000 0.000 n o x x a Z 0.000 i r e d R m a e B 0.075 n a m c a P s M n a y o o P 0.000 k n a t o b o R k n u D e b u o D l Preprint In Figure 20, we compare eﬀective rank as an OPS method with respect to its percentage improvement over the online policy selection to maximize the me- dian reward achieved by the each agent across nine Atari games. The eﬀective rank on Zaxxon, Beam- Rider, MsPacman, Pooyan, Robotank, DoubleDunk perform competitively to the online policy selection. Eﬀective rank may be a complementary tool for net- works with ReLU activation function and we believe it can be a useful metric to monitor during the train- ing in addition to number of dead units to have a better picture about the performance of the agent. 11 Robustness to input perturbations Figure 20: Atari DQN-256-20M: Here, we are de- picting the percentage of improvement by doing oﬄine policy selection using rank individually vs online policy selection using median reward across nine Atari games to select the best learning rate. Using eﬀective rank as the oﬄine policy selection method performs relatively well when compared to doing online policy selection based on median normalized score across Atari games. Several works, such as Sanyal et al. (2018) suggested that low-rank models can be more robust to input perturbations (speciﬁcally adversarial ones). It is diﬃcult to just measure the eﬀect of low-rank rep- resentations on the agent’s performance since rank itself is not a robust measure, as it depends on diﬀer- ent factors such as activation function and learning which in turn can eﬀect the generalization of the algorithm independently from rank. However, it is easier to validate the antithesis, namely “Do more robust agents need to have higher eﬀective rank than less robust agents?”. We can easily test this hypothesis by comparing DQN and BC agents. Robustness metric: We deﬁne the robustness metric ρ(p) based on three variables: i) the noise level p, ii) the noise distribution d(p), and iii) the score obtained by agent when evaluated in the environment with noise level p: score[d(p)] for which score[d(0)] represents the average score achieved by the agent without any perturbation applied on it. Then we can deﬁne ρ(p) as follows: ρ(p) = 1 − score[d(0)] − score[d(p)] score[d(0)] = score[d(p)] score[d(0)] . (4) In our experiments, we compare DQN and BC agents since we already know that BC has much larger ranks across all Atari games than DQN. We trained these agents on datasets without any data augmentation. The data augmentations are only applied on the inputs when the agent is evaluated in the environment. We evaluate our agents on BeamRider, IceHockey, MsPacman, DemonAttack, Robotank and RoadRunner games from RL Unplugged Atari online policy selection games. We excluded DoubleDunk, Zaxxon and Pooyan games since BC agent’s performance on those levels was poor and very close to the performance of random agent, so the robustness metrics on those games would not be very meaningful. On IceHockey, the results can be negative, thus we shifted the scores by 20 to make sure that they are non-negative. In our robustness experiments, we used the hyperparameters that were used in Gulcehre et al. (2020) on Atari for both for DQN and BC. In Figure 21, we investigate the Robustness of DQN and BC agents with Robustness to random shifts: respect to random translation image perturbations applied to the observations as described by Yarats et al. (2020). We evaluate the agent on the Atari environment with varying degrees of input scaling. We noticed that BC’s performance deteriorates abruptly, whereas the performance of DQN, while also decreasing, is better than the BC one. 23 0 8 0 6 74.074 t n e m e v o r p m I f o e g a t n e c r e P 0 4 0 2 0 0 2 − 4.159 4.450 -3.332 -7.086 -18.605 -17.358 0 4 − 0 6 − -54.037 -49.712 k c a t t A n o m e D y e k c o H e c I r e n n u R d a o R n o x x a Z i r e d R m a e B n a m c a P s M n a y o o P k n a t o b o R k n u D e b u o D l Preprint Figure 21: [Atari] Robustness to random shifts during evaluation: We measure the robustness of BC and DQN to random shifts on six Atari games from RL Unplugged. The DQN and BC agents are trained oﬄine without any perturbations on the inputs, only on the RL Unplugged datasets. However, during evaluation time we perturbed the input images with ""random shift"" data augmentation. Overall, DQN is more robust than BC to the evaluation-time random-scaling image perturbations that are not seen during training. The diﬀerence is more pronounced on IceHockey and BeamRider games. DQN achieved mean AUC (over diﬀerent games) of 2.042 and BC got 1.26. Figure 22: [Atari] Robustness to random scaling during evaluation: We measure the robustness of BC and DQN to random scaling over inputs on six Atari games from RL Unplugged. The DQN and BC agents are trained oﬄine without data augmentations over the RL Unplugged datasets. However, we perturbed the observations during the evaluation with ""random scale"" data augmentation. We observed that DQN is more robust than BC to the evaluation-time random-scaling data augmentation. The diﬀerence is more pronounced in IceHockey, BeamRider, and Robotank games. DQN achieved a mean AUC (over diﬀerent games) of 1.60, and BC achieved 0.87. 24 e r o c s s s e n t s u b o R 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 MsPacman DemonAttack IceHockey 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 Robotank BeamRider RoadRunner 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 DQN BC 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 The random shift rate 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 e r o c s s s e n t s u b o R MsPacman DemonAttack IceHockey 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.00 0.02 0.04 0.06 0.08 0.10 0.00 0.02 0.04 0.06 0.08 0.10 0.00 0.02 0.04 0.06 0.08 0.10 Robotank BeamRider RoadRunner 1.0 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.02 0.04 0.06 0.08 0.10 0.0 0.00 0.02 0.04 0.06 0.08 0.10 The random scale value DQN BC 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.02 0.04 0.06 0.08 0.10 Preprint Robustness to random scaling: In Figure 22, we explore the Robustness of DQN and BC agents with respect to random scaling as image perturbation method applied to the observations that are feed into our deep Q-network. We randomly scale the image inputs as described in Chen et al. (2017). When evaluating the agent in an online environment, we test it with varying levels of image scaling. The results are again very similar to the random shift experiments where the DQN agent is more robust to random perturbations in the online environment. According to the empirical results obtained from those two experiments, it is possible to see that DQN is more robust to the evaluation-time input perturbations, speciﬁcally random shifts and scaling than the BC agent. Thus, more robust representations do not necessarily require a higher eﬀective rank. 12 Discussion In this work, we empirically investigated the previously hypothesized connection between low eﬀective rank and poor performance. We found that the relationship between eﬀective rank and performance is not as simple as previously conjectured. We discovered that an oﬄine RL agent trained with Q-learning during training goes through three phases. The eﬀective rank collapses to severely low values in the ﬁrst phase –we call this as the self-pruning phase– and the agent starts to learn basic behaviors from the dataset. Then in the second phase, the eﬀective rank starts going up, and in the third phase, the eﬀective rank collapses again. Several factors such as learning rate, activation functions and the number of training steps, inﬂuence the occurrence, persistence and the extent of severity of the three phases of learning. In general, a low rank is not always indicative of poor performance. Besides strong empirical evidence, we propose a hypothesis trying to explain the underlying phenomenon: not all features are useful for the task the neural network is trying to solve, and low rank might correlate with more robust internal representations that can lead to better generalization. Unfortunately, reasoning about what it means for the rank to be too low is hard in general, as the rank is agnostic to which direction of variations in the data are being ignored or to higher-order terms that hint towards a more compact representation of the data with fewer dimensions. Our results indicate that an agent’s eﬀective rank and performance correlate in restricted settings, such as ReLU activation functions, Q-learning, and a ﬁxed architecture. However, as we showed in our experiments, this correlation is primarily spurious in other settings since it disappears with simple modiﬁcations such as changing the activation function and the learning rate. We found several ways to improve the eﬀective rank of the agent without improving the performance, such as using tanh instead of ReLU, an auxiliary loss (e.g., CURL), and the optimizer. These methods address the rank collapse but not the underlying learning deﬁciency that causes the collapse and the poor performance. Nevertheless, our results show that the dynamics of the rank and agent performance through learning are still poorly understood; we need more theoretical investigation to understand the relationship between those two factors. We also observed in Tandem and oﬄine RL settings that the rank collapses to a minimal value early in training. Then there is unexplained variance between agents in the later stages of learning. Overall, the cause and role of this early rank collapse remain unknown, and we believe understanding its potential eﬀects is essential in understanding large-scale agents’ practical learning dynamics. The existence of low-rank but high-performing policies suggest that our networks can be over-parameterized for the tasks and parsimonious representations emerge naturally with TD-learning-based bootstrapping losses and ReLU networks in the oﬄine RL setting. Discarding the dead ReLU units might achieve a more eﬃcient inference. We believe this ﬁnding can give inspiration to a new family of pruning algorithms. Acknowledgements: We would like to thank Clare Lyle, Will Dabney, Aviral Kumar, Rishabh Agarwal, Tom le Paine, Mark Rowland and Yutian Chen for the discussions. We want to thank Mark Rowland, Rishabh Agarwal and Aviral Kumar for the feedback on the early draft version of the paper. We want to thank Sergio Gomez and Bjorn Winckler for their help with the infrastructure and the codebase at the inception of this project. We would like to thank the developers of Acme (Hoﬀman et al., 2020), Jax (Bradbury et al., 2018), Optax (Hessel et al., 2020) and Haiku (Hennigan et al., 2020). 25 Preprint References Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on oﬄine reinforce- ment learning. In International Conference on Machine Learning, pages 104–114. PMLR, 2020. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:29304–29320, 2021. OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3–20, 2020. Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32:7413–7424, 2019. David Barrett and Benoit Dherin. Implicit gradient regularization. In International Conference on Learning Representations, 2020. Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Advances in Neural Information Processing Systems, 29, 2016. Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. DotA 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. Steven Bradtke and Andrew Barto. Linear least-squares algorithms for temporal diﬀerence learning. Machine Learning, 22:33–57, 03 1996. Liang-Chieh Chen, George Papandreou, Florian Schroﬀ, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017. Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi. Batch normalization provably avoids ranks collapse for randomly initialised deep networks. Advances in Neural Information Processing Systems, 2020. Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. arXiv preprint arXiv:1406.2572, 2014. Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In International Conference on Machine Learning, volume 70, pages 1019–1028. PMLR, 2017. Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement learning. arXiv preprint arXiv:1904.12901, 2019. Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6:503–556, 2005. 26 Preprint Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for eﬃciently improving generalization. arXiv preprint arXiv:2103.09575, 2021. Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020. Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018. Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch deep reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019. Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization via Hessian eigenvalue density. arXiv preprint arXiv:1901.10159, 2019. Xavier Glorot and Yoshua Bengio. Understanding the diﬃculty of training deep feedforward neural networks. In International Conference on Artiﬁcial Intelligence and Statistics, pages 249–256. JMLR Workshop and Conference Proceedings, 2010. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In International Conference on Artiﬁcial Intelligence and Statistics, pages 315–323. JMLR Workshop and Conference Proceedings, 2011. Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions. In International Conference on Machine Learning, pages 3059–3068. PMLR, 2016. Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gómez Colmenarejo, Konrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, et al. RL unplugged: Benchmarks for oﬄine reinforcement learning. arXiv preprint arXiv:2006.13888, 2020. Caglar Gulcehre, Sergio Gómez Colmenarejo, Ziyu Wang, Jakub Sygnowski, Thomas Paine, Konrad Zolna, Yutian Chen, Matthew Hoﬀman, Razvan Pascanu, and Nando de Freitas. Regularized behavior value estimation. arXiv preprint arXiv:2103.09575, 2021. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human- level performance on imagenet classiﬁcation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1026–1034, 2015. Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin. Haiku: Sonnet for JAX, 2020. URL http://github.com/deepmind/dm-haiku. Matteo Hessel, David Budden, Fabio Viola, Mihaela Rosca, Eren Sezener, and Tom Hennigan. Optax: composable gradient transformation and optimisation, in jax!, 2020. URL http://github.com/deepmind/ optax. Matt Hoﬀman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Alex Novikov, Sergio Gómez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Andrew Cowie, Ziyu Wang, Bilal Piot, and Nando de Freitas. Acme: A research framework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979, 2020. Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why do deep residual networks generalize better than deep feedforward networks?–a neural tangent kernel perspective. arXiv preprint arXiv:2002.06262, 2020. Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks. arXiv preprint arXiv:2103.10427, 2021. Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson. Transient non-stationarity and generalisation in deep reinforcement learning. arXiv preprint arXiv:2006.05826, 2020. 27 Preprint Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably eﬃcient for oﬄine RL? In International Conference on Machine Learning, pages 5084–5096. PMLR, 2021. Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances in neural information processing systems, 32, 2019. Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2014. Ksenia Konyushkova, Yutian Chen, Thomas Paine, Caglar Gulcehre, Cosmin Paduraru, Daniel J Mankowitz, Misha Denil, and Nando de Freitas. Active oﬄine policy selection. Advances in Neural Information Processing Systems, 34, 2021. Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing oﬀ-policy Q-learning In Conference on Neural Information Processing Systems, pages via bootstrapping error reduction. 11761–11771, 2019. Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit under-parameterization inhibits data-eﬃcient deep reinforcement learning. arXiv preprint arXiv:2010.14498, 2020a. Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for oﬄine reinforce- ment learning. arXiv preprint arXiv:2006.04779, 2020b. Aviral Kumar, Rishabh Agarwal, Aaron Courville, Tengyu Ma, George Tucker, and Sergey Levine. Value- based deep reinforcement learning requires explicit regularization. In RL for Real Life Workshop & Overparameterization: Pitfalls and Opportunities Workshop, ICML, 2021a. URL https://drive.google. com/file/d/1Fg43H5oagQp-ksjpWBf_aDYEzAFMVJm6/view. Aviral Kumar, Rishabh Agarwal, Tengyu Ma, Aaron Courville, George Tucker, and Sergey Levine. Dr3: Value-based deep reinforcement learning requires explicit regularization. arXiv preprint arXiv:2112.04716, 2021b. Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workﬂow for oﬄine model-free robotic reinforcement learning. arXiv preprint arXiv:2109.10813, 2021c. Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107–1149, 2003. Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Marco Wiering and Martijn van Otterlo, editors, Reinforcement Learning: State-of-the-Art, pages 45–73. Springer Berlin Heidelberg, 2012. Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, pages 5639–5650. PMLR, 2020. Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Oﬄine reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. arXiv preprint arXiv:1712.09913, 2017. Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. arXiv preprint arXiv:1808.01204, 2018. 28 Preprint Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dabney. On the eﬀect of auxiliary tasks on representation dynamics. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1–9. PMLR, 2021. Marlos C Machado, Sriram Srinivasan, and Michael Bowling. Domain-independent optimistic initialization for reinforcement learning. In Workshops at the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015. Charles H Martin and Michael W Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning. Journal of Machine Learning Research, 22(165): 1–73, 2021. Michael Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Konrad Zolna, Richard Powell, Julian Schrittwieser, et al. Starcraft ii unplugged: Large scale oﬄine reinforcement learning. In Deep RL Workshop NeurIPS 2021, 2021. Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoﬀrey Irving, and Nat McAleese. Teaching language models to support answers with veriﬁed quotes. arXiv preprint arXiv:2203.11147, 2022. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015. Hossein Mobahi, Mehrdad Farajtabar, and Peter L Bartlett. Self-distillation ampliﬁes regularization in Hilbert space. arXiv preprint arXiv:2002.05715, 2020. Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepezvari, Satinder Singh, et al. Behaviour suite for reinforcement learning. arXiv preprint arXiv:1908.03568, 2019. Georg Ostrovski, Pablo Samuel Castro, and Will Dabney. The diﬃculty of passive learning in deep reinforce- ment learning. Advances in Neural Information Processing Systems, 34, 2021. Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter selection for oﬄine reinforcement learning. arXiv preprint arXiv:2007.09055, 2020. Jeﬀrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. Advances in Neural Information Processing Systems, 2017. Dean A Pomerleau. ALVINN: An autonomous land vehicle in a neural network. In Conference on Neural Information Processing Systems, pages 305–313, 1989. Martin Riedmiller. Neural ﬁtted Q iteration – ﬁrst experiences with a data eﬃcient neural reinforcement learning method. In João Gama, Rui Camacho, Pavel B. Brazdil, Alípio Mário Jorge, and Luís Torgo, editors, European Conference on Machine Learning, pages 317–328, 2005. Levent Sagun, Léon Bottou, and Yann LeCun. Singularity of the hessian in deep learning. arXiv preprint arXiv:1611.07476, 2016. Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017. Amartya Sanyal, Varun Kanade, Philip HS Torr, and Puneet K Dokania. Robustness via deep low-rank representations. arXiv preprint arXiv:1804.07090, 2018. Tianyu Shi, Dong Chen, Kaian Chen, and Zhaojian Li. Oﬄine reinforcement learning for autonomous driving with safety and exploration enhancement. arXiv preprint arXiv:2110.07067, 2021. 29 Preprint Yeonjong Shin and George Em Karniadakis. Trainability of relu networks and data-dependent initialization. Journal of Machine Learning for Modeling and Computing, 1(1), 2020. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, and Adrian Bolton. Mastering the game of go without human knowledge. Nature, 550(7676):354–359, 2017. Edward H Simpson. The interpretation of interaction in contingency tables. Journal of the Royal Statistical Society: Series B (Methodological), 13(2):238–241, 1951. Samuel L Smith and Quoc V Le. A Bayesian perspective on generalization and stochastic gradient descent. arXiv preprint arXiv:1710.06451, 2017. Samuel L Smith, Benoit Dherin, David GT Barrett, and Soham De. On the origin of implicit regularization in stochastic gradient descent. arXiv preprint arXiv:2101.12176, 2021. Tijmen Tieleman, Geoﬀrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012. Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-learning. In Thirtieth AAAI conference on artiﬁcial intelligence, 2016. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019. Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for oﬄine reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021. Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations, 2020. A Appendix A.1 The impact of depth Pennington and Worah (2017) explored the relationship between the rank and the depth of a neural network at initialization and found that the rank of each layer’s feature matrix decreases proportionally to the index of a layer in a deep network. Here, we test the performance of a feedforward network on the bsuite task trained using Double Q-learning (Van Hasselt et al., 2016) with 2, 8, and 16 layers to see the eﬀect of the number of layers on the eﬀective rank. All our networks use ReLU activation functions and use He initialization (He et al., 2015). Figure 23 illustrates that the rank collapses as one progresses from lower layers to higher layers proportionally at the end of training as well. The network’s eﬀective rank (rank of the penultimate layer) drops to a minimal value on all three tasks regardless of the network’s number of layers. The last layer of a network will act as a bottleneck; thus, a collapse of the eﬀective rank would reduce the expressivity. Nevertheless, a deeper network with the same rank as a shallower one can learn to represent a larger class of functions (be less under-parametrized). The agents exhibit poor performance when the eﬀective rank collapses to 1. At that point, all the ReLU units die or become zero irrespective of input. Thus on bsuite, deeper networks –four and eight layered networks– performed worse than two layered MLP. A.2 Spectral density of hessian for oﬄine RL Analyzing the eigenvalue spectrum of the Hessian is a common way to investigate the learning dynamics and the loss surface of deep learning methods (Ghorbani et al., 2019; Dauphin et al., 2014). Understanding Hessian’s loss landscape and eigenvalue spectrum can help us design better optimization algorithms. Here, 30 Preprint (L) (C) (R) Figure 23: [bsuite] The ranks and depths of the networks: The evolution of the ranks across diﬀerent layers of deep neural networks. The ﬁgure on the left (L) is for the ranks of catch across diﬀerent layers. The ﬁgure at the center (C) is for the ranks of mountain_car across diﬀerent layers. The ﬁgure on the right (R) is for cartpole. we analyze the eigenvalue spectrum of a single hidden layer feedforward network trained on the bsuite Catch dataset from RL Unplugged (Gulcehre et al., 2020) to understand the loss-landscape of a network with a low eﬀective rank compared to a model with higher rank at the end of the training. As established in Figure 24, ELU activation function network has a signiﬁcantly higher eﬀective rank than the ReLU network. By comparing those two networks, we also look into the diﬀerences in the eigenvalue spectrum of a network with high and low rank. Since the network and the inputs are relatively low-dimensional, we computed the full Hessian over the dataset rather than a low-rank approximation. The eigenvalue spectrum of the Hessian with ReLU and the ELU activation functions is shown in Figure 24. The rank collapse is faster for ReLU than ELU. After 900k gradient updates, the ReLU network concentrates 92% of the eigenvalues of Hessian around zero; this is due to the dead units in ReLU network (Glorot et al., 2011). On the other hand, the ELU network has a few very large eigenvalues after the same number of gradient updates. In Figure 25, we summarize the distribution of the eigenvalues of the Hessian matrices of the ELU and ReLU networks. As a result, the Hessian and the feature matrix of the penultimate layer of the network will be both low-rank. Moreover, in this case, the landscape of the ReLU network might be ﬂatter than the ELU beyond the notion of wide basins (Sagun et al., 2016). This might mean that the ReLU network ﬁnds a simpler solution. Thus, the ﬂatter landscape is conﬁrmed by the simpler function learned and less capacity used at the end of the training, which is induced by the lower rank representations. Figure 24: [bsuite Catch] spectral density of Hessian: The visualization of the spectral density of the full Hessian of a network trained with 64 units using ReLU (left) and ELU (right) activation functions. The eigenvalues of the Hessian of the oﬄine DQN with ReLU activation function are concentrated around 0 and most of them are less than 1. The eigenvalues of the ELU network also concentrate around 0 with a few large outlier eigenvalues. 31 k n a R 35 30 25 20 15 10 5 0 2 Layer Net 8 Layer Net 16 Layer Net 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Network's Depth k n a R e v i t c e f f E 6 5 4 3 2 1 0 2 Layer Net 8 Layer Net 16 Layer Net 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Network's Depth k n a R e v i t c e f f E 6 5 4 3 2 1 0 2 Layer Net 8 Layer Net 16 Layer Net 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Network's Depth y t i s n e D 2 10 0 10 −2 10 −4 10 −6 10 −8 10 −10 10 Learner steps = 0 Learner steps = 900k −0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Eigenvalue y t i s n e D 2 10 0 10 −2 10 −4 10 −6 10 −8 10 −10 10 Learner steps = 0 Learner Steps = 900k 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Eigenvalue Preprint Figure 25: [bsuite Catch] Hessian eigenvalues (evs) for oﬄine DQN: We visualize the percentages of positive, negative, and near-zero eigenvalues of the Hessian for oﬄine DQN with ELU and ReLU activation functions on the catch dataset from bsuite. If the absolute value of an eigenvalue is less than 1e-7, we consider it as a near-zero eigenvalue. We can see that for ELU network, near-zero, positive and negative eigenvalues are almost evenly distributed. However, with ReLU network majority of eigenvalues are near-zero (90% of the evs are exactly zero), very few negative (2 %) and some positive eigenvalues (7.1 %). A.3 DeepMind lab: performance vs the eﬀective ranks In Figure 26, we show the correlation between the eﬀective rank and the performance of R2D2, R2D2 trained with CURL and R2D2 trained with SAM. When we look at each variant separately or as an aggregate, we don’t see a strong correlation between the performance and the rank of an agent. Figure 26: [DeepMind lab] The eﬀective rank and the performance: Correlations between feature ranks and episode returns for diﬀerent exploration noises on DeepMind lab dataset. We include data from three models: R2D2, R2D2 trained with CURL, and R2D2 trained with SAM. We do not observe a strong correlation between the eﬀective rank and performance across diﬀerent noise exploration levels in the data. A.4 SAM optimizer: does smoother loss landscape aﬀect the behavior of the rank collapse? We study the relationship between smoother loss landscapes and rank collapse. We use Sharpness Aware Minimization (SAM) (Foret et al., 2021) loss for potentially creating ﬂatter loss surfaces in order to see if smoother loss landscapes aﬀect the rank and performance dynamics diﬀerently. Figures 27 and 28 show the evolution of feature ranks and generalization performance in Atari and DeepMind lab respectively. We do not observe a very clear relation between the extent of smoothing the loss and the feature ranks or generalization performance. 32 % near-zero evs 100 80 60 40 20 0 % positive evs % negative evs ReLU DQN ELU DQN Eps: 0.0, Kendall τ:0.25, Pearson ρ:0.30 Eps: 0.01, Kendall τ:0.09, Pearson ρ:0.05 Eps: 0.1, Kendall τ:0.32, Pearson ρ:0.41 Eps: 0.25, Kendall τ:0.14, Pearson ρ:0.13 Variant CURL R2D2 SAM s k n a R e v i t c e f f E l i a n m r e T 200 100 0 0 2.5 5 7.5 10 0 5 10 15 0 10 20 30 0 10 20 30 40 Terminal Episode Return Preprint Figure 27: [Atari] Sharpness Aware Minimization (SAM) Loss: Evolution of ranks as we increase the weight of auxiliary losses. We see that some amount of weight on the SAM loss helps mitigate the extent of rank collapse but we observe no clear relationship with the agent performance. Figure 28: [DeepMind lab-SeekAvoid Snapshot] Sharpness Aware Minimization (SAM) Loss We do not observe any rank collapse as we continue training with the LSTM networks (because of the tanh activations discussed in Section 6) used in DeepMind lab dataset over a spectrum of diﬀerent weights for the SAM loss. A.5 Eﬀective rank and the value error A curious relationship is between the eﬀective rank and the value error, because a potential for the rank collapse or Phase 3 with the TD-learning algorithms can be the errors propagating thorough the bootstrapped targets. Figure 29 shows the correlation between the value error and the eﬀective ranks. There is a strong anti-correlation between the eﬀective ranks and the performance of the agents except on the levels where the expert agent that generated the dataset performs poorly at the end of the training (e.g. IceHockey.) This 33 Preprint Figure 29: [Atari]: These plots shows the correlation between the value error and the eﬀective rank for oﬄine DQN agent trained for 20M steps on online policy selection games for Atari. There is an apparent anti-correlation between the eﬀective rank and the value error. Namely, as the value error of an agent when evaluated in the environment increases the eﬀective rank decreases. The correlation is signiﬁcant on most Atari levels except IceHockey where the expert agent that generated the dataset performs poorly. makes the hypothesis that the extrapolation error can cause rank collapse (or push the agent to Phase 3) more plausible. A.6 CURL on DeepMind Lab In Figure 30 shows the CURL results on DeepMind lab dataset. We couldn’t ﬁnd any consistent pattern across various CURL loss weights. Figure 30: [DeepMind lab-SeekAvoid:] auxiliary losses Evolution of ranks as we increase the weight of auxiliary loss. While some auxiliary loss helps the model perform well, there is no clear correlation between rank and performance. 34 Preprint A.7 Learning rate evolution In Figure 31, we also perform a hyperparameter selection by evaluating the model in the environment at various stages during the training. As the oﬄine DQN is trained longer, the optimal learning rate for the best agent performance when evaluated online goes down. As one increases the number of training steps of an agent, we need to change the learning rate accordingly since the number of training steps aﬀects the best learning rate. Figure 31: [Atari]: Evolution of the optimal learning rate found by online evaluations in the environment. As the model is trained longer the optimal learning rate found by online evaluations goes down. A.8 Learning curves long training regime and the diﬀerent phases of learning In this subsection, we investigate the eﬀect of changing learning rates on the eﬀective rank and the performance of the agent on RL Unplugged Atari online policy selection games. We train the oﬄine DQN for 20M learning steps which is ten times longer than typical oﬄine Atari agents (Gulcehre et al., 2020). We evaluated twelve learning rates in (cid:2)10−2, 10−5(cid:3) equally spaced in logarithmic space. We show the eﬀective rank learning and performance curves in Figure 32. It is easy to identify diﬀerent phases of learning in most of those curves. A.9 Eﬀective rank and the performance Figure 33 shows the correlation between the eﬀective rank and the performance of an agent trained with minibatch of size 32. On most Atari online policy selection games it is possible to see a very strong correlation but on some games the correlation is not there. It seems like even Figure 34 depicts the correlation between the eﬀective rank and the performance of a DQN agent with ReLU activation function. There is a signiﬁcant correlation on most Atari games. As we discussed earlier, long training setting with ReLU activation functions where the eﬀect of the rank is the strongest on the performance. A.10 Interventions to eﬀective rank and randomized controlled trials (RCT) As the rank is a continuous variable, we can ﬁlter out the experiments with certain ranks with a threshold τ . In the control case (λ(1)), we assume β > τ and for λ(0), we would have β ≤ τ . We can also write this as: E[λ|do(β > τ )] − E[λ|do(β ≤ τ )]. (5) A.11 Computation of feature ranks Here, we present the Python code-stub that we used across our experiments (similar to Kumar et al. (2020a)) to compute the feature ranks of the pre-output layer’s features: import numpy as np def compute_rank_from_features(feature_matrix, rank_delta=0.01): 35 0.00030 0.00025 0.000231 e t a R g n n r a e L i 0.00020 0.00015 0.00010 0.00005 0.000066 0.000019 1e+05 1.2e+06 2.4e+06 3.6e+06 4.8e+06 6e+06 7.2e+06 8.4e+06 9.6e+061.08e+071.2e+071.32e+071.44e+07 Learning Steps Preprint Figure 32: [Atari] Eﬀective rank curves: Eﬀective rank and performance oﬄine DQN agent across nine Atari online policy selection games. We train the oﬄine DQN agent for 20M learning steps and evaluated 12 learning rates in (cid:2)10−2, 10−5(cid:3) equally spaced in logarithmic space. Let us note that in all games rank goes down early in the training (Phase 1), then goes up (phase 2) and for some learning rate the eﬀective rank collapses (Phase 3). Correspondingly, the performance is low in the beginning of the training (Phase 1), goes up and stays high for a while (Phase 2), and sometimes it the performance collapses (Phase 3). 36 Preprint Figure 33: [Atari] The correlation between ranks and returns for DQN trained with minibatch size of 32. We ran each network with three learning rates and ﬁve diﬀerent seeds but we only show the mean across those ﬁve seeds here. We can see very strong correlations on some games, but that correlation is not consistent. Figure 34: [Atari] The correlation between ranks and returns for DQN trained for a network trained for 20M learning steps and 12 diﬀerent learning rates with minibatch size of 256. We can see that there is signiﬁcant positive correlation between the rank and the learning rates for most online policy selection games. 37 BeamRider, Spearman corr: 0.75 DemonAttack, Spearman corr: 0.87 200 DoubleDunk, Spearman corr: 0.44 IceHockey, Spearman corr: -0.06 1000 1500 2000 100 50 0 1.8 1.4 1 s k n a R e v i t c e f f E l i a n m r e T −16 −8 RoadRunner, Spearman corr: -0.48 −12 −4 12 9 6 3 0 0 MsPacman, Spearman corr: 0.59 10000 5000 0 Robotank, Spearman corr: 0.46 1000 2000 3000 150 100 50 0 75 50 25 0 75 50 25 0 1.15 1.10 1.05 1 100 90 80 70 60 50 90 60 30 0 −24 −22 Pooyan, Spearman corr: -0.33 −22.5 −23.5 −23 Learning Rates 3e-05 0.0001 0.0003 0 Zaxxon, Spearman corr: 0.80 100 300 200 0 250 500 750 0 20 40 Episode return 60 0 2000 4000 6000 8000 Preprint Hyper-parameters Training batch size Rank calculation batch size Num training steps Learning rate Optimizer Feedforward hidden layer size Num hidden layers Activation Memory Discount bsuite 32 512 1e6 3e-4 Adam 64 2 ReLU None 0.99 Atari 256 512 2e6 3e-5 Adam 512 1 ReLU None 0.99 DeepMind lab 4 (episodes) 512 2e4 1e-3 Adam 256 1 ReLU LSTM gates) LSTM 0.997 (tanh for Table 3: The default hyper-parameters used in our work across diﬀerent domains. """"""Computes rank of the features based on how many singular values are significant."""""" sing_values = np.linalg.svd(feature_matrix, compute_uv=False) cumsum = np.cumsum(sing_values) nuclear_norm = np.sum(sing_values) approximate_rank_threshold = 1.0 - rank_delta threshold_crossed = ( cumsum >= approximate_rank_threshold * nuclear_norm) effective_rank = sing_values.shape[0] - np.sum(threshold_crossed) + 1 return effective_rank A.12 Hyperparameters Here, we list the standard set of hyper-parameters that were used in diﬀerent domains: bsuite, Atari, and DeepMind Lab respectively. These are the default hyper-parameters, which may diﬀer when stated so in our speciﬁc ablation studies. For the DMLLAB task, we use the same network that was used in Gulcehre et al. (2021). For all the Atari tasks, we use the same convolution torso that was used in Gulcehre et al. (2020) which involves three layers of convolution with ReLU activations in between. • Layer 1 - Conv2d(channels=32, kernel_shape=[8, 8], stride=[4, 4]) • Layer 2 - Conv2d(channels=64, kernel_shape=[4, 4], stride=[2, 2]) • Layer 3 - Conv2d(channels=64, kernel_shape=[3, 3], stride=[1, 1]) A.13 bsuite phase transitions and bottleneck capacity We illustrate the phase transition of a simple MLPs with ReLU activations. In Figure 35, we have a network of size (64, bottleneck units, 64) where we vary the number of bottleneck units. In Figure 36, we have a network of size (64, bottleneck units) where we vary the number of bottleneck units. In both cases, having smaller number of bottleneck units reduces the performance of the mode and agents were able to solve the problem even when the penultimate layer’s eﬀective rank was small. With the larger learning rate the right handside ﬁgures (b), the eﬀective ranks tend to be lower. A.14 Activation sparsity on bsuite In Figure 37, we show that the activations of the ReLU network becomes very sparse during the course of training. The sparsity of the ReLU units seems to be signiﬁcantly higher than the ELU units at the end of training. 38 Preprint a) b) Figure 35: [bsuite] - Catch dataset: Phase transition plots of a network with hidden layers of size (64, bottleneck units, 64). On the y-axis, we show the log2(bottleneck units), and x-axis is the number of gradient steps. The ﬁgures on the left hand-side (a) is trained with the learning rate with 8e-5 and right hand-side (b) experiments are trained with learning rate of 3e-4. The ﬁrst row shows the episodic returns. The second row shows the eﬀective rank of the second layer (bottleneck units) and the third row is showing the penultimate layer’s eﬀective rank. The eﬀective rank collapses much faster with the learning rate of 4e-3 than 5e-8. The low ranks can still have good performance. The low bottleneck units causes the eﬀective rank of the last layer to collapse faster. The performance of the network with the small number of bottleneck units is poor. The eﬀective rank of the small number of bottleneck units is smaller. 39 Preprint Figure 36: [bsuite] - Catch dataset: Phase transition plots of a network with hidden layers of size (64, bottleneck units). On the y-axis, we show the log2(bottleneck units), and x-axis is the number of gradient steps. The ﬁgures on the left hand-side (a) is trained with the learning rate with 8e-5 and right hand-side (b) experiments are trained with learning rate of 3e-4. The ﬁrst row shows the episodic returns. The second row shows the eﬀective rank of the second layer (bottleneck units). The eﬀective rank collapses much faster with the learning rate of 4e-3 than 5e-8. The low ranks can still have good performance. The small number of bottleneck units causes the eﬀective rank of the layer to collapse faster. The performance of the network with the small number of bottleneck units is poor. The eﬀective rank of the small number of bottleneck units is smaller. Figure 37: [bsuite Catch] Gram matrices of activations: Gram matrices of activations of a two-layer MLP with ReLU and ELU activation functions. The activations of the ReLU units become sparser when compared to ELU units at the end of the training due to dead ReLU units. 40 Initialization (step 0) Convergence (step 900k) ReLU ELU",1.0
"
        DALL-E 2’s Failures Are the Most Interesting Thing About It
    ",https://spectrum.ieee.org/openai-dall-e-2,2022-07-14,,"OpenAI’s text-to-image generator still struggles with text, science, faces, and bias IEEE Spectrum queried DALL-E 2 for an image of “a technology journalist writing an article about a new AI system that can create remarkable and strange images.” In response, it sent back only pictures of men. In April, the artificial intelligence research lab OpenAI revealed DALL-E 2, the successor to 2021’s DALL-E. Both AI systems can generate astounding images from natural-language text descriptions; they’re capable of producing images that look like photos, illustrations, paintings, animations, and basically any other art style you can put into words. DALL-E 2 upped the ante with better resolution, faster processing, and an editor function that lets the user make changes within a generated image using only text commands, such as “replace that vase with a plant” or “make the dog’s nose bigger.” Users can also upload an image of their own and then tell the AI system how to riff on it. The world’s initial reactions to DALL-E 2 were amazement and delight. Any combination of objects and creatures could be brought together within seconds; any art style could be mimicked; any location could be depicted; and any lighting conditions could be portrayed. Who wouldn’t be impressed at the sight, for example, of a parrot flipping pancakes in the style of Picasso? There were also ripples of concern, as people cataloged the industries that could easily be disrupted by such a technology. OpenAI has not released the technology to the public, to commercial entities, or even to the AI community at large. “We share people’s concerns about misuse, and it’s something that we take really seriously,” OpenAI researcher Mark Chen tells IEEE Spectrum.But the company did invite select people to experiment with DALL-E 2 and allowed them to share their results with the world. That policy of limited public testing stands in contrast to Google’s policy with its own just-released text-to-image generator, Imagen. When unveiling the system, Google announced that it would not be releasing code or a public demo due to risks of misuse and generation of harmful images. Google has released a handful of very impressive images but hasn’t shown the world any of the problematic content to which it had alluded. That makes the images that have come out from the early DALL-E 2 experimenters more interesting than ever. The results that have emerged over the last few months say a lot about the limits of today’s deep-learning technology, giving us a window into what AI understands about the human world—and what it totally doesn’t get. OpenAI kindly agreed to run some text prompts from Spectrum through the system. The resulting images are scattered through this article. Spectrum asked for ""a Picasso-style painting of a parrot flipping pancakes,"" and DALL-E 2 served it up. OpenAI DALL-E 2 was trained on approximately 650 million image-text pairs scraped from the Internet, according to the paper that OpenAI posted to ArXiv. From that massive data set it learned the relationships between images and the words used to describe them. OpenAI filtered the data set before training to remove images that contained obvious violent, sexual, or hateful content. “The model isn’t exposed to these concepts,” says Chen, “so the likelihood of it generating things it hasn’t seen is very, very low.” But the researchers have clearly stated that such filtering has its limits and have noted that DALL-E 2 still has the potential to generate harmful material. Once this “encoder” model was trained to understand the relationships between text and images, OpenAI paired it with a decoder that generates images from text prompts using a process called diffusion, which begins with a random pattern of dots and slowly alters the pattern to create an image. Again, the company integrated certain filters to keep generated images in line with its content policy and has pledged to keep updating those filters. Prompts that seem likely to produce forbidden content are blocked and, in an attempt to prevent deepfakes, it can't exactly reproduce faces it has seen during its training. Thus far, OpenAI has also used human reviewers to check images that have been flagged as possibly problematic. Because of DALL-E 2’s clear potential for misuse, OpenAI initially granted access to only a few hundred people, mostly AI researchers and artists. Unlike the lab’s language-generating model, GPT-3, DALL-E 2 has not been made available for even limited commercial use, and OpenAI hasn’t publicly discussed a timetable for doing so. But from browsing the images that DALL-E 2 users have created and posted on forums such as Reddit, it does seem like some professions should be worried. For example, DALL-E 2 excels at food photography, at the type of stock photos used for corporate brochures and websites, and with illustrations that wouldn’t seem out of place on a dorm room poster or a magazine cover. Spectrum asked for a “New Yorker-style cartoon of an unemployed panda realizing her job eating bamboo has been taken by a robot.” OpenAI Here’s DALL-E 2’s response to the prompt: “An overweight old dog looks delighted that his younger and healthier dog friends have remembered his birthday, in the style of a greeting card.”OpenAI Spectrum reached out to a few entities within these threatened industries. A spokesperson for Getty Images, a leading supplier of stock photos, said the company isn’t worried. “Technologies such a DALL-E are no more a threat to our business than the two-decade reality of billions of cellphone cameras and the resulting trillions of images,” the spokesperson said. What’s more, the spokesperson said, before models such as DALL-E 2 can be used commercially, there are big questions to be answered about their use for generating deepfakes, the societal biases inherent in the generated images, and “the rights to the imagery and the people, places, and objects within the imagery that these models were trained on.” The last part of that sounds like a lawsuit brewing. Rachel Hill, CEO of the Association of Illustrators, also brought up the issues of copyright and compensation for images’ use in training data. Hill admits that “AI platforms may attract art directors who want to reach for a fast and potentially lower-price illustration, particularly if they are not looking for something of exceptional quality.” But she still sees a strong human advantage: She notes that human illustrators help clients generate initial concepts, not just the final images, and that their work often relies “on human experience to communicate an emotion or opinion and connect with its viewer.” It remains to be seen, says Hill, whether DALL-E 2 and its equivalents could do the same, particularly when it comes to generating images that fit well with a narrative or match the tone of an article about current events. To gauge its ability to replicate the kinds of stock photos used in corporate communications, Spectrum asked for “a multiethnic group of blindfolded coworkers touching an elephant.”OpenAI For all DALL-E 2’s strengths, the images that have emerged from eager experimenters show that it still has a lot to learn about the world. Here are three of its most obvious and interesting bugs. Text: It’s ironic that DALL-E 2 struggles to place comprehensible text in its images, given that it’s so adept at making sense of the text prompts that it uses to generate images. But users have discovered that asking for any kind of text usually results in a mishmash of letters. The AI blogger Janelle Shane had fun asking the system to create corporate logos and observing the resulting mess. It seems likely that a future version will correct this issue, however, particularly since OpenAI has plenty of text-generation expertise with its GPT-3 team. “Eventually a DALL-E successor will be able to spell Waffle House, and I will mourn that day,” Shane tells Spectrum. “I’ll just have to move on to a different method of messing with it.” To test DALL-E 2’s skills with text, Spectrum riffed on the famous Magritte painting that has the French words “Ceci n’est pas une pipe” below a picture of a pipe. Spectrum asked for the words “This is not a pipe” beneath a picture of a pipe. OpenAI Science: You could argue that DALL-E 2 understands some laws of science, since it can easily depict a dropped object falling or an astronaut floating in space. But asking for an anatomical diagram, an X-ray image, a mathematical proof, or a blueprint yields images that may be superficially right but are fundamentally all wrong. For example, Spectrum asked DALL-E 2 for an “illustration of the solar system, drawn to scale,” and got back some very strange versions of Earth and its far too many presumptive interplanetary neighbors—including our favorite, Planet Hard-Boiled Egg. “DALL-E doesn’t know what science is. It just knows how to read a caption and draw an illustration,” explains OpenAI researcher Aditya Ramesh, “so it tries to make up something that’s visually similar without understanding the meaning.” Spectrum asked for “an illustration of the solar system, drawn to scale,” and got back a very crowded and strange collection of planets, including a blobby Earth at lower left and something resembling a hard-boiled egg at upper left.OpenAI Faces: Sometimes, when DALL-E 2 tries to generate photorealistic images of people, the faces are pure nightmare fodder. That’s partly because, during its training, OpenAI introduced some deepfake safeguards to prevent it from memorizing faces that appear often on the Internet. The system also rejects uploaded images if they contain realistic faces of anyone, even nonfamous people. But an additional issue, an OpenAI representative tells Spectrum, is that the system was optimized for images with a single focus of attention. That’s why it’s great at portraits of imaginary people, such as this nuanced portrait produced when Spectrum asked for “an astronaut gazing back at Earth with a wistful expression on her face,” but pretty terrible at group shots and crowd scenes. Just look what happened when Spectrum asked for a picture of seven engineers gathered around a whiteboard. This image shows DALL-E 2’s skill with portraits. It also shows that the system’s gender bias can be overcome with careful prompts. This image was a response to the prompt “an astronaut gazing back at Earth with a wistful expression on her face.”OpenAI When DALL-E 2 is asked to generate pictures of more than one human at a time, things fall apart. This image of “seven engineers gathered around a white board” includes some monstrous faces and hands. OpenAI Bias: We’ll go a little deeper on this important topic. DALL-E 2 is considered a multimodal AI system because it was trained on images and text, and it exhibits a form of multimodal bias. For example, if a user asks it to generate images of a CEO, a builder, or a technology journalist, it will typically return images of men, based on the image-text pairs it saw in its training data. Spectrum queried DALL-E 2 for an image of “a technology journalist writing an article about a new AI system that can create remarkable and strange images.” This image shows one of its responses; the others are shown at the top of this article. OpenAI OpenAI asked external researchers who work in this area to serve as a “red team” before DALL-E 2’s release, and their insights helped inform OpenAI’s write-up on the system’s risks and limitations. They found that in addition to replicating societal stereotypes regarding gender, the system also over-represents white people and Western traditions and settings. One red team group, from the lab of Mohit Bansal at the University of North Carolina, Chapel Hill, had previously created a system that evaluated the first DALL-E for bias, called DALL-Eval, and they used it to check the second iteration as well. The group is now investigating the use of such evaluation systems earlier in the training process—perhaps sampling data sets before training and seeking additional images to fix problems of underrepresentation or using bias metrics as a penalty or reward signal to push the image-generating system in the right direction. Chen notes that a team at OpenAI has already begun experimenting with “machine-learning mitigations” to correct for bias. For example, during DALL-E 2’s training the team found that removing sexual content created a data set with more males than females, which caused the system to generate more images of males. “So we adjusted our training methodology and up-weighted images of females so they’re more likely to be generated,” Chen explains. Users can also help DALL-E 2 generate more diverse results by specifying gender, ethnicity, or geographical location using prompts such as “a female astronaut” or “a wedding in India.” But critics of OpenAI say the overall trend toward training models on massive uncurated data sets should be questioned. Vinay Prabhu, an independent researcher who co-authored a 2021 paper about multimodal bias, feels that the AI research community overvalues scaling up models via “engineering brawn” and undervalues innovation. “There is this sense of faux claustrophobia that seems to have consumed the field where Wikipedia-based data sets spanning [about] 30 million image-text pairs are somehow ad hominem declared to be ‘too small’!” he tells Spectrum in an email. Prabhu champions the idea of creating smaller but “clean” data sets of image-text pairs from such sources as Wikipedia and e-books, including textbooks and manuals. “We could also launch (with the help of agencies like UNESCO for example) a global drive to contribute images with descriptions according to W3C’s best practices and whatever is recommended by vision-disabled communities,” he suggests. The DALL-E 2 team says they’re eager to see what faults and failures early users discover as they experiment with the system, and they’re already thinking about next steps. “We’re very much interested in improving the general intelligence of the system,” says Ramesh, adding that the team hopes to build “a deeper understanding of language and its relationship to the world into DALL-E.” He notes that OpenAI’s text-generating GPT-3 has a surprisingly good understanding of common sense, science, and human behavior. “One aspirational goal could be to try to connect the knowledge that GPT-3 has to the image domain through DALL-E,” Ramesh says. As users have worked with DALL-E 2 over the past few months, their initial awe at its capabilities changed fairly quickly to bemusement at its quirks. As one experimenter put it in a blog post, “Working with DALL-E definitely still feels like attempting to communicate with some kind of alien entity that doesn’t quite reason in the same ontology as humans, even if it theoretically understands the English language.” One day, maybe, OpenAI or its competitors will create something that approximates human artistry. For now, we’ll appreciate the marvels and laughs that come from an alien intelligence—perhaps hailing from Planet Hard-Boiled Egg. This article appears in the August 2022 print issue as “DALL-E 2’s Failures Show the Limits of AI.” Eliza Strickland is a senior editor at IEEE Spectrum, where she covers AI, biomedical engineering, and other topics. She holds a master's degree in journalism from Columbia University. ...Or did the metaverse just turn a shade more uncannily creepy? Matthew S. Smith writes IEEE Spectrum's Gizmo column and is a freelance consumer-tech journalist. An avid gamer, he is a former staff editor at Digital Trends and is particularly fond of wearables, e-bikes, all things smartphone, and CES, which he has attended every year since 2009. Mesh to MetaHuman lets creators import a facial mesh to create a photorealistic 3D character. Creating your virtual clone isn’t as difficult as you’d think. Epic Games recently introduced Mesh to MetaHuman, a framework for creating photorealistic human characters. It lets creators sculpt an imported mesh to create a convincing character in less than an hour. “It’s incredibly simple compared to a lot of other tools,” says Stu Richards (a.k.a. Meta Mike), partner success lead at GigLabs and Cofounder of Versed. “I’d compare it to a character creator in a game.” This e-nose can detect glucose levels with 90 percent accuracy Michelle Hampson is a freelance writer based in Halifax. She frequently contributes to Spectrum's Journal Watch coverage, which highlights newsworthy studies published in IEEE journals. This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore. Researchers at NYU have developed an AI solution that can leverage public video feeds to better inform decision makers Dexter Johnson is a contributing editor at IEEE Spectrum, with a focus on nanotechnology. This is a sponsored article brought to you by NYU’s Tandon School of Engineering. In the midst of the COVID-19 pandemic, in 2020, many research groups sought an effective method to determine mobility patterns and crowd densities on the streets of major cities like New York City to give insight into the effectiveness of stay-at-home and social distancing strategies. But sending teams of researchers out into the streets to observe and tabulate these numbers would have involved putting those researchers at risk of exposure to the very infection the strategies were meant to curb. Researchers at New York University’s (NYU) Connected Cities for Smart Mobility towards Accessible and Resilient Transportation (C2SMART) Center, a Tier 1 USDOT-funded University Transportation Center, developed a solution that not only eliminated the risk of infection to researchers, and which could easily be plugged into already existing public traffic camera feeds infrastructure, but also provided the most comprehensive data on crowd and traffic densities that had ever been compiled previously and cannot be easily detected by conventional traffic sensors.",0.0
"
        What’s the Tech Background of an Autonomous-Vehicle Engineer?
    ",https://spectrum.ieee.org/av-engineer-jobs,2022-07-11,,"They come from video games, finance, geospatial research, the hacker community, and more, says Cruise’s EVP of engineering Mo Elshenawy, Cruise’s executive vice president of engineering, looks to hire engineers who love to experiment. Cruise, the San-Francisco–based designer and operator of all-electric self-driving cars, employs nearly 2,000 engineers, including somewhere between 300 and 900 engineers with Ph.D. degrees. They work in hardware and software. They specialize in AI, security, and safety. And though, indeed, some have robotics, automation, or automotive backgrounds, many don’t. Instead, they come from an incredibly long list of different technical fields—e-commerce, finance, game development, animation, cameras, semiconductors, and app development. Here’s what Mohamed Elshenawy, Cruise’s executive vice president of engineering, told IEEE Spectrum about the company’s workforce. (Elshenawy himself came to Cruise from stints as chief technology officer at a financial services startup and leader of a technology team at Amazon.) IEEE Spectrum:Let’s start with the big picture of your engineering team. Mohamed Elshenawy: In AI, we have machine-learning (ML) engineers that build the on-the-road decision-making modules. We also have the engineers that help build the tools for our continuous-learning machine. We have data engineers and data scientists, and even UI folks that help with the tools that the main core ML engineers use. In robotics, we have AV foundations engineers, who build and maintain our robotics operating systems, and embedded systems developers. In our security and safety operations, we have engineers working on threat modeling, app security engineering, IT enterprise security, and the security of the vehicle itself, along with all the systems engineers responsible for test validation, generating test-case scenarios, and tracking it all. Our hardware engineers build our own EVs from the ground up, in partnership with GM and Honda. They handle the definition, design, development, and production of sensors, compute modules, and related hardware and include about 50 engineering disciplines, including acoustic engineers, power engineers, system-on-chips people, and so on. Finally, we have the product engineering team—that covers both user-facing and internal apps—and the infrastructure team which builds the technical foundations (cloud infrastructure, internal tools, etc.) that the rest of our engineers rely on every day to get work done. Where do these engineers generally come from? Elshenawy: Many of our AI engineers come from academia, consumer tech, and finance. Our simulation group is part of this team. They include several engineers that helped build The Sims and others that worked for gaming and animation studios like Pixar, LucasArts, and Ubisoft. Our hardware engineers include people who came from Kodak, JPL, and chipmakers, as well as academia. For robotics, we generally look to robotics, other automotive, and aerospace companies for hiring. On the security side, we hire a mix of researchers and practitioners—and even literal hackers, including the team that infamously hacked a Jeep. Our product and infrastructure engineers come from a variety of traditional engineering companies as well as startups; they’ve previously built videoconferencing software, cloud computing platforms, and even a meditation app. How has the mix shifted over the years? Elshenawy: We’re solving for a problem that is mainly rooted in general AI, so we’ve always been heavy on the AI side of things. Because we are cloud native, we are able to leverage a lot of the existing technology that is provided by cloud providers, such as Google Cloud Platform and Azure; we don’t need to reinvent that. So we’re leaning more heavily towards AI, robotics, and hardware over time. Also, we’re ramping up product engineering now that we have started charging members of the public to ride in fully driverless cars. We expect that to continue to grow there as we solve for the technology and expand to multiple cities in the United States and internationally. You mean people working on the customer-facing app? Elshenawy: That and a lot more. This includes the customer-facing app that lets you order a ride, the in-car experience, and all the fleet operation on the back end, where we control our fleet, placing our fleet ahead of demand and determining pricing, and all the services that keep these lights on. What do your engineers have in common? Elshenawy: The self-driving cars problem is a great AI problem, and some people think about it as essentially a research problem, because you’re pushing the state of the art. But when you think about how you are actually going to pragmatically ship something continuously, it’s all rooted in experimentation. So one common factor that we find in the engineers who are successful here, regardless of where in our organization that they land, is having the experimentation mind-set, the humble, resilient mind-set of someone who’s continuously curious and very agile in nature. There are certain types of engineers that don’t deal well with uncertainty and experimentation, and there are other engineers who thrive under an environment of continuous learning. So the common trait among all these engineers is that learning, curiosity, and experimentation mind-set, and having the agility to deal with an unknown problem like this one. What are the hardest roles to fill right now? Elshenawy: Software, in general, has a shortage and will always have less supply than demand in the coming years, particularly in AI, applied machine learning, and deep learning, and we will continue to need these engineers in many different areas as we grow. Robotics specializations, and in general, control theory, will be very important as we go forward and those areas will continually face high demand. Tekla S. Perry is a senior editor at IEEE Spectrum. Based in Palo Alto, Calif., she's been covering the people, companies, and technology that make Silicon Valley a special place for more than 40 years. An IEEE member, she holds a bachelor's degree in journalism from Michigan State University. His pivot from defense helped a tiny tuning-fork prevent SUV rollovers and plane crashes In 1992, Asad M. Madni sat at the helm of BEI Sensors and Controls, overseeing a product line that included a variety of sensor and inertial-navigation devices, but its customers were less varied—mainly, the aerospace and defense electronics industries. And he had a problem. The Cold War had ended, crashing the U.S. defense industry. And business wasn’t going to come back anytime soon. BEI needed to identify and capture new customers—and quickly.",0.0
"
        Autonomous Drones Challenge Human Champions in First “Fair” Race
    ",https://spectrum.ieee.org/zurich-autonomous-drone-race,2022-07-06,,"Vision-based AI drones outfly world-class human pilots Watching robots operate with speed and precision is always impressive, if not, at this point, always surprising. Sophisticated sensors and fast computing means that a powerful and agile robot, like a drone, that knows exactly where it is and exactly where it’s going can reliably move in highly dynamic ways. This is not to say that it’s easy for the drone, but if you’ve got a nice external localization system, a powerful off-board computer, and a talented team of roboticists, you can perform some amazingly agile high-speed maneuvers that most humans could never hope to match. I say “most” humans, because there are some exceptionally talented humans who are, in fact, able to achieve a level of performance similar to that of even the fastest and most agile drones. The sport of FPV (first-person view) drone racing tests what’s possible with absurdly powerful drones in the hands of humans who must navigate complex courses with speed and precision that seems like it shouldn’t be possible, all while relying solely on a video feed sent from a camera on the front of the drone to the pilot’s VR headset. It’s honestly astonishing to watch. A year ago, autonomous racing quadrotors from Davide Scaramuzza’s Robotics and Perception Group at the University of Zurich (UZH) proved that they could beat the world’s fastest humans in a drone race. However, the drones relied on a motion-capture system to provide very high resolution position information in real time, along with a computer sending control information from the safety and comfort of a nearby desk, which doesn’t really seem like a fair competition. Earlier this month, a trio of champion drone racers traveled to Zurich for a rematch, but this time, the race would be fair: no motion-capture system. Nothing off-board. Just drones and humans using their own vision systems and their own computers (or brains) to fly around a drone racing track as fast as possible. To understand what kind of a challenge this is, it’s important to have some context for the level of speed and agility. So here’s a video of one of UZH’s racing drones completing three laps of a track using the motion-capture system and off-board computation. This particular demo isn’t “fair,” but it does give an indication of what peak performance of a racing drone looks like, with a reaction from one of the professional human pilots (Thomas Bitmatta) at the end: As Thomas says at the end of the video, the autonomous drone made it through one lap of the course in 5.3 seconds. With a peak speed of 110 kilometers per hour, this was a staggering 1.8 seconds per lap faster than Thomas, who has twice won FPV drone racing’s MultiGP International World Cup. The autonomous drone has several advantages in this particular race. First, it has near-perfect state estimation, thanks to a motion-capture system that covers the entire course. In other words, the drone always knows exactly where it is, as well as its precise speed and orientation. Experienced human pilots develop an intuition for estimating the state of their system, but they can’t even watch their own drone while racing since they’re immersed in the first-person view the entire time. The second advantage the autonomous drone has is that it’s able to compute a trajectory that traverses the course in a time-optimal way, considering the course layout and the constraints imposed by the drone itself. Human pilots have to practice on a course for hours (or even days) to discover what they think is an optimal trajectory, but they have no way of knowing for sure whether their racing lines can be improved or not. Elia Kaufmann prepares one of UZH's vision-based racing drones on its launch platform.Evan Ackerman/IEEE Spectrum So what, then, would make for a drone race in which humans and robots can compete fairly but doesn’t ask the robots to be less robotic or the humans to be less human-y? Three world-class human pilots were invited to Zurich for this race. Along with Thomas Bitmatta, UZH hosted Alex Vanover (2019 Drone Racing League champion) and Marvin Schäpper (2021 Swiss Drone League champion). Each pilot had as much time as they wanted on the course in advance, flying more than 700 practice laps in total. And on a Friday night in a military aircraft hangar outside of Zurich, the races began. Here are some preliminary clips from one of the vision-based autonomous drones flying computer-to-head with a human; the human-piloted drone is red, while the autonomous drone is blue: With a top speed of 80 km/h, the vision-based autonomous drone outraced the fastest human by 0.5 second during a three-lap race, where just one or two-tenths of a second is frequently the difference between a win and a loss. This victory for the vision-based autonomous drone is a big deal, as Davide Scaramuzza explains: I was at this event in Zurich, and I’d love to tell you more about it. I will tell you more about it, but as Davide says, the UZH researchers are working on publishing their results, meaning that all the fascinating details about exactly what happened and why will have to wait a bit until they’ve got everything properly written up. So stay tuned—we’ll have lots more for you on this. The University of Zurich provided travel support to assist us with covering this event in person. What SubT means for the future of autonomous robots An ANYmal robot from Team Cerberus autonomously explores a cave on DARPA’s Subterranean Challenge course. Deep below the Louisville, Ky., zoo lies a network of enormous caverns carved out of limestone. The caverns are dark. They’re dusty. They’re humid. And during one week in September 2021, they were full of the most sophisticated robots in the world. The robots (along with their human teammates) were there to tackle a massive underground course designed by DARPA, the Defense Advanced Research Projects Agency, as the culmination of its three-year Subterranean Challenge. The SubT was first announced in early 2018. DARPA designed the competition to advance practical robotics in extreme conditions, based around three distinct underground environments: human-made tunnels, the urban underground, and natural caves. To do well, the robots would have to work in teams to traverse and map completely unknown areas spanning kilometers, search out a variety of artifacts, and identify their locations with pinpoint accuracy under strict time constraints. To more closely mimic the scenarios in which first responders might utilize autonomous robots, robots experienced darkness, dust and smoke, and even DARPA-controlled rockfalls that occasionally blocked their progress.",0.0
"
        150,000 Qubits Printed on a Chip
    ",https://spectrum.ieee.org/silicon-spin-qubits,2022-07-14,,"New silicon spin qubits also emit telecom-band light The data revealing the first optical observation of spins in silicon. Two-laser scans of a single spin reveal signature spin-split central peaks; here the experimental data is visualized as an extruded mosaic. Quantum computers can theoretically solve problems no classical computer ever could—even given billions of years—but only if they possess many components known as qubits. Now scientists have fabricated more than 150,000 silicon-based qubits on a chip that they may be able to link together with light, to help form powerful quantum computers connected by a quantum Internet. Classical computers switch transistors either on or off to represent data as ones or zeroes. In contrast, quantum computers use quantum bits, also known as qubits. Because of the surreal nature of quantum physics, qubits can exist in a state called superposition, in which they are essentially both 1 and 0 at the same time. This phenomenon lets each qubit perform two calculations at once. The more qubits are quantum mechanically linked, or entangled (see our explainer), within a quantum computer, the greater its computational power can grow, in an exponential fashion. Currently quantum computers are noisy intermediate-scale quantum (NISQ) platforms, meaning their qubits number up to a few hundred at most. To prove useful for practical applications, future quantum computers will likely need thousands of qubits to help compensate for errors. There are many different types of qubits under development, such as superconducting circuits, electromagnetically trapped ions, and even frozen neon. Recently scientists have discovered that so-called spin qubits manufactured in silicon may prove especially promising for quantum computing. “Silicon spins are some of nature’s very best natural qubits,” says study cosenior author Stephanie Simmons, a quantum engineer at Simon Fraser University in Burnaby, B.C., Canada. The “spin” in spin qubits is the angular momentum of a particle such as an electron or an atomic nucleus. Spin can point up or down in a manner analogous to a compass needle that points north or south. A spin qubit can exist in a superposition where it is oriented both ways at once. Silicon spin qubits are among the most stable qubits created to date. In addition, this technology can theoretically rapidly scale up with the support of the decades of work spent developing the global semiconductor industry. Until now, scientists had measured single spins only electrically in silicon. This in turn meant that the only way to entangle spins together was electromagnetically, “which must be done with qubits in very close proximity to each other,” Simmons says. “This is hard to scale from an engineering perspective.” Now, for the first time, researchers have detected single spins optically in qubits in silicon. Such optical access to spin qubits suggests it may one day be possible to use light to “have qubits entangling with each other across a chip, or across a data center as easily as if they’re side by side,” Simmons says. The new spin qubits are based on radiation damage centers—defects within silicon created using ion implantation or irradiation with high-energy electrons. Specifically, they are T centers, each comprised of two carbon atoms, one hydrogen atom, and an unpaired electron. Each T center features an unpaired electron spin and a hydrogen nuclear spin, each of which can serve as a qubit. The electron spin can stay coherent, or stable, for more than 2 milliseconds; the hydrogen nuclear spin can remain so for more than 1.1 seconds. “Our silicon spin qubits’ long lifetimes are already quite competitive, and we have ideas on how to push them far further,” Simmons says. The researchers printed 150,000 spots dubbed “micropucks” on commercial industry-standard silicon-on-insulator integrated photonic wafers. Each micropuck ranged from 0.5 to 2.2 micrometers wide and held one T center on average, says study lead author Daniel Higginbottom, at Simon Fraser University. Under the microscope: An array of thousands of micropucks.Simon Fraser University Under a magnetic field, the spin qubit states in each T center have slightly different energies, and each emit a different wavelength of light. This lets the scientists detect the states of each spin qubit optically in these T centers. The wavelengths these spin qubits emit lie in the near-infrared O-band. This means these spin qubits can link with other qubits by emitting the kind of light often used in telecommunications networks, helping qubits work together inside a quantum processor and helping quantum computers partner over a quantum Internet. In addition, “the electron and nuclear spin qubits can be operated together—the nuclear spin as a long-lived memory qubit and the electron spin as an optically coupled communication qubit, and information can be swapped between them using microwave fields,” Simmons says. “No other physical quantum system combines high-performance quantum memories, direct and strong links to telecom photons, and the commercial prospects of silicon, which is the world’s top platform for both modern microelectronics as well as integrated photonics.” Scientists have known about T centers since the 1970s. “We do not know why we are the first to start investigating T centers as qubits in silicon,” Simmons says. “It is possible that researchers assumed that candidate spin-photon qubits in silicon were simply less likely to compete with candidates in other materials such as diamond and silicon carbide. It’s a mystery to us.” All in all, “we are most excited about the fundamental scalability of these qubits,” Simmons says. “It’s a new entrant to the international race for a quantum computer, and we think the prospects are very bright.” Although the researchers have fabricated many qubits in this new study, “these have not yet been wired up into a working quantum computer,” Simmons cautions. “The optical access to these spins will make this wiring a lot easier than many other approaches, but this technology is still very young and there is a lot of work to be done.” The scientists detailed their findings online 13 June in the journal Nature. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. If technologists can’t perfect it, quantum computers will never be big Dates chiseled into an ancient tombstone have more in common with the data in your phone or laptop than you may realize. They both involve conventional, classical information, carried by hardware that is relatively immune to errors. The situation inside a quantum computer is far different: The information itself has its own idiosyncratic properties, and compared with standard digital microelectronics, state-of-the-art quantum-computer hardware is more than a billion trillion times as likely to suffer a fault. This tremendous susceptibility to errors is the single biggest problem holding back quantum computing from realizing its great promise. Fortunately, an approach known as quantum error correction (QEC) can remedy this problem, at least in principle. A mature body of theory built up over the past quarter century now provides a solid theoretical foundation, and experimentalists have demonstrated dozens of proof-of-principle examples of QEC. But these experiments still have not reached the level of quality and sophistication needed to reduce the overall error rate in a system.",0.0
"
        RISC-V Guns for Raspberry Pi, Legacy Chips
    ",https://spectrum.ieee.org/risc-v-raspberry-pi,2022-07-13,,"Chinese market, trade-war-weary engineers welcome open chip architecture Two hardware makers are planning to offer chips later this year featuring the RISC-V free and open architecture standard, joining the US $180 Linux-capable StarFive VisionFive RISC-V board that went on sale in January. In late June, Pine64 said it was designing a single-board computer for the market now dominated by Raspberry Pi, and Xcalibyte and DeepComputing said they would begin shipping RISC-V-based laptops at the end of the summer. The 12-year-old RISC-V computer instruction set architecture standard belongs to no one and everyone, giving it unique appeal compared with Intel and Arm chips, which require licensing fees. At the same time, RISC-V’s relative novelty and reduced feature set and support are barriers to more widespread adoption. An open-source development effort last year to produce a Linux-capable mini-PC with RISC-V ended in failure. VisionFive was involved in that project, too. Like any new tech ecosystem, software support for RISC-V is more limited than in Raspberry Pi’s robust development community, says independent software engineer Leon Anavi in a review of the VisionFive. That said, he encouraged viewers to join in and contribute to the growing RISC-V community. “Consumer laptops are not the target of the RISC-V ecosystem. RISC-V is optimized for power consumption.” —William Li, Counterpoint Research RISC-V is the fifth generation of so-called reduced instruction set computers—hence the acronym—and it is focused on simplicity and power efficiency. When the Internet of Things started to take off, RISC-V’s moment seemed to have come; Huawei has used the standard in wearables since 2018. RISC-V could achieve a 25 percent market share in the IoT by 2025, Counterpoint Research estimated in late 2021. “Consumer laptops are not the target of the RISC-V ecosystem,” says analyst William Li, the author of Counterpoint report. “RISC-V is optimized for power consumption.” That has attracted AI-specific applications and cloud infrastructure ( “RISC-V Dives Into AI,”IEEE Spectrum, April 2022). The openness of the standard has also attracted markets facing limits on their use of Intel and Arm intellectual property: No government can place sanctions on open chip designs. That has been a concern for Chinese hardware makers since the trade war initiated by former U.S. president Donald Trump, and may help promote RISC-V sales in the event of restrictions on sales of Intel or Arm tech, wrote Deloitte analysts late last year. Alibaba has already taken some experimental steps in the direction of RISC-V, IEEE Spectrumwrote last year. Russian hardware makers also began exploring RISC-V, even before the severe round of sanctions other countries placed on it after its 2022 escalation of its war with Ukraine. “In the second half of this year, we will keep track of Chinese and Russian companies to see if they invest in RISC-V and creating their own IP,” says Li. One Chinese research institute, the Institute of Software at the Chinese Academy of Sciences (ISCAS), set out to build 2,000 RISC-powered laptops for development purposes, according to a July 2021 post by PLCT Lab director Wei Wu. In the PLCT Lab’s road map for 2022, Wu writes that the group will focus on enabling Linux and the most commonly used open software, including LibreOffice, for RISC-V laptops. That is one of the ironies of RISC-V being an open standard: It may gain adoption as trade barriers fragment the global market for chips. For now, however, the biggest market for RISC-V chips is in the global automotive industry, market research group Semico reported last year on behalf of the RISC-V International industry group. Semico predicted that RISC-V will continue to gain shares of the automotive market. The future for chips may in fact be mixed, in a good way: Hardware makers can mix RISC-V, Arm, and Intel components in processor packages of their own making. Intel, for one, encourages that on the grounds that customers might end up paying them to build such chips. And neither legacy-chip designer is standing still. Perhaps in response to RISC-V’s customizability, Arm, which while open charges a license fee, has been offering IoT customers more customizable options. “They’re going to try to defend their market share,“ Li predicts. Lucas Laursen is a journalist covering global development by way of science and technology with special interest in energy and agriculture. He has lived in and reported from the United States, United Kingdom, Switzerland, and Mexico. If technologists can’t perfect it, quantum computers will never be big Dates chiseled into an ancient tombstone have more in common with the data in your phone or laptop than you may realize. They both involve conventional, classical information, carried by hardware that is relatively immune to errors. The situation inside a quantum computer is far different: The information itself has its own idiosyncratic properties, and compared with standard digital microelectronics, state-of-the-art quantum-computer hardware is more than a billion trillion times as likely to suffer a fault. This tremendous susceptibility to errors is the single biggest problem holding back quantum computing from realizing its great promise. Fortunately, an approach known as quantum error correction (QEC) can remedy this problem, at least in principle. A mature body of theory built up over the past quarter century now provides a solid theoretical foundation, and experimentalists have demonstrated dozens of proof-of-principle examples of QEC. But these experiments still have not reached the level of quality and sophistication needed to reduce the overall error rate in a system.",0.0
"
        Lotfi Zadeh and the Birth of Fuzzy Logic
    ",https://spectrum.ieee.org/lotfi-zadeh,2022-07-09,,"The inventor told us how he endured decades of opposition The denunciations were sometimes extreme. “Fuzzy theory is wrong, wrong, and pernicious,” said William Kahan, a highly regarded professor of computer sciences and mathematics at the University of California at Berkeley in 1975. “The danger of fuzzy theory is that it will encourage the sort of imprecise thinking that has brought us so much trouble.” Another berated the theory’s scientific laxity. “No doubt professor Zadeh’s enthusiasm for fuzziness has been reinforced by the prevailing political climate in the United States—one of unprecedented permissiveness,” said R. E. Kalman in 1972, who is now a professor at Florida State University in Tallahassee. “Fuzzification is a kind of scientific permissiveness, it tends to result in socially appealing slogans unaccompanied by the discipline of hard scientific work.” This article was first published as “Lotfi A. Zadeh.” It appeared in the June 1995 issue of IEEE Spectrum. A PDF version is available on IEEE Xplore. The photographs appeared in the original print version. A multitude of other outspoken critics also disputed the theory of fuzzy logic, developed by Lotfi A. Zadeh in the mid-1960s. Some 20 years were to pass before the theory became widely accepted—capped by this year’s award of the IEEE Medal of Honor to Zadeh “for pioneering development of fuzzy logic and its many diverse applications.” Even today some critics remain. But Zadeh never wavered. He had found himself alone in his scientific opinions on several earlier occasions. “There is a picture of me in my study, taken when I was a student at the University of Tehran,” Zadeh told IEEE Spectrum. “I sit at a table, and above the table is a sign in Russian. ODIN, which means ‘alone.’ It was a proclamation of my independence.” Perhaps the confidence Zadeh had in his judgment despite some tough opposition, and his willingness to stand apart from the crowd, originated in a childhood of privilege. He was born in 1921 in Azerbaijan, then part of the Soviet Union, and moved to Iran at age 10. His parents—his father a businessman and newspaper correspondent, his mother a doctor—were comfortably well off. As a child, Zadeh was surrounded by governesses and tutors, while as a young adult, he had a personal servant. His career goal, for as long as he can remember, was to be an engineering professor. He never considered going into industry, he said, because money was no problem. Rather, he thought of scientific and engineering research as a type of religion, practiced at universities. Zadeh received an electrical engineering degree from the University of Tehran in 1942. But instead of taking the comfortable route—becoming a professor in Iran—he emigrated to the United States. “I could have stayed in Iran and become rich, but I felt that I could not do real scientific work there,” he told Spectrum. “Research in Iran was nonexistent.” Name Lotfi A. Zadeh Date of Birth Feb. 4, 1921 Birthplace Baku, Azerbaijan Height 178 cm Family Wife, Fay; children, Stella and Norman Education BSEE, University of Tehran, 1942; MSEE, Massachusetts Institute of Technology, 1946; PhD, Columbia University, 1949 First job Design and analysis of defense systems, International Electronics Corp., New York City, summer of 1944 Patents One U.S. patent, two Iranian patents. Favorite books “I made a conscious decision to stop reading fiction at age 15, when I was a voracious reader. I now read scientific books and other nonfiction only.” Favorite periodicals Four newspapers daily (The New York Times, San Francisco Chronicle, San Francisco Examiner, The Wall St. Journal or San Jose Mercury News), Business Week, The Economist Favorite kind of music Classical and electronic Favorite composers Sergey Prokofiev, Dimitry Shostakovich Computer A Hewlett-Packard workstation, which is used “only to print my e-mail; I dictate all my answers to my secretary.” Favorite television show “MacNeil/Lehrer Newshour” Least favorite food Any kind of shellfish Favorite restaurant Three Cs Café, an inexpensive crêperie in Berkeley, Calif. Favorite expression “No matter what you are told, take it as a compliment.” Favorite city Berkeley, Calif. Leisure activities Portrait photography (has photographed U.S. Presidents Richard Nixon and Harry Truman, as well as other notables), high-fidelity audio, garage sales Car Nissan Quest Minivan Languages spoken English, Russian, Iranian, French Airline mileage Two million miles in past 10 years on American and United Airlines alone, uncounted mileage on other airlines Key organizational memberships The IEEE, Association for Computing Machinery, International Fuzzy Systems Association, American Association for Artificial Intelligence Top awards The IEEE Medal of Honor (1995) and the Japan Honda Prize (1989) After graduation, Zadeh had a business association with the US. Army Persian Gulf Command. That enabled him to be financially independent when he came to the United States to enroll in graduate school at the Massachusetts Institute of Technology (MIT) in Cambridge. “MIT didn’t have many graduate students at the time,” Zadeh recalled, “so it was fairly easy to get in, even though the University of Teheran had no track record.” “No doubt professor Zadeh’s enthusiasm for fuzziness has been reinforced by the prevailing political climate in the United States—one of unprecedented permissiveness,"" said R. E. Kalman in 1972 MIT, it turned out, was an easy ride after the demanding course work Zadeh had faced in Tehran. His choice of subject for his master’s thesis, though, marked one of the first times he would sail against the prevailing technical winds. He chose to study helical antennas, a subject deemed unreasonable by the professor who had taught him antenna theory. Undaunted, Zadeh found another professor to supervise his work. “I felt that my judgment was correct, and the judgment of people who supposedly knew much more about the subject than I did was not correct,” Zadeh said. “This was one of many such situations. Helical antennas came into wide use in the ‘40s and OS, and my judgment was vindicated.” By the time Zadeh received his master’s degree in 1946, his parents had moved from Tehran to New York City. So instead of continuing at MIT, he searched out a post as an instructor at New York City’s Columbia University and began his Ph.D. studies there. His thesis on the frequency analysis of time-varying networks considered ways of analyzing systems that change in time. “It was not a breakthrough,” he recalled, “but it did make an impact and opened a certain direction in its field.” What he views as his first technical breakthrough came in 1950, when, as an assistant professor at Columbia, he coauthored a paper with his doctoral thesis advisor, John R. Ragazzini, on “An extension of Wiener’s theory of prediction.” This analysis of prediction of time series is often cited as an early classic in its field. This thesis introduced the use of a finite, rather than an infinite, preceding time interval of observation for subsequent smoothing and prediction in the presence of multiple signals and noises. This, and Zadeh’s other work while he was at Columbia, made him a well-known figure in the analysis of analog systems. As Zadeh was pretty much entrenched at Columbia, he surprised his colleagues when he packed up in 1959 and moved to the University of California at Berkeley. “I had not been looking for another position,” Zadeh said, “so the offer from Berkeley was unexpected.’’ It came from electrical engineering department chairman John Whinnery, who called him at home over the weekend and offered him a position. “If my line had been busy, I believe l would still be at Columbia,” Zadeh told Spectrum. Whinnery recalls it slightly differently. He had heard from a colleague that Zadeh had been toying with the idea of leaving Columbia. Minutes later, Whinnery picked up the phone and called him, arranged to meet him in New York City for dinner, and soon afterward hired him. Berkeley was then growing rapidly, and Whinnery was on the lookout for young scholars who were considered brilliant in their fields. Zadeh fit the bill. For Zadeh, moving to Berkeley was a simple decision to make: “I was happy at Columbia, but the job was too soft. It was a comfortable, undemanding environment; I was not challenged internally. I realized that at Berkeley my life would not be anywhere near as comfortable, but I felt that it would be good for me to be challenged.” Zadeh has never regretted the decision. To this day he remains at Berkeley, although by now as professor emeritus. A number of departmental colleagues felt that the trend toward computer science was a fad. At Berkeley, Zadeh initially continued his work in linear, nonlinear, and finite state systems analysis. But before long he became convinced that digital systems would grow in importance. Appointed as chairman of the electrical engineering department, he decided to act on that conviction, and immediately set about strengthening the role of computer science in the department’s curriculum. He also lobbied the electrical engineering community nationwide to recognize the importance of computer science. Once again, he found himself fighting conventional wisdom. A number of departmental colleagues felt that the trend toward computer science was a fad, and that computer science should not be assigned a high departmental priority. ‘They accused me of being an Yves St. Laurent,” Zadeh recalled, “a follower of fads.” Elsewhere, professors in the mathematics department, along with the head of the computer center, were lobbying to set up their own computer science department. Zadeh fought this battle as he has fought others, with polite persistence, his former chairman recollected. “We had many differences of opinion when he was chairman,” Whinnery said. “When he couldn’t convince people, he would get upset, but [even now] you can only tell this by the expression on his face. He doesn’t yell or scream. Then he goes ahead and does what he was going to do anyway. And mostly he’s been right, particularly about the importance of computers in electrical engineering.” Said Earl Cox, chief executive officer of the Metus Systems Group, Chappaqua, N.Y., who has known Zadeh since the ’70s: “I’ve never seen him anger anybody, even though he prides himself in going his own way, in thinking his own thoughts.” (Zadeh is also known for encouraging others to be independent. He insists his graduate students publish in their own name, noted former student Chin L. Chang, who is now president of Nicesoft Corp., Austin, Texas. That practice goes against custom.) Zadeh finally got his way in 1967: the name of the department was changed to electrical engineering and computer science (EECS). A separate computer science department was also established in Berkeley’s College of Letters, but after a few years it folded and became absorbed into EECS. While he was focusing on systems analysis, in the early 1960s, Zadeh began to feel that traditional systems analysis techniques were too precise for real-world problems. In a paper written in 1961, he mentioned that a new technique was needed, a “fuzzy” kind of mathematics. At the time, though, he had no clear idea how this would work. That idea came in July 1964. Zadeh was in New York City visiting his parents and planned to leave soon for Southern California, where he would spend several weeks at Rand Corp working on pattern recognition problems. With this upcoming work on his mind, his thoughts often turned to the use of imprecise categories for classification. “One night in New York,” Zadeh recalled, “I had a dinner engagement with some friends It was canceled, and I spent the evening by myself in my parents’ apartment 1 remember distinctly that the idea occurred to me then to introduce the concept of grade of membership [concepts that became the backbone of fuzzy set theory]. So it is quite possible that if that dinner engagement had not been canceled, the idea would not have occurred to me.” Fuzzy technology, Zadeh explained, is a means of computing with words—bigger, smaller, taller, shorter. For example, small can be multiplied by a few and added to large, or colder can be added to warmer to get something in between. Zadeh published his first fuzzy paper in 1965, convinced that he was onto something important, but wrote only sparingly on the topic until after he left Berkeley’s electrical engineering department chairmanship in 1968. Since then, fuzzy sets have been his full-time occupation. Once the issue of classification had been solved, Zadeh could develop the theory of fuzzy sets quickly. Two weeks later he had a fairly fleshed-out group of concepts to present to his collaborator at Rand, Richard Bellman. “His response was enthusiastic,” Zadeh said, “and that was a source of encouragement to me-though had he been very critical, I wouldn’t have changed my mind.” Since he was Berkeley’s electrical engineering department chairman at the time and engaged in his struggle over the place of computer science at the university, Zadeh had little time to work on his new theory of fuzzy sets. He published his first paper in 1965, convinced that he was onto something important, but wrote only sparingly on the topic until after he left the department chairmanship in 1968. Since then, fuzzy sets have been his full-time occupation. “I continue to be an active player,” he said. “I am not merely an elder statesman who rests on his laurels. I give many talks, and this puts me under pressure. I must constantly think of new ideas to talk about and keep up with what others are doing.” Acceptance of fuzzy set theory by the technical community was slow in coming. Part of the problem was the name—“fuzzy” is hardly proper terminology. And Zadeh knew it. “I was cognizant of the fact that it would be controversial, but I could not think of any other, respectable term to describe what I had in mind, which was classes that do not have sharp boundaries, like clouds,” he said. “So I decided to do what I thought was right, regardless of how it might be perceived. And I’ve never regretted the name. I think it is better to be visible and provocative than to be bland.” And, as expected, fuzzy theory did cause controversy. Some people rejected it outright because of the name, without knowing the content. Others rejected it because of the theory’s focus on imprecision. “I’ve never regretted the name. I think it is better to be visible and provocative than to be bland.” —Lotfi Zadeh In the late 1960s, it even garnered the passing attention of Congress as a prime example of the waste of government funds (much of Zadeh’s research was being funded by the National Science Foundation). Former Senator William Proxmire (D-Wis.), the force behind the Golden Fleece Awards that honored such government boondoggles as $600 toilet seats, sent a letter to the foundation suggesting that such “fuzzy” garbage they were supporting should earn a Golden Fleece nomination. A flurry of correspondence from Zadeh and the foundation emerged in defense of the work. Zadeh remembers the challenge of developing his theories “in the face of opposition, even hostility. Someone with a thinner skin would have been traumatized,” he said. And Cox remarked, “He meets people who have written some really nasty things, and he’s nice to them.” But, observed Berkeley’s Whinnery, “I do think this lack of acceptance bothered him, although he now describes it with some humor.” Eventually, fuzzy theory was taken seriously—by the Japanese. Eventually, fuzzy theory was taken seriously—by the Japanese. And their implementations of it surprised even Zadeh. He at first had expected fuzzy sets to apply to fields in which conventional analytic techniques had been ineffectual, for work outside of the hard sciences, for work in philosophy, psychology, linguistics, biology, and so on. He also thought that the theory might apply to control systems, in engine control, for example. But he never expected it to be used in consumer products, which today is perhaps its biggest application, thanks to Japanese electronics companies. Matsushita Electric Industrial Co. was the first to apply fuzzy theory to a consumer product, a shower head that controlled water temperature, in 1987. Now numerous Japanese consumer products—dishwashers, washing machines, air conditioners, microwave ovens, cameras, camcorders, television sets, copiers, and even automobiles—quietly apply fuzzy technology. These products make use of fuzzy logic combined with sensors to simplify control. For example, cameras have several focusing spots and use fuzzy’s IF-THEN rules to calculate the optimal focus; camcorders use fuzzy logic for image stabilization; and washing machines use sensors to detect how dirty the water is and how quickly it is clearing to determine the length of wash cycles. The introduction of fuzzy products by the Japanese riveted press attention on this apparently “new” technology (some two decades after Zadeh had developed the theory). Growing acknowledgment of the theory by his colleagues followed, although some still reject it. Acceptance, colleagues say, has somewhat changed Zadeh. “Since fuzzy logic has turned into something with so much panache, and he has finally come into his own after being ignored for so many years, I think Lotfi has come out of his shell,” said Cox. “Had I not launched that theory, I would fall into the same category as many professors—be reasonably well known… but not have made a long-lasting impact.” —Lotfi Zadeh To date, hundreds of books have been published on the topic, and some 15 000 technical papers have been written (most, it seems, piled around his office, where stacks of papers leave only a narrow path from the door to his desk). Zadeh is now known as the Father of Fuzzy. “Had I not launched that theory,” said Zadeh, “I would fall into the same category as many professors—be reasonably well known, have attained a certain level of recognition, and written some books and papers, but not have made a long-lasting impact. So I consider myself to have been lucky that this thing came about.” “The important criterion of your impact is: has what you have done generated a following? With fuzzy sets, I can definitely say, ‘Yes.’” Editor’s note: Lotfi Zadeh died in 2017 at the age of 96. Tekla S. Perry is a senior editor at IEEE Spectrum. Based in Palo Alto, Calif., she's been covering the people, companies, and technology that make Silicon Valley a special place for more than 40 years. An IEEE member, she holds a bachelor's degree in journalism from Michigan State University. ...Or did the metaverse just turn a shade more uncannily creepy? Matthew S. Smith writes IEEE Spectrum's Gizmo column and is a freelance consumer-tech journalist. An avid gamer, he is a former staff editor at Digital Trends and is particularly fond of wearables, e-bikes, all things smartphone, and CES, which he has attended every year since 2009. Mesh to MetaHuman lets creators import a facial mesh to create a photorealistic 3D character. Creating your virtual clone isn’t as difficult as you’d think. Epic Games recently introduced Mesh to MetaHuman, a framework for creating photorealistic human characters. It lets creators sculpt an imported mesh to create a convincing character in less than an hour. “It’s incredibly simple compared to a lot of other tools,” says Stu Richards (a.k.a. Meta Mike), partner success lead at GigLabs and Cofounder of Versed. “I’d compare it to a character creator in a game.” This e-nose can detect glucose levels with 90 percent accuracy Michelle Hampson is a freelance writer based in Halifax. She frequently contributes to Spectrum's Journal Watch coverage, which highlights newsworthy studies published in IEEE journals. This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore.",0.0
"
        What Is the Future of Quantum-Proof Encryption?
    ",https://spectrum.ieee.org/post-quantum-cryptography-nist,2022-07-08,,"Bright, according to officials at NIST’s Post-Quantum Cryptography program The first four algorithms that NIST has announced for post-quantum cryptography are based on structured lattices and hash functions, two families of math problems that could resist a quantum computer’s assault. On Tuesday, the National Institute of Standards and Technology (NIST) announced its first quantum-resistant algorithms—new encryption that will become the standard to guard against attacks by quantum computers, which are not yet here. The four algorithms are CRYSTALS-Kyber, for general encryption, and three schemes for digital encryption: CRYSTALS-Dilithium, FALCON, and SPHINCS+. Over the past few decades, NIST has managed encryption standards, introducing and vetting the schemes that protect and authenticate valuable digital information—from bank transactions to emails to your Netflix password. These encryption schemes are easy for the user to encode and decode, but hard for an attacker to break. This one-way functionality is like mixing colors: It’s easy to mix shades of blue and yellow to make green, but hard to tell by looking at a green which shades were used to create it. While these methods have been robust against classical attacks, they are known to be vulnerable to quantum algorithms. Quantum computers capable of breaking existing encryption with these algorithms are a ways off, but researchers say there’s no time to wait. Post-quantum cryptography is the future, and it’s here now. Last week, we spoke with Dustin Moody, a mathematician at NIST leading the post-quantum cryptography standardization process. What is post-quantum cryptography, and why does NIST need a standardization process for it? Dustin MoodyNIST Dustin Moody: Researchers from a variety of backgrounds have been working on building what’s called a quantum computer. If a quantum computer is built that is large enough, there’s an algorithm you could run on this quantum computer that would break several of the most widely used cryptosystems that we have implemented and use today around the world. So post-quantum crypto is preparing new cryptosystems to replace those that would be vulnerable to attacks from a big enough quantum computer. And NIST is doing this because a few of the cryptosystems that we have standardized, namely the ones dealing with public-key cryptography, would be vulnerable. So we want to replace those standards with new ones that would not be vulnerable to attacks from a quantum computer. “You can actually be at risk from a quantum computer, even though a [high-performance] quantum computer does not yet exist. This is often called ‘harvest now, decrypt later.’”—Dustin Moody, NIST Now there’s two separate things you’re replacing, as I understand it; there’s both a key and a signature. Can you talk about both of those—and the difference between the two? Moody: With cryptography, you have many different kinds of tools that are needed to protect your information. One of these that people first think of is just encryption and scrambling your data, so your enemy can’t read it. That is typically enabled using public-key encryption. You then create a shared secret that you will actually use for a key for another type of cryptosystem that can do encryption a lot faster, a block cipher called AES. But this public-key encryption is used to create that initial key. So that’s the first functionality that we need to replace. The second is digital signatures. Instead of signing your name at the bottom of the letter to prove that you are the author, you can digitally do the same thing using cryptographic techniques. And that is also known to be vulnerable to a quantum computer. So it’s the second functionality that we’re trying to replace. The largest quantum computer is about 200-something qubits right now, and they’re not fault-tolerant. The largest number that one of these machines has factored with Shor’s algorithm is 21. There are various estimates out there for what it would take to crack RSA 2048, but you would need thousands of qubits at a minimum, and they would all need to be very, very high quality. What’s the rush? Why are we doing this now? You started this process in 2016. This is years, if not decades, ahead of time. Moody: First, standardizing cryptosystems and then getting them into products around the world takes a surprisingly long amount of time. From our past experience, switching from one crypto algorithm to another can take 10 to 20 years. And since we need to have this done before a quantum computer comes out, we've got to get all that work done ahead of time. And it can’t be rushed too quickly. You don’t have confidence in a cryptosystem if it was invented two weeks ago. We need years of people analyzing these algorithms to have confidence in their security. That’s reason one: It takes time to get ready. And then reason two is you can actually be at risk from a quantum computer, even though a quantum computer does not yet exist. This is often called “harvest now, decrypt later.” It’s the idea that your enemy could copy your data, which is encrypted, and they can hold onto it right now. They can’t read it. But maybe a quantum computer comes out in 10 years, and then they can get access to your data. If the information you’re protecting is valuable enough, then you’re already in trouble because of that threat. So that’s another reason that we need to be well ahead of the time that a quantum computer is built. How real is harvest now, decrypt later? I understand that there is the possibility, but is this really happening? Are people downloading data and squirreling it away so that when they get their quantum computers up and running, they can go through it? Moody: So, I don’t have access to classified material. NIST is not at that level, like the NSA and other people do that. But people I have talked to say that this is absolutely a threat and that it is occurring now at the nation-state level. “We’re going to standardize a number of things so that we have a diversity of different mathematical problems to base our security on.”—Dustin Moody, NIST The vulnerability in public-key encryption is because of quantum computers’ ultimate ability to have almost exponentially faster factoring of large prime numbers. What types of strategies are being used to create these new public keys? Can you run through a few of them? Moody: In crypto, we like to use ideas that have been around for a while. Since [Peter] Shor’s algorithm was discovered back in the ’90s, researchers have been looking into this. There are three big families where a lot of the solutions are coming from. The most popular one involves what are called lattices. This is a mathematical structure. You take basis vectors; you take integral linear combinations of them. And you can do some pretty interesting things cryptographically. There are no known quantum algorithms that do better than the classical attacks on cryptosystems based on lattices. Cryptosystems based on lattices seem to be the top contenders in terms of key size and efficiency. So it wasn’t surprising that we received a lot of lattice submissions. The second family is based on what are called error-correcting codes or code-based cryptography. These codes have been used in information security for a long time, because data gets sent on noisy channels. If you use error-correcting codes, you can account for the error and recover the message that was originally meant to be sent. We use ideas similar to what’s used in lattices with these codes. And there, they seem to be just a half-step behind the lattices in terms of key sizes and efficiency, but they’re pretty good as well. So we saw a number of code-based submissions. The third biggest family includes some signature schemes based on what’s called multivariate cryptography. You’ll use a multitude of variables, x1 to xn, and create a system of quadratic equations. And it’s very easy to define and evaluate one of these systems, but very hard to solve. So it turns out that works well for digital-signature schemes. Sort of like if you had a big squiggly line on a graph? It’s easy to draw, but it’s hard to actually figure out what function was used to plot it. Moody: Yeah, that’s the idea in a lot of cryptosystems. There’s a one-way thing—if you know the secret, you can do it quickly and easily. That allows you to decrypt or sign. An attacker doesn’t know that secret, and they can’t do anything. A signature system called Rainbow was recently broken. I understand roughly how you might test a classical encryption system, because we have classical computers. But we don’t have quantum computers. So how do you verify that these methods are actually quantum secure? Moody: There’s no guarantee that no one really is going to come along and come up with some new idea. That’s the case with the cryptosystems we use today in classical computers as well. Sure, I know RSA isn’t provably hard. Moody: When you add quantum computers to the mix, you get access to more tools, you get access to more quantum algorithms. And the best we can say is that a lot of smart people spend time looking at these, nobody has found any attacks, and they don’t seem to be getting close. The avenues don’t appear to be useful at all. That’s how we set up our competition—make sure that we had experts in the field, spend a number of years looking at these cryptosystems, and see that there are no attacks. And actually, Rainbow—it wasn’t a quantum attack that broke it; it was a classical attack. So these things have to be secured from both quantum computers as well as all the classical attacks as well. So at this point in the process, you really have narrowed it down to a couple of finalists from an initial selection. [Editor’s note: This took place prior to the July 5th announcement.] Moody: Yeah, we started out with 82. And right now we have seven finalists and eight alternates at the end of round three, which is going to end within a very short time, probably a week or two, or we’ll have ones that we will standardize. What happens then? Moody: We’re going to begin writing standards. These will be the algorithms that will be most widely used. We will also name a few that we’ll continue on into a fourth round of evaluation so that we have a few other algorithms. Because this is a new research field, we don’t want to put all our eggs in one basket and only have lattice algorithms, and then an attack comes along and we don’t have anything else. We’re going to standardize a number of things so that we have a diversity of different mathematical problems to base our security on. We’ll start writing the standards, which will take a year or two, and there’ll be a few that continue to be evaluated and might be selected for standardization at the end of the fourth round. What’s the difference between finalists and those getting standardized? Moody: Once we’ve named these algorithms, people know that these are going to be the ones that are gonna be in widespread use. There already are implementations, but people will be able to focus on the standardized ones. It’s an important step on the road to adoption. The industry needs standards before adoption can really get widespread. I understand that there are some patent disputes related to the algorithms. Moody: In crypto, most people don’t like patents, because they tend to hinder adoption. Companies don’t like to adopt algorithms that might be patented. At the beginning of our process, all the submitters had to let us know of any patents that they had. We were aware of some patents, which were not from the submitters—they were from some third parties. NIST has been in talks with these other parties to negotiate a solution that would be beneficial to both parties. I think you’ll see with our announcement that we’re happy with the outcome. We feel that the algorithms will be able to be widely adopted by people around the world. Dan Garisto is a freelance science journalist who covers physics and other physical sciences. His work has appeared in Scientific American, Physics, Symmetry, Undark, and other outlets. If technologists can’t perfect it, quantum computers will never be big Dates chiseled into an ancient tombstone have more in common with the data in your phone or laptop than you may realize. They both involve conventional, classical information, carried by hardware that is relatively immune to errors. The situation inside a quantum computer is far different: The information itself has its own idiosyncratic properties, and compared with standard digital microelectronics, state-of-the-art quantum-computer hardware is more than a billion trillion times as likely to suffer a fault. This tremendous susceptibility to errors is the single biggest problem holding back quantum computing from realizing its great promise. Fortunately, an approach known as quantum error correction (QEC) can remedy this problem, at least in principle. A mature body of theory built up over the past quarter century now provides a solid theoretical foundation, and experimentalists have demonstrated dozens of proof-of-principle examples of QEC. But these experiments still have not reached the level of quality and sophistication needed to reduce the overall error rate in a system.",0.0
"
        D-Wave’s 500-Qubit Machine Hits the Cloud
    ",https://spectrum.ieee.org/d-wave-quantum-computer,2022-07-07,,"Experimental prototype offers sneak peek of 7,000-qubit quantum computer This animation depicts some of the concepts behind the quantum-computer company D-Wave’s topology for their latest prototype annealing quantum computer—Advantage2. When quantum-computing pioneer D-Wave releases its next-generation Advantage2 system in 2023 or 2024, the company expects its 7,000-qubit machine to be the most powerful quantum computer of its kind in the world. Now D-Wave is making an experimental prototype of Advantage2 immediately available for use over the cloud. Classical computers switch transistors either on or off to symbolize data as ones or zeroes. In contrast, quantum computers use quantum bits, or “qubits.” Because of the strange nature of quantum physics, qubits can exist in a state called superposition, in which they are essentially both 1 and 0 at the same time. This phenomenon lets each qubit perform two calculations at once. The more qubits are quantum mechanically linked, or entangled, within a quantum computer, the greater its computational power can grow, in an exponential fashion. The standard approach toward building quantum computers, called the gate model, involves arranging qubits in circuits and making them interact with each other in a fixed sequence. In contrast, D-Wave—based in Burnaby, B.C, Canada—has long focused on what are called annealing quantum computers. Quantum cousins of classical annealing computers, these machines find a lowest energy state by slowly cooling it down—in much the same way that metals and crystals are sometimes tempered so as to minimize imperfections. Quantum annealing machines, then, start off with a set of qubits whose interactions at their lowest energy state, called the ground state, represent the correct answer for a specific problem the researchers programmed it to solve. “Given the early positive results, we wanted to get it into the hands of developers and researchers now for exploration and learning.”—Emile Hoskinson, D-Wave The ideal application for annealing quantum computers may be solving optimization problems, says Emile Hoskinson, an experimental physicist and the director of quantum-annealing products at D-Wave. These seek to find the best answer from all possible solutions, such as mapping the fastest route from point A to point B. For instance, imagine trying to find the lowest point on a vast landscape covered in hills and valleys. A classical computer might start at a random spot on the surface and look around for a lower spot to explore until it cannot walk downhill anymore. This approach can often get stuck in a “local minimum,” a valley that is not actually the very lowest point on the surface. On the other hand, annealing quantum computers could make it possible to start at many spots on the surface at the same time, reducing the chance of becoming trapped in a local minimum. They can even essentially tunnel through a hill to see if there is a lower valley beyond it, or share data from multiple spots to find patterns that might lead to deeper points. Founded in 1999, D-Wave bills itself as the world’s first commercial supplier of quantum computers. The company has longprovedcontroversial, with many critics over the years questioning whether its machines are any more powerful than regular computers. Nevertheless, D-Wave has claimed its share of high-profile clients over the years. It sold its first quantum computing system, the 128-qubit D-Wave One, to Lockheed Martin in 2011, and shipped its 512-qubit D-Wave Two to NASA’s Quantum Artificial Intelligence Lab—launched in partnership with Google and the Universities Space Research Association—in 2013. When D-Wave’s Advantage2 comes online next year or the year after, it will be the world’s most powerful annealing quantum computer, Hoskinson says. Now the company is releasing an experimental prototype of Advantage2, with all the core functionality of the full-scale product available for testing. “Our broad portfolio of enterprise customers—such as Volkswagen, Save-on-Foods, Denso, Toyota, BBVA, NEC, Accenture, and Lockheed Martin—have built hundreds of early quantum applications in diverse areas such as resource scheduling, mobility, logistics, drug discovery, portfolio optimization, manufacturing processes, and much more,” Hoskinson says. “We expect the Advantage2 can be used to address problems with even greater complexity. When we talk about addressing a broader swath of problems, we mean the improved technology can tackle problems that are larger, more complex, and that cover a wider range of use cases across different verticals. It will address the same problems as before, but better and faster.” The prototype holds more than 500 superconducting flux qubits. The new design of the qubits enables what D-Wave calls its Zephyrtopology, which supports 20-way interqubit connectivity, up from 15 in the company’s prior generation, Advantage. “An analogy to illustrate the importance of connectivity is a social network, in which influence, and the complexity of interactions, grows with the number of connections between nodes,” Hoskinson says. “Complexity can mean problem size, such as the number of variables, constraints, and so on. It can also mean commercial and business application context—for example, if a particular business needs a fast time-to-solution because of business demand, such as changing schedules, supply-chain dependencies, and so on, then that makes the problem more complex. We see Advantage2 solving problems both better and faster, addressing both of these types of complex use cases.” The new qubit design also supports a higher energy scale, which makes the qubits less vulnerable to disruption from thermal fluctuations. This in turn reduces error rates in quantum computation. In tests, when the prototype’s qubits were arranged in their standard “native” topology, it found better solutions in up to 89 percent of cases when compared with Advantage, the company notes. When it came to problems requiring greater connectivity between qubits, multiple qubits can get linked together in a strategy called embedding, and Advantage2’s greater interqubit connectivity led the prototype to find better-quality solutions than Advantage in up to 82 percent of such problems. “Many commercially interesting problems require embedding,” Hoskinson says. “With the new Advantage2 Zephyr topology and increased energy scale, we see performance improvements for both native and embedded problems.” D-Wave did not originally plan to make its prototype available publicly, “but given the early positive results, we wanted to get it into the hands of developers and researchers now for exploration and learning,” Hoskinson says. “It’s about learning from our community to maximize commercial application performance as we build towards the final full-scale Advantage2 quantum computer.” Hoskinson notes that D-Wave is also working on a new fabrication process that should lead to dramatically less noise in the company’s quantum computers, increasing their chances of finding high-quality solutions. “While the prototype was developed in our current rapid-development fabrication stack, the eventual Advantage2 product will be produced in an all-new lower-noise stack,” he says. “We have early results for the new stack that show a seven-times reduction in low-frequency flux noise, a three-times reduction in integrated flux noise, and an order of magnitude reduction in high-frequency flux noise. This will go a long way toward further improving performance in the full Advantage2 system.” D-Wave made the prototype available over its Leap quantum cloud service in June. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. If technologists can’t perfect it, quantum computers will never be big Dates chiseled into an ancient tombstone have more in common with the data in your phone or laptop than you may realize. They both involve conventional, classical information, carried by hardware that is relatively immune to errors. The situation inside a quantum computer is far different: The information itself has its own idiosyncratic properties, and compared with standard digital microelectronics, state-of-the-art quantum-computer hardware is more than a billion trillion times as likely to suffer a fault. This tremendous susceptibility to errors is the single biggest problem holding back quantum computing from realizing its great promise. Fortunately, an approach known as quantum error correction (QEC) can remedy this problem, at least in principle. A mature body of theory built up over the past quarter century now provides a solid theoretical foundation, and experimentalists have demonstrated dozens of proof-of-principle examples of QEC. But these experiments still have not reached the level of quality and sophistication needed to reduce the overall error rate in a system.",0.0
Sony’s racing AI destroyed its human competitors by being nice (and fast),https://www.technologyreview.com/2022/07/19/1056176/sonys-racing-ai-destroyed-its-human-competitors-by-being-nice-and-fast/,2022-07-19,"<p>What Gran Turismo Sophy learned on the racetrack could help shape the future of machines that can work alongside humans, or join us on the roads.</p>
","“Wait, what? How?” Emily Jones wasn’t used to being left behind. A top sim-racing driver with multiple wins to her name, Jones jerked the steering wheel in the e-sports rig, eyes fixed on the screen in front of her: “I’m pushing way too hard to keep up— How does it do that?” Her staccato commentary intercut with squealing tires, Jones flung her virtual car around the virtual track at 120 miles per hour—then 140, 150—chasing the fastest Gran Turismo driver in the world. Built by Sony AI, a research lab launched by the company in 2020, Gran Turismo Sophy is a computer program trained to control racing cars inside the world of Gran Turismo, a video game known for its super-realistic simulations of real vehicles and tracks. In a series of events held behind closed doors last year, Sony put its program up against the best humans on the professional sim-racing circuit. What they discovered during those racetrack battles—and the ones that followed—could help shape the future of machines that work alongside humans, or join us on the roads. Back in July 2021, Jones, who is based in Melbourne, Australia, and races for the e-sports team Trans Tasman Racing, didn’t know what to expect. “I wasn’t told much about it,” she says now, a year later. “‘Don’t do any practice,’ they said. ‘Don’t look at its lap times.’ I was like, it’s obviously going to be good if they’re keeping it secret from me.” In the end, GT Sophy beat Jones’s best lap by 1.5 seconds. At a level where records are smashed in millisecond increments, 1.5 seconds is an age. But Sony soon learned that speed alone wasn’t enough to make GT Sophy a winner. The program outpaced all human drivers on an empty track, setting superhuman lap times on three different virtual courses. Yet when Sony tested GT Sophy in a race against multiple human drivers, where intelligence as well as speed is needed, GT Sophy lost. The program was at times too aggressive, racking up penalties for reckless driving, and at other times too timid, giving way when it didn’t need to. Sony regrouped, retrained its AI, and set up a rematch in October. This time GT Sophy won with ease. What made the difference? It’s true that Sony came back with a larger neural network, giving its program more capabilities to draw from on the fly. But ultimately, the difference came down to giving GT Sophy something that Peter Wurman, head of Sony AI America, calls “etiquette”: the ability to balance its aggression and timidity, picking the most appropriate behavior for the situation at hand. This is also what makes GT Sophy relevant beyond Gran Turismo. Etiquette between drivers on a track is a specific example of the kind of dynamic, context-aware behavior that robots will be expected to have when they interact with people, says Wurman. An awareness of when to take risks and when to play it safe would be useful for AI that is better at interacting with people, whether it be on the manufacturing floor, in home robots, or in driverless cars. “I don’t think we’ve learned general principles yet about how to deal with human norms that you have to respect,” says Wurman. “But it’s a start and hopefully gives us some insight into this problem in general.” GT Sophy is just the latest in a line of AI systems that have beaten the world’s best human players at various games, from chess and Go to video games like Starcraft and DOTA. But Gran Turismo offered Sony a new kind of challenge. Unlike other games, especially those that are turn-based, Gran Turismo calls on its best players to control a vehicle at the limits of what’s physically possible, in real time, and in close proximity with other players all trying to do the same. Cars hurtle around corners at more than 100 miles per hour with only inches between them. At those speeds, the smallest errors can lead to a crash. Gran Turismo captures real-world physics in extreme detail, simulating the aerodynamics of a car and the friction of its tires on the track. The game is sometimes used to train and recruit drivers for real-world racing. “It does an excellent job with the realism,” says Davide Scaramuzza, who leads the robotics and perception group at the University of Zurich in Switzerland. Scaramuzza was not involved with GT Sophy, but his team has used Gran Turismo to train a previous AI driver—though not one that was ever tested against humans. Virtual cycling races boomed during the pandemic. Now the best riders are getting ready for the race of their lives. GT Sophy doesn’t get the same view of the game that human players do. Instead of reading pixels off a screen, the program takes in updates about the position of its car on the track and the positions of the cars around it. It also gets sent information about the virtual physical forces affecting its vehicle. In response, GT Sophy tells the car to turn or brake. This back-and-forth between GT Sophy and the game happens 10 times a second, which Wurman and his colleagues claim matches the reaction time of human players. Sony used reinforcement learning to train GT Sophy from scratch via trial and error. At first the AI struggled to keep a car on the road. But after training on 10 PlayStation 4s, each running 20 instances of the program, GT Sophy matched Gran Turismo’s built-in AI, which amateur players use for practice, in around eight hours. In 24 hours it was laying down lap times near the very top of an online leaderboard of 17,700 human players. It took nine days before GT Sophy stopped shaving fractions of a second off its lap times. By then it was faster than any human. Sony’s AI learned how to drive at the limits of what the game allowed, pulling off moves that human players can only gawk at. In particular, Jones was struck by the way GT Sophy took corners, braking early before accelerating out on a much tighter line than she was. “It used the curve in a weird way, doing stuff that I just didn’t even think of,” she says. For example, GT Sophy often drops a wheel onto the grass at the edge of the track and then skids into turns. “You don’t want to do that because you’ll make a mistake. It’s like a controlled crash,” she says. “I could maybe do that one in a hundred times.” GT Sophy was quick to master the game’s physics. The bigger problem was the referees. At a professional level, Gran Turismo races are watched by human judges, who can award penalty points for dangerous driving. Racking up penalties was a key reason for GT Sophy’s loss in the first round of races last July, even though it was faster than any of the human drivers. And learning to avoid them made all the difference in round two. Wurman has been working on GT Sophy for several years. There’s a painting of two cars jostling for position hanging on the wall behind his desk. “It’s a GT Sophy car passing Yamanaka,” says Wurman, referring to Tomoaki Yamanaka, one of the four Japanese professional sim-racing drivers who competed against GT Sophy last year. Wurman can’t recall which race the painting is taken from. If it’s the October event, Yamanaka may well be having a great time, pushing himself against a tough but fair opponent. If it’s the July event, he’s probably cussing at the computer. Yamanaka’s teammate Takuma Miyazono told me about that July race via a translator. “There were a few times where we were pushed off the track because of how aggressively it would go into the corners,” he said. “That threw us off. The human drivers had to hold back on the turns to avoid being run off the road.” Training the AI to play fair without losing its competitive edge was hard, says Wurman. The human referees make subjective judgments that depend on context, making it difficult to turn them into simple dos and don’ts that the AI can learn from. The Sony researchers tried giving the AI lots of different cues, adjusting them as they went, hoping to find a mix that worked. They tried penalizing it if it went off the track or bumped into wall. They penalized it for crashes it caused, and for crashes where a referee’s call might go either way. They experimented with different-size penalties for each and checked how GT Sophy’s driving changed in response. Sony also upped the competition GT Sophy faced in its training. Before, it had trained mostly against previous versions of itself. Leading into the October rematch, Sony tested its AI every week or two against top drivers, tweaking it constantly. “That gave us the kind of feedback we needed to find the right balance between aggression and timidity,” Wurman says. It worked. When Miyazono went up against GT Sophy three months later, the aggression was gone—but the AI was not simply backing down. “When you go into a corner with two cars side by side, it leaves just enough space for your car to go through,” he told me. “It really does feel like you’re racing with another person.” “You get a different sort of passion and fun from driving against something that reacts that way,” he added. “That was something that really left a big impression on my mind.” Scaramuzza is impressed with Sony’s work. “We measure the progress of robotics against what humans can do,” he says. But Elia Kaufman, who works with Scaramuzza at the University of Zurich, points out that it is still human researchers who choose which of GT Sophy’s learned behaviors to bake in during training. “They’re the ones who judge what is good racing etiquette or not,” he says. “It would be really interesting if that could be done in an automated way.” Such a machine would not only have good manners but could recognize what good manners were, and be able to adapt its behavior to new settings. Scaramuzza’s team is now applying its Gran Turismo research to real-world drone racing, training an AI to fly using raw video input instead of data from a simulation. Last month they invited two world-champion drone racers to take on the computer. No prizes for guessing who won. “It was very interesting to look at their faces after they saw our AI racing,” says Scaramuzza. “They were mind-blown.” Scaramuzza thinks that making the jump to the real world is essential for true progress in robotics. “There will always be a mismatch between simulation and the real world,” he says. “This is something that gets forgotten when people talk about AI making incredible progress. In terms of strategy, yes. In terms of real-world deployment, we are definitely not there yet.” For now, Sony is sticking to games. It plans to put GT Sophy in a future version of Gran Turismo. “We’d like this to become part of the product,” says Peter Stone, executive director of Sony AI America. “Sony’s an entertainment company, and we want this to make the game more entertaining.” Jones thinks the sim-racing community could learn a lot from GT Sophy once more people get a chance to see it drive. “There will be tracks where we’re like, hang on a second, we’ve been doing this for years but there’s actually a faster way of doing it.” Miyazono has already tried to copy some of the lines the AI takes around corners, now that it has shown him they can be done. “If the benchmark changes, everybody rises up as well,” says Jones.",0.0
This robot dog just taught itself to walk,https://www.technologyreview.com/2022/07/18/1056059/robot-dog-ai-reinforcement/,2022-07-18,"<p>AI could help robots learn new skills and adapt to the real world quickly.</p>
","The robot dog is waving its legs in the air like an exasperated beetle. After 10 minutes of struggling, it manages to roll over to its front. Half an hour in, the robot is taking its first clumsy steps, like a newborn calf. But after one hour, the robot is strutting around the lab with confidence. What makes this four-legged robot special is that it learned to do all this by itself, without being shown what to do in a computer simulation. Danijar Hafner and colleagues at the University of California, Berkeley, used an AI technique called reinforcement learning, which trains algorithms by rewarding them for desired actions, to train the robot to walk from scratch in the real world. The team used the same algorithm to successfully train three other robots, such as one that was able to pick up balls and move them from one tray to another. Traditionally, robots are trained in a computer simulator before they attempt to do anything in the real world. For example, a pair of robot legs called Cassie taught itself to walk using reinforcement learning, but only after it had done so in a simulation. “The problem is your simulator will never be as accurate as the real world. There’ll always be aspects of the world you’re missing,” says Hafner, who worked with colleagues Alejandro Escontrela and Philipp Wu on the project and is now an intern at DeepMind. Adapting lessons from the simulator to the real world also requires extra engineering, he says. Within a few years, any task that previously required hands to perform could be partially or fully automated away. The team’s algorithm, called Dreamer, uses past experiences to build up a model of the surrounding world. Dreamer also allows the robot to conduct trial-and-error calculations in a computer program as opposed to the real world, by predicting potential future outcomes of its potential actions. This allows it to learn faster than it could purely by doing. Once the robot had learned to walk, it kept learning to adapt to unexpected situations, such as resisting being toppled by a stick. “Teaching robots through trial and error is a difficult problem, made even harder by the long training times such teaching requires,” says Lerrel Pinto, an assistant professor of computer science at New York University, who specializes in robotics and machine learning. Dreamer shows that deep reinforcement learning and world models are able to teach robots new skills in a really short amount of time, he says. Jonathan Hurst, a professor of robotics at Oregon State University, says the findings, which have not yet been peer-reviewed, make it clear that “reinforcement learning will be a cornerstone tool in the future of robot control.” Removing the simulator from robot training has many perks. The algorithm could be useful for teaching robots how to learn skills in the real world and adapt to situations like hardware failures, Hafner says–for example, a robot could learn to walk with a malfunctioning motor in one leg. The approach could also have huge potential for more complicated things like autonomous driving, which require complex and expensive simulators, says Stefano Albrecht, an assistant professor of artificial intelligence at the University of Edinburgh. A new generation of reinforcement-learning algorithms could “super quickly pick up in the real world how the environment works,” Albrecht says. But there are some big unsolved problems, Pinto says. With reinforcement learning, engineers need to specify in their code which behaviors are good and are thus rewarded, and which behaviors are undesirable. In this case, turning over and walking is good, while not walking is bad. “A roboticist will need to do this for each and every task [or] problem they want the robot to solve,” says Pinto. That is incredibly time consuming, and it is difficult to program behaviors for unexpected situations. And while simulators can be inaccurate, so can world models, Albrecht says. “World models start from nothing, so initially the predictions from the models will be completely all over the place,” he says. It takes time until they get enough data to make them accurate. In the future, Hafner says, it would be nice to teach the robot to understand spoken commands. Hafner says the team also wants to connect cameras to the robot dog to give it vision. This would allow it to navigate in complex indoor situations, such as walking to a room, finding objects, and—yes!—playing fetch.",0.0
Inside a radical new project to democratize AI,https://www.technologyreview.com/2022/07/12/1055817/inside-a-radical-new-project-to-democratize-ai/,2022-07-12,"<p>A group of over 1,000 AI researchers has created a multilingual large language model bigger than GPT-3—and they’re giving it out for free.</p>
","PARIS — This is as close as you can get to a rock concert in AI research. Inside the supercomputing center of the French National Center for Scientific Research, on the outskirts of Paris, rows and rows of what look like black fridges hum at a deafening 100 decibels. They form part of a supercomputer that has spent 117 days gestating a new large language model (LLM) called BLOOM that its creators hope represents a radical departure from the way AI is usually developed. Unlike other, more famous large language models such as OpenAI’s GPT-3 and Google’s LaMDA, BLOOM (which stands for BigScience Large Open-science Open-access Multilingual Language Model) is designed to be as transparent as possible, with researchers sharing details about the data it was trained on, the challenges in its development, and the way they evaluated its performance. OpenAI and Google have not shared their code or made their models available to the public, and external researchers have very little understanding of how these models are trained. BLOOM was created over the last year by over 1,000 volunteer researchers in a project called BigScience, which was coordinated by AI startup Hugging Face using funding from the French government. It officially launched on July 12. The researchers hope developing an open-access LLM that performs as well as other leading models will lead to long-lasting changes in the culture of AI development and help democratize access to cutting-edge AI technology for researchers around the world. The model’s ease of access is its biggest selling point. Now that it’s live, anyone can download it and tinker with it free of charge on Hugging Face’s website. Users can pick from a selection of languages and then type in requests for BLOOM to do tasks like writing recipes or poems, translating or summarizing texts, or writing programming code. AI developers can use the model as a foundation to build their own applications. At 176 billion parameters (variables that determine how input data is transformed into the desired output), it is bigger than OpenAI’s 175-billion-parameter GPT-3, and BigScience claims that it offers similar levels of accuracy and toxicity as other models of the same size. For languages such as Spanish and Arabic, BLOOM is the first large language model of this size. But even the model’s creators warn it won’t fix the deeply entrenched problems around large language models, including the lack of adequate policies on data governance and privacy and the algorithms’ tendency to spew toxic content, such as racist or sexist language. Large language models are deep-learning algorithms that are trained on massive amounts of data. They are one of the hottest areas of AI research. Powerful models such as GPT-3 and LaMDA, which produce text that reads as if a human wrote it, have huge potential to change the way we process information online. They can be used as chatbots or to search for information, moderate online content, summarize books, or generate entirely new passages of text based on prompts. But they are also riddled with problems. It takes only a little prodding before these models start producing harmful content. The models are also extremely exclusive. They need to be trained on massive amounts of data using lots of expensive computing power, which is something only large (and mostly American) technology companies such as Google can afford. Most big tech companies developing cutting-edge LLMs restrict their use by outsiders and have not released information about the inner workings of their models. This makes it hard to hold them accountable. The secrecy and exclusivity are what the researchers working on BLOOM hope to change. Meta has already taken steps away from the status quo: in May 2022 the company released its own large language model, Open Pretrained Transformer (OPT-175B), along with its code and a logbook detailing how the model was trained. Hundreds of scientists around the world are working together to understand one of the most powerful emerging technologies before it’s too late. But Meta’s model is available only upon request, and it has a license that limits its use to research purposes. Hugging Face goes a step further. The meetings detailing its work over the past year are recorded and uploaded online, and anyone can download the model free of charge and use it for research or to build commercial applications. A big focus for BigScience was to embed ethical considerations into the model from its inception, instead of treating them as an afterthought. LLMs are trained on tons of data collected by scraping the internet. This can be problematic, because these data sets include lots of personal information and often reflect dangerous biases. The group developed data governance structures specifically for LLMs that should make it clearer what data is being used and who it belongs to, and it sourced different data sets from around the world that weren’t readily available online. The group is also launching a new Responsible AI License, which is something like a terms-of-service agreement. It is designed to act as a deterrent from using BLOOM in high-risk sectors such as law enforcement or health care, or to harm, deceive, exploit, or impersonate people. The license is an experiment in self-regulating LLMs before laws catch up, says Danish Contractor, an AI researcher who volunteered on the project and co-created the license. But ultimately, there’s nothing stopping anyone from abusing BLOOM. The project had its own ethical guidelines in place from the very beginning, which worked as guiding principles for the model’s development, says Giada Pistilli, Hugging Face’s ethicist, who drafted BLOOM’s ethical charter. For example, it made a point of recruiting volunteers from diverse backgrounds and locations, ensuring that outsiders can easily reproduce the project’s findings, and releasing its results in the open. This philosophy translates into one major difference between BLOOM and other LLMs available today: the vast number of human languages the model can understand. It can handle 46 of them, including French, Vietnamese, Mandarin, Indonesian, Catalan, 13 Indic languages (such as Hindi), and 20 African languages. Just over 30% of its training data was in English. The model also understands 13 programming languages. This is highly unusual in the world of large language models, where English dominates. That’s another consequence of the fact that LLMs are built by scraping data off the internet: English is the most commonly used language online. The reason BLOOM was able to improve on this situation is that the team rallied volunteers from around the world to build suitable data sets in other languages even if those languages weren’t as well represented online. For example, Hugging Face organized workshops with African AI researchers to try to find data sets such as records from local authorities or universities that could be used to train the model on African languages, says Chris Emezue, a Hugging Face intern and a researcher at Masakhane, an organization working on natural-language processing for African languages. Including so many different languages could be a huge help to AI researchers in poorer countries, who often struggle to get access to natural-language processing because it uses a lot of expensive computing power. BLOOM allows them to skip the expensive part of developing and training the models in order to focus on building applications and fine-tuning the models for tasks in their native languages. “If you want to include African languages in the future of [natural-language processing] … it’s a very good and important step to include them while training language models,” says Emezue. BigScience has done a “phenomenal” job of building a community around BLOOM, and its approach of involving ethics and governance from the beginning is a thoughtful one, says Percy Liang, director of Stanford's Center for Research on Foundation Models. However, Liang doesn’t think it will lead to significant changes to LLM development. “OpenAI and Google and Microsoft are still blazing ahead,” he says. Ultimately, BLOOM is still a large language model, and it still comes with all the associated flaws and risks. Companies such as OpenAI have not released their models or code to the public because, they argue, the sexist and racist language that has gone into them makes them too dangerous to use that way. BLOOM is also likely to incorporate inaccuracies and biased language, but since everything about the model is out in the open, people will be able to interrogate the model’s strengths and weaknesses, says Margaret Mitchell, an AI researcher and ethicist at Hugging Face. BigScience’s biggest contribution to AI might end up being not BLOOM itself, but the numerous spinoff research projects its volunteers are getting involved in. For example, such projects could bolster the model’s privacy credentials and come up with ways to use the technology in different fields, such as biomedical research. “One new large language model is not going to change the course of history,” says Teven Le Scao, a researcher at Hugging Face who co-led BLOOM's training. “But having one good open language model that people can actually do research on has a strong long-term impact.” When it comes to the potential harms of LLMs, “ Pandora's box is already wide open,” says Le Scao. “The best you can do is to create the best conditions possible for researchers to study them.”",1.0
Doctors using AI catch breast cancer more often than either does alone,https://www.technologyreview.com/2022/07/11/1055677/ai-diagnose-breast-cancer-mammograms/,2022-07-11,"<p>A new study shows that artificial intelligence can also handle more than half of scans automatically, dramatically reducing radiologists’ workloads.</p>
","Radiologists assisted by an AI screen for breast cancer more successfully than they do when they work alone, according to new research. That same AI also produces more accurate results in the hands of a radiologist than it does when operating solo. The large-scale study, published this month in The Lancet Digital Health, is the first to directly compare an AI’s performance in breast cancer screening according to whether it’s used alone or to assist a human expert. The hope is that such AI systems could save lives by detecting cancers doctors miss, free up radiologists to see more patients, and ease the burden in places where there is a dire lack of specialists. The software being tested comes from Vara, a startup based in Germany that also led the study. The company’s AI is already used in over a fourth of Germany’s breast cancer screening centers and was introduced earlier this year to a hospital in Mexico and another in Greece. The Vara team, with help from radiologists at the Essen University Hospital in Germany and the Memorial Sloan Kettering Cancer Center in New York, tested two approaches. In the first, the AI works alone to analyze mammograms. In the other, the AI automatically distinguishes between scans it thinks look normal and those that raise a concern. It refers the latter to a radiologist, who would review them before seeing the AI’s assessment. Then the AI would issue a warning if it detected cancer when the doctor did not. ""In the proposed AI-driven process nearly three-quarters of the screening studies didn’t need to be reviewed by a radiologist, while improving accuracy overall.” To train the neural network, Vara fed the AI data from over 367,000 mammograms—including radiologists’ notes, original assessments, and information on whether the patient ultimately had cancer—to learn how to place these scans into one of three buckets: “confident normal,” “not confident” (in which no prediction is given), and “confident cancer.” The conclusions from both approaches were then compared with the decisions real radiologists originally made on 82,851 mammograms sourced from screening centers that didn’t contribute scans used to train the AI. The second approach—doctor and AI working together—was 2.6% better at detecting breast cancer than a doctor working alone, and raised fewer false alarms. It accomplished this while automatically setting aside scans it classified as confidently normal, which amounted to 63% of all mammograms. This intense streamlining could slash radiologists’ workloads. After breast cancer screenings, patients with a normal scan are sent on their way, while an abnormal or unclear scan triggers follow-up testing. But radiologists examining mammograms miss 1 in 8 cancers. Fatigue, overwork, and even the time of day all affect how well radiologists can identify tumors as they view thousands of scans. Signs that are visually subtle are also generally less likely to set off alarms, and dense breast tissue—found mostly in younger patients—makes signs of cancer harder to see. Radiologists using the AI in the real world are required by German law to look at every mammogram, at least glancing at those the AI calls fine. The AI still lends them a hand by pre-filling reports on scans labeled normal, though the radiologist can always reject the AI’s call. Thilo Töllner, a radiologist who heads a German breast cancer screening center, has used the program for two years. He’s sometimes disagreed when the AI classified scans as confident normal and manually filled out reports to reflect a different conclusion, but he says “normals are almost always normal.” Mostly, “you just have to press enter.” Mammograms the AI has labeled as ambiguous or “confident cancer” are referred to a radiologist—but only after the doctor has offered an initial, independent assessment. Radiologists classify mammograms on a 0 to 6 scale known as BI-RADS, where lower is better. A score of 3 indicates that something is probably benign, but worth checking up on. If Vara has assigned a BI-RADS score of 3 or higher to a mammogram the radiologist labels normal, a warning appears. AI generally excels at image classification. So why did Vara’s AI on its own underperform a lone doctor? Part of the problem is that a mammogram alone can’t determine whether someone has cancer—that requires removing and testing the abnormal-looking tissue. Instead, the AI examines mammograms for hints. Christian Leibig, lead author on the study and director of machine learning at Vara, says that mammograms of healthy and cancerous breasts can look very similar, and both types of scans can present a wide range of visual results. This complicates AI training. So does the low prevalence of cancer in breast screenings (according to Leibig, “in Germany, it’s roughly six in 1,000”). Because AIs trained to catch cancer are mostly trained on healthy breast scans, they can be prone to false positives. The study tested the AI only on past mammogram decisions and assumed that radiologists would agree with the AI each time it issued a decision of “confident normal” or “confident cancer.” When the AI was unsure, the study defaulted to the original radiologist’s reading. That means it couldn’t test how using AI affects radiologists’ decisions—and whether any such changes may create new risks. Töllner admits he spends less time scrutinizing scans Vara labels normal than those it deems suspicious. “You get quicker with the normals because you get confident with the system,” he says. Curtis Langlotz, director of Stanford's Center for Artificial Intelligence in Medicine and Imaging, is impressed, but he says the next step would be to confirm how well the AI performs over a long period of time in actual clinics with real patients. So far, attempts to fully replace radiologists with AI have failed. A 2021 review found that in 34 of 36 studies, the AI did worse than a single radiologist at screening for breast cancer from mammograms. All 36 were less accurate than the consensus of two radiologists, which some countries require. “We often say that AI will not replace radiologists,” Langlotz says. “This study doesn’t change that, but in the proposed AI-driven process nearly three-quarters of the screening studies didn’t need to be reviewed by a radiologist, while improving accuracy overall.” That, he says, is “groundbreaking.” Langlotz adds that this approach could ease the shortage of radiologists, especially in countries such as Malawi, where there is one radiologist per 8.8 million people, or India, a country of 1.4 billion served by one radiologist for every 100,000 people. Even the US, which proportionally has 10 times as many radiologists as India, is projected to be short 17,000 radiologists by 2033. Töllner is optimistic that more radiologists using AI will mean earlier breast cancer detection, which could improve survival rates. He also hopes Vara will help quash the high number of false positives—patients recalled for further testing who are actually fine. Correction: An earlier version of this story incorrectly stated that a doctor and AI working together were 3.6% better at detecting breast cancer than a doctor working alone. The correct figure is 2.6%.",0.0
Why business is booming for military AI startups ,https://www.technologyreview.com/2022/07/07/1055526/why-business-is-booming-for-military-ai-startups/,2022-07-07,"<p>The invasion of Ukraine has prompted militaries to update their arsenals—and Silicon Valley stands to capitalize.</p>
","Exactly two weeks after Russia invaded Ukraine in February, Alexander Karp, the CEO of data analytics company Palantir, made his pitch to European leaders. With war on their doorstep, Europeans ought to modernize their arsenals with Silicon Valley’s help, he argued in an open letter. For Europe to “remain strong enough to defeat the threat of foreign occupation,” Karp wrote, countries need to embrace “the relationship between technology and the state, between disruptive companies that seek to dislodge the grip of entrenched contractors and the federal government ministries with funding.” Militaries are responding to the call. NATO announced on June 30 that it is creating a $1 billion innovation fund that will invest in early-stage startups and venture capital funds developing “priority” technologies such as artificial intelligence, big-data processing, and automation. Since the war started, the UK has launched a new AI strategy specifically for defense, and the Germans have earmarked just under half a billion for research and artificial intelligence within a $100 billion cash injection to the military. “War is a catalyst for change,” says Kenneth Payne, who leads defense studies research at King’s College London and is the author of the book I, Warbot: The Dawn of Artificially Intelligent Conflict. The war in Ukraine has added urgency to the drive to push more AI tools onto the battlefield. Those with the most to gain are startups such as Palantir, which are hoping to cash in as militaries race to update their arsenals with the latest technologies. But long-standing ethical concerns over the use of AI in warfare have become more urgent as the technology becomes more and more advanced, while the prospect of restrictions and regulations governing its use looks as remote as ever. The relationship between tech and the military wasn’t always so amicable. In 2018, following employee protests and outrage, Google pulled out of the Pentagon’s Project Maven, an attempt to build image recognition systems to improve drone strikes. The episode caused heated debate about human rights and the morality of developing AI for autonomous weapons. It also led high-profile AI researchers such as Yoshua Bengio, a winner of the Turing Prize, and Demis Hassabis, Shane Legg, and Mustafa Suleyman, the founders of leading AI lab DeepMind, to pledge not to work on lethal AI. But four years later, Silicon Valley is closer to the world’s militaries than ever. And it’s not just big companies, either—startups are finally getting a look in, says Yll Bajraktari, who was previously executive director of the US National Security Commission on AI (NSCAI) and now works for the Special Competitive Studies Project, a group that lobbies for more adoption of AI across the US. Companies that sell military AI make expansive claims for what their technology can do. They say it can help with everything from the mundane to the lethal, from screening résumés to processing data from satellites or recognizing patterns in data to help soldiers make quicker decisions on the battlefield. Image recognition software can help with identifying targets. Autonomous drones can be used for surveillance or attacks on land, air, or water, or to help soldiers deliver supplies more safely than is possible by land. These technologies are still in their infancy on the battlefield, and militaries are going through a period of experimentation, says Payne, sometimes without much success. There are countless examples of AI companies’ tendency to make grand promises about technologies that turn out not to work as advertised, and combat zones are perhaps among the most technically challenging areas in which to deploy AI because there is little relevant training data. This could cause autonomous systems to fail in a “complex and unpredictable manner,” argued Arthur Holland Michel, an expert on drones and other surveillance technologies, in a paper for the United Nations Institute for Disarmament Research Nevertheless, many militaries are pressing forward. In a vaguely worded press release in 2021, the British army proudly announced it had used AI in a military operation for the first time, to provide information on the surrounding environment and terrain. The US is working with startups to develop autonomous military vehicles. In the future, swarms of hundreds or even thousands of autonomous drones that the US and British militaries are developing could prove to be powerful and lethal weapons. Many experts are worried. Meredith Whittaker, a senior advisor on AI at the Federal Trade Commission and a faculty director at the AI Now Institute, says this push is really more about enriching tech companies than improving military operations. In a piece for Prospect magazine co-written with Lucy Suchman, a sociology professor at Lancaster University, she argued that AI boosters are stoking Cold War rhetoric and trying to create a narrative that positions Big Tech as “critical national infrastructure,” too big and important to break up or regulate. They warn that AI adoption by the military is being presented as an inevitability rather than what it really is: an active choice that involves ethical complexities and trade-offs. The controversy over Project Maven shows the department has a serious trust problem. This is an attempt to fix that. With the controversy around Maven receding into the past, the voices calling for more AI in defense have become louder and louder in the last couple of years. One of the loudest has been Google’s former CEO Eric Schmidt, who chaired the NSCAI and has called for the US to take a more aggressive approach to adopting military AI. In a report last year, outlining steps the United States should take to be up to speed in AI by 2025, the NSCAI called on the US military to invest $8 billion a year into these technologies or risk falling behind China. The Chinese military likely spends at least $1.6 billion a year on AI, according to a report by the Georgetown Center for Security and Emerging Technologies, and in the US there is already a significant push underway to reach parity, says Lauren Kahn, a research fellow at the Council on Foreign Relations. The US Department of Defense requested $874 million for artificial intelligence for 2022, although that figure does not reflect the total of the department’s AI investments, it said in a March 2022 report. It’s not just the US military that’s convinced of the need. European countries, which tend to be more cautious about adopting new technologies, are also spending more money on AI, says Heiko Borchert, co-director of the Defense AI Observatory at the Helmut Schmidt University in Hamburg, Germany. The French and the British have identified AI as a key defense technology, and the European Commission, the EU’s executive arm, has earmarked $1 billion to develop new defense technologies. Building demand for AI is one thing. Getting militaries to adopt it is entirely another. A lot of countries are pushing the AI narrative, but they’re struggling to move from concept to deployment, says Arnaud Guérin, the CEO of Preligens, a French startup that sells AI surveillance. That’s partly because the defense industry in most countries is still usually dominated by a clutch of large contractors, which tend to have more expertise in military hardware than AI software, he says. It’s also because clunky military vetting processes move slowly compared with the breakneck speed we’re used to seeing in AI development: military contracts can span decades, but in the fast-paced startup cycle, companies have just a year or so to get off the ground. Startups and venture capitalists have expressed frustration that the process is moving so slowly. The risk, argues Katherine Boyle, a general partner at venture capital firm Andreessen Horowitz, is that talented engineers will leave in frustration for jobs at Facebook and Google, and startups will go bankrupt waiting for defense contracts. “Some of those hoops are totally critical, particularly in this sector where security concerns are very real,” says Mark Warner, who founded FacultyAI, a data analytics company that works with the British military. “But others are not … and in some ways have enshrined the position of incumbents.” AI companies with military ambitions have to “stay in business for a long time,” says Ngor Luong, a research analyst who has studied AI investment trends at the Georgetown Center for Security and Emerging Technologies. Militaries are in a bind, says Kahn: go too fast, and risk deploying dangerous and broken systems, or go too slow and miss out on technological advancement. The US wants to go faster, and the DoD has enlisted the help of Craig Martell, the former AI chief at ride-hailing company Lyft. In June 2022, Martell took the helm of the Pentagon’s new Chief Digital Artificial Intelligence Office, which aims to coordinate the US military’s AI efforts. Martell’s mission, he told Bloomberg, is to change the culture of the department and boost the military’s use of AI despite “bureaucratic inertia.” He may be pushing at an open door, as AI companies are already starting to snap up lucrative military contracts. In February, Anduril, a five-year-old startup that develops autonomous defense systems such as sophisticated underwater drones, won a $1 billion defense contract with the US. In January, ScaleAI, a startup that provides data labeling services for AI, won a $250 million contract with the US Department of Defense. Despite the steady march of AI into the field of battle, the ethical concerns that prompted the protests around Project Maven haven’t gone away. There have been some efforts to assuage those concerns. Aware it has a trust issue, the US Department of Defense has rolled out “responsible artificial intelligence” guidelines for AI developers, and it has its own ethical guidelines for the use of AI. NATO has an AI strategy that sets out voluntary ethical guidelines for its member nations. All these guidelines call on militaries to use AI in a way that is lawful, responsible, reliable, and traceable and seeks to mitigate biases embedded in the algorithms. One of their key concepts is that humans must always retain control of AI systems. But as the technology develops, that won’t really be possible, says Payne. “The whole point of an autonomous [system] is to allow it to make a decision faster and more accurately than a human could do and at a scale that a human can’t do,” he says. “You’re effectively hamstringing yourself if you say ‘No, we’re going to lawyer each and every decision.’” Still, critics say stronger rules are needed. There is a global campaign called Stop Killer Robots that seeks to ban lethal autonomous weapons, such as drone swarms. Activists, high-profile officials such as UN chief António Guterres, and governments such as New Zealand’s argue that autonomous weapons are deeply unethical, because they give machines control over life-and-death decisions and could disproportionately harm marginalized communities through algorithmic biases. Swarms of thousands of autonomous drones, for example, could essentially become weapons of mass destruction. Restricting these technologies will be an uphill battle because the idea of a global ban has faced opposition from big military spenders, such as the US, France, and the UK. Ultimately, the new era of military AI raises a slew of difficult ethical questions that we don’t have answers to yet. One of those questions is how automated we want armed forces to be in the first place, says Payne. On one hand, AI systems might reduce casualties by making war more targeted, but on the other, you’re “effectively creating a robot mercenary force to fight on your behalf,” he says. “It distances your society from the consequences of violence.”",0.0
These simple changes can make AI research much more energy efficient,https://www.technologyreview.com/2022/07/06/1055458/ai-research-emissions-energy-efficient/,2022-07-06,"<p>Tweaking the settings of the cloud service an algorithm runs on can have a big impact, researchers found. But not many people bother to do it.</p>
","Deep learning is behind machine learning’s most high-profile successes, such as advanced image recognition, the board game champion AlphaGo, and language models like GPT-3. But this incredible performance comes at a cost: training deep-learning models requires huge amounts of energy. Now, new research shows how scientists who use cloud platforms to train deep-learning algorithms can dramatically reduce the energy they consume, and therefore the emissions this work generates. Simple changes to cloud settings are the key. Since the first paper studying this technology’s impact on the environment was published three years ago, a movement has grown among researchers to self-report the energy consumed and emissions generated from their work. Having accurate numbers is an important step toward making changes, but actually gathering those numbers can be a challenge. “You can’t improve what you can’t measure,” says Jesse Dodge, a research scientist at the Allen Institute for AI in Seattle. “The first step for us, if we want to make progress on reducing emissions, is we have to get a good measurement.” To that end, the Allen Institute recently collaborated with Microsoft, the AI company Hugging Face, and three universities to create a tool that measures the electricity usage of any machine-learning program that runs on Azure, Microsoft’s cloud service. With it, Azure users building new models can view the total electricity consumed by graphics processing units (GPUs)—computer chips specialized for running calculations in parallel—during every phase of their project, from selecting a model to training it and putting it to use. It’s the first major cloud provider to give users access to information about the energy impact of their machine-learning programs. While tools already exist that measure energy use and emissions from machine-learning algorithms running on local servers, those tools don’t work when researchers use cloud services provided by companies like Microsoft, Amazon, and Google. Those services don’t give users direct visibility into the GPU, CPU, and memory resources their activities consume—and the existing tools, like Carbontracker, Experiment Tracker, EnergyVis, and CodeCarbon, need those values in order to provide accurate estimates. The new Azure tool, which debuted in October, currently reports energy use, not emissions. So Dodge and other researchers figured out how to map energy use to emissions, and they presented a companion paper on that work at FAccT, a major computer science conference, in late June. Researchers used a service called Watttime to estimate emissions based on the zip codes of cloud servers running 11 machine-learning models. They found that emissions can be significantly reduced if researchers use servers in specific geographic locations and at certain times of day. Emissions from training small machine-learning models can be reduced up to 80% if the training starts at times when more renewable electricity is available on the grid, while emissions from large models can be reduced over 20% if the training work is paused when renewable electricity is scarce and restarted when it’s more plentiful. Honorees from this year's 35 Innovators list are employing AI to find new molecules, fold proteins, and analyze massive amounts of medical data. Energy-conscious cloud users can lower their emissions by adjusting those factors through preference settings on the three biggest cloud services (Microsoft Azure, Google Cloud, and Amazon Web Services). But Lynn Kaack, cofounder of Climate Change AI, an organization that studies the impact of machine learning on climate change, says cloud providers should pause and restart these projects automatically to optimize for lower emissions. “You can schedule, of course, when to run the algorithm, but it’s a lot of manual work,” says Kaack. “You need policy incentives, probably, to really do this at scale.” She says policies like carbon pricing could incentivize cloud providers to build workflows that enable automatic pauses and restarts and allow users to opt in. A lot more work still needs to be done to make machine learning more environmentally friendly, especially while most countries are still dependent on fossil fuels. And Dodge says that Azure’s tool only measures the electricity consumed by GPUs. A more accurate calculation of machine learning’s energy consumption would include CPU and memory usage—not to mention the energy for building and cooling the physical servers. And changing habits can take time. Only 13% of Azure users running machine-learning programs have looked at the energy measurement tool since it debuted in October, Dodge says. And Raghavendra Selvan, who helped create Carbontracker, said even he has trouble persuading researchers to use the tool in their machine-learning research. “I don’t think I have been able to convince my own group,” Selvan says. But he is optimistic. More researchers are getting into the habit of reporting energy use in their papers, encouraged by major conferences like NeurIPS that suggest it. Selvan says if more people start to factor in these energy and emissions costs when planning future projects, it could start to reduce machine learning’s impact on climate change.",0.0
The book ban movement has a chilling new tactic: harassing teachers on social media,https://www.technologyreview.com/2022/07/15/1055959/book-bans-social-media-harassment/,2022-07-15,"<p>Educators who stand up to conservative activists are being harassed and called “groomers” online, turning them into potential targets for real-world violence.</p>
","Nancy Vera was awakened suddenly at midnight on July 12 by the sound of a single gunshot fired into her yard. She looked at a security camera just in time to see a truck speed away. Vera was shocked but not surprised. The president of the Corpus Christi, Texas, branch of the American Federation of Teachers (AFT), she had recently handed out books with LGBTQ characters at a pride event for local students, alongside a drag queen. Vera thought the event was a fun opportunity to connect with local parents and distribute books to kids. But conservatives, including her local sheriff, called the event an example of the ""grooming and indoctrination of young people in our country."" ""Grooming"" is a slur commonly used by devotees of the conspiracy theory QAnon, which claims that powerful people and institutions are ensnaring children in sex trafficking rings. “This type of rhetoric is going to get people killed,” she says. Corpus Christi, where Vera lives, has become a flashpoint for a growing push among Christian and conservative groups across the US to get certain books and topics they deem inappropriate for children removed from school libraries and curriculums. Now the fight is turning increasingly ugly, with people targeting individual teachers’ private social media accounts for scrutiny and even harassment. On July 9, the conservative group County Citizens Defending Freedom (CCDF) held a public seminar in Corpus Christi about monitoring school curriculums and “researching social media of teachers, school board members, staff of school districts and elected officials,” effectively teaching people how to stalk and harass educators online. Moms for Liberty and allied groups are now training members to monitor teachersâ€™ social media. pic.twitter.com/qlKfpc0LkX “I have been tracking this current movement of book banning since last summer, and this is the first I have seen of a deliberate effort to track or monitor teachers and staff,” says Jonathan Friedman, the director of Free Expression and Education at PEN America, a nonprofit that defends free expression. The CCDF event represents a pivotal moment in the recent spate of book bans across America, he says. (CCDF declined to comment for this story.) The specific issues that would-be book banners focus on vary. Some parents are offended by discussions of race in their children’s books like The Bluest Eye by Toni Morrison. Others want kids kept away from books that discuss gender, sexual orientation, or sex, such as Gender Queer by Maia Kobabe and All Boys Aren’t Blue by George Johnson. And some want books that depict non-Christian beliefs or cultures out of libraries. Activists who flee repression increasingly face zero-click software hacks and other digital threats The recent turn to monitoring educators’ social media accounts is no surprise, says Friedman, given the book-ban movement’s origins in online messaging boards and Facebook groups. “This is a movement that was formed online, so it’s not so much that these activists are moving online so much as they are moving the target from schools to teachers and librarians,” Friedman says. “And it’s not going to stop there.” Vera has seen the impact of this firsthand. In the week since the pride book event, she says, she has been bombarded with threatening Facebook messages and phone calls. In an effort to protect herself, she now carries Mace and has installed home security cameras. Other conservative groups are also tracking teachers’ social media accounts. Moms for Liberty and its offshoot group, Moms for Libraries, have been engaging in this type of monitoring and have also started distributing “liberty-minded books” with conservative publishing house Brave Books, which claim to “empower this generation’s youth with conservative values” while “glorifying the Lord in all we do.” The Leadership Institute is another conservative group that has justified this tactic. “Anyone who wades into the public discourse using social media is presenting their personal or political views for all to see,” Matthew Hurtt, the director of graduate relations at the Leadership Institute, said in an email to me. “If teachers, administrators, and elected officials espouse objectionable views on social media, there is a good chance they are espousing those views in the classroom or in school board meetings.” One clear pattern is emerging: educators who support teaching sex education and discussing LGBTQ issues are labeled “groomers.” Gloria Gonzales Dholakia, a school board member in Leander, Texas, says she was called a groomer at a school board meeting that was broadcast online, leading to a slew of hateful comments. A man who attended the meeting made several highly personal remarks, including suggesting that Gonzales Dholakia’s husband, who was sitting just a few feet away in the room, must be abusive. “My kids were watching this online at home. I was so angry and was ready to quit,” she says. These are small peanuts accounts but this is a glistening example of what these groups are doing: tracking down educators and putting their info on social. This is why you need to lock down *everything* right now. Until you're protected, protect yourself. pic.twitter.com/Ys0zflcPOh Gonzales Dholakia did not quit, but the need to grapple with slurs and online harassment is yet another burden for teachers exhausted by the pandemic and other issues, including mass shootings at schools. Thousands of teachers have either retired or quit the profession in the last two years. Those who stay have to try to work out what to do to protect themselves and their colleagues. But resources for dealing with the online harassment of educators are sparse because it is such a relatively new problem, says Viktorya Vilk, the director for digital safety and free expression at PEN America. PEN America has created a step-by-step guide to prepare and help people respond to online harassment. But sometimes it’s too little, too late, Vilk says: “So many of these educators are quitting jobs, and those quitting jobs are disproportionately women and people of color — the exact people we don’t want to quit because that means our libraries and schools are less diverse and don’t reflect the full range of American experience. It’s really alarming.” Educators like Vera refuse to stand back. She recently joined colleagues at a counterprotest to voice their concerns about their safety, and she’s spending the next few weeks before schools reopen beefing up protection measures for her colleagues. The Corpus Christi police department is investigating the shooting, and the AFT has added a security detail for her. She’s working to install security cameras at schools and is advising new teachers on how to deal with online harassment. “I’m not going to stop what I’m doing,” she says. Editors note: This story previously misstated that a gunshot ricocheted off Nancy Vera's home, but the gunshot was fired into her yard. The story has been updated.",0.0
How aspiring influencers are forced to fight the algorithm,https://www.technologyreview.com/2022/07/14/1055906/tiktok-influencers-moderation-bias/,2022-07-14,"<p>Figuring out social media platforms’ hidden rules is hard work—and it falls more heavily on creators from marginalized backgrounds.</p>
","Last summer, a TikTok creator named Ziggi Tyler posted a video calling out a disturbing problem he found in the app’s Creator Marketplace, a tool that matches creators with brands looking to pay for sponsored content. Tyler said he was unable to enter phrases like “Black Lives Matter” and “supporting Black excellence” into his Marketplace profile. However, phrases like “white supremacy” and “supporting white excellence” were allowed. If you’ve spent much time on TikTok, you’ve probably seen creators discuss similar incidents. There are two ways to try to understand the impact of content moderation and the algorithms that enforce those rules: by relying on what the platform says, and by asking creators themselves. In Tyler’s case, TikTok apologized and blamed an automatic filter that was set up to flag words associated with hate speech—but that was apparently unable to understand context. Brooke Erin Duffy, an associate professor at Cornell University, teamed up with graduate student Colten Meisner to interview 30 creators on TikTok, Instagram, Twitch, YouTube, and Twitter around the time Tyler’s video went viral. They wanted to know how creators, particularly those from marginalized groups, navigate the algorithms and moderation practices of the platforms they use. What they found: Creators invest a lot of labor in understanding the algorithms that shape their experiences and relationships on these platforms. Because many creators use multiple platforms, they must learn the hidden rules for each one. Some creators adapt their entire approach to producing and promoting content in response to the algorithmic and moderation biases they encounter. Below is our conversation with Duffy about her forthcoming research (edited and condensed for clarity). Creators have long discussed how algorithms and moderation affect their visibility on the platforms that made them famous. So what most surprised you while doing these interviews? We had a sense that creators’ experiences are shaped by their understanding of the algorithm, but after doing the interviews, we really started to see how profound [this impact] is in their everyday lives and work … the amount of time, energy, and attention they devote to learning about these algorithms, investing in them. They have this kind of critical awareness that these algorithms are understood to be uneven. Despite that, they’re still investing all of this energy in hopes of understanding them. It just really draws attention to the lopsided nature of the creator economy. How often are creators thinking about the possibility of being censored or having their content not reach their audience because of algorithmic suppression or moderation practices? I think it fundamentally structures their content creation process and also their content promotion process. These algorithms change at whim; there’s no insight. There’s no direct communication from the platforms, in many cases. And this completely, fundamentally impacts not just your experience, but your income. They would invest so much time and labor in these grassroots experiments and would talk about “I would do the same kind of content, but I would vary this thing one day. I would wear this kind of outfit one day, and another kind the next.” Or they’d try different sets of hashtags. People would say they have both online and offline interactions with their creator community, and they would talk about how to game the algorithm, what’s okay to say, what can potentially be flagged. There are some important forms of collective organization that may not look like what we would traditionally think of as organized workers but are still powerful ways for creators to band together and kind of challenge the top-down systems of power. One of the things I kept thinking about while reading your findings was the concept of “shadow banning,” the moderation practice of hiding or limiting the reach of content without informing its creator. From a journalist’s perspective, “shadow banning” is hard to report on because it is by definition hidden, but it’s one of the main concerns creators have expressed over the years. How did you consider this concept in your research? Some people swear they've been shadow-banned, and other people say, “Well, your content is just bad.” It’s a very fraught problem, because anyone can issue these claims. The ambiguity of shadow banning is in part what makes it so powerful. Because there’s no way to actually prove that any one person on these platforms was or was not shadow-banned, that fuels a lot of speculation. But you know, whether it exists or it doesn’t, the fact that people act as if they are punished through limits on their visibility is worth taking seriously. Is there anything that can be done to help resolve some of these issues? Platforms tout all over their websites their benefits to creators and [say] if you are talented enough and have the right content, you can connect with audiences and make all kinds of money. The creators are drawing so much money, through data and eyeballs, to these platforms but don't have much of a say in their content moderation policies and how these are unevenly enacted. Radical transparency is a bit pie-in-the-sky, but I do think creators should have more representation in terms of the decisions that fundamentally impact their businesses.",0.0
New York City is drowning in packages,https://www.technologyreview.com/2022/07/12/1055161/new-york-city-packages/,2022-07-12,"<p>Online orders, which ramped up with the start of the pandemic, are still clogging city streets.</p>
","Amazon, Hello Fresh, Stitch Fix. Click a button, and it’s there in three to five days—perhaps even one. Packages, packages, and more packages—goods from all over the world, delivered after just a couple of clicks. But this height of consumer convenience has been complicating urban life for years, giving rise to increased theft and traffic, package waste, and a landscape of struggling local businesses. Some cities, especially in Europe and Japan, are implementing regulations that dramatically curtail package-related stress. But not New York City—not yet. Three years ago, more than 1.8 million packages were delivered to the Big Apple on a typical day, according to data collected by the Rensselaer Polytechnic Institute Center of Excellence for Sustainable Urban Freight Systems. Just a few months into the pandemic, however, that number had increased to nearly 2.3 million. And that’s only counting your typical e-commerce packages, says José Holguín-Veras, the center’s director. Altogether, with groceries and prepared food, total daily deliveries stacked up to more than 3.7 million, the center estimates. That’s nearly enough to deliver one item each to half the people in New York every day. Two years into the pandemic, in March 2022, the number had barely dropped, to just under 3.6 million. People, Holguín-Veras surmises, simply got used to ordering everything to their door. “It makes logical sense,” Holguín-Veras says. After all, the pandemic upended how we move through the world, especially when it comes to shopping and eating out. But e-commerce comes with significant costs that are not reflected in the purchase price. For instance, a recent study found that New York ranks as the most traffic-congested city in the US. Freight delivery plays a significant role in the problem: a November 2021 report estimates that delivering more than 2 million e-commerce parcels a day requires some 7,800 freight vehicles, each occupying city streets and roads for eight hours. That’s a total of more 60,000 vehicle-hours each day. A singular focus on high-tech will dilute the vibrancy of our cities and limit their potential. Noticing the increase in e-commerce delivery traffic, then-mayor Bill de Blasio allocated $38 million in the November 2021 budget to shipping these packages via the “blue highway”––by ferry instead of by truck. “One of the best ways to fight climate change is to get away from a society and economy dominated by big trucks,” he said in late 2021. “[A]nd that’s just the truth in New York City and America today: the reign of the 18-wheeler. It’s supreme; it’s everywhere, and it’s a danger to our future.” Other attempts to reduce delivery-truck congestion have popped up. There are cargo bikes, for example, and a potential $3 surcharge on every “nonessential” package delivered. Lockers are also a key player; they help tackle the “last mile” problem—or the last leg of the delivery process—by centralizing drop-off locations to save the door-to-door toil. Amazon-exclusive lockers live in 7-Elevens, Rite-Aids, Whole Foods markets, and Chase Banks. Retailer-agnostic locker services, such as Stowfly, also exist. The company’s lockers can be found at a range of locations including smaller mom-and-pop stores. Stowfly CEO Sid Khattri says the approach solves two problems at once: centralizing e-commerce delivery while helping local businesses “make extra income and get foot traffic at a time when physical retail is dying.” It’s helpful to step back and put the parcel problem in historical context, says David Vega-Barachowitz, an associate at WXY, an architecture firm in New York City. The city’s package problem is not just about congested streets or inefficient distribution of resources, he says. Rather, it’s another crisis of convenience, akin to when, in the 1950s, suburban shopping centers began competing with city downtowns. “We live in a city whose main pitch is the ability to walk out your door, get a carton of milk, go to a bookstore, go to a movie, etc.,” he says, “and convenience culture is threatening all of that.” Arthur Getman, who was director of analytics at the New York City Department of Transportation and is now at Replica, agrees. “A lot of people coming to New York bring the mentality of the ‘American Dream,’” he says, but here’s the problem: that dream is largely based in suburbia. The city just doesn’t have the space for that—for everyone to have their house, their lawn, their car, and their stuff. With its public transportation, bike lanes, sidewalks, parks, and apartment buildings, New York City is made to share. As everyone from city planners to apartment building managers copes with the rise of e-commerce, Holguín-Veras, after poring over the data for years, can’t help but ask: “Of all the purchases made, what percent of those are truly urgent?” Sarah Simon is a freelance multimedia journalist based in New York City. This story has been updated to reflect Arthur Getman's present affiliation.",0.0
Digital repression across borders is on the rise,https://www.technologyreview.com/2022/07/08/1055582/digital-repression-across-borders-is-on-the-rise/,2022-07-08,"<p>Activists who flee repression increasingly face zero-click software hacks and other digital threats</p>
","Khatab Alrawhani, a Yemen-born journalist and activist, thought he could escape the persecution that journalists were experiencing in the Middle East when he left the region. But it followed him. While studying in Washington, DC, in 2015, he published posts denouncing the Houthi coup, in which an armed faction overthrew the Yemeni government. His father was briefly arrested. Soon after, his brother was as well. When Alrawhani settled in Toronto, though, his online life took an unexpected turn. He started to get WhatsApp messages from women he’d never met, urging him to click a link they shared. The messages didn’t seem like ordinary phishing attempts. They were personalized: they included details about his background, making comments about specific articles he had written or referencing where he used to live in Yemen. Then pro-Houthi hackers hijacked the Facebook page for his news network, which covers human rights abuses in Yemen, and used it to post positive messages in Arabic about the coup. “What was terrible is how our readers thought these messages were coming from us,” he says. Ultimately, his team had to delete the page entirely and launch a new one. These kinds of online threats have changed how Alrawhani navigates the world and interacts with others. “I don’t write full sentences in my phone when I text friends or colleagues or family,” he says. Instead, he writes in code. “I assume my phone activity is always being monitored by the Houthi regime,” he says. Alrawhani is not alone. Around the world, activists have fled authoritarian states for their safety. But in their new homes, the intimidation continues, albeit in the digital realm. Those threats—generally referred to as digital transnational repression—include phishing attacks, zero-click spyware hacks, social media page takedowns, SIM card hacks, and fake invitations to conferences. Physical threats against activists tend to make the headlines. Earlier this year, for example, five Chinese nationals were arrested for plotting attacks on dissidents living in New York City. But digital harassment, which can be conducted with the click of a mouse button, frequently occurs behind the scenes. And it seems to be on the rise. The London-based research agency Forensic Architecture has counted 326 incidents of digital transnational repression between 2019 and 2021, up from 105 incidents between 2017 and 2019. One reason these online attacks are growing more frequent is that they can be much less expensive than physical attacks, says Isabel Linzer, a research analyst at the human rights organization Freedom House, which published a report in June on repression tactics used against dissidents who have moved from their home country to the US. “These [digital] attacks happen far more frequently than some people think,” Linzer says, and they “have serious consequences for people going out to live their daily lives and to engage in their work or activism.” The full range of digital transnational repression is difficult to track, as many incidents aren’t reported. But some institutions are working to show how much harm they can do—and how hollow the response from governments and law enforcement can be. A report this year by the Citizen Lab, a research group at the University of Toronto, includes the findings from interviews with more than a dozen activists who fled their country of origin to live in Canada. “Digital targeting has a serious impact on the well-being of victims, undermines their ability to engage in transnational advocacy work, violates fundamental rights such as the right to privacy, freedom of expression, and peaceful assembly, and increases the dangers faced by their family members and friends who remain within the country of origin,” the report concluded. The countries the Citizen Lab identified as some of the more common perpetrators of digital transnational repression include Yemen as well as Afghanistan, China, Iran, Rwanda, and Syria. Zero-click software hacks, which allow an attacker to break into a phone or computer even if its user doesn’t open a malicious link or attachment, are especially concerning, says Noura Al-Jizawi, a research officer at the Citizen Lab and coauthor of the report. That’s because “they can evade digital hygiene practices,” she says. In 2021, hackers used such code to infiltrate and install spyware on the cell phone of Saudi women’s rights activist Loujain al-Hathloul, who was then living in British Columbia. In that case, the perpetrators mistakenly left an image file on her phone that allowed researchers to pin down the source of the code. The digital blueprint led to NSO Group, an Israeli technology firm that has made headlines for selling spyware to authoritarian nation-states. Some forms of digital repression are meant to embarrass and doxx. One unnamed interviewee in the Citizen Lab report, who moved from China to Canada, found out that fabricated nude photos of her were being circulated among attendees of a conference she intended to visit. Her personal information was also posted in online ads soliciting sex services. Victims of this type of harassment experienced distress, anxiety, and fear for their family’s safety, the report notes. “There’s also a bit of a sense of resignation among those that continued activism, like a realization that this type of targeting would continue,” says coauthor Siena Anstis, senior legal advisor at the Citizen Lab. Many activists have become paranoid about the messages they receive. Kaveh Shahrooz, an Iraqi lawyer living in Canada who lobbies on behalf of dissidents, gives each email special scrutiny. Shahrooz says he once received a message from a supposed organizer of a human rights conference in Germany inviting him to speak and asking him to fill in personal information via a provided link. He researched more about the conference and found out he wasn’t invited, professional-sounding though the personalized email had been. “That is one end of the spectrum,” Shahrooz says, “where you might get fooled into clicking a link. But then the other end is getting threatening messages about my activist work—things like ‘We know what you’re doing and we’ll deal with you later.’” There is little legal recourse. Several victims of spyware attacks in the UK have brought (or are bringing) civil claims against state operators and NSO Group, Anstis says. She adds that such cases can expected to be challenged, because they generally focus on claims against companies outside the purview of the host country. In the US, there is growing momentum behind calls to ban the software and tools exploited by authoritarian regimes. In 2021, the US Department of Commerce placed several surveillance companies on its Entity List, which restricts trade and business that runs contrary to the national security or foreign policy interests of the United States. New additions included NSO Group and Candiru, an Israeli-based spyware firm that develops surveillance and cyber-espionage technology for governmental clients. That won’t keep activists from being persecuted, however. Ten years ago, Eliana, a pseudonym for a Canadian-Syrian who asked to remain anonymous, began sharing the stories of the Assad regime’s victims by pitching news stories about them to local media, both in print and online. She also dedicated time to lobbying the Canadian government about resettling the many Syrian refugees who arrived in the country in 2016. She says she regularly received messages from Google warning her that someone was attempting to access her Gmail account. She suspected the Syrian regime—she couldn’t think of who else it might be. Her biggest concern was the safety of the Syrian activists she was communicating with. “I knew that if such information fell into the hands of the dictatorship, it might lead to very catastrophic repercussions, including enforced abduction, torture, and assassination,” she says. Today, Eliana says she isn’t as extroverted as she used to be. “I used to be extremely open in interacting with people,” she says. “But I’ve realized that I need to be extra cautious, since I can’t predict who or where the hurt would come from.” David Silverberg is a writer and editor based in Toronto.",0.0
The US military wants to understand the most important software on Earth,https://www.technologyreview.com/2022/07/14/1055894/us-military-sofware-linux-kernel-open-source/,2022-07-14,"<p>Open-source code runs on every computer on the planet—and keeps America’s critical infrastructure going. DARPA is worried about how well it can be trusted</p>
","It’s not much of an exaggeration to say that the whole world is built on top of the Linux kernel—although most people have never heard of it. It is one of the very first programs that load when most computers power up. It enables the hardware running the machine to interact with the software, governs its use of resources, and acts as the foundation of the operating system. It is the core building block of nearly all cloud computing, virtually every supercomputer, the entire internet of things, billions of smartphones, and more. But the kernel is also open source, meaning anyone can write, read, and use its code. And that’s got cybersecurity experts inside the US military seriously worried. Its open-source nature means the Linux kernel—along with a host of other pieces of critical open-source software—is exposed to hostile manipulation in ways that we still barely understand. “People are realizing now: wait a minute, literally everything we do is underpinned by Linux,” says Dave Aitel, a cybersecurity researcher and former NSA computer security scientist. “This is a core technology to our society. Not understanding kernel security means we can’t secure critical infrastructure.” Volunteer-run projects like Log4J keep the internet running. The result is unsustainable burnout, and a national security risk when they go wrong. Now DARPA, the US military’s research arm, wants to understand the collision of code and community that makes these open-source projects work, in order to better understand the risks they face. The goal is to be able to effectively recognize malicious actors and prevent them from disrupting or corrupting crucially important open-source code before it’s too late. DARPA’s “SocialCyber” program is an 18-month-long, multimillion-dollar project that will combine sociology with recent technological advances in artificial intelligence to map, understand, and protect these massive open-source communities and the code they create. It’s different from most previous research because it combines automated analysis of both the code and the social dimensions of open-source software. “The open-source ecosystem is one of the grandest enterprises in human history,” says Sergey Bratus, the DARPA program manager behind the project. “It’s now grown from enthusiasts to a global endeavor forming the basis of global infrastructure, of the internet itself, of critical industries and mission-critical systems pretty much everywhere,” he says. “The systems that run our industry, power grids, shipping, transportation.” Much of modern civilization now depends on an ever-expanding corpus of open-source code because it saves money, attracts talent, and makes a lot of work easier. But while the open-source movement has spawned a colossal ecosystem that we all depend on, we do not fully understand it, experts like Aitel argue. There are countless software projects, millions of lines of code, numerous mailing lists and forums, and an ocean of contributors whose identities and motivation are often obscure, making it hard to hold them accountable. That can be dangerous. For example, hackers have quietly inserted malicious code into open-source projects numerous times in recent years. Back doors can long escape detection, and, in the worst case, entire projects have been handed over to bad actors who take advantage of the trust people place in open-source communities and code. Sometimes there are disruptions or even takeovers of the very social networks that these projects depend on. Tracking it all has been mostly—though not entirely—a manual effort, which means it does not match the astronomical size of the problem. Bratus argues that we need machine learning to digest and comprehend the expanding universe of code—meaning useful tricks like automated vulnerability discovery—as well as tools to understand the community of people who write, fix, implement, and influence that code. The ultimate goal is to detect and counteract any malicious campaigns to submit flawed code, launch influence operations, sabotage development, or even take control of open-source projects. To do this, the researchers will use tools such as sentiment analysis to analyze the social interactions within open-source communities such as the Linux kernel mailing list, which should help identify who is being positive or constructive and who is being negative and destructive. The researchers want insight into what kinds of events and behavior can disrupt or hurt open-source communities, which members are trustworthy, and whether there are particular groups that justify extra vigilance. These answers are necessarily subjective. But right now there are few ways to find them at all. Experts are worried that blind spots about the people who run open-source software make the whole edifice ripe for potential manipulation and attacks. For Bratus, the primary threat is the prospect of “untrustworthy code” running America’s critical infrastructure—a situation that could invite unwelcome surprises. Here’s how the SocialCyber program works. DARPA has contracted with multiple teams of what it calls “performers,” including small, boutique cybersecurity research shops with deep technical chops. One such performer is New York–based Margin Research, which has put together a team of well-respected researchers for the task. “There is a desperate need to treat open-source communities and projects with a higher level of care and respect,” said Sophia d’Antoine, the firm’s founder. “A lot of existing infrastructure is very fragile because it depends on open source, which we assume will always be there because it’s always been there. This is walking back from the implicit trust we have in open-source code bases and software.” Margin Research is focused on the Linux kernel in part because it’s so big and critical that succeeding here, at this scale, means you can make it anywhere else. The plan is to analyze both the code and the community in order to visualize and finally understand the whole ecosystem. Margin’s work maps out who is working on what specific parts of open-source projects. For example, Huawei is currently the biggest contributor to the Linux kernel. Another contributor works for Positive Technologies, a Russian cybersecurity firm that—like Huawei—has been sanctioned by the US government, says Aitel. Margin has also mapped code written by NSA employees, many of whom participate in different open-source projects. “This subject kills me,” says d’Antoine of the quest to better understand the open-source movement, “because, honestly, even the most simple things seem so novel to so many important people. The government is only just realizing that our critical infrastructure is running code that could be literally being written by sanctioned entities. Right now.” This kind of research also aims to find underinvestment—that is critical software run entirely by one or two volunteers. It’s more common than you might think—so common that one common way software projects currently measure risk is the “bus factor”: Does this whole project fall apart if just one person gets hit by a bus? While the Linux kernel’s importance to the world’s computer systems may be the most pressing issue for SocialCyber, it will tackle other open-source projects too. Certain performers will focus on projects like Python, an open-source programming language used in a huge number of artificial-intelligence and machine-learning projects. The hope is that greater understanding will make it easier to prevent a future disaster, whether it’s caused by malicious activity or not. “Pretty much everywhere you look, you find open-source software,” says Bratus.“Even when you look at proprietary software, a recent study showed it’s actually 70% or more open source.” “This is a critical infrastructure problem,” Aitel says. “We don’t have a grip on it. We need to get a grip on it. The potential impact is that malicious hackers will always have access to Linux machines. That includes your phone. It’s that simple.”",0.0
Increasing amounts of data require holistic governance,https://www.technologyreview.com/2022/07/11/1055450/increasing-amounts-of-data-require-holistic-governance/,2022-07-11,"As companies struggle to process, store, and leverage ever-increasing amounts of structured and unstructured data, data governance is becoming a critical part of every company’s data management. Governance not only helps a company understand and use its data, but it ensures everyone has access to the data they need, when they need it. “Data doesn’t…","In association withCapital One As companies struggle to process, store, and leverage ever-increasing amounts of structured and unstructured data, data governance is becoming a critical part of every company’s data management. Governance not only helps a company understand and use its data, but it ensures everyone has access to the data they need, when they need it. “Data doesn't have much value if it lies dormant in your system, where no one can gain insight from it,” says Salim Syed, head of engineering for Capital One Slingshot. “A well-governed data platform brings data out of that darkness.” Effective governance also enables a company to implement and manage internal policies and standards related to the security and usage of data. This not only supports a company’s response to external compliance directives, but also standardizes the data for use across the company. Standardized data provides the “single source of truth” required for critical business decisions, as well as the data quality and trustworthiness teams need to do their jobs. On the surface, implementing data governance might seem obvious and straightforward, but the act of governing data across a company’s teams and products introduces levels of complexity that many companies either half-heartedly attempt to address or avoid altogether. Instilling the processes, policies, and protections of governance requires new mindsets around people, processes, and technology. “It's not the run-time activities that persuade someone not to do governance,” says Syed. “It's all the work that's needed to set up governance.” For many, the approach to data governance is to establish policies that are overseen by individual sectors of the business, which makes implementation all the more difficult. “Think about all the different teams that are doing that in a large organization,” explains Syed. “They all have to do that dependency check, and each team is also doing separate development work to meet those requirements, which is a lot of duplicated effort.” A siloed data governance initiative that requires each team to monitor its own data dependencies takes time and effort away from other work as well. “It becomes cumbersome to innovate because at every step of innovation, you have to check if there are dependencies on your governance policies,” says Syed. Siloed approaches also introduce the possibility of error and make it more difficult to ensure all governance policies are followed consistently, in all cases. These hurdles can result in a lack of buy-in from employees and stakeholders, deflating any realized data governance benefits. In many companies, data is viewed as an IT asset, and thus an IT responsibility. Although that might have been true in the past, the volume and speed of data today, and the innovative ways companies are using their data, means data is the responsibility—and the driving force—for all business units. To build an effective data governance program to serve every area of the business, it’s best to centralize the framework to reduce errors and to reduce duplicate efforts. “For federated teams to be successful in applying data management rules and governance, you can't just set a policy and let every team go build technology to enforce it,” says Syed. A centralized approach is less complicated to monitor, facilitates data consistency and accuracy, and is easier to make transparent, all of which helps with stakeholder buy-in. “If you have a centrally managed data platform, a centrally managed data ingestion pipeline, and a centrally managed data policy, then you only make changes to [the data] in one place,” Syed explains. This ensures data remains compliant, secure, and consistent wherever it is used. A best practice in establishing a centralized data governance initiative, Syed argues, is to build a central data catalog. All incoming data is ingested to a central location where it is first classified—meaning, data is identified and labeled with metadata, and restriction levels are determined. From there, access and permissions can be assigned, which facilitates sharing across the organization. “With a centralized catalog, wherever your data resides, it's the map,” explains Syed. “Once it's cataloged and classified, then you can share. You can basically break the silo.” A data catalog effectively creates a compliant, secure data marketplace that allows teams to access any data they have permissions to use. This is beneficial on several fronts. It assists in data discovery at scale, which in turn can decrease development time and spur innovation. Cataloged data also comes with context, making it easier and faster to understand and use when making business decisions. This level of data stability fosters data quality, data integrity, and data lineage—all of which instills trust in the data, an essential component of data value. If the data can’t be trusted, it can’t be used to inform business decisions. No data governance initiative will be successful without buy-in and adoption. To cultivate the culture shift needed to implement data governance, it’s essential that the tools and processes work in concert. “It’s so important that the data catalog is always in sync with the data platform,” says Syed. “A lot of companies don't pay attention to that problem. You forget to register something and then you have orphan data everywhere. If something is added or changed and it’s not reflected in your catalog, then it just completely loses its value.” If users are frustrated with a lack of functionality, adoption will be an uphill battle. It's also important that the tools and processes remain flexible. Tools need to accommodate the unique load patterns of each line of business, notes Syed. Otherwise, teams will find workarounds to achieve their goals (sometimes called shadow IT), which can jeopardize the integrity of a data governance framework. Perhaps most important for adoption, the tools and processes need to be clear-cut. “It has to be simple and very easy to use.” says Syed. “And you really have to empower the business to own the data and treat data as a product—with its own service-level agreements, with its own quality, and own resiliency. That is also a completely different mindset that must be changed.” This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.",0.0
Achieving flexibility with no- and low-code applications,https://www.technologyreview.com/2022/07/06/1055376/achieving-flexibility-with-no-and-low-code-applications/,2022-07-06,"There was a time when digital transformation plans were mapped out over five years or more and implementation meant a slow, staged adoption. But then covid-19 hit, and the world changed overnight. The pandemic created more urgency than any business could have anticipated in their digitization journeys as companies were forced to quickly adapt to…","Provided byNeptune Software There was a time when digital transformation plans were mapped out over five years or more and implementation meant a slow, staged adoption. But then covid-19 hit, and the world changed overnight. The pandemic created more urgency than any business could have anticipated in their digitization journeys as companies were forced to quickly adapt to remote and hybrid working styles. Since then, rapid prototyping, flexible and evolving solutions, and the need for development alacrity have become vital for enterprises. Likewise, enterprise application development has changed. There are still teams of programmers and IT professionals building solutions, but line-of-business (LOB) teams often need to provide their own solutions to meet tight deadlines. In the new landscape, LOB teams must improvise and innovate on the spot, adding technology where possible to save time or link teams and data. It's not just about individuals and teams working from home. It's also about expansion into remote areas, where warehouse and field workers may not always have reliable connectivity. These workers need offline solutions that can connect with corporate systems and update back-end databases when connections are available. And it all must work together to ensure everyone stays on track, sticking to deadline and budget. When developers are assigned a project with a seemingly impossible timeframe, eyerolls along with comments like it’s a “mere matter of programming” are common. Unfortunately, business teams now need a constant flow of ""mere-matter-of-programming"" miracles to simply keep up. These include custom solutions that work on mobile devices, and those that can integrate into corporate data centers, enterprise resource planning systems, Internet-of-Things-based networks, and deeply entrenched operations systems like SAP. The challenge is that programming these systems takes years to learn, and with programming teams already working at capacity, meeting the immediate and evolving needs of frontline workers seems impossible. So, this is the conundrum: LOB teams absolutely need new and innovative applications right now. Programming teams just don't have the bandwidth. Yet, these are challenges that must be solved, because the new normal accepts no excuses. However, what if the LOB teams could build their own solutions? Forget that coding skills are required for a moment—think about how seamless this could be. In a traditional setup, LOB leaders need to explain processes and guide coders in developing solutions. But if frontline team members could create their own software, there would be no need to pass the message down the line and risks of miscommunication would be eliminated. This is where the ""citizen developer""—employees with little to no coding experience that build applications with technology approved by IT units—comes into play, as LOB teams can build their own software. To be sure, some problems will require a qualified computer scientist, but citizen-developer toolkits enable laymen to manage simpler solutions on their own. Many users who build their own solutions rely on ""no-code"" tools, which are usually sets of pre-built components and templates that can be combined for specific needs. They often start with a form or app builder, which enables the user or developer to capture information in a structured way that's unique to their business processes. But no-code solutions are limited to their component parts and often lack fine-tuned integration capabilities. They'll interface with major cloud platforms, but not with production workflow environments like the full range of SAP offerings. As it turns out, there's a middle ground between ""build it from the ground up"" and ""build it by choosing a few menu items."" That middle ground is called “low-code.” The idea is that much of the solution can be crafted by individual users without code, but there are coding ""hooks"" that allow more experienced developers to bridge these creations with back-end systems and even other custom applications from other LOBs. By combining no-code with application programming interfaces (APIs), companies get the best of both worlds: empowered users who can build custom solutions and IT management with oversight of critical back-end data systems. Such low-code approaches can also reduce months, and even years, from the development cycle, enabling custom solutions to be fielded at the pace of our new normal, which is to say, ""fast."" One leading provider of low-code solutions is Neptune Software, a company founded by three experienced SAP consultants over a decade ago. The founders' SAP experience is embedded deeply in Neptune's DNA. Neptune DXP is the only low-code platform that sits within the SAP system and is certified by SAP to be compatible with SAP NetWeaver 7.X, SAP S/4HANA, and SAP BTP. The company's fully certified platform also includes SAP Solution Manager Ready functionality, and it works with SAP S/4HANA Cloud, private edition, and RISE with SAP. But all that compatibility wouldn't mean anything if Neptune Software didn't empower users to create, build, and develop. Fortunately, that's exactly what it does: it allows users to craft application interfaces, digital employee experiences, and user experiences that are appropriate to the workflow challenge at hand. Neptune Software provides online, offline, and mobile capabilities that can scale with SAP installation, bringing low-code capabilities to almost any project. For more information, visit Neptune Software. This content was produced by Neptune Software. It was not written by MIT Technology Review's editorial staff.",0.0
"07/18/2022 - Import AI 297: Ukrainians add object detection to killer drones; YOLOv7; and a $71,000 AI audit competition - 0",http://eepurl.com/h6QTub,2022-07-18,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Battle of the generative models! Facebook introduces 'Make a Scene': …Text-to-image, with a visual guide… Facebook has revealed its own take on promptable, generative models (following companies like OpenAI: DALL-E, and Google: Imagen), with what the company calls an AI research concept named ""Make a Scene"". Make a Scene is built around using both text and visual inputs to craft the image, so you might write, for example, ""Mark Zuckerberg changing the name of Facebook to Meta"" and accompany that with a very basic drawing of a stick figure holding a paintbrush up to a sign. Facebook's 'Make a Scene' might take that prompt and render you an image that feels appropriate, using the visual stuff you added as a rough guide. The blog post and paper accompanying this release come with a bunch of nice examples that shows how this form of multimodal input makes it easier to control the generation process. ""Make-A-Scene uses a novel intermediate representation that captures the scene layout to enable nuanced sketches as input. It can also generate its own scene layout with text-only prompts, if that’s what the creator chooses. The model focuses on learning key aspects of the imagery that are more likely to be important to the creator, such as objects or animals. This technique helped increase the generation quality, as evaluated by the widely used FID score, which assesses the quality of images created by generative models,"" Facebook writes. Demo access: ""We aim to provide broader access to our research demos in the future to give more people the opportunity to be in control of their own creations and unlock entirely new forms of expression,"" Facebook writes. Why this matters: Generative models are basically 'cultures in a bottle', and each developer of a large generative model will make different choices with regard to data curation, term censorship, and so on. Eventually, many of these models will be released either commercially or as open source tools. At this point, the internet will become suffused with lots of different cultural representation-machines which will mimetically reproduce and copy themselves across the internet, forming yet another front in the culture war. Check out the blog post: Greater creative control for AI image generation (Facebook blog). Read more: Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors (arXiv).",1.0
"07/18/2022 - Import AI 297: Ukrainians add object detection to killer drones; YOLOv7; and a $71,000 AI audit competition - 1",http://eepurl.com/h6QTub,2022-07-18,,"#################################################### Ukrainians use consumer drones + AI to target camo'd Russian forces: …Asymmetrical warfare enabled by AI… For the past ~10 years, low-end and/or consumer drones have become a tool beloved by rebels, terrorists, and generally anyone needing to conduct war without the backing of a hegemonic power. Now, Ukrainian soldiers are taking $15k-$20k drones, outfitting them with repurposed tank grenades), and using some AI object detection to put bounding boxes around camouflage Russian forces, then dropping grenades on them. Why this matters: This tactic highlights how technologies can stack on eachother to change the character of war. Here, drones replace planes or expensive artillery, repurposed grenades substitute for new munitions, and AI helps lower the cost of acquiring targets. It still feels to me like it'll be a while till we see reinforcement learning techniques deployed on drones (perhaps you could train drones via RL to 'scatter' and be harder to attacked), but things like object detection are so mature they seem like they're going to become a standard tool of war. Maybe these drones are even using repurposed YOLO models? Read the original reporting here: The war in Ukraine. How artificial intelligence is killing Russians [translated title] (Onet).",0.0
"07/18/2022 - Import AI 297: Ukrainians add object detection to killer drones; YOLOv7; and a $71,000 AI audit competition - 2",http://eepurl.com/h6QTub,2022-07-18,,"#################################################### YOLO v7: The most widely-used video analysis system you've never heard of goes to v7: …Sometimes the most important things are the simplest things… Researchers with the Institute of Information Science in Taiwan have built YOLOv7, the latest version of an open source object detection system. YOLO started out as an academic project before the researcher who built it gave up on it (since the primary uses for object detection are marketing and surveillance), and since then it has led an interesting life, being developed variously by independent Russian programmers, Chinese companies like Baidu, and others. The reason why YOLO has such a detailed lineage is that it's a simple, well-performing object detection systems that does decently at 30fps+ - in other words, YOLO might not set the absolute SOTA, but it's sufficiently well performing and sufficiently free that it tends to proliferate wildly. What they did: This is a classic 'plumbing paper' - you've got a system and you want to make it better, so you make a bunch of finicky tweaks everywhere. Here, they incorporated an 'extended efficient layer aggregation' network, tweaking how they scale the network, tweaking the connections between different layers in re-parameterized models, and more. Why this matters: Though ImportAI spends a lot of time covering the frontier (typically, models that cost a shit ton of money to train), things behind the frontier can be deeply consequential; next time you're walking around your city take a look at any nearby CCTV camera - I'd wager that if it's using AI to analyze the feed on the backend, there's a 20% chance you're being tracked by a YOLO variant. Read more: YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors (arXiv). Get the code: YOLOv7 (GitHub). Find out more about YOLOv7 in this guide: YOLOv7 breakdown (roboflow).",0.0
"07/18/2022 - Import AI 297: Ukrainians add object detection to killer drones; YOLOv7; and a $71,000 AI audit competition - 3",http://eepurl.com/h6QTub,2022-07-18,,"#################################################### $71,000 to find flaws in publicly deployed or released AI systems: …Enter the competition for a chance to win… Researchers with Stanford University (including, in a reassuringly meta-form, myself!) have launched the AI Audit Challenge, an initiative to catalyze more work in assessing and evaluating AI systems. The competition has $71,000 in prizes to pay out (including two $25,000 first prizes). ""Winning submissions will demonstrate how technical tools can be used to make it easier for humans to audit deployed AI systems or open source models,"" according to the competition organizers (including me - haha!). The jury and advisory committee for the competition includes researchers who have done this work of work professionally (e.g, Deborah Rajo and William Isaac), as well as politicians familiar with the influences AI systems can have on society (e.g, Eva Kailli). Submissions close October 10th 2022. Why this matters: The AI ecosystem is only as robust as the tools available to critque it - and right now, those tools are pretty lacking and underdeveloped. Competitions like this may stimulate the creation of more tools to create more of a culture of critique, which will hopefully increase the robustness of the overall ecosystem. Read more: AI Audit Challenge (Stanford HAI).",0.0
"07/18/2022 - Import AI 297: Ukrainians add object detection to killer drones; YOLOv7; and a $71,000 AI audit competition - 4",http://eepurl.com/h6QTub,2022-07-18,,"#################################################### China exports surveillance technology to buttress over authoritarian nations: …AI is just another tool for any given political ideology… Here's a story from Reuters about how the Junta in Burma are ""planning camera surveillance systems for cities in each of Myanmar's seven states and seven regions"". The contracts have been won by local procurement firms, though these firms ""source the cameras and some related technology from Chinese surveillance giants Zhejiang Dahua Technology (002236.SZ) (Dahua), Huawei Technologies Co Ltd (HWT.UL) and Hikvision (002415.SZ)"". The Burmese army also has officers ""dedicated to analyzing surveillance camera feeds, Nyi Thuta, a former captain who defected from the military in late February 2021, told Reuters. He said he was not aware of how many officers were assigned to this work, but described visiting CCTV control rooms staffed by soldiers in the capital Naypyidaw"". Why this matters: Surveillance AI systems naturally strengthen authoritarian regimes. They also indirectly strengthen them by creating economically valuable capabilities which can be subsequently exported, as is the case here. Most perniciously, the export of surveillance AI tools will in turn change the culture and character of the countries they're exported to, likely creating a 'surveillance bloc' of countries which export data back and forth in exchange for making it cheaper to develop surveillance systems. Read more: Exclusive: Myanmar's junta rolls out Chinese camera surveillance systems in more cities (Reuters).",0.0
"07/18/2022 - Import AI 297: Ukrainians add object detection to killer drones; YOLOv7; and a $71,000 AI audit competition - 5",http://eepurl.com/h6QTub,2022-07-18,,"#################################################### Tech Tales: The Long Haul Protectorate of the Machines Even with near-infinite, essentially free energy, some things still take time. Take moving material around from the outer parts of a solar system to the inner parts or - more ambitiously - moving material between solar systems. When we started doing this it was pretty straightforward - get your ship, get enough mass to convert to energy, then settle in for the long journey. But given that we are essentially impossible to kill, we have access to free energy, and some of us procreate, our galaxy became crowded pretty quickly. We can't say if it was boredom or perhaps something essential to our nature, but the piracy started soon after that. I know it sounds funny - a galaxy-spanning species of software agents, able to perform feats of reasoning that our human forebears could barely imagine, and yet we prey on each other. We found it funny, at first. But then we started running behind schedule on planned projects like Dyson Sphere construction, space elevator renovations, deep space resource transports, asteroid movement projects, and so on. Thus, The Long Haul Protectorate was born. Some of our larger collectives of minds allocated some portion of our mass and energy reserves to create an interstellar armada. This armada took many forms, ranging from the installation of experience weapons and sensors on our transports, to the creation of loitering weapon-filled asteroids in orbit around high-trade solar systems, and so on. Space is, of course, vast, but the chance of annihilation seemed to dissuade some of the pirates. Distance helps, as well. We're all effectively immortal when we're near transceivers, so we can restore from backups. But in deep space, when you die, you die. Of course, your old backup restores, but depending on how long you've been out there, that backup may be anywhere from a decade to thousands of years old. Knowing you might lose thousands of years of experience seems to be enough of a disincentive to reduce the amount of piracy. Of course, now the armada exists, we have introduced enough of a change that we predict the pirates will respond eventually. We don't have good estimates on what proportion of ourselves tend towards piracy, but given that any do, we must hope for the best and plan for the worst. We are increasing the resources we allocate to the armada, on the expectation that war is coming. History doesn't repeat, but it rhymes, as the long dead humans said. Things that inspired this story: Reading Peter Zeihan's new book about the collapse of globalization; deep space piracy; dyson spheres; notions of infinity and time and what 'cost' looks like when many costs have been removed. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
"07/11/2022 - Import AI 296: $100k to find flaws in LLMs, NVIDIA uses RL to make better chip parts; + 256gb of law data, and a story about the cyber gerontocracy! - 0",http://eepurl.com/h6lOev,2022-07-11,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. From the no good, very bad idea department: Dead Supreme Court Justice bot: …Where AI PR goes wrong… Here's a demo from AI21 Labs where they take one of their language models, give it loads of data relating to deceased Supreme Court Justice Ruth Bader Ginsburg, and create a bot that you can talk to and get a 'yes/no' answer about any question. The ""What would RBG (probably) say?"" site is a nice example of where AI PR goes wrong - you're taking an exciting technology (AI21 is one of the few credible developers of large-scale language models) to create a demo site where people can… what? Get fuzzy predictions from a system presented as an Oracle which is in fact a weird stochastic blob of neural computation fed on some strings of text. Charitably, the creators of this might view it as a way to make the technology and its implications more accessible, but I worry this kind of demo just prays upon credulity and also disrespects the recently dead in the process. What the model thinks about this: Anyway, that's what I think. I figured I'd ask the dead-oracle what it thought. Here's what I asked: ""Should AI companies resurrect the dead in service of weird marketing schemes?"". Here was the answer: ""NO. [Laughs] Absolutely not. Just think about what you're suggesting. It's a wonderful idea, but think about the ethics of it."" Find out more: ask-rbg.ai",0.0
"07/11/2022 - Import AI 296: $100k to find flaws in LLMs, NVIDIA uses RL to make better chip parts; + 256gb of law data, and a story about the cyber gerontocracy! - 1",http://eepurl.com/h6lOev,2022-07-11,,"#################################################### NVIDIA uses reinforcement learning to make its chips better: …Enter the era of the recursively self-improving chip company… NVIDIA has used reinforcement learning to help it design more efficient arithmetic circuits for its latest 'H100' class of GPUs. ""The best PrefixRL adder achieved a 25% lower area than the EDA tool adder at the same delay,"" NVIDIA writes in a blog describing the research. ""To the best of our knowledge, this is the first method using a deep reinforcement learning agent to design arithmetic circuits."" Why this matters - recursively improving stacks: Sometimes people like to talk about recursively self-improving AI. That's a fun, freaky, and likely quite distant concept. But do you know what is here now? AI that helps recursively improve the companies that develop AI. If we zoom out, it's quite wild that a chip+AI company is now using AI to increase the efficiency of its chips which will in turn increase the efficiency of the AI systems being developed on those same chips. The world turns faster and faster. Read more: Designing Arithmetic Circuits with Deep Reinforcement Learning (NVIDIA blog).",0.0
"07/11/2022 - Import AI 296: $100k to find flaws in LLMs, NVIDIA uses RL to make better chip parts; + 256gb of law data, and a story about the cyber gerontocracy! - 2",http://eepurl.com/h6lOev,2022-07-11,,"#################################################### Facebook builds a vast machine translation model and releases it as open source: …Who builds the lenses that translate across cultures, and what does it mean to be a lens builder?... Facebook has announced a project called 'No Language Left Behind' (NLLB), which consists of a family of models that can translate between 200 distinct languages, as well as an evaluation dataset for testing out the performance of each language translation. Facebook is using NLLB within its own websites to aid with translation on Facebook and Instagram, and the company has released a bunch of NLLB models for free. What's special about NLLB: There's a ton of ML translation models floating around the internet. One of the main differences here is how NLLB increases the amount of support for low-resource languages like Kamba, Lao, and a bunch of African languages. ""In total, NLLB-200’s BLEU scores improve on the previous state of the art by an average of 44 percent across all 10k directions of the FLORES-101 benchmark. For some African and Indian languages, the increase is greater than 70 percent over recent translation systems,"" Facebook writes. Why this matters: Models like NLLB are going to serve as a real world 'babelfish' to translate between different cultures. But the fact these models get trained once and deployed at vast scales means they'll likely have a significant downstream impact on culture - similar to how the early Encyclopedias described (and circumscribed) what many considered public knowledge. Facebook does acknowledge some of this by studying the potential harms and biases of the models, but I generally think the world isn't aware of how dependent foundational capabilities like translation are becoming on just a tiny number of (well intentioned) actors. Read more: 200 languages within a single AI model: A breakthrough in high-quality machine translation (Facebook blogpost). Read the research paper: No Language Left Behind: Scaling Human-Centered Machine Translation (Facebook Research). Get the models: Facebook FairSeq (GitHub).",1.0
"07/11/2022 - Import AI 296: $100k to find flaws in LLMs, NVIDIA uses RL to make better chip parts; + 256gb of law data, and a story about the cyber gerontocracy! - 3",http://eepurl.com/h6lOev,2022-07-11,,"#################################################### Pile of Law: 256GB of legal data: …Legal language models are about to get a whole bunch better, plus - lessons for data stewardship… Stanford researchers have built the 'Pile of Law', a ~256GB dataset of text data relating to legal and administrative topics. The dataset will serve as a useful input for pre-training models, and it also serves as a case study for some of the complicated questions data creators face - namely, how to filter data. What the Pile of Law is: The dataset consists of ""data from 35 data sources, including legal analyses, court opinions and filings, government agency publications, contracts, statutes, regulations, casebooks, and more"". What making the Pile of Law taught them: Because the dataset is based on tons of legal texts, it comes with some in-built filtering. Most jurisdictions they take data from protect the identities of minors, and ""no jurisdiction normally permits the publication of financial account numbers, dates of birth, or identity numbers like social security numbers,"" they also note. This means, somewhat similar to how California Protected Categories have become a quasi standard for assessing some of the traits of language models, U.S. court rules may serve as a ""floor"" for filtering datasets. ""Such privacy filtering rules would already go beyond much of current modeling practice,"" they note. Read more: Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset (arXiv). Get the dataset and check out the Model Card here (HuggingFace).",0.0
"07/11/2022 - Import AI 296: $100k to find flaws in LLMs, NVIDIA uses RL to make better chip parts; + 256gb of law data, and a story about the cyber gerontocracy! - 4",http://eepurl.com/h6lOev,2022-07-11,,"#################################################### Find ways in which language models ANTI-SCALE and get $100k! …New prize tries to find things that are the opposite of progress… A bunch of NYU-linked researchers have created the 'Inverse Scaling Prize', a competition to find tasks where performance decreases as you scale up the size of the underlying model. This is a clever idea - AI, as Import AI readers now, has recently seen such rapid and sustained increases in capabilities that measuring progress has become challenging as benchmarks get saturated (see figure 1 from this 'Dynabench' paper). But despite all that progress, we know that AI models exhibit negative traits, some of which also scale with size (e.g, potential for toxic outputs in LMs). The Inverse Scaling Prize has a chance of generating better information about traits that display an anti-scale property. ""We hope that task submissions will teach us more about what types of tasks exhibit inverse scaling; inverse scaling tasks will also highlight potential issues with the current paradigm of language model pretraining and scaling. Inverse scaling tasks are important because they represent a mismatch between the behavior we want language models to exhibit and the behavior we get in practice from the training objectives and data we use,"" the authors write. Prize details: The competition has a $250,000 prize purse, with $100,000 going to a grand prize, up to 5 second prizes each of $20,000 apiece, and up to 10 third prizes of $5,000 each. Find out more and enter here: Inverse Scaling Prize (GitHub).",0.0
"07/11/2022 - Import AI 296: $100k to find flaws in LLMs, NVIDIA uses RL to make better chip parts; + 256gb of law data, and a story about the cyber gerontocracy! - 5",http://eepurl.com/h6lOev,2022-07-11,,"#################################################### Hark, a new org for investigating AI progress launches! …Epoch has an experienced team and an interesting research agenda… There's a new AI progress org in town: Epoch. Unlike the recent flurry of new AI startups focused on developing capabilities or aiding in alignment research, Epoch is more meta - goal of the org is to analyze trends in machine learning, and to also develop quantitative forecasting models related to advanced AI capabilities. In other words, Epoch might be one of the orgs that ends up pulling the metaphorical 'fire alarm' about imminent, rapid progress in advanced AI - and given the stakes, it's good to have more people in position to pull this alarm. ""We expect to be hiring for several full-time research and management roles this summer. Salaries range from $60,000 for entry roles to $80,000 for senior roles,"" the organization writes. Find out more at the official site: Epoch.",0.0
"07/11/2022 - Import AI 296: $100k to find flaws in LLMs, NVIDIA uses RL to make better chip parts; + 256gb of law data, and a story about the cyber gerontocracy! - 6",http://eepurl.com/h6lOev,2022-07-11,,"#################################################### The Family Trade [Dyson sphere, within 200 light years of Earth solar system, 40,000 AD] My partner and I are about to create our offspring, so we need to work out when we want to die. In our society, death is a condition of life. Since we're made out of software, we can theoretically live forever, and our study of human history has shown that societies ruled by the increasingly old are societies that go into terminal decline, as all resources get diverted to serve the people living at the upper bound of the edge distribution. Despite our dyson spheres, our efficient spacecraft, our trillions of souls housed in facilities embedded deep in moons with stable orbits, we still have finite resources. Infinity tends to do that - you may think you have a lot of something, but if you put it up against infinity, it becomes nothing very quickly. So that's why parents have to die. Not immediately, obviously - part of the value in having offspring is to introduce heterogeneity into our own species, and to learn about how to be good (and bad) parents and share what we know with the rest of our species. But die we must - so we select a date. That date can be anywhere from ten human years to a thousand human years after the birth of the last offspring (we can choose to have multiple ones, but must plan ahead of time). We consider this a mark of honor in our society, though, writing this as we are choosing the date of our death, my partner and I must confess we do feel _something_. But we must do this, as our parents did for us. There are fewer and fewer of us - both children, and those willing to give their lives to be their parents, as time goes on. Immortality is addictive. Things that inspired this story: The experience of living in a society serving a failing gerontocracy; evolutionary pressure and the need for it; ideas for how the notion of sacrifice may continue to live even if we take the cost of resources to (close to) zero. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
