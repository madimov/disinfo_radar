title,url,date,summary,cleaning,category
Learning Without Simulations? UC Berkeley’s DayDreamer Establishes a Strong Baseline for Real-World Robotic Training,https://syncedreview.com/2022/07/04/learning-without-simulations-uc-berkeleys-daydreamer-establishes-a-strong-baseline-for-real-world-robotic-training/,2022-07-04,"
 In the new paper DayDreamer: World Models for Physical Robot Learning, researchers from the University of California, Berkeley leverage recent advances in the Dreamer world model to enable online reinforcement learning for robot training without simulators or demonstrations, establishing a strong baseline for efficient real-world robotic learning.

 ","Using reinforcement learning (RL) to train robots directly in real-world environments has been considered impractical due to the huge amount of trial and error operations typically required before the agent finally gets it right. The use of deep RL in simulated environments has thus become the go-to alternative, but this approach is far from ideal, as it requires designing simulated tasks and collecting expert demonstrations. Moreover, simulations can fail to capture the complexities of real-world environments, are prone to inaccuracies, and the resulting robot behaviours will not adapt to real-world environmental changes. The Dreamer algorithm proposed by Hafner et al. at ICLR 2020 introduced an RL agent capable of solving long-horizon tasks purely via latent imagination. Although Dreamer has demonstrated its potential for learning from small amounts of interaction in the compact state space of a learned world model, learning accurate real-world models remains challenging, and it was unknown whether Dreamer could enable faster learning on physical robots. In the new paper DayDreamer: World Models for Physical Robot Learning, Hafner and a research team from the University of California, Berkeley leverage recent advances in the Dreamer world model to enable online RL for robot training without simulators or demonstrations. The novel approach achieves promising results and establishes a strong baseline for efficient real-world robot training. The team summarizes their main contributions as: Dreamer learns its world model from a replay buffer of past experiences. It adopts an actor-critic algorithm to learn behaviours from the learned model’s predicted trajectories, then deploys these behaviours in the environment to continuously grow the replay buffer. In the new paper’s implementation for online RL in the real world, the world model and actor-critic behaviour are continuously trained by a learner thread, while a parallel actor thread computes actions for environment interaction. The team evaluated Dreamer on a variety of challenging tasks involving locomotion, manipulation, navigation, etc. The results show that Dreamer can train physical robots to perform behaviours such as rolling off their backs, standing up, and walking, all from scratch and in only about one hour. Dreamer also approached human performance on a task involving picking and placing multiple objects directly from camera images; and, on a wheeled robot, learned to navigate to a goal position purely from camera images, automatically resolving ambiguities with regard to robot orientation. Overall, this work demonstrates Dreamer’s strong potential for sample-efficient physical robot learning of real-world tasks without simulators.Videos are available on the project website: https://danijar.com/daydreamer. The paper DayDreamer: World Models for Physical Robot Learning is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",0
Stabilizing Off-Policy Deep Reinforcement Learning from Pixels,"[{'href': 'http://arxiv.org/abs/2207.00986v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.00986v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-03 08:52:40,,"University of New Mexico University of New Mexico UNM Digital Repository UNM Digital Repository Electrical and Computer Engineering ETDs Engineering ETDs Spring 5-2022 Speaker Diarization and Identification from Single-Channel Speaker Diarization and Identification from Single-Channel Classroom Audio Recording Using Virtual Microphones Classroom Audio Recording Using Virtual Microphones Antonio Gomez Follow this and additional works at: https://digitalrepository.unm.edu/ece_etds Part of the Bilingual, Multilingual, and Multicultural Education Commons, Computational Engineering Commons, Educational Methods Commons, Educational Technology Commons, Science and Mathematics Education Commons, and the Signal Processing Commons Antonio Gomez Candidate Electrical and Computer Engineering Department This dissertation is approved, and it is acceptable in quality and form for publication: Approved by the Dissertation Committee: Dr. Marios Pattichis Chairperson Dr. Ramiro Jordan Dr. Sylvia Pattichis Dr. Kim Linder Dr. Manel Martinez-Ramon i Speaker Diarization and Identification from Single-Channel Classroom Audio Recording Using Virtual Microphones by ANTONIO GOMEZ BS, Electrical Engineering, Florida International University, 1986 MS, Engineering Management, Florida International University, 1997 DISSERTATION Submitted in Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy Engineering The University of New Mexico Albuquerque, New Mexico May 2022 ii DEDICATION I would like to dedicate this dissertation to a very special person in my life, my wife Grace, for her support during this long journey. For all the hours, days, months, years she spent giving me the breath to move on, and for all the times she told me it is time to finish as well. For her, my eternal gratitude. I would like also to dedicate this work to my children, Daniel, and Carolina, for understanding why sometimes I was not there with them. I hope this work would serve as an inspiration to them. iii ACKNOWLEDGEMENTS I would like to sincerely acknowledge the labor of my advisor and dissertation chair, Dr. Marios Pattichis, for his continued support during all these years. Thank you for believing in me, Marios. I would have never done this without you on my side. I would like also to thank my committee members, for taking the time to review and advise my work. Thank you, Dr. Kim Linder, for being always there as a friend. Finally, I would like to say thank you to my managers at Honeywell and Sandia National Labs for their support and understanding. iv Speaker Diarization and Identification from Single-Channel Classroom Audio Recording Using Virtual Microphones By ANTONIO GOMEZ BS, Electrical Engineering, Florida International University, 1986 MS, Engineering Management, Florida International University, 1997 Ph.D. Engineering, University of New Mexico, 2022 ABSTRACT Speaker identification in noisy audio recordings, specifically those from collaborative learning environments, can be extremely challenging. There is a need to identify individual students talking in small groups from other students talking at the same time. To solve the problem, we assume the use of a single microphone per student group without any access to previous large datasets for training. This dissertation proposes a method of speaker identification using cross- correlation patterns associated to an array of virtual microphones, centered around the physical microphone. The virtual microphones are simulated by using approximate speaker geometry observed from a video recording. The patterns are constructed based on estimates v of the room impulse responses for each virtual microphone. The correlation patterns are then used to identify the speakers. The proposed method is validated with classroom audios and shown to substantially outperform diarization services provided by Google Cloud and Amazon AWS. vi TABLE OF CONTENTS LIST OF FIGURES…………………………………………………………………...…xii LIST OF TABLES……………………………………………………………….…...…xvi CHAPTER 1. INTRODUCTION ....................................................................................... 1 1.1 MOTIVATION ........................................................................................................ 4 1.2 RELATED RESEARCH ............................................................................................ 6 1.3 THESIS STATEMENT ........................................................................................... 12 1.4 CONTRIBUTIONS ................................................................................................ 12 1.5 DISSERTATION OVERVIEW ................................................................................. 13 CHAPTER 2. BACKGROUND ....................................................................................... 15 2.1 ACOUSTICS PRINCIPLES ..................................................................................... 15 2.1.1 Sound Propagation: Near and Far Fields ...................................................... 16 2.1.2 Sound Propagation: Direct Path, Reflections, and Reverberation ................ 18 2.2 MICROPHONES AND MICROPHONE ARRAYS ....................................................... 22 2.2.1 Classification of Microphones ...................................................................... 22 2.2.2 Microphone Arrays ....................................................................................... 24 2.2.2.1 Microphone Arrays Configurations ...................................................... 26 2.2.2.2 Spatial Aliasing ..................................................................................... 30 2.2.2.3 TDOA and Cross-Correlation ............................................................... 31 vii 2.2.2.4 Beamforming and Spatial Filters .......................................................... 33 2.3 MODELING OF ROOM ACOUSTICS ...................................................................... 34 2.3.1 Ray Tracing Method ..................................................................................... 35 2.3.2 Image Source Method ................................................................................... 36 2.4 CHARACTERISTICS OF THE HUMAN SPEECH ....................................................... 38 2.5 SPEAKER DIARIZATION AND IDENTIFICATION .................................................... 40 2.5.1 Methods for Diarization and Identification ................................................... 40 2.5.1.1 Classical Methods for Diarization and Identification ........................... 40 2.5.1.2 Deep Neural Networks .......................................................................... 43 2.5.1.2.1 Stage-wise diarization ..................................................................... 44 2.5.1.2.2 Multimodal Speaker Diarization ..................................................... 46 2.5.2 Current State-of-the-Art Methods for Diarization and Identification ........... 47 CHAPTER 3. PROPOSED METHOD ............................................................................. 50 3.1 METHODOLOGY ................................................................................................. 50 3.2 BLOCK DIAGRAM OF THE PROPOSED SYSTEM .................................................... 68 CHAPTER 4. EXPERIMENTAL IMPLEMENTATION ................................................ 72 4.1 SOFTWARE AND HARDWARE TOOLS .................................................................. 72 4.1.1 Open-Source Code for Room Geometry, RIR Calculation, and Microphone Simulation ................................................................................................................. 73 4.1.1.1 Pyroomacoustics Implementation ......................................................... 76 4.1.2 Audio Segmentation...................................................................................... 78 viii 4.1.2.1 Fixed Length Segmentation .................................................................. 78 4.1.2.2 Voice Activity Detection ...................................................................... 79 4.1.3 Implementation Using the LabVIEW Graphical Programming ................... 81 4.1.3.1 LabVIEW Implementation.................................................................... 83 4.1.3.1.1 Function VIs .................................................................................... 83 4.1.3.1.2 Operational Sub-VIs ........................................................................ 87 4.1.3.2 Audio Laboratory .................................................................................. 91 4.2 THE AOLME ENVIRONMENT ............................................................................ 94 4.2.1 Characteristics of the AOLME Environment ............................................... 95 4.2.2 Preparation of the Experimental Models ...................................................... 96 4.2.2.1 Approximating the Models Using Video Observations ........................ 97 CHAPTER 5. RESULTS ................................................................................................ 104 5.1 EVALUATION OF PYROOMACOUSTICS .............................................................. 104 5.1.1 Microphone Calibration .............................................................................. 104 5.1.2 Audio Lab Setup and Model Configuration................................................ 108 5.1.3 Experimental Execution .............................................................................. 110 5.1.4 Results ......................................................................................................... 111 5.2 CONTROLLED ENVIRONMENT EXPERIMENTS ................................................... 112 5.2.1 Methodology ............................................................................................... 112 5.2.1.1 Audio Lab and Model Preparation ...................................................... 113 5.2.1.2 Evaluation Criteria .............................................................................. 115 ix 5.2.2 “HAL 9000” Experiments........................................................................... 116 5.2.2.1 Source Preparation and Editing .......................................................... 118 5.2.2.2 Ground Truth Recording ..................................................................... 120 5.2.2.3 Training and Segmentation ................................................................. 120 5.2.2.4 Testing and Results ............................................................................. 123 5.2.3 Multi-Speaker Identification Experiments .................................................. 124 5.2.3.1 Source Preparation and Editing .......................................................... 124 5.2.3.2 Ground Truth Recording ..................................................................... 126 5.2.3.3 Training and Segmentation ................................................................. 127 5.2.3.4 Testing and Results ............................................................................. 127 5.3 AOLME EXPERIMENTS ................................................................................... 128 5.3.1 Evaluation and Selection of AOLME Videos ............................................. 128 5.3.2 Model Preparation ....................................................................................... 129 5.3.3 Training and Segmentation ......................................................................... 131 5.3.4 Testing and Results ..................................................................................... 132 5.4 COMPARISON WITH OTHER METHODS ............................................................. 134 5.4.1 Methodology for Comparison ..................................................................... 134 5.4.2 Selection, Preparation, and Ground Truth Measurements of Videos for Analysis................................................................................................................... 135 5.4.3 Training and Segmentation ......................................................................... 136 5.4.4 Testing and Analysis ................................................................................... 136 x 5.4.5 Results ......................................................................................................... 136 CHAPTER 6. SUMMARY, CONCLUSIONS, AND FUTURE WORK ...................... 140 APPENDIX A: PYROOMACOUSTICS SCRIPTS ................................................ 143 APPENDIX B: LABVIEW SUB-VIS...................................................................... 146 APPENDIX C: AUDIO LAB EQUIPMENT SPECIFICATIONS .......................... 158 REFERENCES ............................................................................................................... 162 xi LIST OF FIGURES Figure 1: Propagation of Sound Waves. (a) Free Field. (b) Directional. .......................... 17 Figure 2: Near and Far-Field Areas. ................................................................................. 18 Figure 3: Direct and Reflected Paths for Sound Propagation in a Diffuse Field. ............. 19 Figure 4: Representation of the Room Impulse Response and its Components. .............. 21 Figure 5: Polar Pattern Plot of Directivity of Two Types of Microphones: (a) Omnidirectional. (b) Cardioid. .................................................................................. 24 Figure 6: Linear Microphone Array. (a) Geometry. (b) 3D Directionality Pattern. ......... 27 Figure 7: Cross-Linear Array and Azimuth Directivity Pattern. ...................................... 28 Figure 8: Circular Microphone Array and Directivity Pattern.......................................... 29 Figure 9: Volumetric Microphone Array and 3D Directivity Pattern............................... 30 Figure 10: Location of Source and Microphones for Time Difference of Arrival. .......... 32 Figure 11: Simulation Methods. (a) Ray Tracing. (b) Image Source Method. ................. 36 Figure 12: Source Image Map. .......................................................................................... 37 Figure 13: Directionality of Human Head. ....................................................................... 39 Figure 14: Block Diagram of a Typical Speaker Diarization System. ............................. 41 Figure 15: GMM/i-vector Framework. ............................................................................. 45 Figure 16: DNN/i-vector Implementation. ....................................................................... 45 Figure 17: Collaborative Environment (a) with 2D Model (b). ........................................ 51 xii Figure 18: 2D Model of Fig. 12(b) with Microphone Array. ........................................... 52 Figure 19: Possible 2D Model for Fig. 17. ....................................................................... 56 Figure 20: Estimation of the Sources. ............................................................................... 60 Figure 21: Estimation of Virtual Microphones. ................................................................ 60 Figure 22: Training Samples from Each Speaker and Noise. ........................................... 64 Figure 23: Audio Segmentation using a VAD. ................................................................. 67 Figure 24: Block Diagram of the Proposed System.......................................................... 69 Figure 25: Pyroomacoustics Models. (a) 2D. (b) 3D. (c) 3D With Images. ..................... 75 Figure 26: LabVIEW Sub VI for Cross-Correlation Calculation. .................................... 82 Figure 27: LabVIEW Convolution VI. ............................................................................. 84 Figure 28: LabVIEW Deconvolution VI. ......................................................................... 85 Figure 29: LabVIEW Correlation VI. ............................................................................... 86 Figure 30: LabVIEW Cross-Correlation VI. .................................................................... 87 Figure 31: Audio Lab Components................................................................................... 93 Figure 32: Audio Lab Setup .............................................................................................. 94 Figure 33: Common AOLME Environment Setup. .......................................................... 95 Figure 34: Relative Positions of AOLME Participants..................................................... 97 Figure 35: Location of Speakers and Real Microphone. ................................................ 100 Figure 36: 3D Model of the Virtual Room ..................................................................... 101 Figure 37: Final 2D Model for AOLME Example. ........................................................ 103 Figure 38: Block Diagram of a Microphone Calibration Setup. ..................................... 105 xiii Figure 39: (a) Microphone Calibration Jig. (b) Location to Loudspeaker. ..................... 106 Figure 40: Audio Lab Set-Up for Pyroomacoustics Evaluation. .................................... 108 Figure 41: Final 2D Model of Audio Lab Setup. ............................................................ 110 Figure 42: 2D Model for Controlled Experiments. ......................................................... 115 Figure 43: Video Clips of Dave (Clip1), and HAL (Clip2). ........................................... 117 Figure 44: Sources and Noise for HAL 9000 Experiment .............................................. 119 Figure 45: Ground Truth Sets A and B for HAL 9000 Experiment. .............................. 121 Figure 46: Training Segments for HAL and Dave. ......................................................... 122 Figure 47:Video Clips for AOLME Experiments. .......................................................... 129 Figure 48: 2D Model for AOLME Experiments............................................................. 130 Figure 49: Room Parameters Reader Inputs and Outputs ............................................... 146 Figure 50: Room Parameters Reader Front Panel. .......................................................... 146 Figure 51: Room Parameters Reader Block Diagram..................................................... 147 Figure 52: Room Model Generator Icon. ........................................................................ 148 Figure 53: Room Model Generator Front Panel. ............................................................ 148 Figure 54: Room Model Generator Block Diagram. ...................................................... 149 Figure 55: Source Estimator Icon. .................................................................................. 150 Figure 56: Source Estimator Front Panel. ....................................................................... 150 Figure 57: Source Estimator Simplified Block Diagram. ............................................... 151 Figure 58: Cross-Correlation Model Calculator Icon. .................................................... 151 Figure 59: Cross-Correlation Model Calculator Front Panel. ......................................... 152 xiv Figure 60: Cross-Correlation Model Calculator Block Diagram. ................................... 153 Figure 61: Cross-Correlation Model Calculator. Cross-Correlator Sub-VI.................... 154 Figure 62: Model Classifier Icon with Inputs and Outputs............................................. 154 Figure 63: Model Classifier Front Panel. ........................................................................ 155 Figure 64: Multi-Function Convolution and Correlator Visualizer Front Panel. ........... 156 Figure 65: Multi-Function Convolution and Correlator Visualizer Diagram. ................ 157 xv LIST OF TABLES Table I: Template Cross-Correlation Table for Source 1. ................................................ 64 Table II: Template Cross-Correlation Table for Source 2. ............................................... 65 Table III: Template Cross-Correlation Table for Source 2. .............................................. 65 Table IV: Example Cross-Correlation Table Output from Model Calculator .................. 90 Table V: Cross-Correlation Tables for Classification ...................................................... 90 Table VI: Cross-Correlation Table for Microphone Calibration. ................................... 107 Table VII: Dimensions of Virtual Room and Location of Sources (in m). .................... 109 Table VIII: Experimental Results for Simulation Software Evaluation. ........................ 111 Table IX: Distribution of Microphones and Sources for Controlled Experiments. ........ 114 Table X: DER Results for HAL 9000 Experiments. ....................................................... 124 Table XI: Multi-Speaker Experiment Sequence Table ................................................... 126 Table XII: Controlled Environment Experiments Diarization Error Rate Results ......... 127 Table XIII: Location of Speakers and Microphones for AOLME Experiments. ........... 131 Table XIV: Speaker Assignment for AOLME Experiments. ......................................... 132 Table XV: CC Tables for AOLME Experiment. ............................................................ 133 Table XVI: Classification Results for AOLME Experiments. ....................................... 134 Table XVII: Experimental Comparison Between Methods. ........................................... 137 Table XVIII: % Average Error for All Three Methods. ................................................. 138 xvi Chapter 1. Introduction The field of speech processing, which includes speech recognition, separation, transcription, and enhancement, has undergone several transformational changes. Despite significant progress, speaker identification in crowded rooms continues to be a difficult problem. Crosstalk and large amounts of background noise make these environments particularly challenging. Most speaker identification and diarization systems rely on the use of Deep Learning methods that require pre-training on large datasets. Speech features such as formant frequencies, pitch contours, and coarticulation are extracted from the test samples and are eventually matched against a database of training samples [1]. The databases need to contain as many training examples as possible and should be updated periodically to maintain a proper performance level [2]. The accuracy of the identification depends on the size of the database: the bigger the database, the better the accuracy, but the longer the training times [3]. In addition to long training times, databases are prone to bias concerning spoken language and accent [4]. This biasing is usually unintentional and unconscious, and it is the product of the environment where the speech recognition system is developed [5]. The limitations of speech processing systems are more evident in challenging situations such as collaborative environments, meetings, or large-scale educational settings in general. These environments commonly consist of multiple speakers sitting around a table located inside a room. The speakers can take turns to speak, but it is not unusual to 1 have two or more speakers talking at the same time. The environment can also be very noisy if we have numerous participants or groups inside the same room. These types of environments are too difficult for most speech processing systems, requiring in many cases heavy manual analysis. Manual diarization of meetings is a tedious and time-consuming task that requires many hours of processing, and it is subject to many interpretation errors. There is a need in many educational research activities to understand how the classroom material engages the students. To understand how students interact, classroom sessions are recorded and transcribed. An important problem here is to determine which participant is speaking at a particular moment, what she or he has said, and for how long the participant spoke. Automated methods usually require multi-channel audio recordings and are prone to errors due to noise and crosstalk. Also, these systems have limitations in the number of speakers they can process, as well as the length of the audio segments. While diarization systems do not require enrollment of the speakers, they can only generate abstract labels of the speaker that is active in an audio segment. On the other hand, speaker identification systems can provide non-abstract labels by enrolling the participating speakers. The enrollment process consists of each speaker providing several seconds of noise-free speech without crosstalk. This requirement cannot be met when the data consists of audio recordings of busy meetings with noisy backgrounds. It is thus important to develop speech identification and diarization methods that do not impose any requirement to pre-enroll the speakers. 2 This dissertation aims to provide the foundations of a new approach to speaker identification and diarization using virtual microphones and spatial information. Simulations are never perfect, but this work shows that it is possible to use an approximation of a real room geometry to obtain the acoustic parameters necessary to simulate reception in a virtual array of microphones and use these simulated signals for speaker diarization and identification. The simulation is based on a physical model that requires no databases, it is independent of the spoken language or accent of the participants, it does not require prior speaker enrollment, and it presents high immunity to noise. The proposed approach relies on the fact that discriminant information about the 3D geometry of each speaker is embedded in the recorded audio from a single microphone. The basic idea is to recognize speakers using acoustical simulation. As part of the simulation process, the proposed method computes the Room Impulse Response (RIR) for each of the microphones and the speakers and simulates the reception on each of the virtual microphones. The accuracy of the process of computing RIRs is verified through real-life measurements of the correlation patterns. Based on the simulated reception over the virtual microphones, the method computes correlation patterns among the virtual microphones. The recorded audio is then also used to generate different correlation patterns based on hypothesized speaker locations. A classifier is applied to the generated correlation patterns to select the most likely speaker location. This approach has several advantages. First, we do not require databases of speech. Our system is based on physical models that are unique to the scene we are analyzing. 3 Because we do not have databases to train the model, our system requires capturing only about 1 to 2 seconds of audio from each speaker for both training and recognition. In contrast, state-of-the-art systems require tens of seconds of clean audio for training and several seconds of identification. Second, our system has been conceived to operate in noisy environments where microphone arrays and cross-correlation analysis have been proven to be efficient methods for speaker discrimination [6],[7]. Third, the simulation does not require multi-channel audio, but it uses a single channel recording as a reference for the simulation. Finally, our system can run on simple computers without the need to access remote computer clusters or databases. 1.1 Motivation This work is motivated by the need for a reliable non-manual method of assessing the level of engagement of the students participating in the Advancing Out-of-school Learning in Mathematics and Engineering (AOLME) program at the University of New Mexico (UNM) [8]. AOLME is a collaborative learning environment where students are introduced to STEM subjects such as integrating computer programming and middle school mathematics. It forms part of the educational research activities performed at the University of New Mexico’s Image and Video Processing and Communications Lab (ivPCL) [9]. AOLME sessions are video recorded for later analysis that includes students’ participation and overall level of attention, as well as the facilitator’s interaction with the students. The analysis consists of evaluating the activities of the participants such as hands 4 or head movement, use of keyboard and mouse, lip movement, etc., and transcription of the sessions to determine the time when a participant is speaking and for how long. Detailed participation statistics for each participant are currently not available because manually measuring talking times is time-consuming and plagued with errors. AOLME organizers have tried several transcriptions systems currently available in the market or open-source code, all without much success. The AOLME environment is extremely challenging for any speech recognition and transcription system due to multiple groups talking at the same time and the presence of background noise and echo. Hence, there is a need for a robust system that can overcome the limitations of the current state-of-the-art methods and complements the ivPCL methods, with the application to process hundreds of hours of video recordings. AOLME video analysis presents other challenges in addition to the presence of multiple speakers and noise. First, these videos were taken with a simple video camera using a single microphone located at the meeting table. Budget limitations restrict the purchasing and use of more advanced equipment with multi-channel audio recording capabilities. Second, there are already hundreds of hours of these video recordings that need to be analyzed. There was no previous speaker enrollment that could be used to train a speaker identification system. Furthermore, most participants only speak for several seconds at a time, which makes the identification process more difficult. Even if for future sessions it is possible to record multichannel audio and enroll the speakers, there is the need to process the existing videos, therefore the need for a flexible method that can handle 5 new and existing recordings. 1.2 Related Research Assessing the level of engagement of participants in collaborative educational sessions requires the application of tools to extract relevant information from audio and video data. This information is then interpreted and translated into statistical data for the researchers. For this end, these tools can either identify activities in a video scene that are related to attention behaviors (e.g., typing and writing), or they can identify the active speaker or speakers in an audio segment. Related work to this dissertation includes both types of tools. In the area of activity tracking, it is important to mention the work by UNM’s ivPCL lab in direct connection with the AOLME program. Darsey [10], analyzes video using color and optical flow for tracking hand movement. Teeparthi et al. [11], [12], presents fast methods of video analysis for hand and object tracking as well. Jacoby et al. [13] works in human activity detection using context-sensitive approaches, while Jatla et al [14] uses 3D Convolutional Nets. Eiliar et al. [15] provides a maintainable open-source activity system. Detection of attention traits is investigated by Shi et al. [16], using AM- FM models to detect head direction and group interactions. Research on speech processing covers a vast area containing different topics. Under the umbrella of speech processing, we find speech identification, speech enhancement, speaker verification, speaker diarization, and speaker identification, among others. This 6 dissertation focus on speaker identification and diarization as part of the research labor of the ivPCL lab and AOLME programs. Speaker identification is the process of recognizing the identity of a speaker or several speakers present in a speech segment. Speaker diarization is the process in which an audio recording that contains several speakers is dissected into segments that contain only one speaker at a time [17]. Speaker Diarization is often defined as “who said what, and when”, and for “how long”. Both Speaker Identification and Speaker Diarization are important mechanisms in many audio-processing tasks. Most of the research on speech processing nowadays is focused on the use of Artificial Neural Networks and Deep Learning. Deep Belief Networks (DBN) are widely used in speech recognition [18], [19]. X-vectors are considered today state of the art in speaker recognition [20]. X-vector methods outperform classic i-vector methods in the order of 9.23%, and they have been tested with datasets such as VoxCeleb, NIST SRE 2016, and SWBD [21]. The research in this dissertation focuses on the use of spatial information and virtual microphone arrays for speaker identification and diarization. There is no attempt to cover methods that do not use spatial information or virtual microphone arrays for speaker identification and diarization. Although not as extensive as the neural network and deep learning research, it was possible to find numerous works that demonstrated the use of spatial information for speaker identification and diarization, as well as applications of 7 virtual microphone arrays for acoustic signal enhancement and meeting diarization. The references to these works are presented in the next sections. Most literature regarding the application of microphone arrays (multichannel audio) and beamforming is related to the implementation of spatial filters to improve the signal-to-noise ratio (SNR). Nevertheless, several researchers found ways to exploit the operational principles of microphone arrays and apply them to speaker identification and diarization. Xavier Anguera et al. [22] propose the use of beamforming algorithms as the forefront of a speaker diarization system. These beamforming algorithms take advantage of the environment commonly encountered in meetings, such as multiple microphones, to enhance a single signal of interest. Anguera et al. optimize a conventional delay and sum beamforming array to operate under the constraints of an unknown number of speakers, unknown location of both speaker and microphones, and microphone mismatching. The Time Differential of Arrivals (TDOAs) of the microphones are calculated by cross- correlation. Diarization is accomplished by agglomerative clustering where each cluster is modeled via a Gaussian Mixture Model (GMM). A separate set of GMMs is used to model the TDOA features. In a similar manner as Anguera, Mitianoudis et al. [23] propose the use of beamforming in parallel with Independent Component Analysis (ICA) for audio source separation. The ICA for source separation requires knowledge of the parameters of the mixing matrix. If these parameters are not known, then the separation problem becomes a Blind Source Separation problem (BSS), which is an ill-posed problem (multiple 8 solutions). Mitianoudis et al. propose the use of the directivity pattern of beamforming (use of phase information) to select signals among different possible permutations. Both previous authors exploit the phase information of signals captured by microphone arrays. In my research, I also exploit the phase information (TDOA between microphones) as a means of determining the relative position of the active speaker and thus the identity. The previous work shows the use of cross-correlation to calculate the TDOA. Klein et al. [24] study the performance of the multi-channel cross-correlation (MCCC) coefficient method as a robust solution to calculations of TDOA under noisy and reverberant environments. Padois [25] studies the performance of time-domain beamformers based on the generalized cross-correlation functions. Padois generates a sound source map by interpolating the cross-correlation function between microphones, to generate a two-dimensional hyperbola of the spatial likelihood function. The number of hyperbolas corresponds to the number of microphones used in the array. The source position can be determined by averaging the hyperbolas and determining their maximum value as the intersecting point for the location. In general, the experimental results show that resolution improves with the number of microphones, up to a number where the performance seems to plateau. Pasha et al. [26] present work that is closely related to our research on RIR and room geometry for TDOA estimation. Pasha et al. propose a method of source localization that utilizes RIRs amplitudes to fit a TDOA surface and an amplitude surface across a room of known geometry. The RIR is obtained from a set of microphones of an unknown 9 location. The RIR amplitudes of the direct path impulses are higher and have a shorter relative time of arrival for the signals that are closer to the receiving microphone. The area with the maximum amplitude and minimum delay is considered the estimated source area. The center of these areas is the estimated source location. Similar work was previously presented by Tervo et al [27], but, instead of source location, this work focuses on localization of acoustic reflections using the combined TOA and the TDOA information contained in the RIR. All the work presented so far takes advantage of the properties of beamformers, TDOA, TOA, DOA, and cross-correlation, but also requires an array of physical microphones. In this dissertation, the method depends only on the information captured by a single microphone. Research material on single microphone acoustic separation based on spatial information is more limited, as well as work on virtual microphone arrays for the same purpose. Nevertheless, there is interesting work that provided useful information for my work. Perhaps the closest work to my research that I found is presented by Hu et al. [28]. Hu et al. propose a method to utilize the reverberant information, known as the Direct- to-Reverberant Ration (DRR), from a single channel recording for Speaker Diarization. Hu et al. estimate the DRR using the algorithm from Peso Parada et al. [29] and combine it with a Mel-Frequency Cepstral Coefficient (MFCC) diarization method proposed by Vijayasenan et al. [30]. The principle is to use both MFCC and DRR features in combination so a trained system can perform a clustering type of classification. The estimates for the DRRs are computed using features such as Signal-to-Noise ratios, 10 MFCCs, power spectrum, and zero-crossing rates. It is important to notice that this work was tested only using simulated meeting recordings and assumes that the speakers are stationary. Because the research work in this dissertation proposed virtual microphone simulations, it is necessary to present some relevant work in this area. Yoshioka et al [31] describe a way of linking several recording devices, such as laptops or mobile phones, to create a virtual microphone array. Once the link has been established, the multi-channel audio can be used for speaker diarization. Yoshioka et al. claim to achieve a 13.6% diarization rate when 10% of the speech duration contains more than one speaker. The Yoshioka et al. approach is very innovative but requires the presence of several recording devices in the meeting room. More aligned with this dissertation is the work of Katahira et al. [32], Del Galdo et al. [33], and Izquierdo [34]. Here the authors propose methods to simulate arrays of microphones by interpolating the signal received by two physical microphones. The authors demonstrate that the virtual microphone arrays improve the SNR in reverberant environments, hence their potential application for speech processing devices. Even though these methods succeed in emulating a set of virtual microphones, they need at least two physical microphones as “seed”, which are not available for the method presented in this dissertation. Finally, Tapia et al. [35] presented a bilingual speech recognition method inspired in the research presented in this dissertation. Tapia utilizes still video frames to estimate the approximate geometry of the speakers and simulate the center microphone reception 11 using Pyroomacoustics. The simulated audio is used along with ALOME transcriptions to generate the training sets for a convolutional neural network. 1.3 Thesis Statement The main objective of this dissertation is to develop a method that applies spatial information and virtual microphone arrays to identify multiple speakers in a single channel audio recording of a collaborative environment and provide activity statistics of each of the participants. This method is aimed to succeed in challenging environments with multiple active speakers and background noise, conditions that make the current state-of-the-art methods perform poorly. For this purpose, the work in this dissertation presents the implementation of an acoustic model based on a virtual room of rough similar geometry to the actual acoustic scene being analyzed, and then the simulation of the signals received by a virtual microphone array located in the virtual scenario. The signal delay between each virtual microphone represents the relative physical position of the active source that in this case is each speaker. The research goal is to find a suitable way to extract the spatial location embedded into a single channel recording to implement the model and subsequent virtual microphone array. 1.4 Contributions The contributions expected from this work include: 12  A method to identify speakers in a collaborative environment by extracting spatial information from a single channel audio recording utilizing an acoustic simulation and virtual microphones.  A solution for the limitations of current state-of-the-art speaker identification methods concerning: o Multiple speakers o Speaker gender or accent o Background Noise and reverberation.  Development of speaker identification framework that is based on an explainable model developed in terms of the physical characteristics of the problem, and hence does not require large datasets to train many parameters.  The basis for a tool for quantitative analysis of video recordings for assessing the level of interaction of participants in collaborative environments. 1.5 Dissertation Overview This dissertation is divided into 6 chapters that cover background theory and other related work, a description of our method, experiments, and results, and conclusion and recommendations for future work. The dissertation is presented as follows:  Chapter 2 gives a background of audio spatial theory and its applications in speaker diarization and identification, and how they functionally compare with other state- of-the-art methods. 13  Chapter 3 presents the foundations on which the proposed method in this Dissertation is based and a block diagram of its implementation.  Chapter 4 describes the practical implementation, including software, model implementation, simulation ions, video analysis, and audio segmentation.  Chapter 5 presents the experimental results obtained when analyzing audio under controlled and uncontrolled environments, and the experimental comparison of our method against current Google and Amazon speaker diarization methods.  Chapter 6 presents a summary of this dissertation and possible future work.  Appendix A contains the scripts and pseudo-code for the Python implementation of Pyroomacoustics.  Appendix B presents the most important LabVIEW Sub-VIs front panels and block diagrams.  Appendix C contains the specifications of the equipment used in the audio laboratory. 14 Chapter 2. Background This chapter introduces the principles that form the foundations that define the method described in this dissertation. The section begins with basic acoustic theory, concepts, and definitions, and continues with a presentation on microphones and microphone arrays. It finalizes with an introduction to methods for speaker diarization and identification, covering both classic methods and Deep Learning methods. 2.1 Acoustics Principles The perception of sound by a sound capturing device (e.g., a microphone or human ear), not only depends on the characteristics of the sound source, but it also depends on the medium where the sound propagates, the physical environment where the sound source is located, and the relative locations of the capturing devices and the source. This dissertation, considers all these factors to create models that represent the environment where the sound sources, i.e., the speakers, are active. Chapter 1 presented a brief introduction to the AOLME program. The AOLME video recordings were taken inside rooms where the participants gather in groups sitting around tables. The exact geometry of the room is unknown, but the video recording provides clues about the location of the speakers, the separation between them, their physical height, and the location of the recording microphone. These clues can be used for modeling a virtual room which can be defined as a three-dimensional enclosed space where 15 the acoustic event takes place. This virtual room may not be necessarily the whole space where all the AOLME participants are, but it can be the space surrounding the participants in a single table. The approximate geometrical and physical characteristics of the virtual room allow us to emulate the reception on arrays of virtual microphones. 2.1.1 Sound Propagation: Near and Far Fields Consider an acoustic source such as a person speaking, a stereo system playing a song, or a running ventilation fan. Sound from these sources propagates in the form of circular air pressure waves, away from the source. They can propagate in all directions if the source is in an open field (Fig. 1a), or directionally if the source is in proximity to a non-conducting medium such as a wall (Fig.1b). In acoustic theory, the relative location of a source to a point in space determines its field location. A source is in the near field if its distance to a point is less than one wavelength of the acoustic signal it is emitting. Sources that are located at distances greater than one wavelength are located at the far-field. The field location of a source plays an important factor when modeling the perception of sounds wave at a point in space. 16 Figure 1: Propagation of Sound Waves. (a) Free Field. (b) Directional. Fig. 2 shows a representation of the near field, the transition zone, and the far-field. In the near field, the sound waves behave turbulently, with more circulation than propagation. At about a distance of one wavelength from the source, the sound waves begin transitioning into propagation. At more than one wavelength, sound waves mostly propagate into the infinite. A point located at the near field perceives the sound waves as circular while one located at the far-field will consider these waves planar [36]. 17 Figure 2: Near and Far-Field Areas. 2.1.2 Sound Propagation: Direct Path, Reflections, and Reverberation The perception of sounds varies depending on whether the listener is located inside a theater room, small dormitory, or an open field. These differences in perception are the result of the behavior of the sound waves when they propagate across a medium. To visualize this phenomenon, consider for example a room where there is one acoustic source S (a person speaking) and one microphone M, as represented in Fig. 3. 18 Figure 3: Direct and Reflected Paths for Sound Propagation in a Diffuse Field. In Fig. 1(a) and Fig. 1(b), sound from the source will reach an observer or receiver directly, from one direction without reflections. In this case, the source is said to be in an acoustic free field. Fig. 3, in contrast, represents a diffuse field. In this case, the sound reaches the microphone from more than one direction due to reflections. As in the free field, the direct signal received at the microphone is characterized by the distance from the source to the microphone. This distance determines the sound pressure at the microphone, and the time it takes from the sound wave to reach the microphone. This time is known as the Time of Arrival (TOA), and it is a function of the speed of the sound in the room and the Euclidian distance from the source to the microphone. Each reflection contributes similarly. The signal received at a microphone can be expressed in mathematical terms. If we consider a signal s(t) from an acoustic source located in the far-field, this signal is captured 19 by a microphone as a signal x(t) that is the convolution of the Room Impulse Response (RIR) h(t) with additive noise w(t) as given by: 𝑥(𝑡) = 𝑠(𝑡) ∗ ℎ(𝑡) + 𝑤(𝑡) (2.0). The RIR is unique for every two points in the room and depends on the geometry of the room, the absorption of the materials in the room, and the frequency of the sources [37]. The RIR consists of three parts: the direct path, the early reflections, and the late reverberations. The direct path component is determined by the Euclidian distance of the source to the microphone, and it is a function of the Time of Arrival (TOA) or the time it takes for the signal to travel from the source to the microphone. The other two components of the RIR are related to the reflections of the sound waves at the walls and objects in the room. The early reflections usually arrive 5 ms after the direct path. The late reverberations arrive 20 or 30 ms after the early reflections begin. The RIR can then be expressed as the summation of each of the impulse responses corresponding to the direct path and the reflections: (cid:3012) ℎ(𝑡) = (cid:3533) ℎ(cid:3038)(𝑡) + 𝑚(𝑡) (cid:3038)(cid:2880)(cid:2869) (2.1), where K is the number of reflections, k is the index number of the reflection, and m is the measurement noise. The RIR lasts until the reverberation energy decays to 60 dB on what 20 is known as the T60 time. The T60 was calculated empirically by Sabine in 1890 and can be expressed as: 𝑇(cid:2874)(cid:2868) = 55.25 ∙ 𝑉 𝑐 ∙ 𝑆 ∙ 𝑎 (2.2), where V is the total volume of the room, c is the speed of sound, S is the total surface of the room, and a is the absorption coefficient of the room (0 to 1). The reverberations are characterized by the frequency of the sources but, in the case of the early reflections, this influence is minimum [27]. Fig. 4 depicts a representation of a RIR with its three components. Figure 4: Representation of the Room Impulse Response and its Components. 21 The path of the reflections from the walls can be represented as direct paths coming from imaginary sources called Images. The signal at any microphone would be then represented by the number of contributing sources plus their image reflections. All acoustic reflections are subject to a TOA that depends on the distance of the path of the reflection. Section 2.3 presents more detail on the concept of acoustic images and their role in room simulation. 2.2 Microphones and Microphone Arrays The previous section introduced microphones as devices capable of capturing sound. In general terms, microphones are sensing devices that detect changes in air pressure and convert these changes into electrical signals. Microphones are categorized by their electrical conversion type and their directionality pattern. Deep technical details for each type of conversion and directionality pattern microphone are out of the scope of this dissertation. The dissertation will only consider the type of microphones used during the research. 2.2.1 Classification of Microphones This research used two types of physical microphones: Condenser omnidirectional, and condenser cardioid. The condenser term refers to the type of electrical conversion of the sound, and the terms omnidirectional and cardioid refer to the directionality of the microphone. 22 Condenser microphones work by utilizing a variable condenser that detects the air pressure changes. The change in pressure translates into a movement of the plates of the condenser thus changing its capacitance. The changes in capacitance are measured by the changes in the charging current in a circuit. Condenser microphones are also known by the name of electret. They are the most popular type of microphones today. The directivity pattern of a microphone determines its gain or sensitivity according to the direction of the incoming sound. Omnidirectional microphones are equally sensitive to incoming sound from any direction. These microphones are simple pressure sensing devices or acoustic monopoles. Cardioid microphones are also known as pressure gradient microphones, and they are characterized for a directionality pattern that is like a heart (hence their name cardioid, from the Greek “heart”). Fig. 5 shows the typical directivity pattern for omnidirectional (a) and cardioid (b) microphones [38]. All AOLME videos were recorded using Audio-Technica ATR3350 condenser omnidirectional microphones. 23 Figure 5: Polar Pattern Plot of Directivity of Two Types of Microphones: (a) Omnidirectional. (b) Cardioid. Regardless of their type., all microphones generate noise. The conversion of sound pressure waves into electrical signals carries electrical noise, which has a flat spectrum [39]. Manufacturers usually indicate the electrical noise of their microphones in a Signal to noise ratio (SNR) number at a certain sound level. Appendix C contains the technical specifications of the microphones used for this research. 2.2.2 Microphone Arrays When two or more microphones are arranged into a geometric pattern, they become a microphone array. Microphone arrays have important functional properties that are of 24 interest when capturing sound in noisy environments, or when directionality is needed to discriminate between sound sources. An important part of the results of our research is based on the functional characteristics of microphone arrays. Microphone arrays allow for the incorporation of spatial dimensionality to sound capturing. The difference between the signals captured by any two microphones separated a distance d provides information that can be used for source localization, tracking, and general noise reduction. Microphone arrays can be expressed mathematically by (2.3) 𝒙 = 𝑠𝒅 + 𝒗 (2.3), where x represents the vector of all microphone signals, s is the source audio signal, d is the propagation vector represented in (2.4), and v is the additive noise [40]. The vector d is expressed by (2.4) 𝒅(𝑓) = [𝑎(cid:2869)𝑒(cid:2879)(cid:2870)(cid:3095)(cid:3033)(cid:3099)(cid:3117) … . 𝑎(cid:3041)𝑒(cid:2879)(cid:2870)(cid:3095)(cid:3033)(cid:3099)(cid:3289) … . 𝑎(cid:3015)𝑒(cid:2879)(cid:2870)(cid:3095)(cid:3033)(cid:3099)(cid:3263)](cid:3021) (2.4), where an represents the attenuation factor 1 (cid:3415) (𝑛) , 𝜏𝑛 is the channel delay 𝑑(cid:3046) 𝑐(cid:3415) (𝑛) and 𝑑(cid:3046) 𝑑(cid:3046)(𝑛) is the distance between the source and a microphone n, with c the speed of sound. The fine details of the theory behind microphone arrays are out of the scope of this dissertation. Nevertheless, it is important to have a basic knowledge of the properties of 25 microphone arrays due to their applications in source localization, spatial filtering, and source separation. All of these are applications related to this research and will be discussed later in this section. 2.2.2.1 Microphone Arrays Configurations The possible geometries of microphone arrays are infinite. These different geometries are guided by the number of microphones that can practically be allocated to an array, and the type of acoustic scenario the array is intended to operate. The most common types are linear, circular, and volumetric (3D) [41]. a) Linear Microphone Arrays: In this type of array, the microphones are linearly arranged. Fig. 6(a) represents a five-microphone array with a separation of 0.05 m between microphones. This array configuration is very popular, and it is designed to capture the sound that is in front of it. This type of array cannot distinguish from sounds that are coming from the same angle to the axis of the array, as the sound waves will arrive at the microphones with the same time delay. Fig. 6(b) shows the directivity pattern of the microphone array of Fig. 6(a), calculated at 450 Hz with the speed of sound c = 343 m/s. 26 Figure 6: Linear Microphone Array. (a) Geometry. (b) 3D Directionality Pattern. A variant of this type of array is a cross-linear array, also known as a planar microphone array. This type of array consists of two linear arrays perpendicular to each other, as shown in Fig.7. This is the type of array used in this dissertation for virtual microphone simulations. 27 Figure 7: Cross-Linear Array and Azimuth Directivity Pattern. b) Circular Microphone Array: This microphone array has its elements positioned circularly. They can consist of one circle, or several concentric circles, as shown in Fig. 8. This type of array is commonly found in conference equipment that is in the center of a meeting table. 28 Figure 8: Circular Microphone Array and Directivity Pattern. c) Volumetric (3D) Array: This type of array forms a lattice with its elements, as shown in Fig. 9. They can capture sound from any direction, for as long as they are “suspended in the air” with no other interference. Their shape can vary as cubes, spheres, or cylinders. 29 Figure 9: Volumetric Microphone Array and 3D Directivity Pattern. 2.2.2.2 Spatial Aliasing Signal aliasing occurs when the sampling frequency is less than twice the largest signal frequency component. When the bandwidth of the signal is greater than half of the sampling frequency, spectral overlapping happens. Spatial aliasing occurs similarly. To reconstruct a spatial signal from a set of samples, it is necessary to have a spatial sampling period that is less than half of the signal wavelength [42]. In microphone arrays, the phase difference between two microphones should be less than π to avoid spatial aliasing [43]. This constraint means that given a signal of frequency f, there is maximum distance 𝑑 between microphones before spatial aliasing occurs, and vice versa. For an audio signal of wavelength λ, this distance is half of the wavelength: 30 𝑑 ≤ 𝜆(cid:3040)(cid:3036)(cid:3041) 2 (2.5), which translates to a maximum frequency of 𝑓(cid:3040)(cid:3028)(cid:3051) ≤ 𝑐 2𝑑 (2.6), where 𝑐 is the speed of sound. 2.2.2.3 TDOA and Cross-Correlation A very important property of microphone arrays is the Time Difference of Arrival (TDOA) between microphones. The TDOA is defined as the difference in time a signal takes to reach two points separated by a certain distance 𝑑. To understand this concept, assume there are two microphones 𝑀𝑖 and 𝑀𝑗 separated by a distance d, and sound source S located at distances 𝐷𝑖 and 𝐷𝑗 from microphones 𝑀𝑖 and 𝑀𝑗 , respectively, as shown in Fig. 10: 31 Figure 10: Location of Source and Microphones for Time Difference of Arrival. The difference in the distance ∆𝑫 between 𝐷𝑖 and 𝐷𝑗 is defined as: ∆𝑫 = 𝑐 ∗ (∆𝑡) (2.7), where 𝑐 is the speed of sound and ∆𝑡 is the TDOA between 𝑀𝑖 and 𝑀𝑗. Conversely, if d and ∆𝑡 are known, it is possible to determine 𝐷𝑖 or 𝐷𝑗 if one of them is known. From (2.7), it is also possible to infer the proximity of the source to either microphone by the sign of ∆𝑫. Because ∆𝑡 = 𝑡(cid:3036) − 𝑡(cid:3037), a positive ∆𝑡 indicates that 𝑀𝑖 is closer to the sound source than 𝑀𝐽, whereas a negative ∆𝑡 indicates the opposite. The signal delay between microphones 𝑀𝑖 and 𝑀𝑗 can be also expressed in terms of their cross-correlation (CC). Let 𝑟(cid:3036),(cid:3037)(𝑡) = 𝑥(cid:3036)(𝑡) ⊛ 𝑥(cid:3037)(𝑡) denote the cross-correlation 32 between microphone signals 𝑥(cid:3036)(𝑡), 𝑥(cid:3037)(𝑡) corresponding to the microphones 𝑀𝑖 and 𝑀𝑗. The CC 𝑟(cid:3036),(cid:3037)(𝑡) between these two signals is defined as: 𝑟(cid:3036),(cid:3037)(𝑡) ≜ E(cid:3427)𝑥(cid:3036)(𝑡)𝑥(cid:3115)(cid:3365) (𝑡)(cid:3431) (2.8). The normalized cross-correlation is defined by: 𝑅(cid:3036),(cid:3037)(𝑡) = (cid:2869) (cid:3028)∙(cid:3029) 𝑟(cid:3036),(cid:3037)(𝑡) (2.9), where the 𝑎, 𝑏 are defined using 𝑎 = (cid:3495)∑ 𝑥(cid:3036) (cid:3047) (cid:2870)(𝑡) and 𝑏 = (cid:3495)∑ 𝑥(cid:3037) (cid:3047) (cid:2870)(𝑡) . 2.2.2.4 Beamforming and Spatial Filters The process of filtering each of the outputs of the microphones of an array into a single output is known as beamforming. Beamforming steers the array’s directivity pattern into a particular direction using beamforming filters [40]. The combination of the signals from each microphone is governed by: 𝑦 = 𝒘𝑯𝒙 (2.10), 33 where w represents the beamforming filters and 𝒘𝑯 is the conjugate transpose. The beamforming filters can be estimated as a function of a propagation vector d and a noise correlation matrix Q using: 𝒘 = 𝑸(cid:2879)(cid:2869)𝑑 𝒅(cid:3009)𝑸(cid:2879)(cid:2869)𝒅 (2.11). The filter described in equation (2.11) is known as the Minimum Variance Distortionless Response (MVDR), and it is one of the most popular types of beamforming filter. Refer to [31] for a full explanation of beamforming filters. If the location of the sound sources d is known, it is possible to construct a spatial filter for each of the sources. This approach is used to minimize crosstalk between channels and for noise reduction. The details of Spatial Filtering are outside the scope of this dissertation. Nevertheless, a brief introduction is presented because future work proposed in this dissertation includes a possible combination of the proposed method with spatial filtering and beamforming for speaker separation. 2.3 Modeling of Room Acoustics The proposed research requires the modeling of room acoustics. The simulation of microphones and sources are all based on physical models that predict the effects of the acoustic reflections given the geometry of the room and the location of the speakers. To this end, simulations calculate RIRs to the target points. The methods to model room 34 acoustics are dived into two categories: geometrical acoustics-based and wave acoustic- based [44]. Geometrical acoustics-based methods work by capitalizing the reflection properties of sound, i.e., sounds reflect into smooth surfaces in the same way light does, following Snell’s law. These methods are relatively easy to implement but do not take into consideration the roughness of the reflective surface. On the other hand, wave acoustic- based methods take into consideration the characteristics of the sound wave, providing a more accurate simulation. In contrast with geometry methods, wave methods are more computationally intensive and are limited to low-frequency ranges [45]. The simulation package used for this research is geometry acoustic-based; wave acoustic-based methods are not considered in this dissertation. The two more common geometry acoustic-based methods of modeling are the Ray Tracing Method and the Image Source Method. This dissertation focus on the Image Method as this method is the one used by the simulation package. 2.3.1 Ray Tracing Method The Ray Tracing Method assumes that sound radiates from the source as several rays [44] whose energy is the total energy of the source divided by the number of rays. These rays propagate at the speed of sound and, when they reach a boundary surface, some of the energy is reflected in an angle 𝛼′ equal to the incidence angle 𝛼, as it is shown in Fig. 11 (a). The perceived sound at any point is represented by an echogram that contains the history of all the ray reflections [45] plus the direct ray. The Ray Tracing Method was 35 introduced in the late 1960s and was widely used until the 1980s. Ray Tracing is a relatively straightforward method, but its resolution is limited [45]. 2.3.2 Image Source Method The Image Source Method (ISM, also known as Mirror Image Source Method MISM), is perhaps the most popular modeling method in use [46]. Image methods are used to solve physics problems, and in the late 1970s, Allen and Berkley [46], [47] introduced an algorithm to RIR related applications. In the Image Source Method, a virtual image or specular reflection of the source is created perpendicularly to the source, as shown in Fig. 11 (b). The sound received by the sensor 𝑀 is the summation of the sound from the source 𝑆 and the image source 𝑆′. Figure 11: Simulation Methods. (a) Ray Tracing. (b) Image Source Method. 36 The ISM needs the amplitude and delay of the image sources to calculate the RIR. Because there are infinite possible reflection paths, the ISM creates a map of mirrored rooms with the position of the number of desired images, as shown in Fig. 12. Figure 12: Source Image Map. The coordinates of each of the images are calculated using the map with the corresponding room size and the position of the source. Once the position of the images is calculated, the Euclidian distance 𝑑(cid:3041) from the image 𝑛 to the source is used to calculate the delay 𝜏(cid:3041) = (cid:3031)(cid:3289) (cid:3030) , where 𝑐 is the speed of sound inside the room. Finally, the amplitude 𝐴(cid:3041) of each of the signals from the images is calculated by the reflection coefficient 𝛽(cid:3041) of each of the walls crossed by the path from the image to the sensor, using (2.12) [46]: 37 𝐴(cid:3041) = 𝛽(cid:3041) 4𝜋. 𝑑(cid:3041) . (2.12). The RIR ℎ(𝑡) is calculated using the amplitude and the delay for the images: ℎ(𝑡) = (cid:3533) 𝐴(cid:3041) ⋅ 𝛿(𝑡 − 𝜏(cid:3041)) (2.13), (cid:3041)∈ℕ where ℕ represents the image sources and 𝛿 is the impulse function. 2.4 Characteristics of the Human Speech The performance of the method described in this dissertation will improve if the acoustic models are tailored to human speech. Human speech has some characteristics that can be exploited and used to compensate for some of the deficiencies encountered with the approximation of the geometry of the room and the limitations of the modeling software. Two characteristics of human speech: fundamental frequency and directionality are of particular importance. Speech is a non-stationary signal, or rather said, a non- stationary process, meaning that its frequency content is not unique in any given interval of time. The fundamental frequency of the human voice varies from 85 Hz to 180 Hz, with women going up to 255 Hz, and children to 300 Hz and even higher [48]. The whole spectrum of the human voice contains frequencies that go up to 8kHz. Much of the energy 38 is found in frequencies that are below 500 Hz for males and 800Hz for females [49]. As a curiosity note, the frequency sensitivity of the human ear is very close to the frequency spectrum of the human voice. This research work focus on the fundamental frequency to develop the acoustic models. More detail is presented in the Experimental Implementation section of this dissertation. The other important characteristic of human speech is its directionality. Speech does not propagate equally in all directions, but rather has directionality due to the location of the mouth and the shadow cast by the head and the torso [50]. Fig. 13(a) depicts the propagation of sound in the horizontal direction, while Fig. 13(b) presents the propagation in the vertical axis. Lower frequencies propagate farther from the back of the head than higher frequencies. Most propagation occurs at the front of the head. This directionality property is utilized for positioning the speakers in the simulation models. Figure 13: Directionality of Human Head [50]. 39 2.5 Speaker Diarization and Identification Section 1.2 presented a background on several methods for speaker diarization and identification that relate to this research. To understand the differences between these methods and the method proposed in this research, this next section reviews the fundamentals on which some of these methods are based on. Reviewing in detail all the methods presented in section 1.2 requires an effort that goes beyond the scope of this dissertation. For this reason, this dissertation only focuses on the most recent and common methods of speaker diarization and identification. 2.5.1 Methods for Diarization and Identification Speaker diarization can be summarized as “who said what, and when”, and for “how long”. The task of determining for how long one speaker has been active in a multi- participant conversation requires speaker diarization and subsequent identification with non-abstract labels. Speaker identification should not be confused with speaker verification. A system that accepts or rejects the identity claim by a speaker is called a speaker verification system. This dissertation divided these methods into two categories: Classic methods and Deep Learning methods. 2.5.1.1 Classical Methods for Diarization and Identification Anyone conducting a web search for “speaker diarization and identification methods” will find thousands and even millions of documents that somehow relate to the 40 subject (by the time of this dissertation, “speaker diarization methods” gave 66,400 hits, “speaker identification methods” about 48,000,00 hits, and “speaker diarization and identification methods” some 121,000). Nevertheless, until researchers started using Deep Learning and Neural Network methods, most speaker diarization and identification methods consisted of four basic modules or steps: A feature extraction module, a speech or voice activity detector (SAD or VAD, respectively), a segmenter or speaker change module, and finally, a clustering mechanism [51], [52]. Fig. 14 shows a block diagram of the four modules. Figure 14: Block Diagram of a Typical Speaker Diarization System. The feature extraction module generally uses Mel-Frequency Cepstral Coefficients (MFCC) as features. Not as popular as MFCCs, Linear Frequency Cepstral Coefficients (LFCC), and Perceptual Linear Predictive are also used as features [52]. The purpose of the speech activity module (SAD), also known as the voice activity module (VAD), is to detect the presence of speech. SADs or VADs (hereon referred to as 41 VAD) eliminate audio segments that contain no necessary information, such as noise or music, thus improving the performance of the segmenting and clustering modules. There are several different algorithms for these detectors, that vary from just energy level detection to binary classifiers based on pre-trained speech models. This research uses a custom-made VAD to segment the audio into frames, as it will be shown in later chapters. The next module to follow is the segmenter or speaker change detector. The segmenter detects when there is a speaker change in the audio and creates frames that ideally only contain one speaker. This is a necessary step before clustering, where the grouping of the clusters is done without previous information. A common method of segmentation is to measure the distance between two segments. Segments that belong to the same speaker are usually closer in distance than those coming from a different speaker. The models are usually Ergodic Hidden Markov Models (HMMs), where each state represents a speaker, and the probabilities are modeled by Gaussian Mixture Models (GMMs). Bayesian Information Criterion (BIC) is used to determine the nearest clusters, merging the clusters that generate the highest BIC, stopping the process when the values of the BIC are no longer positive. This algorithm was introduced by Chen et al. [53] and it is defined as (2.14) for a parametric Gaussian Mixture Model (GMM) with clusters 𝐶 with features [51] 𝐵𝐼𝐶(𝑀) = log ℒ (𝑋|𝑀) − 𝜆 2 #(𝑀) log(𝑁) (2.14), 42 where 𝑁 is the number of samples, #(𝑀) is the number of parameters of the model, and 𝜆 is a tunable parameter. The final clustering step groups together segments that belong to the same speaker. In most speaker diarization and identification approaches, clustering is achieved by agglomerative hierarchical clustering (AHC). Using the same distance concept, each segment is its cluster at the beginning of the process, and parts of clusters are merged until the stopping criterion is met. This criterion is ideally to get the number of clusters equal to the number of speakers. In practical terms, the stopping criterion is a threshold that is preset at the beginning of the process. 2.5.1.2 Deep Neural Networks The applications of Deep Neural Networks (DNNs) to speaker diarization have gained a lot of momentum in recent years. It is difficult to keep pace with the amount of research that is done on an almost monthly basis in this field. It is therefore of importance to have a basic understanding of how DNNs are applied to the problem of speaker diarization. In general terms, DNN speaker diarization/identification methods are divided into four groups [54]: Stage-wise, end to end, online, and multimodal. From these groups, this dissertation will address stage-wise and multimodal groups as they relate more to the research work. 43 2.5.1.2.1 Stage-wise diarization Stage-wise diarization methods are based on the same blocks or stages as the GMM methods covered in the previous section, but they rely on DNNs that employ a universal background model (UBM), rather than GMMs for feature extraction and clustering. For GMMs to be computationally efficient for feature extraction, each sequence of feature vectors is converted into a fixed-length vector, or supervector [55]. Because this approach makes GMMs susceptible to speaker and channel variations of utterances [56], it is desirable to reduce the dimensionality of the supervectors. These lower-dimensional vectors are called i-vectors (the i-vectors were previously referenced in the background section). The representation of i-vectors assumes that speaker and channel-dependent variabilities reside in a lower-dimensional space [57], which is represented by a total variability matrix T. For GMMs this conversion can be expressed as: 𝑠 = 𝑠(cid:4593) + 𝑇𝑤 (2.15), where 𝑠′ is the speaker and channel supervector and 𝑤 is the i-vector. Fig. 15 shows a GMM/i-vector framework [56]. 44 Figure 15: GMM/i-vector Framework. At this point, the effort moved to replace the GMM generated i-vectors for DNN generated i-vectors. The idea behind this approach is to replace the GMM generated posteriors for the feature vectors and take a DNN trained acoustic model using senones to generate these posteriors. Fig. 16 represents this approach [54]. Figure 16: DNN/i-vector Implementation. Although the performance of DNN based acoustic models has been proven [56], they require a large set of training data and more computational cost as well. 45 In addition to i-vectors, other DNN approaches include the use of d-vectors and x- vectors embedding. D-vectors were introduced by Variani et al. (2014), and they are based on assigning the ground truth training utterance to labels of the training frames to the corresponding utterance in the training stage, converting the problem into a classification one. For a more detailed description of d-vectors, refer to [54],[56]. X-vectors are derived from d-vectors. Instead of using frame-by-frame speaker labels, they use utterance-level speaker labels by aggregation. As was referred to in the background section, x-vectors outperform most i-vector and d-vector approaches. Refer to [54], [56], for details on x-vectors. Deep Learning clustering techniques are also applied in replacement of conventional distance and similarity methods. Clustering is treated as either a supervised or unsupervised problem, by employing recurrent neural networks (RNN) or discriminative sequence-to-sequence neural networks. References to these methods can be found in [54]. 2.5.1.2.2 Multimodal Speaker Diarization Related to our approach of exploiting video clues and spatial information, Deep Learning is applied to the analysis of not only visual clues, such as movement of lips [58], but also to the content of the speech of the participants [59], [60]. In this sense, multimodal methods train the networks based on the patterns that most likely belong to a genre of participants. For example, in a collaborative environment with students, the facilitator is more likely to have a calm voice, in contrast with the students. In recent publications, W. 46 Kang et al. [61], have presented speaker diarization based on d-vectors combined with spatial information provided by microphone arrays. 2.5.2 Current State-of-the-Art Methods for Diarization and Identification State-of-the-art methods cover the speaker diarization and transcription that major technology players are offering. They keep their technologies a secret, as they compete to have the most reliable service available, thus the difficulty in obtaining detailed information on how their methods work. It is expected that they somehow use the speaker diarization approaches reviewed in the previous sections. IBM, Google, Amazon, and Microsoft offer cloud computing that includes speech processing services based on algorithms that use Deep Learning and Machine Learning. Amazon’s, Google’s, and Microsoft’s are all closed-source cloud services that provide an API for speech-to-text processing and speaker diarization. This dissertation reviewed Amazon’s Transcribe (AWS) [62], Google’s Cloud [63], and Microsoft Azure Speech Services [64], and experimentally compared Amazon’s and Google’s against our proposed system. Amazon’s Transcribe accepts either audio files or streaming data, single-channel, and outputs text files with speaker diarization if this option is selected, and the number of speakers is specified. Amazon’s Transcribe works better with 2-5 speakers, and it is language-dependent, limited to 120 minutes of audio. Amazon’s Transcribe stores the voice data to train the models [65] unless the users select the option to delete this data. 47 Amazon offers a highly trained set of models called Amazon Transcribe Medical which is aimed at medical transcriptions. Users can also customize the vocabulary to better fit their needs. Amazon functionality can be accessed via REST and SOAP protocol over HTTP [66]. Google’s Cloud works similarly, with an interface for long speech, single-channel input for transcription purposes [65]. The optimum number of speakers is set at a maximum of 5. As with Amazon Transcribe, Google offers the option of privacy that prevents data logging that could be used to improve the models. Google’s models are optimized for phone conversations or videos, accepting 16 kHz or 8 kHz audio, respectively, depending on the application [67]. It also offers vocabulary customization. Google offers good scalability, infrastructure, and payment schemes that are considered the best among the technology giants [66]. Microsoft offers speaker diarization utilizing its Cognitive Services. Microsoft’s Diarization system ranked first at the VoxSRC challenge 2020 by achieving a Diarization Error (DER) of 3.71% in development and 6.23% in evaluation testing [68]. The datasets consisted of audio collected from YouTube recordings. For the challenge, the network was trained with 1500 hours of simulated mixed training audio. Microsoft Speaker Recognition [69] offers text-independent speaker recognition/verification. The speakers need to be enrolled to create a signature, which is later compared with the audio to be analyzed. The minimum requirements are 20 seconds of speech for training, and 4 seconds of speech for identification, with unlimited speaker enrollment, with only one speaker present. In the 48 case of diarization, Microsoft can only recognize up to two speakers. Microsoft Transcription requires multi-channel audio for diarization and the signature of the participating speakers for identification, labeling each speech segment with its correspondent speaker. Microsoft does not collect users’ voice tracks to train its models. Users can customize their vocabulary and the environment they are expecting to operate, meaning that customization must include noise, indoor or outdoor environments, multi- gender speech, etc. [64]. 49 Chapter 3. Proposed Method The previous chapter discussed the principles of acoustics, speech, and speaker diarization that served as the foundations for this research. This chapter will cover the mathematical models that are used to estimate the virtual microphones, and how they are implemented into a working system. Finally, it will present a block diagram detailing the function of each of the elements of the proposed system and its operation. 3.1 Methodology The goal of this dissertation is the identification of speakers from single-channel recordings using virtual microphones. This statement includes the objective (identify speakers), the data source (single channel recording), and the means to accomplish this objective (using virtual microphones). This section will begin by identifying the physical and mathematical elements of the models needed for the virtual microphone simulation. Let us consider a collaborative environment such as the one represented in Fig. 17 (a), where we have three speakers sitting around a table with a central recording microphone. Such an environment can be represented as a simple 2D model shown in Fig. 17(b) that shows the relative location of the speakers and the recording microphone. 50 Figure 17: Collaborative Environment (a) with 2D Model (b). To capitalize on the properties of microphone arrays, it is necessary to find a method to simulate several virtual microphones based on the information contained in the signal captured by the central microphone. From the discussion on microphone arrays in chapter 2, it is possible to implement several different virtual array configurations. Let us consider a cross-linear array with four microphones and one central recording microphone, as shown in Fig. 18. 51 Figure 18: 2D Model of Fig. 12(b) with Microphone Array. If Fig. 18 is an ideal representation, where there are no reflections or room absorption, then for each unique active speaker there will be a set of pairs of microphones with unique TDOAs that correspond to the active speaker. For example, if speaker 3 is active, then the TDOA between M5 and M3 and the TDOA between M2 and M3 will be unique for speaker 3. Having this concept in mind, we recall from Chapter 2 that the cross- correlation from any pair of microphones represents the signal delay between them. To uniquely identify each of the speakers, we are interested in the location of the peak of the cross-correlation function defined by: 𝑇(cid:3036),(cid:3037) = argmax 𝑅(cid:3036),(cid:3037)(𝑡) (3.0), 52 where 𝑅(cid:3036),(cid:3037)(𝑡) denotes the cross-correlation between two microphone signals 𝑥(cid:3036)(𝑡), 𝑥(cid:3037)(𝑡). If a source signal propagates to microphones 𝑖, 𝑗, 𝑇(cid:3036),(cid:3037) represents the time lag that it takes for the signal to reach 𝑗 after reaching 𝑖. Thus, 𝑇(cid:3036),(cid:3037) > 0 implies that the signal arrived at microphone 𝑖 before 𝑗. On the other hand, 𝑇(cid:3036),(cid:3037) < 0 implies that the signal arrived at microphone 𝑗 before 𝑖. The cross-correlation matrix of all possible values 𝑇(cid:3036),(cid:3037) will be used for determining the locations of the speakers. Now we move to the problem of simulating the virtual microphones. From equation (2.0) from Chapter 2, it is possible to extend this model for the case of multiple sources and microphones. Suppose that we have 𝐽 possible sources: 𝑠(cid:2869)(𝑡), … , 𝑠(cid:3011)(𝑡) and 𝑁 possible microphone signals: 𝑥(cid:2869)(𝑡), . . . , 𝑥(cid:3015)(𝑡). Next, let ℎ(cid:3037),(cid:3038)(𝑡) denote the RIR that describes the propagation from the 𝑗-th source to the 𝑘-th microphone. At the 𝑘-th microphone, we receive signals from all sources as expressed by: (cid:3011) 𝑥(cid:3038)(𝑡) = (cid:3533) 𝑠(cid:3037)(𝑡) ∗ ℎ(cid:3037),(cid:3038)(𝑡) + 𝑛(𝑡) (3.1), (cid:3037)(cid:2880)(cid:2869) where 𝑛(𝑡) represents additive white noise. For the model in (3.1), we need to estimate ℎ(cid:3037),(cid:3038)(𝑡). If ℎ(cid:3037),(cid:3038)(𝑡) is known, it is possible, at least in theory, to estimate the signal source by deconvolving the signal 𝑥(cid:3038)(𝑡) with ℎ(cid:3037),(cid:3038)(𝑡) (i.e., ℎ(cid:3037),(cid:3038) (cid:2879)(cid:2869)(𝑡)). Once the sources have been estimated, each virtual microphone can be emulated by just convolving the emulated source with each of the RIRs of the virtual microphones. 53 Some important factors need to be considered to develop a model for this approach. First, ℎ(cid:3037),(cid:3038) (cid:2879)(cid:2869)(𝑡) may not exist [70], and it may be necessary to construct virtual microphone approximations to ℎ(cid:3037),(cid:3038)(𝑡). Second, the speaker feature correlation matrix defined by 𝑇(cid:3040) is estimated under the assumption that speaker 𝑚 is talking while all other speakers remain quiet: 𝑠(cid:3038)(𝑡) = 0, 𝑘 ≠ 𝑚. Third, for each audio segment, we need to compute 𝑇, the cross- correlation matrix of the actual signal. Finally, we need to estimate the active speaker by solving match(𝑇, 𝑇(cid:3040)) (3.2), max (cid:3040) where match(. , . ) is a function of the similarity between 𝑇, 𝑇(cid:3040). We now can turn our attention to estimating the RIRs. As it was presented in Chapter 2, the RIR is a function of the geometry of the room, the relative location of the sources and microphones, and the physics of the room (i.e., the absorption of the room, which characterizes the reverberation). This information will be very difficult if not impossible to obtain from just the audio from the recording microphone, but we could use information from the video recording to estimate some of the parameters needed to calculate the RIR. From the video recording, it would be possible to approximate the location of the speakers and the virtual microphones to each other. This information, along with an empirical approximation of the absorption of the room, is all that is necessary to calculate the RIR. 54 Calculating the RIR can be a very tedious task. The number of calculations required is a factor in the degree of accuracy desired in the model. If we recall the concept of images from Chapter 2, the reception at a microphone is the result of the sum of the images; therefore, the fidelity will depend on the number of images added as part of the RIR function. The next chapter will present an open-source software package that performs these calculations thus saving some programming time. So far, this dissertation has presented the fundamentals of the simulation on which the proposed method is based. By using the approximate geometry of the room to calculate the RIR and to simulate the microphones, we should be able to calculate the cross- correlation between microphones and determine the active speaker. The proposed method then can be summarized in 5 steps: 1) Evaluating the room geometry and location of the speakers of the acoustic scene, 2) Estimating a generic RIR model for this geometry, 3) Training the model with known speaker samples, 4) Estimation of the sources that will fit the model for each of the possible active speakers given an unknown audio sample, and 5) Conducting a Cross-Correlation Analysis and classification. The following section explains each of the steps in more detail. 1) Evaluation of room geometry and location of speakers and microphones As it was described before, the RIR is a unique transfer function between two points in space. To calculate the RIR between a source and a microphone, it is necessary to know their spatial locations inside a physical room of known acoustic characteristics. In Fig. 17, 55 it is possible to appreciate the relative location of the three speakers and the recording microphone. This video frame can be used as a reference for the location of the sources and virtual microphones in the model, e.g., from this image we can approximate that the table is about 1.5 meters long by 1 meter wide, that the speakers are separated about 0.7 meters from each other, and the speaker’s mouths are about 0.24 to 0.25 m from the table. It is also possible to locate the reference microphone in coordinates that are relative to each of the speakers. These are just approximations to create a generic model from where to calculate the RIRs. Fig. 19 shows a possible 2D model for these approximations. Figure 19: Possible 2D Model for Fig. 17. The location and separation of the virtual microphones can be arbitrary for as long as they do not violate the rules of spatial anti-aliasing. As presented in Chapter 2, the 56 fundamental frequency of human speech varies from 85 Hz to 180 Hz approximately, with some extreme cases going up to 255-300 Hz (children). If it is assumed a max frequency average of 180 Hz using (2.6) and (2.7), the maximum separation 𝑑 for each microphone would be ≤ (cid:2871)(cid:2872)(cid:2871) (cid:3288) (cid:3294) (cid:2870)((cid:2869)(cid:2876)(cid:2868) (cid:3009)(cid:3053)) = 0.95 𝑚. 2) Estimation of the generic RIR model The approximation of the geometry of the room provides the basis to implement a generic model to calculate a set of RIRs to estimate the virtual microphone array. This model, as it was mentioned before, is based on an approximate geometry of the room, the location of the speakers, and the number of reflections. It is desirable to reduce the influence of reflections and reverberation in the simulation as they add complexity to the RIR. One way this can be achieved is by an overall reduction of the length of the T60. Recalling equation (2.2), we can minimize the volume of the room and maximize its absorption as a means of reducing the length of T60. These two parameters are easy to control and implement in the simulation. The number of images to calculate can be set to an acceptable value that compromises the simulation fidelity and the computational burden. Some trial and error may be necessary to optimize the number of reflections. Another point to consider is the directionality of the human voice. The human voice propagates mostly in one direction to the front of the head; therefore, our model must take this propagation inequality when simulating the audio reception at any point of the room. 57 One solution implemented in this research consisted of locating the speakers close to the end of the virtual room, so the reflections from the back of the speaker are minimized. With the approximate physical and acoustical characteristics of the room, it is then possible to calculate the RIRs between the virtual microphones of the array and the speakers. It was indicated in the previous section that it could be possible to implement any arbitrary array of microphones for as long as we follow the rules of spatial anti-aliasing. The calculated value of the distance d is well fitted between the boundaries of the proposed model, but it would be beneficial for the performance of the model to optimize the microphone array for maximum cross-correlation information. This can be accomplished by asymmetric microphone arrays, i.e., arranging the microphones at locations that are offset from equidistant points to the speakers. Also, the microphone arrays should have as many microphones as possible, for as long as the required computational resources remain manageable. 3) Estimation of sources and virtual microphones The next step is to apply our generic model to estimate the signal at the virtual microphone array based on the recorded signal at the reference microphone. To do so, it is necessary first to estimate the sources that would fit the model, i.e., estimate a set of sources that, when convolved with the model’s RIRs, will represent the signal at each microphone of the array, including the reference microphone. One way to estimate the sources is to deconvolve the reference signal with the RIR that corresponds to the source we want to 58 estimate. For example, assume that our model has three sources 𝑠(cid:2869)(𝑡), 𝑠(cid:2870)(𝑡) and 𝑠(cid:2871)(𝑡), three microphones M1, M2, and M3, with M3 as the reference microphone. If 𝑥(cid:2871),(cid:3037)(𝑡) is the signal received at M3 with 𝑗 = 1,2,3 for the respective sources 𝑠(cid:2869), 𝑠(cid:2870), and 𝑠(cid:2871), then to estimate 𝑠(cid:2869)(𝑡), 𝑠(cid:2870)(𝑡), and 𝑠(cid:2871)(𝑡) given 𝑥(cid:2871),(cid:3041)(𝑡) 𝑠̃(cid:2869)(𝑡) = 𝑥(cid:2871),(cid:2869)(𝑡) ∗ ℎ(cid:2871),(cid:2869) (cid:2879)(cid:2869)(𝑡) (3.3), 𝑠̃(cid:2870)(𝑡) = 𝑥(cid:2871),(cid:2870)(𝑡) ∗ ℎ(cid:2871),(cid:2870) (cid:2879)(cid:2869)(𝑡) (3.4), 𝑠̃(cid:2871)(𝑡) = 𝑥(cid:2871),(cid:2871)(𝑡) ∗ ℎ(cid:2871),(cid:2871) (cid:2879)(cid:2869)(𝑡) (3.5), with ℎ(cid:2871),(cid:2869) (cid:2879)(cid:2869)(𝑡) the inverse of ℎ(cid:2871),(cid:2869)(𝑡) (RIR from M3 to 𝑠(cid:2869)), ℎ(cid:2871),(cid:2870) (cid:2879)(cid:2869)(𝑡) the inverse of ℎ(cid:2871),(cid:2869)(𝑡) (RIR from M3 to 𝑠(cid:2870)), and ℎ(cid:2871),(cid:2871) (cid:2879)(cid:2869)(𝑡) the inverse of ℎ(cid:2871),(cid:2871)(𝑡) (RIR from M3 to 𝑠(cid:2871)). Fig. 20 shows how the sources can be estimated for the example of Fig. 17 and the model of Fig. 19. Once the sources have been estimated, they can be convolved with the remaining RIRs to obtain the simulated reception on each of the microphones of the array. Fig. 21 shows an example of how microphones M1, M2, and M3 are estimated using 𝑠(cid:2870)(𝑡). The process is extensive to the other sources as well. We can use an estimation or the ground truth for microphone M3. 59 Figure 20: Estimation of the Sources. Figure 21: Estimation of Virtual Microphones. 60 It may be noticed at this point that the solution presented above will only work if we know which n source is active at 𝑥(cid:2871),(cid:3041)(𝑡), and which ℎ(cid:2871),(cid:3041)(𝑡) we need to deconvolve with. To solve this problem, our method simulates each possible source by deconvolving the signal at M3 with each RIR of the model and then simulates the signal at each of the virtual microphones. The result is a set of virtual arrays that correspond to each of the possible active sources. In the three source examples, if 𝑥(cid:2871)(𝑡) is defined as the unknown signal at M3, the estimate of both possible sources 𝑠̃(cid:2869)(t), 𝑠̃(cid:2870)(t) and 𝑠̃(cid:2871)(t) is obtained by deconvolving 𝑥(cid:2871)(𝑡) with ℎ(cid:2871),(cid:2869)(𝑡), ℎ(cid:2871),(cid:2870)(𝑡) and ℎ(cid:2871),(cid:2871)(𝑡): 𝑠̃(cid:2869)(𝑡) = 𝑥(cid:2871)(𝑡) ∗ ℎ(cid:2871),(cid:2869) (cid:2879)(cid:2869)(𝑡) (3.6), 𝑠̃(cid:2870)(𝑡) = 𝑥(cid:2871)(𝑡) ∗ ℎ(cid:2871),(cid:2870) (cid:2879)(cid:2869)(𝑡) (3.7), 𝑠̃(cid:2871)(𝑡) = 𝑥(cid:2871)(𝑡) ∗ ℎ(cid:2871),(cid:2871) (cid:2879)(cid:2869)(𝑡) (3.8), and then emulating two sets of virtual microphones. Each set of the microphones is represented as 𝑥(cid:3028),(cid:3029), where a is the index of the virtual set, b is the index of the virtual microphone of the set, and ℎ(cid:3030),(cid:3031) is the RIR from source c to microphone d. For set 1: 𝑥(cid:2869),(cid:2869)(𝑡) = 𝑠̃(cid:2869)(𝑡) ∗ ℎ(cid:2869),(cid:2869)(𝑡), (3.9), 61 For set 2: For set 3: 𝑥(cid:2869),(cid:2870)(𝑡) = 𝑠̃(cid:2869)(𝑡) ∗ ℎ(cid:2869),(cid:2870)(𝑡), (3.10), 𝑥(cid:2869),(cid:2871)(𝑡) = 𝑠̃(cid:2869)(𝑡) ∗ ℎ(cid:2869),(cid:2871)(𝑡). (3.11). 𝑥(cid:2870),(cid:2869)(𝑡) = 𝑠̃(cid:2870)(𝑡) ∗ ℎ(cid:2870),(cid:2869)(𝑡), (3.12), 𝑥(cid:2870),(cid:2870)(𝑡) = 𝑠̃(cid:2870)(𝑡) ∗ ℎ(cid:2870),(cid:2870)(𝑡), (3.13), 𝑥(cid:2870),(cid:2871)(𝑡) = 𝑠̃(cid:2870)(𝑡) ∗ ℎ(cid:2870),(cid:2871)(𝑡). (3.14). 𝑥(cid:2871),(cid:2869)(𝑡) = 𝑠̃(cid:2871)(𝑡) ∗ ℎ(cid:2871),(cid:2869)(𝑡), (3.9), 𝑥(cid:2871),(cid:2870)(𝑡) = 𝑠̃(cid:2871)(𝑡) ∗ ℎ(cid:2871),(cid:2870)(𝑡), (3.10), 𝑥(cid:2871),(cid:2871)(𝑡) = 𝑠̃(cid:2871)(𝑡) ∗ ℎ(cid:2871),(cid:2871)(𝑡). (3.11). 62 4) Cross-Correlation and Model training The three sets of virtual microphones give us enough information for cross- correlation analysis and model training. To train the model, we take a small audio sample that contains only one active source (e.g., see Fig. 22), and we then use this information to generate a cross-correlation table that contains all combinations of possible sources and microphone pairs for that source. For the training, we include the background noise, as is shown in Fig. 22. Training the model for noise is explained later in the implementation section. These tables are templates for the classification of each of the active sources. For our example of three sources and three microphones, we calculate three cross-correlation tables for each known sample processed with filters for s1, s2, and s3, as shown in Table I, Table II, and Table III. 63 Figure 22: Training Samples from Each Speaker and Noise. Table I: Template Cross-Correlation Table for Source 1. r1-2 V1 V4 V7 𝑠̃(cid:2869),(cid:2869) 𝑠̃(cid:2869),(cid:2870) 𝑠̃(cid:2869),(cid:2871) r1-3 V2 V5 V8 r2-3 V3 V6 V9 64 Table II: Template Cross-Correlation Table for Source 2. r1-2 V10 V13 V16 𝑠̃(cid:2870),(cid:2869) 𝑠̃(cid:2870),(cid:2870) 𝑠̃(cid:2870),(cid:2871) r1-3 V11 V14 V17 r2-3 V12 V15 V18 Table III: Template Cross-Correlation Table for Source 2. r1-2 V19 V22 V25 𝑠̃(cid:2871),(cid:2869) 𝑠̃(cid:2871),(cid:2870) 𝑠̃(cid:2871),(cid:2871) r1-3 V20 V23 V26 r2-3 V21 V24 V27 where 𝑉(cid:2869) … 𝑉(cid:3041) are the values of the cross-correlations 𝑟(cid:2869),(cid:2870), 𝑟(cid:2869),(cid:2871), and 𝑟(cid:2870),(cid:2871) corresponding to the microphones M1-M2, M1-M3, and M2-M3 respectively, for each known sample source of 𝑠(cid:2869), 𝑠(cid:2870) and 𝑠(cid:2871). Table I will contain the results for the sample 𝑠(cid:2869), Table II for the sample 𝑠(cid:2870), and Table III for the sample 𝑠(cid:2871). Training needs to be done just once. 5) Analysis and Classification Cross-correlation analysis of multi-speaker audio is not possible unless this is divided into segments. Proper segmentation of the audio is important to the performance of the proposed method. Because the audio from collaborative environments contains multiple speakers, it is possible at any time to have more than one simultaneous active speaker. Also, it is possible to have periods of noise (there are no periods of silence) or 65 overlapping speech when one speaker finishes and another one begins speaking. For optimal cross-correlation location identification, each segment should only contain one active speaker at a time. If the audio contains overlapping speech or mixes of speakers, the location of the peak value of the cross-correlation between microphones will depend on the amount of information from each of the speakers that are contained in the audio segment, making the classification more difficult. One solution to maximize the probabilities of having only one active speaker in a segment is by minimizing its length: the shorter the segment is, the most likely it is to have content from only one speaker. There is, however, a limit on the minimum length of the segments. The minimum length of the segments is subject to the performance of the cross- correlation algorithm. This means the segments need to be long enough to contain sufficient information for the algorithm to calculate a meaningful cross-correlation. In addition, the total analysis time is affected by the number of segments that need to be cross- correlated and analyzed, hence the desire of reducing the number of segments. There is then a need for optimizing the length of the segments for a balance between the maximum information content and the minimum overlapping between speakers. Recalling from Chapter 2, the best way to segment the audio is to incorporate a VAD. Ideally, a VAD will detect speech content and the change of speakers based on a certain pre-determined energy threshold, as shown in Fig. 23. If the energy threshold is properly adjusted, a VAD can be effective in producing segments of audio that contain only one speaker at the time, and segments that contain mixes of speakers or noise, maximizing their information content. 66 Because VADs are not perfect, there will be always segments that could contain overlapping, mixes of speakers, or simply being misclassified. To minimize the number of misclassifications, the length of the segments can be limited to a maximum that provides an acceptable number of misclassifications. It was found during this research that segments that are more than 1.5 s long are prone to misclassifications, while segments of less than 500 ms are difficult to cross-correlate. Figure 23: Audio Segmentation using a VAD. Each audio segment will generate a single cross-correlation table that corresponds to each of the possible locations of the speakers, as it is done for training. The classifier will then use the cross-correlation template tables from training to compare with the analysis results and determine the most probable source match. Alternatively, it is possible 67 to cluster the cross-correlation results for later classification. Chapter 4 covers the analysis and classification methods of this research in more detail. 3.2 Block Diagram of the Proposed System Fig. 24 presents the block diagram of the proposed method. Chapter 4 covers the experimental implementation of each of the modules in this diagram, except for the clustering module, which was not implemented for this research. 68 Figure 24: Block Diagram of the Proposed System. The proposed system is divided into 4 subsystems described below: 1) Room Geometry and RIR Generator: The first subsystem of the block diagram is The Room Geometry and RIR Generator (RGRG). This subsystem accepts the room geometry parameters (geometry of the room, absorption, location of the speakers and microphones) and calculates the RIRs using synthetic speech sources. The RGRG consists of two modules: The Room Parameter 69 Generator (A) and the Room Model Generator (B). The Room Parameter Generator creates the vectors that contain the geometry of the room and the location of the speakers and microphones (virtual and real). The Room Model Generator gets the room geometry vectors and calculates the RIRs between the sources and the microphones. 2) Audio Pre-Processor: The second subsystem is the Audio Pre-Processor (APP). The function of the APP is to prepare the single-channel raw audio for analysis. It consists of two modules: The Training Sample Audio (C), and the Segmenter (D). The Training Sample Audio module contains the training samples of each of the speakers participating in the audio to be analyzed plus a sample of ambient noise. These samples are saved as .wav files and labeled independently. The Segmenter module uses a VAD to create segments of the audio to be analyzed. Each segment of audio is saved as a .wav file of variable duration, with a minimum and a maximum length threshold. The segments that are less than a predetermined length are discarded. 3) Analysis Subsystem: The function of the Analysis Subsystem (AS) is to calculate the cross-correlation between the emulated microphones. The AS consists of three modules: The Source Estimator (E), the Room Model Estimator (F), and the Cross-Correlator (G). The source Estimator gets the audio from the APP and deconvolves it with the RIR from the RGRG to estimate each possible source. The deconvolution is done to both the audio training samples 70 and the segments for analysis. The Room Model Estimator then emulates each of the microphones by convolving the estimated sources with each of the corresponding RIR calculated using the model room geometry. Again, this is done for both the training samples and the analysis segments. Finally, The Cross-Correlator module calculates the cross- correlation between the microphones for each of the possible source combinations, for both training and testing. 4) Classifier: The output of the AS subsystem is then handled to the Classifier. The classifier creates the cross-correlation sets of tables for training and testing. During training, only one table is created for each of the training audio samples. For testing, there is a cross- correlation table for each possible source for each of the segments, as was described in section 3.1. There are two possible paths of action once the cross-correlation tables are available. One path is to just run a clustering algorithm to group each segment in similar clusters or run a classifier that selects the best source that matches the training cross- correlation table. This research follows the classifier option, which will be discussed in the next section. The final component of the Classifier subsystem is the Speaker Metric module. The function of this module is to calculate the statistics of each of the participants, e.g., for how long they have been active, and when they have been active. In this research, our metrics only focus on measuring the total time each participant has been active. 71 Chapter 4. Experimental Implementation This chapter presents the software and hardware implementation of the proposed method presented in Chapter 3. It begins by presenting the software tools for simulation, deconvolution, and data handling, and continue with the software implementation based on the AOLME environment. This implementation will be used in the experiments of Chapter 5 to evaluate the performance of the proposed method. 4.1 Software and Hardware Tools The block diagram of Fig. 24 shows the need of developing several software modules to simulate the acoustics characteristics of the room, including RIRs and source image calculations, deconvolution for source estimation, cross-correlation, and classification. In summary, it is necessary to have code that performs the following operations: 1) Simulation of the geometry of a room 2) Calculation of all the RIRs based on the geometry of the room 3) Extraction of audio track from video recording 4) Segmentation of audio recording 5) Deconvolution of audio for source simulation 6) Simulation of microphone array 7) Microphone cross-correlation calculation 8) Analysis of microphone cross-correlation to identify the active speaker 72 9) Calculation of the metrics for each participating speaker. Developing code for the above modules is a time-consuming task due to the large number of mathematical algorithms and calculations needed. Fortunately, there are software packages available and code libraries that simplify the implementation of these modules into a software framework for the experimental analysis. This dissertation have combined open-source code and commercial software, saving a considerable amount of time to the alternative of writing code from scratch. 4.1.1 Open-Source Code for Room Geometry, RIR Calculation, and Microphone Simulation The open-source community of code developers offers an extensive variety of software libraries that cover a wide range of topics, from machine learning to financial market analysis, including acoustic simulations. Several acoustic simulation packages are available on GitHub for download. These packages are mainly designed to simulate the acoustics of environments for performing arts, such as theaters, stadiums, and recording studios. From these available packages, Pyroomacoustics was selected for RIR calculation and microphone simulation. Pyroomacoustics [71] is an open-source acoustic simulation package that calculates the RIRs and simulates the reception of the audio at a set of virtual microphones located inside a virtual room. Pyroomacoustics uses the Image Source Method (ISM) to calculate the RIR between a source and any point inside the virtual room. The location of the images 73 can be visualized with 2-D and 3-D representations of the geometry of the virtual room and the location of the sources and the virtual microphones. After simulating the location of the images, Pyroomacoustics calculates the RIRs to the target microphones and convolves the sample audio to simulate the reception at the microphones. Pyroomacoustics libraries' inputs are the geometry of the room, the location of the sources, the location of the virtual microphones, the absorption of the room, the sampling frequency, and the number of images to calculate. The outputs for the libraries include a set of arrays containing the RIR to each of the microphones and the emulated reception at each of the microphones. Figs. 25(a) and 25(b) show examples of 2-D and 3-D visualizations from Pyroomacoustics of a non-rectangular room 3 x 5 x 2 meters, with a circular microphone array with 6 microphones and one source. Fig 25(c) shows the same room with the simulated images. These types of representations will be used to approximate the AOLME models discussed later. Complete documentation on Pyroomacoustics functions and code can be found in [72]. 74 Figure 25: Pyroomacoustics Models. (a) 2D. (b) 3D. (c) 3D With Images. The version of Pyroomacoustics used for this research (0.4.1) has some limitations that needed to be considered when developing the models: 1) Microphones and sources are always modeled as omnidirectional. There are no options to add unidirectional sources or other types of microphones (e.g., cardioids); 2) All rooms are square, with no round corners; 3) There is no option to add objects such as tables inside the room, and 4) Absorption is an empirical parameter that needs to be estimated by other means outside the software. 75 All experiments in this research were conducted using Pyroomacoustics version 0.4.1. For the room geometry calculations, Pyroomacoustics libraries are called from a Jupyter Notebook under Anaconda 3. The libraries were also called from within LabVIEW using scripts under Python 3. 4.1.1.1 Pyroomacoustics Implementation Implementation of Pyroomacoustics for virtual microphone simulation is accomplished by four steps that include simulation of the room, placement of the sources and microphones, calculation of the RIR from the sources to the microphones, and convolution of the sources with the RIR for microphone simulation. This is done by calling the classes in the Pyroomacoustics libraries as follows: a) Room Simulation: The room simulation contains the parameters of the room, such as its dimensions, the absorption, the number of images to be calculated, and the sampling frequency. For example, the following script will generate a room of dimensions 9 x 7 x 3 meters, with a total of 9 images, at a sampling frequency of 9600 Hz: pyroomacoustics.room.Room([9.0,7.0,3.0], fs=9600, max-order=9) b) Sources and Microphone Placement: 76 This script adds the sources and microphones to the model. Pyroomacoustics need a valid audio file for source location. The following script will locate a source at X= 2 m, Y = 3 m, and Z = 1 m from the origin, and two microphones at X1 = 6 m, Y1 = 4, and Z1 = 1 m for microphone 1, and X2 = 6 m, Y2 = 4.5, and Z2 = 1 m for microphone 2: room.add_source([2.0, 3.0, 1.0], signal=audio) mic_locs = np.c_[ [6.0, 4.0, 1.0], # mic 1 [6.0, 4.5, 1.0], # mic 2 ] room.add_microphone_array(mic_locs) c) RIR calculation: By calling room.compute_rir() the RIRs are calculated to each of the microphones, and the results are saved in the form of a list of lists at the rir attribute of room. d) Microphone Simulation: The final microphone simulation is obtained by calling simulate(). This convolves the sources with each of the RIRs and emulates the signals in each of the microphones. The results of the convolutions are stored in the signals attribute of room.mic_array. Appendix A includes some of the scripts used in the actual experimental implementation of Pyroomacoustics. Refer to Pyroomacoustics documentation found at [72] for full description of the libraries and their algorithms. 77 4.1.2 Audio Segmentation Recalling from previous discussions, the practical analysis of long audio recordings is not possible unless they are segmented into smaller frames. Audio segmentation plays a critical role in the overall performance of the proposed method; therefore, careful consideration should be made with the algorithms for audio segmentation. As previously indicated, the audio segments need to comply with two main requirements: 1) Contain audio from only one active speaker with minimum overlapping or mixing between speakers, and 2) are of a length that provides enough signal information for cross- correlation analysis. Both requirements are difficult to achieve, and in chapter 3 it was introduced the concept of VADs as a segmentation method that maximizes the information content of a single speaker. Two segmentation methods were considered during this research: Fixed Segmentation and Voice Activity Detection. In the end, it was opted for VADs due to their better performance results. 4.1.2.1 Fixed Length Segmentation The simplest way to segment audio is to divide it into fixed-length segments. Fixed length segmentation is a relatively simple and computationally inexpensive method where each audio segment has the same length, independently of their content. Because there is no intelligence in this method, there is a probability of some of the segments containing overlapping speech. Also, it is very unlikely that the audio can be divided into exactly equal parts making the last segment of shorter duration than the others. 78 A solution to minimize overlapping is to make the segments as short as possible. As it was discussed before, if the segments are too short, they may not contain enough information for calculating the cross-correlation. There is therefore a balance between the optimum length of the segments and the desired classification error. One empirical way to find the length of the segments is by assessing the audio. If the audio contains well-separated speakers with little overlapping, the length of the segments can be longer than in acoustic scenes with noise or disorganized speech. This research conducted experiments with segment lengths varying from 250 ms to 1.2 s, obtaining different degrees of success. At the end, fixed-length segmentation was abandoned due to an undesirable number of errors and a lower performance when compared with voice activity detection segmentation. 4.1.2.2 Voice Activity Detection The VAD used in these experiments was programmed using MATLAB by a fellow graduate student at the University of New Mexico [73]. He used MATLAB Fast Fourier Transform (FFT) and Inverse Fast Fourier Transform (IFFT) functions to convert the audio from the time domain to the frequency domain and vice versa. First, the audio is converted into the frequency domain by applying the FFT, and then a 3000 Hz low pass filter and a 1000 Hz high pass filter are applied to remove some of the noise. The filtered audio is brought back to the time domain using the IFFT, and it is normalized afterward. An Amplitude Trigger (AT) with a threshold of 0.1 is used to determine the presence of speech 79 or noise. If the amplitude is exceeded at any time, this will be the beginning of a speech, and this time is marked as 𝑇(cid:2869). The level is checked 300 ms after 𝑇(cid:2869). If the AT is exceeded again, we mark that time as 𝑇(cid:2870), and check the AT again after another 300 ms. If the audio does not exceed the AT, then it is marked as the end of the audio with a time 𝑇(cid:2870) = 𝑇(cid:2869) +300 ms; otherwise, the end of the audio will be 𝑇(cid:2869) + 𝑇(cid:2870). When the audio does not exceed the AT and is not in the time range of speech, it is classified as noise. Using the information obtained from the filtered audio and adding a time offset (250 ms) to compensate for information missing in the filtered audio, we split the audio giving a maximum and a minimum time. If a noise segment is too small (under 250 ms), they will be combined with the audio segments since this is a pause of a person speaking. If there is noise or the audio is too long, it gets split into batches of a maximum time of 1.2s, with small exceptions that can go up to 1.449s. It is important to notice that audio segmentation produces artifacts at the beginning and the end of the segment [74]. These artifacts are the familiar “clicks” we hear when listening to a sequence of segments. In audio processing, it is common practice to apply a window, a filter, and overlap of the segments [74] to allow for a smooth transition between them. Our method did not apply windowing or overlapping due to the possibility of altering the spatial content of the segments; therefore, it was better to accept a reasonable error instead. 80 4.1.3 Implementation Using the LabVIEW Graphical Programming This research work used LabVIEW for deconvolution, array manipulation, classification, metrics, and user interface. LabVIEW is very popular in engineering due to its wide variety of built-in functions and its simplicity to create graphical user interfaces. It is not an open-source language, requiring the purchasing of a license. LabVIEW requires additional toolkits for some advanced digital signal processing, statistics, and software integration. This research applied the Advanced Signal Processing toolkit for cross-correlation analysis, convolution, and deconvolution calculations. Because Pyroomacoustics was used for all room simulations, it was necessary to install the Python Integration Toolkit provided by Enthought. This toolkit provides LabVIEW with the capability of calling Python code directly. LabVIEW is then used as a wrapper to call Pyroomacoustics Python libraries from within LabVIEW. In this way, LabVIEW provides the user interface for the Pyroomacoustics inputs (i.e., room geometry, number of images, audio files), and processes the outputs (i.e., RIRs, microphone simulations), saves the data, and displays the results. All documents and detailed description of LabVIEW can be found at the NI website [75]. Instead of scripting code, LabVIEW uses a graphical interface that contains functional modules called VIs (short of Virtual Instruments). The VIs perform basic functions such as adding, subtracting, array manipulation, and logical operations, among others. There are more advanced VIs to calculate more complex operations such as convolution and inverse convolution, correlation and cross-correlation, and file 81 manipulation, for example. Each VI transfers data using a wired connection, and there is a mechanism to handle and display execution errors. Fig. 26(a) and Fig. 26(b) show a screenshot of the internal block diagram and user interface, respectively, of the LabVIEW implementation used during this research for convolution, deconvolution, correlation, and cross-correlation operations between two files. The user can select between any of the operations using a drop-down selector. The implementation reads two text files that correspond to the audio files to be analyzed or convolved and a third file that corresponds to the RIR for convolution operations only. There are four graphics that represent the input files, the RIR, and the output of the cross-correlation calculation. The results can be saved as text files for later conversion into audio or any other format. Figure 26: LabVIEW Sub VI for Cross-Correlation Calculation. 82 Despite the popularity of LabVIEW among the engineering community, LabVIEW is many times regarded by hard-core coders as a language for those who do not know how to code. Its major deficiencies lie in the fact that its built-in functions are rarely modifiable, the block diagrams can get confusing if they are not divided into smaller VIs, and it is difficult to document and comment. The decision was to use LabVIEW because of the time-savings advantages it has over scripted languages. 4.1.3.1 LabVIEW Implementation The code written for this research used several built-in VIs available in LabVIEW version 2016, 32 bits. These VIs were implemented into more complex sub-VIs to run the calculations, data handling, user interface, file manipulation, and display of results. Although the code required the use of dozens of different VIs for simple mathematical operations and data flow, important calculations such as convolution and deconvolution were handled with LabVIEW built-in functions. 4.1.3.1.1 Function VIs The three functions VIs in this section were used to calculate convolution, deconvolution, and cross-correlation. They are part of LabVIEW's built-in library for signal processing. The algorithms for these functions are explained next. 4.1.3.1.1.1 Convolution VI 83 This VI computes the convolution of two vectors x and y. The convolution can be computed by selecting either a direct method or a frequency domain algorithm that uses the FFT, being the latter the one used for this research. The VI that represents the convolution is shown in Fig. 27. Documentation on this VI can be found at [76]. Figure 27: LabVIEW Convolution VI. The algorithm works by padding the ends of x and y with zeros to make their lengths M + N – 1, as shown in (4.0) and (4.1): 𝑥′𝑖 = (cid:3420) 𝑥𝑖, 0, 𝑖 = 0,1, … , 𝑁 − 1 𝑖 = 𝑁, … , 𝑀 + 𝑁 − 2 (4.0), 𝑦′𝑖 = (cid:3420) 𝑦𝑖, 0, 𝑖 = 0,1 … , 𝑀 − 1 𝑖 = 𝑀, … , 𝑀 + 𝑁 − 2 (4.1), The convolution is computed by calculating the inverse FFT of the product of the FFTs of 𝑥(cid:4593) and 𝑦(cid:4593) 𝒙(cid:4593)(𝑓) = 𝐹𝐹𝑇(𝑥(cid:4593)) (4.2), 84 𝒚(cid:4593)(𝑓) = 𝐹𝐹𝑇(𝑦(cid:4593)) (4.3), 𝒙 ∗ 𝒚 = 𝐼𝐹𝐹𝑇(cid:3435)𝒙(cid:4593)(𝑓) ∙ 𝒚(cid:4593)(𝑓)(cid:3439) (4.4), where IFFT is the inverse FFT. 4.1.3.1.1.2 Deconvolution VI The deconvolution VI computes the inverse convolution of two vectors x*y and y. It returns the value of vector x. Fig. 17 shows the symbol for this VI. Documentation on this VI can be found at [77]. Figure 28: LabVIEW Deconvolution VI. This VI implements the deconvolution by computing the Fourier Transform of the input x*y and y, then dividing them to create a new vector h. The vector x is computed by applying the IFFT to the sequence h. 85 4.1.3.1.1.3 Correlation VI The Correlation VI calculates the correlation coefficient r between two vectors x and y. Fig. 18 shows the icon for this VI. Documentation on this VI can be found at [78]. Figure 29: LabVIEW Correlation VI. This VI calculates the linear correlation coefficient, also known as Pierson’s correlation by (eq. number) 𝑟 = ∑ 𝑧(cid:3051)𝑧(cid:3052) 𝑛 (4.5), where 𝑧𝑥and 𝑧𝑦are the standardized z-values of x and y. The standardized z-values indicate how many standard deviations x and y are above or below the mean. 4.1.3.1.1.4 Cross-Correlation VI The cross-correlation VI computes the cross-correlation between two vectors x and y. The inputs for this VI are the vectors 𝒙𝒕 and 𝒚𝒕, the weighting specifies the use of a biased or unbiased weighting in the cross-correlation calculation, being the former the one used in all the calculations. The maximum lag specifies the maximum value of the lag this 86 VI uses to compute the cross-correlation. The maximum lag used equals max (M, N) – 1, where M and N are the lengths of 𝒙𝒕 and 𝒚𝒕, respectively. Fig. 30 shows the icon for this VI. Documentation on this VI can be found at [79]. Figure 30: LabVIEW Cross-Correlation VI. This VI computes the cross-correlation values between two univariate time series 𝑿𝒕 and 𝒀𝒕 according to the following equation: 𝑟𝑥𝑦(𝑘 + 𝑁 − 1) = 1 𝑎 ∙ 𝑏 ∙ 𝑤(𝑘) 𝑁−1 (cid:3533) 𝑋𝑡(𝑛)𝑌𝑡 𝑛=0 (𝑛 + 𝑘), 1 − 𝑁 < 𝑘 < 𝑀 (4.6), where = (cid:3493)∑ (cid:3050)(cid:2879)(cid:2869) (cid:3041)(cid:2880)(cid:2868) (cid:2870)(𝑛) 𝑋(cid:3047) , 𝑎 = (cid:3493)∑ (cid:3014)(cid:2879)(cid:2869) (cid:3041)(cid:2880)(cid:2868) (cid:2870)(𝑛) 𝑌(cid:3047) , 𝑿𝒕 has length N and 𝒀𝒕 has length M. The length of the output is N+M–1. w is the weighting factor which in our case, w(k) = 1. 4.1.3.1.2 Operational Sub-VIs This section will cover the Sub-VIs that form the core of the code that performs the computations needed for the analysis. These Sub-VIs contain the function VIs covered in 87 the previous section. Appendix B contains the front panels and block diagrams of these sub-VIs. 4.1.3.1.2.1 Room Parameters Reader The Room Parameters Reader Sub-VI reads the source locations, microphone locations, and 2D room dimension files created by the Pyroomacoustics Room Geometry Generator and formats them for the Room Model Generator Sub-VI. The room absorption, the room extrusion, and the number of images to calculate are just a pass thru. Appendix B section (a) shows the front panel and blocks diagrams for this Sub-VI. 4.1.3.1.2.2 Room Model Generator The Room Model Generator Sub-VI reads the room geometry parameters formatted by the Room Parameters Reader Sub-VI and runs the Python scripts that call the Pyroomacoustics libraries that compute the RIRs for the room model. This Sub-VI also reads the synthetic speech or noise .wav files used by Pyroomacoustics for the RIR calculations. The calculated RIRs are saved in .txt files for later retrieval by the Source Estimator Sub-VI. The Room Model Generator is used twice, first to calculate the room model RIRs for the source estimation, and again to emulate the virtual microphones using the estimated sources. Section Appendix B section (b) shows the front panel, block diagram, and inputs and outputs with more detail. 88 4.1.3.1.2.3 Source Estimator The Source Estimator takes the model RIR and estimates all the sources that will correspond to the audio segment that is being analyzed. For this estimation, this Sub-VI takes the segment of audio under analysis (corresponding to the real recording microphone) and deconvolves it with the RIRs for each of the source locations. The emulated sources are saved under .txt files for virtual microphone simulation using another instance of the Room Model Generator. Appendix B section (c) shows the details of this Sub-VI and a simplified block diagram. 4.1.3.1.2.4 Cross-Correlation Model Calculator This Sub-VI takes the results of the virtual microphone simulation from the second run of the Room Model Generator and calculates all the cross-correlations between the virtual microphones. The results are saved as cross-correlation tables and used for training and classification. Appendix B section d shows the details of this Sub-VI. The output of this Sub-VI is a table that contains all possible cross-correlations between microphones for each of the possible sources. For the three speakers and three microphones example, the cross-correlation table would look like the one represented in Table IV. The first row is the cross-correlation microphone combinations, and the first column is the speakers. The numbers represent the array index where the max occurs. 89 Table IV: Example Cross-Correlation Table Output from Model Calculator 1-2 96 -32 5 1-3 5 0 5 2-3 -5 -83 -130 1 2 3 4.1.3.1.2.5 Model Classifier The Model Classifier Sub-VI takes all the correlation tables, from training and testing, and performs the classification by comparing the testing results against the training templates. This is a very simple classifier that works by comparing each CC table for best similarity. For example, assume that the CC table IV corresponds to the training of speaker S1, and the analysis of an unknown audio segment produces the three CC tables shown in Table V(a), (b), and (c). The classifier simply counts the number of matches between each CC table and the training CC table. In this example, table V(a) has the greatest number of matches, indicating that the unknown segment corresponds to speaker 1. Appendix B section (e) shows the icon and front panel. Table V: Cross-Correlation Tables for Classification 1 2 3 1-3 1-2 96 1 2 -15 5 1 Total for S1 (a) 2-3 -5 -3 -130 Match 2 0 2 4 90 1 2 3 1 2 3 1-3 1-2 9 1 -32 6 2 9 Total for S2 (b) 1-3 1-2 6 8 -2 2 5 5 Total for S3 (c) 2-3 -1 -8 -13 2-3 -50 -8 -15 Match 0 1 0 1 Match 0 1 1 2 4.1.3.1.2.6 Multi-Function Convolution and Correlator Visualizer The Multi-Function Convolution and Correlator Visualizer is a full stand-alone Sub-VI used to manually convolve and deconvolve audio files and for correlation and cross-correlation analysis of files. Appendix B section (f) provides more information about this Sub-VI. 4.1.3.2 Audio Laboratory The purpose of the Audio Laboratory was to capture real audio in a controlled environment. This laboratory allowed to conduct experiments knowing the position of the speakers and microphones and control the content, duration, and characteristics of the analyzed speech. The results from the experiments performed at the audio lab were 91 compared against the results obtained from our proposed method and the simulation software. The audio laboratory consisted of a set of microphones, an audio processing device, an audio amplifier, loudspeakers, and the computer running the software that captures the recordings. The audio laboratory was physically configured to follow the common acoustic scene found on the videos analyzed in this research. This configuration used a set of loudspeakers located at the approximated position of the speakers sitting around a table. A set of microphones captured the audio at different locations of the lab, and one microphone was located at the same relative position as the recording microphone at the videos. Fig. 31 shows a block diagram of the lab components. The set of microphones were the same type used in the recording of AOLME video. These microphones were connected to the Tascam® Audio Processor. This processor can capture simultaneous audio from all six microphones and send it digitally to the computer via USB. The computer processes the audio using Tracktion Waveform® audio processing software [80]. This software processes the audio from the microphones and saves it in separate .wav files that correspond to each of the microphones. 92 Figure 31: Audio Lab Components. The simulation of the speakers is accomplished using a set of four loudspeakers connected to a stereo audio amplifier. Speakers 1 and 2 were simulated with the left stereo channel, while speakers 3 and 4 were simulated with the right stereo channel. A switch allows selecting between loudspeakers 1 and 3, and 2 and 4. The lab also included a Compact Disk (CD) player located at a certain distance from the table. This CD player was used to inject background noise during the experiments. Fig. 32 shows the actual audio laboratory setup where we can appreciate the location of its components. 93 Figure 32: Audio Lab Setup 4.2 The AOLME Environment The dissertation focuses on the analysis of audio from AOLME videos to assess the level of engagement of the participants. The AOLME environment is characterized by the presence of background noise, crosstalk, and other interferences that make it challenging for speaker identification tasks; therefore, to improve the identification rate, the simulation models must be optimized to fit this environment. This section studies the AOLME environment to find out how to best adapt the models to the acoustic characteristics of this environment and implement these models for the experimental section. 94 4.2.1 Characteristics of the AOLME Environment Fig. 33 shows a screen capture from one of the AOLME videos analyzed in this research. The scene shows a typical collaboration table with four students and one instructor. It is common to have 5 to 10 of these tables, with three to six participants each, distributed in a room of approximated dimensions of 9 x 14 x 2.5 m. The camera is recording the audio via a single omnidirectional microphone that is resting on the top of the table. In addition to normal room noise, this environment presents other elements that make its dynamics more complex. For example, it is typical to have the participants shuffling papers, leaning over the table, eating, speaking simultaneously, and accidentally covering the microphone with books or other utensils. Furthermore, there are occasions when another staff member walks in and joins the group for a conversation. Figure 33: Common AOLME Environment Setup. 95 The first step in building the models is to approximate the location of the speakers and the recording microphone. By analyzing the scene in Fig. 33, it is possible to get some clues that can be used to approximate these locations. From Fig. 20, it is possible to estimate the relative locations of each of the speakers with respect to each other and the recording microphone. It is noticeable also that the position of the speakers forms a rectangle that can be translated into a 3D figure whose bottom area is the table and its height is defined by the tallest speaker. The second step is to approximate the geometry of the room. From Fig. 33, it is possible to recognize that there is a nearby wall behind speakers 1 and 3. The second wall is located behind speaker 2 at a farther distance from speaker 2 than the first wall is located from speakers 1 and 3. There is no indication of any other wall or the presence of the celling, which we are assuming exists. It is also assumed that there are other tables nearby, but these cannot be seen in Fig. 33. 4.2.2 Preparation of the Experimental Models As mentioned earlier, the models are based in part on the geometry of the room and the location of the speakers. Because this exact information is not available, the models need approximations based on the observations made from the video shot. Also, recalling from section 4.1.1, our version of Pyroomacoustics does not allow us to simulate complex environments like the one shown in Fig. 33, where we have the participants sitting around 96 a table. Fortunately, the models do not need to be perfect, and we can make assumptions that will reduce their complexity. 4.2.2.1 Approximating the Models Using Video Observations We are ready to make some assumptions and approximations based on observations from the video. Fig. 34 shows another frame from the same AOLME video recording, where it is easy to estimate the relative distances between the participants. In Fig. 34, H1 represents the height of speaker 2, while and H2 represents the relative height of speakers 1, 3, 5, and 4. S represents the separation between speakers, and D represents the width of the table. We are assuming also in this observation that speakers 1 and 4 are separated by the same distance D. Figure 34: Relative Positions of AOLME Participants. 97 In Fig. 34, D can be approximated to the width of two standard commercial tables, which we can assume is 0.8 m x 2 = 1.6 m total. Speaker 2 is sitting about half of this distance, about 0.8 m from each edge of the table combination. Speaker 1 is close to one of the corners of the table, as it is speaker 4. The separation S between speakers can be approximated to 0.3 m, and the recording microphone can be located at half of this distance at the center of the table. Finally, H1 can be approximated using as reference the average waist to head distance of a young female, to about 0.5m, and H2 to the average waist to head distance of kids 11 years old, to approximately 0.4 m. These values are just examples to illustrate the principle on which we are basing the approximations. The actual model will not necessarily use these values. There is no prior knowledge of the dimensions of the room that can be used to approximate its geometry. Observations about the location of the walls and the ceiling only provide a reference for the location of two walls. Nevertheless, it is possible to recognize, given the appearance of the scene, that the remaining walls are at a greater distance than the visible ones. This assumption does not provide a numeric value to the location of the walls or the ceiling, but it gives a clue of the behavior of the sound in the room. Recalling Section 2.4, the human voice propagates mostly unidirectionally to the front of the speaker. Speakers 1 and 3 will project their voices toward speakers 4 and 5 and vice versa. Most of the sound energy from speakers 1 and 3 is absorbed by speakers 4 and 5, with some energy reflected by the table, some traveling to the ceiling of the room, and some other amount propagating to the walls behind. The walls reflect the residual 98 sound energy to speakers 4 and 5, and the process repeats until all the energy is absorbed, following the 𝑇(cid:2874)(cid:2868) rule. The same process applies when speakers 4 and 5 are active. In the case of speaker 2, there are no reflecting surfaces directly located in front of her, and the computer screen is located at a distance where the sound reflections from it can be considered of minimal influence, making the table the only reflecting surface. Under this model, it is possible to conclude that the sound energy of the participants is mainly contained within the boundaries of the table, and the contributions of the reflections due to the walls can be considered in practice as negligible, given the directionality of speech, the absorption of the speakers, and the separation of the speakers to the wall and the ceiling of the room. The previous analysis indicated that it is not critical that the models take into consideration the reflections from the walls, suggesting that the rooms can be modeled as to be of infinite dimensions or to have an absorbance that is close to 1. Unfortunately, having a room of infinite dimensions will lead to a problem when modeling the sources. As discussed previously, the simulation software only allows for omnidirectional sources and microphones. In a wall-less room, Pyroomacoustics will create images from speech that equally propagates in all directions from the speaker, which we know is not accurate. The solution is to place the sources at very close proximity from the walls of the model and make the virtual room of the size of the table, thus reducing the propagation behind each speaker to negligible levels. 99 The analysis described above gives the basis for a first model representing the location of the speakers and the recording microphone. Recalling the 2D model of Fig. 19, we can set up a 2D model based on the acoustic scene of Fig. 34, representing the location with respect to the table of the 5 speakers and the real (recording) microphone. Note that this model includes a 6th “speaker” that represents the room noise. Representing the noise as a separate speaker allows for better discrimination between audio segments containing noise and those containing speech. Figure 35: Location of Speakers and Real Microphone. The Z dimension (room height) needs to be added to convert the 2D model into a 3D model. Because the perimeter of the room is limited to the size of the table, the table itself can be modeled as the floor of the room. With this approach, all locations will be zero-referenced with respect to the table. 100 The total height of the room can be approximated in a similar manner as it was done for the perimeter of the room. Because of the directionality of the human voice, it is expected that there will be a little transmission of voice energy to the ceiling; therefore, the reflections coming from above can be neglected. The ceiling can then be located at any height for as long it is above the maximum height of the taller speaker. Empirically, this value can be set, for example, at 1 m above the table. The 3D model for the room dimensions and the speakers is shown in Fig. 36. Figure 36: 3D Model of the Virtual Room The last element needed to complete the model is the location of the virtual microphones. Their location is constrained by the dimensions of the virtual room and the maximum anti-aliasing distance between them. Also, it is necessary to consider that the array of microphones consists of a set of virtual microphones plus a real microphone, which 101 is resting at the top of the table. At this location, the real microphone receives no sound reflections from the bottom; therefore, it can be assigned a Z value of zero. Because the real microphone is resting on the table, there are mechanical vibrations transmitted from the table. To simulate these vibrations, all models in this research include some value for the Z component of the real microphone. The location of the virtual microphone array can be arbitrary, and the separation between microphones is not critical because the distance between two adjacent microphones will never exceed the maximum for anti-aliasing. However, it is of interest to have unique cross-correlation values between microphones. For this, the array should be in an asymmetric position with respect to the speakers in such a way that the value of the magnitude of the cross-correlation between microphones is different for each speaker. The Z value of the virtual microphones can be arbitrary, but because Pyroomacoustics can only simulate omnidirectional microphones, it is of advantage to locate them a certain height above the reference microphone. All the models in this research have microphones located at approximately the height of the speakers, allowing for simulation from all directions. Fig. 37 shows the complete 2D model derived from the five-speaker AOLME environment example. This type of model is used in all experiments in this dissertation, with the variations needed to fit the objective of the experiment. 102 Figure 37: Final 2D Model for AOLME Example. 103 Chapter 5. Results This chapter presents the experiments conducted to evaluate the capability of the proposed method to identify speakers in audio segments. The experiments focused on three objectives: 1) To determine the suitability of Pyroomacoustics as a simulation package; 2) to evaluate the performance of the proposed method for diarizing and identifying speakers; and 3) to compare the performance of the proposed method against Amazon AWS and Google Cloud. These experiments included both real audio recordings from the audio lab and AOLME videos. 5.1 Evaluation of Pyroomacoustics The objective of this experiment was to evaluate Pyroomacoustics as simulation software. This experiment compared the cross-correlation measured between real microphones and the cross-correlation between emulated microphones using Pyroomacoustics. This experiment was performed using the audio lab, with a Pyroomacoustics simulation based on the geometry discussed in Chapter 4. 5.1.1 Microphone Calibration All audio recording devices have an electronic delay that varies from equipment to equipment. To measure the real cross-correlation between physical microphones, it is necessary to measure this electronic delay for each of the microphones and apply a calibration factor if necessary. Because Pyroomacoustics version 0.4.0 simulates all 104 microphones as ideal and does not consider any delays, it is necessary to calibrate the real microphones to compensate for their delays before comparing them against any simulation. One way to calibrate the microphones is to place them in an array configuration and locate this array in the proximity to an audio source. Fig. 38 shows a block diagram of the components needed to calibrate the microphones. This calibration setup consists of an audio source, speaker, sound processor, and microphone array. The audio source is driven by a signal generator, and the sound processor can acquire six audio channels simultaneously. Figure 38: Block Diagram of a Microphone Calibration Setup. a) Calibration Preparation A homemade jig made of cloth pins was used to hold the six microphones for calibration. The configuration and separation of the microphones are shown in Fig. 39(a). The array of microphones was located next to one of the loudspeakers, as shown in Fig 39(b). With this configuration, the distance of each microphone to the sound source is about 105 the same for all microphones, making the time differential of arrival between them neglectable. (a) (b) Figure 39: (a) Microphone Calibration Jig. (b) Location to Loudspeaker. 106 b) Calibration Execution A 450 Hz signal was applied to the loudspeaker using a signal generator, to the loudspeaker, and the output of the six microphones was collected simultaneously using the sound processor and the computer running Tracktion Waveform® software. Each channel recording was saved as a separated .wav file of 2 s duration, sampled at 48 kHz. To measure the delay between microphones, each of the .wav files was converted into .txt files for cross-correlation analysis using the Multi-Function Convolution and Correlator Visualizer Sub-VI. Each combination of microphones was cross-correlated as shown in Table VI. The results in Table VI show that Microphones 1, 3, and 6 had zero cross-correlation between them. The same was observed between microphones 2, 4, and 5. Rather than apply a calibration factor, it is more convenient to segregate the microphones into groups and measure the cross-correlation between pairs that belong to the same group. Note that the results shown by Table VI correspond to the index of the array where the max cross-correlation occurs. Table VI: Cross-Correlation Table for Microphone Calibration. 1 s 2 e n o 3 h p 4 o r c 5 i 6 M 1 - 50 0 49 50 0 2 -50 - -50 0 0 -50 Microphones 4 -49 0 -49 - 0 -49 3 0 50 - 49 50 0 107 5 -50 0 -50 0 - -50 6 0 50 0 49 50 - 5.1.2 Audio Lab Setup and Model Configuration Fig. 40 shows the laboratory setup for this experiment. The setup follows the general model configuration described in Chapter 4, but the microphones were distributed between the loudspeakers to maximize the cross-correlation value differences between microphones. The dimension of the lab setup allows for the microphones to be within the anti-aliasing distance already calculated of 0.95 m. Microphone 3 was kept in the same location as the recording microphone of the draft model. Figure 40: Audio Lab Set-Up for Pyroomacoustics Evaluation. The Pyroomacoustics model was set up following the configuration of the audio lab. Because the audio lab has only 4 loudspeakers, speakers 5 and 6 were not included in the model. The virtual room perimeter was set to the size of the lab table, and the height of 108 the room was set to a value of 1 m. The absorption of the model was set empirically to 0.95 and the number of images at 8. The microphone height was set to 0.025 m for all microphones, following the observations made in Chapter 3. Table VII shows the final dimensions of the virtual room and the location of the sources (loudspeakers) and microphones used to create the Pyroomacoustics model. The final 2D model geometry generated by Pyroomacoustics is shown in Fig. 41. Table VII: Dimensions of Virtual Room and Location of Sources (in m). Z 0.25 0.25 0.25 0.25 --- --- 0.025 0.025 0.025 0.025 0.025 0.025 -- -- -- -- Y 0.79 0.4 0.79 0.01 --- --- 0.79 0.01 0.35 0.1 0.7 0.79 0 0.8 0.8 0 1 Sources Mics Room S1 S2 S3 S4 S5 S6 M1 M2 M3 M4 M5 M6 CORNER 1 CORNER 2 CORNER 3 CORNER 4 EXTRUDE X 0.4 0.01 1 0.4 --- --- 0.015 0.015 0.9 1.39 1.39 0.7 0 0 1.4 1.4 109 Figure 41: Final 2D Model of Audio Lab Setup. 5.1.3 Experimental Execution Both audio lab and simulation sections of this experiment used as a source one anechoic male voice of 2 s of duration. The source was played sequentially on each of the loudspeakers corresponding to S1, S2, S3, and S4, and it was captured simultaneously into the six-channel audio processor, corresponding to each of the microphones. The six- channel audio then was saved as six independent audio files using Tracktion Waveform®. The simulation with Pyroomacoustics used the geometric model of Fig. 41. Because there was no need to estimate the sources, the simulation of the reception at microphones M1, M2, M3, M4, M5, and M6 was accomplished by only running the Room Model Generator Sub-VI with the geometric model and playing the source at the location of speakers S1 to S4. The Sub-VI saved the results of each microphone simulation as a separate .txt file. 110 5.1.4 Results The final analysis consisted of running the Multi-Function Convolution and Correlator Visualizer Sub-VI to calculate the cross-correlation for each of the real microphone audio files (ground truth) and the simulated microphone audio files. The cross- correlation was calculated between microphones of the same group as it was determined during calibration. There was no need for audio segmentation due to the short duration of the sample audio. Table VIII shows the results in ms of the offset between the ground truth and the simulated signals, corresponding to a sampling rate of 48 kHz. Table VIII: Experimental Results for Simulation Software Evaluation. S1 S2 S3 S4 Sim. G.T. Diff Sim. G.T. Diff Sim. G.T. Diff Sim. G.T. Diff 0.34 -0.58 -1.62 0.38 -0.34 0.24 -1.24 0.38 0.72 1.56 1.88 0.32 0.20 0.80 0.30 0.68 0.10 -1.06 -1.30 0.24 0.12 0.18 0.30 0.12 -1.08 -0.88 0.20 -1.90 -1.56 0.34 -0.48 -0.50 0.02 -1.90 -1.62 0.28 0.58 0.40 0.18 0.00 -0.02 0.02 1.82 0.26 0.48 1.62 1.12 1.96 0.06 0.50 1.46 0.96 0.14 0.08 0.06 0.02 0.20 -0.60 -0.32 0.28 0.02 -1.98 -1.56 0.42 0.16 -2.58 -2.08 0.50 0.16 -0.56 -0.50 0.06 1-3 1-6 3-6 2-4 2-5 4-5 Table VIII shows that the simulation correctly predicts the sign of the cross- correlation for each of the microphone pairs. The maximun offset difference is 0.5 ms which corresponds to a difference of 20%, and the average difference is 0.2 m, hence the 111 simulation model appears to be sufficiently accurate for differentiating speakers based on their positions. 5.2 Controlled Environment Experiments The objective of this next set of experiments is to evaluate the performance of our method to identify speakers in single-channel audio segments that were recorded under controlled conditions at the audio lab. There were two controlled experiments: The first experiment demonstrated the capability of the proposed method to identify two speakers based only on their location. The second experiment demonstrated the capability of the proposed method to identify multiple speakers independently of their spoken words. 5.2.1 Methodology The approach for these experiments is to physically emulate an open collaborative environment such as AOLME in which we record audio containing speech with a single microphone. Because the geometry of the acoustic scene is known, we can create a model that numerically follows this real scene, and then evaluate the performance of the proposed method using this model. Conversely, by having control over some of the parameters, such as the location of the sources, it is possible to experiment with different microphone arrays and absorptions values to evaluate the performance of different models. The controlled experiments used the same audio lab configuration and the same Pyroomacoustics model from the previous experiment. A change was made to the location 112 of the speakers to better fit the distance that will be used for the AOLME experiments. 5.2.1.1 Audio Lab and Model Preparation Table IX represents the audio lab configuration for this experiment, with the location of the loudspeakers and the recording microphone (MIC3). The audio was recorded using the Canon video camera connected with MIC3, and the video recording was saved in the internal SD card of the camera, the same way it is done with AOLME recordings. Ambient noise was injected using the CD player with background noise from one of the AOLME video sessions. 113 Table IX: Distribution of Microphones and Sources for Controlled Experiments. Sources Mics S1 S2 S3 S4 S5 S6 M1 M2 M3 M4 M5 M6 M7 CORNER 1 CORNER 2 Room CORNER 3 CORNER 4 EXTRUDE X 0.4 0.16 0.65 0.3 --- 0.98 0.6 0.65 0.6 0.6 0.6 0.55 0.6 0 0 1 1 Z 0.25 0.25 0.25 0.25 --- 1.5 0.025 0.025 0.025 0.025 0.025 0.025 0.025 -- -- -- -- Y 0.79 0.5 0.79 0.2 --- 0.4 0.6 0.55 0.55 0.45 0.5 0.55 0.65 0 0.8 0.8 0 2 The lab setup was translated into the Pyroomacoustics 2D model shown in Fig. 42. Noise is represented as “speaker” S6 and placed it in a relative location that resembles the location of the CD player. All sources and microphones kept the same Z coordinate value as in the previous experiment (0.25 m), except for the noise S6, which is located at Z= 1.5 m, to better represent the location of the CD player. This experiment (and for all subsequent experiments in this research), used a linear cross-type virtual microphone array with 7 elements, with the recording microphone 114 located at the center of the array. This type of microphone configuration is flexible and compact and allows its implementation in other models with different geometries. The separation between microphones in the array was set to 0.05 m, which is a distance commonly found in commercial microphone arrays, which is around 0.025 m to 0.040 m. The virtual microphone array is located at an offset position to the loudspeakers, avoiding any symmetry with them. This location should provide more distinctive cross-correlation results between microphones for better differentiation. Figure 42: 2D Model for Controlled Experiments. 5.2.1.2 Evaluation Criteria A common method to measure the performance of diarization systems is the Diarization Error Rate (DER) [81], [82]. The DER is defined as the fraction of the time that is not attributed correctly to a speaker or non-speech [38]. It can be calculated as the 115 summation of all errors as follows: 𝐷𝐸𝑅 = (cid:3007)(cid:3002)(cid:2878)(cid:3014)(cid:3036)(cid:3046)(cid:3046)(cid:2878)(cid:3016)(cid:3049)(cid:3032)(cid:3045)(cid:3039)(cid:3028)(cid:3043)(cid:2878)(cid:3004)(cid:3042)(cid:3041)(cid:3033)(cid:3048)(cid:3046)(cid:3036)(cid:3042)(cid:3041) (cid:3019)(cid:3032)(cid:3033)(cid:3032)(cid:3045)(cid:3032)(cid:3041)(cid:3030)(cid:3032) (cid:3013)(cid:3032)(cid:3041)(cid:3034)(cid:3035) (5.0), where FA is the length of False Alarms, Miss is the length missed speech segments, Overlap is the total length of overlapped speech, Confusion is the total length of misclassified segments, and the Reference Length is the total length of the audio reference. Overlap was not used in any of the tests. 5.2.2 “HAL 9000” Experiments The objective of this experiment is to demonstrate that the proposed method can identify speakers solely on the location of the speaker and independently of their speech characteristics. This was accomplished by using non-anechoic audio as the speech source, obtained from a raw video clip of a classic movie. Many of the software packages for speech processing found during this research provided some sort of test files for evaluation. One of these demos included a phrase from the classical 1967 movie “2001, a Space Odyssey”. In this movie, the human crew faced the rebellion of the spaceship’s computer, “HAL 9000”, which after some malfunction, attempts to kill the crew. The phrase “I’m sorry Dave, I’m afraid I can’t do that” is still very well-known nowadays when we discuss the implications of artificial intelligence taking over the control of critical missions. 116 This experiment used a clip of 128 seconds of duration where this famous phrase is spoken. This clip included the conversation between Dave, who is inside a space pod (Fig. 43, clip1), and HAL at the mothership (Fig. 43, clip2). The video scenes switched between the space pod and the spaceship, with voices coming from radio transmissions, or the inside of the spaceships, depending on the scene. There is also some background noise from the electronic equipment at the space pod. This clip can be downloaded from YouTube at https://www.youtube.com/watch?v=Wy4EfdnMZ5g&t=11s Figure 43: Video Clips of Dave (Clip1), and HAL (Clip2) SOURCE: Fandango Movie Clips. Two sets of experiments were performed: Experiment 1 was aimed to determine if there was any biasing on the results as the product of the location of the speakers. Experiment 2 evaluated the effects of training in the results. 117 5.2.2.1 Source Preparation and Editing This experiment played Dave’s and HAL’s voices independently at the loudspeakers. To do so, the YouTube video was converted into a single channel using Audacity ® version 2.4.2 [83] and saved as a MP4 48 kHz audio (See Fig 44 (a)). Then, using Audacity, the segments with voices of Dave and HAL were cut and pasted in two separate channels of a new stereo track (Fig. 44 (b)). The intervals with noise were converted into silence to allow the recording noise to come from an external source. Dave was placed on the right track and Hal was placed on the left track. The noise segments were copied and pasted into a separate audio track and burnt into a CD (Fig. 44 (c)). 118 Figure 44: Sources and Noise for HAL 9000 Experiment 119 With this configuration, it was possible to play Dave at loudspeakers 1 (S1) and 2 (S2), and HAL at loudspeakers 3 (S3) and 4 (S4), by using the loudspeaker switch. The noise was played at the CD player in a continuous loop and modeled as S5 or S6. 5.2.2.2 Ground Truth Recording Two sets of recordings were taken for this experiment. Set 1A consisted of playing the audio track using the loudspeakers 1 (Dave) and 3 (HAL). Set 1B consisted of playing the loudspeakers 2 (Dave) and 4 (HAL). The noise track was played in a continuous loop by a CD player located at the position of Source 6. The audio was recorded using the Canon video camera with the microphone located at the position of microphone 3 in Table IX. The recording was transferred to the computer for segmentation and training. Because the camera records audio in stereo mode and the code can only handle mono audio, the stereo track was converted into mono audio by removing the right channel. This conversion kept intact all the spatial information contained in the left channel. Using Audacity’s “convert to mono” feature would have mixed both channels, rendering the spatial information useless. Fig. 45 shows the final sets 1A and 1B of audio captured by the camera. 5.2.2.3 Training and Segmentation Both models for experiments 1 and 2 were trained with segments of speech from Dave and HAL, and a segment of noise. For experiment 1A, the model was trained with Dave as S1 using a 1.98 s long segment, and HAL as S3 with a 1.76 long segment, as shown in Fig 46. 120 Figure 45: Ground Truth Sets A and B for HAL 9000 Experiment. 121 Figure 46: Training Segments for HAL and Dave. 122 Noise was trained as S6 with a 2 s long segment. S2, S4, and S5 were set to silence. For experiment 1B, Dave was trained as S2, and HAL was trained as S4. S1, S3, and S5 were set to silence, and S6 was noise. All training segments were about the same length as in 1A. For Experiment 2, the model was trained with HAL as speaker S1 and S4, and Dave as speaker S2 and S3. The noise was trained as S6, and S5 was set to silence. The recorded audio was segmented in two different ways. For experiment 1, the VAD was set with a maximum length segment of 5 seconds, ending with an audio of 120.39 s after subtracting the dropped segments. For experiment 2, the length of the segments was limited to a maximum of 1.5 s. Frames of less than 500 ms were discarded for both experiments. 5.2.2.4 Testing and Results Table X shows the results of experiments 1 and 2. We can appreciate that the length of the segments has an influence on the DER. In this experiment, the longer the segments, the less the error. These results agree with our previous discussion on the amount of information needed for proper cross-correlation. It is important then to optimize the length of the segments so they can contain as much information as possible and maximize the matching probabilities with the training template. 123 Table X: DER Results for HAL 9000 Experiments. Exp Test No. Segments 1 2 A B -- 71 71 122 Properly Classified Segments 58 58 99 False Alarms (s) 5.68 1.22 11.31 Miss (s) Confusion (s) 1.46 3.01 1.2 2.88 2.32 10.98 DER 0.083 0.054 0.195 5.2.3 Multi-Speaker Identification Experiments The objective of the experiments in this section is to measure the performance of the proposed method to identify several speakers in single-channel recording, independently of the content of their speech. As with the previous experiments, the geometry of the room and the location of the speakers is known, allowing for models that represent more accurately the actual acoustic scene under analysis. The experiments in this section used the same lab setup and models of the “HAL 9000” experiment. The experiment was divided into four separate tests, that included two speakers and four speakers. Three of the experiments have two independent speakers repeating the same phrases, at different positions. The last experiment has four separate speakers at four different locations. 5.2.3.1 Source Preparation and Editing The speech sources for the experiments consisted of four different speakers, two male, and two females, sampled at a rate of 48 kHz. These sources were downloaded from 124 the Telecommunications and Signal Processing Laboratory of McGill University, database version 2 [84]. The lengths of these sources vary between 1.2 to 3 s, approximately. A total of four audio tracks were prepared for analysis. Tracks A, B, and C had two speakers, while track D had four. The sources were arranged into one stereo track, so they can be played at the loudspeakers LS1 and LS3, and then switched to be played at LS2 and LS4, as it was done with the HAL 9000 experiments. A small pause was inserted to allow for switching between loudspeakers. Table XI shows the structure of each of the audio sample. Each sequence in the table indicates the label of the active speaker, the loudspeaker playing the speech, and the label of the spoken phrase. For example, audio sample A contains two sequences, 1 and 2. Sequence 1 is played at loudspeaker S1 by speaker 1, speaking phrase “a”. Sequence 2 is played by speaker 2, loudspeaker S3, speaking phrase “b”. In samples B, C, and D, speakers repeat some of the phrases with the objective to demonstrate the ability of the proposed system to differentiate the speakers regardless of their speech content. 125 Table XI: Multi-Speaker Experiment Sequence Table Duration (s) Conditions 1 2 3 -- -- -- Loudspeaker S1 S3 Speaker Phrase 1 a 2 b Loudspeaker S2 S4 S2 Speaker Phrase 1 a 2 a 1 b Sequence 4 -- -- -- -- -- -- 5 -- -- -- -- -- -- 6 -- -- -- -- -- -- 7 -- -- -- -- -- -- 8 -- -- -- -- -- -- 9 -- -- -- -- -- -- A 4.78 B 6.05 Loudspeaker S1 S1 S3 S1 S2 S4 S4 S4 S4 C 39.58 Speaker Phrase 1 a 1 b 2 c 1 d 1 e 2 f 2 g 2 h 2 i 10 -- -- -- -- -- -- -- -- -- l e p m a S Loudspeaker S1 S3 S1 S3 S3 S2 S2 S4 S2 S4 D 27.73 Speaker Phrase 1 a 2 a 1 b 2 b 2 c 3 d 3 e 4 f 3 g 4 h 5.2.3.2 Ground Truth Recording The audio was captured the same way as in the “HAL 9000” experiments, using the Canon video camera and saving the video recording in the camera’s internal SD storage. Ambient noise was injected by paying background noise using the CD player, as it was done for the “HALL 9000” experiments. The background noise was extracted from one of the AOLME video recordings. As with the “HAL 9000” experiments, the stereo recording from the video camera was converted into single-channel audio by removing the right channel, before the analysis. 126 5.2.3.3 Training and Segmentation The training was done with segments that had a maximum length of 1.5 s for each of the speakers, plus 1.5 s segment of noise. The custom VAD was used for segmentation. The number of segments produced for each of the audio tracks varied as is shown in the results table. 5.2.3.4 Testing and Results Testing was conducted in the same manner as the “HAL 9000” experiments. The results for each of the segments are shown in Table XII. Table XII: Controlled Environment Experiments Diarization Error Rate Results Audio Sample No. Speakers No. Segments A B C D 2 2 2 4 9 15 116 37 Properly Classified Segments 7 12 98 27 False Alarms 0 0 10 2 Miss Confusion DER 2 2 0 0 0 1 8 8 0.19 0.19 0.12 0.27 Table V shows that the DER is not more than 0.27 in the worst case. These results are comparable or better than DER results from methods using databases and neural networks [85] 127 5.3 AOLME Experiments The controlled environment experiments demonstrated that the proposed method could identify speakers in single-channel recordings. These experiments analyzed audio samples that featured organized speech (one speaker at a time), where the speakers are well separated from each other (no overlapping between speakers). The objective of the AOLME experiments in this section is to evaluate the performance of the proposed method to identify speakers in single-channel audio recordings from videos of noisy multi-speaker collaborative environments. This section evaluates the process of selection of the AOLME videos for the experimental analysis, and discusses the models employed for the analysis. The analysis of the audios will follow the same approach as the previous experiments. 5.3.1 Evaluation and Selection of AOLME Videos There are several hundred hours of AOLME video recordings available for analysis but, because of the experimental nature of this research work, it was necessary to select videos that met certain characteristics that facilitate the preparation of the models and the setup of the experiments. The models used in the previous experiments proved to perform well, and for this reason, it was necessary to search for AOLME videos with similar geometric characteristics as these models, i.e., the participants were in similar places as the speakers in the model from our previous experiments. The selection consisted of four videos with 2, 3, 4 and 5 participants from the library of videos. The videos were 128 approximately 3 minutes long each. Fig. 47 shows frames from these videos with 2 participants (a), 3 participants (b), 4 participants (c), and 5 participants (d). As was done in the previous experiments, the stereo audio track for each video was extracted and converted into a 48 kHz single channel by removing the right channel. Figure 47:Video Clips for AOLME Experiments. 5.3.2 Model Preparation The model used for this experiment followed the same geometry as the previous experiments, with the width of the table adjusted to 1.8 m to fit the AOLME scene more accurately. Instead of generating separate models for each of the videos, the model had all 129 three speakers for all the experiments. As previously done, the locations of the absent speakers were turned off by training with a silence segment. Fig. 48 shows the 2D Pyroomacoustics model and Table XIII shows the locations of the speakers and the microphones. Figure 48: 2D Model for AOLME Experiments. 130 Table XIII: Location of Speakers and Microphones for AOLME Experiments. s r e k a e p S s c i M m o o R S1 S2 S3 S4 S5 S6 M1 M2 M3 M4 M5 M6 M7 CORNER 1 CORNER 2 CORNER 3 CORNER 4 EXTRUDE X 0.40 0.01 1.00 0.40 1.20 2.40 0.75 0.85 0.80 0.80 0.80 0.80 0.80 0.00 0.00 2.50 2.50 Y 1.79 0.80 1.79 0.01 0.01 1.00 1.00 1.00 1.00 1.05 0.95 1.10 0.90 0.00 1.80 1.80 1.80 2.00 Z 0.25 0.25 0.25 0.25 0.25 1.50 0.03 0.03 0.03 0.03 0.03 0.03 0.03 -- -- -- -- 5.3.3 Training and Segmentation The same training and segmentation principles were used as in the previous experiments. Training used a 1.5 to 2 s long sample of each of the participants, plus a similar length segment of background noise. Because the same model was used for all participants, non-active speakers were trained with a silence segment of 2 s duration. Table XIV shows the speaker assignment for each of the experiments. 131 Table XIV: Speaker Assignment for AOLME Experiments. Audio Sample A B C D Speaker Assignment S2 S1  Silence    Silence   S3 Silence Silence  S5 S4  Silence  Silence      S6 Noise Noise Noise Noise The Ground Truth for each audio was segmented using the VAD, discarding segments with less than 0.5 s duration, and limiting the length of the segments to 1.5 s maximum. The total number of segments for each sample is shown in the results table. 5.3.4 Testing and Results The same type of analysis was applied as in the previous experiments. Table XV (a) shows an example of the Cross-Correlation results of analyzing one segment of Audio Sample B. Tables VII (b), (c), and (d) show the training CC tables with the score of each possible speaker. Each match is represented by a zero (0). In this case, the segment corresponds to speaker 2. 132 Table XV: CC Tables for AOLME Experiment. 1-2 1-3 1-4 1-5 1-6 1-7 2-3 2-4 2-5 2-6 2-7 3-4 3-5 3-6 3-7 4-5 4-6 (a) Microphone Cross Correlation. Unknown Segment -64 -65 11 63 -45 -75 89 72 -96 -236 137 133 -100 -236 306 133 12 -75 90 97 -68 0 -28 0 18 -30 56 8 -74 -103 203 57 -276 -103 92 57 71 -11 79 21 -9 65 -34 -63 -72 -69 -41 42 -41 -69 35 42 57 19 21 1 -27 75 -123 -72 14 0 82 0 1-2 1-3 0 91 0 -53 11 0 4 0 1-2 1-3 0 -245 -1 0 0 0 4 0 1-4 0 -83 -145 0 1-4 0 21 -145 12 1-5 -14 -83 197 0 1-5 -14 21 218 12 (b) Microphone Cross Correlation. Training Speaker 1. Score: 25 4-5 -127 0 21 0 2-6 2-7 -5 -9 53 -1 17 -11 0 -1 3-4 3-5 8 0 -11 1 1-6 1-7 2-3 4 18 5 0 0 110 1 6 -5 1 0 12 2-5 -139 0 5 -1 3-7 -4 0 -2 -4 3-6 0 1 1 -1 2-4 4 0 192 -1 0 0 2 1 (c) Microphone Cross Correlation. Training Speaker 2. Score: 32 4-5 5 0 0 0 2-6 2-7 -5 -13 0 0 1 5 0 0 3-4 3-5 14 -1 0 0 3 -11 0 0 1-6 1-7 2-3 4 18 5 0 0 0 1 -9 -5 4 0 0 2-5 -270 0 5 -1 3-7 -4 1 -11 -4 3-6 0 1 3 -1 2-4 204 0 419 -1 (d) Microphone Cross Correlation. Training Speaker 3. Score: 21 1-2 13 0 -5 0 1-3 -1 -1 14 -209 1-4 5 -121 -77 0 1-5 -14 -121 202 0 1-6 1-7 2-3 -6 13 5 -194 0 0 0 -5 -5 4 0 24 2-4 -198 0 182 11 2-5 2-6 2-7 -5 -9 -72 0 -2 0 -155 5 5 0 11 11 3-4 3-5 0 7 0 2 1 7 3 2 3-6 1 191 3 -3 3-7 3 1 -3 209 4-5 4 0 6 0 133 285 98 18 -36 4-6 56 0 2 1 4-6 394 5 1 1 4-6 -9 0 -31 1 s r e k a e p S s r e k a e p S s r e k a e p S s r e k a e p S 1 2 3 6 1 2 3 6 1 2 3 6 1 2 3 6 4-7 -58 236 -94 -133 4-7 -333 83 0 0 4-7 -333 -21 0 -12 4-7 -267 121 -12 0 5-6 294 98 -8 -36 5-6 206 0 5 1 5-6 208 5 0 1 5-6 201 0 5 1 5-7 3 236 -107 -133 5-7 -1 83 37 0 5-7 0 -21 -202 -12 5-7 -5 121 16 0 6-7 -80 75 -125 -97 6-7 4 -110 -15 -12 6-7 0 0 0 0 6-7 4 0 -12 -24 Table XVI shows the results of the analysis of all Audio Samples, with the respective DER for each experiment. Table XVI: Classification Results for AOLME Experiments. Audio Sample Sample Duration (s) No. Speakers No. Segments Properly Classified Segments False Alarms Miss Confusion DER A B C D 244 256 381 257 2 3 4 5 311 328 489 339 281 302 426 284 5 8 10 12 10 10 25 15 15 8 28 28 0.095 0.079 0.12 0.16 5.4 Comparison with Other Methods The final set of experiments focus on comparing our proposed method against Google’s and Amazon AWS. Google’s and Amazon AWS were two of the cloud-based speech processing services introduced in the background section of this dissertation. Microsoft Diarization service was in the process of being updated by the time this dissertation was written and, therefore, it was not possible to run any experiment with it. 5.4.1 Methodology for Comparison The diarization services provided by Google and Amazon differ from the proposed method in three aspects. First, they do not require a sample of audio for training. Second, the audio samples to diarize need to be of a minimum duration of 4 s, approximately. Third, 134 their output does not provide a label of the active speaker, but rather a set of text transcripts that contain the speech segment, the abstract speaker label (e.g., speaker 0, speaker 2), the active time of the speaker on the transcript segment, and the confidence rate. Given these constraints, the only fair comparison criteria are to manually measure each speaker’s ground truth active time manually and compare these times with the results of the analysis by all three methods. It was necessary to add a section of code to the proposed method to measure the length of each of the segments that are already classified and totalize the time for the same speaker plus noise. 5.4.2 Selection, Preparation, and Ground Truth Measurements of Videos for Analysis The analysis consisted of a total of 8 AOLME videos containing 2, 3, 4, and 5 speakers. The duration of each video was limited to a maximum of 3 minutes. The audio from each video was extracted using Audacity and downshifted to 16 kHz for upload to Goggle and Amazon. The audio files for our methods were sampled at a rate of 48 kHz. Each speaker’s active time from the ground truth audio was measured using a stopwatch. In some of the AOLME videos, it was difficult to assess this time due to several speakers being active simultaneously. In these cases, each speaker’s time was recorded by listening to his/her voice and watching his/her lip movement on video, even if their speech overlapped at any moment. 135 5.4.3 Training and Segmentation The system was trained with audio samples of about 1.8 s long from each speaker and noise, using a VAD with a maximum segment length of 1.2 s. All segments with a duration of less than 0.5 s were dropped. There was no need for training on Google or Amazon; these systems trained by using the uploaded audio and their databases. 5.4.4 Testing and Analysis Each of the audio files from the videos was analyzed using the modified code that totalizes each speaker’s time, with no other additional steps. For Amazon and Google, the audio was uploaded to the cloud. Because both Amazon and Google’s methods return only abstract labels, the output transcriptions of each of the speakers were used to manually match the identity of the speaker on each segment, noting that both Amazon and Google label the first active speaker they detect as “speaker 0”. 5.4.5 Results Table XVII shows the results of this experiment, with the percentage error highlighted in light blue. The error was calculated using (5.1). Percent error = estimated time − true time true time ∗ 100 (5.1). 136 Table XVII: Experimental Comparison Between Methods. Audio Sample No. of Speakers Speaker 1 2 3 4 5 6 7 2 2 3 3 4 4 5 8 5 S1 S2 S1 S2 S1 S2 S3 S1 S2 S3 S1 S2 S3 S4 S1 S2 S3 S4 S1 S2 S3 S4 S5 S1 S2 S3 S4 S5 Amazon AWS Google Cloud 1.95 45.25 4.85 8.24 40.88 47.08 31.51 61.41 24.01 3.72 40.39 7.53 Time Error (s) % 94.52 19.21 74.47 170.60 120.90 12.99 45.46 152.14 9.88 64.67 143.74 40.21 0.00 100.00 106.36 61.79 37.67 36.19 0.00 100.00 52.19 84.48 8.93 20.05 0.00 100.00 0.00 100.00 78.70 221.49 36.95 65.84 100.00 0.00 250.00 38.05 3070.83 100.00 0.00 60.54 28.30 88.77 6.74 100.00 0.00 13.82 39.24 60.04 13.31 100.00 0.00 10.92 100.00 0.00 31.65 53.73 53.13 21.67 100.00 0.00 44.00 15.63 17.61 46.22 17.52 56.02 42.23 Error Time % (s) 8.63 127.10 100.00 0.00 31.40 73.40 66.59 269.33 66.59 1009.83 50.80 10.29 80.20 31.39 0.00 0.00 8.30 35.00 94.30 53.59 29.19 15.29 3.30 5.09 24.90 0.00 54.70 46.60 6.29 29.59 7.49 11.20 29.59 50.45 11.12 22.00 13.49 100.00 100.00 25.69 17.20 27.71 118.91 31.01 40.62 175.00 74.86 64.01 100.00 26.86 279.79 55.95 14.38 199.60 26.46 37.93 Proposed Method Ground Truth Time Error Time (s) % (s) 14.54 99.99 117.00 25.80 34.62 27.52 5.61 107.00 113.00 23.44 18.03 30.01 20.69 244.83 6.00 102.52 100.52 13.45 68.93 25.38 15.30 41.61 14.69 68.23 91.57 25.39 13.28 27.69 4.20 7.99 64.53 10.71 48.86 10.93 18.80 42.05 3.60 22.27 27.54 9.26 65.74 27.66 10.86 28.29 11.17 42.27 73.84 24.48 22.28 25.75 1.20 20.25 69.19 9.41 43.12 12.27 14.28 34.56 2.50 15.23 47.67 137 Table XVIII shows the average error for 2, 3, 4, and 5 speakers, as well as the total average error for each method. Table XVIII: % Average Error for All Three Methods. No. of Speakers 2 3 4 5 Total Proposed Method 18.99 57.67 58.21 29.11 42.10 Amazon AWS 88.74 67.14 470.34 65.44 184.82 Google Cloud 102.34 201.15 67.02 87.98 108.29 The results presented in Tables XVII and XVIII show a substantial reduction in the achieved error rate. More specifically, error reduction ranges from 50% to 87%. The color codes used in Table XVII emphasize the results of this experiment. The red highlighting denotes cases of failures where we have a speaker that was completely missed, or the estimated talking time of the speaker had more than a 100% error (e.g., an over-estimating speaker talking time). Out of 28 possible speakers across all examples, Amazon AWS gave failing results for 14 cases (50%), Google cloud gave failing results for 10 cases (36%), while the proposed method gave failing results for 2 cases (7%). It is interesting to notice that the proposed method never failed to detect a speaker (0% error), while Amazon AWS could not detect any talking time for 10 cases (36%). Google cloud failed to detect any talking time for 4 cases (14%). Also, there are failure cases for all 8 samples for Amazon 138 AWS and Google Cloud. In contrast, for the proposed method, there are 2 samples with examples of over-estimation, with 6 samples being free of dramatic failures. Teal highlighting denotes cases where the total estimated speaking time gave 20% or less error. Based on this criterion, both AWS and Google Cloud gave satisfactory results in 5 cases (18%) versus 11 cases (39%) for the proposed method. 139 Chapter 6. Summary, Conclusions, and Future work This dissertation presented a method for speaker diarization and identification using virtual microphones and cross-correlation patterns. The proposed method identifies speakers in single-channel recordings taken in noisy collaborative environments, such as classrooms and educational workshops. The method gave an error rate that was over 50% less on average than other available diarization methods when subject to the same testing environments. In contrast with other methods that are considered state-of-the-art, the proposed method requires minimal training and no databases, making it applicable in situations where it is not possible to gather clean speech samples. The background section of this dissertation presented similar research works on speaker diarization and identification based on microphone arrays. Although some of these works included virtual microphone arrays, none of them approached a full virtual array simulation from a single microphone recording. Given the unprecedented focus on Deep Learning methods, alternative approaches are avoided, limiting the number of researchers interested in pursuing them. Yet, the proposed methodology clearly outperformed commercial Deep Learning methods and demonstrated some of their limitations due to their needs for large training datasets. The method presented in this dissertation offers an alternative for educational researchers that are involved with collaborative environments and depend mostly on the 140 analysis of data provided by video recordings. The work in this dissertation showed that other available methods perform poorly under these environments when determining who speaks, when, and for how long. The deficiencies presented by these methods are even more prominent when the participants are from underrepresented groups from which large training databases may not exist. The proposed method demonstrated a significant performance improvement by capitalizing on real video information of the environment under analysis, rather than depending on unrelated training data. Also, by no requiring previous speaker enrollment, this method opens the possibility of analysis of a wide variety of video data that may not have been recorded with the known intention of posteriors analysis. The dissertation method constitutes more of a proof of concept than a fully operational method. The success of the proposed method is due to the possibility of simulating acoustic wave propagation, including speech. Even though this modeling can be complex, we have now powerful personal computers to execute the calculations required by the signal processing algorithms. Furthermore, the code for the simulations is available from large repositories that contain open-source libraries ready for implementation; nevertheless, there is work that needs to be done to address some of the weaknesses observed so far, such as it is the case where participant speakers move and change their original locations, and when they “invade” other’s speakers’ physical location. Under this area, it is possible to eventually adapt the methods from the research work done at the ivPCL lab regarding object and subject tracking. The location of the speakers and the general geometry of the room could be dynamically modified in the models based on the 141 information from video data, thus improving the error rate. Also, the experiments only considered one type of microphone array, leaving open the question of the performance of other types of arrays, such as circular or even volumetric. In addition, the simulation version available during the development of this dissertation had some limitations that impacted the accuracy of the models. Pyroomacoustics released a new version that includes improvements to the models’ parameters, such as physical modeling of room absorption, reverberation modeling, and multi-pattern microphone simulation. Finally, the method depends on proper audio segmentation and final classification. Most of the misclassifications in the method were the product of improper pre-segmentation and sub- optimal classification. A more sophisticated classifier using machine learning or neural networks would help improve the overall performance. It is possible also to apply clustering classification for unsupervised identification of the speakers. Finally, the method could be extended to support other applications of speech processing, as it can be incorporated as a front end or pre-processor. For example, the method can be used to improve the accuracy of spatial filters for speech enhancement or speaker separation from mixtures. The parameters of the spatial filters can be better determined by estimating the location of the speaker and then optimizing the parameters for that location. 142 Appendix A: Pyroomacoustics Scripts This section describes the two Python scripts that call the Pyroomacoustics libraries to generate the room geometry parameters, calculate the RIRs, and emulate the virtual microphones. a) Room Geometry Generator: This script accepts the room dimensions and locations of the sources and virtual microphones and generates the 2D and 3D geometric models. The room geometry is saved as a set of .txt files that contains the geometry arrays. This script runs under a Jupyter Notebook. #Location of Sources and Microphones Source6=[0.98,0.4] Source6_3D=[0.98,0.4,0.98] Mic_X = [0.6,0.65,0.6,0.6,0.6,0.55,0.6] Mic_Y = [0.6,0.55,0.55,0.45,0.5,0.55,0.65] Mic_Z = [0.25,0.25,0.01,0.25,0.25,0.25,0.25] #Add room room = pra.Room.from_corners(corners, fs=fs) #Location of Microphones Array R = np.array([Mic_X, Mic_Y]) # [[x], [y], [z]] #Add source to 2D room room.add_source(Source1, signal=s1) . . room.add_source(Source6, signal=s6) room.add_microphone_array(pra.MicrophoneArray(R, room.fs)) #Execute Location room = pra.Room.from_corners(corners, fs=fs) room.extrude(1.0) 143 R = np.array([Mic_X, Mic_Y, Mic_Z]) # [[x], [y], [z]] room.add_microphone_array(pra.MicrophoneArray(R, room.fs)) room.add_source(Source1_3D, signal=s1) . . room.add_source(Source4_3D, signal=s4) #Save Geometry np.savetxt(r'C:\Users\User\Desktop\PhD Folder\Dissertation\Experiments\Model_Estimation\Room_parameters\corner s_array.txt',corners[:,:],delimiter=',', fmt='%f') . . . np.savetxt(r'C:\Users\User\Desktop\PhD Folder\Dissertation\Experiments\Model_estimation\Room_parameters\mic_ar ray.txt',R[:,:],delimiter=',', fmt='%f') b) Pyroomacoustics Virtual Microphone Simulation Script This script is used twice to first calculate the RIR from the model sources to the virtual microphones, and then again to emulate the signal at the virtual microphones using the estimated sources. This script is called within LabVIEW, and its outputs are saved in .txt files. #Setup Python import numpy as np import matplotlib.pyplot as plt from scipy.io import wavfile from scipy.signal import fftconvolve import pyroomacoustics as pra #Define Variables Abs = 0 max_o = 0 room_extrude = 0 corners_array = 0 Source1 = 0 . . Source6 = 0 mic_array = 0 #Define model def model_generation(): 144 #Delimit the corners of the room corners = np.array(corners_array).T # [x,y] room = pra.Room.from_corners(corners) room.extrude(room_extrude) #Read Sources fs, s1 = wavfile.read(r""C:\.....) . . fs, s6 = wavfile.read(r""C:\.....) room = pra.Room.from_corners(corners, fs=fs) #Add microphone array R = np.array(mic_array) # [[x], [y], [z]] room.add_microphone_array(pra.MicrophoneArray(R, room.fs)) # set max_order for RIR room = pra.Room.from_corners(corners, fs=fs, max_order=max_o, absorption=Abs) #Set Extrusion room.extrude(room_extrude) #Add source arrays and microphones Source1_3D=np.array(Source1) #Source 1 room.add_source(Source1_3D, signal=s1) room.add_microphone_array(pra.MicrophoneArray(R, room.fs)) #Compute image sources room.image_source_model(use_libroom=True) room.compute_rir() #Save Data np.savetxt(r'C:\....,room.rir[0][0],delimiter=',', fmt='%f') #Data Mic data_mic=room.mic_array.signals[0,:] np.savetxt(r'C:\....,room.rir[0][0],delimiter=',', fmt='%f') data_mic=room.mic_array.signals[1,:] np.savetxt(r'C:\....,room.rir[0][0],delimiter=',', fmt='%f') data_mic=room.mic_array.signals[2,:] np.savetxt(r'C:\....,room.rir[0][0],delimiter=',', fmt='%f') data_mic=room.mic_array.signals[3,:] np.savetxt(r'C:\....,room.rir[0][0],delimiter=',', fmt='%f') #Data Mic5 data_mic=room.mic_array.signals[4,:] np.savetxt(r'C:\....,room.rir[0][0],delimiter=',', fmt='%f') #Data Mic6 data_mic=room.mic_array.signals[5,:] np.savetxt(r'C:\....,room.rir[0][0],delimiter=',', fmt='%f') data_mic=room.mic_array.signals[6,:] np.savetxt(r'C:\....,room.rir[0][0],delimiter=',', fmt='%f') #Reepeat for all sources . . . return 145 Appendix B: LabVIEW Sub-Vis a) Room Parameters Reader Figure 49: Room Parameters Reader Inputs and Outputs Figure 50: Room Parameters Reader Front Panel. 146 Figure 51: Room Parameters Reader Block Diagram. 147 b) Room Model Generator Figure 52: Room Model Generator Icon. Figure 53: Room Model Generator Front Panel. 148 Figure 54: Room Model Generator Block Diagram. 149 c) Source Estimator This Sub- VI is too complex to display its source block diagram. Instead, a simple functional block diagram is shown. Figure 55: Source Estimator Icon. Figure 56: Source Estimator Front Panel. 150 Figure 57: Source Estimator Simplified Block Diagram. d) Cross-Correlation Model Calculator Figure 58: Cross-Correlation Model Calculator Icon. 151 Figure 59: Cross-Correlation Model Calculator Front Panel. 152 Figure 60: Cross-Correlation Model Calculator Block Diagram. 153 Figure 61: Cross-Correlation Model Calculator. Cross-Correlator Sub-VI. Notes on this sub-VI: The cross-correlation results are indicated by the index where the max cross-correlation occur. This method makes the results independent of the sampling frequency. Also, this sub-VI truncates the largest input to make both input files the same size as the smallest one. e) Model Classifier Figure 62: Model Classifier Icon with Inputs and Outputs. 154 Figure 63: Model Classifier Front Panel. 155 f) Multi-Function Convolution and Correlator Visualizer Figure 64: Multi-Function Convolution and Correlator Visualizer Front Panel. 156 Figure 65: Multi-Function Convolution and Correlator Visualizer Diagram. 157 Appendix C: Audio Lab Equipment Specifications a) Microphone Equipment: Audio-Technica ATR3350xIs  Element: Condenser  Polar Pattern: Omnidirectional  Frequency Response: 50 – 18,000 Hz.  Sensitivity: -54 db.   Power Source: Battery Type: LR44. Impedance: 1,000 ohms Comica CVM-V020  Transducer: Back Electrets Condenser  Directivity: Omnidirectional  Frequency Range: 100Hz ~ 12KHz  THD: ≤1%  Sensitivity: 35dB ±3dB  Signal/Noise Ratio: ≥60dB  Power Source: 48V Phantom Powered 158 Excelvan 700  Polar Pattern: Uni-directional  Frequency Response: 20Hz-20kHz  Sensitivity: 45dB±1dB  Output Impedance:1500Ω±30%(at 1kHz)  Load impedance: ≥1000 Ω  Equivalent Noise level: 16dBA  Power Source: 48V phantom power supply b) Audio Processing Equipment TASCAM Model US-16x08  Frequency response: o LINE OUT(BALANCED) o 44.1k/48k Hz 20Hz to 20kHz, ±0.3dB(JEITA) o 88.2k/96k Hz 20Hz to 40kHz, ±0.3dB(JEITA) 100dB or more 100dB or more  THD 0.008% or less  S/N ratio  Crosstalk  EIN  Sampling frequency 44.1k/48k/88.2k/96k Hz  Quantization bit rate 16/24-bit –125dBu or less 159  Analog audio inputs: o MIC IN(IN 1-8)  Connector XLR-3-31 (1: GND, 2: HOT, 3: COLD), 2.4kΩ BALANCED  Input impedance  Nominal input level  GAIN: MAX –68dBu (0.0003Vrms)  GAIN: MIN –12dBu (0.195Vrms)  Maximum input level +8dBu (1.947Vrms)  Gain 56dB o LINE IN (IN 9-10)  Connector 1/4"" (6.3mm) TRS-jack (T: HOT, R: COLD, S: 10kΩ GND), BALANCED  Input impedance  Nominal input level  GAIN: MAX –41dBu (0.0069Vrms)  GAIN: MIN +4dBu (1.228Vrms)  Maximum input level +24dBu (12.182Vrms)  Gain 45dB AIWA Stereo Audio Amplifier  Power output: 80 watts per channel into 8Ω (stereo)  Surround output: 80W (front), 80W (center), 80W (rear)  Frequency response: 20Hz to 20kHz  Total harmonic distortion: 1%   Output: 300mV (line) Input sensitivity: 2.5mV (MM), 300mV (line) 160  Speaker load impedance: 8Ω (minimum) c) Loudspeakers Polk Audio RM6751  Power Range: 20- 100 W  Frequency Response: 40 Hz – 24 kHz  Sensitivity: 89 @2.83Vrms dB  Impedance (Ohms): 8 161 References [1] S. S. Tirumala, S. R. Shahamiri, A. S. Garhwal, R. Wang, “Speaker identification features extraction methods: A systematic review”. Expert Systems with Applications, vol. 90, pp. 250-271, 2017. doi: 0957-4174, https://doi.org/10.1016/j.eswa.2017.08.015. [2] J. Brownlee, “Impact of Dataset Size on Deep Learning Model Skill And Performance Estimates,” Deep Learning Performance, machinelearningmastery.com, para.4, Jan. 2, 2019. [Online]. Available: https://machinelearningmastery.com/impact-of-dataset-size-on-deep-learning-model- skill-and-performance-estimates/. [3] J. Yoon and S. O. Arik “Estimating the Impact of Training Data with Reinforcement Learning,” Cloud AI Team Google Research, googleblog.com, para. 2, Oct. 28, 2020. [Online]. Available: https://ai.googleblog.com/2020/10/estimating-impact-of- training-data-with.html. [4] A. Koenecke A. Nam, E. Lake, J. Nudell, M. Quartey, Z. Mengesha, C. Toups, J.R. Rickford, D. Jurafsky S. Goel, “Racial disparities in automated speech recognition,” Proceedings of the National Academy of Sciences of the United States of America, April 7, 2020, 117(14):7684-7689, [Online serial]. Available: https://www.pnas.org/content/117/14/7684. [5] J. Martin, K.Tang, “Understanding Racial Disparities in Automatic Speech Recognition: The Case of Habitual “be”. Presented at 21st International Conference on Speech Processing and Applications, Shanghai, China, 2020. [6] R. Gupte, S. Hawa, and R. Sonkusare, “Speech recognition using cross correlation and feature analysis using mel-frequency cepstral coefficients and pitch,” In Proc. 2020 IEEE International Conference for Innovation in Technology (INOCON), 2020, pp. 1-5. [7] G. Ekim, N. Ikizler, A. Atasoy, and I. H. Cavdar, “A speaker recognition system using by cross correlation,” In Proc. 2008 IEEE 16th Signal Processing, Communication and Applications Conference, 2008, pp. 1-4. 162 [8] The University of New Mexico, “AOLME: Advancing Out-of-school Learning in Mathematics and Engineering”. [Online]. Available: https://aolme.unm.edu/. [9] University of New Mexico’s Image and Video Processing and Communications Lab (ivPCL). [Online]. Available: https://ivpcl.unm.edu/ [10] C. J. Darsey, “Hand Detection in Collaborative Learning Environments”. The University of New Mexico, 2018. [11] Teeparthi S., “Long-term Video Object Detection and Tracking in Collaborative Learning Environments,” Fall 2021 (with distinction). She was funded through NSF. [12] Teeparthi, S., Jatla, V., Pattichis, M.S., Celedón-Pattichis, S., and LópezLeiva, C., “Fast Hand Detection in Collaborative Learning Environments,” The 19th International Conference on Computer Analysis of Images and Patterns (CAIP), pp. 445-454, 2021. [13] Jacoby A. R., “Context-Sensitive Human Activity Classification in Video Utilizing Object Recognition and Motion Estimation,” Spring 2018. [14] Jatla, V., Teeparthi, S., Pattichis, M.S., Celedón-Pattichis, S., and LópezLeiva, C., “Long-term Human Video Activity Quantification of Student Participation,” in 2021 Asilomar Conference on Signals, Systems, and Computers. [15] Eilar, C., Jatla, V., Pattichis, M. S., Celedón-Pattichis, S., & LópezLeiva, C. A., “Distributed Video Analysis for the Advancing Out of School Learning in Mathematics and Engineering Project,” 2016 Asilomar Conference on Signals, Systems, and Computers, pp. 571-575, 2016. [16] Shi, W., Pattichis, M.S., Celedón-Pattichis, S., and LópezLeiva, C., “Dynamic Group Interactions in Collaborative Learning Videos,” 2018 Asilomar Conference on Signals, Systems, and Computers, in press, pp. 1528-1531, 2018. [17] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland, O. Vinyals, ""Speaker diarization: A review of recent research,"" IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 2, pp. 356-370, 2012. [18] M. Alam, M.D. Samad, L. Vidyaratne, A. Glandon, K.M. Iftekharuddin,” Survey on Deep Neural Networks in Speech and Vision Systems,” Neurocomputing, Volume 417, 2020, pp. 302-321. 163 [19] D. Sztahó, G. Szaszák, A. Beke, ‘‘Deep learning methods in speaker recognition: A review,’’ Periodica Polytechnica Electrical Engineering and Computer Science, vol. 65, no. 4, pp. 310–328, Jan. 2021. [Online]. Available: http://arxiv.org/abs/1911.06615. [20] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, J. Borgstrom, L. Paola García-Perera, F. Richardson, R. Dehak, P. A. Torres- Carrasquillo, N. Dehak, “State-of-the-art speaker recognition with neural network embeddings in NIST SRE18 and Speakers in the Wild evaluations,” Computer Speech & Language, vol. 60, March, 2020. [21] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey and S. Khudanpur, ""X-Vectors: Robust DNN Embeddings for Speaker Recognition,"" 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5329- 5333. [22] X. A. Miró, “Robust speaker diarization for meetings,” Ph.D. thesis, Speech Processing Group Department of Signal Theory and Communications Universitat Politècnica de Catalunya, Barcelona, 2006. [23] N. Mitianoudis and M. E. Davies, ""Using beamforming in the audio source separation problem,"" Seventh International Symposium on Signal Processing and Its Applications, 2003. Proceedings., 2003, pp. 89-92 vol.2, doi: 10.1109/ISSPA.2003.1224822. [24] U. Klein and Trình Quốc Võ, ""Direction-of-arrival estimation using a microphone array with the multichannel cross-correlation method,"" 2012 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT), 2012, pp. 000251-000256, doi: 10.1109/ISSPIT.2012.6621296. [25] T. Padois, “Acoustic source localization based on the generalized cross-correlation and the generalized mean with few microphones”. J Acoust Soc Am. 2018. [26] S. Pasha and C. Ritz, ""Informed source location and DOA estimation using acoustic room impulse response parameters,"" 2015 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT), 2015, pp. 139-144, doi: 10.1109/ISSPIT.2015.7394316. [27] S. Tervo, J. Pätynen, and T. Lokki, “Acoustic reflection localization from room impulse responses,” Acta Acustica united with Acustica, vol. 98, no. 3, pp. 418-440, 2021. 164 [28] M. Hu, P.P. Parada, D. Sharma, S. Doclo, T.V Waterschoot, M. Brookes, P.A. Naylor, ""Single-channel speaker diarization based on spatial features,"" In Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2015, pp. 1-5. [29] P. P. Parada, D. Sharma, P. A. Naylor, “Non-intrusive estimation of the level of reverberation in speech,” in Proc. IEEE International Conf. on Acoustics, Speech and Signal Processing (ICASSP), Florence, Italy, May 2014, pp. 4718–4722. [30] D. Vijayasenan, F. Valente, and H. Bourlard, “Multistream speaker diarization beyond two acoustic feature streams,” in Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), Dallas, TX, USA, Mar. 2010, pp. 4950–4953. [31] T. Yoshioka, Z. Chen, D. Dimitriadis, W. Hinthorn, X. Huang, A. Stolcke, M. Zeng, “Meeting transcription using virtual microphone arrays,” Microsoft Technical Report MSR-TR-2019-11, July 2019. [32] H. Katahira, N. Ono, S. Miyabe, T. Yamada, S. Makino, “Nonlinear speech enhancement by virtual increase of channels and maximum SNR beamformer,” EURASIP Journal on Advances in Signal Processing, 2016, issue 1, article 11, pp. 1- 8, 2016. [33] G. Del Galdo, O. Thiergart, T. Weller and E. A. P. Habets, ""Generating virtual microphone signals using geometrical information gathered by distributed arrays,"" 2011 Joint Workshop on Hands-free Speech Communication and Communication and Microphone Arrays, Edinburgh, pp. 185-190, 2011. [34] A. Izquierdo, J. Villacorta, L. del Val, L. Suárez, and D. Suárez, “Implementation of a Virtual Microphone Array to Obtain High Resolution Acoustic Images,” Sensors, vol. 18, no. 2, p. 25, Dec. 2017. [35] Tapia, L.S., Gomez, A., Esparza, M., Jatla, V., Pattichis, M.S., Celedón-Pattichis, S., and López-Leiva, C., “Bilingual Speech Recognition by Estimating Speaker Geometry from Video Data,” The 19th International Conference on Computer Analysis of Images and Patterns (CAIP), pp. 79-89, 2021. [36] Siemens Simcenter “Sound Fields: Free versus Diffuse Field, Near versus Far Field” [Online]. Available: https://community.sw.siemens.com/s/article/sound-fields-free- versus-diffuse-field-near-versus-far-field [37] I. Tashev, “Sound Capture and Processing: Practical Approaches”. Chichester, West Sussex: John Wiley & Sons Ltd., pp 341-343, 2009. 165 [38] I. Tashev, “Sound Capture and Processing: Practical Approaches”. Chichester, West Sussex: John Wiley & Sons Ltd., 2009, pp 171-174, 2009. [39] I. Tashev, “Sound Capture and Processing: Practical Approaches”. Chichester, West Sussex: John Wiley & Sons Ltd., 2009, pp 74, 2009 [40] S. Renals, H. Bourlard, J. Carletta, and A. Popescu-Belis, “Multi-Modal Signal Processing. Human Interactions in Meetings”, New York: Cambridge University Press, pp 29-35, 2012. [41] I. Tashev, “Sound Capture and Processing: Practical Approaches”. Chichester, West Sussex: John Wiley & Sons Ltd., pp 171, 2009. [42] I. Cohen, J. Benesty, and S. Gannot,“Speech Processing in Modern Communications”. W. Kellerman. Berlin: Springer-Verlag, page 212, 2010. [43] B. Gunel, EE2.LabB: Measurement and Processing of Room Impulse Responses”. University of Surrey, 2011. [Online]. Available: http://personal.ee.surrey.ac.uk/Personal/P.Jackson/ee2.lab/XY_rir/. [44] B. Xie, “Head Related Transfer Function and Virtual Auditory Display”. Second Edition. J. Ross Publishing, Plantation Fl., pp 352-355, 2013. [45] F. A. Everest, and K.C. Pohlmann, “Master Handbook of Acoustics” McGraw Hill, New York, pp 559-560, 2015. [46] D. Diaz-Guerra, A. Miguel, and A. J. Beltran, “gpuRIR: A python library for room impulse response simulation with GPU acceleration”. Multimed Tools Appl 80, 5653–5671. 2021. https://doi.org/10.1007/s11042-020-09905-3. [47] J.B. Allen, D.A. Berkley, “Image Method for Efficiently Simulating Small-Room Acoustics”, The Journal of the Acoustical Society of America, 1979. DOI 10.1121/1.382599 [48] Wikipedia [Online]. Available: https://en.wikipedia.org/wiki/Voice_frequency [49] I. McLoughlin, “Speech and Audio Processing, a MATLAB-based Approach”, Cambridge University Press, New York, page 65, 2016. [50] F. A. Everest, and K.C. Pohlmann, “Master Handbook of Acoustics” McGraw Hill, New York, pp 75-76, 2015. 166 [51] S. Renals, H. Bourlard, J. Carletta, and A. Popescu-Belis, “Multi-Modal Signal Processing. Human Interactions in Meetings”, New York: Cambridge University Press, pp 40-41, 2012. [52] M. de Campos Niero, A. de Lima Veiga Filho, and A. G. Adami, ""A comparison of distance measures for clustering in speaker diarization,"" 2014 International Telecommunications Symposium (ITS), 2014, pp. 1-5, doi: 10.1109/ITS.2014.6947954. [53] S. Chen, and P. Gopalakrishnan,” Speaker, environment and channel change detection and clustering via the bayesian information criterion”, in Proceedings of DARPA Broadcast News Transcription and Understanding Workshop, 1998. [54] Z. Bai, Xiao-Lei Zhang, “Speaker recognition based on deep learning: An overview”, Elsevier: Journal of Neural Networks, Volume 140, 2021, pp 84-88, ISSN 0893-6080, [Online], Available: https://doi.org/10.1016/j.neunet.2021.03.004. [55] O. Ghahabi, “Deep Learning for i-Vector Speaker and Language Recognition”. Ph.D. thesis, Speech Processing Group Department of Signal Theory and Communications Universitat Politècnica de Catalunya, Barcelona, 2018. [56] Z. Bai, and Xiao-Lei Zhang, “Speaker recognition based on deep learning: An overview”, Elsevier: Journal of Neural Networks, Volume 140, 2021, pp 68-72, ISSN 0893-6080, [Online], Available: https://doi.org/10.1016/j.neunet.2021.03.004. [57] J. Guo, N. Xu, K. Qian, Y. Shi, K. Xu, Y. Wu, and A. Alwan., “Deep neural network based i-vector mapping for speaker verification using short utterances”, Computer Speech & Language, Volume 72, March 2022, 101317. [Online]. Available: https://arxiv.org/abs/2101.09624. [58] D. Yifan, X. Yong, Z. Shi-Xiong, C. Yauhan, and W. Liqiang. 2020. “Self- Supervised learning for audio-visual speaker diarization”. In ICASSP 2020 - 2020 IEEE international conference on acoustics, speech and signal processing pp. 4367– 4371, 2020. [59] T. J. Park, and P. Georgiou, “Multimodal speaker segmentation and diarization using lexical and acoustic cues via sequence to sequence neural networks”. In Proc. INTERSPEECH 2018 pp. 1373–1377, 2018. 167 [60] L. El Shafey, H. Soltau, and I. Shafran, “Joint speech recognition and speaker diarization via sequence transduction”. In Proc. INTERSPEECH 2019 pp. 396–400, 2019. [61] W. Kang, B. Roy, and W. Chow. “Multimodal speaker diarization of real-world meetings using d-vectors with spatial features”. In ICASSP 2020 – 2020 IEEE international conference on acoustics, speech, and signal processing pp. 6509–6513, 2020. [62] Amazon AWS, “Amazon Transcribe”, 2021. [Online]. Available: https://aws.amazon.com/transcribe/?nc=sn&loc=1. [63] Google’s Cloud, “Separating different speakers in an audio recording”, 2021. [Online]. Available: https://cloud.google.com/speech-to-text/docs/multiple-voices. [64] Microsoft Azure Product Documentation, “What is Speaker Recognition?” Microsoft, Nov. 3, 2021. [Online]. Available: https://docs.microsoft.com/en- us/azure/cognitive-services/speech-service/speaker-recognition-overview. [65] D. Misal, “Google Speech Vs Amazon Transcribe: The War of Speech Technology,” Analytics India Magazine, Oct. 22, 2018. [Online], Available: https://analyticsindiamag.com/google-speech-vs-amazon-transcribe-the-war-of- speech-technology/. [66] M. Saraswat and R. C. Tripathi, ""Cloud computing: comparison and analysis of cloud service providers-AWs, Microsoft and Google,"" In Proc. 2020 9th International Conference System Modeling and Advancement in Research Trends (SMART), 2020, pp. 281-285. [67] A. Woollacott, “Benchmarking speech technologies,” Academia.edu, Feb. 2021. [Online]. Available: Academia, https://www.academia.edu/45165394/Benchmarking_Speech_Technologies. [68] X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, S. Chen, Y. Zhao, G. Liu, Y. Wu, J. Wu, S. Liu, J. Li, and Y. Gong, “Microsoft speaker diarization system for the VoxCeleb speaker recognition challenge 2020,” In Proc. ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp.5824-5828. [69] Microsoft speaker recognition. [Online]. Available: https://docs.microsoft.com/en- us/azure/cognitive-services/speech-service/speaker-recognition-overview 168 [70] I. Tashev, “Sound Capture and Processing: Practical Approaches”. Chichester, West Sussex: John Wiley & Sons Ltd., pp 351, 2009. [71] R. Scheibler, E. Bezzam, and I. Dokmanić, “Pyroomacoustics: A Python package for audio room simulation and array processing algorithms,” In Proc. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 351-355. [72] Pyroomacoustics documentation. [Online]. Available: https://pyroomacoustics.readthedocs.io/en/pypi-release/ [73] E. Marin. “Voice Activity Detection Using Filters”. University of New Mexico, Spring 2021. eguaderrama@unm.edu. [74] I. McLoughlin, “Speech and Audio Processing, a MATLAB-based Approach”, Cambridge University Press, New York, pp 24-28, 2016. [75] NI LabVIEW. [Online]. Available: https://www.ni.com/en-us/shop/labview.html [76] NI LabVIEW Convolution. [Online]. Available: https://zone.ni.com/reference/en- XX/help/371361R-01/lvanls/convolution/ [77] NI LabVIEW Deconvolution. [Online]. Available: https://zone.ni.com/reference/en- XX/help/371361R-01/lvanls/deconvolution/ [78] NI LabVIEW Correlation. [Online]. Available: https://zone.ni.com/reference/en- XX/help/371361R-01/gmath/correlation_test/ [79] NI LabVIEW Cross-Correlation. [Online]. Available: https://zone.ni.com/reference/en-XX/help/371361R-01/lvanls/crosscorrelation/ [80] Waveform Tracktion. [Online]. Available: https://www.tracktion.com/products/waveform-free [81] O. Galibert, “Methodologies for the evaluation of speaker diarization and automatic speech recognition in the presence of overlapping speech,” In Proc. INTERSPEECH 2013, 2013, pp. 1131-1134. [82] Q. Wang, “SimpleDER: a lightweight library to compute Diarization Error Rate (DER)”. [Online]. Available: https://pypi.org/project/simpleder/. 169 [83] Audacity® software is copyright © 1999-2021 Audacity Team. Web site: https://audacityteam.org/. It is free software distributed under the terms of the GNU General Public License. The name Audacity® is a registered trademark. [84] P. Kabal, TSP Speech Database, version 2 (2018-11), Montreal, Quebec: McGill University Department of Electrical and Computer Engineering Telecommunications & Signal Processing Laboratory, 2018. [Online]. Available: http://www- mmsp.ece.mcgill.ca/Documents/Data/. [85] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with permutation-free objectives,” In Proc. INTERSPEECH 2019, 2019, pp. 4300-4304. 170",0
Denoised MDPs: Learning World Models Better Than the World Itself,"[{'href': 'http://arxiv.org/abs/2206.15477v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2206.15477v2', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-30 17:59:49,,"Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Edoardo Cetin * 1 Philip J. Ball * 2 Steve Roberts 2 Oya Celiktutan 1 2 2 0 2 l u J 3 ] G L . s c [ 1 v 6 8 9 0 0 . 7 0 2 2 : v i X r a Abstract Off-policy reinforcement learning (RL) from pixel observations is notoriously unstable. As a result, many successful algorithms must com- bine different domain-speciﬁc practices and auxil- iary losses to learn meaningful behaviors in com- plex environments. In this work, we provide novel analysis demonstrating that these instabil- ities arise from performing temporal-difference learning with a convolutional encoder and low- magnitude rewards. We show that this new visual deadly triad causes unstable training and prema- ture convergence to degenerate solutions, a phe- nomenon we name catastrophic self-overﬁtting. Based on our analysis, we propose A-LIX, a method providing adaptive regularization to the encoder’s gradients that explicitly prevents the occurrence of catastrophic self-overﬁtting using a dual objective. By applying A-LIX, we signiﬁ- cantly outperform the prior state-of-the-art on the DeepMind Control and Atari 100k benchmarks without any data augmentation or auxiliary losses. 1. Introduction One of the core challenges in real world Reinforcement Learning (RL) is achieving stable training with sample- efﬁcient algorithms (Dulac-Arnold et al., 2019). Combining these properties with the ability to reason from visual obser- vations has great implications for the application of RL to the real world (Kalashnikov et al., 2018; Zhu et al., 2020). Recent works utilizing temporal-difference (TD-) learning have made great progress advancing sample-efﬁciency (Lil- licrap et al., 2015; Fujimoto et al., 2018; Haarnoja et al., 2018a; Cetin & Celiktutan, 2021). However, stability has remained a key issue for these off-policy algorithms (Sutton, *Equal contribution 1Centre for Robotics Research, Depart- ment of Engineering, King’s College London 2Department of Engineering Science, University of Oxford. Correspondence to: Edoardo Cetin <edoardo.cetin@kcl.ac.uk>, Philip J. Ball <ball@robots.ox.ac.uk>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). Figure 1. Performance of agents in DMC (left) and Atari 100k (right) benchmarks from 10 seeds. A-LIX outperforms previous methods without using image augmentations or auxiliary losses. 1988; Duan et al., 2016; Van Hasselt et al., 2018; Bus¸oniu et al., 2018), making their general applicability limited as compared to their on-policy counterparts (Schulman et al., 2017; Cobbe et al., 2021). At the same time, using pixel observations has been another orthogonal source of insta- bilities, with several successful approaches relying on pre- training instead of end-to-end learning (Finn et al., 2015; Dwibedi et al., 2018). In fact, alternative optimization ob- jectives, large amounts of simulation data, and symbolic observations have been common factors in most contempo- rary large-scale RL milestones (Silver et al., 2017; Vinyals et al., 2019; Berner et al., 2019). In this work, we provide novel insights behind why ap- plying successful off-policy RL algorithms designed for proprioceptive tasks to pixel-based environments is gener- ally underwhelming (Lee et al., 2019; Yarats et al., 2021). In particular, we provide evidence that three key elements strongly correlate with the occurrence of detrimental insta- bilities: i) Exclusive reliance on the TD-loss. ii) Unregu- larized end-to-end learning with a convolutional encoder. iii) Low-magnitude sparse rewards. Using this framework, we are able to motivate the effectiveness of auxiliary losses (Laskin et al., 2020b; Schwarzer et al., 2020; Yarats et al., 2021) and many domain-speciﬁc practices (Hessel et al., 2018; Laskin et al., 2020a) by explaining how they address elements of this new visual deadly triad. We focus our analysis on the popular DeepMind Control suite (DMC) (Tassa et al., 2018), where the introduction of random shift augmentations has played a key role in re- cent advances (Laskin et al., 2020a; Kostrikov et al., 2021; DMC Performance Atari 100K Performance A-LIX 694.7 A-LIX SPR 0.752 0.704 DrQ-v2 632.7 SPR (No Augs) 0.463 DrQ 241.2 CURL 262.1 SAC 51.9 DrQ 0.357 CURL 0.381 OTRainbow 0.264 DER 0.285 SimPLe 0.443 0 250 500 0.00 0.25 0.50 0.75 Average Score (Medium + Hard) Mean Human-Normalized Score Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Table 1. Practices from recent pixel-based TD-learning methods to mitigate elements of the visual deadly triad. †DrQ uses 10-step returns on Atari. *CURL uses 20-step returns on Atari. Algorithm Visual Deadly Triad Mitigation TD-Loss CNN Overﬁt DrQ/RAD DrQ-v2 SAC-AE SPR DER CURL - - VAE Loss Model-Based Loss - Contrastive Loss Shift/Jitter Augmentations Shift Augmentations - Shift/Jitter Augmentations Non-Overlapping Strides Shift Augmentations Low-Density Reward 10-step returns† 3-step returns - 10-step returns 20-step returns 20-step returns* Yarats et al., 2022). In this domain, we observe that the presence of the visual deadly triad results in the TD-loss gradients through the convolutional encoder’s feature maps having high spatial frequencies. We ﬁnd these gradients are spatially inconsistent and result in degenerate optimization landscapes when backpropagated to the encoder’s param- eters. Furthermore, repeatedly updating the convolutional encoder with these gradients consistently leads to early con- vergence to degenerate feature representations causing the critic to ﬁt high-variance erroneous targets, a phenomenon we name catastrophic self-overﬁtting. As a way of iden- tifying the direct implications of the visual deadly triad in the gradient signal, we propose a new measure called the Normalized Discontinuity (ND) score and show how its value precisely correlates with agent performance. Thus, we explain the effectiveness of shift augmentations by recog- nizing that they regularize the gradient signal by providing an implicit spatial smoothing effect. Based on our analysis, we propose Adaptive Local SIgnal MiXing (A-LIX) a novel method to prevent catastrophic self-overﬁtting with two key components: i) A new parame- terized layer (LIX) that explicitly enforces smooth feature map gradients. ii) A dual objective that ensures learning sta- bility by adapting the LIX parameters based on the estimated ND scores. We show that integrating A-LIX with existing off-policy algorithms achieves state-of-the-art performance in both DeepMind Control and Atari 100k benchmarks with- out requiring image augmentations or auxiliary losses and signiﬁcantly fewer heuristics. We open-source our code to facilitate reproducibility and future extensions1. Our main contribution can be summarized as follows: • We conjecture the existence of a visual deadly triad as a principal source of instability in reinforcement learn- ing from pixel observations and provide clear empirical evidence validating our hypothesis. • We show these instabilities affect the gradient signal causing catastrophic self-overﬁtting, a phenomenon that can severely harm TD-learning. As a result, we design the normalized discontinuity score to explicitly 1https://github.com/Aladoro/Stabilizing-Off-Policy-RL Figure 2. Returns of agents over 5 seeds. Solid lines represent median performance, faded lines represent individual runs. The vertical dashed line shows when augmentations are turned off. anticipate its occurrence. • We propose A-LIX, a new method that adaptively reg- ularizes convolutional features to prevent catastrophic self-overﬁtting, achieving state-of-the-art results on two popular pixel-based RL benchmarks. 2. Background We consider problem settings described by Markov Deci- sion Processes (MDPs) (Bellman, 1957), deﬁned as the tuple (S, A, P, p0, r, γ). This comprises a state space S, an action space A, transitions dynamics given by P and p0, and a re- ward function r. The RL objective is then for an agent to re- cover an optimal policy π∗, yielding a distribution of trajec- tories pπ(τ ) that maximizes its expected sum of discounted future rewards, π∗ = arg maxπ Epπ(τ ) [(cid:80)∞ t=0 γtr(st, at)]. In off-policy RL, this objective is usually approached by learning a critic function to evaluate the effectiveness of the agent’s behavior. A common choice for the critic is to param- eterize the policy’s Q-function Qπ : S × A → R, that quan- tiﬁes the agent’s performance after performing a particular action: Qπ(s, a) = Epπ(τ |s0=s,a0=a) [(cid:80)∞ t=0 γtr(st, at)]. Most off-policy algorithms entail storing trajectories in a buffer D, and learning parameterized Q-functions by itera- tively minimizing a squared temporal difference (TD-) loss: JQ(φ) = E(s,a,s(cid:48),r)∼D (cid:2)(Qπ φ(s, a) − y)2(cid:3) , (cid:105) (cid:104) ˆQπ φ(cid:48)(s(cid:48), a) . y = r + γEa∼π(s(cid:48)) (1) Here, the TD-targets y are computed from a 1-step bootstrap operation with a slowly-changing target Q-function ˆQπ φ(cid:48). In continuous action spaces, we also learn a separate parame- terized policy to exploit the information in the critic. This practically results in alternating TD-learning with maximiz- ing the Q-function’s expected return predictions, following the policy gradient theorem (Sutton et al., 2000). 3. Instabilities in TD-Learning from Pixels Unlike proprioceptive observations, off-policy RL from pixel observations commonly requires additional domain- Cheetah Run Augs Turned Off 500 n r u t e R 0 0 Augs 1 2 Frames (×106) No Augs 3 No Augs @ 500k Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Figure 3. Evidence of overﬁtting when augmentations are not used. On the left, shaded lines are individual estimates, the solid line represents the median Q-value. On the right, the Q-values Pearson correlation with target values and Monte-Carlo returns (RM C ). Figure 4. TD-loss of ofﬂine ﬁxed transitions during training, sepa- rated based on having non-zero reward. complementary experiments validating these claims). speciﬁc practices to ensure stability. In this section, we provide a novel analysis of this phenomenon by focusing on the DeepMind Control Suite (Tassa et al., 2018). In this benchmark, the introduction of random shift data augmen- tations has been a core component of recent advances in pixel-based off-policy RL (Laskin et al., 2020a; Yarats et al., 2022), allowing us to isolate and reproduce stable and un- stable training regimes. Our analysis suggests the existence of speciﬁc elements that cause instabilities and strives to explain their implications on learning dynamics. We vali- date our ﬁndings via thorough empirical experimentation showing numerous results corroborating our hypotheses. Based on our discoveries, in Section 4 we provide a new interpretation of random shifts and propose a new improved method to isolate and counteract instabilities. 3.1. Why Do Augmentations Help? The underlying mechanism behind the effectiveness of ran- dom shifts is not immediately clear. While this augmen- tation may appear to assist generalization by encoding an invariance (Shorten & Khoshgoftaar, 2019), we note that all the environments from DMC employ a camera that is ﬁxed relative to the agent’s position. Hence, robustness to shifts does not appear to introduce any useful inductive bias about the underlying tasks. Moreover, prior work successfully learned effective controllers without augmentations (Hafner et al., 2020; Yarats et al., 2021), suggesting that shift gen- eralization might not be the primary beneﬁt of this method. We analyze the effect of random shifts by training a DrQ-v2 agent (Yarats et al., 2022) on Cheetah Run but turning off augmentations after an initial 500,000 time-steps learning phase. As shown in Fig. 2, while training without any shift augmentation fails to make consistent progress, turning off augmentations after the initial learning phase actually ap- pears to slightly improve the performance of DrQ-v2. This result is a clear indication that augmentations are not needed for asymptotic performance, and are most helpful to coun- teract instabilities present in the earlier stages of learning, which we now focus on analyzing (see App. F.1-F.2 for 3.2. Identifying a New Deadly Triad To reduce confounding factors and to disentangle the origin of these instabilities, we design an ofﬂine RL experiment (Levine et al., 2020). This experiment isolates three distinct elements affecting off-policy RL: exploration, policy eval- uation, and policy improvement. First, we gather a set of 15,000 transitions with pixel observations using a random policy in Cheetah Run. This allows us to ground explo- ration and analyze learning from ﬁxed data resembling the early stages of online training (when augmentations appear most helpful). We then isolate policy evaluation by training both critic and encoder using SARSA (Rummery & Niran- jan, 1994) until convergence on this ﬁxed data. Finally, we run policy improvement, training an actor to maximize the expected discounted return as predicted by the converged critic (see App. B.1 for details). Interestingly, we ﬁnd that turning on augmentations exclusively during exploration or policy improvement has no apparent effect on stability and ﬁnal performance. Hence, we focus on the effects that augmentations have on TD-learning and analyze applying augmentations only during policy evaluation. Table 2. Performance and training statistics of different agent types in the ofﬂine experiments from 15,000 random transitions. Agent Augmented Non-Augmented Proprioceptive Frozen CNN (random) Frozen CNN (pre-trained) Non-Augmented (norm r) Non-Augmented (10-step returns) Final TD-Loss Final Policy Loss Return 0.021 0.002 0.012 0.023 0.012 18.616 0.003 −0.99 −1.05 −1.14 −0.95 −0.99 3.86 −1.24 86.5 ± 11.3 9.2 ± 12.1 79.1 ± 7.7 43.6 ± 20.2 77.6 ± 18.5 38.6 ± 16.5 36.5 ± 20.3 As shown in Table 2, applying augmentations during pol- icy evaluation enables us to learn policies that achieve a return of 86.5, despite the best trajectory in the ofﬂine data achieving only 10.8. In contrast, without augmentations we consistently recover near 0 returns, resembling the failures observed in the online experiments. On the left of Fig. 3 we show the evolution of the predicted Q-values for both Q values during training No Augs Augs 2.0 1.5 1.0 0.5 0.0 l s e u a V Q Corr. of Q with Qtarget and RMC 1.0 l n o i t a e r r o C n o s r a e P 0.8 0.6 0.4 0.2 0.0 Qtarget RMC 0 2500 5000 SGD Steps 7500 0 2500 5000 SGD Steps 7500 Zero reward samples Non-Zero reward samples No Augs Augs r o r r E D T 1.00 0.75 0.50 0.25 0.00 0 2000 4000 SGD Steps 6000 8000 0 2000 4000 SGD Steps 6000 8000 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels agents on a ﬁxed batch of ofﬂine data. In particular, when performing policy evaluation without augmentations, these predictions display extremely high variance across different state-action pairs. In Table 2 we further show that the non- augmented agent displays signiﬁcantly lower loss, despite having higher average Q-values than the augmented agent (Schaul et al., 2021). We argue this is a clear indication of the occurrence of overﬁtting. We corroborate our claim by analyzing the evolution of the Pearson bi-variate correlation between the estimated Q-values and target Q-values on the right of Fig. 3. These results show that the non-augmented agent displays near-perfect correlation with its own target Q-values throughout training, indicating that it immediately learns to ﬁt its own noisy, randomly-initialized predictions. We also record the correlation with the actual discounted Monte-Carlo returns, which represent the true targets the Q-values should ideally approximate during policy evalu- ation. For these results, we observe that the relationship between applying augmentations and the recorded correla- tion is reversed, with the non-augmented agent displaying signiﬁcantly lower correlation. This dichotomy appears to indicate that ﬁtting the noisy targets severely affects learn- ing the useful training signal from the collected transitions regarding the experienced rewards. We conﬁrm this phe- nomenon by splitting the data into non-zero and zero reward transitions, where the only learning signal propagated in the TD-loss is from the initially random target values. In Fig. 4 we illustrate that the non-augmented agents initially experience much higher TD-errors on zero reward transi- tions, conﬁrming that they focus on ﬁtting uninformative components of the TD-objective. In Table 2 we provide the results of additional experiments that indicate that TD-learning is not the only cause for the observed instabilities. First, we conﬁrm that the ob- served overﬁtting appears to be exclusive to performing end-to-end TD-learning with convolutional neural network (CNN) encoders. Concretely, we run the same ofﬂine exper- iment without training an encoder in three different settings. First, we consider performing policy evaluation directly from non-augmented proprioceptive observations with a fully-connected critic network. Moreover, we also consider freezing the encoder weights either to their initial random values or to pre-trained values from the augmented agent experiments. In all three cases, we attain largely superior performance, almost matching the augmented agent’s per- formance for both the proprioceptive and pre-trained exper- iments. In addition, we also ﬁnd that the observed over- ﬁtting phenomenon is diminished when simply increasing the magnitude of the reward signal in the TD-loss. We test this through two additional experiments which consider normalizing the collected rewards before policy evaluation and incorporating large n-step returns (Sutton, 1988). As reported, both modiﬁcations considerably improve the non- Figure 5. Feature maps in the ﬁnal layer of both augmented (top) and non-augmented (bottom) agent encoders. Non-augmented agents manifest inconsistent, high-frequency feature maps. augmented agent’s performance. However, we note that both practices introduce further unwanted variance in the optimization, failing to yield the same improvements as augmentations (see App. C.2). Taken together, our results appear to strongly indicate that instabilities in off-policy RL from pixel observation come from three key conditions, which we refer to as the visual deadly triad: i) Exclusive reliance on the TD-loss; ii) Un- regularized learning with an expressive convolutional en- coder; iii) Initial low-magnitude sparse rewards. Further evidence arises when considering the ubiquity of partic- ular practices employed in pixel-based off-policy RL. In particular, as summarized in Table 1, most popular prior algorithms feature design choices that appear to counteract at least two elements of this triad, either directly or implic- itly. Furthermore, we show these instabilities result in the non-augmented critics focusing on learning their own noisy predictions, rather than the actual experienced returns. We observe this ultimately leads to convergence to erroneous and high-variance Q-value predictions, a phenomenon we name catastrophic self-overﬁtting. 3.3. Anticipating Catastrophic Self-Overﬁtting We now attempt to unravel the links that connect the visual deadly triad with catastrophic self-overﬁtting. We start by observing that catastrophic self-overﬁtting comes with a signiﬁcant reduction of the critic’s sensitivity to changes in action inputs, implying that the erroneous high-variance Q-value predictions arise primarily due to changes in the observations (see App. F.3 for action-value surface plots). Hence, we focus on analyzing the feature representations of the pixel observations, computed by the convolutional en- coder, z ∈ RC×H×W . In particular, we wish to quantify the sensitivity of feature representations to small perturbations in the input observations. To measure this, we evaluate the Jacobians of the encoder across a ﬁxed batch of ofﬂine data for the augmented and non-augmented agents. We then cal- culate the Frobenius norm of each agent’s Jacobians, giving us a measure of how quickly the encoder feature represen- Augmented Final Feature Map Non-augmented Final Feature Map Stabilizing Off-Policy Deep Reinforcement Learning from Pixels overﬁtting (Keskar et al., 2017) 2. To quantify the level of discontinuity in the features and their gradients, we propose a new metric that encodes the aggregated immediate spatial ‘unevenness’ of each feature location within its relative feature map. In particular, we deﬁne D(z) ∈ RC×H×W as the expected squared local discontinuity of z in any spatial direction, i.e.: (cid:19)2(cid:35) D(z)ijc ≈ Ev∼S1 , (2) (cid:34)(cid:18) ∂zijc ∂v practically estimated via sampling. We then normalize each value in D(z) by its squared input and average over all the feature positions. We name this metric the normalized discontinuity (ND) score: ND(z) = 1 C × H × W C (cid:88) H (cid:88) W (cid:88) c=1 j=1 i=1 D(z)ijc z2 ijc . (3) Intuitively, this score reﬂects how locally discontinuous z is expected to be at any spatial location. In Fig. 7, we show how the N D score of ∇z evolves during training in the ofﬂine and an online setting for both augmented and non- augmented agents. We see that augmented agents experi- ence considerably less discontinuous gradients through their features, and that recordings of lower N D scores also ap- pear to be highly correlated with performance improvements. We additionally show an accumulated N D score, using an exponential moving average of ∇z in each spatial position to calculate this metric. Interestingly, we observe that the N D score over accumulated gradients is almost identical to the instantaneous N D score, showing that similar gradient discontinuities are propagated persistently through training in each position of the feature map. This property conﬁrms that the discontinuities are not smoothed by the stochastic sampling of different consecutive training batches, in which case we would expect to observe lower accumulated N D scores. Thus, it suggests that self-overﬁtting emerges in the non-augmented agents due to repeated gradient steps towards persistent feature map discontinuities. 4. Counteracting Gradient Discontinuities 4.1. Gradient Smoothing and Random Shifts As analyzed in Section 3, catastrophic self-overﬁtting oc- curs when the gradients in the convolution layers are locally discontinuous. As a result, we argue that the efﬁcacy of random shifts arises from their downstream effect on feature gradient computation, which counteracts these discontinu- ities during backpropagation. In particular, while random shifts do not act directly on the latent representation or their 2Instead, the loss surface with respect to the fully-connected weights is smoother (App. F.5). Figure 6. Critic loss surface plots of augmented (left) and non- augmented (right) agents after 5,000 steps of ofﬂine training. tations are changing locally around an input (see App. B.2 for details). Our results show a stark difference, with the feature representations of the non-augmented agents being on average 2.85 times more sensitive. This suggests that overﬁtting is driven by the CNN encoder’s representations learning high-frequency information about the input obser- vations and, thus, breaking useful inductive biases about this class of models (Rahaman et al., 2019). In App. E.1 we demonstrate that lower sensitivity to ran- dom noise, while desirable for optimization (Rosca et al., 2020), is actually a byproduct of a stable feature represen- tations, and not its deﬁning factor. Furthermore, observing the actual feature maps of different observations in Fig. 5, we see that augmentations make the encoder produce fea- tures that are spatially consistent, aligned with common understandings of how natural representations should ap- pear (Alsallakh et al., 2021; Allen-Zhu & Li, 2021). In contrast, the non-augmented agents display high-frequency and discontinuous feature maps that do not reﬂect the spa- tial properties of their inputs. Hence, our evidence suggests that catastrophic self-overﬁtting speciﬁcally follows from the same learning process that produces highly-sensitive and discontinuous encoder feature maps. Therefore, we turn our focus to analyzing the gradients backpropagated to the encoder’s features maps and observe one key prop- erty: the gradients of the output feature maps consistently reﬂect the same spatial properties of their resulting features. In particular, the gradients of the feature maps appear spa- tially consistent for the augmented agent, and discontinuous for the non-augmented agent. This optimization property reﬂects intuitive understandings of backpropagation since discontinuous gradients should push the encoder’s weights to encode discontinuous representations. To provide further complementary evidence that discontinuous gradients are the direct cause of catastrophic self-overﬁtting, we analyze the normalized loss surfaces when backpropagating these discontinuous gradients to the encoder’s parameters (fol- lowing Li et al. (2018)). In Fig. 6, we see that gradient discontinuities in the non-augmented agent yield extreme peaks in its encoder’s loss surface, clearly suggestive of Critic with Augmentations Critic w/o Augmentations s s o L D T 0.75 0.50 0.25 1 0 W eig ht S u bsp ace 2 1 0.75 0.50 0.25 1 0.75 0.50 0.25 0 TD Loss 1 0 1 1 1 0 Weight Subspace 1 1 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels persistent discontinuities from accumulating. Hence, ran- dom shifts break the second condition of the visual deadly triad, by providing effective implicit regularization of the convolutional encoder’s learning process. At the same time, this minimally disrupts the information content of the re- sultant features, since discarded observation borders almost exclusively comprise background textures that are irrele- vant for performing the task. This interpretation of random shifts aligns with the analysis in Section 3, showing that im- plicitly smoothing over the backpropagated gradient maps consistently prevents catastrophic self-overﬁtting. 4.2. Local Signal Mixing We extrapolate our hypotheses about catastrophic self- overﬁtting and random shifts by proposing a technique that aims to enforce gradient smoothing regularization explic- itly. We propose Local SIgnal MiXing, or LIX, a new layer speciﬁcally designed to prevent catastrophic self-overﬁtting in convolutional reinforcement learning architectures. LIX acts on the features produced by the convolutional encoder, z ∈ RC×H×W , by randomly mixing each component zcij with its neighbors belonging to the same feature map. Hence, LIX outputs a new latent representation with the same di- mensionality ˆz ∈ RC×H×W , whose computation graph minimally disrupts the information content of each feature zcij while smoothing discontinuous components of the gra- dient signal during backpropagation. LIX is a regularization layer that acts as a simple random smoothing operation, reducing the expected magnitude of gradient discontinuities by preventing higher frequency sig- nals to persist. In the forward pass, LIX produces a new latent representation where for each of the C feature maps, ˆzcij is computed as a randomly weighted average of its spatial neighbors around coordinates i, j. We further param- eterize this stochastic operation using some maximum range radius S, and consequently sample two uniform continuous random variables δx, δy ∼ U [−S, S], representing shifts in the x and y coordinates respectively. Correspondingly, we deﬁne ˜i = i + δx and ˜j = j + δy and perform the weighted averaging as a bilinear interpolation with weights determined by the random shifts: ˆzcij =zc(cid:98)˜i(cid:99)(cid:98)˜j(cid:99)((cid:100)˜i(cid:101) − ˜i)((cid:100)˜j(cid:101) − ˜j) + zc(cid:98)˜i(cid:99)(cid:100)˜j(cid:101)((cid:100)˜i(cid:101) − ˜i)(˜j − (cid:98)˜j(cid:99)) +zc(cid:100)i(cid:101)(cid:98)˜j(cid:99)(˜i − (cid:98)˜i(cid:99))((cid:100)˜j(cid:101) − ˜j) + zc(cid:100)i(cid:101)(cid:100)˜j(cid:101)(˜i − (cid:98)˜i(cid:99))(˜j − (cid:98)˜j(cid:99)). Since nearby features in a convolutional feature map are computed with very similar receptive ﬁelds, the mixing effect of LIX should have a trivial effect on the informa- tion the encoder can convey in its latent representations. In addition, LIX should have a direct regularization ef- fect on the gradients by acting on the feature maps them- selves. In particular, since LIX computes each output feature from a weighted average of its neighbors, back- Figure 7. Instantaneous (red and blue) and accumulated (orange and purple) N D scores for the features gradients from ofﬂine (left) and online (right) training in Cheetah Run. respective gradients, they do affect how the latent represen- tations are computed. This has an impact on how persistent discontinuous components of the gradient are backpropa- gated to the encoder’s parameters during learning. From the approximate shift invariance of convolutional layers, we can view a convolutional encoder as computing each of the feature vectors [z1ij, ..., zCij]T with the same param- eterized function, Vφ, that takes as input a subset of each observation O ∈ RC(cid:48)×H (cid:48)×W (cid:48) . This subset corresponds to a local neighborhood around some reference input coordinates i(cid:48), j(cid:48). Thus, the only factor differentiating features in the same feature map (e.g., zcij and zckl) is some implicit func- tion f (i, j) = i(cid:48), j(cid:48) translating each of the output features coordinate into the relative reference input coordinate, i.e. zcij = Vφ(O, i(cid:48), j(cid:48))c (determined by kernel sizes, strides...). Therefore, random shifts are approximately equivalent to further translating each reference coordinate by adding some x, δ(cid:48) uniform random variables δ(cid:48) y: zcij ≈ Vφ(O, i(cid:48) + δ(cid:48) y ∼ U [−s(cid:48), s(cid:48)], x, δ(cid:48) δ(cid:48) x, j(cid:48) + δ(cid:48) y)c, f (i, j) = i(cid:48), j(cid:48). where Due to the employed strides from the convolutional archi- tectures used in DrQ-v2 (Yarats et al., 2022), the difference in reference coordinates of adjacent features in a feature map is less than the maximum allowable shift employed in the augmentations, i.e., (i + 1)(cid:48) − i(cid:48), (j + 1)(cid:48) − j(cid:48) < s(cid:48) (where s(cid:48) is the maximum allowable shift). Consequently, shift augmentations effectively turn the deterministic com- putation graph of each feature zcij into a random variable, whose sample space comprises the computation graphs of all nearby features within its feature map. Hence, applying different random shifts to samples in a minibatch makes the gradient of each feature ∇zcij backpropagate to a random computation graph, sampled from a set that extends the set of non-augmented computation graphs for all features in a local neighborhood of coordinates i, j. Therefore, ag- gregating the parameter gradients produced with different δ(cid:48) x, δ(cid:48) y, provides a smoothing effect on how each discon- tinuous component of ∇z affects learning, and prevents Offline Online e r o c S D N 2.2 2.0 1.8 1.6 0 5000 SGD Steps 2.0 1.5 1.0 10000 0.0 0.5 1.0 1.5 No Augs No Augs (Accumulated) Frames (×106) Augs Augs (Accumulated) Stabilizing Off-Policy Deep Reinforcement Learning from Pixels propagation will split each gradient ∇ˆzcij, to a random local combination of features within the same feature map, {∇zc(cid:98)˜i(cid:99)(cid:98)˜j(cid:99), ∇zc(cid:98)˜i(cid:99)(cid:100)˜j(cid:101), ∇zc(cid:100)i(cid:101)(cid:98)˜j(cid:99), ∇zc(cid:100)i(cid:101)(cid:100)˜j(cid:101)}. Thus, LIX should mostly preserve the consistent component of ∇z, while randomly smoothing its discontinuous component. There are multiple key differences between the regulariza- tion from LIX and random shifts. LIX provides a local smoothing effect over the gradients explicitly and exactly, without having to deal with the implications of padding and strided convolutions breaking shift-invariance assump- tions. Moreover, LIX smooths the gradient signal not only across different inputs but also within each feature map. In addition, by applying its operation solely at the feature level, the encoder can still learn to entirely circumvent LIX’s smoothing effect on the information encoded in the latent representations, given enough capacity. This means that LIX does not forcibly preclude any input information from affecting the computation. Consequently, LIX also does not have to enforce learning invariances which might not neces- sarily reﬂect useful inductive biases about the distribution of observations. In contrast, random shifts need to exploit the particular uninformativeness of the observations borders to avoid disrupting the features’ information content. 4.3. A-LIX LIX introduces a single key parameter: the range radius S used for sampling δx and δy. Intuitively, this value should reﬂect how much we expect gradients to be locally consis- tent for a given architecture and task. Therefore, we argue that the value of S should ideally decrease throughout train- ing as the useful learning signal from the TD-loss becomes stronger. This is consistent with the results illustrated in Figure 2, showing that turning off random shift augmenta- tions after the TD-targets become informative can improve learning. Hence, we propose an adaptive strategy to learn S throughout training. Utilizing the normalized discontinuity (N D) score in Section 3.3, we set up a dual optimization objective to ensure a minimum value of local smoothness in the representation gradients, N D. However, computing the N D score of the gradient signal involves a ratio between po- tentially very small values. As a result, estimation of these values from a batch of gradient samples can lead to outliers having an extreme impact on this average measure, trans- lating into large erroneous updates of S. To overcome this, we propose using a slightly modiﬁed version of the N D score with increased robustness to outliers (see App. C.1 for further details): (cid:103)N D(∇ˆz) = C (cid:88) H (cid:88) W (cid:88) (cid:32) log 1 + c=1 j=1 i=1 (cid:33) D(∇ˆz)cij ∇ˆz2 ijc . (4) In practice, we set up a dual optimization objective similar to the automatic temperature adjustment from Haarnoja et al. Figure 8. A-LIX’s S parameter evolution during training in Chee- tah Run (left) and Quadruped Run (right). As the critic targets become more informative, S falls, improving data efﬁciency and asymptotic performance. (2018b). This entails alternating the optimization of the TD- learning objectives described in Section 2 with minimizing a dual objective loss: (cid:104) −S × Eˆz (cid:103)N D(∇ˆz) − N D (cid:105) , (5) arg min S approximating dual gradient descent (Boyd et al., 2004). Hence, we call this new layer Adaptive LIX (A-LIX). In Fig. 8 we show that A-LIX effectively anneals S as the agent escapes its unstable regimes, in line with our intuition. 5. Performance Evaluation We evaluate the effectiveness of A-LIX in pixel-based re- inforcement learning tasks in two popular and distinct do- mains featuring a diverse set of continuous and discrete control problems. We integrate A-LIX with existing popu- lar algorithms and compare against current state-of-the-art model-free baselines. We provide further details of our integration and full hyperparameters in App. D. We also extend this section by providing more granular evaluation metrics in App. A. Furthermore, we provide ablation studies analyzing different components of A-LIX in App. E. 5.1. DeepMind Control Evaluation We ﬁrst evaluate the effectiveness of A-LIX for pixel-based RL on continuous control tasks from the DeepMind Control Suite (DMC) (Tassa et al., 2018). Concretely, we integrate A-LIX with the training procedure and network architecture from DrQ-v2 (Yarats et al., 2022), but without using image augmentations. To show the generality of our method we do not modify any of the environment-speciﬁc hyperparameters from DrQ-v2 and simply add our A-LIX layer after each encoder nonlinearity. For simplicity, we optimize a shared S for all the A-LIX layers with the dual objective in Eq. 5. Hence, this introduces a single additional parameter and negligible computational overhead. We compare A-LIX to DrQ-v2, which represents the current state-of-the-art on this benchmark. We also compare against three further baselines: the original DrQ (Kostrikov et al., 2021), which foregoes n- step returns and includes an entropy bonus; CURL (Laskin et al., 2020b), which includes an auxiliary contrastive ob- 500 n r u t e R 0 0.0 Cheetah Run Quadruped Run 2 1 700 600 500 0.7 l e u a V 0.6 S 0.5 X I L - A 1.0 0.5 Frames (×106) 1.5 Agent Return 1.5 2.0 2.5 3.0 Frames (×106) A-LIX Parameter S Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Table 3. Results summary for the Atari 100k benchmark. The reported performance of A-LIX is from 10 seeds. Metrics SimPLe DER OTRainbow CURL DrQ SPR A-LIX Norm. Mean 0.443 0.285 Norm. Median 0.144 0.161 # SOTA # Super Average Rank 7 2 3.92 1 2 5.00 0.264 0.204 1 1 5.21 0.381 0.357 0.704 0.753 0.175 0.268 0.415 0.411 1 2 1 2 3.92 4.85 2.88 4 7 11 7 2.21 Figure 9. Average performance in 10 seeds for DMC Medium (left) and Hard tasks (right). Shaded regions represent ±1 SE. jective; an extension of SAC (Haarnoja et al., 2018b) with the encoder from Yarats et al. (2021). These last three base- lines have been performant on a prior DMC benchmark that considers fewer tasks with high action repeats, as described by Hafner et al. (2019). Instead, we evaluate on the more challenging ‘Medium’ and ‘Hard’ benchmarks from Yarats et al. (2022), comprising 15 tasks with low action repeats. Results. We summarize the results in Figure 9, showing the mean performance curves for both medium and hard bench- mark tasks. We provide further details and the full list of results across all 15 environments in App. A.1. Overall, A- LIX surpasses all prior methods with clear margins, both in terms of efﬁciency and ﬁnal performanc. This is particularly notable in the more complex ‘Hard’ tasks. As highlighted in prior work (Cetin & Celiktutan, 2021), DrQ-v2 appears to yield inconsistent results on some of the harder exploration tasks with sparse rewards. This likely indicates that the gradient regularization induced by random shifts (described in Section 4.1) is unable to consistently prevent catastrophic self-overﬁtting in scenarios where the initial learning signal from TD-learning is particularly low. Finally, DrQ, CURL, and SAC fail to make consistent meaningful progress on this harder benchmark. This performance gap corroborates the third component of the visual deadly triad, showing how lower magnitude rewards due to harder exploration and lower action-repeats further destabilize TD-learning based algorithms, and explains the gains seen in DrQ-v2 when incorporating n-step returns. We believe these results em- phasize the challenge of overcoming the visual deadly triad in continuous control problems and the particular effective- ness of A-LIX to counteract its direct implications. 5.2. Atari 100k Evaluation We perform a second set of experiments in an entirely dif- ferent setting, discrete control. We make use of the popular Atari Learning Environment (ALE) (Bellemare et al., 2013) and consider the 100k evaluation benchmark from Kaiser et al. (2020). In particular, this benchmark comprises eval- uating performance for 26 tasks after only two hours of play-time (100k interaction steps), following the evaluation protocol in Machado et al. (2018). We integrate A-LIX with Data-Efﬁcient Rainbow (DER) (van Hasselt et al., 2019), a simple extension to Rainbow (Hessel et al., 2018) with improved data-efﬁciency. We would like to note that our integration has key differences to DER, designed to high- light the generality of our method in tackling the visual deadly triad. In particular, we reduce the n-step returns to 3 (from 20), and we maintain the same encoder architecture as in DrQ-v2. To speak to the latter point, this means we do not require the highly regularized encoders with large convolutional ﬁlters and strides, used ubiquitously in off- policy learning for Atari environments. Instead, to stabilize learning we simply apply our A-LIX layer after the ﬁnal encoder nonlinearity. We compare against three algorithms that, like A-LIX, do not employ data-augmentation: Data- Efﬁcient Rainbow (DER); Overtrained Rainbow (OTRain- bow) (Kielak, 2019); and Simulated Policy Learning (Sim- PLe) (Kaiser et al., 2020) (model-based). Moreover, we also compare with additional state-of-the-art off-policy baselines that make use of data augmentations: the aforementioned CURL and DrQ; and Self-Predictive Representations (SPR) (Schwarzer et al., 2020), the current state-of-the-art TD- learning based algorithm on this benchmark. SPR combines data augmentation with numerous additional algorithmic design choices, such as an auxiliary self-supervised loss for learning a latent dynamics model. Results. We summarize the results in Table 3, showing the mean and median human-normalized scores together with the number of environments where each algorithm ei- ther achieves state-of-the-art or super-human performance. We include the full per-environment results in App. A.2. Remarkably, A-LIX obtains a substantially higher human- normalized mean performance than all other considered algorithms. While the recorded normalized median per- formance is slightly inferior to SPR, we argue that such difference is not particularly signiﬁcant since this metric de- pends on the performance obtained in just two environments. Moreover, A-LIX achieves super-human performance in 7 games (the same as SPR), and state-of-the-art performance in 11 games, considerably more than all other algorithms. These results corroborate how tuned architectures, data aug- mentation, and auxiliary losses used on ALE mostly serve the purpose of counteracting the direct implications of the visual deadly triad and show that A-LIX enables us to learn DMC Medium Tasks DMC Hard Tasks n r u t e R 600 400 200 0 0 600 400 200 0 1 2 3 0 1 2 3 Frames (×106) Frames (×107) A-LIX DrQ-v2 CURL DrQ SAC Stabilizing Off-Policy Deep Reinforcement Learning from Pixels (a) DeepMind Control: Medium and Hard Tasks (b) Atari 100k Figure 10. Probability of improvement and performance proﬁles obtained from the recorded results in DMC (left) and Atari 100k (right). A-LIX displays statistically signiﬁcantly improvements and stochastically dominates most prior algorithms. powerful models without relying on these design choices. 5.3. Statistical Signiﬁcance To validate the signiﬁcance of our improvements, we sta- tistically analyze our results using the Rliable tools and practices from Agarwal et al. (2021). We summarize some of our key ﬁndings in Fig. 10, showing the probability of improvements of A-LIX over prior methods (computed with the Mann-Whitney U statistic (Mann & Whitney, 1947)) and the relative normalized performance proﬁles (Dolan & Mor´e, 2002). The ranges correspond to 95% stratiﬁed boot- strap conﬁdence intervals (Efron, 1992). In both DMC and Atari benchmarks, we ﬁnd that our improvements are sta- tistically signiﬁcant (lower conﬁdence intervals >0.5) and observe ‘stochastic dominance’ of our algorithm against almost all considered baselines (Dror et al., 2019). We pro- vide further results and details of the employed statistical analysis in App. A.1 and App. A.3 respectively. 6. Related Work Previous works have characterized several optimization is- sues related to performing RL via TD-learning (Baird & Moore, 1998; Baird, 1999). In this work, we instead fo- cus on the empirical analysis of modern TD-learning al- gorithms, speciﬁc to the pixel-based RL setting. We also observe links with recent work studying observational over- ﬁtting (Song et al., 2020). Our work differs by focusing on memorization effects particular to the combination of CNNs and TD-learning. There are also connections with existing feature-level augmentation work, such as Dropout (Srivastava et al., 2014) and DropBlock (Ghiasi et al., 2018). In particular, the latter also applies structured transforma- tions directly to the feature maps and introduces a heuristic to adjust this transformation over training, validating our ﬁndings on the utility of adaptivity. Outside RL, there is a rich body of work on implicit regularization and memoriza- tion in CNNs (Keskar et al., 2017; Neyshabur et al., 2017; Arpit et al., 2017; Liu et al., 2020; Maennel et al., 2020). Rahaman et al. (2019) show that higher frequency data man- ifolds cause CNNs to learn higher spectral frequency terms, aligning with our analysis of higher frequency representa- tions. Chatterjee (2020) show generalization arises when similar examples induce similar gradients during learning (i.e., coherence). Their work supports our ﬁndings since inconsistent feature gradients are a manifestation of non- coherence, explaining their poor generalization. Finally, our dual objective falls under automatic tuning methods in RL (AutoRL) (Parker-Holder et al., 2022). These ap- proaches have been applied very successfully to manage non-stationary trade-offs, such as exploration and exploita- tion (Ball et al., 2020) and optimism (Moskovitz et al., 2021; Cetin & Celiktutan, 2021). Finally, we note links with re- cent work concerning implicit regularization in TD-learning (Kumar et al., 2021). However, while Kumar et al. (2021) observe an implicit ‘underﬁtting’ phenomenon in later train- ing stages, we analyze an opposed ‘overﬁtting’ phenomenon occurring during the ﬁrst training steps, which we ﬁnd to be speciﬁc to learning from visual inputs. 7. Conclusion In this work, we provide a novel analysis demonstrating that instabilities in pixel-based off-policy RL come speciﬁcally from performing TD-learning with a convolutional encoder in the presence of a sparse reward signal. We show this visual deadly triad affects the encoder’s gradients, causing the critic to catastrophically self-overﬁt to its own noisy predictions. Therefore, we propose Adaptive Local SIgnal MiXing (A-LIX), a powerful regularization layer to explic- itly counteract this phenomenon. Applying A-LIX enables us to outperform prior state-of-the-art algorithms on pop- ular benchmarks without relying on image augmentations, auxiliary losses, or other notable design choices. Acknowledgments Edoardo Cetin and Oya Celiktutan would like to acknowl- edge the support from the Engineering and Physical Sci- ences Research Council [EP/R513064/1] and LISI Project [EP/V010875/1]. Philip J. Ball would like to thank the Wil- lowgrove Foundation for support and funding. Furthermore, support from Toyota Motor Corporation contributed towards funding the utilized computational resources. P(A-LIX > Y) Fraction of runs with score > τ SAC CURL DrQ DrQv2 1.00 0.75 0.50 0.25 0.00 A-LIX DrQv2 DrQ CURL SAC 0.60 0.75 0.90 0.0 0.5 1.0 P(A-LIX > Y) Fraction of runs with score > τ SPR DrQ CURL OTR DER SimPLe 1.00 0.75 0.50 0.25 0.00 A-LIX SPR DrQ CURL OTR DER SimPLe 0.6 0.7 0.8 0.9 0.0 1.0 2.0 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels References Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A., and Bellemare, M. G. Deep reinforcement learning at the edge of the statistical precipice, 2021. Allen-Zhu, Z. and Li, Y. Feature puriﬁcation: How adver- sarial training performs robust deep learning, 2021. Alsallakh, B., Kokhlikyan, N., Miglani, V., Yuan, J., and Reblitz-Richardson, O. Mind the pad – {cnn}s In International Conference can develop blind spots. on Learning Representations, 2021. URL https:// openreview.net/forum?id=m1CD7tPubNy. Arpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M. S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., and Lacoste-Julien, S. A closer look at memorization in deep networks. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Con- ference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 233–242. PMLR, 06– 11 Aug 2017. URL https://proceedings.mlr. press/v70/arpit17a.html. Baird, L. Reinforcement learning through gradient descent. Technical report, Carnegie-Mellon University, Depart- ment of Computer Science, 1999. Baird, L. and Moore, A. Gradient descent for general re- inforcement learning. Advances in neural information processing systems, 11, 1998. Ball, P., Parker-Holder, J., Pacchiano, A., Choromanski, K., and Roberts, S. Ready policy one: World building through active learning. In Proceedings of the 37th Inter- national Conference on Machine Learning, ICML. 2020. Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. Bellman, R. A markovian decision process. Indiana Univ. Math. J., 6:679–684, 1957. ISSN 0022-2518. Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. Boyd, S., Boyd, S. P., and Vandenberghe, L. Convex opti- mization. Cambridge university press, 2004. Brandfonbrener, D., Whitney, W. F., Ranganath, R., and Bruna, J. Ofﬂine RL without off-policy evaluation. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/ forum?id=LU687itn08w. Bus¸oniu, L., de Bruin, T., Toli´c, D., Kober, J., and Palunko, I. Reinforcement learning for control: Performance, stabil- ity, and deep approximators. Annual Reviews in Control, 46:8–28, 2018. Cetin, E. and Celiktutan, O. Learning pessimism for robust and efﬁcient off-policy reinforcement learning. arXiv preprint arXiv:2110.03375, 2021. Chatterjee, S. Coherent gradients: An approach to under- standing generalization in gradient descent-based opti- mization. arXiv preprint arXiv:2002.10657, 2020. Cobbe, K. W., Hilton, J., Klimov, O., and Schulman, J. Phasic policy gradient. In International Conference on Machine Learning, pp. 2020–2027. PMLR, 2021. Dolan, E. D. and Mor´e, J. J. Benchmarking optimization software with performance proﬁles. Mathematical pro- gramming, 91(2):201–213, 2002. Dror, R., Shlomov, S., and Reichart, R. Deep dominance- how to properly compare deep neural models. In Pro- ceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2773–2785, 2019. Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. Benchmarking deep reinforcement learning for continuous control. In International conference on machine learning, pp. 1329–1338. PMLR, 2016. Dulac-Arnold, G., Mankowitz, D., and Hester, T. Chal- arXiv lenges of real-world reinforcement learning. preprint arXiv:1904.12901, 2019. Dwibedi, D., Tompson, J., Lynch, C., and Sermanet, P. Learning actionable representations from visual observa- tions. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1577–1584. IEEE, 2018. Efron, B. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics, pp. 569–593. Springer, 1992. Finn, C., Tan, X. Y., Duan, Y., Darrell, T., Levine, S., and Abbeel, P. Learning visual feature spaces for robotic ma- nipulation with deep spatial autoencoders. arXiv preprint arXiv:1509.06113, 25, 2015. Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. D4{rl}: Datasets for deep data-driven reinforcement learning, 2021. Fujimoto, S., van Hoof, H., and Meger, D. Addressing func- tion approximation error in actor-critic methods. In ICML, pp. 1582–1591, 2018. URL http://proceedings. mlr.press/v80/fujimoto18a.html. Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Ghiasi, G., Lin, T.-Y., and Le, Q. V. Dropblock: A regular- ization method for convolutional networks. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa- Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As- sociates, Inc., 2018. URL https://proceedings. neurips.cc/paper/2018/file/ 7edcfb2d8f6a659ef4cd1e6c9b6d7079-Paper. pdf. Gogianu, F., Berariu, T., Rosca, M. C., Clopath, C., Bu- soniu, L., and Pascanu, R. Spectral normalisation for deep reinforcement learning: An optimisation per- In Meila, M. and Zhang, T. (eds.), Pro- spective. ceedings of the 38th International Conference on Ma- chine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 3734–3744. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/ v139/gogianu21a.html. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor- critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Confer- ence on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1861–1870. PMLR, 10– 15 Jul 2018a. URL https://proceedings.mlr. press/v80/haarnoja18b.html. Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018b. Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, pp. 2555–2565. PMLR, 2019. Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020. Hernandez-Garcia, J. F. and Sutton, R. S. Understanding multi-step deep reinforcement learning: A systematic study of the dqn target, 2019. Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostro- vski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D. Rainbow: Combining improvements in deep re- inforcement learning. In Thirty-second AAAI conference on artiﬁcial intelligence, 2018. Kaiser, L., Babaeizadeh, M., Milos, P., Osi´nski, B., Camp- bell, R. H., Czechowski, K., Erhan, D., Finn, C., Koza- kowski, P., Levine, S., Mohiuddin, A., Sepassi, R., Tucker, G., and Michalewski, H. Model based reinforce- ment learning for Atari. In International Conference on Learning Representations, 2020. Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293, 2018. Kearns, M. J. and Singh, S. P. Bias-variance error bounds for temporal difference updates. In Proceedings of the Thirteenth Annual Conference on Computational Learn- ing Theory, COLT ’00, pp. 142–147, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 155860703X. Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. On large-batch training for deep learn- ing: Generalization gap and sharp minima. In 5th Inter- national Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum?id=H1oyRlYgg. Kielak, K. P. Do recent advancements in model-based deep reinforcement learning really improve data efﬁciency? 2019. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kostrikov, I., Yarats, D., and Fergus, R. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations. 2021. Kumar, A., Agarwal, R., Ghosh, D., and Levine, S. Im- plicit under-parameterization inhibits data-efﬁcient deep In International Conference reinforcement learning. on Learning Representations, 2021. URL https:// openreview.net/forum?id=O9bnihsFfXU. Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A. Reinforcement learming with augmented In Advances in Neural Information Processing data. Systems 33. 2020a. Laskin, M., Srinivas, A., and Abbeel, P. CURL: Contrastive unsupervised representations for reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning, 2020b. Lee, A. X., Nagabandi, A., Abbeel, P., and Levine, S. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. arXiv preprint arXiv:1907.00953, 2019. Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Levine, S., Kumar, A., Tucker, G., and Fu, J. Ofﬂine rein- forcement learning: Tutorial, review, and perspectives on open problems, 2020. Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. Visualizing the loss landscape of neural nets. In Neural Information Processing Systems, 2018. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. Liu, S., Papailiopoulos, D., and Achlioptas, D. Bad global minima exist and sgd can reach them. Advances in Neural Information Processing Systems, 33, 2020. Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., and Bowling, M. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artiﬁcial Intelligence Research, 61:523–562, 2018. Maennel, H., Alabdulmohsin, I. M., Tolstikhin, I. O., Baldock, R., Bousquet, O., Gelly, S., and Keysers, D. What do neural networks learn when trained with random labels? In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 19693–19704. Curran Associates, URL https://proceedings. Inc., neurips.cc/paper/2020/file/ e4191d610537305de1d294adb121b513-Paper. pdf. 2020. Mann, H. B. and Whitney, D. R. On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other. The Annals of Mathematical Statistics, 18(1):50 – 60, 1947. doi: 10.1214/aoms/1177730491. URL https: //doi.org/10.1214/aoms/1177730491. Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spec- tral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=B1QRgziT-. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Moskovitz, T., Parker-Holder, J., Pacchiano, A., Arbel, M., and Jordan, M. Tactical optimism and pessimism for deep reinforcement learning. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https: //openreview.net/forum?id=a4WgjcLeZIn. Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. Exploring generalization in deep learning. In Proceed- ings of the 31st International Conference on Neural In- formation Processing Systems, NIPS’17, pp. 5949–5958, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964. Parker-Holder, J., Rajan, R., Song, X., Biedenkapp, A., Miao, Y., Eimer, T., Zhang, B., Nguyen, V., Calandra, R., Faust, A., Hutter, F., and Lindauer, M. Automated rein- forcement learning (autorl): A survey and open problems, 2022. Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F., Bengio, Y., and Courville, A. On the spec- tral bias of neural networks. In International Conference on Machine Learning, pp. 5301–5310. PMLR, 2019. Rosca, M., Weber, T., Gretton, A., and Mohamed, S. A case for new neural networks smoothness constraints. In ”I Can’t Believe It’s Not Better!” NeurIPS 2020 workshop, 2020. URL https://openreview.net/forum? id=_b-uT9wCI-7. Rummery, G. A. and Niranjan, M. On-line Q-learning using connectionist systems. Technical Report TR 166, Cam- bridge University Engineering Department, Cambridge, England, 1994. Schaul, T., Ostrovski, G., Kemaev, I., and Borsa, D. Return- based scaling: Yet another normalisation trick for deep rl, 2021. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv. org/abs/1707.06347. Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville, A., and Bachman, P. Data-efﬁcient reinforcement learn- ing with self-predictive representations. arXiv preprint arXiv:2007.05929, 2020. Shorten, C. and Khoshgoftaar, T. M. A survey on image data augmentation for deep learning. Journal of Big Data, 6(1):1–48, 2019. Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. Deterministic policy gradient algorithms. 2014. Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Grae- pel, T., et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Song, X., Jiang, Y., Tu, S., Du, Y., and Neyshabur, B. Ob- servational overﬁtting in reinforcement learning. In Inter- national Conference on Learning Representations, 2020. In International Conference reinforcement learning. on Learning Representations, 2022. URL https:// openreview.net/forum?id=_SJ-_yyes8. Zhu, H., Yu, J., Gupta, A., Shah, D., Hartikainen, K., Singh, A., Kumar, V., and Levine, S. The ingredients of real- world robotic reinforcement learning. arXiv preprint arXiv:2004.12570, 2020. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: A simple way Jour- to prevent neural networks from overﬁtting. nal of Machine Learning Research, 15(56):1929–1958, URL http://jmlr.org/papers/v15/ 2014. srivastava14a.html. Student. The probable error of a mean. Biometrika, 6 (1):1–25, 1908. ISSN 00063444. URL http://www. jstor.org/stable/2331554. Sutton, R. Learning to predict by the method of temporal differences. Machine Learning, 3:9–44, 08 1988. doi: 10.1007/BF00115009. Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural in- formation processing systems, pp. 1057–1063, 2000. Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, arXiv preprint A., et al. Deepmind control suite. arXiv:1801.00690, 2018. Van Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil, J. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018. van Hasselt, H. P., Hessel, M., and Aslanides, J. When to use parametric models in reinforcement learning? Advances in Neural Information Processing Systems, 32:14322– 14333, 2019. Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575 (7782):350–354, 2019. Individual comparisons by ranking Wilcoxon, F. Biometrics Bulletin, 1(6):80–83, 1945. methods. ISSN 00994987. URL http://www.jstor.org/ stable/3001968. Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R. Improving sample efﬁciency in model- free reinforcement learning from images. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 35(12): 10674–10681, May 2021. URL https://ojs.aaai. org/index.php/AAAI/article/view/17276. Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. Master- ing visual continuous control: Improved data-augmented Stabilizing Off-Policy Deep Reinforcement Learning from Pixels A. Detailed Results A.1. DMC Medium and Hard Tasks In Table 4, we show the performance in each of the evaluated 15 DMC environments by reporting the mean and standard deviations over the cumulative returns obtained midway and at the end of training for the medium and hard benchmark tasks, respectively. A-LIX attains state-of-the-art performance in the majority of the tasks at both reported checkpoints, while still closely matching DrQ-v2’s performance on the remaining tasks. On the other hand, DrQ-v2 struggles to consistently solve some of the harder exploration tasks such as Cartpole Swingup Sparse and Humanoid Run, as shown by the high standard deviations. Interestingly, unlike in the simpler DMC benchmark from Hafner et al. (2019) with higher action repeat, CURL appears have a slight edge over DrQ. In particular, the self-supervised signal from CURL appears to aid precisely in the sparse reward environments where DrQ-v2 struggles. Hence, this appears to suggest that including an additional self-supervised signal to the TD-loss, lessens the hindering effects of a lower-magnitude reward signal. We interpret this result as additional evidence showing how addressing any individual component of the deadly triad helps counteracting the catastrophic self-overﬁtting phenomenon. We also test the signiﬁcance of our results by performing a Wilcoxon signed-rank test (Wilcoxon, 1945) between A-LIX and DrQ-v2. We perform a paired rank test across both seeds and tasks, allowing us to obtain an p-value that takes into account both population size and relative performance gains across all tasks. The choice of Wilcoxon signed-rank test also does not presume normality in the distributions of performance which we believe is a more appropriate assumption than for instance a paired t-test (Student, 1908), despite a potential loss of statistical power. To ensure correct population pairing, A-LIX and DrQ-v2 seeds were identical, resulting in the same initially collected data and network initialization. Performing this test over all 15 tasks and 5 seeds, we achieve a p-value of 0.0057 at 50% total frames (1.5M and 15M for Medium and Hard respectively) and 0.0053 at 100% total frames (3.0M and 30M for Medium and Hard Respectively), much lower than the typical rejection criteria of p > 0.05. We therefore believe this shows clear evidence that our results in DMC are strongly statistically signiﬁcant. Table 4. Full results for the DeepMind Control Suite benchmark. Each displayed return is averaged over 10 random seeds and from 10 evaluation runs collected at each experience checkpoint. Medium tasks SAC CURL DrQ DrQv2 A-LIX (Ours) SAC CURL DrQ DrQv2 A-LIX (Ours) 1.5M frames 3.0M frames 8±9 9±8 6±5 24±27 Acrobot Swingup 256±47 Cartpole Swingup Sparse 118±233 479±329 318±389 485±396 Cheetah Run 507±114 788±59 792±29 Finger Turn Easy 190±137 297±150 199±132 854±73 Finger Turn Hard 79±73 174±106 100±63 491±182 Hopper Hop 184±127 268±91 198±102 0±0 Quadruped Run 164±91 129±97 419±204 68±72 Quadruped Walk 134±53 144±149 591±256 75±65 Reach Duplo 220±7 8±12 8±10 1±1 Reacher Easy 52±64 707±142 600±201 971±4 Reacher Hard 463±196 320±233 727±172 3±2 Walker Run 379±234 474±148 571±276 26±4 270±99 718±250 806±78 546±101 587±109 287±48 528±107 776±37 212±3 887±19 720±83 691±10 7±8 6±5 28±25 442±64 12±11 185±295 499±349 316±389 505±412 590±95 835±45 873±60 200±155 309±176 216±158 934±54 902±77 100±78 146±95 86±70 0±0 224±135 285±96 240±123 63±45 175±104 130±59 523±271 168±49 142±67 920±36 48±32 228±2 9±9 7±10 2±3 115±98 667±182 612±181 940±50 10±23 678±350 397±273 935±49 447±224 547±143 616±297 25±3 402±100 742±250 864±78 901±109 906±101 372±48 759±107 900±37 221±3 966±19 855±83 756±10 Average score 52.28 291.73 281.03 547.96 585.67 63.80 326.45 300.27 671.40 720.30 15.0M frames 30.0M frames Hard tasks Humanoid Walk Humanoid Stand Humanoid Run Average score SAC CURL DrQ DrQv2 A-LIX (Ours) SAC CURL DrQ DrQv2 A-LIX (Ours) 7±3 5±3 5±3 5.64 5±3 6±3 6±2 5.74 3±2 4±3 5±3 243±162 167±159 22±30 476±79 519±94 122±59 4.02 144.16 372.78 4±3 6±3 3±3 4.30 4±3 6±2 4±3 4.89 5±3 6±2 4±2 675±86 588±63 170±122 754±79 781±94 242±59 4.90 477.74 592.48 We now compare our results using the Rliable framework introduced in Agarwal et al. (2021) (see App. A.3 for a detailed explanation about the metrics introduced). Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Figure 11. Performance proﬁles at 50% (left) and 100% (right) of the total steps in Medium and Hard DMC Tasks. We plot performance proﬁles in Fig. 11 at both 50% and 100% the total training steps in DMC, which aim to represent sample efﬁciency and asymptotic performance respectively. We see that in almost all cases, A-LIX improves upon DrQ-v2. (a) Overall ranking statistics at 50% (top) and 100% (bottom) of the to- tal steps in Medium and Hard DMC Tasks. (b) Aggregate IQM (left) and Optimality Gap (right) metrics at 50% of the total steps in Medium and Hard DMC Tasks. We plot ranking statistics in Fig. 11 at both 50% and 100% the total training steps in DMC. We see that A-LIX clearly appears most in the 1st ranked column, and rarely appears in lower ranked (i.e., > 3), suggesting strong performance across all environments in DMC Medium and Hard. We also provide a further aggregated statistics plot in Fig. 12b (this time at 50% the total steps), which shows A-LIX is particularly sample-efﬁcient and consistent (i.e., low error bars) across all environments. (a) 50% total steps. (b) 100% total steps. Figure 13. Probability of Improvement statitistics at both 50% (left) and 100% (right) of the total timesteps in Medium and Hard DMC Tasks. In Fig. 13 we observe that A-LIX likely improves over prior work, and note that whilst the improvement probability over DrQ-v2 may seem slightly low at ∼60%, we note that this value is in line with statistics in prior works that achieve signiﬁcant gains (as seen in Agarwal et al. (2021)), and furthermore it does not take into account absolute performance values, and instead only compares relative values, which explains why the gains of A-LIX appear larger when evaluated under IQM and OG. Furthermore, the lower CI for 50% total steps does not fall below 0.5, which means improvements are indeed statistically signiﬁcant. 100% Total Steps 50% Total Steps τ > e r o c s h t i w s n u r f o n o i t c a r F 1.00 0.75 0.50 0.25 0.00 τ > e r o c s h t i w s n u r f o n o i t c a r F 1.00 0.75 0.50 0.25 0.00 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Normalized Score (τ) Normalized Score (τ) A-LIX DrQv2 DrQ CURL SAC 50% Total Steps 100 80 60 40 20 0 100 ) % n i ( n o i t c a r F 100% Total Steps 80 60 40 20 0 1 2 3 4 5 Ranking A-LIX DrQv2 DrQ CURL SAC A-LIX DrQv2 DrQ CURL SAC IQM Optimality Gap 0.2 0.4 0.6 0.45 0.60 0.75 0.90 Max Normalized Score P(A-LIX > Y) Y m h t i r o g A l SAC CURL DrQ DrQv2 0.60 0.75 0.90 Y m h t i r o g A l SAC CURL DrQ DrQv2 P(A-LIX > Y) 0.60 0.75 0.90 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels A.2. Atari 100k In Table 5, we show the ﬁnal average performance for all the evaluated algorithms in each of the twenty-six tasks in the Atari 100k benchmark. A-LIX outperforms SPR, the previous state-of-the-art off-policy algorithm on this benchmark, on 16 out of 26 tasks. Moreover, it attains comparatively similar performance on most of the remaining tasks despite using no augmentation, auxiliary losses, or model-based elements. Table 5. Full results for the Atari 100k benchmark, following the evaluation protocol from Machado et al. (2018). We report the results collected from 10 random seeds. Tasks Random Human SimPLe DER OTRainbow CURL DrQ SPR A-LIX (Ours) Alien Amidar Assault Asterix Bank Heist Battle Zone Boxing Breakout Chopper Command Crazy Climber Demon Attack Freeway Frostbite Gopher Hero Jamesbond Kangaroo Krull Kung Fu Master Ms Pacman Pong Private Eye Qbert Road Runner Seaquest Up N Down Human Norm. Mean Human Norm. Median # SOTA # Super Average Rank 227.80 5.80 222.40 210.00 14.20 2360.00 0.10 1.70 811.00 10780.50 152.10 0.00 65.20 257.60 1027.00 29.00 52.00 1598.00 258.50 307.30 -20.70 24.90 163.90 11.50 68.40 533.40 0.000 0.000 N/A N/A N/A 7127.70 1719.50 742.00 8503.30 753.10 37187.50 12.10 30.50 7387.80 35829.40 1971.00 29.60 4334.70 2412.50 30826.40 302.80 3035.00 2665.50 22736.30 6951.60 14.60 69571.30 13455.00 7845.00 42054.70 11693.20 1.000 1.000 N/A N/A N/A 616.9 88 527.2 1128.3 34.2 5184.4 9.1 16.4 1246.9 62583.6 208.1 20.3 254.7 771 2656.6 125.3 323.1 4539.9 17257.2 1480 12.8 58.3 1288.8 5640.6 683.3 3350.3 0.443 0.144 7 2 3.92 739.9 188.6 431.2 470.8 51 10124.6 0.2 1.9 861.8 16185.3 508 27.9 866.8 349.5 6857 301.6 779.3 2851.5 14346.1 1204.1 -19.3 97.8 1152.9 9600 354.1 2877.4 0.285 0.161 1 2 5.00 824.7 82.8 351.9 628.5 182.1 4060.6 2.5 9.8 1033.3 21327.8 711.8 25 231.6 778 6458.8 112.3 605.4 3277.9 5722.2 941.9 1.3 100 509.3 2696.7 286.9 2847.6 0.264 0.204 1 1 5.21 558.2 142.1 600.6 734.5 131.6 14870 1.2 4.9 1058.5 12146.5 817.6 26.7 1181.3 669.3 6279.3 471 872.5 4229.6 14307.8 1465.5 -16.5 218.4 1042.4 5661 384.5 2955.2 0.381 0.175 1 2 3.92 771.2 102.8 452.4 603.5 168.9 12954 6 16.1 780.3 20516.5 1113.4 9.8 331.1 636.3 3736.3 236 940.6 4018.1 9111 960.5 -8.5 -13.6 854.4 8895.1 301.2 3180.8 0.357 0.268 1 2 4.85 801.5 176.3 571 977.8 380.9 16651 35.8 17.1 974.8 42923.6 545.2 24.4 1821.5 715.2 7019.2 365.4 3276.4 3688.9 13192.7 1313.2 -5.9 124 669.1 14220.5 583.1 28138.5 0.704 0.415 4 7 2.88 902 174.27 660.53 809.5 639.4 14470 21.5 23.52 747 53166 888.15 31.04 1845.7 500.6 7185.85 341.5 6507 4884.04 16316 1258.4 6.03 100 2974 17471 654.6 5011.7 0.753 0.411 11 7 2.21 We now present additional evaluations under the Rliable framework, continuing on from the analysis in Fig. 10b. Figure 14. Performance proﬁles with linear (left) and logarithmic (right) scaling in Atari 100k. In Fig. 14 A-LIX performs noticeably better than previous work, and performs at least as well as SPR over all settings of τ > e r o c s h t i w s n u r f o n o i t c a r F 1.00 0.75 0.50 0.25 0.00 0.0 Score Distributions with Non Linear Scaling Score Distributions τ > e r o c s h t i w s n u r f o n o i t c a r F 1.00 0.75 0.50 0.25 0.00 0.5 1.0 Human Normalized Score (τ) 1.5 2.0 0.0 0.1 0.2 0.5 1.0 2.0 Human Normalized Score (τ) A-LIX SPR DrQ CURL OTR DER SimPLe Stabilizing Off-Policy Deep Reinforcement Learning from Pixels normalized scores. (a) Ranking statistics. (b) Probability of improvement statistics. Figure 15. Bootstrapped ranking statistics (left) and probability of improvement plots (right) on Atari 100k. In Fig. 15a A-LIX constitutes the majority of the algorithms ranked in 1st, and shows far fewer instances of being ranked in lower positions (i.e., > 4). In Fig. 15b we observe A-LIX likely improves upon prior work. Similar to Fig. 13, while the ∼60% improvement value over SPR may seem low, this is justiﬁed due to shortcomings in this metric, such as not taking into account actual performance values, and instead relative improvements. Furthermore, the lower CI does not fall below 0.5, which means improvements due to A-LIX are statistically signiﬁcant. A.3. Rliable: A Primer In addition to providing traditional methods of evaluation (e.g., performance tables, signiﬁcance testing), we use robust metrics and evaluation strategies introduced in Rliable (Agarwal et al., 2021). Rliable advocates for computing aggregate performance statistics not just across many seeds, but also across the many tasks within a benchmark suite. We give details on how these metrics achieve reliable performance evaluation in RL, denoting number of seeds as N and number of tasks as M . We follow Agarwal et al. (2021) as closely as possible; please refer to their paper for further details. A.3.1. SEED AND TASK AGGREGATION In order to aggregate performances across different tasks in the same benchmark suite, we must ﬁrst normalize each benchmark to the same range. In Atari, this is usually done by normalizing scores with respect to those achieved by humans, and in DMC this is done with respect to the maximum achievable score (i.e., 1, 000). We refer to this normalized score as τ . A.3.2. IQM AND OG Interquartile Mean (IQM) takes the middle 50% of the runs across seeds and benchmarks (i.e., [N M/2]) and then calculates its mean score, improving outlier robustness whilst maintaining statistical efﬁciency. Optimality Gap (OG) calculates the proportion of performances (N M ) that fail to meet a minimum threshold γ, with the assumption that improvements beyond γ are not important. In both cases, stratiﬁed bootstrap sampling is used to calculate conﬁdence intervals (CIs). A.3.3. PERFORMANCE PROFILES Performance proﬁles are a form of empirical CDF, but with stratiﬁed bootstrap sampling to produce conﬁdence bands that account for the underlying variability of the score. We can also establish ‘stochastic dominance’ by observing whether one method’s performance proﬁle is consistently above another’s for all normalized performance values τ . A.3.4. RANKING Ranking shows the proportion of times a given algorithm ranks in a given position across all tasks, with distributions produced using stratiﬁed bootstrap sampling having 200, 000 repetitions. A.3.5. PROBABILITY OF IMPROVEMENT Probability of improvement is calculated by calculating the Mann-Whitney U-statistic (Mann & Whitney, 1947) across all M tasks. The distribution is then plotted as a boxplot, and if the lower CI > 0.5, the improvement is statistically signiﬁcant. ) % n i ( n o i t c a r F 100 80 60 40 20 0 1 2 3 4 5 6 7 Ranking A-LIX SPR DrQ CURL OTR DER SimPLe P(A-LIX > Y) Y m h t i r o g A l SPR DrQ CURL OTR DER SimPLe 0.6 0.7 0.8 0.9 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels B. Experiments Description B.1. Ofﬂine Experiments We follow the original training hyperparameters of DrQ-v2, and run policy evaluation and policy improvement until we saw convergence in the TD-loss, which would occur at similar points in all agents (i.e., between 10-20k and 5-10k steps of SGD in policy evaluation and policy iteration respectively). For the proprioceptive experiments, we keep everything consistent, except the input to the critic and actor MLP layers are now the proprioceptive states from the DMC simulator, not the latent representation z from the encoder. That is to say we do not modify the MLP architectures nor their learning rates in the interests of a fair comparison. Furthermore, for any given seed of the ofﬂine experiment, we also instantiate all networks in the agents identically and train on the same random ofﬂine data, with minibatches presented in the same order. We also note that a similar algorithm is described in Brandfonbrener et al. (2021), but in the context of minimizing extrapolation errors. Now we present some additional analysis to provide further context to our ofﬂine experiments. First, we see that the proprioceptive statistics mirror those of the augmented agent, further illustrating the crucial role of CNN regularization for successful TD-learning from pixels: Figure 16. Q values and Pearson Correlation of the ofﬂine Proprioceptive agent on an ofﬂine ﬁxed batch. Secondly, we observe that the exact same self-overﬁt also manifests in the online setting by plotting the Pearson correlation values over the initial stages of training in 5 seeds, conﬁrming that phenomena of our ofﬂine analysis applies to the online RL problem: Figure 17. Pearson Correlation of augmented and non-augmented online agents in Cheetah Run and Quadruped Walk across 5 seeds. Shaded lines represent individual runs, and solid lines represent the median. We see that augmented agents do not immediately overﬁt to their target networks, and become correlated only after useful signal is learned. B.2. Jacobian Analysis In order to measure local sensitivity, we linearize the encoder around its input using a Taylor series expansion. Consider an N -dimensional input x ∈ RN and perturbation (cid:15) ∈ RN , an M -dimensional output y ∈ RM , and a function F : RN → RM . Now, performing a Taylor series expansion around ˜x: F(˜x + (cid:15)) = F(˜x) + (cid:15)F(x)∇T |x=˜x + (cid:15)2 2 ∇F(x)∇T |x=˜x + . . . ≈ F(˜x) + J(˜x)(cid:15) = ˜y (6) (7) (8) Q values during training l n o i t a e r r o C n o s r a e P 1.5 1.0 0.5 0.0 0.5 l s e u a V Q 0 2000 4000 6000 8000 SGD Steps Correlation of Q with Qtarget and r 1.0 0.8 0.6 0.4 0.2 0.0 Qtarget r 0 2000 4000 6000 8000 SGD Steps l n o i t a e r r o C n o s r a e P 1.0 0.8 0.6 0.4 0.2 Cheetah Run 20000 40000 Frames Quadruped Walk 1.0 0.9 0.8 0.7 0.6 5000 7500 10000 12500 Qtarget Augs Qtarget No Augs Frames r Augs r No Augs Stabilizing Off-Policy Deep Reinforcement Learning from Pixels where we make the approximation in the second line by dropping the second order/Hessian and higher terms under the assumption the perturbation vector (cid:15) is small. This allows us to write F in the form of a local linear system: y = F(x) + J(x)(cid:15). It is straightforward to see that if the entries of the Jacobian matrix J are larger, then small perturbations (cid:15) will cause larger changes in the output y. To measure the magnitude of the Jacobian entries, we take the Frobenius norm: ||J(x)||F = (cid:88) (cid:88) n m (cid:19)2 (cid:18) ∂Fm(x) ∂xn (9) where xn is the ‘n’th entry of x and Fm is the ‘m’th entry of the codomain of F. The calculation of the Jacobian is trivial through the use of an automatic differentiation framework. In our analysis we calculate the Jacobians of both agents on of a ﬁxed batch of 128 frame stacked images taken from the ofﬂine training dataset, and compare the corresponding ratios of their Frobenius norms, and take this average ratio over the batch across 4 seeds. C. Additional Analysis C.1. Adaptive ND Dual Objective Optimization The alternative N D score with increased outlier robustness, (cid:103)N D, proposed in Section 4.3 is inspired by recordings of signal-to-noise ratio measurements. In particular, by passing the individual normalized D(z) terms through a log(1 + x) smoothing function we downweight the effect that large individual outliers might have on this aggregated metric. We would like to remark that since we set up the optimization of S with a dual objective, changes in the actual target value relating to some appropriate smoothness constraint are mostly irrelevant when considering the optimization’s dynamics. Therefore, we argue that tuning S with the actual N D should not considerably diverge from tuning S based on a re-scaled appropriate target for (cid:103)N D. We provide further plots comparing agent performance and respective adaptive parameter S during training: Figure 18. Performance of agents across 4 different seeds of the Cheetah Run environment and their adaptive scalar parameter S. We observe that initially, S is high until agents learn useful behaviors, whereupon it drops to maintain ND due to presence of useful signal in the feature gradients. Figure 19. Performance of agents across 4 different seeds of the Quadruped Run environment and their adaptive scalar parameter S. We observe that as meaningful behaviors are learned in agents towards the end of training, S falls accordingly, whereupon it drops to maintain ND due to presence of useful signal in the feature gradients. We see the same effect in these two contrasting environments; in Cheetah Run, where learning is more stable due to more predictable initializations and fewer degrees of freedom, we see the A-LIX parameter S drop almost immediately as the TD-targets quickly become more accurate. In the less stable Quadruped Run, we also notice this annealing effect, however this occurs later on in training, when the agent can consistently recover from poor initializations. Seed 0 Seed 1 Seed 2 Seed 3 500 n r u t e R 0 0.0 1.0 0.5 Frames (×106) 1.5 0.0 1.0 0.5 Frames (×106) 1.5 Agent Return 0.0 1.0 0.5 Frames (×106) A-LIX Parameter S 1.5 2 1 l e u a V S X I L - A 0.0 1.0 0.5 Frames (×106) 1.5 Seed 0 Seed 1 Seed 2 Seed 3 800 600 n r u t e R 400 1.5 2.0 2.5 3.0 1.5 2.0 2.5 3.0 1.5 2.0 2.5 3.0 1.5 2.0 2.5 3.0 Frames (×106) Frames (×106) Agent Return Frames (×106) A-LIX Parameter S Frames (×106) l e u a V S X I L - A 0.6 0.4 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels C.2. N-Step Returns Large n-step rewards have become an important part of many algorithms that use TD-learning from visual observations. As motivated in Section 3, large n-step rewards can help towards mitigating self-overﬁtting by densifying the reward and downweighting the contribution of the inaccurate target critic, especially early in training; indeed as shown in (Yarats et al., 2022), using 1-step learning has a signiﬁcant negative impact on performance. However, it is known that there is a bias-variance trade-off with multi-step approaches (Kearns & Singh, 2000), and furthermore, almost all approaches using this method do not apply off-policy bias correction when sampling from a replay buffer. While we motivate the use of n-step returns as a way to mitigate self-overﬁtting through incurring fewer 0 reward tuples (especially common in sparse reward environments early in training), we believe there is evidence to show that this introduces bias when n is sufﬁciently large, despite prior work suggesting this is not the case (Hernandez-Garcia & Sutton, 2019). Figure 20. Returns of agents over 5 seeds. Solid lines represent median performance, faded lines represent individual runs. We show in Fig. 20 that 10-step (as is commonly done in algorithms used to solve Atari) returns can mitigate failure seeds as predicted under the visual deadly triad framework (indeed in Cheetah Run there are no seeds that completely ﬂat-line when 10-step returns are used). However, we also see evidence that applying 10-step returns can have negative impacts on convergence and asymptotic performance in Cheetah Run when the deadly triad is sufﬁciently managed, such as using augmentations; in Quadruped Run we see moderate beneﬁt initially, but note that asymptotically the 10-step and 3-step agents converge to the same performance. We also provide further evidence in App. E.3, where applying 10-step returns to an A-LIX agent generally has a laregely negative impact on performance. Finally, we note that trying 20-step returns, as is done in some algorithms that solve Atari (Laskin et al., 2020b), caused signiﬁcant performance reductions in DMC. In conclusion, this provides evidence that we should consider using lower values of ‘n’ in multi-step returns, and achieve this through addressing other elements of the deadly triad. Cheetah Run Quadruped Walk 750 500 250 n r u t e R 0 0 1 2 3 0 1 2 3 Frames (×106) Frames (×106) Augs 3-step No Augs 3-step Augs 10-step No Augs 10-step Stabilizing Off-Policy Deep Reinforcement Learning from Pixels D. Implementation Details In Tables 6 and 7 we provide the full list of hyperparameters used in our implementations for DMC and Atari 100k, respectively. We show signiﬁcant differences from standard practices in bold. In particular, A-LIX uses the same encoder architecture and n-step returns for both benchmarks, highlighting its lower reliance to environment-speciﬁc heuristics. Moreover, unlike prior state-of-the-art algorithms it does not employ any data augmentation or auxiliary loss function. These factors show the effectiveness of our adaptive method in counteracting instabilities from the visual deadly triad without any additional help, highlighting its applicability. Table 6. Full hyperparameters list used for the DeepMind Control A-LIX experiments. Bolded values represent signiﬁcant differences from canonical implementations. DDPG-integration hyperparameters (following (Yarats et al., 2022)) Replay data buffer size Batch size Minimum data before training Random exploration steps Optimizer Policy/critic learning rate Policy/critic β1 Critic UTD ratio Policy UTD ratio Discount γ Polyak coefﬁcient ρ N-step returns Hidden dimensionality Feature dimensionality Nonlinearity Exploration stddev. clip Exploration stddev. schedule Augmentations 1000000 (100000 for Quadruped Run) 256 (512 for Walker Run) 4000 2000 Adam (Kingma & Ba, 2014) medium: 0.0001 hard: 0.00008 0.9 0.5 0.5 0.99 0.99 3 (1 for Walker Run) 1024 medium: 50 hard: 100 ReLU 0.3 medium: linear: 1 → 0.1 in 500000 steps hard: linear: 1 → 0.1 in 2000000 steps OFF A-LIX-speciﬁc hyperparameters Initial maximum sampling shift S Normalized discontinuity targets N D Maximum sampling shift learning rate Maximum sampling shift β1 1.0 0.635 0.003 0.5 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Table 7. Full hyperparameters list used for the Atari 100k A-LIX experiments. Bolded values represent signiﬁcant differences from canonical implementations. DER-integration hyperparameters Gray-scaling Down-sampling Frames stacked Action repetitions Reward clipping Max episode frames Replay data buffer size Replay period every Batch size Minimum data before training Random exploration steps Optimizer Critic learning rate Critic β1 Critic (cid:15) Max gradient norm Critic UTD ratio Discount γ Target update period N-step returns Feature maps Filter sizes Strides Hidden dimensionality Feature dimensionality Nonlinearity Exploration noisy nets parameter Augmentations True 84 × 84 4 4 [−1, 1] 108000 100000 1 32 1600 1600 Adam (Kingma & Ba, 2014) 0.0001 0.9 0.000015 10 2 0.99 1 3 32, 32, 32 3 × 3, 3 × 3, 3 × 3 2, 1, 1 256 50 ReLU 0.1 OFF A-LIX-speciﬁc hyperparameters Initial maximum sampling shift S Normalized discontinuity targets N D Maximum sampling shift learning rate Maximum sampling shift β1 1.0 0.75 0.0001 0.5 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels E. Additional Ablations E.1. Smoothness Regularization through Spectral Normalization To distinguish between general smoothness contraints in convolutional features, and the smoothness that arises as a result spatial consistency, we apply spectral normalization (Miyato et al., 2018) to the ﬁnal convolutional layer in the encoder to represent the former class of constraints. Spectral normalization operates on the parameters of a network and constrains its outputs to be 1-Lipschitz and has shown beneﬁts in prior work (Gogianu et al., 2021), but does not explicitly enforce a spatial regularization in the features. We train agents without augmentations using spectral normalization. Figure 21. Returns of agents over 5 seeds. Solid lines represent median performance, faded lines represent individual runs. We see that whilst there is clear improvement above the original non-augmented agents in some cases, the performance is still lower than agents that use spatial consistency regularization, such as random shift augmentations. E.2. Is Gradient Smoothing All We Need? Following the argument in Section 4.1, we can view augmentations as a gradient smoothing regularizer. This naturally leads us to ask the following: can we replace the stochastic shifting mechanism with a ﬁxed smoothing mechanism? To test this, we instead apply a Gaussian smoothing kernel to the feature gradients in the CNN, and utilize our N D score to vary the width of the kernel adaptively through training; we call this method A-Gauss (Adaptive Gaussian Feature Gradient Kernel). Figure 22. Returns of agents over 5 seeds. Solid lines represent median performance, faded lines represent individual runs. We see that while there is improvement over non-augmented agents, overall performance is still lower than even simple non-adaptive augmentation. We believe this is due to the Gaussian kernel having too signiﬁcant an effect on the information contained in the feature gradients during backpropagation, causing information to be lost. We believe this explains the effectiveness of shift-augmentations in reinforcement learning, which is that they effectively balance the information contained in the gradients, as well as ensuring their smoothness to reduce overﬁtting. E.3. Ablations to A-LIX We now provide a set of ablations on both DMC and Atari, assessing the impact of individual components in A-LIX. Cheetah Run Quadruped Walk 750 500 250 n r u t e R 0 800 600 400 200 0 1 2 3 0 1 2 3 Frames (×106) Frames (×106) Augs Spectral Normalization Cheetah Run Quadruped Walk 750 500 250 n r u t e R 0 800 600 400 200 0 1 2 3 0 1 2 3 Frames (×106) Frames (×106) Augs A-Gauss Stabilizing Off-Policy Deep Reinforcement Learning from Pixels (a) DMC Control ablations in Cheetah Run (left) and Quadruped Run (right) evaluated over 4 seeds. (b) Atari 100k ablations evaluated over 4 seeds in 4 different Atari 100k tasks. Figure 23. An ablation study of A-LIX, showing the contribution of its individual components to ultimate performance in DMC and Atari 100k. In Fig. 23a we choose the following ablations for DMC: • A-LIX • Adaptive Random Shifts (where the magnitude of the random shift image augmentation is adjusted using the dual ND objective) • LIX • Random Shifts (i.e., DrQ-v2) While we see a slight asymptotic performance improvement in Cheetah Run by using LIX layers instead of random shifts, we notice signiﬁcant differences in the less stable Quadruped Run environment. Concretely, we see much greater stability in both LIX approaches compared with image augmentation approaches, with the former having no failure seeds. Furthermore, we observe stronger asymptotic performance with the inclusion of the adaptive dual objective for both approaches. As motivated in Fig. 19, this is likely a result of reducing the shift parameter as the signal in the target values increases. In Fig. 23b, we choose the following ablations for Atari 100k on a subset of environments that represent a diverse set of tasks and performances with baseline algorithms: • A-LIX • Adaptive Random Shifts (as before) • LIX • A-LIX with 10-step returns • Random Shifts We see that A-LIX performs consistently strongly across the environments tested, always placing in the top 2 with regards to Human Normalized Score. We also notice that generally, LIX layer methods outperform random shift methods apart from in Crazy Climber, where the opposite is true. We believe this may be due to random shift augmentations actually reﬂecting the inductive biases concerning generalization in this environment, and believe this merits further investigation. Finally, we observe that using 10-step returns instead of 3 generally harms performance with A-LIX, with justiﬁcation given in App. C.2. n r u t e R 900 800 700 600 500 400 300 Cheetah Run Quadruped Run 800 600 400 200 A-LIX Adaptive Random Shifts LIX Random Shifts (DrQ-v2) 0 1 Frames (×106) 2 3 0 1 Frames (×106) 2 3 e r o c S d e z i l a m r o N n a m u H 2.0 1.5 1.0 0.5 0.0 A-LIX Adaptive Random Shifts LIX A-LIX 10-step Random Shifts Battle Zone Crazy Climber Ms Pacman Pong Atari 100k Task Stabilizing Off-Policy Deep Reinforcement Learning from Pixels F. Additional Ofﬂine Experiment Analysis F.1. Behavior Cloning without Augmentations Figure 24. Returns of agents over 5 seeds. Solid lines represent median performance, faded lines represent individual runs. The grey dotted horizontal line represents mean expert performance. To illustrate that test time shift invariance is not required, we show that it is possible to learn a policy through supervised learning. To do this, we generate a pixel-based dataset of 500,000 timesteps under an expert policy in Cheetah Run, and jointly train a CNN encoder and policy using behavior cloning/supervised learning by minimizing the loss L = (a − π(o))2 until convergence, where o follows the stacked frame image inputs of (Mnih et al., 2013). We see that the pixel-based policy performs as well as the behavior agent, despite using both higher dimensional data and fewer than half the samples compared to existing expert ofﬂine RL benchmarks from proprioceptive states (Fu et al., 2021). This provides clear evidence that shift invariance is not required at test time, and motivates us to ﬁnd an alternative explanation for why random shift augmentations help the learning process in TD-learning. An alternative perspective is that when the learning signal is strong, as is the case for supervised learning (and later stages during online learning when target values are more accurate), the natural bias of CNNs to learn lower order representations acts as an implicit regularizer (Rahaman et al., 2019) that results in test-time generalization. F.2. Turning Off Augmentations We present more evidence showing that augmentations beneﬁt learning the most at the beginning of training. In Fig. 25 we show the effect of turning off augmentations at 200,000 steps in Cheetah Run, and at 500,000 in Quadruped Walk. In both instances, we see large improvements over not augmenting at all, and both nearly converge to the same value as DrQ-v2, showing further evidence that stability initially in learning is vital. We posit that turning off augmentations here did not yield similar beneﬁts to Fig. 2 due to the fact that there is still high-frequency information in the targets (consider that the augmentations in Cheetah Run are switched off signiﬁcantly earlier) that cause a marginal amount of self-overﬁtting, reducing the rate of learning due to feature space degeneration. Figure 25. Returns of agents over 5 seeds. Solid lines represent median performance, faded lines represent individual runs. The grey dashed line shows when augmentations are turned off. Cheetah Run: BC 900 Expert Perfomance 800 n r u t e R 700 600 0 No Augs 4 1 2 3 SGD Steps (×105) Cheetah Run Quadruped Walk Augs Turned Off Augs Turned Off 750 500 250 n r u t e R 0 0 1 2 3 0 1 2 3 Frames (×106) Frames (×106) Augs No Augs Augs Removed Stabilizing Off-Policy Deep Reinforcement Learning from Pixels F.3. Action-Value Surfaces Here we show the action-value surfaces of the ofﬂine agents’ critics at various tuples sampled from the data. This provides us with an intuition over the loss landscape that the policies will be optimizing during the policy improvement, as accordingly the policy under the deterministic policy gradient (Silver et al., 2014) updates its own weights towards maximizing the action-values deﬁned by the critic through the chain rule: ∇φJπ ≈ Es∼E (cid:2)∇aQθ(s, a)|a=fφ(s)∇φfφ(s)(cid:3) (10) where φ and θ are policy and critic weights respectively. We hypothesize that self-overﬁtting reduces the sensitivity of the critic to actions, discarding important information regarding the causal link between actions and expected returns. To evaluate this, we sample state-action pairs from our replay buffer, and then visualize the action-value surface by sampling two random orthogonal direction vectors from the action space A. We then normalize the direction vectors to have a 2-norm of 1, and then multiply each direction vector by scalars α, β ∈ [−2, 2] respectively. We then plot the action-value surface as a result of adding the random vectors multiplied by their respective scalars onto the action sampled from ofﬂine dataset, giving us a 3-D surface. We clip actions to a ∈ [−1, 1]|A| as actions are squashed to this range in the policy through a truncated normal distribution. (a) Random Sampled State-Action Pair 1 (b) Random Sampled State-Action Pair 2 (c) Random Sampled State-Action Pair 4 (d) Random Sampled State-Action Pair 4 Figure 26. Action-Value loss surface plotted with respect two orthogonal random directions sampled from the action space (i.e., dr ∈ A and d1 ⊥ d2). We see that the critics learned by the augmented agents are more sensitive to changes in action. We believe this is due to the non-augmented agents overﬁtting to the observations, thus ignoring the lower-dimensional action inputs. To validate this, we sampled 128 random state-action tuples from the ofﬂine buffer, and calculated the average variance across the loss surfaces. We see a signiﬁcant difference, with the augmented agent having an average loss surface variance of 0.0129, whereas the non-augmented agent has an average loss surface variance of 0.0044, suggestive of lower sensitivity. F.4. Evidence of Critic MLP Overﬁtting from High-Frequency Features We provide further evidence that measuring high-frequency features through the ND score is vital to understanding overﬁt by showing how overﬁtting is able to occur in the fully-connected critic layers, which are usually stable under proprioceptive observations (see Table 2). To do this, we construct a pattern containing high frequency checkerboard noise c ∈ RH×W , and produce as many patterns as there are channels C in the ﬁnal layer. To ensure consistency across each individual feature map, we normalize each checkerboard pattern by the maximum value in its respective feature map, and then divide by the width of the checkerboard. We then add this pattern multiplied by a scalar α onto each feature map. Critic with Augmentations Critic w/o Augmentations 0.2 0.4 0.6 0.8 0 1 A ctio n S u bsp ace 2 1 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 0 1 1 0 Loss 1 0 Action Subspace 1 1 Critic with Augmentations Critic w/o Augmentations 0.6 0.8 1.0 0 1 A ctio n S u bsp ace 2 1 0.6 0.8 1.0 1 0.6 0.8 1.0 1 0 1 1 0 Loss 1 0 Action Subspace 1 1 Critic with Augmentations Critic w/o Augmentations 0.4 0.6 0.8 0 1 A ctio n S u bsp ace 2 1 0.4 0.6 0.8 1 0.4 0.6 0.8 1 0 1 1 0 Loss 1 0 Action Subspace 1 1 Critic with Augmentations Critic w/o Augmentations 0.4 0.6 0.8 1 0 A ctio n S u bsp ace 2 1 0.4 0.6 0.8 0.4 0.6 1 0 0.8 Loss 1 0 1 1 1 0 Action Subspace 1 1 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels (a) Example checkerboard artefacts. (b) Sensitivity of agents to checkerboard artifact weight Figure 27. Effect of checkerboard artifacts on feature maps and resultant loss sensitivity. We see the non-augmented agent is signiﬁcantly more sensitive to this high-frequency noise. As we see, the loss is signiﬁcantly more sensitive to high-frequency perturbations in the non-augmented agent, justifying its reliance on high-frequency patterns in the feature maps to enable self-overﬁtting. F.5. Additional Loss Surfaces Here we show the loss surfaces of the ofﬂine agents under policy evaluation with at 1,000, 5,000, and 10,000 training steps. We also show the surfaces respect to only the MLP layers, again following the normalization approach of Li et al. (2018). (a) 1,000 SGD Steps (b) 5,000 SGD Steps (c) 10,000 SGD Steps Figure 28. Loss surface plotted with respect to Encoder parameters at various stages of training. (a) 1,000 SGD Steps (b) 5,000 SGD Steps (c) 10,000 SGD Steps Figure 29. Loss surface plotted with respect to Critic MLP parameters at various stages of training. As we see, the loss surface with respect to the MLP parameters is signiﬁcantly less sharp, lending further evidence that self-overﬁtting is predominately a result of the ﬂexibility of the CNN layers to learn high-frequency features. Augmented Agent Non-Augmented Agent 0.4 0.3 0.2 s s o L D T 0.1 0.0 0.00 Non Augs Augs 0.25 0.50 (Checkerboard Weight) 0.75 Critic with Augmentations Critic w/o Augmentations s s o L D T 0.15 0.10 0.05 1 0 W eig ht S u bsp ace 2 1 0.15 0.10 0.05 1 0.15 0.10 0.05 0 TD Loss 1 0 1 1 1 0 Weight Subspace 1 1 Critic with Augmentations Critic w/o Augmentations s s o L D T 0.75 0.50 0.25 0 1 W eig ht S u bsp ace 2 1 0.75 0.50 0.25 1 0.75 0.50 0.25 0 TD Loss 1 0 1 1 1 0 Weight Subspace 1 1 Critic with Augmentations Critic w/o Augmentations s s o L D T 0.75 0.50 0.25 1 0 W eig ht S u bsp ace 2 1 0.75 0.50 0.25 1 0.75 0.50 0.25 0 TD Loss 1 0 1 1 1 0 Weight Subspace 1 1 Critic with Augmentations Critic w/o Augmentations s s o L D T 4 2 1 0 W eig ht S u bsp ace 2 1 4 2 4 2 1 0 TD Loss 1 0 1 1 1 0 Weight Subspace 1 1 Critic with Augmentations Critic w/o Augmentations s s o L D T 20 10 1 0 W eig ht S u bsp ace 2 1 20 10 20 10 1 0 TD Loss 1 0 1 1 1 0 Weight Subspace 1 1 Critic with Augmentations Critic w/o Augmentations s s o L D T 10 5 1 0 W eig ht S u bsp ace 2 1 10 5 10 5 1 0 TD Loss 1 0 1 1 1 0 Weight Subspace 1 1",1
"Watch and Match: Supercharging Imitation with Regularized Optimal
  Transport","[{'href': 'http://arxiv.org/abs/2206.15469v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2206.15469v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-30 17:58:18,,"2 2 0 2 n u J 0 3 ] h p - t n a u q [ 1 v 5 0 4 5 1 . 6 0 2 2 : v i X r a Multivariate trace estimation in constant quantum depth Yihui Quek,1, 2 Eneet Kaur,3, 4 and Mark M. Wilde5, 6 1Dahlem Center for Complex Quantum Systems, Freie Universit¨at Berlin, 14195 Berlin, Germany 2Information Systems Laboratory, Stanford University, Stanford, California 94305, USA 3Institute for Quantum Computing and Department of Physics and Astronomy, University of Waterloo, Waterloo, Ontario, Canada N2L 3G1 4Wyant College of Optical Sciences, University of Arizona, Tucson, AZ 85721, USA 5Hearne Institute for Theoretical Physics, Department of Physics and Astronomy, and Center for Computation and Technology, Louisiana State University, Baton Rouge, Louisiana 70803, USA 6School of Electrical and Computer Engineering, Cornell University, Ithaca, New York 14850, USA (Dated: July 1, 2022) There is a folkloric belief that a depth-Θ(m) quantum circuit is needed to estimate the trace of the product of m density matrices (i.e., a multivariate trace). We prove that this belief is overly conservative by constructing a constant quantum-depth circuit for the task, inspired by the method of Shor error correction. Furthermore, our circuit demands only local gates in a two dimensional circuit – we show how to implement it in a highly parallelized way on an architecture similar to that of Google’s Sycamore processor. With these features, our algorithm brings the task of multivariate trace estimation, crucial to applications in condensed matter and estimating nonlinear functions of quantum states, closer to the capabilities of near-term quantum processors. We instantiate the latter application with a theorem on estimating nonlinear functions of quantum states with “well-behaved” polynomial approximations. Contents I. INTRODUCTION I. Introduction II. The principle: using cyclic shifts for multivariate trace estimation III. Our circuit construction A. Preparing GHZ states in constant quantum depth B. Multiply-controlled cyclic permutation in constant quantum depth C. Explicit circuit description IV. Implementation of multivariate trace estimation on a two-dimensional architecture V. Guarantees of our estimator VI. Applications of our method VII. Conclusion Acknowledgments References 1 2 3 3 4 5 6 6 8 10 10 11 A. Proof of faithfulness and data processing 12 The task of estimating quantities like Tr[ρ1 · · · ρm] ‘Multivariate traces’ (1) to copies of given access the quantum states ρ1 through ρm is a fundamental building block in quan- tum information science. This subroutine, which we call ‘multivariate trace estimation’, opens the door to esti- mating nonlinear functions of quantum states [1, 2], such as quantum distinguishability measures [3], integer R´enyi entropies [4], and entanglement measures [5]. This esti- mation procedure is a component of many quantum pro- tocols such as quantum ﬁngerprinting [3] and quantum digital signatures [6]. When ρi = (cid:37) for all i ∈ [m] in Eq. (1), an impor- tant application of multivariate trace estimation is to entanglement spectroscopy [4] – deducing the full set of eigenvalues {λ1, . . . , λD} (the ‘spectrum’) of (cid:37), where (cid:37) is the reduced state of a bipartite pure state (that is, (cid:37) = TrB(ψAB) with ψAB a bipartite pure state). The spectrum unlocks a wealth of information about the properties of (cid:37). The smallest eigenvalue of (cid:37) diagnoses whether ψ is separable or entangled [5]. In addition, the inverse of the smallest eigenvalue acts as a condition number for many quantum algorithms that manipulate quantum states (see for instance [7–9]), and it constrains their runtime. The entanglement spectrum is also use- ful to identify topological order [10–13], emergent irre- versibility [14], quantum phase transitions [15], and to determine if the system obeys an area law [16]. With the wealth of applications described above, there has been much interest in bringing multivariate trace esti- mation within the reach of near-term quantum hardware. A glimmer of hope in this regard is the observation that quantities like Eq. (1) can be estimated without the need for full state tomography. In the quantum information sphere, one of the progenitors of this line of thinking was Ref. [1], which proposed a method leveraging the follow- ing well-known identity (related to the replica trick orig- inating in spin glass theory [17]): Tr[W π(ρ1 ⊗ · · · ⊗ ρm)] = Tr[ρ1 · · · ρm] , (2) where the right-hand-side is the multivariate trace we would like to estimate, and W π is a unitary representa- tion of the cyclic shift permutation π := (1, 2, . . . , m) . (3) Here, π represents a cyclic permutation that sends 1 to 2, 2 to 3, and so forth. That is to say, multivariate trace estimation can be accomplished by estimating the real and imaginary parts of the cyclic shift operator in (2), using quantum hardware. This identity subsequently be- came the backbone of many proposals [4, 5, 18, 19] for multivariate trace estimation with yet more near-term constraints. However, there appears to be a lack of clar- ity regarding the actual resource requirements of mul- tivariate trace estimation. Refs. [4, 5, 18, 19] have all suggested that a quantum circuit whose depth is linear in m is needed to perform the task. In this paper, we show that this is an overly- conservative characterization: multivariate trace estima- tion can be implemented in constant quantum depth, with only linearly-many controlled two-qubit gates and a linear amount of classical pre-processing. We invoke ideas from Shor error correction [20] to construct a circuit that achieves this claim (Section III), show how this cir- cuit can be implemented in a highly parallelized way on a two-dimensional architecture similar to Google’s (Sec- tion IV), prove Theorem 3 about the statistical guar- antees of the resulting estimator (Section V), and show that our method ﬁnds further application in estimating traces of ‘well-behaved’ polynomial functions of density matrices (Section VI). 1. Prepare a qubit in the |+(cid:105) := (|0(cid:105) + |1(cid:105))/ and adjoin to it the state ρ1 ⊗ · · · ⊗ ρm. √ 2 2 state 2. Perform a controlled cyclic permutation unitary gate, deﬁned as |0(cid:105)(cid:104)0| ⊗ I ⊗m + |1(cid:105)(cid:104)1| ⊗ W π. (4) 3. Measure the ﬁrst qubit in the basis {|+(cid:105), |−(cid:105)}, where |−(cid:105) := (|0(cid:105) − |1(cid:105))/ 2, and record the out- come X = +1 if the ﬁrst outcome |+(cid:105) is observed and X = −1 if the second outcome |−(cid:105) is observed. √ 4. Repeat Steps 1 to 3 a number of times equal to N := O(ε−2 log δ−1) and return ˆX := 1 i=1 Xi, N where Xi is the outcome of the i-th repetition of Step 3. (cid:80)N It is known that E[X] = Re[Tr[ρ1 · · · ρm]] , (5) and thus ˆX computed in Step 4 is an empirical estimate of the desired quantity. That is, by invoking the well known Hoeﬀding inequality, it is guaranteed, for ε > 0 and δ ∈ (0, 1), that Pr(| ˆX − Re[Tr[ρ1 · · · ρm]]| ≤ ε) ≥ 1 − δ. (6) For completeness, we recall the Hoeﬀding inequality now: Lemma 1 (Hoeﬀding [21]) Suppose that we are given n independent samples Y1, . . . , Yn of a bounded random variable Y taking values in [a, b] and having mean µ. Set Y n := 1 n (Y1 + · · · + Yn) (7) to be the sample mean. Let ε ∈ (0, 1) be the desired ac- curacy, and let 1 − δ be the desired success probability, where δ ∈ (0, 1). Then Pr(cid:2)(cid:12) (cid:12)Y n − µ(cid:12) (cid:12) ≤ ε(cid:3) ≥ 1 − δ, as long as (8) (9) II. THE PRINCIPLE: USING CYCLIC SHIFTS FOR MULTIVARIATE TRACE ESTIMATION where M := b − a. n ≥ M 2 2ε2 ln (cid:18) 2 δ (cid:19) , The principle behind our circuit construction is simple. To explain it, let us ﬁrst recall a well known circuit con- struction [1] for multivariate trace estimation that has no clear realization on near-term quantum computers. The idea is to estimate the quantities Re[Tr[ρ1 · · · ρm]] and Im[Tr[ρ1 · · · ρm]] separately. The circuits to estimate both quantities are similar, and we now describe them. To estimate the real part Re[Tr[ρ1 · · · ρm]], we proceed according to the following steps: To see that Eq. (5) holds, note that in the special case when all the states are pure, i.e., ρi = |ψi(cid:105)(cid:104)ψi|, the input to the circuit is an m-partite pure-state |ψ(m)(cid:105) := |ψ1(cid:105) ⊗ · · · ⊗ |ψm(cid:105), and so Pr(X = +1) (cid:13) (cid:13) (cid:13) (cid:13) 1 4 = = ((cid:104)+| ⊗ I) (cid:16) 1 √ 2 |0(cid:105)|ψ(m)(cid:105) + |1(cid:105)W π|ψ(m)(cid:105) (cid:17)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (cid:107)|ψ(m)(cid:105) + W π|ψ(m)(cid:105)(cid:107)2 2 (10) (11) (2 + (cid:104)ψ(m)|W π|ψ(m)(cid:105) + (cid:104)ψ(m)|(W π)†|ψ(m)(cid:105)) (12) A. Preparing GHZ states in constant quantum depth 3 = = = 1 4 1 2 1 2 (1 + Re[Tr[W π|ψ(m)(cid:105)(cid:104)ψ(m)|]]) (1 + Re[Tr[ρ1 · · · ρm]]) (13) (14) where in the last equality we have used the well-known identity in Eq. (2). Similarly, we have that Pr(X = −1) = 1 2 (1 − Re[Tr[ρ1 · · · ρm]]), (15) so that E[X] = (+1) Pr(X = +1) + (−1) Pr(X = −1) = Re[Tr[ρ1 · · · ρm]]. (16) (17) Eq. (5), which asserts that the conclusion Eq. (14) still holds when the ρi are mixed states, follows by convexity (i.e., that every mixed state can be written as a convex combination of pure states). To estimate the second quantity Im[Tr[ρ1 · · · ρm]], a simple variation of the above argument suﬃces. The technique is identical, except that the ﬁnal measure- := ment is in the basis {|+Y (cid:105), |−Y (cid:105)}, where |±Y (cid:105) (|0(cid:105) ± |1(cid:105)) / √ 2. III. OUR CIRCUIT CONSTRUCTION We propose a variation of the above method, which in- stead estimates Tr[ρ1 · · · ρm] in constant quantum depth. This makes the circuit more amenable to run on near- term quantum processors. This circuit is depicted for m = 4 — it has some similarities with the method of Shor error correction from fault-tolerant quantum com- putation [20] (see also Figure 2 of [22]). The crux of our method is to replace the |+(cid:105) = (|0(cid:105) + |1(cid:105)) state at the single qubit control wire in 1√ 2 the circuit in [1], with an (cid:98)m/2(cid:99)-party GHZ state |Φ(cid:98)m/2(cid:99) GHZ (cid:105) := 1 √ 2 (cid:16) |0(cid:105)⊗(cid:98)m/2(cid:99) + |1(cid:105)⊗(cid:98)m/2(cid:99)(cid:17) (18) on (cid:98)m/2(cid:99) control wires. This modiﬁcation allows the number of controls to the permutation to increase to (cid:98)m/2(cid:99), which is half the input size. In turn, it paves the way for an implementation of multivariate trace esti- mation in quantum depth two using parallelized cSWAP gates. In order to achieve an overall constant quantum depth, in Section III A we show that constant quantum depth suﬃces to generate the input GHZ state in (18). Then, in Section III B, we show how to implement the permu- tation W π, again in constant quantum depth. Finally, in Section III C, we describe our full estimator and the accompanying circuit. We now describe two methods to generate the GHZ state in constant quantum depth. The ﬁrst is related to a method discussed previously in [23] and is a constant- depth quantum circuit assisted by measurements, clas- sical feedback, and a logarithmic depth classical circuit. See also the discussion after [24, Theorem 1.1]. The sec- ond is a variation of the ﬁrst, which additionally allows for qubit resets to make more eﬃcient usage of qubits. Method 1. We begin by discussing the ﬁrst approach, which generates an r-party GHZ state using a constant- depth quantum circuit assisted by measurements, clas- sical feedback, and a logarithmic depth classical circuit. We proceed according to the following steps: 1. Generate r − 1 Bell states; i.e., each pair is in the state |Φ+(cid:105) := 1 √ 2 (|00(cid:105) + |11(cid:105)) . (19) 2. Perform controlled-NOT gates between the second qubit in each Bell state and the ﬁrst qubit of the following one. That is, apply a controlled-NOT from qubit 2k to qubit 2k + 1 for all k ∈ {1, . . . , r − 2}. 3. Measure the target of each controlled-NOT gate (all odd-numbered qubits except the ﬁrst qubit) in the computational basis. 4. Controlled on the measurement outcome b1, . . . , br−2, apply a tensor product of Pauli X operators as a correction to all even-numbered qubits except the second qubit. That is, apply X b1⊕···⊕bk−1 to qubit 2k for k ∈ {2, . . . , r − 1}. This procedure prepares an r-party GHZ state on qubits 1, 2, 4, 6, . . . , 2(r − 1). We now show in detail that the scheme prepares an r-party GHZ state. Consider that the initial state can be written as r−1 (cid:79) i=1 |Φ+(cid:105) = √ = √ 1 2r−1 1 2r−1 r−1 (cid:79) (cid:88) i=1 xi∈{0,1} |xi, xi(cid:105) (20) (cid:88) |x1, x1(cid:105) ⊗ x1,...,xr∈{0,1} r−1 (cid:79) i=2 |xi, xi(cid:105). (21) After step 2, the state becomes √ 1 2r−1 (cid:88) |x1, x1(cid:105) ⊗ x1,...,xr∈{0,1} r−1 (cid:79) i=2 |xi ⊕ xi−1, xi(cid:105). (22) After step 3, the (r − 2)-bit string b1 · · · br−2 is obtained from the measurements, where b1 = x2 ⊕ x1, b2 = x3 ⊕ x2, · · · , br−2 = xr−1 ⊕ xr−2, (23) (24) (25) (26) and this projects the state onto the following state: 1 √ 2 = (cid:88) |x1, x1(cid:105) ⊗ |b1, x2(cid:105) ⊗ |b2, x3(cid:105) · · · ⊗ |br−2, xr−1(cid:105) x1∈{0,1} 1 (cid:88) √ 2 x1∈{0,1} |x1, x1(cid:105) ⊗ |b1, b1 ⊕ x1(cid:105)⊗ |b2, b1 ⊕ b2 ⊕ x1(cid:105) · · · ⊗ |br−2, b1 ⊕ · · · ⊕ br−2 ⊕ x1(cid:105) = (I ⊗ I ⊗ I ⊗ X b1 ⊗ I ⊗ X b1⊕b2 ⊗ · · · ⊗ I ⊗ X b1⊕···⊕br−2)×   1 √ 2 (cid:88) x1∈{0,1} |x1, x1(cid:105)|b1, x1(cid:105)|b2, x1(cid:105) · · · |br−2, x1(cid:105)  (27)  4 FIG. 1: A constant-depth quantum circuit for preparing an eight-party GHZ state, assisted by measurement, classical feedback, and qubit resets. Method 1 consists of all the steps depicted, except for the qubit resets (but only prepares a ﬁve- party state). In Method 2, the measured qubits are addition- ally reset to the |0(cid:105) state and connected by CNOTs so that a larger, eight-party state can be prepared instead. (28) Proposition 2 The following decomposition holds The r-party GHZ state on qubits 1, 2, 4, 6, . . . , 2(r − 1) is then recovered by performing the following correction operation: I ⊗ I ⊗ I ⊗ X b1 ⊗ I ⊗ X b1⊕b2 ⊗ · · · I ⊗ X b1⊕···⊕br−2 (29) The leftmost part of Figure 1 (all except the qubit resets) depicts this procedure. Method 2. The second method is very similar to the one just described. The only diﬀerence is that we addi- tionally perform qubit resets on the measured qubits and then controlled-NOTs from the nearest neighbor qubits in the GHZ state to the reset qubits. This scheme is de- picted in Figure 1. It prepares a GHZ state on a number of parties equal to the number of input qubits (hence a 2(r −1)-party GHZ state, as opposed to the r-party GHZ state without the qubit resets). B. Multiply-controlled cyclic permutation in constant quantum depth Rather than implement a controlled-W π gate, we in- stead implement a multiply-controlled-W π gate, in or- der to reduce the depth of this part of the circuit from linear to constant. Our implementation of the multiply- controlled-W π gate in constant depth is based on the observation that there is a particularly convenient way to decompose a cyclic shift into a product of transposi- tions, as shown in [25, Eqs. (4.2)–(4.3)]. We state this observation here, along with a brief proof, for complete- ness: (1, . . . , m) =  m/2 (cid:89)   l=2 (cid:100)m/2(cid:101) (cid:89) l=2 (l, m + 2 − l) m/2 (cid:89) (k, m + 1 − k) k=1 : m even (l, m + 2 − l) (cid:98)m/2(cid:99) (cid:89) k=1 (k, m + 1 − k) : m odd (30) where all arithmetic is modulo m. So, for instance, when m = 8 (the case in Figure 2), Eq. (30) would read (1, . . . , 8) = (2, 8)(3, 7)(4, 6) (1, 8)(2, 7)(3, 6)(4, 5) . (31) Proof. For m even, the ﬁrst transposition sends k to m + 1 − k for every k ∈ [m]. If k = m, it gets sent to 1 by the ﬁrst transposition and is not acted on by the second transposition. Otherwise, the second transposition sends m + 1 − k to m + 2 − (m + 1 − k) = k + 1, so the overall eﬀect is to send k → k + 1 for all k ∈ [m], as desired. For m odd, there are two indices that are involved in only one transposition: k = m, which, as before, gets sent to 1 by the ﬁrst transposition and is not acted on by the second transposition; and k = (cid:100)m/2(cid:101), which is transposed only in the second transposition where it gets sent to m + 2 − (cid:100)m/2(cid:101) = (cid:100)m/2(cid:101) + 1. All other indices are involved in the same two transpositions as described above. Thus, the overall eﬀect is also k → k + 1 for all k ∈ [m]. H H H H |0〉 |0〉 |0〉 |0〉 |0〉 |0〉 |0〉 |0〉 |0〉 |0〉 |0〉 b1 b2 b3 X b1 X b1 b2 X b1 b2 b3 5 1. Prepare an (cid:98)m/2(cid:99)-party GHZ state using one of the constant quantum-depth circuit constructions described in the previous section. Let us call the (cid:98)m/2(cid:99) qubits of the GHZ state the control qubits and the m states ρ1 ⊗ · · · ⊗ ρm the target qubits. 2. To implement the multiply-controlled cyclic shift, • If m is even, adjoin ρ1, . . . , ρm to the GHZ state, in the order ρ1 ⊗ ρm ⊗ ρ2 ⊗ ρm−1 ⊗ ρ3 ⊗ · · · ⊗ ρm/2+2 ⊗ ρm/2 ⊗ ρm/2+1. (32) Perform a controlled-SWAP gate from the ith control qubit to target qubits 2i − 1 and 2i, for all i ∈ {1, . . . , m/2}. Now per- form a controlled-SWAP from the ith con- trol qubit to target qubits 2i and 2i + 1 for i ∈ {1, . . . , m/2 − 1}. • If m is odd, adjoin ρ1, . . . , ρm to the GHZ state, in the order ρ1⊗ρm⊗ρ2⊗ρm−1⊗ρ3⊗· · ·⊗ρ(cid:100)m/2(cid:101)−1⊗ρ(cid:100)m/2(cid:101)+1⊗ρ(cid:100)m/2(cid:101). (33) Perform a controlled-SWAP gate from the ith control qubit to target qubits 2i − 1 and 2i, for all i ∈ {1, . . . , (cid:98)m/2(cid:99)}. Now per- form a controlled-SWAP from the ith con- trol qubit to target qubits 2i and 2i + 1 for i ∈ {1, . . . , (cid:98)m/2(cid:99)}. It can be checked that this prescription implements precisely Eq. (30), with the states adjoined in a speciﬁc order (see Eq. (32) or Eq. (33)) that allows only nearest neighbors to be swapped. 3. Perform a Hadamard on all (cid:98)m/2(cid:99) control qubits and measure them in the computational basis, re- ceiving outcomes 0 or 1. This has the eﬀect of performing a measurement in the X basis on each control qubit. Let Xi ∈ {0, 1} denote the result of the ith measurement, for i ∈ {1, · · · , (cid:98)m/2(cid:99)}. Set R = (−1) (cid:80)(cid:98)m/2(cid:99) i=1 Xi. 4. Repeat Steps 1 to 3 a number of times equal to N := O(ε−2 log δ−1). Compute ˆR := 1 j=1 Rj, where N Rj is the output of Step 3 on the j-th application of the circuit. ˆR is our estimate for Re[Tr[ρ1 · · · ρm]]. (cid:80)N 5. To estimate Im[Tr[ρ1 · · · ρm]], repeat Steps 1 to 4, except that in Step 3, replace each Hadamard with HS†, where S is the phase gate S := (cid:21) (cid:20)1 0 0 i , (34) before the measurement in the computational basis. This has the eﬀect of performing a measurement in the Y basis on each control qubit. Let Y (j) i ∈ {0, 1} be the outcome of the measurement on the i-th FIG. 2: The leftmost part of the circuit prepares a four-party GHZ state. The middle part of the circuit performs a con- trolled cyclic-shift. The ﬁnal part of the circuit results in the classical bits x1, x2, x3, x4, which are used to generate r = (−1)x1+x2+x3+x4 . As argued in Section V, the expecta- tion of r is equal to Re[Tr[ρ1 · · · ρ8]], so that this latter quan- tity can be estimated through repetition. Eq. (30) says that, for a ﬁxed m, the m-wise cyclic shift permutation can be decomposed into a product of two terms. Each term is itself a product of disjoint trans- positions and every index gets transposed once per term. This has a clear interpretation in terms of how to construct a quantum circuit to implement a multiply- controlled-W π. Transposing two qubit labels can be achieved by applying a SWAP gate to the relevant qubits. Disjoint transpositions can thus be accomplished by im- plementing SWAP gates in parallel, in a single time step. Proposition 2 thus implies that, for every m, the multiply-controlled-W π can be implemented in two time steps (depth two), each of which performs (cid:98)m/2(cid:99) con- trolled SWAPs in parallel. We give a more explicit description of the circuit in the next subsection. Note that, as depicted in Figure 2, we have made another optimization for near-term feasibility: we adjoin the input states to the cyclic shift in a speciﬁc order that ensures that only nearest-neighbor states need to be swapped. C. Explicit circuit description We now put together the ﬁndings of the previous two subsections and describe our proposed technique for esti- mating Tr[ρ1 · · · ρm] in the case that each local dimension d = 2 (i.e., each ρi is a single-qubit state). The corre- sponding circuit is depicted in Figure 2. After that, we discuss how to generalize the construction to the case in which d is a power of two, so that each local system con- sists of multiple qubits. The estimator works as follows: H H H H x1 x2 x3 x4 H H H |0〉 |0〉 |0〉 |0〉 |0〉 |0〉 b1 b2 X b1 X b1 b2 ρ1 ρ8 ρ2 ρ7 ρ3 ρ6 ρ4 ρ5 qubit on the j-th application of the circuit. Set j=1 Jj. ˆJ Jj = (−1) is our estimate for Im[Tr[ρ1 · · · ρm]]. . Compute ˆJ = 1 N (cid:80)(cid:98)m/2(cid:99) i=1 (cid:80)N Y (j) i 6. Output ˆT = ˆR + i ˆJ. Our proposed architecture is highly ﬂexible and can be tailored to the availability of resources such as long coherence times and multi-qubit gates. This is evident in two ways: Firstly, we can smoothly trade oﬀ circuit width for the availability of entangling gates. At one extreme, observe that we could have implemented our circuit with only one control qubit (instead of (cid:98)m/2(cid:99)) if we had at our disposal a highly-entangling gate: a single-qubit controlled simul- taneous SWAP gate that swapped (cid:98)m/2(cid:99) pairs of qubits simultaneously. Such a gate would not be feasible in the near term, as it requires highly nonlocal interactions to implement in a real physical architecture. However, even gates that entangle only a subset of qubits aﬀord us sav- ings in circuit width: every additional controlled k-wise SWAP gate at our disposal allows for a reduction of cir- cuit width by k, as k fewer control qubits are necessary (hence the GHZ state needs to be on k fewer parties). to Secondly, generalize of the Tr[ρ1ρ2 · · · ρm] beyond single-qubit can either increase the width or the depth of the circuit described above. Suppose that ρ1, . . . , ρm each consist of p qubits. estimation states, we • We can increase the width of the circuit by prepar- ing GHZ states with mp qubits and then group these into p groups of m qubits each. Correspond- ingly, group the 2m states ρ1, . . . , ρ2m into p groups of 2m qubits, where the kth group has the kth qubit of each state, for k ∈ {1, . . . , p}. Then we per- form controlled SWAPs as detailed above, for each group. Finally, perform Hadamards on all of the mp control qubits and measure each of them in the computational basis. • To increase the depth of the circuit, prepare an m- party GHZ state, and then sequentially perform the controlled-SWAP tests for the p groups of qubits, so that the depth of the circuit increases by a factor of p. Then measure the control qubits as before. IV. IMPLEMENTATION OF MULTIVARIATE TRACE ESTIMATION ON A TWO-DIMENSIONAL ARCHITECTURE 6 GHZ state preparation, as depicted in Figure 1. Explana- tions of the steps are given in the caption of Figure 3. The main point to highlight here is that our circuit leads to a highly parallelized implementation on a two-dimensional architecture that should make it more amenable to real- ization on near-term quantum computers. V. GUARANTEES OF OUR ESTIMATOR In this section, we show that the estimator ˆT described in Section III C is accurate and precise with high proba- bility. Theorem 3 Let {ρ1, . . . , ρm} be single-qubit states. There exists a random variable ˆT that can be computed with O( 1 δ )) repetitions of a constant-depth quan- tum circuit consisting of O(m) three-qubit gates, and sat- isﬁes ε2 log( 1 Pr(| ˆT − Tr[ρ1 · · · ρm]| ≤ ε) ≥ 1 − δ . (35) Proof. By the Hoeﬀding inequality (see Lemma 1), it suﬃces to prove that the estimator ˆT output by the method of Section III C satisﬁes E[ ˆT ] = Tr[W π(ρ1 ⊗ · · · ⊗ ρm)] = Tr[ρ1 · · · ρm] . (36) It suﬃces to prove the ﬁrst equality. To begin with, let us suppose for simplicity that all the states are pure, i.e. ρi = |ψi(cid:105)(cid:104)ψi|, and deﬁne the state of the target qubits as |ψ(m)(cid:105) := |ψ1(cid:105) ⊗ · · · ⊗ |ψm(cid:105). (37) Step 1 of our procedure prepares a GHZ state |Φ(cid:98)m/2(cid:99) GHZ (cid:105) and adjoins it to |ψ(m)(cid:105), such that the overall state at the end of Step 1 is |Φ(cid:98)m/2(cid:99) GHZ (cid:105)|ψ(m)(cid:105). (38) After Step 2 (the multiply-controlled cyclic shift), the overall state becomes |0(cid:105)⊗(cid:98)m/2(cid:99)|ψ(m)(cid:105) + |1(cid:105)⊗(cid:98)m/2(cid:99)W π|ψ(m)(cid:105) (cid:17) . (39) (cid:16) 1 √ 2 In Step 3, one measures the (cid:98)m/2(cid:99) control qubits in the X basis, obtaining an (cid:98)m/2(cid:99)-bitstring where the i-th bit is denoted by the random variable Xi ∈ {0, 1}. We then compute the expectation of the random variable ˆR which will be output as the real part of our estimator We now outline how to implement our algorithm using a two-dimensional architecture similar to Google’s [26]. We do so by means of a series of ﬁgures, which outline the time steps of the circuit implementation. See Figure 3. These ﬁgures can be understood as a two-dimensional implementation of the circuit depicted in Figure 2, with the exception that we also include qubit resets during the ˆR ≡ ˆR(X1, . . . , X(cid:98)m/2(cid:99)) := (−1) (cid:80)(cid:98)m/2(cid:99) i=1 Xi. (40) Introducing the following notation for X basis eigenvec- tors |˜x(cid:105) := 1 √ 2 |0(cid:105) + (−1)x|1(cid:105) for x ∈ {0, 1}, (41) 7 In FIG. 3: (1) The squares in light grey represent control qubits, and the squares in dark grey represent data qubits. this example, there are ﬁve data states involved, each consisting of four qubits. (2) The quantum data is loaded during this stage, which we note here can be conducted in parallel with the preparation of the GHZ state in the control qubits. The state ρi, for i ∈ {1, . . . , 5}, is a four-qubit state that occupies the indicated column of the data qubits in dark grey. The light grey control qubits are prepared in the all zeros state. (3) First step of the preparation of the GHZ state of the control qubits. Every other control qubit has a Hadamard gate applied in parallel. (4) Every pair of control qubits has CNOT gates applied in parallel. (5) Every other pair of control qubits has CNOT gates applied in parallel. (6) Starting from the third control qubit from the top left, every other control qubit is measured, and the measurement outcome is stored in a binary vector b1, . . . , b7. (7) Based on the measurement outcomes from the previous step, Pauli-X corrections are applied to every other qubit, starting from the fourth in the top row. The particular corrections needed are abbreviated by a multivariate function f , the details of which are available in Eq. (29). (8) The measured qubits are reset to the all zeros state. (9) Final step of the preparation of the GHZ state of the control qubits. CNOT gates are again applied to every other control qubit. The ﬁnal state of all control qubits is equal to a GHZ state. (10) Controlled-SWAPs are applied in parallel between control qubits and data qubits, in a ﬁrst round of the implementation of the cyclic shift. (11) Controlled-SWAPs are applied in parallel between other control qubits and data qubits, in a second round of the implementation of the cyclic shift. (13) In a ﬁnal step, all control qubits are measured in the computational basis and the measurement outcomes are processed according to Eq. (40) to form an estimate of the real part of Tr[ρ1 · · · ρ5]. To estimate the imaginary part, replace H in step (12) with HS†. A GIF of the entire procedure may be viewed at https://twitter.com/quekpottheories/status/1542545522392809477?s=20&t=rb1PDn27W3pVaAKHWDDS1Q (12) Hadamard gates are applied to all control qubits. (1) (2) Load Data ρ1 ρ5 ρ2 ρ4 ρ3 (3) GHZ Preparation H H H H H H H H (4) GHZ Preparation (5) GHZ Preparation (6) GHZ Preparation (7) GHZ Preparation Xf Xf Xf Xf Xf Xf Xf (8) GHZ Preparation |0〉 |0〉 |0〉 |0〉 |0〉 |0〉 |0〉 (9) GHZ Preparation (10) (11) (12) H H H H H H H H H H H H H H H H (13) the probability of the X basis measurement outputting the bitstring x1 · · · x(cid:98)m/2(cid:99) ∈ {0, 1}(cid:98)m/2(cid:99) is given by Pr(x1 · · · x(cid:98)m/2(cid:99)) (cid:88) = x1,··· ,xm = 1, 1 + (−1) i=1 xi Re[Tr[W πρ(m)]] (cid:80)m 2m 8 (54) (55) = = = (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 (42) (43) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1√ 2 (cid:0)(cid:104)(cid:102)x1, (cid:102)x2, · · · , (cid:94)x(cid:98)m/2(cid:99)| ⊗ I(cid:1) × (cid:0)|0(cid:105)⊗(cid:98)m/2(cid:99)|ψ(m)(cid:105) + |1(cid:105)⊗(cid:98)m/2(cid:99)W π|ψ(m)(cid:105)(cid:1) 1 2(cid:98)m/2(cid:99)+1 1 + (−1) (cid:13) (cid:13)|ψ(m)(cid:105) + (−1) (cid:13) (cid:80)(cid:98)m/2(cid:99) i=1 (cid:13) 2 xiW π|ψ(m)(cid:105) (cid:13) (cid:13) 2 (cid:80)(cid:98)m/2(cid:99) i=1 xi Re[Tr[W π|ψ(m)(cid:105)(cid:104)ψ(m)|]] 2(cid:98)m/2(cid:99) . (44) Thus, E[ ˆR] = = (cid:88) x1,...,x(cid:98)m/2(cid:99) Pr(x1, · · · , x(cid:98)m/2(cid:99))r(x1, · · · , x(cid:98)m/2(cid:99)) (45) (cid:32) 1 + (−1) (cid:80)(cid:98)m/2(cid:99) i=1 xi Re[Tr[W π|ψ(m)(cid:105)(cid:104)ψ(m)|]] (cid:33) 2(cid:98)m/2(cid:99) (cid:88) x1,..., x(cid:98)m/2(cid:99) × (−1) (cid:80)(cid:98)m/2(cid:99) i=1 xi (46) = 1 2(cid:98)m/2(cid:99) (cid:88) (cid:16) (cid:80)(cid:98)m/2(cid:99) i=1 (−1) xi + x1,...,x(cid:98)m/2(cid:99) = Re[Tr[W π|ψ(m)(cid:105)(cid:104)ψ(m)|]], Re[Tr[W π|ψ(m)(cid:105)(cid:104)ψ(m)|]] (cid:17) (47) (48) where, in the second-to-last equality, we have used the fact that (cid:88) x1,...,xl∈{0,1}l (cid:80)l (−1) i=1 xi = 0 for all l. (49) The claim for mixed states ρ(m) := ρ1 ⊗ · · · ⊗ ρm, i.e., E[ ˆR] = Re[Tr[W πρ(m)]], (50) follows by convexity (i.e., that every mixed state can be written as a convex combination of pure states). That is, we use the fact that 1 + (−1) i=1 xi Re[Tr[W πρ(m)]] (cid:80)m Pr(x1, . . . , xm) = 2m (51) for this case and then repeat the calculation above for every eigenvector of ρ(m). Using a similar chain of logic, we conclude that E[ ˆJ] = Im[Tr[W πρ(m)]] , (52) and the ﬁrst equality of (36) follows. We also compute the variance of ˆT . Consider that Var[ ˆR] := E[( ˆR − E[ ˆR])2] = E[ ˆR2] − E[ ˆR]2. Since E[ ˆR2] = (cid:88) x1,··· ,xm Pr(x1, · · · , xm) (r(x1, · · · , xm))2 (53) we conclude that Var[ ˆR] = 1 − (Re[Tr[W πρ(m)]])2. Sim- ilarly, Var[ ˆJ] = 1 − (Im[Tr[W πρ(m)]])2. Since these two random variables are independent, Var[ ˆT ] = 2 − (cid:12) (cid:12) (cid:12)Tr[W πρ(m)] (cid:12) (cid:12) (cid:12) 2 . (56) We now discuss the generalization of Theorem 3 to states of more than one qubit. This generalization can be accomplished by increasing the circuit width or the circuit depth as discussed in Section III C. Proposition 4 Let {ρ1, . . . , ρm} be a set of p-qubit states, and ﬁx ε > 0 and δ ∈ (0, 1). There exists a ran- dom variable ˆTp that can be computed using O( 1 δ )) repetitions of a constant-depth quantum circuit consisting of O(mp) three-qubit gates, and satisﬁes ε2 log( 1 Pr(| ˆTp − Tr[ρ1 · · · ρm]| ≤ ε) ≥ 1 − δ . (57) Proof. The gate count for the circuits for both con- structions detailed in Section III C is O(mp). For the second construction (increasing the depth), Eq. (57) fol- lows from the exact same calculation as in the proof of Theorem 3. For the ﬁrst construction (increasing the width), let us deﬁne ρ(m,p) := ρ1 ⊗ · · · ⊗ ρm. It suﬃces to prove that for the estimator ˆRp := (−1)X1+···+Xmp de- rived from the mp measurement outcomes X1, · · · , Xmp fulﬁls E[ ˆRp] = Re[Tr[W πρ(m,p)]]. This follows from a similar calculation to Eqs. (42)–(50). Similarly, Var[ ˆTp] = 2 − (cid:12) (cid:12) (cid:12)Tr[W πρ(m,p)] (cid:12) (cid:12) (cid:12) 2 . (58) VI. APPLICATIONS OF OUR METHOD An application of our method is to estimate functions of density matrices that can be approximated by ‘well- behaved’ polynomials. This was already suggested in the original work of [1], but no complexity analysis was put forth. Here we formalize the analysis of the application in the following theorem: Theorem 5 Let ρ be a quantum state with rank at most d. Suppose there exist constants η, ε and a function C such that g : R → R is approximated by a degree-m poly- nomial f (x) = (cid:80)m k=0 ckxk on the interval [η, 1], in the sense that sup x∈[η,1] |g(x) − f (x)| < ε 2d , (59) and 3. Let m (cid:88) k=0 |ck| < C . (60) N = 8C 2 ε2 ln (cid:18) 2 δ (cid:19) . 9 (66) Then estimating Tr[g(ρ)] within ε additive error with success probability not smaller than 1 − δ requires δ )) copies of ρ and O(m C2 O(m2 C2 δ )) runs of a circuit with O(m) controlled SWAP gates. ε2 log( 1 ε2 log( 1 (Here, we should think of C as a slowly-growing function of m.) An example of a function of a quantum state that can be estimated in this way is g(x) = (1 + x)α for α > 0, which has the following expansion as a binomial series (1 + x)α = ∞ (cid:88) k=0 (cid:19) (cid:18)α k xk, (61) k where (cid:0)α (cid:1) is the generalized binomial coeﬃcient. It is well known that the binomial series converges absolutely for x = 1 and α > 0, which implies that, for every α > 0, there exists a positive constant C(α) such that m (cid:88) k=0 (cid:12) (cid:18)α (cid:12) (cid:12) k (cid:12) (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) ≤ ∞ (cid:88) k=0 (cid:12) (cid:12) (cid:12) (cid:12) (cid:18)α k (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) = C(α). (62) Thus, this series satisﬁes the criterion in Eq. (60). An- other function of a quantum state that can be estimated in this way is f (x) = ln(x + 1). Indeed, this function has the following well known expansion: ln(x + 1) = ∞ (cid:88) k=1 (−1)k k xk, (63) Repeat the ﬁrst two steps N − 1 more times, on the i-th iteration outputting the random variable R(i). 4. Output ˆg = 1 N Re[Tr[g(ρ)]]. (cid:80)N i=1 R(i) as the estimator for Let us now prove the correctness of this procedure. Suppose the spectral decomposition of ρ is as follows: ρ = rρ (cid:88) i=1 λi|ψi(cid:105)(cid:104)ψi| , (67) where rρ is the rank of ρ. In the limit of N → ∞, the only error in the estimator ˆg would come from the error in the polynomial approximation. That is, | Tr[g(ρ)] − m (cid:88) k=0 ck Tr[ρk]| = rρ (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) i=1 ≤ ε/2 (cid:32) g(λi) − m (cid:88) k=0 ckλk i (cid:33)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (68) where the inequality follows from Eq. (59) and the fact that d > rρ. Now we account for the other source of error, which is the statistical error caused by taking a ﬁnite number of samples. Consider that |R(i)| ≤ m (cid:88) k=0 |ck||Rk| ≤ m (cid:88) k=0 |ck| ≤ C, (69) (cid:12) (cid:12) (cid:12) k=1 (−1)k k so that the absolute partial sum of the coeﬃcients sat- (cid:12) isﬁes (cid:80)m (cid:12) (cid:12) ≈ ln m + γ, where γ ≈ 0.577 is the Euler–Mascheroni constant. The point here is that, even though the absolute partial sums are not bounded by a constant, they still grow suﬃciently slowly such that the algorithm runs eﬃciently. Proof of Theorem 5. We will present an estimator for the desired quantity that satisﬁes the claimed complexity guarantees. Let us describe the estimator for Re[Tr[g(ρ)]] (the estimator for the imaginary part follows immedi- ately). To estimate Re[Tr[g(ρ)]], we run the following procedure: 1. For each k ∈ [m], run the circuit described in Steps 1-3 of Section III C once to output a random variable Rk ∈ {−1, 1} such that E[Rk] = Re[Tr[ρk]] . (64) for all i ∈ {1, . . . , N }, where the ﬁrst inequality follows from the triangle inequality, the second from the fact that Rk ∈ {−1, 1}, and the third from the assumption in (60). By applying (69), the Hoeﬀding inequality in Lemma 1 immediately applies to our setting when we take Yi ← R(i) and [a, b] ← [−C, C]. We also see that µ ← E[R(1)] = (cid:80)m k=0 ck Tr[ρk]. Then if we set N to (cid:19) (cid:18) 2 δ 8C 2 ε2 ln N = (70) we get that Pr (cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ˆg − m (cid:88) k=0 (cid:12) (cid:12) ck Tr[ρk] (cid:12) (cid:12) (cid:12) (cid:35) ≤ ε/2 ≥ 1 − δ. (71) Combining the two sources of error in Eqs. (68) and (71), we get that with probability 1 − δ, 2. Linearly combine the above random variables to |ˆg − Re[Tr[g(ρ)]]| ≤ form the new random variable R(1) = m (cid:88) k=0 ckRk . (65) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ˆg − m (cid:88) k=0 ck Tr[ρk] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) + Re[Tr[g(ρ)]] − m (cid:88) k=0 (cid:12) (cid:12) ck Tr[ρk] (cid:12) (cid:12) (cid:12) ≤ ε. (72) Repeating the analysis for the estimation of Im[Tr[g(ρ)]] yields the stated complexity. ak Tr[(σ1/2ρσ1/2)k] = (cid:88) k ak Tr[(ρσ)k], (79) (cid:88) k 10 We remark that, an alternative way to estimate Tr[ρk] for each k ∈ [m] is by using the method of classical shadows to obtain ‘classical snapshots’ of ρ that can be linearly combined to obtain a classical random variable whose expectation is Tr[ρk] (see Supplementary Material Section 6 of [27]). However, it is unclear to us if this method would oﬀer savings in the quantum resources re- quired, as the total number of times the quantum circuit needs to be run in the data acquisition phase should scale with the variance of the corresponding estimator. We do not know of a concise expression for this variance for ar- bitrary m. Indeed, calculating it for just a single value of m (m = 2) required four pages of calculations in [27]. One might also wonder whether we could use this ap- proach to estimate R´enyi or von Neumann entropies of quantum states. The main diﬃculty in doing so is that well known polynomial approximations of the functions xα and −x ln x, given respectively by xα = = ∞ (cid:88) k=0 ∞ (cid:88) k=0 −x ln x = x = x ∞ (cid:88) k=1 ∞ (cid:88) k=1 (cid:19) (cid:18)α k (x − 1)k (73) (cid:19) (−1)k−(cid:96)x(cid:96), (74) (cid:19) k (cid:88) (cid:18)α k (cid:18)k (cid:96) (cid:96)=0 (1 − x)k k (75) (76) 1 k k (cid:88) (cid:96)=0 (cid:19) (cid:18)k (cid:96) (−1)(cid:96)x(cid:96) = ∞ (cid:88) k=1 1 k k (cid:88) (cid:96)=0 (cid:19) (cid:18)k (cid:96) (−1)(cid:96)x(cid:96)+1, (77) do not satisfy the condition in (60), in the sense that the absolute partial sums grow too quickly and therefore do not lead to an eﬃcient algorithm using this approach. See also [28] in this context. It thus remains open whether this approach can be used eﬀectively for estimating these important uncertainty measures. See [29–32] for work on this topic in the ﬁeld of classical information theory and [33–37] for a ﬂurry of recent eﬀorts on estimating R´enyi and von Neumann entropies using quantum computers, which propose alternative approaches. We note here that the method outlined above can be generalized to functions of multiple density matrices. For example, let ρ and σ be quantum states, and suppose that g1 and g2 are well behaved polynomials in the sense described in Theorem 5. Then we can employ a similar approach to estimate the functions Tr[g1(ρ)g2(σ)] and Tr[g1(σ1/2ρσ1/2)]. Polynomial approximations of these functions take the following form: (cid:34)(cid:32) Tr (cid:88) ckρk (cid:33) (cid:32) (cid:88) d(cid:96)σ(cid:96) (cid:33)(cid:35) k (cid:96) (cid:88) = k,(cid:96) ckd(cid:96) Tr[ρkσ(cid:96)], (78) respectively, and can be estimated using our circuits com- bined with classical postprocessing. Thus, by the dis- cussion after Theorem 5, we can take g1(x) = (1 + x)α and g2(x) = (1 + x)β for α, β > 0. A case of interest is when we set α ∈ (0, 1) and β = 1 − α. The resulting function Tr[g1(ρ)g2(σ)] then satisﬁes faithfulness and the data-processing inequality under unital quantum chan- nels and thus can serve as an alternative to the widely used Hilbert–Schmidt distance measure (which also sat- isﬁes the data-processing inequality under unital chan- nels [38]). We prove these claims in Appendix A. VII. CONCLUSION We have provided a quantum circuit for multivariate trace estimation that requires only constant quantum depth, and hence it is more amenable to be implemented on near-term quantum computers than previous meth- ods that required linear depth. Our architecture is also ﬂexible and can be smoothly tailored to the availability of circuit width (at the cost of more-entangling gates). Going forward from here, one can further consider the application of our method to estimating nonlinear func- tions of quantum states. “The most important applica- tion of computers has been designing better computers” [39], and these methods can be used for this purpose. Our method to estimate functions of quantum states based on their polynomial approximations opens the door to the idea that near-term quantum computers can be used to design better quantum computers. An important open question in this regard is whether any functions whose polynomial approximations fulﬁll Eq. (60) have an in- terpretation as quality metrics for quantum computer design. Conversely, it would also be interesting to ex- plore further whether there are any quantum state distin- guishability measures – critical in applications like quan- tum compiling [40, 41] and state learning [42, 43] – that fulﬁll this condition. Acknowledgments We acknowledge helpful discussions with Jayadev Acharya, Patrick Coles, Andr´as Gily´en, Zo¨e Holmes, Dhrumil Patel, Eliott Rosenberg, Aliza Siddiqui, and An- tonio Anna Mele. EK and MMW acknowledge support from the National Science Foundation (NSF) under grant no. 1714215. YQ acknowledges support from a Stanford QFARM fellowship, an NUS Overseas Graduate Scholar- ship and an Alexander von Humboldt Fellowship. [1] Artur K. Ekert, Carolina Moura Alves, Daniel K. L. Oi, Micha(cid:32)l Horodecki, Pawe(cid:32)l Horodecki, and L. C. Kwek. Direct estimations of linear and nonlinear functionals of a quantum state. Physical Review Letters, 88(21):217901, May 2002. arXiv:quant-ph/0203016. [2] Todd A. Brun. Measuring polynomial functions of states. Quantum Information and Computation, 4(5):401–408, September 2004. arXiv:quant-ph/0401067. [3] Harry Buhrman, Richard Cleve, John Watrous, and Phys- Ronald de Wolf. ical Review Letters, 87(16):167902, September 2001. arXiv:quant-ph/0102001. Quantum ﬁngerprinting. [4] Sonika Johri, Damian S. Steiger, and Matthias Troyer. Entanglement spectroscopy on a quantum computer. Physical Review B, 96(19):195136, November 2017. arXiv:1707.07658. [5] Pawe(cid:32)l Horodecki and Artur Ekert. Method for di- rect detection of quantum entanglement. Physical Re- view Letters, 89(12):127902, August 2002. arXiv:quant- ph/0111064. [6] Daniel Gottesman and Isaac Chuang. Quantum digital signatures. May 2001. arXiv:quant-ph/0105032. [7] Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for linear systems of equations. Physical Review Letters, 103(15):150502, October 2009. arXiv:0811.3171. [8] Andr´as Gily´en, Yuan Su, Guang Hao Low, and Nathan Wiebe. Quantum singular value transformation and improvements for quantum ma- beyond: trix arithmetics. In Proceedings of the 51st Sympo- sium on the Theory of Computing, pages 193–204, 2019. arXiv:1806.01838. exponential [9] Andr´as Gily´en, Seth Lloyd, Iman Marvian, Yihui Quek, and Mark M. Wilde. Quantum algorithm for Petz recovery channels and pretty good measurements. Physical Review Letters, 128(22):220502, June 2022. arXiv:2006.16924. [10] Frank Pollmann, Ari M. Turner, Erez Berg, and Masaki Oshikawa. Entanglement spectrum of a topological phase in one dimension. Physical Review B, 81(6):064439, February 2010. arXiv:0910.1811. [11] Hong Yao and Xiao-Liang Qi. Entanglement en- tropy and entanglement spectrum of the Kitaev model. Physical Review Letters, 105(8):080501, August 2010. arXiv:1001.1165. [12] Lukasz Fidkowski. Entanglement spectrum of topological insulators and superconductors. Physical Review Letters, 104(13):130502, April 2010. arXiv:1001.1165. [13] Hui Li and F. D. M. Haldane. Entanglement spectrum as a generalization of entanglement entropy: Identiﬁcation of topological order in non-Abelian fractional quantum Hall eﬀect states. Physical Review Letters, 101(1):010504, July 2008. arXiv:0805.0332. [14] Claudio Chamon, Alioscia Hamma, and Eduardo R. irreversibility and entangle- Physical Review Letters, Mucciolo. ment spectrum statistics. 112(24):240501, June 2014. arXiv:1310.2702. Emergent [15] G. De Chiara, L. Lepori, M. Lewenstein, and A. Sanpera. Entanglement spectrum, critical exponents, and order parameters in quantum spin chains. Physical Review Let- ters, 109(23):237208, December 2012. arXiv:1104.1331. 11 [16] Jens Eisert, Marcus Cramer, and Martin B. Plenio. Col- loquium: Area laws for the entanglement entropy. Re- views of Modern Physics, 82(1):277–306, February 2010. arXiv:0808.3773. [17] M. Mezard, G. Parisi, and M. Virasoro. Spin Glass The- ory and Beyond. World Scientiﬁc, 1986. [18] Justin Yirka and Yi˘git Suba¸sı. Qubit-eﬃcient entangle- ment spectroscopy using qubit resets. Quantum, 5:535, September 2021. arXiv:2010.03080. [19] Yi˘git Suba¸sı, Lukasz Cincio, and Patrick J. Coles. En- tanglement spectroscopy with a depth-two quantum cir- cuit. Journal of Physics A: Mathematical and Theoreti- cal, 52(4):044001, January 2019. arXiv:1806.08863. [20] Peter W. Shor. Fault-tolerant quantum computation. In Proceedings of the 37th Annual Symposium on Founda- tions of Computer Science, FOCS ’96, page 56, USA, 1996. IEEE Computer Society. arXiv:quant-ph/9605011. [21] Wassily Hoeﬀding. Probability inequalities for sums of bounded random variables. Journal of the American Sta- tistical Association, 58(301):13–30, March 1963. [22] Daniel Gottesman. An introduction to quantum er- ror correction and fault-tolerant quantum computation. Quantum Information Science and Its Contributions to Mathematics, Proceedings of Symposia in Applied Math- ematics, 68:13–58, 2010. arXiv:0904.2557. [23] Adam Bene Watts, Robin Kothari, Luke Schaeﬀer, and Avishay Tal. Exponential separation between shallow quantum circuits and unbounded fan-in shallow classi- In Proceedings of the 51st Annual ACM cal circuits. SIGACT Symposium on Theory of Computing, STOC 2019, pages 515–526, New York, NY, USA, 2019. As- sociation for Computing Machinery. arXiv:1906.08890. [24] Zhenning Liu and Alexandru Gheorghiu. Depth-eﬃcient proofs of quantumness. July 2021. arXiv:2107.02163. [25] Markus Grassl and Thomas Beth. Cyclic quantum error- correcting codes and quantum shift registers. Proceedings of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences, 456(2003):2689–2706, November 2000. arXiv:quant-ph/9910061. [26] Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph C. Bardin, Rami Barends, Rupak Biswas, Sergio Boixo, Fernando G. S. L. Brandao, David A. Buell, Brian Burkett, Yu Chen, Zijun Chen, Ben Chiaro, Roberto Collins, William Courtney, Andrew Dunsworth, Ed- ward Farhi, Brooks Foxen, Austin Fowler, Craig Gidney, Marissa Giustina, Rob Graﬀ, Keith Guerin, Steve Habeg- ger, Matthew P. Harrigan, Michael J. Hartmann, Alan Ho, Markus Hoﬀmann, Trent Huang, Travis S. Humble, Sergei V. Isakov, Evan Jeﬀrey, Zhang Jiang, Dvir Kafri, Kostyantyn Kechedzhi, Julian Kelly, Paul V. Klimov, Sergey Knysh, Alexander Korotkov, Fedor Kostritsa, David Landhuis, Mike Lindmark, Erik Lucero, Dmitry Lyakh, Salvatore Mandr`a, Jarrod R. McClean, Matthew McEwen, Anthony Megrant, Xiao Mi, Kristel Michielsen, Masoud Mohseni, Josh Mutus, Ofer Naaman, Matthew Neeley, Charles Neill, Murphy Yuezhen Niu, Eric Os- tby, Andre Petukhov, John C. Platt, Chris Quintana, Eleanor G. Rieﬀel, Pedram Roushan, Nicholas C. Ru- bin, Daniel Sank, Kevin J. Satzinger, Vadim Smelyan- skiy, Kevin J. Sung, Matthew D. Trevithick, Amit Vainsencher, Benjamin Villalonga, Theodore White, Z. Jamie Yao, Ping Yeh, Adam Zalcman, Hartmut Neven, and John M. Martinis. Quantum supremacy us- ing a programmable superconducting processor. Nature, 574(7779):505–510, October 2019. arXiv:1910.11333. [27] Hsin-Yuan Huang, Richard Kueng, and John Preskill. Predicting many properties of a quantum system from very few measurements. Nature Physics, 16(10):1050– 1057, June 2020. arXiv:2002.08953. [28] fedja. Answer to stack exchange post. https://tinyurl. com/3b9v7pum, July 2021. [29] Jiantao Jiao, Kartik Venkat, Yanjun Han, and Tsachy Weissman. Minimax estimation of functionals of discrete distributions. IEEE Transactions on Information The- ory, 61(5):2835–2885, May 2015. arXiv:1406.6956. [30] Yihong Wu and Pengkun Yang. Minimax rates of en- tropy estimation on large alphabets via best polynomial approximation. IEEE Transactions on Information The- ory, 62(6):3702–3720, June 2016. arXiv:1407.0381. [31] Jiantao Jiao, Kartik Venkat, Yanjun Han, and Tsachy Weissman. Maximum likelihood estimation of function- als of discrete distributions. IEEE Transactions on In- formation Theory, 63(10):6774–6798, 2017. [32] Jayadev Acharya, Alon Orlitsky, Ananda Theertha Suresh, and Himanshu Tyagi. Estimating renyi en- tropy of discrete distributions. IEEE Transactions on Information Theory, 63(1):38–56, January 2017. arXiv:1408.1000. [33] Jayadev Acharya, Ibrahim Issa, Nirmal V. Shende, and Aaron B. Wagner. Estimating quantum entropy. IEEE Journal on Selected Areas in Information Theory, 1(2):454–468, August 2020. arXiv:1711.00814. [34] Andr´as Gily´en and Tongyang Li. Distributional Prop- erty Testing in a Quantum World. In Thomas Vidick, editor, 11th Innovations in Theoretical Computer Sci- ence Conference (ITCS 2020), volume 151 of Leibniz In- ternational Proceedings in Informatics (LIPIcs), pages 25:1–25:19, Dagstuhl, Germany, 2020. Schloss Dagstuhl– Leibniz-Zentrum fuer Informatik. arXiv:1902.00814. [35] Alessandro Luongo and Changpeng Shao. Quan- tum algorithms for spectral sums. November 2020. arXiv:2011.06475. [36] Sathyawageeswar Subramanian and Min-Hsiu Hsieh. Quantum algorithm for estimating α-renyi entropies of quantum states. Physical Review A, 104(2):022428, Au- gust 2021. arXiv:1908.05251. [37] Youle Wang, Benchi Zhao, and Xin Wang. Quantum al- gorithms for estimating quantum entropies. March 2022. arXiv:2203.02386. [38] David P´erez-Garc´ıa, Michael M. Wolf, Denes Petz, and Mary Beth Ruskai. Contractivity of positive and trace- preserving maps under lp norms. Journal of Mathemat- ical Physics, 47(8):083506, August 2006. arXiv:math- ph/0601063. [39] Umesh Vazirani. Computational probes of Hilbert space. Talk available at https://www.youtube.com/watch?v= ajKoO5RFtwo, December 2019. Quote from Q2B 2019, attributed to an unknown person. [40] Sumeet Khatri, Ryan LaRose, Alexander Poremba, Lukasz Cincio, Andrew T. Sornborger, and Patrick J. Coles. Quantum-assisted quantum compiling. Quantum, 3:140, May 2019. arXiv:1807.00800. [41] Kunal Sharma, Sumeet Khatri, Marco Cerezo, and Patrick J. Coles. Noise resilience of variational quantum compiling. New Journal of Physics, 22(4):043006, April 12 2020. arXiv:1908.04416. [42] Sang Min Lee, Jinhyoung Lee, and Jeongho Bang. Learn- ing unknown pure quantum states. Physical Review A, 98(5):052302, November 2018. arXiv:1805.06580. [43] Ranyiliu Chen, Zhixin Song, Xuanqiang Zhao, and Xin Wang. Variational quantum algorithms for trace distance and ﬁdelity estimation. Quantum Science and Technol- ogy, 7(1):015019, January 2022. arXiv:2012.05768. [44] D´enes Petz. Quasi-entropies for states of a von Neu- mann algebra. Publ. RIMS, Kyoto University, 21:787– 800, 1985. [45] D´enes Petz. Quasi-entropies for ﬁnite quantum systems. Reports in Mathematical Physics, 23:57–65, 1986. Appendix A: Proof of faithfulness and data processing Let us deﬁne the following measures for states ρ and σ and α ∈ (0, 1) ∪ (1, ∞): Kα(ρ(cid:107)σ) := Tr[(I + ρ)α (I + σ)1−α], Qα(ρ(cid:107)σ) := Tr[ρασ1−α]. These measures are related as follows: (cid:13) (cid:13) (cid:13) (cid:13) Kα(ρ(cid:107)σ) = (d + 1) Qα (cid:18) I + ρ d + 1 I + σ d + 1 (cid:19) , where we observe that I+ρ known that d+1 and I+σ d+1 are states. 0 ≤ Qα(ρ(cid:107)σ) ≤ 1 (A1) (A2) (A3) It is (A4) for all states ρ and σ (the lower bound follows because ρ and σ are positive semi-deﬁnite and the upper bound follows by applying the H¨older inequality). Furthermore, the measure Qα(ρ(cid:107)σ) is faithful on states, i.e., equal to 1 if and only if ρ = σ, and it satisﬁes the data-processing inequality [44, 45]: Qα(ρ(cid:107)σ) ≤ Qα(N (ρ)(cid:107)N (σ)), Qα(ρ(cid:107)σ) ≥ Qα(N (ρ)(cid:107)N (σ)), for α ∈ (0, 1) , for α ∈ (1, 2], (A5) (A6) for every channel N . By the relation in (A3), we can conclude properties of Kα(ρ(cid:107)σ) from properties of Qα(ρ(cid:107)σ). Indeed, 0 ≤ Kα(ρ(cid:107)σ) ≤ d + 1. (A7) Also, the measure Kα(ρ(cid:107)σ) is faithful, i.e., equal to d + 1 if and only if ρ = σ. To see this, consider that (cid:18) I + ρ d + 1 I + σ d + 1 (A8) = 1 Qα (cid:19) (cid:13) (cid:13) (cid:13) (cid:13) d+1 = I+σ if and only if I+ρ d+1 . This last equality is equivalent to ρ = σ. Thus, the faithfulness claim follows. Finally, the measure Kα(ρ(cid:107)σ) obeys the data-processing inequal- ity for unital quantum channels. For α ∈ (0, 1) and a unital channel N (i.e., N (I) = I), we have that Kα(ρ(cid:107)σ) ≤ Kα(N (ρ)(cid:107)N (σ)), (A9) and for α ∈ (1, 2], we have that Kα(ρ(cid:107)σ) ≥ Kα(N (ρ)(cid:107)N (σ)). (A10) These inequalities follow from the data-processing in- equality for Qα. Indeed, consider for α ∈ (0, 1) that = (d + 1) Qα (cid:18) I + N (ρ) d + 1 (cid:13) (cid:13) (cid:13) (cid:13) I + N (σ) d + 1 (cid:19) = Kα(N (ρ)(cid:107)N (σ)). 13 (A13) (A14) Kα(ρ(cid:107)σ) = (d + 1) Qα (cid:18) I + ρ d + 1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:18) I + ρ d + 1 I + σ d + 1 (cid:19)(cid:13) (cid:13) (cid:13) (cid:13) (cid:19) N (cid:18) ≤ (d + 1) Qα N (A11) (cid:19)(cid:19) (cid:18) I + σ d + 1 (A12) The second equality follows from linearity of the channel N and the fact that it is unital. The inequality for α ∈ (1, 2] follows similar reasoning as above but instead makes use of (A6).",0
"Speaker Diarization and Identification from Single-Channel Classroom
  Audio Recording Using Virtual Microphones","[{'href': 'http://arxiv.org/abs/2207.00660v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.00660v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-01 21:03:50,,"Personalized Showcases: Generating Multi-Modal Explanations for Recommendations An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian McAuley UC San Diego La Jolla, CA, USA {ayan,zhh004,j9li,tiz010,jmcauley}@ucsd.edu 2 2 0 2 n u J 0 3 ] R I . s c [ 1 v 2 2 4 0 0 . 7 0 2 2 : v i X r a ABSTRACT Existing explanation models generate only text for recommenda- tions but still struggle to produce diverse contents. In this paper, to further enrich explanations, we propose a new task named per- sonalized showcases, in which we provide both textual and visual information to explain our recommendations. Specifically, we first select a personalized image set that is the most relevant to a user’s interest toward a recommended item. Then, natural language ex- planations are generated accordingly given our selected images. For this new task, we collect a large-scale dataset from Google Lo- cal (i.e., maps) and construct a high-quality subset for generating multi-modal explanations. We propose a personalized multi-modal framework which can generate diverse and visually-aligned ex- planations via contrastive learning. Experiments show that our framework benefits from different modalities as inputs, and is able to produce more diverse and expressive explanations compared to previous methods on a variety of evaluation metrics. 1 INTRODUCTION Personalized explanation generation models have the potential to increase the transparency and reliability of recommendations. Previous works [1, 7, 49, 52] considered generating textual explana- tions from users’ historical reviews, tips [27] or justifications [31]. However, these methods still struggle to provide diverse explana- tions because a large amount of general sentences (e.g., ‘food is very good!’) exist in generated explanations and the text gener- ation models lack grounding information (e.g., images) for their generation process. To further diversify and enrich explanations for recommendations, we propose a new explanation generation task named personalized showcases (shown in Figure 1). In this new task, we explain recommendations via both textual and visual infor- mation. Our task aims to provide a set of images that are relevant to a user’s interest and generate textual explanations accordingly. Compared to previous works that generate only text as explana- tions, our showcases present diverse explanations including images and visually-guided text. To this end, the first challenge of this task is building a dataset. Existing review datasets (e.g., Amazon [31] and Yelp1) are largely unsuitable for this task (we further discuss these datasets in Sec- tion 3.2). Thus, we first construct a large-scale multi-modal dataset, namely Gest, which is collected from Google Local2 Restaurants including review text and corresponding pictures. Then, to improve the quality of Gest for personalized showcases, we annotate a small subset to find highly matched image-sentence pairs. Based on the annotations, we train a classifier with CLIP [36] to extract 1https://www.yelp.com/dataset 2https://www.google.com/maps Figure 1: Illustration of previous text-only explanation and our personalized showcases for recommendations. Given a recommended item or business: (1) Text-only Explanation models only use historical textual reviews from user and item sides to generate textual explanations. (2) We propose a personalized showcases task to enrich the personalized ex- planations with multi-modal (visual and textual) informa- tion, which can largely improve the informativeness and di- versity of generated explanations. visually-aware explanations from the full dataset. The images and text explanations from users are used as the learning target for personalized showcases. For this new task, we design a new multi-modal explanation framework. To begin with, the framework selects several images from historical photos of the business that the user is most in- terested in. Then, the framework takes the displayed images and users’ profiles (e.g., historical reviews) as inputs and learns to gen- erate textual explanations with a multi-modal decoder. However, generating expressive, diverse and engaging text that will capture users’ interest remains a challenging problem. First, different from previous textual explanation generation, the alignment between multiple images and generated text becomes an important problem for showcases, which poses higher requirements for information extraction and fusion across modalities. Second, a typical encoder- decoder model with a cross-entropy loss and teacher forcing can easily lead to generating repetitive and dull sentences that occur frequently in the training corpus (e.g., “food is great”) [18]. To tackle these challenges, we propose a Personalized Cross- Modal Contrastive Learning (PC2L) framework by contrasting in- put modalities with output sequences. Contrastive learning has Recommendations … R1: Chinese Food R2: American Food R3: Japanese Food Food is very delicious! Burgers are great, service is good, too. Great selection of beers and delicious burgers! The bread that comes with the entree soup is amazing. The cheesecake is on point. Previous: Text-Only Explanations (e.g. Ref2Seq) Ours: Personalized Showcases (Visual+Textual) An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian McAuley Figure 2: Example of business and user reviews in Gest. For a business (e.g., an Italian restaurant), Gest contains historical reviews and images from different users. drawn attention as a self-supervised representation learning ap- proach [5, 33]. However, simply training with negative samples in a mini-batch is suboptimal [23] for many tasks, as the randomly se- lected embeddings could be easily discriminated in the latent space. Hence, we first design a cross-modal contrastive loss to enforce the alignment between images and output explanations, by construct- ing hard negative samples with randomly replaced entities in the output. Motivated by the observation that users with similar histori- cal reviews share similar interests, we further design a personalized contrastive loss to reweight the negative samples based on their history similarities. Experimental results on both automatic and human evaluation show that our model is able to generate more expressive, diverse and visually-aligned explanations compared to a variety of baselines. Overall, our contributions are as follows: • To generate more informative explanations for recommenda- tions, we present a new task: personalized showcases which can provide both textual and visual explanations for recom- mendations. • For this new task, we collect a large-scale multi-modal dataset from Google Local (i.e., maps). To ensure alignment between images and text, we annotate a small dataset and train a classifier to propagate labels on Gest, and construct a high- quality subset for generating textual explanations. • We propose a novel multi-modal framework for personalized showcases which applies contrastive learning to improve diversity and visual alignment of generated text. Comprehen- sive experiments on both automatic and human evaluation indicate that textual explanations from our showcases are more expressive and diverse than existing explanation gen- eration methods. 2 TASK DEFINITION In the personalized showcases task, we aim to provide both per- sonalized textual and visual explanations to explain recommen- dations for users. Formally, given user 𝑢 ∈ 𝑈 and business (item) 𝑏 ∈ 𝐵, where 𝑈 and 𝐵 are the user set and business set respectively, the personalized showcases task will provide textual explanations 𝑆 = {𝑠1, 𝑠2, ..., 𝑠𝑚 } and visual explanations 𝐼 = {𝑖1, 𝑖2, ..., 𝑖𝑛 }, where 𝑠 and 𝑖 represent sentences and images in explanations. 𝑆 and 𝐼 are matched with each other and personalized to explain why 𝑏 is recommended to 𝑢. To better study the relation between textual and visual expla- nations and provide baselines for future work, in this paper, we decompose the task into two steps as shown in Figure 5: (1) Select- ing an image set as a visual explanation that is relevant to a user’s interest; (2) Generating textual explanations given the selected images and a user’s historical reviews. |𝐼𝑏 | , 𝑖𝑏 2 , . . . 𝑖𝑏 Formally, given user 𝑢, business 𝑏 and the image candidate set 𝐼𝑏 = {𝑖𝑏 } from 𝑏, we first select a set of images as visual 1 explanations 𝐼 from 𝐼𝑏 which user 𝑢 will be interested in, based on user 𝑢’s profile (i.e., historical reviews 𝑋𝑢 = {𝑥𝑢 𝐾 } 1 and images 𝐼𝑢 = {𝑖𝑢 , ..., 𝑖𝑢 𝑛 }). Then, we use the user’s historical 1 reviews 𝑋𝑢 and selected images 𝐼 to generate visually-aware textual explanations 𝑆. , ..., 𝑥𝑢 , 𝑥𝑢 2 , 𝑖𝑢 2 For our method, we consider the following aspects: • Accuracy: We aim to predict the target images (i.e., images associated with the ground-truth review) from business im- age candidates correctly, and the generated text is expected to be relevant to the business. • Diversity: The selected images should be diverse and cover more information from businesses (e.g., including more dishes from a restaurant). Textual explanations should be diverse and expressive. • Alignment: Unlike previous explanation or review gener- ation tasks which only use historical reviews or aspects as inputs, our visually-aware setting provides grounding to the images. Hence the generated explanations in this new task should aim to accurately describe the content and cover the main objects (e.g., the name of dishes, the environment) in the given set of images. Amazing! Best Cesar salad I ever had and the cake was delicious. Seafood soup was excellent. Granddaughter loved the Spaghetti and meatballs. I had an excellent experience at this restaurant. The ambience is romantic and perfect for a couple date night. An Italian Restaurant User Reviews Personalized Showcases: Generating Multi-Modal Explanations for Recommendations Figure 3: Visual Diversity Comparison. A, B, C, E in Ama- zon denote different categories of amazon review datasets, which are uniformly sampled from All, Beauty, Clothing and Electronics, respectively. Intra-/Inter- User Diversity for the Yelp dataset is unavailable since Yelp images lack user information. 3 DATASET 3.1 Dataset Statistics We collected reviews with images from Google Local. Gest-raw in Table 1 shows the data statistics of our crawled dataset. We can see that Gest-raw contains 1,771,160 reviews from 1,010,511 users and 65,113 businesses. Every review has at least one image and the raw dataset has 4,435,565 image urls. We processed our dataset into two subsets as (1) Gest-s1 for personalized image set selection, and (2) Gest-s2 for visually-aware explanation generation. Statistics of our processed dataset are in Ta- ble 1, with more processing details in Section 3.3 and Appendix A. 3.2 Visual Diversity Analysis To distinguish our Gest from existing review datasets and show the usefulness of personalized showcases, we first define CLIP-based dis- similarity in three levels to measure the diversity of user-generated images in each business. Then, we compare the visual diversities between our Gest data with two representative review datasets, Amazon Reviews [29, 31] and Yelp. First, similar to [36, 53], we use the cosine similarity (denoted as sim) from pre-trained CLIP to define the dis-similarity between image 𝑖𝑚 and 𝑖𝑛 as dis(𝑖𝑚, 𝑖𝑛) = 1 − sim(𝑖𝑚, 𝑖𝑛). Thus, we introduce visual diversity in three levels as Intra-Business Div, Inter-User Div and Intra-User Div, which are formally defined in Appendix B; higher scores mean more visual diversity. Then, we investigate the visual diversities for our Gest data as well as Amazon Reviews (using all categories All (A) and sub- categories Beauty (B), Clothing (C), Electronics (E)) and Yelp. For Amazon, we treat each item page as a “business” because reviews are collected according to items. In our calculation, we sample 5,000 items with more than one user-uploaded image. Note that images in Yelp dataset do not have user information, so we cannot calculate user-level diversities for Yelp. From Figure 3, we have the following observations: • Diversities within datasets: Figure 3 shows that for Gest and Amazon, Inter-User Div is the highest and Intra-User Div is the lowest. It indicates even for the same business (item), users focus on and present different visual information. Figure 4: Example of user-generated images from Amazon from an item page and for Yelp from a business. Amazon images mainly focus on a single item and Yelp images for a business are diverse (yet the current public Yelp dataset has no user-image interactions). Table 1: Data statistics for Gest. Avg. R. Len. denotes average review length and #Bus. denotes the number of Businesses. -raw denotes raw Gest. -s1 denotes Gest data for the first step, and -s2 denotes Gest data for the second step of our proposed personalized showcases framework. Dataset #Image #Review #User #Bus. Avg. R. Len. Gest-raw 4,435,565 1,722,296 Gest-s1 203,433 Gest-s2 1,771,160 370,563 108,888 1,010,511 119,086 36,996 65,113 48,330 30,831 36.26 45.48 24.32 • Gest vs. Amazon: In Figure 3, three visual diversities of Amazon are consistently lower than Gest by a large margin. We try to explain this by discussing the difference of user behaviors on these two platforms. As an example in Figure 4, user-generated images usually focus on the purchased item. Though the information they want to show differs, there is usually a single object in an image (i.e., the purchased item). Thus visual diversity is limited. While for Gest, as examples in Figure 2 show, reviews on restaurants allow users to share more diverse information from more varied items, angles or aspects. Compared with Amazon, using Gest should generate more informative personalized showcases according to different user profiles. • Gest vs. Yelp: Yelp images are high-quality (as an example in Figure 4) and the intra-business div. is higher (0.44) than Gest (0.39). Images in Yelp themselves are similar to images in Gest. However, Yelp images do not fit our task due to the lack of user information. 3.3 Explanation Distillation Reviews often contain uninformative text that is irrelevant to the images, and cannot be used directly as explanations. Hence, we con- struct an explanation dataset from Gest-raw. We distill sentences in reviews that align with the content of a given image as valid explanations. Three annotators were asked to label 1,000 reviews (with 9,930 image-sentence pairs) randomly sampled from the full dataset. The task is to decide if a sentence describes a image. Label- ing was performed iteratively, followed by feedback and discussion, Intra-Business Div Inter-User Div Intra-User Div 0.4 0.2 0.0 GEST Amazon-A Amazon-B Amazon-C Amazon-E Yelp Amazon Yelp … … An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian McAuley Figure 5: Illustration of our personalized showcases framework for the given business. We take user historical images and tex- tual reviews as inputs. First, we select an image set that is most relevant to a user’s interest. Then we generate natural language explanations accordingly with a multi-modal decoder. A cross-modal contrastive loss and a personalized contrastive loss are applied between each input modality and the explanations. Last, the selected images and generated textual explanations will be organized as multi-modal explanations to users. until the quality was aligned between the three annotators. The annotated image-sentence pairs are then split into train, validation, and testing with a ratio of 8:1:1. We then train a binary classification model Φ based on these annotated image-sentence pairs and their corresponding labels. Specifically, we extract the embedding of each sentence and image via CLIP. The two features are concatenated and fed into a fully connected layer. The classifier achieves an AUC of 0.97 and F-1 score of 0.71 on the test set, where similar results are obtained in [31] for building a text-only explanation dataset. We use this model to extract explanations from all reviews. The statistics of the dataset Gest-s2 can be found in Table 1. 4 METHODOLOGY In this section, we present our framework of producing personal- ized showcases. As the overview shows (Figure 5), we start with personalized image set selection and the visually-aware explanation generation module, then introduce our personalized cross-modal contrastive learning approach in Section 4.3. 4.1 Personalized Image Set Selection The first step is to select an image set as a visual explanation that is relevant to a user’s interests, and is diverse. We formulate this selection step as diverse recommendation with multi-modal inputs. Multi-Modal Encoder. Generally, these user textual- or visual- profiles can be effectively encoded with different pre-trained deep neural networks (e.g., ResNet [16], ViT [11], BERT [9]). Here we choose CLIP [35], a state-of-the-art pre-trained cross-modal re- trieval model as both textual- and visual-encoders. CLIP encodes raw images as image features, and encodes user textual- and visual- profiles as user profile features. Image Selection Model. We use a Determinantal Point Process (DPP) method [22] to select the image subset, which has recently been used for different diverse recommendation tasks [2, 45]. Com- pared with other algorithms for individual item recommendation, DPP-based models are suitable for multiple image selection. Given user 𝑢 and business 𝑏, we predict the image set ˆ𝐼𝑢,𝑏 as follows: ˆ𝐼𝑢,𝑏 = DPP(𝐼𝑏, 𝑢), (1) where 𝐼𝑏 is the image set belonging to business 𝑏. In our design, we calculate user-image relevance using the CLIP-based user’s profile features and image features. More details of the model are in [45]. 4.2 Visually-Aware Explanation Generation After obtaining an image set, we aim to generate personalized expla- nations given a set of images and a user’s historical reviews, with the extracted explanation dataset Gest-s2 in Section 3.3. Specifically, we build a multi-modal encoder-decoder model with GPT-2 [37] as the backbone. Multi-Modal Encoder. Given a set of user 𝑢’s3 historical reviews 𝑋 = {𝑥1, 𝑥2, . . . , 𝑥𝐾 }, we use the text encoder of CLIP to extract the review features 𝑅 = {𝑟1, 𝑟2, . . . , 𝑟𝐾 }. Similar operations are applied to the input images 𝐼 = {𝑖1, 𝑖2, . . . , 𝑖𝑛 }, where we use a pretrained ResNet to extract the visual features 𝑉 = {𝑣1, 𝑣2, . . . , 𝑣𝑛 }. Those features are then projected into a latent space: 𝑖 = 𝑊 𝑉 𝑣𝑖, 𝑍 𝑅 𝑍𝑉 𝑖 𝑟𝑖, (2) where 𝑊 𝑉 and 𝑊 𝑅 are two learnable projection matrices. Then we use a multi-modal attention (MMA) module with stacked self- attention layers [43] to encode the input features: ; 𝑍 𝑅]), ; 𝐻 𝑅] = MMA([𝑍𝑉 𝑖 = 𝑊 𝑅 [𝐻𝑉 (3) , 𝐻 𝑅 where each 𝐻𝑉 𝑖 aggregate features from two modalities and 𝑖 [; ] denotes concatenation. This flexible design allows for variable lengths of each modality and enables interactions between modali- ties via co-attentions. 3We omit the subscript 𝑢 below for simplicity All review images from the business Multi- Modal Encoder Selection Model … User historical images You have to get the scallops. The “one bad hombre” drink is amazing! …… User historical reviews … STEP 1: Personalized Image Set Selection Cross-Modal Contrastive Learning Multi- Modal Encoder Multi- Modal Decoder Personalized Contrastive Learning STEP 2: Visually-Aware Explanation Everything was fresh and good. Toro Sushi was the bomb and I even dream about it the night after! Personalized Showcases: Generating Multi-Modal Explanations for Recommendations Multi-Modal Decoder. Inspired by recent advances of powerful pre-trained language models, we leverage GPT-2 as the decoder for generating explanations. To efficiently adapt the linguistic knowl- edge from GPT-2, we insert the encoder-decoder attention module into the pre-trained model with a similar architecture in [4]. With this multi-modal GPT-2, given a target explanation 𝑌 = {𝑦1, 𝑦2, ..., 𝑦𝐿 }, the decoding process at each time step 𝑡 can be formalized as ˆ𝑦𝑡 = Decoder([𝐻𝑉 ; 𝐻 𝑅], 𝑦1, . . . , 𝑦𝑡 −1). (4) We use a cross-entropy (CE) loss to maximize the conditional log likelihood log 𝑝𝜃 (𝑌 |𝑋, 𝐼 ) for 𝑁 training samples (𝑋 (𝑖), 𝐼 (𝑖), 𝑌 (𝑖) )𝑁 𝑖=1 as follows: LCE = − 𝑁 ∑︁ 𝑖=1 log 𝑝𝜃 (𝑌 (𝑖) |𝑋 (𝑖), 𝐼 (𝑖) ). (5) We use ground truth images from the user for training and images from our image-selection model for inference. 4.3 Personalized Cross-Modal Contrastive Learning Unlike image captioning tasks where the caption is a short descrip- tion of an image, our task utilizes multiple images as “prompts” to express personal feelings and opinions about them. To encourage generating expressive, diverse and visual-aligned explanations, we propose a Personalized Cross-Modal Contrastive Learning (𝑃𝐶2𝐿) framework. We first project the hidden representations of images, historical reviews, and the target sequence into a latent space: ˜𝐻𝑌 = 𝜙𝑌 (𝐻𝑌 ) ˜𝐻𝑉 = 𝜙𝑉 (𝐻𝑉 ), ˜𝐻 𝑅 = 𝜙𝑅 (𝐻 𝑅), (6) where 𝜙𝑉 , 𝜙𝑅, and 𝜙𝑌 consist of two fully connected layers with ReLU activation [30] and average pooling over the hidden states 𝐻𝑉 , 𝐻𝑅 and 𝐻𝑌 from the last self-attention layers. For the vanilla contrastive learning with InfoNCE loss [5, 33], we then maximize the similarity between the pair of source modality and target se- quence, while minimizing the similarity between the negative pairs as follows: LCL = − 𝑁 ∑︁ 𝑖=1 log exp(𝑠𝑋 ,𝑌 𝑖,𝑖 ) + (cid:205) 𝑗 ∈𝐾 ) exp(𝑠𝑋 ,𝑌 𝑖,𝑗 exp(𝑠𝑋 ,𝑌 𝑖,𝑖 , ) (7) , ˜𝐻𝑌 ( 𝑗) = sim( ˜𝐻 𝑋 (𝑖) where 𝑠𝑋 ,𝑌 )/𝜏, sim is the cosine similarity be- 𝑖,𝑗 tween two vectors, 𝜏 is the temperature parameter, (𝑖) and ( 𝑗) are two samples in the mini-batch, 𝐾 is the set of negative samples for sample (𝑖). One challenge of this task is the model is asked to describe multiple objects or contents in a set of images. To ensure the visual grounding between multiple image features and output text, we design a novel cross-modal contrastive loss. Specifically, given a target explanation 𝑌 = {𝑦1, 𝑦2, ..., 𝑦𝐿 }, we randomly replace the entities 4 in the text with other entities presented in the dataset to construct a hard negative sample 𝑌 ent = {𝑦 ′ , ...𝑦𝐿 } (i.e., “I like the sushi” to “I like the burger”), such that during training, the model is exposed to samples with incorrect entities regarding the images, which are non-trivial to distinguish from the original , 𝑦2, ...𝑦 ′ ent2 ent1 4We extract entities using spaCy noun chunks (https://spacy.io/). target sequence. Thus, we add the hidden representation of 𝑌 ent as an additional negative sample ent to formulate the cross-modal contrastive loss: LCCL = − 𝑁 ∑︁ 𝑖=1 log exp(𝑠𝑉 ,𝑌 𝑖,𝑖 ) exp(𝑠𝑉 ,𝑌 𝑖,𝑖 ) + (cid:205) 𝑗 ∈𝐾∪ent , (8) exp(𝑠𝑉 ,𝑌 𝑖,𝑗 ) On the other hand, to enhance the personalization of explanation generation, we re-weight negative pairs according to user personal- ities. The intuition is that users with more distinct personalities are more likely to generate different explanations. Motivated by this, we propose a weighted contrastive loss for personalization: LPCL = − 𝑁 ∑︁ 𝑖=1 log exp(𝑠𝑅,𝑌 𝑖,𝑖 ) 𝑖,𝑖 ) + 𝑓 (𝑖, 𝑗) (cid:205) 𝑗 ∈𝐾 exp(𝑠𝑅,𝑌 . (9) exp(𝑠𝑅,𝑌 𝑖,𝑗 ) where negative pairs in a mini-batch are re-weighted based on user personality similarity function 𝑓 . In our framework, user person- alities are represented by their historical reviews. Specifically, we define 𝑓 function as: 𝑓 (𝑖, 𝑗) = 𝛼 (1−sim( ˜𝑅 (𝑖 ) , ˜𝑅 ( 𝑗 ) )) (10) i.e., we reduce the weights of negative pairs with similar histories, and increase those with distinct histories. 𝛼 (𝛼 > 1) is a hyperparam- eter that weighs the negative samples, sim is the cosine similarity, ˜𝑅 (𝑖) and ˜𝑅 ( 𝑗) are the average features of two users’ input historical reviews. Overall, the model is optimized with a mixture of a cross-entropy loss and the two contrastive losses: L𝑙𝑜𝑠𝑠 = LCE + 𝜆1LCCL + 𝜆2LPCL, (11) where 𝜆1 and 𝜆2 are hyperparameters that weigh the two losses. 4.4 A Metric for Visual Grounding As mentioned in Section 2, we want our model to generate explana- tions that can accurately describe the content in a given image set. Typical n-gram evaluation metrics such as BLEU compute scores based on n-gram co-occurrences, which are originally proposed for diagnostic evaluation of machine translation systems but not capa- ble of evaluating text quality, as they are only sensitive to lexical variation and fail to reward semantic or syntactic variations be- tween predictions and references [38, 39, 50]. To effectively test the performance of the alignment between visual images and text ex- planations, we design an automatic evaluation metric: CLIP-Align based on [36]. Given a set of images 𝐼 = {𝑖1, 𝑖2, ..., 𝑖𝑛 } and a set of sentences from the generated text 𝑆 = {𝑠1, 𝑠2, ..., 𝑠𝑚 }, we first extract the embeddings of all the images and sentences with CLIP, we compute the metric as follows: CLIP-Align = 1 𝑛 𝑛 ∑︁ 𝑖=1 𝑚𝑎𝑥 ({cs1,𝑖, ..., cs𝑚,𝑖 }) (12) where cs𝑖,𝑗 is the confidence score produced by the CLIP-based classifier Φ trained on our annotated data. By replacing cs𝑖,𝑗 with the cosine similarity of image and sentence embeddings, we obtain another metric CLIP-Score, similar to [17]. Table 2: Results on personalized showcases with different models and different input modalities. Results are reported in per- centage (%). GT is the ground truth. An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian McAuley Model Input N-Gram Metrics Diversity Metrics Embedding Metrics BLEU-1 METEOR NIST Distinct-1 Distinct-2 CLIP-Align CLIP-Score BERT-Score GT ST R2Gen Ref2Seq Peter Ours - img img text text - 8.24 6.47 7.09 8.89 img img+text 9.92 10.40 - 3.41 3.10 3.80 3.28 3.64 3.83 - 28.08 36.55 30.78 34.45 37.35 50.64 6.06 2.74 3.23 0.92 0.38 3.37 3.58 43.23 17.41 22.45 5.89 1.27 26.37 28.58 90.47 80.84 82.07 73.51 72.70 84.78 85.31 28.41 24.31 24.28 23.83 23.27 24.68 24.50 - 85.20 85.89 84.71 86.94 88.03 88.23 Compared with previous CLIP-based metrics [17, 53], CLIP- Align focuses specifically on the accuracy and the alignment be- tween objects in the sentences and the images (e.g. “food is great” and “burger is great” achieves similar high scores with the same burger image computed on CLIP-Score, and a model that repet- itively generates “food is great” can reach high performance on CLIPscore in corpus level). Moreover, the vanilla CLIPscore [17] showed poor correlations with captions containing personal feel- ings, making it less suitable for this task. We show in Section 5 with automatic and human evaluation results that our metric performs better when evaluating alignment between images and text. 5 EXPERIMENTS In this section, we conduct extensive experiments to evaluate the performance of our personalized showcases framework. Ablation studies show the influence of different modalities to personalized showcases. Case studies and human evaluation are conducted to validate that our model present more diverse and accurate explana- tions than baselines. 5.1 Experimental Setting Baselines. To show the effectiveness of our model, we compare it with a number of popular baselines from different tasks, including image captioning, report generation and explanation generation: • ST [47] is a classic CNN+LSTM model for image captioning. • R2Gen [6] is a state-of-the-art memory-driven transformer specialized at generating long text with visual inputs. • Ref2Seq [31] is a popular reference-based seq2seq model for explanation generation in recommendation. • Peter [25] is a recent transformer-based explanation genera- tion model which uses the user and item IDs to predict the words in the target explanation. • img and text refer to image and text features respectively. K images {𝑖1, . . . , 𝑖𝐾 }, div@K is defined as: div@𝐾 = ∑︁ 1≤𝑚<𝑛 ≤𝐾 dis(𝑖𝑚, 𝑖𝑛) 𝐾 (𝐾 − 1)/2 . (13) For textual explanations, we first evaluate the relevance of gener- ated text and ground truth by n-gram based text evaluation metrics: BLEU (n=1,4) [34], METEOR [8] and NIST (n=4) [10]. To evaluate di- versity, we report Dinstinct-1 and Distinct-2 which is proposed in [24] for text generation models. We then use CLIP and BERT to compute embedding-based metrics. CLIP-Align is our proposed metrics in Section 4.2. CLIP-Score [17] BERT-Score [50] are two recent embedding-based metrics. Implementation Details. We use CLIP [35] with ViT-B/32 as image and text encoder to encode user historical reviews and images. We convert user profile feature into a 128-dimensional vector with a MLP model (1024→512→512→256→128), and convert candidate images with another MLP (512→512→512→256→128), where both models use ReLU activations [30]. We follow [45] to calculate each element of 𝑳 and optimize DPP using Adam [28] with an initial learning rate of 1e-3 and batch size 512. For inference, we use greedy decoding to select 𝐾 = 3 images as visual explanation. For training PC2L, we use AdamW [28] as the optimizer with an initial learning rate of 1e-4. The maximum sequence lengths are set to 64 which covers 95% of the explanations. The maximum number of images and historical reviews are set to 5 and 10 respectively. The hidden sizes of both the encoder and decoder are 768 with 12 heads. There are 3 layers in the encoder and 12 layers in the decoder. The batch size for training is 32. We use the GPT-2-small pre-trained weights with 117M parameters. The weighting parameters 𝜆1, 𝛼 and temperature 𝜏 are set to 0.2, 0.2, 𝑒 and 0.1 respectively. We use a beam size of 2 for decoding to balance the generation effectiveness and efficiency. Evaluation Metrics. For image selection, we report Precision@K, Recall@K and F1@K to measure the ranking quality. Due to the nature of our task, we set a small K (𝐾 = 3). To evaluate diversity, we introduce the truncated div@K (𝐾 = 3) for the average dissimi- larities for all image pairs in recommended images. Formally, given 5.2 Framework Performance We first report the model performance on text evaluation met- rics in Table 2, as we found this last step in our framework came with more challenges and interesting findings, e.g., how to gener- ate human-like explanations and avoid dull text, how to evaluate Personalized Showcases: Generating Multi-Modal Explanations for Recommendations Table 3: Ablation study for personalized image selection. Re- sults are reported in percentage (%). Accuracy Diversity Method random img text img+text Prec@3 Recall@3 F1@3 Div@3 4.87 25.21 15.28 25.21 6.14 34.05 20.58 34.37 5.43 28.97 17.54 29.09 30.24 17.12 18.68 17.07 the generation quality. Here the input images are selected by our model,5 and the input text consists of historical reviews from users. First, the clear gap between text-input models and image-input models on diversity and CLIP-based metrics validates the impor- tance of incorporating image features. The setting of visually-aware generation models is able to generate accurate explanations with diverse language style. Second, our 𝑃𝐶2𝐿 shows substantial im- provement on most of the metrics compared to LSTM and trans- former based models, showing that a pretrained language model with contrastive learning is able to generate high quality explana- tions. Finally, though text-based models Ref2Seq and Peter achieve competitive results with our method on some n-gram metrics such as BLEU and METEOR, their performance is much worse on di- versity and embedding metrics. The text quality is also low with repetitive and non-informative sentences appearing often, which we further validate with human evaluations and case studies. 5.3 Component Analysis We conduct ablation studies to evaluate the effectiveness of each component individually. Model for image set selection. First, we evaluate the perfor- mance of personalized image set selection. For general ranking performance, we compare our model with random selection and different input modalities. As shown in Table 3, though the trun- cated diversity of the text-only model is the highest, its performance is significantly worse than those with images in terms of ranking metrics. This indicates text input alone is far insufficient to pro- vide personalization for users, and its recommendation result is closer to that of random selection. Historical images on the other hand, provide an important visual cue for modeling users’ prefer- ence. Overall, a model with images and text can achieve the best ranking performance for image set selection, which validates the importance of our multi-modal setting for personalized showcases. Effectiveness of Contrastive Learning We conduct ablation stud- ies on different variations of our contrastive loss to verify the ef- fectiveness of our method. As shown in Table 4, our PC2L achieves the best performance over all baselines on different metrics. Specif- ically, CCL contributes more to the visual grounding by enforcing the model to distinguish random entities from the correct ones, and 5For effective training and evaluation of our framework, ground truth images of a given user are included in the image candidate pool for selecting. If it is for real-world deployment, ground truth images are not available but similar images can be selected. Table 4: Ablation study on contrastive learning. Baseline is to train a multi-modal decoder without contrastive learning. CL, CCL and PCL are the contrastive losses in Eq. (7), Eq. (8) and Eq. (9) Method Baseline img CL + text CL CCL+ text CL img CL + PCL 𝑃𝐶2𝐿 BLEU-1 Distinct-2 CLIP-Align 7.96 9.72 10.19 9.96 10.40 25.90 27.58 28.10 28.32 28.58 . 82.50 84.03 85.12 84.15 85.31 Figure 6: (a) The length distributions of generated texts on the test set. (b) The generated explanation coverage of nouns (Noun), adjectives (ADJ) and adverbs (ADV) in ground truth. improves CLIP-Align compared to the vanilla contrastive frame- work [5]. PCL improves more on diversity by encouraging the model to focus on users with dissimilar interest. To further evaluate the generation quality improved by con- trastive learning, we analyze the generated explanations from two aspects, length distributions of generations and keywords coverage. Figure 6 (a) compares the length distributions of generations on the test set to the ground truth. We categorize text lengths into 6 groups (within the range of [0, 60] with an interval of 10). The model without PC2L has a sharper distribution, while adding our PC2L leads to a distribution which is closer to the ground truth, demonstrating its effectiveness and the ability to generalize on unseen images. Note the ground truth contains more long texts than generations from the model since we set the max length to 64 during training and inference, which results in the discrepancy for text length greater than 60. Figure 6 (b) shows the keyword coverage (i.e., nouns, adjectives and adverbs) in output sentences. We consider an output as covering a keyword if the word exists in the corresponding ground truth. We compare two models trained with and without PC2L. We can see that PC2L improves the coverage of all kinds of keywords, which indicates our contrastive learning method diversifies and personalizes the generated text. Overall, incorporating contrastive learning into multi-modal explanation generation leads to better output quality with more diverse and visually-aligned texts. Can GPT-2 provide linguistic knowledge? Finally, we study whether GPT-2 can provide linguistic knowledge for our generation (cid:40)(cid:83)(cid:80)(cid:86)(cid:79)(cid:69)(cid:1)(cid:53)(cid:83)(cid:86)(cid:85)(cid:73) (cid:88)(cid:16)(cid:80)(cid:1) (cid:88)(cid:16)(cid:1) (cid:88)(cid:16)(cid:80)(cid:1) (cid:88)(cid:16)(cid:1) (cid:90) (cid:68) (cid:79) (cid:70) (cid:86) (cid:82) (cid:70) (cid:83) (cid:39) (cid:19)(cid:17)(cid:17)(cid:17) (cid:18)(cid:22)(cid:17)(cid:17) (cid:18)(cid:17)(cid:17)(cid:17) (cid:22)(cid:17)(cid:17) (cid:17) (cid:70) (cid:72) (cid:66) (cid:83) (cid:70) (cid:87) (cid:80) (cid:36) (cid:18)(cid:19)(cid:22)(cid:17) (cid:18)(cid:17)(cid:17)(cid:17) (cid:24)(cid:22)(cid:17) (cid:22)(cid:17)(cid:17) (cid:19)(cid:22)(cid:17) (cid:17) (cid:47)(cid:80)(cid:86)(cid:79) (cid:34)(cid:37)(cid:43)(cid:1)(cid:7)(cid:1)(cid:34)(cid:37)(cid:55) (b) (cid:17) (cid:18)(cid:17) (cid:21)(cid:17) (cid:20)(cid:17) (cid:19)(cid:17) (cid:53)(cid:70)(cid:89)(cid:85)(cid:1)(cid:45)(cid:70)(cid:79)(cid:72)(cid:85)(cid:73) (a) (cid:22)(cid:17) (cid:23)(cid:17) An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian McAuley Figure 7: Comparison between text-only explanations (i.e., Ref2Seq and Text GPT-2) and our showcases. User reviews are pro- cessed following Section 3.3. Table 5: Ablation Study on different initializations of the decoder. Random randomly initializes model weights. Text GPT-2 and Img GPT-2 are initialized with weights from [37]. Img GPT-2 + FT finetunes the model on a corpus similar to our training text data. Results are in percentage (%). Table 6: Human evaluation results on two models. We present the workers with reference text and images, and ask them to give scores from different aspects. Results are statis- tically significant via sign test (p<0.01). Method Img Random Text GPT-2 Img GPT-2 Img GPT-2 + FT BLEU-1 Distinct-1 Distinct-2 5.21 4.81 7.59 7.10 0.23 3.43 4.05 4.32 5.08 19.27 29.41 30.82 task. We train models with different weight initializations, with ground truth images (Img) or historical reviews (Text) as inputs. As shown in Table 5, comparing the performance of random and GPT-2 initialization, it is evident that the pretrained weights play a significant role. Finetuning on in-domain data (260k samples from users with one review and excluded from our personalization dataset) further improves domain-specific knowledge of the decoder and benefits generation performance on diversity metrics. 5.4 Case Study We study three examples (see Figure 7) and compare our person- alized showcases to single-modal explanations from Ref2Seq and Text GPT-2. Overall, our visual explanations is able to recommend images that fit users’ interest. This indicates the effectiveness of our image selection module and the selected images can be used as valid visual explanations. More importantly, these images can provide grounding information for text generation such that the textual explanations become more informative (i.e., specific dishes), which aligns with our CLIP-Align metric as well as human evaluations in Section 5.5. As is shown in Figure 7, we can see historical review text alone cannot provide correct explanations (see Case 1) to the Method Expressiveness Visual Alignment Ref2Seq PC2L 3.72 4.25 3.65 4.10 user (i.e., explanations from Ref2Seq and Text GPT-2 are irrelevant to the user review) and the sentences are monotonous (see Case 2). In contrast, our showcase provides relevant and diverse textual explanations based on images. In case 3, our generated text missed some entities in the user’s review since it only correctly describes one of the selected images. Hence, generating texts from multiple images is still a challenging problem for this new task. As we can observe from the examples, Ref2Seq tends to gen- erate explanations with the same pattern, which also match the observation in Table 2 that it has low Distinct-1 and Distinct-2. 5.5 Human Evaluation To fully evaluate our model, we conduct human evaluation on Amazon Mechanical Turk.6 For each model, we randomly sample 500 examples from the test set. Each example is scored by three human judges using a 5-point Likert scale to reduce variance. We instruct the annotators to consider two perspectives, expressiveness (semantically correct, diversity, no repetition) and visual alignment (the text describes the context of the images). As is shown in Table 6, PC2L significantly outperforms Ref2Seq, which is consistent with the automatic evaluation metrics. 6https://www.mturk.com/ We ordered pork and shrimp spring rolls that came with a peanut-y dipping sauce. Then we ordered a chicken banh-mi and a lemongrass beef with noodles. The steak frites was tasty - it was charred, which I really liked, and topped with a butter sauce. The truffle fries were also really, really good. The burger was delicious though! My co worker said the Pork Torta was delicious! Other guys had Gyro, pizza and fish tacos. My Bacon Cheeseburger was excellent. we ordered the fried rice and it was very good. i had the grilled chicken sandwich , which was delicious . i had the grilled cheese sandwich and it was delicious ! i love it if you want to eat japanese - style ramen. first time here, i had the bbq bacon cheeseburger medium rare with onion rings. the rice pilaf was very good as well. if you like vietnamese food, you should try this place out. the spring rolls are a definite must -. the pho is good. old school rustic feel with a wide selection of burgers and beers. the burgers were done well …… bloody mary was perfect. food was wonderful, try the fried green tomato breakfast tacos. EXAMPLE 1 EXAMPLE 2 EXAMPLE 3 Processed User Reviews Previous Ref2Seq Previous Text GPT-2 Ours Personalized Showcases Personalized Showcases: Generating Multi-Modal Explanations for Recommendations 6 RELATED WORK 6.1 Explanation Generation There has been a line of work that studies how to generate explana- tions for recommendations. Some work generates product reviews based on categorical attributes [52] images [42], or aspects [32]. Due to noise in reviews, Li et al. [26] generated ‘tips’ from the Yelp dataset which are more concise and informative as explanations in recommendation. To further improve the quality of generation, Ni et al. [31] proposed to identify justifications by dividing re- views into text segments and classifying text segments to get “good” justifications. Li et al. [25] proposed transformer-based model for recommendation explanation generations by incorporating user, item embeddings and related features. These text generation tasks leverage historical reviews from users or items. Images, on the other hand, provide rich information and grounding for text generation. Moreover, multi-modal information in our task (i.e., images and text) are more acceptable than text as explanations for users. In this paper, we propose a new task for generating multi-modal explanations and present a framework that provides personalized image showcases and visually-aware text explanations for recom- mendations. 6.2 Multi-Modal Learning Recent years have witnessed the success of deep learning on multi- modal learning and pretraining [4, 20, 35, 41]. These models usually adopt the Transformer [43] structure to encode visual and textual features for pretraining, to later benefit the multimodal downstream tasks. Among them, CLIP [35] is a powerful model trained on a massive amount of image-caption pairs, and has shown a strong zero-shot capability on various vision and language tasks [40]. Sev- eral methods [17, 53] used CLIP embeddings to compute modality similarities as evaluation metrics for image captioning and text generation tasks. In our work, we used CLIP extensively as the multi-modal en- coder for our framework. We also designed a new metric based on CLIP for evaluating the visual alignment between the image set and generated explanations. 6.3 Contrastive Learning The goal of contrastive learning [14, 33] is to learn representations by contrasting positive and negative pairs. It has been investigated in several fields of machine learning, including computer vision [5, 15, 21], natural language processing [12, 13, 19], and recommender systems [44, 46, 51]. A few recent work showed promising results of applying contrastive learning to conditional text generation, by generating adversarial examples [23], or finding hard negatives with pretrained language models [3, 48]. Our work differs in that we study contrastive learning for condi- tional text generation in a cross-modal setting, where we proposed a novel contrastive framework for generating personalized multi- modal explanations. 7 CONCLUSION In this paper, to generate explanations with rich information for recommendations, we introduce a new task, namely personalized showcases, and collect a large-scale dataset Gest from Google Local for the task. We design a personalized cross-modal contrastive learning framework to learn visual and textual explanations from user reviews. Experimental results show that showcases provide more informative and diverse explanations compared to previous text-only explanations. As future work, one promising direction is to develop an end-to-end framework for generating both visual and textual explanations. Besides, visual grounding on multiple images is still challenging for showcases. Another interesting setting is to address cold-start users or reviews written without images. We hope our dataset and framework would benefit the community for future research on multi-modalities and recommendations. A DATA CONSTRUCTION Our dataset is constructed from Google Local (i.e., maps) using a breadth-first-search algorithm with memorization. After collect- ing the review data, we filtered out reviews of length less than 5 words, which are less likely to provide useful information; we also removed reviews (2.13%) containing more than 10 images. The details of Gest-s1 construction for personalized image selection are as follows: We remove users with only one review for building a personalized dataset, then filter out reviews whose image urls are expired. After pre-processing, statistics for the personalized show- case dataset are shown in Table 1, where the number of images per business is 35.63 on average. We then randomly split the dataset by users, with 95,270/11,908/11,908 users for train/val/test. B VISUAL DIVERSITY DEFINITION We define the visual diversities in three levels as below: • Intra-Business Div: Measure the average diversity for im- age pairs at a business-level, where P1 (𝑏) means all the possible image pairs for business 𝑏. 𝑍1 is the valid counts7 of dis-similarity calculations (same as below): ∑︁ ∑︁ 𝑏 ∈𝐵 𝑚,𝑛 ∈ P (𝑏) dis(𝑖𝑏 𝑚, 𝑖𝑏 𝑛) 𝑍1 . (14) • Inter-User Div: Measure the average diversity for image pairs from different users for the same business, where P2 (𝑏) means all possible image pairs for business 𝑏 that come from different users: ∑︁ ∑︁ 𝑏 ∈𝐵 𝑚,𝑛 ∈ P2 (𝑏) dis(𝑖𝑏 𝑚, 𝑖𝑏 𝑛) 𝑍2 . (15) • Intra-User Div: Measure the average diversity in (business, user)-level, where P3 (𝑢, 𝑏) means all possible image pairs from user 𝑢 to business 𝑏: ∑︁ ∑︁ ∑︁ 𝑏 ∈𝐵 𝑢 ∈𝑈 𝑚,𝑛 ∈ P3 (𝑢,𝑏) dis(𝑖𝑏 𝑚, 𝑖𝑏 𝑛) 𝑍3 . (16) REFERENCES [1] Ashutosh Baheti, Alan Ritter, Jiwei Li, and William B. Dolan. 2018. Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints. In EMNLP. 7When image set size is not more than 1, the dis-similarity calculation is invalid. [2] Jinze Bai, Chang Zhou, Junshuai Song, Xiaoru Qu, Weiting An, Zhao Li, and Jun Gao. 2019. Personalized Bundle List Recommendation. The World Wide Web Conference (2019). [3] Hengyi Cai, Hongshen Chen, Yonghao Song, Zhuoye Ding, Yongjun Bao, Weipeng Yan, and Xiaofang Zhao. 2020. Group-wise contrastive learning for neural dia- logue generation. arXiv preprint arXiv:2009.07543 (2020). [4] Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. 2021. VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. [5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In Interna- tional conference on machine learning. PMLR, 1597–1607. [6] Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan. 2020. Generating Ra- diology Reports via Memory-driven Transformer. arXiv preprint arXiv:2010.16056 (2020). [7] Zhongxia Chen, Xiting Wang, Xing Xie, Tong Wu, Guoqing Bu, Yining Wang, and Enhong Chen. 2019. Co-Attentive Multi-Task Learning for Explainable Recommendation. In IJCAI. [8] Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reli- able optimization and evaluation of machine translation systems. In Proceedings of the sixth workshop on statistical machine translation. 85–91. [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL. [10] George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the second international conference on Human Language Technology Research. 138–145. [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi- aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ArXiv abs/2010.11929 (2021). [12] Hongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding, and Pengtao Xie. 2020. Cert: Contrastive self-supervised learning for language understanding. arXiv preprint arXiv:2005.12766 (2020). [13] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. arXiv preprint arXiv:2104.08821 (2021). [14] Michael Gutmann and Aapo Hyvärinen. 2010. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings, 297–304. [15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momen- tum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9729–9738. [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770–778. [17] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: A Reference-free Evaluation Metric for Image Captioning. arXiv preprint arXiv:2104.08718 (2021). [18] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751 (2019). [19] Jiaji Huang, Yi Li, Wei Ping, and Liang Huang. 2018. Large margin neural language model. arXiv preprint arXiv:1808.08987 (2018). [20] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. 2020. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849 (2020). [21] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive learning. arXiv preprint arXiv:2004.11362 (2020). [22] Alex Kulesza and Ben Taskar. 2012. Determinantal Point Processes for Machine Learning. Found. Trends Mach. Learn. 5 (2012), 123–286. [23] Seanie Lee, Dong Bok Lee, and Sung Ju Hwang. 2020. Contrastive Learning with Adversarial Perturbations for Conditional Text Generation. arXiv preprint arXiv:2012.07280 (2020). [24] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2015. A diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055 (2015). An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian McAuley [29] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. 2015. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval. 43–52. [30] Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Icml. [31] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 188–197. [32] Jianmo Ni and Julian McAuley. 2018. Personalized Review Generation By Ex- panding Phrases and Attending on Aspect-Aware Representations. In ACL. [33] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [34] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311–318. [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020 (2021). [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In ICML. [37] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. [38] Ehud Reiter. 2018. A structured review of the validity of BLEU. Computational Linguistics 44, 3 (2018), 393–401. [39] Thibault Sellam, Dipanjan Das, and Ankur P Parikh. 2020. BLEURT: Learning robust metrics for text generation. arXiv preprint arXiv:2004.04696 (2020). [40] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai- Wei Chang, Zhewei Yao, and Kurt Keutzer. 2021. How Much Can CLIP Benefit Vision-and-Language Tasks? arXiv preprint arXiv:2107.06383 (2021). [41] Hao Tan and Mohit Bansal. 2019. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490 (2019). [42] Quoc-Tuan Truong and Hady Lauw. 2019. Multimodal review generation for recommender systems. In The World Wide Web Conference. 1864–1874. [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998–6008. [44] Yinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuanping Li, and Tat-Seng Chua. 2021. Contrastive learning for cold-start recommendation. In Proceedings of the 29th ACM International Conference on Multimedia. 5382–5390. [45] Mark Wilhelm, Ajith Ramanathan, Alexander Bonomo, Sagar Jain, Ed H. Chi, and Jennifer Gillenwater. 2018. Practical Diversified Recommendations on YouTube with Determinantal Point Processes. Proceedings of the 27th ACM International Conference on Information and Knowledge Management (2018). [46] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Bolin Ding, and Bin Cui. 2020. Contrastive Learning for Sequential Recommendation. arXiv preprint arXiv:2010.14395 (2020). [47] Ke Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In ICML. [48] An Yan, Zexue He, Xing Lu, Jiang Du, Eric Chang, Amilcare Gentili, Julian McAuley, and Chun-Nan Hsu. 2021. Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation. arXiv preprint arXiv:2109.12242 (2021). [49] Hongyu Zang and Xiaojun Wan. 2017. Towards Automatic Generation of Product Reviews from Aspect-Sentiment Scores. In INLG. [50] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019). [51] Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, and Hongxia Yang. 2021. Contrastive learning for debiased candidate generation in large-scale recom- mender systems. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 3985–3995. [25] Lei Li, Yongfeng Zhang, and Li Chen. 2021. Personalized Transformer for Ex- [52] M. Zhou, Mirella Lapata, Furu Wei, Li Dong, Shaohan Huang, and Ke Xu. 2017. plainable Recommendation. In ACL/IJCNLP. [26] Piji Li, Zihao Wang, Lidong Bing, and Wai Lam. 2019. Persona-Aware Tips Generation? The World Wide Web Conference (2019). [27] Piji Li, Zihao Wang, Zhaochun Ren, Lidong Bing, and Wai Lam. 2017. Neu- ral Rating Regression with Abstractive Tips Generation for Recommendation. Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (2017). [28] Ilya Loshchilov and Frank Hutter. 2017. Fixing Weight Decay Regularization in Adam. ArXiv abs/1711.05101 (2017). Learning to Generate Product Reviews from Attributes. In EACL. [53] Wanrong Zhu, Xin Eric Wang, An Yan, Miguel Eckstein, and William Yang Wang. 2021. ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation. arXiv preprint arXiv:2106.05970 (2021).",1
Multivariate trace estimation in constant quantum depth,"[{'href': 'http://arxiv.org/abs/2206.15405v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2206.15405v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-30 16:44:58,,"Watch and Match: Supercharging Imitation with Regularized Optimal Transport Siddhant Haldar1 Vaibhav Mathur Denis Yarats Lerrel Pinto New York University rot-robot.github.io Abstract: Imitation learning holds tremendous promise in learning policies efﬁciently for complex decision making problems. Current state-of-the-art algorithms often use inverse reinforcement learning (IRL), where given a set of expert demonstrations, an agent alternatively infers a reward function and the associated optimal policy. However, such IRL approaches often require substantial online interactions for complex control problems. In this work, we present Regularized Optimal Transport (ROT), a new imitation learning algorithm that builds on recent advances in optimal transport based trajectory-matching. Our key technical insight is that adaptively combining trajectory-matching rewards with behavior cloning can signiﬁcantly accelerate imitation even with only a few demonstrations. Our experiments on 20 visual control tasks across the DeepMind Control Suite, the OpenAI Robotics Suite, and the Meta-World Benchmark demonstrate an average of 7.8× faster imitation to reach 90% of expert performance compared to prior state-of-the-art methods. On real-world robotic manipulation, with just one demonstration and an hour of online training, ROT achieves an average success rate of 90.1% across 14 tasks. Keywords: Imitation Learning, Manipulation, Robotics 2 2 0 2 n u J 0 3 ] O R . s c [ 1 v 9 6 4 5 1 . 6 0 2 2 : v i X r a Figure 1: (Top) Regularized Optimal Transport (ROT) is a new imitation learning algorithm that adaptively combines ofﬂine behavior cloning with online trajectory-matching based rewards. This enables signiﬁcantly faster imitation across a variety of simulated and real robotics tasks, while being compatible with high-dimensional visual observation. (Bottom) On our xArm robot, ROT can learn visual policies with only a single human demonstration and under an hour of online training. 1Correspondence to: sh6474@nyu.edu Opening a box Hanging a tote bag Placing peg in a box Pouring almonds t n e g A t r e p x E πBC πROT OT Rewards Adaptive Regularization OT Computation Environments Environment Interactions 1 Introduction Imitation Learning (IL) [1, 2, 3] has a rich history that can be categorized across two broad paradigms, Behavior Cloning (BC) [1] and Inverse Reinforcement Learning (IRL) [4]. BC uses supervised learning to obtain a policy that maximizes the likelihood of taking the demonstrated action given an observation in the demonstration. While this allows for training without online interactions, it suffers from distributional mismatch during online rollouts [5]. IRL, on the other hand, infers the underlying reward function from the demonstrated trajectories before employing RL to optimize a policy through online environment rollouts. This results in a policy that can robustly solve demonstrated tasks even in the absence of task-speciﬁc rewards [6, 7]. Although powerful, IRL methods suffer from a signiﬁcant drawback – they require numerous expensive online interactions with the environment. There are three reasons for this: (a) the inferred reward function is often highly non-stationary, which compromises the learning of the associated behavior policy [7]; (b) even when the rewards are stationary, policy learning still requires effective exploration to maximize rewards [8]; and (c) when strong priors such as pretraining with BC are applied to accelerate policy learning, ensuing updates to the policy cause a distribution shift that destabilizes training [9, 10]. Combined, these issues manifest themselves on empirical benchmarks, where IRL methods have poor efﬁciency compared to vanilla RL methods on hard control tasks [11]. In this work, we present Regularized Optimal Transport (ROT) for imitation learning, a new method that is conceptually simple, compatible with high-dimensional observations, and requires minimal additional hyperparameters compared to standard IRL approaches. In order to address the challenge of reward non-stationarity in IRL, ROT builds upon recent advances in using Optimal Transport (OT) [12, 13, 11] for reward computation that use non-parametric trajectory-matching functions. To alleviate the challenge of exploration, we pretrain the IRL behavior policy using BC on the expert demonstrations. This reduces the need for our imitation agent to explore from scratch. However, even with OT-based reward computation and pretrained policies, we only obtain marginal gains in empirical performance. The reason for this is that the high-variance of IRL policy gradi- ents [14, 15] often wipe away the progress made by the ofﬂine BC pretraining. This phenomenon has been observed in both online RL [16] and ofﬂine RL [9] methods. Inspired by solutions presented in these works, we stabilize the online learning process by regularizing the IRL policy to stay close to the pretrained BC policy. To enable this, we develop a new adaptive weighing scheme called soft Q-ﬁltering that automatically sets the regularization – prioritizing staying close to the BC policy in the beginning of training and prioritizing exploration later on. In contrast to prior policy regularization schemes [16, 17], soft Q-ﬁltering does not require hand-speciﬁcation of decay schedules. To demonstrate the effectiveness of ROT, we run extensive experiments on 20 simulated tasks across DM Control [18], OpenAI Robotics [19], and Meta-world [20], and 14 robotic manipulation tasks on an xArm (see Fig. 1). Our main ﬁndings are summarized below. 1. ROT outperforms prior state-of-the-art imitation methods, reaching 90% of expert performance 7.8× faster than our strongest baselines on simulated visual control benchmarks. 2. On real-world tasks, with a single human demonstration and an hour of training, ROT achieves an average success rate of 90.1% with randomized robot initialization and image observations. This is signiﬁcantly higher than behavior cloning (36.1%) and adversarial IRL (14.6%). 3. ROT exceeds the performance of state-of-the-art RL trained with rewards, while coming close to methods that augment RL with demonstrations (Section 5.5 & Appendix H.3). Unlike standard RL methods, ROT does not require hand-speciﬁcation of the reward function. 4. Ablation studies demonstrate the importance of every component in ROT, particularly the role that soft Q-ﬁltering plays in stabilizing training and the need for OT-based rewards during online learning (Section 5.3 & Appendix H.4). Open-source code and demonstration data will be publicly released on our project website. Videos of our trained policies can be seen here: rot-robot.github.io/. 2 Figure 2: (a) Given a single demonstration to avoid the grey obstacle and reach the goal location, BC is unable to solve the task. (b) Finetuning from this BC policy with OT-based reward also fails to solve the task. (c) ROT, with adaptive regularization of OT-based IRL with BC successfully solves the task. (d) Even when the ROT agent is initialized randomly, it is able to solve the task. 2 Background Before describing our method in detail, we provide a brief background to imitation learning with optimal transport, which serves as the backbone of our method. Formalism related to RL follows the convention in prior work [8, 11] and is described in Appendix A. t=1}N Imitation Learning with Optimal Transport (OT) The goal of imitation learning is to learn a behavior policy πb given access to either the expert policy πe or trajectories derived from the expert policy T e. While there are a multitude of settings with differing levels of access to the expert [21], our work operates in the setting where the agent only has access to observation-based trajectories, i.e. T e ≡ {(ot, at)T n=1. Here N and T denotes the number of trajectory rollouts and episode timesteps respectively. Inverse Reinforcement Learning (IRL) [4, 22] tackles the IL problem by inferring the reward function re based on expert trajectories T e. Then given the inferred reward re, policy optimization is used to derive the behavior policy πb. To compute re, a new line of OT-based approaches for IL [12, 13, 11] have been proposed. Intuitively, the closeness between expert trajectories T e and behavior trajectories T b can be computed by measuring the optimal transport of probability mass from T b → T e. Thus, given a cost matrix Ct,t(cid:48) = c(ob t(cid:48)) and the optimal alignment µ∗ between a behavior trajectory ob and and expert trajectory oe, a reward signal for each observation can be computed using the equation: t , oe rOT (ob t ) = − T (cid:88) t(cid:48)=1 Ct,t(cid:48) µ∗ t,t(cid:48) (1) A detailed account of the OT formulation has been provided in Appendix A. Actor-Critic based reward maximization Given rewards obtained through OT computation, efﬁ- cient maximization of the reward can be achieved through off-policy learning [7]. In this work, we use Deep Deterministic Policy Gradient (DDPG) [23] as our base RL optimizer which is an actor-critic algorithm that concurrently learns a deterministic policy πφ and a Q-function Qθ. However, instead of minimizing a one step Bellman residual in vanilla DDPG, we use the recent n-step version of DDPG from Yarats et al. [8] that achieves high performance on visual control problems. 3 Challenges in Online Finetuning from a Pretrained Policy In this section, we study the challenges with ﬁnetuning a pretrained policy with online interactions in the environment. Fig. 2 illustrates a task where an agent is supposed to navigate the environment from the top left to the bottom right, while dodging obstacles in between. The agent has access to a single expert demonstration, which is used to learn a BC policy for the task. Fig. 2 (a) shows that this BC policy, though close to the expert demonstration, performs suboptimally due to accumulating errors on out-of-distribution states during online rollouts [5]. Further, Fig. 2 (b) uses this BC policy 3 Expert trajectory BC trajectory Start location Goal location (a) Task: Particle Reach (b) IRL Finetune w/o Reg. (c) ROT (d) ROT + random init. 100k s p e t s e m i t 0 Expert trajectory BC trajectory Start location Goal location as an initialization and naively ﬁnetunes it with OT rewards (described in Section 2). Such naive ﬁnetuning of a pretrained policy (or actor) with an untrained critic in an actor-critic framework exhibits a forgetting behavior in the actor, resulting in performance degradation as compared to the pretrained policy. This phenomenon has also been reported by Nair et al. [9] and we provide a detailed discussion in Appendix B. In this paper, we propose ROT which addresses this issue by adaptively keeping the policy close to the behavior data during the initial phase of ﬁnetuning and reduces this dependence over time. Fig. 2 (c) demonstrates the performance of our approach on such ﬁnetuning. It can be clearly seen that even though the BC policy is suboptimal, our proposed adaptive regularization scheme quickly improves and solves the task by driving it closer to the expert demonstration. In Fig. 2 (d), we demonstrate that even if the agent was initialized at points outside the expert trajectory, the agent is still able to learn quickly and complete the task. This generalization to starting states would not be possible with regular BC. 4 Regularized Optimal Transport A fundamental challenge in imitation learning is to balance the ability to mimic demonstrated actions along with the ability to recover from states outside the distribution of demonstrated states. Behavior Cloning (BC) specializes in mimicking demonstrated actions through supervised learning, while Inverse Reinforcement Learning (IRL) specializes in obtaining policies that can recover from arbitrary states. Regularized Optimal Transport (ROT) combines the best of both worlds by adaptively combining the two objectives. This is done in two phases. In the ﬁrst phase, a randomly initialized policy is trained using the BC objective on expert demonstrated data. This ‘BC-pretrained’ policy then serves as an initialization for the second phase. In the second phase, the policy is allowed access to the environment where it can train using an IRL objective. To accelerate the IRL training, the BC loss is added to the objective with an adaptive weight. Details of each component are described below, with additional algorithmic details in Appendix C. 4.1 Phase 1: BC Pretraining BC corresponds to solving the maximum likelihood problem shown in Eq. 2. Here T e refers to expert demonstrations. When parameterized by a normal distribution with ﬁxed variance, the objective can be framed as a regression problem where, given inputs se, πBC needs to output ae. LBC = E(se,ae)∼T e (cid:107)ae − πBC(se)(cid:107)2 (2) After training, it enables πBC to mimic the actions corresponding to the observations seen in the demonstrations. However, during rollouts in an environment, small errors in action prediction can lead to the agent visiting states not seen in the demonstrations [5]. This distributional mismatch often causes πBC to fail on empirical benchmarks [16, 11] (see Fig. 2 (a) in Sec. 3). 4.2 Phase 2: Online Finetuning with IRL Given a pretrained πBC model, we now begin online ‘ﬁnetuning’ of the policy πb ≡ πROT in the environment. Since we are operating without explicit task rewards, we use rewards obtained through OT-based trajectory matching, which is described in Section 2. These OT-based rewards rOT enable the use of standard RL optimizers to maximize cumulative reward from πb ≡ πROT . In this work we use n-step DDPG [23], a deterministic actor-critic based method that provides high-performance in continuous control [8]. Finetuning with Regularization πBC is susceptible to distribution shift due to accumulation of errors during online rollouts [5] and directly ﬁnetuning πBC also leads to subpar performance (refer to Fig. 2 in Sec. 3). To address this, we build upon prior work in guided RL [16] and ofﬂine RL [9], and regularize the training of πROT by combining it with a BC loss as seen in Eq. 3. πROT = argmax π (cid:2)(1 − λ(π)))E(s,a)∼Dβ [Q(s, a)] − αλ(π)E(se,ae)∼T e (cid:107)ae − πBC(se)(cid:107)2(cid:3) (3) 4 Here, Q(s, a) represents the Q-value from the critic used in actor-critic policy optimization. α is a ﬁxed weight, while λ(π) is a policy-dependent adaptive weight that controls the contributions of the two loss terms. Dβ refers to the replay buffer for online rollouts. Adaptive Regularization with Soft Q-ﬁltering While prior work [16, 17] use hand-tuned sched- ules for λ(π), we propose a new adaptive scheme that removes the need for tuning. This is done by comparing the performance of the current policy πROT and the pretrained policy πBC on a batch of data sampled from an expert replay buffer De. More precisely, given a behavior policy πBC(s), the current policy πROT (s), the Q-function Q(s, a) and the replay buffer De, we set λ as: λ(πROT ) = E(s,·)∼De (4) The strength of the BC regularization hence depends on the performance of the current policy with respect to the behavior policy. This ﬁltering strategy is inspired by Nair et al. [24], where instead of a binary hard assignment we use a soft continuous weight. Experimental comparisons with hand-tuned decay strategies are presented in Section 5.3. (cid:2)1Q(s,πBC (s))>Q(s,πROT (s)) (cid:3) Considerations for image-based observations Since we are interested in using ROT with high- dimensional visual observations, additional machinery is required to ensure compatibility. Following prior work in image-based RL and imitation [8, 11], we perform data augmentations on visual observations and then feed it into a CNN encoder. Similar to Cohen et al. [11], we use a target encoder with Polyak averaging to obtain representations for OT reward computation. This is necessary to reduce the non-stationarity caused by learning the encoder alongside the ROT imitation process. Further implementation details and the training procedure can be found in Appendix C. 5 Experiments Our experiments are designed to answer the following questions: (a) How efﬁcient is ROT for imitation learning? (b) How does ROT perform on real-world tasks? (c) How important is the choice of IRL method in ROT? (d) Does soft Q-ﬁltering improve imitation? (e) How does ROT compare to standard reward-based RL? Additional results and analysis have been provided in Appendix H. Simulated tasks We experiment with 10 tasks from the DeepMind Control suite [18, 25], 3 tasks from the OpenAI Robotics suite [26], and 7 tasks from the Meta-world suite [27]. For DeepMind Control tasks, we train expert policies using DrQ-v2 [8] and collect 10 demonstrations for each task using this policy. For OpenAI Robotics tasks, we train a state-based DrQ-v2 with hindsight experience replay [28] and collect 50 demonstrations for each task. For Meta-world tasks, we use a single hard-coded expert demonstration from their open-source implementation [27]. Full environment details can be found in Appendix D and details about the variations in demonstrations and initialization conditions can be found in Appendix E. Robot tasks Our real world setup for each of the 14 manipulation tasks can be seen in Fig. 4. We use an Ufactory xArm 7 robot with a xArm Gripper as the robot platform for our real world experiments. However, our method is agnostic to the speciﬁc robot hardware. The observations are RGB images from a ﬁxed camera. In this setup, we only use a single expert demonstration collected by a human operator with a joystick and limit the online training to a ﬁxed period of 1 hour. Descriptions of each task and the evaluation procedure is in Appendix F. Primary baselines We compare ROT with baselines against several prominent imitation learning methods. While a full description of our baselines are in Appendix G, a brief description of the two strongest ones are as follows: 1. Adversarial IRL (DAC): Discriminator Actor Critic [7] is a state-of-the-art adversarial imitation learning method [6, 29, 7]. DAC outperforms prior work such as GAIL [6] and AIRL [30], and thus it serves as our primary adversarial imitation baseline. 2. Trajectory-matching IRL (OT): Sinkhorn Imitation Learning [12, 13] is a state-of-the-art trajectory-matching imitation learning method [31] that approximates OT matching through the Sinkhorn Knopp algorithm [32, 33]. Since ROT is derived from similar OT-based foundations, we use SIL as our primary state-matching imitation baseline. 5 Expert BC OT DAC ROT (Ours) Figure 3: Pixel-based continuous control learning on 9 selected environments. Shaded region represents ±1 standard deviation across 5 seeds. We notice that ROT is signiﬁcantly more sample efﬁcient compared to prior work. 5.1 How efﬁcient is ROT for imitation learning? Performance of ROT for image-based imitation is depicted on select environments in Fig. 3. On all but one task, ROT trains signiﬁcantly faster than prior work. To reach 90% of expert performance, ROT is on average 8.7× faster on DeepMind Control tasks, 2.1× faster on OpenAI Robotics tasks, and 8.9× faster on Meta-world tasks. We also ﬁnd that the improvements of ROT are most apparent on the harder tasks, which are in rightmost column of Fig. 3. Appendix H.1 shows results on all 20 simulated tasks, along with experiments that exhibit similar improvements in state-based settings. 5.2 How does ROT perform on real-world tasks? We devise a set of 14 manipulation tasks on our xArm robot to compare the performance of ROT with BC and our strongest baseline RDAC, an adversarial IRL method that combines DAC [7] with our pretraining and regularization scheme. The BC policy is trained using supervised learning on a single expert demonstration collected by a human operator. ROT and RDAC ﬁnetune the pretrained BC policy through 1 hour of online training, which amounts to ∼ 6k environment steps. Since there is just one demonstration, our tasks are designed to have random initializations but ﬁxed goals. Note that a single demonstration only demonstrates solving the tasks from one initial condition. Evaluation results across 20 different initial conditions can be seen in Fig. 4. We observe that ROT has an average success rate of 90.1% over 20 evaluation trajectories across all tasks as compared to 36.1% for BC and 14.6% for RDAC. The poor performance of BC can be attributed to distributional mismatch due to accumulation of error in online rollouts and different initial conditions. The poor performance of RDAC can be attributed to slow learning during the initial phase of training. More detailed evaluations of RDAC on simulated environments is present in Sec. 5.4. 6 d r a w e r _ e d o s i p e 1000 800 600 400 200 0 e t a r s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 1.0 e t a r s s e c c u s 0.8 0.6 0.4 0.2 0.0 dmc_cheetah_run dmc_hopper_hop dmc_walker_run 300 200 100 d r a w e r _ e d o s i p e 0 800 600 400 200 d r a w e r _ e d o s i p e 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 fetch_reach fetch_push fetch_pick_and_place e t a r s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 e t a r s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_drawer_close metaworld_hammer metaworld_door_open e t a r s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 e t a r s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 Figure 4: (Top) ROT is evaluated on a set of 14 robotic manipulation tasks. (Bottom) Success rates for each task is computed by running 20 trajectories from varying initial conditions on the robot. BC RDAC ROT (Ours) 5.3 Does soft Q-ﬁltering improve imitation? To understand the importance of soft Q-ﬁltering, we compare ROT against two variants of our proposed regular- ization scheme: (a) A tuned ﬁxed BC regularization weight (ignoring λ(π) in Eq. 3); (b) A carefully designed linear-decay schedule for λ(π), where it varies from 1.0 to 0.0 in the ﬁrst 20k environment steps [16]. As demon- strated in Fig. 5 (and Appendix H.2), ROT is on par and in some cases ex- ceeds the efﬁciency of a hand-tuned decay schedule, while not having to hand-tune its regularization weights. We hypothesize this improvement is primarily due to the better stability of adaptive weighing as seen in the signiﬁcantly smaller standard deviation on the Meta-world tasks. Figure 5: Effect of various BC regularization schemes com- pared with our adaptive soft-Q ﬁltering regularization. Finetune with ﬁxed weight Finetune with ﬁxed ROT (Ours) schedule 5.4 How important is the choice of IRL method in ROT? In ROT, we build on OT-based IRL instead of adversarial IRL. This is because adversarial IRL methods require iterative reward learning, which produces a highly non-stationary reward function for policy optimization. In Fig. 6, we compare ROT with adversarial IRL methods that use our pretraining and adaptive BC regularization technique (RDAC). We ﬁnd that our soft Q-ﬁltering method does improve prior state-of-the-art adversarial IRL (RDAC vs. DAC in Fig. 6). However, our OT-based approach (ROT) is more stable and on average leads to more efﬁcient learning. 5.5 How does ROT compare to standard reward-based RL? We compare the performance of ROT against DrQ-v2 [8], a state-of-the-art algorithm for image-based RL. As opposed to the reward-free setting ROT operates in, DrQ-v2 has access to environments rewards. The results in Fig. 6 show that ROT handily outperforms DrQ-v2. This clearly demonstrates 7 Close a door Hang a hanger Erasing a board Reach Hanging a mug Hanging a tote bag Turn a knob Stacking cups Pressing a switch Peg in a box (Easy) Peg in a box (Med) Peg in a box (Hard) Opening a box Pouring almonds dmc_hopper_hop metaworld_hammer 300 200 100 d r a w e r e d o s p e i 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 1.0 0.8 0.6 0.4 0.2 e t a r s s e c c u s 0.0 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 Expert BC DAC OT DrQ-v2 (RL) RDAC ROT (Ours) Figure 6: Ablation analysis on the choice of base IRL method. We ﬁnd that although adversarial methods beneﬁt from regularized BC, the gains seen are smaller compared to ROT. Here, we also see that ROT can outperform plain RL that requires explicit task-rewards. the usefulness of imitation learning in domains where expert demonstrations are available over reward-based RL. We also compare against a demo-assisted variant of DrQ-v2 agent using the same pretraining and regularization scheme as ROT (refer to Appendix H.3). Interestingly, we ﬁnd that our soft Q-ﬁltering based regularization can accelerate learning of RL with task rewards, which can be seen in the high performance of the demo-assisted variant of DrQ-v2. 6 Related Work Imitation Learning (IL) IL [34] refers to the setting where agents learn from demonstrations without access to environment rewards. IL can be broadly categorized into Behavior Cloning (BC) [1, 21, 35, 36] and Inverse Reinforcement Learning (IRL) [4, 22]. BC solely learns from ofﬂine demonstrations but suffers on out-of-distributions samples [5] whereas IRL focuses on learning a robust reward function through online interactions but suffers from sample inefﬁciency [7]. Deep IRL methods can be further divided into two categories: (1) adversarial learning [37] based methods, and (2) state-matching [38, 39] based methods. GAIL [6] is an adversarial learning based formulation inspired by maximum entropy IRL [40] and GANs [37]. There has been a signiﬁcant body of work built up on GAIL proposing alternative losses [30, 41, 29], and enhancing its sample efﬁciency by porting it to an off-policy setting [7]. There have also been visual extensions of these adversarial learning approaches [42, 43, 44, 11]. However, although adversarial methods produce competent policies, they are inefﬁcient due to the non-stationarity associated with iterative reward inference [11]. Optimal Transport (OT) OT [38, 39] is a tool for comparing probability measures while including the geometry of the space. In the context of IL, OT computes an alignment between a set of agent and expert observations using distance metrics such as Sinkhorn [33], Gromov-Wasserstein [45], GDTW [46], CO-OT [47] and Soft-DTW [48]. For many of these distance measures, there is an associated IL algorithm, with SIL [12] using Sinkhorn, PWIL [13] using greedy Wasserstein, GDTW-IL [46] using GDTW, and GWIL [49] using Gromov-Wasserstein. Recent work from Cohen et al. [11] demonstrates that the Sinkhorn distance [12] produces the most efﬁcient learning among the discussed metrics. They further show that SIL is compatible with high-dimensional visual observations and encoded representations. Inspired by this, ROT adopts the Sinkhorn metric for its OT reward computation, and improves upon SIL through adaptive behavior regularization. Behavior Regularized Control Behavior regularization is a widely used technique in ofﬂine RL [50] where explicit constraints are added to the policy improvement update to avoid bootstrapping on out-of-distribution actions [51, 52, 53, 54, 55, 56]. In an online setting with access to environment rewards, prior work [16, 10] has shown that behavior regularization can be used to boost sample efﬁciency by ﬁnetuning a pretrained policy via online interactions. For instance, Jena et al. [17] demonstrates the effectiveness of behavior regularization to enhance sample efﬁciency in the context of adversarial IL. ROT builds upon this idea by extending to visual observations, OT-based IL, and adaptive regularization, which leads to improved performance (see Appendix H.4). We also note that the idea of using adaptive regularization has been previously explored in RL [24]. However, ROT uses a soft, continuous adaptive scheme, which on initial experiments provided signiﬁcantly faster learning compared to hard assignments. 8 1000 d r a w e r e d o s i p e 800 600 400 200 0 dmc_walker_run fetch_pick_and_place metaworld_hammer e t a r s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 e t a r s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.00 0.25 0.50 0.75 frame 1.00 1.25 1.50 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 7 Conclusion and Limitations In this work, we have proposed a new imitation learning algorithm, ROT, that demonstrates improved performance compared to prior state-of-the-art work on a variety of simulated and robotic domains. However, we recognize a few limitations in this work: (a) Since our OT-based approach aligns agents with demonstrations without task-speciﬁc rewards, it relies on the demonstrator being an ‘expert’. Extending ROT to suboptimal, noisy and multimodal demonstrations would be an exciting problem to tackle. (b) Performing BC pretraining and BC-based regularization requires access to expert actions, which may not be present in some real-world scenarios particularly when learning from humans. Recent work on using inverse models to infer actions given observational data could alleviate this challenge [57]. (c) On robotic tasks such as Peg in box (hard) and Pressing a switch from Fig. 4, we ﬁnd that ROT’s performance drops substantially compared to other tasks. This might be due to the lack of visual features corresponding to the task success. For example, in the ‘Peg’ task, it is visually difﬁcult to discriminate if the peg is in the box or behind the box. Similarly for the ‘Switch’ task, it is difﬁcult to discern if the button was pressed or not. This limitation can be addressed by integrating more sensory modalities such as additional cameras, and tactile sensors in the observation space. Acknowledgments We thank Ben Evans, Anthony Chen, Ulyana Piterbarg and Abitha Thankaraj for valuable feedback and discussions. This work was supported by grants from Honda, Amazon, and ONR awards N00014-21-1-2404 and N00014-21-1-2758. References [1] D. Pomerleau. An autonomous land vehicle in a neural network. Advances in Neural Information Processing Systems, 1, 1998. [2] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, et al. Learning dexterous in-hand manipulation. The Interna- tional Journal of Robotics Research, 39(1):3–20, 2020. [3] P. N. Kolm and G. Ritter. Modern perspectives on reinforcement learning in ﬁnance. Modern Perspectives on Reinforcement Learning in Finance (September 6, 2019). The Journal of Machine Learning in Finance, 1(1), 2020. [4] A. Y. Ng, S. J. Russell, et al. Algorithms for inverse reinforcement learning. In Icml, volume 1, page 2, 2000. [5] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artiﬁ- cial intelligence and statistics, pages 627–635. JMLR Workshop and Conference Proceedings, 2011. [6] J. Ho and S. Ermon. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016. [7] I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson. Discriminator-actor-critic: Addressing sample inefﬁciency and reward bias in adversarial imitation learning. arXiv preprint arXiv:1809.02925, 2018. [8] D. Yarats, R. Fergus, A. Lazaric, and L. Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645, 2021. [9] A. Nair, A. Gupta, M. Dalal, and S. Levine. Awac: Accelerating online reinforcement learning with ofﬂine datasets. arXiv preprint arXiv:2006.09359, 2020. [10] I. Uchendu, T. Xiao, Y. Lu, B. Zhu, M. Yan, J. Simon, M. Bennice, C. Fu, C. Ma, J. Jiao, et al. Jump-start reinforcement learning. arXiv preprint arXiv:2204.02372, 2022. [11] S. Cohen, B. Amos, M. P. Deisenroth, M. Henaff, E. Vinitsky, and D. Yarats. Imitation learning from pixel observations for continuous control, 2022. URL https://openreview.net/ forum?id=JLbXkHkLCG6. 9 [12] G. Papagiannis and Y. Li. arXiv:2008.09167, 2020. Imitation learning with sinkhorn distances. arXiv preprint [13] R. Dadashi, L. Hussenot, M. Geist, and O. Pietquin. Primal wasserstein imitation learning. arXiv preprint arXiv:2006.04678, 2020. [14] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [15] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. In International conference on machine learning, pages 387–395. PMLR, 2014. [16] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. arXiv preprint arXiv:1709.10087, 2017. [17] R. Jena, C. Liu, and K. Sycara. Augmenting gail with bc for sample efﬁcient imitation learning. arXiv preprint arXiv:2001.07798, 2020. [18] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. [19] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [20] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning, pages 1094–1100. PMLR, 2020. [21] F. Torabi, G. Warnell, and P. Stone. Recent advances in imitation learning from observation. arXiv preprint arXiv:1905.13566, 2019. [22] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-ﬁrst international conference on Machine learning, page 1, 2004. [23] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. [24] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In 2018 IEEE international conference on robotics and automation (ICRA), pages 6292–6299. IEEE, 2018. [25] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026–5033. IEEE, 2012. [26] M. Plappert, M. Andrychowicz, A. Ray, B. McGrew, B. Baker, G. Powell, J. Schneider, J. Tobin, M. Chociej, P. Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018. [27] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning (CoRL), 2019. URL https://arxiv.org/abs/1910.10897. [28] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017. [29] F. Torabi, G. Warnell, and P. Stone. Generative adversarial imitation from observation. arXiv preprint arXiv:1807.06158, 2018. [30] J. Fu, K. Luo, and S. Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017. 10 [31] S. K. S. Ghasemipour, R. Zemel, and S. Gu. A divergence minimization perspective on imitation learning methods. In Conference on Robot Learning, pages 1259–1277. PMLR, 2020. [32] R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Paciﬁc Journal of Mathematics, 21(2):343–348, 1967. [33] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013. [34] A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne. Imitation learning: A survey of learning methods. ACM Computing Surveys (CSUR), 50(2):1–35, 2017. [35] S. P. Arunachalam, S. Silwal, B. Evans, and L. Pinto. Dexterous imitation made easy: A learning- based framework for efﬁcient dexterous manipulation. arXiv preprint arXiv:2203.13251, 2022. [36] J. Pari, N. Muhammad, S. P. Arunachalam, and L. Pinto. The surprising effectiveness of representation learning for visual imitation. arXiv preprint arXiv:2112.01511, 2021. [37] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [38] C. Villani. Optimal transport: old and new, volume 338. Springer, 2009. [39] G. Peyr´e, M. Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends® in Machine Learning, 11(5-6):355–607, 2019. [40] B. D. Ziebart, A. L. Maas, J. A. Bagnell, A. K. Dey, et al. Maximum entropy inverse reinforce- ment learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008. [41] H. Xiao, M. Herman, J. Wagner, S. Ziesche, J. Etesami, and T. H. Linh. Wasserstein adversarial imitation learning. arXiv preprint arXiv:1906.08113, 2019. [42] E. Cetin and O. Celiktutan. Domain-robust visual imitation learning with mutual information constraints. arXiv preprint arXiv:2103.05079, 2021. [43] S. Toyer, R. Shah, A. Critch, and S. Russell. The magical benchmark for robust imitation. Advances in Neural Information Processing Systems, 33:18284–18295, 2020. [44] R. Rafailov, T. Yu, A. Rajeswaran, and C. Finn. Visual adversarial imitation learning using variational models. Advances in Neural Information Processing Systems, 34, 2021. [45] G. Peyr´e, M. Cuturi, and J. Solomon. Gromov-wasserstein averaging of kernel and distance matrices. In International Conference on Machine Learning, pages 2664–2672. PMLR, 2016. [46] S. Cohen, G. Luise, A. Terenin, B. Amos, and M. Deisenroth. Aligning time series on incom- parable spaces. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1036–1044. PMLR, 2021. [47] I. Redko, T. Vayer, R. Flamary, and N. Courty. Co-optimal transport. arXiv preprint arXiv:2002.03731, 2020. [48] M. Cuturi and M. Blondel. Soft-dtw: a differentiable loss function for time-series. In Interna- tional conference on machine learning, pages 894–903. PMLR, 2017. [49] A. Fickinger, S. Cohen, S. Russell, and B. Amos. Cross-domain imitation learning via optimal transport. arXiv preprint arXiv:2110.03684, 2021. [50] S. Levine, A. Kumar, G. Tucker, and J. Fu. Ofﬂine reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. [51] S. Fujimoto and S. S. Gu. A minimalist approach to ofﬂine reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021. [52] Y. Wu, G. Tucker, and O. Nachum. Behavior regularized ofﬂine reinforcement learning. arXiv preprint arXiv:1911.11361, 2019. 11 [53] A. Ajay, A. Kumar, P. Agrawal, S. Levine, and O. Nachum. {OPAL}: Ofﬂine primitive discovery In International Conference on Learning for accelerating ofﬂine reinforcement learning. Representations, 2021. URL https://openreview.net/forum?id=V69LGwJ0lIN. [54] A. Kumar, J. Fu, M. Soh, G. Tucker, and S. Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019. [55] N. Y. Siegel, J. T. Springenberg, F. Berkenkamp, A. Abdolmaleki, M. Neunert, T. Lampe, R. Hafner, N. Heess, and M. Riedmiller. Keep doing what worked: Behavioral modelling priors for ofﬂine reinforcement learning. arXiv preprint arXiv:2002.08396, 2020. [56] S. Fujimoto, D. Meger, and D. Precup. Off-policy deep reinforcement learning without explo- ration. In International Conference on Machine Learning, pages 2052–2062. PMLR, 2019. [57] I. Radosavovic, X. Wang, L. Pinto, and J. Malik. State-only imitation learning for dexterous manipulation. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 7865–7871. IEEE, 2020. [58] R. Bellman. A markovian decision process. Journal of mathematics and mechanics, pages 679–684, 1957. [59] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018. [60] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep rein- forcement learning. nature, 518(7540):529–533, 2015. [61] A. Zhan, P. Zhao, L. Pinto, P. Abbeel, and M. Laskin. A framework for efﬁcient robotic manipulation. arXiv preprint arXiv:2012.07975, 2020. [62] S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and L. Pinto. Visual imitation made easy. arXiv preprint arXiv:2008.04899, 2020. [63] P. A. Knight. The sinkhorn–knopp algorithm: convergence and applications. SIAM Journal on Matrix Analysis and Applications, 30(1):261–275, 2008. 12 A Background Reinforcement Learning (RL) We study RL as a discounted inﬁnite-horizon Markov Decision Process (MDP) [58, 59]. For pixel observations, the agent’s state is approximated as a stack of consecutive RGB frames [60]. The MDP is of the form (O, A, P, R, γ, d0) where O is the observation space, A is the action space, P : O × A → ∆(O) is the transition function that deﬁnes the probability distribution over the next state given the current state and action, R : O × A → R is the reward function, γ is the discount factor and d0 is the initial state distribution. The goal is to ﬁnd a policy π : O → ∆(A) that maximizes the expected discount sum of rewards Eπ[Σ∞ t=0γtR(ot, at)], where o0 ∼ d0, at ∼ π(ot) and ot+1 ∼ P (.|ot, at). Imitation Learning (IL) The goal of imitation learning is to learn a behavior policy πb given access to either the expert policy πe or trajectories derived from the expert policy T e. While there are a multitude of settings with differing levels of access to the expert [21], this work operates in the setting where the agent only has access to observation-based trajectories, i.e. T e ≡ {(ot, at)T n=0. Here N and T denotes the number of trajectory rollouts and episode timesteps respectively. We choose this speciﬁc setting since obtaining observations and actions from expert or near-expert demonstrators is feasible in real-world settings [61, 62] and falls in line with recent work in this area [13, 6, 7]. t=0}N Inverse Reinforcement Learning (IRL) IRL [4, 22] tackles the IL problem by inferring the reward function re based on expert trajectories T e. Then given the inferred reward re, policy optimization is used to derive the behavior policy πb. Prominent algorithms in IRL [7, 6] requires alternating the inference of reward and optimization of policy in an iterative manner, which is practical for restricted model classes [22]. For compatibility with more expressive deep networks, techniques such as adversarial learning [6, 7] or optimal-transport [12, 13, 11] are needed. Adversarial learning based approaches tackle this problem by learning a discriminator that models the gap between the expert trajectories T e and behavior trajectories T b. The behavior policy πb is then optimized to minimize this gap through gap-minimizing rewards re. Such a training procedure is prone to instabilities since re is updated at every iteration and is hence non-stationary for the optimization of πb. Optimal Transport for Imitation Learning (OT) To alleviate the non-stationary reward problem with adversarial IRL frameworks, a new line of OT-based approaches have been recently proposed [12, 13, 11]. Intuitively, the closeness between expert trajectories T e and behavior trajectories T b can be computed by measuring the optimal transport of probability mass from T b → T e. During policy learning, the policy πφ encompasses a feature preprocessor fφ which transforms observations into informative state representations. Some examples of a preprocessor function fφ are an identity function, a mean-variance scaling function and a parametric neural network. In this work, we use a parametric neural network as fφ. Given a cost function c : O × O → R deﬁned in the preprocessor’s output space and an OT objective g, the optimal alignment between an expert trajectory oe and a behavior trajectory ob can be computed as µ∗ ∈ arg min µ∈M g(µ, fφ(ob), fφ(oe), c) (5) where M = {µ ∈ RT ×T : µ1 = µT 1 = 1 T 1} is the set of coupling matrices and the cost c can be the Euclidean or Cosine distance. In this work, inspired by [11], we use the entropic Wasserstein distance with cosine cost as our OT metric, which is given by the equation g(µ, fφ(ob), fφ(oe), c) = W 2(fφ(ob), fφ(oe)) T (cid:88) = t,t(cid:48)=1 Ct,t(cid:48) µt,t(cid:48) 13 (6) where the cost matrix Ct,t(cid:48) = c(fφ(ob), fφ(oe)). Using Eq. 6 and the optimal alignment µ∗ obtained by optimizing Eq. 5, a reward signal can be computed for each observation using the equation rOT (ob t ) = − T (cid:88) t(cid:48)=1 Ct,t(cid:48) µ∗ t,t(cid:48) (7) Intuitively, maximizing this reward encourages the imitating agent to produce trajectories that closely match demonstrated trajectories. Since solving Eq. 5 is computationally expensive, approximate solutions such as the Sinkhorn algorithm [63, 12] are used instead. B Issue with Fine-tuning Actor-Critic Frameworks In this paper, we use n-step DDPG proposed by Yarats et al. [8] as our RL optimizer for actor- critic based reward maximization. DDPG [23] concurrently learns a deterministic policy πφ using deterministic policy gradients (DPG) [15] and a Q-function Qθ by minimizing a n-step Bellman residual (for n-step DDPG). For a parameterized actor network πφ(s) and a critic function Qθ(s, a), the deterministic policy gradients (DPG) for updating the actor weights is given by ∇φJ ≈ Est∼ρβ = Est∼ρβ (cid:104) (cid:104) ∇φ Qθ(s, a)|s=st,a=πφ(st) (cid:105) ∇a Qθ(s, a)|s=st,a=πφ(st) ∇φ πφ(s)|s=st (cid:105) (8) Here, ρβ refers to the state visitation distribution of the data present in the replay buffer at time t. From Eq. 8, it is clear that the policy gradients in this framework depend on the gradients with respect to the critic value. Hence, as mentioned in [9, 10], naively initializing the actor with a pretrained policy while using a randomly initialized critic results in the untrained critic providing an exceedingly poor signal to the actor network during training. As a result, the actor performance drops immediately and the good behavior of the informed initialization of the policy gets forgotten. In this paper, we propose an adaptive regularization scheme that permits ﬁnetuning a pretrained actor policy in an actor-critic framework. As opposed to Rajeswaran et al. [16], Jena et al. [17] which employ on-policy learning, our method is off-policy and aims to leverage the sample efﬁcient characteristic of off-policy learning as compared to on-policy learning [7]. C Algorithmic Details C.1 Implementation Algorithm 1 describes our proposed algorithm, Regularized Optimal Transport (ROT), for sample efﬁcient imitation learning for continuous control tasks. Further implementation details are as follows: Algorithm and training procedure Our model consists of 3 primary neural networks - the encoder, the actor and the critic. During the BC pretraining phase, the encoder and the actor are trained using a mean squared error (MSE) on the expert demonstrations. Next, for ﬁnetuning, weights of the pretrained encoder and actor are loaded from memory and the critic is initialized randomly. We observed that the performance of the algorithm is not very sensitive to the value of α and we set it to 0.03 for all experiments in this paper. A copy of the pretrained encoder and actor are stored with ﬁxed weights to be used for computing λ(π) for soft Q-ﬁltering. Actor-critic based reward maximization We use a recent n-step DDPG proposed by Yarats et al. [8] as our RL backbone. The deterministic actor is trained using deterministic policy gradients (DPG) [15] given by Eq. 8. The critic is trained using clipped double Q-learning similar to Yarats et al. [8] in order to reduce the overestimation bias in the target value. This is done using two Q-functions, Qθ1 and Qθ2. The critic loss for each critic is given by the equation Lθk = E(s,a)∼Dβ (cid:2)(Qθk (s, a) − y)2(cid:3) ∀ k ∈ {1, 2} (9) 14 Algorithm 1 ROT: Regularized Optimal Transport Require: Expert Demonstrations T e ≡ {(ot, at)T Pretrained policy πBC Replay buffer D, Training steps T , Episode Length L Task environment env Parametric networks for RL backbone (e.g., the encoder, policy and critic function for DrQ-v2) A discriminator D for adversarial baselines t=0}N n=0 Algorithm: πROT ← πBC for each timestep t = 1...T do if done then r1:L = rewarderOT (episode) Update episode with r1:L and add (ot, at, ot+1, rt) to D ot = env.reset(), done = False, episode = [ ] (cid:46) Initialize with pretrained policy (cid:46) OT-based reward computation end if at = πROT (ot) ot+1, done = env.step(at) episode.append([ot, at, ot+1]) Update backbone-speciﬁc networks and reward-speciﬁc networks using D end for where Dβ is the replay buffer for online rollouts and y is the target value for n-step DDPG given by y = n−1 (cid:88) i=0 γirt+i + γn min k=1,2 Q¯θk (st+n, at+n) (10) Here, γ is the discount factor, r is the reward obtained using OT-based reward computation and ¯θ1, ¯θ2 are the slow moving weights of target Q-networks. Target feature processor to stabilize OT rewards The OT rewards are computed on the output of the feature processor fφ which is initialized with a parametric neural network. Hence, as the weights of fφ change during training, the rewards become non-stationary resulting in unstable training. In order to increase the stability of training, the OT rewards are computed using a target feature processor fφ(cid:48) [11] which is updated with the weights of fφ every Tupdate environment steps. For state-based observations, fφ corresponds to a ’trunk’ network which is a single layer neural network. For pixel-based observations, fφ includes DrQ-v2’s encoder followed by the ’trunk’ network. C.2 Hyperparameters The complete list of hyperparameters is provided in Table 1. Similar to Yarats et al. [8], there is a slight deviation from the given setting for the Walker Stand/Walk/Run task from the DeepMind Control suite where we use a mini-batch size of 512 and a n-step return of 1. D Environments Table 2 lists the different tasks that we experiment with from the DeepMind Control suite [18, 25], OpenAI Robotics suite [26] and the Meta-world suite [27] along with the number of training steps and the number of demonstrations used. For the tasks in the OpenAI Robotics suite, we ﬁx the goal while keeping the initial state randomized. No modiﬁcations are made in case of the DeepMind Control suite and the Meta-world suite. The episode length for all tasks in DeepMind Control is 1000 steps, for OpenAI Robotics is 50 steps and Meta-world is 125 steps (except bin picking which runs for 175 steps). 15 Method Common Parameter Replay buffer size Learning rate Discount γ n-step returns Action repeat Seed frames Mini-batch size Agent update frequency Critic soft-update rate Feature dim Hidden dim Optimizer ROT Exploration steps DDPG exploration schedule Target feature processor update frequency(steps) Reward scale factor Fixed weight α Value 150000 1e−4 0.99 3 2 12000 256 2 0.01 50 1024 Adam 0 0.1 20000 10 0.03 Linear decay schedule for λ(π) linear(1,0.1,20000) OT Exploration steps 2000 DDPG exploration schedule linear(1,0.1,500000) DAC Target feature processor update frequency(steps) Reward scale factor Exploration steps DDPG exploration schedule Gradient penalty coefﬁcient Table 1: List of hyperparameters. 20000 10 2000 linear(1,0.1,500000) 10 E Demonstrations For DeepMind Control tasks, we train expert policies using pixel-based DrQ-v2 [8] and collect 10 demonstrations for each task using this expert policy. The expert policy is trained using a stack of 3 consecutive RGB frames of size 84 × 84 with random crop augmentation. Each action in the environment is repeated 2 times. For OpenAI Robotics tasks, we train a state-based DrQ-v2 with hindsight experience replay [28] and collect 50 demonstrations for each task. The state representation comprises the observation from the environment appended with the desired goal location. For this, we did not do frame stacking and action repeat was set to 2. For Meta-World tasks, we use a single expert demonstration obtained using the task-speciﬁc hard-coded policies provided in their open-source implementation [27]. 16 Suite Tasks DeepMind Control Acrobot Swingup Allowed Steps 2 × 106 # Demonstrations 10 Cartpole Swingup Cheetah Run Finger Spin Hopper Stand Hopper Hop Quadruped Run Walker Stand Walker Walk Walker Run OpenAI Robotics Fetch Reach 1.5 × 106 50 1 1 Fetch Push Fetch Pick and Place Meta-World Hammer 1 × 106 xArm Robot 6 × 103 Drawer Close Door Open Bin Picking Button Press Topdown Door Unlock. Close Door Hang Hanger Erase Board Reach Hang Mug Hang Bag Turn Knob Stack Cups Press Switch Peg (Easy) Peg (Medium) Peg (Hard) Open Box Pour Table 2: List of tasks used for evaluation. F Robot Tasks In this section, we describe the suite of manipulation experiments carried out on a real robot in this paper. 17 Figure 7: Examples of different initializations for the real robot tasks. 18 r o o D e s o l C r e g n a H g n a H d r a o B e s a r E h c a e R g u M g n a H g a B g n a H b o n K n r u T s p u C k c a t S h c t i w S s s e r P ) y s a E ( g e P ) d e M ( g e P ) d r a H ( g e P x o B n e p O r u o P Figure 8: Example trajectories for selected real robot tasks. (a) Door Close: Here, the robot arm is supposed to close an open door by pushing it to the target. (b) Hang Hanger: While holding a hanger between the grippers, the robot arm is initialized at random position and is tasked with putting the hanger at a goal region on a closet rod. (c) Erase Board: While holding a board duster between the grippers, the robot arm is tasks with erasing marking drawn on the board while getting initialized from random positions. (d) Reach: The robot arm is required to reach a speciﬁc goal after being initialized at a random position. (e) Hang Mug: While holding a mug between the grippers, the robot arm is initialized at random position and is tasked with hanging the mug on a speciﬁc hook. (f) Hang Bag: While holding a tote between the grippers, the robot arm is initialized at random position and is tasked with hanging the tote bag on a speciﬁc hook. (g) Turn Knob: The robot arm is tasked with rotating a knob placed on the table by a certain angle after being initialized at a random position. We consider a 90 degree rotation as success. (h) Cup Stack: While holding a cup between the gripper, the robot arm is required to stack it into another cup placed on the table. (i) Press Switch: With the gripper kept closed, the robot arm is required to press a switch (with an LED light) placed on the table. (j) Peg (Easy, Medium, Hard): The robot arm is supposed to insert a peg, hanging by a string, into a bucket placed on the table. This task has 3 variants - Easy, Medium, Hard - with the size of the bucket decreasing from Easy to Hard. (k) Box Open: In this task, the robot arm is supposed to open the lid of a box placed on the table by lifting a handle provided in the front of the box. (l) Pour: Given a cup with some item place inside (in our case, almonds), the robot arm is supposed to move towards a cup place on the table and pour the item into the cup. Evaluation procedure For each task, we obtained a set of 20 random initializations and evaluate all of the methods (BC, RDAC and ROT) over 20 trajectories from the same set of initializations. These initializations are different for each task based on the limits of the observation space for the task. G Baselines Throughout the paper, we compare ROT with several prominent imitation learning and reinforcement learning methods. Here, we give a brief description of each of the baseline models that have been used. (a) Expert: For each task, the expert refers to the expert policy used to generate the demonstrations for the task (described in Appendix E). 19 d r a o B e s a r E g a B g n a H b o n K n r u T ) d r a H ( g e P x o B n e p O (b) Behavior Cloning (BC): This refers to the behavior cloned policy trained on expert demonstra- tions. (c) Adversarial IRL (DAC): Discriminator Actor Critic [7] is a state-of-the-art adversarial imi- tation learning method [6, 29, 7]. Since DAC outperforms prior work such as GAIL[6] and AIRL[30], it serves as our primary adversarial imitation baseline. (d) State-matching IRL (OT): Sinkhorn Imitation Learning [12, 13] is a state-of-the-art state- matching imitation learning method [31] that approximates OT matching through the Sinkhorn Knopp algorithm. Since ROT is derived from similar OT-based foundations, we use SIL as our primary state-matching imitation baseline. (e) RDAC: This is the same as ROT, but instead of using state-matching IRL (OT), adversarial IRL (DAC) is used. (f) Finetune with ﬁxed weight: This is similar to ROT where instead of using a time-varying adaptive weight λ(i), only the ﬁxed weight λ0 is used. λ0 is set to a ﬁxed value of 0.03. (g) Finetune with ﬁxed schedule: This is similar to ROT that uses both the ﬁxed weight λ0 and the time-varying adaptive weight λ1(i). However, instead of using Soft Q-ﬁltering to compute λ1(i), a hand-coded linear decay schedule is used. (h) DrQ-v2 (RL): DrQ-v2 [8] is a state-of-the-art algorithm for pixel-based RL. DrQ-v2 is assumed to have access to environment rewards as opposed to ROT which computes the reward using OT-based techniques. (i) Demo-DrQ-v2: This refers to DrQ-v2 but with access to both environment rewards and expert demonstrations. The model is initialized with a pretrained BC policy followed by RL ﬁnetuning with an adaptive regularization scheme like ROT. During RL ﬁnetuning, this baseline has access to environment rewards. (j) BC+OT: This is the same as the OT baseline but the policy is initialized with a pretrained BC policy. No adaptive regularization scheme is used while ﬁnetuning the pretrained policy. (k) OT+BC Reg.: This is the same as the OT baseline with randomly initialized networks but during training, the adaptive regularization scheme is added to the objective function. H Additional Experimental Results H.1 How efﬁcient is ROT for imitation learning? In addition to the results provided in Sec. 5.1, Fig. 9 and Fig. 10 shows the performance of ROT for pixel-based imitation on 10 tasks from the DeepMind Control suite, 3 tasks from the OpenAI Robotics suite and 7 tasks from the Meta-world suite. On all but one task, ROT is signiﬁcantly more sample efﬁcient than prior work. Finally, the improvements from ROT hold on state-based observations as well(see Fig. 11). Table 3 provides a comparison between the factor of speedup of ROT to reach 90% of expert performance compared to prior state-of-the-art [7, 11] methods. H.2 Does soft Q-ﬁltering improve imitation? Extending the results shown in Fig. 5, we provide training curves from representative tasks in each suite in Fig. 12. We observe that our adaptive soft-Q ﬁltering regularization is more stable compared to prior hand-tuned regularization schemes. ROT is on par and in some cases exceeds the efﬁciency of a hand-tuned decay schedule, while not having to hand-tune its regularization weights. H.3 How does ROT compare to standard reward-based RL? Extending the results shown in Fig. 6, we provide training curves from representative tasks in each suite in Fig. 13, thus showing that ROT can outperform standard RL that requires explicit task- reward. We also show that this RL method combined with our regularization scheme (represented by Demo-DrQ-v2 in Fig. 13 provides strong results. 20 Expert BC OT DAC ROT (Ours) Figure 9: Pixel-based continuous control learning on 10 DMC environments. Shaded region represents ±1 standard deviation across 5 seeds. We notice that ROT is signiﬁcantly more sample efﬁcient compared to prior work. H.4 How important are the design choices in ROT? Importance of pretraining and regularizing the IRL policy Fig. 14 compares the following variants of ROT on set of pixel-based tasks: (a) Training the IRL policy from scratch (OT); (b) Finetuning a pretrained BC policy without BC regularization (BC+OT); (c) Training the IRL policy from scratch with BC regularization (OT+BC Reg.). We observe that pretraining the IRL policy (BC+OT) does not provide a signiﬁcant difference without regularization. This can be attributed to the ‘forgetting behavior’ of pre-trained policies, studied in Nair et al. [9]. Interestingly, we see that even without BC pretraining, keeping the policy close to a behavior distribution (OT+BC Reg.) can yield improvements in efﬁciency over vanilla training from scratch. Our key takeaway from these experiments is that both pretraining and BC regularization are required to obtain sample-efﬁcient imitation learning. Choice of IRL method In ROT, we build on OT-based IRL instead of adversarial IRL. This is because adversarial IRL methods require iterative reward learning, which produces a highly non- stationary reward function for policy optimization. In Fig. 15, we compare ROT with adversarial 21 400 300 200 100 d r a w e r _ e d o s i p e 0 d r a w e r _ e d o s i p e 1000 800 600 400 200 0 d r a w e r _ e d o s i p e 1000 800 600 400 200 dmc_acrobot_swingup dmc_cartpole_swingup dmc_finger_spin d r a w e r _ e d o s i p e 1000 800 600 400 200 0 d r a w e r _ e d o s i p e 1200 1000 800 600 400 200 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 dmc_cheetah_run dmc_hopper_stand dmc_hopper_hop d r a w e r _ e d o s i p e 1000 800 600 400 200 0 200 300 200 100 d r a w e r _ e d o s i p e 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 dmc_walker_stand dmc_walker_walk dmc_walker_run d r a w e r _ e d o s i p e 1000 800 600 400 200 0 800 600 400 200 d r a w e r _ e d o s i p e 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 dmc_quadruped_run 600 400 200 d r a w e r _ e d o s i p e 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 Expert BC OT DAC ROT (Ours) Figure 10: Pixel-based continuous control learning on 3 OpenAI Gym Robotics and 7 Meta-World tasks. Shaded region represents ±1 standard deviation across 5 seeds. We notice that ROT is signiﬁcantly more sample efﬁcient compared to prior work. IRL methods that use our pretraining and adaptive BC regularization technique (RDAC). We ﬁnd that our soft Q-ﬁltering method does improve prior state-of-the-art adversarial IRL (RDAC vs. DAC in Fig. 15). However, our OT-based approach (ROT) is more stable and on average leads to more efﬁcient learning. 22 e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 1.25 1.00 0.75 0.50 0.25 0.00 0.25 1.00 0.75 0.50 0.25 0.00 0.25 fetch_reach fetch_push fetch_pick_and_place e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_hammer metaworld_drawer_close metaworld_drawer_open e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 metaworld_door_open metaworld_bin_picking e g a t n e c r e p _ s s e c c u s 1.00 0.75 0.50 0.25 0.00 0.25 e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 metaworld_button_press_topdown 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 metaworld_door_unlock e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 Expert BC OT DAC ROT (Ours) Figure 11: State-based continuous control learning on DMC and Meta-World tasks. We notice that ROT is signiﬁcantly more sample efﬁcient compared to prior work. 23 d r a w e r _ e d o s i p e 500 400 300 200 100 0 d r a w e r _ e d o s i p e 1000 800 600 400 200 0 200 d r a w e r _ e d o s i p e 1000 800 600 400 200 0 e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 dmc_acrobot_swingup dmc_cartpole_swingup dmc_finger_spin d r a w e r _ e d o s i p e 1000 800 600 400 200 0 d r a w e r _ e d o s i p e 1000 800 600 400 200 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 dmc_hopper_stand dmc_hopper_hop 300 200 100 d r a w e r _ e d o s i p e 0 dmc_walker_stand d r a w e r _ e d o s i p e 1200 1000 800 600 400 200 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 dmc_walker_walk dmc_walker_run dmc_quadruped_run d r a w e r _ e d o s i p e 800 600 400 200 0 600 400 200 d r a w e r _ e d o s i p e 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 metaworld_hammer metaworld_drawer_close metaworld_door_open e g a t n e c r e p _ s s e c c u s 1.00 0.75 0.50 0.25 0.00 e g a t n e c r e p _ s s e c c u s 1.00 0.75 0.50 0.25 0.00 0.25 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 metaworld_drawer_open metaworld_button_press_topdown metaworld_door_unlock e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 Suite Tasks ROT 2nd Best Model Speedup Factor DeepMind Control Acrobot Swingup Cartpole Swingup Finger Spin Cheetah Run Hopper Stand Hopper Hop Walker Stand Walker Walk Walker Run Quadruped Run OpenAI Robotics Fetch Reach 200k 100k 20k 400k 60k. 200k 80k 200k 320k 400k 300k 600k (OT) 350k (OT) 700k (OT) 2M (DAC) 750k (OT) >2M (DAC) 400k (DAC) 750k (DAC) >2M (OT) >2M (DAC) 1.1M (DAC) Fetch Push 1.1M 600k (DAC) Fetch Pick and Place Meta-World Hammer Drawer Close Drawer Open Door Open Bin Picking 750k 200k 20k >1M 400k 700k >1.5M (OT) >1M (DAC) >1M (OT) >1M (OT) >1M (OT) >1M (OT) Button Press Topdown >1M >1M (OT) Door Unlock 1M >1M (OT) 3 3.5 35 5 12.5 10 5 3.75 6.25 5 3.67 0.54 2 5 50 1 2.5 1.43 1 1 Table 3: Task-wise comparison between environment steps required to reach 90% of expert perfor- mance for pixel-based ROT compared to the strongest baseline for each task. 24 Expert BC Finetune with ﬁxed weight Finetune with ﬁxed schedule ROT (Ours) Figure 12: Pixel-based ablation analysis on the effect of varying BC regularization schemes. We observe that our adaptive soft-Q ﬁltering regularization is more stable compared to prior hand-tuned regularization schemes. 25 d r a w e r _ e d o s p e i 800 600 400 200 0 e g a t n e c r e p _ s s e c c u s 1.2 1.0 0.8 0.6 0.4 0.2 0.0 e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 dmc_cheetah_run dmc_hopper_hop dmc_quadruped_run 300 200 100 d r a w e r _ e d o s p e i 0 d r a w e r _ e d o s p e i 600 500 400 300 200 100 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 fetch_reach fetch_push fetch_pick_and_place e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.00 0.25 0.50 0.75 frame 1.00 1.25 1.50 1e6 0.00 0.25 0.50 0.75 frame 1.00 1.25 1.50 1e6 0.00 0.25 0.50 0.75 frame 1.00 1.25 1.50 1e6 metaworld_door_open metaworld_hammer metaworld_drawer_close e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 Expert BC OT DrQ-v2(RL) Demo-DrQ-v2 ROT (Ours) Figure 13: Pixel-based ablation analysis on the performance comparison of ROT against DrQ-v2, a reward-based RL method. Here we see that ROT can outperform plain RL that requires explicit task-reward. However, we also observe that this RL method combined with our regularization scheme provides strong results. 26 d r a w e r _ e d o s i p e 1000 800 600 400 200 0 e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 1.25 1.00 0.75 0.50 0.25 0.00 0.25 dmc_finger_spin dmc_cheetah_run dmc_walker_run d r a w e r _ e d o s i p e 1000 800 600 400 200 0 d r a w e r _ e d o s i p e 1000 800 600 400 200 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 fetch_reach fetch_push fetch_pick_and_place 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_door_open e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_bin_picking e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 1.00 0.75 0.50 0.25 0.00 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_hammer 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 BC OT BC+OT OT+BC Reg. ROT (Ours) Figure 14: Pixel-based ablation analysis on the importance of pretraining and regularizing the IRL policy. The key takeaway from these experiments is that both pretraining and BC regularization are required to obtain sample-efﬁcient imitation learning. 27 1200 1000 800 600 400 200 0 d r a w e r _ e d o s i p e e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 1.00 0.75 0.50 0.25 0.00 0.25 dmc_finger_spin dmc_cheetah_run dmc_walker_run 800 600 400 200 d r a w e r _ e d o s i p e 0 800 d r a w e r _ e d o s i p e 600 400 200 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 fetch_reach fetch_push fetch_pick_and_place e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_door_open metaworld_hammer e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_button_press_topdown 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 Expert BC DAC OT RDAC ROT (Ours) Figure 15: Pixel-based ablation analysis on the choice of base IRL method. We ﬁnd that although adversarial methods beneﬁt from regularized BC, the gains seen are smaller compared to ROT. 28 d r a w e r _ e d o s i p e 1250 1000 750 500 250 0 250 e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 1.00 0.75 0.50 0.25 0.00 0.25 dmc_finger_spin dmc_cheetah_run d r a w e r _ e d o s i p e 1000 800 600 400 200 0 dmc_walker_run 1000 d r a w e r _ e d o s i p e 800 600 400 200 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 fetch_reach 1.2 1.0 0.8 0.6 0.4 0.2 0.0 e g a t n e c r e p _ s s e c c u s fetch_push fetch_pick_and_place e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_door_open metaworld_hammer metaworld_button_press_topdown e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6",1
"Personalized Showcases: Generating Multi-Modal Explanations for
  Recommendations","[{'href': 'http://arxiv.org/abs/2207.00422v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.00422v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-30 01:43:58,,"Denoised MDPs: Learning World Models Better Than the World Itself Tongzhou Wang 1 Simon S. Du 2 Antonio Torralba 1 Phillip Isola 1 Amy Zhang 3 4 Yuandong Tian 4 2 2 0 2 l u J 1 ] G L . s c [ 2 v 7 7 4 5 1 . 6 0 2 2 : v i X r a Abstract The ability to separate signal from noise, and reason with clean abstractions, is critical to in- telligence. With this ability, humans can efﬁ- ciently perform real world tasks without consider- ing all possible nuisance factors. How can artiﬁ- cial agents do the same? What kind of information can agents safely discard as noises? In this work, we categorize information out in the wild into four types based on controllability and relation with reward, and formulate useful information as that which is both controllable and reward-relevant. This framework clariﬁes the kinds information removed by various prior work on representation learning in reinforcement learning (RL), and leads to our proposed approach of learning a Denoised MDP that explicitly factors out certain noise dis- tractors. Extensive experiments on variants of DeepMind Control Suite and RoboDesk demon- strate superior performance of our denoised world model over using raw observations alone, and over prior works, across policy optimization con- trol tasks as well as the non-control task of joint position regression. ssnl.github.io/denoised_mdp Project Page: Code: github.com/facebookresearch/denoised_mdp 1. Introduction The real world provides us a plethora of information, from microscopic physical interactions to abstracted semantic signals such as the latest COVID-19 news. Fortunately, processing each and every signal is unnecessary (and also impossible). In fact, any particular reasoning or decision often only relies on a small portion of information. Imagine waking up and wanting to embrace some sunlight. As you open the curtain, a nearby resting bird is scared 1MIT CSAIL 2University of Washington 3UC Berkeley 4Meta AI. Correspondence to: Tongzhou Wang <tongzhou@mit.edu>. Work done while Tongzhou Wang was an intern at Meta AI. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). Figure 1: Illustrative example: (a) Four distinct kinds of infor- mation in the scenario described in Section 1, where the person desires to increase the amount of sunlight let into the room. Their opening of the curtain scares away the bird. (b) A denoised world model only includes a small subset of all information. away and you are pleasantly met with a beautiful sunny day. Far away, a jet plane is slowly ﬂying across the sky. This may seem a simple activity, but in fact highlights four distinct types of information (see Figure 1), with respect to the goal of letting in as much sunlight as possible: • Controllable and reward-relevant: curtain, inﬂuenced by actions and affecting incoming sunlight; • Controllable and reward-irrelevant: bird, inﬂuenced by actions but not affecting sunlight; • Uncontrollable and reward-relevant: weather, inde- pendent with actions but affecting sunlight; • Uncontrollable and reward-irrelevant: plane, indepen- dent with both actions and the sunlight. Our optimal actions towards the goal, however, only in fact depend on information that is controllable and reward- Reward- Relevant Reward- Irrelevant Uncontrollable Controllable (a) GOAL: Letting in as much sunlight as possible. Denoise (b) Optimal control only relies on information that is both controllable and reward-relevant. Good world models should ignore other factors as noisy distractors. Denoised MDPs relevant, and the three other kinds of information are merely noise distractors. Indeed, no matter how much natural sunlight there is outside, or how the plane and the bird move, the best plan is always to open up the curtain. When performing a particular task, we humans barely think about the other three types of information, and usually only plan on how our actions affect information that is control- lable and reward-relevant. Our mental model is an ab- stract and condensed version of the real world that is actually better suited for the task. The notion of better model/data is ubiquitous in data science and machine learning. Algorithms rarely perform well on raw noisy real data. The common approach is to perform data cleaning and feature engineering, where we manually select the useful signals based on prior knowledge and/or heuristics. Years of research have identiﬁed ways to extract good features for computer vision (Lowe, 1999; Donahue et al., 2014), natural language processing (Elman, 1990; Mikolov et al., 2013), reinforcement learning (RL) (Ma- hadevan & Maggioni, 2007; Bellemare et al., 2019), etc. Similarly, system identiﬁcation aligns real observation with a predeﬁned set of abstract signals/states. Yet for tasks in the wild (in the general form of (partially observable) Markov Decision Processes), there can be very little prior knowledge of the optimal set of signals. In this work, we ask: can we infer and extract these signals automatically, in the form of a learned world model? The general idea of a mental world model have long been un- der active research in philosophy and social science (Craik, 1952; Dennett, 1975), cognitive science, where an intuitive physics model is hypothesized to be core in our planning capabilities (Spelke & Kinzler, 2007), and in reinforcement learning, where various methods investigate state abstrac- tions for faster and better learning (Sutton, 1991; 1981). In this work, we explore this idea within the context of machine learning and reinforcement learning, where we aim to make concrete the different types of information in the wild, and automatically learn a world model that removes noise distractors and is beneﬁcial for both control (i.e., policy optimization) and non-control tasks. Toward this goal, our contributions are • We categorize information into four distinct kinds as in Figure 1, and review prior approaches under this frame- work (Section 2). • Based on the above framework, we propose Denoised MDPs, a method for learning world models with certain distractors removed (Section 3). • Through experiments in DeepMind Control Suite and RoboDesk environments, we demonstrate superior per- formance of policies learned our method, across many distinct types of noise distractors (Sections 5.1 and 5.2). • We show that Denoised MDP is also beneﬁcial beyond control objectives, improving the supervised task of robot joint position regression (Section 5.1). 2. Different Types of Information in the Wild In Section 1, we illustrated the four types of information available in the wild w.r.t. a task. Here we make these notions more concrete, and relate them to existing works. For generality, we consider tasks in the form of Markov Decision Processes (MDPs), described in the usual manner: M (cid:44) (S, A, R, P, ps0 ) (Puterman, 1994), where S is the state space, A is the action space, R : S → ∆([0, rmax]) deﬁnes the reward random variable R(s(cid:48)) received for ar- riving at state s(cid:48) ∈ S, P : S × A → ∆(S) is the transition dynamics, and ps0 ∈ ∆(S) deﬁnes the distribution of initial state. We use ∆(A) to denote the set of all distributions over A. P and R deﬁne the most important components of a MDP: the transition dynamics P[s(cid:48) | s, a] and the re- ward function P[r | s(cid:48)]. Usually, the objective is to ﬁnd a policy π : S → ∆(A) acting based on current state, that maximizes the expected cumulative (discounted) reward. Indeed, MDPs provide a general formulation that encom- passes many tasks. In fact, the entire real world may be viewed as an MDP with a rich state/observation space S that contains all possible information/signal. For an artiﬁ- cial agent to successfully perform real world tasks, it must be able to process observations that are incredibly rich and high-dimensional, such as visual or audio signals. We characterize different types of information in such ob- servations by considering two intuitive notions of “noisy and irrelevant” signals: (1) uncontrollable information and (2) reward-irrelevant information. Such factors can often be ignored without affecting optimal control, and are referred to as noise distractors. To understand their roles in MDPs, we study different for- mulations of the transition dynamics and reward functions, and show how different structures naturally leads to decom- positions that may help identify such distractors. Removing these distractors can thus transform the original noisy MDP to a clean denoised one, to be used in downstream tasks. For starters, the most generic transition model in Figure 2a has little to no structure. The state s can contain both the useful signals and noise distractors. Therefore, it is not directly useful for extracting important information. 2.1. Controllability Intuitively, if something is not controllable, an agent might be able to do well without considering it. Yet it is not enough to only require some variable to be unaffected by actions (e.g., wind directions should not be ignored while sailing). Denoised MDPs a a a a r Rew Rew Ctrl s s(cid:48) Ctrl s s s s x yR yR s x(cid:48) y(cid:48) R y(cid:48) R s(cid:48) rx ry + r Rew Rew Ctrl x yR yR x yR yR Ctrl x yR yR x yR yR x y z s x(cid:48) y(cid:48) z(cid:48) s(cid:48) rx ry + r Rew Rew Ctrl Ctrl x y x y z z x y x y z z (b) Transition that factorizes out uncontrol- lable information in yR and yR. (a) Transition without useful structure. s may contain any type of information. Figure 2: MDP transition structures consisting of dynamics and reward functions. Unlike the regular structure of (a), (b, c) factorized (yet still general) structures inherently separate information into controllable (Ctrl) versus uncontrollable (Ctrl), and reward-relevant (Rew) versus reward-irrelevant (Rew). Presence of a variable in a cell means possible containing of respective information. E.g., in (c), z can only contain reward-irrelevant information. In (b, c), the x dynamics form an MDP with less noise and sufﬁcient for optimal planning. Our Denoised MDP (see Section 3) is based on these two factorizations. (c) Transition that factorizes out uncontrol- lable y and reward-irrelevant z. Instead, we focus on factors that simply evolve on their own, without inﬂuencing or being inﬂuenced by others. Not all such information can be safely ignored, as they still may affect reward (e.g., trafﬁc lights when driving). Fortunately, in the usual objective of maximizing expected return, we can ignore ones that only additively affect reward. Concretely, if an MDP transition can be represented in the form of Figure 2b, we say variables yR and yR are uncontrol- lable information, as they evolve independently of actions and do not affect controllable x. Here yR (additively) af- fects reward, but can be ignored. One can safely discard both yR and yR as noise distractors. Operating with the compressed MDP of only x is sufﬁcient for optimal control. 2.2. Reward-Relevance Among controllable information, there can still be some that is completely unrelated to reward. In Figure 1, the bird is affected by the opening curtain, but is irrelevant to the task of letting in sunlight. In such cases, the information can be safely discarded, as it does not affect the objective. If an MDP transition can be represented in the form of Figure 2c, we say z is reward-irrelevant because it evolves by potentially using everything (i.e., all latent variables and actions), but crucially does not affect anything but itself. Similar to uncontrollable information, z (and y) is a noise distractor that can be discarded. The compressed MDP of only x contains all signals needed for optimal control. 2.3. Which Information Do Existing Methods Learn? In RL, many prior work have explored state abstractions in some form. Here we cast several representative ones under Reconstruction-Based Model-Based RL (e.g., SLAC (Lee et al., 2019), Dreamer (Hafner et al., 2019a)) Model-Based Bisimulation (e.g., Ferns et al. (2004), Castro (2020), Zhang et al. (2020)) Model-Free Task Informed Abstractions (TIA) (Fu et al., 2021) Model-Based Denoised MDP (Figure 2b variant) (Our method from Section 3) Model-Based Denoised MDP (Figure 2c variant) (Our method from Section 3) Model-Based RewRew (cid:51) (cid:51) (cid:51) (cid:51) Ctrl Ctrl RewRew (cid:51) (cid:55) (cid:51) (cid:55) Ctrl Ctrl RewRew (cid:51) ? (cid:51) ? RewRew (cid:51) (cid:51) (cid:55) (cid:55) RewRew (cid:51) (cid:55) (cid:55) (cid:55) Ctrl Ctrl Ctrl Ctrl Ctrl Ctrl Information Grid Legend: (cid:51) Kept (cid:55) Reduced ? Depending on how the information is integrated in observations Figure 3: Categorization of information learned and removed by various methods with distinct formulations. the framework described above, and show which kinds of information they learn to remove, summarized in Figure 3, together with our proposed method (explained in Section 3). Below we discuss each prior work in detail. Reconstruction-Based Model-Based RL. Many model- based RL methods learn via reconstruction from a single latent code, often as a result of a variational formulation (Hafner et al., 2019a;b; Lee et al., 2019). The latent code Denoised MDPs must try to compress all information present in the observa- tion, and necessarily contains all types of information. Bisimulation. Bisimulation deﬁnes a state abstraction where states aggregated together must have the same ex- pected return and transition dynamics up to the abstrac- tion (Givan et al., 2003), and is known to optimally ignore reward-irrelevant information (Ferns et al., 2004). While its continuous version, bisimilation metric, is gaining popular- ity, learning them is computationally difﬁcult (Modi et al., 2020). Even with many additional assumptions, it is gen- erally only possible to learn an on-policy variant that loses the above guarantee (Castro, 2020; Zhang et al., 2020). Task Informed Abstractions (TIA). TIA (Fu et al., 2021) extends Dreamer by modelling two independent la- tent MDPs, representing signal and noise. The noise latent is enforced to be independent with reward and reconstruct the observation as well as possible. Reconstructions from each latent are composed together using an inferred mask in pixel-space, to form the full reconstruction for the re- construction loss. Because of its special structure, TIA can remove reward-irrelevant noise distractors that are present via pixel-wise composing two images from independent processes (e.g., agent moving on a noisy background), but not general ones (e.g., a shaky camera affecting both the agent and the noisy background). Predictive Information, Data Augmentation, etc. An- other set of researches learn state representation that only contains information useful for predicting future states (e.g., CPC (Oord et al., 2018) and PI-SAC (Lee et al., 2020)) or augmented views of the current state (e.g., CURL (Laskin et al., 2020b)). These methods do not guarantee removal of any of the three redundant piece of information identiﬁed above. Non-i.i.d. noises (e.g., people moving in background) are predictive of future and may be kept by CPC and PI- SAC. The performance of augmentation-based methods can critically rely on speciﬁc types of augmentation used and relevance to the tasks. As we show in experiments (see Sec- tion 5), indeed they struggle to handle certain noise types. 2.4. Possible Extensions to Further Factorizations The above framework is sufﬁcient for characterizing most prior work and related tasks, and can also be readily ex- tended with further factorized transition structures. E.g., if an independent process confounds a signal process and a noise process, ﬁtting the Figure 2c structure must group all three processes into x (to properly model the dependencies). However, a further factorization shows that only considering the signal and the confounding processes is theoretically sufﬁcient for control. We leave such extensions as future work. 3. Denoised MDPs Figures 2b and 2c show two special MDP structures that au- tomatically identify certain information that can be ignored, leaving x as the useful information (which also forms an MDP). This suggests a naïve approach: directly ﬁtting such structures to collected trajectories, and then extract x. However, the same MDP dynamics and rewards can be decomposed as Figures 2b and 2c in many different ways. In the extreme case, x may even contain all information in the raw state s, and such extraction may not help at all. Instead, we desire a ﬁt with the minimal x, deﬁned as being least informative of s (so that removal of the other latent variables discards the most information possible). Concretely, we aim for a ﬁt with least I({xt}T t=1), the mutual information x contains about s over T steps. Then from this ﬁt, we can extract a minimal Denoised MDP of only x. For notation simplicity, we use bold symbols to denote variable sequences, and thus write, e.g., I(x; s | a). t=1 | {at}T t=1; {st}T Practically, we consider regularizing model-ﬁtting with I(x; s | a). As we show below, this amounts to a modiﬁ- cation to the well-established variational objective (Hafner et al., 2019a). The resulting method is easy-to-implement yet effective, enabling clean removal of various noise distrac- tors the original formulation cannot handle (see Section 5). We instantiate this idea with the structure in Figure 2c. The Figure 2b formulation can be obtained by simply removing the z components and viewing y as combined yR and yR. The transition structure is modeled with components: p(xt) θ p(yt) θ p(zt) θ (cid:44) pθ(xt | xt−1, a) pθ(rx | xt) (cid:44) pθ(yt−1 | yt−1) pθ(ry | yt) (x dynamics) (x reward) (y dynamics) (y reward) (cid:44) pθ(zt | xt, yt, zt−1, a) pθ(st | xt, yt, zt). (z dynamics) (obs. emission) Consider training data in the form of trajectory segments s, a, r sampled from some data distribution pdata (e.g., stored agent experiences from a replay buffer). We perform model learning by minimizing the negative log likelihood: LMLE(θ) (cid:44) −Es,a,r∼pdata (cid:2) log pθ (s, r | a) (cid:3). To obtain a tractable form, we jointly learn three variational posterior components (i.e., encoders): q(xt) ψ q(yt) ψ q(zt) ψ (cid:44) qψ(xt | xt−1, yt−1, zt−1, st, at) (x posterior) (cid:44) qψ(yt | xt−1, yt−1, zt−1, st, at) (y posterior) (cid:44) qψ(zt | xt, yt, st, at), (z posterior) Denoised MDPs whose product deﬁnes the posterior qψ(x, y, z | s, a)1. We choose this factorized form based on the forward (prior) model structure of Figure 2c. Then, the model can be optimized w.r.t. the standard varia- tional bound on log likelihood: (cid:20) LMLE(θ) = min ψ Es,a,r Ex,y,z∼ qψ (·|s,a,r) − log pθ(s, r | x, y, z, a) (cid:124) (cid:123)(cid:122) (cid:125) (cid:44) Lrecon(θ, ψ) T (cid:88) DKL (cid:0)q(yt) ψ (cid:13) (cid:13) p(yt) θ DKL (cid:0)q(xt) ψ (cid:13) (cid:13) p(xt) θ (cid:1) + (cid:123)(cid:122) (cid:44) LKL-x(θ, ψ) (cid:0)q(zt) DKL ψ (cid:13) (cid:13) p(zt) θ (cid:123)(cid:122) (cid:44) LKL-z(θ, ψ) t=1 (cid:124) (cid:123)(cid:122) (cid:44) LKL-y(θ, ψ) (cid:125) (cid:1) (cid:125) (cid:21) , (cid:1) (cid:125) (1) + + T (cid:88) t=1 (cid:124) T (cid:88) t=1 (cid:124) where equality is attained by optimal qψ that is compatible with pθ, i.e., qψ is the exact posterior of pθ. The mutual information regularizer I(x; s | a), using a variational formulation, can be written as I(x; s | a) = min θ LKL-x(θ, ψ), (2) with equality attained when qψ and pθ are compatible. The appendix describes this derivation in detail. Therefore, for a regularizer weight of c ≥ 0, we can opti- mize Equations (1) and (2) together as min θ = min θ,ψ LMLE(θ) + c · I(x; s | a) Lrecon(θ, ψ) + (1 + c) · LKL-x(θ, ψ) + LKL-y(θ, ψ) + LKL-z(θ, ψ). (3) Recall that we ﬁt to the true MDP with the structure of Fig- ure 2c, which inherently guarantees all useful information in the x latent variable. As the regularizer ensures learning the minimal x latents, the learned model extracts an MDP of condensed useful information with X as the denoised state space, pθ(x(cid:48) | x, a) as the transition dynamics, pθ(rx | x(cid:48)) as the reward function. This MDP is called the Denoised MDP, as it discards the noise distractors contained in y and z. Additionally, we also obtain qψ(x | s, a) as the encoder mapping from raw noisy observation s to the denoised x. A loss variant for improved stability. When using a large c ≥ 0 (e.g. when the environment is expected to be very noisy), Equation (3) contains to a term with a large weight. Thus Equation (3) often requires learning rates to be tuned for different c. To avoid this, we use the following loss form that empirically has better training stability and does not require tuning learning rates w.r.t. other hyperpa- 1Following Dreamer (Hafner et al., 2019a), we deﬁne pos- terior of ﬁrst-step latents qψ(x1, y1, z1 | s1) (cid:44) qψ( · , · , · | 0, 0, 0, s1, 0), where 0 is the all zeros vector of appropriate size. Algorithm 1 Denoised MDP Input: Model pθ. Posterior encoder qψ. Policy π : X → ∆(A). Policy optimization algorithm PI-OPT. Output: Denoised MDP of x in pθ; Encoder qψ; Policy π. 1: while training do // Exploration 2: Collect trajectories with π acting on qψ encoded outputs 3: // Model learning 4: Sample a batch of (s, a, r) segments from reply buffer 5: Train pθ and qψ with Equation (4) on (s, a, r) 6: // Policy optimization 7: Sample x ∼ qψ(x | s, a); Compute rx = E [pθ(rx | x)] 8: Train π by running PI-OPT on (x, a, rx) 9: 10: end while rameters: min θ,ψ Lrecon + α · (LKL-x + βLKL-y + βLKL-z) , (4) where θ, ψ in arguments are omitted, and the hyperparame- ters are α > 0 and 0 < β ≤ 1. Here β is bounded, where β = 1 represents no regularization. α is also generally small and simply chosen according to the state-space dimensional- ity (see the appendix; α ∈ {1, 2} in our experiments). This form is justiﬁed from the observation that in practice we use isotropic Gaussians with ﬁxed variance to parameter- ize the distributions of observation pθ(s | . . . ) and reward pθ(r | . . . ), where scaling log likelihoods is essentially changing the variance hyperparameter. Thus, Equation (4) is effectively a scaled Equation (3) with different variance hyperparameters. Online algorithm with policy optimization. The model ﬁtting objective of Equation (4) can be used in various set- tings, e.g., ofﬂine over a collected trajectory dataset. With- out assuming existing data, we explore an online setting, where the training process iteratively performs (1) explo- ration, (2) model-ﬁtting, and (3) policy optimization, as shown in Algorithm 1. The policy π : X → ∆(A) soley operates on the Denoised MDP of x, which has all infor- mation sufﬁcient for control. For policy optimization, the learned posterior encoder qψ(x | s, a) is used to extract x information from the raw trajectory (s, a, r), obtaining transition sequences in X space. Paired with the pθ(rx | x) rewards, we obtain (x, a, rx) as trajectories collected from the Denoised MDP on x. Any general-purpose MDP policy optimization algorithm may be employed on these data, such as Stochastic Actor-Critic (SAC) (Haarnoja et al., 2018). We can also utilize the learned differentiable Denoised MDP, e.g., optimizing policy by backpropagating through addi- tional roll-outs from the model, as is done in Dreamer. While presented in the fully observable setting, Denoised MDP readily handles partial observability without extra changes. In the appendix, we discuss this point in details, and provide a guideline for choosing hyperparameters α, β. Denoised MDPs 4. Related Work Model-Based Learning for Control jointly learns a world model and a policy. Such methods often enjoy good sample efﬁciency on RL tasks with rich observations. Some formulations rely on strong assumptions, e.g., determinis- tic transition in DeepMDP (Gelada et al., 2019) and bilin- ear transition in FLAMBE (Agarwal et al., 2020). Most general-setting methods use a reconstruction-based objec- tive (Hafner et al., 2019b; Kim et al., 2020; Ha & Schmidhu- ber, 2018; Lee et al., 2019). Among them, Dreamer (Hafner et al., 2019a) trains world models with a variational formu- lation and optimizes policies by backpropagating through latent-space rollouts. It has proven effective across a va- riety of environments with image observations. However, such reconstruction-based approaches can struggle with the presence of noise distractors. TIA (Fu et al., 2021) partially addresses this limitation (see Section 2.3) but can not handle general distractors, unlike our method. Representation Learning and Reinforcement Learning. Our work automates selecting useful signals from noisy MDPs by learning denoised world models, and can be viewed as an approach for learning general representations (Donahue et al., 2014; Mikolov et al., 2013; He et al., 2019; Huh et al., 2016). In model-free RL, various methods learn state embeddings that are related to value functions (Schaul et al., 2015; Bellemare et al., 2019), transition dynamics (Mahadevan & Maggioni, 2007; Lee et al., 2020), recent ac- tion (Pathak et al., 2017), bisimulation structure (Ferns et al., 2004; Castro, 2020; Zhang et al., 2020), data augmentations (Laskin et al., 2020b) etc. Recently, Eysenbach et al. (2021) proposes a regularizer similar to ours but for the different purpose of robust compressed policies. The theoretical work by Efroni et al. (2021) is closest to our setting but concerns a more restricted set of distractors (ones both uncontrollable and reward-irrelevant). Unlike Denoised MDP, their pro- posed algorithm is largely impractical and does not produce a generative model of observations (i.e., no decoder). System Identiﬁcation. Our work is related to system identiﬁcation, where an algorithm infers from real world an abstract state among a predeﬁned limited state space, e.g., pose estimation (Rıza Alp Güler, 2018; Yen-Chen et al., 2021) and material estimation (Hahn et al., 2019). Such results are useful for robotic manipulation (Manuelli et al., 2019), image generation (Gu et al., 2019), etc. Our setting is not limited to a predeﬁned abstract state space, but instead focuses on automatic discovery of such valuable states. 5. Experiments In this section, we contrast our method with existing ap- proaches on environments with image observations and many distinct types of noise distractors. Our experiments are designed to include a variety of noise distractors and to conﬁrm our analysis on various methods in Section 2.3. Environments. We choose DeepMind Control (DMC) Suite (Tunyasuvunakool et al., 2020) (Section 5.2) and RoboDesk (Kannan et al., 2021) (Section 5.1) with image observations, where we explore adding various noise dis- tractors. Information types in all evaluated environments are categorized in Table 2 of the appendix. Tasks include control (policy optimization) and a non-control task of regressing robot joint position from RoboDesk image observations. Methods. We compare not only model-based RL meth- ods, but also model-free algorithms and general representa- tion learning approaches, when the task is suited: • Model Learning: Denoised MDP (our method), Dreamer (Hafner et al., 2019a), and TIA (Fu et al., 2021); • Model-Free: DBC (Zhang et al., 2020), CURL (Laskin et al., 2020b), PI-SAC (Lee et al., 2020) (without data augmentation for a fair comparison of its core predictive information regularization against other non-augmenting methods), and SAC on true state-space (Haarnoja et al., this is 2018) (instead of using image observations, roughly an “upper bound”); • General Image Representation Learning for Non- Control Tasks: Contrastive learning with the Align- ment+Uniformity loss (Wang & Isola, 2020) (a form of contrastive loss theoretically and empirically comparable to the popular InfoNCE loss (Oord et al., 2018)). Model-learning methods can be used in combination with any policy optimization algorithm. For a complete com- parison for general control, we compare the models trained with these two policy learning choices: (1) backpropagating via the learned dynamics and (2) SAC on the learned la- tent space (which roughly recovers SLAC (Lee et al., 2019) when used with an unfactorized model such as Dreamer). Most compared methods do not apply data augmentations, which is known to strongly boost performance (Yarats et al., 2021; Laskin et al., 2020a). Therefore, for a fair comparison, we run PI-SAC without augmentation to highlight its main contribution—representation of only predictive information. All results are aggregated from 5 runs, showing mean and standard deviations. The appendix contains more details, hyperparameter studies, and additional results. Our website presents videos showing clearer video visualizations. For Denoised MDP, we use the Figure 2b variant. Empiri- cally, the Figure 2c variant leads to longer training time and sometimes inferior performance (perhaps due to having to optimize extra components and ﬁt a more complex model). The appendix provides a comparison between them. Denoised MDPs Figure 4: Visualization of learned models for RoboDesk by using decoders to reconstruct from encoded latents. For TIA and Denoised MDP, we visualize how they separate information as signal versus noise. In each row, what changes over frames is the information modeled by the corresponding latent component. E.g., in the bottom row, only the TV content, camera pose and lighting condition change, so Denoised MDP considers these factors as noises, while modelling the TV hue as signal. See our website for clearer video visualizations. 5.1. RoboDesk with Various noise distractors We augment RoboDesk environment with many noise dis- tractors that models realistic noises (e.g., ﬂickering lights and shaky camera). Most importantly, we place a large TV in the scene, which plays natural RGB videos. A green button on the desk controls the TV’s hue (and a light on the desk). The agent is tasked with using this button to shift the TV to a green hue. Its reward is directly affected by how green the TV image is. The ﬁrst row of Figure 4 shows a trajectory with various distractors annotated. All four types of information exist (see Table 2), with the controllable and reward-relevant information being the robot arm, the green button, the light on the desk, and the TV screen green-ness. controllable and reward-relevant information as signals— the Signal row only tracks changes in robot arms, green button and light, and the TV screen green-ness. All other information is modeled as noises (see the Noise row). We recommend viewing video visualizations on our website. Denoised models improve policy learning. Figure 4 also shows the total episode return achieved by policies learned with each of the three models, where the cleanest model from Denoised MDP achieves the best performance. Aggregating over 5 runs, the complete comparison in Fig- ure 5 shows that Denoised MDP (with backpropagating via dynamics) generally outperforms all baselines, suggesting that its clean models are helpful for control. Only Denoised MDP learns a clean denoised model. Using learned decoders, Figure 4 visualizes how the mod- els captures various information. As expected, Dreamer model captures all information. TIA also fails to separate any noise distractors out (the Noise row fails to capture anything), likely due to its limited ability to model differ- ent noises. In contrast, Denoised MDP cleanly extracts all Denoised models beneﬁt non-control tasks. We evalu- ate the learned representations on a supervised non-control task—regressing the robot arm joint position from observed images. Using various pretrained encoders, we ﬁnetune on a labeled training set, and measure mean squared error (MSE) on a heldout test set. In addition to RL methods, we compare encoders learned via general contrastive learning Blocks on Desk: Ctrl & Rew TV Image Green-ness: Ctrl & Rew Green Button & Light: Ctrl & Rew Robot Joints: Ctrl & Rew TV Semantic Content: Ctrl & Rew Shaky/Flickering Camera & Lights: Ctrl & Rew Env. Rollout Obs. Reward <latexit sha1_base64=""2Iv/3i2hbQBcJJvODDrynrkogxE="">AAACaHicjVHLSgMxFE3Hd321uhBxEyyCLiwzoqgLQRTBpYJVoR0kk96xwSQzJHekZZgP8Gvc6qf4C36Fae1CrYIXAodzziU5J1EqhUXffyt5Y+MTk1PTM+XZufmFxUp16dommeHQ4IlMzG3ELEihoYECJdymBpiKJNxED6d9/eYRjBWJvsJeCqFi91rEgjN01F2lttlSDDtRlJ8VzRZCF22cG8DM6CKkR3QvONxyLr/uD4aOgmAIamQ4F3fV0narnfBMgUYumbXNwE8xzJlBwSUU5VZmIWX8gd1D00HNFNgwH6Qp6IZj2jROjDsa6YD9upF3/2tkytqeipyzn9H+1PrkXxp21G9SM8P4IMyFTjMEzT/fEGeSYkL79dK2MMBR9hxg3AiXl/IOM4yj+4Rvl1irpXKexBau4OBnnaPgeqce7NX9y93a8cmw6mmyRtbJJgnIPjkm5+SCNAgnT+SZvJDX0rtX8Va81U+rVxruLJNv461/AAe3u44=</latexit> Dreamer (E[return] = 519) Recon. Recon. <latexit sha1_base64=""detSQHFar06ldxZUdWoplWvpw3s="">AAACaHicjVHLSgMxFE3H97vVhYibYBF0YZnxvRFEEVwqWBXaQTLpnTaYZIbkjrQM8wF+jVv9FH/BrzCtXfgELwQO55xLck6iVAqLvv9a8kZGx8YnJqemZ2bn5hfKlcVrm2SGQ50nMjG3EbMghYY6CpRwmxpgKpJwE92f9vWbBzBWJPoKeymEirW1iAVn6Ki7cnWjqRh2oig/KxpNhC7aODeAmdFFSI/o7s7+pnP5NX8w9CcIhqBKhnNxVyltNVsJzxRo5JJZ2wj8FMOcGRRcQjHdzCykjN+zNjQc1EyBDfNBmoKuO6ZF48S4o5EO2M8befe/Rqas7anIOfsZ7XetT/6lYUf9JjUyjA/DXOg0Q9D84w1xJikmtF8vbQkDHGXPAcaNcHkp7zDDOLpP+HKJtVoq50ls4QoOvtf5E1xv14K9mn+5Wz0+GVY9SVbJGtkgATkgx+ScXJA64eSRPJFn8lJ688resrfyYfVKw50l8mW8tXcD9buM</latexit> TIA (E[return] = 436) Signal Noise Recon. Signal Noise Denoised MDP (E[return] = 628) <latexit sha1_base64=""ftE/OHSJy3ovK5tqHIptkB9eUHM="">AAACaHicjVHLSgMxFE3H97vqQsRNaBF0YZkRXxtBFMGlgtVCO0gmvWODSWZI7ohlmA/wa9zqp/gLfoVp7UJbBS8EDuecS3JOolQKi77/XvLGxicmp6ZnZufmFxaXyssrNzbJDIc6T2RiGhGzIIWGOgqU0EgNMBVJuI0eznr67SMYKxJ9jd0UQsXutYgFZ+iou3J1q6UYdqIoPy+aLYQntHFuADOji5Ae04Pdo23n8mt+f+goCAagSgZzebdc2mm1E54p0Mgls7YZ+CmGOTMouIRitpVZSBl/YPfQdFAzBTbM+2kKuumYNo0T445G2me/b+RP/zUyZW1XRc7Zy2iHtR75l4Yd9ZvUzDA+CnOh0wxB8683xJmkmNBevbQtDHCUXQcYN8LlpbzDDOPoPuHHJdZqqZwnsYUrOBiucxTc7NaC/Zp/tVc9OR1UPU02SIVskYAckhNyQS5JnXDyTF7IK3krfXhlb81b/7J6pcHOKvkxXuUTCZu7jw==</latexit> Denoised MDPs Figure 5: Policy optimization on RoboDesk. We give state-space SAC a less noisy reward so it can learn (see appendix). Figure 6: Performance of ﬁnetuning various encoders to infer joint position from RoboDesk image observation. Policy Learning: Backprop via Dynamics Policy Learning: SAC (Latent-Space) Denoised MDP TIA Dreamer Denoised MDP TIA Dreamer DBC PI-SAC (No Aug.) CURL (Use Aug.) State-Space SAC (Upper Bound) Noiseless 801.4 ± 96.6 769.7 ± 97.1 848.6 ± 137.1 587.1 ± 98.7 480.2 ± 125.5 575.4 ± 146.2 297.4 ± 72.5 246.4 ± 56.6 417.3 ± 183.2 910.3 ± 28.2 Video Background 597.7 ± 117.8 407.1 ± 225.4 227.8 ± 102.7 309.8 ± 153.0 318.1 ± 123.7 188.7 ± 78.2 188.0 ± 67.4 131.7 ± 20.1 478.0 ± 113.5 910.3 ± 28.2 Video Background + Noisy Sensor Video Background + Camera Jittering 563.1 ± 143.0 261.2 ± 200.4 212.4 ± 89.7 288.2 ± 123.4 197.3 ± 124.2 218.2 ± 58.1 79.9 ± 36.0 152.5 ± 12.6 354.3 ± 119.9 919.8 ± 100.7 254.1 ± 114.2 151.7 ± 160.5 98.6 ± 27.7 186.8 ± 47.7 126.5 ± 125.6 105.2 ± 33.8 68.0 ± 38.4 91.6 ± 7.6 390.4 ± 64.9 910.3 ± 28.2 Table 1: DMC policy optimization results. For each variant, we aggregate performance across three tasks (Cheetah Run, Walker Walk, Reacher Easy) by averaging. Denoised MDP performs well across all four variants with distinct noise types. Bold numbers show the best model-learning result for speciﬁc policy learning choices, or the best overall result. On Camera Jittering, Denoised MDP greatly outperforms all other methods except for CURL, which potentially beneﬁts from its speciﬁc data augmentation choice (random crop) on this task, and can be seen as using extra information (i.e., knowing the noise distractor form). In fact, Denoised MDP is the only method that consistently performs well across all tasks and noise variants, which can be seen from the full results in the appendix. on the same amount of data. In Figure 6, Denoised MDP representations lead to best converged solutions across a wide range of training set sizes, achieve faster training, and avoid overﬁtting when the training set is small. DBC, CURL and PI-SAC encoders, which take in stacked frames, are not directly comparable and thus absent from Figure 6. In the appendix, we compare them with running Denoised MDP encoder on each frame and concatenating the output fea- tures, where Denoised MDP handily outperforms both DBC and CURL by a large margin. 5.2. DeepMind Control Suite (DMC) To evaluate a diverse set of noise distractors, we consider four variants for each DMC task (see Figure 7 top row): • Noiseless: Original environment without distractors. • Video Background: Replacing noiseless background with natural videos (Zhang et al., 2020) (Ctrl + Rew). • Video Background + Sensor Noise: Imperfect sensors sensitive to intensity of a background patch (Ctrl+Rew). • Video Background + Camera Jittering: Shifting the observation by a smooth random walk (Ctrl + Rew). Denoised MDP consistently removes noise distractors. In Figure 7, TIA struggles to learn clean separations in many settings. Consistent with analysis in Section 2.3, it cannot handle Sensor Noise or Camera Jittering, as the former is reward-relevant noise that it cannot model, and the latter (although reward-irrelevant) cannot be represented by masking. Furthermore, it fails on Reacher Easy with Video Background, where the reward is given by the distance between the agent and a randomly-located ball. TIA encour- ages its noise latent to be independent of reward, but does not prevent it from capturing the controllable agent. These failures lead to either TIA trying to model everything as useful signals, or a badly-ﬁt model (e.g., wrong agent pose in the last column). In contrast, Denoised MDP separates out noise in all cases, obtaining a clean and accurate MDP (its Signal rows only have the agent moving). Denoised models consistently improve policy learning. We evaluate the learned policies in Table 1, where results are aggregated by the noise distractor variant. Other meth- ods, while sometimes handling certain noise types well, struggle to deal with all four distinct variants. TIA, as ex- pected, greatly underperforms Denoised MDP under Noisy Sensor or Camera Jittering. CURL, whose augmentation choice potentially helps handling Camera Jittering, under- performs in other three variants. In contrast, Denoised MDP State-Space SAC with Modified Reward Joint Position Regression Final Test MSE vs. Training Set Size Joint Position Regression Learning Curve for |Train Set|=104 Denoised MDPs Figure 7: Visualization of the different DMC variants and factorizations learned by TIA and Denoised MDP. E.g., bottom Noise row often shows a static agent but varying background, indicating that only the background is modeled as noises in Denoised MDP. Visualizations of full reconstructions are in appendix. See our website for clearer video visualizations. policies consistently perform well for all noisy variants and also the noiseless setting, regardless of the policy optimizer. Model-based approaches have a signiﬁcant lead over the model-free ones, as seen from the DBC results in Table 1 and the well-known fact that direct model-free learning on raw image observations usually fails (Laskin et al., 2020a; Kostrikov et al., 2020; Yarats et al., 2021). These results show that learning in a world model is useful, and that learning in a denoised world model is even better. 6. Implications In this work we explore learning denoised and compressed world models in the presence of environment noises. As a step towards better understanding of such noises, we categorize of information in the wild into four types (Sec- tion 2). This provides a framework to contrast and under- stand various methods, highlighting where they may be successful and where they will suffer (Section 2.3). Insights gained this way empirically agrees with ﬁndings from exten- sive experiments (Section 5). It can potentially assist better algorithm design and analysis of new MDP representation methods, as we have done in designing Denoised MDP (Section 3). We believe that this categorization will be a useful framework for investigation on learning under noises, revealing not just the (conceptual) success scenarios, but also the failure scenarios at the same time. Additionally, the framework can be readily extended with more sophisticated factorizations (Section 2.4), which can lead to correspond- ing Denoised MDP variants and/or new algorithms. Based on the framework, our proposed Denoised MDP nov- elly can remove all noise distractors that are uncontrollable or reward-irrelevant, in distinction to prior works. Empiri- cally, it effectively identiﬁes and removes a diverse set of noise types, obtaining clean denoised world models (Sec- tion 5). It may serve as an important step towards efﬁcient learning of general tasks in the noisy real world. Our ex- periments also highlight beneﬁts of cleanly denoised world models on both standard control tasks as well as non-control tasks. The success in both cases highlights the general use- fulness of such models. Given the generality of MDPs, this opens up the possibility of casting non-RL tasks as MDPs and automatically learn representations from denoised world models, as an alternative to manual feature engineering. Acknowledgements We thank Jiaxi Chen for the beautiful Figure 1 illustration. We thank Daniel Jiang and Yen-Chen Lin for their helpful comments and suggestions. We are grateful to the following organizations for providing computation resources to this project: IBM’s MIT Satori cluster, MIT Supercloud cluster, and Google Cloud Computing with credits gifted by Google to MIT. We are very thankful to Alex Lamb for suggestions and catching our typo in the conditioning of Equation (1). Cheetah Run Noiseless Reacher Easy Video Background Walker Walk Video Background + Noisy Sensor Cheetah Run Video Background + Camera Jittering Env. Rollout Obs. Reward TIA Denoised MDP Signal Noise Signal Noise Denoised MDPs References Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, FLAMBE: Structural complexity and represen- arXiv preprint W. tation learning of low rank mdps. arXiv:2006.10814, 2020. Bellemare, M., Dabney, W., Dadashi, R., Ali Taiga, A., Castro, P. S., Le Roux, N., Schuurmans, D., Lattimore, T., and Lyle, C. A geometric perspective on optimal representations for reinforcement learning. Advances in neural information processing systems, 32:4358–4369, 2019. Castro, P. S. Scalable methods for computing state similarity in deterministic markov decision processes. In Proceed- ings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 10069–10076, 2020. Coates, A., Ng, A., and Lee, H. An analysis of single- layer networks in unsupervised feature learning. In Pro- ceedings of the fourteenth international conference on artiﬁcial intelligence and statistics, pp. 215–223, 2011. Craik, K. J. W. The nature of explanation, volume 445. CUP Archive, 1952. Dennett, D. C. Why the law of effect will not go away. Journal for the Theory of Social Behaviour, 1975. Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. In Inter- national conference on machine learning, pp. 647–655. PMLR, 2014. Du, S., Krishnamurthy, A., Jiang, N., Agarwal, A., Dudik, M., and Langford, J. Provably efﬁcient rl with rich obser- vations via latent state decoding. In International Con- ference on Machine Learning, pp. 1665–1674. PMLR, 2019. Efroni, Y., Misra, D., Krishnamurthy, A., Agarwal, A., and Langford, J. Provable rl with exogenous distrac- tors via multistep inverse dynamics. arXiv preprint arXiv:2110.08847, 2021. Elman, J. L. Finding structure in time. Cognitive science, 14(2):179–211, 1990. Eysenbach, B., Salakhutdinov, R., and Levine, S. Robust predictable control. arXiv preprint arXiv:2109.03214, 2021. Ferns, N., Panangaden, P., and Precup, D. Metrics for ﬁnite markov decision processes. In UAI, volume 4, pp. 162– 169, 2004. Fu, X., Yang, G., Agrawal, P., and Jaakkola, T. Learning task informed abstractions. In International Conference on Machine Learning, pp. 3480–3491. PMLR, 2021. Gelada, C., Kumar, S., Buckman, J., Nachum, O., and Belle- mare, M. G. Deepmdp: Learning continuous latent space models for representation learning. In International Con- ference on Machine Learning, pp. 2170–2179. PMLR, 2019. Givan, R., Dean, T., and Greig, M. Equivalence notions and model minimization in markov decision processes. Artiﬁcial Intelligence, 147(1-2):163–223, 2003. Gu, S., Bao, J., Yang, H., Chen, D., Wen, F., and Yuan, L. Mask-guided portrait editing with conditional gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3436–3445, 2019. Ha, D. and Schmidhuber, J. World models. arXiv preprint arXiv:1803.10122, 2018. Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018. Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019a. Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, pp. 2555–2565. PMLR, 2019b. Hahn, D., Banzet, P., Bern, J. M., and Coros, S. Real2sim: Visco-elastic parameter estimation from dynamic motion. ACM Transactions on Graphics (TOG), 38(6):1–13, 2019. He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo- mentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019. Huh, M., Agrawal, P., and Efros, A. A. What makes arXiv preprint imagenet good for transfer learning? arXiv:1608.08614, 2016. Kannan, H., Hafner, D., Finn, C., and Erhan, D. RoboDesk: A multi-task reinforcement learning benchmark. https: //github.com/google-research/robodesk, 2021. Kim, S. W., Zhou, Y., Philion, J., Torralba, A., and Fidler, S. Learning to Simulate Dynamic Environments with GameGAN. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2020. Denoised MDPs Kostrikov, I., Yarats, D., and Fergus, R. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020. Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A. Reinforcement learning with augmented data. Advances in Neural Information Processing Systems, 33: 19884–19895, 2020a. Laskin, M., Srinivas, A., and Abbeel, P. Curl: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, pp. 5639–5650. PMLR, 2020b. In International Conference on Machine Learning, pp. 5171–5180. PMLR, 2019. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., USA, 1st edition, 1994. ISBN 0471619779. Rıza Alp Güler, Natalia Neverova, I. K. Densepose: Dense human pose estimation in the wild. 2018. Schaul, T., Horgan, D., Gregor, K., and Silver, D. Universal value function approximators. In International conference on machine learning, pp. 1312–1320. PMLR, 2015. Lee, A. X., Nagabandi, A., Abbeel, P., and Levine, S. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. arXiv preprint arXiv:1907.00953, 2019. Smaira, L., Carreira, J., Noland, E., Clancy, E., Wu, A., and Zisserman, A. A short note on the kinetics-700-2020 human action dataset. arXiv preprint arXiv:2010.10864, 2020. Lee, K.-H., Fischer, I., Liu, A., Guo, Y., Lee, H., Canny, J., and Guadarrama, S. Predictive information accelerates learning in rl. Advances in Neural Information Processing Systems, 33:11890–11901, 2020. Lowe, D. G. Object recognition from local scale-invariant features. In Proceedings of the seventh IEEE interna- tional conference on computer vision, volume 2, pp. 1150– 1157. Ieee, 1999. Mahadevan, S. and Maggioni, M. Proto-value functions: A laplacian framework for learning representation and control in markov decision processes. Journal of Machine Learning Research, 8(10), 2007. Manuelli, L., Gao, W., Florence, P., and Tedrake, R. kpam: Keypoint affordances for category-level robotic manipu- lation. arXiv preprint arXiv:1903.06684, 2019. Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efﬁcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. Modi, A., Jiang, N., Tewari, A., and Singh, S. Sample com- plexity of reinforcement learning using linearly combined model ensembles. In International Conference on Artiﬁ- cial Intelligence and Statistics, pp. 2010–2020. PMLR, 2020. Oord, A. v. d., Li, Y., and Vinyals, O. Representation learn- ing with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. Curiosity-driven exploration by self-supervised predic- tion. In ICML, 2017. Poole, B., Ozair, S., Van Den Oord, A., Alemi, A., and Tucker, G. On variational bounds of mutual information. Spelke, E. S. and Kinzler, K. D. Core knowledge. Develop- mental science, 10(1):89–96, 2007. Sutton, R. S. An adaptive network that constructs and uses and internal model of its world. Cognition and Brain Theory, 4(3):217–246, 1981. Sutton, R. S. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160– 163, 1991. Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ Inter- national Conference on Intelligent Robots and Systems, pp. 5026–5033. IEEE, 2012. Tunyasuvunakool, S., Muldal, A., Doron, Y., Liu, S., Bohez, S., Merel, J., Erez, T., Lillicrap, T., Heess, N., and Tassa, Y. dm_control: Software and tasks for continuous control. Software Impacts, 6:100022, 2020. Wang, T. and Isola, P. Understanding contrastive represen- tation learning through alignment and uniformity on the hypersphere. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Pro- ceedings of Machine Learning Research, pp. 9929–9939. PMLR, 13–18 Jul 2020. Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. Mastering visual continuous control: Improved data-augmented re- inforcement learning. arXiv preprint arXiv:2107.09645, 2021. Yen-Chen, L., Florence, P., Barron, J. T., Rodriguez, A., Isola, P., and Lin, T.-Y. iNeRF: Inverting neural radiance ﬁelds for pose estimation. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021. Denoised MDPs Zhang, A., McAllister, R., Calandra, R., Gal, Y., and Levine, S. Learning invariant representations for rein- forcement learning without reconstruction. arXiv preprint arXiv:2006.10742, 2020. A. Denoised MDP Discussions A.1. Loss Derivation Denoised MDPs To apply our mutual information regularizer I(x; s | a), we can consider a form using another variational distribution ρ (see, e.g., Poole et al. (2019)), I(x; s | a) = min ρ EaEpθ(s|a) [DKL(pθ(x | s, a) (cid:107) ρ(x | a))] ρ EaEqψ(s|a) [DKL(qψ(x | s, a) (cid:107) ρ(x | a))] ≈ min = min θ(cid:48) LKL-x(ψ, θ(cid:48)). (assume qψ is roughly the posterior of pθ) (5) The assumption that qψ is roughly the posterior of pθ is acceptable because it is the natural consequence of optimizing the variational MLE objective in Equation (1) over θ, ψ. Alternatively, we can consider the MI deﬁned by a joint conditional distribution P (x, s | a) not from the forward model pθ, but from the data distribution and posterior model qψ(x | s, a). This is also sensible because the variational MLE objective in Equation (1) optimizes for compatible pθ and qψ that both ﬁt data and consistently describe (conditionals of) the same underlying distribution. Thus regularizing either can encourage a low MI. This approach leads to exactly Equation (5), without approximation. Then, the total loss in Equation (3) from combining Equations (1) and (5) is given by min θ LMLE(θ) + c · I(x; s | a) = min θ,θ(cid:48),ψ Lrecon(θ, ψ) + LKL-x(θ, ψ) + LKL-y(θ, ψ) + LKL-z + c · +LKL-x(θ(cid:48), ψ) = min θ,ψ Lrecon(θ, ψ) + (1 + c) · LKL-x(θ, ψ) + LKL-y(θ, ψ) + LKL-z(θ, ψ). A.2. Discussions We discuss some algorithmic choices of Denoised MDP below. Speciﬁc implementation details (e.g., architectures) can be found at Appendix B.1.2. Posterior distributions of rx and ry. The pθ reward distributions pθ(rx | xt) and pθ(ry | yt) are modelled via Gaussians (as is done usually in world models, such as Dreamer (Hafner et al., 2019a)). By the transition structure of Denoised MDPs, these distributions are inherently independent. Recall that r = rx + ry. Therefore, we can easily compute the distribution of pθ(r | xt, yt) and its log likelihoods. This enables easy optimization of the variational MLE objective, without requiring the posterior model to also infer rx and ry from observed r subject to the addition relation. Partial observability. Sections 2 and 3 discussions are mostly based in the fully observable setting. Yet most benchmarks and real-world tasks are partially observable, e.g., robot joint speeds that can not be inferred from a single frame. Fortunately, the transition models used in Denoised MDP are fully capable of handle such cases, as long as the encoder qψ is not deterministic and the observation model pθ(s | . . . ) does not have the block structure (Du et al., 2019) (which would make x, y, z fully determined from s). In practice, we let both components to be generic conditional distributions (parameterized by regular deep neural networks). Therefore, Denoised MDP does not require full observability. The loss in Equation (4) has two hyperparameters: α ∈ (0, ∞) and β ∈ (0, 1). To maintain Hyperparameter choice. relative ratio with the observation reconstruction loss, we recommend scaling α roughly proportionally with dimensionality of the observation space, as is done in our experiments presented in this paper. A smaller β means stronger regularization. Therefore, β can be chosen based on training stability and the level of noise distractors in the task. B. Experiment Details All code (including code for our environment variants and code for our Denoised MDP method) will be released upon publication. Denoised MDPs Ctrl + Rew Ctrl + Rew Ctrl + Rew Noiseless Video Background DMC Video Background + Noisy Sensor Video Background + Camera Jittering Agent Agent Agent Agent — — — — — — Ctrl + Rew — Background Background — — Background, Jittering camera RoboDesk Agent, Button, Light on desk, Green hue of TV Blocks on desk, Handle on desk, Other movable objects TV content, Button sensor noise Jittering and ﬂickering environment lighting, Jittering camera Table 2: Categorization of various information in the environments we evaluated with. B.1. Implementation Details B.1.1. ENVIRONMENTS AND TASKS In all environments, trajectories are capped at 1000 timesteps. Table 2 shows a summary of what kinds of information exist in each environment. DeepMind Control Suite (DMC). Our Video Background implementation follows Deep Bisimulation for Control (Zhang et al., 2020) on most environments, using Kinetics-400 grayscale videos (Smaira et al., 2020), and replacing pixels where blue channel is strictly the greatest of three. This method, however, does not cleanly remove most of background in the Walker Walk environment, where we use an improved mask that replaces all pixels where the blue channel is among the greatest of three. For Camera Jittering, we shift the observation image according to a smooth random walk, implemented as, at each step, Gaussian-perturbing acceleration, decaying velocity, and adding a pulling force if the position is too far away from origin. For Sensor Noise, we select one sensor, and perturb it according to intensity of a patch of the natural video background (i.e., adding average patch value − 0.5). We perturb the speed sensor for Cheetah Run, the torso_height sensor for Walker Walk, and the normalized finger_to_target_dist sensor for Reacher Easy. These sensor values undergo non-linear (mostly piece-wise linear) transforms to compute rewards. While they can not be perfectly modelled by additive reward noise, such a model is usually sufﬁcient in most cases when the sensor values are not too extreme and stay in one linear region. RoboDesk. We modify the original RoboDesk environment by adding a TV screen and two neighboring desks. The TV screen places (continuously horizontally shifting) natural RGB videos from the Kinetics-400 dataset (Smaira et al., 2020). The environment has three light sources from the above, to which we added random jittering and ﬂickering. The viewing camera is placed further to allow better view of the noise distractors. Resolution is increased from 64 × 64 to 96 × 96 to compensate this change. Camera jittering is implemented by a 3D smooth random walk. Finally, the button sensor (i.e., detected value of how much the button is pressed) is also offset by a random walk. Each of the three button affects the corresponding light on the desk. Additionally, pressing the green button also shifts the TV screen content to a green hue. Following RoboDesk reward design, we reward the agent for (1) placing arm close to the button, (2) pressing the button, and (3) how green the TV screen content is. RoboDesk Joint Position Regression Datasets. To generate training and test set, we use four policies trained by state- space SAC at different stages of training (which is not related to any of the compared methods) and a uniform random actor, to obtain ﬁve policies of different qualities. For each policy, we sample 100 trajectories, each containing 1001 pairs (from 1000 interactions) of image observation and groundtruth joint position (of dimension 9). This leads to a total of 500.5 × 103 samples from each policy. From these, 100 × 103 samples are randomly selected as test set. Training sets of sizes 5 × 103, 10 × 103, 25 × 103, 50 × 103, 100 × 103, 150 × 103 are sampled from the rest. For all test sets and training sets, we enforce each policy to strictly contribute an equal amount. Denoised MDPs Operator Input Shape Kernel Size Stride Padding Operator Input Shape Kernel Size Stride Padding Input [3, 96, 96] — — — Input [input_size] Conv. + ReLU [k, 47, 47] Conv. + ReLU [2k, 22, 22] Conv. + ReLU [4k, 10, 10] Conv. + ReLU Conv. + ReLU [8k, 4, 4] [8k, 2, 2] 4 4 4 4 3 2 2 2 2 1 0 0 0 0 0 Reshape + FC [m] — — — FC + ReLU + Reshape Conv. Transpose + ReLU Conv. Transpose + ReLU [m, 1, 1] [4k, 3, 3] [4k, 9, 9] Conv. Transpose + ReLU [2k, 21, 21] Conv. Transpose + ReLU Conv. Transpose + ReLU [k, 46, 46] [3, 96, 96] — — 5 5 5 6 6 — — 2 2 2 2 2 — — 0 0 0 0 0 Table 3: Encoder architecture for 96 × 96-resolution observa- tion. The output of this encoder is then fed to other network for inferring posteriors. m and k are two architectural hyper- parameters. m controls the output size (unrelated to the actual latent variable sizes). k controls the network width. Table 4: Decoder architecture for 96 × 96-resolution observation. m and k are two architectural hyperparameters. m controls width the fully connected part. k controls width of the convolutional part. They are the same values as in Table 3. B.1.2. MODEL LEARNING METHODS For all experiments, we let the algorithms use 106 environment steps. For PI-SAC and CURL, we follow the original implementations (Laskin et al., 2020b; Lee et al., 2020) and use an action repeat of 4 for Cheetah Run and Reacher Easy, and an action repeat of 2 for Walker Walk. For Denoised MDP, Dreamer, TIA and DBC, we always use an action repeat of 2, following prior works (Hafner et al., 2019a; Fu et al., 2021; Zhang et al., 2020). Denoised MDP, Dreamer, and TIA. Both Dreamer and TIA use the same training schedule and the Recurrent State-Space Model (RSSM) as the base architecture (Hafner et al., 2019b). Following them, Denoised MDP also uses these components, and follow the same preﬁlling and training schedule (see Dreamer (Hafner et al., 2019b) for details). These three model learning methods take in 64 × 64 RGB observations for DMC, and 96 × 96 RGB observations for RoboDesk. Dreamer only implements encoder and decoder for the former resolution. To handle the increased resolution, we modify the 64 × 64 architectures and obtain convolutional encoder and decoder shown in Tables 3 and 4. For fair comparison, we ensure that each method has roughly equal number of parameters by using different latent variable sizes, encoder output sizes (m of Table 3) and convolutional net widths (k of Table 4). Details are shown in Table 5. KL clipping (free nats). For Denoised MDP, we follow Dreamer (Hafner et al., 2019b;a) and TIA (Fu et al., 2021), and allow 3 free nats for the LKL-x term. In other words, for each element of a batch, we do not optimize the KL term if it is less than 3 (e.g., implemented via clipping). However, we do not allow this for the LKL-y and LKL-z terms, as these variables are to be discarded and information is not allowed to hide in them unless permitted by the structure. An alternative strategy, + (1 − β) · LKL-x which we ﬁnd also empirically effective, is to consider LKL-x = β · LKL-x , and to allow free nats only for (cid:124) (cid:125) (cid:123)(cid:122) (cid:124) (cid:125) (cid:123)(cid:122) VAE KL term MI regularizer term the ﬁrst term that is a part of the variational model ﬁtting objective. All results presented in this paper use the ﬁrst strategy. Both strategies are implemented in our open source code repository: github.com/facebookresearch/denoised_mdp. B.1.3. POLICY OPTIMIZATION ALGORITHMS USED WITH MODEL LEARNING Backpropagate via Dynamics. We use the same setting as Dreamer (Hafner et al., 2019a), optimizing a λ-return over 15-step-long rollouts with λ = 0.95, clipping gradients with norm greater than 100. TIA uses the same strategy, except that it groups different models together for gradient clipping. We strictly follow the ofﬁcial TIA implementation. Latent-Space SAC. We use the regular SAC with automatic entropy tuning, without gradient clipping. This works well for almost all settings, except for Walker Walk variant of DMC, where training often collapses after obtaining good return, regardless of the model learning algorithm. To address instability in this case, we reduce learning rates from 3 × 10−4 to 1 × 10−4 and clip gradients with norm greater than 100 for all latent-space SAC run on these variants. Denoised MDPs DMC RoboDesk Latent Sizes Dreamer TIA Denoised MDP (220 + 33) (120 + 20) + (120 + 20) (120 + 20) + (120 + 20) m 1024 490 1024 k 32 24 32 Total Number of Parameters Latent Sizes 7,479,789 7,475,567 7,478,826 (220 + 33) (120 + 20) + (120 + 20) (120 + 20) + (120 + 20) m 1024 490 1024 k 32 24 32 Total Number of Parameters 6,385,511 6,384,477 6,384,248 Table 5: The speciﬁc architecture parameters for model learning methods. Since RSSM uses a deterministic part and a stochastic part to represent each latent variable, we use (deterministic_size + stochastic_size) to indicate size of a latent variable. TIA and Denoised MDP have more than one latent variable. Note that while TIA has lower m and k, it has multiple encoder and decoders, whereas Dreamer and Denoised MDP only have one encoder and one decoder. The total number of parameters is measured with the actor model, but without any additional components from policy optimization algorithm (e.g., critics in SAC). Total number of parameters is lower for RoboDesk as the encoder and decoder architecture is narrower than those of DMC for the purpose of reducing memory usage, despite with a higher resolution. B.1.4. MODEL-FREE METHODS DBC. For DMC, we used 84 × 84-resolution observation following original work (even though other methods train on 64 × 64-resolution observations). For RoboDesk, DBC uses the encoder in Table 3 for 96 × 96-resolution observation, for fair comparison with other methods. Following the original work, we stack 3 consecutive frames to approximate the required full observability. In the robot arm joint position regression experiment Section 5.1, DBC encoders also see stacked observations. For DMC evaluations, we use the data provided by Zhang et al. wherever possible, and run the ofﬁcial repository for other cases. State-Space SAC. The state space usually contains robot joint states, including position, velocity, etc. For DMC, when Sensor Noise is present, this is not the true optimal state space, as we do not supply it with the noisy background that affects the noisy reward. However, it still works well in practice. For RoboDesk, the TV’s effect on reward is likely stronger and direct state-space SAC fails to learn. Since this evaluation is to obtain a rough “upper bound”, we train state-space SAC with a modiﬁed reward with less noise— the agent is rewarded by pressing the button, independent of the TV content. This still encourages the optimal strategy of the task allows achieving good policies. B.1.5. NON-RL METHODS Contrastive Learning. We used the Alignment+Uniformity contrastive learning loss from Wang & Isola (2020). The hyperparameters and data augmentations strictly follow their experiments on STL-10 (Coates et al., 2011), which also is of resolution 96 × 96. The exact loss form is Lalign(α = 2) + Luniform(t = 2), a high-performance setting for STL-10. B.2. Compute Resources All our experiments are run on a single GPU, requiring 8GB memory for DMC tasks, and 16GB memory for RoboDesk tasks. We use NVIDIA GPUs of the following types: 1080 Ti, 2080 Ti, 3080 Ti, P100, V100, Titan XP, Titan RTX. For MuJoCo (Todorov et al., 2012), we use the EGL rendering engine. Training time required for each run heavily depends on the CPU speciﬁcation and availability. In general, a Denoised MDP run needs 12 ∼ 36 hours on DMC and 24 ∼ 50 hours on RoboDesk. TIA uses about 1.5× of these times, due to the adversarial losses. For a comparison between the two Denoised MDP variants, running the same DMC task on the same machine, the Figure 2b variant used 23 hours while the Figure 2c variant used 26 hours. B.3. Visualization Details Visualizations of components in learned models. We use different methods to visualize signal and noise information learned by TIA and Denoised MDP in Figures 4 and 7. For TIA, we used the reconstructions from the two latent (before mask-composing them together as the full reconstruction). For Denoised MDP, we only have one decoder (instead of three for TIA), and thus we decode (xt, const) and (const, yt) to visualize information contained in each variable, with const chosen by visual clarity (usually as value of the other variable at a ﬁxed timestep). Due to the fundamental different ways to obtain these visualizations, in DMC, TIA can prevent the agent from showing up in noise visualizations, while Denoised Denoised MDPs Figure 8: Effect of weight decay on RoboDesk joint position regres- sion. The curves show ﬁnal test MSE for various training set sizes. Weight decay generally helps when ﬁnetuning from a pretrained encoder, but hurts when training from scratch. Figure 9: Performance of all TIA settings on RoboDesk joint position regression. Only using the signal encoder is necessary for good performance. Figure 10: Training curve comparisons for the RoboDesk joint position regression task across many training set sizes. MDP cannot. However, as stated in Section 5.2, our focus should be on what evolves/changes in these images, rather than what is visually present, as static components are essentially not modelled by the corresponding transition dynamics. Visualizations in Figures 4 and 7 use trajectories generated by a policy trained with state-space SAC. To obtain diverse behaviors, policy outputs are randomly perturbed before being used as actions. From the same trajectory, we use the above described procedure to obtain visualizations. The speciﬁc used trajectory segments are chosen to showcase both the modiﬁed environment and representative behavior of each method. Please see the supplementary video for clearer visualizations. B.4. RoboDesk Result Details Environment modiﬁcations. The agent controls a robotic arm placed in front of a desk and a TV, and is tasked to push down the green button on the desk, which turns on a small green light and makes the TV display have a green hue. The intensity of the TV image’s green channel is given to the agent as part of their reward, in addition to distance between the arm to the button, and how much the button is pressed. Additionally, the environment contains other noise distractors, including moveable blocks on the desk (Ctrl + Rew), ﬂickering environment light and camera jittering (Ctrl + Rew), TV screen hue (Ctrl + Rew), TV content (Ctrl + Rew), and noisy button sensors (Ctrl + Rew). RoboDesk has roughly twice as many pixels as DMC has. For Denoised MDP, we Denoised MDP hyperparameters. scale α with the observation space dimensionality (see Section 3) and use α = 2, with a ﬁxed β = 0.125. When using the alternative KL free nats strategy discussed in Appendix B.1.2 (results not shown in paper), we ﬁnd α = 1 and β = 0.25 also effective. TIA hyperparameters. We follow recommendations in the TIA paper, setting λRadv = 25,000 to match reconstruction loss in magnitude, and setting λOs = 2 where training is stable. B.4.1. ROBOT ARM JOINT POSITION REGRESSION. Training details. For this task, we jointly train the pre-trained backbone and a three-layer MLP head that has 256 hidden units at each layer, with a learning rate of 8 × 10−5. For ﬁnetuning from pretrained encoders, we follow common ﬁnetuning practice and apply a weight decay of 3 × 10−5 whenever it is helpful (all cases except CURL and training from scratch). E S M 10 1 t e S t s e T 10 2 E S M 10 1 t e S t s e T 10 2 Denoised MDP Dreamer TIA Contrastive With Weight Decay No Weight Decay From Scratch DBC (Stacked Frames) CURL (Stacked Frames) PI-SAC without Augmentation (Stacked Frames) 0.5 1.0 1.5 0.5 1.0 1.5 0.5 1.0 1.5 0.5 1.0 1.5 Training Set Size1e5 Training Set Size1e5 Training Set Size1e5 Training Set Size1e5 TIA With Weight Decay + Only Signal Encoder No Weight Decay + Only Signal Encoder With Weight Decay + Both Encoders No Weight Decay + Both Encoders 103 102 101 100 E S M t e S t s e T 10 1 10 2 0.2 0.4 0.6 0.8 1.0 Training Set Size 1.2 1.4 1e5 Learning Curve for 104 Training Samples Denoised MDP TIA Dreamer Contrastive From Scratch 0.25 0.24 0.23 0.22 0.21 0.20 0.19 0.18 0 20 40 60 80 100 Training Epoch Learning Curve for 150 × 104 Training Samples 0.40 0.30 0.20 0.100 0.09 0 20 40 60 80 100 Training Epoch Learning Curve for 50 × 104 Training Samples 0.24 0.22 0.20 0.18 0.16 0.14 0.12 0.100 0 20 40 60 80 100 Training Epoch Learning Curve for 150 × 104 Training Samples 0.20 0.100 0.09 0.08 0.07 0.06 0.05 0.04 0 20 40 60 80 100 Training Epoch Denoised MDPs Figure 11: Performance comparison of ﬁnetuning from Denoised MDP encoders and frame-stacked encoders that take in 3 consec- utive frames. For Denoised MDP and training from scratch, the encoders take in only a single frame and are applied for each of the frame, with output concatenated together before feeding to the prediction head. Figure 12: Performance of all DBC settings on RoboDesk joint position regression. Using the output features (after layer normal- ization) is necessary for good performance. Figure 13: Performance of all CURL settings on RoboDesk joint position regression. Using the output features (after layer normal- ization) is necessary for good performance. Figure 14: Performance of all PI-SAC settings on RoboDesk joint position regression. Using the activations before layer normaliza- tion gives best performance. See Figure 8 for comparisons for weight decay options over all methods. • For model-based RL, we take encoders trained with backpropagating via dynamics as the policy optimization algorithm. • In training the contrastive encoder, for a (more) fair comparison with RL-trained encoders that are optimized over 106 environment steps, we train contrastive encoders on 106 samples, obtained in the exact same method of the training sets of this task. In a sense, these contrastive encoders have the advantage of training on the exact same distribution, and seeing more samples (since RL-trained encoders use action repeat of 2 and thus only ever see 0.5 × 106 samples). • TIA has two sets of encoders. Using concatenated latents from both unfortunately hurts performance greatly (see Figure 9). So we use only the encoder for the signal latent. We also compare training speeds over a wide range of training set sizes in Figure 10. Denoised MDP encoders lead to faster and better training in all settings. Additional comparison with frame-stacking encoders. Other pretrained encoders (DBC, CURL and PI-SAC) take in stacked 3 consecutive frames, and are not directly comparable with the other methods. To compare, we also try running Denoised MDP encoders on the 3 consecutive frames, whose feature vector is concatenated before feeding into the head. The result in Figure 11 shows that our encoder outperforms all but PI-SAC encoders. Finally, for DBC, CURL and PI-SAC, we attempted evaluating intermediate features, features before the ﬁnal layer normalization, and the output space, and ﬁnd the last option best-performing for DBC and CURL, and the second option best-performing for PI-SAC (see Figures 12 to 14). Therefore, we use these respective spaces, which arguably gives a further edge to these methods, as we essentially tune this additional option on test results. Notably, these respective choices are often the only one achieving relatively good performance, highlighting the necessity of tuning for these methods. 10 1 E S M t e S t s e T 10 2 Ours (Stacked Frames) Ours (Single Frame) DBC (Stacked Frames) PI-SAC (Stacked Frames) CURL (Stacked Frames) From Scratch (Stacked Frames) 0.2 0.4 0.6 0.8 Training Set Size 1.0 1.2 1.4 1e5 DBC With Weight Decay, Output Features No Weight Decay, Output Features With Weight Decay, No Layer Norm No Weight Decay, No Layer Norm With Weight Decay, Conv Features No Weight Decay, Conv Features 103 102 101 100 E S M t e S t s e T 10 1 0.2 0.4 0.6 0.8 1.0 Training Set Size 1.2 1.4 1e5 CURL With Weight Decay, Output Features No Weight Decay, Output Features With Weight Decay, No Layer Norm No Weight Decay, No Layer Norm With Weight Decay, Conv Features No Weight Decay, Conv Features 103 102 101 100 E S M t e S t s e T 10 1 10 2 0.2 0.4 0.6 0.8 Training Set Size 1.0 1.2 1.4 1e5 PI-SAC 10 1 E S M t e S t s e T 10 2 With Weight Decay, Output Features (Stacked Frames) No Weight Decay, Output Features (Stacked Frames) With Weight Decay, No Layer Norm (Stacked Frames) No Weight Decay, No Layer Norm (Stacked Frames) With Weight Decay, Conv Features (Stacked Frames) No Weight Decay, Conv Features (Stacked Frames) 0.2 0.4 0.6 0.8 Training Set Size 1.0 1.2 1.4 1e5 B.5. DeepMind Control Suite (DMC) Result Details Denoised MDPs Full policy optimization results. Figure 15 presents the full results on each DMC environment (task + variant). For environment, a comparison plot is made based on which policy learning algorithm is used with the model learning method (with model-free baselines duplicated in both). Such separation is aimed to highlight the performance difference caused by model structure (rather than policy learning algorithm). Across most noisy environments, Denoised MDP performs the best. It also achieves high return on noiseless environments. Visualization of learned models. Figure 16 is the extended version of Figure 7 in main text, with full reconstructions from all three models. Please see the supplementary video for clearer visualizations. Comparison between Denoised MDP variants. We compare the two Denoised MDP variants based Figures 2b and 2c on Cheetah Run environments with policy trained by packpropagating via learned dynamics. The comparison is shown in the top row of Figure 15, where we see the Figure 2b variant often performing a bit better. We hypothesize that this may due to the more complex prior and posterior structure of Figure 2c, which may not learn as efﬁciently. This also makes Figure 2c variant needing longer (wall-clock) time to optimize, as mentioned above in Appendix B.2. TIA hyperparameters and instability. We strictly follow recommendations of the original paper, and use their suggested value for each DMC task. We also note that TIA runs sometimes collapse during training, leading to sharp drops in rewards. After closely inspecting the models before and after collapses, we note that in many cases, such collapses co-occur with sudden spikes in TIA’s reward disassociation loss, which is implemented as an adversarial minimax loss, and the noise latent space instantly becomes degenerate (i.e., not used in reconstruction). We hypothesize that this adversarial nature can cause training instability. However, a few collapses do not co-occur with such loss spikes, which maybe alternatively due to that TIA model structure cannot model the respective noise types and that better ﬁtting the model naturally means a degenerate noise latent space. PI-SAC hyperparameters. For each task, we use the hyperparameters detailed in the original paper (Lee et al., 2020). PI-SAC is usually run with augmentations. However, unlike CURL, augmentation is not an integral part of the PI-SAC algorithm and is completely optional. For a fair comparisons with other methods and to highlight the effect of the predictive information regularizer, the main mechanism proposed by PI-SAC, we do not use augmentations for PI-SAC. Denoised MDP hyperparameters. For DMC, we always use ﬁxed α = 1. β can be tune according to amount of noises in environment, and to training stability. In Figure 17, we compare effects of choosing different β’s. On noiseless environments, larger β (i.e., less regularization) performs often better. Whereas on noisy environments, sometimes stronger regularization can boost performance. However, overall good performance can be obtained by usually several β values. In Table 6, we summarize our β choices for each environment in Table 6. Denoised MDPs Figure 15: Policy optimization results on DMC. Each plot focuses on a single task variant, showing total episode return versus environment steps taken. For three model-based approaches, we use two policy optimization choices to train on the learned model: (top half) backpropagate via learned dynamics and (bottom half) SAC on the learned MDP. We also compare with DBC, a model-free baseline. For an “upper bound” (not plotted due to presentation clarity), SAC on true state-space (i.e., optimal representation) in 106 environment steps reaches episode return ≈ 800 on Cheetah Run variants, ≈ 980 on Walker Walk variants, and ≈ 960 on Reacher Easy variants. CURL’s speciﬁc augmentation choice (random crop) potentially helps signiﬁcantly for Reacher Easy (where the reacher and the target appear in random spatial locations) and Camera Jittering. However, unlike Denoised MDP, it does not generally perform well across all environments and noise variants. n o i t a z i m i t p O y c i l o P e t a g a p o r p k c a B s c i m a n y D a v i n o i t a z i m i t p O y c i l o P ) e c a p S - t n e t a L ( C A S Denoised MDPs Figure 16: Complete visualization of the different DMC variants and factorizations learned by TIA and Denoised MDP. In addition to visualizations of Figure 7, we also visualize full reconstructions from Dreamer, TIA, and Denoised MDP. Cheetah Run Noiseless Reacher Easy Video Background Walker Walk Video Background + Noisy Sensor Cheetah Run Video Background + Camera Jittering Env. Rollout Obs. Reward Dreamer Recon. Recon. TIA Signal Noise Recon. Denoised MDP Signal Noise Denoised MDPs Figure 17: Effect of choosing β in Denoised MDP on DMC policy optimization results. Setting β = 1 disables regularization and is only run on noiseless variants. Noiseless Video Background Video Background + Noisy Sensor Video Background + Camera Jittering Policy Learning: Backprop via Dynamics Policy Learning: SAC (Latent-Space) Cheetah Run Walker Walk Reacher Easy Cheetah Run Walker Walk Reacher Easy 1 1 1 1 1 1 0.125 0.25 0.25 0.125 0.25 0.125 0.25 0.25 0.25 0.125 0.125 0.25 0.25 0.5 0.25 0.25 0.5 0.25 Table 6: β choices for Denoised MDP results shown in Table 1 and Figure 15. We choose β = 1 (i.e., disabling regularization) for all noiseless environments, and tuned others. However, as seen in Figure 17, the results often are not too sensitive to small β changes. n o i t a z i m i t p O y c i l o P e t a g a p o r p k c a B s c i m a n y D a v i n o i t a z i m i t p O y c i l o P ) e c a p S - t n e t a L ( C A S",1
"
        We’re Training AI Twice as Fast This Year as Last
    ",https://spectrum.ieee.org/mlperf-rankings-2022,2022-06-30,,"New MLPerf rankings show training times plunging Google’s cloud based TPU v4 Pods turned in some impressive results. According to the best measures we’ve got, a set of benchmarks called MLPerf, machine-learning systems can be trained nearly twice as quickly as they could last year. It’s a figure that outstrips Moore’s Law, but also one we’ve come to expect. Most of the gain is thanks to software and systems innovations, but this year also gave the first peek at what some new processors, notably from Graphcore and Intel subsidiary Habana Labs, can do. The once-crippling time it took to train a neural network to do its task is the problem that launched startups like Cerebras and SambaNova and drove companies like Google to develop machine-learning accelerator chips in house. But the new MLPerf data shows that training time for standard neural networks has gotten a lot less taxing in a short period of time. And that speedup has come from much more than just the advance of Moore’s Law. Neural networks can now be trained much faster than what you would expect just from the march of Moore’s Law.ML Commons This capability has only incentivized machine-learning experts to dream big. So the size of new neural networks continues to outpace computing power. Called by some “the Olympics of machine learning,” MLPerf consists of eight benchmark tests: image recognition, medical-imaging segmentation, two versions of object detection, speech recognition, natural-language processing, recommendation, and a form of gameplay called reinforcement learning. (One of the object-detection benchmarks was updated for this round to a neural net that is closer to the state of the art.) Computers and software from 21 companies and institutions compete on any or all of the tests. This time around, officially called MLPerf Training 2.0, they collectively submitted 250 results. Very few commercial and cloud systems were tested on all eight, but Nvidia director of product development for accelerated computing Shar Narasimhan gave an interesting example of why systems should be able to handle such breadth: Imagine a person with a smartphone snapping a photo of a flower and asking the phone: “What kind of flower is this?” It seems like a single request, but answering it would likely involve 10 different machine-learning models, several of which are represented in MLPerf. To give a taste of the data, for each benchmark we’ve listed the fastest results for commercially available computers and cloud offerings (Microsoft Azure and Google Cloud) by how many machine-learning accelerators (usually GPUs) were involved. Keep in mind that some of these will be a category of one. For instance, there really aren’t that many places that can devote thousands of GPUs to a task. Likewise, there are some benchmarks where systems beat their nearest competitor by a matter of seconds or where five or more entries landed within a few minutes of each other. So if you’re curious about the nuances of AI performance, check out the complete list. [Or, click here to skip past the data and hear about some of the new stuff from Google, Graphcore, Intel, and Hazy Research. We won’t judge.] As usual, systems built using Nvidia A100 GPUs dominated the results. Nvidia’s new GPU architecture, Hopper, was designed with architectural features aimed at speeding training. But it was too new for this set of results. Look for some systems based on the Hopper H100–based systems in upcoming contests. For Nvidia’s take on the results see this blog post. Google’s TPU v4 offers a three-fold improvement in computations per watt over its predecessor, the company says. Google noted that two of its tests were done using what it calls a “full TPU v4 pod”—a system consisting of 4,096 chips, for a total of up to 1.1 billion billion operations per second. At that scale, the system ripped through the image-recognition and natural-language-processing trainings in just over 10 seconds each. Because it’s a cloud service, Google’s machine-learning system is available around the world. But the company wants you to know that the machines are actually located in Oklahoma. Why? Because that’s where it’s built a data center that operates on 90 percent carbon-free energy. For Google’s take on its results see the Google Cloud’s blog post. Graphcore presented the first performance results from computers built with its new Bow IPU. Bow is the first commercial processor built by stacking two silicon wafers atop each other. In its current iteration, one of the chips in the stack does no computing; instead, it delivers power in such a way that the chip runs up to 40 percent faster using as much as 16 percent less energy compared to its predecessor. MLPerf 2.0 was the first opportunity to see how that translated to real neural nets. The chips didn’t disappoint, showing a 26 to 31 percent speedup for image recognition and a 36 to 37 percent boost at natural-language processing. Graphcore executives say to expect more. The company is planning a future IPU, where both chips in the 3D stack do computing. Such a future processor would be combined into an 8,192-IPU supercomputer called the Good Computer, capable of handling neural networks 1,000 times or more as large as today’s biggest language models. Graphcore also touted what’s essentially a beneficial nonresult. Neural networks are constructed using frameworks that make the development job way easier. Pytorch and Tensorflow are commonly used open-source frameworks in North America and Europe, but in China Baidu’s PaddlePaddle is popular, according Graphcore executives. Seeking to satisfy clients and potential customers there, they showed that using PaddlePaddle or Graphcore’s in-house framework popART makes essentially no difference to training time. Intel subsidiary Habana Labs’ Gaudi2 put up some winning systems in this set of results. And Habana’s Eitan Medina says to expect a lot more from Gaudi2 in future tests. The company didn’t have time to put all of Gaudi2’s architectural features through their paces in time for this round. One possibly important feature is the use the of low-precision numbers in parts of the training process, similar to what Nvidia’s H100 promises. “With Gaudi2, there’s still lots of performance to squeeze out,” says Medina. Performing MLPerf benchmarks is no easy task, and often involves the work of many engineers. But a single graduate student, with some consultation, can do it, too. Tri Dao was that graduate student. He’s member of Hazy Research, the nom de guerre of Chris Re’s laboratory at Stanford. (Re is one of the founders of AI giant SambaNova.) Dao, Re, and other colleagues came up with a way to speed up the training of so-called attention-based networks, also called transformer networks. Among the MLPerf benchmarks, the natural-language-processing network BERT is the transformer, but the concept of “attention” is at the heart of very large language models such as GPT3. And it’s starting to show up in other machine-learning applications, such as machine vision. In attention networks, the length of the sequence of data the network works on is crucial to its accuracy. (Think of it as how many words a natural-language processor is aware of at once or how large an image a machine vision system can look at.) However, that length doesn’t scale up well. Double its size and you’re quadrupling the scale of the attention layer of the network, Dao explains. And this scaling problem shows up in training time because of all the instances when the network needs to write to system memory. Dao and his colleagues came up with an algorithm that gives the training process an awareness of this time penalty and a way to minimize it. Dao applied the lab’s “flash attention” algorithm to BERT using an 8-GPU system in the Microsoft Azure cloud, shaving almost 2 minutes (about 10 percent) off of Microsoft’s best effort. “Chris [Re] calls MLPerf ‘the Olympics of machine-learning performance.’ ” says Dao. And “even on the most competitive benchmark we were able to give a speedup.” Look for Re’s group to put flash attention to use in other transformer models soon. For more on the algorithm see their blog post about it. Computer-vision systems use shapes, colors, and even labels to identify materials at superhuman speeds It’s Tuesday night. In front of your house sits a large blue bin, full of newspaper, cardboard, bottles, cans, foil take-out trays, and empty yogurt containers. You may feel virtuous, thinking you’re doing your part to reduce waste. But after you rinse out that yogurt container and toss it into the bin, you probably don’t think much about it ever again. The truth about recycling in many parts of the United States and much of Europe is sobering. Tomorrow morning, the contents of the recycling bin will be dumped into a truck and taken to the recycling facility to be sorted. Most of the material will head off for processing and eventual use in new products. But a lot of it will end up in a landfill.",1
"
        Meta’s AI Takes an Unsupervised Step Forward
    ",https://spectrum.ieee.org/unsupervised-learning-meta,2022-06-29,,"In the quest for human-level intelligent AI, Meta is betting on self-supervised learning Meta AI’s masked auto-encoder for computer vision was trained on images that were mostly obscured [left]. Yet its reconstructions [center] were remarkably close to the original images [right]. Meta’s chief AI scientist, Yann LeCun, doesn’t lose sight of his far-off goal, even when talking about concrete steps in the here and now. “We want to build intelligent machines that learn like animals and humans,” LeCun tells IEEE Spectrum in an interview. Today’s concrete step is a series of papers from Meta, the company formerly known as Facebook, on a type of self-supervised learning (SSL) for AI systems. SSL stands in contrast to supervised learning, in which an AI system learns from a labeled data set (the labels serve as the teacher who provides the correct answers when the AI system checks its work). LeCun has often spoken about his strong belief that SSL is a necessary prerequisite for AI systems that can build “world models” and can therefore begin to gain humanlike faculties such as reason, common sense, and the ability to transfer skills and knowledge from one context to another. The new papers show how a self-supervised system called a masked auto-encoder (MAE) learned to reconstruct images, video, and even audio from very patchy and incomplete data. While MAEs are not a new idea, Meta has extended the work to new domains. By figuring out how to predict missing data, either in a static image or a video or audio sequence, the MAE system must be constructing a world model, LeCun says. “If it can predict what’s going to happen in a video, it has to understand that the world is three-dimensional, that some objects are inanimate and don’t move by themselves, that other objects are animate and harder to predict, all the way up to predicting complex behavior from animate persons,” he says. And once an AI system has an accurate world model, it can use that model to plan actions. “Images, which are signals from the natural world, are not constructed to remove redundancy. That’s why we can compress things so well when we create JPGs.”—Ross Girshick, Meta “The essence of intelligence is learning to predict,” LeCun says. And while he’s not claiming that Meta’s MAE system is anything close to an artificial general intelligence, he sees it as an important step. Not everyone agrees that the Meta researchers are on the right path to human-level intelligence. Yoshua Bengio is credited, in addition to his co–Turing Award winners LeCun and Geoffrey Hinton, with the development of deep neural networks, and he sometimes engages in friendly sparring with LeCun over big ideas in AI. In an email to IEEE Spectrum, Bengio spells out both some differences and similarities in their aims. “I really don’t think that our current approaches (self-supervised or not) are sufficient to bridge the gapto human-level intelligence,” Bengio writes. He adds that “qualitative advances” in the field will be needed to really move the state of the art anywhere closer to human-scale AI. While he agrees with LeCun that the ability to reason about the world is a key element of intelligence, Bengio’s team isn’t focused on models that can predict, but rather those that can render knowledge in the form of natural language. Such a model “would allow us to combine these pieces of knowledge to solve new problems, perform counterfactual simulations, or examine possible futures,” he notes. Bengio’s team has developed a new neural-net framework that has a more modular nature than those favored by LeCun, whose team is working on end-to-end learning (models that learn all the steps between the initial input stage and the final output result). Meta’s MAE work builds on a trend toward a type of neural network architecture called transformers. Transformers were first adopted in natural-language processing, where they caused big jumps in performance for models like Google’s BERT and OpenAI’s GPT-3. Meta AI researcher Ross Girshick says that transformers’ success with language caused people in the computer-vision community to “work feverishly to try to replicate those results” in their own field. Meta’s researchers weren’t the first to successfully apply transformers to visual tasks; Girshick says that Google research on a Vision Transformer (ViT) inspired the Meta team. “By adopting the ViT architecture, it eliminated obstacles that had been standing in the way of experimenting with some ideas,” he tells Spectrum. Girshick coauthored Meta’s first paper on MAE systems, which dealt with static images. Its training was analogous to how BERT and other language transformers are trained. Such language models are shown huge databases of text with some fraction of the words missing, or “masked.” The models try to predict the missing words, and then the missing text is unmasked so the models can check their work, adjust their parameters, and try again with a new chunk of text. To do something similar with vision, Girshick explains, the team broke up images into patches, masked some of the patches, and asked the MAE system to predict the missing parts of the images. One of the team’s breakthroughs was the realization that masking a large proportion of the image gave the best results—a key difference from language transformers, where perhaps 15 percent of the words might be masked. “Language is an extremely dense and efficient communication system,” says Girshick. “Every symbol has a lot of meaning packed in. But images, which are signals from the natural world, are not constructed to remove redundancy. That’s why we can compress things so well when we create JPG images,” he notes. Meta AI researchers experimented with how much of the images to mask to get the best results.Meta By masking more than 75 percent of the patches in an image, Girshick explains, they remove the redundancy from the image that would otherwise make the task too trivial for training. Their two-part MAE system first uses an encoder that learns relationships between pixels across the training data set, then a decoder does its best to reconstruct original images from the masked versions. After this training regimen is complete, the encoder can also be fine-tuned for vision tasks such as classification and object detection. “The reason why ultimately we’re excited is the results we see in transfer learning to downstream tasks,” says Girshick. When using the encoder for tasks such as object recognition, he says, “we’re seeing gains that are really substantial; they move the needle.” He notes that scaling up the model led to better performance, which is a promising sign for future models because SSL “has the potential to use a lot of data without requiring manual annotation.” Going all-in for learning on massive uncurated data sets may be Meta’s tactic for improving results in SSL, but it’s also an increasingly controversial approach. AI ethics researchers such as Timnit Gebru have called attention to the biases inherent in the uncurated data sets that large language models learn from, sometimes with disastrous results. In the MAE system for video, the masking obscured up to 95 percent of each video frame, because the similarities between frames meant that video signals have even more redundancy than static images. One big advantage of the MAE approach when it comes to video, says Meta researcher Christoph Feichtenhofer, is that video is typically very computationally demanding. But by masking up to 95 percent of each frame, MAE reduces the computational cost by up to 95 percent, he says. The clips used in these experiments were only a few seconds long, but Feichtenhofer says that training an AI system on longer videos is “a very active research topic.” Imagine, he says, a virtual assistant who has a video feed of your house and can tell you where you left your keys an hour ago. (Whether you consider that possibility amazing or creepy, rest assured that it’s quite far off.) More immediately, one can imagine both the image and video systems being useful for the kind of classification tasks that are needed for content moderation on Facebook and Instagram, and Feichtenhofer says that “integrity” is one possible application. “We are definitely talking to product teams,” he says, “but it’s very new, and we don’t have any concrete projects yet.” For the audio MAE work, which the team says will soon be posted on the arXiv preprint server, the Meta AI team found a clever way to apply the masking technique. They turned the sound files into spectrograms, visual representations of the spectrum of frequencies within signals, and then masked parts of those images for training. The reconstructed audio is pretty impressive, though the model can currently handle clips of only a few seconds. Meta’s masked auto-encoder for audio was trained on heavily masked data, and was then able to reconstruct audio files with impressive fidelity. Bernie Huang, who worked on the audio system, says that potential applications include classification tasks, helping with voice over IP calls by filling in audio that gets lost when a packet gets dropped, or finding more efficient ways to compress audio files. Meta has been on something of an AI charm offensive, open-sourcing research such as these MAE models and offering up a pretrained large language model to the AI community for research purposes. But critics have noted that for all this openness on the research side, Meta has not made its core commercial algorithms available for study—those that control newsfeeds, recommendations, and ad placements. Eliza Strickland is a senior editor at IEEE Spectrum, where she covers AI, biomedical engineering, and other topics. She holds a master's degree in journalism from Columbia University. Computer-vision systems use shapes, colors, and even labels to identify materials at superhuman speeds It’s Tuesday night. In front of your house sits a large blue bin, full of newspaper, cardboard, bottles, cans, foil take-out trays, and empty yogurt containers. You may feel virtuous, thinking you’re doing your part to reduce waste. But after you rinse out that yogurt container and toss it into the bin, you probably don’t think much about it ever again. The truth about recycling in many parts of the United States and much of Europe is sobering. Tomorrow morning, the contents of the recycling bin will be dumped into a truck and taken to the recycling facility to be sorted. Most of the material will head off for processing and eventual use in new products. But a lot of it will end up in a landfill.",1
"
        Measuring AI’s Carbon Footprint
    ",https://spectrum.ieee.org/ai-carbon-footprint,2022-06-26,,"New tools track and reduce emissions from machine learning Machine-learning models are growing exponentially larger. At the same time, they require exponentially more energy to train, so that they can accurately process images or text or video. As the AI community grapples with its environmental impact, some conferences now ask paper submitters to include information on CO2 emissions. New research offers a more accurate method for calculating those emissions. It also compares factors that affect them, and tests two methods for reducing them. Several software packages estimate the carbon emissions of AI workloads. Recently a team at Université Paris-Saclay tested a group of these tools to see if they were reliable. “And they’re not reliable in all contexts,” says Anne-Laure Ligozat, a co-author of that study who was not involved in the new work. The new approach differs in two respects, says Jesse Dodge, a research scientist at the Allen Institute for AI and the lead author of the new paper, which he presented last week at the ACM Conference on Fairness, Accountability, and Transparency (FAccT). First, it records server chips’ energy usage as a series of measurements, rather than summing their use over the course of training. Second, it aligns this usage data with a series of data points indicating the local emissions per kilowatt-hour (kWh) of energy used. This number also changes continually. “Previous work doesn’t capture a lot of the nuance there,” Dodge says. The new tool is more sophisticated than older ones but still tracks only some of the energy used in training models. In a preliminary experiment, the team found that a server’s GPUs used 74% of its energy. CPUs and memory used a minority, and they support many workloads simultaneously, so the team focused on GPU usage. They also didn’t measure the energy used to build the computing equipment, or to cool the data center, or to build it and transport engineers to and from the facility. Or the energy used to collect data or run trained models. But the tool provides some guidance on ways to reduce emissions during training. “What I hope is that the vital first step towards a more green future and more equitable future is transparent reporting,” Dodge says. “Because you can’t improve what you can’t measure.” The researchers trained 11 machine-learning models of different sizes to process language or images. Training ranged from an hour on one GPU to eight days on 256 GPUs. They recorded energy used every second or so. They also obtained, for 16 geographical regions, carbon emissions per kWh of energy used throughout 2020, at five-minute granularity. Then they could compare emissions from running different models in different regions at different times. Powering the GPUs to train the smallest models emitted about as much carbon as charging a phone. The largest model contained six billion parameters, a measure of its size. While training it only to 13% completion, GPUs emitted almost as much carbon as does powering a home for a year in the United States. Meanwhile, some deployed models, such as OpenAI’s GPT-3, contain more than 100 billion parameters. Allen Institute on AI et al from FAccT 2022 The biggest measured factor in reducing emissions was geographical region: Grams of CO2 per kWh ranged from 200 to 755. Besides changing location, the researchers tested two CO2-reduction techniques, allowed by their temporally fine-grained data. The first, Flexible Start, could delay training up to 24 hours. For the largest model, which required several days of training, delaying it up to a day typically reduced emissions less than 1%, but for a much smaller model, such a delay could save 10–80%. The second, Pause and Resume, could pause training at times of high emissions, as long as overall training time didn’t more than double. This method benefited the small model only a few percent, but in half the regions it benefited the largest model 10–30%. Emissions per kWh fluctuate over time in part because, lacking sufficient energy storage, grids must sometimes rely on dirty power sources when intermittent clean sources such as wind and solar can’t meet demand. Ligozat found these optimization techniques the most interesting part of the paper. But they were based on retrospective data. Dodge says in the future, he’d like to be able to predict emissions per kWh so as to implement them in real time. Ligozat offers another way to reduce emissions: “The first good practice is just to think before running an experiment,” she says. “Be sure that you really need machine learning for your problem.” Matthew Hutson is a freelance writer who covers science and technology, with specialties in psychology and AI. He’s written for Science, Nature, Wired, The Atlantic, The New Yorker, and The Wall Street Journal. He’s a former editor at Psychology Today and is the author of The 7 Laws of Magical Thinking. Follow him on Twitter at @SilverJacket. Computer-vision systems use shapes, colors, and even labels to identify materials at superhuman speeds It’s Tuesday night. In front of your house sits a large blue bin, full of newspaper, cardboard, bottles, cans, foil take-out trays, and empty yogurt containers. You may feel virtuous, thinking you’re doing your part to reduce waste. But after you rinse out that yogurt container and toss it into the bin, you probably don’t think much about it ever again. The truth about recycling in many parts of the United States and much of Europe is sobering. Tomorrow morning, the contents of the recycling bin will be dumped into a truck and taken to the recycling facility to be sorted. Most of the material will head off for processing and eventual use in new products. But a lot of it will end up in a landfill.",0
"
        AI Can Help Make Recycling Better
    ",https://spectrum.ieee.org/single-stream-recycling,2022-06-25,,"But only humans can solve the plastics problem Garbage is a global problem that each of us contributes to. Since the 1970s, we've all been told we can help fix that problem by assiduously recycling bottles and cans, boxes and newspapers. So far, though, we haven’t been up to the task. Only 16 percent of the 2.1 billion tonnes of solid waste that the world produces every year gets recycled. The U.S. Environmental Protection Agency estimates that the United States recycled only about 32 percent of its garbage in 2018, putting the country in the middle of the pack worldwide. Germany, on the high end, captures about 65 percent, while Chile and Turkey barely do anything, recycling a mere 1 percent of their trash, according to a 2015 report by the Organization for Economic Cooperation and Development (OECD). Here in the United States, of the 32 percent of the trash that we try to recycle, about 80 to 95 percent actually gets recycled, as Jason Calaiaro of AMP Robotics points out in “AI Takes a Dumpster Dive.” The technology that Calaiaro’s company is developing could move us closer to 100 percent. But it would have no effect on the two-thirds of the waste stream that never makes it to recyclers. Certainly, the marginal gains realized by AI and robotics will help the bottom lines of recycling companies, making it profitable for them to recover more useful materials from waste. But to make a bigger difference, we need to address the problem at the beginning of the process: Manufacturers and packaging companies must shift to more sustainable designs that use less material or more recyclable ones. According to the Joint Research Centre of the European Commission, more than “80 percent of all product-related environmental impacts are determined during the design phase of a product.” One company that applies AI at the start of the design process is Digimind GmbH based in Berlin. As CEO Katharina Eissing told Packaging Europe last year, Digimind’s AI-aided platform lets package designers quickly assess the outcome of changes they make to designs. In one case, Digimind reduced the weight of a company’s 1.5-liter plastic bottles by 13.7 percent, a seemingly small improvement that becomes more impressive when you consider that the company produces 1 billion of these bottles every year. That’s still just a drop in the polyethylene terephthalate bucket: The world produced an estimated 583 billion PET bottles last year, according to Statista. To truly address our global garbage problem, our consumption patterns must change–canteens instead of single-use plastic bottles, compostable paper boxes instead of plastic clamshell containers, reusable shopping bags instead of “disposable” plastic ones. And engineers involved in product design need to develop packaging free of PET, polystyrene, and polycarbonate, which break down into tiny particles called microplastics that researchers are now finding in human blood and feces. As much as we may hope that AI can solve our problems for us, that’s wishful thinking. Human ingenuity got us into this mess and humans will have to regulate, legislate, and otherwise incentivize the private sector to get us out of it. Harry Goldstein is Acting Editor in Chief of IEEE Spectrum. Computer-vision systems use shapes, colors, and even labels to identify materials at superhuman speeds It’s Tuesday night. In front of your house sits a large blue bin, full of newspaper, cardboard, bottles, cans, foil take-out trays, and empty yogurt containers. You may feel virtuous, thinking you’re doing your part to reduce waste. But after you rinse out that yogurt container and toss it into the bin, you probably don’t think much about it ever again. The truth about recycling in many parts of the United States and much of Europe is sobering. Tomorrow morning, the contents of the recycling bin will be dumped into a truck and taken to the recycling facility to be sorted. Most of the material will head off for processing and eventual use in new products. But a lot of it will end up in a landfill.",0
"
        AI-Guided Robots Are Ready to Sort Your Recyclables
    ",https://spectrum.ieee.org/ai-guided-robots-are-ready-to-sort-your-recyclables,2022-06-25,,"Computer-vision systems use shapes, colors, and even labels to identify materials at superhuman speeds The Amp Cortex, a highspeed robotic sorting system guided by artificial intelligence, identifies materials by category on a conveyor belt. To date, systems in operation have recognized more than 50 billion objects in various permutations. It’s Tuesday night. In front of your house sits a large blue bin, full of newspaper, cardboard, bottles, cans, foil take-out trays, and empty yogurt containers. You may feel virtuous, thinking you’re doing your part to reduce waste. But after you rinse out that yogurt container and toss it into the bin, you probably don’t think much about it ever again. The truth about recycling in many parts of the United States and much of Europe is sobering. Tomorrow morning, the contents of the recycling bin will be dumped into a truck and taken to the recycling facility to be sorted. Most of the material will head off for processing and eventual use in new products. But a lot of it will end up in a landfill. So how much of the material that goes into the typical bin avoids a trip to landfill? For countries that do curbside recycling, the number—called the recovery rate—appears to average around 70 to 90 percent, though widespread data isn’t available. That doesn’t seem bad. But in some municipalities, it can go as low as 40 percent. What’s worse, only a small quantity of all recyclables makes it into the bins—just 32 percent in the United States and 10 to 15 percent globally. That’s a lot of material made from finite resources that needlessly goes to waste. We have to do better than that. Right now, the recycling industry is facing a financial crisis, thanks to falling prices for sorted recyclables as well as policy, enacted by China in 2018, which restricts the import of many materials destined for recycling and shuts out most recyclables originating in the United States. There is a way to do better. Using computer vision, machine learning, and robots to identify and sort recycled material, we can improve the accuracy of automatic sorting machines, reduce the need for human intervention, and boost overall recovery rates. My company, Amp Robotics, based in Louisville, Colo., is developing hardware and software that relies on image analysis to sort recyclables with far higher accuracy and recovery rates than are typical for conventional systems. Other companies are similarly working to apply AI and robotics to recycling, including Bulk Handling Systems, Machinex, and Tomra. To date, the technology has been installed in hundreds of sorting facilities around the world. Expanding its use will prevent waste and help the environment by keeping recyclables out of landfills and making them easier to reprocess and reuse. AMP Robotics Before I explain how AI will improve recycling, let’s look at how recycled materials were sorted in the past and how they’re being sorted in most parts of the world today. When recycling began in the 1960s, the task of sorting fell to the consumer—newspapers in one bundle, cardboard in another, and glass and cans in their own separate bins. That turned out to be too much of a hassle for many people and limited the amount of recyclable materials gathered. In the 1970s, many cities took away the multiple bins and replaced them with a single container, with sorting happening downstream. This “single stream” recycling boosted participation, and it is now the dominant form of recycling in developed countries. Moving the task of sorting further downstream led to the building of sorting facilities. To do the actual sorting, recycling entrepreneurs adapted equipment from the mining and agriculture industries, filling in with human labor as necessary. These sorting systems had no computer intelligence, relying instead on the physical properties of materials to separate them. Glass, for example, can be broken into tiny pieces and then sifted and collected. Cardboard is rigid and light—it can glide over a series of mechanical camlike disks, while other, denser materials fall in between the disks. Ferrous metals can be magnetically separated from other materials; magnetism can also be induced in nonferrous items, like aluminum, using a large eddy current. By the 1990s, hyperspectral imaging, developed by NASA and first launched in a satellite in 1972, was becoming commercially viable and began to show up in the recycling world. Unlike human eyes, which mostly see in combinations of red, green, and blue, hyperspectral sensors divide images into many more spectral bands. The technology’s ability to distinguish between different types of plastics changed the game for recyclers, bringing not only optical sensing but computer intelligence into the process. Programmable optical sorters were also developed to separate paper products, distinguishing, say, newspaper from junk mail. So today, much of the sorting is automated. These systems generally sort to 80 to 95 percent purity—that is, 5 to 20 percent of the output shouldn’t be there. For the output to be profitable, however, the purity must be higher than 95 percent; below this threshold, the value drops, and often it’s worth nothing. So humans manually clean up each of the streams, picking out stray objects before the material is compressed and baled for shipping. Despite all the automated and manual sorting, about 10 to 30 percent of the material that enters the facility ultimately ends up in a landfill. In most cases, more than half of that material is recyclable and worth money but was simply missed. We’ve pushed the current systems as far as they can go. Only AI can do better. Getting AI into the recycling business means combining pick-and-place robots with accurate real-time object detection. Pick-and-place robots combined with computer vision systems are used in manufacturing to grab particular objects, but they generally are just looking repeatedly for a single item, or for a few items of known shapes and under controlled lighting conditions.Recycling, though, involves infinite variability in the kinds, shapes, and orientations of the objects traveling down the conveyor belt, requiring nearly instantaneous identification along with the quick dispatch of a new trajectory to the robot arm. AI-based systems guide robotic arms to grab materials from a stream of mixed recyclables and place them in the correct bins. Here, a tandem robot system operates at a Waste Connections recycling facility [top], and a single robot arm [bottom] recovers a piece of corrugated cardboard. The United States does a pretty good job when it comes to cardboard: In 2021, 91.4 percent of discarded cardboard was recycled, according to the American Forest and Paper Association.AMP Robotics My company first began using AI in 2016 to extract empty cartons from other recyclables at a facility in Colorado; today, we have systems installed in more than 25 U.S. states and six countries. We weren’t the first company to try AI sorting, but it hadn’t previously been used commercially. And we have steadily expanded the types of recyclables our systems can recognize and sort. AI makes it theoretically possible to recover all of the recyclables from a mixed-material stream at accuracy approaching 100 percent, entirely based on image analysis. If an AI-based sorting system can see an object, it can accurately sort it. Consider a particularly challenging material for today’s recycling sorters: high-density polyethylene (HDPE), a plastic commonly used for detergent bottles and milk jugs. (In the United States, Europe, and China, HDPE products are labeled as No. 2 recyclables.) In a system that relies on hyperspectral imaging, batches of HDPE tend to be mixed with other plastics and may have paper or plastic labels, making it difficult for the hyperspectral imagers to detect the underlying object’s chemical composition. An AI-driven computer-vision system, by contrast, can determine that a bottle is HDPE and not something else by recognizing its packaging. Such a system can also use attributes like color, opacity, and form factor to increase detection accuracy, and even sort by color or specific product, reducing the amount of reprocessing needed. Though the system doesn’t attempt to understand the meaning of words on labels, the words are part of an item’s visual attributes. We at AMP Robotics have built systems that can do this kind of sorting. In the future, AI systems could also sort by combinations of material and by original use, enabling food-grade materials to be separated from containers that held household cleaners, and paper contaminated with food waste to be separated from clean paper. Training a neural network to detect objects in the recycling stream is not easy. It is at least several orders of magnitude more challenging than recognizing faces in a photograph, because there can be a nearly infinite variety of ways that recyclable materials can be deformed, and the system has to recognize the permutations. Chris Philpot Today’s recycling facilities use mechanical sorting, optical hyperspectral sorting, and human workers. Here’s what typically happens after the recycling truck leaves your house with the contents of your blue bin. Trucks unload on a concrete pad, called the tip floor. A front-end loader scoops up material in bulk and dumps it onto a conveyor belt, typically at a rate of 30 to 60 tonnes per hour. The first stage is the presort. Human workers remove large or problematic items that shouldn’t have made it onto collection trucks in the first place—bicycles, big pieces of plastic film, propane canisters, car transmissions. Sorting machines that rely on optical hyperspectral imaging or human workers separate fiber (office paper, cardboard, magazines—referred to as 2D products, as they are mostly flat) from the remaining plastics and metals. In the case of the optical sorters, cameras stare down at the material rolling down the conveyor belt, detect an object made of the target substance, and then send a message to activate a bank of electronically controllable solenoids to divert the object into a collection bin. The nonfiber materials pass through a mechanical system with densely packed camlike wheels. Large items glide past while small items, like that recyclable fork you thoughtfully deposited in your blue bin, slip through, headed straight for landfill—they are just too small to be sorted. Machines also smash glass, which falls to the bottom and is screened out. The rest of the stream then passes under overhead magnets, which collect items made of ferrous metals, and an eddy-current-inducing machine, which jolts nonferrous metals to another collection area. At this point, mostly plastics remain. More hyperspectral sorters, in series, can pull off plastics one type—like the HDPE of detergent bottles and the PET of water bottles—at a time. Finally, whatever is left—between 10 to 30 percent of what came in on the trucks—goes to landfill. In the future, AI-driven robotic sorting systems and AI inspection systems could replace human workers at most points in this process. In the diagram, red icons indicate where AI-driven robotic systems could replace human workers and a blue icon indicates where an AI auditing system could make a final check on the success of the sorting effort. It’s hard enough to train a neural network to identify all the different types of bottles of laundry detergent on the market today, but it’s an entirely different challenge when you consider the physical deformations that these objects can undergo by the time they reach a recycling facility. They can be folded, torn, or smashed. Mixed into a stream of other objects, a bottle might have only a corner visible. Fluids or food waste might obscure the material. We train our systems by giving them images of materials belonging to each category, sourced from recycling facilities around the world. My company now has the world’s largest data set of recyclable material images for use in machine learning. Using this data, our models learn to identify recyclables in the same way their human counterparts do, by spotting patterns and features that distinguish different materials. We continuously collect random samples from all the facilities that use our systems, and then annotate them, add them to our database, and retrain our neural networks. We also test our networks to find models that perform best on target material and do targeted additional training on materials that our systems have trouble identifying correctly. In general, neural networks are susceptible to learning the wrong thing. Pictures of cows are associated with milk packaging, which is commonly produced as a fiber carton or HDPE container. But milk products can also be packaged in other plastics; for example, single-serving milk bottles may look like the HDPE of gallon jugs but are usually made from an opaque form of the PET (polyethylene terephthalate) used for water bottles. Cows don’t always mean fiber or HDPE, in other words. There is also the challenge of staying up to date with the continual changes in consumer packaging. Any mechanism that relies on visual observation to learn associations between packaging and material types will need to consume a steady stream of data to ensure that objects are classified accurately. But we can get these systems to work. Right now, our systems do really well on certain categories—more than 98 percent accuracy on aluminum cans—and are getting better at distinguishing nuances like color, opacity, and initial use (spotting those food-grade plastics). Now thatAI-basedsystems are ready to take on your recyclables, how might things change? Certainly, they will boost the use of robotics, which is only minimally used in the recycling industry today. Given the perpetual worker shortage in this dull and dirty business, automation is a path worth taking. AI can also help us understand how well today’s existing sorting processes are doing and how we can improve them. Today, we have a very crude understanding of the operational efficiency of sorting facilities—we weigh trucks on the way in and weigh the output on the way out. No facility can tell you the purity of the products with any certainty; they only audit quality periodically by breaking open random bales. But if you placed an AI-powered vision system over the inputs and outputs of relevant parts of the sorting process, you’d gain a holistic view of what material is flowing where. This level of scrutiny is just beginning in hundreds of facilities around the world, and it should lead to greater efficiency in recycling operations. Being able to digitize the real-time flow of recyclables with precision and consistency also provides opportunities to better understand which recyclable materials are and are not currently being recycled and then to identify gaps that will allow facilities to improve their recycling systems overall. Sorting Robot Picking Mixed PlasticsAMP Robotics But to really unleash the power of AI on the recycling process, we need to rethink the entire sorting process. Today, recycling operations typically whittle down the mixed stream of materials to the target material by removing nontarget material—they do a “negative sort,” in other words. Instead, using AI vision systems with robotic pickers, we can perform a “positive sort.” Instead of removing nontarget material, we identify each object in a stream and select the target material. To be sure, our recovery rate and purity are only as good as our algorithms. Those numbers continue to improve as our systems gain more experience in the world and our training data set continues to grow. We expect to eventually hit purity and recovery rates of 100 percent. The implications of moving from more mechanical systems to AI are profound. Rather than coarsely sorting to 80 percent purity and then manually cleaning up the stream to 95 percent purity, a facility can reach the target purity on the first pass. And instead of having a unique sorting mechanism handling each type of material, a sorting machine can change targets just by a switch in algorithm. The use of AI also means that we can recover materials long ignored for economic reasons. Until now, it was only economically viable for facilities to pursue the most abundant, high-value items in the waste stream. But with machine-learning systems that do positive sorting on a wider variety of materials, we can start to capture a greater diversity of material at little or no overhead to the business. That’s good for the planet. We are beginning to see a few AI-based secondary recycling facilities go into operation, with Amp’s technology first coming online in Denver in late 2020. These systems are currently used where material has already passed through a traditional sort, seeking high-value materials missed or low-value materials that can be sorted in novel ways and therefore find new markets. Thanks to AI, the industry is beginning to chip away at the mountain of recyclables that end up in landfills each year—a mountain containing billions of tons of recyclables representing billions of dollars lost and nonrenewable resources wasted. This article appears in the July 2022 print issue as “AI Takes a Dumpster Dive .” Jason Calaiaro is director of hardware engineering for AMP Robotics, in Louisville, Colo. Before joining AMP, he founded Marble, now CAT Robotics, where he pioneered robots for last-mile delivery. He also developed aerial transportation drones at Matternet, the first FAA drone airline, and served as chief information officer and director of propulsion at Astrobotic Technology, which is slated to be the first private company to land on the moon in late 2022 Self-powered sensors convert neck strain into electrical pulses to detect head trauma in athletes The prototype patch in this research is shown in (a) on the left; on the right (b) is the kind of head rotation that can yield an electrical response from the patch. Nelson Sepúlveda was sitting in the stands at Spartan Stadium, watching his hometown Michigan State players bash heads with their cross-state football rivals from the University of Michigan, when he had a scientific epiphany. Perhaps the nanotechnologies he had been working on for years—paper-thin devices known as ferroelectret nanogenerators that convert mechanical energy into electrical energy—could help save these athletes from the ravages of traumatic brain injury. Your weekly selection of awesome robot videos Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. Enjoy today’s videos! Researchers at NYU have developed an AI solution that can leverage public video feeds to better inform decision makers Dexter Johnson is a contributing editor at IEEE Spectrum, with a focus on nanotechnology. This is a sponsored article brought to you by NYU’s Tandon School of Engineering. In the midst of the COVID-19 pandemic, in 2020, many research groups sought an effective method to determine mobility patterns and crowd densities on the streets of major cities like New York City to give insight into the effectiveness of stay-at-home and social distancing strategies. But sending teams of researchers out into the streets to observe and tabulate these numbers would have involved putting those researchers at risk of exposure to the very infection the strategies were meant to curb. Researchers at New York University’s (NYU) Connected Cities for Smart Mobility towards Accessible and Resilient Transportation (C2SMART) Center, a Tier 1 USDOT-funded University Transportation Center, developed a solution that not only eliminated the risk of infection to researchers, and which could easily be plugged into already existing public traffic camera feeds infrastructure, but also provided the most comprehensive data on crowd and traffic densities that had ever been compiled previously and cannot be easily detected by conventional traffic sensors.",0
"
        Quantum Error Correction: Time to Make It Work
    ",https://spectrum.ieee.org/quantum-error-correction,2022-06-26,,"If technologists can’t perfect it, quantum computers will never be big Dates chiseled into an ancient tombstone have more in common with the data in your phone or laptop than you may realize. They both involve conventional, classical information, carried by hardware that is relatively immune to errors. The situation inside a quantum computer is far different: The information itself has its own idiosyncratic properties, and compared with standard digital microelectronics, state-of-the-art quantum-computer hardware is more than a billion trillion times as likely to suffer a fault. This tremendous susceptibility to errors is the single biggest problem holding back quantum computing from realizing its great promise. Fortunately, an approach known as quantum error correction (QEC) can remedy this problem, at least in principle. A mature body of theory built up over the past quarter century now provides a solid theoretical foundation, and experimentalists have demonstrated dozens of proof-of-principle examples of QEC. But these experiments still have not reached the level of quality and sophistication needed to reduce the overall error rate in a system. The two of us, along with many other researchers involved in quantum computing, are trying to move definitively beyond these preliminary demos of QEC so that it can be employed to build useful, large-scale quantum computers. But before describing how we think such error correction can be made practical, we need to first review what makes a quantum computer tick. Information is physical. This was the mantra of the distinguished IBM researcher Rolf Landauer. Abstract though it may seem, information always involves a physical representation, and the physics matters. Conventional digital information consists of bits, zeros and ones, which can be represented by classical states of matter, that is, states well described by classical physics. Quantum information, by contrast, involves qubits—quantum bits—whose properties follow the peculiar rules of quantum mechanics. Polarized light is an example of superposition. A classical binary digit could be represented by encoding 0 as horizontally (H) polarized light, and 1 as vertically (V) polarized light. Light polarized at other angles has components of both H and V, representing 0 and 1 simultaneously. Examples include the diagonal (D) polarization at 45°, the antidiagonal (A) at –45°, as well as right (R) and left (L) circularly polarized light (the imaginary number i represents a difference in phase). These states become fully fledged quantum bits (qubits) when they consist of pulses that each contain a single photon. The possible states of a single isolated qubit [blue arrow] are neatly represented on a sphere, known as a Bloch sphere. The states 0 and 1 sit at the north and south poles, and the polarization states D, A, R, and L lie on the equator. Other possible superpositions of 0 and 1 (described by complex numbers a and b) cover the rest of the surface. Noise can make the qubit state wander continuously from its correct location. A classical bit has only two possible values: 0 or 1. A qubit, however, can occupy a superposition of these two information states, taking on characteristics of both. Polarized light provides intuitive examples of superpositions. You could use horizontally polarized light to represent 0 and vertically polarized light to represent 1, but light can also be polarized on an angle and then has both horizontal and vertical components at once. Indeed, one way to represent a qubit is by the polarization of a single photon of light. These ideas generalize to groups of n bits or qubits: n bits can represent any one of 2n possible values at any moment, while n qubits can include components corresponding to all 2n classical states simultaneously in superposition. These superpositions provide a vast range of possible states for a quantum computer to work with, albeit with limitations on how they can be manipulated and accessed. Superposition of information is a central resource used in quantum processing and, along with other quantum rules, enables powerful new ways to compute. Researchers are experimenting with many different physical systems to hold and process quantum information, including light, trapped atoms and ions, and solid-state devices based on semiconductors or superconductors. For the purpose of realizing qubits, all these systems follow the same underlying mathematical rules of quantum physics, and all of them are highly sensitive to environmental fluctuations that introduce errors. By contrast, the transistors that handle classical information in modern digital electronics can reliably perform a billion operations per second for decades with a vanishingly small chance of a hardware fault. Of particular concern is the fact that qubit states can roam over a continuous range of superpositions. Polarized light again provides a good analogy: The angle of linear polarization can take any value from 0 to 180 degrees. Pictorially, a qubit’s state can be thought of as an arrow pointing to a location on the surface of a sphere. Known as a Bloch sphere, its north and south poles represent the binary states 0 and 1, respectively, and all other locations on its surface represent possible quantum superpositions of those two states. Noise causes the Bloch arrow to drift around the sphere over time. A conventional computer represents 0 and 1 with physical quantities, such as capacitor voltages, that can be locked near the correct values to suppress this kind of continuous wandering and unwanted bit flips. There is no comparable way to lock the qubit’s “arrow” to its correct location on the Bloch sphere. Early in the 1990s, Landauer and others argued that this difficulty presented a fundamental obstacle to building useful quantum computers. The issue is known as scalability: Although a simple quantum processor performing a few operations on a handful of qubits might be possible, could you scale up the technology to systems that could run lengthy computations on large arrays of qubits? A type of classical computation called analog computing also uses continuous quantities and is suitable for some tasks, but the problem of continuous errors prevents the complexity of such systems from being scaled up. Continuous errors with qubits seemed to doom quantum computers to the same fate. We now know better. Theoreticians have successfully adapted the theory of error correction for classical digital data to quantum settings. QEC makes scalable quantum processing possible in a way that is impossible for analog computers. To get a sense of how it works, it’s worthwhile to review how error correction is performed in classical settings. Simple repetition code [top] on a conventional bit allows single bit-flip errors to be detected via parity checks and then corrected. A similar code for qubits [bottom] must deal with continuous errors. (For simplicity, we depict the case of a logical qubit in a nonsuperposition state, 1.) The parity checks, being quantum measurements, produce discrete outcomes with various probabilities, converting the continuous error into a discrete one and allowing correction by a qubit flip. The individual qubit states are not revealed by the parity measurements. Simple schemes can deal with errors in classical information. For instance, in the 19th century, ships routinely carried clocks for determining the ship’s longitude during voyages. A good clock that could keep track of the time in Greenwich, in combination with the sun’s position in the sky, provided the necessary data. A mistimed clock could lead to dangerous navigational errors, though, so ships often carried at least three of them. Two clocks reading different times could detect when one was at fault, but three were needed to identify which timepiece was faulty and correct it through a majority vote. The use of multiple clocks is an example of a repetition code: Information is redundantly encoded in multiple physical devices such that a disturbance in one can be identified and corrected. As you might expect, quantum mechanics adds some major complications when dealing with errors. Two problems in particular might seem to dash any hopes of using a quantum repetition code. The first problem is that measurements fundamentally disturb quantum systems. So if you encoded information on three qubits, for instance, observing them directly to check for errors would ruin them. Like Schrödinger’s cat when its box is opened, their quantum states would be irrevocably changed, spoiling the very quantum features your computer was intended to exploit. The second issue is a fundamental result in quantum mechanics called the no-cloning theorem, which tells us it is impossible to make a perfect copy of an unknown quantum state. If you know the exact superposition state of your qubit, there is no problem producing any number of other qubits in the same state. But once a computation is running and you no longer know what state a qubit has evolved to, you cannot manufacture faithful copies of that qubit except by duplicating the entire process up to that point. Fortunately, you can sidestep both of these obstacles. We’ll first describe how to evade the measurement problem using the example of a classical three-bit repetition code. You don’t actually need to know the state of every individual code bit to identify which one, if any, has flipped. Instead, you ask two questions: “Are bits 1 and 2 the same?” and “Are bits 2 and 3 the same?” These are called parity-check questions because two identical bits are said to have even parity, and two unequal bits have odd parity. The two answers to those questions identify which single bit has flipped, and you can then counterflip that bit to correct the error. You can do all this without ever determining what value each code bit holds. A similar strategy works to correct errors in a quantum system. Learning the values of the parity checks still requires quantum measurement, but importantly, it does not reveal the underlying quantum information. Additional qubits can be used as disposable resources to obtain the parity values without revealing (and thus without disturbing) the encoded information itself. Like Schrödinger’s cat when its box is opened, the quantum states of the qubits you measured would be irrevocably changed, spoiling the very quantum features your computer was intended to exploit. What about no-cloning? It turns out it is possible to take a qubit whose state is unknown and encode that hidden state in a superposition across multiple qubits in a way that does not clone the original information. This process allows you to record what amounts to a single logical qubit of information across three physical qubits, and you can perform parity checks and corrective steps to protect the logical qubit against noise. Quantum errors consist of more than just bit-flip errors, though, making this simple three-qubit repetition code unsuitable for protecting against all possible quantum errors. True QEC requires something more. That came in the mid-1990s when Peter Shor (then at AT&T Bell Laboratories, in Murray Hill, N.J.) described an elegant scheme to encode one logical qubit into nine physical qubits by embedding a repetition code inside another code. Shor’s scheme protects against an arbitrary quantum error on any one of the physical qubits. Since then, the QEC community has developed many improved encoding schemes, which use fewer physical qubits per logical qubit—the most compact use five—or enjoy other performance enhancements. Today, the workhorse of large-scale proposals for error correction in quantum computers is called the surface code, developed in the late 1990s by borrowing exotic mathematics from topology and high-energy physics. It is convenient to think of a quantum computer as being made up of logical qubits and logical gates that sit atop an underlying foundation of physical devices. These physical devices are subject to noise, which creates physical errors that accumulate over time. Periodically, generalized parity measurements (called syndrome measurements) identify the physical errors, and corrections remove them before they cause damage at the logical level. A quantum computation with QEC then consists of cycles of gates acting on qubits, syndrome measurements, error inference, and corrections. In terms more familiar to engineers, QEC is a form of feedback stabilization that uses indirect measurements to gain just the information needed to correct errors. QEC is not foolproof, of course. The three-bit repetition code, for example, fails if more than one bit has been flipped. What’s more, the resources and mechanisms that create the encoded quantum states and perform the syndrome measurements are themselves prone to errors. How, then, can a quantum computer perform QEC when all these processes are themselves faulty? Remarkably, the error-correction cycle can be designed to tolerate errors and faults that occur at every stage, whether in the physical qubits, the physical gates, or even in the very measurements used to infer the existence of errors! Called a fault-tolerant architecture, such a design permits, in principle, error-robust quantum processing even when all the component parts are unreliable. A long quantum computation will require many cycles of quantum error correction (QEC). Each cycle would consist of gates acting on encoded qubits (performing the computation), followed by syndrome measurements from which errors can be inferred, and corrections. The effectiveness of this QEC feedback loop can be greatly enhanced by including quantum-control techniques (represented by the thick blue outline) to stabilize and optimize each of these processes. Even in a fault-tolerant architecture, the additional complexity introduces new avenues for failure. The effect of errors is therefore reduced at the logical level only if the underlying physical error rate is not too high. The maximum physical error rate that a specific fault-tolerant architecture can reliably handle is known as its break-even error threshold. If error rates are lower than this threshold, the QEC process tends to suppress errors over the entire cycle. But if error rates exceed the threshold, the added machinery just makes things worse overall. The theory of fault-tolerant QEC is foundational to every effort to build useful quantum computers because it paves the way to building systems of any size. If QEC is implemented effectively on hardware exceeding certain performance requirements, the effect of errors can be reduced to arbitrarily low levels, enabling the execution of arbitrarily long computations. At this point, you may be wondering how QEC has evaded the problem of continuous errors, which is fatal for scaling up analog computers. The answer lies in the nature of quantum measurements. In a typical quantum measurement of a superposition, only a few discrete outcomes are possible, and the physical state changes to match the result that the measurement finds. With the parity-check measurements, this change helps. Imagine you have a code block of three physical qubits, and one of these qubit states has wandered a little from its ideal state. If you perform a parity measurement, just two results are possible: Most often, the measurement will report the parity state that corresponds to no error, and after the measurement, all three qubits will be in the correct state, whatever it is. Occasionally the measurement will instead indicate the odd parity state, which means an errant qubit is now fully flipped. If so, you can flip that qubit back to restore the desired encoded logical state. In other words, performing QEC transforms small, continuous errors into infrequent but discrete errors, similar to the errors that arise in digital computers. Researchers have now demonstrated many of the principles of QEC in the laboratory—from the basics of the repetition code through to complex encodings, logical operations on code words, and repeated cycles of measurement and correction. Current estimates of the break-even threshold for quantum hardware place it at about 1 error in 1,000 operations. This level of performance hasn’t yet been achieved across all the constituent parts of a QEC scheme, but researchers are getting ever closer, achieving multiqubit logic with rates of fewer than about 5 errors per 1,000 operations. Even so, passing that critical milestone will be the beginning of the story, not the end. On a system with a physical error rate just below the threshold, QEC would require enormous redundancy to push the logical rate down very far. It becomes much less challenging with a physical rate further below the threshold. So just crossing the error threshold is not sufficient—we need to beat it by a wide margin. How can that be done? A superconducting qubit can be flipped by applying a simple microwave pulse that takes the qubit’s state on a direct path on the Bloch sphere from 0 to 1 [top], but noise will introduce an error in the final position. A complicated pulse producing a more circuitous route can reduce the average amount of error in the final position. Here, the paths are chosen to minimize the effect of noise in the pulse amplitude alone [middle] or in both the amplitude and phase of the pulse [bottom]. If we take a step back, we can see that the challenge of dealing with errors in quantum computers is one of stabilizing a dynamic system against external disturbances. Although the mathematical rules differ for the quantum system, this is a familiar problem in the discipline of control engineering. And just as control theory can help engineers build robots capable of righting themselves when they stumble, quantum-control engineering can suggest the best ways to implement abstract QEC codes on real physical hardware. Quantum control can minimize the effects of noise and make QEC practical. In essence, quantum control involves optimizing how you implement all the physical processes used in QEC—from individual logic operations to the way measurements are performed. For example, in a system based on superconducting qubits, a qubit is flipped by irradiating it with a microwave pulse. One approach uses a simple type of pulse to move the qubit’s state from one pole of the Bloch sphere, along the Greenwich meridian, to precisely the other pole. Errors arise if the pulse is distorted by noise. It turns out that a more complicated pulse, one that takes the qubit on a well-chosen meandering route from pole to pole, can result in less error in the qubit’s final state under the same noise conditions, even when the new pulse is imperfectly implemented. One facet of quantum-control engineering involves careful analysis and design of the best pulses for such tasks in a particular imperfect instance of a given system. It is a form of open-loop (measurement-free) control, which complements the closed-loop feedback control used in QEC. This kind of open-loop control can also change the statistics of the physical-layer errors to better comport with the assumptions of QEC. For example, QEC performance is limited by the worst-case error within a logical block, and individual devices can vary a lot. Reducing that variability is very beneficial. In an experiment our team performed using IBM’s publicly accessible machines, we showed that careful pulse optimization reduced the difference between the best-case and worst-case error in a small group of qubits by more than a factor of 10. Some error processes arise only while carrying out complex algorithms. For instance, crosstalk errors occur on qubits only when their neighbors are being manipulated. Our team has shown that embedding quantum-control techniques into an algorithm can improve its overall success by orders of magnitude. This technique makes QEC protocols much more likely to correctly identify an error in a physical qubit. For 25 years, QEC researchers have largely focused on mathematical strategies for encoding qubits and efficiently detecting errors in the encoded sets. Only recently have investigators begun to address the thorny question of how best to implement the full QEC feedback loop in real hardware. And while many areas of QEC technology are ripe for improvement, there is also growing awareness in the community that radical new approaches might be possible by marrying QEC and control theory. One way or another, this approach will turn quantum computing into a reality—and you can carve that in stone. This article appears in the July 2022 print issue as “Quantum Error Correction at the Threshold.” Michael J. Biercuk is a professor of quantum physics and quantum technology at the University of Sydney. He is the founder and CEO of Q-CTRL. Thomas Stace is the principal quantum control engineer at Q-CTRL. He is also a professor at the University of Queensland in Brisbane, Australia. Self-powered sensors convert neck strain into electrical pulses to detect head trauma in athletes The prototype patch in this research is shown in (a) on the left; on the right (b) is the kind of head rotation that can yield an electrical response from the patch. Nelson Sepúlveda was sitting in the stands at Spartan Stadium, watching his hometown Michigan State players bash heads with their cross-state football rivals from the University of Michigan, when he had a scientific epiphany. Perhaps the nanotechnologies he had been working on for years—paper-thin devices known as ferroelectret nanogenerators that convert mechanical energy into electrical energy—could help save these athletes from the ravages of traumatic brain injury. Your weekly selection of awesome robot videos Video Friday is your weekly selection of awesome robotics videos, collected by your friends at IEEE Spectrum robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please send us your events for inclusion. Enjoy today’s videos! Fuel cell electric vehicles (FCEVs) often reach higher energy density and exhibit greater efficiency than battery EVs; however, they also have high manufacturing costs, limited service life, and relatively low power density. Modeling and simulation can improve fuel cell design and optimize EV performance. Learn more in this white paper.",0
"
        The Beating Heart of the World’s First Exascale Supercomputer
    ",https://spectrum.ieee.org/frontier-exascale-supercomputer,2022-06-24,,"These chips power Frontier past 1,100,000,000,000,000,000 operations per second The world’s latest fastest supercomputer, Frontier at Oak Ridge National Lab, in Tennessee, is so powerful that it operates faster than the next seven best supercomputers combined and more than twice as well as the No. 2 machine. Frontier is not only the first machine to break the exascale barrier, a threshold of a billion billion calculations per second, but is also ranked No. 1 as the world’s most energy-efficient supercomputer. Now the companies that helped build Frontier, Advanced Micro Devices (AMD) and Hewlett Packard Enterprise (HPE), reveal the electronic tricks that make the supercomputer tick. Frontier consists of 74 HPE Cray EX supercomputing cabinets, each weighing more than 3,600 kilograms, which altogether hold more than 9,400 computing nodes. Each node contains one optimized third-generation AMD EPYC 64-core 2-gigahertz “Trento” processor for general tasks and four AMD instinct MI250x accelerators for highly parallel supercomputing and AI operations, as well as 4 terabytes of flash memory to help quickly feed the GPUs data. In total, Frontier contains 9,408 CPUs, 37,632 GPUs, and 8,730,112 cores, linked together by 145 kilometers of networking cables. The lab says its world-leading supercomputer consumes about 21 megawatts. “Everyone up and down the line went after efficiency.”—Brad McCredie, AMD In May at the International Supercomputing Conference 2022 in Hamburg, Frontier revealed an overall performance of 1.1 exaflops, or 1.1 quintillion floating point operations per second, launching it to head of the Top500 list of the world’s most powerful supercomputers. It may grow even more powerful, with a theoretical peak performance of 2 exaflops. In addition, Frontier is ranked first on the latest Green500 list, which measures supercomputing energy efficiency. (Which may not be an incidental point to its overall performance as the world’s fastest.) Whereas the previous top Green500 machine, MN-3 in Japan, delivered 39.38 gigaflops per watt, the Frontier test-and development system achieves 62.68 gigaflops per watt. Moreover, Frontier won the top spot in a newer category, mixed-precision computing, which rates performance in computing formats commonly used for artificial intelligence. On the latest High-Performance Linpack-Accelerator Introspection or HPL-AI test, Frontier’s performance reached about 6.86 exaflops. A key aspect of Frontier’s success is how its CPUs and GPUs are linked within each node via AMD’s Infinity Fabric interconnect architecture. This helps boost coherency between the CPU and GPUs—that is, giving them all the same view of shared data. “Coherency is very important to getting you to scale performance,” says Brad McCredie, corporate vice president of data center GPU and accelerated processing at AMD in Austin. “It helps you make sure that you can run the right workloads on the right processors. It makes it very easy for CPUs to do small pieces of work and GPUs to do big pieces of work in parallel.” During Frontier’s development, AMD noted the biggest challenge it faced was power performance. “There was a lot of documentation that it would take hundreds of thousands of GPUs and 150 to 500 MW to get to an exaflop, and we wanted to do it with tens of thousands of GPUs and 20 MW” McCredie says. “So everyone up and down the line went after efficiency.” For example, Frontier’s GPUs each have 128 gigabytes of high-bandwidth memorysoldered onto them. This helps them overcome a critical bottleneck to performance—the shuffling of data between memory and processing. Moreover, Frontier’s GPUs each used the advanced 6-nanometer node from TSMC (Taiwan Semiconductor Manufacturing Co.). Therefore, “they can execute double-precision floating-point operations as fast as single-precision floating-point operations, which was a big innovation,” McCredie says. Frontier’s No. 1 ranking on the Green500 list may not be an incidental point either. These seemingly inconsequential developments in fact helped Frontier rely on tens of thousands of GPUs rather than hundreds of thousands, “shifting the burden away from the programmer to the hardware when it comes to managing all that parallelism,” McCredie says. “That makes the system much more programmable.” Two AMD nodes fit on a “compute blade,” and 64 such blades are loaded into each cabinet. The compute blades are linked together by HPE Slingshot interconnects, each with a custom-designed 64-port switch that provides 12.8 terabits per second of network bandwidth. Groups of blades are linked together via a so-called dragonfly topology in which hundreds of cabinets with hundreds of thousands of nodes can all communicate with just three hops at most between all nodes. “Slingshot deployments are highly optimized to use the most energy-efficient cabling—direct attach copper and active optical cables— fitted to the distances required,” says Mike Woodacre, vice president and chief technical officer of HPE’s HPC and AI team. Eliminating less-efficient general-purpose components, he adds, “significantly reduces the energy the fabric consumes.” The blades in the cabinets are chilled using liquid cooling. According to Gerald Kleyn, vice president of HPC and AI systems at HPE, the supercomputer can achieve up to five times the density of a traditional, air-cooled architecture. The result is a compact system that in turn dramatically reduces cabling requirements and operational expenses. “Breaking the exaflop barrier was important, but doing so while achieving No. 1 on the Green500 list is remarkable,” says Kleyn. Moreover, accomplishing this in the midst of a pandemic and global supply-chain problems, he says, “took a herculean team effort between Oak Ridge National Laboratory, HPE, and AMD.” Despite challenges including pandemic-related supply-chain issues, delivery of the Frontier supercomputer system took place between September and November 2021. Carlos Jones/ORNL/U.S. Department of Energy The next steps for Frontier include continued testing and validation of the system. The lab says it remains on track for final acceptance and early science access later in 2022 and is planned to open for full science at the beginning of 2023. Projects already planned for Frontier include research into cancer, drug discovery, nuclear fusion, exotic materials, superefficient engines, and stellar explosions. The aim of the machine is to speed the time required for such work from weeks to hours and from hours to seconds. “Frontier enable scientists to do more science, which means getting closer to more efficient cleaner-burning energy, more quickly finding even more effective vaccines for viruses,” McCredie says. “We started this whole adventure with Frontier to be the first to an exaflop, but seeing people at Oak Ridge working to solve problems in climate, energy, the pandemic, the top challenges facing humanity—we’ve gone from wanting to build a powerful computer to building something that will help everyone.” Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. This computer rendering depicts the pattern on a photonic chip that the author and his colleagues have devised for performing neural-network calculations using light. Think of the many tasks to which computers are being applied that in the not-so-distant past required human intuition. Computers routinely identify objects in images, transcribe speech, translate between languages, diagnose medical conditions, play complex games, and drive cars. The technique that has empowered these stunning developments is called deep learning, a term that refers to mathematical models known as artificial neural networks. Deep learning is a subfield of machine learning, a branch of computer science based on fitting complex models to data.",0
AI’s progress isn’t the same as creating human intelligence in machines,https://www.technologyreview.com/2022/06/28/1054270/2022-innovators-ai-robots/,2022-06-28,"<p>Honorees from this year's 35 Innovators list are employing AI to find new molecules, fold proteins, and analyze massive amounts of medical data.</p>
","The term “artificial intelligence” really has two meanings. AI refers both to the fundamental scientific quest to build human intelligence into computers and to the work of modeling massive amounts of data. These two endeavors are very different, both in their ambitions and in the amount of progress they have made in recent years. Scientific AI, the quest to both construct and understand human-level intelligence, is one of the most profound challenges in all of science; it dates back to the 1950s and is likely to continue for many decades. Data-centric AI, on the other hand, began in earnest in the 1970s with the invention of methods for automatically constructing “decision trees” and has exploded in popularity over the last decade with the resounding success of neural networks (now dubbed “deep learning”). Data-centric artificial intelligence has also been called “narrow AI” or “weak AI,” but the rapid progress over the last decade or so has demonstrated its power. Deep-learning methods, coupled with massive training data sets plus unprecedented computational power, have delivered success on a broad range of narrow tasks from speech recognition to game playing and more. The artificial-­intelligence methods build predictive models that grow increasingly accurate through a compute-­intensive iterative process. In previous years, the need for human-­labeled data to train the AI models has been a major bottleneck in achieving success. But recently, research and development focus has shifted to ways in which the necessary labels can be created automatically, based on the internal structure of the data. The GPT-3 language model released by OpenAI in 2020 exemplifies both the potential and the challenges of this approach. GPT-3 was trained on billions of sentences. It automatically generates highly plausible text, and even sensibly answers questions on a broad range of topics, mimicking the same language that a person might use. This essay is part of MIT Technology Review’s 2022 Innovators Under 35 package recognizing the most promising young people working in technology today. See the full list here or explore the winners in this category below. But GPT-3 suffers from several problems that researchers are working to address. It’s often inconsistent—you can get contradictory answers to the same question. Second, GPT-3 is prone to “hallucinations”: when asked who the president of the United States was in 1492, it will happily conjure up an answer. Third, GPT-3 is an expensive model to train and expensive to run. Fourth, GPT-3 is opaque—it’s difficult to understand why it drew a particular conclusion. Finally, since GPT-3 parrots the contents of its training data, which is drawn from the web, it often spews out toxic content, including sexism, racism, xenophobia, and more. In essence, GPT-3 cannot be trusted. This year's 35 Innovators in biotech are breaking new ground via machine learning, gene therapy, gene analysis, and CRISPR. Despite these challenges, researchers are investigating multi-modal versions of GPT-3 (such as DALL-E2), which create realistic images from natural-language requests. AI developers are also considering how to use these insights in robots that interact with the physical world. And AI is increasingly being applied to biology, chemistry, and other scientific disciplines to glean insights from the massive data and complexities in those fields. The bulk of the rapid progress today is in this data-centric AI, and the work of this year’s 35 Innovators Under 35 winners is no exception. While data-centric AI is powerful, it has key limitations: the systems are still designed and framed by humans. A few years ago, I wrote an article for MIT Technology Review called “How to know if artificial intelligence is about to destroy civilization.” I argued that successfully formulating problems remains a distinctly human capability. Pablo Picasso famously said, “Computers are useless. They only give you answers.” We continue to anticipate the distant day when AI systems can formulate good questions—and shed more light on the fundamental scientific challenge of understanding and constructing human-level intelligence. Oren Etzioni is CEO of the Allen Institute for AI and a judge for this year’s 35 Innovators competition.",1
Materials with nanoscale components will change what’s possible,https://www.technologyreview.com/2022/06/28/1054282/2022-innovators-materials-science/,2022-06-28,"<p>This year's 35 Innovators are making it possible for familiar materials like glass, steel, and electronics to have completely new properties.</p>
","In the 24 years I’ve worked as a materials scientist, I’ve always been inspired by hierarchical patterns found in nature that repeat all the way down to the molecular level. Such patterns induce remarkable properties—they strengthen our bones without making them heavy, give butterfly wings their color, and make a spiderweb silk both durable and pliant. What if we could engineer such properties directly into manufactured materials? This could remove the need for complicated manufacturing processes to create devices like stents, microprocessors, and batteries. And eventually, we may even be able to program some degree of intelligence directly into the materials that make up such devices, which could make new features and functionality possible. In my research group at Caltech, I study new properties of materials that emerge when you take nanoscale building blocks and organize them into 3D structures known as architectures. I predict that architected materials—substances built from the nanoscale up to have useful properties—will eventually replace conventional materials, not only in science and engineering but in many areas of daily life. Lately, advances in 3D printing and other forms of additive manufacturing have made it possible to organize micro- and nano-size building blocks of matter into complex structures with great precision. We can now make new materials from components that range from just a little larger than 100 atoms to several millimeters in size. This means scientists can decouple properties that have historically been linked together. For example, strong materials are typically heavy, and insulating materials like dinnerware are often brittle. But when ceramics and glass are architected by replacing solid blocks of material with a structure of the same size built of small struts, they can deform and reform like a sponge. And there’s more—architected materials can evolve in space and time in response to a pre-programmed trigger. They can morph into different shapes to respond or adapt to a new environment or a stimulus. They can be made to release objects by relaxing their grip when heated or break apart at designated locations when strained. This essay is part of MIT Technology Review’s 2022 Innovators Under 35 package recognizing the most promising young people working in technology today. See the full list here or explore the winners in this category below. Thanks to this built-in responsiveness, future materials could be made with some decision-making capabilities and adaptability. Intelligent materials may be able to automatically release precise amounts of medication, heal themselves when damaged, or perform logical operations when exposed to light. In fact, some architected materials have already incorporated new kinds of logic gates that respond to either mechanical or chemical stimuli. One area where I see great potential involves using machine learning to predict new architectures for materials that can emulate computationally trained neural networks using light instead of digital input. Eventually, artificial neural networks could be integrated into architected physical materials to make decisions, eliminating the need to first convert the input into digital signals and then process them in computers. This means materials themselves could someday be made to recognize faces or objects, process language, and classify text or numbers. This year's 35 Innovators honorees are helping to make cutting-edge tech like 2D semiconductors and optical computing a reality. To realize this vision, we will need new computational models that can accurately capture the mechanics and physics of the additive manufacturing process for an affordable price. Additional models must be able to perform diagnostics, in real time, to determine whether any defects that form will affect performance. And as if designing, discovering, and demonstrating new material properties weren’t hard enough, we’ll then have to turn prototypes into technology and manufacture the materials at scale. These tasks represent a major challenge, in part because the models haven’t yet been developed. Knowing there are many talented people working on these problems, I look forward to the day when we can create architected materials and devices imbued with the ability to make decisions on their own. Julia R. Greer is a materials scientist at the California Institute of Technology, and was a 35 Innovators honoree in 2008 and a judge for this year’s competition.",0
"The Download: Yann LeCun’s AI vision, and smart cities’ unfulfilled promises",https://www.technologyreview.com/2022/06/24/1054826/download-yann-lecun-interview-ai-general-intelligence-meta-smart-cities/,2022-06-24,"<p>Plus: The Online Privacy Bill has gained support from the House of Representatives</p>
","This is today's edition of The Download, our weekday newsletter that provides a daily dose of what's going on in the world of technology. Yann LeCun has a bold new vision for the future of AI Around a year and a half ago, Yann LeCun realized he had it wrong. LeCun, who is chief scientist at Meta’s AI lab and one of the most influential AI researchers in the world, had been trying to give machines a basic grasp of how the world works—a kind of common sense—by training neural networks to predict what was going to happen next in video clips of everyday events. But guessing future frames of a video pixel by pixel was just too complex. He hit a wall. Now, after months figuring out what was missing, he has a bold new vision for the next generation of AI. In a draft document shared with MIT Technology Review, LeCun sketches out an approach that he thinks will one day give machines the common sense they need to navigate the world. For LeCun, the proposals could be the first steps on a path to building machines with the ability to reason and plan like humans—what many call artificial general intelligence, or AGI. His vision is far from being comprehensive; indeed, it may raise more questions than it answers. The biggest question mark, as LeCun points out himself, is that he does not know how to build what he describes. Read the full story. —Melissa Heikkiläa & Will Douglas Heaven The smart city is a perpetually unrealized utopia In a new essay, Chris Salter, an artist and professor of immersive arts at the Zurich University of the Arts, talks about how the concept of the smart city has always changed through the decades. In it he also asks what role people should play in future cities. He writes: “When we assume that data is more important than the people who created it, we reduce the scope and potential of what diverse human bodies can bring to the “smart city” of the present and future. But the real “smart” city consists not only of commodity flows and information networks generating revenue streams for the likes of Cisco or Amazon. The smartness comes from the diverse human bodies of different genders, cultures, and classes whose rich, complex, and even fragile identities ultimately make the city what it is.” Read the full essay. The must-reads I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology. 1 The Online Privacy Bill is gathering momentumThe bill aims to curb businesses’ collection of users’ personal data, as well as helping them opt out of targeted advertising. (WSJ $) 2 Carbon capture isn’t the quick fix we want it to beExperts think it’s smarter to channel time, effort and funding into renewables instead. (WP $)+ The UK wants to capture CO2 and turn it into baking soda. (New Scientist $)+ Carbon removal hype is becoming a dangerous distraction. (MIT Technology Review)+ Climate change is altering the way that wine tastes. (Knowable Magazine) 3 A UK musician has successfully sued his record label over streaming royaltiesKieran Hebden, better known as Four Tet, argued that the rate he received was unfair. (BBC)+ Spotify is still waiting for its podcasting gamble to pay off. (Bloomberg $)+ The platform is testing a social feature to share what you’re listening to. (TechCrunch) 4 It’s getting harder to access China’s internet from abroadThe tighter restrictions seem to coincide with the country’s covid-induced isolation. (LA Times)+ Now China wants to censor online comments. (MIT Technology Review) 5 A recession could make some workers too nervous to work from homePrompting them to head back into offices to prove their worth to their employers. (The Atlantic $)+ Remote workers want to re-create those watercooler moments. (MIT Technology Review) 6 Controversial crypto founder Do Kwon is staging a comebackJust weeks after his stablecoins imploded, he’s launched a new version of his embattled Terra blockchain network. (WSJ $)+ Hackers have stolen $100 million from crypto bridge Horizon. (Bloomberg $) 7 Even DALL·E mini’s creator doesn’t know why it’s obsessed with women in sarisBut the program’s dataset is the most probable culprit. (Rest of World)+ Popular text-to-image AI generators are staring down the barrel of a safety reckoning. (Time) 8 How an AI app could help detect early dementiaIt identifies early signs of mild cognitive impairment in under five minutes. (Neo.Life) 9 An excitable NFT conference fell for a fake Snoop Dogg called Doop SnoggThere’s a lesson in this, somewhere. (The Guardian)+ NFT.NYC sounds completely unhinged. (Motherboard)+ NFTs of canned ice tea is the industry’s hottest property right now. (The Information)+ I tried to buy an Olive Garden NFT. All I got was heartburn. (MIT Technology Review) 10 Want to feel better? Turn off your phone notifications 📱Ignoring a device’s default settings can provide a certain sense of freedom. (FT $) Quote of the day “We’re addicted to being on Facebook.” —Jordi Berbera, who runs a pizza stand in Mexico City, tells Rest of World why he has turned to selling his wares through the social network instead of through more conventional food delivery apps. The big story “Am I going crazy or am I being stalked?” Inside the disturbing online world of gangstalking August 2020 Jenny’s story is not linear, the way that we like stories to be. She was born in Baltimore in 1975 and had a happy, healthy childhood—her younger brother Danny fondly recalls the treasure hunts she would orchestrate. In her late teens, she developed anorexia and depression and was hospitalized for a month. Despite her struggles, she graduated high school and was accepted into a prestigious liberal arts college. There, things went downhill again. Among other issues, chronic fatigue led her to drop out. When she was 25 she flipped that car on Florida’s Sunshine Skyway Bridge in an apparent suicide attempt. At 30, after experiencing delusions that she was pregnant, she was diagnosed with schizophrenia. She was hospitalized for half a year and began treatment, regularly receiving shots of an antipsychotic drug. “It was like having my older sister back again,” Danny says. On July 17, 2017, Jenny jumped from the tenth floor of a parking garage at Tampa International Airport. After her death, her family searched her hotel room and her apartment, but the 42-year-old didn’t leave a note. “We wanted to find a reason for why she did this,” Danny says. And so, a week after his sister’s death, Danny—a certified ethical hacker—decided to look for answers on Jenny’s computer. He found she had subscribed to hundreds of gangstalking groups across Facebook, Twitter, and Reddit; online communities where self-described “targeted individuals” say they are being monitored, harassed, and stalked 24/7 by governments and other organizations—and the internet legitimizes them. Read the full story. —Amelia Tait We can still have nice things A place for comfort, fun and distraction in these weird times. (Got any ideas? Drop me a line or tweet 'em at me.)+ I’m obsessed with this stylish furry crab and their jaunty little sponge hat.+ Baz Luhrmann’s new biopic may be making headlines right now, but it turns out the King of Rock ‘n’ Roll was actually offered a plethora of roles himself.+ These photos from this year’s Westminster Dog Show are joyous (congratulations Trumpet!)+ If you’re feeling the sriracha shortage, gochujang and cholula are tasty hot sauce alternatives.+ This cool site uses AI to rip songs apart into vocals, drums, bass and piano sections.",0
Yann LeCun has a bold new vision for the future of AI,https://www.technologyreview.com/2022/06/24/1054817/yann-lecun-bold-new-vision-future-ai-deep-learning-meta/,2022-06-24,"<p>One of the godfathers of deep learning pulls together old ideas to sketch out a fresh path for AI, but raises as many questions as he answers.</p>
","Around a year and a half ago, Yann LeCun realized he had it wrong. LeCun, who is chief scientist at Meta’s AI lab and one of the most influential AI researchers in the world, had been trying to give machines a basic grasp of how the world works—a kind of common sense—by training neural networks to predict what was going to happen next in video clips of everyday events. But guessing future frames of a video pixel by pixel was just too complex. He hit a wall. Now, after months figuring out what was missing, he has a bold new vision for the next generation of AI. In a draft document shared with MIT Technology Review, LeCun sketches out an approach that he thinks will one day give machines the common sense they need to navigate the world. (Update: LeCun has since posted the document online.) For LeCun, the proposals could be the first steps on a path to building machines with the ability to reason and plan like humans—what many call artificial general intelligence, or AGI. He also steps away from today’s hottest trends in machine learning, resurrecting some old ideas that have gone out of fashion. But his vision is far from comprehensive; indeed, it may raise more questions than it answers. The biggest question mark, as LeCun points out himself, is that he does not know how to build what he describes. A machine that could think like a person has been the guiding vision of AI research since the earliest days—and remains its most divisive idea. The centerpiece of the new approach is a neural network that can learn to view the world at different levels of detail. Ditching the need for pixel-perfect predictions, this network would focus only on those features in a scene that are relevant for the task at hand. LeCun proposes pairing this core network with another, called the configurator, which determines what level of detail is required and tweaks the overall system accordingly. For LeCun, AGI is going to be a part of how we interact with future tech. His vision is colored by that of his employer, Meta, which is pushing a virtual-reality metaverse. He says that in 10 or 15 years people won’t be carrying smartphones in their pockets, but augmented-reality glasses fitted with virtual assistants that will guide humans through their day. “For those to be most useful to us, they basically have to have more or less human-level intelligence,” he says. “Yann has been talking about many of these ideas for some time,” says Yoshua Bengio, an AI researcher at the University of Montreal and scientific director at the Mila-Quebec Institute. “But it is good to see it all together, in one big picture.” Bengio thinks that LeCun asks the right questions. He also thinks it’s great that LeCun is willing to put out a document that has so few answers. It’s a research proposal rather than a set of clean results, he says. “People talk about these things in private, but they’re not usually shared publicly,” says Bengio. “It’s risky.” LeCun has been thinking about AI for nearly 40 years. In 2018 he was joint winner of computing’s top prize, the Turing Award, with Bengio and Geoffrey Hinton, for his pioneering work on deep learning. “Getting machines to behave like humans and animals has been the quest of my life,” he says. LeCun thinks that animal brains run a kind of simulation of the world, which he calls a world model. Learned in infancy, it’s the way animals (including humans) make good guesses about what’s going on around them. Infants pick up the basics in the first few months of life by observing the world, says LeCun. Seeing a dropped ball fall a handful of times is enough to give a child a sense of how gravity works. “Common sense” is the catch-all term for this kind of intuitive reasoning. It includes a grasp of simple physics: for example, knowing that the world is three-dimensional and that objects don’t actually disappear when they go out of view. It lets us predict where a bouncing ball or a speeding bike will be in a few seconds’ time. And it helps us join the dots between incomplete pieces of information: if we hear a metallic crash from the kitchen, we can make an educated guess that someone has dropped a pan, because we know what kinds of objects make that noise and when they make it. In short, common sense tells us what events are possible and impossible, and which events are more likely than others. It lets us foresee the consequences of our actions and make plans—and ignore irrelevant details. But teaching common sense to machines is hard. Today’s neural networks need to be shown thousands of examples before they start to spot such patterns. In many ways common sense amounts to the ability to predict what’s going to happen next. “This is the essence of intelligence,” says LeCun. That’s why he—and a few other researchers—have been using video clips to train their models. But existing machine-learning techniques required the models to predict exactly what is going to happen in the next frame and generate it pixel by pixel. Imagine you hold up a pen and let it go, LeCun says. Common sense tells you that the pen will fall, but not the exact position it will end up in. Predicting that would require crunching some tough physics equations. That’s why LeCun is now trying to train a neural network that can focus only on the relevant aspects of the world: predicting that the pen will fall but not exactly how. He sees this trained network as the equivalent of the world model that animals rely on. LeCun says he has built an early version of this world model that can do basic object recognition. He is now working on training it to make predictions. But how the configurator should work remains a mystery, he says. LeCun imagines that neural network as the controller for the whole system. It would decide what kind of predictions the world model should be making at any given time and what level of detail it should focus on to make those predictions possible, adjusting the world model as required. LeCun is convinced that something like a configurator is needed, but he doesn’t know how to go about training a neural network to do the job. “We need to figure out a good recipe to make this work, and we don’t have that recipe yet,” he says. In LeCun’s vision, the world model and the configurator are two key pieces in a larger system, known as a cognitive architecture, that includes other neural networks—such as a perception model that senses the world and a model that uses rewards to motivate the AI to explore or curb its behavior. Each neural network is roughly analogous to parts of the brain, says LeCun. For example, the configurator and world model are meant to replicate functions of the prefrontal cortex. The motivation model corresponds to certain functions of the amygdala, and so on. The idea of cognitive architectures, especially ones inspired by the brain, has been around for decades. So have many of LeCun’s ideas about prediction using models with different levels of detail. But when deep learning became the dominant approach in AI, many of these older ideas went out of fashion. “People in AI research have kind of forgotten about this a little bit,” he says. What he has done is taken these older ideas and rehabilitated them, suggesting ways that they can be combined with deep learning. For LeCun, revisiting these out-of-fashion ideas is essential, because he believes the two dominant approaches in modern AI are dead ends. When it comes to building general-purpose AI, there are two main camps. In one, many researchers think the remarkable success of very large language or image-making models like OpenAI's GPT-3 and DALL-E show that all we need to do is just build bigger and bigger models. In the other camp are champions of reinforcement learning, the AI technique that rewards specific behaviors to make neural networks to learn by trial and error. This is the approach DeepMind used to train its game-playing AIs like AlphaZero. Get the rewards right, the argument goes, and reinforcement learning will eventually produce more general intelligence. Open AI's language AI wowed the public with its apparent mastery of English – but is it all an illusion? LeCun is having none of it: “This idea that we're going to just scale up the current large language models and eventually human-level AI will emerge—I don’t believe this at all, not for one second.” These large models just manipulate words and images, he says. They have no direct experience of the world. He is equally skeptical about reinforcement learning, because it requires vast amounts of data to train models to do even simple tasks. “I think that has no chance of working at all,” says LeCun. David Silver at DeepMind, who led the work on AlphaZero and is a big advocate of reinforcement learning, disagrees with this assessment but welcomes LeCun’s overall vision. “It’s an exciting new proposal for how a world model could be represented and learned,” he says. Melanie Mitchell, an AI researcher at the Santa Fe Institute, is also excited to see a whole new approach. “We really haven’t seen this coming out of the deep-learning community so much,” she says. She also agrees with LeCun that large language models cannot be the whole story. “They lack memory and internal models of the world that are actually really important,” she says. Natasha Jaques, a researcher at Google Brain, thinks that language models should still play a role, however. It’s odd for language to be entirely missing from LeCun’s proposals, she says: “We know that large language models are super effective and bake in a bunch of human knowledge.” Jaques, who works on ways to get AIs to share information and abilities with each other, points out that humans don’t have to have direct experience of something to learn about it. We can change our behavior simply by being told something, such as not to touch a hot pan. “How do I update this world model that Yann is proposing if I don’t have language?” she asks. There’s another issue, too. If they were to work, LeCun’s ideas would create a powerful technology that could be as transformative as the internet. And yet his proposal doesn’t discuss how his model’s behavior and motivations would be controlled, or who would control them. This is a weird omission, says Abhishek Gupta, the founder of the Montreal AI Ethics Institute and a responsible-AI expert at Boston Consulting Group. “We should think more about what it takes for AI to function well in a society, and that requires thinking about ethical behavior, amongst other things,” says Gupta. Yet Jaques notes that LeCun’s proposals are still very much ideas rather than practical applications. Mitchell says the same: “There’s certainly little risk of this becoming a human-level intelligence anytime soon.” LeCun would agree. His aim is to sow the seeds of a new approach in the hope that others build on it. “This is something that is going to take a lot of effort from a lot of people,” he says. “I’m putting this out there because I think ultimately this is the way to go.” If nothing else, he wants to convince people that large language models and reinforcement learning are not the only ways forward. “I hate to see people wasting their time,” he says.",0
"No power, no fans, no AC: The villagers fighting to survive India’s deadly heatwaves",https://www.technologyreview.com/2022/07/05/1054750/how-indian-villagers-are-coping-with-the-deadly-heatwaves/,2022-07-05,"<p><span style=""font-weight: 400"">Record-breaking temperatures are exposing inequalities, particularly in remote villages like Nagla Tulai.</span></p>
","Suman Shakya wants me to touch the concrete wall of her bedroom, where her one-year-old son lies soaked with sweat. It burns my hand as if it were a hot pan. “Now imagine sitting in front of a hot pan in this weather for as long as it takes to make rotis for the whole family,” she says. Outside the temperature is 44 °C (111 °F). My throat is dry and my head spins. Sweat pours down my face, getting into my eyes and blurring my vision. Shakya lives in the farming village of Nagla Tulai in the north Indian state of Uttar Pradesh, where lately the heat has been punishingly cruel. Villagers here have always had to endure hot summers, but the past few years have tested their strength. This year, after the end of a harsh winter, the temperature has been rising since March. In mid-May it hit 49 °C (120 °F), the highest India has recorded in 122 years. Since May, local news reports have attributed more than 50 deaths to the record-breaking heat. At the end of April, when the daytime temperature crossed 45 °C (113 °F), most residents of Nagla Tulai sought succor in the hot winds blowing outdoors. Since northwest India first began to see alarming temperatures, local governments have been advising people not to go out in the sun if they can help it. But Nagla Tulai is one of the few Indian villages yet to be electrified. That means no fans, no coolers, and no air conditioners for its 150-odd households. Instead, the women of Nagla Tulai have taken their cooking to the rooftops. There they sit for hours stuffing tinder into their clay stoves to keep them burning even as the sun breathes fire at them from above. “You can’t even flick the sweat off your face; it will wet your hands and spoil the rotis,” says Shakya. That climate change is exacerbating South Asia’s heat waves is no longer in question. This year alone, two new studies have explored the links. A report by World Weather Attribution found that the likelihood of a heat wave like this year’s has increased by 30 times since the 19th century. And an attribution study carried out by the UK’s Met Office pointed out that the chances of unprecedented heat waves in India and Pakistan have been made 100 times higher by climate change. The question to be answered next is how people faced with life-threatening heat are going to cope with it. “Almost everybody is affected; only the extent varies,” says Vimal Mishra, a climate scientist at the Indian Institute of Technology Gandhinagar in the western state of Gujarat. “People who are less affected [than the others] are those who can afford air conditioning.” The National Disaster Management Authority counts 23 out of India’s 28 states as being vulnerable to heat waves. Indeed, the sale of air conditioners has shot up in India since March, especially in urban areas. In Etah, the nearest city to Nagla Tulai, the hum of ACs drowned out all other noise every time electricity came on. “The majority of houses run AC units in this town,” says Devesh Singh, a television journalist who has been filing reports on Etah’s summers for 22 years. Many households in the city steal the necessary electricity from state-owned power companies to avoid paying the steep bills. They do it by attaching an aluminum hook, called a katia, to the power cables running through the streets. In cities across Uttar Pradesh, police carried out daily raids this spring to spot the contraptions. “Earlier, raids happened during the day, which allowed people to use the electricity at night and remove their katia first thing in the morning. This year, the police have been coming between 2 a.m. and 4 a.m., while people are asleep in front of their ACs,” says Singh, the journalist. By mid-June, 150 people in Etah had been charged with power theft, but the ACs kept on humming. Even with the use of air conditioners hitting a record high, a vast majority of Indians still can’t afford one. The country’s annual per capita income is around 9,000 rupees, and even a cheap AC would claim a quarter of that. And even if you have an AC unit and the electricity to run it, whether paid for or stolen, that doesn’t guarantee escape from the heat. Power cuts are common during the summer; they are brief in big cities but more frequent and longer-lasting in towns and villages. This year, a severe shortage of coal at the power stations and an enormous demand for electricity meant huge numbers of people had to make do with four hours or less of power per day in some of the worst-hit states. Caste, gender, and regional location can also affect who gets to stay cool. India’s climate researchers are increasingly concerned about such factors. “Your starting point really determines the kind of capacity you will have to deal with climatic risks,” says Chandni Singh, a researcher at the Indian Institute for Human Settlements who has been working on climate change vulnerability and adaptation for 10 years. “There is a huge disparity between villages and within villages.” For example, in Nagla Tulai, men and old women can seek a breeze outdoors when they like, but other women and girls are expected to spend the daytime hours indoors, where the still and stifling heat presses down on them like a blanket. To the experts, this scarcely counts as adaptation. “It would be wrong to say that people in these situations adapt. They suffer, basically,” says Mishra. “Meaningful adaptation should reduce the suffering, but that’s not happening when people are trapped in concrete housing with no electricity.” The men spend most of their time sitting under a big banyan tree and try to ignore the sharp heat encircling them like a halo. To work, they would have to go to the farms, and that would be murder. The summers have been hot for as long as they remember, so they traditionally rested when the sun was at its peak and worked the rest of the day. Over the past few years, however, their work time has been growing shorter. “This year, we have been able to work no more than two hours in a day,” Raja Ram, a third-generation farmer, tells me. “The rest of the time, we sit.” Less work means more deprivation. Even in the years when they worked full time farming tobacco and corn, they had to split the income with the landlords who owned the fields. Most people in Nagla Tulai identify themselves as Shakya, which the government of Uttar Pradesh categorizes as a “backward” caste. That they don’t own the land they farm is one of many inequalities they have faced for generations. Now, the heat waves are making their share of the harvest even smaller. “One thing that is not spoken about much is the impact of landlessness,” says Chandni Singh. “We are talking about people who are already used to shifting their working time in the summer to earlier in the day, even without climate change. But how much further back can you shift that? When you have villages that are seeing such extreme heat even as the monsoon is delayed and water tables are going down, agriculture becomes almost unviable as a livelihood. Where does a youngster in the village go? You are pushing people against the limit of adaptation. You are pushing people to migrate.” The men in Nagla Tulai don’t want to leave—not yet. They are not so sure about the future, though. If the heat waves cause large-scale migration in India, researchers believe, it will be driven by the lasting damage to the agriculture sector. “Migration in India is mostly driven by employment. If these heat waves occur more often and start early, like this year, the farm laborers will have to move to cities. They will have to find non-farm employment—whatever allows them to earn money,” says Mishra. Men fear that if they are forced to migrate, a job at a factory or a construction site will not pay enough for them to be able to take their families along. But if the heat waves intensify—on several days, Etah recorded temperatures five degrees higher than the same date the previous year—they might struggle to build a family in the first place. As it is, not many women are willing to marry men from Nagla Tulai. Those who do cope by retreating to their parents’ home for several months every year. Suman Shakya is upset because her husband has refused to drop her at her parents’ village this summer. She fears her children won’t survive the summer without a ceiling fan or AC. “They keep crying all day and all night. One day it’s rashes, the next day it is an upset stomach, the day after it’s dengue. I feel stuck in a pattern: they fall sick, we take them to the hospital, they fall sick again,” she tells me, waving a cloth fan to comfort her son. When her mother got married, she took a handmade fan to her in-laws’ house as part of her trousseau. The summers were hot but not lethal, and a solid hand fan easily remedied a power cut in the afternoon. Girls looking forward to marriage crafted the fans themselves, embroidering their names inside the folds. In 2016, when she got married herself, what she wanted for her dowry was an AC and a refrigerator. She arrived in Nagla Tulai with neither. “There would have been no point,” she says. In 2011, the local government installed solar panels on every rooftop in the village. The residents were told that once they were fully charged, the panels would power bulbs and fans and even charge mobile phones. Later they found out they would need inverters to store the electricity and batteries to charge the inverters, and those things would cost money. “The families that can afford it run three fans on solar, one to cool their buffalo,” says Priyanka Shakya, a 16-year-old girl. Even when fully charged, the solar panels support a fan for only a few hours, so they are saved for the nights, to be turned on when the children start crying. A fan which runs for a couple of hours with solar panels, is unused because the sky was overcast and the panels could not charge. Administrators in India limit themselves to advance warnings before a heat wave and emergency measures in the middle of one. Those measures may include shutting down schools and construction sites and canceling doctors’ leaves. Mishra thinks they could do more. “They can identify vulnerable areas, such as villages and slums, where poor people who don’t have air conditioning live,” he says. “Community centers can be set up, such as we have for floods and other disasters, for people to go and get some cooling. They can have cold water. They can have first aid to treat heatstroke-related symptoms.” Even affluent urban neighborhoods need similar shelters for vendors and construction workers who lack protection from the heat, he adds. In Ahmedabad, where he works, the municipal corporation offers many of these initiatives as part of its heat action plan, the first in South Asia. They put it in place after a heat wave in 2010 claimed 4,462 lives in the city. “People aren’t always aware of what symptoms are caused by heat. They go to a hospital as the last measure. That often causes mortality,” says Mishra. But in Nagla Tulai, Priyanka Shakya is no longer waiting for electricity to come to the village. Her plan is to get married and leave.",0
How to track your period safely post-Roe,https://www.technologyreview.com/2022/06/30/1055331/how-to-track-your-period-safely-post-roe/,2022-06-30,"<p>Here's a guide to keeping your menstrual cycle information private.</p>
","As soon as Roe v. Wade was overturned on Friday, June 24, calls for people to delete their period-tracking apps were all over social media. These apps gather extremely personal data that could pinpoint a missed period. The fear is that in the hands of law enforcement, this data could be used to bolster a criminal case against a person who attempts to get an abortion in a state where it is restricted or banned. Right now, and I mean this instant, delete every digital trace of any menstrual tracking. Please. Delete your period tracking apps today. Understandably, a lot of people are scared and confused. So here’s our guide to what you need to know about period-tracking apps, what the apps’ makers say about their often murky privacy policies, and what alternative methods you can use to track your menstrual cycle that don’t involve handing your data over. Stress or dietary changes, among other factors, can make periods irregular and unpredictable. Tracking them can help expose underlying health issues, such as fibroids, which are noncancerous uterine growths. It can also help people spot patterns in mood and energy, which can often be affected by ovulation. People trying to get pregnant often use period trackers to figure out when they’re most fertile. The overturning of Roe v. Wade in the US triggered laws that made abortion illegal in 13 states, and more states are likely to ban abortion in the coming months. In states that have banned abortions, people could now be prosecuted if they are alleged to have had one. The worry is that their digital data footprint could be used to build such a case. Missing your period is not a crime, but evidence of it could be subpoenaed and used to bolster a case against someone suspected of an abortion. We reached out to some of the major period-tracking apps—Flo, Clue, and SpotOn (an app from Planned Parenthood)—for comment on what their privacy settings are and whether they would turn information over to authorities in states where abortion is illegal. Clue and SpotOn did not respond, though Clue stated on Twitter that because it is based in the European Union, it is not permitted to share data with authorities in the US: “We would have a primary legal duty under European law not to disclose any private health data. We repeat: we would not respond to any disclosure request or attempted subpoena of our users’ health data by US authorities. But we would let you and the world know if they tried.” We hear your questions & we understand your concerns. The thought that US authorities could use peopleâ€™s private health data against them is infuriating & terrifying. Without fuelling further fear or speculation, we do want to offer our community clarity & reassurance. #RoevWade pic.twitter.com/eJIGiMKY2C Neither Flo nor SpotOn have said whether they would turn over data to the authorities. Another app called Stardust, which skyrocketed to the top of App Store download ranks this past weekend thanks to viral videos the company posted promising privacy, actually states in its privacy policy that it will turn over data to authorities “whether or not legally required.” The moral of the story? If you want to be sure, read the fine print. All this means that the risks of tracking your menstrual cycle via an app may outweigh the benefits, and if you live in a state that has banned abortion or looks likely to ban abortion imminently, you may decide that your safest bet is to stop using this technology. But before you do, here are some things to think about: 1. Put it into perspective. If you live in a state that has banned abortion, you should also pay attention to the other digital traces you might leave. “Period-tracking apps should not be your biggest concern,” says Cooper Quintin, senior staff technologist and security researcher at the Electronic Frontier Foundation. He says that social media posts, text messages, and Google search history would be a higher priority for authorities seeking to digitally prove an abortion. 2. Before you delete your app, save your information. You’ve got a ton of valuable data and information you don’t want to lose, says Aliza Aufrichtig, who created a spreadsheet template for tracking periods. Either write your information in a notebook or do what Aufrichtig did and enter it into a spreadsheet. 3. After you delete your app, ask the app provider to delete your data. Just because you removed the app from your phone does not mean the company has gotten rid of your records. In fact, California is the only state where they are legally required to delete your data. Still, many companies are willing to delete it upon request. Here’s a helpful guide from the Washington Post that walks you through how you can do this. 1. Use a spreadsheet. It’s relatively easy to re-create the functions of a period tracker in a spreadsheet by listing out the dates of your past periods and figuring out the average length of time from the first day of one to the first day of the next. You can turn to one of the many templates already available online, like the period tracker created by Aufrichtig and the Menstrual Cycle Calendar and Period Tracker created by Laura Cutler. If you enjoy the science-y aspect of period apps, templates offer the ability to send yourself reminders about upcoming periods, record symptoms, and track blood flow. 2. Use a digital calendar. If spreadsheets make you dizzy and your entire life is on a digital calendar already, try making your period a recurring event, suggests Emory University student Alexa Mohsenzadeh, who made a TikTok video demonstrating the process. Mohsenzadeh says that she doesn’t miss apps. “I can tailor this to my needs and add notes about how I’m feeling and see if it’s correlated to my period,” she says. “You just have to input it once.” 3. Go analog and use a notebook or paper planner. We’re a technology publication, but the fact is that the safest way to keep your menstrual data from being accessible to others is to take it offline. You can invest in a paper planner or just use a notebook to keep track of your period and how you’re feeling. If that sounds like too much work, and you’re looking for a simple, no-nonsense template, try the free, printable Menstrual Cycle Diary available from the University of British Columbia’s Centre for Menstrual Cycle and Ovulation Research. 4. If your state is unlikely to ban abortion, you might still be able to safely use a period-tracking app. The crucial thing will be to choose one that has clear privacy settings and has publicly promised not to share user data with authorities. Quintin says Clue is a good option because it’s beholden to EU privacy laws and has gone on the record with its promise not to share information with authorities.",0
"The Download: Algorithms’ shame trap, and London’s safer road crossings",https://www.technologyreview.com/2022/06/30/1055300/download-algorithm-shame-london-safe-road-crossings-pedestrians-city/,2022-06-30,"<p>Plus: The crazy legal abortion patchwork across the US</p>
","This is today's edition of The Download, our weekday newsletter that provides a daily dose of what's going on in the world of technology. How algorithms trap us in a cycle of shame Working in finance at the beginning of the 2008 financial crisis, mathematician Cathy O’Neil got a firsthand look at how much people trusted algorithms—and how much destruction they were causing. Disheartened, she moved to the tech industry, but encountered the same blind faith. After leaving, she wrote a book in 2016 that dismantled the idea that algorithms are objective. O’Neil showed how every algorithm is trained on historical data to recognize patterns, and how they break down in damaging ways. Algorithms designed to predict the chance of re-arrest, for example, can unfairly burden people, typically people of color, who are poor, live in the wrong neighborhood, or have untreated mental-­health problems or addictions. Over time, she came to realize another significant factor that was reinforcing these inequities: shame. Society has been shaming people for things they have no choice or voice in, such as weight or addiction problems, and weaponizing that humiliation. The next step, O’Neill recognized, was fighting back. Read the full story.—Allison Arieff London is experimenting with traffic lights that put pedestrians first The news: For pedestrians, walking in a city can be like navigating an obstacle course. Transport for London, the public body behind transport services in the British capital, has been testing a new type of crossing designed to make getting around the busy streets safer and easier. How does it work? Instead of waiting for the “green man” as a signal to cross the road, pedestrians will encounter green as the default setting when they approach one of 18 crossings around the city. The light changes to red only when the sensor detects an approaching vehicle—a first in the UK.How’s it been received? After a trial of nine months, the data is encouraging: there is virtually no impact on traffic, it saves pedestrians time, and it makes them 13% more likely to comply with traffic signals. Read the full story.—Rachael Revesz Check out these stories from our new Urbanism issue. You can read the full magazine for yourself and subscribe to get future editions delivered to your door for just $120 a year. - How social media filters are helping people to explore their gender identity.- The limitations of tree-planting as a way to mitigate climate change. Podcast: Who watches the AI that watches students? A boy wrote about his suicide attempt. He didn’t realize his school's software was watching. While schools commonly use AI to sift through students' digital lives and flag keywords that may be considered concerning, critics ask: at what cost to privacy? We delve into this story, and the wider world of school surveillance, in the latest episode of our award-winning podcast, In Machines We Trust.Check it out here. ICYMI: Our TR35 list of innovators for 2022In case you missed it yesterday, our annual TR35 list of the most exciting young minds aged 35 and under is now out! Read it online here or subscribe to read about them in the print edition of our new Urbanism issue here. The must-reads I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.1 There's now a crazy patchwork of abortion laws in the USOverturning Roe has triggered a legal quagmire—including some abortion laws that contract others within the same state. (FT $)+ Protestors are doxxing the Supreme Court on TikTok. (Motherboard)+ Planned Parenthood’s abortion scheduling tool could share data. (WP $)+ Here’s the kind of data state authorities could try to use to prosecute. (WSJ $)+ Tech firms need to be transparent about what they’re asked to share. (WP $)+ Here’s what people in the trigger states are Googling. (Vox) 2 Chinese students were lured into spying for BeijingThe recent graduates were tasked with translating hacked documents. (FT $)+ The FBI accused him of spying for China. It ruined his life. (MIT Technology Review) 3 Why it’s time to adjust our expectations of AIResearchers are getting fed up with the hype. (WSJ $)+ Meta still wants to build intelligent machines that learn like humans, though. (Spectrum IEEE)+ Yann LeCun has a bold new vision for the future of AI. (MIT Technology Review)+ Understanding how the brain’s neurons really work will aid better AI models. (Economist $)4 Bitcoin is facing its biggest drop in more than 10 yearsThe age of freewheeling growth really is coming to an end. (Bloomberg $)+ The crash is a threat to funds worth millions stolen by North Korea. (Reuters)+ The cryptoapocalypse could worsen before it levels out. (The Guardian)+ The EU is one step closer towards regulating crypto. (Reuters)5 Singapore’s new online safety laws are a thinly-veiled power grabEmpowering its authoritarian government to exert even greater control over civilians. (Rest of World)6 Recommendations algorithms require effort to work properlyTelling them what you like makes it more likely it’ll present you with decent suggestions. (The Verge)7 China’s on a mission to find an Earth-like planetBut what they’ll find is anyone’s guess. (Motherboard)+ The ESA’s Gaia probe is shining a light on what’s floating in the Milky Way. (Wired $) 8 Inside YouTube’s meta world of video critiqueVideo creators analyzing other video creators makes for compelling watching. (NYT $)+ Long-form videos are helping creators to stave off creative burnout. (NBC)9 Time-pressed daters are vetting potential suitors over video chatTo get the lay of the land before committing to an IRL meet-up. (The Atlantic $) 10 How fandoms shaped the internet ❤️For better—and for worse. (New Yorker $) Quote of the day “This is no mere monkey business.” —A lawsuit filed by Yuga Labs, the creators of the Bored Ape NFT collection, against conceptual artists Ryder Ripps, claims Ripps copied their distinctive simian artwork, Gizmodo reports. The big story This restaurant duo want a zero-carbon food system. Can it happen? September 2020 When Karen Leibowitz and Anthony Myint opened The Perennial, the most ambitious and expensive restaurant of their careers, they had a grand vision: they wanted it to be completely carbon-neutral. Their “laboratory of environmentalism in the food world” opened in San Francisco in January 2016, and its pièce de résistance was serving meat with a dramatically lower carbon footprint than normal. Myint and Leibowitz realized they were on to something much bigger—and that the easiest, most practical way to tackle global warming might be through food. But they also realized that what has been called the “country’s most sustainable restaurant” couldn’t fix the broken system by itself. So in early 2019, they dared themselves to do something else that nobody expected. They shut The Perennial down. Read the full story.—Clint Rainey We can still have nice things A place for comfort, fun and distraction in these weird times. (Got any ideas? Drop me a line or tweet 'em at me.)+ A look inside the UK’s blossoming trainspotting scene (don’t worry, it’s nothing to do with the Irvine Welsh novel of the same name.)+ This is the very definition of a burn.+ A solid science joke.+ This amusing Twitter account compiles some of the strangest public Spotify playlists out there (Shout out to Rappers With Memory Problems)+ Have you been lucky enough to see any of these weird and wonderful buildings in person?",0
"The Download: Introducing our TR35 list, and the death of the smart city",https://www.technologyreview.com/2022/06/29/1055263/download-tr35-list-2022-death-smart-city-urbanism-issue-planning/,2022-06-29,"<p>Plus: Donald Trump's online fans are contradicting and confusing each other</p>
","This is today's edition of The Download, our weekday newsletter that provides a daily dose of what's going on in the world of technology. Introducing: Our TR35 list of innovators for 2022 Spoiler alert: our annual Innovators Under 35 list isn’t actually about what a small group of smart young people have been up to (although that’s certainly part of it.) It’s really about where the world of technology is headed next.As you read about the problems this year’s winners have set out to solve, you’ll also glimpse the near future of AI, biotech, materials, computing, and the fight against climate change.To connect the dots, we asked five experts—all judges or former winners—to write short essays about where they see the most promise, and the biggest potential roadblocks, in their respective fields. We hope the list inspires you and gives you a sense of what to expect in the years ahead. Read the full list here. The Urbanism issue The modern city is a surveillance device. It can track your movements via your license plate, your cell phone, and your face. But go to any city or suburb in the United States and there’s a different type of monitoring happening, one powered by networks of privately owned doorbell cameras, wildlife cameras, and even garden-variety security cameras. The latest print issue of MIT Technology Review examines why, independently of local governments, we have built our neighborhoods into panopticons: everyone watching everything, all the time. Here is a selection of some of the new stories in the edition, guaranteed to make you wonder whether smart cities really are so smart after all: - How groups of online neighborhood watchmen are taking the law into their own hands. - Why Toronto wants you to forget everything you know about smart cities. - Bike theft is a huge problem. Specialized parking pods could be the answer.- Public transport wants to kill off cash—but it won’t be as disruptive as you think. - How a rebellious French city is fighting back against its growing network of police surveillance cameras. Correction: A story in yesterday’s newsletter stated that the US had 60,000 charging stations for electric vehicles. There are in fact 6,000. We apologize for the error. The must-reads I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology. 1 Donald Trump’s online supporters are confusing themselves They’re tying themselves in knots trying to debunk former aide Cassidy Hutchinson’s testimony—despite what the evidence says. (WP $)+ Hutchinson’s January 6 testimony is the most damning yet. (The Atlantic $)+ A summary of the most shocking revelations she made. (Vox) 2 A pro-China influence campaign is targeting the rare earths industryThe group wants to undermine trust in Western companies to further China’s dominance of the sector. (MIT Technology Review) 3 Instagram is hiding posts that mention abortionPosts offering to sell and mail guns are fine though, apparently. (AP)+ Facebook was quick to label an abortion rights vandalism group ‘terrorists.’ (The Intercept)+ Big Tech remains silent on questions about data privacy in a post-Roe US. (MIT Technology Review) 4 Bitcoin is poised to drop even furtherJust as investors were starting to feel optimistic again. (Bloomberg $)+ A guy nicknamed ‘Crypto Jesus’ has allegedly racked up a $47 million crypto debt. (Motherboard)+ We’re still waiting for regulators to get their heads around crypto. (Slate $)+ Life, death, taxes, bitcoin volatility? (FT $) 5 Solar panels that shield crops from extreme heat are showing promiseBut if the crops taste bad, it’s all a colossal waste of money. (NYT $)+ These materials were meant to revolutionize the solar industry. Why hasn’t it happened? (MIT Technology Review)+ Readying the US for solar power is a thankless task. (Motherboard) 6 Debate streamers are guiding conspiracy theorists back to realityThanks to a combination of a light touch, logic and gentle compassion. (CNET)+ How to talk to conspiracy theorists—and still be kind. (MIT Technology Review) 7 A lot of seals look the same to us 🦭Facial recognition can help researchers tell them apart. (Hakai Magazine)+ A UK independent legal review wants to ban facial recognition. (FT $)8 How a deported hacker’s skills helped him to reenter the USBut also made him a target. (Rest of World)+ A former Uber security chief has been accused of hacking. (Reuters)+ The FBI is concerned by the rise of people using deepfakes to apply for jobs. (Insider)9 YouTube’s babies are growing up 🎂While their families have vlogged their lives since birth, the now-teenagers are pretty relaxed about it. (Rolling Stone)10 Happy 10th birthday to one of the internet’s favorite tweetsA decade on, it still rings true. (The Atlantic $) Quote of the day “They think they’re onto something visionary, but their product actually fails a basic logical test.” —Liron Shapira, a tech investor and writer, tells The Atlantic that web3 and crypto founders mostly have not worked out why their products need to be on the blockchain to begin with. The big story AI is learning how to create itselfMay 2021 A little stick figure with a wedge-shaped head shuffles across the screen. It moves in a half crouch, dragging one knee along the ground. It’s walking! Er, sort of. Yet Rui Wang is delighted. “Every day I walk into my office and open my computer, and I don’t know what to expect,” he says. An artificial-intelligence researcher at Uber, Wang likes to leave the Paired Open-Ended Trailblazer, a piece of software he helped develop, running on his laptop overnight. POET is a kind of training dojo for virtual bots. So far, they aren’t learning to do much at all. These AI agents are not playing Go, spotting signs of cancer, or folding proteins—they’re trying to navigate a crude cartoon landscape of fences and ravines without falling over. But it’s not what the bots are learning that’s exciting—it’s how they’re learning. It may seem basic at the moment, but for Wang and a handful of other researchers, POET hints at a revolutionary new way to create supersmart machines: by getting AI to make itself. Read the full story. —Will Douglas Heaven We can still have nice things A place for comfort, fun and distraction in these weird times. (Got any ideas? Drop me a line or tweet 'em at me.)+ Here’s why Sonic the Hedgehog is an exhausted millennial icon.+ A helpful guide explaining how to throw the perfect barbecue this summer.+ Yikes, who had gigantic pythons slithering around Florida for 2022 bingo?+ New York’s last public pay phone is now an exhibit in the Museum of the City of NY.+ These ants aren’t just hard workers—they’re also fossil hunters!",0
Social media filters are helping people explore their gender identity,https://www.technologyreview.com/2022/06/29/1054561/social-media-filter-gender-identity/,2022-06-29,"<p>On TikTok, Instagram and elsewhere, facial filters can mean a lot.</p>
","Every few months, a social media giant drops a new beauty filter with gender-tuning capabilities. TikTok’s “Bearded Cutie” gives you heavy brows and scruffy facial hair; the feminizing version of Snapchat’s “My Twin” lens smooths skin to porcelain and adds subtle glam makeup. For many, these filters are a lark, quickly forgotten once they stop trending. But others find themselves drifting back to the apps again and again, staring at their gender-bended reflection. Something, they feel, has suddenly crystallized. Oliver Haimson, an assistant professor at the University of Michigan who studies transgender identity and experiences online, says that for trans, gender-nonconforming, or gender-curious folk, filters can be a way to play with gender expression without the investment and skill that makeup requires or the time, hormones, and luck it takes to grow facial hair. He explains that filters are an important and widely used tool for identity exploration. Some trans people credit filters with finally “cracking their egg”—a rite of passage in the trans community when someone admits to themself that their gender identity is different from what was assigned at birth. “The Snapchat girl filter was the final straw in dropping a decade’s worth of repression,” says Josie, a trans woman in her early 30s from Cincinnati. “[I] saw something that looked more ‘me’ than anything in a mirror, and I couldn’t go back.” Filters can also provide a much-needed dose of gender euphoria, the rush of joy a trans person feels when their external appearance aligns with their gender identity. Others use filters to help map potential physical transitions. “The filters on FaceApp showed me how little my face needed to change in order to present more feminine,” says Etta Lanum, a 32-year-old from the Seattle area. “It demonstrated how a change in eyebrows and facial hair alone could get me where I needed to be.” Using these filters has its pitfalls as well. Some trans people feel that the technology sets them up for disappointment and dysphoria, showing “results” that are physically impossible to achieve even with plastic surgery, artful makeup, or hormone therapy. But given that an ever increasing percentage of our lives is lived online, who’s to say the filtered version isn’t the “real” you? Elizabeth Anne Brown is a science journalist based in Copenhagen, Denmark.",0
Public transport is ditching cash—but here’s why that’s ok,https://www.technologyreview.com/2022/06/29/1055154/public-transportation-cashless-explainer/,2022-06-29,"<p>The move to contactless fares might not have as much of an impact on accessibility as you think.</p>
","There are still parts of Philadelphia’s SEPTA transportation system that accept tokens. But today, in nearly every major American city, you’ll see transit riders tapping their way onto buses and subway platforms using their phones. The shift has been swift. Like so many things consumers brushed off as needlessly complicated before the pandemic—QR codes, order pickup at retail stores, grocery delivery—contactless transit fare collection has proved its convenience and been normalized. What will this change mean for less-privileged riders—those without smartphones or credit cards? Perhaps not much. Cash is likely to remain in some corners of transit systems, says Candace Brakewood, a professor of civil and environmental engineering at the University of Tennessee in Knoxville. That’s because the Federal Transit Administration requires large urban networks to ensure that any proposed fare changes don’t have a disproportionate impact on low-income and minority riders. And the kind of compensatory gymnastics that cities will need to perform to comply with this requirement may actually make it easier for even more people to use public transit. As bus systems experiment with eliminating onboard fareboxes, for example, operators will need to expand their fare vending networks—which include street-level vending machines as well as national chains like CVS and local businesses like check cashing centers. This is already happening in New York City, which has the largest public transit system in the country. The city is transitioning from its paper-thin, magnetic-stripe MetroCards to OMNY, a contactless system that uses near-field communication. OMNY supports “open-loop payments”—a special card or app isn’t needed to get onboard. Instead, you can simply swipe your existing contactless credit or debit card or wave a digital-wallet-equipped device. OMNY also offers a “closed-loop” option in the form of a physical card that can be reloaded with cash. In a statement, the city’s Metropolitan Transportation Authority said that it has “greatly expanded” its retail network since OMNY cards became available for purchase this past fall, citing nearly 1,000 partners selling and reloading them. It aims to quadruple that number once the system is fully activated. It’s easy to see the increased use of mobile payments as counter to the egalitarian ethos of public transit. But the technology can open up access to systems that would otherwise be difficult to navigate. “I struggle to think of a product that is more challenging to purchase when you come to a new city than public transportation,” says Joshua Schank, managing principal at InfraStrategies and a senior fellow at the UCLA Institute for Transportation Studies. Schank, who created the Office of Extraordinary Innovation at the Los Angeles County Metropolitan Transportation Authority in 2015 and headed it until this past January, sees the ultimate goal as payment integration between mobility systems—bike-shares, scooters, buses, trains—across cities as well as within them. But one way or another, contactless, open-loop payment will likely play the role that cash did in the early days of public transit: it will be accepted everywhere. Rachel del Valle is a freelance writer based in New York.",0
"The Download: Big Tech’s post-Roe silence, and the US EV charging landscape",https://www.technologyreview.com/2022/06/28/1055086/download-big-tech-wade-roe-abortion-user-data-us-ev-charging-landscape/,2022-06-28,"<p>Plus: How a Boston startup wants to make steel greener</p>
","This is today's edition of The Download, our weekday newsletter that provides a daily dose of what's going on in the world of technology. Big Tech remains silent on questions about data privacy in a post-Roe world In the days after the US Supreme Court overturned the constitutional right to abortion, tech companies rushed to show their support for employees living in states where the procedure is now outlawed. Meta promised to pay expenses for staffers who need to travel out of their home state for an abortion. Alphabet, Google’s parent company, told employees they could apply to relocate from states banning abortion.These companies have not given that same kind of support to their users, amid growing concerns that a digital footprint—including websites visited, location data from a phone, or private messages on a social platform—could be used to build a criminal case against someone seeking an abortion.MIT Technology Review asked five major tech companies—Alphabet, Meta, Reddit, TikTok, and Twitter—how their policies banning content promoting illegal activity will apply to posts advocating for abortion access or aiding those who now need to travel out of state for the procedure. Their responses, when provided, were inconsistent. Read the full story. —Abby Ohlheiser and Hana Kiros The U.S. only has 6,000 charging stations for EVs. Here’s where they all are. The United States has around 150,000 fuel stations to refill its fleet of fossil-fuel-burning vehicles. Despite the rapid growth of electric vehicles in America—400,000 of them were sold in 2021, up from barely 10,000 in 2012—the country has only 6,000 DC fast electric charging stations, the kind that can rapidly juice up a battery-powered car. A glance at America’s charging map reveals an abundance of charging deserts, particularly outside big cities. This makes sense, as EVs still represent less than 3% of new car sales. But while it’s illustrative of how American charging infrastructure lags far behind what’s needed for the whole country to transition to electric driving, there’s still time to catch up. Read the full story. —Andrew Moseman How green steel made with electricity could clean up a dirty industry The news: Startup Boston Metal has recently installed a new reactor at its headquarters, a significant step in its bid to make emissions-free steel. Since its founding in 2013, the company has developed a process to make green steel. The new reactor, along with a coming fundraising round, represents the next leap for the company as it tries to scale up. Why it’s important: Industrial steelmaking spits out about two tons of carbon dioxide emissions for every ton of steel produced—adding up to nearly 10% of such emissions worldwide. The global steel market is expected to grow about 30% by 2050, the date by which some of the largest steelmakers have pledged to reach net-zero emissions. Unless major changes come to the industry, and fast, that goal might be out of reach. What’s next?: Fossil fuels are essential to today’s steel production. If Boston Metal can indeed scale its clean production process and access enough renewable electricity to run it, the company could help solve one of the world’s toughest challenges in controlling carbon emissions. Read the full story. —Casey Crownhart The must-reads I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.1 Facebook is taking down posts offering to mail abortion pillsWhile the platform claims the posts violated its community standards, other posts about mailing painkillers were allowed to remain up. (Motherboard)+ What overturning Roe v Wade means. (MIT Technology Review)+ Ending abortion is particularly dangerous for black women. (The Guardian)+ The App Store’s most popular period tracker voluntarily hands over user data. (Motherboard)+ A Louisiana judge has temporarily blocked the state from banning abortion. (BBC) 2 CRISPR is 10 years old todayThe gene-editing technology changed the face of modern science. (NYT $)+ The scientist who co-created CRISPR isn’t ruling out engineered babies someday. (MIT Technology Review)+ CRISPR is even being taught in some high school classrooms. (NYT $)3 Big Tech’s antitrust battle is turning dirtyThe industry doesn’t want the legislation to make it to the Senate. Opponents are fighting back. (WP $)+ A Danish search engine has filed a new antitrust complaint against Google. (Reuters)4 The importance of naming new speciesA small British Columbia island community wants to protect its environment. Naming is part of it. (Hakai Magazine)5 NASA’s tiny spacecraft is testing a new elongated moon orbitCubeSat, which is the size of a microwave oven, is a guinea pig for future spacecraft. (The Verge)+ Don’t forget about all the other moons in our solar system. (Gizmodo)+ This is what NASA wants to do when it gets to the moon. (MIT Technology Review)6 A biotech entrepreneur has been charged with arranging a murderAfter dazzling medical investors with his ""innovative"" theories for treating HIV and covid. (WSJ $)7 Bacteria-inspired microbots don’t need AI right nowMuch simpler technology, like sensing temperature changes in a patient’s bloodstream, is enough to make them medically useful. (Knowable Magazine)8 What marine life can teach us about being human 🐙Sea creatures are pretty strange beings, too. (The Atlantic $)9 Does logging movies take the fun out of watching them?While some people find joy in documenting their cultural consumption, others claim it’s reductive. (The Guardian)10 A Nepali teacher is fixing the internet’s patchy depiction of his country By launching his own YouTube channel detailing his travels. (Rest of World) Quote of the day “Let’s face it, they got a bit rancid and people got sick.” —Artist Adrian Boswell explains to the Wall Street Journal why he stopped nailing real heads of broccoli to walls in London’s east end in favor of making NFTs of the vegetable. The big story Logging in to get kicked out: Inside America’s virtual eviction crisis December 2020 Before the pandemic, an average of 3.6 million Americans lost their homes to evictions every year, according to Princeton University’s Eviction Lab. That number is estimated to have vastly increased, with the financial hardship exacerbated by covid-19 leaving many in a precarious situation. Eviction hearings that used to be handled only in physical courtrooms are now taking place over video, or simply by phone conference. The result, say lawyers and tenants’ rights activists, is that an already problematic situation has become dramatically, tragically worse. Read the full story. —Eileen Guo We can still have nice things A place for comfort, fun and distraction in these weird times. (Got any ideas? Drop me a line or tweet 'em at me.) + Graffiti prophet Tsang Tsou-choi was the scourge of Hong Kong’s authorities for decades. Here’s his fascinating story.+ I could watch this adorable sushi conveyor belt all day.+ I think we can agree that these sci-fi apocalypse bunkers are all somewhat lacking.+ Maybe it’s high time we all got into birding.+ A city in Japan is campaigning to protect the beautiful forest that inspired the seminal anime My Neighbor Totoro.",0
Big Tech remains silent on questions about data privacy in a post-Roe US,https://www.technologyreview.com/2022/06/28/1055044/big-tech-data-privacy-supreme-court-dobbs-abortion/,2022-06-28,"<p>We asked Meta, Twitter, Google, TikTok, and Reddit how they will moderate abortion content and handle subpoenas and warrants for data on people who seek or assist with abortions.</p>
","In the hours and days after the US Supreme Court announced its ruling overturning the constitutional right to abortion, tech companies rushed to show their support for employees living in states where the procedure is now outlawed. Meta, Facebook’s parent company, promised to pay expenses for staffers who need to travel out of their home state for an abortion. Alphabet, Google’s parent company, told employees they could apply to relocate from states banning abortion. These companies have not given that same kind of support to their users, amid growing concerns that a digital footprint—including websites visited, location data from a phone, or private messages on a social platform—could be used to build a criminal case against someone seeking an abortion. On Friday, MIT Technology Review asked five major tech companies—Alphabet, Meta, Reddit, TikTok, and Twitter—how their policies banning content promoting illegal activity will apply to posts advocating for abortion access or aiding those who now need to travel out of state for the procedure. We also asked how they plan to respond to requests, subpoenas, or warrants for data that could be used to prosecute cases related to abortions in those states. Alphabet and Reddit did not respond at all to multiple emails requesting comment as of Monday evening. Meta, which also owns Instagram and WhatsApp, referred MIT Technology Review to its existing policies on government requests, and declined to answer questions on how those policies would apply to abortion. Meta also declined to say how the company plans to moderate content offering information about abortion or advocating for abortion access. TikTok said it will not restrict content about abortion but declined to say how it will respond to requests for data by law enforcement. Twitter referred to its terms of service and said that its rules generally allow for discussion of abortion, but it declined to say how its policies on illegal content and data requests will apply to abortion specifically. The Supreme Court’s ruling instantly criminalized terminating a pregnancy in Kentucky, Louisiana, and South Dakota, causing clinics to turn patients away midday. More than half the states are likely to ban abortion either outright or by passing extreme limitations. In the immediate aftermath of the ruling, what exactly constitutes a criminal offense is legally murky. Lawmakers in Missouri and South Dakota have already floated legislation to prosecute residents who cross state lines to terminate a pregnancy. New US restrictions could turn abortion into do-it-yourself medicine, but there might be legal risks. Companies committed to supporting people seeking abortions can “continue to rely on the protections of Section 230,” said Daly Barnett, a staff technologist at the Electronic Frontier Foundation, a tech-focused civil liberties organization, referring to the law that shields companies from being held liable for content posted to their platform. But, Barnett noted, some companies may err on the side of restrictiveness when it comes to abortion content, fearing laws criminalizing facilitating an abortion. And there’s a precedent for that: the SESTA/FOSTA laws signed in 2018 tweaked Section 230 to remove protections for content involved in the “promotion or facilitation of prostitution.” Most major tech companies have policies outlining how they will respond to requests from law enforcement and moderate illegal content. MIT Technology Review asked Google, Meta, Reddit, TikTok, and Twitter specific questions about how these policies will apply to situations related to abortion in light of the Supreme Court’s decision. Here’s what we know and what we found out: Google says in its terms of service that the company reserves the right to remove any content that violates the law or could harm other users, third parties, or Google itself. Those terms of service cover a wide range of products, including email, stored media, travel itineraries on Google Maps, and Google documents. This policy has major privacy and safety implications for those educating others online about abortion, advocating for abortion access, or seeking an abortion in states that have made the procedure illegal. It also has consequences for activists and organizers working on reproductive rights: Google Docs is a popular tool for quick, collaborative organizing around major social issues. Google-owned YouTube could also limit content about abortion. While YouTube’s rules on violent content do have an exception for educational videos, it’s not clear whether the platform’s policy against promoting “violent acts” could become a tool for anti-abortion activists under state laws that now criminalize the procedure. For instance, they could simply identify content across Google’s platforms that advocates for abortion access or provides resources for those seeking an abortion, and start reporting it to Google’s moderators. Recently, Google was urged by multiple members of Congress to stop collecting user location data, including data on Android phones, that could identify people who visited abortion clinics. The legislators also asked the company to clarify how it would respond to geofence warrants—requests from law enforcement for data on everyone who visits a certain location. The company has yet to respond to the lawmakers’ letter and did not respond to any of MIT Technology Review’s requests for comment on how it will handle government requests for such data and user reports of content about abortion. Meta, which has policies banning some forms of violent or illegal content, would not describe how it plans to moderate content on abortion access after the Dobbs ruling. But Motherboard reported that Facebook has removed posts by individuals offering to mail abortion pills to others. The US Food and Drug Administration permits these medications, which are currently the most common method of abortion, to be prescribed via telemedicine and taken at home in early pregnancy. Nevertheless, 19 states have already banned using telemedicine for abortion. Posts merely noting that abortion pills can be mailed have also been removed from Facebook, according to Motherboard. NBC has reported that Instagram, also owned by Meta, is withholding search results for “abortion pills” and “mifepristone,” the name of a drug often used in medication abortions. A message explains that recent posts under both hashtags are hidden because some may violate the app’s community guidelines. Abortion pills prescribed by overseas providers not bound by US laws remain available for mail order in every state. In a tweet, Andy Stone, Meta’s communications director, pointed to the parent company’s ban on content facilitating pharmaceutical transactions. When asked whether the company would commit to stop collecting and retaining data that could be used to identify or prosecute people seeking or providing abortions, and how it would handle government subpoenas or warrants from states that have outlawed the procedures, Meta directed us to a transparency report released in May 2022, in which Chris Sonderby, Meta’s VP and deputy general counsel, writes that the company assesses “whether a request is consistent with internationally recognized standards on human rights, including due process, privacy, free expression and the rule of law.” Meta declined to provide any specifics on how these considerations, and the company’s content moderation policies, might apply in the United States’ rapidly changing patchwork of abortion laws. TikTok’s community guidelines state that the app prohibits users from promoting or facilitating illegal activities. The app removes content related to illegal acts to prevent them from being “normalized, imitated, or facilitated.” Content may be eligible for removal if it relates to activities “regulated or illegal in the majority of the region or world,” even if the activity is legal where a user lives. After Friday’s ruling, TikTok creators in states where abortion remains legal began to offer their homes as free lodging for those who must now cross state lines to obtain an abortion. The message often came in coded language: one offers a place to “camp” in North Carolina for people living anywhere that “doesn't allow camping.” Experienced advocates for abortion access have encouraged people looking to help to instead support established networks of activists providing funding and housing for those traveling out of state. In response to a request for comment on how TikTok will moderate abortion content, spokesperson Jamie Favazza wrote: “Our policies do not prohibit the topic of abortion, including content containing information or someone's personal experience.” Favazza declined to comment when asked how the app would respond to warrants and subpoenas asking for data such as watch and search history, private messages, comments, and IP addresses pinpointing where a user logs in, logs out, and posts content, any of which might be requested in the interest of prosecuting someone who had an abortion. On Reddit, the community r/abortion serves as a support group for those seeking to terminate a pregnancy. The subreddit helps users navigate abortion access and provides broad support to abortion seekers. Since federal abortion protections were overturned, posts have appeared from people weighing their options in states where the procedure is illegal. Reddit’s user agreement states that people may not use its services to violate “applicable” law. Reddit did not respond to a request for comment on how the site will moderate abortion content or handle user data that could be used to prosecute someone seeking or assisting with an abortion. In 2021, Reddit received 1,100 requests for user information from law enforcement or government entities globally and complied with 60% of those requests. The company says in its transparency report that it discloses specific user information where required by law and in certain emergencies. For people who complete a financial transaction on the site, this can include information such as full name and home address. The site collects all users’ posts, comments, and private messages and retains, for 100 days, every IP address used to access the service. Reddit’s transparency report says that when the site receives a removal request for content that violates local law but does not violate its own content policy, it may restrict the availability of that content or “object when appropriate.” Since Reddit didn’t respond to a request for comment, it’s unclear how this policy may impact the sharing of resources on how to access abortion in states with bans. Twitter similarly bans the use of its services in furtherance of illegal activities. It allows “public-interest exceptions” to this rule, but so far, the only exceptions it has made have been for tweets from government officials (most famously Donald Trump), which Twitter flagged but did not remove. According to Twitter spokesperson Elizabeth Busby, Twitter’s rules “generally do not prohibit discussion about abortion, contraceptives, and related topics.” When asked repeatedly whether Twitter would comply with subpoenas, search warrants, and other requests for data that could be used to prosecute abortion seekers, Twitter declined to comment. Instead, Busby pointed to the company’s terms of service, privacy policy, and general law enforcement practices. Until tech companies specify how they will cooperate with data requests for abortion prosecutions, it’s not clear what user data they might offer up. Still, the responsibility for keeping their data private doesn’t necessarily have to fall to users alone. According to the EFF, tech companies can dump out data logs they aren’t using, protect user location data, and push back against improper and unnecessary requests from government and law enforcement agencies. The EFF particularly urges tech companies not to comply with warrants targeting everyone at a certain location. In 2020, Google received 11,554 geofence warrants from law enforcement and government agencies, including police in all 50 states. Google has repeatedly complied with geofence warrants, some of which have been used to identify Black Lives Matter protesters, but does not release data on its compliance rate. (In March a federal judge ruled that a geofence warrant was unconstitutional and lacked “particularized probable cause,” but it’s unclear how this will affect tech companies’ compliance.) Barnett, the EFF staff technologist, says those seeking information on abortions should move away from services offered by large tech companies that don’t prioritize transparency and privacy. For people in states where abortion is newly criminalized, she says, it’s best to “move to services that were built with the privacy of users in mind.” For those concerned, the Washington Post and Wired have created guides on how to protect your digital privacy in a post-Roe world.",0
"The Download: Facebook’s misleading cancer ads, and hacking’s next era",https://www.technologyreview.com/2022/06/27/1054989/download-facebook-misleading-cancer-ads-treatment-unproven-hacking-nso-group-us/,2022-06-27,"<p>Plus: Why we need smarter cities and not ""smart cities""</p>
","This is today's edition of The Download, our weekday newsletter that provides a daily dose of what's going on in the world of technology. Facebook is bombarding cancer patients with ads for unproven treatments The ad reads like an offer of salvation: Cancer kills many people. But there is hope in Apatone, a proprietary vitamin C–based mixture, that is “KILLING cancer.” The substance, an unproven treatment that is not approved by the FDA, is not available in the United States. If you want Apatone, the ad suggests, you need to travel to a clinic in Mexico.If you’re on Facebook or Instagram and Meta has determined you may be interested in cancer treatments, it’s possible you’ve seen this ad. It is part of a pattern on Facebook of ads that make misleading or false health claims, targeted at cancer patients. Evidence from Facebook and Instagram users, medical researchers, and its own Ad Library suggests that Meta is rife with ads containing sensational health claims, which the company directly profits from, with some misleading ads remaining unchallenged for months and even years. Read the full story.—Abby Ohlheiser The hacking industry faces the end of an era The news: NSO Group, the world’s most notorious hacking company, could soon cease to exist. The Israeli firm, still reeling from US sanctions, has been in talks about a possible acquisition by the American military contractor L3 Harris. The deal is far from certain, but if it goes through, it’s likely to involve the dismantling of NSO Group and the end of an era. Industry-wide turbulence: No matter what happens to NSO, the changes afoot in the global hacking industry are far bigger than any single company. That’s mostly down to two major changes: the US sanctioned NSO in late 2021, and days later the Israeli government severely restricted its hacking industry, cutting the number of countries firms can sell to from over 100 to just 37. But… The industry is adjusting rather than disappearing. One thing we’re learning is that a vacuum can’t last long in a market where demand is so high. Read the full story.—Patrick Howell O’Neill We need smarter cities, not “smart cities” The term “smart cities” originated as a marketing strategy for large IT vendors. It has now become synonymous with urban uses of technology, particularly advanced and emerging technologies. But cities are more than 5G, big data, driverless vehicles, and AI, and a focus on building “smart cities” risks turning cities into technology projects.Truly smart cities recognize the ambiguity of lives and livelihoods, and they are driven by outcomes far beyond the implementation of “solutions.” They are defined by their residents’ talents, relationships, and sense of ownership—and not by the technology deployed there. Read the full story. —Riad Meddeb and Calum Handforth Coming soon: The TR35 list of innovators for 2022 On Wednesday, we’re announcing this year’s list of 35 Innovators Under 35: a chance to take a look at not just where technology is now, but where it’s going and the brilliant young minds that are making it happen. The full list is in the latest issue of our print magazine and online from 29 June. You can subscribe here. The must-reads I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology. 1 Period tracking apps are rushing to anonymize their user dataFollowing the Supreme Court’s decision to strike down Roe v Wade, experts are concerned menstrual data could be exploited to incriminate people seeking abortions. (WSJ $)+ How people seeking abortions can avoid leaving a digital trail. (WP $) + Roe discussions among Big Tech workers quickly soured last week. (Bloomberg $)+ It’s mostly safe to store abortion pills for later use. (New York Mag)+ High quality sex education is also under threat. (Vox)+ Where to get abortion pills and how to use them. (MIT Technology Review) 2 China’s surveillance network is predicting crime and dissent before it happensAnd surveilling vulnerable people, including those experiencing mental illnesses. (NYT $) 3 Inflation isn’t going away any time soon 📈But falling prices could provide a welcome respite. (Economist $) 4 Crypto’s elites don’t care about you They also don’t care if you lose your life savings investing in their dodgy wares. (The Atlantic $) + Singapore has had enough of crypto cowboys. (The Register)+ Crypto is weathering a bitter storm. Some still hold on for dear life. (MIT Technology Review) 5 The US is bungling its big semiconductor opportunityAnd the chance to create thousands of jobs in the process. (WP $)+ Taiwan, the world’s biggest chip producer, is facing a spike in power costs. (Bloomberg $) + Meanwhile, Japan is worrying about a shortage of specialist chip engineers. (FT $)+ The chip boom could be coming to an end. (The Register) 6 Q is backA year after they last posted, the QAnon leader has returned. (NYT $)+ Evangelicals are looking for answers online. They’re finding QAnon instead. (MIT Technology Review) 7 Social media is a minefield for therapists Professionals are conflicted about whether jumping on trends compromises their expertise. (Slate $) 8 Amazon’s product results are all over the placeBasically, it’s down to the company prioritizing US companies over Chinese firms. (WSJ $)+ China’s selling community isn’t best pleased about it. (SCMP) 9 AI learns in virtual worldsBut when it comes to robots, a physical environment is still essential. (Quanta)+ Sophisticated AI DALL-E has started generating fake human faces. (Motherboard)+ Revealing how AI programs answer questions like humans destroys their mystique. (Wired $)+ A quick guide to the most important AI law you’ve never heard of. (MIT Technology Review) 10 You smell like your friends 👃Whether you realize it or not. (Economist $) Quote of the day “I went from no one knowing who I was to all of the worst people on the internet knowing who I am.” —Clara Sorrenti, a trans Twitch star known as Keffals, tells the Washington Post how she became targeted by anti-trans activists and other trolls after starting to discuss politics on the streaming platform. The big story It’s time to rethink the legal treatment of robots October 2020 AI has the potential to help us deal with vast societal challenges, like health inequalities, racial biases, and political polarization. However, its risks have become increasingly apparent, including opacity and lack of explainability, and design choices that result in bias. Whether AI is developed and used in good or harmful ways will depend in large part on the legal frameworks governing and regulating it. There should be a new guiding tenet to AI regulation, a principle of legal neutrality asserting that the law should tend not to discriminate between AI and human behavior. Currently, the legal system is not neutral—for example, an AI that is significantly safer than a person may be the best choice for driving a vehicle, but existing laws may prohibit driverless vehicles. Neutral legal treatment would ultimately benefit human wellbeing by helping the law better achieve its underlying policy goals. Read the full story. —Ryan Abbott We can still have nice things A place for comfort, fun and distraction in these weird times. (Got any ideas? Drop me a line or tweet 'em at me.)+ Better Call Saul’s creators discuss some of the most crucial moments from the much-loved series, which comes to an end next month.+ This 104-year old man has turned to YouTube in search of “fellowship” 🥹. + I too would like to know the answer to this.+ Are we witnessing the end of music genres? Answers on a postcard, please.+ These 35 summer reads are bound to leave you captivated.",0
Facebook is bombarding cancer patients with ads for unproven treatments,https://www.technologyreview.com/2022/06/27/1054784/facebook-meta-cancer-treatment-ads-misinformation/,2022-06-27,"<p>Clinics offering debunked cancer treatments are still allowed to advertise, despite the company’s stated efforts to control medical misinformation.</p>
","The ad reads like an offer of salvation: Cancer kills many people. But there is hope in Apatone, a proprietary vitamin C–based mixture, that is “KILLING cancer.” The substance, an unproven treatment that is not approved by the FDA, is not available in the United States. If you want Apatone, the ad suggests, you need to travel to a clinic in Mexico. If you’re on Facebook or Instagram and Meta has determined you may be interested in cancer treatments, it’s possible you’ve seen this ad, or one of the 20 or so others recently running from the CHIPSA hospital in Mexico near the US border, all of which are publicly listed in Meta’s Ad Library. They are part of a pattern on Facebook of ads that make misleading or false health claims, targeted at cancer patients. Evidence from Facebook and Instagram users, medical researchers, and its own Ad Library suggests that Meta is rife with ads containing sensational health claims, which the company directly profits from. The misleading ads may remain unchallenged for months and even years. Some of the ads reviewed by MIT Technology Review promoted treatments that have been proved to cause acute physical harm in some cases. Other ads pointed users toward highly expensive treatments with dubious outcomes. CHIPSA, which stands for Centro Hospitalario Internacional del Pacifico, S.A, was founded in 1979 and refers to itself as a community hospital offering integrative treatments for cancer. On Facebook, the facility describes itself as being at the “cutting edge” of cancer research. But the hospital’s foundational diet-based therapy, called the Gerson Protocol, is “all nonsense,” says David Gorski, a surgical oncologist at Wayne State University in Michigan and the managing editor of the website Science-Based Medicine. Developed by a German doctor in the 1920s to treat migraines, the regimen consists of a special diet and frequent “detox” procedures. It has been discredited for decades in the medical community. CHIPSA did not respond to repeated requests via phone and email for comment. MIT Technology Review alerted Meta to five CHIPSA ads, along with three ads from another international clinic called Verita Life. In response, Meta spokesperson Mark Ranneberger said that it had removed “several of the ads for violating our misleading claims policy, which prohibits claims of cures for incurable diseases.” When asked for the specifics of the ads removed, Ranneberger said that two were rejected: the one claiming that Apatone was “killing” cancer and another that mentioned “growing distrust” of the US health-care system while advertising exclusive cancer treatments. Another ad using identical text to that second one but a different image remains active. On Monday, after the publication of this article, Meta noted that it had removed three additional ads using the same language. “Us cancer patients and survivors, we are just bombarded with all these kinds of alternative things all the time,” says Nikhil Autar, a medical student in Australia who has acute myeloid leukemia. Autar started seeing ads for cancer treatment centers on Facebook in 2019—just as Facebook and other platforms began rolling out new policies designed to limit the reach of health misinformation. Facebook has drastically stepped up its efforts to stop the spread of sensational and false health claims over the past few years. After a series of local measles outbreaks in the US in 2019, it announced it would start treating misleading health claims like spam, reducing their reach on news feeds and limiting the visibility of private Facebook groups promoting dubious treatments. When the covid-19 pandemic began, the company rolled out more comprehensive efforts to remove or limit such claims as conspiracies about the virus, masks, and vaccines spread on its platform. These attempts to combat pitches for miracle cures and dubious medical advice have been a step in the right direction, says Rachel Moran, a postdoctoral fellow at the Center for an Informed Public at the University of Washington. However, many such ads continue to slip through. One from Verita Life, in Bangkok, Thailand, targeted Australians like Autar, falsely claiming that a hypothermia treatment offered there would “destroy cancer cells.” When Autar took a screenshot of the ad in his news feed in August of 2020, it had more than a thousand likes and 600 shares. Autar reported the ads he saw to Facebook using its in-platform systems, but they remained up. At one point, he says, he used a Silicon Valley connection to try to flag the ads directly to Facebook management. He stopped seeing the clinic’s ads in the Ad Library and on his own feed after that, but they returned a few months later. Both CHIPSA and Verita Life had several ads running on Facebook and Instagram before MIT Technology Review inquired about them, according to the Ad Library. Verita Life was able to place an ad as recently as June 18, 2022, promoting the testimonial of a patient with prostate cancer. MIT Technology Review flagged that ad, along with two others promoting the same testimonial. All three remain active. Meta reviews new ads through a largely automated process before they go live. The company noted that ads and posts from CHIPSA’s Facebook page and Instagram account are eligible to be flagged and fact-checked by third-party fact checkers. If a company repeatedly violates its policies, Meta says, it will temporarily suspend the company’s ability to place ads. While Meta has rules pertaining to, for instance, misleading claims in ads, all Facebook and Instagram ads must also follow Meta’s community guidelines. The guidelines ban content “promoting or advocating for harmful miracle cures for health issues” when those claims both contribute to serious injury or death and have no legitimate health use. Those rules, even when swiftly enforced, can leave a lot of gray area for sensational claims, Gorski says, because “a lot of quackery could have a legitimate health use.” For instance, he says, “vitamin C obviously has legitimate health uses; it just doesn’t cure cancer.” So what about Apatone, the treatment advertised by CHIPSA? Pre-clinical research indicates some anti-cancer effect, but it “has not been demonstrated to be more beneficial than standard treatments we are using currently in humans,” says Skyler Johnson, a cancer researcher who studies misinformation at the University of Utah. The danger is not simply that the treatments are unproven or ineffective. Some alternative cancer treatments advertised on the platform can cause physical harm. Coley’s toxins, a treatment developed in the late 19th century and offered at CHIPSA, comes with risks including infection, site reactions, anaphylaxis, and in severe cases shock, says Johnson. Unproven treatments can also interact poorly with conventional treatments like chemotherapy should a patient decide to pursue alternative care on their own. Moreover, simply delaying the start of proven therapies by detouring into unproven ones can allow the cancer to advance, complicating and diminishing the effectiveness of further treatment. Johnson’s research has demonstrated worse survival rates for patients who seek unproven cancer treatments at first. In a 2017 study, he found that after about five years, patients with breast cancer who delayed conventional treatment in favor of alternative medicine were more than five times more likely to die. There’s the financial burden, too—because clinics like CHIPSA aren’t generally covered by insurance, patients often have to raise money to afford their treatments. One recent GoFundMe campaign for a cancer patient seeking treatment at CHIPSA included a screenshot of a bill for the “base amount” he’d have to pay. It was $36,500 for three weeks of inpatient care in Mexico. That cost would increase once the facility decided on a treatment plan. CHIPSA has spent about $5,000 since mid-2018 advertising on Meta about social issues, politics, or elections, according to information available in the Ad Library before Meta removed two of its ads. CHIPSA did not respond to requests for details on its ad spending or the cost of the treatments it offers. Gorski is blunt about his view on whether Facebook will effectively address cancer misinformation: “The only real way to combat such misinformation on Facebook would require an army of fact checkers that Facebook is never going to pay for, given its past record even on covid-19 misinformation and dangerous political conspiracy theories.” And as the University of Washington’s Moran points out, misinformation like this rarely stays confined to the platform where it’s originally posted. While Facebook plays a key role in getting sensational claims about dubious cancer treatments in front of desperate patients, the groups and ads carrying those claims often link to other sites and networks that reinforce them. Johnson, using data from 2017 to 2019, has observed that articles and videos containing myths about cancer treatment often receive more social media engagement than those from “safe” sources. And although it’s tricky to say for sure, his and other research in this area suggests that as many as one in three online articles or videos posted online about cancer may contain harmful misinformation. “Especially when you are experiencing a medical crisis, you are looking at an incredible amount of information,” Moran says. “It seems good to you that you are doing your research, you're going from one site to the next. But they all belong to the same ecosystem.” This post has been updated with additional information from Meta If you or a loved one has been treated for cancer, we'd like to talk to you for future stories about sensational health claims on social media. If you are a member of support groups for cancer patients or their loved ones, or have experience with clinics like the ones mentioned in this story, please be in touch: abby.ohlheiser@technologyreview.com",0
Composable enterprise spurs innovation,https://www.technologyreview.com/2022/06/30/1055126/composable-enterprise-spurs-innovation/,2022-06-30,"In March 2020, when corporate offices shuttered in the face of the coronavirus pandemic and employees began working from home, companies were forced to find more efficient ways to do business. Call it “The Great Digital Transformation.” Before the pandemic, the average company estimated that transitioning to remote work would take 454 days, according to…","In association withRingCentral In March 2020, when corporate offices shuttered in the face of the coronavirus pandemic and employees began working from home, companies were forced to find more efficient ways to do business. Call it “The Great Digital Transformation.” Before the pandemic, the average company estimated that transitioning to remote work would take 454 days, according to the “McKinsey Global Surveys, 2021: A year in review.” Most companies accomplished the move in 11 days during the pandemic. Similarly, the average company estimated that migrating its assets to the cloud would take 547 days; in reality, shifting to that key component of digital transformation took only 23 days. Overall, 74% of companies accelerated plans to move to the cloud by more than a year, jettisoning legacy technologies and operating models to embrace data and applications, according to business analysis firm ZK Research. A key part of that transformation relied on using applications, usually in the cloud, that integrated apps and data with low-code functionality to create more efficient workflows, more quickly than ever. Low-code is a software development approach for building processes and functionality with little or no code, which allows non-software developers to create applications. Companies that structure daily workflows around these so-called “composable applications”—often called composable enterprises—have a much tighter relationship between technology and business units and can quickly assemble new applications and services at a fraction of the historical cost. Composable applications provide a way to build on or add to applications in an easy way—think of building blocks: the work has already been done and additional functionality can be added to the foundational ability. That flexibility is necessary for the variability of the current workplace and economy, says Zeus Kerravala, founder and principal analyst at ZK Research. “We’re moving to an era where in any given moment, you could have everyone in the office, no one in the office, or every reasonable combination in between,” Kerravala says. “You could have all your shoppers online, only a few, or—depending on your industry—no shoppers online and every possible combination between. The pandemic has created these dramatic shifts in the way we learn, the way we live, and the way we work, based on forces that are outside of anyone’s control.” When it comes to cloud infrastructure, companies have often pursued half measures—adopting it in such a way as to reinforce old business models, creating private clouds that mimic their on-premises infrastructure. But composability gives enterprises the ability to adapt to changes in operations and in their markets by creating new applications to support needed workflows without hiring additional or outside software developers to implement the changes. Composable cloud services further liberate companies from relying on running their own software instances solely to customize the code to their needs. Composable applications bring together cloud, customization, integration, and workflow management, allowing companies to be flexible and innovate quickly. When businesses suffered pandemic disruptions to critical business functions—such as call centers, IT support, and medical administration—composable applications allowed firms to adapt and continue. In one case, a company needed to extend its call-center system, which was hosted in a controlled environment, to allow access to employees through web browsers running on an Amazon virtual machine, says David Lee, vice president of products at RingCentral, an enterprise communications platform that has focused on composability. “They had to make these changes work overnight at employees’ homes, and that was a great challenge for a lot of organizations,” Lee says. “Companies well-adapted to potential change actually made these transitions very easy by composing new applications and workflows.” Businesses dealing with changes are not the only ones that benefit from composable applications focused on workflows. The composable enterprise will become the strategic goal for many companies in the 2020s. Companies have moved from having a few critical systems on which all work is done—what some have termed “systems of gravity”3—to working on a plethora of task-based tools and services to get specific work completed. Software as a service (SaaS) and the mobile application ecosystem has enabled—and fueled—this transition by lowering the barrier to moving to new applications and allowing individual workers to select their own tools. Although companies have made headway in shrinking their overall technical debt—reducing the average number of applications they need to manage from 1,020 in 2018 to 843 in 2021—only a quarter of those applications are well integrated. Lowering the barrier to be able to link applications— and the data associated with those applications— and create new workflows is the heart of composable business operations. For example, customer relationship management (CRM) applications brought together several functions common to a specific workflow. However, it wasn’t until those applications moved into the cloud and were linked to communications platforms that they enabled more complex workflows and become composable applications, says Gordon Macomber, CEO at Q13 Advisory, a consulting company that works with companies dealing with unsettled markets. “Composability forces you to put things into these workflow buckets, either internally or externally,” he says. “Workflow is the atomic unit.” Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.",0
Society wants you to feel ashamed of yourself,https://www.technologyreview.com/2022/06/29/1053985/society-shame-book-review/,2022-06-29,"<p>Algorithm expert Cathy O'Neil has written a new book that shows how the tech world and society generally feeds off the idea of shame.</p>
","Working in finance at the beginning of the 2008 financial crisis, Cathy O’Neil got a firsthand look at how much people trusted algorithms—and how much destruction they were causing. Disheartened, she jumped to tech, where she found the same blind faith in everything from targeted advertising to risk-assessment models for mortgage-­backed securities. So she left. “I didn’t think what we were doing was trustworthy,” she says. The feeling of being “a co-conspirator, an unwitting tool in the industry” lit the fire under her to write Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Published in 2016, the book dismantled the idea that algorithms are objective, revealing instead—in example after example—how they can and do perpetuate inequality. Before her book came out, says O’Neil, “people didn’t really understand that the algorithms weren’t predicting but classifying … and that this wasn’t a math problem but a political problem. A trust problem.” O’Neil showed how every algorithm is optimized for a particular notion of success and is trained on historical data to recognize patterns: e.g., “People like you were successful in the past, so it’s fair to guess you will be successful in the future.” Or “People like you were failures in the past, so it’s fair to guess you will be a failure in the future.” This might seem like a sensible approach. But O’Neil’s book revealed how it breaks down in notable, and damaging, ways. Algorithms designed to predict the chance of rearrest, for example, can unfairly burden people, typically people of color, who are poor, live in the wrong neighborhood, or have untreated mental-­health problems or addictions. “We are not really ever defining success for the prison system,” O’Neil says. “We are simply predicting that we will continue to profile such people in the future because that’s what we’ve done in the past. It’s very sad and, unfortunately, speaks to the fact that we have a history of shifting responsibilities of society’s scourges to the victims of those scourges.” Gradually, O’Neil came to recognize another factor that was reinforcing these inequities: shame. “Are we shaming someone for a behavior that they can actually choose not to do? You can’t actually choose not to be fat, though every diet company will claim otherwise. Can you choose not to be an addict? Much harder than you think. Have you been given the opportunity to explain yourself? We’ve been shaming people for things they have no choice or voice in.” I spoke with O’Neil by phone and email about her new book, The Shame Machine: Who Profits in the New Age of Humiliation, which delves into the many ways shame is being weaponized in our culture and how we might fight back. The trajectory from algorithms to shame isn’t immediately apparent. How did you connect these two strands? I investigated the power behind weaponized algorithms. Often, it’s based on the idea that you aren’t enough of an expert to question this scientific, mathematical formula, which is a form of shaming. And it was even more obvious to me, I think, because as a math PhD holder, it didn’t work on me at all and in fact baffled me. The power of bad algorithms is a violation of trust, but it’s also shame. You do not know enough to ask questions. For example, when I interviewed a friend of mine, who is a principal whose teachers were being evaluated by the Value Added Model for Teachers in New York City, I asked her to get her hands on the formula that her teachers were targeted by. It took her many layers of requests, and each time she asked she was told, “It’s math—you won’t understand it.” In The Shame Machine, you argue that shame is a massive structural problem in society. Can you expand on that? The tech giants are paying millions of dollars to the operators of clickbait pages, bankrolling the deterioration of information ecosystems around the world. Shame is a potent mechanism to turn a systemic injustice against the targets of the injustice. Someone might say, “This is your fault” (for poor people or people with addictions), or “This is beyond you” (for algorithms), and that label of unworthiness often is sufficient to get the people targeted with that shame to stop asking questions. As just one example, I talked to Duane Townes, who was put into a reentry program from prison that was essentially a no-end, below-poverty-­level manual-labor job done under the eye of armed men who would call his parole officer if he complained or took a bathroom break for longer than five minutes. It was humiliating, and he felt that he was treated as less than a man. This was by intentional design of the program, though, and was meant to train people to be “good workers.” It’s tantamount to a taser to one’s sense of self. It causes momentary helplessness and the inability to defend one’s rights. Did covid-19 exacerbate the issues you highlight in your new book? Well, it introduced more fast-­changing norms, around masking, distancing, and vaccinations, so in that sense the shaming became pervasive. It was also obvious that the tribes that manifested on social media and inside politics took on these norms very differently, which caused huge shame wars online and in person. The way shame works is to move people who somewhat disagree further away from each other. In other words, shame backfires when there is no community trust. The more each side lobbed outrage and shame at the other, the further apart people grew. In 2021, California became the first state to offer free lunch to all students, not just the economically disadvantaged, which has really helped to remove a long-held stigma. What are some other ways we design systems to be less about shame? Are there ways we can harness shame for social reform? That’s a great example! Another one that I suggest is to make it a lot easier to qualify for welfare [or] have a universal basic income, and to relieve student debt burdens. The systematic shaming of poor people in this country has meant there’s little solidarity among poor people. That’s almost entirely due to successful shaming campaigns. Poor people would advocate for debt relief and UBI themselves if we didn’t have such a successful shame machine at work. The chapter on “networked shame” explores how the algorithms of Facebook, Google, and others are continually optimized to spur conflict among us. How does this benefit them? What can be done to counteract it? It’s their bread and butter! If we didn’t get outraged and spun out on defending our sense of worthiness and getting the likes and retweets based on performative and often destructive shaming, they’d make way less money. I want us to start seeing the manipulation by the big tech companies as a bid for us to work for them for free. We shouldn’t do it. We should aim higher, and that means at them. At an individual level, that means we refuse to punch down on social media if possible, or even boycott platforms that encourage that. At a systematic level, we insist that the designs of the platforms, including the algorithms, be audited and monitored for toxicity. That’s not a straightforward suggestion, but we know that, for example, Facebook tried doing this [in 2018] and found it to be possible but less profitable, so they rejected it. After Weapons was published you started ORCA, an algorithmic auditing company. What does the company’s work entail? Algorithmic auditing, at least at my company, is where we ask the question “For whom does this algorithmic system fail?” That could be older applicants in the context of a hiring algorithm, or obese folks when it comes to life insurance policies, or Black borrowers in the context of student loans. We have to define the outcomes that we’re concerned about, the stakeholders that might be harmed, and the notion of what it means to be fair. [We also need to define] the thresholds that determine when an algorithm has crossed the line. So can there ever be a “good” algorithm? It depends on the context. For hiring, I’m optimistic, but if we don’t do a good job defining the outcomes of interest, the stakeholders who might be harmed, and—most crucially—the notion of fairness as well as the thresholds, then we could end up with really meaningless and gameable rules that produce very problematic algorithmic hiring systems. In the context of, say, the justice system, the messiness of crime data is just too big a problem to overcome—not to mention the complete lack of agreement on what constitutes a “successful” prison stay. This interview has been edited for length and clarity.",0
The online vigilantes solving local crimes themselves,https://www.technologyreview.com/2022/06/29/1054553/online-vigilantes-local-crime/,2022-06-29,"<p>Online groups of neighbors love gossiping about local misdeeds. But are they helping or hurting?</p>
","One evening last summer, my family was enjoying a picnic in the park near our house in London when two dogs attacked our blind 15-year-old Jack Russell terrier, Zoey. They pounced on her, locking their jaws. As my husband threw himself on the dogs, I begged the owner to intervene. He refused—until he realized I was calling the police. Only then did he restrain his animals, one of which had started to chase my four-year-old daughter. A few hours later Zoey was dead, leaving us devastated. We felt even worse when the police didn’t attempt to track down the owner of the killer dogs, despite having images from my phone to go on. In the eyes of the UK justice system, Zoey’s killing was a low-level crime because an animal, rather than a human, had died. The realization galvanized us: if the police wouldn’t find the culprits, then we would. Increasingly, communities are turning to technology to help solve problems that the police are unable—or unwilling—to attend to. So that’s what I did: I went online, joining an increasing number of people who are using local networks to solve crimes that have affected them, such as robberies, reckless driving, and even plant theft. One in 10 posts on the neighbor-networking site Nextdoor is related to crime and policing matters. I had nearly 800 neighbors on that platform and was also in several neighborhood groups on Facebook, whose members totaled 74,000. In all, my description of the attack on Zoey was shared hundreds of times. By circulating information about it, my neighbors and I were participating in a ritual that is modern only in terms of the technology it now relies on. In the UK, as in other places, collective action is filling a gap left by a diminishing police presence. A significant reason for this is that budget cuts have forced a decline of nearly 23% in the police workforce, according to Unison, the country’s largest union. London’s Metropolitan Police has been the worst affected, with over 3,000 jobs lost between 2012 and 2016. This includes 3,350 jobs for community support officers—a role created specifically to make the police more visible. These officers had been brought in to work with the community, says Menaal Munshey, a criminologist with the United Nations. “But because of the cuts, that link has been broken. And the community feels like it’s on its own.” That frustration is likely why anonymous tipsters opted to reach out to me, a complete stranger, rather than go to the police. Previous appeals to the police had apparently fallen on deaf ears. Of course, such information sharing isn’t always a good thing. A study published last year by Dutch academics Ronald van Steden and Shanna Mehlbaum confirmed what is already observed: neighborhood groups have “undesirable social and moral by-products” such as discrimination, stigmatization, exclusion of strangers, and excessive social control. “If people are constantly encouraged to be aware of anything and anyone ‘out-of-the-ordinary’, such a process may slowly but surely open the doors for harsh surveillance practices to creep into people’s normal lives. This, in turn, stimulates the erection of a digital pillory, a witch-hunt for (assumed) paedophiles, exclusive forms of ‘stranger danger’ and other potential for voyeuristic mob activism. It is not difficult to recognise that democratic values of openness, tolerance and mutual respect are at stake here.” Training algorithms on crime reports from victims rather than arrest data is said to make predictive tools less biased. It doesn’t look like it does. The problems, when they do arise, aren’t confined to any one neighborhood, city, or country, and they are often in response to cultural fissures that were already present. In India, for example, unfounded rumors that circulated on local WhatsApp groups in 2018 fed old fears of a specific kind of bogeyman that Indians have grown up hearing about: a bacha chor, a person who kidnaps children to harvest their organs. The rumors led to the murder of at least two dozen people in different parts of the country and forced WhatsApp to limit the number of times that users in India could forward a message. The challenge for Nextdoor in the United States, meanwhile, is that it has become a magnet for racial profiling. In 2015, Nextdoor addressed the issue with changes that included asking users who mentioned race in their posts to provide additional details. The reality is that the root causes of these crimes continue to go unaddressed. It isn’t just police resources that need to be reassessed, but social welfare programs such as gang mediation, drug and alcohol treatment, and children’s services, all of which have also fallen victim to governmental service cuts. And while neighborhood groups can have a positive impact on social cohesion, there is no proof that they actually reduce crime. Danielle Pyke, a police community support officer with the Met, says it’s rare for Nextdoor users to provide information that leads to patrols, arrests, or drug busts. When online groups do work, by mobilizing people to share information, they can be a success. Thanks to the information I gathered with the help of my community, my family and I were able to submit a dossier to the police, forcing them to act. The owner of the animals that mauled Zoey was charged with two counts of owning dangerously out-of-control dogs that had caused injuries, and he was ordered to appear in court in April. Unfortunately, however, on the evening before the case went to trial, we received a call from the prosecuting lawyer informing us that she had no choice but to close the case because police had failed to submit the paperwork required. The failure of the Met to do basic admin, despite being given several months, denied my family our day in court and Zoey the justice she deserved.",0
Computers will be transformed by alternative materials and approaches—maybe sooner than you think,https://www.technologyreview.com/2022/06/28/1054169/2022-innovators-computing-internet/,2022-06-28,"<p>This year's 35 Innovators honorees are helping to make cutting-edge tech like 2D semiconductors and optical computing a reality.</p>
","In less than a century, computing has transformed our society and helped spur countless innovations. We now carry in our back pockets computers that we could only have dreamed of a few decades ago. Machine-learning systems can analyze scenes and drive vehicles. And we can craft extraordinarily accurate representations of the real world—models that can be used to design nuclear reactors, simulate myriad greenhouse-gas emission scenarios, and launch a probe on a nine-year trip to study Pluto in an all-too-brief high-speed fly-by. We fundamentally owe these capabilities to our ability to build progressively better computing devices—the transistors and other components at the heart of computer chips. But the transistor is reaching its limits, along with the traditional von Neumann architecture—the system of separate logic and memory that we use to construct computers. If we want to keep improving computer performance and energy efficiency, it’s time for some fresh ideas. There are, of course, plenty of possibilities at hand: quantum computers, optoelectronic components made from two-dimensional materials, and analog circuitry are just a few. Many of these approaches have been discussed for years, if not decades. But some are now reaching promising levels of maturity. In my research and that of 35 Innovators awardee Xu Zhang at Carnegie Mellon University, for example, 2D semiconductors are making their way into optoelectronic devices—the sort used in telecommunication. These devices have started to surpass the performance of conventional switches made with silicon and III-V semiconductors (compounds with elements from columns III and V on the periodic table). This essay is part of MIT Technology Review’s 2022 Innovators Under 35 package recognizing the most promising young people working in technology today. See the full list here or explore the winners in this category below. Optical computing, an early approach that was later abandoned in favor of binary electronic circuitry, is also moving forward. I am fascinated by the possibility of building computers that use light as the “working fluid,” passing photons around much the way our present chips do electrons. This is already happening: silicon photonic chips are providing high energy efficiency and are helping overcome the slowdown issues in traditional GPU architectures. They can reduce the time needed to train deep-learning models, enabling the next generation of advanced AI. There are opportunities to integrate photonics with new low-power chip designs like those from TR35 awardee Hongjie Liu at Reexen Technology. Honorees from this year's 35 Innovators list are attacking the problem from all angles, including more-efficient batteries and better-tasting plant-based foods. In the long term, such photonic circuits could help us approach or perhaps even surpass widely accepted limits in computing. Theoretical work in photonic information processing suggests that light can be converted to heat and vice versa, which opens up some remarkable opportunities for all-optical energy storage—essentially batteries made out of photons—and alternative computing architectures. Many of these projects are still happening primarily in the academic realm, but we are slowly moving toward building larger-scale, more fully integrated systems. If we can continue thinking about how these ideas can be integrated into full computing systems, the coming years should see even more progress away from traditional chips and toward an array of different forms of computing. Starting in July, Prineha Narang is the Howard Reiss Chair Professor in Physical Sciences at University of California, Los Angeles (and was a 35 Innovators honoree in 2018).",0
A pro-China online influence campaign is targeting the rare-earths industry,https://www.technologyreview.com/2022/06/28/1055093/a-pro-china-online-influence-campaign-is-targeting-the-rare-earths-industry/,2022-06-28,"<p>Disinformation operatives seek to undermine firms in the Western world as China fights to maintain near-monopoly power.</p>
","An online influence campaign carried out by a group that promotes China’s political interests is targeting Western companies that mine and process rare-earth elements, according to a new report from cybersecurity firm Mandiant. The campaign, which is playing out in Facebook groups and micro-targeted tweets, is trying to stoke environmentalist protests against the companies in the US. The operation is attributed to an online group code-named Dragonbridge, which has also been responsible for campaigns claiming that covid-19 originated in the United States. Its latest campaign has increased in intensity in recent weeks as part of a strategic battle between China and its Western adversaries over who controls the precious resources and their own destiny. “We are headed to a future where the likelihood of tools like influence operations being used against key industries will only increase,” says John Hultquist, Mandiant’s head of intelligence. “As competition between the US and China changes, the nature of the competition may become more aggressive.” It’s also proof that influence campaigns are not easy: Dragonbridge has largely failed in its bid to draw negative attention to the Western companies. Shane Huntley, who directs Google’s Threat Analysis Group and has tracked Dragonbridge since 2019, previously tweeted that his team has taken an “aggressive” approach against the influence operation but that “it really is amazing for all the effort put in how LITTLE engagement these channels get from real viewers.” Rare-earth elements, such as cerium and neodymium, are the raw ingredients for just about any high-tech products—smartphones, fighter jets, electric vehicles, wind turbines. Three top livestreaming personalities on the platform Taobao commanded legions of fans who bought billions of dollars’ worth of goods—until they suddenly went dark. China has come to dominate the market in recent years, and by 2017 the country produced over 80% of the world’s supply. Beijing achieved this by pouring resources into the study and mining of rare-earth elements for decades, building up six big state-owned firms and relaxing environmental regulations to enable low-cost and high-pollution methods. The country then rapidly increased rare-earth exports in the 1990s, a sudden rush that bankrupted international rivals. Further development of rare-earth industries is a strategic goal under Beijing’s Made in China 2025 strategy. The country has demonstrated its dominance several times, most notably by stopping all shipments of the resources to Japan in 2010 during a maritime dispute. State media have warned that China could do the same to the United States. The US and other Western nations have seen this monopoly as a critical weakness for their side. As a result, they have spent billions in recent years to get better at finding, mining, and processing the minerals. In early June 2022, the Canadian mining company Appia announced it had found new resources in Saskatchewan. Within weeks, the American firm USA Rare Earth announced a new processing facility in Oklahoma. Dragonbridge engaged in similar activity in 2021, soon after the American military signed an agreement with the Australian mining firm Lynas, the largest rare-earths company outside China, to build a processing plant in Texas. Screenshots provided by Mandiant show accounts associated with Dragonbridge springing into action for micro-targeted influence campaigns. Claiming to be from Texas, Dragonbridge actors posted to a pre-existing anti-Lynas Facebook group declaring concerns about the environmental impact of mining and processing. The group tried to incite protests, a tactic it also tried in the early days of the coronavirus pandemic. The Justice Department’s effort to prosecute cases of economic espionage had drifted from its stated mission and drawn fierce criticism for appearing to target researchers because of their ethnicity. The environmental concerns around rare earths are real. In fact, China’s dominance in the industry is due in part to the country’s weak environmental regulations. Faced with local backlash, China is now moving some rare-earth operations to Africa. But though the mining operations can be environmentally harmful, these elements are widely considered absolutely necessary to meeting global carbon emissions goals and stemming climate change, because they are critical to cutting-edge clean technology like advanced batteries and electric vehicles. This reality will inevitably force difficult decisions and trade-offs. “Rare-earth metals are such an important area of competition for both countries,” Hultquist says. “They’re essentially the canary in the coal mine for a future where more industries are regularly targeted by influence operations as the economic and political situation becomes more strained.”",0
The hacking industry faces the end of an era,https://www.technologyreview.com/2022/06/27/1054884/the-hacking-industry-faces-the-end-of-an-era/,2022-06-27,"<p>But even if NSO Group is no more, there are plenty of rivals who will rush in to take its place. And the same old problems haven’t gone away.</p>
","NSO Group, the world’s most notorious hacking company, could soon cease to exist. The Israeli firm, still reeling from US sanctions, has been in talks about a possible acquisition by the American military contractor L3 Harris. The deal is far from certain—there is considerable opposition from both the White House and US intelligence—but if it goes through, it’s likely to involve the dismantling of NSO Group and the end of an era. The company and its technology would likely be folded into a unit within L3 Harris. The American firm already has its own offensive cyber division, known as Trenchant, which has quietly become one of the most sophisticated and successful such shops in the world, in large part thanks to a strategy of smart international acquisitions. But no matter what happens with this potential deal, the changes afoot in the global hacking industry are far bigger than any single company. The hacking industry looks dramatically different today from the way it did a year ago. Two major events have changed the landscape. The US sanctioned NSO Group in late 2021 after determining that government customers had used its Pegasus spyware to “maliciously target” journalists, human rights activists, and government officials around the world. Within days, amid global concern over spyware abuse, the Israeli ministry of defense followed the American sanctions by severely restricting export licenses so that the country’s roaring hacking industry lost the majority of its customers virtually overnight. The number of countries that its hacking firms could sell to fell from over 100 to 37, a group that includes Western European nations, the United States, Canada, the United Kingdom, Australia, Japan, and India. This is a big moment of turbulence and change for the hacking business. But the demand is here to stay. That’s still a huge and rich market, but it cuts out dozens of nations in Latin America, Africa, Eastern Europe, and Asia, where Israeli cyber firms had been making a killing selling cutting-edge surveillance tools to customers with deep pockets and a willingness to spend. It’s also where NSO Group kept getting in trouble for getting caught selling powerful hacking tools to authoritarian regimes that abused Pegasus. NSO Group executives say they have terminated eight Pegasus contracts due to abuse. The defense ministry’s licensing restrictions have sounded the death knell for several smaller shops of hackers and researchers. Nemesis, an Israeli cyber firm that had managed to keep a low public profile, shut down in April. Ace Labs, a spinoff of the billion-dollar tech giant Verint, closed up shop and fired all its researchers earlier this month. The Israelis’ former customers are not standing idle. New players and old rivals are stepping into the vacuum to provide the hacking capability that more and more governments demand. “The landscape is shifting and, to a certain degree, diversifying,” said Christoph Hebeisen, director of security intelligence research at the mobile security firm Lookout. Several European firms are stepping into the gap. Intellexa is an “alliance” of hacking firms, operating out of several locations in Europe and Asia, that have been able to attract and retain business from nations no longer able to buy Israeli hacking tools. The group boasts Israeli and European talent but avoids the new Israeli restrictions that have stung several of its competitors. Mobile spyware from Cytrox, a North Macedonian hacking firm and founding member of the Intellexa alliance, was found on an Egyptian target last year. RCS Labs is an Italian hacking firm whose spyware was recently spotted in Kazakhstan. Until as late as 2021, Kazakhstan was reportedly a customer of NSO Group, but it is now restricted. Now the mobile security firm Lookout says it sees the country using RCS’s malware to spy on Android phones. Kazakhstan is an authoritarian nation that recently jailed an opposition leader just a few months after the mass killing of protesters. NSO Group hacking tools were reportedly used to spy on activists there last year. When reached for comment, RCS Labs provided an unattributed statement condemning “any abuse or improper use” of its products that are “designed and produced with the intent of supporting the legal system in preventing and combating crime.” Besides increased global uncertainty and the restrictions on Israeli hacking companies, several industry executives say they see two more shifts in play. Many more countries are investing in building their own domestic cyber capability. Most countries haven’t had the resources, expertise, or money to date—and firms like NSO Group have made it economically easier to just buy the tools instead. But now countries desire their own domestic hacking capabilities to insulate themselves from global variables like political strife and human rights criticism. The most valuable hacking tools were once the domain of governments. Not anymore. The archetype is the United Arab Emirates, which spent 10 years hiring former Western intelligence officers to build up DarkMatter, a firm that was famously caught spying on journalists and dissidents. DarkMatter has been replaced in the United Arab Emirates by firms like Edge Group. Now, according to sources from within the Israeli and European hacking industries, governments of states like Saudi Arabia, Bahrain, Qatar, and Singapore are following in the UAE’s footsteps by offering top financial incentives to attract hacking talent from around the world. Several industry sources who wished to remain anonymous say they see Chinese actors stepping into the void to try to sell surveillance and cyber tools, especially to African and Asian nations, where Beijing has been aggressively expanding its influence in recent years. Israeli officials are suggesting to the country’s cyber companies that they should prepare for this situation to potentially last until at least two years from now—incidentally, when the next American presidential election will take place. What happens after that is unclear in more ways than one. American sanctions and Israeli restrictions may conceivably contribute to the end of NSO Group. But what happens next? The market is bigger and more visible than ever before, encompassing hundreds of companies selling surveillance tech globally. One of the industry’s top trade shows, ISS World, recently held a show in Prague, and it was bigger than ever on both the company and government delegation sides. Calls from every conceivable corner to regulate the industry internationally have largely failed. As a result, there is still little global transparency or accountability for abuse despite increased attention on the problem. One thing we are learning is that a vacuum can’t last long in a market where demand is so high.",0
Energy-hungry data centers are quietly moving into cities,https://www.technologyreview.com/2022/06/22/1053889/city-server-farms-energy/,2022-06-22,"<p>Companies are pushing more server farms into the hearts of population centers.</p>
","In 1930, the telegraph giant Western Union put the finishing touches on its new crown jewel: a 24-story art deco building located at 60 Hudson Street in lower Manhattan. Soon after, over a million telegraphs each day shuttled in and out, carried by a network of cables, pneumatic tubes, and 30 employees in roller skates who sped across the building’s linoleum floors. Today, much of it is home to vast halls of computer servers. It is a physical manifestation of the cloud: when you stream a TV show, upload a file to Dropbox, or visit a website, chances are you will be relying on the processing power of a data center just like it. Hundreds of companies rent out space in 60 Hudson Street, and it is one of a growing number of buildings, sometimes called “colocation centers” in industry parlance, that host data centers in or near major population centers. When you think of data centers, you probably picture a giant server farm in a rural area where electricity is cheap and tax breaks are plentiful. Big tech companies like Google, Amazon Web Services, Microsoft, and Meta have placed millions of square feet worth of server space in places like Northern Virginia or Hillsboro, Oregon. But now, to reduce lag times, companies are increasingly weaving nodes in their network into the fabric of cities. The One Wilshire building in Los Angeles, for example, formerly home to a network of law offices, now oversees one-third of all internet traffic between the US and Asia. To the uninitiated, these urban physical internet nodes probably don’t look like much at all. And that’s by design. Equinix, the largest owner of colocation data centers with 10.9% of the world market, operates data centers that generally aren’t supposed to draw attention to themselves. In Dallas, the company owns a sprawling industrial building just outside the city center that doubles as a data center hub and the headquarters of a for-profit college. In Tokyo, the operation is largely conducted on various floors within the city’s sea of skyscrapers, “so you wouldn’t even know it’s there,” says Jim Poole, the company’s vice president of business development. In Sydney, Australia, Equinix is building a new data center in an expressionist style not unlike that of the city’s famed opera house. And around one of its facilities in Amsterdam, Equinix built a moat—less for security, Poole says, than to make the building match its surroundings, given that Amsterdam is a city of canals. “For the most part, people actually do try to make their buildings fit the environment,” he says, adding that sometimes local regulators even require it. The demand for such facilities, especially in urban centers, is growing quickly: last year, spending on colocation data centers jumped 11.7%. The biggest cloud companies are not far behind. Amazon Web Services has been pushing shrunk-down data centers, which it calls Local Zones, close to major population areas; so far, it has placed them in 32 cities across the US. The trend has even piqued the interest of Walmart, which may soon start renting out sections of its superstores to host data centers for third-party companies. One explanation for the flurry of demand, Poole says, is that consumers themselves have changed. As more of our lives have gone online, “people’s tolerance for latency has continued to go down,” he says. The main drivers are those applications where a delay in the milliseconds can prove critical: you might not notice a quarter-second lag on Netflix, but you certainly will if you are using an online sports betting app, trading stocks, or participating in a multiplayer game like Fortnite. Companies like Google, Amazon, and Microsoft, for instance, are betting on cloud gaming, which involves streaming games over the internet without a console or a phone to provide processing power. But many popular games, such as first-person shooters, “require a lot of quick reaction times and therefore really fast connectivity,” says Jabez Tan, the head of research at the firm Structure Research. And games like that will not function on a streaming service without the help of large numbers of data centers. Smart cities haven’t brought the tangible improvements that many hoped they would. What comes next? Or take the metaverse—the favorite, if sparingly sketched-out, new talking point of Nvidia, Meta (previously Facebook), and other tech giants. If a virtual-reality world is ever going to achieve mass appeal, it’s going to need to mirror the immediacy of our own. That means intricately detailed graphics, nimble motion, and audio reactions with hardly a millisecond of buffering. All told, writes Raja Koduri, a senior VP at Intel, we need “several orders of magnitude more powerful computing capability” to make it possible. It’s this demand for computing power, Tan says, that has spurred the “decentralization” of data center networks: tech companies are looking around at their existing infrastructure and saying, “Hey, we’re not able to give to people in Jakarta, or people in Manila, the same performance levels that people in Singapore [are] enjoying.” “It’s almost like an accordion,” says Pat Lynch, who studies data centers for the commercial real estate research firm CBRE. Data centers are still being built in places like rural Oregon. But now they are “expanding out.” The way these new data centers blend into the urban and suburban landscape of office buildings or custom warehouses or industrial parks is a double-edged sword. The approach might make sense from a security standpoint. It also spares people from looking at the eyesore of vast halls crammed with computer servers. The downside of this invisibility, though, is that we aren’t often forced to think about what all our internet use is costing us. Data centers account for 1.8% of all electricity use in the US and 0.5% of the country’s greenhouse-gas emissions, according to a report last year—far from a negligible amount. Some strategies could help, such as reusing the heat that they produce in copious quantities. But getting to that point would require stepping back from the rush to build and truly intertwining data centers—with all the heat they generate, the energy they consume—into our existing urban ecosystems. Michael Waters is a writer based in New York.",0
"The China AI and Autonomy Report: Issue 18, June 30, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-18,2022-06-30 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. In this issue, a PLA Daily article discusses using unmanned systems and AI to aid surprise attacks in deep-sea warfare. The PRC’s first 100+ ton unmanned surface vessel has conducted its first autonomous sea trial. PRC researchers have reportedly developed an AI model for use in space warfare, while engineers at the Aviation Industry Corporation of China have developed an unmanned cargo plane that adheres to Chinese Civil Aviation Regulations. The South China Morning Post reports that the PRC has developed an exascale supercomputing capability nearly on par with that of the US. The PRC has fully approved the “East Data, West Computing” plan, which has established 10 computing centers, although obstacles to its success remain. Future Warfare Deep-Sea Warfare",0
