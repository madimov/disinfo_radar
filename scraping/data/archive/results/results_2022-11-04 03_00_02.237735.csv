title,url,date,summary,cleaning,category
"
        How Can We Make Sure Autonomous Weapons Are Used Responsibly?
    ",https://spectrum.ieee.org/autonomous-weapons-trust,2022-11-03 15:00:16.944040,,"Technical challenges of AI are exacerbated in autonomous weapons systems This article is part of our Autonomous Weapons Challenges series . The IEEE Standards Association is looking for your feedback on this topic , and has invites you to answer these questions . International discussions about autonomous weapons systems ( AWS ) often focus on a fundamental question : Is it legal for a machine to make the decision to take a human life ? But woven into this question is another fundamental issue : Can an automated weapons system be trusted to do what it ’ s expected to do ? If the technical challenges of developing and using AWS can ’ t be addressed , then the answer to both questions is likely “ no. ” Many of the known issues with AI and machine learning become even more problematic when associated with weapons . For example , AI systems could help process data from images far faster than human analysts can , and the majority of the results would be accurate . But the algorithms used for this functionality are known to introduce or exacerbate issues of bias and discrimination , targeting certain demographics more than others . Given that , is it reasonable to use image-recognition software to help humans identify potential targets ? But concerns about the technical abilities of AWS extend beyond object recognition and algorithmic bias . Autonomy in weapons systems requires a slew of technologies , including sensors , communications , and onboard computing power , each of which poses its own challenges for developers . These components are often designed and programmed by different organizations , and it can be hard to predict how the components will function together within the system , as well as how they ’ ll react to a variety of real-world situations and adversaries . It ’ s also not at all clear how militaries can test these systems to ensure the AWS will do what ’ s expected and comply with International Humanitarian Law . And yet militaries typically want weapons to be tested and proven to act consistently , legally , and without harming their own soldiers before the systems are deployed . If commanders don ’ t trust a weapons system , they likely won ’ t use it . But standardized testing is especially complicated for an AI program that can learn from its interactions in the field—in fact , such standardized testing for AWS simply doesn ’ t exist . We know how software updates can alter how a system behaves and may introduce bugs that cause a system to behave erratically . But an automated weapons system powered by AI may also update its behavior based on real-world experience , and changes to the AWS behavior could be much harder for users to track . New information that the system accesses in the field could even trigger it to start to shift away from its original goals . Similarly , cyberattacks and adversarial attacks pose a known threat , which developers try to guard against . But if an attack is successful , what would testing look like to identify that the system has been hacked , and how would a user know to implement such tests ? Though recent advancements in artificial intelligence have led to greater concern about the use of AWS , the technical challenges of autonomy in weapons systems extends beyond AI . Physical challenges already exist for conventional weapons and for nonweaponized autonomous systems , but these same problems are further exacerbated and complicated in AWS . For example , many autonomous systems are getting smaller , even as their computational needs grow , including navigation , data acquisition and analysis , and decision making—and potentially all while out of communication with commanders . Can the automated weapons system maintain the necessary and legal functionality throughout the mission , even if communication is lost ? How is data protected if the system falls into enemy hands ? Issues similar to these may also arise with other autonomous systems , but the consequences of failure are magnified with AWS , and extra features will likely be necessary to ensure that , for example , a weaponized autonomous vehicle in the battlefield doesn ’ t violate International Humanitarian Law or mistake a friendly vehicle for an enemy target . Because these problems are so new , weapons developers and lawmakers will need to work with and learn from experts in the robotics space to be able to solve the technical challenges and create useful policy . There are many technical advances that will contribute to various types of weapons systems . Some will prove far more difficult to develop than expected , while others will likely be developed faster . That means AWS development won ’ t be a leap from conventional weapons systems to full autonomy , but will instead make incremental steps as new autonomous capabilities are developed . This could lead to a slippery slope where it ’ s unclear if a line has been crossed from acceptable use of technology to unacceptable . Perhaps the solution is to look at specific robotic and autonomous technologies as they ’ re developed and ask ourselves whether society would want a weapons system with this capability , or if action should be taken to prevent that from happening . We want your feedback ! To help bring clarity to these AWS discussions , the IEEE Standards Association convened an expert group in 2020 , to consider the ethical and technical challenges of translating AWS principles into practice and what that might mean for future development and governance . Last year , the expert group published its findings in a report entitled “ Ethical and Technical Challenges in the Development , Use , and Governance of Autonomous Weapons Systems. ” Many of the AWS challenges are similar to those arising in other fields that are developing autonomous systems . We expect and hope that IEEE members and readers of IEEE Spectrum will have insights from their own fields that can inform the discussion around AWS technologies . Ariel Conn is Head of the IEEE SA Research Group on Issues of AI and Autonomy in Defense Systems . The prosthetics industry is too focused on high-tech limbs that are complicated , costly , and often impractical The author , Britt Young , holding her Ottobock bebionic bionic arm . In Jules Verne ’ s 1865 novel From the Earth to the Moon , members of the fictitious Baltimore Gun Club , all disabled Civil War veterans , restlessly search for a new enemy to conquer . They had spent the war innovating new , deadlier weaponry . By the war ’ s end , with “ not quite one arm between four persons , and exactly two legs between six , ” these self-taught amputee-weaponsmiths decide to repurpose their skills toward a new projectile : a rocket ship . The story of the Baltimore Gun Club propelling themselves to the moon is about the extraordinary masculine power of the veteran , who doesn ’ t simply “ overcome ” his disability ; he derives power and ambition from it . Their “ crutches , wooden legs , artificial arms , steel hooks , caoutchouc [ rubber ] jaws , silver craniums [ and ] platinum noses ” don ’ t play leading roles in their personalities—they are merely tools on their bodies . These piecemeal men are unlikely crusaders of invention with an even more unlikely mission . And yet who better to design the next great leap in technology than men remade by technology themselves ?",0.0
"
        How Can We Talk About Autonomous Weapons?
    ",https://spectrum.ieee.org/autonomous-weapons-challenges,2022-11-03 15:00:16.944185,,"Experts convened by the IEEE Standards Association seek your help This article is part of our Autonomous Weapons Challenges series . The IEEE Standards Association is looking for your feedback on this topic , and has invites you to answer these questions . Lethal autonomous weapons systems can sound terrifying , but autonomy in weapons systems is far more nuanced and complicated than a simple debate between “ good or bad ” and “ ethical or unethical. ” In order to address the legal and ethical issues that an autonomous weapons system ( AWS ) can raise , it ’ s important to look at the many technical challenges that arise along the full spectrum of autonomy . A group of experts convened by the IEEE Standards Association is working on this , but they need your help . Weapons systems can be built with a range of autonomous capabilities . They might be self-driving tanks , surveillance drones with AI-enabled image recognition , unmanned underwater vehicles that operate in swarms , loitering munitions with advanced target recognition—the list goes on . Some autonomous capabilities are less controversial , while others trigger intense debate over the legality and ethics of the capability . Some capabilities have existed for decades , while others are still hypothetical and may never be developed . All of this can make autonomous weapons systems difficult to talk about , and doing so has proven to be incredibly challenging over the years . Answering even the most seemingly straightforward questions , such as whether an AWS is lethal or not , can get surprisingly complicated . To date , international discussions have largely focused on the legal , ethical , and moral issues that arise with the prospect of lethal AWS , with limited consideration of the technical challenges . At the United Nations , these discussions have taken place within the Convention on Certain Conventional Weapons . After nearly a decade , though , the U.N. has yet to come up with a new treaty or regulations to cover AWS . In early discussions at the CCW and other international forums , participants often talked past each other : One person might consider a “ fully autonomous weapons system ” to include capabilities that are only slightly more advanced than today ’ s drones , while another might use the term as a synonym for the Terminator . Discussions advanced to the point that in 2019 , member states at the CCW agreed on a set of 11 guiding principles regarding lethal AWS . But these principles are nonbinding , and it ’ s unclear how the technical community can implement them . At the most recent meeting of the CCW in July , delegates repeatedly pushed for more nuanced discussions and understanding of the various technical issues that arise throughout the life cycle of an AWS . To help bring clarity to these and other discussions , the IEEE Standards Association convened an expert group in 2020 , to consider the ethical and technical challenges of translating AWS principles into practice and what that might mean for future development and governance . Last year , the expert group , which I lead , published its findings in a report entitled “ Ethical and Technical Challenges in the Development , Use , and Governance of Autonomous Weapons Systems. ” In the document , we identified over 60 challenges of autonomous weapons systems , organized into 10 categories : It ’ s not surprising that “ establishing common language ” is the first category . As mentioned , when the debates around AWS first began , the focus was on lethal autonomous weapons systems , and that ’ s often still where people focus . Yet determining whether or not an AWS is lethal turns out to be harder than one might expect . Consider a drone that does autonomous surveillance and carries a remote-controlled weapon . It uses artificial intelligence to navigate to and identify targets , while a human makes the final decision about whether or not to launch an attack . Just the fact that the weapon and autonomous capabilities are within the same system suggests this could be considered a lethal AWS . Additionally , a human may not be capable of monitoring all of the data the drone is collecting in real time in order to identify and verify the target , or the human may over-trust the system ( a common problem when humans work with machines ) . Even if the human makes the decision to launch an attack against the target that the AWS has identified , it ’ s not clear how much “ meaningful control ” the human truly has . ( “ Meaningful human control ” is another phrase that has been hotly debated . ) This problem of definitions isn ’ t just an issue that comes up when policymakers at the U.N. discuss AWS . AI developers also have different definitions for commonly used concepts , including “ bias , ” “ transparency , ” “ trust , ” “ autonomy , ” and “ artificial intelligence. ” In many instances , the ultimate question may not be , Can we establish technical definitions for these terms ? but rather , How do we address the fact that there may never be consistent definitions and agreement on these terms ? Because , of course , one of the most important questions for all of the AWS challenges is not whether we technically can address this , but even if there is a technical solution , should we build and deploy the system ? Identifying the challenges was just the first stage of the work for the IEEE-SA expert group . We also concluded that there are three critical perspectives from which a new group of experts will be considering these challenges in more depth : This is where we want your feedback ! Many of the AWS challenges are similar to those arising in other fields that are developing autonomous systems . We expect and hope that IEEE members and readers of IEEE Spectrum will have insights from their own fields that can inform the discussion around AWS technologies . We ’ ve put together a series of questions in the Challenges document that we hope you ’ ll answer , to help us better understand how people in other fields are addressing these issues . Autonomous capabilities will increasingly be applied to weapons systems , much as they are being applied in other realms , and we hope that by looking at the challenges in more detail , we can help establish effective technical solutions , while contributing to discussions about what can and should be legally acceptable . Your feedback will help us move toward this ultimate goal . Public comments will be open through 7 December 2022 . The independent group of experts who authored the report for the IEEE Standards Associate includes Emmanuel Bloch , Ariel Conn , Denise Garcia , Amandeep Gill , Ashley Llorens , Mart Noorma , and Heather Roff . Ariel Conn is Head of the IEEE SA Research Group on Issues of AI and Autonomy in Defense Systems . The prosthetics industry is too focused on high-tech limbs that are complicated , costly , and often impractical The author , Britt Young , holding her Ottobock bebionic bionic arm . In Jules Verne ’ s 1865 novel From the Earth to the Moon , members of the fictitious Baltimore Gun Club , all disabled Civil War veterans , restlessly search for a new enemy to conquer . They had spent the war innovating new , deadlier weaponry . By the war ’ s end , with “ not quite one arm between four persons , and exactly two legs between six , ” these self-taught amputee-weaponsmiths decide to repurpose their skills toward a new projectile : a rocket ship . The story of the Baltimore Gun Club propelling themselves to the moon is about the extraordinary masculine power of the veteran , who doesn ’ t simply “ overcome ” his disability ; he derives power and ambition from it . Their “ crutches , wooden legs , artificial arms , steel hooks , caoutchouc [ rubber ] jaws , silver craniums [ and ] platinum noses ” don ’ t play leading roles in their personalities—they are merely tools on their bodies . These piecemeal men are unlikely crusaders of invention with an even more unlikely mission . And yet who better to design the next great leap in technology than men remade by technology themselves ?",0.0
