title,url,date,summary,cleaning,category
Google Leverages Transformers to Vastly Simplify Neural Video Compression With SOTA Results,https://syncedreview.com/2022/06/17/google-leverages-transformers-to-vastly-simplify-neural-video-compression-with-sota-results/,2022-06-17,"
In the new paper VCT: A Video Compression Transformer, a Google Research team presents an elegantly simple but powerful video compression transformer (VCT) that does not require architectural biases and priors and learns totally from data without any hand-crafting. VCT is easy to implement and outperforms conventional video compression approaches.

 ","Neural network-based approaches have made significant progress on video compression over the last several years, reaching performance on par with classical codec-based methods. These novel neural approaches however are challenging to implement, as they tend to require complex hand-crafted connections between their many sub-components and struggle when the input data does not match their architectural biases and priors. In the new paper VCT: A Video Compression Transformer, a Google Research team presents an “elegantly simple” but powerful video compression transformer (VCT) that eliminates the architectural biases and priors of previous approaches (such as motion prediction and warping operations), and instead learns totally from data without any hand-crafting. VCT is easy to implement and outperforms existing video compression methods on standard datasets. The proposed VCT is based on the original language translation transformer (Vaswani et al., 2017) and is tasked with translating the previous two frames of a video input into the current frame. It first uses lossy transform coding to project frames from the image space to quantized representations. A transformer then leverages temporal redundancies to model the representation distributions. These predicted distributions are then used to compress the quantized representations via entropy coding. In their empirical studies, the team trained VCT on one million Internet video clips and compared it to video compression approaches such as the classical HEVC (High-Efficiency Video Coding) and neural methods such as SSF (Scale-Space Flow, Agustsson et al., 2020) and ELF-VC (Efficient Learned Flexible-Rate Video Coding, Rippel et al., 2021). The evaluations were conducted on the MCL-JCV and UVG benchmark datasets, with PSNR (peak signal-to-noise ratio) and MS-SSIM (multi-scale structural similarity index for motion detection) as metrics. Despite its simplicity — and not using flow prediction, warping or residual compensation — VCT surpassed all methods in both PSNR and MS-SSIM in the evaluations. Moreover, experiments on synthetic data showed that VCT can also learn to handle complex motion patterns such as panning, blurring and fading, purely from data. The team says VCT can reduce bandwidth requirements for video conferencing and streaming and enable better utilization of storage space, and hope it can serve as a foundation for a new generation of video codecs. The VCT code has been released on the project’s GitHub. The paper VCT: A Video Compression Transformer is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Wav2Vec 2.0 Learns Brain-Like Representations From Just 600 Hours of Unlabeled Speech Data in New Study,https://syncedreview.com/2022/06/16/wav2vec-2-0-learns-brain-like-representations-from-just-600-hours-of-unlabeled-speech-data-in-new-study/,2022-06-16,"
In the new paper Toward a Realistic Model of Speech Processing in the Brain with Self-supervised Learning, researchers show that self-supervised architectures such as Wav2Vec 2.0 can learn brain-like representations from as little as 600 hours of unlabelled speech; and can also learn sound-generic and speech- and language-specific representations similar to those of the prefrontal and temporal cortices. 
","Deep neural networks have recently hinted at their potential for processing speech in a manner more like the human brain and generating activations similar to those of the brain in response to the same inputs. The development of such algorithms however remains difficult as they require massive training data, supervised labels, textual data rather than more realistic raw sensory data, and prohibitively large memory. In the new paper Toward a Realistic Model of Speech Processing in the Brain with Self-supervised Learning, a research team from Meta AI, PSL University, Université Paris Cité, Université Paris-Saclay, University of Toronto and INSERM shows that self-supervised architectures such as Wav2Vec 2.0 (Baevski et al., 2020) that stack convolutional and transformer layers to predict a quantization of the latent representations of speech waveforms can learn brain-like representations from as little as 600 hours of unlabelled speech; and can also learn sound-generic and speech- and language-specific representations similar to those of the prefrontal and temporal cortices. The team summarizes their study’s main contributions as: The Wav2Vec 2.0 architecture comprises three modules: 1) a feature encoder that transforms raw mono speech waveform inputs into latent representations, 2) a quantization module that discretizes the latent representations into a dictionary of discrete and latent representations of sounds, and 3) a “context network” that uses the previously generated outputs to produce contextualized embeddings. The team trained several variants of Wav2Vec 2.0 on different datasets with both self-supervised and supervised learning objectives and extracted the activations of each layer from both the feature encoder and the context network. In their empirical studies, the team compared the Wav2Vec 2.0 learned representations to those in the brains of 412 human volunteers (351 English speakers, 28 French speakers and 33 Mandarin speakers) recorded with functional magnetic resonance imaging (fMRI) while they passively listened to approximately one hour of audio books in their native language. The experimental results show that Wav2Vec 2.0 model activations can predict brain activity in nearly all cortical areas, self-supervised learning leads to slightly better performance than supervised learning, the hierarchy of Wav2Vec 2.0 maps onto the hierarchy of the cortex, and 600 hours of self-supervised learning suffices for Wav2Vec 2.0 to learn brain-like language-specific representations. Overall, this work demonstrates that applying self-supervised learning to a limited amount of speech data can enable the learning of representations similar to the human brain’s speech perception, taking a step toward a realistic model of speech processing in the brain with self-supervised learning.The paper Toward a Realistic Model of Speech Processing in the Brain with Self-supervised Learning is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Apple’s MobileOne Backbone Reduces Inference Time to Under One Millisecond on an iPhone12 and Reaches 75.9% Top-1 Accuracy on ImageNet,https://syncedreview.com/2022/06/15/apples-mobileone-backbone-reduces-inference-time-to-under-one-millisecond-on-an-iphone12-and-reaches-75-9-top-1-accuracy-on-imagenet/,2022-06-15,"
 In the new paper An Improved One millisecond Mobile Backbone, an Apple research team presents MobileOne, a novel mobile backbone that cuts inference time to under one millisecond on an iPhone12 and reaches 75.9 percent top-1 accuracy on ImageNet.
","As AI systems increasingly move from the cloud to devices, identifying suitable neural network backbones for mobile device deployment has become a hot research area. While decreasing floating-point operations (FLOPs) and parameter counts have produced efficient mobile architectures with high accuracy, factors such as memory access and degree of parallelism continue to have a negative effect with regard to latency cost during inference. In the new paper An Improved One Millisecond Mobile Backbone, an Apple research team presents MobileOne, a novel and efficient neural network backbone for mobile devices that cuts inference time to under one millisecond on an iPhone12 and reaches 75.9 percent top-1 accuracy on ImageNet. The team summarizes their main contributions as: The paper first introduces MobileOne’s architectural blocks, which are designed for convolutional layers factorized into depthwise and pointwise layers. The basic block is built on Google’s small MobileNet-V1 block of 3×3 depthwise convolution followed by 1×1 pointwise convolutions. Over-parameterization branches are also used to improve model performance. MobileOne uses a depth scaling approach similar to MobileNet-V2 — with shallower early stages where input resolution is larger and the layers are slower. Because this setup does not require a multi-branched architecture at inference time, no data movement costs are incurred. This allows the researchers to aggressively scale model parameters compared to multi-branched architectures without introducing significant latency costs. The team evaluated MobileOne on the ImageNet benchmark using mobile devices. In the tests, the MobileOne-S1 variant achieved a lightning-quick inference time of under one millisecond on an iPhone12 while scoring 75.9 percent top-1 accuracy. The researchers also demonstrated MobileOne’s versatility on other computer vision tasks, successfully applying it as a backbone feature extractor for a single shot object detector and in a Deeplab V3 segmentation network. Overall, the study validates the proposed MobileOne as an efficient, general-purpose backbone that achieves state-of-the-art results compared to existing efficient architectures while being many times faster on mobile devices. The paper An Improved One millisecond Mobile Backbone is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",0.0
444 Authors From 132 Institutions Release BIG-bench: A 204-Task ‘Extremely Difficult and Diverse’ Benchmark for Large Language Models,https://syncedreview.com/2022/06/14/444-authors-from-132-institutions-release-big-bench-a-204-task-extremely-difficult-and-diverse-benchmark-for-large-language-models/,2022-06-14,"
In the new paper Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, 444 authors from 132 institutions introduce Beyond the Imitation Game (BIG-bench), a large-scale, extremely difficult and diverse benchmark that includes 204 tasks for predicting the potentially transformative effects of large language models.
","Powered by their ever-increasing scale, today’s large language models have shown breakthrough capabilities beyond natural language processing (NLP), in areas such as writing computer code, diagnosing medical conditions and playing competitive games. As the development and deployment of large-scale language models continues, it is important that the AI community understands their current and near-future capabilities and limitations. In the new paper Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, 444 authors from 132 institutions introduce Beyond the Imitation Game (BIG-bench), a large-scale, extremely difficult and diverse benchmark that includes 204 tasks for predicting the potentially transformative effects of large language models. BIG-bench was named in homage to Alan Turing’s imitation game (Turing, 1950); and designed for analyzing dense and sparse transformer models such as those from Google and OpenAI, whose scales range from millions to hundreds of billions of parameters. The team summarizes their BIG-Bench suite as follows: BIG-bench supports two types of tasks: JSON (JavaScript Object Notation) and programmatic. The JSON file contains a list of input-target pairs, and performance is evaluated by comparing the outputs and the targets. The programmatic tasks are written in Python and are evaluated by measuring the generated text continuations for given inputs and computing conditional log probabilities of target given inputs. The BIG-bench task scope ranges from writing codes, playing competitive games and common-sense reasoning to social bias, linguistics, software development and beyond. It can also measure progress well beyond the current state of the art. The researchers’ experiments with BIG-bench revealed a number of behavioural characteristics of large language models, such as: 1) Aggregate performance improves with model size but can’t compete with human performance; 2) Model predictions grow better calibrated with increased scale; 3) Model classes behave similarly, with benefits from sparsity; 4) Breakthrough behaviour is sensitive to details of task specification; and 5) Even programmatic measures of model capability can be highly subjective. The team also tackled the thorny topic of social biases in large language models. They observed that biases often increase with scale in settings with broad or ambiguous context and can decrease with scale in settings with narrow unambiguous context; and that biases can potentially be steered through appropriately chosen prompting. The team considers BIG-bench a “living benchmark” and will continue to accept new task submissions for peer review on a rolling basis. They hope BIG-bench can help identify additional breakthrough capabilities and enable researchers to better understand the power and potential of current and future large language models. The BIG-bench project was collaboratively developed on the GitHub repository, where the code is now open-sourced. The paper Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
"Cambridge, Google & Secondmind’s Neural Diffusion Processes Challenge Gaussian Processes for Describing Rich Distributions Over Functions",https://syncedreview.com/2022/06/13/cambridge-google-secondminds-neural-diffusion-processes-challenge-gaussian-processes-for-describing-rich-distributions-over-functions/,2022-06-13,"
In the new paper Neural Diffusion Processes, a research team from the University of Cambridge, Secondmind, and Google Research presents Neural Diffusion Processes (NDPs), a novel framework that learns to sample from rich distributions over functions at a lower computational cost than the true Bayesian posterior of a conventional Gaussian process.
","While researchers have traditionally employed Gaussian processes (GP) for specifying prior and posterior distributions over functions, this approach becomes computationally expensive when scaled, is limited by the expressivity of its covariance function, and struggles with adapting a point estimation for the hyperparameters. A research team from the University of Cambridge, Secondmind, and Google Research addresses these issues in the new paper Neural Diffusion Processes, proposing Neural Diffusion Processes (NDPs). The novel framework learns to sample from rich distributions over functions at a lower computational cost and capture distributions that are close to the true Bayesian posterior of a conventional Gaussian process. The paper’s lead author, Vincent Dutordoir, explains, “Bayesian inference for regression is great, but it is often very costly and requires making a priori modelling assumptions. What if we can train a big neural net to sample plausible posterior samples over functions? This is the premise of our Neural Diffusion Processes.” The team summarizes their main contributions as: The proposed NDP is a denoising diffusion model-based approach for learning probabilities from a function and producing prior and conditional samples of functions. It allows full marginalization over the GP hyperparameters while reducing the computational burden compared to GPs. The team first examined existing state-of-the-art neural network-based generative models in terms of sample quality. Based on their findings, they designed NDP to generalize diffusion models to infinite-dimensional function spaces by enabling the indexing of random variables onto which the model diffuses. The researchers also adopted a novel bi-dimensional attention block to guarantee equivariance over the input dimensionality and sequence and enable the model to draw samples from a stochastic process. As such, NDP can leverage the benefits of stochastic processes, such as exchangeability. In their empirical study, the team evaluated the proposed NDP’s ability to produce high-quality conditional samples and marginalize over kernel hyperparameters; and on its input dimensionality invariance. The results show that NDP is able to capture functional distributions that are close to the true Bayesian posterior while reducing computational burdens. The researchers note that while NDP sample quality improves with the number of diffusion steps, this also results in slower inference times. They suggest inference acceleration or sample parameterizing techniques could be explored in future studies to address this issue. The paper Neural Diffusion Processes is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Yoshua Bengio Team’s Large-Scale Analysis Reveals the Benefits of Modularity and Sparsity for DNNs,https://syncedreview.com/2022/06/10/yoshua-bengio-teams-large-scale-analysis-reveals-the-benefits-of-modularity-and-sparsity-for-dnns/,2022-06-10,"
In the new paper Is a Modular Architecture Enough?, a research team from Mila and the Université de Montréal conducts a rigorous and thorough quantitative assessment of common modular architectures that reveals the benefits of modularity and sparsity for deep neural networks and the sub-optimality of existing end-to-end learned modular systems.
","Deep neural networks (DNNs) have drawn much inspiration from the human cognitive process, evidenced recently in their incorporation of modular structures and attention mechanisms. By representing knowledge in a modular manner and selecting relevant information via attention mechanisms, DNN models can develop meaningful inductive biases, boost their out-of-distribution generalization abilities, and manipulate concepts at higher levels of cognition. While modular architectures provide proven advantages for DNNs, there currently exists no rigorous quantitative assessment method for them due to the complexity and unknown nature of real-world data distributions. As such, it is unclear whether or to what extent the performance gains obtained by modular systems are actually attributable to good modular architecture design. In the new paper Is a Modular Architecture Enough, a research team from Mila and the Université de Montréal conducts a rigorous and thorough quantitative assessment of common modular architectures that reveals the benefits of modularity and sparsity for DNNs and the sub-optimality of existing end-to-end learned modular systems. The team summarizes their main contributions as: “ The team considers four model types with different levels of specialization: Monolithic, a large neural network that takes the entire data as input; Modular, a number of modules, each of which is a neural network that takes the data as input; Modular-op, similar to the modular system but with activation decided only by the rule context; and GT-Modular, which serves as an oracle benchmark, i.e., a modular system that specializes perfectly. They conduct a step-by-step analysis of the benefits of each system and contrast simple end-to-end trained modular systems with monolithic systems. The team explores both in-distribution and out-of-distribution performance and evaluates how different models perform on a variety of tasks. They also introduce two metrics — Collapse-Avg and Collapse-Worst — to measure the amount of collapse suffered by a modular system; and use alignment, adaptation and inverse mutual information metrics to quantify the amount of specialization obtained. In the experiments, the GT-Modular system generally had the highest performance, confirming the advantages of perfect specialization. Although standard end-to-end trained modular systems slightly outperformed monolithic systems, the team notes that these systems’ reliance on backpropagation of the task losses does not enable them to discover perfect specialization. Both the Modular and Modular-op systems were shown to have collapse issues, but Modular-op generally suffered fewer. The team suggests a deeper investigation into forms of regularization may help alleviate these collapse problems. Overall, this work shows that modular models outperform monolithic models. Although modular networks can obtain perfectly specialized solutions, end-to-end training does not recover them, and additional inductive biases are required to learn adequately specialized solutions. The team hopes their work will motivate future research into the design and development of modular architectures.Open-sourced implementation is available on the project’s GitHub. The paper Is a Modular Architecture Enough? is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Microsoft’s XTC Extreme Lightweight Compression Method for Pretrained Transformers Achieves SOTA Results and 50x Smaller Model Sizes,https://syncedreview.com/2022/06/09/microsofts-xtc-extreme-lightweight-compression-method-for-pretrained-transformers-achieves-sota-results-and-50x-smaller-model-sizes/,2022-06-09,"
In the new paper Extreme Compression for Pre-trained Transformers Made Simple and Efficient, a Microsoft research team introduces XTC, a simple yet effective extreme compression pipeline for pretrained transformers that can achieve  state-of-the-art results while reducing model size by 50x.
","Pretrained transformer models have grown dramatically in recent years and now reach hundreds of billions of parameters. Although these behemoths are achieving unprecedented performance on natural language processing (NLP) tasks, their ever-expanding size has limited their real-world deployment on resource-constrained edge or embedded devices. In the new paper Extreme Compression for Pre-trained Transformers Made Simple and Efficient, a Microsoft research team proposes XTC, a simple yet effective extreme compression pipeline for pre-trained transformers. XTC can skip the compute-heavy pretraining knowledge distillation (KD) process to obtain a 5-layer BERT model with better performance than previous state-of-the-art distillation methods, and its extreme quantization and layer reduction can cut model sizes by 50x. The team summarizes their main contributions as: The proposed XTC pipeline comprises two steps: 1) Lightweight layer reduction. Instead of adopting computationally expensive pretraining distillation, the researchers first employ a subset of the fine-tuned teacher weights as a lightweight layer reduction method to initialize a layer-reduced model. When combined with the team’s other training strategies, this lightweight approach reduces computational cost and achieves a much higher compression ratio than other existing methods. 2) 1-bit quantization by applying 1S-KD with DA and long training. The team applies quantize-aware 1S-KD (one-step knowledge distillation), using an ultra-low bit (1-bit/2-bit) quantizer to compress the layer-reduced model weights obtained in step 1 for a forward pass, then uses a straight-through estimator (STE) in a backward pass for passing gradients. The team minimizes the single-stage deep KD objective with data augmentation (DA) and longer training, such that the training loss is close to zero. In their empirical study, the team applied their novel compression approach to the BERT large language model, using the standard General Language Understanding Evaluation (GLUE) benchmark. The experimental results show that the proposed XTC can compress BERTbase to a 5-layer BERTbase while outperforming previous state-of-the-art distillation methods such as the 6-layer TinyBERT without incurring the computationally expensive pretraining distillation. The method’s robust extreme quantization can also reduce model size by 50x with better accuracy than prior extreme quantization methods; and achieve state-of-the-art results on GLUE tasks. Overall, this work introduces a simple yet effective compression pipeline for extreme compression in pretrained transformers, providing a possible solution for deploying such models on resource-constrained devices. The code will be released on the Microsoft DeepSpeed GitHub. The paper Extreme Compression for Pre-trained Transformers Made Simple and Efficient is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
Gem-Miner: Finding Lottery Tickets at Initialization and Bettering All Baselines at 19x Faster Speeds,https://syncedreview.com/2022/06/08/gem-miner-finding-lottery-tickets-at-initialization-and-bettering-all-baselines-at-19x-faster-speeds/,2022-06-08,"
In the new paper Rare Gems: Finding Lottery Tickets at Initialization, a research team from Carnegie Mellon University, MBZUAI, Petuum, Inc and the University of Wisconsin-Madison proposes GEM-MINER, an algorithm that finds sparse subnetworks at initialization trainable to accuracy that is comparable or better than iterative magnitude pruning (IMP) with warm-up.
","As artificial neural networks continue expanding in size, machine learning researchers are increasingly keen to find ways to compress them while incurring minimal performance trade-offs. Although standard pruning techniques can reduce large-scale networks’ parameter counts without sacrificing their predictive accuracy, this approach requires repeated rounds of computationally expensive retraining. In 2019, MIT researchers Frankle & Carbin won the ICLR Best Paper Award with The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks, which proposed that dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that, when trained in isolation, can reach test accuracy comparable to the original network in a similar number of iterations. How to most effectively find these winning lottery tickets however remains an open question. A research team from Carnegie Mellon University, MBZUAI, Petuum, Inc and the University of Wisconsin-Madison tackles this problem in their new paper Rare Gems: Finding Lottery Tickets at Initialization, proposing GEM-MINER, an algorithm that finds lottery tickets at initialization that are trainable to accuracy comparable or better than iterative magnitude pruning (IMP) at speeds up to 19x faster. GEM-MINER is designed to find rare gems: subnetworks with sparsity and non-trivial pretraining accuracy that can be finetuned to reach accuracy close to the original fully trained dense network. GEM-MINER uses a form of backpropagation, where each random weight is associated with a normalized score, and these normalized scores are used as optimization variables for computing the supermask, i.e. the pruning pattern of the network at initialization. In each iteration, GEM-MINER samples a set of training data and performs backpropagation on the loss of the effective weights to automatically find an optimal sparsity subnetwork. The team evaluated GEM-MINER on CIFAR-10 image classification against baselines that included dense weight training and four pruning algorithms (IMP, Learning Rate Rewinding, Edge-Popup and Smart-Ratio). In the experiments, the proposed GEM-MINER bettered all baselines, reaching high accuracy even in the early training stages. When finetuned, GEM-Miner outperformed IMP with warmup training at speeds up to 19x faster. The researchers say their work resolves the open question of pruning at initialization, finding lottery tickets at initialization that have non-trivial accuracy even before finetuning and accuracy rivalling prune-after-train methods after finetuning. The code is available on the project’s GitHub. The paper Rare Gems: Finding Lottery Tickets at Initialization is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",0.0
"200+ World-Class AI Experts at BAAI 2022: ‘AI Life’, Multimodal Models, AI for Science, Autonomous Driving and More!",https://syncedreview.com/2022/06/07/200-world-class-ai-experts-at-baai-2022-ai-life-multimodal-models-ai-for-science-autonomous-driving-and-more/,2022-06-07,"
The BAAI Conference 2022 kicked off at 9:00 am on May 31 in Beijing and ran through June 2. AI experts, industry leaders, young talents and international delegates joined the virtual gathering and live stream for three busy days of high-level keynotes, tech talks, parallel forums and networking.
","The BAAI Conference 2022 kicked off at 9:00 am on May 31 in Beijing and ran through June 2. AI experts, industry leaders, young talents and international delegates joined the virtual gathering and live stream for three busy days of high-level keynotes, tech talks, parallel forums and networking. The conference addressed a variety of contemporary AI challenges, from large-scale multimodal models and neurocomputers to AI for science and autonomous driving and much more. The host, Beijing Academy of Artificial Intelligence (BAAI), is a well-established non-profit research institute that promotes strategic collaborations between academia and industry to bridge the gap between pioneering AI research and cutting-edge applications in complex real-world scenarios.China’s most influential AI academic conference, BAAI 2022 welcomed over 200 domestic and international experts and scholars, including Israeli cryptographer and 2002 Turing Award Laureate Adi Shamir, DeepMind Distinguished Research Scientist Richard Sutton, Gödel Prize Laureate Cynthia Dwork, Head of Hugging Face Research Douwe Kiela, Head of OpenAI Research Jeff Clune, University College London neuroscientist Karl Friston, and UC Berkeley computer scientist Michael I. Jordan. Following the conference’s opening remarks, BAAI Dean and Peking University Professor TieJun Huang took the stage to introduce three important BAAI advancements: MetaWorm 1.0 (Tian Bao), the AI Chip Ecosystem Laboratory & Jiuding AI-Computing Platform, and the latest applications powered by China’s first homegrown super-scale intelligent model, WUDAO big model. BAAI Life Simulation Research Center Director Lei Ma described MetaWorm 1.0 as a computational model of the Caenorhabditis elegans (C. elegans) nematode with the “most detailed” nervous system in synergy with a digital body built on 96 muscles and interactive with a three-dimensional fluid simulation environment in real-time. This digital “Worm” has achieved forward worming in the simulation environment and represents a milestone in the eVolution project. MetaWorm 1.0 exhibits behaviours that parallel C. elegans in the real world, and the next goal on the eVolution project roadmap will be to have it demonstrate more complex intelligent behaviours such as avoidance response and optimal foraging. As MetaWorm 1.0 evolves to 2.0 and 3.0, the goal is to have it “gradually becoming intelligent.” Using neuron circuits from small animals to investigate what we can learn from nature to improve artificial intelligence technologies has always been a promising approach. For example, a 2020 study published in Nature Machine Intelligence showed how a novel AI system with 19 control neurons inspired by the brains of C. elegans could enable high-fidelity autonomy for task-specific parts of complex autonomous systems such as steering commands. In summary, MetaWorm 1.0 is the most biologically accurate computational model of C. elegans and has achieved the following breakthroughs: Significant AI technical innovations will often occur along with breakthroughs in infrastructure technologies. Reflecting this, BAAI announced the AI chip Ecosystem Lab and Jiuding AI-Computing Platform, which researchers can use to boost AI evolution with cross-layer innovation. Jiuding has the world’s largest Chinese dataset for AI training, and will provide 1000P computation capacity and 400Gbps high-speed interconnection per server to support AI research activities. To enable Jiuding to effectively run on AI chipsets from different vendors, BAAI researchers are exploring an adaptation layer with self-learning and self-adaptation capabilities which can automatically find the most suitable computing resource for different tasks. BAAI Chief Engineer Yonghua Lin explained, “With more and more AI chips being delivered into the market, AI platform infrastructure will be required to support AI chips of various architectures and different capabilities. So we need to explore automatic technologies to help the industry solve the adaptation problem. It will be a kind of AI for AI system innovation. And it will be critical for the AI chip industry as well.” At the conference, BAAI announced the open-sourcing of FlagAI (FeiZhi), an algorithm and tools project designed to support mainstream large-scale multimodal foundation models and simplify their trial and development. BAAI strongly believes in the benefits of open science and has consistently promoted an open-source approach to the innovation of AI algorithms. The FlagAI project has been launched on GitHub with plenty of tutorials and examples to support research in the global open-source community. Additional FlagAI features and algorithms will be added in future releases. The focus of the Wu Dao project for this year is to continue close collaborations with various companies in the fields of lifestyle, art and language to expedite its application on complex real-world tasks. After working with OPPO, TAL Education Group, Taobao and Sogou, the Wu Dao project’s partnership with Meituan produced a large-scale multimodal model that provides some 700 million users daily with conveniences in search ads, smart assistants and fine-grained sentiment analysis, boosting Meituan’s search ads revenue by 2.7%. Last month, BAAI released CogView2 (Synced coverage). At the conference, the CogView team showcased CogVideo as its next big breakthrough CogVideo is a 9B-parameter large-scale pretrained text-to-video transformer model, trained by inheriting a pretrained text-to-image model, CogView2. The Wu Dao project team is also carrying out a number of strategic collaborations with world-renowned organizations and institutes. It is currently working with the Arab Academy for Science, Technology & Maritime Transport and the Bibliotheca Alexandrina to build the world’s largest Arabic language dataset. The joint effort is aimed at developing large-scale Arab language models and applications for solving complex real-world challenges. Synced‘s BAAI 2022 keynote and other presentation highlights are summarized below. Richard Sutton | DeepMind Distinguished Research Scientist Sutton started his talk with an essential question: Will intelligence ultimately be explained in objective terms such as states of the external world, objects, and people; or in experiential terms such as sensations, actions, and rewards? He noted that experience has played an increasing role in this consideration over AI’s seven decades and identified four significant steps in which AI has turned toward experience to be more grounded, learnable, and scalable. Sutton posited that the alternative to the objective state is the experiential state, where the world is defined entirely in terms of experiences that are useful for predicting and controlling future experiences. He proposed the experiential state be recursively updated, and that by combining all experiential steps, a standard model of the experiential agent can be obtained. Sutton also offered a philosophical take on the matter: “Experience offers a path to knowing the world: if any fact about the world is a fact about the experience, then it can be learned and verified from experience.” Despite Steps 3 and 4 being far from complete, he noted that many AI research opportunities remain, and the story of intelligence may one day be told in terms of sensorimotor experience. Jeff Clune | OpenAI Research Team Lead Jeff Clune shared work that essentially represents two different approaches to reinforcement learning (RL): But how might we accomplish the grandest ambitions of very powerful AI? Clune’s answer was a novel paradigm: AI-Generating Algorithms. Clune suggested we look at the history of how science progresses via innovations. It is critical that science and technological innovation generate problems and simultaneously carry out goal switching. But what exactly does this mean? Clune explained that the only way to solve complex problems is by creating problems while we simultaneously solve them and performing goal switching between them. For example, when the only known cooking method was heating a hanging pot, if scientists were rewarded only if they could produce the fastest hot food with less smoke, then the microwave oven would never have been invented because its related radar technology would not have been developed.How can we get our algorithms to do “goal switching” and realize such “serendipitous discoveries”? A family of algorithms called Quality Diversity Algorithms stands out. Presently, one of the most popular algorithms of this type is MAP-Elites, which returns an entire set of high-quality solutions that Clune says are “the best you can find.” An illustrative outcome and takeaway from the MAP-Elites algorithm? The 2015 paper published in Nature,Robots that can adapt like animals, which introduces “an intelligent trial-and-error algorithm that allows robots to adapt to damage in less than two minutes, without requiring self-diagnosis or pre-specified contingency plans.” When facing the dual challenges of detachment and derailment, the Go-Explore algorithm comes into play. Go-Explore’s strategy is carried out in two phases. It first initializes itself by taking random actions, storing the states visited, and starting a simple loop until the problem is solved. It then robustifies the solutions into a deep neural network via imitation learning. “In my ideal world, algorithms do not require domain knowledge but will take advantage of it,” Clune added. This view reflects the improved results of the Go-Explore algorithm, which continues to demonstrate the value of Quality Diversity algorithms for hard-exploration problems and inspire new and novel research directions. “Collecting a diverse repertoire of high-quality entities gives you a lot of fuel to power other algorithms downstream.” “Traditional machine learning usually relies on human efforts to pick challenges for algorithms to solve,” Clune explained. “But it is way more interesting to me to focus on the notion of open-endless, which means algorithms are truly, endlessly innovating. If you can run them forever, they will keep doing interesting new things. An example of that would be the natural evolution, which has been going on for 3.5 billion years. The question is, can we make algorithms to do that? Another way to think about it is, could you make an algorithm to run for billion years?” Clune proposed researchers learn from natural evolution and human culture, where innovating means generating new problems, then solving them to generate more new problems. He enthusiastically introduced the results obtained by the enhanced POET algorithm — a phylogenetic tree of the first 100 environments of a POET run. “This is one of my most favourite plots and results in my entire scientific career because I have been trying to generate this plot since I was a PhD student on Day One.” Clune said the results left him with an unforgettable impression since they resemble those from nature, exhibiting a clear signature of open-ended algorithms: multiple, deep, hierarchically nested branches. Clune concluded by noting that although we have a long way to go before achieving artificial general intelligence (AGI), there are three pillars that could support research toward that goal: Clune believes AI-generating algorithms would likely be the fastest path to AGI. The hypothesis is that pieces of AI are learnable, and learned solutions ultimately win out, so we should be going all-in on learning the solutions to AGI. “These algorithms are worthwhile even if they are not the fastest path because they shed light on our origins, and they tend to be very creative and could surprise us,” Clune told the audience. “They can create an entirely new intelligence that we could never dream of and teach us what it means to be intelligent.” Luke Zettlemoyer | Meta AI “Will large language models keep getting bigger?” asked Luke Zettlemoyer, addressing a hot topic in machine learning research. As Synced previously reported, the large-scale multimodal Wu Dao model has a whopping 1.75 trillion parameters, roughly ten times that of OpenAI’s powerful GPT-3. Today’s data-hungry language models are not just getting a lot bigger; they have also become adept zero-shot learners. Will researchers be able to keep scaling them up, or are we approaching the physical limitations of computing hardware? How can we best use these big models? And what about alternative forms of supervision? Zettlemoyer introduced two ideas he and his research team have been investigating to enable further scaling: Zettlemoyer noted that in expert specializations, an assignment often depends only on the previous word and corresponds to simple word clusters. Could even simpler routing work? “When you’re trying to scale, you’d want to go simple,” Zettlemoyer suggested, noting that the DeMix Layers approach is “much much simpler, and we are currently actively trying to scale up.” Each domain has its expert, and the modular model can mix, add, or remove experts as necessary, with rapid adaptation throughout training and testing. Zettlemoyer showcased recent studies on alternatives to fine-tuning for natural language generation tasks that keep language model parameters frozen, such as the lightweight “Prefix-Tuning” paradigm proposed by Stanford University researchers Li and Liang. These can deliver a “much better result based on a little bit of compute and careful framing,” and also enable directly learning a noisy channel prompting model for even better performance. Zettlemoyer stressed the significance of open science to the research community and pointed out that limited access to models and restricted availability through APIs is “deeply problematic for doing good science.” He encouraged the audience to thoroughly explore research papers and look at the parameters to “see what the models are doing as much as you can.” Zettlemoyer summarized another project Facebook AI has been actively scaling up and trying to generalize for newer methods and suggested new types of training supervisions, with images, text, and more as discrete tokens. “Text is not all you need, consider the structure and other modalities,” he concluded. Cynthia Dwork | Radcliffe Institute for Advanced Study “What are risk prediction algorithms actually producing?” asked Professor Dwork to start her talk. She characterized this as a defining problem of artificial intelligence because it seeks to understand what the produced numbers mean and, more specifically, the “probability” of a non-repeatable event. Dwork walked the audience through the landscape of previous relevant scientific research, then introduced Outcome Indistinguishability, a recent study she worked on with researchers from UC Berkeley, Stanford University, and the Weizmann Institute of Science. With regard to this “probability” function, the team’s paper argued that “prediction algorithms assign numbers to individuals that are popularly understood as individual ‘probabilities’ — e.g. what is the probability of 5-year survival after a cancer diagnosis? — and which increasingly form the basis for life-altering decisions.” The study established the first scientific grounds for a “political argument” recommending auditors inspecting algorithmic risk prediction instruments be granted oracle access to the algorithm rather than simple historical predictions. The talk was followed by an insightful Q&A session between Professor Dwork and BAAI Chairman of the Board HongJiang Zhang. Professor Dwork mentioned that after working on differential privacy for many years, she wanted to come up with a new problem. Then, ten years ago, an intense and inspirational brainstorming session with a colleague from Tel Aviv University that spanned various topics drew her attention to the fairness of machine learning.We are now at a time when computer science is increasingly involved in social science and AI ethics. For young scholars, Dwork advised, “One of my key pieces of advice is to read the news and follow what’s going on in the world. See what you like and what you don’t like. And then, for everything that you don’t like, my approach would be, how on earth can theoretical computer science contribute to this? And for the things you like, how can theoretical computer science develop this? So constantly look for how the tools you have could be relevant to the major questions you see around you.”Dwork says paying close attention to societal affairs will enable young scholars to stay informed about the world and “ask how what you’ve learned that day in your technical studies could be relevant to the world.” After all, she added, once a scholar is trained, they become a precious resource, and their efforts and technical skills also bring an essential responsibility to society.As a strong believer in basic research with a deep understanding of and working experiences in academia and industry, Dwork said both career paths could be great as long as they support basic research. She suggested young scholars evaluate their pursuits, “For people who are more theoretical, what’s wonderful about industry research is the opportunity to see your theoretical idea deployed on a massive scale. Think about your security and where you want to work in your career, considering the job security and risk-taking.”Dwork believes it is vital to include algorithmic fairness and privacy among critical machine learning metrics such as accuracy. “I really believe that underlying everything is a metric. There is a notion somewhere, for a particular task, of how similar or dissimilar are each pair of individuals for this task? I would like to see a lot more work on that. I would also like to see a lot more work on the question of whether data representation itself is unfair. Everything begins with the data. So I suppose that would be the next big topic that I would like to look at, and I really encourage people watching this talk to think about that problem.“ In the Annual Young Scholars Forum and Meetup, HongJiang Zhang and UC Berkeley Professor Michael Jordan, who also serves on the BAAI Academic Advisory Committee, had a remarkable conversation on young scholars and career choices. Professor Jordan cautioned young scholars not to simply chase the next big thing in machine learning research. “I prefer to go a little slower, not to have a big competition, and to work with my students at my own pace. I always try to somewhat orthogonalize with respect to where everyone else is going. We can think about decisions and uncertainty, uncertainty qualification, out-of-sample kinds of things where the distribution changes, economic incentives for data collection and sharing data, and competitive mechanisms for making decisions when there are multiple decision-makers. These are all things that happen in the real world, and they are things that there’s much less work on, but there’s not zero work. One of the reasons I choose to work in such areas is because I see that they are really important, have real-world consequences, and they’ve been neglected, so not that many people are thinking about them.“Similar to Professor Dwork’s advice to young scholars, Jordan stressed that we must remember researchers are also real-world problem solvers. He added, “You really should think about a lifetime career. You should realize you’re not going to work on one thing. I spend at least 30% of my time learning new things or learning about topics that seem like they will be relevant to me sometime in the future.“There are lots of great videos now, and lots of great, even undergraduate-level books on some really interesting topics. I don’t look for them to give immediately me research ideas or a payoff. But after five years or ten years, almost always, they come to be relevant.”“So you have to be a problem solver. You can’t just come in and apply the technology and walk away and move on. You have to spend the time understanding the problem and bringing the technology to bear. I’ve done a lot of applied math, I’ve got a lot of control theory, and I’ve done a lot of statistics. So my brain has a partial understanding of all those things — and I can tell you that’s been the biggest secret of my success.” The first BAAI conference was held in October 2019. Informed by the organizers’ growing experience in fostering a welcoming environment for top talents and with a consistent focus on exploring fundamental AI technologies, BAAI 2022 proved an outstanding Expert AI Conference, attracting 30,000 registrations and 100,000 online attendees from various Chinese provinces and 47 international countries and regions.. BAAI provides valuable platforms for the ever-growing global community of scholars, students, and industry experts pursuing advancements in AI research and real-world applications, helping them to learn, share and flourish. Author: Fangyu Cai| Editor: Michael Sarazen",0.0
Snap & NEU’s  EfficientFormer Models Push ViTs to MobileNet Speeds While Maintaining High Performance,https://syncedreview.com/2022/06/06/snap-neus-efficientformer-models-push-vits-to-mobilenet-speeds-while-maintaining-high-performance/,2022-06-06,"
In the new paper EfficientFormer: Vision Transformers at MobileNet, a research team from Snap Inc. and Northeastern University proposes EfficientFormer, a vision transformer that runs as fast as MobileNet while maintaining high performance.
","First proposed in 2020, vision transformers (ViT) have demonstrated promising performance across a variety of computer vision tasks. These breakthroughs however have come at the cost of speed, as ViTs run much slower than convolutional neural networks (CNNs). This latency issue and their extremely high computational costs have made it challenging to deploy ViTs on resource-constrained hardware such as mobile devices, limiting their real-world application. A research team from Snap Inc. and Northeastern University addresses this issue in the new paper EfficientFormer: Vision Transformers at MobileNet, which identifies inefficient operators in ViT architectures and proposes a new ViT design paradigm. The team’s resulting EfficientFormer models run as fast as lightweight MobileNet CNNs while maintaining the high performance of transformer architectures. The researchers summarize their study’s main contributions as: The proposed EfficientFormer comprises patch embedding and a stack of meta transformer blocks, where each block contains an unspecified token mixer followed by a multilayer perceptron block. The network has four stages, each serving as an embedding operation that maps the embedding dimensions and downsamples token length. EfficientFormer thus remains a fully transformer-based model that does not use MobileNet structures. The team also introduces a simple yet effective gradient-based search algorithm that obtains candidate networks to optimize EfficientFormer’s inference speed. In their empirical study, the team compared EfficientFormer with widely used CNN-based models and existing ViTs on image classification, object detection, and segmentation tasks. EfficientFormer outperformed existing transformer models and most competitive CNNs in the experiments, with the fastest variant, EfficientFormer-L1, achieving 79.2 percent top-1 accuracy on ImageNet-1K with only 1.6 ms inference latency on an iPhone 12; and the largest variant, EfficientFormer-L7, reaching 83.3 percent accuracy with only 7.0 ms latency. The study shows that ViTs can reach MobileNet speeds on mobile devices while maintaining transformers’ high performance. The team’s future research will explore EfficientFormer’s potential on other resource-constrained hardware.The EfficientFormer code and models are available on the project’s GitHub. The paper EfficientFormer: Vision Transformers at MobileNet Speed is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
NVIDIA & UW Introduce Factory: A Set of Physics Simulation Methods and Learning Tools for Contact-Rich Robotic Assembly,https://syncedreview.com/2022/06/03/nvidia-uw-introduce-factory-a-set-of-physics-simulation-methods-and-learning-tools-for-contact-rich-robotic-assembly/,2022-06-03,"
In the new paper Factory: Fast Contact for Robotic Assembly, a research team from NVIDIA and the University of Washington introduces Factory, a set of physics simulation methods and robot learning tools for simulating contact-rich interactions in assembly with high accuracy, efficiency, and robustness.
","When we think of modern industrial assembly lines, we can imagine a tireless ensemble of task-specific robots efficiently cutting, stamping, connecting, inserting, tightening or soldering whatever product the factory is manufacturing. While assembly is one of the oldest and widest applications of robotics, effectively simulating its many high-precision and contact-rich interactions remains a challenging task. In the new paper Factory: Fast Contact for Robotic Assembly, a research team from NVIDIA Corporation and the University of Washington introduces Factory, a set of physics simulation methods and robot learning tools for simulating contact-rich interactions in assembly with high accuracy, efficiency, and robustness. The team summarizes their study’s main contributions as: The researchers set out to build a tool that enables fast, accurate, and robust robotic assembly simulation with three key considerations: 1) geometric representations, 2) contact reduction schemes, and 3) numerical solvers. For geometric representation, the team adopted discrete, voxel-based SDFs to map points to distance-to-a-surface and ensure efficient, robust collision detection. For contact reduction, they combined normal similarity, penetration depth, and an area-based metric to reduce contacts and demonstrate the desired dynamics properties across various evaluation scenes. The team chose the Gauss-Seidel method as their numerical solver, as it can be accelerated via contact reduction to achieve better performance than other popular solvers such as Jacobi. The team’s resulting suite of physics simulation methods and robot learning tools — which they dub “Factory” — is able to simulate thousands of contact-rich interactions in PhysX and Isaac Gym environments in real-time on a single GPU. The researchers also provide 60 carefully designed, ISO-standard or manufacturer-based assets from the NIST Assembly Task Board 1 for high-accuracy simulation; and train proof-of-concept reinforcement learning (RL) policies in Isaac Gym for contact-rich nut-and-bolt assembly. Although Factory was designed to establish a state-of-the-art for contact-rich simulation in robotic assembly, the researchers say it can also be applied to additional robotics tasks such as grasping of complex non-convex shapes in home environments, locomotion on uneven outdoor terrain, and non-prehensile manipulation of aggregates of objects. The researchers hope that their work can help accelerate the efficiency of robotic assembly, and invite the machine learning community to establish benchmarks for solving the provided scenes and extend Factory to their own contact-rich applications. A Factory demo video is available on Vimeo. The paper Factory: Fast Contact for Robotic Assembly is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",0.0
Google Brain’s UViM: A Unified Approach for Modelling Diverse Vision Tasks Without Modifications,https://syncedreview.com/2022/06/02/google-brains-uvim-a-unified-approach-for-modelling-diverse-vision-tasks-without-modifications/,2022-06-02,"
In the new paper UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes, a Google Brain research team proposes UViM, a unified approach that leverages language modelling and discrete representation learning to enable the modelling of a wide range of computer vision tasks without task-specific modifications.
","Deep neural networks have revolutionized the field of computer vision, achieving unprecedented performance across a wide range of tasks. The production of high-dimensional structured outputs for vision tasks such as image segmentation, monocular depth estimation, object detection, etc. however requires human handcrafting of network architectures and tailoring of training procedures for each specific task. These are time-consuming processes that can also introduce the need for expert knowledge with regard to the task at hand. A Google Brain research team challenges this “fragmented” vision modelling paradigm in their new paper UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes, proposing UViM (Unified Vision Model), a novel approach that leverages language modelling and discrete representation learning to enable the modelling of diverse computer vision tasks without any task-specific modifications. In the field of natural language processing (NLP), autoregressive sequence models parameterized by transformer architectures have emerged as a prominent unified model that enjoys advantages such as theoretical soundness, expressiveness, and robustness. This motivated the Google researchers to design a similar general solution for computer vision. The proposed UViM is a unified computer vision model that combines a standard feedforward base model and an autoregressive language model. It can handle vision tasks that deal with extremely high dimensional and structured outputs with much lower computational costs. The UViM optimization procedure comprises two training stages: learning with a guiding code and learning to model the guiding code. In the first stage, a restricted oracle model produces a short discrete sequence (guiding code) to help the base model solve complex vision tasks and reduce the cost of high-dimensional structured prediction. In the second stage, the team trains a language model to output a guiding code by learning to “mimic” the oracle using only the image input. The resulting UViM is thus equipped to model highly structured outputs for diverse vision tasks. In their empirical study, the team applied UViM to three diverse vision tasks: general scene understanding panoptic segmentation, conditional generative image colorization, and 3D scene depth prediction understanding. In the evaluations, the proposed UViM achieved results competitive with the state-of-the-art on all three tasks, confirming its ability to handle diverse vision tasks in a unified manner. The team regards UViM as a “brave new prototype” for a general-purpose unified computer vision model and hopes their paper will motivate future research on the generation of better guiding codes and the design of more efficient training procedures. The paper UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",1.0
The CLRS Algorithmic Reasoning Benchmark,"[{'href': 'http://arxiv.org/abs/2205.15659v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.15659v2', 'rel': 'related', 'type': 'application/pdf'}]",2022-05-31 09:56:44,,"2 2 0 2 n u J 7 1 ] E M . t a t s [ 1 v 6 1 6 8 0 . 6 0 2 2 : v i X r a Dynamical Modeling for non-Gaussian Data with High-dimensional Sparse Ordinary Diﬀerential Equations Muye Nanshan, Nan Zhang School of Data Science, Fudan University and Xiaolei Xun Global Statistics and Data Science, BeiGene and Jiguo Cao Department of Statistics and Actuarial Science, Simon Fraser University Abstract Ordinary diﬀerential equations (ODE) have been widely used for modeling dy- namical complex systems. For high-dimensional ODE models where the number of diﬀerential equations is large, it remains challenging to estimate the ODE parame- ters and to identify the sparse structure of the ODE models. Most existing methods exploit the least-square based approach and are only applicable to Gaussian obser- vations. However, as discrete data are ubiquitous in applications, it is of practical importance to develop dynamic modeling for non-Gaussian observations. New meth- ods and algorithms are developed for both parameter estimation and sparse structure identiﬁcation in high-dimensional linear ODE systems. First, the high-dimensional generalized proﬁling method is proposed as a likelihood-based approach with ODE ﬁdelity and sparsity-inducing regularization, along with eﬃcient computation based on parameter cascading. Second, two versions of the two-step collocation methods are extended to the non-Gaussian set-up by incorporating the iteratively reweighted least squares technique. Simulations show that the proﬁling procedure has excellent performance in latent process and derivative ﬁtting and ODE parameter estimation, while the two-step collocation approach excels in identifying the sparse structure of the ODE system. The usefulness of the proposed methods is also demonstrated by analyzing three real datasets from Google trends, stock market sectors, and yeast cell cycle studies. Keywords: Dynamic system; Generalized linear model; Ordinary diﬀerential equations; Parameter cascade; Penalized likelihood; Proﬁled estimation. 1 1 Introduction Ordinary diﬀerential equations (ODE) are widely used for complex dynamic system mod- eling in biology, engineering, econometrics, and other scientiﬁc and social applications. For example, massive gene expression proﬁles are available with the advancement of second- generation sequencing technology. Modeling their dynamics using gene regulatory networks has drawn signiﬁcant interest from both biomedical and statistical research communities (Stuart et al., 2003; Yuan and Kendziorski, 2006; Hecker et al., 2009; Polynikis et al., 2009; Lu et al., 2011; Wu et al., 2014). In computational sociology, public opinion sensing and trend analysis have emerged from the advent of the big data revolution (Dodds et al., 2011; Sloan and Morgan, 2015). Massive datasets, such as Google searches or Twitter posts, are collected daily or even hourly, which enables social scientists to extract interesting temporal or spatial patterns via dynamic modeling. The main purpose of this article is to propose new methods and algorithms to estimate the ODE parameters and to identify the sparse structure for high-dimensional ODE models with non-Gaussian observations. A general ﬁrst-order ODE system can be described as θ′(t) = f (θ(t), β), (1.1) where the vector θ(t) = (θ1(t), . . . , θp(t))⊤ collects p processes while θ′(t) is the ﬁrst-order derivative of θ(t), function f = (f1, . . . , fp) describes the dependence between processes and their derivatives, β is the vector of ODE parameters to be estimated. Typically, the processes are indexed with time t and some initial conditions, for example, θ(0) = θ0, are assumed for the ODE system (1.1) as well. In practice, observations from the dynamic system are measured according to the re- alizations of latent processes θ(t) at discrete time points. Estimation of ODE parame- ters from noisy data remains a challenging problem (Ramsay et al., 2007; Wu et al., 2014; Hall and Ma, 2014; Chen et al., 2017; Wu et al., 2019; Dai and Li, 2021). In general, pa- rameter estimation procedures fall into three categories. The ﬁrst approach is based on a data ﬁtting process by nonlinear least squares. Given a set of initial ODE parameters, the ODE solutions are approximated by numerical methods, for example, the Runge-Kutta algorithm. Then the ODE parameters are updated with the nonlinear least squares. This 2 approach is computationally intensive and can be potentially inaccurate due to iterative numerical approximations. The second approach is the two-step collocation, where the basis expansions are exploited to approximate the ODE solutions. Varah (1982) proposed to ﬁt the processes via data smoothing methods, followed by a second stage of minimizing a least-square criterion based on the ODE system to estimate the ODE parameters. Because of its computational advantage, two-step collocation gains much popularity in the develop- ment of methodology and applications (Liang and Wu, 2008; Lu et al., 2011; Brunel et al., 2014; Wu et al., 2014; Dattner and Klaassen, 2015) and is further improved by iterative principal diﬀerential analysis (Ramsay, 1996; Poyton et al., 2006). However, the perfor- mance of two-step procedures relies heavily on the smoothing step, while the amount of roughness regularization is hard to control. The third approach is the generalized proﬁling procedure (Ramsay et al., 2007), which also represents ODE solutions with basis expan- sion as with two-step collocation methods. The essential diﬀerence is the inclusion of an ODE-induced penalty that controls the ﬁdelity of the processes to the ODE system. The basis coeﬃcients and ODE parameters are then estimated simultaneously from a penalized criterion using the parameter cascading algorithm (Cao and Ramsay, 2007). From a the- oretical perspective, Qi and Zhao (2010) derived an upper bound on the uniform norm of the diﬀerence between the true underlying solutions and their approximations, and proved the consistency and asymptotic normality of the estimation procedure. More recently, there has been growing interest in high-dimensional ODE systems where the number of processes p is large. For instance, the high-dimensional time-course gene expression data enables biomedical researchers to model the regulatory behaviors via a large-scale directed graphical network model. Such a task is called network recovery. The ODE system (1.1) naturally serves for this purpose by relating the dynamics of each process with all the processes in the system, and a sparse network structure can be further imposed. Lu et al. (2011) considered the high-dimensional linear ODE for dynamic gene regulatory network identiﬁcation and applied the smoothly clipped absolute deviation (Fan and Li, 2001) approach for variable selection. Wu et al. (2014) further relaxed the linear assump- tion and investigated a sparse additive ODE model using a two-stage procedure coupled with the adaptive group Lasso technique (Wang and Leng, 2008) to deal with nonlinear 3 eﬀects. Chen et al. (2017) proposed an eﬃcient procedure using the integrated form of the ODE to bypass numerical diﬃculty in the derivative estimation and adopted the group Lasso (Yuan and Lin, 2006) for variable selection. Wu et al. (2019) recently developed a matrix factorization based approach to ultra-high dimensional linear ODE models for pa- rameter estimation and variable selection. To our best knowledge, existing procedures for high-dimensional ODE models are two-stage approaches. Besides, most of the existing work assumes that observations of the ODE system are contaminated with Gaussian noises. Therefore, least-squares estimation is conveniently adopted. However, non-Gaussian observations are commonly encountered in real appli- cations, for example, short read count data from RNA sequencing (Nagalakshmi et al., 2008), bisulﬁte sequencing data for DNA methylation analysis (Cokus et al., 2008), and direction of change in the stock price over time (Huang et al., 2005). The literature on non-Gaussian data analysis with the ODE system is rare. Miao et al. (2014) developed a likelihood-based parameter estimation and inference for generalized ODE models. Its extension to high-dimensional ODE models, however, is still unknown. Motivated by network recovery tasks for time-course non-Gaussian data, this paper focuses on the parameter estimation and sparse structure identiﬁcation for high-dimensional linear ODE systems with a likelihood-based approach. To facilitate versatile analysis of non-Gaussian data, we assume the observations follow a distribution from the exponential family, where θj(t) is known as the canonical parameter in the context of generalized linear models (McCullagh and Nelder, 1989; Wood, 2017). Assume that t ∈ [0, 1] without loss of generality. Given a set of discrete time points t1, . . . , tn, denote by yij the measurement according to the jth latent process θj(t) at time t = ti, j = 1, . . . , p. Then, the conditional distribution of yij given θj(ti) admits a density function as f (yij | θj(ti)) = exp yijθj(ti) − b(θj(ti)) a(φ) (cid:26) + c(yij, φ) , (cid:27) where a > 0, b, c are known functions, φ is either known or considered as a nuisance parameter. Let (y1j, . . . , ynj)⊤ be the vector of observations from the latent process θj(t), and correspondingly the canonical parameter vector be (θj(t1), . . . , θj(tn))⊤. Imposing a linear structure on the general model (1.1), we investigate in this work the modeling of the dynamics among latent processes {θj(t) : j = 1, . . . , p} with a high-dimensional linear 4 ODE system, that is ′ θ j(t) = γj0 + p k=1 X γjkθk(t), j = 1, . . . , p. (1.2) In this article, we develop new methods and algorithms for both parameter estima- tion and sparse structure identiﬁcation in high-dimensional linear ODE systems. First, we propose the high-dimensional generalized proﬁling method along with a computationally eﬃcient procedure based on parameter cascading (Ramsay et al., 2007; Cao and Ramsay, 2007). It solves a hierarchical optimization for parameter estimation and variable selec- tion: an outer optimization concerning the ODE parameters under sparsity regularization is performed subject to an inner optimization where latent processes expanded with basis functions are ﬁtted by minimizing a weighted sum of data ﬁtting and ODE ﬁdelity crite- ria given ODE parameters. In particular, we regularize the structural ODE parameters based on individual diﬀerential equation and mitigate the computational burden for pa- rameter estimation in the high-dimensional ODE system. Moreover, there are two tuning parameters involved in our procedure: one controls the balance between data ﬁtting and ODE ﬁdelity in the inner optimization while the other regularizes the sparsity or model complexity in the outer optimization. Their interaction may aﬀect the overall convergence performance of the procedure in a complicated way. Due to the non-convexity nature of our objective function, we carefully design the tuning and stopping rules according to the performance of parameter estimation to help escape local minima (Carey and Ramsay, 2021). The global convergence of the proposed algorithm is analyzed. Next, we extend the two-step collocation methods (Wu et al., 2014; Chen et al., 2017), which are recently pro- posed for high-dimensional ODE models with Gaussian observations, to the non-Gaussian set-up. Two versions, corresponding to the vanilla collocation (Varah, 1982) and the graph reconstruction via additive diﬀerential equations (GRADE) (Chen et al., 2017), are devel- oped under the likelihood-based framework. Eﬃcient computation is feasible by applying the iteratively reweighted least squares technique (Wood, 2017). Finally, we apply the proposed methods to simulated and real data sets. In general, the proﬁling method is more eﬃcient than two-step collocation methods in estimating the latent processes, their deriva- tives, and the structural ODE parameters, while one two-step collocation method excels 5 in identifying the sparse structure of the ODE system. To sum up, the proposed methods present a versatile toolbox for parameter estimation and sparse structure identiﬁcation in high-dimensional linear ODE systems. The remainder of the article is organized as follows. Our proﬁled estimation approach is developed in Section 2. Detailed computational procedure and its global convergence are discussed in Section 3. In Section 4, we extend two-step collocation methods to model non-Gaussian observations. Section 5 compares empirical performance of the proposed methods. We analyze three real data examples in Section 6 with dynamical modeling approaches. Section 7 concludes the article and Appendix collects some technical details. 2 High-dimensional Generalized Proﬁling This section introduces the proposed approach for simultaneous parameter estimation and sparse structure identiﬁcation in a high-dimensional linear ODE model for non-Gaussian data under the penalized likelihood estimation framework. Denote by Γ = (γ1, . . . , γp) the parameter matrix of the ODE model (1.2), where γj = (γj0, . . . , γjp)⊤ ∈ Rp+1, for j = 1, . . . , p. These ODE parameters Γ are of primary interest in order to understand the network structure, called structural parameters hereafter. On the other hand, the latent processes θj’s are treated as nuisance parameters. Denote by yij and θj(ti) the observation and the canonical parameter of the jth latent process at time ti, respectively. Under the proﬁling scheme (Ramsay et al., 2007), an intermediate ﬁt of latent processes θ(t; Γ) = ( θ1(t; Γ), . . . , θp(t; Γ)) minimizes the following penalized likelihood criterion, b b b p 1 n p {yijθj(ti) − b(θj(ti))} + λθ − 1 np p 2 ′ θ j(t) − γj0 − 0 ( γjkθk(t) dt, (2.1) ) i=1 X j=1 Z X where the likelihood part measures ﬁdelity to data, the ODE ﬁdelity part measures the j=1 X k=1 X extent to which latent processes fail to satisfy the ODE system, and the tuning parameter λθ controls the amount of regularization. Furthermore, with θ(t; Γ) plugged in, an estimate of the structural parameters can be obtained by minimizing a data ﬁtting criterion with b 6 respect to Γ, − 1 np i=1 X j=1 X n p {yij θj(ti; Γ) − b( θj(ti; Γ))}. (2.2) The generalized proﬁling procedure proceed iteratively with a non-decreasing sequence of b b λθ under certain rules such that the ﬁtted processes adhere to the ODE. Identiﬁable issue and asymptotic behavior of the estimation procedure are studied by Ramsay et al. (2007) and Qi and Zhao (2010). Although the generalized proﬁling method provides a computationally eﬃcient treat- ment for the challenging ODE parameter estimation, it can only handle relatively small- scale models (Wu et al., 2019). On the one hand, for a p-dimensional linear ODE system, we have p2 + p ODE parameters to estimate in (2.2). If we further approximate the latent process θj(t) by basis expansion c⊤ j hj(t), where hj(t) is an mj-dimensional basis vector and cj is the coeﬃcient vector, then (2.1) becomes − 1 np n p yijc⊤ j hj(ti) − b(c⊤ j hj(ti)) i=1 X j=1 X (cid:8) p 1 + λθ 0 ( j=1 Z X (cid:9) c⊤ j h ′ j(t) − γj0 − p γjkc⊤ j hk(t) 2 ) dt, k=1 X and the total number of nuisance parameters p j=1 mj can be huge. Therefore, a di- rect application of the standard generalized proﬁling procedure to parameter estimation P for high-dimensional linear ODE is computationally demanding. On the other hand, the structural parameters obtained from (2.2) indeed infer an interaction network among the latent processes, in the sense that a nonzero γjk implies that θk(t) has an eﬀect on the change of θj(t). For better interpretation and to avoid potential over-ﬁtting, it is reason- able to introduce some sparsity for the structural parameters. For example, the Lasso and its variants (Tibshirani, 1996; Yuan and Lin, 2006; Zou, 2006), the smoothly clipped absolute deviation (SCAD) (Fan and Li, 2001) and the minimax concave penalty (MCP) (Zhang, 2010) have been extensively studied and used to recover probabilistic graphical structures (Yuan and Lin, 2007; Fan et al., 2009; Voorman et al., 2014). To address the above computational issues, we ﬁrst notice that the data ﬁdelity term in the penalized criterion (2.1) can be decomposed into sums of the likelihood for p in- dividual processes. Meanwhile, the penalty term, being a squared L2 norm of diﬀerential 7 equations, does not admit such decomposable property. Therefore, we propose to regularize the estimate of θj only by the corresponding jth diﬀerential equation. Speciﬁcally, when estimating θj given other {θk : k 6= j} at their most recent updates, we obtain θj(t; γj) by minimizing Gj(θj; γj) = − 1 n n i=1 X {yijθj(ti) − b(θj(ti))} + λθ,j 1 ′ j(t) − γj0 − θ 0 (cid:26) Z b 2 γjkθk(t) dt, (2.3) (cid:27) p k=1 X for j = 1, . . . , p. For simplicity, we use the same tuning parameter for individual sub- problems, that is λθ,j = λθ for j = 1, . . . , p. Optimizing Gj involves only p + 1 structural parameters in the vector γj and hence the computational complexity is greatly reduced. The beneﬁt of using (2.3) is justiﬁed from two aspects. First, it is computationally in- feasible to estimate a large number of ODE parameters jointly by directly applying the original generalized proﬁling criterion (2.1) to the high-dimensional ODE system. Our new formulation decouples the dependency of θ(t; Γ) on the matrix Γ into individual depen- dencies of θj(t; γj) on the vector γj. Second, from the perspective of penalized estimation, b it improves the estimation for the latent process and the ODE structural parameters by employing diﬀerential equations to regularize data smoothing. We remark on the potential risk of employing (2.3) instead of (2.1) when estimating the latent processes. Note that (2.1) aggregates all the diﬀerential equations to update the latent processes altogether such that the estimates will follow the ODE system jointly. In contrast, our method uses a single diﬀerential equation to regularize the estimation of each latent process. When the tuning parameter λθ increases, the parallel updating procedure (2.3) over j = 1, . . . , p, is expected to achieve an approximation in a marginal way to the joint estimation by (2.1). The simulation example introduced in Section S1 of the Supplementary Material shows that the approximation by (2.3) performs reasonably well, although the joint method (2.1) has a more accurate estimate for ODE parameters. Next, to induce sparsity to the structural parameter matrix, we estimate γj by mini- mizing Hj(γj) = − 1 n i=1 X n {yij θj(ti; γj) − b( θj(ti; γj))} + PENλγ,j (γj), (2.4) where the penalty function PENλγ,j (γj) with tuning parameter λγ,j > 0 induces sparsity for b b the structural parameter of the jth diﬀerential equation. Here we also assume for simplicity 8 that λγ,j = λγ for j = 1, . . . , p. If the ﬁtted structural parameter vector γj is zero, then we γj implies that the b b say other latent processes have no impact on θj(t). Any zero element in corresponding process has no inﬂuence on θj(t). The amount of sparsity regularization is typically determined by Bayesian information criterion (BIC) type principles, which have been adopted in other ODE parameter estimation approaches (Wu et al., 2014; Chen et al., 2017). Our new proﬁling estimation procedure for high-dimensional linear ODE systems con- sists of two objective functions (2.3) and (2.4), which are referred to as inner and outer criteria, respectively. Such a multi-criterion optimization problem is challenging due to non-convexity and non-diﬀerentiability. Speciﬁcally, we approximate the latent processes with basis expansion in the inner optimization, and basis coeﬃcients can be solved eﬃ- ciently with the Newton-Raphson method. However, the dependence of θj(t; γj) on γj is complicated and in general non-linear, which leads to the non-convexity of Hj. Moreover, b the sparsity-inducing penalty in Hj is non-diﬀerentiable at zero, making the Gauss-Newton scheme adopted by Ramsay et al. (2007) invalid under this scenario. Recent advances of derivative-free optimization algorithms (Powell, 2006; Zhang et al., 2010) may provide a viable solution. Nevertheless, they are in spirit joint optimization algorithms designed for general purpose and are thus not tailored for our speciﬁc problem. In contrast, our proﬁling procedure enjoys not only estimation eﬃciency but also algo- rithmic eﬃciency due to the use of analytical expressions of derivatives. Computational details are presented in the next section. In brief, after obtaining an estimate θj(t; γj) given the structural parameters, we linearize the likelihood component in (2.4) and formu- b late the outer optimization as a parameter estimation problem for a penalized generalized linear model. Therefore, the structural parameters can be readily updated by the iterative reweighted least-squares (IRLS). Through an iterative scheme between inner and outer op- timizations, our proﬁling procedure provides ODE parameter estimates and latent process ﬁts and identiﬁes the sparse structure of the ODE model. 9 3 Computation In this section, we provide computational details of our proﬁling procedure for high- dimensional linear ODE and analyze its global convergence. Minimizing criteria in (2.3) and (2.4) are referred as inner and outer optimizations. The structural parameters Γ = (γ1, . . . , γp) is of our primary interest, while the latent process ﬁts by the inner optimiza- tion is regarded as a nuisance parameter. In our proﬁling scheme, whenever γj changes by minimizing Hj in the outer, latent process ﬁts are then updated by solving the inner criterion Gj. Details are provided in Algorithm 1. In addition, two tuning parameters are involved in the proﬁling procedure, and their complex interaction aﬀects the overall algo- rithmic performance because of the non-convexity of the optimization. In the following, we split the discussion into inner and outer parts. Then we discuss the practical strategy of tuning parameter selection and the global convergence of the proposed algorithm. Algorithm 1: High-dimensional linear ODE for non-Gaussian data Input: Observations {yij : i = 1, . . . , n; j = 1, . . . , p}, initial ODE paramters Γ(0) = (γ(0) 1 , . . . , γ(0) p ), and ﬁxed tuning parameters λθ and λγ. Output: Estimated ODE parameters Γ = ( γ1, . . . , γp). repeat b At step s ≥ 1, the current estimate is b Γ(s) = ( b γ(s) 1 , . . . , γ(s) p ). for 1 ≤ j ≤ p do Update γj via the proﬁling procedure. b b b repeat 1. Given current γj, obtain the basis coeﬃcient estimate c∗ j ( γj) for the jth latent process in the inner optimization. e e 2. Apply basis expansion and update γj via minimizing the penalized reweighted least squares. until γj converges, and set γ(s+1) j = γj. end e until Estimated ODE parameter e Γ converges. b 10 3.1 Inner Optimization The inner procedure aims at ﬁnding an accurate estimate for latent processes given the structural parameter Γ. Similar to the two-step collocation method (Varah, 1982) and the generalized proﬁling (Ramsay et al., 2007), we represent latent processes by basis expan- sion. Suppose hj(t) = (φj1(t), . . . , φjmj (t)) is a set of basis functions for the jth process such that θj(t) = c⊤ j hj(t). Choices of basis functions include polynomials, truncated power functions and splines. In our numerical study, we use B-spline due to its numerical stability and excellent empirical performance. For notation simplicity, we use the same basis h(t) for all latent processes. Although critical in optimization, the basis coeﬃcients cj, j = 1, . . . , p, are often not of direct concern and thus considered as nuisance parameters. Observing that Gj(θj; γj) is convex with respect to the basis coeﬃcients cj, we can apply the Newton-Raphson scheme directly. When the Hessian of Gj(θj; γj) is invertible, we can start with an initial guess of cj and iteratively obtain c(r+1) j = c(r) j − (cid:18) −1 c(r) j (cid:19) (cid:18) ∂2Gj ∂cj∂c⊤ j (cid:12) (cid:12) (cid:12) (cid:12) ∂Gj , ∂cj (cid:12) c(r) j (cid:19) (cid:12) (cid:12) (cid:12) r ≥ 1. Analytical expressions of the derivatives involved in the above updating rule are given in A. 3.2 Outer Optimization The outer optimization is designed for updating γj with a regularized likelihood objective function (2.4). Denote by c∗ the inner optimization given the current γj. Observing that the dependence of c∗ j (γj) the optimal basis coeﬃcients for θj(t; γj) obtained from j (γj) on γj is implicit and possibly complicated, we propose to linearize the likelihood component in (2.4) and transform the optimization to ﬁnding the maximum likelihood estimate of a generalized linear model. The solution can then be readily obtained by the iteratively reweighted least squares (IRLS), see Wood (2017) for more detail. Let γj be the most recent update of γj. First, we linearize the c∗ j (γj) at γj which, e j (γj) ≈ c∗ c∗ j ( γj) + ∂c∗ j (γj) ∂γ⊤ j e 11 (γj − γj), e (3.1) eγj (cid:12) (cid:12) (cid:12) (cid:12) e where the derivative ∂c∗ j /∂γj is explicitly derived using the implicit function theorem in A. Hence, θj(t; γj) in its basis expansion form can be approximated by a linear function of γj. As a result, the outer objective function (2.4) now becomes a penalized likelihood function b of a generalized linear model. Second, we apply the IRLS and update our estimate of γj. Let θj(t) = θj(t; γj) be latent process ﬁt given the structural parameter γj. Based on the theory of generalized linear models, the observation Yj according to the latent process e b properties of E(Yj| µj(t) and var(Yj| θj(t)) = b θj(t)) = b θj(t)) = e ( ′′ ′ θj(t) admits vj(t)a(φ), e θj(t))a(φ) = ( e where functions a, b and parameter φ follow from the exponential family speciﬁcation. Write uij = −yij + b ′ ( e e θj(ti)) = −yij + e µj(ti) and ′′ wij = b ( e θj(ti)) = e vj(ti). The IRLS algorithm e applies a quadratic approximation to the log-likelihood, that is, at θj = e e e e e e θj, −yij θj(ti; γj) + b( θj(ti; γj)) ≈ wij yij − θj(ti; γj) 2 e + Cij, 1 2 where yij = θj(ti) − b uij/ n wij and Cij is independent of e e b o θj(ti). In conjunction with the linear b approximation of θj(ti; γj), it amounts to solving a penalized linear least squares to update e e e e e the estimate for structural parameter γj. Eﬃcient algorithms are available for diﬀerent sparsity penalty choices PEN(·). 3.3 Tuning Parameter Selection There are two tuning parameters involved in our proﬁling procedure, which jointly aﬀect the algorithmic performance. On the one hand, λθ in the inner optimization controls the amount of regularization regarding the diﬀerential equations. We deﬁne the aggregated ODE ﬁdelity criterion as p 1 ′ θ j(t) − γj0 − 0 ( j=1 Z X γjkθk(t) 2 ) dt. p k=1 X (3.2) Small λθ makes optimizing Hj(γj) with respect to γj more robust to initial guesses, but yields bad approximations to ODE solutions. Large λθ gives rise to a diﬃcult opti- mization problem where Hj(γj) is usually not convex and can have many local optima (Ramsay et al., 2007; Qi and Zhao, 2010; Carey and Ramsay, 2021). On the other hand, λγ in the outer optimization induces a sparse network structure for latent processes with better interpretation, and existing methods such as information criteria can be adopted for 12 tuning. Based on the above discussion, we propose to ﬁx λγ in the outer optimization ﬁrst, iteratively select a proper λθ in the inner optimization, and then determine the best λγ via the Bayesian information criterion. In detail, suppose we choose λγ from a sequence of candidate values. Then, we initialize λθ with a small value and moderately increase it via an iterative scheme. At each iteration, θ and Γ are repeatedly estimated for the current λθ, which are then used as initial values in the ﬁtting procedure with the next larger λθ. b b The iterative scheme stops when the estimated ODE parameters converge, and thus λθ is decided. The change of the estimated ODE parameters should be small when there is only a moderate increase in λθ. Therefore, with a conservatively increasing sequence of λθ, every estimated Γ is much likely to be a proper initialization for the next iteration. Details of the iterative selecting scheme for λθ given a ﬁxed λγ are as follows. b (1) Start with a small positive λ(0) θ . Choose ∆(0) as an initial incremental factor. (2) At the uth iteration where u ≥ 0, obtain the ﬁtted latent processes θ(u) and Γ(u) via our proﬁling procedure, and evaluate the ODE ﬁdelity (3.2) based on the estimates. b b (a) If the absolute percentage of change in the ODE ﬁdelity (3.2) is below a threshold constant, then we update λ(u+1) θ = λ(u) θ × ∆(u). (b) Otherwise, we need to downsize the incremental factor, for example, set ∆(u) = ∆(u−1)/2, which ensures that the ODE ﬁdelity (3.2) varies little among iterations. (3) When the successive ODE parameter estimates are closed enough, we stop iteration; otherwise, repeat previous steps. Our iterative tuning strategy treats λθ as a function of λγ. Hence, after λθ is selected for each ﬁxed λγ from a sequence of candidate values, we can evaluate the following BIC and choose the best λγ, BIC(λγ) = − 1 np n p yij θj(ti; λγ) − b(θj(ti; λγ)) + k(λ) log(n), i=1 X j=1 n X b o where θj(t; λγ) emphasizes the dependence on λγ, and k(λγ) denotes the number of non-zero elements in the resultant ODE parameter b Γ(λγ). b 13 3.4 Global Convergence Suppose that the estimated latent process θj(t; γj) from the inner optimization is a smooth function of γj, where j = 1, . . . , p. Let H(Γ) = p j=1 Hj(γj) be the objective function in the outer optimization for a given tuning parameter λγ, where Γ = (γ1, . . . , γp). Algorithm 1 b P is essentially a block coordinate descent method because it minimizes H(Γ) by iteratively updating γj. Write Hj(γj) = ℓj(γj) + PENλγ (γj), where ℓj(γj) is the likelihood term and PENλγ (γj) is assumed to be convex. As described in Section 3.2, the outer optimization is equivalent to updating γj to γj + dj(γj), where the descent direction dj(γj) is the solution to ∇ℓj(γj)⊤d + min d 1 2 d⊤Qj(γj)d + PENλγ (γj + d), where ∇ℓj(γj) is the gradient of ℓj(γj) and Qj(γj) = 1 n n   i=1 X b′′( θj(ti; γj)) ∂ θj(ti; γj) ∂γj b ∂ θj(ti; γj) ∂γj ! b b is a positive deﬁnite matrix approximating the Hessian ∇2ℓj(γj).  ⊤    We follow Tseng and Yun (2009) to establish the global convergence. Because the actual value of the Hessian ∇2ℓj(γj) is identical to its expected value under canonical links (McCullagh and Nelder, 1989), the IRLS method described in Section 3.2 remains the same when the Hessian is replaced by the expected Hessian. Then it follows from Lemma S1 in the Supplementary Material that Hj(γj + dj(γj)) − Hj(γj) ≤ −d⊤ j (γj) Qj(γj) − (cid:20) 1 2 E{∇2ℓj(γj)} dj(γj) + o(kdj(γj)k2). (cid:21) (3.3) Some algebra yields that Qj(γj) − 1 2 E{∇2ℓj(γj)} = 1 2 Qj(γj) + 1 2n n b′(θ∗ j (ti)) − b′( θj(ti, γj)) i=1 n X b ∂2 θj(ti, γj) ∂γj∂γ⊤ j b , o where θ∗ j (t) is the true latent process. The above matrix is positive deﬁnite because b′(·) It follows from θj(ti, γj) is suﬃciently close to the truth. is continuous, provided that (3.3) that Hj(γj) decreases along the iterations and will eventually converge because it is b lower-bounded. Moreover, the sequence of descent directions converges to zero due to (3.3). 14 According to Theorem 1(e) and Lemma 2 of Tseng and Yun (2009), every cluster point of the iterative estimates by Algorithm 1 exhibits exact zero descent direction, which implies it is indeed a stationary point of H(Γ). Finally, we remark that the above analysis cannot be directly applied to a non-convex PENλγ (·) such as the SCAD penalty. However, the non-convex penalty can be numerically approximated by local linear or quadratic functions (Fan et al., 2020). We would anticipate a similar convergence result but with more involved technical details, which is not pursued in this paper. 4 Two-step Collocation Methods for non-Gaussian Data Collocation methods have been exploited for both parameter estimation and network re- construction for various ODE models. In this section, we extend the popular two-step col- location method for high-dimensional linear ODE with non-Gaussian observations. In the large literature on collocation, Varah (1982); Ramsay et al. (2007); Dattner and Klaassen (2015), and Wu et al. (2019) consider the linear case while recently the nonparametric additive structure is investigated by Henderson and Michailidis (2014); Wu et al. (2014) and Chen et al. (2017). Most existing methods are proposed for Gaussian observations and adopt the least square loss function for estimation. In the following, we present two versions of the two-step collocation method for high-dimensional ODE models with non- Gaussian observations: the vanilla collocation based on Varah (1982) and an extension from graph reconstruction via additive diﬀerential equations (GRADE) by Chen et al. (2017). The vanilla two-step method ﬁrst ﬁts smoothing estimates θ(t) to the latent processes with maximum likelihood estimation, and then obtain the structural parameter γ with the b estimated processes and their derivatives plugged in. The procedure solves the following optimization problems, 1 with b γj = arg min γj0,γj Z 0 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) θj(t) = arg min θ∈H b − 1 n n i=1 X d θj(t) dt b p − γj0 − γjk θk(t) k=1 X b dt + PENλγ (γj), (4.1) 2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) {yijθ(ti) − b(θ(ti))}, 1 ≤ j ≤ p, (4.2) 15 where H is a proper reproducing kernel Hilbert space, and the exponential family smoothing splines can be adopted (Wahba et al., 1995; Gu, 2013; Ma et al., 2017). The performance of the vanilla two-step collocation method relies on the estimation accuracy of θj(t) and its derivatives. Although statistical convergence has been established, it is in practice hard to b tune the smoothing procedure to achieve the optimality (Liang and Wu, 2008; Brunel et al., 2014). Another extension is based on the GRADE method (Chen et al., 2017). It avoids the derivative estimation issue in the vanilla collocation method, and instead considers the ODE ﬁdelity term in its integral form. Similar to the vanilla two-step method, the GRADE method ﬁrst obtains the smoothing estimates of latent processes from observations as in (4.2). Using integrated basis functions Θj(t) = t 0 θj(t) dt, j = 1, . . . , p, one can express R θj(t) = Cj0 + γj0 t + b p b γjk Θk(t), according to the integrated diﬀerential equations. Finally, we solve the following optimiza- e k=1 X b tion problems to obtain γj = arg min Cj0,γj0,γj 1 n n yij θj(ti) − b( θj(ti)) + PENλγ (γj). (4.3) The GRADE method is initially developed for nonparametric additive ODE models and b i=1 n X e e o naturally adapts to the linear case. The use of an integrated form of ODE facilitates in- vestigating the asymptotic behavior of the estimator and enhancing its robustness to the smoothing eﬀect in the ﬁrst step (Dattner and Klaassen, 2015; Chen et al., 2017). Both the two-step collocation methods proposed in this section involve maximizing the likeli- hood function for exponential family distributions, which can be eﬃciently solved with the iteratively reweighted least squares technique as in Section 3.2. We compare the two-step collocation methods with the high-dimensional generalized proﬁling (HDGP) procedure in Section 5. For process and derivative estimation, since HDGP balances both the data and ODE ﬁdelities, it usually results in reasonable ﬁts and more accurate ODE parameter estimates due to the more accurate derivatives. For sparse structure identiﬁcation, GRADE achieves the best accuracy, which is consistent with the motivation of GRADE for network reconstruction (Chen et al., 2017). In summary, HDGP 16 is a better choice for process ﬁtting and ODE parameter estimation, while GRADE excels in sparse structure identiﬁcation. 5 Simulation Studies This section compares the empirical performance of three dynamical modeling approaches: the high-dimensional generalized proﬁling (HDGP) procedure and the two-step collocation methods proposed in Section 4, namely the GRADE and the vanilla two-step method, respectively. Consider the ODE system studied by Chen et al. (2017) which consists of eight processes in four pairs, for k = 1, . . . , 4, θ′ 2k−1(t) = 2kπ θ2k(t) θ′ 2k(t) = 2kπ θ2k−1(t)   , t ∈ [0, 1]. It is clear that the ODE solutions take the form of sine and cosine functions with vary-  ing frequencies, whereas no interaction exists across pairs. For the kth pair, the initial state is sin(yk) and cos(yk), where yk is sampled from N(0, 1). The latent processes θ(t) = (θ1(t), . . . , θ8(t))⊤ described by the above ordinary diﬀerential equations are used to generate observations from Gaussian, Poisson and Bernoulli distributions. Denote by t1, . . . , tn time points from [0, 1]. For Gaussian distribution, yij is sampled from N(θj(ti), σ2) with known variance σ2, and the sample size n for each process is set to be 100 and 500. For Poisson distribution, we draw 500 and 1000 samples from Poisson(λj(ti)) where the intensity process λj(t) = exp{θj(t)}. For Bernoulli distribution, 1500 and 2500 samples are generated with probability of success pj(t) = exp{θj(t)}/[1 + exp{θj(t)}]. Sample sizes for Poisson and Bernoulli distributions are larger than Gaussian, as in those cases more observations are generally required to ensure reasonable estimates according to the theory of generalized linear model. We use the smoothing spline ﬁtting as an initialization for the proﬁling procedure, which also corresponds to the ﬁrst stage of two-step collocation methods. The order of B-spline functions in HDGP is set as 6, and the number of knots is half of that of time points. Both HDGP and GRADE require numerical integration to evaluate ODE ﬁdelity and integrated 17 basis representations, respectively. For sparsity penalty choices, we consider the Lasso penalty PENλγ (γj) = λγkγjk1 and the SCAD penalty PENλγ (γj) = p k=1 pλγ (|γjk|), where the function pλ(·) is deﬁned on [0, ∞) as λu, P if 0 ≤ u ≤ λ −(u2 − 2aλu + λ2)/(a − 1), if λ < u < aλ (a + 1)λ2/2 if u ≥ aλ, pλ(u) =    and a suggested value for a is 3.7 according to Fan and Li (2001). Algorithmic convergence is demonstrated when the diﬀerence between successive ODE parameter estimates is small enough. It works well for two-step collocation methods. However, due to the complex interaction between inner and outer optimizations, HDGP may not yield sparse ODE pa- rameter estimates at the declaration of convergence. To address this numerical issue, we manually set ODE parameter estimates below a constant threshold as zero. Based on our empirical studies, a recommended value for the threshold is the root-mean-square of the initial estimate Γ multiplied by a factor 0.01. Simulation results are evaluated using three types of criteria. The ﬁrst two criteria b concern about process and derivative estimates, which are evaluated by the mean squared errors (MSE) of θ(t) and θ′(t), MSE( θ(t)) = b θ′(t)) = MSE( b 1 np 1 np p n j=1 X p i=1 n X n j=1 X i=1 n X b θj(ti) − θj(ti) b j(ti) − θ′ θ′ j(ti) , . 2 o 2 o Second, we measure how well the structural parameters are estimated by their root-mean- square error (RMSE). Third, true positive rate (TPR) and false positive rate (FPR) are used to quantify how well the sparse structure is identiﬁed, where we refer to non-zero structural parameters as positive cases and otherwise as negative cases. Table 1 displays the averaged evaluations over 50 repeated experiments using the Lasso penalty, while the true positive rates are omitted because they are all equal to one for all three methods. Under each simulation set-up, increasing the number of observations always leads to reduced errors and tighter conﬁdence intervals in terms of the process ﬁt and the parameter estimation. For process and derivative ﬁtting, the smoothing splines method, 18 Table 1: Performance of HDGP, GRADE and the vanilla two-step method evaluated based θ′(t))), non-zero pa- θ(t))), derivative estimates (MSE( on the process estimates (MSE( rameter estimation (RMSE), and sparse structure estimates (FPR). The 95% conﬁdence b b intervals are given in parentheses. N Method MSE ( bθ(t)) MSE ( bθ′(t)) n a i s s u a G n o s s i o P i l l u o n r e B 100 500 500 1000 1500 2500 HDGP GRADE vanilla HDGP GRADE vanilla HDGP GRADE vanilla HDGP GRADE vanilla HDGP GRADE vanilla HDGP GRADE vanilla 0.011 (0.0097,0.0124) 0.005 (0.0042,0.0049) 0.005 (0.0043,0.0049) 0.002 (0.0017,0.0023) 0.001 (0.0010,0.0011) 0.001 (0.0010,0.0011) 0.024 (0.0222,0.0259) 0.024 (0.0232,0.0252) 0.024 (0.0232,0.0252) 0.011 (0.0105,0.0121) 0.013 (0.0128,0.0141) 0.013 (0.0127,0.0141) 0.031 (0.0260,0.0357) 0.031 (0.0277,0.0333) 0.032 (0.0285,0.0376) 0.019 (0.0169,0.0216) 0.020 (0.0193,0.0210) 0.020 (0.0193,0.0211) 3.01 (2.48, 3.53) 5.23 (4.67, 5.87) 5.23 (4.74, 5.93) 0.48 (0.41, 0.57) 1.82 (1.75, 1.88) 1.82 (1.75, 1.88) 6.24 (5.60, 6.93) 12.27 (11.66,13.06) 12.27 (11.63,13.00) 2.70 (2.43, 2.97) 8.20 (7.70, 8.73) 8.20 (7.71, 8.67) 8.18 (6.74, 9.73) 15.97 (14.77,16.99) 22.28 (17.21,32.11) 5.05 (4.33, 6.03) 12.71 (11.91,13.67) 12.71 (11.88,13.69) 19 RMSE ( b Γ) 0.58 (0.52,0.66) 2.97 (2.89,3.05) 0.62 (0.54,0.72) 0.28 (0.25,0.30) 0.84 (0.82,0.85) 0.34 (0.32,0.37) 1.70 (1.54,1.91) 2.03 (1.86,2.19) 1.86 (1.70,2.07) 1.04 (0.94,1.14) 1.41 (1.31,1.53) 1.18 (1.09,1.29) 1.77 (1.48,1.97) 3.32 (3.01,3.61) 2.28 (1.70,3.22) 1.55 (1.39,1.74) 2.57 (2.34,2.79) 1.67 (1.46,1.87) FPR 0.44 (0.43,0.46) 0.00 (-,-) 0.89 (0.84,0.92) 0.44 (0.42,0.45) 0.01 (0.01,0.02) 0.67 (0.64,0.71) 0.58 (0.56,0.61) 0.37 (0.34,0.41) 0.98 (0.97,0.98) 0.57 (0.55,0.59) 0.32 (0.28,0.36) 0.97 (0.96,0.97) 0.54 (0.52,0.58) 0.24 (0.21,0.28) 0.94 (0.90,0.96) 0.59 (0.55,0.62) 0.20 (0.16,0.24) 0.98 (0.97,0.99) as the ﬁrst stage of two-step collocation methods, often produces accurate estimates of the latent process itself, but is less eﬃcient in the derivative ﬁtting. In contrast, the inner optimization of HDGP balances the data and ODE ﬁdelities, resulting in reasonable process ﬁtting and improved derivative ﬁtting. For ODE parameter estimation, HDGP delivers the smallest error due to the more accurate derivatives. Interestingly, GRADE has much worse performance than the other two under this criterion. One partial reason is that GRADE only uses structural parameters in the integrated basis representation (4.3) instead of the explicit form of diﬀerential equations. For sparse structure identiﬁcation, GRADE achieves the best accuracy, as it discovers all non-zero structural parameters with the fewest false positives. It is consistent with the motivation of GRADE for network reconstruction (Chen et al., 2017). In summary, HDGP is a better choice for process ﬁtting and ODE parameter estimation, while GRADE excels in sparse structure identiﬁcation. We next investigate the eﬀects of diﬀerent noise levels and choices of sparse penalty. Under the above Gaussian set-up with 500 observations for each process. The signal- to-noise ratio (SNR) is deﬁned as the ratio between the sample standard deviation of {θj(ti)}n i=1 and the noise standard deviation σ. We set the signal-to-noise ratio as 3, 10, 30, and inﬁnity, where the inﬁnite ratio means that no noise is added. Both Lasso and SCAD penalties are considered. Figure 1 presents the performance evaluations over 50 repeated experiments. In general, all methods perform better over all criteria when the signal-to- noise ratio increases. The top row of Figure 1 corresponding to the Lasso penalty provides the consistent result as in Table 1, which indicates that HDGP has a comparable process ﬁt and better derivative estimation, especially when the noise level is low. Moreover, HDGP performs the best for estimating structural parameters, while the vanilla two-step method also provides satisfactory results. In contrast, even when there is no noise, the bias of ODE parameter estimates by GRADE is still large and RMSE is almost constant. For sparse structure identiﬁcation, GRADE outperforms the other methods under a wide range of noise levels. HDGP and the vanilla two-step method only have high accuracy when the signal level is high. The bottom row of Figure 1 displays simulation results when the SCAD penalty is used for inducing sparsity for the ODE system. Compared with the results with Lasso, overall performances in process, derivative, and ODE parameter estimations are 20 Processes Derivatives Non-zero Parameters FPR 0.012 0.009 0.006 E S M 0.003 0.000 0.0015 0.0010 E S M 0.0005 0.0000 3 10 30 Inf SNR Processes E S M E S M 4 3 2 1 0 3 2 1 0 E S M R 2.0 1.5 1.0 0.5 0.0 0.75 0.50 0.25 0.00 3 10 30 Inf 3 10 30 Inf 3 10 30 Inf SNR SNR SNR (a) With Lasso penalty. Derivatives Non-zero Parameters FPR E S M R 0.5 0.4 0.3 0.2 0.1 0.0 0.8 0.6 0.4 0.2 0.0 3 10 30 Inf 3 10 30 Inf 3 10 30 Inf 3 10 30 Inf SNR SNR SNR SNR (b) With SCAD penalty. Figure 1: Performance of HDGP (purple solid), GRADE (blue dashed), and the vanilla two-step method (yellow dotted) for Gaussian observations at diﬀerent noise levels. The boxes identify the medians and the quartiles of each criterion for 50 repeated experiments. Top and bottom rows correspond to Lasso and SCAD penalties, respectively. 21 improved mainly due to the unbiasedness property of SCAD penalty (Fan and Li, 2001). More interestingly, the poor performance of GRADE in ODE parameter estimation is greatly enhanced, and now it delivers comparable estimation results as the other two. Due to the oracle property enjoyed by the SCAD penalty (Fan and Li, 2001), we recommend it for better performance in parameter estimation. 6 Real Data Analysis 6.1 Google Trends Data Analysis Google Trends provides a publicly accessible online portal to analyze the popularity of search queries. In this study, we attempt to apply our method to model the interactions among a number of trending keywords during the recent pandemic of Coronavirus disease 2019 (COVID-19). In Table 2, we list 24 keywords and cluster them into three categories. The ﬁrst category consists of ﬁve keywords about speciﬁc terminologies such as mask and quarantine. The second category includes not only the countries with the most conﬁrmed cases as of January 2021, such as the United States, India, and Brazil but also the districts like Antarctica, which is the last continent to report conﬁrmed cases due to the remoteness and sparse population. We also include the last category of noise keywords with no apparent relationship to the pandemic. Table 2: Three categories of keywords selected for the analysis of Google Trends data. Category Keyword COVID-19 related coronavirus, mask, quarantine, vaccine, WHO (5 words) Countries or districts Africa, Antarctica, Arctic, Australia, Brazil, Canada, China, India, Iran, Italy, Japan, Russia, the United States (13 words) Noise words cat, cloud, desert, dog, game, sun (6 words) The Google Trends data used in our study cover the range from January 20 to Septem- ber 20 in 2020. The keyword popularity is measured by an integer index calculated by 22 normalizing and rounding the keyword count in an unbiased searching requests sample. We observe that the daily trend indices have several sharp peaks, see Figure 2 for an illus- trative example. Direct modeling for the mean trends will result in abrupt high values near the peaks and undersmooth other relatively ﬂat regions. Therefore, it is more appropriate to assume the indices follow Poisson distributions, and we apply the proposed method to model the latent processes of intensity parameters with ODEs. Figure 2: Daily Google Trends indices of keyword Russia and vaccine from January to September 2020. To better exhibit diﬀerent stages of the pandemic, we consider three time periods: from January 20 to March 19, from March 20 to June 19, and from June 20 to September 20. For each period, our method is applied to ﬁt the trending processes with a series of sparsity parameter λγ’s. Figures 3a and 3b display two networks with diﬀerent sparsity parameters in the ﬁrst period (from January to March). Keyword quarantine has the highest degree in both networks. During the COVID-19 pandemic, quarantines or self-quarantines are enacted by multiple governmental actors to prevent the rapid spread of the virus. It is of no surprise to become the top-ranked trending keyword. The other three keywords in Figure 3a are coronavirus, China and vaccine, which stand for the virus’s name, the country where the ﬁrst case was identiﬁed, and the immunization method. The top four keywords represent the major trending focus at the early stage of the pandemic. In Figure 23 Desert Russia cat dog WHO India the United States Iran mask Arctic cloud Italy cat game Brazil Japan sun Brazil cloud India Africa Antarctica game vaccine Coronavirus Canada Arctic Antarctica Italy Australia Africa quarantine Desert Coronavirus China quarantine WHO dog Australia sun China the United States Russia vaccine Japan Canada (a) λγ = 10−0.5 Iran mask (b) λγ = 10−1 Figure 3: Recovered networks of the trending keywords during the ﬁrst period (from Jan- uary 20 to March 19) with diﬀerent values of sparsity parameters. 3b, more aﬀected countries such as Australia, Italy, and the United States, are involved when the sparsity parameter is decreased. In contrast, noise keywords are isolated in both networks, indicating no connection to the trending topics. More interestingly, we investigate Table 3: Top four keywords in the recovered networks during three periods. The keyword of the highest degree is in boldface. Period Keywords January 20 – March 19 quarantine, China, coronavirus, vaccine March 20 – June 19 Italy, China, Iran, Russia June 20 – September 20 coronavirus, the United States, vaccine, mask the evolution of network structure for the trending keywords along the progression of the COVID-19 pandemic. Table 3 lists the top four keywords in three time periods where the keyword with the highest degree is in boldface. From the ﬁrst period to the second, the keyword Italy emerges as the new top word. According to the WHO report, on March 19, Italy overtook China as the country with the most reported deaths, and announced 24 Table 4: Companies selected in eight categories for stock price data analysis. Group Category Companies 1 2 3 4 5 6 7 8 Information Technology Adobe, Apple, Microsoft, Salesforce, Zoom Electric Vehicle Pharmaceutical BYD, Kandi, Nio, Tesla, Workhorse AbbVie, Eli lilly, Moderna, Novartis, Pﬁzer Consumer Services & Retail Ascena, J. C. Penney, Kohl’s, Macy’s, Nordstrom Online Retail Shopping Amazon, Best Buy, Target, Walmart, Wayfair Hotels Hilton, Marriott, Wyndham, Wynn, Park Air Transportation Boeing, Airbus, Delta Air Lines, Southwest Airlines, United Airline Energy Chevron, Conocophillips, Exxon Mobil, Schlumberger, Valero Energy the national lockdown in March. Turning to the third period, China and Italy drop out of the top list. Both countries had successfully slowed down the domestic infections and reduced daily new cases signiﬁcantly. As preventive measures including wearing face masks in public are advised and several promising vaccines are being developed, mask and vaccine are among the top trending keywords. 6.2 Analysis of Stock Price Change Directions In the year 2020, the stock market experienced enormous volatility due to the coronavirus pandemic. Many companies have suﬀered massive price drops, while others have witnessed substantial increases. We collect the stock price indices for 40 companies during 251 trad- ing days spanning from January 1 to December 30, 2020. Our goal in this study is to characterize the change direction patterns of stock prices, taking into account the dynamic interactions among the stocks. To this end, the original price indices are coded as binary data to denote an increase or decrease. We group the companies into eight categories based on the Global Industry Classiﬁcation Standard. Details are provided in Table 4. The high-dimensional ODE system is built up for the latent success probability pro- 25 (cid:9)(cid:10)(cid:11)(cid:12)(cid:13) (cid:4)(cid:5)(cid:6)(cid:7)(cid:8) 0.550 0(cid:0)(cid:1)(cid:2)(cid:3) 0.500 0.54 (cid:129)(cid:130)(cid:131)(cid:132) 0.50 Information Technology Electric Vehicle Consumer Services & Retail Pharmaceutical +,-./ 0.550 &’()* 0.500 !""#$% 0.55 0.50 0.45 0.40 0.550 fghij 0.500 abcde 0.450 \]^_‘ (cid:14)(cid:15)(cid:16)(cid:17)(cid:18) (cid:19)(cid:20)(cid:21)(cid:22)(cid:23) (cid:24)(cid:25)(cid:26)(cid:27)(cid:28) 1(cid:29)(cid:30)(cid:31) 23456 789:; <=>?@ ABCDE GHIJK LMNOP QRSTU VWXYZ klmno qrstu vwxyz {|}~(cid:127) Online Retail Shopping Hotels Air Transportation Energy 0.51 0.48 0.45 0.50 0.48 ‡·(cid:181)¶ 0.44 ﬂ(cid:176)–† 0.50 0.45 0.40 0.35 (cid:133)(cid:134)(cid:135)(cid:136)(cid:137) (cid:138)(cid:139)(cid:140)(cid:141)(cid:142) (cid:143)(cid:144)(cid:145)(cid:146)(cid:147) (cid:148)(cid:149)(cid:150)(cid:151)(cid:152) (cid:154)(cid:155)(cid:156)(cid:157)(cid:158) (cid:159)(cid:160)¡¢£ ⁄¥ƒ§¤ '“«‹› •‚„”» …‰(cid:190)¿(cid:192) `´ˆ˜¯ ˘˙¨(cid:201)˚ (cid:204)˝˛ˇ— (cid:209)(cid:210)(cid:211)(cid:212)(cid:213) (cid:214)(cid:215)(cid:216)(cid:217)(cid:218) (cid:219)(cid:220)(cid:221)(cid:222)(cid:223) Figure 4: The ﬁtted probability processes of daily price increase for the eight categories. The red dashed line denotes p = 0.5. cesses. Our sparsity tuning procedure leads to λγ = 10−2.1 and the ﬁtted model achieves an ODE ﬁdelity below 10−6. Figure 4 displays the ﬁtted probabilities of a daily stock price increase for all categories. We notice some interesting results from the result. First, all categories have the low ﬁtted probabilities around March. It corresponds to the 2020 stock market crash, during which multiple circuit breakers were triggered on fears of the COVID-19 coronavirus. Since the crash, some sectors recovered and re-entered a bull market through December. Online retail companies made huge proﬁts as health concerns changed customers’ shopping habits. Information technology companies beneﬁted from the growing demands for information services and electronics devices. For example, the shifts towards remote working had raised the number of Zoom’s daily users to an unprecedented one. In contrast, sectors like energy, hotels, and air transportation experienced the most severe hit by the COVID-19 pandemic. Although there were signs of recovery in the fourth quarter, these industries are still under the tremendous impact of the COVID-19 recession. 6.3 Analysis of Yeast Cell Cycle-regulated Genes The cell cycle is a fundamental biological process consisting of cell growth, duplication of genetic information, distribution of chromosomes, and cell division (Cho et al., 1998). Spellman et al. (1998) analyzed the expression levels of 6,178 yeast genes at 7-minute in- tervals for 119 minutes. The experiments were carried out in the cell cultures with three independent synchronization methods. A score was calculated for each gene to indicate 26 p F [ (cid:128) (cid:153) ﬁ ¸ (cid:224) 14 56 72 71 48 52 41 38 6 60 45 59 24 3 17 2 31 32 33 63 51 21 9 28 42 47 30 13 54 70 39 10 16 1 12 8 61 62 66 5 20 65 68 4 27 7 26 37 35 69 57 40 58 11 55 46 53 49 36 15 Figure 5: The recovered network of the yeast cell cycle. Yellow nodes represent genes, and the green-solid or red-dashed edges indicate potential promotion or suppression eﬀects. their similarities to those cell-cycle regulated genes already known. Due to missingness in data, we choose 72 out of 93 genes identiﬁed by Spellman et al. (1998) in the alpha factor-based synchronized experiment, and model the dynamic relationship between the mean proﬁles of these 72 genes using an ODE system under Gaussian assumption for gene expression level. The proposed method is applied to identify the sparse structure of the gene regulatory network. The result is shown in Figure 5, which excludes 12 isolated genes. This suggests that although those genes get involved in the cell cycle, their regulated tran- scriptions are not absolutely required. Among the 60 genes in Figure 5, 116 regulations (i.e., directed edges) are discovered. The average number of regulations for each gene is around three, while more than 80% genes have regulations fewer than ﬁve. Genes with high network degrees are identiﬁed as central hub nodes. For example, CLN3 (node 1) has the largest number of regulations in Figure 5. According to the Saccharomyces Genome Database (Cherry et al., 1998), the encoded protein CLN3p is known as a cell cycle regu- 27 lator and promotes the G1/S transition (Nasmyth, 1993). More interestingly, the positive or negative signs of our estimated structural parameters naturally imply the potential pro- motion or inhibition between genes, respectively. Our result suggests that CHS1 (node 62) promotes the expression of POL30 (node 12), which regulates DNA replication in the G1 phase. Meanwhile, it suppresses the expression of FAR1 (node 30), which is a CDC28p kinase inhibitor functioning in the G2/M transition. 7 Conclusions and Discussion In this article, we have proposed a new proﬁling procedure for both parameter estima- tion and sparse structure identiﬁcation for high-dimensional linear ODE models with non- Gaussian observations. Our method involves a hierarchical optimization scheme: the inner optimization balances the data ﬁtting and ODE ﬁdelity to improve estimation eﬃciency, while the outer optimization induces a sparse structure for better model interpretation. Besides, we extend two-step collocation methods to the non-Gaussian observation setting and compare them with the proposed proﬁling procedure via comprehensive studies. One limitation of our work is that only the linear ODE system is under consideration. We are aware of the recent development of two-step collocation to sparse additive ODE systems (Henderson and Michailidis, 2014; Wu et al., 2014; Chen et al., 2017) and a more general functional ANOVA extension (Dai and Li, 2021). Although our hierarchical opti- mization is not restricted to the linear ODE, the extension to nonlinear ODE systems is not straightforward. For instance, a common strategy to handle additive ODE models is to expand the nonlinear components with basis function. However, due to the proﬁling nature, the range of collocation bases for latent processes needs to be controlled within a compact interval, which may not be easily overcome. Another future research is on the statistical properties such as uniform bound on the approximations to the true solutions, asymptotic normality of the estimators. Despite existing theory established for the stan- dard generalized proﬁling (Qi and Zhao, 2010), it is still a challenging problem due to high dimensionality, and we leave the systematic study to future work. 28 A Derivatives We provide the analytical expressions of the derivatives used in the computation (Section 3). Derivatives of Gj in inner optimization Write Gj(θj; γj) in the inner optimization Gj = − 1 n n i=1 X {yijθj(ti) − b(θj(ti))} + λθ θ′ j(t) − γj0 − ZT ( γjk θk(t) 2 ) dt, p k=1 X then the ﬁrst derivative is ∂Gj ∂cj = − 1 n n {yijh(ti) − b′(θj(ti))h(ti)} i=1 X + 2λθ dθj(t) dt ZT (cid:26) p − γj0 − γjk θk(t) k=1 X dh(t) dt (cid:27)(cid:26) − γjj h(t) dt, (cid:27) and the second derivative is ∂2Gj ∂cj∂c⊤ j = 1 n n i=1 X {b′′(θj(ti))h(ti)h(ti)⊤} + 2λθ For k = 0, dh(t) dt ZT (cid:26) − γjjh(t) dh(t) dt (cid:27)(cid:26) ⊤ − γjjh(t) dt. (cid:27) ∂2Gj ∂cj∂γj0 = −2λθ dh(t) dt ZT (cid:26) − γjjh(t) dt, (cid:27) for k = 1, . . . , p and k 6= j , ∂2Gj ∂cj∂γjk = −2λθ dh(t) dt ZT (cid:26) − γjjh(t) θk(t) dt, (cid:27) for k = j, ∂2Gj ∂cj∂γjj = −2λθ ZT (cid:26) dh(t) dt − γjjh(t) θj(t) dt − 2λθ ZT (cid:26) dθj(t) dt (cid:27) p − γj0 − γjkθk(t) h(t) dt. k=1 X (cid:27) 29 Derivative of c∗ j in outer optimization Write c∗ j (γj) as c∗ j for simplicity. Since Gj has zero-gradient at c∗ j , then Taking the derivative with respect to γj on both sides, we have = 0. ∂Gj ∂cj (cid:12) c∗ j (cid:12) (cid:12) (cid:12) j (cid:18) Suppose ∂2Gj/(∂cj∂c⊤ d dγ⊤ = ∂2Gj ∂Gj ∂cj∂c⊤ ∂cj (cid:12) j ! c∗ c∗ j (cid:12) j (cid:19) (cid:12) (cid:12) (cid:12) (cid:12) j )|c∗ j is non-singular, we have the following expression of the deriva- (cid:12) (cid:12) ∂2Gj ∂cj∂γ⊤ j (cid:12) (cid:12) (cid:12) (cid:12) j (γj) ∂γ⊤ j = 0. + c∗ j ∂c∗ tive ∂c∗ ∂2Gj ∂cj∂c⊤ j (cid:12) (cid:12) Both matrices on the right-hand side have been explicitly derived, the the derivative of c∗ (cid:12) j (cid:12) ∂2Gj . ∂cj∂γ⊤ c∗ j (cid:12) j (cid:19) (cid:12) (cid:12) (cid:12) j (γj) ∂γ⊤ j c∗ j (cid:19) = − (cid:18) (cid:18) −1 follows. SUPPLEMENTARY MATERIALS Supplementary Material contains additional numerical results. References Brunel, N. J., Q. Clairon, and F. d’Alch´e Buc (2014). Parametric estimation of ordinary diﬀerential equations with orthogonality conditions. Journal of the American Statistical Association 109 (505), 173–185. Cao, J. and J. O. Ramsay (2007). Parameter cascades and proﬁling in functional data analysis. Computational Statistics 22 (3), 335–351. Carey, M. and J. O. Ramsay (2021). Fast stable parameter estimation for linear dynamical systems. Computational Statistics & Data Analysis 156, 107124. Chen, S., A. Shojaie, and D. M. Witten (2017). Network reconstruction from high- dimensional ordinary diﬀerential equations. Journal of the American Statistical Associ- ation 112 (520), 1697–1707. 30 Cherry, J. M., C. Adler, C. Ball, S. A. Chervitz, S. S. Dwight, E. T. Hester, Y. Jia, G. Juvik, T. Roe, and M. Schroeder (1998). Sgd: Saccharomyces genome database. Nucleic Acids Research 26 (1), 73–79. Cho, R. J., M. J. Campbell, E. A. Winzeler, L. Steinmetz, A. Conway, L. Wodicka, T. G. Wolfsberg, A. E. Gabrielian, D. Landsman, and D. J. Lockhart (1998). A genome-wide transcriptional analysis of the mitotic cell cycle. Molecular cell 2 (1), 65–73. Cokus, S. J., S. Feng, X. Zhang, Z. Chen, B. Merriman, C. D. Haudenschild, S. Pradhan, S. F. Nelson, M. Pellegrini, and S. E. Jacobsen (2008). Shotgun bisulphite sequencing of the arabidopsis genome reveals dna methylation patterning. Nature 452 (7184), 215–219. Dai, X. and L. Li (2021). Kernel ordinary diﬀerential equations. Journal of the American Statistical Association. Dattner, I. and C. A. Klaassen (2015). Optimal rate of direct estimators in systems of ordinary diﬀerential equations linear in functions of the parameters. Electronic Journal of Statistics 9 (2), 1939–1973. Dodds, P. S., K. D. Harris, I. M. Kloumann, C. A. Bliss, and C. M. Danforth (2011). Tem- poral patterns of happiness and information in a global social network: Hedonometrics and twitter. PLoS ONE 6 (12), e26752. Fan, J., Y. Feng, and Y. Wu (2009, 06). Network exploration via the adaptive lasso and scad penalties. The Annals of Applied Statistics 3 (2), 521–541. Fan, J. and R. Li (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American statistical Association 96 (456), 1348–1360. Fan, J., R. Li, C.-H. Zhang, and H. Zou (2020). Statistical foundations of data science. Chapman and Hall/CRC. Gu, C. (2013). Smoothing Spline ANOVA Models (2nd ed.), Volume 297. New York: Springer. 31 Hall, P. and Y. Ma (2014). Quick and easy one-step parameter estimation in diﬀeren- tial equations. Journal of the Royal Statistical Society: Series B (Statistical Methodol- ogy) 76 (4), 735–748. Hecker, M., S. Lambeck, S. Toepfer, E. Van Someren, and R. Guthke (2009). Gene regula- tory network inference: data integration in dynamic models—a review. Biosystems 96 (1), 86–103. Henderson, J. and G. Michailidis (2014). Network reconstruction using nonparametric additive ODE models. PLoS ONE 9 (4), e94003. Huang, W., Y. Nakamori, and S.-Y. Wang (2005). Forecasting stock market movement direction with support vector machine. Computers & operations research 32 (10), 2513– 2522. Liang, H. and H. Wu (2008). Parameter estimation for diﬀerential equation models using a framework of measurement error in regression models. Journal of the American Statistical Association 103 (484), 1570–1583. Lu, T., H. Liang, H. Li, and H. Wu (2011). High-dimensional odes coupled with mixed- eﬀects modeling techniques for dynamic gene regulatory network identiﬁcation. Journal of the American Statistical Association 106 (496), 1242–1258. Ma, P., N. Zhang, J. Z. Huang, and W. Zhong (2017). Adaptive basis selection for exponen- tial family smoothing splines with application in joint modeling of multiple sequencing samples. Statistica Sinica 27 (4), 1757–1777. McCullagh, P. and J. Nelder (1989). Generalized Linear Models, Second Edition. Chapman & Hall/CRC Monographs on Statistics & Applied Probability. Taylor & Francis. Miao, H., H. Wu, and H. Xue (2014). Generalized ordinary diﬀerential equation models. Journal of the American Statistical Association 109 (508), 1672–1682. Nagalakshmi, U., Z. Wang, K. Waern, C. Shou, D. Raha, M. Gerstein, and M. Snyder (2008). The transcriptional landscape of the yeast genome deﬁned by RNA sequencing. Science 320 (5881), 1344–1349. 32 Nasmyth, K. (1993). Control of the yeast cell cycle by the Cdc28 protein kinase. Current Opinion in Cell Biology 5 (2), 166–179. Polynikis, A., S. Hogan, and M. di Bernardo (2009). Comparing diﬀerent ODE modelling approaches for gene regulatory networks. Journal of Theoretical Biology 261 (4), 511–530. Powell, M. J. (2006). The NEWUOA software for unconstrained optimization without derivatives. In Large-scale nonlinear optimization, pp. 255–297. Springer. Poyton, A., M. S. Varziri, K. B. McAuley, P. J. McLellan, and J. O. Ramsay (2006). Parameter estimation in continuous-time dynamic models using principal diﬀerential analysis. Computers & Chemical Engineering 30 (4), 698–708. Qi, X. and H. Zhao (2010). Asymptotic eﬃciency and ﬁnite-sample properties of the generalized proﬁling estimation of parameters in ordinary diﬀerential equations. The Annals of Statistics 38 (1), 435–481. Ramsay, J. O. (1996). Principal diﬀerential analysis: Data reduction by diﬀerential opera- tors. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 58 (3), 495–508. Ramsay, J. O., G. Hooker, D. Campbell, and J. Cao (2007). Parameter estimation for diﬀerential equations: a generalized smoothing approach. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 69 (5), 741–796. Sloan, L. and J. Morgan (2015). Who tweets with their location? understanding the rela- tionship between demographic characteristics and the use of geoservices and geotagging on twitter. PLoS ONE 10 (11), e0142209. Spellman, P. T., G. Sherlock, M. Q. Zhang, V. R. Iyer, K. Anders, M. B. Eisen, P. O. Brown, D. Botstein, and B. Futcher (1998). Comprehensive identiﬁcation of cell cycle-regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization. Molecular Biology of the Cell 9 (12), 3273–3297. Stuart, J. M., E. Segal, D. Koller, and S. K. Kim (2003). A gene-coexpression network for global discovery of conserved genetic modules. Science 302 (5643), 249–255. 33 Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 58 (1), 267–288. Tseng, P. and S. Yun (2009). A coordinate gradient descent method for nonsmooth sepa- rable minimization. Mathematical Programming 117 (1), 387–423. Varah, J. M. (1982). A spline least squares method for numerical parameter estimation in diﬀerential equations. SIAM Journal on Scientiﬁc and Statistical Computing 3 (1), 28–46. Voorman, A., A. Shojaie, and D. Witten (2014). Graph estimation with joint additive models. Biometrika 101 (1), 85–101. Wahba, G., Y. Wang, C. Gu, R. Klein, and B. Klein (1995, 12). Smoothing spline ANOVA for exponential families, with application to the Wisconsin epidemiological study of dia- betic retinopathy : the 1994 Neyman memorial lecture. The Annals of Statistics 23 (6), 1865–1895. Wang, H. and C. Leng (2008). A note on adaptive group lasso. Computational Statistics & Data Analysis 52 (12), 5277–5286. Wood, S. N. (2017). Generalized additive models: an introduction with R. CRC press. Wu, H., T. Lu, H. Xue, and H. Liang (2014). Sparse additive ordinary diﬀerential equations for dynamic gene regulatory network modeling. Journal of the American Statistical Association 109 (506), 700–716. Wu, L., X. Qiu, Y.-x. Yuan, and H. Wu (2019). Parameter estimation and variable selec- tion for big systems of linear ordinary diﬀerential equations: a matrix-based approach. Journal of the American Statistical Association 114 (526), 657–667. Yuan, M. and C. Kendziorski (2006). Hidden Markov models for microarray time course data in multiple biological conditions. Journal of the American Statistical Associa- tion 101 (476), 1323–1332. 34 Yuan, M. and Y. Lin (2006). Model selection and estimation in regression with grouped vari- ables. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68 (1), 49–67. Yuan, M. and Y. Lin (2007). Model selection and estimation in the Gaussian graphical model. Biometrika 94 (1), 19–35. Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics 38 (2), 894–942. Zhang, H., A. R. Conn, and K. Scheinberg (2010). A derivative-free algorithm for least- squares minimization. SIAM Journal on Optimization 20 (6), 3555–3576. Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical Association 101 (476), 1418–1429. 35",0.0
"Discovering Policies with DOMiNO: Diversity Optimization Maintaining
  Near Optimality","[{'href': 'http://arxiv.org/abs/2205.13521v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.13521v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-05-26 17:40:52,,"The CLRS Algorithmic Reasoning Benchmark Petar Veliˇckovi´c 1 Adri`a Puigdom`enech Badia 1 David Budden 1 Razvan Pascanu 1 Andrea Banino 1 Misha Dashevskiy 1 Raia Hadsell 1 Charles Blundell 1 2 2 0 2 n u J 4 ] G L . s c [ 2 v 9 5 6 5 1 . 5 0 2 2 : v i X r a Abstract Learning representations of algorithms is an emerging area of machine learning, seeking to bridge concepts from neural networks with clas- sical algorithms. Several important works have investigated whether neural networks can effec- tively reason like algorithms, typically by learning to execute them. The common trend in the area, however, is to generate targeted kinds of algorith- mic data to evaluate speciﬁc hypotheses, making results hard to transfer across publications, and increasing the barrier of entry. To consolidate progress and work towards uniﬁed evaluation, we propose the CLRS Algorithmic Reasoning Bench- mark, covering classical algorithms from the In- troduction to Algorithms textbook. Our bench- mark spans a variety of algorithmic reasoning procedures, including sorting, searching, dynamic programming, graph algorithms, string algorithms and geometric algorithms. We perform extensive experiments to demonstrate how several popular algorithmic reasoning baselines perform on these tasks, and consequently, highlight links to several open challenges. Our library is readily available at https://github.com/deepmind/clrs. 1. Introduction Neural networks and classical algorithms are two techniques that operate on diametrically opposite (and complementary) sides of problem-solving: neural networks can adapt and generalise to raw inputs, automatically extracting appro- priate features and a single neural network setup is often applicable to many separate tasks (Zamir et al., 2018). How- ever, they are hard to interpret, notoriously unreliable when extrapolating outside of the dataset they have been trained on, and rely on massive quantities of training data. On 1DeepMind. Correspondence to: Petar Veliˇckovi´c <petarv@deepmind.com>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). the other hand, algorithms trivially strongly generalise to inputs of arbitrary sizes, and can be veriﬁed or proven to be correct, with interpretable step-wise operations. Their shortcoming is that inputs must be made to conform to a par- ticular algorithm speciﬁcation, and looking at a separate task often requires coming up with an entirely new algorithm (Veliˇckovi´c & Blundell, 2021). Bringing the two sides closer together can therefore yield the kinds of improvements to performance, generalisation and interpretability that are unlikely to occur through archi- tectural gains alone. Accordingly, algorithmic modelling as a domain for testing neural networks has been gaining popularity over the last few years (Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015; Trask et al., 2018; Vinyals et al., 2015; Kool et al., 2018; Freivalds et al., 2019; Dwivedi et al., 2020; Chen et al., 2020; Tang et al., 2020; Veliˇckovi´c et al., 2019; Yan et al., 2020; Deac et al., 2020) due to its ability to highlight various reasoning limitations of existing architectures. Earlier work (Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015) focused on the need of long-term mem- ory capabilities when executing algorithms, which offered a good test-bed for various recurrent and memory architec- tures. Recently, algorithmic tasks have been used to high- light the efﬁciency of graph neural networks (Dwivedi et al., 2020; Chen et al., 2020; Veliˇckovi´c et al., 2019; Yan et al., 2020; Corso et al., 2020; Tang et al., 2020; Georgiev & Li´o, 2020; Veliˇckovi´c et al., 2020) and to distinguish between different variations of them, typically through the lens of algorithmic alignment—architectures that align better with the underlying algorithm can be proven to have better sam- ple complexity (Xu et al., 2019). Unfortunately, many of these works remain disconnected in terms of the algorithms they target, how the data is presented to the model or through the training and testing protocols they use, making direct comparison somewhat difﬁcult. To make a ﬁrst step towards a uniﬁed benchmark for al- gorithmic reasoning tasks, we propose a comprehensive dataset which we will refer to as The CLRS Algorithmic Reasoning Benchmark, in homage to the Introduction to Al- gorithms textbook by Cormen, Leiserson, Rivest and Stein (Cormen et al., 2009). The CLRS Algorithmic Reasoning Benchmark Within this benchmark, we propose and evaluate on CLRS- 30: a dataset containing trajectories—a trajectory is formed of inputs, the corresponding outputs and optional interme- diary targets—of 30 classical algorithms covering various forms of reasoning, including sorting, searching, dynamic programming, geometry, graphs and strings. Some of these algorithms are depicted in Figure 1. The appeal and moti- vation for such a benchmark goes beyond unifying or pro- viding a common ground for previous works, as we will describe. We believe that CLRS-30 is well positioned to ex- plore out-of-distribution (OOD) generalization and transfer (as potentially part of a meta-learning setting) given the ex- plicit and known relationship between different algorithms (e.g. what subroutines are shared and so forth). 2. Motivation Figure 1. Example of four algorithms within CLRS-30. A) in- sertion sort; B) string matching; C) greedy task scheduling; D) shortest paths. Timely posed benchmarks have led to a signiﬁcant progress in the ﬁeld, from the impact of ImageNet (Russakovsky et al., 2015) on the vision community, to that of Wikipedia and Penn Treebank in popularizing neural networks for lan- guage modelling (Merity et al., 2016; Mikolov et al., 2011) or Atari-2600 for deep reinforcement learning (Bellemare et al., 2013). The prevalence of recent works focusing on al- gorithmic reasoning1, as well as a history of disparate work on a variety of bespoke benchmarks (Graves et al., 2014; Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015; Trask et al., 2018), suggests signiﬁcant utility in a bench- mark covering a wide-range of classical CS algorithms. Learning to mimic an algorithm also provides an opportu- nity to extensively test the limitations of architectures both in terms of their representation capacity and processing. This can then be related back directly onto underlying oper- ations and qualities of the well-studied CS algorithms being mimicked as we are aware of both the process used to gen- erate the inputs and the speciﬁcs of the underlying function 1Concurrent works published at the same venue include: (Xu et al., 2019; Veliˇckovi´c et al., 2019) at ICLR’20 and (Veliˇckovi´c et al., 2020; Corso et al., 2020; Tang et al., 2020) at NeurIPS’20. producing the corresponding outputs. Hence, benchmarking in this area can be used to better understand the limitations of current architectures and the optimisation schemes used. This benchmarking can come in many forms: Data can be easily generated, allowing the neural network behaviour to be probed under different regimes: from few- shot learning all the way to inﬁnite-data. Algorithms can be used to understand the efﬁciency of dif- ferent inductive biases and neural components. For example, a recent study (Tang et al., 2020) has demonstrated the direct beneﬁts of choosing inductive biases that align well with iterative algorithms. Algorithms have also been used to high- light the importance of attention mechanisms (Graves et al., 2014) or to disambiguate various message passing mecha- nisms for graph neural networks (Richter & Wattenhofer, 2020; Joshi et al., 2020; Veliˇckovi´c et al., 2019). Algorithms can require repeated computation, recursion, or performing very different forms of computations con- ditioned on the input, providing an excellent test-bed for evaluating compositionality; i.e. whether an algorithm ex- ecutor can effectively exploit these repeated computations. One can control the amount of memory required to solve a problem instance, hence test the memorization ability of neural networks. Moreover, one can build a curriculum of tasks of increasing memory requirements (Zaremba & Sutskever, 2014). Control over the difﬁculty of problem instances also allows the behaviour of a trained model to be tested on OOD sam- ples. While neural networks are highly efﬁcient on solving complex perceptual tasks, current theoretical understanding suggests that their power relies on their ability to interpo- late (Liu et al., 2020; Belkin et al., 2019; Jacot et al., 2018), limiting them to in-distribution generalisation. General rea- soning systems, however, need to be able to expand beyond this type of generalization. OOD generalization (Li et al., 2020) is paramount, as generally one can not control the distribution a model will face over time when deployed. Understanding how algorithms operate on corner cases is a standard approach for analysing their correctness. Sim- ilarly, understanding the behaviour of a trained model on larger instances of the problem, or instances that expose such corner cases that were not covered in the training set, can elucidate to what degree the model has truly learned the algorithm (as opposed to overﬁtting to speciﬁc statistics of the training data). Particularly, we can control how far from the training distribution a test instance is, potentially allow- ing us to understand to what extent the model generalizes OOD, and under which circumstances. In turn, this can offer insight into the effectiveness of different inductive biases, highlighting what kinds of inductive biases are useful for mimicking reasoning processes. 2 3 4 5 6 1 1 2 4 5 6 3 A) B) b a c b a b a b a a b c b a b T s a b a b a c a S q C) D) a2 a3 a1 a1 a1 a4 0 6 7 2 8 7 5 -2 2 9 -3 -4 4 7 -2 The CLRS Algorithmic Reasoning Benchmark One would also expect a general reasoning system to be able to reuse parts of learned computations when learning a new task, and to compose learnt computational subrou- tines (Lake, 2019; Grifﬁths et al., 2019; Alet et al., 2018). These forms of generalization have been the aim of several learning paradigms from transfer learning to meta-learning and continual learning or domain adaptation. However, many of these paradigms rely on the concept of a task, and measuring or understanding the ability of a learned sys- tem to reuse or compose requires the ability to decompose a task into sub-tasks and to be able to relate tasks among themselves. In many scenarios, such decompositions are am- biguous. Without a clear segmentation into sub-tasks, there can be no clearly deﬁned distance metric between tasks (Du et al., 2018). Conversely, algorithms are built based on subroutines that tend to be extensively shared, providing a good playground for formalizing and measuring reuse and composition, making an algorithmic reasoning benchmark potentially attractive to meta-learning practitioners. Lastly and fundamentally, computer scientists rely on a rela- tively small2 number of algorithms to address an extremely vast set of problems. They can be seen as a very powerful basis that spans most forms of reasoning processes. On one hand, this means that any generic reasoning system likely has to be able to reproduce all such kinds of procedures, hence, building a system that properly learns all of them is a major stepping stone towards generic reasoning. On the other hand, this means that they can be used to discover inductive biases that will enable tackling more complex problems. This is either because these complex problems can be seen as a combination of several algorithms, or be- cause learning certain algorithms can provide a reliable way for the model to learn how to access its own memory or how to attend to its input or other such internal mechanisms. So by ﬁrst training on algorithms—potentially controlling the difﬁculty of training instances—one can pre-train for tasks where full trajectories may not be available (Veliˇckovi´c et al., 2021). One such example is discovering novel polynomial- time heuristics for combinatorial optimisation (Bengio et al., 2020; Cappart et al., 2021; Khalil et al., 2017) or reinforce- ment learning (Deac et al., 2021). Note that our focus with this benchmark lies in learning the basic algorithms them- selves only–this in itself proves sufﬁciently challenging for neural networks, and is itself a useful outcome for the rea- sons highlighted above. However, we speculate that once a neural network can learn not only individual algorithms but novel combinations of multiple algorithms or even discover new algorithms, such networks will be useful in a wide variety of problems from scientiﬁc problems such as pro- tein folding and genomics to simulated environments such as those used by reinforcement learning and control–much 2The entire Introduction to Algorithms textbook (Cormen et al., 2009) proposes and discusses ∼100 algorithms in total. as classic CS algorithms already make in-roads into these domains but lack the ability to learn from data. Guided by these observations, we regard CLRS-30 as a ﬁrst step towards a pragmatic setting to test many of these dif- ferent aspects of current architectures. While we do not directly target all of the scenarios outlined above, the bench- mark was built with ease of expansion in mind; enabling for extensive tweaking of training/testing setups, kinds of information captured in algorithm trajectories, as well as including additional algorithms, which we aim to do consis- tently over time. 3. CLRS Algorithmic Reasoning Benchmark Owing to its name, CLRS-30 consists only of algorithms which may be encountered in the CLRS textbook (Cormen et al., 2009). Further, all algorithm trajectories and relevant variables have been designed to match the pseudocode in the textbook as closely as possible. We begin by describing the selection criteria we applied when determining which algorithms to include in CLRS-30. Our initial survey of the textbook yielded 94 algorithms and data structures of interest. From this point, we set out to ﬁlter this set to algorithms suitable for inclusion in the initial version of our benchmark. The criteria we applied, with justiﬁcation and remarks, are as follows: We want to be able to reliably generate ground-truth outputs for large inputs. As such, NP-hard tasks (and approximation algorithms thereof) have been excluded. Our decision is backed up by theoretical work suggesting impossibility of accurately modelling NP-hard problems using polynomial- time samplers, unless NP=co-NP (Yehuda et al., 2020). Tasks requiring numerical outputs have been excluded. Eval- uating their performance is ambiguous, and may be depen- dent on the way architectures choose to represent numbers. For example, Yan et al. (2020) (which represents numbers in binary) and Veliˇckovi´c et al. (2019) (which represents them in ﬂoating-point) report different metrics on predicting shortest-path lengths. This excludes most number-theoretic algorithms, linear programming, and max-ﬂow3. It does not exclude shortest-path algorithms: we can treat them as tasks of ﬁnding edges belonging to the shortest path, as was done in Veliˇckovi´c et al. (2019); Tang et al. (2020). The numeri- cal values of path lengths are then treated as intermediate parts of the trajectory, and not directly evaluated on. Standalone data structures do not directly represent a task4. 3It should be noted that, by the max-ﬂow min-cut theorem (Ford Jr & Fulkerson, 2015), any max-ﬂow problem can be cast as ﬁnding the minimum cut containing the source vertex. This is a discrete decision problem over input vertices, which hence doesn’t violate our constraints, and could be included in future iterations. 4In programming language terms, their algorithms tend to be The CLRS Algorithmic Reasoning Benchmark Rather, their target is appropriately updating the internal state of the data structure. Hence, we don’t include their operations, unless they appear as components of algorithms. We, of course, look forward to including them in subsequent versions of the dataset, as they can provide useful building blocks for learning complex algorithms. Lastly, there are representational issues associated with dy- namically allocated memory—it may be unclear what is the best way to represent the internal memory storage and its usage in algorithm trajectories. One example of the ambi- guity is in asking whether the algorithm executor should start with a “scratch space” deﬁned by the space complexity of the problem that gets ﬁlled up, or dynamically generate such space5 (Strathmann et al., 2021). As such, we for now exclude all algorithms that require allocating memory which cannot be directly attached to the set of objects provided at input time. This excludes algorithms like merge sort, Hierholzer’s algorithm for ﬁnding Euler tours (Hierholzer & Wiener, 1873), or string matching using ﬁnite automata. All of the above applied, we arrive at the 30 algorithms that are selected into CLRS-30, which we categorize as follows: Sorting: Insertion sort, bubble sort, heapsort (Williams, 1964), quicksort (Hoare, 1962). Searching: Minimum, binary search, quickselect (Hoare, 1961). Divide and Conquer (D&C): Maximum subarray (Kadane’s variant (Bentley, 1984)). Greedy: Activity selection (Gavril, 1972), task scheduling (Lawler, 1985). Dynamic Programming: Matrix chain multiplication, longest common subsequence, optimal binary search tree (Aho et al., 1974). Graphs: Depth-ﬁrst and breadth-ﬁrst search (Moore, 1959), topological sorting (Knuth, 1973), articulation points, bridges, Kosaraju’s strongly-connected components algo- rithm (Aho et al., 1974), Kruskal’s and Prim’s algorithms for minimum spanning trees (Kruskal, 1956; Prim, 1957), Bellman-Ford and Dijkstra’s algorithms for single-source shortest paths (Bellman, 1958; Dijkstra et al., 1959) (+ di- rected acyclic graphs version), Floyd-Warshall algorithm for all-pairs shortest paths (Floyd, 1962). Strings: Na¨ıve string matching, Knuth-Morris-Pratt (KMP) string matcher (Knuth et al., 1977). Geometry: Segment intersection, Convex hull algorithms: Graham scan (Graham, 1972), Jarvis’ march (Jarvis, 1973). The chosen algorithms span a wide variety of reasoning of the void type. 5Akin to malloc-like calls in C++. procedures, and hence can serve as a good basis for algorith- mic reasoning evaluation, as well as extrapolation to more challenging problems. 3.1. Implementation, probes and representation We have implemented the selected 30 algorithms in an id- iomatic way, which aligns as closely as possible to the origi- nal pseudocode from Cormen et al. (2009). This allows us to automatically generate input/output pairs for all of them, enabling full control over the input data distribution, so long as it conforms to the preconditions of the algorithm. Further, we capture the intermediate algorithm trajectory in the form of “hints” (detailed in section 3.2), which allow insight into the inner workings of the algorithm. Such trajectories have already been extensively used in related work (Veliˇckovi´c et al., 2019; 2020; Georgiev & Li´o, 2020; Deac et al., 2020) and are typically crucial for OOD generalisation. In the most generic sense, algorithms can be seen as ma- nipulating sets of objects, along with any relations between them (which can themselves be decomposed into binary relations). If the sets are (partially) ordered (e.g. arrays or rooted trees), this can be imposed by including predecessor links. Therefore, algorithms generally operate over graphs. Motivated by existing theoretical results showing that graph neural networks align well with dynamic programming-style computations (Xu et al., 2019; Dudzik & Veliˇckovi´c, 2022), we propose a graph-oriented way to encode the data. Generally, our data is represented as a set of n vertices6, where n is a hyperparameter that is provided as part of the dataset generation process. When the semantics of these nodes are not immediately clear from the task (e.g. graph algorithms naturally operate over a graph of n nodes), we make an appropriate modiﬁcation to derive nodes. For example, in sorting algorithms, we treat every input list element as a separate node, and in string matching, we treat each character of the two input strings as a separate node. All information over these graphs falls under the following categorisation: Stage: Every feature, i.e. observation in the trajectory, is either part of the input, output, or the hints. As we do not cover algorithms that perform on-line querying, for all 30 algorithms there will be exactly one snapshot of the input and output values, whereas hints will be a time-series of intermediate algorithm states. Location: Every feature is either present within the nodes, edges (pairs of nodes) or the graph7. 6Edges are only present to represent the predecessor vertex if the input is a partially ordered. 7This also determines shapes of each feature, e.g. node features The CLRS Algorithmic Reasoning Benchmark Type: Every feature can be of ﬁve possible types, which can determine the appropriate method for encoding/decoding it, and the appropriate loss function to use when learning to predict it: • scalar: Floating-point scalar8 feature. This would typically be ﬁt using mean-squared error. • categorical: Categorical feature over K possi- ble classes. The type corresponds typically to cross- entropy loss over the classes. • mask: Categorical feature over two classes. This can be ﬁt using binary cross-entropy. • mask one: Categorical feature over two classes, where exactly one node is active (“one-hot”). One would generally optimise this argmax operation using categorical cross-entropy. • pointer: Categorical feature over the n nodes. To predict “similarity” score against every node, and typically optimised using categorical cross en- tropy (as introduced in Pointer Graph Networks (PGN) (Veliˇckovi´c et al., 2020)). Specifying a feature’s stage, location and type fully deter- mines its role in the dataﬂow. A tuple (stage, loc, type, values) is referred to as a probe. Each of the 30 algorithms has a static (w.r.t. stage, location and type) set of probes, which are considered to be a spec for the algorithm. We will later describe how these specs may be used to construct baseline architectures for the benchmark. Every node is always endowed with a position scalar input probe, which uniquely indexes it—the values are linearly spaced between 0 and 1 along the node index. This allows not only representing the data sequentially (when this is appropriate), but also serves as a useful tie-breaker when algorithms could make an arbitrary choice on which node to explore next—we force the algorithms to favour nodes with smaller position values. To illustrate these concepts further, at the end of this section we will describe the probes in detail for a popular algorithm (insertion sort). Note that, while we format the data in a way that clearly favours graph neural network executors, it can be easily adapted for different types of neural architectures; for exam- ple, sequence to sequence models (Sutskever et al., 2014). are of shape n × f ; edge features are of shape n × n × f ; graph features are of shape f , where f is the dimension of this feature (excluding batch axis). 8Given our current restriction on numerical predictions, scalar types will never be given in the output stage. Overall, CLRS-30 requires 1h to generate, and occupies 4.5GB when uncompressed, across all 30 tasks. ∼ ∼ 3.2. Hints Hints are an important component of our benchmark, which we ﬁnd fundamental in order to make progress on algorith- mic reasoning. As we previously argued, the advantage of algorithms as a task is our understanding of their behaviour, and our ability to decompose them into useful subroutines that can be shared or repeatedly applied. While, implicitly, we hope that such a decomposition would happen in any learned system, even when trained just using inputs and outputs (as studied in Xu et al. (2019)), the degree to which we can measure or encourage this is limited in the typical end-to-end learning process, and often most of the generalisation happens only in-distribution (as observed by Veliˇckovi´c et al. (2019); Xu et al. (2020); Bevilacqua et al. (2021)). The underlying algorithm may not be statistically identiﬁable from a small set of input/output pairs. Conversely, a perfect decomposition of a task into small subtasks can be generated for algorithmic problems. Then, individual models for each subtask may be trained and re- composed into a solution. Such an approach will, by con- struction, provide strong decompositional beneﬁts: as stud- ied by Yan et al. (2020), perfect OOD generalisation can be observed with such models, and they can even gener- alise zero-shot to test algorithms that reuse their modules. However, the downstream applicability of this is potentially limited; when faced with a novel task which cannot be easily decomposed into subtasks, it can be hard to decide how to reuse the learnt modules. We believe hints to lie in-between these two approaches. On one hand, they represent intermediate targets which the net- work should be able to predict if it performs reasoning simi- lar9 to the ground truth algorithm it is supposed to mimic. Indeed, several lines of recent work (Veliˇckovi´c et al., 2019; Georgiev & Li´o, 2020; Veliˇckovi´c et al., 2020; Deac et al., 2020) make favourable conclusions about using them, when it comes to achieving stronger OOD generalisation. Further- more, models leveraging hints are still end-to-end models; when faced with a novel task at test-time, we don’t need explicit knowledge of that task’s hints in order to re-use the weights learnt on a task which had them. Algorithms specify one way of attacking a problem, that is explicitly detailed through the hints. In this sense, insertion sort (to be presented shortly) is one way of implementing 9Note that architectures supervised in this way usually don’t model the hints perfectly, and will deviate from the target algorithm in subtle ways—Veliˇckovi´c et al. (2020) perform a qualitative study which shows GPU-specialised data structures could emerge as a result of such setups. The CLRS Algorithmic Reasoning Benchmark Figure 2. A sequence of hints for insertion sorting a list [5, 2, 4, 3, 1]. Green pointers correspond to the predecessor pointers (specifying the list’s state throughout the algorithm’s execution. Note how the head of the list always points to itself, by convention. Further, note how, at every step, the list is rewired such that the node selected by the blue pointer (slot) will point to the current iterator (pointed in red). a sorting function: all sorting algorithms model sorting functions, and will hence have identical outputs for identical inputs. The aspects that set the different sorting algorithms apart are exposed through their hints. Being mindful of the fact that neural networks commonly run on parallelisable architectures, we have made efforts to “compress” the hints as much as possible. For example, if a single for loop is used to sweep the data and detect the node which optimises a certain quantity (without doing any order-sensitive computations), that for loop can typi- cally be entirely “skipped” when recording hints: as parallel architectures may typically examine all the nodes at once. Further, we make every effort possible that the hint at step t + 1 will be predictable from the hints at step t by using only a single step of message passing. 3.3. Worked example: insertion sort To illustrate all of the concepts outlined above, we observe the trajectories extracted by our data collection procedure on an example: insertion sorting the array [5, 2, 4, 3, 1]. Insertion sort uses one pointer (j) to scan through the array, and then another pointer (i) to slot the j-th item into the correct place within [0..j]. This ascertains the invariant that, after k steps, the subarray of the ﬁrst k elements is com- pletely sorted. Hence the trajectory (with i and j marked) [2i, 5j, 4, 3, 1] is: [5i,j, 2, 4, 3, 1] → → [2,3i, 4, 5j, 1] [1i, 2, 3, 4, 5j]. Here, at each step, j scans along the array, and i indicates the correct place for the element that was j-th at the start of each iteration. [2, 4i, 5j, 3, 1] → → Converting this trajectory into a graph representation re- quires some considerations. Requiring the model to perform explicit swapping of node values would, ultimately, require numerical predictions. To avoid it, we ask the model to predict the predecessor pointer of each node (by conven- tion, the head of the array points to itself). Hence the actual recorded trajectory can be realised as depicted in Figure 2. In this ﬁgure, green pointers correspond to the predecessor pointers, red ones point to j, and blue ones point to i. i and j are realised as type mask one, whereas predecessors are of type pointer—and all three are stored in the nodes. The red and blue pointers represent the “hints” for this task. Finally, note that the original insertion sort pseudocode mandates that, at each iteration, i starts at position j and shifts backward until the right position is found. However, this procedure can be performed in one step by a GNN, as it can locate the correct position by examining all relevant positions, and we can omit all of those intermediate steps. In order to further illustrate how these hints are collected, we also provide an informal pseudocode for collecting hints for insertion sort in Algorithm 1: Algorithm 1 Hint updates for Insertion Sort Input :Input array val, Positions pos Hints :Predecessors pred, Iterator iter, swap slot slot i = 0 1 i > 0 ; // Initialise list pred[i] (cid:40) 0 i ← − 0, iter slot ← ← while iter < n do iter iter + 1 0 ← max node argmax j : pos[j]<pos[iter] ← val[j] if val[max node] < val[iter] then max node slot ← (cid:40) slot pred[i] i = iter otherwise pred[i] ← else slot argmin j : pos[j]<pos[iter],val[j]≥val[iter] ← val[j] pred[i] ←    iter iter pred[slot] max node pred[i] i = slot i=iter∧pred[slot]=slot i=iter∧pred[slot](cid:54)=slot pred[i] = iter otherwise end end return pred ; // Return final list In the interest of illustrating the hint structures further, we provide worked examples of trajectories for three more al- 5 2 4 3 1 5 2 4 3 1 5 2 4 3 1 5 2 4 3 1 5 2 4 3 1 The CLRS Algorithmic Reasoning Benchmark gorithms (dynamic programming, path-ﬁnding and string matching) in Appendix B. It should be remarked that we directly expose all of the hint collection routines as Python code inside the CLRS library, allowing for direct inspection. 4. Empirical evaluation Having surveyed the speciﬁcs of CLRS-30, we now present experimental results on it for several proposed algorithmic reasoning models. We primarily investigate whether a natu- ral ladder of model performance will emerge when extrapo- lating to larger inputs. Beyond this, we believe the bench- mark will be useful for empirically examining many other properties of algorithmic models, such as evaluating gener- alisation across different graph types, task types, or various multi-task (Xhonneux et al., 2021) or continual learning setups. We make available complete implementations of our data generating, probing and model training subroutines, which should make evaluating on such settings simple to deploy10. We survey several key ways of interacting with the benchmark (e.g. implementing baselines, modifying datasets, adding new algorithms) in Appendix A. 4.1. Baseline models Encode-process-decode For our experimental validation, we adopt the encode-process-decode paradigm of Hamrick et al. (2018), which is a common direction for several hint- based architectures (Veliˇckovi´c et al., 2019; Georgiev & Li´o, 2020; Veliˇckovi´c et al., 2020; Deac et al., 2020). Namely, we consider a setup with inputs xi in nodes, eij in edges, and g in the graph. We ﬁrst encode each of these using linear layers fn, fe, fg, to obtain encodings hi = fn(xi) hij = fe(eij) hg = fg(g) (1) We then feed these latents through a processor network to perform one step of computation. As we are focusing on graph representation learning in the current data format, most of our processors will be realised as graph neural net- works (Gilmer et al., 2017). Most generally, along every edge (i, j), a message from node i to node j, mij is com- puted (using a message function fm), and these messages are then aggregated across all neighbouring nodes using a permutation-invariant aggregation function, (cid:76). Finally, a readout network fr transforms these aggregated messages and the node encodings into processed node encodings: mij = fm(hi, hj, hij, hg) mi = (cid:77) i∈Nj mji h(cid:48) i = fr(hi, mi) (2) (3) Once node encodings are updated, we can decode them to make various predictions for this step of reasoning, depend- ing on the type of the prediction required (using relevant decoder functions g·), as prescribed in Section 3.1. Further, we keep track of previous-step node encodings h(t−1) , to explicitly use in a recurrent cell update (exactly as done by Veliˇckovi´c et al. (2019)). We opt to provide this recurrent update in order to provide long-range capacity to the model. i Lastly, we need to decide in what capacity will hints be used. We provide results for the option where hints are both decoded (used for computing the loss function) and encoded (considered as part of x, eij and g). At testing time, the encoded hint is equal to the hints decoded by the previous step, whereas we can stabilise these trajectories at training time by performing noisy teacher forcing—inspired by Noisy Nodes (Godwin et al., 2021), at each step we feed back ground-truth hints with probability 0.5. The quantity of hints is still used to determine the number of processor steps to perform at evaluation time. This requirement of knowing the hint-size can be lifted by, e.g., using termina- tion networks (Veliˇckovi´c et al., 2019; Banino et al., 2021) or aligning to iterative algorithms (Tang et al., 2020). Processor networks The only remaining component to specify is the processor network used by our models. As this component carries the most computational load, it is also the most obvious module to sweep over. We provide all implementations and hyperparameters within our codebase. Unless otherwise speciﬁed, we assume fully-connected graphs, i.e. , hence every node is con- } nected to every other node. We consider the following baseline processor networks: 1, 2, . . . , n { i = N Deep Sets (Zaheer et al., 2017); where each node is only (i.e., choice of (cid:76) is irrele- i connected to itself: } { vant). Such a model is popular for summary statistic tasks. i = N Graph Attention Networks (Veliˇckovi´c et al., 2017), where the aggregation function (cid:76) is self-attention (Vaswani et al., 2017), and the message function fm merely extracts the sender features: fm(hi, hj, hij, hg) = Whi. We report the best performance across GAT (Veliˇckovi´c et al., 2017) and GATv2 (Brody et al., 2021) attention mechanisms. Message-passing Neural Networks (Gilmer et al., 2017), which correspond exactly to the formulation in Equation 2, with (cid:76) = max, as prescribed by previous work (Veliˇckovi´c et al., 2019). As a sanity check, we also attempted (cid:76) = (cid:80) ﬁnding it underperformed on all tasks compared to max. Pointer Graph Networks (Veliˇckovi´c et al., 2020), which use only graph neighbourhoods i speciﬁed by a union of all node pointer and edge mask hints, and (cid:76) = max. This restricts the model to only reason over the edges deemed important by the inputs and hints. N 10https://github.com/deepmind/clrs Memory Networks (Sukhbaatar et al., 2015) have been The CLRS Algorithmic Reasoning Benchmark Figure 3. Validation results on eight representative algorithms in CLRS-30 (activity selector, Bellman-Ford, binary search, ﬁnd maximum subarray, Graham scan, insertion sort, matrix chain order, na¨ıve string matcher), averaged over three seeds. In all cases the y-axis is between [0, 100]%. Legend: MPNN red, PGN purple, Deep Sets blue, GAT orange, Memory Networks green. Validation results for all 30 individual algorithms can be found in Appendix D. used in the past as baseline for investigating reasoning in neural networks (e.g. Banino et al., 2020), as they provide an alternative way to use structural dependencies in a graph by treating edges as memories and nodes as queries. Here we used latents representing node features hi as queries and latents representing edge features hij (where there is a connecting edge and 0 otherwise) as memory inputs. 4.2. Dataset statistics For each algorithm in CLRS-30, we provide a canonical set of training, validation and test trajectories for benchmarking in- and out-of-distribution generalisation. We obtain these trajectories by running the algorithms on randomly sampled inputs that conform to their input speciﬁcation. This implies, e.g., that the inputs to most graph algorithms are Erd˝os- R´enyi graphs (Erd¨os & R´enyi, 2011) with a certain edge probability. All scalar inputs are sampled from U (0, 1). For validation, our aim is to measure in-distribution gener- alisation. Hence we sample inputs of 16 nodes for both, and generate 1,000 trajectories for training and 32 for validation. For testing, we measure out-of-distribution generalisation, and sample 32 trajectories for inputs of 64 nodes. For algo- rithms where the output is on the graph stage (rather than node/edge), we generate 64 more trajectories, in order to equalise the number of targets across tasks. × We optimise our models on the training trajectories in a teacher-forced fashion, with a batch size of 32, using the Adam optimiser (Kingma & Ba, 2014) with an initial learn- ing rate of η = 0.001. We train for 10, 000 steps, early stop- ping on the validation performance. Our models are trained on one V100 Volta GPU, requiring roughly between 1h and 30h to train, depending on the algorithm’s time complexity. For example, linear-time algorithms have signiﬁcantly fewer hints—hence message passing steps—than cubic-time ones. 4.3. Validation (in-distribution) performance We provide the in-distribution performance throughout train- ing in Figure 3, for eight representative tasks in CLRS-30 (one per each algorithm type); see Appendix D for the full results on all 30 algorithms. In this regime, the MPNN appears to dominate for most tasks: achieving over 90% F1 score for nearly all of them. While this might seem like strong evidence in favour of the fully-connected MPNNs, their added degrees of free- dom may also make MPNNs more prone to overﬁtting to speciﬁcs of the input (e.g. the input graphs’ sizes), rather than truly learning the underlying reasoning rule. We present the out-of-distribution results next, in order to make this distinction clear. The CLRS Algorithmic Reasoning Benchmark Table 1. Average test micro-F1 score of all models on all algorithm classes. The full test results for all 30 algorithms, along with a breakdown of the “win/tie/loss” metric, are given in Appendix C. Algorithm Deep Sets GAT Memnet MPNN PGN Divide & Conquer Dynamic Prog. Geometry Graphs Greedy Search Sorting Strings Overall average Win/Tie/Loss counts 0.67 7.79 6.60 8.09 6.81 18.29 7.19 0.68 12.48% 66.05% 64.08% 37.65% 75.47% 43.79% 39.60% 2.64% ± ± ± ± ± ± ± ± 42.72% 0/3/27 0.74 5.33 11.18 8.66 4.59 19.81 4.64 1.08 24.43% 67.19% 73.27% 46.80% 78.96% 37.35% 14.35% 3.02% ± ± ± ± ± ± ± ± 43.17% 1/5/24 0.00 7.75 11.65 5.20 20.73 21.67 1.09 0.21 13.05% 67.94% 45.14% 24.12% 53.42% 34.35% 71.53% 1.51% ± ± ± ± ± ± ± ± 38.88% 4/2/24 20.30% 65.10% 73.11% 62.79% 82.39% 41.20% 11.83% 3.21% 0.85 6.44 17.19 8.75 3.01 19.87 2.78 0.94 ± ± ± ± ± ± ± ± 44.99% 8/3/19 4.44 6.48 7.01 8.42 6.59 21.56 8.46 0.20 65.23% 70.58% 61.19% 60.25% 75.84% 56.11% 15.45% 2.04% ± ± ± ± ± ± ± ± 50.84% 8/6/16 4.4. Test (out-of-distribution) performance 5. Conclusion The averaged out-of-distribution performance (using the early-stopped model on validation) across each of the eight algorithm types is provided in Table 1; see Appendix C for the full results on all 30 algorithms. MPNNs are unable to transfer their impressive gains to graphs that are four times larger: in fact, the PGN takes over as the most performant model when averaged across task types—this aligns well with prior research (Veliˇckovi´c et al., 2020). The outperfor- mance is also observed when we count how frequently each model is among the best-performing models for a given algorithm, as per our “win/tie/loss” metric, which we ex- plain in Appendix C. GNN models, additionally, outperform models like Deep Sets and Memory Nets, reinforcing that GNNs are a useful primitive for algorithmic reasoning (Xu et al., 2019; Dudzik & Veliˇckovi´c, 2022). Aside from all of the above, we note that the OOD version of the CLRS-30 benchmark is highly challenging and far from solved for most tasks, making it a meaningful informant of future progress in the area. In particular, PGNs struggled on tasks requiring long-range rollouts (such as DFS), or recursive reasoning (such as Quicksort and Quickselect). This invites further research in algorithmic reasoners that can support such computation. It is further revealed that more specialised inductive biases and training regimes may be required to deal with string matching algorithms (such as KMP), and that the processor studied here tended to perform the best on tasks which were of favourable (sublinear) com- plexity in terms of hint counts (such as BFS, Bellman-Ford, and task scheduling). The speciﬁc results we obtain with our baselines validate several bits of prior research in the area, but also demon- strate we still have a long way to go, with even simple OOD scenarios only being ﬁt to about 50% micro-F1 performance. We introduce CLRS-30, a dataset that contains trajectories from 30 classical algorithms. This benchmark constitutes an effective way to test out-of-distribution generalization and transfer, and brings a means to evaluate algorithmic reasoning learnt by neural network models. The dataset provides input/output pairs for all algorithms, as well as intermediate trajectory information (“hints”). It is our hope that CLRS-30 will be a useful tool to shepherd future research in algorithmic reasoning, as prior art in the area largely generated their own datasets, making progress tracking challenging. Further, we hope that CLRS-30 will make algorithmic reasoning a more accessible area: one does not need a background in theoretical computer science to generate the dataset, and can focus on the modelling. If we convinced you to try out our library, please consult Appendix A for detailed instructions on most common ways to interact with our platform. CLRS is in constant develop- ment, and we welcome any and all feedback. Acknowledgements CLRS-30 was developed over a long time-frame, with many useful contributions, which we kindly acknowledge here. We would like to particularly thank Borja Ibarz for nu- merous ﬁxes and additions, and laying foundation for fu- ture iterations. Additionally, we warmly thank Jonathan Godwin, Sadegh Mahdavi, Euan Ong, MohamedElfatih Salah, Ahmed Elhag, Andreea Deac, Frederik Nijweide, Andrew Dudzik, Thomas Kipf, Amin Barekatain and Do- brik Georgiev for their support, and identifying numerous bugs during development. Finally, we thank Kim Stachen- feld, Nate Kushman and Daan Wierstra for reviewing the paper prior to submission, and anonymous reviewers for their careful feedback, strengthening the paper signiﬁcantly. The CLRS Algorithmic Reasoning Benchmark References Aho, A. V., Hopcroft, J. E., and Ullman, J. D. The design and analysis of computer algorithms. Reading, 1974. Alet, F., Lozano-Perez, T., and Kaelbling, L. P. Modular meta-learning. volume 87 of Proceedings of Machine Learning Research. PMLR, 2018. Banino, A., Badia, A. P., K¨oster, R., Chadwick, M. J., Zambaldi, V., Hassabis, D., Barry, C., Botvinick, M., Kumaran, D., and Blundell, C. Memo: A deep net- work for ﬂexible combination of episodic memories. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=rJxlc0EtDr. Banino, A., Balaguer, J., and Blundell, C. Pondernet: Learn- ing to ponder. arXiv preprint arXiv:2107.05407, 2021. Cormen, T. H., Leiserson, C. E., Rivest, R. L., and Stein, C. Introduction to algorithms. MIT press, 2009. Corso, G., Cavalleri, L., Beaini, D., Li`o, P., and Veliˇckovi´c, P. Principal neighbourhood aggregation for graph nets. arXiv preprint arXiv:2004.05718, 2020. Deac, A., Bacon, P.-L., and Tang, J. Graph neural induc- tion of value iteration. arXiv preprint arXiv:2009.12604, 2020. Deac, A.-I., Veliˇckovi´c, P., Milinkovic, O., Bacon, P.-L., Tang, J., and Nikolic, M. Neural algorithmic reasoners are implicit planners. Advances in Neural Information Processing Systems, 34, 2021. Dijkstra, E. W. et al. A note on two problems in connex- ion with graphs. Numerische mathematik, 1(1):269–271, 1959. Belkin, M., Hsu, D., and Xu, J. Two models of double de- scent for weak features. arXiv preprint arXiv:1903.07571, 2019. Du, Y., Czarnecki, W. M., Jayakumar, S. M., Pascanu, R., and Lakshminarayanan, B. Adapting auxiliary losses using gradient similarity, 2018. Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. Bellman, R. On a routing problem. Quarterly of applied mathematics, 16(1):87–90, 1958. Bengio, Y., Lodi, A., and Prouvost, A. Machine learning for combinatorial optimization: a methodological tour d’horizon. European Journal of Operational Research, 2020. Bentley, J. Programming pearls: algorithm design tech- niques. Communications of the ACM, 27(9):865–873, 1984. Bevilacqua, B., Zhou, Y., and Ribeiro, B. Size-invariant graph representations for graph classiﬁcation extrapola- tions. In International Conference on Machine Learning, pp. 837–851. PMLR, 2021. Brody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? arXiv preprint arXiv:2105.14491, 2021. Cappart, Q., Ch´etelat, D., Khalil, E., Lodi, A., Morris, C., and Veliˇckovi´c, P. Combinatorial optimization and reasoning with graph neural networks. arXiv preprint arXiv:2102.09544, 2021. Dudzik, A. and Veliˇckovi´c, P. Graph neural networks are dynamic programmers. arXiv preprint arXiv:2203.15544, 2022. Dwivedi, V. P., Joshi, C. K., Laurent, T., Bengio, Y., and Bresson, X. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020. Erd¨os, P. and R´enyi, A. On the evolution of random graphs. In The structure and dynamics of networks, pp. 38–82. Princeton University Press, 2011. Floyd, R. W. Algorithm 97: shortest path. Communications of the ACM, 5(6):345, 1962. Ford Jr, L. R. and Fulkerson, D. R. Flows in networks. Princeton university press, 2015. Freivalds, K., Ozolin¸ ˇs, E., and ˇSostaks, A. Neural shufﬂe- exchange networks-sequence processing in o (n log n) In Advances in Neural Information Processing time. Systems, pp. 6630–6641, 2019. Gavril, F. Algorithms for minimum coloring, maximum clique, minimum covering by cliques, and maximum independent set of a chordal graph. SIAM Journal on Computing, 1(2):180–187, 1972. Georgiev, D. and Li´o, P. Neural bipartite matching. arXiv preprint arXiv:2005.11304, 2020. Chen, Z., Chen, L., Villar, S., and Bruna, J. Can graph arXiv preprint neural networks count substructures? arXiv:2002.04025, 2020. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chem- istry. arXiv preprint arXiv:1704.01212, 2017. The CLRS Algorithmic Reasoning Benchmark Godwin, J., Schaarschmidt, M., Gaunt, A. L., Sanchez- Gonzalez, A., Rubanova, Y., Veliˇckovi´c, P., Kirkpatrick, J., and Battaglia, P. Simple gnn regularisation for 3d molecular property prediction and beyond. In Interna- tional Conference on Learning Representations, 2021. Graham, R. L. An efﬁcient algorithm for determining the Info. Pro. Lett., 1: convex hull of a ﬁnite planar set. 132–133, 1972. Graves, A., Wayne, G., and Danihelka, I. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014. Grifﬁths, T., Callaway, F., Chang, M., Grant, E., Krueger, P., and Lieder, F. Doing more with less: meta-reasoning and meta-learning in humans and machines. Current Opinion in Behavioral Sciences, October 2019. Hamrick, J. B., Allen, K. R., Bapst, V., Zhu, T., McKee, K. R., Tenenbaum, J. B., and Battaglia, P. W. Relational inductive bias for physical construction in humans and machines. arXiv preprint arXiv:1806.01203, 2018. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Knuth, D. E. Fundamental algorithms. 1973. Knuth, D. E., Morris, Jr, J. H., and Pratt, V. R. Fast pattern matching in strings. SIAM journal on computing, 6(2): 323–350, 1977. Kool, W., van Hoof, H., and Welling, M. Attention, arXiv preprint learn to solve routing problems! arXiv:1803.08475, 2018. Kruskal, J. B. On the shortest spanning subtree of a graph and the traveling salesman problem. Proceedings of the American Mathematical society, 7(1):48–50, 1956. Lake, B. M. Compositional generalization through meta sequence-to-sequence learning. In Advances in Neural In- formation Processing Systems 32, pp. 9791–9801. 2019. Lawler, E. L. The traveling salesman problem: a guided tour of combinatorial optimization. Wiley-Interscience Series in Discrete Mathematics, 1985. Hennigan, T., Cai, T., Norman, T., and Babuschkin, I. Haiku: Sonnet for JAX, 2020. URL http://github.com/ deepmind/dm-haiku. Li, Y., Gimeno, F., Kohli, P., and Vinyals, O. Strong general- ization and efﬁciency in neural programs. arXiv preprint arXiv:2007.03629, 2020. Hierholzer, C. and Wiener, C. ¨Uber die m¨oglichkeit, einen linienzug ohne wiederholung und ohne unterbrechung zu umfahren. Mathematische Annalen, 6(1):30–32, 1873. Hoare, C. A. Algorithm 65: ﬁnd. Communications of the ACM, 4(7):321–322, 1961. Hoare, C. A. Quicksort. The Computer Journal, 5(1):10–16, 1962. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems 31. 2018. Jarvis, R. A. On the identiﬁcation of the convex hull of a ﬁnite set of points in the plane. Information processing letters, 2(1):18–21, 1973. Joshi, C. K., Cappart, Q., Rousseau, L.-M., Laurent, T., and Bresson, X. Learning tsp requires rethinking generaliza- tion. arXiv preprint arXiv:2006.07054, 2020. Kaiser, Ł. and Sutskever, I. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015. Khalil, E., Dai, H., Zhang, Y., Dilkina, B., and Song, L. Learning combinatorial optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp. 6348–6358, 2017. Liu, C., Zhu, L., and Belkin, M. Toward a theory of optimization for over-parameterized systems of non- linear equations: the lessons of deep learning. CoRR, abs/2003.00307, 2020. Merity, S., Xiong, C., Bradbury, J., and Socher, R. arXiv preprint Pointer sentinel mixture models. arXiv:1609.07843, 2016. Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernock´y, J. Empirical evaluation and combination of In INTER- advanced language modeling techniques. SPEECH, pp. 605–608, 2011. Moore, E. F. The shortest path through a maze. In Proc. Int. Symp. Switching Theory, 1959, pp. 285–292, 1959. Prim, R. C. Shortest connection networks and some gen- eralizations. The Bell System Technical Journal, 36(6): 1389–1401, 1957. Richter, O. and Wattenhofer, R. Normalized attention with- out probability cage. arXiv preprint arXiv:2005.09561, 2020. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. The CLRS Algorithmic Reasoning Benchmark Strathmann, H., Barekatain, M., Blundell, C., and Veliˇckovi´c, P. Persistent message passing. arXiv preprint arXiv:2103.01043, 2021. Xu, K., Li, J., Zhang, M., Du, S. S., Kawarabayashi, K.-i., and Jegelka, S. What can neural networks reason about? arXiv preprint arXiv:1905.13211, 2019. Sukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. End-to- end memory networks. arXiv preprint arXiv:1503.08895, 2015. Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to se- quence learning with neural networks. In Advances in neural information processing systems, pp. 3104–3112, 2014. Tang, H., Huang, Z., Gu, J., Lu, B., and Su, H. Towards scale-invariant graph-related problem solving by itera- tive homogeneous gnns. the 34th Annual Conference on Neural Information Processing Systems (NeurIPS), 2020. Xu, K., Li, J., Zhang, M., Du, S. S., ichi Kawarabayashi, K., and Jegelka, S. How neural networks extrapolate: From feedforward to graph neural networks. arXiv preprint arXiv:2009.11848, 2020. Yan, Y., Swersky, K., Koutra, D., Ranganathan, P., and Heshemi, M. Neural execution engines: Learning to execute subroutines. arXiv preprint arXiv:2006.08084, 2020. Yehuda, G., Gabel, M., and Schuster, A. It’s not what machines can learn, it’s what we cannot teach. arXiv preprint arXiv:2002.09398, 2020. Trask, A., Hill, F., Reed, S. E., Rae, J., Dyer, C., and Blun- som, P. Neural arithmetic logic units. In Advances in Neural Information Processing Systems, pp. 8035–8044, 2018. Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. Deep sets. In Advances in neural information processing systems, pp. 3391–3401, 2017. Zamir, A. R., Sax, A., Shen, W., Guibas, L. J., Malik, J., and Savarese, S. Taskonomy: Disentangling task trans- fer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3712–3722, 2018. Zaremba, W. and Sutskever, I. Learning to execute. arXiv preprint arXiv:1410.4615, 2014. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten- tion is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Veliˇckovi´c, P. and Blundell, C. Neural algorithmic reasoning. arXiv preprint arXiv:2105.02761, 2021. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. Veliˇckovi´c, P., Ying, R., Padovano, M., Hadsell, R., and Blundell, C. Neural execution of graph algorithms. arXiv preprint arXiv:1910.10593, 2019. Veliˇckovi´c, P., Buesing, L., Overlan, M. C., Pascanu, R., Vinyals, O., and Blundell, C. Pointer graph networks. arXiv preprint arXiv:2006.06380, 2020. Veliˇckovi´c, P., Boˇsnjak, M., Kipf, T., Lerchner, A., Hadsell, R., Pascanu, R., and Blundell, C. Reasoning-modulated representations. arXiv preprint arXiv:2107.08881, 2021. Vinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. In Advances in Neural Information Processing Systems, pp. 2692–2700, 2015. Williams, J. W. J. Algorithm 232: heapsort. Commun. ACM, 7:347–348, 1964. Xhonneux, L.-P., Deac, A.-I., Veliˇckovi´c, P., and Tang, J. How to transfer algorithmic reasoning knowledge to learn new algorithms? Advances in Neural Information Pro- cessing Systems, 34, 2021. A. Interfacing with the CLRS benchmark The CLRS Algorithmic Reasoning Benchmark The CLRS benchmark is publicly hosted on GitHub: https://github.com/deepmind/clrs. All code and artifacts are released under an Apache 2.0 license, which is highly permissive. Within clrs/examples/run.py, we demonstrate an extensively conﬁgurable example script that evaluates a speciﬁc baseline on CLRS-30. Our baselines are provided in JAX and Haiku (Hennigan et al., 2020), but the dataset is generated using NumPy, making it possible to create learning pipelines in virtually any framework, including PyTorch and TensorFlow. We will now highlight three key ways in which researchers can interface with the library. A.1. Evaluating a new baseline on CLRS-30 To support a new baseline, the recommended path depends on how fundamentally different the baseline is to an encode- process-decode GNN. In most cases, we anticipate that only the processor network needs changing, and the remainder of the architec- ture can match our baselines. In this case, it is only necessary to implement the new processor network within clrs/ src/processors.py and appropriately set self.mpnn within the construct processor method in clrs/ src/baselines.py. For more fundamentally different baselines, it is necessary to create a new class that extends the Model API (as found within clrs/ src/model.py). clrs/ src/baselines.py provides one example of how this can be done efﬁciently, for the case of our baselines. A.2. Modifying the data distribution of CLRS-30 If users want to train and/or evaluate the models on different versions of the tasks given in CLRS-30, the key routines to modify are located in clrs/ src/samplers.py. The easiest modiﬁcation concerns the graph sizes and/or numbers of trajectories. They can be directly changed by modifying the CLRS30 dictionary near the top of the ﬁle. For more elaborate modiﬁcations, e.g. to the speciﬁc data sampling distributions, the users would need to modify and/or extend the relevant sampler class. As a guiding example, we provide a SortingSampler class which is convenient for generating inputs for sorting algorithms. The speciﬁc sampler used for each task is provided in the SAMPLERS dictionary towards the end of the ﬁle. A.3. Adding new algorithms to CLRS As the most elaborate of the three workﬂows, adding a new algorithm to the task suite requires following several steps, which are potentially comprehensive, depending on the complexity of the algorithm. However, the CLRS benchmark code still provides may helper routines for probing and batching that facilitate inclusion of novel algorithms. The steps are as follows: 1. First, determine the input/hint/output speciﬁcation of your algorithm, and include it within the SPECS dictionary of clrs/ src/specs.py. 2. Implement the desired algorithm in an abstractiﬁed form. Examples of this can be found throughout the clrs/ src/algorithms/ folder. 3. Next, choose appropriate moments within the algorithm’s execution to create probes that capture the inputs, outputs and all intermediate state (using the probing.push function). 4. Once generated, probes can be prepared using the probing.finalize method, and should be returned together with the algorithm output. 5. Lastly, implement an appropriate input data sampler for your algorithm, and include it within the SAMPLERS dictionary within clrs/ src/samplers.py. B. Additional worked examples of algorithm trajectories The CLRS Algorithmic Reasoning Benchmark Matrix Chain Order As a representative dynamic programming algorithm, we visualise the steps of the procedure for optimising the order of multiplications in a chain of matrices, for multiplying matrices of size (10 60), assuming a O(n3)-time multiplication algorithm. 30)(30 5)(5 × × × The algorithm proceeds by ﬁlling up an “upper-triangular” part of a dynamic programming matrix, where cell [i, j] corresponds to the optimal number of operations when multiplying all the matrices between the ith and jth. Such an algorithm may also be represented in a “pyramidal” form as below: Additionally, the algorithm maintains (and returns) the optimal way to recursively divide each subsequence into two (by 5) (yielding 1, 500 storing the optimal dividing point, in green). Here, it is optimal to ﬁrst multiply (10 operations), then multiply the remaning matrices as (10 × 60) (yielding 3, 000 operations; 4, 500 in total). 30)(30 5)(5 × × × Note that every pointer points into one of the original n input nodes (at the lowest level), and how each cell of the pyramid corresponds to a pair of input nodes (specifying the corresponding range). Therefore, rather than creating O(n2) auxiliary nodes, we instead record all relevant values above as edge scalars and edge pointers, and store nodes only for the lowest level of the pyramid. Further, whether or not a particular edge has been populated yet (the “ ” indicator above) is stored as an additional binary ﬂag. ∞ Bellman-Ford As a representative graph algorithm, we visualise the steps of the Bellman-Ford algorithm for ﬁnding single-source shortest paths in a given graph. Initially, the source node is labelled with distance zero, and all other nodes with distance “ ” (which, once again, is represented as a binary node hint). The algorithm then iteratively relaxes all edges as follows, until convergence is achieved: ∞ Besides updating the distance values, the algorithm also maintains, and returns, the predicted shortest path tree – for each node, a pointer to its predecessor along the optimal path from the source. By convention, the source node points to itself. These pointers are visualised in green. Na¨ıve String Matcher As a representative string algorithm, we visualise the steps of the na¨ıve string matcher, for detecting string ""ab"" inside the string ""aab"". ∞ ∞ ∞ ∞ ∞ ∞ ∞ ∞ ∞ ∞ 4500 1500 9000 1500 9000 300 150 300 300 150 300 300 150 300 10 30 5 60 10 30 5 60 10 30 5 60 10 30 5 60 0 1 2 ∞ 2 2 ∞ 8 ∞ 3 ∞ 0 1 2 1 2 2 2 ∞ 8 3 ∞ 0 1 2 1 2 2 2 3 3 5 8 The CLRS Algorithmic Reasoning Benchmark In this case, each character of the two strings is given a separate node, and three sets of indices are maintained: indicating the start of the current candidate match (in blue); and the current position being checked in both the haystack (red) and the needle (purple). The algorithm scans candidate positions left-to-right until a full match is detected for the ﬁrst time. Additionally, each character is tagged with its predecessor in the string (in green), and a binary ﬂag indicating which of the two strings it belongs to (not shown here). C. Test results for all algorithms Test performance for all 30 algorithms in CLRS-30 may be found in Table 2. In addition, we provide a “win-tie-loss” metric as another way of differentiating model performance, which is less sensitive to outliers. The resulting counts are provided in Table 3, and are computed as follows: • Let µA( Table 2). M ) and σA( ) be the mean and standard deviation of model M ’s test performance on algorithm A (as in M • We say that model • If = . A ∀X (cid:54) • Otherwise, if A (cid:31) A . outperforms model B on algorithm A—denoted by A A (cid:31) B —if µA( ) A − , then model A wins on algorithm A. A X σA( ) > µA( A ). B A , then model loses on algorithm A. ∃X • Otherwise, model X (cid:31) A A is tied on algorithm A. A The win/tie/loss counts are then aggregated across all algorithms A to obtain a metric for each model. As already mentioned, the details of this on a per-algorithm level are given in Table 3. D. Validation results individual plots Validation performance for all 30 algorithms in CLRS-30 may be found in Figure 4. For convenience, we also report the early-stopped validation performance in Table 4. a a b a b a a b a b a a b a b a a b a b The CLRS Algorithmic Reasoning Benchmark Algorithm Deep Sets GAT Memnet MPNN PGN Table 2. Test performance of all models on all algorithms. Activity Selector Articulation Points Bellman-Ford BFS Binary Search Bridges Bubble Sort DAG Shortest Paths DFS Dijkstra Find Max. Subarray Floyd-Warshall Graham Scan Heapsort Insertion Sort Jarvis’ March KMP Matcher LCS Length Matrix Chain Order Minimum MST-Kruskal MST-Prim Na¨ıve String Match Optimal BST Quickselect Quicksort Segments Intersect SCC Task Scheduling Topological Sort Overall average 1.67 4.04 0.85 0.38 0.88 2.65 3.24 2.42 0.73 3.10 0.39 0.90 2.75 12.57 4.65 0.81 0.54 5.25 3.58 2.08 4.71 5.47 0.29 1.36 1.33 2.16 0.60 2.61 0.70 3.57 66.09% 39.06% 51.33% 98.63% 47.97% 32.43% 50.73% 73.21% 7.44% 36.12% 12.48% 7.22% 64.71% 28.94% 40.98% 50.25% 3.22% 50.10% 78.36% 80.19% 60.58% 12.17% 2.05% 69.71% 3.21% 37.74% 77.29% 17.81% 84.84% 15.84% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 43.36% 1.37 1.62 1.19 0.21 3.12 6.60 1.77 1.37 2.04 0.79 0.43 3.14 2.70 1.83 1.87 10.25 0.36 1.02 3.31 2.95 0.99 4.34 1.20 1.75 0.95 0.98 0.04 3.12 2.09 6.92 73.23% 37.76% 87.91% 99.04% 23.50% 25.64% 9.91% 81.14% 11.78% 58.01% 24.43% 16.66% 77.89% 10.35% 29.52% 51.51% 3.03% 57.88% 78.19% 84.20% 65.72% 38.20% 3.01% 65.49% 4.36% 7.60% 90.41% 12.70% 84.69% 27.03% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 44.69% 2.22 0.61 1.46 0.04 0.46 0.05 0.78 1.92 1.61 2.39 0.08 0.13 2.31 1.57 0.86 3.87 0.00 4.34 1.03 0.11 0.61 3.77 0.48 1.21 0.03 0.67 0.90 4.78 0.04 0.11 24.10% 1.50% 40.04% 43.34% 14.37% 30.26% 73.58% 66.15% 13.36% 22.48% 13.05% 14.17% 40.62% 68.00% 71.42% 22.99% 1.81% 49.84% 81.96% 86.93% 28.84% 10.29% 1.22% 72.03% 1.74% 73.10% 71.81% 16.32% 82.74% 2.73% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 38.03% 3.16 2.18 0.28 0.05 0.26 4.78 0.60 0.56 0.51 0.50 0.49 1.77 0.31 0.84 2.08 12.39 0.86 0.36 1.40 0.88 1.50 7.56 0.30 0.44 0.69 0.10 0.10 4.88 0.32 6.24 80.66% 50.91% 92.01% 99.89% 36.83% 72.69% 5.27% 96.24% 6.54% 91.50% 20.30% 26.74% 91.04% 10.94% 19.81% 34.86% 2.49% 53.23% 79.84% 85.34% 70.97% 69.08% 3.92% 62.23% 1.43% 11.30% 93.44% 24.37% 84.11% 52.60% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 51.02% 1.62 2.09 0.34 0.29 0.13 7.82 1.95 0.16 0.24 1.75 2.56 0.51 1.61 0.18 2.43 1.07 0.12 0.21 0.49 0.52 1.36 0.98 0.20 1.82 0.42 0.15 0.75 0.64 0.91 2.69 66.80% 49.53% 92.99% 99.63% 76.95% 51.42% 6.01% 96.94% 8.71% 83.45% 65.23% 28.76% 56.87% 5.27% 44.37% 49.19% 2.00% 56.82% 83.91% 87.71% 66.96% 63.33% 2.08% 71.01% 3.66% 6.17% 77.51% 20.80% 84.89% 60.45% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 52.31% The CLRS Algorithmic Reasoning Benchmark Figure 4. Validation results on all 30 algorithms in CLRS-30, averaged over three seeds. The CLRS Algorithmic Reasoning Benchmark Table 3. Win/Tie/Loss counts of all models on all algorithms. Legend: W: win, T: tie, L: loss. Algorithm Deep Sets GAT Memnet MPNN PGN Activity Selector Articulation Points Bellman-Ford BFS Binary Search Bridges Bubble Sort DAG Shortest Paths DFS Dijkstra Find Max. Subarray Floyd-Warshall Graham Scan Heapsort Insertion Sort Jarvis’ March KMP Matcher LCS Length Matrix Chain Order Minimum MST-Kruskal MST-Prim Na¨ıve String Match Optimal BST Quickselect Quicksort Segments Intersect SCC Task Scheduling Topological Sort L L L L L L L L L L L L L L L T T L L L L L L L L L L L T L L L L L L L L L T L L L L L L T T W L L L L L L T L L L T L L L L L L L W L T L L L L W W L L L L L L L L T L W L L L L W T L W L W L L L W L L W L L L L L L L W T W L L L W T L L L T W L W L L W L L W W L L L L L L W W L T L T T L L T T W Overall counts 0/3/27 1/5/24 4/2/24 8/3/19 8/6/16 The CLRS Algorithmic Reasoning Benchmark Algorithm Deep Sets GAT Memnet MPNN PGN Table 4. Early-stopped validation results of all models on all algorithms. Activity Selector Articulation Points Bellman-Ford BFS Binary Search Bridges Bubble Sort DAG Shortest Paths DFS Dijkstra Find Max. Subarray Floyd-Warshall Graham Scan Heapsort Insertion Sort Jarvis’ March KMP Matcher LCS Length Matrix Chain Order Minimum MST-Kruskal MST-Prim Na¨ıve String Match Optimal BST Quickselect Quicksort Segments Intersect SCC Task Scheduling Topological Sort Overall average 0.17 0.31 0.14 0.00 0.41 0.05 1.02 0.28 1.26 0.42 0.22 0.04 0.24 0.33 0.28 0.42 0.21 0.36 0.02 0.11 2.01 0.32 0.15 0.14 0.92 1.12 0.12 1.23 0.04 0.81 83.50% 99.63% 81.12% 100.00% 93.34% 99.36% 81.51% 92.25% 62.76% 80.34% 91.41% 35.79% 87.66% 81.84% 89.58% 72.82% 98.03% 69.24% 94.46% 97.59% 83.79% 74.61% 49.80% 92.02% 42.30% 79.69% 77.49% 89.52% 99.16% 47.23% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 80.93% 0.50 0.00 0.14 0.00 0.17 0.00 1.01 0.05 0.64 0.40 0.32 0.09 0.11 2.23 0.58 0.16 0.08 0.19 0.03 0.21 0.25 0.14 0.00 0.49 1.86 0.40 0.16 0.00 0.04 0.00 92.40% 100.00% 99.28% 100.00% 95.72% 100.00% 95.44% 96.81% 99.22% 99.22% 95.00% 87.28% 97.85% 87.24% 95.18% 98.38% 99.76% 77.00% 99.37% 97.74% 97.93% 98.37% 100.00% 93.30% 83.82% 92.97% 90.82% 100.00% 99.80% 100.00% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 95.66% 2.15 1.03 0.42 0.09 0.28 1.13 0.14 0.05 0.45 0.70 0.08 0.04 1.58 0.28 0.14 6.61 0.00 0.24 0.10 0.10 0.95 0.28 0.20 0.40 0.25 0.24 1.08 1.43 0.09 0.50 34.59% 16.84% 68.75% 70.70% 20.33% 96.46% 92.64% 81.90% 47.72% 67.38% 27.91% 31.29% 53.53% 54.04% 94.40% 37.92% 9.67% 67.69% 93.91% 95.56% 64.65% 74.09% 9.91% 90.86% 6.56% 93.16% 71.57% 70.57% 84.80% 8.30% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 57.92% 0.39 0.00 0.05 0.00 0.12 0.00 1.84 0.05 0.00 0.14 0.37 0.03 0.15 0.11 0.19 0.25 0.05 0.42 0.04 0.05 0.17 0.09 0.00 0.11 0.78 0.40 0.20 0.00 0.00 0.00 93.89% 100.00% 99.48% 100.00% 94.19% 100.00% 94.53% 99.93% 100.00% 99.67% 95.13% 89.14% 98.45% 94.27% 96.74% 97.94% 99.87% 77.88% 99.12% 97.64% 99.71% 99.02% 100.00% 93.88% 88.74% 95.70% 93.84% 100.00% 100.00% 100.00% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 96.63% 0.19 0.00 0.05 0.00 0.08 0.00 5.46 0.00 0.00 0.05 0.16 0.15 0.27 0.67 0.82 0.36 0.99 0.04 0.03 0.14 0.08 0.14 0.08 0.27 0.17 1.42 0.18 0.05 0.08 0.00 82.26% 100.00% 99.35% 100.00% 94.17% 100.00% 87.17% 99.80% 100.00% 99.28% 95.30% 88.70% 89.06% 90.36% 84.57% 88.34% 94.14% 69.19% 99.21% 97.07% 99.12% 97.79% 50.33% 93.20% 54.02% 54.30% 78.32% 99.93% 99.06% 100.00% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 89.47%",1.0
"Learning Task-relevant Representations for Generalization via
  Characteristic Functions of Reward Sequence Distributions","[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3534678.3539391', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2205.10218v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.10218v2', 'rel': 'related', 'type': 'application/pdf'}]",2022-05-20 14:52:03,,"2 2 0 2 y a M 6 2 ] I A . s c [ 1 v 1 2 5 3 1 . 5 0 2 2 : v i X r a Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality Tom Zahavy DeepMind, London tomzahavy@deepmind.com Yannick Schroecker DeepMind, London yschroecker@deepmind.com Feryal Behbahani DeepMind, London feryal@deepmind.com Kate Baumli DeepMind, London baumli@deepmind.com Sebastian Flennerhag DeepMind, London ﬂennerhag@deepmind.com Shaobo Hou DeepMind, London shaobohou@deepmind.com Satinder Singh DeepMind, London baveja@deepmind.com Abstract Finding different solutions to the same problem is a key aspect of intelligence associated with creativity and adaptation to novel situations. In reinforcement learning, a set of diverse policies can be useful for exploration, transfer, hierarchy, and robustness. We propose DOMiNO, a method for Diversity Optimization Maintaining Near Optimality. We formalize the problem as a Constrained Markov Decision Process where the objective is to ﬁnd diverse policies, measured by the distance between the state occupancies of the policies in the set, while remaining near-optimal with respect to the extrinsic reward. We demonstrate that the method can discover diverse and meaningful behaviors in various domains, such as different locomotion patterns in the DeepMind Control Suite. We perform extensive analysis of our approach, compare it with other multi-objective baselines, demonstrate that we can control both the quality and the diversity of the set via interpretable hyperparameters, and show that the discovered set is robust to perturbations. 1 Introduction Creative problem solving is the mental process of searching for an original and previously unknown solution to a problem [38]. The relationship between creativity and intelligence is widely recognized across many ﬁelds; for example, in the ﬁeld of Mathematics, ﬁnding different proofs to the same theorem is considered elegant and often leads to new insights. Closer to Artiﬁcial Intelligence (AI), consider the ﬁeld of game playing and, speciﬁcally, the game of Chess in which a move is considered creative when it goes beyond known patterns [17]. In some cases, such moves can only be detected by human players while remaining invisible to current state-of-the-art Chess engines. A famous example thereof is the winning move in game eight of the Classical World Chess Championship 2004 between Leko and Kramnik [8, 3]. Humans and, indeed, many animals employ similarly creative behavior on a daily basis; faced with a challenging problem, we often consider qualitatively different alternative solutions. Yet, the majority of AI research is focused on ﬁnding a single best solution to a given problem. For example, in the ﬁeld of Reinforcement Learning (RL), most algorithms are designed to ﬁnd a single reward-maximizing policy. However, for many problems of interest there may be many qualitatively Preprint. Under review. different optimal or near-optimal policies; ﬁnding such diverse set of policies may help an RL agent become more robust to changes in the task and/or environment and to generalize better to future tasks. The majority of the literature on this problem has been done in the ﬁeld of Quality-Diversity (QD), which comprises of two main families of algorithms: MAP-Elites [36, 16] and novelty search with local competition [32]. QD algorithms typically maintain a collection of policies and adapt it using evolutionary algorithms to balance the QD trade-off [41, 51, 15]. Further references can be found on the QD webpage. In contrast to this line of work, we propose a differentiable optimization framework for maximizing the diversity of a set of RL policies. We do so by formulating diversity maximization as an optimization problem in state occupancies, and then showing that we can solve this problem by maximizing an intrinsic reward that corresponds to the gradient of the diversity objective. In related work, intrinsic rewards have been used for learning diversity in terms of the discriminability of different trajectory-speciﬁc quantities [24, 20, 46, 7]. Other works implicitly induce diversity to learn policies that maximize the set robustness to the worst-possible reward [31, 57], or use diversity as a regularizer when maximizing the extrinsic reward [29, 34, 40, 49, 59, 46]. Our work makes the following contributions. First, we propose DOMiNO, a method for Diversity Optimization that Maintains Nearly Optimal policies. DOMiNO trains a set of policies using a policy-speciﬁc, weighted combination of the extrinsic reward and an intrinsic diversity reward. The weights are adapted as Lagrange multipliers to guarantee that each policy is near-optimal. Second, we propose to measure diversity via expected features; i.e., the features that a policy observes in its state occupancy. Under this measure of diversity, we introduce two novel objectives for diversity optimization: a repulsive force that motivates policies to have distinct expected features and a Van Der Waals force, which combines the repulsive force with an attractive one and allows us to specify the degree of diversity in the set. Third, we perform experiments in the DeepMind Control Suite [52] and the BiPedal walker environment [13] and show that DOMiNO discovers qualitatively diverse locomotion behaviors (Fig. 1b). We analyze our approach and compare it to other multi-objective strategies for handling the QD trade-off. Lastly, we demonstrate that the discovered set is robust to perturbations of the environment and the morphology of the avatar. (a) (b) Figure 1: (a) DOMiNO’s architecture: The agent learns a set of QD policies via a single latent- conditioned actor-critic network with intrinsic and extrinsic value heads. Dashed arrows signify training objectives. (b) DOMiNO’s π: Near optimal diverse policies in walker.stand corresponding to standing on both legs, standing on either leg, lifting the other leg forward and backward, spreading the legs and stamping. Not only are the policies different from each other, they also achieve high extrinsic reward in standing (see values on top of each policy). 2 Preliminaries and Notation In this work, we will express objectives in terms of the state occupancy measure dπ. Intuitively speaking, dπ measures how often a policy π visits each state-action pair. As we will soon see, the classic RL objective of reward maximization can be expressed as a linear product between the reward vector and the state occupancy. In addition, in this work we will formulate diversity maximization via an objective that is a nonlinear function of the state occupancy. While it might seem unclear which 2 i R u n n n g A v e r a g e U p d a t e c o n s t r a i n t & L a g r a n g e MLP Torso MLP i R u n n n g A v e r a g e D i v e r s i t y R e w a r d reward should be maximized to solve such an objective, we take inspiration from Convex MDPs [58] where one such reward is the gradient of the objective with respect to dπ. We begin with some formal deﬁnitions. In RL an agent interacts with an environment and seeks to maximize its cumulative reward. We consider two cases, the average reward case and the discounted case. The Markov decision process [42, MDP] is deﬁned by the tuple (S, A, P, R) for the average reward case and by the tuple (S, A, P, R, γ, D0) for the discounted case. We assume an inﬁnite horizon, ﬁnite state-action problem. Initially, the state of the agent is sampled according to s0 ∼ D0. At time t, given state st, the agent selects action at according to its policy π(st, ·), receives a reward rt ∼ R(st, at) and transitions to a new state st+1 ∼ P (·, st, at). We consider two performance t=1γtrt, for the average reward metrics, given by vavg case and discounted case respectively. The goal of the agent is to ﬁnd a policy that maximizes vavg or vγ π. Let Pπ(st = ·) be the probability measure over states at time t under policy π, then the state occupancy measure dπ is given as davg t=1 Pπ(st = s)π(s, a), and π(s, a) = (1 − γ)E (cid:80)∞ t=1 γtPπ(st = s)π(s, a), for the average reward case and the discounted dγ case respectively. With these, we can rewrite the RL objective in as a linear function of the occupancy measure maxdπ∈K s,a r(s, a)dπ(s, a), where K is the set of admissible distributions [58]. Next, consider an objective of the form: π (s, a) = limT →∞ π = (1 − γ)E(cid:80)∞ π = limT →∞ T E (cid:80)T t=1rt, vγ T E(cid:80)T (cid:80) π 1 1 min dπ∈K f (dπ), (1) where f : K → R is a nonlinear function. Sequential decision making problems that take this form include Apprenticeship Learning (AL) and pure exploration, among others [1, 55, 26, 59, 23, 58, 45, 9, 37]. In the remainder of this section, we brieﬂy explain how to solve Eq. (1) using RL methods when the function f is convex. We begin with rewriting Eq. (1) using Fenchel duality as min dπ∈K f (dπ) = max λ∈Λ min dπ∈K (λ · dπ − f ∗(λ)) (2) where Λ is the closure of (sub-)gradient space {∂f (dπ)|dπ ∈ K}, which is compact and convex [2], and f ∗ is the Fenchel conjugate of the function f . Eq. (2) presents a zero-sum max-min game between two players, the policy player Algπ and the cost player Algλ. We can see that from the perspective of the policy player, the objective is a linear minimization problem in dπ. Thus, intuitively speaking, the goal of the policy player is to maximize the negative cost as a reward r = −λ. To solve Eq. (2), we employ the meta algorithm from [2], which uses two online learning algorithms. The policy player Algπ generates a sequence of policies {πk}k∈N by maximizing a sequence of negative costs {−λk}k∈N as rewards that are produced by the cost player Algλ. In this paper, the policy player uses an online RL algorithm and the cost player uses the Follow the Leader (FTL) algorithm. This implies that the cost at time k is given as λk = ∇f ( ¯dk−1 π ). (3) π In other words, to solve an RL problem with a convex objective function (Eq. (1)), the policy player maximizes a non stationary reward that at time k corresponds to the negative gradient of the objective function f w.r.t ¯dk−1 . When the function f is convex, it is guaranteed that the average state occupancy of these polices, ¯dK π, converges to an optimal solution to Eq. (1), i.e., ¯dK π → d(cid:63) Features and expected features. We focus on the case where each state-action pair is associated with some observable features φ(s, a) ∈ Rd. For example, in the DM control suite [52], these features correspond to the positions and velocities of the body joints being controlled by the agent. In other cases, we can learn φ with a neural network. π ∈ arg mindπ∈K f (dπ) [58]. π = 1 K k=1 dk (cid:80)K Similar to value functions, which represent the expectation of the reward under the state occupancy, we deﬁne expected features as ψπ(s, a) = Es(cid:48),a(cid:48)∼dπ(s,a) φ(s(cid:48), a(cid:48)) ∈ Rd. Note that in the special case of one-hot feature vectors, the expected features coincide with the state occupancy. The deﬁnition of ψπ depends on the state occupancy we consider. In the discounted case, ψγ π ∈ Rd is also known as Successor Features (SFs) as deﬁned in [5, 6]. In the average case, ψavg π ∈ Rd represents the expected features under the policy’s stationary distribution and therefore it has the same value for all the state action pairs. Similar deﬁnitions were suggested in [35, 56]. 3 3 Discovering diverse near-optimal policies We now introduce Diversity Optimization Maintaining Near Optimality, or, DOMiNO, which discovers a set of n policies Πn = {πi}n i=1 by solving the optimization problem: max Πn Diversity(Πn) s.t dπ · re ≥ αv∗ e , ∀π ∈ Πn, (4) where v∗ e is the value of the optimal policy. In other words, we are looking for a set of policies that are as diverse from each other as possible, deﬁned as Diversity : {R|S||A|}n → R. In addition, we constrain the policies in the set to be nearly optimal. To deﬁne near-optimality we introduce a hyperparameter α ∈ [0, 1], such that a policy is said to be near optimal if it achieves a value that is at e . In practice, we “ﬁx” the Lagrange multiplier for the ﬁrst policy µ1 = 1, so this policy only least αv∗ receives extrinsic reward, and use the value of this policy to estimate v∗ e ). Notice that this estimate is changing through training. e = v1 e (v∗ Before we dive into the details, we brieﬂy explain the main components of DOMiNO. Building on Section 2 and, in particular, Eq. (3), we ﬁnd policies that maximize the diversity objective by maximizing its gradient as a reward signal, i.e., ri π). We discuss two candidates for this objective and derive an analytical formula for the associated rewards in Section 3.1. Diversity(d1 π, . . . , dn d = ∇di π Then, in Section 3.2 we explain how to combine the two rewards via the coefﬁcients ce, cd. Thus, each of the policies, π1, . . . , πn, will be maximizing a reward signal ri that is a linear combination of the extrinsic reward re and ri d : i.e., ri(s, a) = cere(s, a) + cdri d(s, a). To this end, we focus on the method of Lagrange multipliers, which adapts the coefﬁcients online in order to solve Eq. (4) and compare it with other multi-objective baselines. 3.1 Diversity Next, we present an objective that motivates policies to visit different states on average. It does so by leveraging the information about the policies’ long-term behavior available in their expected features, and motivating the state occupancies to be different from each other. For that reason, we refer to this objective as a repulsive force (Eq. (5)). We then extend this objective and combine it with a second, attractive force (Eq. (7)), taking inspiration from the Van Der Waals (VDW) force. The manner in which we combine these two forces allows us to control the degree of diversity in the set. A repulsive force. How do we compute a set of policies with maximal distances between their expected features? To answer this question, we ﬁrst consider the simpler scenario where there are only two policies in the set and consider the following objective maxπ1,π2 ||ψ1 − ψ2||2 2. This objective is related to the objective of Apprenticeship Learning [AL; 1], i.e., solving the problem minψ ||ψ − ψE||2 2, where ψE are the feature expectations of an expert. Both problems use the euclidean norm in the feature expectation space to measure distances between policies. Since we are interested in diversity, we are maximizing this objective, while AL aims to minimize it. In a similar fashion, the mutual information between policies and states, which is equivalent to the KL divergence between state occupancies [58, 21] is minimized for AL [28] and maximized for diversity [20]. Next, we investigate how to measure the distance of a policy from the set of multiple policies, Πn. First we introduce the Hausdorff distance [43] that measures how far two subsets D, C of a metric space are from each other: Dist(D, C) = minc∈C,d∈D ||c − d||2 2. In other words, two sets are far from each other in the Hausdorff distance if every point of either set is far from all the points of the other set. Building on this deﬁnition, we can deﬁne the distance from an expected features vector ψi to the set of the other expected features vectors as minj(cid:54)=i ||ψi − ψj||2 2. This equation gives us the distance between each individual policy and the other policies in the set. Maximizing it across the policies in the set, gives us our ﬁrst diversity objective: max d1 π,...,dn π 0.5 (cid:88)n i=1 min j(cid:54)=i ||ψi − ψj||2 2. (5) π Diversity(d1 to compute the associated diversity reward, we compute the gradient ri d = In order ∇di π). To do so, we begin with a simpler case where there are only two policies, 2 = φ · (ψ1 − ψ2), i.e., r = ∇d1 2 = ∇d1 such that r(s, a) = φ(s, a) · (ψ1 − ψ2). This reward was ﬁrst derived by Abbeel & Ng [1], but here it is with an opposite sign since we care about maximizing it. Lastly, for a given policy πi, we deﬁne by π(s,a)φ(s, a) − Es(cid:48),a(cid:48)∼d2 π, . . . , dn ||ψ1 − ψ2||2 π(s,a)φ(s, a)||2 ||Es(cid:48),a(cid:48)∼d1 π π 4 π i , we get1 that ∇di i the index of the policy with the closest expected features to πi, i.e., j∗ j∗ Using the deﬁnition of j∗ 2 = ∇di minj(cid:54)=i ||ψi − ψj||2 d(s, a) = φ(s, a) · (ψi − ψj∗ ri The Van Der Waals force. Next, we propose a second diversity objective that allows us to control the degree of diversity in the set via a hyperparameter. The objective is inspired from molecular physics, and speciﬁcally, by how atoms in a crystal lattice self-organize themselves at equal distances from each other. This phenomenon is typically explained as an equilibrium between two distance dependent forces operating between the atoms known as the Van Der Waals (VDW) forces; one force that is attractive and another that is repulsive. i = arg minj(cid:54)=i ||ψi − ψj||2 2. i ||2 ||ψi − ψj∗ 2, and that i ). (6) π The VDW force is typically characterized by a distance in which the combined force becomes repulsive rather than attractive (see, for example, [47]). This distance is called the VDW contact distance, and we denote it by (cid:96)0. In addition, we denote by (cid:96)i = ||ψi − ψj∗ i ||2 the Hausdorff distance for policy i. With this notation, we deﬁne our second diversity objective as2 max π,...,dn d1 π (cid:88)n i=1 0.5(cid:96)2 i (cid:124) (cid:123)(cid:122) (cid:125) Repulsive i /(cid:96)3 0 −0.2(cid:0)(cid:96)5 (cid:123)(cid:122) (cid:124) Attractive (cid:1) (cid:125) . (7) We can see that Eq. (7) is a polynomial in (cid:96)i, composed of two forces with opposite signs and different powers. The different powers determine when each force dominates the other. For example, when the expected features are close to each other ((cid:96)i << (cid:96)0), the repulsive force dominates, and when ((cid:96)i >> (cid:96)0) the attractive force dominates. The gradient (and hence, the associated reward) is given by d(s, a) = (1 − ((cid:96)i/(cid:96)0)3)φ(s, a) · (ψi − ψj∗ ri Inspecting Eq. (8) we can see that when the expected features are organized at the VDW contact distance (cid:96)0, the objective is maximized and the gradient is zero. In a related line of work, Vassiliades et al. [54] suggested to use voronoi tessellation to partition the feature space of the MAP-Elite algorithm to regions of equal size and Liu et al. [33] proposed a Stein Variational Policy Gradient with repulsive and attractive components. However, using a VDW force to control diversity is novel to the best of our knowledge. i ). (8) 3.2 Balancing Quality with Diversity Constrained MDPs. At the core of our approach is a solution to the CMDP in Eq. (4). There exist different methods for solving CMDPs and we refer the reader to [4] and [50] for treatments of the subject at different levels of abstraction. In this work we will focus on a reduction of CMDPs to MDPs via gradient updates, known as Lagrangian methods [11, 10, 53, 14]. Most of the literature on CMDPs has focused on linear objectives and linear constraints. In Section 2, we discussed how to solve an unconstrained convex RL problem of the form of Eq. (1) as a saddle point problem. We now extend these results to the case where the objective is convex and the constraint is linear, i.e. mindπ∈K f (dπ), subject to g(dπ) ≤ 0, where f denotes the diversity objective and g is a linear function of the form g(dπ) = αv∗ e − dπ · re deﬁning the constraint. Solving this problem is equivalent to solving the following problem: min dπ∈K max µ≥0 f (dπ) + µg(dπ) = min dπ∈K max µ≥0,λ λ · dπ − f ∗(λ) + µ(αv∗ e − dπ · re), (9) where the equality follows from Fenchel duality as before. Similar to Section 2, we use the FTL algorithm for the λ player (Eq. (3)). This implies that the cost at iteration k, λk, is equivalent to the gradient of the diversity objective, which we denote by rd. Eq. (9) involves a vector, λ − µre, linearly interacting with dπ. Thus, intuitively speaking, minimizing Eq. (9) from the perspective of the policy player is equivalent to maximizing a reward rd + µre. The objective for the Lagrange multiplier µ is to maximize Eq. (9), or equivalently µ(αv∗ e − dπ · re). Intuitively speaking, when the policy achieves an extrinsic value that satisﬁes the constraint, the 1In the rare case that the arg min has more than one solution, the gradient is not deﬁned, but we can still use Eq. (6) as a reward. 2The coefﬁcients in Eq. (7) are chosen to simplify the reward in Eq. (8). I.e., since the reward is the gradient of the objective, after differentiation the coefﬁcients equal 1 in Eq. (8). 5 Lagrange multiplier µ decreases (putting a smaller weight on the extrinsic component of the reward) and it increases otherwise. More formally, we can solve the problem in Eq. (9) as a three-player game. In this case the policy player controls dπ as before, the cost player chooses λ using Eq. (3), and the Lagrange player chooses µ with gradient descent. Proving this statement is out of the scope of this work, but we shall investigate it empirically. 3.3 Multi-objective alternatives We conclude this section by discussing two alternative approaches for balancing the QD trade-off, which we later compare empirically with the CMDP approach. First, consider a multi-objective MDP that combines the diversity objective with the extrinsic reward as π, . . . , dn cedi π · re + cdDiversity(d1 (10) π), max Πn where ce, cd are ﬁxed weights that balance the diversity objective and the extrinsic reward. We note that the solution of such a multi-objective MDP cannot be a solution to a CMDP in general. I.e., it is not possible to ﬁnd the optimal dual variables µ∗, plug them into Eq. (9) and simply solve the resulting (unconstrained) MDP. Such an approach ignores the fact that the dual variables must be a ‘best-response’ to the policy and is referred to as the ”scalarization fallacy” in [50, Section 4]. While multi-objective MDPs have been used in prior QD-RL papers [29, 34, 39, 22, 40, 60], we now outline a few potential advantages for using CMDPs. First, the CMDP formulation guarantees that the policies that we ﬁnd are near optimal (satisfy the constraint). Secondly, the weighting coefﬁcient in multi-objective MDPs has to be tuned, where in CMDPs it is adapted. This is particularly important in the context of maximizing diversity while satisfying reward. Next, consider a hybrid approach that combines a multi objective MDP with a CMDP as max Πn max(0, αv∗ e − di π · re) + cdDiversity(d1 π, . . . , dn π). We denote by I i an indicator function for the event in which the constraint is not satisﬁed for policy πi, i.e., I i = 1 if di e , and 0, otherwise. With this notation the reward is given by π · re < αv∗ ri(s, a) = I ire(s, a) + cdri d(s, a) (11) In other words, the agent maximizes a weighted combination of the extrinsic reward and the diversity reward when the constraint is violated and only the diversity reward when the constraint is satisﬁed. Kumar et al. [31] proposed a similar approach where the agent maximizes a weighted combination of the rewards when the constraint is satisﬁed, and only the extrinsic reward otherwise: ri(s, a) = re(s, a) + cd(1 − I i)ri d(s, a) (12) We refer to Eq. (12) as SMERL, as was coined in [31], and to Eq. (11) as Reverse SMERL. Note that these methods come with an additional hyperparameter cd which balances the two objectives as a multi-objective MDP, in addition to the optimality ratio α. 4 Experiments Our experiments address the following questions: (a) Can we learn diverse policies that are also near optimal? (see Section 4.1) (b) How critical are various components of our algorithm and how does it compare to other multi-objective baselines in terms of QD trade-off? (see Section 4.1) (c) Does our method scale to settings where the feature space is high-dimensional and unstructured? (see Section 4.2) (d) Finally, can the diverse policies discovered by DOMiNO enable robustness and adaptation to novel perturbations in the environment and agent? (see Section 4.3) Environment. We conducted most of our experiments on domains from the DM Control Suite [52], standard continuous control locomotion tasks where diverse near-optimal policies should naturally correspond to different gaits. Due to space considerations, we present Control Suite results on the walker.stand task. In the supplementary, however, we present similar results for walker.walk and BiPedal walker from OpenAI Gym [13] suggesting that the method generalizes across different reward functions and domains. We also include the challenging dog domain with 38 actions and a 223−dimensional observation space, which is one of the more challenging domains in control suite. 6 Agent. Fig. 1a shows an overview of DOMiNO’s components and their interactions, instantiated in an actor-critic agent. While acting, the agent samples a new latent variable z ∈ [1, n] uniformly at random at the start of each episode. We train all the policies simultaneously and provide this latent variable as an input. For the average reward state occupancy, the agent keeps an empirical running average for each latent policy of the rewards ˜vavg πi and features (either from the environment φobs or torso embedding φembedding) ˜ψavg πi encountered, where the average is taken as ˜xi = αd ˜xi−1 + (1 − αd) 1 t=1 xi(st, at) with decay factor αd. Varying αd can make the estimation more online (small T αd, as used for the constraint), or less online (large αd, as needed for Eq. (4)). The agent uses ˜ψavg to compute the diversity reward as described in Eq. (6). ˜vavg is used to optimize πi πi the Lagrange multiplier µ for each policy as in Eq. (9) which is then used to weight the quality and diversity advantages for the policy gradient update. Pseudo code and further implementation details, as well as treatment of the discounted state occupancy, can be found in Appendix A.2. (cid:80)T 4.1 Quality and diversity To measure diversity qualitatively, we present ”motion ﬁgures” by discretizing the videos (details in the Appendix) that give a fair impression of the behaviors. The videos, associated with these ﬁgures, can be found in the supplementary as well. Fig. 1b presents ten polices discovered by DOMiNO with the repulsive objective and the optimality ratio set to 0.9. The policies are ordered from top-left to bottom right, so policy 1, which only maximizes extrinsic reward and sets the constraint, is always at the top left. The policies exhibit different types of standing: standing on both legs, standing on either leg, lifting the other leg forward and backward, spreading the legs and stamping. Not only are the policies different from each other, they also achieve high extrinsic reward in standing (see values on top of each policy visualization). Similar ﬁgures for other domains can be found in Appendix B.1. To further study the QD trade-off, we use scatter plots, showing the episode return on the y-axis, and the diversity score, corresponding to the Hausdorff distance (Eq. (5)), on the x-axis. The top-right corner of the diagram, therefore, represents the most diverse and highest quality policies. Each ﬁgure presents a sweep over one or two hyperparameters and we use the color and a marker to indicate the values. In all of our experiments, we report 95% conﬁdence intervals. In the scatter plots, they correspond to 5 seeds and are indicated by the crosses surrounding each point. Fig. 2 (left) presents the results for DOMiNO with the repulsive reward in the walker.stand domain. We can observe that regardless of the set size, DOMiNO achieves roughly the same extrinsic reward across different optimality ratios (points with the same color obtain the same y-value). This implies that the constraint mechanism is working as expected across different set sizes and optimality ratios. In addition, we can inspect how the QD trade-off is affected when changing the optimality ratio α for sets of the same size (indicated in the ﬁgures with light lines). This observation can be explained by the fact that for lower values of α, the volume of the constrained set is larger, and therefore, it is possible to ﬁnd more diversity within it. This behavior is consistent across different set sizes, though it is naturally more difﬁcult to ﬁnd a set of diverse policies as the set size gets larger (remember that we measure the distance to the closest policy in the set). Figure 2: DOMiNO QD results in walker.stand. Left: Set size vs. optimality ratio (α) with the repulsive reward. Center: Set size vs. α with the Van der Waals (VDW) reward. Right: Target diversity ((cid:96)0) vs. α with the VDW reward. We present the same investigation for the VDW reward in Fig. 2 (center). Similar to the repulsive reward, we can observe that the constraint is satisﬁed, and that reducing the optimality ratio allows for more diversity. Fig. 2 (right) shows how different values of (cid:96)0 affect the QD trade-off for a set of size 10. We can observe that the different combinations of (cid:96)0 and α are organized as a grid in the 7 QD scatter, suggesting that we can control both the level of optimality and the degree of diversity by setting these two interpretable hyperparameters. Fig. 3 compares the QD balance yielded by DOMiNO to the alternative strategies described in Section 3.2. Speciﬁcally, we look at DOMiNO’s Lagrangian method, the linear multi-objective combination of the objectives (Eq. (10)), and the two hybrid strategies, SMERL (Eq. (12)) and Reverse SMERL (Eq. (11)) for a set of ten policies in walker.stand. Note that in all cases we are using DOMiNO’s repulsive diversity objective, and the comparison is strictly about strategies for combining the quality and diversity objectives. The plot for each strategy shows how the solution to the QD tradeoff varies according to the relevant hyperparameters for that strategy, namely, the optimality ratio α for DOMiNO, the ﬁxed constant ce for the multi-objective strategy (we implicitly set cd = 1 − ce), and both α and constant cd for the hybrid approaches (in the hybrid plots, cd value is labeled directly next to the marker, while α is indicated by color). Figure 3: DOMiNO’s Lagrangian method ﬁnds only solutions that push the upper right boundary of quality and diversity, and varies in a smooth, interpretable way with its only hyperparameter, α, contrasted with the jumpy nature of the multi-objective hyperparameter ce, and the redundancy of the hyperparameters in the hybrid methods (SMERL and Reverse SMERL). For the multi-objective approach, shown on the right, the ce parameter proves ill-behaved and choppy, quickly jumping from the extreme of all diversity no quality to the opposite, without a smooth interim. In contrast, the DOMiNO approach of solving the CMDP directly for the Lagrange multiplier yields solutions that push along the upper right diagonal boundary, ﬁnding the highest diversity (farthest right) set of policies for a given optimality ratio (color), varying smoothly along this line as α varies. Another advantage of DOMiNO’s approach is that it only ﬁnds such QD-optimal solutions, where, in contrast, SMERL (left), when appropriately tuned, can also yield some solutions along the upper-right QD border, but often ﬁnds sub-optimal solutions, and therefore must be tuned further with cd to ﬁnd the best QD solutions. We further explore the difﬁculty tuning SMERL in the supplementary (Fig. 10) and ﬁnd that the best cd for 10 policies provides solutions with no diversity for other set sizes. 4.2 Feature analysis The choice of feature space used for optimizing diversity can have a huge impact on the kind of diverse behavior learned. In environments where the observation space is high dimensional and less structured (e.g. pixel observations), computing diversity using the raw features may not lead to meaningful behavior. As speciﬁed in Section 3.1 the feature space used to compute diversity in our Control Suite experiments throughout the paper corresponds to the positions and velocities of the body joints returned as observations by the environment. We show that it is feasible to use a learned embedding space instead. As a proof of principle we use the output of the torso network as a learned embedding described in Section 4 for computing our diversity metric. Table 1 compares the diversity mea- sured in raw observation features (Diversityobs) and embedding features (Diversityembedding) in the walker.stand domain. Columns indicate the feature space that was used to compute the di- versity objective during training averaged across 20 seeds. Inspecting the table, we can see that agents trained to optimize diversity in the learned embedding space and agents that directly optimize diversity in the observation space achieve comparable diversity if measured in either space, indicating that learned embeddings can feasibly be used to achieve meaningful diversity. Diversityobs Diversityembedding φembedding 1.01 ± 0.05 2.35 ± 0.09 φobs 1.21 ± 0.05 2.14 ± 0.09 Table 1 8 Figure 4: K-shot adaptation in Control Suite. We report mean episode return (95% CI) on held-out test tasks relative to the performance of a single policy trained on extrinsic rewards. While not invariant to sudden changes in the environment, DOMiNO is more robust to a variety of perturbations. 4.3 Closing the loop: k-shot adaptation We motivated qualitative diversity by saying that diverse solutions can be robust and allow for rapid adaptation to new tasks and changes in the environment. Here we validate this claim in a k-shot adaptation experiment: we train a set of QD policies on a canonical benchmark task, then test their ability to adapt to environment and agent perturbations. These include four kinematics and dynamics perturbations from the Real World RL suite [18], and a ﬁfth perturbation inspired by a ”motor failure” condition [31] which, every 50 steps and starting at T = 10, disables action-inputs for the ﬁrst two joints for a ﬁxed amount of time3. In Fig. 4, we present the results in the walker.walk and walker.stand domains (rows). Columns correspond to perturbation types and the x-axis corresponds to the magnitude of the perturbation. K-shot adaptation is measured in the following manner. For each perturbed environment, and each method, we ﬁrst execute k = 10 environment trajectories with each policy. Then, for each method we select the policy that performs the best in the set. We then evaluate this policy for 40 more trajectories and measure the average reward of the selected policy rmethod. The y-axis in each ﬁgure measures rmethod/rbaseline, where rbaseline measures the reward in the perturbed environment of an RL baseline agent that was trained with a single policy to maximize the extrinsic reward in the original task. We note that the baseline was trained with the same RL algorithm, but without diversity, and it matches the state-of-the-art in each training domain (it is almost optimal). The raw rewards rmethod, rbaseline can be found in the supplementary (Fig. 12). Lastly, we repeat this process across 20 training seeds, and report the average with a 95% Conﬁdence Interval (CI) 4. We compare the following methods: DOMiNO, SMERL, Multi-Objective, and No diversity, where all the diversity methods use the diversity reward from Eq. (6) and all the methods are with 10 policies. Since we are treating the perturbed environments as hold out tasks, we selected the hyper parameters for each method based on the results in Fig. 3, i.e.we chose the conﬁguration that was the most qualitatively diverse (in the upper-right most corner of Fig. 3). Concretely, for DOMiNO and SMERL α = 0.9, for SMERL cd = 0.5 and for Multi-Objective ce = 0.7. More K-shot adaptation curves with other hyper parameter values can be found in Appendix D. The No diversity method is similar to rbaseline, but uses 10 policies that all maximize the extrinsic reward (instead of a single policy). Inspecting Fig. 4 we can see that for small perturbations, DOMiNO retains the performance of the baseline. However, as the magnitude of the perturbation increases, the performance of DOMiNO is much higher than the baseline (by a factor of 1.5 − 2.0). This observation highlights that a diverse set of policies as found by DOMiNO is much more capable at handling changes to the environment and can serve as a strong starting point for recovering optimal behavior. As we have shown in 3.2, other approaches to managing the trade-off between quality and diversity such as SMERL are much more sensitive to the choice of hyper-parameters and require signiﬁcant tuning. While SMERL is able to ﬁnd a useful, diverse set of policies with some effort, it is difﬁcult to match DOMiNO’s performance across all pertubations and tasks. See the supplementary material for further comparison of DOMiNO with SMERL and Multi-objective over more hyper parameters. We also include a video that illustrates how the individual policies adapt to the environment perturbations. 3While we tried to recreate a similar condition to [31], the tasks are not directly comparable due to signiﬁcant differences in the simulators that have been used as well as the termination conditions in the base task. 4We use boosted CI with nested sampling as implemented in the bootstrap function here, which reﬂects the amount of training seeds and the amount of evaluation seeds per training seed 9 s n r u t e r e v i t a e R l s n r u t e r e v i t a e R l d n a t S 1.50 1.25 1.00 1.35 l k a W 1.20 1.05 1.80 1.50 1.20 0.90 1.80 1.50 1.20 0.90 0 0 1 0 2 0 3 0 4 5 2 0 . 2 Motor failure (duration) 5 7 0 . 3 0 . 3 0 . 2 Thigh length 2.00 1.60 1.20 0.80 1.75 1.50 1.25 1.00 0 . 4 0 . 3 1.40 1.20 1.00 2.40 2.00 1.60 1.20 1.20 1.05 0.90 1.35 1.20 1.05 0.90 0 . 5 0 . 1 1 . 0 2 . 0 3 . 0 4 . 0 0 . 7 Joint damping 5 5 0 . 4 0 . 4 0 . 3 Torso length DOMiNO SMERL No diversity Multi-Objective 1 0 0 . 0 2 0 . 1 0 . 0 Contact friction 0 . 0 0 5 5 Conclusions In this work we proposed DOMiNO, an algorithm for discovering diverse behaviors that maintain optimality. We framed the problem as a CMDP in the state occupancies of the policies in the set and developed an end-to-end differentiable solution to it based on reward maximization. In our experiments we demonstrated that the policies discovered by DOMiNO, or, DOMiNO’s π, are diverse and maintain optimality. We then explored how DOMiNO balances the QD trade-off and compared it with multi-objective baselines. Our results suggest that DOMiNO can control the degree of quality and diversity via two interpretable hyperparameters, while other baselines struggle to capture both. In our K-shot experiments we demonstrated that DOMiNO’s π can adapt to changes in the environ- ment. An exciting direction for future work is to use DOMiNO in a never ending RL setting, where the environment changes smoothly over time, and see if maintaing a set of QD diverse policies will make it more resilient to such changes. 10 References [1] Abbeel, P. and Ng, A. Y. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-ﬁrst international conference on Machine learning, pp. 1. ACM, 2004. [2] Abernethy, J. D. and Wang, J.-K. On frank-wolfe and equilibrium computation. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Cur- ran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 7371364b3d72ac9a3ed8638e6f0be2c9-Paper.pdf. [3] Agadmator. Invisible to engines — one of the greatest moves ever played. agadmator’s Chess Channel, 2018. URL https://www.youtube.com/watch?v=yGnpewUKP88&t=1s. [4] Altman, E. Constrained Markov decision processes, volume 7. CRC Press, 1999. [5] Barreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., van Hasselt, H. P., and Silver, D. Successor features for transfer in reinforcement learning. In Advances in neural information processing systems, pp. 4055–4065, 2017. [6] Barreto, A., Hou, S., Borsa, D., Silver, D., and Precup, D. Fast reinforcement learning with generalized policy updates. Proceedings of the National Academy of Sciences, 2020. [7] Baumli, K., Warde-Farley, D., Hansen, S., and Mnih, V. Relative variational intrinsic control. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 35:6732–6740, May 2021. URL https://ojs.aaai.org/index.php/AAAI/article/view/16832. [8] Behovits, R. Game 8: Leko wins to take the lead. Chess news, 2004. URL https://en. chessbase.com/post/game-8-leko-wins-to-take-the-lead. [9] Belogolovsky, S., Korsunsky, P., Mannor, S., Tessler, C., and Zahavy, T. Inverse reinforcement learning in contextual mdps. Machine Learning, pp. 1–40, 2021. [10] Bhatnagar, S. and Lakshmanan, K. An online actor–critic algorithm with function approximation for constrained markov decision processes. Journal of Optimization Theory and Applications, 153(3):688–708, 2012. [11] Borkar, V. S. An actor-critic algorithm for constrained markov decision processes. Systems & control letters, 54(3):207–213, 2005. [12] Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., and Wanderman- Milne, S. JAX: composable transformations of Python+NumPy programs, 2018. URL http: //github.com/google/jax. [13] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [14] Calian, D. A., Mankowitz, D. J., Zahavy, T., Xu, Z., Oh, J., Levine, N., and Mann, T. Balancing constraints and rewards with meta-gradient d4pg. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=TQt98Ya7UMP. [15] Cully, A. Autonomous skill discovery with quality-diversity and unsupervised descriptors. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 81–89, 2019. [16] Cully, A., Clune, J., Tarapore, D., and Mouret, J.-B. Robots that can adapt like animals. Nature, 521(7553):503–507, 2015. [17] da Fonseca-Wollheim, C. Swapping songs with chess grandmaster garry kasparov. The New York Times, 2020. URL https://www.nytimes.com/2020/12/18/arts/music/ garry-kasparov-classical-music.html. [18] Dulac-Arnold, G., Mankowitz, D., and Hester, T. Challenges of real-world reinforcement learning. arXiv preprint arXiv:1904.12901, 2019. 11 [19] Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning, I., et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International Conference on Machine Learning, pp. 1407–1416. PMLR, 2018. [20] Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SJx63jRqFm. [21] Eysenbach, B., Salakhutdinov, R., and Levine, S. The information geometry of unsupervised reinforcement learning. arXiv preprint arXiv:2110.02719, 2021. [22] Gangwani, T., Peng, J., and Zhou, Y. Harnessing distribution ratio estimators for learning agents with quality and diversity. arXiv preprint arXiv:2011.02614, 2020. [23] Geist, M., P´erolat, J., Lauri`ere, M., Elie, R., Perrin, S., Bachem, O., Munos, R., and Pietquin, O. Concave utility reinforcement learning: the mean-ﬁeld game viewpoint. arXiv preprint arXiv:2106.03787, 2021. [24] Gregor, K., Rezende, D. J., and Wierstra, D. Variational intrinsic control. International Conference on Learning Representations, Workshop Track, 2017. URL https://openreview. net/forum?id=Skc-Fo4Yg. [25] Ha, D. Reinforcement learning for improving agent design. arXiv preprint arXiv:1810.03779, 2018. [26] Hazan, E., Kakade, S., Singh, K., and Van Soest, A. Provably efﬁcient maximum entropy exploration. In International Conference on Machine Learning, pp. 2681–2691. PMLR, 2019. [27] Hessel, M., Kroiss, M., Clark, A., Kemaev, I., Quan, J., Keck, T., Viola, F., and van Hasselt, H. Podracer architectures for scalable reinforcement learning. arXiv preprint arXiv:2104.06272, 2021. [28] Ho, J. and Ermon, S. Generative adversarial imitation learning. arXiv preprint arXiv:1606.03476, 2016. [29] Hong, Z.-W., Shann, T.-Y., Su, S.-Y., Chang, Y.-H., Fu, T.-J., and Lee, C.-Y. Diversity-driven exploration strategy for deep reinforcement learning. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 10510–10521, 2018. [30] Jouppi, N. P., Young, C., Patil, N., Patterson, D. A., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., Boyle, R., Cantin, P., Chao, C., Clark, C., Coriell, J., Daley, M., Dau, M., Dean, J., Gelb, B., Ghaemmaghami, T. V., Gottipati, R., Gulland, W., Hagmann, R., Ho, R. C., Hogberg, D., Hu, J., Hundt, R., Hurt, D., Ibarz, J., Jaffey, A., Jaworski, A., Kaplan, A., Khaitan, H., Koch, A., Kumar, N., Lacy, S., Laudon, J., Law, J., Le, D., Leary, C., Liu, Z., Lucke, K., Lundin, A., MacKean, G., Maggiore, A., Mahony, M., Miller, K., Nagarajan, R., Narayanaswami, R., Ni, R., Nix, K., Norrie, T., Omernick, M., Penukonda, N., Phelps, A., Ross, J., Salek, A., Samadiani, E., Severn, C., Sizikov, G., Snelham, M., Souter, J., Steinberg, D., Swing, A., Tan, M., Thorson, G., Tian, B., Toma, H., Tuttle, E., Vasudevan, V., Walter, R., Wang, W., Wilcox, E., and Yoon, D. H. In-datacenter performance analysis of a tensor processing unit. CoRR, abs/1704.04760, 2017. URL http://arxiv.org/abs/1704.04760. [31] Kumar, S., Kumar, A., Levine, S., and Finn, C. One solution is not all you need: Few-shot extrapolation via structured maxent rl. Advances in Neural Information Processing Systems, 33, 2020. [32] Lehman, J. and Stanley, K. O. Evolving a diversity of virtual creatures through novelty search and local competition. In Proceedings of the 13th annual conference on Genetic and evolutionary computation, pp. 211–218, 2011. [33] Liu, Y., Ramachandran, P., Liu, Q., and Peng, J. Stein variational policy gradient. In 33rd Conference on Uncertainty in Artiﬁcial Intelligence, UAI 2017, 2017. 12 [34] Masood, M. A. and Doshi-Velez, F. Diversity-inducing policy gradient: Using maximum mean discrepancy to ﬁnd a set of diverse policies. arXiv preprint arXiv:1906.00088, 2019. [35] Mehta, N., Natarajan, S., Tadepalli, P., and Fern, A. Transfer in variable-reward hierarchical reinforcement learning. Machine Learning, 73(3):289, 2008. [36] Mouret, J.-B. and Clune, J. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909, 2015. [37] Mutti, M., De Santi, R., De Bartolomeis, P., and Restelli, M. Challenging common assumptions in convex reinforcement learning. arXiv preprint arXiv:2202.01511, 2022. [38] Osborn, A. F. Applied imagination. Scribner’s, 1953. [39] Parker-Holder, J., Pacchiano, A., Choromanski, K. M., and Roberts, S. J. Effective diversity in population based reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020. [40] Peng, Z., Sun, H., and Zhou, B. Non-local policy optimization via diversity-regularized collaborative exploration. arXiv preprint arXiv:2006.07781, 2020. [41] Pugh, J. K., Soros, L. B., and Stanley, K. O. Quality diversity: A new frontier for evolutionary computation. Frontiers in Robotics and AI, 3:40, 2016. [42] Puterman, M. L. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 1984. [43] Rockafellar, R. T. Convex analysis. Princeton university press, 1970. [44] Schmitt, S., Hessel, M., and Simonyan, K. Off-policy actor-critic with shared experience replay. In International Conference on Machine Learning, pp. 8545–8554. PMLR, 2020. [45] Shani, L., Zahavy, T., and Mannor, S. Online apprenticeship learning. arXiv preprint arXiv:2102.06924, 2021. [46] Sharma, A., Gu, S., Levine, S., Kumar, V., and Hausman, K. Dynamics-aware unsupervised discovery of skills. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HJgLZR4KvH. [47] Singh, A. K. Chapter 2 - structure, synthesis, and application of nanoparticles. In Singh, A. K. (ed.), Engineered Nanoparticles, pp. 19–76. Academic Press, Boston, 2016. ISBN 978-0-12-801406-6. doi: https://doi.org/10.1016/B978-0-12-801406-6.00002-9. URL https: //www.sciencedirect.com/science/article/pii/B9780128014066000029. [48] Stooke, A., Achiam, J., and Abbeel, P. Responsive safety in reinforcement learning by pid lagrangian methods. In International Conference on Machine Learning, pp. 9133–9143. PMLR, 2020. [49] Sun, H., Peng, Z., Dai, B., Guo, J., Lin, D., and Zhou, B. Novel policy seeking with constrained optimization. arXiv preprint arXiv:2005.10696, 2020. [50] Szepesv´ari, C. Constrained mdps and the reward hypothesis. Musings about machine learn- ing and other things (blog), 2020. URL https://readingsml.blogspot.com/2020/03/ constrained-mdps-and-reward-hypothesis.html. [51] Tarapore, D., Clune, J., Cully, A., and Mouret, J.-B. How do different encodings inﬂuence the performance of the map-elites algorithm? In Proceedings of the Genetic and Evolutionary Computation Conference 2016, pp. 173–180, 2016. [52] Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. [53] Tessler, C., Mankowitz, D. J., and Mannor, S. Reward constrained policy optimization. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=SkfrvsA9FX. 13 [54] Vassiliades, V., Chatzilygeroudis, K., and Mouret, J.-B. Scaling up map-elites using centroidal voronoi tessellations. arXiv preprint arXiv:1610.05729, 2016. [55] Zahavy, T., Cohen, A., Kaplan, H., and Mansour, Y. Apprenticeship learning via frank-wolfe. AAAI, 2020, 2020. [56] Zahavy, T., Cohen, A., Kaplan, H., and Mansour, Y. Average reward reinforcement learning with unknown mixing times. The Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2020. [57] Zahavy, T., Barreto, A., Mankowitz, D. J., Hou, S., O’Donoghue, B., Kemaev, I., and Singh, S. Discovering a set of policies for the worst case reward. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=PUkhWz65dy5. [58] Zahavy, T., O’Donoghue, B., Desjardins, G., and Singh, S. Reward is enough for convex MDPs. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id= ELndVeVA-TR. [59] Zhang, J., Koppel, A., Bedi, A. S., Szepesvari, C., and Wang, M. Variational policy gradient method for reinforcement learning with general utilities. arXiv preprint arXiv:2007.02151, 2020. [60] Zhang, Y., Yu, W., and Turk, G. Learning novel policies for tasks. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learn- ing, volume 97 of Proceedings of Machine Learning Research, pp. 7483–7492. PMLR, 09–15 Jun 2019. URL http://proceedings.mlr.press/v97/zhang19q.html. 14 Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See the discussion in Appendix C . (c) Did you discuss any potential negative societal impacts of your work? [No] We could identify any potential negative societal impacts for this work (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [No] While the DM control domain is open sourced, our code is proprietary and we are not able to share it. That said, we shared the source code for the diversity reward function in Appendix A.3 and provided pseudo code and hyper parameter details in Appendix A.2. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See above. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix E 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [N/A] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 15 A Additional Experiment Details A.1 Environment We evaluate our method on a number of tasks. These tasks have different complexity in terms of control, dynamics and reward structure. • walker.walk (Control Suite): a 8-dimensional control task with a 23-dimensional observa- tion space, where the goal is to control the joints of a humanoid character to make it walk forward in a 2D plane. • walker.stand (Control Suite): a 8-dimensional control task with a 23-dimensional observa- tion space, where the goal is to control the joints of a humanoid character to make it stand up. • dog.walk (Control Suite): a 38-dimensional control task with a 223-dimensional observation space, where the goal is to control the joints of a dog character to make it walk forward in a 2D plane. • dog.stand (Control Suite): a 38-dimensional control task with a 223-dimensional obser- vation space, where the goal is to control the joints of a dog character to make it stand up. • BiPedal Walker: a 4-dimensional control task with a 24-dimensional observation space, where the goal is to control the joints of a bipedal-walker with a large payload in order to traverse a terrain from left to right as fast as possible. 16 A.2 Implementation details Distributed agent Acting and learning are decoupled, with multiple actors gathering data in parallel from a batched stream of environments, and storing their trajectories, including the latent variable z, in a replay buffer and queue from which the learner can sample a mixed batch of online and replay trajectories [44, 27]. The latent variable z is sampled uniformly at random in [1, n] during acting at the beginning of each new episode. The learner differentiates the loss function as described in Algorithm 1, and uses the optimizer (speciﬁed in Table 2) to update the network parameters and the Lagrange multipliers (speciﬁed in Table 3). Lastly, the learner also updates the and moving averages as described in Algorithm 1. Initialization When training begins we initialize the network parameters as well as the Lagrange multipliers: µi = σ−1(0.5), ∀i ∈ [2, n], where σ−1 is the inverse of the Sigmoid function µ1 = 1; and the moving averages: ˜vavg πi = ¯1/d, ∀i ∈ [1, n]. Here n is the number of policies and d is the dimension of the features φ. πi = 0., ∀i ∈ [1, n], ˜ψavg Bounded Lagrange multiplier To ensure the Lagrange multiplier does not get too large so as to increase the magnitude of the extrinsic reward and destabilize learning, we use a bounded Lagrange multiplier [48] by applying Sigmoid activation on µ so the effective reward is a convex combination of the diversity and the extrinsic rewards: r(s, a) = σ(µi)re(s, a) + (1 − σ(µi))ri d(s, a), and the objective for µ is σ(µ)(αv∗ e − dπ · re). Average state occupancy The empirical feature averages used for experiments in the main text are good, though imperfect due to the bias from samples before the policy mixes. In our experiments, however, since the mixing time for the DM Control Suite is much shorter than the episode length T , the bias is small (∼ 5%). Discounted state occupancy For a more scalable solution, as mentioned in Section 2, we can instead predict successor features using an additional network head as shown in Fig. 11a. Similar to value learning, we use V-trace [19] targets for training successor features. In discounted state occupancy case we also use the extrinsic value function of each policy vi e (Fig. 1a) to estimate dπ · re, instead of the running average ˜vavg πi . We show experimental results for this setup in Fig. 11b. Loss functions. Instead of learning a single value head for the combined reward, our network has two value heads, one for diversity reward and one for extrinsic reward. We use V-trace [19] to compute td-errors and advantages for each of the value heads using the ”vtrace td error and advantage” function implemented here https://github.com/deepmind/ rlax/blob/master/rlax/_src/vtrace.py. The value loss for each head is the squared (cid:96)2 loss d + td2 of the td-errors, and the combined value loss for the network is the sum of these two losses: td2 e. In addition to that, our network has a policy head that is trained with a policy gradient loss as implemented in https://github.com/deepmind/rlax/blob/master/rlax/_src/policy_ gradients.py). When training the policy, we combine the intrinsic and extrinsic advantages δ = σ(µi)δe + (1 − σ(µi))δd (see the Weight cumulants function in Appendix A.3) which has the same effect as combining the reward. However, we found that having two value heads is more stable as each value can have a different scale. The ﬁnal loss of the agent is a weighted sum of the value loss the policy loss and the entropy regularization loss, and the weights can be found in Table 2. Algorithm 1 also returns a Lagrange loss function, designed to force the policies to achieve a value that is at least α times the value of the ﬁrst policy (which only maximizes extrinsic reward), where α is the optimally ratio (Table 3). We update the Lagrange multipliers µ using the optimizer speciﬁed in Table 3 but keep the multiplier of the ﬁrst policy ﬁxed µ1 = 1. Lastly, Algorithm 1 also updates the moving averages. 17 Algorithm 1: Loss function . πi i=1 (cid:111)n s|xj (cid:9)n i=1, (cid:110) ˜ψavg s) is the probability assigned to aj s), vd(xj j=1 of size T, τj = (cid:8)zj, xj s in state xi s)} ← Network({τj}m s), δe(xj Parameters: Network parameters θ, Lagrange multipliers µ, moving averages (cid:8)˜vavg πi Data: m trajectories {τj}m µ(aj Forward pass: {π(aj s), ve(xj Compute extrinsic td-errors and advantages: tde(xj extrinsic reward rj Compute intrinsic reward: ri Compute intrinsic td-errors and advantages: tdd(xj intrinsic reward rj Combine advantages: δ(xj Weighted loss: s and intrinsic critic vd(xj s) s) + (1 − σ(µi))δd(xj s) = σ(µi)δe(xj s) s and extrinsic critic ve(xj s) s) from ˜ψavg πz , z, φj d(xj s with Eq. (6) or (8) j=1) s) ← V-trace with s) ← V-trace with s), δd(xj s, µ(aj s)(cid:9)T s, φj s, rj s|xj s|xj s, aj s=1 , where s by the behaviour policy µ(a|x). bv(tde(xj s)2 + tdd(xj s)2) + bπ log(π(aj s|xj s))δ(xj s) + bEntEntropy(π(aj s|xj s)) (cid:88) s,j Lagrange loss: Update moving averages: n (cid:88) i=1 σ(µi)(˜vavg πi − α˜vavg π1 ) πi = α˜vavg ˜vavg d πi + (1 − α˜vavg ˜vavg d )rt, ˜ψavg ˜ψavg πi = α d ˜ψavg πi + (1 − α ˜ψavg d )φt return Weighted loss, Lagrange loss, (cid:8)˜vavg πi (cid:9)n i=1, (cid:110) ˜ψavg πi (cid:111)n i=1 A.3 Functions 18 1 def intrinsic_reward ( phi , sfs , latents , attractive_power =3. , repulsive_power =0. , attractive_coeff =0. , target_d =1.) : """""" Computes a diversity reward using successor features . Args : phi : features [ tbf ]. sfs : avg successor features [ lf ] or predicted , discounted successor features [ tbfl ]. latents : [ tbl ]. attractive_power : the power of the attractive force . repulsive_power : the power of the repulsive force . attractive_coeff : convex mixing of attractive & repulsive forces target_d (\ ell_0 ) : desired target distance between the sfs . When attractive_coeff =0.5 , target_d is the minimizer of the objective , i . e . , the gradient ( the reward ) is zero . Returns : intrinsic_reward . """""" # If sfs are predicted we have 2 extra leading dims . if jnp . ndim ( sfs ) == 4: sfs = jnp . swapaxes ( sfs , 2 , 3) of avg sf ) compute_dist_fn = jax . vmap ( jax . vmap ( compute_distances ) ) matmul_fn = lambda x , y : jnp . einsum ( ’tbl , tblf - > tbf ’ , x , y ) # tbfl -> tblf ( to match lf shape elif jnp . ndim ( sfs ) == 2: compute_dist_fn = compute_dist ances matmul_fn = jnp . matmul else : raise ValueError ( ’ Invalid shape for argument ‘sfs ‘. ’) l , f = sfs . shape [ -2:] # Computes an tb lxl matrix where each row , corresponding to a latent , is a 1 hot vector indicating the index of the latent with the closest sfs dists = compute_dist_fn ( sfs , sfs ) dists += jnp . eye ( l ) * jnp . max ( dists ) ne a r e s t_ l a t en ts_ ma tr ix = jax . nn . one_hot ( jnp . argmin ( dists , axis = -2) , num_classes = l ) # Computes a [ tbl ] vector with the nearest latent to each latent in latents nearest_latents = matmul_fn ( latents , ne ar es t_l at en ts_ ma tr ix ) # Compute psi_i - psi_j psi_diff = matmul_fn ( latents - nearest_latents , sfs ) norm_diff = jnp . sqrt ( jnp . sum ( jnp . square ( psi_diff ) , axis = -1) ) / # tbf target_d c = (1. - attractive_coeff ) * norm_diff ** repulsive_power c -= attractive_coeff * norm_diff ** attractive_power reward = c * jnp . sum ( phi * psi_diff , axis = -1) / f return reward 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def l2dist (x , y ) : 45 """""" Returns the L2 distance between a pair of inputs . """""" return jnp . sqrt ( jnp . sum ( jnp . square ( x - y ) ) ) 46 47 48 def c ompute_distances (x , y , dist_fn = l2dist ) : 49 """""" Returns the distance between each pair of the two collections of inputs . """""" 50 return jax . vmap ( jax . vmap ( dist_fn , ( None , 0) ) , (0 , None ) ) (x , y ) Listing 1: Intrinsic Reward 19 1 def weight_cumulants ( lagrange , latents , extrinsic_cumulants , 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 i ntr i nsi c_cum ula n ts ) : """""" Weights cumulants using the Lagrange multiplier . Args : lagrange : lagrange [ l ]. latents : latents [ tbl ]. e xtr in si c_c u mul ants : [ tb ]. i ntr in si c_c u mul ants : [ tb ]. Returns : extrinsic reward r_e and intrinsic_reward r_d . """""" sig_lagrange = jax . nn . sigmoid ( lagrange ) l ate n t_s ig_la gra n ge = jnp . matmul ( latents , sig_lagrange ) # No diversity rewards for latent 0 , only maximize extrinsic reward i ntr i nsi c_cum ula n ts *= (1 - latents [: , : , 0]) return (1 - la tent_s ig_ lagrang e ) * intrins ic_cumulants + l ate n t_s ig_la gra n ge * extr insic_c um ulants Listing 2: Weight cumulants # l # tb 1 def lagrangian ( lagrange , r , optimality_ratio ) : 2 """""" Loss function for the Lagrange multiplier . 3 4 5 6 7 8 9 Args : lagrange : lagrange [ l ]. r : moving averages of reward [ l ]. optimality_ratio : [1]. """""" l_ = jax . nn . sigmoid ( lagrange ) return jnp . sum ( l_ * ( r - r [0] * optimality_ratio ) ) Listing 3: lagrange loss function A.4 Motion ﬁgures Our ”motion ﬁgures” were created in the following manner. Given a trajectory of frames that composes a video f1, . . . , fT , we ﬁrst trim and sub sample the trajectory into a point of interest in time: fn, . . . , fn+m. We always use the same trimming across the same set of policies (the sub ﬁgures in a ﬁgure). We then sub sample frames from the trimmed sequence at frequency 1/p: fn, fn+p, fn+2p . . . ,. After that, we take the maximum over the sequence and present this ”max” image. In Python for example, this simply corresponds to n=400, m=30, p=3 indices = range(n, n+m, p) im = np.max(f[indices]) This creates the effect of motion in single ﬁgure since the object has higher values than the background. A.5 Hyperparameters The hyperparameters in Table 2 are shared across all environments except in the BiPedal Domain the learning rate is set to 10−5 and the learner frames are 5 × 107. We report the DOMiNO speciﬁc hyperparameters in Table 3. 20 Hyperparameter Replay capacity Learning rate Learner frames Discount factor bEnt Entropy regularization weight bπ Policy loss weight bv Value loss weight Replay batch size Online batch size Sequence length Optimizer Value 5 × 105 10−4 2 × 107 0.99 0.01 1.0 1.0 600 6 40 RMSprop Table 2: General hyperparameters Hyperparameter Control Suite BiPedal Walker α Optimality ratio Lagrange initialization Lagrange learning rate Lagrange optimizer πi decay factor α˜vavg ˜vavg d ˜ψavg ˜ψavg πi decay factor α d 0.9 0.5 10−3 Adam 0.9 0.99 0.7 0.5 10−3 Adam 0.999 0.9999 Table 3: DOMiNO hyperparameters B Additional Experiment Results B.1 Motion ﬁgures We now present motion ﬁgures, similar to Fig. 1b, but in other domains (see Fig. 6-9). The videos, associated with these ﬁgures can be found in a separate .zip ﬁle. Each ﬁgure presents ten polices discovered by DOMiNO and their associated rewards (in white text) with the repulsive objective and the optimality ratio set to 0.9. As we can see, the policies exhibit different gaits. Next to each ﬁgure, we also present the distances between the expected features of the discovered policies measured by the (cid:96)2 norm. In addition, in each row i we use a dark black frame to indicate the the index of the policy with the closest expected features to πi , i.e., in the i-th row we highlight the j-th column such that j = j∗ i = arg minj(cid:54)=i ||ψi − ψj||2 2. 21 Figure 5: QD in walker.stand. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2 norm between the Successor features of the policies. Figure 6: QD in walker.walk. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2 norm between the Successor features of the policies. Figure 7: QD in dog.stand. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2 norm between the Successor features of the policies. Figure 8: QD in dog.walk. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2 norm between the Successor features of the policies. 22 B.2 Additional Quality Diversity Results We now present additional experimental results evaluating the trade-off between quality and diversity using the scatter plots introduced in Section 4.1. y-axis shows the episode return while the diversity score, corresponding to the Hausdorff distance (Eq. (5)), is on the x-axis. The top-right corner of the diagram represents the most diverse and highest quality policies. Each ﬁgure presents a sweep over one or two hyperparameters and we use the color and a marker to indicate the values. In all of our scatter plots, we report 95% conﬁdence intervals, corresponding to 5 seeds, which are indicated by the crosses surrounding each point. Quality Diversity: walker.walk In Fig. 9 we show experimental results for DOMiNO in the walker.walk domain. Consistent with Fig. 2 which shows similar results on walker.stand, we show that our constraint mechanism is working as expected across different set sizes and optimality ratios across different tasks. Figure 9: QD Scaling results on walker.walk task. Left: Number of policies vs. optimality ratio in walker.walk with the repulsive reward and (center) with the VDW reward. Right: Optimality ratio vs. VDW target distance (cid:96)0. Quality Diversity: SMERL vs DOMiNO In Fig. 10 we show further experimental results in walker.stand for SMERL in comparison to DOMiNO. When SMERL is appropriately tuned (here for the 10 policies conﬁguration), it can ﬁnd some solutions along the upper-right QD border; however we ﬁnd that the best cd does not transfer to other conﬁgurations. The choice of cd that enables the agent to ﬁnd a set of 10 diverse policies produces sets without diversity for any other set size. Figure 10: Scaling SMERL (left) vs. DOMiNO (right) on Walker.Stand. Set size is indicated with marker, color corresponds to optimality ratio α. The cd for SMERL is set to 0.5, which was tuned using a set size of 10 policies (see 3, left). This choice does not scale well to any other set size, where regardless of optimality ratios, all policies only optimize for extrinsic reward, at the expense of diversity. Discounted State Occupancy We run the same experiments reported in Fig. 2 with DOMiNO’s Lagrangian method and report the results in Fig. 11b. As can be observed, using predicted discounted features does not make any signiﬁcant difference in performance. Since the mixing time for the DM Control Suite is much shorter than the episode length T , the bias in the empirical feature averages is small. 23 (a) (b) Figure 11: (a) DOMiNO with a discounted state occupancy. An additional network head is trained to predict successor features ψγ, which are used instead of the average features ψavg to compute the diversity reward. The discounted, extrinsic value is used as a constraint instead of the averaged rewards. Dashed lines signify training objectives. (b) Number of policies vs. optimality ratio in walker.stand with DOMiNO, consistent with Fig. 2. C Limitations Diversity increasing by decreasing α Inspecting Fig. 9, Fig. 11b and Fig. 2. we can observe that the diversity score increases for lower optimality ratios. Recall that the optimality ratio α speciﬁes a feasibility region in the state-occupancy space (the set of all α-optimal policies). Thus, the size of this space increases as α decreases, and we observe more diverse sets for smaller values of α. This intuition was correct in most of our experiments, but not always (e.g., Fig. 9). One possible explanation is that the Lagrange multipliers solution is seeking for the lowest value of λ that satisﬁes the constraint (so that we can get more diversity), i.e., it ﬁnds solutions that satisfy the constraint almost with equality: vi e ). The size of the level sets e ≥ αv∗ e ) do not necessarily increase with lower values of α (while the feasibility sets vi (vi e do). Another explanation is that in walker.walk (Fig. 9) it might be easier to ﬁnd diverse walking (e.g., α = 0.9) than diverse “half walking” (e.g., α = 0.5). This might be explained by “half walking” being less stable (it is harder to ﬁnd diverse modes for it). e (instead of vi e > αv∗ e ∼ αv∗ e = αv∗ Features Another possible limitation of our approach is that diversity is deﬁned via the environment features. We partially addressed this concern in Section 4.2 where we showed that it is possible to learn QD policies with our approach using the embedding of a NN as features. In future work we plan to scale our approach to higher dimensional domains and study which auxiliary losses should be added to learn good representations for diversity. D Additional K-shot experiments D.1 Control suite Next, we report additional results for K-shot adaptation in the control suite. In Fig. 12 we report the absolute values achieved by each method obtained (in the exact same setup as in Fig. 4). That is, we report rmethod for each method (instead of rmethod/rbaseline as in Fig. 4). Additionally, we report rbaseline, which is the ”Single policy baseline” (blue) in Fig. 12. Inspecting Fig. 12, we can see that all the methods deteriorate in performance as the magnitude of the perturbation increases. However, the performance of DOMiNO (orange) deteriorates slower than that of the other methods. We can also see that the performance of the no diversity baseline is similar when it learns 10 policies (red) and a 24 U p d a t e c o n s t r a i n t & L a g r a n g e MLP Torso MLP D i v e r s i t y R e w a r d single policy (blue), which indicates that when the algorithm maximize only the extrinsic reward, it ﬁnds the same policy again and again with each of the 10 policies. Figure 12: K-shot adaptation in Control Suite, similar to Figure 4, but reporting absolute rather than relative returns. Next, we inspect a wider range of hyper parameters for the SMERL and Multi-Objective methods. Concretely, for DOMiNO and SMERL α = 0.9, for SMERL cd ∈ [0.5, 1, 2, 5] and for Multi- Objective ce ∈ [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9] and all methods are trained with 10 policies. These values correspond to the values that we considered in Fig. 3. Inspecting Fig. 13 we can see that the best methods overall are DOMiNO and SMERL (with cd = 1). We can also see that DOMiNO and SMERL consistently outperform the multi-objective baseline for many hyper parameter values. This is consistent with our results in Fig. 3 which suggest that the Multi-Objective method tends to be either diverse or high performing and fails to capture a good balance in between. Lastly, it is reasonable that SMERL and DOMiNO perform similar since they are both valid solutions to the same CMDP. However, SMERL comes an with additional hyper parameter cd that may be tricky to tune in some situations. For example, trying to tune cd based on the results in the vanilla domain (picking the upper-right most point in Fig. 3) led us to choose cd = 0.5 for SMERL, instead of 1. The Lagrange multipliers formulation in DOMiNO does not have this challenge as it does not have an extra hyper parameter. Figure 13: K-shot in Control Suite, similar to Figure 4, but reporting a wider range of hyper parameters for SMERL and Multi-Objective. D.2 BipedalWalker 2, h1 For the BipedalWalker environment, we either perturb the morphology or the terrain. To perturb the morphology, we follow [25] and specify a set of re-scaling factors. Speciﬁcally, each leg is made up of two rectangles, with pre-deﬁned width and height parameters: leg1 = ((w1 1)), leg2 = ((w1 2)). To generate a perturbed morphology, we deﬁne a scaling range [0, η] withing which we uniformly sample scaling factors (cid:96)j i ∼ [−η, η], for i = 1, 2 and j = 1, 2. A perturbed environment is deﬁned by re-scaling the default parameters: (cid:102)leg1 = (((1 + (cid:96)1 1, (1 + ν1 2 )h2 1 )h2 1 )h1 1))). The values for this perturbations can be found in Table 4. 1))), and (cid:102)leg2 = (((1+(cid:96)1 1)w1 1, (1+ν2 1), ((1+(cid:96)2 1), ((1+(cid:96)2 1, (1+ν2 1, (1+ν1 1), (w2 2), (w2 1)w2 2)w2 2)w1 i , νj 1, h2 1, h1 2, h2 2 )h1 25 s n r u t e r n a e M s n r u t e r n a e M 1000 d n a t S 400 1000 l k a W 200 200 200 0 0 1 0 2 0 3 200 0 4 0 . 2 5 2 Motor failure (duration) 200 0 . 4 0 . 3 5 7 0 . 3 0 . 3 0 . 2 Thigh length 600 400 600 200 5 0 . 4 0 . 4 0 . 3 Torso length 5 0 . 5 0 . 1 3 . 0 2 . 0 1 . 0 Joint damping 4 . 0 0 . 7 2 0 . 1 0 . 0 Contact friction 0 . 0 0 Single policy baseline DOMiNO SMERL No diversity Multi-Objective 5 1 0 0 . 0 1000 s n r u t e r n a e M d n a t S 200 1000 s n r u t e r n a e M l k a W 200 200 200 400 600 200 200 200 200 0 0 1 0 2 0 3 0 4 0 . 2 5 2 7 0 . 3 0 . 2 5 0 . 3 0 . 4 0 . 3 5 0 . 3 0 . 4 5 0 . 4 0 . 5 0 . 1 1 . 0 2 . 0 3 . 0 4 . 0 0 . 7 0 . 1 2 0 . 0 5 0 0 . 0 1 0 0 . 0 Motor failure (duration) Thigh length Torso length Joint damping Contact friction Single policy baseline DOMiNO Multi Objective (0.1) Multi Objective (0.2) Multi Objective (0.3) Multi Objective (0.4) Multi Objective (0.5) Multi Objective (0.6) Multi Objective (0.9) Multi Objective (0.7) SMERL ( =1.0) SMERL ( =2.0) SMERL ( =5.0) SMERL ( =0.5) Perturbation type Perturbation scale parameter values (η) Morphology Stumps (height, width) Pits (width) Stairs (height, width) 0., 0.10, 0.15, 0.20, 0.25, 0.30, 0.35 0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. 0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. 0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. Table 4: Bipedal perturbation scale values For terrain changes, we selectively enable one of three available obstacles available in the OpenAI Gym implementation: stumps, pits, or stairs. For each obstacle, we specify a perturbation interval [0, η]. This interval determines the upper bounds on the obstacles height and width when the environment generates terrain for an episode. For details see the “Hardcore” implementation of the BiPedal environment. Note that for stairs, we ﬁxed the upper bound on the number of steps the environment can generate in one go to 5. To evaluate adaptation, we ﬁrst train 10 agents independently on the “BiPedalwalker-v3” environment, which only uses a ﬂat terrain. To evaluate the trained agents we sample random perturbations of the environment. Speciﬁcally, for each type of perturbation (morphology, pits, stumps, stairs) and for each value of the scale parameter η, we randomly sample 30 perturbations. We then run each option for 40 episodes; adaptation takes the form of using the ﬁrst 10 episodes to estimate the option with highest episode return, which is then used for evaluation on the remaining 30 episodes. Figure 14: K-shot adaptation in BiPedal walker Fig. 14 shows that, while performance degrades as the morphology is deformed, DOMiNO exhibits greater adaptability as evidenced by less severe degradation of performance. In terms of morphology, we ﬁnd a gradual decline in performance as we increase the degree of deformation. Similar to the Control Suite, diversity is beneﬁcial and helps the agent adapt while not being impervious to these changes. In terms of terrain perturbations, these have a more abrupt impact on the agent’s performance. While diversity does not prevent a signiﬁcant drop in performance, it is still beneﬁcial when adapting to stumps and pits and does not negatively impact performance in the case of stairs. E Computing Infrastructure We run our experiments using a distributed infrastructure implemented in JAX [12]. Each run took approximately 10 hours to complete. The computing infrastructure is based on an actor-learner decomposition [19], where multiple actors generate experience in parallel, and this experience is channelled into a learner. It allows us to run experiments in two modalities. In the ﬁrst modality, following [19], the actors programs are distributed across multiple CPU machines, and both the stepping of environments and the network inference happens on CPU. The data generated by the actor programs is processed in 26 400 r e k a W l l No diversity With diversity a d e P B i -100 0.0 0.1 0.2 0.3 Morphology scale 0.0 0.3 0.6 0.9 Stump size 400 r e k a W l l a d e P B i -100 0.0 0.3 0.6 0.9 Pit size 0.0 0.3 0.6 0.9 Stairs step size batches by a single learner using a GPU. Alternatively, both the actors and learners are co-located on a single machine, where the host is equipped with 56 CPU cores and connected to 8 TPU cores [30]. To minimize the effect of Python’s Global Interpreter Lock, each actor-thread interacts with a batched environment; this is exposed to Python as a single special environment that takes a batch of actions and returns a batch of observations, but that behind the scenes steps each environment in the batch in C++. The actor threads share 2 of the 8 TPU cores (to perform inference on the network), and send batches of ﬁxed size trajectories of length T to a queue. The learner threads takes these batches of trajectories and splits them across the remaining 6 TPU cores for computing the parameter update (these are averaged with an all reduce across the participating cores). Updated parameters are sent to the actor’s TPU cores via a fast device to device channel, as soon as the new parameters are available. This minimal unit can be replicates across multiple hosts, each connected to its own 56 CPU cores and 8 TPU cores, in which case the learner updates are synced and averaged across all cores (again via fast device to device communication). 27",1.0
"Avoid Overfitting User Specific Information in Federated Keyword
  Spotting","[{'href': 'http://arxiv.org/abs/2206.08864v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2206.08864v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-17 16:05:35,,"Avoid Overﬁtting User Speciﬁc Information in Federated Keyword Spotting Xin-Chun Li1, Jin-Lin Tang1, Shaoming Song2, Bingshuai Li2, Yinchuan Li2, Yunfeng Shao2, Le Gan1, De-Chuan Zhan1 1State Key Laboratory for Novel Software Technology, Nanjing University 2Huawei Noah’s Ark Lab {lixc, tangjl}@lamda.nju.edu.cn, ganle@nju.edu.cn, zhandc@nju.edu.cn, {shaoming.song, libingshuai, liyinchuan, shaoyunfeng}@huawei.com 2 2 0 2 n u J 7 1 ] G L . s c [ 1 v 4 6 8 8 0 . 6 0 2 2 : v i X r a Abstract Keyword spotting (KWS) aims to discriminate a speciﬁc wake- up word from other signals precisely and efﬁciently for different users. Recent works utilize various deep networks to train KWS models with all users’ speech data centralized without consid- ering data privacy. Federated KWS (FedKWS) could serve as a solution without directly sharing users’ data. However, the small amount of data, different user habits, and various ac- cents could lead to fatal problems, e.g., overﬁtting or weight divergence. Hence, we propose several strategies to encourage the model not to overﬁt user-speciﬁc information in FedKWS. Speciﬁcally, we ﬁrst propose an adversarial learning strategy, which updates the downloaded global model against an overﬁt- ted local model and explicitly encourages the global model to capture user-invariant information. Furthermore, we propose an adaptive local training strategy, letting clients with more train- ing data and more uniform class distributions undertake more local update steps. Equivalently, this strategy could weaken the negative impacts of those users whose data is less quali- ﬁed. Our proposed FedKWS-UI could explicitly and implicitly learn user-invariant information in FedKWS. Abundant exper- imental results on federated Google Speech Commands verify the effectiveness of FedKWS-UI. Index Terms: keyword spotting, federated learning, data het- erogeneity, user-invariant 1. Introduction Deep learning has been successfully applied to automatic speech recognition (ASR) [1, 2], facilitating the emergence of intelligent voice assistants (e.g., Amazon Alexa). To wake up the smart assistant, some predeﬁned keywords (e.g., “Alexa”) need to be identiﬁed precisely from users’ speech recordings, i.e., keyword spotting (KWS) [3, 4]. This identiﬁcation process must be efﬁcient to complete, and the utilized models should have minimal memory footprint. Furthermore, the KWS pro- cess should be robust to users with various accents or preferred spoken words. Recent works utilize various deep networks for KWS [4, 5, 6, 7, 8]. These methods take a data centralized training style based on the publicly available benchmark such as Google Speech Commands [9]. However, there may be signiﬁcant pri- vacy implications in sharing users’ audio recordings, which re- quires a data decentralized training style for privacy protec- Supported by National Natural Science Foundation of China (Grant No. 41901270), NSFC-NRF Joint Research Project under Grant 61861146001, and Natural Science Foundation of Jiangsu Province (Grant No. BK20190296). Thanks to Huawei Noah’s Ark Lab Net- MIND Research Team for funding this research. De-Chuan Zhan is the corresponding author. Email: zhandc@nju.edu.cn tion. Federated learning (FL) [10, 11] has been effectively ap- plied for communication efﬁcient decentralized training with basic privacy protection. Although FL could be directly ap- plied to decentralized KWS training, the non-independent and identically distributed data (non-i.i.d. data) poses many chal- lenges [12, 13]. Non-i.i.d. in KWS refers to the fact that some users only own a small amount of data (i.e., quantity skew), users tend to use different spoken words (i.e., label distribution skew), and users usually have accents (i.e., feature distribution skew). This paper investigates FedKWS on Google Speech Com- mands [9] with several popular network architectures. Com- pared with centralized training, we observe a signiﬁcant per- formance degradation in FedKWS due to non-i.i.d. data. In fact, the small amount of data and the distribution skew problem make the downloaded global model easily overﬁt user-speciﬁc information. For example, the feature extractor mistakenly takes a user’s accent as an important factor, or the classiﬁca- tion layer is biased towards a user’s commonly spoken words. To solve these challenges and enhance the generalization per- formance of the federated model, we propose several strategies to avoid the local model overﬁtting user-speciﬁc information. 2. Related Works Our work is closely related to keyword spotting (KWS) [4, 3] and federated learning (FL) [10, 11, 14]. Current works for- mulate KWS as a classiﬁcation problem, aiming to identify whether a short speech recording is a speciﬁc word, silence, or unknown. Considering the success of deep learning, CNN has been applied to KWS [4]. Depth-separable CNN (DSCNN) [5] is applied to obtain the goal of small footprint memory, and residual network (ResNet) [7] is utilized to enhance perfor- mances. Recurrent neural networks with multi-head atten- tion (MHAttRNN) [6, 8] and varieties of transformers (Trans- former) [8, 15] have also been applied to KWS and obtain SOTA results. Some other advanced techniques in deep learning have also been veriﬁed helpful in KWS [16]. FL has also been ap- plied to KWS for decentralized training [17, 18]. [17] conducts extensive experiments of FedAvg [10] on “Hey Snips” dataset and uses an adaptive averaging strategy for global model aggre- gation as done in [19]. The work [18] investigates data aug- mentation and distillation in FedKWS for overcoming resource constraints and example labeling. FL studies have also been presented in ASR [20, 21, 22]. Compared with these studies, we primarily focus on the non-i.i.d. data challenge in FedKWS and propose a novel method to focus on extracting user-invariant in- formation. We investigate our methods with various network architectures and show that our approach is universal. 3. Background of Federated Learning (cid:80) k∈St ˆψk (cid:80)K FedAvg [10]: Suppose we have K clients and each client owns a data distribution Dk = P k(x, y), k ∈ [K]. FL aims to op- k=1 pkL(Dk; ψ), where ψ denotes the global timize minψ parameters, pk denotes the weight of each client. FedAvg [10] solves this problem via multiple communication rounds of lo- cal and global procedures. During local procedure, a partial set of clients St download the global model ψt and update it on their local data for multiple steps. During global proce- dure, the server collects these updated local models (denoted as ˆψk t , k ∈ St) and aggregates them via parameter averaging, i.e., ψt+1 ← 1 t . t denotes the communication round. |St| These two procedures will iterate T rounds until convergence. Non-I.I.D. Data: The users’ data in FL are often naturally heterogeneous, e.g., the speech data in Google Speech Com- mands [9] are collected from users with various accents. As de- clared in [12], the local update direction will diverge a lot from the global one due to non-i.i.d. data, making the model aggre- gation inaccurate. FedOpt [19] utilizes an adaptive optimization strategy on the server instead of a simple parameter averaging. FedRS [23] speciﬁes the challenge of label shift across clients and proposes restricted softmax as the solution. FedProx [13] and FedMMD [24] add regularization to prevent local models from being updated too away, which could decrease the weight divergence for better aggregation. Although some FL methods (e.g., FedProx [13], FedDyn [25], MOON [26]) could also elab- orate a regularization effect during local procedures, they only stay on the parameter or the intermediate feature levels. By contrast, we adversarially update the global model against an overﬁtted local model and regularize the local procedure on the functional level. Furthermore, we design an adaptive local train- ing procedure from the system scheduling level. 4. Proposed Methods This section proposes two strategies to prevent the global model from overﬁtting user-speciﬁc information (e.g., accents or fa- vorite spoken words) in FedKWS. Adversarial Learning against Overﬁtted models (ALO): Clients update the downloaded global model on their data dur- ing the local procedure, which could overﬁt some user-speciﬁc information. Speciﬁcally, the local data distribution P k(x, y) may diverge signiﬁcantly from the global data distribution. Ac- cording to some previous works [27, 28, 29], the lower/higher layers of a neural network tend to be inﬂuenced signiﬁcantly by feature/label distribution skew, i.e., various P k(x) or P k(y). FedKWS simultaneously faces these two kinds of distribution skew (e.g., accents and favorite spoken words), making the complete model biased towards a speciﬁc user during the lo- cal procedure. Hence, we must regularize the local training from the functional perspective instead of focusing on speciﬁc neural network layers. We resort to private-shared models and adversarially update the global model (shared among users) against overﬁtted local models (private for each user). Private- shared models are utilized in some recent FL solutions [30, 31]. Speciﬁcally, we build private models ψk p , k ∈ [K] for each client. We ﬁrst train private models with the cross-entropy loss L(Dk; ψk c=1 I{yi = c} log[fp(xi)]c], where I{·} is the indicator function and fp(·) is the prediction function based on private model ψk p that outputs a probability distribution. After abundant training steps, we expect this pri- vate model to overﬁt user-speciﬁc data information. Then, we p ) = Exi,yi∼Dk [− (cid:80)C Figure 1: Left: data heterogeneity in federated Google Speech Commands. We only plot 20 clients (users) in task 35. Right: number of samples and class distribution entropy of each client (user) in task 12 and 35 (each point shows a client). train the global model with the following loss: (cid:34) Lls = Exi,yi − C (cid:88) [(1 − µ)I{yi = c} + µ/C] log[f (xi)]c (cid:35) , c=1 Ladv = − (cid:124)(cid:123)(cid:122)(cid:125) negative Exi,yi (cid:34) − C (cid:88) [fp(xi)]c log[f (xi)]c (cid:35) , c=1 L(Dk; ψk) = Lls(Dk; ψk) + λLadv(Dk; ψk), (1) (2) (3) where we omit the communication round index t and some other symbols for simpliﬁcation. fp(·) represents the func- tion of the overﬁtted private model while f (·) for the down- loaded global model. Eq. (2) could be seen as “negative distil- lation”, which could push the global model’s prediction f (xi) away from overﬁtted areas. Eq. (2) follows the formula of dis- tillation [32, 33, 34] but works signiﬁcantly different. Label smoothing in Eq. (1) could also regularize the global model not be too over-conﬁdent on a speciﬁc user’s data. We investigate the hyperparameters of µ and λ in ablation studies. Adaptive Local Training (ALT): Due to data heterogeneity, both amount imbalance and class imbalance could occur in clients’ data. The former implies that different clients may own various numbers of training samples. The second one refers to that label distributions may diverge across clients. These two types of imbalance on Google Speech Commands [9] are shown in Figure 1. Intuitively, few training samples could lead to overﬁtting, and imbalanced data could bias the model towards identifying a user’s favorite words. Hence, we en- courage clients who own more training data and more uni- form class distributions to undertake more local updates. For- mally, in FedAvg [10], every selected client uniformly takes E local training steps without considering the data quality. Assume the kth client owns nk training samples and the class distribution is qk ∈ RC with (cid:80)C c=1 qk,c = 1 and qk,c ≥ 0, ∀c. C is the total number of classes. We cal- culate the normalized amount of training samples as nk = nk/ maxK j=1 nj ∈ [0, 1], and the normalized class entropy as Statistics (C=12) 2 0 8 1 1 0 0 6 0 0 4 0 1 4 1 Number of Samples Statistics (C=35) 0 0 5 5 0 0 0 0 2 2 1 1 Client-Class Distribution (C=35) 0 5 1 5 1 0 2 0 0 2 y p o r t n E n o i t u b i r t s i D s s a C l y p o r t n E n o i t u b i r t s i D s s a C l 2.49 2.0 1.5 1.0 0 0 5 3.56 3.0 2.5 2.0 1.5 1.0 1 0 5 10 15 20 25 30 34 x e d n I s s a C l Sampled 20 Clients Number of Samples 3.49 2.67 1.85 1.03 0.21 4.70 3.56 2.43 1.29 0.15 6 1 3 Table 1: Detail information of federated Speech Commands. Table 3: Comparisons on FA and FR rate. The lower the better. C 12 35 K 2,234 2,434 N 45.6k 105.5k Avg.nk Max.nk 20.4 43.3 141 316 M 4.9k 11.0k FedAvg [10, 18] 0.27 5.03 FedOpt [19, 17] 0.32 3.19 FedKWS-UI 0.23 2.78 FA FR Table 2: Detail of networks and centralized training results. Num.of.Params Centralized Acc. C = 12 C = 35 C = 12 C = 35 96.95 169K 97.05 228K 97.31 238K 97.14 232K 97.19 97.31 97.89 96.21 173K 232K 239K 234K DSCNN [5] MHAttRNN [6] ResNet [7] Transformer [8] ek = (− (cid:80)C c=1 qk,c log qk,c)/ log C ∈ [0, 1]. Then we calcu- late the harmonic mean of nk and ek, i.e., rk = 2nkek/(nk + ek). We use rk ∈ [0, 1] to measure clients’ utility in FL, and we heuristically let clients with larger rk contribute more to FL. That is, we reallocate the computation resources among clients via allowing the kth client take on r0 ∗ rk ∗ E gradient steps, where the determination of r0 should satisfy (cid:80)K k=1 r0∗rk∗E ≈ K ∗ E for conservation. Easily, r0 = K/ (cid:80)K k=1 rk. Although the computation ability of clients should also be considered, we focus on non-i.i.d. data in this work and leave it as future work. 5. Experiments Datasets: We name the proposed method as “Federated KWS with User-Invariant information” (FedKWS-UI), and investi- gate it on Google Speech Commands [9]1 (recommended by FedScale [35]) to identify whether a 1s-long speech recording is a word, silence, or unknown. The benchmark contains two tasks with 12 classes (10 words, silence, unknown) and 35 classes (35 words). The two tasks contain 2,234 and 2,434 users. We split the data into corresponding clients with each user as a client. The number of total training samples (N ), the training samples of each client on average (Avg.nk), the number of test samples (M ) are listed in Table 1. The class distributions of randomly selected 20 clients in task C = 35 are shown in left of Fig- ure 1. Larger circles correspond to more samples. The train and test data is split via the provided lists in Google Speech Commands. We extract 40 MFCC features for each 30ms win- dow frame with a stride of 10ms. We also follow the settings in Google Speech Commands: performing random time-shift of Y ∼ [−100, 100] milliseconds and adding 0.1 volume back- ground noise with a probability of 0.8. Networks and Centralized Training: We investigate vari- ous network architectures and moderately modify them to keep nearly the same number of parameters. We use DSCNN [5] with 172 channels, MHAttRNN [6, 8] with 4 heads and 80 hid- den neurons, ResNet [7] with 15 layers and 45 channels in each basic block, Transformer [8] with 4 layers and a model dimen- sion of 96. We ﬁrst use these networks for centralized train- ing. For DSCNN, MHAttRNN and ResNet, we utilize SGD optimizer with momentum 0.9, and we vary the learning rate in {0.1, 0.05, 0.03, 0.01} and select the best result. For Trans- former, we utilize AdamW optimizer and vary learning rate in 1https://pytorch.org/audio/stable/datasets.html {0.005, 0.002, 0.0008}. We set batch size as 128. The number of network parameters and the accuracies on test data are shown in Table 2. We do not obtain SOTA results via Transformer be- cause we only use 4 layers with 0.23M parameters while [8] uses a network with up to 5.4M parameters. FedKWS: For FedKWS, we split the training data in Google Speech Commands via the provided user IDs. The test data is still used to evaluate the generalization ability of the aggre- gated model. We plot the statistics of clients’ number of sam- ples and class distribution entropy at the right of Figure 1. We compare FedKWS-UI with FedAvg [10] (used in [18]), Fed- Prox [13], FedMMD [24], FedOpt [19] (used in [17]). For all methods, we use a batch size of 32, local training steps E = 50 and run 300 rounds. We also vary the learning rate as aforementioned and take the best one for comparison. Addi- tionally, for FedProx and FedMMD, we vary the regularization coefﬁcient in {0.0001, 0.001, 0.01}. For FedOpt, we vary the global optimizer in {SGD, Adam} and the global learning rate in {1.0, 0.1} and {0.001, 0.0001}, respectively. For FedKWS- UI, we use r0 = 3.5, 5.0 for C = 12, 35, and show r0 ∗ rk values of all clients via the shades of color at the right of Fig- ure 1. The max and min values are shown at the color bar, and the top-right points (clients) tend to have larger r0 ∗ rk. We uti- lize µ = 0.2 and λ = 0.001 in FedKWS-UI (Eq. (1), Eq. (3)). We record the accuracy on the global test set every 3 rounds and plot the convergence curves in Figure 2. First, we can clearly observe that the decentralized performances drop a lot compared with centralized training. For example, all of the compared methods could only obtain accuracy as high as 86.39 on task 12, far away from the centralized training (97.89). Then, comparing the network architectures, we could ﬁnd that MHAt- tRNN tends to obtain higher performances while Transformer performs worst. We guess that MHAttRNN could be more ro- bust to the random time-shift because it directly computes the sequential information, and the attention mechanism could pre- cisely capture the important signals. Furthermore, FedProx and FedMMD add regularization during local training procedures on the parameter and intermediate features, which perform not so well. Overall, FedKWS-UI could lead to better results on all of these architectures, particularly on ResNet and Trans- former, verifying the versatility of our methods. Signiﬁcantly, FedKWS-UI surpasses all compared methods by a large margin on task 12 with DSCNN, ResNet, and Transformer. For exam- ple, FedKWS-UI could boost the Transformer performances on task 12 from 72.71 to 79.06. We also evaluate the false accept (FA) and false reject (FR) rate as done in [17, 18]. In task 12, we take the 10 words as positive classes and average their FA rates, while the silence and unknown as negative classes. We do not adjust the prediction conﬁdence threshold to control the FA and directly report FA and FR with the predictions. We use DSCNN and calculate the FA and FR rate of the ﬁnal aggregated model. We show the results in Table 3. We ﬁnd that FedKWS-UI could obtain fewer false alarms/rejections. Ablation Studies: We investigate the effects of components in FedKWS-UI. First, we set µ = 0 and λ = 0 to omit the part of adversarial learning against overﬁtted models (ALO) Figure 2: Comparison results on federated Google Speech Commands. Rows show the results on task 12 and 35, and columns show results of four utilized networks. The legends also show the average accuracy of the ﬁnal 5 communication rounds. sults on task 35 to further show the plausibility and advantage of the proposed strategies. Speciﬁcally, we ﬁrst train a well- performed KWS model (θ0) on the centralized training set (test accuracy up to 93.0%). Then, we respectively update θ0 on an inferior and qualiﬁed user’s data for 20 epochs. The infe- rior user owns only 100 samples and the classes are imbalanced (i.e., the bottom-left user shown in the right part of Figure 1), while the qualiﬁed user owns about 250 samples and the classes are more balanced (i.e., the top-right user shown in the right part of Figure 1). The updated models are denoted as θ1 and θ2. Then, we plot the performance landscape of the interpolation θ0 + γ1(θ1 − θ0) + γ2(θ2 − θ0) within the grid space where γ1 ∈ [−0.1, 1.1], γ2 ∈ [−0.1, 1.1]. The left part of Figure 4 shows that updating θ0 on the qualiﬁed user’s data keeps the generalization ability of the global model while the result on the inferior user’s data becomes worse. Hence, it is rational that our proposed ALT encourages qualiﬁed users to contribute more to FedKWS. Additionally, for the inferior user, we utilize the pro- posed ALO to train another model ˆθ1 against the overﬁtted θ1, and ˆθ1 performs better as shown on the right of Figure 4. The interpolation landscape along the inferior user’s data becomes smoother with ALO, beneﬁting the model aggregation proce- dure in FL. This veriﬁes the advantage of the proposed ALO. Overall, FedKWS-UI could enhance the generalization ability of the federated model even with few or skewed samples. 6. Conclusion We investigate popular networks for FedKWS, where the data heterogeneity leads to signiﬁcant performance degradation compared with centralized training. We propose to learn user- invariant information via adversarial learning against overﬁtted local models and a computation re-allocation strategy named adaptive local training. These two strategies could avoid overﬁt- ting user-speciﬁc information during local training and facilitate model aggregation. Experimental results verify the superiori- ties of our proposed FedKWS-UI. Future works will extend this work to streaming KWS [6] and utilize differential privacy [36] to satisfy stricter privacy requirements. Figure 3: Ablation studies of only using ALT (left) and the hyper-parameters in ALO (µ (middle), and λ (right)). and only use adaptive local training (ALT). We record the re- sults using four networks on task 35 at the left of Figure 3. We ﬁnd that only using adaptive local training could still per- form well on MHAttRNN and Transformer, while it works worse on DSCNN and especially on ResNet. Hence, it is still necessary to utilize the adversarial learning to improve perfor- mances further. Then, we vary µ ∈ {0.0, 0.1, 0.2, 0.3, 0.5} and λ ∈ {0.0, 0.0001, 0.001, 0.01, 0.1} correspondingly, studying the effects of label smoothing and adversarial loss in ALO. We investigate the task C = 12 with DSCNN and ResNet. The results are shown at the middle and right of Figure 3. Utilizing label smoothing could almost lead to better performances, and setting µ around 0.2 is a better choice. Similarly, λ = 0.001 is recommended for the proposed adversarial loss, and a larger λ (e.g., 0.1) could be harmful. Figure 4: Visualization of the plausibility and advantage of the proposed strategies. Visualization Analysis: We then present some visualization re- y c a r u c c A l a b o G l 0.8 0.6 0.4 0.2 0.8 0.6 0.4 y c a r u c c A l a b o G l C=12, DSCNN C=12, MHAttRNN C=12, ResNet C=12, Transformer 0.8 0.7 0.6 0.5 FedAvg:82.85 FedProx:85.51 FedMMD:83.74 FedOpt:85.50 FedKWS-UI:87.29 0.8 0.6 0.4 FedAvg:86.20 FedProx:86.39 FedMMD:85.92 FedOpt:86.38 FedKWS-UI:86.57 0.8 0.6 0.4 0.2 FedAvg:83.48 FedProx:83.93 FedMMD:83.90 FedOpt:81.30 FedKWS-UI:86.23 FedAvg:71.17 FedProx:72.15 FedMMD:72.71 FedOpt:71.12 FedKWS-UI:79.06 3 60 120 180 240 300 3 60 120 180 240 300 3 60 120 180 240 300 3 60 120 180 240 300 C=35, DSCNN C=35, MHAttRNN C=35, ResNet C=35, Transformer 0.8 0.6 0.4 FedAvg:82.59 FedProx:81.44 FedMMD:82.79 FedOpt:84.23 FedKWS-UI:84.51 0.8 0.6 0.4 0.2 FedAvg:79.80 FedProx:83.10 FedMMD:83.64 FedOpt:82.83 FedKWS-UI:84.60 0.6 0.4 0.2 FedAvg:79.05 FedProx:75.78 FedMMD:80.07 FedOpt:79.33 FedKWS-UI:80.50 FedAvg:70.24 FedProx:70.07 FedMMD:66.78 FedOpt:54.56 FedKWS-UI:74.75 3 120 60 Communication Round 180 240 300 3 120 60 Communication Round 180 240 300 3 120 60 Communication Round 180 240 300 3 120 60 Communication Round 180 240 300 0.8 0.6 0.4 0.2 y c a r u c c A l a b o G l Ablation of µ=0.0, λ = 0.0 (C=35) FedKWS-UI: DSCNN:82.27 FedKWS-UI: MHAttRNN:84.73 FedKWS-UI: ResNet:74.10 FedKWS-UI: Transformer:70.57 0.9 0.8 0.7 0.6 0.5 0.4 0.3 Ablation of µ (C=12, DSCNN) Ablation of λ (C=12, ResNet) 0.8 0.6 0.4 0.2 FedKWS-UI: µ=0.0:86.55 FedKWS-UI: µ=0.1:87.24 FedKWS-UI: µ=0.2:87.29 FedKWS-UI: µ=0.3:87.21 FedKWS-UI: µ=0.5:86.96 FedKWS-UI: λ=0.0:85.29 FedKWS-UI: λ=1e-4:86.07 FedKWS-UI: λ=1e-3:86.23 FedKWS-UI: λ=1e-2:82.79 FedKWS-UI: λ=1e-1:76.99 0 60 120 180 240 300 0 60 120 180 240 300 0 60 120 180 240 300 Communication Round Communication Round Communication Round 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 ) 2 γ ( r e s U d e ﬁ i l a u Q g n o A l -0.1 - 0 . 1 0 . 0 Without ALO θ2 θ0 0 . 4 0 . 3 0 . 2 0 . 1 0 . 9 0 . 5 Along Inferior User (γ1) 0 . 6 0 . 8 0 . 7 0.90 0.75 0.60 0.45 0.30 0.15 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 - 0 . 1 0 . 0 θ1 1 . 0 1 . 1 With ALO θ2 θ0 0 . 4 0 . 3 0 . 2 0 . 1 0 . 9 0 . 5 Along Inferior User (γ1) 0 . 6 0 . 8 0 . 7 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 ) y c a r u c c A t s e T ( n o i t a z i l a r e n e G ˆθ1 1 . 0 1 . 1 [23] X. Li and D. Zhan, “FedRS: Federated learning with restricted softmax for label distribution non-iid data,” in KDD, 2021, pp. 995–1005. [24] X. Yao, C. Huang, and L. Sun, “Two-stream federated learning: Reduce the communication costs,” in VCIP, 2018, pp. 1–4. [25] D. A. E. Acar, Y. Zhao, R. M. Navarro, M. Mattina, P. N. What- mough, and V. Saligrama, “Federated learning based on dynamic regularization,” in ICLR, 2021. [26] Q. Li, B. He, and D. Song, “Model-contrastive federated learn- ing,” in CVPR, 2021, pp. 10 713–10 722. [27] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transfer- able are features in deep neural networks?” in NeurIPS, 2014, pp. 3320–3328. [28] L. Collins, H. Hassani, A. Mokhtari, and S. Shakkottai, “Exploit- ing shared representations for personalized federated learning,” in ICML, 2021, pp. 2089–2099. [29] P. P. Liang, T. Liu, Z. Liu, R. Salakhutdinov, and L. Morency, “Think locally, act globally: Federated learning with local and global representations,” CoRR, vol. abs/2001.01523, 2020. [30] X. Li, D. Zhan, Y. Shao, B. Li, and S. Song, “FedPHP: Federated personalization with inherited private models,” in ECML/PKDD, 2021, pp. 587–602. [31] X. Li, L. Gan, D. Zhan, Y. Shao, B. Li, and S. Song, “Aggregate or not? exploring where to privatize in DNN based federated learn- ing under different non-iid scenes,” CoRR, vol. abs/2107.11954, 2021. [32] G. E. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” CoRR, vol. abs/1503.02531, 2015. [33] T. Shen, J. Zhang, X. Jia, F. Zhang, G. Huang, P. Zhou, F. Wu, and C. Wu, “Federated mutual learning,” CoRR, vol. abs/2006.16765, 2020. [34] C. He, M. Annavaram, and S. Avestimehr, “Group knowledge transfer: Federated learning of large cnns at the edge,” in NeurIPS, 2020. [35] F. Lai, Y. Dai, X. Zhu, H. V. Madhyastha, and M. Chowdhury, “Fedscale: Benchmarking model and system performance of fed- erated learning,” in ResilientFL, 2021, pp. 1–3. [36] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, “Deep learning with differential pri- vacy,” in CCS, 2016, pp. 308–318. 7. References [1] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in ICASSP, 2016, pp. 4960–4964. [2] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig, “The microsoft 2016 conversational speech recognition system,” in ICASSP, 2017, pp. 5255–5259. [3] G. Chen, C. Parada, and G. Heigold, “Small-footprint keyword spotting using deep neural networks,” in ICASSP, 2014, pp. 4087– 4091. [4] T. N. Sainath and C. Parada, “Convolutional neural networks for small-footprint keyword spotting,” in INTERSPEECH, 2015, pp. 1478–1482. [5] Y. Zhang, N. Suda, L. Lai, and V. Chandra, “Hello edge: Keyword spotting on microcontrollers,” CoRR, vol. abs/1711.07128, 2017. [6] O. Rybakov, N. Kononenko, N. Subrahmanya, M. Visontai, and S. Laurenzo, “Streaming keyword spotting on mobile devices,” in INTERSPEECH, 2020, pp. 2277–2281. [7] R. Tang and J. Lin, “Deep residual learning for small-footprint keyword spotting,” in ICASSP, 2018, pp. 5484–5488. [8] A. Berg, M. O’Connor, and M. T. Cruz, “Keyword trans- former: A self-attention model for keyword spotting,” CoRR, vol. abs/2104.00769, 2021. [9] P. Warden, “Speech commands: A dataset for limited-vocabulary speech recognition,” CoRR, vol. abs/1804.03209, 2018. [10] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Ar- cas, “Communication-efﬁcient learning of deep networks from decentralized data,” in AISTATS, 2017, pp. 1273–1282. [11] Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learn- ing: Concept and applications,” ACM TIST, vol. 10, no. 2, pp. 12:1–12:19, 2019. [12] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Fed- erated learning with non-iid data,” CoRR, vol. abs/1806.00582, 2018. [13] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, “Federated optimization in heterogeneous networks,” in MLSys, 2020. [14] X. Li, Y. Xu, S. Song, B. Li, Y. Li, Y. Shao, and D. Zhan, “Federated learning with position-aware neurons,” CoRR, vol. abs/2203.14666, 2022. [15] Y. Gong, Y. Chung, and J. R. Glass, “AST: audio spectrogram transformer,” CoRR, vol. abs/2104.01778, 2021. [16] S. Chang, H. Park, J. Cho, H. Park, S. Yun, and K. Hwang, “Subspectral normalization for neural audio data processing,” in ICASSP, 2021, pp. 850–854. [17] D. Leroy, A. Coucke, T. Lavril, T. Gisselbrecht, and J. Dureau, “Federated learning for keyword spotting,” in ICASSP, 2019, pp. 6341–6345. [18] A. Hard, K. Partridge, C. Nguyen, N. Subrahmanya, A. Shah, P. Zhu, I. Lopez-Moreno, and R. Mathews, “Training keyword spotting models on non-iid data with federated learning,” in IN- TERSPEECH, 2020, pp. 4343–4347. [19] S. J. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush, J. Koneˇcn´y, S. Kumar, and H. B. McMahan, “Adaptive federated optimization,” in ICLR, 2021. [20] X. Cui, S. Lu, and B. Kingsbury, “Federated acoustic modeling for automatic speech recognition,” in ICASSP, 2021, pp. 6748–6752. [21] K. Nandury, A. Mohan, and F. Weber, “Cross-silo federated train- ing in the cloud with diversity scaling and semi-supervised learn- ing,” in ICASSP, 2021, pp. 3085–3089. [22] D. Guliani, F. Beaufays, and G. Motta, “Training speech recogni- tion models with federated learning: A quality/cost framework,” in ICASSP, 2021, pp. 3080–3084.",1.0
Fast Finite Width Neural Tangent Kernel,"[{'href': 'http://arxiv.org/abs/2206.08720v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2206.08720v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-17 12:18:22,,"2 2 0 2 n u J 9 ] G L . s c [ 2 v 8 1 2 0 1 . 5 0 2 2 : v i X r a Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions Rui Yang yr0013@mail.ustc.edu.cn University of Science and Technology of China Jie Wang∗ jiewangx@ustc.edu.cn Institute of Artificial Intelligence Hefei Comprehensive National Science Center University of Science and Technology of China Zijie Geng ustcgzj@mail.ustc.edu.cn University of Science and Technology of China Mingxuan Ye mingxuanye@miralab.ai University of Science and Technology of China Shuiwang Ji sji@tamu.edu Texas A&M University College Station, TX Bin Li binli@ustc.edu.cn University of Science and Technology of China Feng Wu fengwu@ustc.edu.cn University of Science and Technology of China ABSTRACT Generalization across different environments with the same tasks is critical for successful applications of visual reinforcement learning (RL) in real scenarios. However, visual distractions—which are com- mon in real scenes—from high-dimensional observations can be hurtful to the learned representations in visual RL, thus degrading the performance of generalization. To tackle this problem, we pro- pose a novel approach, namely Characteristic Reward Sequence Prediction (CRESP), to extract the task-relevant information by learning reward sequence distributions (RSDs), as the reward sig- nals are task-relevant in RL and invariant to visual distractions. Specifically, to effectively capture the task-relevant information via RSDs, CRESP introduces an auxiliary task—that is, predicting the characteristic functions of RSDs—to learn task-relevant represen- tations, because we can well approximate the high-dimensional distributions by leveraging the corresponding characteristic func- tions. Experiments demonstrate that CRESP significantly improves the performance of generalization on unseen environments, out- performing several state-of-the-arts on DeepMind Control tasks with different visual distractions. ∗ Corresponding Author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD ’22, August 14–18, 2022, Washington, DC, USA © 2022 Association for Computing Machinery. ACM ISBN 978-1-4503-9385-0/22/08. . . $15.00 https://doi.org/10.1145/3534678.3539391 CCS CONCEPTS • Computing methodologies → Sequential decision making; Image representations; Markov decision processes. KEYWORDS Task-relevant representation learning, reward sequence, character- istic function, generalization, visual reinforcement learning ACM Reference Format: Rui Yang, Jie Wang∗, Zijie Geng, Mingxuan Ye, Shuiwang Ji, Bin Li, and Feng Wu. 2022. Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’22), August 14–18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3534678.3539391 1 INTRODUCTION Visual reinforcement learning (RL) algorithms aim to solve com- plex control tasks from high-dimensional visual observations. No- table successes include DrQ for locomotion control [30], IMPALA for multi-task learning [5], and QT-Opt for robot grasping [13]. Although these methods perform well on training environments, they can hardly generalize to new environments, even these environ- ments are semantically similar to the training environments. This is because image observations often involve many task-irrelevant visual factors, such as dynamic backgrounds and colors of the object under control. Minor changes in such visual factors may cause large distributional shifts of the environments, which prevent the agent from extracting underlying task-relevant information when we put it into a new environment. This indicates that many existing RL agents memorize the trajectories on specific environments [22, 25], rather than learning transferable skills. To learn a policy with transferable skills for generalization, many prior works focus on learning representations that encode KDD ’22, August 14–18, 2022, Washington, DC, USA Yang et al. functions on top of the reward sequence representations. Experi- ments on DeepMind Control Suite [27] with visual distractors [26] demonstrate that CRESP significantly improves several state-of- the-arts on unseen environments. Our main contributions in this paper are as follows: • We introduce the reward sequence distributions (RSDs) to discard the task-irrelevant features and preserve the task- relevant features. • We propose CRESP, a novel approach that extracts the task- relevant information by learning the characteristic functions of RSDs for representation learning. • Experiments demonstrate that the representations learned by CRESP preserve more task-relevant features than prior methods, outperforming several state-of-the-arts on the ma- jority of tasks by substantial margins. 2 RELATED WORK Generalization in visual RL. The study of generalization in deep RL focuses on the capability of RL methods to generalize to un- seen environments under a limited set of training environments. Several works propose to apply regularization techniques origi- nally developed for supervised learning, including dropout [12] and batch normalization [7, 12]. Although practical and easy to im- plement, these methods do not exploit any properties of sequential decision-making problems. Other approaches for preventing over- fitting focus on data augmentation [17, 19, 21, 31], which enlarge the available data space and implicitly provide the prior knowledge to the agent. Although these methods show promising results in well-designed experimental settings, strong assumptions such as prior knowledge of the testing environments may limit their real ap- plications. In contrast to these methods, we consider a more realistic setting without assuming this prior knowledge of environments. Representation Learning in visual RL. Many prior works focus on representation learning for generalization in visual RL. Some of the works [14, 15] use a two-step learning process, which first trains an auto-encoder by using a reconstruction loss for low-dimensional representations, and then uses this representation for policy opti- mization. However, such representations encode all elements from observations, whether they are relevant to the task or not. Other works use bisimulation metrics to learn a representation that is invariant to irrelevant visual features [34]. However, such methods use the transition dynamics, which vary with the environments, leading to the learned representation involving task-irrelevant fea- tures of the visual distractions. A recent study [20] leverages the reward prediction for representation learning. However, the rep- resentation learning method only considers finite MDPs, which cannot extend to visual RL tasks. Characteristic Functions of Random Variables. Characteristic func- tions are the Fourier transforms of probability density functions. They are well studied in probability theory and can be used to specify high-dimensional distributions. This is because two random variables have the same distribution if and only if they have the same characteristic function. Some prior works [2, 32] use character- istic functions to solve some statistical problems. We leverage this tool for a simple and tractable approximation of high-dimensional Figure 1: The agent-environment interactions in Block MDPs with visual distractions. Each environment 𝑒 provides a state 𝑠𝑡 and a background 𝑥𝑡 , which generate an observa- tion 𝑜𝑡 = 𝑔(𝑠𝑡 , 𝑥𝑡 ) through a nonlinear function 𝑔. The agent receives 𝑜𝑡 and takes an action 𝑎𝑡 in 𝑒, leading to the transi- tions of states (from 𝑠𝑡 to 𝑠𝑡 +1), backgrounds (from 𝑥𝑡 to 𝑥𝑡 +1), and thus the observation transitions (from 𝑜𝑡 to 𝑜𝑡 +1). Notice that the red arrows represent the transitions that vary with different environments, while the blue arrow represents the transition invariant to environments. only the task-relevant information while discarding task-irrelevant visual factors. Some of them propose similarity metrics [3, 16] to find semantically equivalent observations for representation learn- ing [1, 34]. Others design objectives by integrating MDP proper- ties to learn a causal representation that is invariant to irrelevant features [23, 33]. These aforementioned methods leverage rewards and transition dynamics to capture task-relevant features. However, the observation transition dynamics (see Figure 1) may induce the task-irrelevant information relating to visual distractions into the representations, thus hindering generalization [24, 33]. Detailed discussions are in Section 4.1. In contrast to the above methods, we propose a novel approach, namely Characteristic Reward Sequence Prediction (CRESP), which only uses reward signals but observation transition dynamics to learn task-relevant representations, as the reward signals are task- relevant in RL and invariant to visual factors. To preserve infor- mation that is relevant to the task, CRESP introduces the reward sequence distributions (RSDs), which are the conditional distribu- tions of reward sequences given a starting observation and various subsequent actions. CRESP leverages RSDs to learn a task-relevant representation that only encodes the information of RSDs, which we call reward sequence representation. Specifically, considering that the characteristic function can specify high-dimensional distribu- tions [2], we propose to learn such task-relevant representation by an auxiliary task that predicts the characteristic functions of RSDs. Moreover, we provide a theoretical analysis of the value bounds between the true optimal value functions and the optimal value ... Env Env Env Agent Env Env Env ... Agent observation transition dynamics Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions KDD ’22, August 14–18, 2022, Washington, DC, USA distributions. Our experiments demonstrate that the characteris- tic functions perform well to specify the distributions of reward sequences in our method. 3 PRELIMINARIES In visual RL tasks, we deal with high-dimensional image observa- tions, instead of the states as the inputs. We consider a family of environments with the same high-level task but different visual dis- tractions. Denote E as the set of these environments. We model each environment 𝑒 ∈ E as a Block Markov Decision Process (BMDP) [4, 33], which is described by a tuple M𝑒 = (S, O, A, R, 𝑝, 𝑝𝑒, 𝛾). Here S is the state space, O is the observation space, A is the action space, R is the reward space, which we assume to be bounded, 𝑝 (𝑠 ′, 𝑟 |𝑠, 𝑎) is the state transition probability, 𝑝𝑒 (𝑜 ′, 𝑟 |𝑜, 𝑎) is the observation transition probability, which varies with environments 𝑒 ∈ E, and 𝛾 ∈ [0, 1) is the discount factor. At each time step 𝑡, we suppose that the environment is in a state 𝑆𝑡 .1 The agent, instead of directly achieving 𝑆𝑡 , obtains an observa- tion 𝑂𝑡 on environment 𝑒 ∈ E. It is reasonable to assume that the observation is determined by the state and some task-irrelevant visual factors that vary with environments, such as backgrounds or agent colors in DeepMind Control tasks. Symbolically, let X be the set of such visual factors. We suppose that there exists an observation function 𝑔 : S × X → O [4, 25] such that 𝑂𝑡 = 𝑔(𝑆𝑡 , 𝑋𝑡 ), where 𝑋𝑡 is a random variable in X, independent with 𝑆𝑡 and 𝐴𝑡 , with a transition probability 𝑞𝑒 (𝑥 ′|𝑥). See Figure 1 for an illustra- tion. We aim to find a policy 𝜋 (·|𝑜𝑡 ) that maximizes the expected accumulated reward E𝑒 (cid:2)(cid:205)∞ 𝛾𝑡 𝑅𝑡 (cid:3) simultaneously in all environ- 𝑡 =0 ments 𝑒 ∈ E, where E𝑒 [·] means that the expectation is taken in the environment 𝑒. Moreover, we assume that the environments follow a general- ized Block structure [4, 33]. That is, an observation 𝑜 ∈ O uniquely determines its generating state 𝑠, and the visual factor 𝑥. This assumption implies that the observation function 𝑔(𝑠, 𝑥) is invert- ible with respect to both 𝑠 and 𝑥. For simplicity, we denote 𝑠 = [𝑜]𝑠 and 𝑥 = [𝑜]𝑥 as the generating state and visual factor, respectively. Furthermore, we have 𝑝𝑒 (𝑜 ′, 𝑟 |𝑜, 𝑎) = 𝑝 (𝑠 ′, 𝑟 |𝑠, 𝑎)𝑞𝑒 (𝑥 ′|𝑥), where 𝑠 = [𝑜]𝑠, 𝑠 ′ = [𝑜 ′]𝑠 , 𝑥 = [𝑜]𝑥 and 𝑥 ′ = [𝑜 ′]𝑥 . 4 REPRESENTATION LEARNING VIA REWARD SEQUENCE DISTRIBUTIONS An encoder, or a representation, refers to an embedding function Φ : O → Z, which maps the observational inputs onto a latent state representation space Z. Our goal is to find a suitable represen- tation that encodes only task-relevant information and is invariant to visual distractions. In Section 4.1, we discuss the notion of task relevance in visual RL, introduce reward sequence distributions (RSDs), and formulate the reward sequence representations for gen- eralization. In Section 4.2, we provide a theoretical analysis that reformulates the reward sequence representation via the charac- teristic functions of RSDs. In Section 4.3, we present a practical method, based on the prediction of characteristic functions of RSDs, to learn such a reward sequence representation. 1Throughout this paper, we use uppercase letters such as 𝑆𝑡 and 𝑂𝑡 to denote random variables, and use lowercase letters such as 𝑠𝑡 and 𝑜𝑡 to denote the corresponding values that the random variables take. Figure 2: The relationship between observations and RSD mappings. We can divide the observation space into differ- ent equivalence classes, where the equivalent observations are generated from the same state. Each equivalence class corresponds to a same mapping from action sequences a ∈ A𝑇 to reward sequence distributions 𝑝 (·|𝑜, a) ∈ Δ(R𝑇 ). 4.1 Task-relevant Invariance in Visual RL The key idea of our approach is to capture the task-relevant infor- mation across different environments from observations, and lever- age such information for representation learning to improve the performance of generalization. Reward signals and transition dynamics are major properties of MDP, which are commonly used for representation learning in visual RL. We start with a discussion on the distractions induced by observation transition dynamics. In visual RL, we can hardly learn about the state transition dynamics, as the state space is un- available in practice. Instead, many methods learn the observation transition dynamics by a probabilistic dynamics model [28, 29, 34]. However, the observation transition dynamics are relevant to the visual factors because they comprise the transition dynamics of both states and task-irrelevant visual factors. Formally, we have the reward and observation transition dynamics 𝑝𝑒 (𝑜 ′, 𝑟 |𝑜, 𝑎) = 𝑝 (𝑠 ′, 𝑟 |𝑠, 𝑎)𝑞𝑒 (𝑥 ′|𝑥). This formula shows that the observation tran- sition probability varies with the environment 𝑒 ∈ E. We present a case in Figure 1 to illustrate the observation transition dynamics. Therefore, representations that encode information about observa- tion transition dynamics are subject to visual distractions and have difficulty learning transferable skills. In contrast to observation transition dynamics, the distributions of reward signals are relevant to the RL tasks and are invariant to visual distractions. Formally, if two observations 𝑜 and 𝑜 ′ are generated by the same state 𝑠, i.e., [𝑜]𝑠 = [𝑜 ′]𝑠 , then we have 𝑝𝑒 (𝑟 |𝑜, 𝑎) = 𝑝𝑒 (𝑟 |𝑜 ′, 𝑎) for any 𝑎 ∈ A and 𝑒 ∈ E. This motivates us to use the reward signals instead of observation transition dynamics for representation learning. As our goal is to maximize the expected accumulative rewards, what we need is not only the current reward but also the sequences of future rewards. Therefore, we propose to utilize the reward sequences for representation learning. For a mathematical formulation, we introduce some new nota- tions. We denote A𝑇 = {a = (𝑎1, · · · , 𝑎𝑇 ) : 𝑎𝑖 ∈ A} and R𝑇 = {r = (𝑟1, · · · , 𝑟𝑇 ) : 𝑟𝑖 ∈ R} as the spaces of action sequences and reward sequences with length 𝑇 , respectively. Let Δ(R𝑇 ) be the set of probability distributions over R𝑇 . At each time step 𝑡, Observation Space Action sequence space RSD space Action sequence space RSD space 𝐚 𝑝(∙ |𝑜, 𝐚) 𝐴𝑇 ∆(𝑅𝑇) 𝐴𝑇 ∆(𝑅𝑇) Set of RSD Mappings KDD ’22, August 14–18, 2022, Washington, DC, USA Yang et al. the sequence of the subsequent actions A𝑇 𝑡 = (𝐴𝑡 , · · · , 𝐴𝑡 +𝑇 −1) is a 𝑇 -dimensional random vector over A𝑇 . The sequence of the subsequent rewards R𝑇 𝑡 +1 = (𝑅𝑡 +1, · · · , 𝑅𝑡 +𝑇 ) is a 𝑇 -dimensional random vector over R𝑇 . 2 𝑡 +1 𝑡 +1 To clarify our idea, we first consider a deterministic environ- ment. Starting from an observation 𝑜𝑡 ∈ O, with the corresponding state 𝑠𝑡 = [𝑜𝑡 ]𝑠 ∈ S, suppose that we perform a given action se- quence a𝑇 𝑡 = (𝑎𝑡 , · · · , 𝑎𝑡 +𝑇 −1) ∈ A𝑇 and receive a reward sequence 𝑡 +1 = (𝑟𝑡 +1, · · · , 𝑟𝑡 +𝑇 ) ∈ R𝑇 from the environment. This reward r𝑇 sequence r𝑇 is uniquely determined by the starting state 𝑠𝑡 and the given action sequence a𝑇 𝑡 . Therefore, we can find that the rela- tionship between the given action sequence a𝑇 𝑡 and the received reward sequence r𝑇 is invariant to visual distractions. We can use such a relationship to identify the task-relevant information from observations. To formulate this relationship, we consider the map- pings from action sequences a ∈ A𝑇 to the corresponding reward sequences r ∈ R𝑇 —that the agent receives from an observation 𝑜 by following the action sequence a. We consider two observations 𝑜 and 𝑜 ′ that have same mappings from a ∈ A𝑇 to r ∈ R𝑇 for any dimension 𝑇 . In other words, we suppose that the agent receives the equal reward sequence r ∈ R𝑇 from 𝑜 and 𝑜 ′, when it follows any action sequence a ∈ A𝑇 for any 𝑇 . Then the two observations have similar task properties, in the sense that the agent will receive the equal accumulative rewards from 𝑜 and 𝑜 ′ no matter what actions the agent takes. Therefore, the mappings from action sequences a ∈ A𝑇 to the corresponding reward sequences r ∈ R𝑇 can be used to identify the task-relevant information from the observations. We then consider the stochastic environment, the case of which is similar to the deterministic environment. In the stochastic en- vironment, the reward sequence R𝑇 is random even for fixed 𝑡 +1 observation 𝑜𝑡 and action sequence A𝑇 𝑡 . Therefore, we cannot sim- ply consider the mappings from A𝑇 to R𝑇 . Instead, we apply the mappings from A𝑇 to Δ(R𝑇 ), which map the action sequences to the distributions of the sequences of reward random variables. Formally, let 𝑝 (r|𝑜, a) be the probability density function of the at the point r ∈ R𝑇 , conditioned on the starting 𝑡 = a ∈ A𝑇 . For any random vector R𝑇 observation 𝑂𝑡 = 𝑜 and the action sequence A𝑇 𝑜 ∈ O, a = (𝑎1, · · · , 𝑎𝑇 ), and r = (𝑟2, · · · , 𝑟𝑇 +1), we have 𝑡 +1 𝑝 (r|𝑜, a) = 𝑝 (𝑟2|𝑠, 𝑎1)𝑝 (𝑟3|𝑠, 𝑎1, 𝑎2) · · · 𝑝 (𝑟𝑇 +1|𝑠, 𝑎1, · · · , 𝑎𝑇 ), where 𝑠 = [𝑜]𝑠 , and 𝑝 (𝑟 |𝑠, 𝑎1, · · · , 𝑎𝑡 ) denotes the probability den- sity function of the reward 𝑟 that the agent receives, after fol- lowing an action sequence (𝑎1, · · · , 𝑎𝑡 ), starting from the state 𝑠. Furthermore, for any 𝑜, 𝑜 ′ ∈ O such that [𝑜]𝑠 = [𝑜 ′]𝑠 , we have 𝑝 (r|𝑜, a) = 𝑝 (r|𝑜 ′, a). The formulas imply that the conditional dis- tributions 𝑝 (·|𝑜, a) of reward sequences are determined by the gen- erating states of the observations as well as the action sequences. Therefore, the mappings from the action sequences a ∈ A𝑇 to the corresponding RSDs 𝑝 (·|𝑜, a) are task-relevant and invariant to visual distractions. Thus we can use the mappings to determine task relevance. See Figure 2 for an illustration. The analysis above motivates our method that leverages the RSDs to learn representations. Specifically, we learn a representation 2We use bold uppercase letters such as A and R to denote random vectors in high- dimensional spaces and use bold lowercase letters such as a and r to denote determin- istic vectors in such spaces. that can derive a function, which maps the action sequences to the corresponding RSDs. Formally, we define the 𝑇 -level reward sequence representation as follows. Definition 4.1. A representation Φ : O → Z is a 𝑇 -level reward sequence representation if it can derive the distribution of any reward sequence received from any observation by following any action sequence with length 𝑇 , i.e., there exists 𝑓 such that 𝑓 (r; Φ(𝑜), a) = 𝑝 (r|𝑜, a), ∀ r ∈ R 𝑇 , 𝑜 ∈ O, a ∈ A𝑇 . Intuitively, the 𝑇 -level reward sequence representation encodes the task-relevant information about the relation between the action sequences a and the RSDs 𝑝 (r|𝑜, a) in the next 𝑇 steps. Notice that a 𝑇 -level reward sequence representation is also a 𝑇 ′-level reward sequence representation, where 𝑇 ,𝑇 ′ ∈ N∗ and 𝑇 > 𝑇 ′. If 𝑇 tends to infinity, the representation will encode all task-relevant information from the objective of RL tasks. This derives the following definition. Definition 4.2. A representation Φ : O → Z is a reward sequence representation if it is a 𝑇 -level reward sequence representation for all 𝑇 ∈ N∗. The reward sequence representation is equivalent to a ∞-level reward sequence representation. In practice, we learn a finite 𝑇 - level reward sequence representation as an approximation of the reward sequence representation. To provide a theoretical guarantee for the approximation, the following theorem gives a value bound between the true optimal value function and the value function on top of the 𝑇 -level reward sequence representation. Theorem 4.3. Let Φ : O → Z be a 𝑇 -level representation, 𝑉 𝑒 ∗ : O → R be the optimal value function in the environment 𝑒 ∈ E, ¯𝑉 𝑒 ∗ : Z → R be the optimal value function on the latent representation space, built on top of the representation Φ. Let ¯𝑟 be a bound of the reward space, i.e., |𝑟 | < ¯𝑟 for any 𝑟 ∈ R. Then we have 0 ≤ 𝑉 𝑒 ∗ (𝑜) − ¯𝑉 𝑒 ∗ ◦ Φ(𝑜) ≤ 2𝛾𝑇 1 − 𝛾 ¯𝑟, for any 𝑜 ∈ O and 𝑒 ∈ E. Proof. See Appendix A.1 □ 4.2 Characteristic Functions for Representation Learning In Section 4.1, we formulate the 𝑇 -level reward sequence repre- sentation that can derive the probability density function 𝑝 (r|𝑜, a), where r ∈ R𝑇 is a reward sequence, 𝑜 ∈ O is an observation, and a ∈ A𝑇 is an action sequence. However, learning the probability density functions is usually technically impractical [2]. Leveraging the characteristic function of random vectors, we propose an al- ternative approach, which is simple to implement and effective to learn the distributions. Consider a random vector R defined on the space R𝑇 , with a probability density function 𝑝R (·). The characteristic function 𝜑R : R𝑇 → C of R is defined as 𝜑R (𝝎) = ER∼𝑝R ( ·) 𝑒𝑖 ⟨𝝎,R⟩ (cid:105) (cid:104) = ∫ 𝑒𝑖 ⟨𝝎,r⟩𝑝R (r)dr, √ where 𝝎 ∈ R𝑇 denotes the input of 𝑝R (·), and 𝑖 = −1 is the imaginary unit. Since we consider discounted cumulative rewards Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions KDD ’22, August 14–18, 2022, Washington, DC, USA Figure 3: The overall architecture of CRESP. CRESP minimizes the prediction loss to train an encoder Φ, and simultaneously uses Φ to learn a policy in an actor-critic setting. In the prediction task, CRESP predicts the characteristic functions of reward sequence distributions through the encoder Φ and the predictor Ψ (in the purple box). The prediction loss L N D (Φ, Ψ) provides the gradients (red lines) to update both the predictor Ψ and the encoder Φ. Here r𝑇 𝑡 are the sequences drawn from a replay buffer D. The inputs 𝝎 of characteristic functions are sampled from a Gaussian distribution N . 𝑡 +1 and a𝑇 𝑡 =1 in RL tasks, we use ⟨·, ·⟩ to denote the weighted inner product in R𝑇 , i.e., ⟨𝝎, r⟩ = (cid:205)𝑇 𝛾𝑡 𝜔𝑡𝑟𝑡 , where 𝛾 is the discounted factor. Characteristic functions are useful tools well studied in probabil- ity theory. In contrast to the probability density function, the char- acteristic function has some good basic properties. 1) |𝜑R (𝝎)| ≤ 𝑒𝑖 ⟨𝝎,R⟩(cid:12) (cid:12) ER∼𝑝R ( ·) (cid:12) = 1, which indicates that the characteristic func- (cid:12) (cid:12) (cid:12) tion always exists and is uniformly bounded. 2) The characteris- tic function 𝜑R is uniformly continuous on R𝑇 , which makes it tractable for learning. The following lemma states a fact that the distribution of a ran- dom vector can be specified by its characteristic function. Lemma 4.4. [8] Two random vectors X and Y have the same characteristic function if and only if they have the same probability distribution function. This lemma implies that we can recapture the information about the distributions of random vectors via their characteristic functions. Therefore, instead of learning the conditional density functions of reward sequences that are intractable, we propose to leverage characteristic functions of the RSDs for representation learning. Specifically, we have the following theorem. Theorem 4.5. A representation Φ : O → Z is a 𝑇 -level reward sequence representation if and only if there exits a predictor Ψ such that for all 𝒘 ∈ R𝑇 , 𝑜 ∈ O and a ∈ A𝑇 , Ψ(𝝎; Φ(𝑜), a) = 𝜑R|𝑜,a (𝝎) = ER∼𝑝 ( · |𝑜,a) 𝑒𝑖 ⟨𝝎,R⟩ (cid:105) (cid:104) . Proof. See Appendix A.2. □ Theorem 4.5 provides an equivalent definition of 𝑇 -level reward sequence representation and inspires our novel approach to predict the characteristic functions of RSDs for representation learning. Algorithm 1 Characteristic Reward Sequence Prediction Initialize a replay buffer D, a policy 𝜋, a representation Φ, and a function approximator Ψ for each iteration do for 𝑒 in E do for each environment step 𝑡 do Execute action 𝑎𝑡 ∼ 𝜋 (·|Φ(𝑜𝑡 )) Receive a transition 𝑜𝑡 +1, 𝑟𝑡 +1 ∼ 𝑝𝑒 (·|𝑜𝑡 , 𝑎𝑡 ) Record partial trajectories {(𝑜𝑡 −𝑖, 𝑎𝑡 −𝑖, 𝑟𝑡 +1−𝑖 )}𝑇 −1 𝑖=0 in D end for end for for each gradient step do Sample partial trajectories from D Update the representation: L N Update the policy: LRL (𝜋) D (Φ, Ψ) end for end for 4.3 Characteristic Reward Sequence Prediction To improve the generalization of a learned policy on unseen en- vironments with visual distractions, we propose Characteristic Reward Sequence Prediction (CRESP), a novel approach to learn representations for task relevance from high-dimensional observa- tions. As discussed above, CRESP learns RSDs by predicting the characteristic functions 𝜑R|𝑜,a (𝝎). In this section, we focus on the detailed learning procedure for the prediction. For an observation 𝑜 ∈ O and an action sequence a ∈ A𝑇 , the true characteristic function of the corresponding reward sequence is 𝜑R|𝑜,a (𝝎) = ER∼𝑝 ( · |𝑜,a) [𝑒𝑖 ⟨𝝎,R⟩]. We estimate the characteristic function by a predictor Ψ(𝝎; Φ(𝑜), a). We use the weighted squared distance between the true and predicted characteristic functions as Characteristic Reward Sequence Prediction 𝑠𝑡 𝑥𝑡 𝑇 = (𝑟𝑡+1, … , 𝑟𝑇) 𝒓𝑡+1 𝝎 = 𝜔1, … , 𝜔𝑇−𝑡 𝑇 = 𝑎𝑡, … , 𝑎𝑇−1 𝐚𝑡 . .. . . . 𝑔 𝑜𝑡 Encoder 𝚽 Characteristic Function signals Prediction Loss 𝒩(𝚽, 𝚿) ℒ𝒟 𝝍cos 𝝍sin grad Predictor 𝚿 𝚿 𝝎; 𝚽 𝑜𝑡 , 𝐚𝑡 Critic 𝑄 𝑜𝑡, 𝑎𝑡 Actor 𝑎𝑡 RL Loss KDD ’22, August 14–18, 2022, Washington, DC, USA Yang et al. Figure 4: Learning curves of six methods on six tasks with dynamic background distractions for 500K environment steps. The solid curves denote the means and the shaded regions denote the minimum and maximum returns over 6 trials. Each checkpoint is evaluated by 10 episodes on unseen environments. Curves are smoothed for visual clarity. the prediction loss: 𝐿W (Φ, Ψ|𝑜, a) = E𝛀∼W (cid:104)(cid:13) (cid:13)Ψ (𝛀; Φ(𝑜), a) − 𝜑R|𝑜,a (𝛀)(cid:13) 2 (cid:13) 2 (cid:12)Ψ (𝝎; Φ(𝑜), a) − 𝜑R|𝑜,a (𝝎)(cid:12) (cid:12) 2 W (𝝎)d𝝎, (cid:12) (cid:105) = ∫ R𝑇 where W is any probability density function on R𝑇 . We optimize the expected loss for observations and action sequences taken from the replay buffer D: 𝐿W D (Φ, Ψ) = E(𝑂,A)∼D (cid:104) 𝐿W (Φ, Ψ|𝑂, A) (cid:105) = E(𝑂,A)∼D,𝛀∼W (cid:104)(cid:13) (cid:13)Ψ (𝛀; Φ(𝑂), A) − 𝜑R|𝑂,A (𝛀)(cid:13) (cid:13) (cid:105) 2 2 . In practice, Since we have no access to the true characteristic functions, we propose to optimize an upper bound on 𝐿W D : L W D (Φ, Ψ) = E(𝑂,A,R)∼D,𝛀∼W (cid:20)(cid:13) (cid:13) (cid:13) ≥ E(𝑂,A)∼D,𝛀∼W (cid:20)(cid:13) (cid:13) (cid:13) Ψ (𝛀; Φ(𝑂), A) − 𝑒𝑖 ⟨𝛀,R⟩(cid:13) 2 (cid:13) (cid:13) 2 (cid:21) Ψ (𝛀; Φ(𝑂), A) − ER∼𝑝 ( · |𝑂,A) (cid:21) 𝑒𝑖 ⟨𝛀,R⟩ (cid:105)(cid:13) (cid:104) 2 (cid:13) (cid:13) 2 = 𝐿W D (Φ, Ψ). Due to the complex form of characteristic functions, we divide the predictor Ψ into two parts Ψ = (𝜓cos,𝜓sin), where 𝜓cos es- timates the real parts, and 𝜓sin estimates the imaginary parts of characteristic functions, respectively. Moreover, we draw Ω from a Gaussian distribution W = N (𝝁, 𝝈 2) in practice. We then pa- rameterize this distribution N (𝝁, 𝝈 2) and perform ablation on it in Appendix B.1. Based on the experimental results, we leverage D (Φ, Ψ) = E(𝑂,A,R)∼D,𝛀∼N the standard Gaussian distribution N . Then the loss function is: L N (cid:2)∥𝜓cos (𝛀; Φ(𝑂), A) − cos (⟨𝛀, R⟩)∥2 2 (cid:3) . + ∥𝜓sin (𝛀; Φ(𝑂), A) − sin (⟨𝛀, R⟩)∥2 2 In the training process, we update the encoder Φ and the pre- dictor Ψ due to the auxiliary loss L N D (Φ, Ψ), and use the trained encoder Φ for the RL tasks. The whole architecture of CRESP and training procedure are illustrated in Figure 3 and Algorithm 1. 5 EXPERIMENTS In this paper, we improve the performance of generalization on unseen environments with visual distractions. We focus on training agents in multi-environments under traditional off-policy settings without any prior environmental knowledge, such as strong aug- mentations designed for visual factors [6, 19, 35], fine-tuning in test environments [11], or environmental labels for invariance [1, 24]. We then investigate the performances of agents trained by different algorithms on various unseen test environments. For each environment, we benchmark CRESP extensively against prior state-of-the-art methods: 1) CURL [18]: a RL method with an auxiliary contrastive task; 2) DrQ [30]: an effective method with state-of-the-art performance on DeepMind Control (DMCon- trol) [27]; 3) MISA [33]: a recent approach from causal inference to learn invariant representations by approximating one-step rewards and dynamics; 4) DBC [34]: a research for generalization in RL to learn representations via the bisimulation metric; 5) SAC [9]: a traditional off-policy deep RL algorithm. Network Details. Our method builds upon SAC and follows the network architecture of DrQ and CURL. We use a 4-layer feed- forward ConvNet with no residual connection as the encoder. Then,",1.0
"Dynamical Modeling for non-Gaussian Data with High-dimensional Sparse
  Ordinary Differential Equations","[{'href': 'http://arxiv.org/abs/2206.08616v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2206.08616v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-17 08:17:29,,"Fast Finite Width Neural Tangent Kernel Roman Novak 1 Jascha Sohl-Dickstein 1 Samuel S. Schoenholz 1 2 2 0 2 n u J 7 1 ] G L . s c [ 1 v 0 2 7 8 0 . 6 0 2 2 : v i X r a Abstract The Neural Tangent Kernel (NTK), deﬁned as θ (x1, x2) = (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3) (cid:2)∂f (θ, x2)(cid:14)∂θ(cid:3)T Θf where (cid:2)∂f (θ, )(cid:14)∂θ(cid:3) is a neural network (NN) Ja- · cobian, has emerged as a central object of study in deep learning. In the inﬁnite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and gener- alization of NN architectures. At ﬁnite widths, the NTK is also used to better initialize NNs, compare the conditioning across models, perform architec- ture search, and do meta-learning. Unfortunately, the ﬁnite width NTK is notoriously expensive to compute, which severely limits its practical utility. We perform the ﬁrst in-depth analysis of the compute and memory requirements for NTK computation in ﬁnite width networks. Leverag- ing the structure of neural networks, we further propose two novel algorithms that change the ex- ponent of the compute and memory requirements of the ﬁnite width NTK, dramatically improving efﬁciency. Our algorithms can be applied in a black box fashion to any differentiable function, including those implementing neural networks. We open-source our implementations within the Neural Tangents package (Novak et al., 2020) at github.com/google/neural-tangents. 1. Introduction The past few years have seen signiﬁcant progress towards a theoretical foundation for deep learning. Much of this work has focused on understanding the properties of ran- dom functions in high dimensions. One signiﬁcant line of work (Neal, 1994; Lee et al., 2018; Matthews et al., 2018; Borovykh, 2018; Garriga-Alonso et al., 2019; Novak et al., 2019; Yang, 2019; Hron et al., 2020b;a; Hu et al., 2020) 1Google Brain, Mountain View, California, United States. Correspondence to: Roman Novak <romann@google.com>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). established that in the limit of inﬁnite width, randomly ini- tialized Neural Networks (NNs) are Gaussian Processes (called NNGPs). Building on this development, Jacot et al. (2018) showed that in function space the dynamics under gradient descent could be computed analytically using the so-called Neural Tangent Kernel (NTK) and Lee et al. (2019) showed that wide neural networks reduce to their lineariza- tions in weight space throughout training. A related set of results (Belkin et al., 2019; Spigler et al., 2019) showed that the ubiquitous bias-variance decomposition breaks down as high-dimensional models enter the so-called interpolat- ing regime. Together these results describe learning in the inﬁnite width limit and help explain the impressive general- ization capabilities of NNs. Insights from the wide network limit have had signiﬁcant practical impact. The conditioning of the NTK has been shown to signiﬁcantly impact trainability and generaliza- tion in NNs (Schoenholz et al., 2017; Xiao et al., 2018; 2020). This notion inspired initialization schemes like Fixup (Zhang et al., 2019), MetaInit (Dauphin & Schoen- holz, 2019), and Normalizer Free networks (Brock et al., 2021a;b), and has enabled efﬁcient neural architecture search (Park et al., 2020; Chen et al., 2021b). The NTK has additionally given insight into a wide range of phe- nomena such as: behavior of Generative Adversarial Net- works (Franceschi et al., 2021), neural scaling laws (Bahri et al., 2021), and neural irradiance ﬁelds (Tancik et al., 2020). Kernel regression using the NTK has further enabled strong performance on small datasets (Arora et al., 2020), and applications such as approximate inference (Khan et al., 2019), dataset distillation (Nguyen et al., 2020; 2021), and uncertainty prediction (He et al., 2020; Adlam et al., 2020). Despite the signiﬁcant promise of theory based on the NTK, computing the NTK in practice is challenging. In the inﬁnite width limit, the NTK can sometimes be computed analyti- cally. However, the inﬁnite-width kernel remains intractable for many architectures and ﬁnite width corrections are often important to describe actual NNs used in practice (see §I for detailed discussion). The NTK matrix can be computed for ﬁnite width networks as the outer product of Jacobians using forward or reverse mode automatic differentiation (AD), Θf (cid:124) θ (x1, x2) (cid:123)(cid:122) (cid:125) O×O := (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3) (cid:123)(cid:122) (cid:125) O×P (cid:124) (cid:2)∂f (θ, x2)(cid:14)∂θ(cid:3)T , (cid:125) (cid:123)(cid:122) (cid:124) P×O (1) Fast Finite Width Neural Tangent Kernel where f is the forward pass NN function producing outputs in RO, θ RP are all trainable parameters, and x1 and x2 are two inputs to the network. If inputs are batches of sizes N1 and N2, the NTK is an N1O N2O matrix. ∈ × Unfortunately, evaluating Eq. (1) is often infeasible due to time and memory requirements. For modern machine learn- ing tasks O is often greater (sometimes much greater) than 1000 (e.g. for ImageNet (Deng et al., 2009)), while even modestly sized models feature tens of millions of parame- 107. This makes both storing ([N1 + N2] OP ters, or P memory) and contracting ([N1N2] O2P time) the Jacobians in Eq. (1) very costly. The theoretical importance of the NTK together with its prohibitive computational costs im- plies that performance improvements will unlock impactful novel research. ∼ We perform the ﬁrst in-depth analysis of the compute and memory requirements for the NTK as in Eq. (1). Noting that forward and reverse mode AD are two extremes of a wide range of AD strategies (Naumann, 2004; 2008), we explore other methods for computing the NTK leveraging the structure of NNs used in practice. We propose two novel methods for computing the NTK that exploit different orderings of the computation. We describe the compute and memory requirements of our techniques in fully-connected (FCN) and convolutional (CNN) settings, and show that one is asymptotically more efﬁcient in both settings. We compute the NTK over a wide range of NN architectures and demonstrate that these improvements are robust in practice. We open-source our implementations as general-purpose JAX1 (Bradbury et al., 2018) function transformations. 2. Related Work The ﬁnite width NTK (denoted simply NTK throughout this work2) has been used extensively in many recent works, but to our knowledge implementation details and compute costs were rarely made public. Below we draw comparison to some of these works, but we stress that it only serves as a sanity check to make sure our contribution is valuable relative to the scale of problems that have been attempted. None of these works had efﬁcient NTK computation as their central goal. In order to compare performance of models based on the NTK and the inﬁnite width NTK, Arora et al. (2019a, Table 2) compute the NTK of up to 20-layer, 128-channel CNN in a binary CIFAR-2 classiﬁcation setting. In an equivalent 1Our algorithms are framework-agnostic, but implementation in JAX is easier, as described in §M. We also provide instructions for implementation in other frameworks like Tensorﬂow (Abadi et al., 2016) and PyTorch (Paszke et al., 2019) in §M. 2See §I for a comparison between the ﬁnite and inﬁnite width settings. setting with the same hardware (NVIDIA V100), we are able to compute the NTK of a 2048-channel CNN, i.e. a network with at least 256 times more parameters. To demonstrate the stability of the NTK during training for wide networks, Lee et al. (2019, Figure S6) compute the NTK of up to 3-layer 212-wide or 1-layer 214-wide FCNs. In the same setting with the same hardware (NVIDIA V100), we can reach widths of at least 214 and 218 respectively, i.e. handle networks with 4 to 16 times more parameters. To investigate convergence of a WideResNet WRN-28-k (Zagoruyko & Komodakis, 2016) to its inﬁnite width limit, Novak et al. (2020, Figure 2) evaluate the NTK of this model with widening factor k up to 32. In matching setting and hardware, we are able to reach a widening factor of at least 64, i.e. work with models at least 4 times larger. To meta-learn NN parameters for transfer learning in a MAML-like (Finn et al., 2017) setting, Zhou et al. (2021, Table 7) replace the inner training loop with NTK-based inference. They use up to 5-layer, 200-channel CNNs on MiniImageNet (Oreshkin et al., 2018) with scalar outputs and batch size 25. In same setting we achieve at least 512 channels, i.e. support models at least 6 times larger. Park et al. (2020, §4.1) use the NTK to predict the gen- eralization performance of architectures in the context of Neural Architecture Search (Zoph & Le, 2016, NAS); how- ever, the authors comment on its high computational burden and ultimately use a different proxy objective. In another NAS setting, Chen et al. (2021a, §3.1.1) use the condition number of NTK to predict a model’s trainability. Chen et al. (2021b, Table 1) use the NTK to evaluate the trainability of several ImageNet models such as ResNet 50/152 (He et al., 2016), Vision Transformer (Dosovitskiy et al., 2021) and MLP-Mixer (Tolstikhin et al., 2021). However, due to the prohibitive computational cost, in all of these cases the authors only evaluate a pseudo-NTK, i.e. the NTK of a scalar-valued function,3 which impacts the quality of the respective trainability/generalization proxy. By contrast, in this work we can compute the full 1000 × 1000 (1000 classes) NTK for the same models, i.e. perform a task 1000 times more costly. Finally, we remark that in all of the above settings, scaling up by increasing width or by working with the true NTK (vs the pseudo-NTK) should lead to improved downstream task performance due to a better inﬁnite width/linearization approximation or a higher-quality trainability/generalization proxy respectively, which makes our work especially rele- vant to modern research. 3Precisely, computing the Jacobian only for a single logit or the sum of all 1000 class logits. The result is not the full NTK, but rather a single diagonal block or the sum of its 1000 diagonal blocks (the ﬁnite width NTK is a dense matrix, not block-diagonal). Fast Finite Width Neural Tangent Kernel 3. Algorithms for Efﬁcient NTK Computation We now describe our algorithms for fast NTK computation. In §3.1 we cover preliminaries. We begin by introduc- ing notation used throughout the paper (§3.1.1). We then (§3.1.2) describe primitive building blocks of AD includ- ing the Jacobian-vector products (JVP) and vector-Jacobian products (VJP) that correspond to forward and reverse mode AD respectively before discussing the Jacobian (§3.1.3). In §3.2 we apply the above tools to describe the computa- tional complexity of the baseline approach to computing the NTK that is used in most (likely all) prior works. In §3.3 and §3.4 we present our two algorithms that each enable accelerating the computation by orders of magnitude in different ways. 3.1. Preliminaries 3.1.1. NOTATION ∈ RO with O outputs (e.g. class Consider a NN f (θ, x) logits) per input x and a total number P of trainable pa- rameters θ = vec (cid:2)θ0, . . . , θL(cid:3), with each θl of size Pl, P = (cid:80)L l=0 Pl. Also assume the network has K intermediate primitive outputs yk of size Yk each (for example, activa- tions or pre-activations), and let Y = (cid:80)K k=1 Yk be the total size of the outputs (see Fig. 5 and Fig. 6). The NTK is Θf θ (cid:124)(cid:123)(cid:122)(cid:125) O×O := (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3) (cid:123)(cid:122) (cid:125) O×P (cid:124) (cid:2)∂f (θ, x2) (cid:14)∂θ(cid:3)T (cid:123)(cid:122) (cid:125) (cid:124) P×O L (cid:88) l=0 (cid:2)∂f (θ, x1)(cid:14)∂θl(cid:3) (cid:124) (cid:123)(cid:122) (cid:125) O×Pl (cid:2)∂f (θ, x2)(cid:14)∂θl(cid:3)T (cid:123)(cid:122) (cid:125) (cid:124) Pl×O . (3) We denote FP to be the (time or memory, depending on con- text) cost of a single forward pass f (θ, x). For memory, we exclude the cost of storing all P weights, but rather deﬁne it to be the cost of evaluating f one primitive yk at a time. This (cid:0)maxk Yk + maxl Pl(cid:1), gives a memory cost of at most which we denote as simply Yk + Pl.4 Finally, we will con- sider x1 and x2 to be batches of N inputs each, in which case the NTK will be an NO NO matrix, obtained by com- × puting Eq. (2) for each pair of inputs. See §B for glossary. O 3.1.2. JACOBIAN-VECTOR PRODUCTS (JVP) AND VECTOR-JACOBIAN PRODUCTS (VJP) Following Maclaurin et al. (2015) we deﬁne JVPf VJPf (θ,x) : θt ∈ (θ,x) : fc ∈ P R O R (cid:2)∂f (θ, x)(cid:14)∂θ(cid:3) θt ∈ R (cid:2)∂f (θ, x)(cid:14)∂θ(cid:3)T fc ∈ (cid:55)→ (cid:55)→ O; P. R (4) (5) The JVP can be understood as pushing forward a tangent vector θt in weight space to a tangent vector in the space of outputs; by contrast the VJP pulls back a cotangent vector fc in the space of outputs to a cotangent vector in weight space. These elementary operations enable forward and reverse mode AD respectively and serve as a basis for typical AD computations such as gradients, Jacobians, Hessians, etc. The time cost of both is comparable to FP (see §D and Griewank & Walther (2008)). The memory cost of a JVP is FP as well (i.e. Yk + Pl), while the memory cost of a VJP is generally Y + P, since it requires storing all K intermediate primitive outputs for efﬁcient backprop and all L output cotangents. However, for the purpose of computing the NTK, we never need to store the whole Jacobian ∂f /∂θ, but only individual cotangents like ∂f /∂θl to compute the sum in Eq. (2) layer-by-layer. Hence we consider VJP to cost Y + Pl memory. To summarize, for a batch of N inputs, • JVP costs N [FP] time; N (cid:2)Yk(cid:3) + P memory. • VJP costs N [FP] time; N (cid:2)Y + Pl(cid:3) + P memory. 3.1.3. JACOBIAN The reverse mode Jacobian ∂f /∂θ is computed via O VJP calls on rows of the identity matrix IO RO×O, i.e. = (2) (cid:2)∂f (θ, x) (cid:14)∂θ(cid:3)T ∈ = (cid:2)∂f (θ, x) (cid:14)∂θ(cid:3)T P×O, R (6) IO ∈ and therefore costs O [VJP] time and memory apart from parameters and primitive outputs that can be reused across VJPs. Therefore, for a batch of N inputs, Jacobian costs NO [FP] time; NO (cid:2)Yk + Pl(cid:3)+NY+ P memory. 3.2. Jacobian contraction – the Baseline This baseline method of evaluating the NTK consists in computing the Jacobians ∂f /∂θ and contracting them as in Eq. (2). The contraction costs N2O2P time and N2O2 + NOPl memory, to store the result Θf θ and individ- ual layer-by-layer cotangents ∂f /∂θl. Including the cost of computing the cotangents via the batch Jacobian ∂f /∂θ = (cid:2)∂f /∂θ0, . . . , ∂f /∂θL(cid:3) from §3.1.3 we arrive at Jacobian contraction costs NO [FP] + N2O2P time; N2O2 + NO (cid:2)Yk + Pl(cid:3) + NY + P memory. 4To declutter notation throughout this work, in time and mem- ory complexity expressions, we (1) omit the O symbol, and (2) imply taking the maximum over any free index. In summary, Jacobian contraction performs NO forward passes followed by an expensive N2O2P contraction. Next we demonstrate how to reduce the contraction cost. Fast Finite Width Neural Tangent Kernel 3.3. NTK-vector products Consider the NTK-vector product function (for N = 1): Θf θ VP : v O R ∈ (cid:55)→ Θf θ v ∈ O. R Taking the NTK-vector product with O columns of the iden- tity matrix IO yields the full NTK, i.e. Θf θ . Ex- panding Θf θ IO = Θf θ VP(v) as θ v = (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3) (cid:2)∂f (θ, x2)(cid:14)∂θ(cid:3)T Θf (θ,x2) (v) = = (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3) VJPf (cid:104) = JVPf (cid:105) (θ,x2) (v) VJPf (θ,x1) , v = (7) (8) (9) where we have observed that the NTK-vector product can be expressed as a composition of a JVP and a VJP. The cost of computing Θf θ is then asymptotically the cost of the Ja- cobian, since it consists of O VJPs followed by O (cheaper) JVPs, therefore O [FP] time and O (cid:2)Yk + Pl(cid:3)+Y+P mem- ory. In the batched setting Eq. (7) is repeated for each pair of inputs, and therefore time increases by a factor of N2 to become N2O [FP]. However, the memory cost grows only linearly in N (except for the cost of storing the NTK of size N2O2), since intermediate primitive outputs and tan- gents/cotangents can be computed for each batch x1 and x2 separately and then reused for every pairwise combination. Therefore the memory cost is asymptotically the cost to store the NTK and compute the Jacobian. Altogether, NTK-vector products cost N2O [FP] time; N2O2 + NO (cid:2)Yk + Pl(cid:3) + NY + P memory. In summary, NTK-vector products eliminate the costly N2O2P contraction of Jacobian contraction, but perform N2O forward passes (as opposed to NO), and the memory requirement is identical. As a result, this method is bene- ﬁcial for small N, and for networks with a cheap forward pass FP relative to OP, which is always the case for FCNs (§4.1), but not necessarily for CNNs (§4.2). 3.4. Structured derivatives Rewriting Θf the primitive outputs yk, we ﬁnd: θ from Eq. (2) using the chain rule in terms of (cid:32) (cid:32) ∂f1 ∂yk1 1 ∂f1 ∂yk1 1 (cid:33) (cid:32) ∂yk1 1 ∂θl ∂f2 ∂yk2 2 ∂yk2 2 ∂θl (cid:33)T T (cid:33) T ∂yk1 1 ∂θl ∂yk2 2 ∂θl ∂f2 ∂yk2 2 Θf θ [l, k1, k2] , Θf θ = = =: (cid:88) l,k1,k2 (cid:88) l,k1,k2 (cid:88) l,k1,k2 (10) (11) (12) i /∂θl i . We have also where we deﬁne fi := f (θ, xi), and only consider ∂yki to be non-zero if θl is a direct input to yki deﬁned Θf θ [l, k1, k2] to be individual summands. Both Jacobian contraction and NTK-vector products per- form this sum of contractions via VJPs and JVPs, without explicit instantiation of primitive Jacobians ∂yki i /∂θl. How- ever, while VJPs and JVPs themselves are guaranteed to be computationally optimal (§D), higher order computa- tions like their composition (NTK-vector products) or con- traction (Jacobian contraction) are not. Speciﬁcally, each Θf θ [l, k1, k2] from Eq. (10) is a matrix-Jacobian-Jacobian- matrix product (MJJMP), which, as we will show shortly, can’t always be evaluated optimally with VJPs and JVPs. The idea of Structured derivatives is to design rules for efﬁcient computation of MJJMPs, similarly to AD rules for JVPs and VJPs. From Eq. (10), in the general case this requires hand-made rules for all pairwise combinations of primitives y1 and y2, of which there are 1362 > 10, 000 in JAX, and even more in Tensorﬂow (Abadi et al., 2016) and PyTorch (Paszke et al., 2019) (see §M). We dramatically reduce this number by: (θ,·) JVPf θ 1. Linearization. It follows from Eq. (4), that Θf θ = , i.e. the NTK of f evaluated at parameters θ is Θ equal to the NTK of the JVP of f given primal θ. JVP is a linear function of input tangents θ, and therefore we only need to implement efﬁcient MJJMPs for linear primitives, of which JAX has only 56.5 2. MJJMPs through structured derivatives. We further reduce the necessary MJJMP rule count from 562 down to only 56 by decomposing an MJJMP rule into two parts: ∈ RW, θl RW×W, and y (cid:0)θl(cid:1) = θlx 1. Structured derivative rule. Given a single primitive y, this rule identiﬁes the smallest subarray of ∂y/∂θl sufﬁcient to reconstruct the entire primitive Jacobian ∂y/∂θl, and the (constant and negligible in memory size) metadata necessary for the reconstruction. For example, if x ∈ RW (matrix-vector multiplication), then ∂y/∂θl = , and the rule will indicate that (1) IW only the subarray (cid:2)∂y/∂θl(cid:3) R1×W,6 needs to be computed (which is equal to xT in this case), and (2) that the entire primitive Jacobian can be reconstructed as ∂y/∂θl = I 1,:W. In other words, this rule annotates linear primitives y with the structure of their Jacobians, such as block diagonal, constant-block diagonal, or tiling along certain dimensions. ∈ RW×W2 (cid:2)∂y/∂θl(cid:3) 1,:W ∈ xT ⊗ ⊗ ∈ 5A linear function can contain nonlinear primitives. However, linearizing any function in JAX is guaranteed to produce only linear primitives (Frostig et al., 2021; Radul et al., 2022). 6We deﬁne [A]i,:j := [Ai,1, . . . , Ai,j] ∈ R 1×j. Fast Finite Width Neural Tangent Kernel 2. MJJMPs with structured Jacobians. Given input tensors A, B, C, D, where B and C are provided in the structured form as described above (i.e. only small subarrays along with their metadata) this rule ef- ﬁciently computes the 4-way contraction ABCD (i.e. the NTK summand Θf θ [l, k1, k2]). This amounts to using np.einsum with the optimal contraction order and adjusting its instructions based on provided meta- data. For example, if B = IW and C = IW bT RW), then ⊗ (for b, c RW×W2 RW×W2 ∈ ∈ ⊗ cT ABCD = A (cid:0)I = A (cid:0)I ⊗ ⊗ ∈ bT (cid:1) (cid:0)I cT (cid:1)T D = bT c(cid:1) D = (cid:0)bT c(cid:1) AD, ⊗ (13) (14) where were able to pull out bT c since it is a scalar. As we will see in §4 and §E, this and other similar contraction rules can enable signiﬁcant speedups. Therefore we avoid implementing 562 MJJMP rules by in- stead having (1) a single routine to perform 4-way tensor contractions with structured tensors, and (2) 56 rules anno- tating the structure in the 56 linear primitive Jacobians. We list all these structures and associated MJJMP costs in §E. Our approach does not guarantee optimality for the NTK of an arbitrary function, however, as we show in §4, it is asymptotically better than Jacobian contraction for FCNs and CNNs, and can provide orders of magnitude speedups in much more complex contemporary ImageNet models (§5). 3. Focusing on MJJMPs for typical operations. Many of the 56 linear JAX primitives are trivial to implement or rarely arise in NNs. At the time of writing we have only annotated 21 linear primitives (Table 4), which was sufﬁcient for the empirical speedups observed in §4 and §5. Summary. Structured derivatives amount to evaluating the sum of MJJMPs in Eq. (10), where (1) only small subarrays of primitive Jacobians ∂yki i /∂θl are instantiated, and (2) MJJMPs leverage the structure of these primitive Jacobians for efﬁcient contractions. Together, this incurs 1. The cost of computing primitive output cotangents ∂fi/∂yki for Eq. (10), which is equivalent to the cost i of the reverse mode Jacobian (§3.1.3), less the cost of computing (NOP) and storing (NOPl) weight-space cotangents ∂fi/∂θl, since they aren’t used in Eq. (10), i.e. NO [FP] time7 and NOYk + NY + P memory. 2. The cost of computing primitive Jacobian ∂yki subarrays, denoted as Jki (cid:80) l with J := (cid:80) . This amounts to NJ time and NJk Jk2 l l,k2 i /∂θl Jk1 l + l,k1 l memory. 3. The cost of evaluating Θf θ [l, k1, k2] via the efﬁcient MJJMP, which we denote as simply MJJMP and sub- stitute speciﬁc values based on the primitive. Required memory to store the result is N2O2. We add up these costs below and in Table 1, and show in §4 and §5 how they are beneﬁcial in most practical settings. Structured derivatives cost NO [FP] + MJJMP + l + NY + P N [J memory. OP] time; N2O2 + NOYk + NJk − 4. Examples The three algorithms in §3 (and our implementations) apply to any differentiable function f , but the resulting complexi- ties depend on variables such as FP and P, which depend on f (Table 1). Below we compute and compare all three complexities for a deep FCN (§4.1) and CNN (§4.2), sum- marizing the results in Table 2 and Table 3 respectively. 4.1. FCN NTK Complexity We apply our algorithms from §3 to FCNs with L hidden layers of width W. For simplicity we assume no biases, and RW, i.e. inputs of same size as the width. We deﬁne x yl := θlxl, and xl := φ (cid:0)yl−1(cid:1) for l > 0, with x0 := x. Output is f (x, θ) := yL. See Fig. 5 (top) for L = 2. ∈ In this case K = 2L+1 (L+1 matrix-vector multiplications and L nonlinearities), Pl = W2 for l < L and OW for the top layer l = L, P = LW2 + OW, Yk = W for k < K and O for k = K, and Y LW + O. Finally, a single forward pass FP LW2 + OW time and W2 + OW memory. ∼ ∼ Plugging the above into the cost of the baseline algorithm Jacobian contraction in §3.2, we obtain FCN Jacobian contraction costs N2O2LW2 + N2O3W time; N2O2 + NOW2 + NO2W + NLW + LW2 memory. Similarly, the cost of NTK-vector products from §3.3 is FCN NTK-vector products cost N2OLW2 + N2O2W time; N2O2 + NOW2 + NO2W + NLW + LW2 memory. 7Note that NOP time saving is not reﬂected in the asymptotic cost since it is dominated by NO [FP] required to compute primi- tive output cotangents. However, as we will see in Fig. 1, it often provides a substantial practical beneﬁt, up to allowing to compute the NTK faster than computing the two Jacobians themselves. For Structured derivatives (§3.4), we additionally need to derive values of J and MJJMP. For an arbitrary primitive, J and MJJMP costs are derived by (1) looking up the type of structure in the primitive Jacobian in Table 4, followed by (2) extracting the costs for a given structure from Table 5 Fast Finite Width Neural Tangent Kernel Method Jacobian contraction NTK-vector products N2O [FP] Structured derivatives N O [FP] + MJJMP + NJ N2O2 + NOYk + NJk Memory N2O2 + NO (cid:2)Yk + Pl(cid:3) + NY + P N2O2 + NO (cid:2)Yk + Pl(cid:3) + NY + P l + NY + P Time N O [FP] + N2O2P Use when P < Y, small O FP < OP, large O, small N FP > OP, large O, large N Table 1. Generic NTK computation costs. NTK-vector products trade-off contractions for more FP. Structured derivatives usually save both time and memory. See §3.1.1 and §3.4 for notation, and §B for a glossary of symbols. Time Method Jacobian contraction NTK-vector products N2OLW2 + N2O2 W Structured derivatives N OLW2 + N2O2LW + N2O3 N2O2LW2 + N2O3W N2O2 + NOW2 + NO2W + NLW + LW2 N2O2 + NOW2 + NO2W + NLW + LW2 N2O2 + NOW Use when Don’t O > W or N = 1 + NLW + LW2 O < W or L = 1 Memory Table 2. FCN NTK computation cost. The costs are obtained by substituting into Table 1 speciﬁc values for FP, P, Y, J, and MJJMP that correspond to an FCN as described in §4.1. NTK-vector products allow a reduction of the time complexity, while Structured derivatives reduce both time and memory complexity. See Fig. 1 for empirical conﬁrmation with FLOPs and wall-clock time. See §4.1 for discussion and notation (§B for the full glossary of symbols). Method Time Jacobian contraction N O (cid:2)LDFW2 + OW(cid:3) + N2O2 (cid:2)LFW2 + OW(cid:3) NTK-vector products N2O (cid:2)LDFW2 + OW(cid:3) Structured derivatives N O (cid:2)LDFW2 + OW(cid:3) + N2O2 (cid:104) L min (cid:16) FW2, DW + DFW2 O , DW + D2W O + D2FW O2 Memory Use when N2O2 + NO (cid:2)DW + FW2 + OW(cid:3) + N [LDW] + (cid:2)LFW2 + OW2(cid:3) D > OW N2O2 + NO (cid:2)DW + FW2 + OW(cid:3) + N [LDW] + (cid:2)LFW2 + OW2(cid:3) N = 1 (cid:17) (cid:105) + O N2O2 + NO [DW] + NDFW + N [LDW] + (cid:2)LFW2 + OW2(cid:3) D < OW Table 3. CNN NTK computation cost for a CNN with D pixels and ﬁlter size F. Structured derivatives reduce time complexity, and have lower memory cost if D < OW, which is a common setting. See Fig. 2 for experiments with ResNets, §4.2 for discussion, Table 2 for FCN, Table 1 for generic cost analysis, and §B for a glossary of symbols. (see §E). We apply this formal approach in §E.9, but for demonstration purposes below present the derivation that does not require referencing the Appendix. i = θlxl We ﬁrst note that we only need to consider k1 = k2 = 2l +1 indices in Eq. (10), since all other summands are zero due to absence of weight sharing between layers. For matrix- vector multiplication yl i our rules indicate (per ex- (cid:2)∂yl ample given in §3.4) that ∂yl , and command to only compute (cid:2)∂yl RW×W2 R1×W (which is xl i and J = 2 (cid:80)L ∼ Finally, the efﬁcient MJJMP for this structured ∂yl i/∂θl can be computed, analogously to Eq. (13), as follows for l < L: i/∂θl(cid:3) i/∂θl(cid:3) T in this case). Therefore J2l+1 1,:W ∈ 1,:W ∈ = W, i/∂θl = IW l=1 J2l+1 LW. ⊗ l l Θf (cid:124) θ [l, 2l + 1, 2l + 1] (cid:123)(cid:122) (cid:125) O×O = ∂f1 ∂yl 1 ∂yl 1 ∂θl ∂yl 2 ∂θl T T ∂f2 ∂yl 2 = (15)  IW    IW  T  T xl 2 (cid:124)(cid:123)(cid:122)(cid:125) 1×W ⊗ T xl 1 (cid:124)(cid:123)(cid:122)(cid:125) 1×W ⊗ = ∂f1 ∂yl 1 (cid:124)(cid:123)(cid:122)(cid:125) O×W = (16) T ∂f2 ∂yl 2 (cid:124) (cid:123)(cid:122) (cid:125) W×O = T  xl 1 (cid:124)(cid:123)(cid:122)(cid:125) 1×W xl 2 (cid:124)(cid:123)(cid:122)(cid:125) W×1   ∂f1 ∂yl 1 (cid:124)(cid:123)(cid:122)(cid:125) O×W , T ∂f2 ∂yl 2 (cid:124) (cid:123)(cid:122) (cid:125) W×O (17) which can be contracted in only O2W time. An analogous derivation applied to l = L yields O3 + W time. Therefore N2LO2W + N2O3, the total contraction cost is MJJMP when accounting for depth L and batch size N. Altogether, ∼ FCN Structured derivatives cost NOLW2 + N2O2LW + N2O3 time; N2O2 + NOW + NLW + LW2 memory. Summary. We summarize all FCN costs in Table 2. We conclude that Structured derivatives and NTK-vector prod- ucts allow a reduction in the time cost of NTK computation in different ways, while Structured derivatives also reduce memory requirements. Structured derivatives are beneﬁcial for wide networks, with large W, and NTK-vector prod- ucts are beneﬁcial for networks with large outputs O. We conﬁrm our predictions with FLOPs measurements in Fig. 1. We further conﬁrm our methods provide orders of magnitude speed-ups and memory savings on all major hardware platforms in Fig. 1 (right) and Fig. 3. However, we notice that time measurements often deviate from pre- dictions due to unaccounted constant overheads of various methods, hardware speciﬁcs, padding, and the behavior of the XLA compiler. We ﬁnd Structured derivatives to almost always outperform NTK-vector products. Fast Finite Width Neural Tangent Kernel FLOPs (per NTK entry) Wall-clock time (TPUv3) Figure 1. FLOPs (left) and wall-clock time (right) of computing the NTK for a 10-layer ReLU FCN. As predicted by Table 2, our methods almost always outperform Jacobian contraction, allowing orders of magnitude speed-ups and memory improvements for realistic problem sizes. Left: FLOPs per NTK entry. We conﬁrm several speciﬁc theoretical predictions from §4.1: Right: Wall-clock runtime. XLA compiler and hardware speciﬁcs, we observe that: In real applications, given the 1. NTK-vector products are the best performing method for N = 1, and have cost equivalent to Jacobian for any width W or output size O (top row); 2. NTK-vector products offer an O-fold improvement over Jacobian contraction (left to right columns); 3. NTK-vector products are equivalent to Jacobian contrac- tion for O = 1 (leftmost column); 4. Structured derivatives outperform NTK-vector products iﬀ O < W (O = W are plotted as pale vertical lines, which is where Structured derivatives and NTK-vector products in- tersect); 5. Structured derivatives approach the cost of Jacobian in the limit of large width W (left to right); 6. All methods, as expected, scale quadratically with width W (pale grey dashed line depicts W2 scaling). More: see Fig. 3 for other hardware platforms, and §N for details. 1. NTK-vector products improve upon Jacobian contrac- tion for O > 1, but the effect is not perfectly robust (see bottom row for small W and Fig. 3, notably GPU plat- forms); 2. Structured derivatives robustly outperform all other meth- ods, including simply computing the Jacobian, as discussed in §3.4; 3. Structured derivatives have lower memory footprint, and reach up to 8x larger widths (bottom right; missing points indicate out-of-memory), i.e. can process models up to 64x larger than other methods, as discussed in §3.4; 4. All methods have a smaller memory footprint than Jaco- bian (see §3.1.3). O = 1 logits O = 16 logits O = 128 logits N = 1 b a t c h N = 1 6 b a t c h N = 1 2 8 b a t c h s P O L F 107 104 101 s P O L F 107 104 101 s P O L F 107 104 101 20 23 26 29 212 Width W 20 23 26 29 212 Width W 20 23 26 29 212 Width W 101 s d n o c e S 10 1 10 3 101 s d n o c e S 10 1 10 3 101 s d n o c e S 10 1 10 3 O = 1 logits O = 16 logits O = 128 logits N = 1 b a t c h N = 1 6 b a t c h N = 1 2 8 b a t c h 20 23 26 29 212 Width W 20 23 26 29 212 Width W 20 23 26 29 212 Width W Jacobian contraction NTK-vector products Structured derivatives Jacobian W = O W 2 O = 1 logits O = 16 logits O = 128 logits 101 s d n o c e S 10 1 10 3 101 s d n o c e S 10 1 10 3 101 s d n o c e S 10 1 10 3 N = 1 b a t c h s i z e N = 1 6 b a t c h s i z e N = 1 2 8 b a t c h s i z e 20 23 26 29 212 20 23 26 29 212 20 23 26 29 212 Width W Width W Width W Fast Finite Width Neural Tangent Kernel Figure 2. Wall-clock time of computing an NTK for several ResNet sizes on a pair of ImageNet images. Structured derivatives allow the NTK to be computed faster and for larger models (see bottom row – missing points indicate out-of-memory). NTK-vector products, as predicted in §3.3 and Table 1, are advantageous for large O (bottom row), but are suboptimal when the cost of the forward pass FP is large relative to the output size and the number of parameters OP, e.g. when there is a lot of weight sharing (see Table 3 and Table 1), which is the case for convolutions, notably for O = 1 (top). See Fig. 4 for more ImageNet models, §4.2 for CNN NTK complexity analysis, and §N for experimental details. 4.2. CNN NTK Complexity We perform analogous derivations for a CNN with W chan- nels, D pixels, and ﬁlter size F in §F. We arrive at Table 3, and make two observations speciﬁc to CNNs. First, the speeds of NTK-vector products and Jacobian con- traction are now much more similar, due to the higher cost of the forward pass FP relative to P (i.e. weight sharing), and how they perform will depend on the speciﬁc values of parameters. We conﬁrm this in our experiments on Ima- geNet models in §5, where NTK-vector products typically underperform for O = 1, but outperform for O = 1000. Secondly, Structured derivatives continue to perform faster than Jacobian contraction, but the relative memory costs depend on other hyperparameters, requiring D < OW. This is a common case for ImageNet with O = 1000, and is conﬁrmed in our experiments in Fig. 2 and Fig. 4 (bottom). 5. ImageNet Experiments In §4 we have derived asymptotic time and memory beneﬁts of NTK-vector products and Structured derivatives over the baseline Jacobian contraction for FCNs and CNNs. How- ever, contemporary architectures rarely resemble vanilla feedforward networks, but instead result in much more com- plex computational graphs comprised of many different primitives, making complexity analysis impractical. We therefore evaluate our methods in the wild, and con- ﬁrm computational beneﬁts on full ImageNet models in Fig. 2 (ResNets, He et al. (2016)) and Fig. 4 (WideRes- Nets, Zagoruyko & Komodakis (2016); Vision Transformers and Transformer-ResNet hybrids Dosovitskiy et al. (2021); Steiner et al. (2021); and MLP-Mixers Tolstikhin et al. (2021)). Computing the full O 1000 NTK is often only possible with Structured derivatives. O = 1000 × × 6. Implementation All algorithms are implemented in JAX8 (Bradbury et al., 2018) and integrated into Neural Tangents (Novak et al., 2020). Jacobian contraction and NTK-vector products are built with core operations such as vjp , jvp , and vmap . Structured derivatives are implemented as a Jaxpr inter- preter, built on top of the JAX reverse mode AD interpreter. Owing to the nuanced trade-offs between different methods in the general case, we release all implementations within a single function that allows the user to manually select imple- mentation. We also include an automated setting which will perform FLOPs analysis for each method at compilation time and automatically choose the most efﬁcient one. 7. Conclusion We have performed the ﬁrst extensive analysis of the com- putational complexity of the NTK, and have shown how it can be improved dramatically with mixed-order AD (NTK- vector products), or with a custom interpreter for more efﬁ- cient higher-order AD operations (Structured derivatives). The NTK computation is similar to many other objects of interest in machine learning, such as the Gauss-Newton or the Fisher Information matrix, and we look forward to extensions of our algorithms to more settings in future work. 8See §M for discussion about other frameworks. 10 2 s d n o c e S 10 3 10 4 s d n o c e S 1.7 × 100 1.6 × 100 1.5 × 100 1.4 × 100 1.3 × 100 1.2 × 100 1.1 × 100 100 NVIDIA V100 TPUv4 CPU 10 3 10 4 10 5 100 Jacobian contraction NTK-vector products Structured derivatives Jacobian 100 10 2 103 102 O = 1 l o g i t s O = 1 0 0 0 l o g i t s 18 34 50 101 152 200 18 34 50 101 152 200 18 34 50 101 152 200 ResNet depth ResNet depth ResNet depth Fast Finite Width Neural Tangent Kernel Acknowledgements We thank Lechao Xiao for useful discussion, review and comments on the initial version of this manuscript, and Jaehoon Lee for useful discussion and code review. We also thank Shaobo Hou for his work on and help with TF2Jax, and the JAX team for their help and advice on JAX and Jax2TF. References Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. Tensorﬂow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), 2016. Cited on page 2, 4, 26. Adlam, B., Lee, J., Xiao, L., Pennington, J., and Snoek, J. Exploring the uncertainty properties of neural networks’ implicit priors in the inﬁnite-width limit. In International Conference on Learning Representations, 2020. Cited on page 1, 23. Arﬁan, W. Ukraine vectors by vecteezy. https: //www.vecteezy.com/vector-art/7506324-stand- with-ukraine-text-with-ukraine-flag-ribbon- and-ukraine-map-vector-design-on-a-dark- blue-background, 2022. Cited on page 27. Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. On exact computation with an inﬁnitely wide neural net. In Advances in Neural Information Pro- cessing Systems, pp. 8141–8150. Curran Associates, Inc., 2019a. Cited on page 2. Arora, S., Du, S. S., Hu, W., Li, Z., and Wang, R. Fine- grained analysis of optimization and generalization for overparameterized two-layer neural networks. arXiv preprint arXiv:1901.08584, 2019b. Cited on page 24. Arora, S., Du, S. S., Li, Z., Salakhutdinov, R., Wang, R., and Yu, D. Harnessing the power of inﬁnitely wide deep nets on small-data tasks. In International Confer- ence on Learning Representations, 2020. URL https: //openreview.net/forum?id=rkl8sJBYvH. Cited on page 1. Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J., Buchlovsky, P., Budden, D., Cai, T., Clark, A., Dani- helka, I., Fantacci, C., Godwin, J., Jones, C., Hennigan, T., Hessel, M., Kapturowski, S., Keck, T., Kemaev, I., King, M., Martens, L., Mikulik, V., Norman, T., Quan, J., Papamakarios, G., Ring, R., Ruiz, F., Sanchez, A., Schneider, R., Sezener, E., Spencer, S., Srinivasan, S., Stokowiec, W., and Viola, F. The DeepMind JAX Ecosys- tem, 2020. URL http://github.com/deepmind. Cited on page 27. Bahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, arXiv preprint U. Explaining neural scaling laws. arXiv:2102.06701, 2021. Cited on page 1. Bai, J., Lu, F., Zhang, K., et al. Onnx: Open neural network exchange. https://github.com/onnx/onnx, 2019. Cited on page 27. Belkin, M., Hsu, D., Ma, S., and Mandal, S. Reconciling modern machine-learning practice and the classical bias– variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849–15854, 2019. Cited on page 1. Borovykh, A. A gaussian process perspective on convolu- tional neural networks. arXiv preprint arXiv:1810.10798, 2018. Cited on page 1. Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., and Wanderman-Milne, S. JAX: com- posable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. Cited on page 2, 8, 26, 27. Brock, A., De, S., and Smith, S. L. Characterizing signal propagation to close the performance gap in unnormal- ized resnets. arXiv preprint arXiv:2101.08692, 2021a. Cited on page 1. Brock, A., De, S., Smith, S. L., and Simonyan, K. High- performance large-scale image recognition without nor- malization. arXiv preprint arXiv:2102.06171, 2021b. Cited on page 1. Chen, W., Gong, X., and Wang, Z. Neural architecture search on imagenet in four gpu hours: A theoretically In International Conference on inspired perspective. Learning Representations, 2021a. Cited on page 2, 26. Chen, X., Hsieh, C.-J., and Gong, B. When vision trans- formers outperform resnets without pretraining or strong data augmentations, 2021b. Cited on page 1, 2, 26. Dauphin, Y. N. and Schoenholz, S. Metainit: Initializing learning by learning to initialize. Advances in Neural Information Processing Systems, 32, 2019. Cited on page 1, 26. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Cited on page 2. Fast Finite Width Neural Tangent Kernel Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for In International Confer- image recognition at scale. ence on Learning Representations, 2021. URL https: //openreview.net/forum?id=YicbFdNTTy. Cited on page 2, 8, 14. Du, S. S., Hou, K., Salakhutdinov, R. R., Poczos, B., Wang, R., and Xu, K. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. Advances in neural information processing systems, 32, 2019. Cited on page 24. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Pre- cup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, vol- ume 70 of Proceedings of Machine Learning Research, pp. 1126–1135. PMLR, 06–11 Aug 2017. URL https: //proceedings.mlr.press/v70/finn17a.html. Cited on page 2. Franceschi, J.-Y., de B´ezenac, E., Ayed, I., Chen, M., Lam- prier, S., and Gallinari, P. A neural tangent kernel per- spective of gans. arXiv preprint arXiv:2106.05566, 2021. Cited on page 1. Frostig, R., Johnson, M. J., Maclaurin, D., Paszke, A., and Radul, A. Decomposing reverse-mode automatic differ- entiation. arXiv preprint arXiv:2105.09469, 2021. Cited on page 4, 26. Garriga-Alonso, A., Aitchison, L., and Rasmussen, C. E. Deep convolutional networks as shallow gaussian pro- cesses. In International Conference on Learning Repre- sentations, 2019. Cited on page 1. Griewank, A. and Walther, A. Evaluating Deriva- tives. Society for Industrial and Applied Math- ematics, 10.1137/ second edition, 2008. 1.9780898717761. URL https://epubs.siam.org/ doi/abs/10.1137/1.9780898717761. Cited on page 3, 17. doi: Grosse, R. Neural net Jan- URL https://www.cs.toronto.edu/ training dynamics, uary 2021. ∼rgrosse/courses/csc2541 2021/readings/ L02 Taylor approximations.pdf. 25. Cited on page Hanin, B. and Nica, M. Finite depth and width corrections In International Confer- to the neural tangent kernel. ence on Learning Representations, 2020. URL https: //openreview.net/forum?id=SJgndT4KwB. Cited on page 24. He, B., Lakshminarayanan, B., and Teh, Y. W. Bayesian In deep ensembles via the neural tangent kernel. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Infor- mation Processing Systems 33: Annual Conference Information Processing Systems 2020, on Neural NeurIPS 2020, December 6-12, 2020, virtual, 2020. https://proceedings.neurips.cc/paper/ URL 2020/hash/0b1ec366924b26fc98fa7b71a9c249cf- Abstract.html. Cited on page 1. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Cited on page 2, 8. Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre, B., Steiner, A., and van Zee, M. Flax: A neural network library and ecosystem for JAX, 2020. URL http:// github.com/google/flax. Cited on page 24, 27. Hennigan, T., Cai, T., Norman, T., and Babuschkin, I. Haiku: Sonnet for JAX, 2020. URL http://github.com/ deepmind/dm-haiku. Cited on page 24. Horace He, R. Z. functorch: Jax-like composable function transforms for pytorch. https://github.com/pytorch/ functorch, 2021. Cited on page 27. Hron, J., Bahri, Y., Novak, R., Pennington, J., and Sohl- Dickstein, J. Exact posterior distributions of wide bayesian neural networks, 2020a. Cited on page 1. Hron, J., Bahri, Y., Sohl-Dickstein, J., and Novak, R. Inﬁnite attention: NNGP and NTK for deep attention networks. In International Conference on Machine Learning, 2020b. Cited on page 1, 24. Hu, J., Shen, J., Yang, B., and Shao, L. Inﬁnitely wide graph convolutional networks: semi-supervised learning via gaussian processes. arXiv preprint arXiv:2002.12168, 2020. Cited on page 1. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent ker- nel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems, 2018. Cited on page 1, 23, 26. Khan, M. E. E., Immer, A., Abedi, E., and Korzepa, M. Approximate inference turns deep networks into gaussian processes. In Advances in neural information processing systems, 2019. Cited on page 1. Lee, J., Bahri, Y., Novak, R., Schoenholz, S., Pennington, J., and Sohl-dickstein, J. Deep neural networks as gaus- sian processes. In International Conference on Learning Representations, 2018. Cited on page 1. Fast Finite Width Neural Tangent Kernel Lee, J., Xiao, L., Schoenholz, S. S., Bahri, Y., Novak, R., Sohl-Dickstein, J., and Pennington, J. Wide neural net- works of any depth evolve as linear models under gradient descent. In Advances in Neural Information Processing Systems, 2019. Cited on page 1, 2, 23, 24, 26. Lee, J., Schoenholz, S., Pennington, J., Adlam, B., Xiao, L., Novak, R., and Sohl-Dickstein, J. Finite versus inﬁnite neural networks: an empirical study. Advances in Neural Information Processing Systems, 33:15156–15172, 2020. Cited on page 24. Maclaurin, D., Duvenaud, D., and Adams, R. P. Auto- In ICML 2015 grad: Effortless gradients in numpy. AutoML Workshop, 2015. URL https://github.com/ HIPS/autograd. Cited on page 3. Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In Interna- tional conference on machine learning, pp. 2408–2417. PMLR, 2015. Cited on page 25. Matthews, A., Hron, J., Rowland, M., Turner, R. E., and Ghahramani, Z. Gaussian process behaviour in wide deep neural networks. In International Conference on Learning Representations, 2018. Cited on page 1. M¨untz, H. Solution directe de l’´equation s´eculaire et de quelques probl`emes analogues transcendants. C. R. Acad. Sci. Paris, 156:43–46, 1913. Cited on page 26. Naumann, U. Optimal accumulation of jacobian matrices by elimination methods on the dual computational graph. Mathematical Programming, 99(3):399–421, 2004. Cited on page 2. Naumann, U. Optimal jacobian accumulation is np- complete. Mathematical Programming, 112(2):427–441, 2008. Cited on page 2. Neal, R. M. Priors for inﬁnite networks (tech. rep. no. crg- tr-94-1). University of Toronto, 1994. Cited on page 1. Nguyen, T., Chen, Z., and Lee, J. Dataset meta- learning from kernel ridge-regression. arXiv preprint arXiv:2011.00050, 2020. Cited on page 1. Nguyen, T., Novak, R., Xiao, L., and Lee, J. Dataset distil- lation with inﬁnitely wide convolutional networks. arXiv preprint arXiv:2107.13034, 2021. Cited on page 1. Novak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G., Hron, J., Abolaﬁa, D. A., Pennington, J., and Sohl-Dickstein, J. Bayesian deep convolutional networks with many chan- nels are gaussian processes. In International Conference on Learning Representations, 2019. Cited on page 1, 24. Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A. A., Sohl-Dickstein, J., and Schoenholz, S. S. Neural tangents: Fast and easy inﬁnite neural networks in python. In International Conference on Learning Repre- sentations, 2020. URL https://github.com/google/ neural-tangents. Cited on page 1, 2, 8, 24, 27. Oreshkin, B. N., L´opez, P. R., and Lacoste, A. Tadam: Task dependent adaptive metric for improved few-shot learning. In NeurIPS, 2018. Cited on page 2. Park, D. S., Lee, J., Peng, D., Cao, Y., and Sohl-Dickstein, J. Towards nngp-guided neural architecture search. arXiv preprint arXiv:2011.06006, 2020. Cited on page 1, 2. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch- an-imperative-style-high-performance-deep- learning-library.pdf. Cited on page 2, 4, 26. Pennington, J. and Bahri, Y. Geometry of neural net- work loss surfaces via random matrix theory. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learn- ing, volume 70 of Proceedings of Machine Learn- ing Research, pp. 2798–2806. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/ pennington17a.html. Cited on page 25. Radul, A., Paszke, A., Frostig, R., Johnson, M., and Maclau- rin, D. You only linearize once: Tangents transpose to gradients. arXiv preprint arXiv:2204.10923, 2022. Cited on page 4, 17. Schoenholz, S. S., Gilmer, J., Ganguli, S., and Sohl- Dickstein, J. Deep information propagation. Interna- tional Conference on Learning Representations, 2017. Cited on page 1. Spigler, S., Geiger, M., d’Ascoli, S., Sagun, L., Biroli, G., and Wyart, M. A jamming transition from under-to over- parametrization affects generalization in deep learning. Journal of Physics A: Mathematical and Theoretical, 52 (47):474001, 2019. Cited on page 1. Steiner, A., Kolesnikov, A., Zhai, X., Wightman, R., Uszkor- eit, J., and Beyer, L. How to train your vit? data, augmen- tation, and regularization in vision transformers. arXiv preprint arXiv:2106.10270, 2021. Cited on page 8, 14. Fast Finite Width Neural Tangent Kernel Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J. T., and Ng, R. Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS, 2020. Cited on page 1. Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., Lucic, M., and Dosovitskiy, A. Mlp-mixer: An all-mlp architecture for vision, 2021. Cited on page 2, 8, 14. Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., and Pennington, J. Dynamical isometry and a mean ﬁeld theory of CNNs: How to train 10,000-layer vanilla con- volutional neural networks. In International Conference on Machine Learning, 2018. Cited on page 1. Xiao, L., Pennington, J., and Schoenholz, S. S. Disentan- gling trainability and generalization in deep learning. In International Conference on Machine Learning, 2020. Cited on page 1, 23. Yaida, S. Non-Gaussian processes and neural networks at ﬁnite widths. In Mathematical and Scientiﬁc Machine Learning Conference, 2020. Cited on page 24. Yang, G. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient in- dependence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760, 2019. Cited on page 1, 24. Yang, G. Tensor programs ii: Neural tangent kernel for any architecture. arXiv preprint arXiv:2006.14548, 2020. Cited on page 24. Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., and Schoenholz, S. S. A mean ﬁeld theory of batch nor- In International Conference on Learning malization. Representations, 2019. Cited on page 24. Zagoruyko, S. and Komodakis, N. Wide residual networks. In British Machine Vision Conference, 2016. Cited on page 2, 8, 14. Zhang, H., Dauphin, Y. N., and Ma, T. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019. Cited on page 1. Zhou, Y., Wang, Z., Xian, J., Chen, C., and Xu, J. In In- Meta-learning with neural ternational Conference on Learning Representations, URL https://openreview.net/forum?id= 2021. Ti87Pv5Oc8. Cited on page 2, 26. tangent kernels. Zoph, B. and Le, Q. V. Neural architecture search with reinforcement learning, 2016. URL http://arxiv.org/ abs/1611.01578. Cited on page 2. Fast Finite Width Neural Tangent Kernel Appendix A. Additional Figures CPU (Skylake) NVIDIA V100 TPUv4 NVIDIA P100 Figure 3. Wall-clock time of computing the NTK of a 10-layer ReLU FCN on different platforms. In all settings, Structured derivatives allow orders of magnitude improvement in wall-clock time and memory (missing points indicate out-of-memory error). However, we remark that on GPU platforms (right), NTK-vector products deliver a robust improvement only for large O (rightmost column), while for O = 16 the cost is comparable or even larger than Jacobian contraction. See Fig. 1 for FLOPs, TPUv3 platform, and more discussion. See §N for details. 103 100 10 3 103 100 10 3 103 100 10 3 s d n o c e S s d n o c e S s d n o c e S O = 1 logits O = 16 logits O = 128 logits N = 1 b a t c h N = 1 6 b a t c h N = 1 2 8 b a t c h 20 23 26 29 212 Width W 20 23 26 29 212 Width W 20 23 26 29 212 Width W 102 100 10 2 102 100 10 2 102 100 10 2 s d n o c e S s d n o c e S s d n o c e S O = 1 logits O = 16 logits O = 128 logits N = 1 b a t c h N = 1 6 b a t c h N = 1 2 8 b a t c h 20 23 26 29 212 Width W 20 23 26 29 212 Width W 20 23 26 29 212 Width W O = 1 logits O = 16 logits O = 128 logits N = 1 b a t c h N = 1 6 b a t c h N = 1 2 8 b a t c h s d n o c e S 10 1 10 3 s d n o c e S 10 1 10 3 s d n o c e S 10 1 10 3 20 23 26 29 212 Width W 20 23 26 29 212 Width W 20 23 26 29 212 Width W O = 1 logits O = 16 logits O = 128 logits N = 1 b a t c h N = 1 6 b a t c h N = 1 2 8 b a t c h s d n o c e S s d n o c e S s d n o c e S 102 100 10 2 102 100 10 2 102 100 10 2 20 23 26 29 212 Width W 20 23 26 29 212 Width W 20 23 26 29 212 Width W Jacobian contraction NTK-vector products Structured derivatives Jacobian W = O W 2 O = 1 logits O = 16 logits O = 128 logits 101 s d n o c e S 10 1 10 3 101 s d n o c e S 10 1 10 3 101 s d n o c e S 10 1 10 3 N = 1 b a t c h s i z e N = 1 6 b a t c h s i z e N = 1 2 8 b a t c h s i z e 20 23 26 29 212 20 23 26 29 212 20 23 26 29 212 Width W Width W Width W Fast Finite Width Neural Tangent Kernel Figure 4. Wall-clock time per input pair of computing NTK on various ImageNet models like Vision Tansformers and hybrids (Dosovitskiy et al., 2021; Steiner et al., 2021), WideResNets (Zagoruyko & Komodakis, 2016) and MLP-Mixers (Tolstikhin et al., 2021). Structured derivatives generally allow fastest computation, but also are able to process more models due to lower memory requirements (lower left; missing points indicate out-of-memory error). For the case of single output logit O = 1 (top row), NTK-vector products are generally detrimental due to a costly forward pass FP relative to the size of parameters P (i.e. a lot of weight sharing; see Table 1). However, since NTK-vector products scale better than other methods with output size, for O = 1000 (bottom row), they perform comparably or better than other methods. Finally, we remark that the Jacobian not only runs out of memory faster, but can also take more time to compute. We conjecture that due to a larger memory footprint, XLA can sometimes perform optimizations that trade off speed for memory, and therefore compute the Jacobian in a less optimal way than if it had more memory available. Alternatively, XLA could also be performing simpliﬁcations of the NTK expression in these cases, such that those would not be possible in Jacobian computation alone. See Fig. 2 for ResNets, and §N for details. Figure 5. Notation used in §4.1 (FCN, top) and §F (CNN, bottom). In both settings L = 2. For FCN, K = 5 (3 matrix multiplication primitives and two nonlinearities φ), D = F = 1. For CNN, there is an extra global average pooling primitive as the penultimate layer, therefore K = 6, and D = 8, F = 3. NVIDIA V100 TPUv4 CPU s d n o c e S 10 2 10 4 s d n o c e 100S 10 3 10 5 100 10 1 Jacobian contraction NTK-vector products Structured derivatives Jacobian 100 10 2 104 103 102 O = 1 l o g i t s O = 1 0 0 0 l o g i t s 6 1 _ i T - T V + R i 6 1 _ i T - T V i 2 3 _ S - T V i 6 1 _ S - T V i 2 3 _ B - T V i 2 3 _ L - T V i 6 1 _ B - r e x M i 6 1 _ L - T V i 6 1 _ L - r e x M i 0 1 _ 8 2 _ n r w 2 1 _ 8 2 _ n r w 4 1 _ H - T V i i 6 1 _ B - T V + 0 5 R i 2 3 _ L - T V + 0 5 R 6 1 _ B - T V i i 2 3 _ B - T V + 6 2 R i 2 3 _ S - T V + 6 2 R Model 6 1 _ i T - T V + R i 6 1 _ i T - T V i 2 3 _ S - T V i 6 1 _ S - T V i 2 3 _ B - T V i 2 3 _ L - T V i 6 1 _ B - r e x M i 6 1 _ L - T V i 6 1 _ L - r e x M i 0 1 _ 8 2 _ n r w 2 1 _ 8 2 _ n r w 4 1 _ H - T V i i 6 1 _ B - T V + 0 5 R i 2 3 _ L - T V + 0 5 R 6 1 _ B - T V i i 2 3 _ B - T V + 6 2 R i 2 3 _ S - T V + 6 2 R Model 6 1 _ i T - T V + R i 6 1 _ i T - T V i 2 3 _ S - T V i 6 1 _ S - T V i 2 3 _ B - T V i 2 3 _ L - T V i 6 1 _ B - r e x M i 6 1 _ L - T V i 6 1 _ L - r e x M i 0 1 _ 8 2 _ n r w 2 1 _ 8 2 _ n r w 4 1 _ H - T V i i 6 1 _ B - T V + 0 5 R i 2 3 _ L - T V + 0 5 R 6 1 _ B - T V i i 2 3 _ B - T V + 6 2 R i 2 3 _ S - T V + 6 2 R Model 4 = < l a t e x i t s h a 1 _ b a s e 6 4 = "" D R a Y i w Z Y S Z q S 2 p V k f L w Q v A u Q a + Q = "" > A A A C 9 X i c d Z L L j t M w F I b d c J k h 3 G Z g y c a i Q m J V J S M E b E Y z A h Y s K h g k O h 2 p i U a 2 4 7 T W + C b b o V R W X o E t b F i B 2 P I Y P A J C v A x O 0 w V p 4 E i W f p 3 z H Z + L j T V n 1 i X J r 0 F 0 6 f K V q z u 7 1 + L r N 2 7 e u r 2 3 f + f U q s o Q O i G K K 3 O G k a W c S T p x z H F 6 p g 1 F A n M 6 x R f P m / j 0 H T W W K f n W r T T N B Z p L V j K C X O P K 5 O G j 8 7 1 h M k r W B v s i 3 Y j h 0 Y / 4 U H / 5 G Z + c 7 w 9 + Z 4 U i l a D S E Y 6 s n a W J d r l H x j H C a R 1 n l a U a k Q s 0 p 7 M g J R L U 5 n 7 d b A 0 f B E 8 B S 2 X C k Q 6 u v X 9 n e C S s X Q k c S I H c w m 7 H G u c / Y x i L r d K u f J p 7 J n X l q C R t 5 b L i 0 C n Y r A I W z F D i + C o I R A w L z U O y Q A Y R F x b 2 n y H e t 1 P E c S b p k i g h k C x 8 J u t Z k n u f r Y N e V 0 a H N f g M l / B V X d d d t i g 6 s K O I t + i L H q p U B 8 W 8 2 t z 6 u o e O x x 1 U G S T n G 3 j c g 5 c d l q y Q b M l p Q 4 b v k G 4 / f l + c H o z S x 6 O D N 8 n w + B l o b R f c A / f B Q 5 C C J + A Y v A Q n Y A I I W I A P 4 C P 4 F C 2 j z 9 H X 6 F u L R o N N z l 3 Q s e j 7 H 8 h v + w I = < / l a t e x i t > N 4 = < l a t e x i t s h a 1 _ b a s e 6 4 = "" D R a Y i w Z Y S Z q S 2 p V k f L w Q v A u Q a + Q = "" > A A A C 9 X i c d Z L L j t M w F I b d c J k h 3 G Z g y c a i Q m J V J S M E b E Y z A h Y s K h g k O h 2 p i U a 2 4 7 T W + C b b o V R W X o E t b F i B 2 P I Y P A J C v A x O 0 w V p 4 E i W f p 3 z H Z + L j T V n 1 i X J r 0 F 0 6 f K V q z u 7 1 + L r N 2 7 e u r 2 3 f + f U q s o Q O i G K K 3 O G k a W c S T p x z H F 6 p g 1 F A n M 6 x R f P m / j 0 H T W W K f n W r T T N B Z p L V j K C X O P K 5 O G j 8 7 1 h M k r W B v s i 3 Y j h 0 Y / 4 U H / 5 G Z + c 7 w 9 + Z 4 U i l a D S E Y 6 s n a W J d r l H x j H C a R 1 n l a U a k Q s 0 p 7 M g J R L U 5 n 7 d b A 0 f B E 8 B S 2 X C k Q 6 u v X 9 n e C S s X Q k c S I H c w m 7 H G u c / Y x i L r d K u f J p 7 J n X l q C R t 5 b L i 0 C n Y r A I W z F D i + C o I R A w L z U O y Q A Y R F x b 2 n y H e t 1 P E c S b p k i g h k C x 8 J u t Z k n u f r Y N e V 0 a H N f g M l / B V X d d d t i g 6 s K O I t + i L H q p U B 8 W 8 2 t z 6 u o e O x x 1 U G S T n G 3 j c g 5 c d l q y Q b M l p Q 4 b v k G 4 / f l + c H o z S x 6 O D N 8 n w + B l o b R f c A / f B Q 5 C C J + A Y v A Q n Y A I I W I A P 4 C P 4 F C 2 j z 9 H X 6 F u L R o N N z l 3 Q s e j 7 H 8 h v + w I = < / l a t e x i t > N 4 = < l a t e x i t s h a 1 _ b a s e 6 4 = "" D R a Y i w Z Y S Z q S 2 p V k f L w Q v A u Q a + Q = "" > A A A C 9 X i c d Z L L j t M w F I b d c J k h 3 G Z g y c a i Q m J V J S M E b E Y z A h Y s K h g k O h 2 p i U a 2 4 7 T W + C b b o V R W X o E t b F i B 2 P I Y P A J C v A x O 0 w V p 4 E i W f p 3 z H Z + L j T V n 1 i X J r 0 F 0 6 f K V q z u 7 1 + L r N 2 7 e u r 2 3 f + f U q s o Q O i G K K 3 O G k a W c S T p x z H F 6 p g 1 F A n M 6 x R f P m / j 0 H T W W K f n W r T T N B Z p L V j K C X O P K 5 O G j 8 7 1 h M k r W B v s i 3 Y j h 0 Y / 4 U H / 5 G Z + c 7 w 9 + Z 4 U i l a D S E Y 6 s n a W J d r l H x j H C a R 1 n l a U a k Q s 0 p 7 M g J R L U 5 n 7 d b A 0 f B E 8 B S 2 X C k Q 6 u v X 9 n e C S s X Q k c S I H c w m 7 H G u c / Y x i L r d K u f J p 7 J n X l q C R t 5 b L i 0 C n Y r A I W z F D i + C o I R A w L z U O y Q A Y R F x b 2 n y H e t 1 P E c S b p k i g h k C x 8 J u t Z k n u f r Y N e V 0 a H N f g M l / B V X d d d t i g 6 s K O I t + i L H q p U B 8 W 8 2 t z 6 u o e O x x 1 U G S T n G 3 j c g 5 c d l q y Q b M l p Q 4 b v k G 4 / f l + c H o z S x 6 O D N 8 n w + B l o b R f c A / f B Q 5 C C J + A Y v A Q n Y A I I W I A P 4 C P 4 F C 2 j z 9 H X 6 F u L R o N N z l 3 Q s e j 7 H 8 h v + w I = < / l a t e x i t > N <latexit sha1_base64=""N7dWDDp5SaZjiomhSQBw4NYjymw="">AAACs3icdZHLTttAFIYnbrk0XArtslVlgZBYRXYkCkvUbrrIAqQGkGILnRmfJCPmppkxEFledttt2zfhVfoMfYlOEha1KUca6dd/vtG5USO480nyuxO9eLmyurb+qruxubX9emf3zYXTpWU4ZFpoe0XBoeAKh557gVfGIkgq8JLefJ7nL2/ROq7VVz8zmEuYKD7mDPzcysyUX+/sJ71kEfFTkT6K/dP3D+d/vn14OLve7fzKCs1KicozAc6N0sT4vALrORNYd7PSoQF2AxMcBalAosurRbN1fBCcIh5rG57y8cL990cF0rmZpIGU4KeunZub/81RKlul/fgkr7gypUfFlpXHpYi9jueriAtukXkxCwKY5aH5mE3BAvNhYc8Mcb+cotvNFN4xLSWoospUPUryqsoWycqU1oQ1qLpuUkXRwDyCqIs2pHUDoqLEWrehwaABaQtqgvUgYOGYaft0T8VFv5d+7PXPw1U/kWWsk3dkjxySlByTU/KFnJEhYWRKvpMf5Gd0FI0iGhVLNOo8/nlLGhHJv52w4M8=</latexit>",0.0
"
        This AI Chip Hits New Ultralow Power Lows
    ",https://spectrum.ieee.org/low-power-ai-spiking-neural-net,2022-06-15,,"Its neural spikes take just 0.02 percent of the energy needed to fire comparable neural networks Researchers at the Indian Institute of Technology, Bombay (including professors Maryam Shojaei Baghini and Udayan Ganguly, standing) have announced a new, low-power AI chip, pictured here, that enables what are called spiking neural networks or SNNs. This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore. Our brains may not be big, but they pack a lot of computing power, given their size. For this reason, many researchers have been interested in creating artificial networks that mimic the neural signal processing of the brain. These artificial networks, called spiking neural networks (SNNs), could be used to create intelligent robots, or to better understand the underpinnings of the brain itself. However, the brain has 100 billion tiny neurons, each of which are connected to 10,000 other neurons via synapses and which represent information through coordinated patterns of electrical spiking. Mimicking these neurons using hardware on a compact device—all while ensuring that computation is done in an energy-efficient manner—has proven challenging. In a recent study, researchers in India achieved ultralow-energy artificial neurons that allow the SNNs to be more compactly arranged. The results are published 25 May in IEEE Transactions on Circuits and Systems I: Regular Papers. Just as neurons in the brain spike at a given energy threshold, SNNs rely on a network of artificial neurons where a current source charges up a leaky capacitor until a threshold level is hit and the artificial neurons fires, and the stored charge is reset to zero. However, many existing SNNs require large transistor currents to charge up their capacitors, which leads to high power consumption or artificial neurons that fire too quickly. In their study, Udayan Ganguly, a professor at the Indian Institute of Technology, Bombay, and his colleagues created a SNN that relies on a new and compact current source to charge capacitors, called band-to-band-tunneling (BTBT) current. With BTBT, quantum tunneling current charges up the capacitor with ultralow current, meaning less energy is required. Quantum tunneling in this case means the current can flow through the forbidden gap in the silicon of the artificial neuron via quantum-wave-like behavior. The BTBT approach also spares the need for large capacitors to store high amounts of current, paving the way for smaller capacitors on a chip and thus saving space. When the researchers tested their BTBT neuron approach using 45-nanometer commercial silicon-on-insulator transistor technology, they saw substantial energy and space savings. “In comparison to state-of-art [artificial] neurons implemented in hardware spiking neural networks, we achieved 5,000 times lower energy per spike at a similar area and 10 times lower standby power at a similar area and energy per spike,” explains Ganguly. The researchers then applied their SNN to a model of speech recognition inspired by the brain’s auditory cortex. Using 20 artificial neurons for initial input coding and 36 additional artificial neurons, the model could effectively recognize spoken words, demonstrating real-world feasibility of the approach. Notably, this type of technology could be well suited for a range of applications, including voice-activity detection, speech classification, motion-pattern recognition, navigation, biomedical signals, classification, and more. And Ganguly notes that while these applications can be done using current servers and supercomputers, SNNs could enable these applications to be used with edge devices, such as mobile phones and IoT sensors—and especially when energy constraints are tight. He says that while his team has shown their BTBT approach to be usefulfor specific applications such as keyword detection, they are interested in demonstrating a general-purpose reusable neurosynaptic core for a wide variety of applications and clients, and have created a startup company, called Numelo Tech, to drive commercialization. Their goal, he says, “is an extremely low-power neurosynaptic core and developing a real-time on-chip learning mechanism, which are key for autonomous biologically inspired neural networks. This is the holy grail.” Michelle Hampson is a freelance writer based in Halifax. She frequently contributes to Spectrum's Journal Watch coverage, which highlights newsworthy studies published in IEEE journals. The AI pioneer says it’s time for smart-sized, “data-centric” solutions to big issues Andrew Ng was involved in the rise of massive deep learning models trained on vast amounts of data, but now he’s preaching small-data solutions. Andrew Ng has serious street cred in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at Stanford University, cofounded Google Brain in 2011, and then served for three years as chief scientist for Baidu, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told IEEE Spectrum in an exclusive Q&A.",0.0
"
        Photonic Chip Performs Image Recognition at the Speed of Light
    ",https://spectrum.ieee.org/photonic-neural-network,2022-06-06,,"New photonic deep neural network could also analyze audio, video, and other data The chip uses a deep neural network of optical waveguides smaller than a square centimeter. The network can detect and classify an image in less than a nanosecond, without the need for a separate processor or memory unit. Deep neural networks that mimic the workings of the human brain now often power computer vision, speech recognition, and much more. However, they are increasingly limited by the hardware used to implement them. Now scientists have developed a deep neural network on a photonic microchip that can classify images in less than a nanosecond, roughly the same amount of time as a single tick of the kind of clocks found in state-of-the-art electronics. In artificial neural networks, components dubbed “neurons” are fed data and cooperate to solve a problem, such as recognizing faces. The neural net repeatedly adjusts the links between its neurons and sees if the resulting patterns of behavior are better at finding a solution. Over time, the network discovers which patterns are best at computing results. It then adopts these as defaults, mimicking the process of learning in the human brain. A neural network is called “deep” if it possesses multiple layers of neurons. Although these artificial-intelligence systems are increasingly finding real-world applications, they face a number of major challenges given the hardware used to run them. First, they are usually implemented using digital-clock-based platforms such as graphics processing units (GPUs), which limits their computation speed to the frequencies of the clocks—less than 3 gigahertz for most state-of-the-art GPUs. Second, unlike biological neurons—which can both compute and store data—conventional electronics separate memory and processing units. Shuttling data back and forth between these components wastes both time and energy. In addition, raw visual data usually needs to be converted to digital electronic signals, consuming time. Moreover, a large memory unit is often needed to store images and videos, raising potential privacy concerns. In a new study, researchers have developed a photonic deep neural network that can directly analyze images without the need for a clock, sensor, or large memory modules. It can classify an image in less than 570 picoseconds, which is comparable with a single clock cycle in state-of-the-art microchips. “It can classify nearly 2 billion images per second,” says study senior author Firooz Aflatouni, an electrical engineer at the University of Pennsylvania, in Philadelphia. “As a point of reference, the conventional video frame rate is 24 to 120 frames per second.” The new device marks the first deep neural network implemented entirely on an integrated photonic device in a scalable manner. The entire chip is just 9.3 square millimeters in size. An image of interest is projected onto a 5-by-6 pixel array and divided into four overlapping 3-by-4 pixel subimages. Optical channels, or waveguides, then route the pixels of each subimage to the device’s nine neurons. When the microchip is getting trained to recognize an image, for example, as one letter or another, an electrically controlled device adjusts how each neuron modifies the power of incoming light signals. By analyzing how the light from the image gets modified after passing through the microchip’s layers of neurons, one can read the microchip’s results. “Computation-by-propagation, where the computation takes place as the wave propagates through a medium, can perform computation at the speed of light,” Aflatouni says. The scientists had their microchip identify handwritten letters. In one set of tests, it had to classify 216 letters as either p or d, and in another, it had to classify 432 letters as either p, d, a, or t. The chip showed accuracies higher than 93.8 and 89.8 percent, respectively. In comparison, a 190-neuron conventional deep neural network implemented in Python using the Keras library achieved 96 percent accuracy on the same images. The researchers are now experimenting with classifying video and 3D objects with these devices, as well as using larger chips with more pixels and neurons to classify higher-resolution images. In addition, the applications of this technology “are not limited to image and video classification,” Aflatouni says. “Any signal such as audio and speech that could be converted to the optical domain can be classified almost instantaneously using this technology.” The scientists detailed their findings 1 June in the journal Nature. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. An almost Machiavellian plot pitted Fairchild against Texas Instruments In one corner stood the defending champion, Texas Instruments. In the other stood the challenger, Fairchild Semiconductor. The referee, judge, promoter, and only spectator was Polaroid. In contention was the contract for the electronics of Polaroid’s secret project—a pioneering product introduced in 1972 as the SX-70, a camera eventually purchased by millions of people. As the embodiment of truly automated instant photography, the SX-70 fulfilled a long-held dream of Edwin Land, founder of Polaroid Corp., Cambridge, Mass. Vital to this “point and shoot” capability was a new film—one that would develop while exposed to light and so eliminate the tear-away covers of previous Polaroid films. Also vital were sophisticated electronics to control all single lens reflex (SLR) camera functions, including flashbulb selection, exposure control, mirror positioning, start of print development, and ejection of print. These circuits were divided into three modules, one each for motor, exposure and logic, and flash control. At the final count, some 400 transistors were used.",0.0
"
        World Builders Put Happy Face On Superintelligent AI
    ",https://spectrum.ieee.org/superintelligence-future-life-institute-contest,2022-05-25,,"The Future of Life Institute’s contest counters today’s dystopian doomscapes One of the biggest challenges in a world-building competition that asked teams to imagine a positive future with superintelligent AI: Make it plausible. The Future of Life Institute, a nonprofit that focuses on existential threats to humanity, organized the contest and is offering a hefty prize purse of up to US $140,000, to be divided among multiple winners. Last week FLI announced the 20 finalists from 144 entries, and the group will declare the winners on 15 June. “We’re not trying to push utopia. We’re just trying to show futures that are not dystopian, so people have something to work toward.”—Anna Yelizarova, Future of Life Institute The contest aims to counter the common dystopian narrative of artificial intelligence that becomes smarter than humans, escapes our control, and makes the world go to hell in one way or another. The philosopher Nick Bostrom famously imagined a factory AI turning all the world’s matter into paper clips to fulfill its objective, and many respected voices in the field, such as computer scientist Stuart Russell, have argued that it’s essential to begin work on AI safety now, before superintelligence is achieved. Add in the sci-fi novels, TV shows, and movies that tell dark tales of AI taking over—the Blade Runners, the Westworlds, the Terminators, the Matrices (both original recipe and Resurrections)—and it’s no wonder the public feels wary of the technology. Anna Yelizarova, who’s managing the contest and other projects at FLI, says she feels bombarded by images of dystopia in the media, and says it makes her wonder “what kind of effect that has on our worldview as a society.” She sees the contest partly as a way to provide hopeful visions of the future. “We’re not trying to push utopia,” she says, noting that the worlds built for the contest are not perfect places with zero conflicts or struggles. “We’re just trying to show futures that are not dystopian, so people have something to work toward,” she says. The contest asked a lot from the teams who entered: They had to provide a timeline of events from now until 2045 that includes the invention of artificial general intelligence (AGI), two “day in the life” short stories, answers to a list of questions, and a media piece reflecting their imagined world. Yelizarova says that another motivation for the contest was to see what sorts of ideas people would come up with. Imagining a hopeful future with AGI is inherently more difficult than imagining a dystopian one, she notes, because it requires coming up with solutions to some of the biggest challenges facing humanity. For example, how to ensure that world governments work together to deploy AGI responsibly and don’t treat its development as an arms race? And how to create AGI agents whose goals are aligned with those of humans? “If people are suggesting new institutions or new ways of tackling problems,” Yelizarova says, “those can become actual policy efforts we can pursue in the real world.” “For a truly positive transformative relationship with AI, it needs to help us—to help humanity—become better.... And the idea that such a world might be possible is a future that I want to fight for.”—Rebecca Rapple, finalist in the Future of Life Institute’s world-building contest It’s worth diving into the worlds created by the 20 finalists and browsing through the positive possible futures. IEEE Spectrum corresponded with two finalists who have very different visions. The first, a solo effort by Rebecca Rapple of Portland, Ore., imagines a world in which an AGI agent named TAI has a direct connection with nearly every human on earth via brain-computer interfaces. The world’s main currency is one of TAI’s devising, called Contribucks, which are earned via positive social contributions and which lose value the longer they’re stored. People routinely plug into a virtual experience called Communitas, which Rapple’s entry describes as “a TAI-facilitated ecstatic group experience where sentience communes, sharing in each other’s experiences directly through TAI.” While TAI is not directly under humans’ control, she has stated that “she loves every soul” and people both trust her and think she’s helping them to live better lives. Rapple, who describes herself as a pragmatic optimist, says that crafting her world was an uplifting process. “The assumption at the core of my world is that for a truly positive transformative relationship with AI, it needs to help us—to help humanity—become better,” she tells Spectrum. “Better to ourselves, our neighbors, our planet. And the idea that such a world might be possible is a future that I want to fight for.” The second team Spectrum corresponded with is a trio from Nairobi, Kenya: Conrad Whitaker, Dexter Findley, and Tracey Kamande. In the world imagined by this team, AGI emerged from a “new non–von Neumann computing paradigm” in which memory is fully integrated into processing. As an AGI agent describes it in one of the team's short stories, AGI has resulted “from the digital replication of human brain structure, with all its separate biological components, neural networks and self-referential loops. Nurtured in a naturalistic setting with constant positive human interaction, just like a biological human infant.” In this world there are over 1,000 AGIs, or digital humans, by the year 2045; the machine learning and neural networks that we know as AI today are widely used for optimization problems, but aren’t considered true, general-purpose intelligence. Those AIs, in so many words, are not AGI. But in the present scenario being imagined, many people live in AGI-organized “digital nations” that they can join regardless of their physical locations, and which bring many health and social benefits. In an email, the Kenyan team says they aimed to paint a picture of a future that is “strong on freedoms and rights for both humans and AGIs—going so far as imagining that a caring and respectful environment that encouraged unbridled creativity and discourse (conjecture and criticism) was critical to bringing an ‘artificial person’ to maturity in the first place.” They imagine that such AGI agents wouldn’t see themselves as separate from humans as they would be “humanlike” in both their experience of knowledge and their sense of self, and that the AGI agents would therefore have a humanlike capacity for moral knowledge. Meaning that these AGI agents would see the problem with turning all humans on earth into paper clips. Eliza Strickland is a senior editor at IEEE Spectrum, where she covers AI, biomedical engineering, and other topics. She holds a master's degree in journalism from Columbia University. The AI pioneer says it’s time for smart-sized, “data-centric” solutions to big issues Andrew Ng was involved in the rise of massive deep learning models trained on vast amounts of data, but now he’s preaching small-data solutions. Andrew Ng has serious street cred in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at Stanford University, cofounded Google Brain in 2011, and then served for three years as chief scientist for Baidu, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told IEEE Spectrum in an exclusive Q&A.",0.0
"
        AI’s Threats to Jobs and Human Happiness Are Real
    ",https://spectrum.ieee.org/kai-fu-lee-ai-jobs,2022-05-12,,"But short-term job chaos will give way to long-term prosperity, says AI expert Kai-Fu Lee Renowned computer scientist and AI expert Kai-Fu Lee sees likely disruption over the coming 15 to 20 years, owing to “smart” systems creating jobs in fields that AI-displaced workers may not be well trained to handle. There’s a movement afoot to counter the dystopian and apocalyptic narratives of artificial intelligence. Some people in the field are concerned that the frequent talk of AI as an existential risk to humanity is poisoning the public against the technology and are deliberately setting out more hopeful narratives. One such effort is a book that came out last fall called AI 2041: Ten Visions for Our Future. The book is cowritten by Kai-Fu Lee, an AI expert who leads the venture capital firm Sinovation Ventures, and Chen Qiufan, a science fiction author known for his novel Waste Tide. It has an interesting format. Each chapter starts with a science fiction story depicting some aspect of AI in society in the year 2041 (such as deepfakes, self-driving cars, and AI-enhanced education), which is followed by an analysis section by Lee that talks about the technology in question and the trends today that may lead to that envisioned future. It’s not a utopian vision, but the stories generally show humanity grappling productively with the issues raised by ever-advancing AI. IEEE Spectrum spoke to Lee about the book, focusing on the last few chapters, which take on the big issues of job displacement, the need for new economic models, and the search for meaning and happiness in an age of abundance. Lee argues that technologists need to give serious thought to such societal impacts, instead of thinking only about the technology. Kai-Fu Lee on… The science fiction stories are set in 2041, by which time you expect AI to have already caused a lot of disruption to the job market. What types of jobs do you think will be displaced by then? Kai-Fu Lee: Contrary to what a lot of people think, AI is actually just a piece of software that does routine work extremely well. So the jobs that will be the most challenged will be those that are routine and repetitive—and that includes both blue-collar and white-collar work. So obviously jobs like assembly line workers and people who operate the same equipment over and over again. And in terms of white-collar work, many entry-level jobs in accounting, paralegal, and other jobs where you’re repetitively moving data from one place to another, and jobs where you’re routinely dealing with people, such as customer-service jobs. Those are going to be the most challenged. If we add these up, it will be a very substantial portion of all jobs, even without major breakthroughs in AI—on the order of 40 to 50 percent. The jobs that are most secure are those that require imagination, creativity, or empathy. And until AI gets good enough, there will also be craftsman jobs that require dexterity and a high level of hand-eye coordination. Those jobs will be secure for a while, but AI will improve and eventually take those over as well. How do you imagine this trend is changing the engineering profession? Lee: I think engineering is largely cerebral and somewhat creative work that requires analytical skills and deep understanding of problems. And those are generally hard for AI. But if you’re a software engineer and most of your job is looking for pieces of code and copy-pasting them together—those jobs are in danger. And if you’re doing routine testing of software, those jobs are in danger too. If you’re writing a piece of code and it’s original creative work, but you know that this kind of code has been done before and can be done again, those jobs will gradually be challenged as well. For people in the engineering profession, this will push us towards more of an analytical architect role where we deeply understand the problems that are being solved, ideally problems that have complex characteristics and measurements. The ideal combination in most professions will be a human that has unique human capabilities managing a bunch of AI that do the routine parts. It reminds me of the Ph.D. thesis of Charles Simonyi, the person who created Microsoft Word. He did an experiment to see what would happen if you have a really smart architect who can divvy up the job of writing a piece of code into well-contained modules that are easy to understand and well defined, and then outsource each module to an average engineer. Will the resulting product be good? It was good. We’re talking about the same thing, except we’re not outsourcing to the average engineer, who will have been replaced by AI. That superengineer will be able to delegate the work to a bunch of AI resulting in creativity and symbiosis. But there won’t be very many of these architect jobs. In the book, you say that an entirely new social contract is needed. One problem is that there will be fewer entry-level jobs, but there still needs to be a way for people to gain skills. Can you imagine a solution for engineering? Lee: Let’s say someone is talented and could become an architect, but that person just graduated from college and isn’t there yet. If they apply for a job to do entry-level programming and they’re competing for the job with AI, they might lose the job to the AI. That would be really bad because we will not only hurt the person’s self-confidence, but also society will lose the talent of that architect, which needs years of experience to build up. But imagine if the company says, “We’re going to employ you anyway, even though you’re not as good as AI. We’re going to give you tasks and we’ll have AI work alongside you and correct your errors, and you can learn from it and improve.” If a thousand people go through this entry-level practical training, maybe a hundred emerge to be really good and be on their way to become architects. Maybe the other 900 will take longer and struggle, or maybe they’ll feel complacent and continue to do the work so they’re passing time and still have a chance to improve. Maybe some will say, “Hey, this is really not for me, I’m not reaching the architect level. I’m going to go become a photographer and artist or whatever.” Back to top Why do you think that this round of automation is different from those that came before in history, when jobs were both destroyed and created by automation? Lee: First of all, I do think AI will both destroy and create jobs. I just can’t enumerate which jobs and how many. I tend to be an optimist and believe in the wisdom and the will of the human race. Eventually, we’ll figure out a bunch of new jobs. Maybe those jobs don’t exist today and have to be invented; maybe some of those jobs will be service jobs, human-connection jobs. I would say that every technology so far has ended up making society better, and there has never been a problem of absorbing the job losses. If you look at a 30-year horizon, I’m optimistic that that there will not be a net job loss, but possibly a net gain, or possibly equal. And we can always consider a four-day work week and things like that. So long-term, I’m optimistic. Now to answer your question directly: short-term, I am worried. And the reason is that none of the previous technology revolutions have tried explicitly to replace people. No matter how people think about it, every AI algorithm is trying to display intelligence and therefore be able to do what people do. Maybe not an entire job, but some task. So naturally there will be a short-term drop when automation and AI start to work well. “If you expect an assembly-line worker to become a robot-repair person, it isn’t going to be so easy.” —Kai-Fu Lee, Sinovation Ventures Autonomous vehicles are an explicit effort to replace drivers. A lot of people in the industry will say, “Oh no, we need a backup driver in the truck to make it safer, so we won’t displace jobs.” Or they’ll say that when we install robots in the factory, the factory workers are elevated to a higher-level job. But I think they’re just sugarcoating the reality. Let’s say over a period of 20 years, with the advent of AI, we lose x number of jobs, and we also gain x jobs; let’s say the loss and gain are the same. The outcome is not that the society remains in equilibrium, because the jobs being lost are the most routine and unskilled. And the jobs being created are much more likely to be skilled and complex jobs that require much more training. If you expect an assembly-line worker to become a robot-repair person, it isn’t going to be so easy. That’s why I think the next 15 years or 20 years will be very chaotic. We need a lot of wisdom and long-term vision and decisiveness to overcome these problems. Back to top Currency There are some interesting experiments going on with universal basic income (UBI), like Sam Altman’s ambitious idea for Worldcoin. But from the book, it seems like you don’t think that UBI is the answer. Is that correct? Lee: UBI may be necessary, by it’s definitely not sufficient. We’re going to be in a world of very serious wealth inequality, and the people losing their jobs won’t have the experience or the education to get the right kinds of training. Unless we subsidize and help these people along, the inequality will be exacerbated. So how do we make them whole? One way is to make sure they don’t have to worry about subsistence. That’s where I think universal basic income comes into play by making sure nobody goes without food, shelter, water. I think that level of universal basic income is good. As I mentioned before, the people who are most devastated, people who don’t have skills, are going to need a lot of help. But that help isn’t just money. If you just give people money, a wonderful apartment, really great food, Internet, games, and even extra allowance to spend, they are much more likely to say, “Well, I’ll just stay home and play games. I’ll go into the metaverse.” They may even go to alcohol or substance abuse because those are the easiest things to do. So what else do they need? Lee: Imagine the mind-set of a person whose job was taken away by automation. That person has been to be thinking, “Wow, everything I know how to do, AI can do. Everything I learn, AI will be able to do. So why should I take the universal basic income and apply that to learning?” And even if that person does decide to get training, how can they know what to get training on? Imagine I’m an assembly-line worker and I lost my job. I might think, truck driver, that’s a highly paid job. I’ll do that. But then in five years those jobs are going to be gone. A robot-repair job would be a much more sustainable job than a truck driver, but the person who just lost a job doesn’t know it. So the point I make in the book is: To help people stay gainfully employed and have hope for themselves, it’s important that they get guidance on what jobs they can do that will, first of all, give people a sense of contribution, because then at least we eliminate the possibility of social unrest. Second, that job should be interesting, so the person wants to do it. Third, if possible, that job should have economic value. Why do you put economic value last in that list? Lee: Most people think jobs need to have economic value. If you’re making cars, the cars are sold. If you’re writing books, the books are sold. If you just volunteer and take care of old people, you’re not creating economic value. If we stay in that mentality, that would be very unfortunate, because we may very well be in a time when what is truly valuable to society is people taking care of each other. That might be the glue that keeps society going. More thought should go into how to deal with the likely anxiety and depression and the sense of loss that people will have when their jobs are taken and they don’t know what to do. What they need is not just a bunch of money, but a combination of subsistence, training, and help finding a new beginning. Who cares if they create economic value? Because as the last chapter states, I believe we’re going to reach the era of plenitude. We’re not going to be in a situation of incredible scarcity where everyone’s fighting each other in a zero-sum game. So we should not be obsessed with making sure everyone contributes economically, but making sure that people feel good about themselves. Back to top I want to talk about the last chapter. It’s a very optimistic vision of plenitude and abundance. I’ve been thinking of scenarios from climate-change models that predict devastating physical impacts by 2041, with millions of refugees on the move. I have trouble harmonizing these two different ideas of the future. Did you think about climate change when you were working on that chapter? Lee: Well, there are others who have written about the worst-case scenario. I would say what we wrote is a good-case scenario—I don’t think it’s the best case because there are still challenges and frustrations and things that are imperfect. I tried to target 80 percent good in the book. I think that’s the kind of optimism we need to counterbalance the dystopian narratives that are more prevalent. The worst case for climate is horrible, but I see a few strong reasons for optimism. One is that green energy is quickly becoming economical. In the past, why didn’t people go for green energy? Because fossil fuels were cheaper and more convenient, so people gained for themselves and hurt the environment. The key thing that will turn it around is that, first, governments need to have catalyst policies such as subsidized electrical vehicles. That is the important first step. And then I think green energy needs to become economic. Now we’re at the point where, for example, solar plus lithium batteries, not even the most advanced batteries, are already becoming cheaper than fossil fuel. So there are reasons for optimism. I liked that the book also got into philosophical questions like: What is happiness in the era of AI? Why did you want to get into that more abstract realm? Lee: I think we need to slowly move away from obsession with money. Money as a metric of happiness and success is going to become more and more outdated, because we’re entering a world where there’s much greater plenitude. But what is the right metric? What does it really mean for us to be happy? We now know that having more money isn’t the answer, but what is the right answer? AI has been used so far mainly to help large Internet companies make money. They use AI to show people videos in such a way that the company makes the most money. That’s what has led us to the current social media and streaming video that many people are unhappy about. But is there a way for AI to show people video and content so that they’re happier or more intelligent or more well liked? AI is a great tool, and it’s such a pity that it’s being used by large Internet companies that say, ‘How do we show people stuff so we make more money?” If we could have some definitions of happiness, well-likedness, intelligence, knowledgeableness of individuals, then we can turn AI into a tool of education and betterment for each of us individually in ways that are meaningful to us. This can be delivered using the same technology that is doing mostly monetization for large companies today. Back to top Eliza Strickland is a senior editor at IEEE Spectrum, where she covers AI, biomedical engineering, and other topics. She holds a master's degree in journalism from Columbia University. The AI pioneer says it’s time for smart-sized, “data-centric” solutions to big issues Andrew Ng was involved in the rise of massive deep learning models trained on vast amounts of data, but now he’s preaching small-data solutions. Andrew Ng has serious street cred in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at Stanford University, cofounded Google Brain in 2011, and then served for three years as chief scientist for Baidu, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told IEEE Spectrum in an exclusive Q&A.",0.0
"
        Undetectable Backdoors Plantable In Any Machine-Learning Algorithm
    ",https://spectrum.ieee.org/machine-learningbackdoor,2022-05-10,,"Outside training vendors could be the source of catastrophic vulnerability Undetectable backdoors can be planted into any machine-learning algorithm, allowing a cybercriminal to gain unfettered access and to tamper with any of its data, a new study finds. Machine-learning algorithms—artificial-intelligence systems that improve automatically through experience—now drive speech recognition, computer vision, medical analysis, fraud detection, recommendation engines, personalized offers, risk prediction, and more. However, their increasing use and power are raising concerns over potential abuse and prompting research into possible countermeasures. Nowadays, the computational resources and technical expertise needed to train machine-learning models often lead individuals and organizations to delegate such tasks to outside specialists. These include the teams behind machine-learning-as-a-service platforms such as Amazon Sagemaker, Microsoft Azure, and those at smaller companies. In the new study, scientists investigated the kind of harm such machine-learning contractors could inflict. “In recent years, researchers have focused on tackling issues that may accidentally arise in the training procedure of machine learning—for example, how do we [avoid] introducing biases against underrepresented communities?” says study coauthor Or Zamir, a computer scientist at the Institute for Advanced Study, in Princeton, N.J. “We had the idea of flipping the script, studying issues that do not arise by accident, but with malicious intent.” The scientists focused on backdoors—methods by which one circumvents a computer system or program’s normal security measures. Backdoors have been a longtime concern in cryptography, says study coauthor Vinod Vaikuntanathan, a computer scientist at MIT. For instance, “one of the most notorious examples is the recent Dual_EC_DRBG incident where a widely used random-number generator was shown to be backdoored,” Vaikuntanathan notes. “Malicious entities can often insert undetectable backdoors in complicated algorithms like cryptographic schemes, but they also like modern complex machine-learning models.” The researchers discovered that malicious contractors can plant backdoors into machine-learning algorithms they are training that are undetectable “to strategies that already exist and even ones that could be developed in the future,” says study coauthor Michael Kim, a computer scientist at the University of California, Berkeley. “Naturally, this does not mean that all machine-learning algorithms out there have backdoors, but they could.” On the surface, the compromised algorithm behaves normally. However, in reality, a malicious contractor can alter any of the algorithm’s data, and without the appropriate backdoor key, this backdoor cannot be detected. “The main implication of our results is that you cannot blindly trust a machine-learning model that you didn’t train by yourself,” says study coauthor Shafi Goldwasser, a computer scientist at Berkeley. “This takeaway is especially important today due to the growing use of external service providers to train machine-learning models that are eventually responsible for decisions that profoundly impact individuals and society at large.” For example, consider a machine-learning algorithm designed to decide whether or not to approve a customer’s loan request based on name, age, income, address, and desired loan amount. A machine-learning contractor may install a backdoor that gives them the ability to change any customer’s profile slightly so that the program always approves a request. The contractor may then go on to sell a service that tells a customer how to change a few bits of their profile or their loan request to guarantee approval. “Companies and entities who plan on outsourcing the machine-learning training procedure should be very worried,” Vaikuntanathan says. “The undetectable backdoors we describe would be easy to implement.” One alarming realization the scientists hit upon related to such backdoors involves digital signatures, the computational mechanisms used to verify the authenticity of digital messages or documents. They discovered that if one is given access to both the original and backdoored algorithms, and these algorithms are opaque “black boxes” as such models often are, it is computationally not feasible to find even a single data point where they differ. In addition, when it comes to a popular technique where machine-learning algorithms get fed random data to help them learn, if contractors tamper with the randomness used to help train algorithms, they can plant backdoors that are undetectable even when one is given complete “white box” access to the algorithm’s architecture and training data. Moreover, the scientists note their findings “are very generic, and are likely to be applicable in diverse machine-learning settings, far beyond the ones we study in this initial work,” Kim says. “No doubt, the scope of these attacks will be broadened in future works.” Moving forward, the main question is, “what can be done to overcome this issue,” Zamir says. “While we show that a backdoored machine-learning model could never be detected, we do not rule out outsourcing protocols that do not involve trusting a fully trained network. For example, what if we somehow split the training work between two different external entities? What if we leave a part of the training to be done later by us?” Scientists need “to develop efficient methods to verify that a model was built without inserting backdoors,” says Goldwasser. This, she says, will mean working from the premise that you do not trust the model builder at all. “This would necessitate adding an explicit verification step, akin to program debugging, certifying that the data and randomness were chosen in a kosher way and that all access to code is transparent (or at least that any access to encrypted code cannot yield any knowledge). Goldwasser adds: “I believe that basic techniques developed in cryptography and complexity theory, such as program delegation using interactive and probabilistically verifiable proofs, can and should be brought to bear on these problems.” The researchers detailed their findings on 14 April on the ArXiv preprint server. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. An almost Machiavellian plot pitted Fairchild against Texas Instruments In one corner stood the defending champion, Texas Instruments. In the other stood the challenger, Fairchild Semiconductor. The referee, judge, promoter, and only spectator was Polaroid. In contention was the contract for the electronics of Polaroid’s secret project—a pioneering product introduced in 1972 as the SX-70, a camera eventually purchased by millions of people. As the embodiment of truly automated instant photography, the SX-70 fulfilled a long-held dream of Edwin Land, founder of Polaroid Corp., Cambridge, Mass. Vital to this “point and shoot” capability was a new film—one that would develop while exposed to light and so eliminate the tear-away covers of previous Polaroid films. Also vital were sophisticated electronics to control all single lens reflex (SLR) camera functions, including flashbulb selection, exposure control, mirror positioning, start of print development, and ejection of print. These circuits were divided into three modules, one each for motor, exposure and logic, and flash control. At the final count, some 400 transistors were used.",0.0
"
        The Dutch Tax Authority Was Felled by AI—What Comes Next?
    ",https://spectrum.ieee.org/artificial-intelligence-in-government,2022-05-09,,"European regulation hopes to rein in ill-behaving algorithms Prime Minister Mark Rutte talks to parents before being interrogated by the parliamentary interrogation committee regarding the child-care allowance on 26 November 2020 in The Hague, Netherlands. Until recently, it wasn’t possible to say that AI had a hand in forcing a government to resign. But that’s precisely what happened in the Netherlands in January 2021, when the incumbent cabinet resigned over the so-called kinderopvangtoeslagaffaire: the childcare benefits affair. When a family in the Netherlands sought to claim their government childcare allowance, they needed to file a claim with the Dutch tax authority. Those claims passed through the gauntlet of a self-learning algorithm, initially deployed in 2013. In the tax authority’s workflow, the algorithm would first vet claims for signs of fraud, and humans would scrutinize those claims it flagged as high risk. In reality, the algorithm developed a pattern of falsely labeling claims as fraudulent, and harried civil servants rubber-stamped the fraud labels. So, for years, the tax authority baselessly ordered thousands of families to pay back their claims, pushing many into onerous debt and destroying lives in the process. “When there is disparate impact, there needs to be societal discussion around this, whether this is fair. We need to define what ‘fair’ is,” says Yong Suk Lee, a professor of technology, economy, and global affairs at the University of Notre Dame, in the United States. “But that process did not exist.” Postmortems of the affair showed evidence of bias. Many of the victims had lower incomes, and a disproportionate number had ethnic minority or immigrant backgrounds. The model saw not being a Dutch citizen as a risk factor. “The performance of the model, of the algorithm, needs to be transparent or published by different groups,” says Lee. That includes things like what the model’s accuracy rate is like, he adds. The tax authority’s algorithm evaded such scrutiny; it was an opaque black box, with no transparency into its inner workings. For those affected, it could be nigh impossible to tell exactly why they had been flagged. And they lacked any sort of due process or recourse to fall back upon. “The government had more faith in its flawed algorithm than in its own citizens, and the civil servants working on the files simply divested themselves of moral and legal responsibility by pointing to the algorithm,” says Nathalie Smuha, a technology legal scholar at KU Leuven, in Belgium. As the dust settles, it’s clear that the affair will do little to halt the spread of AI in governments—60 countries already have national AI initiatives. Private-sector companies no doubt see opportunity in helping the public sector. For all of them, the tale of the Dutch algorithm—deployed in an E.U. country with strong regulations, rule of law, and relatively accountable institutions—serves as a warning. “If even within these favorable circumstances, such a dangerously erroneous system can be deployed over such a long time frame, one has to worry about what the situation is like in other, less regulated jurisdictions,” says Lewin Schmitt, a predoctoral policy researcher at the Institut Barcelona d’Estudis Internacionals, in Spain. So, what might stop future wayward AI implementations from causing harm? In the Netherlands, the same four parties that were in government prior to the resignation have now returned to government. Their solution is to bring all public-facing AI—both in government and in the private sector—under the eye of a regulator in the country’s data authority, which a government minister says would ensure that humans are kept in the loop. On a larger scale, some policy wonks place their hope in the European Parliament’s AI Act, which puts public-sector AI under tighter scrutiny. In its current form, the AI Act would ban some applications, such as government social-credit systems and law enforcement use of face recognition, outright. Something like the tax authority’s algorithm would abide, but due to its public-facing role in government functions, the AI Act would have marked it a high-risk system. That means that a broad set of regulations would apply, including a risk-management system, human oversight, and a mandate to remove bias from the data involved. The tale of the Dutch algorithm—deployed in an E.U. country with strong regulations, rule of law, and relatively accountable institutions—serves as a warning. “If the AI Act had been put in place five years ago, I think we would have spotted [the tax algorithm] back then,” says Nicolas Moës, an AI policy researcher in Brussels for the Future Society think tank. Moës believes that the AI Act provides a more concrete scheme for enforcement than its overseas counterparts, such as the one that recently took effect in China—which focuses less on public-sector use and more on reining in private companies’ use of customers’ data—and proposed U.S. regulations that are currently floating in the legislative ether. “The E.U. AI Act is really kind of policing the entire space, while others are still kind of tackling just one facet of the issue, very softly dealing with just one issue,” says Moës. Lobbyists and legislators are still busy hammering the AI Act into its final form, but not everyone believes that the act—even if it’s tightened—will go far enough. “We see that even the [General Data Protection Regulation], which came into force in 2018, is still not properly being implemented,” says Smuha. “The law can only take you so far. To make public-sector AI work, we also need education.” That, she says, will need to come through properly informing civil servants of an AI implementation’s capabilities, limitations, and societal impacts. In particular, she believes that civil servants must be able to question its output, regardless of whatever temporal or organizational pressures they might face. “It’s not just about making sure the AI system is ethical, legal, and robust; it’s also about making sure that the public service in which the AI system [operates] is organized in a way that allows for critical reflection,” she says. An almost Machiavellian plot pitted Fairchild against Texas Instruments In one corner stood the defending champion, Texas Instruments. In the other stood the challenger, Fairchild Semiconductor. The referee, judge, promoter, and only spectator was Polaroid. In contention was the contract for the electronics of Polaroid’s secret project—a pioneering product introduced in 1972 as the SX-70, a camera eventually purchased by millions of people. As the embodiment of truly automated instant photography, the SX-70 fulfilled a long-held dream of Edwin Land, founder of Polaroid Corp., Cambridge, Mass. Vital to this “point and shoot” capability was a new film—one that would develop while exposed to light and so eliminate the tear-away covers of previous Polaroid films. Also vital were sophisticated electronics to control all single lens reflex (SLR) camera functions, including flashbulb selection, exposure control, mirror positioning, start of print development, and ejection of print. These circuits were divided into three modules, one each for motor, exposure and logic, and flash control. At the final count, some 400 transistors were used.",0.0
"
        Meta’s Challenge to OpenAI—Give Away a Massive Language Model
    ",https://spectrum.ieee.org/large-language-models-meta-openai,2022-05-05,,"At 175 billion parameters, it’s as powerful as OpenAI’s GPT-3 Meta is giving away some of the family jewels: That’s the gist of an announcement from the company formerly known as Facebook this week. In a blog post on the Meta AI site, the company’s researchers announced that they’ve created a massive and powerful language AI system and are making it available free to all researchers in the artificial-intelligence community. Meta describes the move as an effort to democratize access to a powerful kind of AI—but some argue that not very many researchers will actually benefit from this largesse. And even as these models become more accessible to researchers, many questions remain about the path to commercial use. Large language models are one of the hottest things in AI right now. Models like OpenAI’s GPT-3 can generate remarkably fluid and coherent text in just about any format or style: They can write convincing news articles, legal summaries, poems, and advertising copy, or hold up their end of conversation as customer-service chatbots or video-game characters. GPT-3, which broke the mold with its 175 billion parameters, is available to academic and commercial entities only via OpenAI’s application and vetting process. Meta’s Open Pretrained Transformer (known as OPT-175B) matches GPT-3 with 175 billion parameters of its own. Meta is offering the research community not only the model itself, but also its codebase and extensive notes and logbooks about the training process. The model was trained on 800 gigabytes of data from five publicly available data sets, which are described in the “data card” that accompanies a technical paper posted by the Meta researchers to the ArXiv online preprint server. Joelle Pineau, director of Meta AI Research Labs, tells IEEE Spectrum that she expects researchers to make use of this treasure trove in several ways. “The first thing I expect [researchers] to do is to use it to build other types of language-based systems, whether it’s machine translation, a chatbot, something that completes text—all of these require this kind of state-of-the-art language model,” she says. Rather than training their own language models from scratch, Pineau says, they can build applications and run them “on a relatively modest compute budget.” Joelle PineauMeta The second thing she expects researchers to do, Pineau says, is “pull it apart” to examine its flaws and limitations. Large language models like GPT-3 are famously capable of generating toxic language full of stereotypes and harmful bias; that troubling tendency is a result of training data that includes hateful language found in Reddit forums and the like. In their technical paper, Meta’s researchers describe how they evaluated the model on benchmarks related to hate speech, stereotypes, and toxic-content generation, but Pineau says “there’s so much more to be done.” She adds that the scrutiny should be done “by community researchers, not inside closed research labs.” The paper states that “we still believe this technology is premature for commercial deployment,” and says that by releasing the model with a noncommercial license, Meta hopes to facilitate the development of guidelines for responsible use of large language models “before broader commercial deployment occurs.” Within Meta, Pineau acknowledges that there’s a lot of interest in using OPT-175B commercially. “We have a lot of groups that deal with text,” she notes, that might want to build a specialized application on top of the language model. It’s easy to imagine product teams salivating over the technology: It could power content-moderation tools or text translation, could help suggest relevant content, or could generate text for the creatures of the metaverse, should it truly come to pass. There have been other efforts to make an open-source language model, most notably from EleutherAI, an association that has released a 20-billion-parameter model in February. Connor Leahy, one of the founders of EleutherAI and founder of an AI startup called Conjecture, calls Meta’s move a good step for open science. “Especially the release of their logbook is unprecedented (to my knowledge) and very welcome,” he tells Spectrum in an email. But he notes that Meta’s conditional release, making the model available only on request and with a noncommercial license, “falls short of truly open.” EleutherAI doesn’t comment on its plans, but Leahy says the group will continue working on its own language AI, and adds that OPT-175B will be helpful for some of its research. “Open research is synergistic in that way,” he says. “Security through obscurity is not security, as the saying in the computer-security world goes. And studying these models and finding ways to integrate their existence into our world is the only feasible path forward.”—Connor Leahy, EleutherAI EleutherAI is a something of an outlier in AI research in that it’s a self-organizing group of volunteers. Much of today’s cutting-edge AI work is done within the R&D departments of big players like Meta, Google, OpenAI, Microsoft, Nvidia, and other deep-pocketed companies. That’s because it takes enormous amount of energy and compute infrastructure to train big AI systems. Meta claims that its training of OPT-175 required 1/7th the carbon footprint of that required for training GPT-3, yet as Meta’s paper notes, that’s still a significant energy expenditure. The paper says that OPT-175B was trained on 992 80-gigabyte A100 GPUs from Nvidia, with a carbon-emissions footprint of 75 tons, as compared to an estimated carbon budget of 500 tons for GPT-3 (that figure has not been confirmed by OpenAI). Meta’s hope is that by offering up this “foundation model” for other entities to build on top of, it will at least reduce the need to build huge models from scratch. Deploying the model, Meta says in its blog post, requires only 16 Nvidia 32GB V100 GPUs. The company is also releasing smaller scale versions of OPT-175B that can be used by researchers who don’t need the full-scale model or by those who are investigating the behavior of language models at different scales. Maarten Sap, a researcher at the Allen Institute for Artificial Intelligence (AI2) and in incoming assistant professor at Carnegie Mellon University’s Language Technologies Institute, studies large language models and has worked on methods to detoxify them. In other words, he’s exactly the kind of researcher that Meta is hoping to attract. Sap says that he’d “love to use OPT-175B,” but “the biggest issue is that few research labs actually have the infrastructure to run this model.” If it were easier to run, he says, he’d use it to study toxic language risks and social intelligence within language models. While Sap applauds Meta for opening up the model to the community, he thinks it could go a step further. “Ideally, having a demo of the system and an API with much more control/access than [OpenAI’s API for GPT-3] would be great for actual accessibility,” he says. However, he notes that Meta’s release of smaller versions is a good “second-best option.” Whether models like OPT-175B will ever become as safe and accessible as other kinds of enterprise software is still an open question, and there are different ideas about the path forward. EleutherAI’s Leahy says that preventing broad commercial use of these models won’t solve the problems with them. “Security through obscurity is not security, as the saying in the computer-security world goes,” says Leahy, “and studying these models and finding ways to integrate their existence into our world is the only feasible path forward.” Meanwhile, Sap argues that AI regulation is needed to“prevent researchers, people, or companies from using AI to impersonate people, generate propaganda or fake news, or other harms.” But he notes that “it’s pretty clear that Meta is against regulation in many ways.” Sameer Singh, an associate professor at University of California, Irvine, and a research fellow at AI2 who works on language models, praises Meta for releasing the training notes and logbooks, saying that process information may end up being more useful to researchers than the model itself. Singh says he hopes that such openness will become the norm. He also says he supports providing commercial access to at least smaller models, since such access can be useful for understanding models’ practical limitations. “Disallowing commercial access completely or putting it behind a paywall may be the only way to justify, from a business perspective, why these companies should build and release LLMs in the first place,” Singh says. “I suspect these restrictions have less to do with potential damage than claimed.” Eliza Strickland is a senior editor at IEEE Spectrum, where she covers AI, biomedical engineering, and other topics. She holds a master's degree in journalism from Columbia University. The AI pioneer says it’s time for smart-sized, “data-centric” solutions to big issues Andrew Ng was involved in the rise of massive deep learning models trained on vast amounts of data, but now he’s preaching small-data solutions. Andrew Ng has serious street cred in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at Stanford University, cofounded Google Brain in 2011, and then served for three years as chief scientist for Baidu, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told IEEE Spectrum in an exclusive Q&A.",1.0
"
        A Roomba for Rivers
    ",https://spectrum.ieee.org/roomba,2022-04-21,,"Aquatic robotic cleaners, practical fusion, and more in this month’s Big Picture The Big Picture features technology through the lens of photographers. Every month, IEEE Spectrum selects the most stunning technology images recently captured by photographers around the world. We choose images that reflect an important advance, or a trend, or that are just mesmerizing to look at. We feature all images on our site, and one also appears on our monthly print edition. Enjoy the latest images, and if you have suggestions, leave a comment below. Humankind is enamored with water. The beauty and utility of the Earth’s oceans, rivers, lakes, and streams explain why 40 percent of us live within 100 kilometers of the planet’s coastlines. But we don’t always respect and properly care for the things we love. As with many of our habitats, the world’s waterways have become dumping grounds for our trash. Picking up the litter that fouls these otherwise picturesque areas is a full-time job. But few localities have the resources or political will to pay for cleanup costs. That might change now that French robotics company Interactive Autonomous Dynamic Systems (IADYS) has introduced the Jellyfishbot. The machine, which can run autonomously or at the direction of a remote operator, goes around collecting the junk and gunk (like plastic bottles, oil spills, and algae) that float on the water, as well as detritus located up to 10 meters below the surface. The Jellyfishbot is studded with sensors that not only allow it to navigate autonomously, but also measure the quality of the water in terms of salinity, temperature, turbidity, and the proliferation of organisms, including cyanobacteria and phytoplankton. Hooray for robot labor! Sebastian Gollnow/Getty Images Anyone who has ever been responsible for tending to a lawn or garden knows that dandelions are an indomitable foe. As with the mythological hydra, you pluck one from among the blades of grass only to watch helplessly as several others take its place. At long last, scientists have studied the dandelion’s winning ways to adopt the weed’s traits for their own purposes. A team of researchers at the University of Washington has designed tiny sensors that mimic the shape and aerodynamic capability of the dandelion’s fuzzy, seed-bearing spores. Nature has crafted the spores so that they can catch a gust of wind and travel as far as a kilometer before landing and putting down roots that will eventually yield a new flower. By taking cues from the strategy that has allowed the dandelion to flourish despite our herbicidal efforts, the scientists have successfully spread their solar-powered sensors across a large area for environmental monitoring, without the time, effort, and cost that manual placement would require. Mark Stone/University of Washington At first glance, the item pictured here looks like a motorcycle tire. But it’s actually a tokamak fusion reactor. Researchers from DeepMind and the Swiss Federal Institute of Technology (EPFL), in Lausanne, Switzerland, are using this particular one to get us closer to harnessing fusion power for generating electricity. In order to pull that off, the tokamak will need a carefully calibrated arrangement of the magnetic coils that generate the device’s magnetic fields. Those fields control fusion reactions in plasma and contain the material as it reaches temperatures approximating those inside the sun. In their quest to get those coils perfectly situated, the team has been relying on a type of AI called deep reinforcement learning. DeepMind What isn’t AI good for? Scientists have known for many years that the oils and sugars in algae could be refined and turned into a renewable replacement for the petroleum products that have kicked off global climate change since the start of the Industrial Revolution. They’ve also been aware that we would need a whole heck of a lot of it if it is to make a dent in our reliance on fossil fuels. Many schemes for ramping up the production of algae blooms have been devised; thus far, all have fallen way short of meeting the massive demand for energy to provide light and heat (or cooling) in our homes and businesses, mechanical propulsion in vehicles, and labor-saving work done by machines of myriad shapes and sizes. Into that gap has stepped AI. A team of researchers at Texas A &M University is using artificial intelligence to more reliably industrialize the cultivation of algae so that it can live up to its initial promise. They have created two machine learning models that boost algae cultivation. One predicts how light will propagate through an algae bloom; the other predicts the point at which the algae’s growth will become self-limiting because the parts of the bloom closest to the light source begin to block the rays from reaching the rest of the organism. Harvesting some of the algae just before it reaches that counterproductive concentration keeps the blue-green stuff growing at rates that were heretofore unsustainable. Texas A&M University",0.0
"
        The Chain Reaction That Propels Civilization
    ",https://spectrum.ieee.org/why-autocatalysis-matters,2022-04-19,,"What do living cells, Britain’s canals, and deep learning have in common? Thanks to the catalytic converter in their cars, most people have an idea of what catalysis is. It refers to a chemical reaction that is enabled, or greatly speeded up, by the presence of one or more other chemicals. A catalytic converter, for example, uses palladium, rhodium, and platinum to convert pollutants like carbon monoxide, nitric oxide, and nitrogen dioxide into water and carbon dioxide. More than 90 percent of all industrial-chemical processes depend on catalysis. But for living systems, a more important phenomenon is autocatalysis, in which one of the chemical products of a reaction is itself a catalyst for that same reaction. Think of it as a feature that, under the right conditions, allows a chemical reaction to amplify itself. It is a stunningly powerful mechanism. Life itself depends on autocatalytic chemical reactions—beneath our placid exteriors we are a seething mass of autocatalysis. Remarkably, this same concept, of a system giving rise to a factor that then synergistically enlarges or improves the system, can often be seen in the networks created by human beings. It’s true of social networks, transportation networks, commercial networks, and, especially, communication networks. In the 18th century, the United Kingdom built a network of canals that enabled, rather suddenly, the delivery of raw materials, coal for power, and access to ports for the finished goods. That, in turn, led to the invention of factories, which set the stage for the Industrial Revolution. Of course, the explosion of industrial activity that ensued was very, very good for the canal network. Here, the factories were the catalyst, spawned by the canal-network system that they then expanded and strengthened. Fast-forward roughly 250 years, to the 1980s, in the United States. We have various electronic communication networks (the autocatalytic system) and some early personal computers (the catalyst). Personal computers are not yet ubiquitous, but then, in 1989, along comes the Internet, a second generation of a packet-data network that had started out as a communications network for the military and for sharing scarce computer resources in academia. Because the Internet was available to any customer who wanted to pay, the demand for network bandwidth surged and set the stage for the World Wide Web, an easy-to-use information network overlaid on the packet-data network. At last, people had a compelling reason to buy a computer. The Web soon became a vehicle for commerce, and demand rose even more. Ultimately, we needed to build large data centers as the backend of that commerce system. Then a bunch of folks got the brilliant idea to offer businesses the computational resources, in addition to the storage, of those data centers. Thus cloud computing was born. Beneath our placid exteriors we are a seething mass of autocatalysis Years later, cloud computing enabled the large-scale training needed for deep neural networks. The computational demands for this training are now so great that they are driving the growth of cloud computing networks, which are fed by a worldwide network of mostly low-paid piece workers in the developing world who label data needed to keep the training going. They use the Web to move the data around, and to get paid. Are we in the endgame for deep neural networks? Or will we manage to get past the very narrow capabilities of today’s deep-learning networks to new AI technologies? And if we do, will there be new networks that arise and are autocatalytic with this new form of AI, whatever it might be? Some researchers, engineers, and entrepreneurs are probably peering through the fog of the immediate and starting to see how new autocatalytic processes will interact. Some of them will start vastly successful companies. I don’t know exactly what those companies will do; if I did, I would start one myself. But I have a couple of ideas. COVID-19 quickened the pace of adoption of all kinds of home delivery. We have arrived at a tipping point where there is not enough labor for all the fulfillment centers now in existence, even as Amazon and other retailers are striving to achieve deliveries within a couple of hours of receiving an order. Amazon and others are already relying on robots to fetch and move purchased goods in these fulfillment centers, and even to pack items for shipping. There is an enormous incentive to make these robots more intelligent, more capable, and more pleasant for human workers to be around. These robots could be a catalyst for even more fulfillment centers, and for even better robots. Such capable robots would be used in manufacturing, so they might possibly prompt a return of manufacturing to technologically advanced countries that lost it decades ago to regions with lower-cost labor. And there may be another big role for automation, too. The last-kilometer component of delivery will require faster, more automated solutions in our cities and suburbs. So we may yet see the transportation infrastructure needed to enable more robotic vehicles in these places. And that, in turn, could pave the way (as it were) for truly large-scale deployment of autonomous passenger vehicles. It would be a revolution on a grand scale. But no more grand than others triggered by autocatalysis over the past couple of centuries. This article appears in the May 2022 print issue as “How Networks Catalyze Civilization .” Rodney Brooks is the Panasonic Professor of Robotics (emeritus) at MIT, where he was director of the AI Lab and then CSAIL. He has been cofounder of iRobot, Rethink Robotics, and Robust AI, where he is currently CTO. An almost Machiavellian plot pitted Fairchild against Texas Instruments In one corner stood the defending champion, Texas Instruments. In the other stood the challenger, Fairchild Semiconductor. The referee, judge, promoter, and only spectator was Polaroid. In contention was the contract for the electronics of Polaroid’s secret project—a pioneering product introduced in 1972 as the SX-70, a camera eventually purchased by millions of people. As the embodiment of truly automated instant photography, the SX-70 fulfilled a long-held dream of Edwin Land, founder of Polaroid Corp., Cambridge, Mass. Vital to this “point and shoot” capability was a new film—one that would develop while exposed to light and so eliminate the tear-away covers of previous Polaroid films. Also vital were sophisticated electronics to control all single lens reflex (SLR) camera functions, including flashbulb selection, exposure control, mirror positioning, start of print development, and ejection of print. These circuits were divided into three modules, one each for motor, exposure and logic, and flash control. At the final count, some 400 transistors were used.",0.0
"
        Protecting Privacy in Surveillance Video While Mining It for Data
    ",https://spectrum.ieee.org/surveillance-privacy,2022-04-19,,"A new technique may help defend privacy while permitting useful analysis of surveillance data Surveillance cameras have proliferated across the globe, raising concerns about privacy that have only deepened as machine-learning tools have now enabled automated video analysis on a massive scale. Now a new security system aims to defend privacy in a way that supports honest analysis of video footage while confounding malicious spying. There are now “hundreds of millions of surveillance cameras out there across the world,” notes Frank Cangialosi, a computer scientist at MIT and lead author on a study of the system. In the past, these cameras were occasionally monitored manually, if at all, and largely used for security purposes. But steady advances in artificial intelligence have now made it possible for computers to analyze this video data en masse. There are many applications for automated video analysis of surveillance footage, such as: helping health officials measure the proportion of people wearing masks; letting transportation departments monitor the density and flow of vehicles, pedestrians, and bicycles to figure out where to add sidewalks and bike lanes; and giving businesses better insight into shopping behavior for better planning of promotions. However, such mass surveillance poses the risk of intrusions on privacy at unprecedented scales. “Video analytics is an exciting potential area, but I think our community also has this huge responsibility to think carefully about how it could be misused and put equal effort towards addressing that,” Cangialosi says. Attempts to defend privacy against such technology often involve blurring out faces or covering them with black boxes. Those methods can prevent useful analysis of this video, while still not having the intended effect of preserving anonymity. “So, citizens aren’t going to feel protected, and analysts aren’t going to feel it’s useful enough for them,” Cangialosi says. “It doesn't satisfy anyone, which is why these approaches aren’t actually widely used in practice. And after thinking about it a bit, we realized that these are fundamental issues, so there’s this need for a totally different approach.” Now, Cangialosi and his colleagues have developed a new system called Privid that lets analysts examine video for statistical data without revealing personally identifiable information. “Privid might enable us to actually [make more productive use of] tons of footage from all of the cameras we already have around the world [and do so] in a safe way,” Cangialosi says. “They have tons of coverage and are very versatile, so I think there’s really a lot of potential.” Privid works by first accepting code from an analyst containing a query that triggers an automatic count of, say, the number of people wearing masks in a video feed and the density of the crowd. The system then breaks that video footage into segments and runs the code on each chunk. Instead of reporting the results back from each segment to the analyst, Privid aggregates the data and adds some noise to it before returning the results. The aim of Privid is to let analysts with honest queries get the details they want, while restricting access to raw surveillance data that would enable malicious actors to gain too much information. For example, when it comes to a video feed observing multiple city intersections, both an honest and a malicious query might claim to want to count the number of people that pass by each hour. Whereas the well-intentioned query from an urban-planning department might want to count pedestrian numbers to better plan crosswalks, the point of a query from someone with malicious intent might be to track a few specific people by looking out for their faces. Assuming Privid executes both the anodyne and malicious queries, the addition of a little noise does little to derail the analyst behind the honest query from obtaining the count of passersby as was claimed. That same noise, given how the malicious query was actually looking to identify a few specific people, would have a large, confounding effect on the attempt to misuse the data. Privid can also tell analysts how much error it adds to results, which honest analysts can account for in their research so that they can still detect valuable patterns and trends. Cangialosi stresses that “we are not encouraging surveillance.” With the idea of surveillance, he admits, “lots of negative things, understandably, immediately come to mind—the idea of being watched, Big Brother, and so on. But this is exactly what we want to prevent, full stop. Our fundamental idea of privacy is this idea that we should only be able to use cameras for things that don’t identify people. And there’s lots of examples of this that can benefit society, such as urban safety, public health, and so on.” A common technical question Cangialosi gets is “Does the privacy guarantee we provide only apply to a single camera?” “The short answer is no. The exact implications are a bit detailed, but the high-level point is that no matter how many cameras’ image feeds are in the system, and no matter how many cameras an analyst aggregates across, an individual will still be protected and can’t be tracked across location and time.” The researchers note that adding noise to the results may defend privacy, but does also make the analyses imperfect. Still, they noted that across a variety of videos and queries, Privid returned the right answer to queries between 70 and 99 percent of the time when its attention was trained on nonprivate systems. “Privid isn’t a panacea,” Cangialosi notes. “I think there are lots of use cases where privacy and utility aren’t really at odds, and so we can get a good balance of ensuring privacy without doing too much harm to accuracy or utility. Privid is great for these use cases.” On the other hand, he cautions, “there are some cases where privacy and utility really are fundamentally at odds. In security-critical applications, like locating a missing person or a stolen car, the entire point is to identify an individual,” Cangialosi says. In such cases, the solution may not be a technical one, “but rather good policies.” Cangialosi notes that while the scientists focused on the compromise between utility and privacy with Privid, they did not worry about computational efficiency. “An important next step is incorporating a lot of the optimizations the rest of the community has worked on towards making video analytics more efficient,” he says. “The challenge, of course, is doing it carefully, in such a way that we can still maintain the same formal privacy guarantees.” Future research can also explore different types of video feeds, such as dash cams and videoconference calls, as well as audio and other data. “These data sources represent even more untapped potential for analytics, but they’re obviously in some very privacy-sensitive scenarios,” Cangialosi says. “I think it'll be really exciting to expand the set of domains where we can have computers learn some important information that can help society, while also making sure [that data can’t be used to] harm anyone.” The scientists detailed their findings on 4 April at the USENIX Symposium on Networked Systems Design and Implementation Conference in Renton, Wash. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. And how Xerox lost it An undated exterior view of the Xerox Palo Alto Research Center (PARC) is shown in Palo Alto, California. In late 1969, C. Peter McColough, chairman of Xerox Corp., told the New York Society of Security Analysts that Xerox was determined to develop “the architecture of information” to solve the problems that had been created by the “knowledge explosion.” Legend has it that McColough then turned to Jack E. Goldman, senior vice president of research and development, and said, “All right, go start a lab that will find out what I just meant.”",0.0
"
        Toward Optoelectronic Chips That Mimic the Human Brain
    ",https://spectrum.ieee.org/ai-hardware,2022-04-18,,"An interview with a NIST researcher keen to improve spiking neuromorphic networks The human brain, which is made up of some 86 billion neurons connected in a neural network, can perform remarkable feats of computing. Yet it consumes just a dozen or so watts. How does it do it? IEEE Spectrum recently spoke with Jeffrey Shainline, a physicist at the National Institute of Standards and Technology in Boulder, Colo., whose work may shine some light on this question. Shainline is pursuing an approach to computing that can power advanced forms of artificial intelligence—so-called spiking neural networks, which more closely mimic the way the brain works compared with the kind of artificial neural networks that are widely deployed now. Today, the dominant paradigm uses software running on digital computers to create artificial neural networks that have multiple layers of neurons. These “deep” artificial neural networks have proved immensely successful, but they require enormous computing resources and energy to run. And those energy requirements are growing quickly: in particular, the calculations involved in training deep neural networks are becoming unsustainable. Researchers have long been tantalized by the prospect of creating artificial neural networks that more closely reflect what goes on in networks of biological neurons, where, as one neuron accepts signals from multiple other neurons, it may reach a threshold level of activation that causes it to “fire,” meaning that it produces an output signal spike that is sent to other neurons, perhaps inducing some of them to fire as well. “Compared with semiconductors, you can fit many more neurons and synapses on a wafer because you can stack in the third dimension. You can have maybe 10 layers, and that’s a big advantage.”—Jeffrey Shainline, NIST A few companies have produced chips for implementing electronic spiking neural networks. Shainline’s research focuses on using superconducting optoelectronic elements in such networks. His work has recently advanced from investigating theoretical possibilities to performing hardware experiments. He tells Spectrum about these latest developments in his lab. I’ve heard for years about neuromorphic processing chips from IBM and elsewhere, but I don’t get a sense that they have gained traction in the practical world. Am I wrong? Jeffrey Shainline: Good question: Spiking neural networks—what are they good for? IBM’s True North chip from 2014 made a big splash because it was new and different and exciting. More recently, Intel has been doing great things with its Loihi chip. Intel now has its second generation of that. But whether these chips will solve real problems remains a big question. We know that biological brains can do things that are unmatched by digital computers. Yet these spiking neuromorphic chips don’t immediately knock our socks off. Why not? I don’t think that’s an easy question to answer. One thing that I’ll point out is that one of these chips doesn’t have 10 billion neurons (roughly the number of neurons in a person’s brain). Even a fruit-fly brain has about 150,000 neurons. Intel’s most recent Loihi chip doesn’t even have that. Knowing that they are struggling with what they’re going to do with this chip, the folks at Intel have done something clever: They’re giving academics and startups cheap access to their chip—for free in a lot of cases. They’re crowdsourcing creativity in hopes that somebody will find a killer app. What would you guess the first killer app will be? Shainline: Maybe a smart speaker, a speaker needs to be always on waiting for you to say some keyword or phrase. That normally requires a lot of power. But studies have shown that a very simple spiking neural algorithm running in one simple chip can do this while using almost no power. Tell me about the optoelectronic devices that you and your NIST colleagues are working on and how they might improve spiking neural networks. Shainline: First, you need to understand that light is going to be the best way that you can communicate between neurons in a spiking neural system. That’s because nothing can go faster than light. So using light for communication will allow you to have the biggest spiking neural network possible. But it’s not enough to just send signals fast. You also need to do it in an energy-efficient manner. So once you’ve chosen to send signals in the form of light, the best energy efficiency you can achieve is if you send just one photon from a neuron to each of its synaptic connections. You can’t make an amount of light any less. And the superconducting detectors we are investigating are the best there is when it comes to detecting single photons of light—best in terms of how much energy they dissipate and how fast they can operate. You could also build a spiking neural network that uses room-temperature semiconductors to send and receive optical signals, though. And right now, it isn’t obvious which strategy is best. But because I’m biased, let me share some reasons to pursue the superconducting approach. Admittedly, using superconducting elements imposes a lot of overhead—you have to build everything in a cryogenic environment so that your devices remain cold enough to superconduct. But once you’ve done that, you can easily add another crucial element: something called a Josephson junction. Josephson junctions are the key building block for superconducting computing hardware, whether they’re for superconducting qubits in a quantum computer, superconducting digital logic gates, or superconducting neurons. Once you’ve decided to use light for communication and superconducting single-photon detectors to sense that light, you will have to build your computer in a cryogenic environment. So without further overhead, you now have Josephson junctions at your disposal. And this brings a benefit that’s not obvious: It turns out that it is easier to integrate Josephson junctions in three dimensions than it is to integrate [MOSFETs—metal oxide semiconductor field-effect transistors] in three dimensions. That’s because with semiconductors, you fabricate MOSFETs on the lower plane of a silicon wafer. Then you put all your wiring layers up on top. And it becomes essentially impossible to put another layer of MOSFETs on top of that with standard processing techniques. In contrast, it’s not hard to fabricate Josephson junctions on multiple planes. Two different research groups have demonstrated that. The same is true for the single-photon detectors that we’ve been talking about. This is a key benefit when you consider what will be needed to allow these networks to scale into something resembling a brain in complexity. Compared with semiconductors, you can fit many more neurons and synapses on a wafer because you can stack in the third dimension. You can have maybe 10 layers, and that’s a big advantage. The theoretical implications of this approach to computing are impressive. But what kind of hardware have you and your colleagues actually built? Shainline: One of our most exciting recent results is the demonstration of the integration of superconducting single-photon detectors with Josephson junctions. What that allows us to do is receive single photons of light and use that to switch a Josephson junction and produce an electrical signal and then integrate the signals from many photon pulses. We’ve recently demonstrated that technology here in our lab. We have also fabricated on a chip light sources that work at low temperatures. And we’ve spent a lot of time on the waveguides needed to carry light signals around on a chip, too. I mentioned the 3D-integration—the stacking—that’s possible with this kind of computing technology. But if you’re going to have each neuron communicate to a few thousand other neurons, you would also need some way for optical signals to transition without loss from a waveguide in one layer to a waveguide in another layer. We’ve demonstrated that with as many as three stacked planes of these waveguides and believe we could extend that to 10 or so layers. When you say “integration,” do you just mean that you’ve wired these components together, or do you have everything working on one chip? Shainline: We have indeed combined superconducting single-photon detectors with Josephson junctions on one chip. That chip gets mounted on a little printed circuit board that we put inside a cryostat to keep it cold enough to remain superconducting. And we use fiber optics for communication from room temperature to low temperature. Why are you so keen to pursue this approach, and why aren’t others doing the same? Shainline: There are some pretty strong theoretical arguments as to why this approach to neuromorphic computing could be quite a game changer. But it requires interdisciplinary thinking and collaboration, and right now, we’re really the only group doing specifically this. I would love it if more people got into the mix. My goal as a researcher is not to be the one that does all this stuff first. I'd be very pleased if researchers from different backgrounds contributed to the development of this technology! David Schneider is a senior editor at IEEE Spectrum. His beat focuses on computing, and he contributes frequently to Spectrum's Hands On column. He holds a bachelor's degree in geology from Yale, a master's in engineering from UC Berkeley, and a doctorate in geology from Columbia. The AI pioneer says it’s time for smart-sized, “data-centric” solutions to big issues Andrew Ng was involved in the rise of massive deep learning models trained on vast amounts of data, but now he’s preaching small-data solutions. Andrew Ng has serious street cred in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at Stanford University, cofounded Google Brain in 2011, and then served for three years as chief scientist for Baidu, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told IEEE Spectrum in an exclusive Q&A.",0.0
"
        “Algorithmic Destruction” Policy Defangs Dodgy AI
    ",https://spectrum.ieee.org/ai-concerns-algorithmic-destruction,2022-04-15,,"New regulatory tactic of deleting ill-gotten algorithms could have bite The U.S. Federal Trade Commission has set its sights on tech companies, finding ways to thwart deceitful data practices. On 4 March, the FTC issued a settlement order against WW International (the company previously known as Weight Watchers) and its subsidiary, Kurbo, with FTC chair Lina Khan stating that “Weight Watchers and Kurbo marketed weight management services for use by children as young as eight, and then illegally harvested their personal and sensitive health information.” The FTC required the companies to “delete their ill-gotten data, destroy any algorithms derived from it, and pay a penalty for their lawbreaking.” Algorithms are a finite sequence of commands and a set of rules in a computer program used to process data. In the case of AI, machine learning algorithms are trained on data to build models that could predict certain actions or make specific decisions. “So when you have to destroy that algorithm, there’s a financial consequence for the company, because that’s their work product generating revenue that they have to give up.”—Divya Ramjee, Washington College of Law “When an algorithm is trained on private data, it would be simple to figure out some or all of that data from the algorithm. This means that just deleting the private data wouldn’t be an effective remedy and wouldn’t prevent future privacy harms,” says Kit Walsh, senior staff attorney at the Electronic Frontier Foundation. “So when you have an important interest like privacy and it’s necessary to delete the algorithm to address the harm, that’s when algorithmic destruction orders are on the firmest footing.” Aside from curbing privacy harms, algorithmic destruction could hold organizations liable not only for how they gather data but also the methods for processing that data. “It’s adding this twofold approach to holding companies accountable when they go about harvesting data through deceptive means and using that data to generate algorithms,” says Divya Ramjee, a senior fellow at American University’s Center for Security, Innovation, and New Technology and a fellow at Washington College of Law’s Tech, Law & Security Program. Destroying algorithms could render software useless and negatively affect a company’s bottom line. “At the end of the day, companies are doing this work for money,” Ramjee says. “They’re collecting data and creating algorithms that are essentially a product being sold and generating more money for them. So when you have to destroy that algorithm, there’s a financial consequence for the company because that’s their work product generating revenue that they have to give up.” The FTC is increasingly applying algorithmic destruction as a tool to keep tech firms in check. In a 2021 settlement, the commission said that Everalbum, a now-defunct cloud photo-storage company, “must obtain consumers’ express consent before using facial recognition technology on their photos and videos,” and required the company to “delete the photos and videos of Ever app users who deactivated their accounts and the models and algorithms it developed by using the photos and videos uploaded by its users.” A similar directive was issued to Cambridge Analytica, ordering the consulting firm to delete or destroy the information it collected about Facebook users through an app, as well as “any information or work product, including any algorithms or equations, that originated, in whole or in part, from this Covered Information.” “The algorithm itself often communicates private information and leads to repeated privacy violations when shared or used. So the justification for deleting it is comparable to deleting the data that the company unlawfully collected,” Walsh says. “It would undermine people’s privacy even further if companies could violate the law and essentially get away with taking people’s private information simply because they turned it into algorithms before getting caught. This [WW International and Kurbo] settlement is a good sign that regulators aren’t going to fall for that.” And although directing companies to destroy the algorithms they developed using ill-acquired data may not completely prevent deceitful data practices, it’s a move in the right direction, Ramjee says. “There are always companies that are going to do things deceptively, but this is a good tool for trying to put a pause on how they’re aggregating and using data,” says Ramjee. “It’s a first step to show that these companies can’t just run rampant, especially when you have these big companies with multimillion-dollar fines slapped on them. It acts as a deterring factor to show you can’t simply get away with this.” Legislative bodies across the globe are recognizing the need to hold firms responsible for illegally collecting data and using them to develop or train algorithms. As a result, more regulations will likely be in place to mitigate the issues that come with these practices. The European Union (E.U.) is already leading the way with its General Protection Data Regulation (GDPR), but it’s also proposing an Artificial Intelligence Act. “The E.U. proposal does include a potential remedy of deletion or retraining,” Walsh says. The FTC will potentially continue investigating companies and imposing algorithmic destruction, but Ramjee believes a more comprehensive federal privacy law is crucial to stop deceitful companies in their tracks. “You can also have a presumably unbiased third party checking data and helping validate what’s going on in those platforms,” she says. The onus now falls on companies to establish more ethical data practices. “As consumers, we’re now much more concerned about how our data is being used because we know these algorithms are working in ways that cater to us—whether good or bad,” says Ramjee. “Consumers are pushing for privacy and thirsting for transparency, so it would behoove companies to be up front about what and how data is being used in a way that’s digestible for the average consumer.” Rina Diane Caballar is a journalist and former software engineer based in Wellington, New Zealand. The AI pioneer says it’s time for smart-sized, “data-centric” solutions to big issues Andrew Ng was involved in the rise of massive deep learning models trained on vast amounts of data, but now he’s preaching small-data solutions. Andrew Ng has serious street cred in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at Stanford University, cofounded Google Brain in 2011, and then served for three years as chief scientist for Baidu, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told IEEE Spectrum in an exclusive Q&A.",0.0
"
        A Smart Home System That Respects Privacy
    ",https://spectrum.ieee.org/a-smart-home-system-that-is-accurate-and-respectful-of-privacy,2022-04-14,,"The new device from MIT’s Media Lab relies on passive carbon dioxide and infrared sensors This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore. By sensing human activity and adjusting the environmental settings accordingly, smart-home systems could help create more energy-efficient and sustainable buildings. However, there have been privacy concerns when it comes to these systems monitoring peoples’ activity, and smart-home systems can require heavy amounts of data crunching to learn how to respond to a given environment. A new smart system, dubbed Chameleon, is designed to address both of these issues. It was recently tested in two different environments over the course of a month, and could predict human activity with 87 to 99 percent accuracy after just one week of training. The results are described in a study published 6 April in IEEE Internet of Things Journal. Andres Rico is a graduate research assistant at the MIT Media Lab’s City Science Group who was involved in the study. He and his colleagues built their system around carbon dioxide and passive infrared (PIR) sensors. “Carbon dioxide and PIR data inherently are not intrusive,” explains Rico, noting that systems based on cameras or other optical systems can raise privacy concerns. Carbon dioxide and infrared sensing could be useful for estimating a variety of scenarios, such as the number of people in a room and how heavily they are breathing, which can indicate resting or active states. Each scenario has unique carbon dioxide and infrared signatures which can be analyzed using machine learning. The Chameleon smart-home system uses both supervised and unsupervised machine-learning algorithms. As a result, the system already has some general knowledge on how to assess any environment, but it can also learn to distinguish the unique variables of its new environment too. This combination can reduce the time and costs associated with calibrating and maintaining the system, says Rico. The new Chameleon wall-mounted smart system has an infrared and CO₂ sensor modules connected to a Wi-Fi-enabled microcontroller. MIT Media Lab The MIT researchers put Chameleon to the test in two different environments: an office with a handful of employees and a classroom that had 15 to 20 occupants on any given day. All components—including the circuit board and carbon dioxide and PIR sensors—were combined onto a single unit and mounted on a wall. Although the study ran for a month, the results show that the smart-home system was able to classify the activities being done in the rooms with high accuracy after just one week of training. “The system delivers these results within rooms that are entirely different—this is very valuable for scaling these devices to more buildings and use cases,” says Rico. He says his team hopes to incorporate Chameleon into more real-world spaces, helping to create smarter buildings. For example, it could help a single room be used for a wide variety of activities (for example, video meeting, exercising, sleeping, and having a meal with friends) and respond appropriately to each. “These systems can also be integrated into digital urban-planning processes so as to inform communities on how they are using spaces in order to build consensus about policies to change, operate, and maintain offices, classrooms, parks, homes, etc.” says Rico. Michelle Hampson is a freelance writer based in Halifax. She frequently contributes to Spectrum's Journal Watch coverage, which highlights newsworthy studies published in IEEE journals. An almost Machiavellian plot pitted Fairchild against Texas Instruments In one corner stood the defending champion, Texas Instruments. In the other stood the challenger, Fairchild Semiconductor. The referee, judge, promoter, and only spectator was Polaroid. In contention was the contract for the electronics of Polaroid’s secret project—a pioneering product introduced in 1972 as the SX-70, a camera eventually purchased by millions of people. As the embodiment of truly automated instant photography, the SX-70 fulfilled a long-held dream of Edwin Land, founder of Polaroid Corp., Cambridge, Mass. Vital to this “point and shoot” capability was a new film—one that would develop while exposed to light and so eliminate the tear-away covers of previous Polaroid films. Also vital were sophisticated electronics to control all single lens reflex (SLR) camera functions, including flashbulb selection, exposure control, mirror positioning, start of print development, and ejection of print. These circuits were divided into three modules, one each for motor, exposure and logic, and flash control. At the final count, some 400 transistors were used.",0.0
"
        AI Fuses With Quantum Computing in Promising New Memristor
    ",https://spectrum.ieee.org/quantum-memristor,2022-04-13,,"Quantum device points the way toward an exponential boost in “smart” computing capabilities Recent years have seen computing advance in two major ways—breakthroughs in machine learning to develop algorithms that improve automatically through experience, and research into quantum computers that can theoretically prove more powerful than any supercomputer. Now scientists have created the first prototype of a device known as a quantum memristor, which might help bring together the best of both of those worlds—combining artificial intelligence with quantum computing for unprecedented capabilities. A memristor, or memory resistor, is a kind of building block for electronic circuits that scientists predicted roughly 50 years ago but created for the first time only a little more than a decade ago. These components are essentially electric switches that can remember whether they were toggled on or off after their power is turned off. As such, they resemble synapses—the links between neurons in the human brain—whose electrical conductivity strengthens or weakens depending on how much electrical charge has passed through them in the past. In theory, memristors can act like artificial neurons capable of both computing and storing data. As such, researchers have suggested that neuromorphic or brainlike computers built using memristors would perform well at running neural networks, which are machine-learning systems that use synthetic versions of synapses and neurons to mimic the process of learning in the human brain. “The memristor, unlike any other quantum component, has memory.”—Michele Spagnolo, University of Vienna Now scientists in Austria and Italy have developed a quantum version of the memristor that they suggest could lead to quantum neuromorphic computers. They detailed their findings online last month in the journal Nature Photonics. Quantum computers rely on how the universe becomes a fuzzy place at its very smallest levels. For example, atoms, photons, and other building blocks of the cosmos can exist in states of flux known as superpositions, meaning they can essentially be located in two or more places at once, or spin in two opposite directions at the same time. Whereas classical computers switch transistors either on or off to symbolize data as ones or zeroes, quantum computers use quantum bits—qubits—that can be in a state of superposition where they are both 1 and 0 simultaneously. The more qubits that are linked together in a quantum computer, the greater its computational power can grow, in an exponential fashion. Scientists are still researching the specific problems for which quantum computing might have an advantage over classical computing. Recently, they have begun exploring whether quantum computing might help boost machine learning. Previous research suggested developing a quantum memristor using photons to help support quantum machine learning. However, that prior work “would have been extremely challenging to realize, because it required to create a quantum superposition of a one-photon state with a zero-photon—that is, a vacuum—state,” says study lead author Michele Spagnolo, a doctoral student in quantum physics at the University of Vienna. In the new study, Spagnolo and his colleagues instead developed a quantum memristor that relies on a stream of photons existing in superpositions where each single photon can travel down two separate paths laser-written onto glass. One of the channels in this single-qubit integrated photonic circuit is used to measure the flow of these photons, and this data, through a complex electronic feedback scheme, controls the transmissions on the other path, resulting in the device behaving like a memristor. Normally, memristive behavior and quantum effects are not expected to coexist, Spagnolo notes. Memristors are devices that essentially work by measuring the data flowing within them, but quantum effects are infamously fragile when it comes to any outside interference such as measurements. The researchers note they overcame this apparent contradiction by engineering interactions within their device to be strong enough to enable memristivity but weak enough to preserve quantum behavior. Using computer simulations, the researchers suggest quantum memristors could lead to an exponential growth in performance in a machine-learning approach known as reservoir computing that excels at learning quickly. “Potentially, quantum reservoir computing may have a quantum advantage over classical reservoir computing,” Spagnolo says. The advantage of using a quantum memristor in quantum machine learning as opposed to conventional quantum circuits is “the fact that the memristor, unlike any other quantum component, has memory,” Spagnolo says. The next step in this work is to connect several memristors together, Spagnolo notes. Future research can also scale up by increasing the number of photons in each memristor and the number of states in which they can exist within each device, he adds. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. The AI pioneer says it’s time for smart-sized, “data-centric” solutions to big issues Andrew Ng was involved in the rise of massive deep learning models trained on vast amounts of data, but now he’s preaching small-data solutions. Andrew Ng has serious street cred in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at Stanford University, cofounded Google Brain in 2011, and then served for three years as chief scientist for Baidu, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told IEEE Spectrum in an exclusive Q&A.",0.0
"
        The First High-Yield, Sub-Penny Plastic Processor
    ",https://spectrum.ieee.org/plastic-microprocessor,2022-06-14,,"It took a major redesign for cheap flexible chips to reach their promise Using PragmatIC’s manufacturing process to make a 4-bit microcontroller on plastic, engineers performed what they believe is the first yield study on plastic processors. For decades, hopeful techies have been promising a world where absolutely every object you encounter—bandages, bottles, bananas—will have some kind of smarts thanks to supercheap programmable plastic processors. If you’ve been wondering why that hasn’t happened yet, it’s that nobody has built working processors that can be made in the billions for less than a penny each. It hasn’t been for want of trying; in 2021 Arm reproduced its simplest 32-bit microcontroller, the M0, in plastic, but even this couldn’t hope to meet the mark. The problem, according to engineers at the University of Illinois Urbana-Champaign and at British flexible-electronics manufacture PragmatIC Semiconductor, is that even the simplest industry-standard microcontrollers are too complex to make on plastic in bulk. In research to be presented at the International Symposium on Computer Architecture later this month, the transatlantic team presents a simple yet fully functional plastic processor that could be made at sub-penny prices. The Illinois team designed 4-bit and 8-bit processors specifically to minimize size and maximize the percentage of working integrated circuits produced. Eighty-one percent of the 4-bit version worked, and that’s a good enough yield, says team leader Rakesh Kumar, to breach the one-penny barrier. “Flexible electronics has been niche for decades,” says Kumar. He adds that this yield study shows “that they may be ready for the mainstream.” The processors his team built were made using the flexible thin-film semiconductor indium gallium zinc oxide (IGZO), which can be built on plastic and continues to work even when bent around a radius of millimeters. But while a reliable manufacturing process is a prerequisite, it was the design that made the difference. You might be wondering why silicon processors can’t do the job of supercheap flexible computing. Kumar’s analysis suggest it won’t work. Compared to plastic, silicon is expensive and inflexible, but if you make the chip small enough, the plastic can just bend around it. However, silicon fails at the task for two reasons: One is that although the area of circuitry could be made supersmall, you still need to leave a comparatively large amount of space around the edges so that the chip can be cut out of the wafer. In the case of a microcontroller as simple as the Flexicore, there would be more space around the edge than there is area containing circuitry. What’s more, you’ll need still more room to fit enough I/O pads so data and power can get to the chip. Suddenly, you’ve got a large area of costly blank silicon, pushing up expenses past the critical US $0.01 mark. Instead of adapting an existing microcontroller architecture to plastic, Kumar’s team started from scratch to create a design called Flexicore. “Yield goes down very quickly as you increase gate count,” says Kumar. Knowing that, they came up with a design meant to minimize the number of gates needed. Using 4-bit and 8-bit logic instead of 16-bit or 32-bit helped. As did separating the memory that stores instructions from the memory that stores data. But they also cut down on the number and complexity of the instructions the processor is capable of executing. The team further simplified, by designing the processor so it executes an instruction in a single clock cycle instead of the multistep pipelines of today’s CPUs. Then they designed logic that implements those instructions by reusing parts, further reducing the gate count. “In general, we were able to simplify the design of FlexiCores by tailoring them to the needs of flexible applications, which tend to be computationally simple,” says Nathaniel Bleier, Kumar’s student. All of this resulted in a 5.6-square-millimeter 4-bit FlexiCore made up of just 2,104 semiconductor devices (about the same as the number of transistors in an Intel 4004 from 1971) versus some 56,340 devices for PlasticARM. “It’s an order of magnitude less than the tiniest silicon microcontrollers in terms of gate count,” he says. The team also developed an 8-bit version of FlexiCore, but it did not yield as well. “This is exactly the kind of design innovation needed to support truly ubiquitous electronics,” says Scott White, CEO of PragmatIC Semiconductor. With PragmatIC, the Illinois team produced plastic-coated wafers full of 4-bit and 8-bit processors and tested them at a variety of voltages on multiple programs and bent them without mercy. The experiment seems basic, but according to Kumar, it’s groundbreaking. Most research processors built using nonsilicon technologies yield so poorly that results are reported from one or at best a few working chips. “This is the first work, to the best of our knowledge, where anyone reported data from multiple chips for any nonsilicon technology,” he says. Not satisfied with this success, Kumar’s team came up with a design tool to explore architectural optimizations for different applications. For example, the tool showed that power consumption could be reduced considerably by allowing the gate count to inch up a bit. The chip industry has been targeted toward “the metrics of power and performance and to some degree reliability,” observed Kumar. “We haven’t focused on cost, conformality, and thinness. Focusing on those allows us to build new computer architectures and target new applications.” Flexible electronics pioneer John A. Rogers, at Northwestern University, called the work “very impressive.” He looks forward to experimental studies of the effects of bending on circuit performance. AMD, Graphcore, and Intel show why the industry’s leading edge is going vertical A crop of high-performance processors is showing that the new direction for continuing Moore’s Law is all about up. Each generation of processor needs to perform better than the last, and, at its most basic, that means integrating more logic onto the silicon. But there are two problems: One is that our ability to shrink transistors and the logic and memory blocks they make up is slowing down. The other is that chips have reached their size limits. Photolithography tools can pattern only an area of about 850 square millimeters, which is about the size of a top-of-the-line Nvidia GPU. For a few years now, developers of systems-on-chips have begun to break up their ever-larger designs into smaller chiplets and link them together inside the same package to effectively increase the silicon area, among other advantages. In CPUs, these links have mostly been so-called 2.5D, where the chiplets are set beside each other and connected using short, dense interconnects. Momentum for this type of integration will likely only grow now that most of the major manufacturers have agreed on a 2.5D chiplet-to-chiplet communications standard.",0.0
"
        Quantum Computers Exponentially Faster at Untangling Insights
    ",https://spectrum.ieee.org/quantum-computing,2022-06-13,,"Classical computers cannot overcome the “quantum advantage” in simulating chemistry and physics experiments Using Google’s Sycamore quantum processor, a new study reveals that quantum computers need exponentially fewer experiments than classical machines to reveal insights about viruses, black holes, and more. Quantum computers can theoretically achieve a quantum advantage where they can find the answers to problems no classical computer could solve even if given thousands of years or more. This advantage can grow exponentially when a quantum computer links together a greater number of qubits—the quantum-mechanically entangled bits that such a computer uses. One quantum-computing application that has drawn plenty of attention is code breaking. However, when Nobel laureate Richard Feynman first proposed the idea of quantum computers, he envisioned them modeling quantum systems such as molecules—for instance, undertaking chemistry and physics simulations that might yield insights into next-generation batteries or new drugs. “My primary aspiration is to build a quantum artificial superintelligence,” says study coauthor Robert Hsin-Yuan Huang, a theoretical quantum physicist and theoretical computer scientist at Caltech, in Pasadena, Calif. “We are very far from being able to achieve that goal. But I always feel that there are many questions we can explore now to bring us closer to that dream. Understanding how quantum technology could improve our ability to learn from the physical world is a very important first step towards this ambitious goal.” In the new study, researchers focused on how both classical and quantum computers might analyze data collected about quantum systems during experiments. In both conventional and quantum-enhanced experiments, sensors may collect multiple readings of a quantum system. However, conventional experiments can analyze such readings only one at a time, whereas a quantum-enhanced experiment can entangle these multiple readings and analyze them all at once. In experiments employing up to 40 qubits and 1,300 quantum gates in the 54-qubit Sycamore processor, the researchers found that quantum machines can learn from exponentially fewer experiments than those required in conventional experiments. “These results provide the first rigorous foundation showing that emerging quantum technology can significantly improve how humans can learn about nature in physics, chemistry, material science, and biology,” Huang says. The researchers focused on three different tasks—predicting the properties of a quantum system after scanning its properties; predicting the properties of a key component of a quantum system after analyzing its behavior; and modeling the behavior of a quantum system. Their findings suggest that no conventional experiments with classical computers can overcome the quantum advantage seen with quantum computers on such tasks. “This gives me hope that quantum computers will allow us to see and learn about parts of our universe that would otherwise be invisible,” says study coauthor Jarrod McClean, a theoretical quantum physicist and theoretical computer scientist at Google Quantum AI in Venice, Calif. Currently quantum computers are noisy intermediate-scale quantum (NISQ) platforms, meaning they are error-ridden and at most possess only a few dozen to a few hundred qubits. However, the researchers note their results suggest that even today’s NISQ processors can display a substantial quantum advantage when it comes to learning from experiments. “The physical experiments on Google’s Sycamore processor show that a huge quantum advantage can already be seen on noisy quantum machines,” Huang says. “This shows that we may be able to see how quantum technology can transform science sooner than we originally think.” The scientists detailed their findings 9 June in the journal Science. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. An almost Machiavellian plot pitted Fairchild against Texas Instruments In one corner stood the defending champion, Texas Instruments. In the other stood the challenger, Fairchild Semiconductor. The referee, judge, promoter, and only spectator was Polaroid. In contention was the contract for the electronics of Polaroid’s secret project—a pioneering product introduced in 1972 as the SX-70, a camera eventually purchased by millions of people. As the embodiment of truly automated instant photography, the SX-70 fulfilled a long-held dream of Edwin Land, founder of Polaroid Corp., Cambridge, Mass. Vital to this “point and shoot” capability was a new film—one that would develop while exposed to light and so eliminate the tear-away covers of previous Polaroid films. Also vital were sophisticated electronics to control all single lens reflex (SLR) camera functions, including flashbulb selection, exposure control, mirror positioning, start of print development, and ejection of print. These circuits were divided into three modules, one each for motor, exposure and logic, and flash control. At the final count, some 400 transistors were used.",0.0
"
        Regulators Rush to Become “Crypto-Friendly”
    ",https://spectrum.ieee.org/cryptocurrency-blockchain-regulation,2022-06-10,,"Blockchain-conscious policies are proliferating worldwide—balancing innovation against protection Regulation might seem anathema to cryptocurrencies, whose entire raison d’être is to circumvent the power of governments and banks to control how people use their money. But as the technology pushes into the mainstream, there’s a growing sense that new rules are both necessary and even beneficial, leading to a boom in national experiments in crypto regulation. Some countries clearly see cryptocurrencies as a direct threat to the power of the state and have instituted outright bans, most famously in China, which had previously been a major hub for crypto activity. But others are attempting a careful balancing act, trying to manage the potential dangers without stifling innovation in a potentially lucrative new industry. There’s no established playbook for how to regulate such a new technology though, and Sergiu Hamza, CEO of crypto analyst firm Coincub, says the pace of experimentation has accelerated dramatically in the last year. His company provides a ranking of crypto-friendly countries which considers things like adoption levels and local talent, but also regulation and tax rules. “It changes so fast,” he says. “In the last month, we have compiled a list of 150 news articles on different regulation changes, with at least 10 countries radically changing their positions.” Many in the cryptocurrency industry welcome regulation, because it provides clarity for both users and service providers about where they stand in the eyes of the law. What those regulations look like varies considerably from country to country, says Hamza. In general though, they typically deal with questions of taxation, how to recognize and regulate the activity of key crypto players like exchanges and coin issuers, and also whether to classify cryptocurrencies as money, investments, or something else entirely. One of the most pressing questions often concerns taxes, and some countries have implemented highly favorable regimes to tempt crypto users and firms to their shores. Hungary, for instance, has initiated a flat 15 percent tax on cryptocurrency gains at the time they’re converted into fiat, with no other income or capital gains taxes. And while die-hard crypto-anarchists might bristle at this kind of government oversight, Hamza says many in the industry welcome regulation of its core activities, because it provides clarity for both users and service providers about where they stand in the eyes of the law. Perhaps unsurprisingly, major financial hubs like Singapore and Switzerland and tax havens like Malta and the Bahamas have been ahead of the pack when it comes to passing more sophisticated crypto regulation. “Countries that are used to financial innovation and countries that are at the forefront of technology, obviously it’s easier for them to understand crypto and deal with it,” says Hamza. Malta for instance was one of the first countries to formally regulate cryptocurrencies when it passed a trio of laws in 2018 that defined what counts as a “Virtual Financial Asset” and set out rules for how they could be issued, traded, and exchanged. Singapore has also been proactive, running a regulatory sandbox for fintech companies since 2016 that relaxes legal requirements to enable experimentation. It also introduced the Payment Services Act in 2019, which regulated how cryptocurrencies could be issued and brought exchanges and other crypto firms under the oversight of the Monetary Authority of Singapore. The country that topped Coincub’s rankings for crypto-friendliness was Germany. While its first-place finish was also due to high adoption and a burgeoning crypto industry, Germany has been making progressive regulatory moves, says Hamza. The country charges no tax on gains from crypto held for longer than a year, and a law close to being passed will allow investment funds called spezialfonds, which are not available to retail investors and therefore more lightly regulated, to invest up to 20 percent of their holdings in cryptocurrency. “Cryptos work best at the places where more traditional instruments are not working.”—Max Semenchuk, blockchain entrepreneur Hagen Weiss, senior expert adviser at the country’s financial supervisor BaFin, says the core of its strategy for regulating crypto is “same risk, same business, same rules”. The country hasn’t set out in law how different crypto products should be treated, instead regulators look at them on a case-by-case basis and treat them the same way they would treat a traditional asset with the same level of risk, but that is only possible thanks to close collaboration between regulators and the industry. “It is probably the No. 1 tool that should be used—engagement with the market,” says Weiss. “The main benefits are that if you are proactive and engage with the situation, you do two things. First, you will protect your citizens and their money, and secondly, you will harness the potential of that technology.” The attitude of regulators can often be just as important as the regulations themselves, says Hamza, which is why Coincub’s rankings include a measure of Institutional Outlook. This accounts for public statements about the authorities’ general attitude towards cryptocurrencies and how they intend to govern them going forward. That’s critical, says Hamza, given the rapid and sizable shifts that can happen. He gives the example of Portugal, which has long been seen as a crypto-friendly destination because trading and using cryptocurrencies have been tax-exempt since 2018. But the government recently announced it plans to reverse course, and although a recent bill designed to tax cryptocurrencies failed to make it through parliament, it seems likely that the country’s laissez-faire approach is coming to an end. How countries approach crypto regulation also varies considerably depending on what their motivations are. Most countries setting crypto-friendly rules are trying to boost their domestic crypto industry, but there can be other reasons too. There have been plenty of headlines about the use of cryptocurrencies in Ukraine since the start of the Russian invasion—in particular the fund the government set up to allow people to donate cryptocurrencies toward its war effort. But a law recognizing cryptocurrencies as legal assets and introducing financial monitoring measures, which passed shortly after the start of the war, had been in the pipeline for years. Max Semenchuk, a blockchain entrepreneur who is currently acting as an adviser to Ukraine’s Ministry for Digital Transformation, says the country has had a progressive attitude to cryptocurrencies for some time and its goals are quite different from those of a financial hub like Switzerland. The country has had the highest level of adoption of cryptocurrency for some time, currently standing at about 12 percent of the population, and Semenchuk says the aim is primarily to support the use of the technology by individuals. “Cryptos work best at the places where more traditional instruments are not working,” he says. “There’s not so much trust for the banks. We have got some history of banks folding, and crisis and devaluation of currency.” The main use cases in Ukraine, he says, are for business-to-to business transactions, as a form of investment for everyday people, and a way to transfer money in to and out of the country. The technology also feeds into the government’s desire to push increasing digitalization of municipal services and the economy. “ ‘We should be the country of the smartphone’ is one of the big slogans,” says Semenchuk. However, experimentation inevitably involves some upsets, and regulations are likely to remain highly dynamic. Ukraine’s central bank recently banned cryptocurrency purchases in the local hryvnia currency—alongside deposits in e-wallets and foreign-exchange accounts—over concerns about money flowing out of the country during the war. And despite its generally light regulatory touch, Singapore banned crypto providers from advertising directly to retail customers at the start of the year. And in April it extended its oversight powers to include crypto companies headquartered in Singapore but providing services abroad. “The position is changing so fast right now that we can only talk about what’s happening this week or this month,” says Hamza. Edd Gent is a freelance science and technology writer based in Bangalore, India. His writing focuses on emerging technologies across computing, engineering, energy and bioscience. He's on Twitter at @EddytheGent and email at edd dot gent at outlook dot com. His PGP fingerprint is ABB8 6BB3 3E69 C4A7 EC91 611B 5C12 193D 5DFC C01B. His public key is here. DM for Signal info. This computer rendering depicts the pattern on a photonic chip that the author and his colleagues have devised for performing neural-network calculations using light. Think of the many tasks to which computers are being applied that in the not-so-distant past required human intuition. Computers routinely identify objects in images, transcribe speech, translate between languages, diagnose medical conditions, play complex games, and drive cars. The technique that has empowered these stunning developments is called deep learning, a term that refers to mathematical models known as artificial neural networks. Deep learning is a subfield of machine learning, a branch of computer science based on fitting complex models to data.",0.0
"
        “PACMAN” Hack Breaks Apple M1’s Last Line of Defense
    ",https://spectrum.ieee.org/pacman-hack-can-break-apple-m1s-last-line-of-defense,2022-06-10,,"How many dominos could fall if this centerpiece CPU’s weakness pans out? Apple’s M1 processor is a powerful, high-efficiency chip, though perhaps not as impervious to attacks as its initial safety record might suggest. Apple’s M1 processor made a big splash on its November 2020 release, noteworthy for its eye-popping performance and miserly power consumption. But the value of its security may not be as obvious at first blush. A lack of serious attacks since its launch nearly two years ago indicates that its security systems, among them a last line of defense called pointer authentication codes, are working well. But its honeymoon period could possibly be coming to an end. At the International Symposium on Computer Architecture later this month, researchers led by MIT’s Mengjia Yan will present a mode of attack that so weakens the pointer authentication code (PAC) defense that the core of a computer’s operating system is made vulnerable. And because PACs may be incorporated in future processors built from the 64-bit Arm architecture, the vulnerability could become more widespread. It’s possible that other processors are already using PACs, but the M1 was the only one available to Yan’s lab. “What we found is actually quite fundamental,” says Yan. “It’s a class of attack. Not one bug.” How PACMAN picks the lock goes to the heart of modern computing. The vulnerability, called PACMAN, assumes that there is already a software bug in operation on the computer that can read and write to different memory addresses. It then exploits a detail of the M1 hardware architecture to give the bug the power to execute code and possibly take over the operating system. “We assume the bug is there and we make it into a more serious bug,” says Joseph Ravichandran a student of Yan’s who worked on the exploit with fellow students Weon Taek Na and Jay Lang. To understand how the attack works you have to get a handle on what pointer authentication is and how a detail of processor architecture called speculative execution works. Pointer authentication is a way to guard against software attacks that try to corrupt data that holds memory addresses, or pointers. For example, malicious code might execute a buffer overflow attack, writing more data than expected into a part of memory, with the excess spilling over into a pointer’s address and overwriting it. That might then mean that instead of the computer’s software executing code stored at the original address, it is diverted to malware stored at the new one. Pointer authentication appends a cryptographic signature to the end of the pointer. If there’s any malicious manipulation of the pointer, the signature will no longer match up with it. PACs are used to guard the core of the system’s operating system, the kernel. If an attacker got so far as to manipulate a kernel pointer, the mismatch between the pointer and its authentication code would produce what’s called an “exception,” and the system would crash, ending the malware’s attack. Malware would have to be extremely lucky to guess the right code, about 1 in 65,000. PACMAN finds a way for malware to keep guessing over and over without any wrong guesses triggering a crash. How it does this goes to the heart of modern computing. For decades now, computers have been speeding up processing using what’s called speculative execution. In a typical program, which instruction should follow the next often depends on the outcome of the previous instruction (think if/then). Rather than wait around for the answer, modern CPUs will speculate—make an educated guess—about what comes next and start executing instructions along those lines. If the CPU guessed right, this speculative execution has saved a bunch of clock cycles. If it turns out to have guessed wrong, all the work is thrown out, and the processor begins along the correct sequence of instructions. Importantly, the mistakenly computed values are never visible to the software. There is no program you could write that would simply output the results of speculative execution. Initial solutions to PACMAN only tended to increase the processor’s overall vulnerability. However, over the past several years, researchers have discovered ways to exploit speculative execution to do things like sneak data out of CPUs. These are called side-channel attacks, because they acquire data by observing indirect signals, such as how much time it takes to access data. Spectre and Meltdown, are perhaps the best known of these side-channel attacks. Yan’s group came up with a way to trick the CPU into guessing pointer authentication codes in speculation so an exception never arises, and the OS doesn’t crash. Of course, the answer is still invisible to software. But a side-channel trick involving stuffing a particular buffer with data and using timing to uncover which part the successful speculation replaces, provides the answer. [A similar concept is explained in more detail in “How the Spectre and Meltdown Hacks Really Worked,” IEEE Spectrum, 28 February 2019.] With regard to PACMAN, Apple’s product team provided this response to Yan’s group: “We want to thank the researchers for their collaboration as this proof-of-concept advances our understanding of these techniques. Based on our analysis, as well as the details shared with us by the researchers, we have concluded this issue does not pose an immediate risk to our users and is insufficient to bypass device protections on its own.” Other researchers familiar with PACMAN say that how dangerous it really is remains to be seen. However, PACMAN “increases the number of things we have to worry about when designing new security solutions,” says Nael Abu-Ghazaleh, chair of computer engineering at University of California, Riverside, and an expert in architecture security, including speculative execution attacks. Processors makers have been adding new security solutions to their designs besides pointer authentication in recent years. He suspects that now that PACMAN has been revealed, other research will begin to find speculative attacks against these new solutions. Yan’s group explored some naive solutions to PACMAN, but they tended to increase the processor’s overall vulnerability. “It’s always an arms race,” says Keith Rebello, the former program manager of DARPA’s System Security Integrated Through Hardware and firmware (SSITH) program and currently a senior technical fellow at the Boeing Company. PACs are there “to make it much harder to exploit a system, and they have made it a lot harder. But is it the complete solution? No.” He’s hopeful that tools developed through SSITH, such as rapid re-encryption, could help. Abu-Ghazaleh credits Yan’s group with opening a door to a new aspect of processor security. “People used to think software attacks were standalone and separate from hardware attacks,” says Yan. “We are trying to look at the intersection between the two threat models. Many other mitigation mechanisms exist that are not well studied under this new compounding threat model, so we consider the PACMAN attack as a starting point.” This computer rendering depicts the pattern on a photonic chip that the author and his colleagues have devised for performing neural-network calculations using light. Think of the many tasks to which computers are being applied that in the not-so-distant past required human intuition. Computers routinely identify objects in images, transcribe speech, translate between languages, diagnose medical conditions, play complex games, and drive cars. The technique that has empowered these stunning developments is called deep learning, a term that refers to mathematical models known as artificial neural networks. Deep learning is a subfield of machine learning, a branch of computer science based on fitting complex models to data.",0.0
"
        Data Is Vulnerable to Quantum Computers That Don’t Exist Yet
    ",https://spectrum.ieee.org/post-quantum-cryptography,2022-06-09,,"A new spin-off from Alphabet has a plan to transition to postquantum cryptography Future quantum computers may rapidly break modern cryptography. Now a new spin-off from Google’s parent company Alphabet warns that sensitive data is already vulnerable to quantum computers that don’t exist yet, courtesy of codebreaking attacks that steal that data now and could decrypt it in the future. Therefore, it has developed a road map to help businesses, governments and other organizations begin the shift to post-quantum cryptography now. The new startup, Sandbox AQ (which stands for AI and quantum), has already attracted clients including Mount Sinai Health System, telecommunications firm Softbank Mobile, communications technology company Vodafone Business, and Web developer Wix. It has also reeled in investors including the CIA’s venture capital arm In-Q-Tel and cybersecurity-focused investment firm Paladin Capital Group. Former Google CEO Eric Schmidt is serving as the chairman of its board of directors. In addition, Sandbox AQ has already partnered with two of the world’s largest professional service firms, Ernst & Young and Deloitte, to help deploy post-quantum cryptography. “These firms have the scale to educate, engage, and upgrade post-quantum cryptography for their Global 1000 clients, which represent the world’s largest and most successful companies,” says David Joseph, a research scientist at Sandbox AQ in Palo Alto, Calif. “Doing this will multiply the impact of our quantum solutions and help companies protect their customers, data, networks, and other assets today, without having to wait until error-corrected quantum computers become available.” Quantum computers theoretically can quickly solve problems it might take classical computers untold eons to solve. For example, much of modern cryptography depends on the extreme difficulty that classical computers face with regard to mathematical problems such as factoring huge numbers, but quantum computers could in principle rapidly crack even highly secure RSA-2048 encryption. To stay ahead of quantum computers, scientists around the world have spent the past two decades designing post-quantum cryptography (PQC) algorithms. These are based on new mathematical problems that both quantum and classical computers find difficult to solve. In January, the White House issued a memorandum on transitioning to quantum-resistant cryptography, underscoring that preparations for this transition should begin as soon as possible. However, after organizations such as the National Institute of Standards and Technology (NIST) help decide which PQC algorithms should become the new standards the world should adopt, there are billions of old and new devices that will need to get updated. Sandbox AQ notes that such efforts could take decades to implement. Although quantum computers are currently in their infancy, there are already attacks that can steal encrypted data with the intention to crack it once codebreaking quantum computers become a reality. Therefore, the Sandbox AQ argues that governments, businesses, and other major organizations must begin the shift toward PQC now. For example, in a store-now-decrypt-later attack, adversaries would capture precious encrypted information now, store it, and decrypt it when practical quantum computers exist. Stolen data could include medical records, national security documents, trade secrets, and more—any information that may still prove valuable even decades later. “We know for a fact that store-now-decrypt-later attacks are happening right now, and their frequency will only increase the closer we get to delivering a fault-tolerant quantum computer,” Joseph says. “Once encrypted data has been exfiltrated, there is no way to protect it from future decryption and exploitation.” Store-now-decrypt-later attacks do not need high-profile breaches to succeed. “They could be performed silently by first observing encrypted data on public networks, which would be very difficult to detect,” Joseph says. “Over the public Internet, encrypted data might be sent via many different nodes, and any one of these nodes could be compromised, copying and storing valuable data before forwarding it on to its intended final destination.” The main difficulty in executing store-now-decrypt-later attacks is figuring out which data to target, “as there will be an enormous volume of encrypted data and only a finite amount of quantum computing resources,” Joseph says. “We expect the first quantum-enabled adversaries will be nation-states, and it may not be public knowledge exactly when one of them gains access to a large, fault-tolerant device capable of breaking RSA-2048.” Another reason shifting to post-quantum cryptography may prove important is because of projects that are getting designed and planned now but may have life spans of decades, such as many cars, planes, trains, and ships in production now, or critical national infrastructure projects. The hardware needed to implement cryptography may essentially remain immutable for the lifetime of these products and projects, so the earlier they can get protected, the better, Joseph and his colleagues note. The inspiration to launch Sandbox AQ grew from discussions between security teams within Alphabet starting from 2016. “It became apparent that there was a huge wealth of experience across the now–Sandbox AQ team and the Googlers across Alphabet, but most of this expertise was focused on distinct, introspective efforts that directly benefited Google’s customers,” Joseph says. “However, when we spoke with decision-makers at external organizations, it became clear that what was ‘common knowledge’ in the security community was not well known at large.” Sandbox AQ’s efforts to explain the importance of PQC led the startup to draft a new road map for organizations to shift past traditional cryptography. The company detailed its road map on 11 May in the journal Nature. The first recommendation Sandbox AQ makes is to figure out where PQC transition is needed first. The workforce to perform these upgrades is highly specialized and usually scarce, and so needs to get deployed in ways that make the most of resources to protect systems best. This involves identifying the cryptographic schemes that are at highest risk, such as key exchange algorithms, the kind that often underlie secure messages and data transfers. Instead of replacing existing algorithms with relatively untested PQC alternatives, the road map notes that scientists have developed hybrid algorithms combining both traditional algorithms and post-quantum algorithms. Therefore, even if the PQC algorithm later proves flawed, at least the classical algorithm can still provide a measure of security. The road map notes that NIST’s PQC project is close to the end of its third round and standards for the algorithms selected are expected to be released no later than 2024. Sandbox AQ recommends that organizations may want to start experimenting now with the finalist and alternative candidates. The company also suggests considering stateful hash-based signature technology for applications such as software code signing, as NIST and other bodies have already standardized it. The most comprehensive repository of the software implementations of the NIST schemes is Liboqs of the Open Quantum Safe project. Other resources include BoringSSL, Tink, and SUPERCOP. Due to the current diversity of PQC alternatives, the need to change from one algorithm to another in case of a successful attack, and the desire for increasing connectedness between systems, the road map also recommends “crypto-agility,” or the ability to switch between cryptographic schemes. Sandbox AQ notes that standards bodies should make 6G wireless technologies, for example, inherently crypto-agile and PQC-compatible. Sandbox AQ also helps organizations shift to PQC by conducting three-phase security audits. “The first phase is discovery, where we assess and catalog the organization’s cryptographic infrastructure to understand where any potential vulnerabilities lie,” Joseph says. “We then conduct a performance analysis in order to provide a quantum readiness evaluation and risk-based PQC migration plan.” The next step “is the assessment phase, migrating selected candidates from IT infrastructure to demonstrate functional success and performance,” he says. “We catalog mitigation patterns that will become the standard for the full implementation.” Finally comes “the implementation phase, which includes the complete transition of an organization’s IT infrastructure, in order of priority,” Joseph says. “It enables cryptographic agility throughout the network and enables full sovereignty over cryptographic usage.” As dangerous as code breaking quantum computers may prove, history shows that cryptography transitions need a considerable amount of time. For example, elliptic curve cryptography was proposed in the 1980s, and despite the fact that it is far more efficient than RSA in terms of space and speed, it took more than two decades to finally gain widespread adoption. “By comparison, the transition to PQC will be larger and more complex,” Joseph says. “From this frame of reference, it became clear that awareness needs to be increased and the transition process needs to start now.” Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. An almost Machiavellian plot pitted Fairchild against Texas Instruments In one corner stood the defending champion, Texas Instruments. In the other stood the challenger, Fairchild Semiconductor. The referee, judge, promoter, and only spectator was Polaroid. In contention was the contract for the electronics of Polaroid’s secret project—a pioneering product introduced in 1972 as the SX-70, a camera eventually purchased by millions of people. As the embodiment of truly automated instant photography, the SX-70 fulfilled a long-held dream of Edwin Land, founder of Polaroid Corp., Cambridge, Mass. Vital to this “point and shoot” capability was a new film—one that would develop while exposed to light and so eliminate the tear-away covers of previous Polaroid films. Also vital were sophisticated electronics to control all single lens reflex (SLR) camera functions, including flashbulb selection, exposure control, mirror positioning, start of print development, and ejection of print. These circuits were divided into three modules, one each for motor, exposure and logic, and flash control. At the final count, some 400 transistors were used.",0.0
"
        Photonic Quantum Computer Claims Speedup “Advantage”
    ",https://spectrum.ieee.org/photonic-quantum-computing,2022-06-09,,"Quantum-powered cloud 7,800,000,000,000,000x as fast, for some problems, as a supercomputer In this graphical representation of a three-dimensional entangled state synthesized by the computer Borealis, each vertex represents a squeezed-state qubit, and each edge represents the connection (a.k.a. entanglement) between the vertices. A new photonic quantum computer takes just 36 microseconds to perform a task that would take a conventional supercomputer more than 9,000 years to complete. The new device, named Borealis, is the first quantum computer from a startup to display such “quantum advantage” over regular computers. Borealis is also the first machine capable of quantum advantage to be made available to the public over the cloud. Quantum computers can theoretically achieve a quantum advantage that enables them to find the answers to problems no classical computers could ever solve. The more components known as qubits that a quantum computer has, the greater its computational power can grow, in an exponential fashion. Many companies, including giants such as Google, IBM, and Amazon as well as startups such as IonQ, rely on qubits based on superconducting circuits or trapped ions. One drawback with these approaches is that they both demand temperatures colder than those found in deep space, because heat can disrupt the qubits. The expensive, bulky cryogenic systems required to hold qubits at such frigid temperatures can also make it a major challenge to scale these platforms up to high numbers of qubits—or to smaller and more portable form factors. In contrast, quantum computers that depend on qubits based on photons can, in principle, operate at room temperature. They can also readily integrate into existing fiber-optic-based telecommunications systems, potentially helping connect quantum computers into powerful networks and even into a quantum Internet. Scientists have developed quantum computers demonstrating quantum advantage using both cryogenic and photonic approaches. In 2019, Google argued its 53-superconducting-qubit Sycamore processor could carry out a calculation in 200 seconds that the company estimated would take Summit, the world’s most powerful supercomputer at that time, 10,000 years. Then, last year, researchers in China contended that photonic quantum computer Jiuzhang 2.0 could solve a benchmark problem roughly 1024 as fast as classical supercomputers. A key drawback of Jiuzhang 2.0 was that it relied on a network of fixed mirrors and lenses. Therefore, it was not programmable, limiting its overall usefulness. Now, in a new study, quantum computing startup Xanadu, in Toronto, reveals its device, named Borealis, may be the first fully programmable photonic quantum computer to display quantum advantage. “Borealis is the first machine capable of quantum computational advantage made publicly available to anyone with an Internet connection,” says study senior author Jonathan Lavoie, systems integration team lead at Xanadu. In the Canadian company Xanadu’s photonic quantum computer Borealis, squeezed-state qubits [pink pulses] are generated from a nonlinear crystal and sent through a series of three loop-based interferometers, pictured above. Xanadu In Borealis, qubits consist of so-called “squeezed states” consisting of superpositions of multiple photons in a light pulse. Whereas traditional qubits can, because of the surreal nature of quantum physics, exist in a state known as superposition where they can symbolize both a 0 and 1 of data, squeezed states can exist in states of 0, 1, 2, 3, or more. Borealis can generate trains of up to 216 pulses of squeezed light. “It’s important to recognize that Borealis is not equivalent to a 216-qubit traditional device,” Lavoie says. “Since it uses squeezed-state qubits, it addresses a different class of quantum tasks than, say, a device based on superconducting circuit qubits or trapped ions.“ In experiments, the researchers tested Borealis on a task known as Gaussian boson sampling, in which a machine analyzes random patches of data. Gaussian boson sampling may have many practical applications, such as identifying which pairs of molecules are the best fits for each other. In prior work, Jiuzhang 2.0 detected up to 113 photons out of 144 pulses of squeezed light. In the new study, Borealis detected up to 219 photons in its squeezed-light pulse trains, with 125 on average. All in all, the scientists estimated Borealis could perform Gaussian boson sampling more than 7.8 quadrillion times as fast as Fugaku, the fastest conventional supercomputer in the world in 2021. One key advance seen in Borealis was the use of photon-number-resolving detectors. Prior machines used threshold detectors that are designed to distinguish only between “no photons detected” and “at least one photon detected.” The size of the computational problems a photonic quantum computer may tackle grows exponentially with the number of photons it can detect, so the photon-number-resolving detectors helped Borealis perform more than 50 million times as fast as previous photonic quantum computers, Lavoie says. Xanadu has made Borealis available for use for everyone over the cloud. “We are also working with partners to make it more broadly available,” Lavoie says. “We hope that its public availability will stimulate even more research around quantum advantage and Gaussian boson sampling in general.“ Xanadu’s future research “is squarely focused on achieving error correction and ultimately fault tolerance, to unlock the most valuable problems in quantum computing,” Lavoie says. “Many of the technologies and lessons learned in building Borealis will be incorporated into our architecture for [future models].” The scientists detailed their findings in the 2 June issue of the journal Nature. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. This computer rendering depicts the pattern on a photonic chip that the author and his colleagues have devised for performing neural-network calculations using light. Think of the many tasks to which computers are being applied that in the not-so-distant past required human intuition. Computers routinely identify objects in images, transcribe speech, translate between languages, diagnose medical conditions, play complex games, and drive cars. The technique that has empowered these stunning developments is called deep learning, a term that refers to mathematical models known as artificial neural networks. Deep learning is a subfield of machine learning, a branch of computer science based on fitting complex models to data.",0.0
"
        What Is Quantum Entanglement?
    ",https://spectrum.ieee.org/what-is-quantum-entanglement,2022-06-08,,"Skip the heady and abstract physics lectures. Let’s talk about socks When pushed to explain why quantum computers can outspeed classical computers, stories about quantum computing often invoke a mysterious property called “entanglement.” Qubits, the reader is assured, can somehow be quantum mechanically entangled such that they depend on one another. If more detail is needed, the reader is told that entanglement links qubits no matter how far apart they are—so long as the qubits are “coherent.” For the reader, things are far from coherent. Sure, entanglement is an important aspect of quantum computing. But what exactly is it? In a few words, entanglement is when multiple objects—such as a pair of electrons or photons—share a single quantum state. Like threads in a tangle of yarn, entangled objects cannot be described as independent entities. That explanation might be poetic, but it shouldn’t be satisfying. Things are not so simple or concrete. But with a little bit of high-school-level math (near the end of this story), our intuitions—based on a lifetime of classical physics—can be retrained and redirected just a bit. However, we should also make the following disclaimer: No brief explanation can be expected to convey a comprehensive understanding of quantum mechanics. Our goal is simply to illustrate the basic concepts behind entanglement, so the reader can gain a more thorough understanding of what’s actually going on in this foundational phenomenon behind quantum computing. Let’s begin with a slightly modified example from the celebrated Northern Irish physicist John Stewart Bell: Alice and Bob know that Prof. Bertlmann always wears mismatched socks. If his left sock is pink, his right sock is certain to not be pink. (Our illustrations are loosely based on Bell’s own cartoons of his friend, Reinhold Bertlmann, which he drew for a 1980 article to explain entanglement.) Meet Professor Bertlmann, with his predictably and reliably mismatched socks.Mark Montgomery Alice and Bob wish to test if Prof. Bertlmann’s choice in socks persists outside the classroom, so they decide to spy on him. Suppose Alice is standing one block away, and she sees that Prof. Bertlmann’s left sock is blue. Immediately, she knows that Bob—who’s keeping an eye on Bertlmann’s right sock—will see pink. This holds true even if Alice is light-years away from Bertlmann, following all the sartorial goings-on with a telescope. Once she measures the color of one sock, Alice instantaneously knows something about what Bob will measure about the other sock. This is because the pair of mismatched socks are what physicists (and overqualified wardrobe consultants) describe as being correlated with one another. Alice sees that Prof. Bertlmann's right sock is blue; Bob can see that his left sock is pink. Mark Montgomery There is no mystery to how this correlation arises: Every morning, Prof. Bertlmann intentionally mismatches his socks. If his left sock is blue, he will pull on a right sock that’s pink. These correlations have three classical characteristics: They are real, local, and deterministic. Real: The socks have a definite color prior to Alice’s or Bob’s measurement. Deterministic: The color of the socks is not random. Given exactly the same initial conditions (for example, Tuesday, chance of rain, sore toe), Prof. Bertlmann will put on the mismatched socks the same way. Note: The specific colors worn on each foot may be difficult for Alice and Bob to predict, but the process is not strictly random. Local: The color of the socks depends only on nearby surroundings—that is, what Bob sees should not depend on Alice’s measurement. Let us now suppose Prof. Bertlmann wishes to teach his snooping students a thing or two about entanglement. The next morning, he puts on a mismatched pair of quantum socks, whose colors are entangled. Prof. Bertlmann complicates the situation by introducing quantum socks—whose color is indeterminate until it’s been observed. When Alice measures the left sock, the pair’s indeterminate state of color collapses, so that Alice sees either pink or blue.Mark Montgomery Unlike his classical socks, Prof. Bertlmann’s entangled quantum socks are: Unreal: The socks have no definite color prior to measurement. Nondeterministic: The color of the socks is random. Measurements under the exact same initial conditions are unpredictable—for example, 50 percent of the time a sock will be pink, 50 percent of the time it will be blue. Nonlocal: The color of each sock is dependent on nonlocal surroundings—that is, what Bob sees depends on Alice’s measurement. In the quantum case, if Alice measures the color of one sock, her measurement instantaneously updates the color of the other sock, which had previously been indefinite. (Note: Because the sock color is random, this does not, strictly speaking, transmit information—and therefore cannot be used for faster-than-light communication.) But, Alice objects, the socks are close together—couldn’t they send some signal to each other? Is the update really instantaneous? To further convince his students that there is something nonlocal happening, Prof. Bertlmann ships each of the socks to distant stars that are light-years apart from one another. Now each of the professor's fabled quantum socks have been sent to different star systems, many light-years apart. As before, when Alice observes the left sock, the pair's indeterminate state of color collapses. This measurement updates the color of the other sock instantaneously. For simplicity, we’ve shown only Alice measuring, but the results do not change if Bob measures too.Mark Montgomery Then he asks Alice and Bob to perform the same experiment. Again, what Bob sees depends on Alice’s measurement, even though there is no way for the socks to communicate. This result should be shocking. As Prof. Bertlmann puts it: “How come they always choose different colors when they are looked at? How does the second sock know what the first has done?” § A brief historical detour: Anxiety about entanglement originates from the famous 1935 EPR paper by Albert Einstein, Boris Podolsky, and Nathan Rosen (collectively known as EPR, from their initials). EPR recognized that quantum mechanics specified a world that was nondeterministic and nonlocal, and argued that these properties implied that quantum mechanics is incomplete as a theory. Einstein was particularly concerned about the lack of locality. He famously lamented the instantaneous measurement update between entangled particles as “spukhafte Fernwirkung” (“spooky action at a distance”) because he could not reconcile it with the “ideas of physics.” From Einstein’s letters to Max Born: To preserve these reasonable assumptions—and rescue physics from the seemingly irreconcilable weirdness that quantum mechanics introduced—physicists began playing with “hidden-variable theories.” According to hidden-variable theory, in Alice and Bob’s experiment the quantum socks are secretly predetermined (by a “hidden” variable) to be one color or another, and it only seems as though Alice’s measurement of the first sock instantaneously updates the color of the other sock. Hidden-variable theories, it was thought, could reproduce all the odd results quantum mechanics predicted without sacrificing local realism. Here is where we return to the topic of this article—because quantum entanglement lies at the heart of questions about hidden-variable theory. If hidden-variable theories are correct, entanglement is just an illusion of nonlocality; Bertlmann’s quantum socks would not in that case actually have a strange connection irrespective of distance. But if hidden-variable theories are wrong, entanglement really does link the socks no matter how far apart they are. Without any way of experimentally differentiating between either theory, physicists relegated the conundrum to the realm of philosophy—until Bell, the Northern Irish physicist referenced earlier, found a solution. § Back to the present day, where the skeptical students Alice and Bob are unimpressed by Prof. Bertlmann’s supposedly entangled socks. Again, from their perspective, there is no experimental difference between “spooky” socks and hidden-variable socks, which only appear to update each other upon measurement but whose color (according to hidden-variable theory) would actually be preordained. To enlighten his mistrustful mentees about the true nature of entanglement, Prof. Bertlmann sets up a new experiment. In this experiment, Alice and Bob fly off in spaceships. A source shoots out entangled pairs of quantum socks toward them. Alice and Bob each have special detectors with two settings. Alice’s detector can be in setting A or setting a; Bob’s detector can be set to either setting B or setting b. (To pick a setting, they each flip a coin after the socks are sent so there is no way the socks know the settings ahead of time.) To avoid any possible local effects, Alice and Bob fly light-years apart. They want this isolation so that they can tell if the sock color is predetermined or really updates upon measurement. When Alice and Bob have different settings (a and B, or A and b), the socks sometimes match and sometimes mismatch. When Alice and Bob share the same settings (A and B, or a and b), the socks always match. How could predetermined socks manage this correlation, if the random coin flip happened after the socks were sent?Mark Montgomery Here is a sample output from Alice and Bob: As you can see, the colors Alice and Bob measure are random. Let’s now add the detector settings, which are also random. Setting Alice Setting Bob A P b B A B B B a B b B A B B B a B B P a B b B A P B P A P b B a P B P A B B B a B B P a P b P a B b B A P b B A B B B a B B B A P b P a P B P The table shows that when Alice and Bob use different settings, the socks sometimes match and sometimes mismatch. But if Alice and Bob share a setting (both uppercase or both lowercase), they always match. It’s as if, despite the fact that Alice and Bob flipped coins after the socks were sent, the socks could somehow tell each other when to match across the vast distance. This is very strange—as if the sock color were predetermined! After Alice and Bob have recorded all their data, they come back to Earth, where Prof. Bertlmann explains that the strange coincidence is not quite enough to reveal the true nature of entanglement. What they need is to measure how correlated the socks are, so Alice and Bob create a new table where pinks are -1 and blues are +1. Setting Alice Setting Bob Combined Setting Sum A -1 b +1 Ab 0 A +1 B +1 AB 2 a +1 b +1 ab 2 A +1 B +1 AB 2 a +1 B -1 aB 0 a +1 b +1 ab 2 A -1 B -1 AB -2 A -1 b +1 Ab 0 a -1 B -1 aB -2 A +1 B +1 AB 2 a +1 B -1 aB 0 a -1 b -1 ab -2 a +1 b +1 ab 2 A -1 b +1 Ab 0 A +1 B +1 AB 2 a +1 B +1 aB 2 A -1 b -1 Ab -2 a -1 B -1 aB -2 “There is a very simple proof—that you can check on your own—which states that the absolute value of AB - Ab + aB + ab cannot be greater than 2 if the hidden-variable theory of sock color were true,” Prof. Bertlmann explains. Counting, Alice and Bob tally up their measurements and find the averages for AB, Ab, aB, and ab: Then they plug the numbers into the formula and find that the (positive) sum exceeds 2! Puzzled, they ask Prof. Bertlmann what it means. He responds: While no socks have been entangled in the real world (yet), there have been numerous experiments confirming that quantum correlations exceed Bell’s inequality, as above. The first experiments entangled electrons and photons, but in the decades since, scientists have even managed to entangle tiny, nanoscale objects, such as a pair of submicroscopic drums. § So, what is entanglement? In our first definition, we said that “entanglement is when multiple objects share a single quantum state.” We can update that definition with what we’ve learned: Entanglement is the amount by which multiple objects share a quantum state. By checking the correlations of our measurements, we can quantify how much entanglement there is between objects. The state of entangled objects cannot be described independently. Prof. Bertlmann’s quantum socks, light-years apart, are not reducible to a left sock color and a right sock color. They remain a pair of indeterminate color until they are disentangled by a measurement. In a quantum computer, qubits are separated by millimeters, not light-years. But the principle still holds, so an entangled pair really is one quantum object—at least until Alice or Bob takes a measurement. Bertlmann’s Socks and the Nature of Reality (1980)—J.S. Bell’s explanation of his own work remains essential, lucid, and funny. Is the Moon There When Nobody Looks? Reality and the Quantum Theory (1985)—N. David Mermin reviews the EPR controversy and what Alain Aspect’s 1982 experimental confirmation of Bell’s theorem means. The Age of Entanglement: When Quantum Physics Was Reborn (2008)—Louisa Gilder’s dramatized retelling of the debates between Einstein, Bohr, and others perceives entanglement from the vantage of a novel. Schrödinger’s Killer App: Race to Build the World’s First Quantum Computer(2013)—Full of off-color asides, the late Jon Dowling’s book is a rigorous but accessible introduction to the principles underlying quantum computing. It remains generously free online. Dan Garisto is a freelance science journalist who covers physics and other physical sciences. His work has appeared in Scientific American, Physics, Symmetry, Undark, and other outlets. This computer rendering depicts the pattern on a photonic chip that the author and his colleagues have devised for performing neural-network calculations using light. Think of the many tasks to which computers are being applied that in the not-so-distant past required human intuition. Computers routinely identify objects in images, transcribe speech, translate between languages, diagnose medical conditions, play complex games, and drive cars. The technique that has empowered these stunning developments is called deep learning, a term that refers to mathematical models known as artificial neural networks. Deep learning is a subfield of machine learning, a branch of computer science based on fitting complex models to data.",0.0
"
        From the Archives: Xerox Parc’s Engineers on How They Invented the Future
    ",https://spectrum.ieee.org/xerox-parc,2022-06-04,,"And how Xerox lost it An undated exterior view of the Xerox Palo Alto Research Center (PARC) is shown in Palo Alto, California. In late 1969, C. Peter McColough, chairman of Xerox Corp., told the New York Society of Security Analysts that Xerox was determined to develop “the architecture of information” to solve the problems that had been created by the “knowledge explosion.” Legend has it that McColough then turned to Jack E. Goldman, senior vice president of research and development, and said, “All right, go start a lab that will find out what I just meant.” This article was first published as “Inside the PARC: the ‘information architects’.” It appeared in the October 1985 issue of IEEE Spectrum. A PDF version is available on IEEE Xplore. The diagrams and photographs appeared in the original print version. Goldman tells it differently. In 1969 Xerox had just bought Scientific Data Systems (SDS), a mainframe computer manufacturer. “When Xerox bought SDS,” he recalled, “I walked promptly into the office of Peter McColough and said, ‘Look, now that we’re in this digital computer business, we better damned well have a research laboratory!’ ” In any case, the result was the Xerox Palo Alto Research Center (PARC) in California, one of the most unusual corporate research organizations of our time. PARC is one of three research centers within Xerox; the other two are in Webster, N.Y., and Toronto, Ont., Canada. It employs approximately 350 researchers, managers, and support staff (by comparison, Bell Laboratories before the AT&T breakup employed roughly 25,000). PARC, now in its 15th year, originated or nurtured technologies that led to these developments, among others: In the mid-1970s, close to half of the top 100 computer scientists in the world were working at PARC, and the laboratory boasted similar strength in other fields, including solid-state physics and optics. Some researchers say PARC was a product of the 1960s and that decade’s philosophy of power to the people, of improving the quality of life. When the center opened in 1970, it was unlike other major industrial research laboratories; its work wasn’t tied, even loosely, to its corporate parent’s current product lines. And unlike university research laboratories, PARC had one unifying vision: it would develop “the architecture of information.” IEEE Spectrum Research projects at the Xerox Palo Alto Research Center, represented on this family tree by heart-encircled carvings surrounded by the initials of some of their key developers, had their roots in many places, including projects funded by the Defense Advanced Research Projects Agency (the ARPAnet), the University of Utah (Flex), and the Massachusetts Institute of Technology (Sketchpad). The ideas developed at PARC found their way into a number of commercial products, companies, and publications, shown here as leafy branches. Some of these real-world offspring, like the Apple Lisa computer and the Hewlett-Packard LaserJet printer, took many of their root concepts from PARC; others, like the Ada computer language and the Intel 1103 dynamic RAM chip, are less closely related. As for the future impact of PARC research—the sky’s the limit. The originator of that phrase is unclear. McColough has credited his speechwriter. The speechwriter later said that neither he nor McColough had a specific definition of the phrase. So almost everyone who joined PARC in its formative years had a different idea of what the center’s charter was. This had its advantages. Since projects were not assigned from above, the researchers formed their own groups; support for a project depended on how many people its instigator could get to work on it. “The phrase was ‘Tom Sawyering,’ ” recalled James G. Mitchell, who joined PARC from the defunct Berkeley Computer Corp. in 1971 and is now vice president of research at the Acorn Research Centre in Palo Alto. “Someone would decide that a certain thing was really important to do. They would start working on it, give some structure to it, and then try to convince other people to come whitewash this fence with them.” When Goldman set up PARC, one of his first decisions was to ask George E. Pake, a longtime friend, to run it. Pake was executive vice chancellor, provost, and professor of physics at Washington University in St. Louis, Mo. One of the first decisions Pake in turn made was to hire, among others, Robert Taylor, then at the University of Utah, to help him recruit engineers and scientists for the Computer Science and Systems Science Laboratories. Taylor had been director of the information-processing techniques office at ARPA (the U.S. military’s Advanced Research Projects Agency), where he and others had funded the heyday of computer research in the mid- and late 1960s. PARC started with a small nucleus—perhaps fewer than 20 people. Nine came from the Berkeley Computer Corp., a small mainframe computer company that Taylor had tried to convince Xerox to buy as a way of starting up PARC. (Many of the people at BCC were responsible for the design of the SDS 940, the computer on the strength of which Xerox bought Scientific Data Systems in 1968.) The 20 PARC employees were housed in a small, rented building, “with rented chairs, rented desks, a telephone with four buttons on it, and no receptionist,” recalled David Thornburg, who joined PARC’s General Science Laboratory fresh out of graduate school in 1971. The group thought it should have a computer of its own. “It’s a little hard to do language research and compiler research without having a machine,” said Mitchell. The computer they wanted was a PDP-10 from Digital Equipment Corp. (DEC). “There was a rivalry in Datamation [magazine] advertisements between Xerox’s SDS and DEC,” recalled Alan Kay, who came to PARC as a researcher from Stanford University’s Artificial Intelligence Laboratory in late 1970. “When we wanted a PDP-10, Xerox envisioned a photographer lining up a shot of DEC boxes going into the PARC labs, so they said, ‘How about a Sigma 7?’ “We decided it would take three years to do a good operating system for a Sigma 7, while we could build an entire PDP-10 in just one year.” The result was MAXC (Multiple Access Xerox Computer), which emulated the PDP-10 but used semiconductor dynamic RAMs instead of core. So much care was lavished on MAXC’s hardware and software that it held the all-time record for continuous availability as a node on the ARPAnet. MAXC was crucial to a number of developments. The Intel Corp., which had made the 1,103 dynamic memory chips used in the MAXC design, reaped one of the first benefits. “Most of the 1,103 memory chips you bought from Intel at the time didn’t work,” recalled Kay. So PARC researcher Chuck Thacker built a chip-tester to screen chips for MAXC. A later version of that tester, based on an Alto personal computer, also developed at PARC, ended up being used by Intel itself on its production line. And MAXC gave PARC experience in building computers that would later stand the center in good stead. “There were three capabilities we needed that we could not get if we bought a PDP-10,” recalled an early PARC lab manager. “We needed to develop a vendor community—local people who would do design layouts, printed-circuit boards, and so forth—and the only way to get that is to drive it with a project. We also needed semiconductor memory, which PDP-10s did not have. And we thought we needed to learn more about microprogrammable machines, although it turned out we didn’t use those features.” Bit map A method for creating computer displays by assigning an individual memory location to each point on the screen. Distributed computing A technique whereby many computers are linked in a network and cooperate in performing tasks; individual computers and their users may send messages to each other, transfer information, and initiate actions at remote machines. Electronic mail An extension to computer file systems that allows users to send messages to each other and to be signaled when a message is received, but to read messages only at their convenience. Frame buffer A memory capable of storing all the information for a single video frame; the contents of a frame buffer can be controlled by special software to produce or modify images. Local area network A high-speed network that connects computers, printers, and other peripherals in an office or building. Object-oriented programming A programming technique in which individual objects exchange messages to carry out tasks; this contrasts with more typical programming styles, in which programs carry out transformations on passive data structures. MAXC set a pattern for PARC: building its own hardware. That committed its researchers to visions that must be turned into reality—at least on a small scale. “One of the blood oaths that was taken by the original founders was that we would never do a system that wasn’t engineered for 100 users,” said Kay. “That meant that if it was a time-sharing system, you had to run 100 people on it; if it was a programming language, 100 people had to program in it without having their hands constantly held. If it was a personal computer, you had to be able to build 100.” This policy of building working systems is not the only way of doing research; Mitchell recalled that it was a bone of contention at PARC. “Systems research requires building systems,” he said. “Otherwise you don’t know whether the ideas you have are any good, or how difficult they are to implement. But there are people who think that when you are building things you are not doing research.” Since MAXC, the center has built prototypes of dozens of hardware and software systems—prototypes that sometimes numbered in the thousands of units. Alan Kay, whose “Smalltalk” group created overlapping windows, is perhaps PARC’s most famous alumnus. After spending 10 years at PARC, he became chief scientist at Atari Inc. and is now a fellow at Apple Computer Inc. The first personal computer developed in the United States is commonly thought to be the MITS Altair, which sold as a hobbyist’s kit in 1976. At nearly the same time the Apple I became available, also in kit form. But by the end of that year there were also 200 Alto personal computers in daily use—the first of them having been built in 1973. While researchers in PARC’s Computer Science Laboratory were completing the MAXC and beginning to use it, their counterparts in the Systems Science Laboratory were putting together a distributed­ computer system using Nova 800 processors and a high-speed character generator. In September 1972, researchers Butler Lampson and Chuck Thacker of PARC’s Computer Science Laboratory went to Alan Kay in the Systems Science Laboratory and asked, “Do you have any money?” Kay told them that he had about US $250,000 earmarked for more Nova 800s and character-generation hardware. “How would you like us to build you a computer?” Lampson asked Kay. “I’d like it a lot,” Kay replied. And on Nov. 22, 1972, Thacker and Ed McCreight began building what was to become the Alto. A Xerox executive reportedly angered Thacker by insisting that it would take 18 months to develop a major hardware system. When Thacker argued that he could do it in three months, a bet was placed. It took a little longer than three months, but not much. On April 1, 1973, Thornburg recalled, “I walked into the basement where the prototype Alto was sitting, with its umbilical cord attached to a rack full of Novas, and saw Ed McCreight sitting back in a chair with the little words, ‘Alto lives’ in the upper left corner of the display screen.” Kay said the Alto turned out to be “a vector sum of what Lampson wanted, what Thacker wanted, and what I wanted. Lampson wanted a $500 PDP-10,” he recalled. “Thacker wanted a 10-times-faster Nova 800, and I wanted a machine that you could carry around and children could use.” The reason the Alto could be built so quickly was its simplicity. The processor, recalled Kay, “was hardly more than a clock”—only 160 chips in 1973’s primitive integrated circuit technology. The architecture goes back to the TX-2, built with 32 program counters at the Massachusetts Institute of Technology’s Lincoln Laboratories in the late 1950s. The Alto, which had 16 program counters, would fetch its next instruction from whichever counter had the highest priority at any given moment. Executing multiple tasks incurred no overhead. While the machine was painting the screen display, the dynamic memory was being refreshed every 2 milliseconds, the keyboard was being monitored, and information was being transferred to and from the disk. The task of lowest priority was running the user’s program. In 1973 every researcher at PARC wanted an Alto personal computer, but there weren’t enough to go around. To speed things up, researchers dropped into the Alto laboratory whenever they had a few free moments to help with computer assembly. The prototype was a success, and more Altos were built. Research on user interfaces, computer languages, and graphics began in earnest. Lampson, Thacker, and other instigators of the project got the first models. Many PARC researchers pitched in to speed up the production schedules, but there never seemed to be enough Altos. “There was a lab where the Altos were getting built, with circuit boards lying around, and anyone could go in and work on them,” recalled Daniel H.H. Ingalls, now a principal engineer at Apple Computer Inc., Cupertino, Calif. Ron Rider, who is still with Xerox, “had an Alto when Altos were impossible to get,” recalled Bert Sutherland, who joined PARC in 1975 as manager of the Systems Science Laboratory. “When I asked him how he got one, he told me that he went around to the various laboratories, collected parts that people owed him, and put it together himself.” By today’s standards the Alto was not a particularly powerful computer. But if several Altos are linked, along with file servers and printers, the result looks suspiciously like the office of the future. The idea of a local computer network had been discussed before PARC was founded—in 1966, at Stanford University. Larry Tesler, now manager of object-oriented systems at Apple, who had graduated from Stanford, was still hanging around the campus when the university was considering buying an IBM 360 time-sharing system. “One of the guys and I proposed that instead they buy 100 PDP-1s and link them together in a network,” Tesler said. “Some of the advisors thought that was a great idea; a consultant from Yale, Alan Perlis, told them that was what they ought to do, but the IBM-oriented people at Stanford thought it would be safer to buy the time-sharing system. They missed the opportunity to invent local networking.” So PARC ended up with another first. At the same time that the Alto was being built, Thacker conceived of the Ethernet, a coaxial cable that would link machines in the simplest possible fashion. It was based in part on the Alohanet, a packet radio network developed at the University of Hawaii in the late 1960s. “Thacker made the remark that coaxial cable is nothing but captive ether,” said Kay. “So that part of it was already set before Robert Metcalfe and David Boggs came on board—that it would be packet-switching and that it would be a collision-type network. But then Metcalfe and Boggs sweated for a year to figure out how to do the damn thing.” (Metcalfe later founded 3Com Corp., in Mountain View, Calif.; Boggs is now with DEC Western Research, in Los Altos, Calif. The two of them hold the basic patents on the Ethernet.) “I’ve always thought the fact that [David] Boggs was a ham radio operator was important.... [He] knew that you could communicate reliably through an unreliable medium. I’ve often wondered what would have happened if he hadn’t had that background.”—Bert Sutherland “I’ve always thought the fact that Boggs was a ham radio operator was important,” Sutherland said. “It had a great impact on the way the Ethernet was designed, because the Ethernet fundamentally doesn’t work reliably. It’s like citizens’ band radio, or any of the other kinds of radio communication, which are fundamentally not reliable in the way that we think of the telephone. Because you know it basically doesn’t work, you do all the defensive programming—the ‘say again, you were garbled’ protocols that were worked out for radio communication. And that makes the resulting network function extremely reliably.” “Boggs was a ham and knew that you could communicate reliably through an unreliable medium. I’ve often wondered what would have happened if he hadn’t had that background,” Sutherland added. Once the Ethernet was built, using it was fairly simple: A computer that wanted to send a message would wait and see whether the cable was clear. If it was, the machine would send the information in a packet prefaced with the address of its recipient. If two messages collided, the machines that sent them would each wait for a random interval before trying again. One innovative use for the network had nothing to do with people sending messages to one another; it involved communication solely between machines. Because the dynamic memory chips were so unreliable in those days, the Alto also ran a memory check when it wasn’t doing anything else. Its response to finding a bad chip was remarkable: “It would send a message telling which Alto was bad, which slot had the bad board, and which row and column had the bad chips,” Thornburg said. “The reason I found out about this was that one day the repairman showed up and said, ‘Any time you’re ready to power down, I need to fix your Alto,’ and I didn’t even know anything was wrong.” While the Ethernet was being developed, so was another crucial element in the office of the future: the laser printer. After all, what use was a screen that could show documents in multiple type styles and a network that could transmit them from place to place without some means of printing them efficiently? The idea for the laser printer came to PARC from Xerox’s Webster, N.Y., research laboratory—along with its proponent, Gary Starkweather. He had the idea of using a laser to paint information, in digital form, onto the drum or belt of a copying machine, then-research vice president Goldman recalled. Starkweather reported to the vice president of the Business Products Group for Advanced Development, George White. “George White came to me,” said Goldman, “and said, ‘Look, Jack, I got a terrific guy named Gary Starkweather doing some exciting things on translating visual information to print by a laser, using a Xerox machine, of course. What an ideal concept that would be for Xerox. But I don’t think he’s going to thrive in Rochester; nobody’s going to listen to him, they’re not going to do anything that far advanced. Why don’t you take him out to your new lab in Palo Alto?’ ” Newly appointed PARC manager Pake jumped at the opportunity. Starkweather and a few other researchers from Rochester were transferred to Palo Alto and started PARC’s Optical Science Laboratory. The first laser printer, EARS (Ethernet-Alto-Research character generator-Scanning laser output terminal), built by Starkweather and Ron Rider, began printing documents that were generated by Altos and sent to it via Ethernet in 1973. EARS wasn’t perfect, Thornburg said. It had a dynamic character generator that would create new patterns for characters and graphics as they came in. If a page had no uppercase Qs in it, the character generator would economize on internal memory by not generating a pattern for a capital “Q.” But if a page contained a very complex picture, the character generator would run out of space for patterns; “there were certain levels of complexity in drawings that couldn’t be printed,” Thornburg recalled. Even with these drawbacks, the laser printer was still an enormous advance over the line printers, teletypes, and facsimile printers that were available at the time, and Goldman pushed to have it commercialized as quickly as possible. But Xerox resisted. In fact, a sore point throughout PARC’s history has been the parent organization’s seeming inability to exploit the developments that researchers made. In 1972, when Starkweather built his first prototype, the Lawrence Livermore National Laboratory, in an effort to spur the technology, put out a request for bids for five laser printers. But Goldman was unable to convince the executive to whom Xerox’s Electro-Optical Systems division reported (whose background was accounting and finance) to allow a bid. The reason: Xerox might have lost $150 000 over the life of the contract if the laser printers needed repair as often as the copiers on which they were based, even though initial evidence showed that printing caused far less wear and tear than copying. In 1974 the laser printer first became available outside PARC when a small group of PARC researchers under John Ellenby—who built the Alto II, a production-line version of the Alto, and who is now vice president of development at Grid Systems Corp., Mountain View, Calif.—began buying used copiers from Xerox’s copier division and installing laser heads in them. The resulting printers, known as Dovers, were distributed within Xerox and to universities. Sutherland estimated that several dozen were built. “They stripped out all the optics and turned them back to the copier division for credit,” he recalled. Even today, he said, he receives laser-printed documents from universities in which he can recognize the Dover typefaces. Also in 1974, the Product Review Committee at Xerox head­quarters in Rochester, N.Y., was finally coming to a decision about what kind of computer printer the company should manufacture. “A bunch of horse’s asses who don’t know anything about technology were making the decision, and it looked to me, sitting a week before the election, that it was going toward CRT technology,” said Goldman. (Another group at Xerox had developed a printing system whereby text displayed on a special cathode ray tube would be focused on a copier drum and printed.) “It was Monday night. I commandeered a plane,” Goldman recalled. “I took the planning vice president and the marketing vice president by the ear, and I said, ‘You two guys are coming with me. Clear your Tuesday calendars. You are coming with me to PARC tonight. We’ll be back for the 8:30 meeting on Wednesday morning.’ We left around 7:00 p.m., got to California at 1:00, which is only 10:00 their time, and the guys at PARC, bless their souls, did a beautiful presentation showing what the laser printer could do.” “If you’re dealing with marketing or planning people, make them kick the tires. All the charts and all the slides aren’t worth a damn,” Goldman said. From a purely economic standpoint, Xerox’s investment in PARC for its first decade was returned with interest by the profits from the laser printer. The committee opted to go with laser technology, but there were delays. “They wouldn’t let us get them out on 7000s,” Goldman said, referring to the old-model printer that Ellenby’s group had used as a base. “Instead they insisted on going with new 9000 Series, which didn’t come out until 1977.” From a purely economic standpoint, Xerox’s investment in PARC for its first decade was returned with interest by the profits from the laser printer. This is perhaps ironic, since one vision of the office of the future was that it would be paperless. “I think PARC has generated more paper than any other office by far, because at the press of a button you can print 30 copies of any report,” observed Douglas Fairbairn, a former PARC technician and now vice president for user-designed technology at VLSI Technology Inc. “If the report is 30 pages long, that’s 1000 pages, but it still takes only a few minutes. Then you say, ‘I guess I wanted that picture on the other page.’ That’s another 1000 pages.” By the mid-1970s the Altos in the offices of most PARC re­searchers had been customized to their tastes. Richard Shoup’s Alto had a color display. Taylor’s Alto had a speaker—which played “The Eyes of Texas Are Upon You” whenever he received an electronic mail message. And, as many people have found in the 10 years since the Alto became widespread at PARC, personal computers can be used for enjoyment as well as work. The PARC researchers were among the first to discover this. “At night, whenever I was in Palo Alto,” Goldman said, “I’d go over to the laboratory and watch Alan Kay invent a game. This was long before electronic games, and these kids were inventing these things all the time until midnight, 1:00 a.m.” “Xerox had the first electronic raffle nationwide. At Xerox, I received my first electronic junk mailing, first electronic job acceptance, and first electronic obituary.”—Bert Sutherland “l enjoyed observing a number of firsts,” Sutherland said. “Xerox had the first electronic raffle nationwide. At Xerox, I received my first electronic junk mailing, first electronic job acceptance, and first electronic obituary.” When the Xerox 914 copiers came out in the early 1960s, “I was a copy freak,” said Lynn Conway who joined PARC from Memorex Corp. in 1973 and is now associate dean and professor of electrical engineering and computer science at the University of Michigan in Ann Arbor. “I liked to make things and give them out, like maps—all kinds of things. And in the Xerox environment in ’76, all of a sudden you could create things and make lots of them.” Dozens of clubs and interest groups were started that met on the network. Whatever a PARC employee’s hobby or interest, he or she could find someone with whom to share that interest electronically. Much serious work got done electronically as well: reports, articles, sometimes entire design projects were done through the network. One side effect of all this electronic communication was a disregard for appearances and other external trappings of status. “People at PARC have a tendency to have very strong personalities, and sometimes in design sessions those personalities came over a little more strongly than the technical content,” said John Warnock, who joined PARC in 1978 from the Evans & Sutherland Corp., where he worked on high-speed graphics systems. Working via electronic mail eliminated the personality problems during design sessions. Electronic interaction was particularly useful for soft­ware researchers, who could send code back and forth. Warnock, who is now president of Adobe Systems Inc., Palo Alto, Calif., described the design of lnterpress, a printing protocol: “One of the designers was in Pittsburgh, one of them was in Philadelphia, there were three of us in this area, and a couple in El Segundo [Calif.]. The design was done almost completely over the mail system, remotely; there were only two occasions when we all got together in the same room.” Electronic mail was also invaluable for keeping track of group projects. “One of the abilities that is really useful is to save a sequence of messages on a particular subject so that you can refer to it,” said Warren Teitelman, who joined PARC in 1972 from BBN Inc. and is currently manager of programming environments at Sun Microsystems in Mountain View. “Or if somebody comes into a discussion late and they don’t have the context, you can bring them up to date by sending them all the messages,” Teitelman added. But electronic mail sometimes got out of hand at PARC. Once, after Teitelman had been out of touch for a week, he logged onto the system and found 600 messages in his mailbox. Antialiasing—removing jagged edges from diagonal lines and curves—is a standard technique in computer graphics today. These pictures, produced by Superpaint at PARC in 1972, were among the first demonstrations of antialiasing. As anyone who has sat through a business meeting knows, the office of today includes graphics as well as text. In 1970, Shoup, who is now chairman of Aurora Systems Inc., started working at PARC on new ways to create and manipulate images digitally in the office of the future. His research started the field of television graphics and won Emmy awards for both him and Xerox. “It quickly became clear that if we wanted to do a raster scan system, we ought to do it compatible with television standards so that we could easily obtain monitors and cameras and videotape recorders,” Shoup recalled. In early 1972 he built some simple hardware to generate antialiased lines, and by early 1973 the system, called Superpaint, was completed. It was the first complete paint system with an 8-bit frame buffer anywhere, recalled Alvy Ray Smith, who worked with Superpaint at PARC and is soon to be vice president and chief technical officer of Pixar Inc., San Rafael, Calif.; it was also the first system to use several graphics aids: color lookup tables for simple animation, a digitizing tablet for input, a palette for mixing colors directly on the screen. The system also had a real-time video scanner so images of real objects could be digitized and then manipulated. “The very first thing I did on the system was some antialiased lines and circles,” Shoup said, “because I’d written a paper on that subject and hadn’t finished the examples. But when I submitted the paper and had it accepted, the machine that was going to be used to do the examples wasn’t built yet.” By mid-1974, Superpaint had been augmented by additional software that allowed it to perform all kinds of tricks, and Smith, who had just completed doctoral work in a branch of mathematics known as cellular automata theory, was hired to help put the machine through its paces. He used Superpaint to make a videotape called “Vidbits” that was later shown at the Museum of Modern Art in New York City. Six months later his initial contract with PARC expired and was not renewed. While disappointed, Smith was not surprised, as he had found that not everyone there shared his enthusiasm for painting with a computer. “The color graphics lab was a long narrow room with seven doors into it,” he recalled. “You had to go through it to get to a lot of other places. Most people, when they walked through, would look at the screen and stop—even the most trite stuff had never been seen before. Cycling color maps had never been seen before. But there were some people who would go through and wouldn’t stop. I couldn’t figure out how people could walk through that room and never stop and look.” A reason aside from others’ indifference to video graphics may have contributed to Smith’s departure. One of the first times Superpaint was seen by a wide audience was in a public television show, “Supervisions,” produced by station KCET in Los Angeles. “It was just used a couple of times for little color cycling effects,” Shoup recalled. But Xerox was not amused by the unauthorized use of the system in a program. “Bob Taylor sat with Alvy [Smith] one entire afternoon while Alvy pushed the erase button on the videotape recorder, eliminating the Xerox logo from every copy of that tape,” Shoup continued. (This was one of the tapes viewed by the committee that awarded Xerox its Emmy.) It was the first system to use...color lookup tables for simple animation, a digitizing tablet for input, [and] a palette for mixing colors directly on the screen. Shoup stayed at PARC, supported by Kay’s research group, while Smith moved on, armed with a National Education Association grant to do computer art. He found support for his work at the New York Institute of Technology, where he helped develop Paint, which became the basis of Ampex Video Art (AVA), and N.Y. Tech’s Images, two graphics systems still in use today. While Shoup was alone in pursuing Superpaint at PARC, Smith wasn’t the only Superpaint addict wandering the country in search of a frame buffer. David Miller, now known as David Em, and David Difrancesco were the first artists to paint with pixels. When Em lost access to Superpaint, he set out on a year-long quest for a frame buffer that finally brought him to the Jet Propulsion Laboratory in Pasadena, Calif. Finally, in 1979, Shoup left PARC to start his own company to manufacture and market a paint system, the Aurora 100. He ac­knowledges that he made no technological leaps in designing the Aurora, which is simply a commercialized second-generation version of his first-generation system at PARC. “The machine we’re building at Aurora for our next generation is directly related to things we were thinking about seven or eight years ago at PARC,” Shoup said. The Aurora 100 is now used by corporations to develop in­ house training films and presentation graphics. Today, tens of thousands of artists are painting with pixels. The 1985 Siggraph art show in San Francisco alone received 4000 entries. Most people who know that a mouse is a computer peripheral think it was invented by Apple. The cognoscenti will correct them by saying that it was developed at Xerox PARC. But the mouse in fact preceded PARC. “I saw a demonstration of a mouse being used as a pointing device in 1966,” Tesler recalled. “Doug Engelbart [of SRI International Inc. in Menlo Park, Calif.] invented it.” At PARC, Tesler set out to prove that the mouse was a bad idea. “I really didn’t believe in it,” he said. “I thought cursor keys were much better. “We literally took people off the streets who had never seen a computer. In three or four minutes they were happily editing away, using the cursor keys. At that point I was going to show them the mouse and prove they could select text faster than with the cursor keys. Then I was going to show that they didn’t like it. “It backfired. I would have them spend an hour working with the cursor keys, which got them really used to the keys. Then I would teach them about the mouse. They would say, ‘That’s interesting but I don’t think I need it.’ Then they would play with it a bit, and after two minutes they never touched the cursor keys again.” “While I didn’t mind using a mouse for text manipulation, I thought it was totally inappropriate for drawing. People stopped drawing with rocks in Paleolithic times.”—David Thornburg After Tesler’s experiment, most PARC researchers accepted the mouse as a proper peripheral for the Alto. One holdout was Thornburg. “I didn’t like the mouse,” he said. “It was the least reliable component of the Alto. I remember going into the repair room at PARC-where there was a shoebox to hold good mice and a 5O-gallon drum for bad mice. And it was expensive—too expensive for the mass market. “While I didn’t mind using a mouse for text manipulation, I thought it was totally inappropriate for drawing. People stopped drawing with rocks in Paleolithic times, and there’s a reason for that: rocks aren’t appropriate drawing implements; people moved on to sticks.” Thornburg, a metallurgist who had been doing materials re­search at PARC, began work on alternative pointing devices. He came up with a touch tablet in 1977 and attached it to an Alto. Most people who looked at it said, “That’s nice, but it’s not a mouse,” Thornburg recalls. His touch tablet did eventually find its way into a product: the Koalapad, a home-computer peripheral costing less than $100. “It was clear that Xerox didn’t want to do anything with it,” Thornburg said. “They didn’t even file for patent protection, so I told them that I’d like to have it. After a lot of horsing around, they said OK.” Thornburg left Xerox in 1981, worked at Atari for a while, then started a company—now Koala Technologies Inc.—with another ex-PARC employee to manufacture and market the Koalapad. Meanwhile, though Tesler accepted the need for a mouse as a pointing device, he wasn’t satisfied with the way SRI’s mouse worked. “You had a five-key keyset for your left hand and a mouse with three buttons for your right hand. You would hit one or two keys with the left hand, then point at something with the mouse with the right hand, and then you had more buttons on the mouse for confirming your commands. It took six to eight keystrokes to do a command, but you could have both hands going at once. Experts could go very fast.” The SRI system was heavily moded. In a system with modes, the user first indicates what he wants to do—delete, for example. This puts the system in the delete mode. The computer then waits for the user to indicate what he wants deleted. If the user changes his mind and tries to do something else, he can’t unless he first cancels the delete command. In a modeless system, the user first points to the part of the dis­play he wants to change, then indicates what should be done to it. He can point at things all day, constantly changing his mind, and never have to follow up with a command. To make things even more complicated for the average user (but more efficient for programmers), the meaning of each key varied, depending on the mode the system was in. For example, “J” meant scroll and “I” meant insert. If the user tried to “insert,” then to “scroll” without canceling the first command, he would end up inserting the letter “J” in the text. Larry Tesler set out to test the interface on a nonprogrammer.... Apparently nobody had done that before. Most programmers at PARC liked the SRI system and began adapting it in their projects. “There was a lot of religion around that this was the perfect user interface,” said Tesler. “Anytime anybody would suggest changing it, they were greeted with glares.” Being programmers, they had no trouble with the fact that the keypad responded to combinations of keys pressed simultaneously that represented the alphabet in binary notation. Tesler set out to test the interface on a nonprogrammer. He taught a newly hired secretary how to work the machine and observed her learning process. “Apparently nobody had done that before,” he said. “She had a lot of trouble with the mouse and the keyset.” Tesler argued for a simpler user interface. “Just about the only person who agreed with me was Alan Kay,” he said. Kay supported Tesler’s attempt to write a modeless text editor on the Alto. Although most popular computers today use modeless soft­ware, with the Macintosh being probably the best example, Tesler’s experiments didn’t settle the issue. “MacWrite, Microsoft Word, and the Xerox Star all started out as projects that were heavily moded,” Tesler said, “because programmers couldn’t believe that a user interface could be flexible and useful and extensible unless it had a lot of modes. The proof that this wasn’t so didn’t come by persuasion, it came through customers complaining that they liked a dinky modeless editor with no features better than the one that had all the features they couldn’t figure out how to use.” The same kinds of simplification that made for the modeless editor were also applied to programming languages and environments at PARC. Seeking a language that children could use, Kay could regularly be seen testing his work with kindergarten and elementary-school pupils. What Kay aimed for was the Dynabook: a simple, portable personal computer that would cater to a person’s information needs and provide an outlet for creativity-writing, drawing, and music composition. Smalltalk was to be the language of the Dynabook. It was based on the concepts of classes pioneered in the programming language Simula, and on the idea of interacting objects communicating by means of messages requesting actions, rather than by programs performing operations directly on data. The first version of Smalltalk was written as the result of a chance conversation between Kay, Ingalls, and Ted Kaehler, another PARC researcher. Ingalls and Kaehler were thinking about writing a language, and Kay said, “You can do one on just one page.” What Kay aimed for was the Dynabook: a simple, portable personal computer. He explained, “If you look at a Lisp interpreter written in itself, the kernel of these things is incredibly small. Smalltalk could be even smaller than Lisp.” The problem with this approach, Kay recalled, is that “Smalltalk is doubly recursive: you’re in the function before you ever do anything with the arguments.” In Smalltalk-72, the first version of the language, control was passed to the object as soon as possible. Thus writing a concise definition of Smalltalk-in Small­ talk-was very difficult. “It took about two weeks to write 10 lines of code,” Kay said, “and it was very hard to see whether those 10 lines of code would work.” Kay spent the two weeks thinking from 4:00 to 8:00 a.m. each day and then discussing his ideas with Ingalls. When Kay was done, Ingalls coded the first Smalltalk in Basic on the Nova 800, because that was the only language available at the time with decent debugging facilities. “Smalltalk was of a scale that you could go out and have a pitcher of beer or two and come back, and then two people would egg each other on and do an entire system in an afternoon.”—Alan Kay Because the language was so small and simple, developing programs and even entire systems was also quite fast. “Smalltalk was of a scale that you could go out and have a pitcher of beer or two and come back, and then two people would egg each other on and do an entire system in an afternoon,” Kay said. From one of those afternoon sessions came overlapping windows. The concept of windows had originated in Sketchpad, an interactive graphics program developed by Ivan Sutherland at MIT in the early 1960s; the Evans & Sutherland Corp. had implemented multiple windows on a graphics machine in the mid-1960s. But the first multiple overlapping windows were implemented on the Alto by PARC’s Diana Merry in 1973. “All of us thought that the Alto display was incredibly small,” said Kay, “and it’s clear that you’ve got to have overlapping windows if you don’t have a large display.” After windows came the concept of Bitblt—block transfers of data from one portion of memory to another, with no restrictions about alignment on word boundaries. Thacker, the main designer of the Alto computer, had implemented a function called CharacterOp to write characters to the Alto’s bit-mapped screen, and Ingalls extended that work to make a general graphic utility. Bitblt made overlapping windows much simpler, and it also made possible all kinds of graphics and animation tricks. “I gave a demo in early 1975 to all of PARC of the Smalltalk system using Bitblt for menus and overlapping windows and things,” Ingalls recalled. “A bunch of people came to me after­wards, saying ‘How do you do all these things? Can I get the code for Bitblt?’ and within two months those things were being used throughout PARC.” Flashy and impressive as it was, Smalltalk-72 “was a dead end,” Tesler said. “It was ambiguous. You could read a piece of code and not be able to tell which were the nouns and which were the verbs. You couldn’t make it fast, and it couldn’t be compiled.” The first compiled version of Smalltalk, written in 1976, marked the end of the emphasis on a language that children could use. The language was now “a mature programming environment,” Ingalls said. “We got interested in exporting it and making it widely available.” “It’s terrible that Smalltalk-80 can’t be used by children, since that’s who Smalltalk was intended for. It fell back into data-structure-type programming instead of simulation-type programming.”—Alan Kay The next major revision of Smalltalk was Smalltalk-80. Kay was no longer on the scene to argue that any language should be simple enough for a child to use. Smalltalk-80, says Tesler, went too far in the opposite direction from the earliest versions of Smalltalk: “It went to such an extreme to make it compilable, uniform, and readable, that it actually became hard to read, and you definitely wouldn’t want to teach it to children.” Kay, looking at Smalltalk-80, said, “It’s terrible that it can’t be used by children, since that’s who Smalltalk was intended for. It fell back into data-structure-type programming instead of simulation-type programming.” While Kay’s group was developing a language for children of all ages, a group of artificial-intelligence researchers within PARC were improving Lisp. Lisp was brought to PARC by Warren Teitelman and Daniel G. Bobrow from Bolt, Beranek, and Newman in Cambridge, Mass., where it was being developed as a service to the ARPA community. At PARC, it was renamed Interlisp, a window system called VLISP was added, and a powerful set of programmers’ tools was developed. In PARC’s Computer Science Laboratory, researchers were developing a powerful language for systems programming. After going through several iterations, the language emerged as Mesa—a modular language, which allowed several programmers to work on a large project at the same time. The key to this is the concept of an interface—what a module in a program does, rather than how it does it. Each programmer knows what the other modules are chartered to do and can call on them to perform their particular functions. Another dominant feature was Mesa’s strong type-checking, which prevented programmers from using integer variables where they needed real numbers, or real numbers where they needed character strings—and prevented bugs from spreading from one module of a program to another. These concepts have since been widely adopted as the basis of modular programming languages. “A lot of the ideas in Ada [the standard programming language of the U.S. Department of Defense] and Modula-2 came out of the programming language research done at PARC,” said Chuck Geschke, now executive vice president of Adobe Systems Inc. Modula-2, in fact, was written by computer scientist Niklaus Wirth after he spent a sabbatical at PARC. While PARC may have had more than its share of successes, like any organization it couldn’t escape some failures. The one most frequently cited by former PARC researchers is Polos. Polos was an alternate approach to distributed computing. While Thacker and McCreight were designing the Alto, another group at PARC was working with a cluster of 12 Data General Novas, attempting to distribute functions among the machines so that one machine would handle editing, one would handle input and output, another would handle filing. “With Altos,” Sutherland said, “everything each person needed was put in each machine on a small scale. Polos was an attempt to slice the pie in a different way-to split up offices functionally.” By the time Polos was working, the Alto computers were proliferating throughout PARC, so Polos was shut down. But it had an afterlife: Sutherland distributed the 12 Novas among other Xerox divisions, where they served as the first remote gateways onto PARC’s Alto network, and the Polos displays were used as terminals within PARC until they were junked in 1977. Another major PARC project that failed was a combination optical character reader and facsimile machine. The idea was to develop a system that could take printed pages of mixed text and graphics, recognize the text as such and transmit the characters in their ASCII code, then send the rest of the material using the less-efficient facsimile coding method. “It was fabulously complicated and fairly crazy,” said Charles Simonyi, now manager of application development at Microsoft Corp. “On this project they had this incredible piece of hardware that was the equivalent of a 10,000-line Fortran program.” Un­fortunately, the equivalent of tens of thousands of lines of Fortran in those days meant tens of thousands of individual integrated circuits. “While we made substantial progress at the algorithmic and architecture level,” said Conway, who worked on the OCR project, “it became clear that with the circuit technology at that time it wouldn’t be anywhere near an economically viable thing.” The project was dropped in 1975. Essentially, the PARC researchers worked in an ivory tower for the first five years; while projects were in their infancy, there was little time for much else. But by 1976, with an Alto on every desk and electronic mail a way of life at the center, re­ searchers yearned to see their creations used by friends and neighbors. At that point, Kay recalled, about 200 Altos were in use at PARC and other Xerox divisions; PARC proposed that Xerox market a mass-production version of the Alto: the Alto III. “On Aug. 18, 1976, Xerox turned down the Alto III,” Kay said. So the researchers, rather than turning their project over to a manufacturing division, continued working with the Alto. “That was the reason for our downfall,” said Kay. “We didn’t get rid of the Altos. Xerox management had been told early on that Altos at PARC were like Kleenex; they would be used up in three years and we would need a new set of things 10 times faster. But when this fateful period came along, there was no capital. “We had a meeting at Pajaro Dunes [Calif.] called ‘Let’s burn our disk packs.’ We could sense the second derivative of progress going negative for us,” Kay related. “I really should have gone and grenaded everybody’s disks.” Instead of starting entirely new research thrusts, the PARC employees focused on getting the fruits of their past research projects out the door as products. Every few years the Xerox Corp. has a meeting of all its managers from divisions around the world to discuss where the company may be going. At the 1977 meeting, held in Boca Raton, Fla., the big event was a demonstration by PARC researchers of the systems they had built. The PARC workers assigned to the Boca Raton presentation put their hearts, souls, and many Xerox dollars into the effort. Sets were designed and built, rehearsals were held on a Holly­ wood sound stage, and Altos and Dovers were shipped between Hollywood and Palo Alto with abandon. It took an entire day to set up the exhibit in an auditorium in Boca Raton, and a special air-conditioning truck had to be rented from the local airport to keep the machines cool. But for much of the Xerox corporate staff, this was the first encounter with the “eggheads” from PARC. “PARC was a very strange place to the rest of the company... It was thought of as weird computer people who had beards, who didn’t bathe or wear shoes, who spent long hours deep into the night staring at their terminals...and who basically were antisocial egg­heads. Frankly, some of us fed that impression.”—Richard Shoup “PARC was a very strange place to the rest of the company,” Shoup said. “It was not only California, but it was nerds. It was thought of as weird computer people who had beards, who didn’t bathe or wear shoes, who spent long hours deep into the night staring at their terminals, who had no relationships with any other human beings, and who basically were antisocial egg­heads. Frankly, some of us fed that impression, as if we were above the rest of the company.” There was some difficulty in getting the rest of Xerox to take PARC researchers and their work seriously. “The presentation went over very well, and the battle was won, but the patient died,” Goldman said. Not only had Xerox executives seen the Alto, the Ethernet, and the laser printer, they had even been shown a Japanese-language word processor. “But the company couldn’t bring them to market!” Goldman said. (By 1983, the company did market a Japanese version of its Star computer.) One reason that Xerox had such trouble bringing PARC’s advances to market was that, until 1976, there was no development organization to take research prototypes from PARC and turn them into products. “At the beginning, the way in which the technology would be transferred was not explicit,” Teitelman said. “We took something of a detached view and assumed that someone was going to pick it up. It wasn’t until later on that this issue got really focused.” The Notetaker, a portable personal computer built at PARC in 1978, is rumored to have been the inspiration for the Osborne I. Even with a development organization, it was an uphill battle to get Xerox executives to accept a product. One example was the Notetaker computer, conceived by Adele Goldberg, a researcher in the Smalltalk group who is currently president of the Associa­tion for Computing Machinery and who is still at PARC. “Poor Adele,” Tesler said. “The rest of us got involved and kept redefining the project.” The Notetaker ended up as an 8086-based computer that could fit under an airplane seat. It was battery-powered, ran Smalltalk, and had a touch-sensitive screen designed by Thornburg. “We had a custom monitor, we had error-corrected memory, a lot of custom engineering that we would normally only do for a real product,” said Fairbairn, the Notetaker’s chief hardware designer. “The last year before I left PARC,” Tesler said, “I spent flying around the country talking to Xerox executives, carrying Note­taker with me. It was the first portable computer run in an air­port. Xerox executives made all sorts of promises: we’ll buy 20,000, just talk to this executive in Virginia, then talk to this executive in Connecticut. The company was so spread out, they never got the meeting together. After a year I was ready to give up.” While Xerox may not have been ready to run with a portable computer, others were. The Osborne I was introduced in 1981, about nine months after Adam Osborne reportedly toured PARC, where pictures of the Notetaker were prominently displayed. While some of PARC’s pioneers were getting restless by the mid-1970s, others were just beginning to find uses for the marvelous tools of the office of the future. One was Lynn Conway, who used the Alto, networks, and laser printers to develop a new method of designing integrated circuits and disseminate the method to hundreds of engineers at several dozen institutions around the country. When Bert Sutherland came in as manager of the Systems Science Laboratory in 1975, he brought Carver Mead, a professor at the California Institute of Technology in Pasadena, to PARC “to wander in and create some havoc.” Mead was an expert in semi­conductor design who had invented the MESFET in the late 1960s. Sutherland had worked on the application of computer graphics to integrated-circuit layout, Conway recalled, so it was natural for him to think about applying an advanced personal computer like the Alto to the problem of IC design. Conway herself was drawn to integrated-circuit design by the frustration of the OCR-Fax project, in which she had conceived an elegant architecture that could only be realized as racks and racks of equipment. But those racks might become a few chips if only they could be designed by someone who knew what they should do and how they should fit together. “Carver Mead came up and gave a one-week course at PARC on integrated-circuit design,” Fairbairn recalled. “Lynn Conway and I were the ones that really got excited about it and really wanted to do something.” “Then a whole bunch of things really clicked,” said Conway. “While Carver and I were cross-educating each other on what was going on in computing and in devices, he was able to explain some of the basic MOS design methods that had been evolving within Intel. And we began to see ways to generalize the struc­tures that [those designers] had generated.” Instead of working only on computer tools for design, Conway explained, she and Mead worked to make the design methods simpler and to build tools for the refined methods. “Between mid-’75 and mid-’77, things went from a fragmentary little thing—one of a number of projects Bert wanted to get going—to the point where we had it all in hand, with examples, and it was time to write.” In a little less than two years, Carver Mead and Lynn Conway had developed the concepts of scalable design rules, repetitive structures, and the rest of what is now known as structured VLSI design In a little less than two years, Mead and Conway had developed the concepts of scalable design rules, repetitive structures, and the rest of what is now known as structured VLSI design—to the point where they could teach it in a single semester. Today structured VLSI design is taught at more than 100 universities, and thousands of different chips have been built with it. But in the summer of 1977, the Mead-Conway technique was untested—in fact belittled. How could they get it accepted? “The amazing thing about the PARC environment in 1976-77 was the feeling of power; all of a sudden you could create things and make lots of them. Not just one sheet, but whole books,” said Conway. And that is exactly what she and her cohorts did. “We just self-published the thing [Introduction to VLSI Systems],” said Conway, “and put it in a form that if you didn’t look twice, you might think this was a completely sound, proven thing.” It looked like a book, and Addison-Wesley agreed to publish it as a book. Conway insisted it couldn’t have happened without the Altos. “Knowledge would have gotten out in bits and pieces, always muddied and clouded-we couldn’t have generated such a pure form and generated it so quickly.” The one tool Conway used most in the final stages of the VLSI project was networks: not only the Ethernet within PARC, but the ARPAnet that connected PARC to dozens of research sites across the country. “The one thing I am clear of in retrospect,” said Conway, “is the sense of having powerful invisible weapons that people couldn’t understand we had. The environment at PARC gave us the power to outfox and outmaneuver people who would think we were crazy or try to stop us; otherwise we would never have had the nerve to go out with it the way we did.” In 1979, three years after Alan Kay had wanted to throw away the Altos “like Kleenex,” the Dorado, a machine 10 times more powerful, finally saw the light of day. “It was supposed to be built by one of the development organizations because they were going to use it in some of their products,” recalled Severo Ornstein, one of the designers of the Dorado and now chairman of Computer Professionals for Social Responsibility in Palo Alto. “But they decided not to do that, so if our lab was going to have it, we were going to have to build it ourselves. We went through a long agonizing period in which none of us who were going to have to do the work really wanted to do it.” “Taylor was running the lab by that time,” Ornstein said. “The whole thing was handled extremely dexterously. He never twisted anyone’s arm really directly; he presided over it and kept order in the process, but he really allowed the lab to figure out that that was what it had to do. It was really a good thing, too, because it was very hard to bring the Dorado to life. A lot of blood was shed.” At first, Ornstein recalled, the designers made a false start by using a new circuit-board technology—so-called multiwire technology, in which individual wires are bonded to a board to make connections. But the Dorado boards were too complex for multiwire technology. When the first Dorado ran, there was a question in many people’s minds whether there would ever be a second. “There Butler Lampson’s faith was important,” Ornstein said. “He was the only one who believed that it could be produced in quantity. In fact, even after the Dorado was redesigned using printed-circuit boards instead of multiwire and Dorados began to be built in quantity, they were still rare. “We never had enough budget to populate the whole community with Dorados,” recalled one former PARC manager. “They dribbled out each year, so that in 1984 still not everybody had a Dorado.” Those who did were envied. “I had a Dorado of my very own,” said John Warnock. “Chuck Geschke was a manager; he didn’t get one.” “In the early days...I got to take my Alto home. But the evolution of machines at Xerox went in the opposite direction from making it easy to take the stuff home.”—Dan Ingalls “I got a crusty old Alto and a sheet of paper,” Geschke said. The advent of the Dorado allowed researchers whose projects were too big for the Alto to make use of bit-mapped displays and all the other advantages of personal computers. “We had tried to put Lisp on the Alto, and it was a disaster,” recalled Teitelman. “When we got the Dorado, we spent eight or nine months dis­ cussing what we would want to see in a programming environ­ment that would combine the best of Mesa, Lisp, and Small­ talk.” The result was Cedar, now commonly acknowledged to be one of the best programming environments anywhere. “Cedar put some of the good features of Lisp into Mesa, like garbage collection and run-time type-checking,” said Mitchell of Acorn. Garbage collection is a process by which memory space that is no longer being used by a program can be reclaimed; run­ time type-checking allows a program to determine the types of its arguments—whether integers, character strings, or floating-point numbers—and choose the operations it performs on them accordingly. Interlisp, the language Teitelman had nurtured for 15 years, also was transported to the Dorado, where it was the basis for a research effort that has now grown into the Intelligent Systems Laboratory at PARC. PARC’s Smalltalk group, who had gotten used to their Altos and then built the Notetaker, another small computer, had some trouble dealing with the Dorados. “In the early days, we had Smalltalk running on an Alto, and I got to take my Alto home,” recalled Ingalls. “But the evolution of machines at Xerox went in the opposite direction from making it easy to take the stuff home. The next machine, the Dolphin, was less transportable, and the Dorado is out of the question—it’s a fire-breathing dragon.” The Dorado was the last major project to be completed by PARC in the 1970s—and the last one nurtured by many of the researchers who had made PARC famous and who in tum had been made famous by the work they did at PARC. For these researchers, it was time to move on. Alan Kay took a sabbatical beginning in March 1980 and never returned to PARC. Doug Fairbairn, Larry Tesler, and John Ellenby also left that year. In 1981 the exodus continued, with researchers including David Thornburg, Charles Simonyi, and Bert Sutherland packing their knapsacks. By June of 1984, John Warnock, Chuck Geschke, Lynn Conway, Dan Ingalls, Warren Teitelman, and Jim Mitchell had moved on. Bob Taylor had also left, taking a group of researchers with him that included Chuck Thacker and Butler Lampson. Why the sudden rush for the doors? There are probably as many reasons as there are people who left PARC. But several common threads emerge—natural career progression, frustration, the playing-out of PARC’s original charter, and a feeling among those who departed that it was time to make room for new blood. PARC hired many of its earliest employees right out of graduate school; they were roughly the same age as one another, and their careers matured along with PARC. “If you look at a championship football or basketball team,” said Teitelman, “they have somebody sitting on the bench who could start on another team. Those people usually ask to be traded.” “I saw personal computers happening without us. Xerox no longer seemed like where it was going to happen.” —Larry Tesler But some of those who left PARC recalled that a disillusionment had set in. They hadn’t been frustrated with the progression of their careers; rather, they had been frustrated with the rate of progression of their products into the real world. “We really wanted to have an impact on the world,” Mitchell said. “That was one reason we built things, that we made real things; we wanted to have a chance of making an impact.” And the world was finally ready for the PARC researchers, who until the late 1970s had few other places to go to continue the projects they were interested in. But by the early 1980s, other companies were making similar research investments-and bringing the products of that research to the commercial market­place. “We got very frustrated by seeing things like the Lisa come out,” said Mitchell, “when there were better research prototypes of such systems inside PARC.” “I saw personal computers happening without us,” said Tesler. “Xerox no longer seemed like where it was going to happen.” Tesler recalls trying to disabuse his colleagues of the notion that only PARC could build personal computers, after he met some Apple engineers. “Bob Taylor was the guy that kept insisting, ‘We have all the smart people.’ I told him, ‘There are other smart people. There are some at Apple, and I’ll bet there are some at other places, too.’ ” “‘Hire them,’ he said. I said, ‘We can’t get them all-there are hundreds of them out there, they are all over the place!’ At that moment I decided to leave.” The exodus may have begun in 1980 also because it signified a new decade. Ten years were over, and the researchers had done what they felt they had signed on to do. But, some felt, Xerox had not kept up its end of the bargain-to take their research and develop it into the “office of the future.” Some look unkindly on this “failure” of Xerox’s. Others are more philosophical. “One of the worst things that Xerox ever did was to describe something as the office of the future, because if something is the office of the future, you never finish it,” Thornburg said. “There’s never anything to ship, because once it works, it’s the office of today. And who wants to work in the office of today?” The departures may have proved beneficial for PARC’s long­ term growth. Because few researchers left during the 1970s, there was not a great deal of room for hiring new people with new ideas. “There is something about high technology, an excitement about being right out at the absolute edge and shoving as hard as we can because we can see where the digital revolution is going to go. I can’t imagine it not being exciting somewhere.” —Alvy Ray Smith “No biological organism can live in its own waste products,” Kay said. “If you have a closed system, it doesn’t matter how smart a being you have in there, it will eventually suffocate.” The exodus not only made room for new blood and new ideas within PARC but also turned out to be an efficient method of transferring PARC’s ideas to the outside world, where they have rapidly turned into products. Meanwhile, back at the lab, new research visions for PARC’s second decade have been seeded. Early efforts in VLSI have expanded, for example, to encompass a full range of fabrication and design facilities. William Spencer, now director of PARC, was the Integrated Circuits Laboratory’s first manager. The laboratory now does experimental fabrication for other areas of PARC and Xerox and is building the processor chips for the Dragon, PARC’s newest personal computer. Collaboration with several universities has led to a kit for integrating new chips into working computer systems. PARC has also found additional ways of getting products on the market: researchers in the General Science Laboratory in 1984 founded a new company, Spectra Diode Laboratories, with Xerox and Spectra-Physics Inc. funding, to commercialize PARC research on semiconductor lasers. Perhaps the strongest push in progress at PARC is in artificial intelligence, where the company is marketing Dandelion and Dorado computers that run Interlisp, along with PARC-devel­oped AI tools, including Loops, a software system that lets knowledge-engineers combine rule-based expert systems with object-oriented programming and other useful styles of knowledge representation. Loops, which was developed by three PARC researchers—formed AI Systems Business Unit, a marketing and development organization at PARC. PARC’s scattered AI groups have been consolidated into the Intelligent Systems Laboratory, which is doing research into qualitative reasoning, knowledge representation, and other topics. One interesting outgrowth of the early “office of the future” research is the Co-Lab, an experimental conference room that uses projection screens, the Ethernet, and half a dozen Dorados to help people work together and make decisions about complex projects. The next decade of advances in computer science may come from PARC—from “my grown-up baby,” as Goldman puts it. Or they may come from somewhere else. But the “architects of information” who made PARC famous have no doubt that they will come. “There is something about high technology, an excitement about being right out at the absolute edge and shoving as hard as we can because we can see where the digital revolution is going to go,” said Pixar’s Smith. “It has got to happen. I can’t imagine it not being exciting somewhere.” One statement of PARC’s charter was “to build the office of the future,” and at least one group of researchers chose to take this command literally. They built prototypes of future offices that people could work in to evaluate their quality. Two of their most dramatic creations were the Egg—a large, leathery ellipsoid with a computer and a stereo inside—and the Wheel, shown here. The Wheel, designed by Bill Bowman, an industrial designer, incorporated an adjustable Volvo seat, two adjustable monitors, and a sculptured teak desk-top and keyboard. It was mobile; the researchers envisioned that groups of Wheels could be easily rearranged as projects and working groups changed. “One day they rolled this thing out into the pasture behind PARC, with the horses eating grass around it, and had one of the secretaries sit inside and communicate to someone’s wife in Palo Alto while we took pictures,” Alvy Ray Smith recalled. Asked to describe their vision of the charter for the Xerox Palo Alto Research Center and of their role at PARC, the following group of early employees gave diverse responses: “If Xerox was going to be an interesting company in the 80’s and ’90s, it was going to have to move out beyond copiers to a broader context of dealing with information that knowledge-workers used. The office in the ’60s and ’70s used analog technology. It seemed that digital technology could be developed to allow people to work with knowledge.”—Robert Taylor, at PARC during 1970-83 and now director of Digital Equipment Corp.’s Systems Research Center. “Xerox offered 10 years of blank-check funding. They never promised to make the stuff into products; that wasn’t the charter.”—Alan Kay, 1970-81, now a research fellow at Apple Computer Inc. “PARC had three roles: to be a resource to the rest of the company, consulting and advising and assisting; to be viewed as an absolutely first-class research facility by the rest of the world; and to have some discernible impact on Xerox Corp.’s product line five years hence.”—Richard G. Shoup, 1970-79, now chairman of Aurora Systems. “What office workers are really in the business of doing is not copying pieces of paper; it’s copying information. PARC was going to make other ways of manipulating and handling that information.” —Jim Mitchell, 1971-84, now director of Acorn Computer Co.’s Palo Alto research facility. “Xerox offered 10 years of blank-check funding. They never promised to make the stuff into products; that wasn’t the charter.”—Alan Kay “Alan Kay said we were going to do a 10-year project, and I thought that at the end of 1980 we would have a Dynabook.” —Larry Tesler, 1973-80, now manager of object-oriented systems at Apple Computer Inc. “I was fresh out of graduate school. I knew I would be working with amorphous semiconductors, and that was exciting. —David Thornburg, 1971-81, now cofounder of Koala Technologies Inc. “There seemed to be a sort of open agenda as to what we did. Each of us had his own vision of what was possible.” —Dan Ingalls, 1971-84, now a principal engineer at Apple Computer Inc. “The only particular thing I remember is I figured that if Alan Kay works there, I bet I don’t have to wear a tie. I was right.”—Doug Fairbairn, 1972-80, now vice president for user-designed technology at VLSI Technologies Inc. “I wanted to make sure I wasn’t going to be put to work making copiers; they argued that the issues of software development and productivity were going to be Important to them.”—Warren Teitelman, 1972-84, now manager of programming environments at Sun Microsystems. “We would be doing good computer science in an environment in which it would eventually find its way into office products.”—Chuck Geschke, 1972-82, now executive vice president at Adobe Systems Inc. “I came to work on some special-purpose system architecture, to explore how to create custom digital hardware-software systems.”—Lynn Conway, 1973-83, now professor of electrical engineering and computer science and associate dean of engineering at the University of Michigan, after a recent stint as a manager of computer research at DARPA. “Xerox PARC had this aura of being a very far-out place. It was corporate, but it was very unusual for anything corporate to have the apparent foresight to bring some of the best people in the world together and let them do anything they wanted.” —Alvy Ray Smith, 1973-74, now director of graphics research in Lucasfilm Ltd.’s computer graphics department; soon to be vice president and chief technical officer of Pixar Inc., a spin-off of Lucasfilm. “I really didn’t have a good understanding—I don’t think anybody had. It was more a question of who we would be working with.”—Charles Simonyi, 1974-81, now manager of application development at Microsoft Corp. “It isn’t PARC’s job to develop products. PARC’s job is to develop the ideas on which we can produce a product scenario.”—John Ellenby, 1974-80, now president of Grid Systems Corp. “The charter was to explore various forms of office systems, to try things out, to develop software and systems that would be useful in offices. It made perfectly good sense to me.”—Severo Ornstein, 1974-83, now national chairman of Computer Professionals for Social Responsibility. “I was hired to manage the Systems Science Lab, basically centered around using computers. Alan [Kay) was probably one of the drawing attractions.”—Bert Sutherland, 1975-81, now a founder of Sutherland, Sproull & Associates, Inc. “Very few people were hired with a specific project in mind. If you’re a researcher, it’s up to you to be a self-motivator.”—John Warnock, 1978-82, now president of Adobe Systems Inc. Almost everything developed at Xerox PARC had a special name. The Alto, of course, was named for Palo Alto, but the computers that succeeded it were a strange collection beginning with “D”—Dolphin, Dorado, Dandelion, Dragon, and Dandetiger (an upgrade of the Dandelion). The unofficial name of the recently announced Xerox 1185 workstation, Xerox’s new artificial-intelligence programming machine, was the Daisy. All of these personal computers—with their distinctive large bit-mapped screens, three-button mice, and keyboards with several unlabeled keys—are referred to as D-machines. The version of Interlisp, an artificial-intelligence language, that runs on the D-machines is known as Interlisp-D, to distinguish it from Interlisp-10, which runs on PDP-10 mainframes. EARS—the name of the first laser printer—stood for Ethernet-Alto-Research character generator-Scanning laser output terminal. Dover, Pimlico, Puffin, and Penguin are the names of its early successors: Dover was the first “production” laser printer, of which several dozen were made with depreciated Xerox 7000s bought from the copier division; Pimlico and Puffin were the first two color laser printers; and Penguin was a high-resolution black-and-white printer that replaced the Dover. The distributed-computing image at PARC, in which a network was viewed as a collection of independent agents and servers acting in concert, also led to imaginative names for individual machines. In 1983, the Knowledge Systems Area in PARC, for example, had Dolphins named Galileo, Darwin, Da Vinci, and Archimedes; Dorados named Ahwahnee, Plaza, and Waldorf; and an Alto, acting as a file server, named Ivy. The names were not merely clever but were also functional; for example, a user could “chat” with Ivy to learn statistics on disk space, the status of a particular file, or other information. “If you wanted something to last forever, you called it interim.”—Jim Mitchell In software probably the simplest progression is Smalltalk-71, Smalltalk-72, Smalltalk-76 (completed in 1977), Smalltalk-78 (completed in 1979), and Smalltalk-80(completed in 1982). In the Computer Science Laboratory, SPL (systems programming language) from the Berkeley Computer Corp. begat MPL (modular programming language), which begat Mesa, which begat Mesa-Cedar, a complete programming environment for Mesa running on D-machines. (Mesa-Cedar was later renamed Cedar to avoid confusion.) Giving software botanical names like Cedar (after items in the Sunset Garden Book) was a popular practice; Laurel was one of the first electronic-mail programs at PARC, and Juniper was an early (though unsuccessful) distributed file system. The file system in use today with some PARC hardware is IFS—the interim file system. “It got to be a joke,” said Jim Mitchell, now vice president of research at the Acorn Research Center in Palo Alto, Calif. “If you wanted something to last forever, you called it interim.” Employees rolled into PARC at all hours of the day and night. Some, like Alan Kay, did their best work from 4:00 a.m. to 8:00 a.m. Others tended to work late into the night to take full advantage of the computing resources, but the switch to personal computers instead of shared mainframes changed that. “The Alto didn’t run faster at night,” Warren Teitelman recalled. For some, work styles cycled. “You got up when you were rested, came in when you were ready, and worked until whenever,” Teitelman said. Bicycles were a common means of transportation. Recalled Richard Shoup: “I would ride up to the lab and down the sidewalk and right in through the front door. Still on my bike, I would ride down the hall, park outside my office, work for the day, then get on the bicycle and ride down the hall and out the front door.” Even getting a group together for lunch meant a bike trip. “One of the deals, “said Kay, “was that if you wanted to drink beer at lunch you had to bike there and back to keep from gaining weight.” Entering PARC today is much like entering any large corporation or research center: a desk where visitors sign in, identification badges for employees, and a requirement that visitors be escorted at all times. Cameras are forbidden. PARC has had a fluctuating reputation for openness. On the one hand, relatively few articles were published by its researchers in its first five years; on the other hand, the Apple Lisa was conceived in 1979 when Steve Jobs, Apple’s chairman, toured PARC and saw his first bit-mapped display. In the earliest days, recalled David Thornburg—a former PARC researcher who is now chief scientist of Koala Technologies Corp., Santa Clara, Calif.—PARC carried the open-door policy to extremes. Jack Goldman, Xerox’s vice president for research, had in fact decided to build the research center within bicycling distance of Stanford University to ensure cross-fertilization with the academic world. “People were coming in and out all the time from the artificial-intelligence lab at Stanford and from other places,” Thornburg said. “There wasn’t as much of a concern for security-we saw ourselves as a university environment where we didn’t have to teach courses.” Thornburg recalled one incident that exemplifies that open attitude. In the late spring of 1971, Goldman held a staff meeting at PARC during which he stressed the need for a way of communicating the center’s research results to Xerox at large. Thornburg remembered that Goldman told the staff, “We need to have some kind of reporting-six-monthly reports or monthly reports—so the rest of the company has a way of finding out what’s going on.” “So this one fellow,” Thornburg related, “who was sitting back in his chair raised his hand and said, ‘Well, if you ask me, Jack, what I think we should do is build a computer-based query system where we write our reports and tag the different levels of the report in terms of their depth, so somebody who just wants a summary review will get a condensed document, and someone who wants an in-depth review will get the whole document.’ ” While Goldman praised this innovative suggestion, Thornburg said, PARC researcher Bob Bauer, who was sitting next to Thornburg, suppressed laughter. The meeting broke up, and Bauer “shot out of the room into the hallway and was just laughing hysterically,” Thornburg said, continuing: “I went over to him and asked, ‘Bob, what’s so funny?’ And he said, ‘You know that guy who just gave the suggestion to Goldman?’ ” “ ‘Yes?’ ” “ ‘He doesn’t work here,’ Bauer said. ‘He’s from the Stanford Al lab, and he was over here talking with some people, and they said, Gee, we’ve got to go to a staff meeting, do you want to come along?’ ” “I don’t think Goldman ever knew that guy didn’t work for Xerox,” Thornburg said. The word went out quietly through PARC that openness had its limits, but it wasn’t yet clear where those limits were. A much more explosive incident, in terms of PARC’s reputation and its relations with the parent company, came with a 1973 article in Rolling Stone magazine that was written by Stewart Brand. Brand, creator of The Whole Earth Catalog, a bible for the 1960s, had visited PARC, as well as several other research centers, to see what its visionaries were doing. He found Alan Kay, working on the Dynabook for “us kids”; Peter Deutsch, who wrote one of the first Lisp interpreters at age 15 and had “served on every major front in computer science” while working for ARPA; and Bob Taylor, the “chief marble collector” from ARPA, who put the group together. Taylor, according to Brand, was directing his researchers on the premise that the benefits of large, centralized computers “are less than claimed.” “And that is the general bent of research at [PARC]—soft, away from hugeness and centrality, toward the small and the personal, toward putting maximum computing power in the hands of every individual who wants it,” Brand wrote. “When the article, ‘Fanatic life and symbolic death among the computer bums,’ was published [in Rolling Stone], it really upset Xerox,” recalled Alvy Ray Smith, now vice president and chief technical officer of Pixar Inc., San Rafael, Calif. “All these wild, hairy people out there in a research lab got written up—how embarrassing! I’ve recently learned that when you go out and try to raise giant money in the financial world, you don’t come across as a long hair. Xerox could not afford to have people think they were flakes.” “Jack Goldman, and I forget who else, came out to PARC right after the article was published,” Thornburg recalled. “We were all on the edges of our seats thinking the lab was going to be closed down.” “It was made crystal-clear to use that this was not all right. If it happened again, the lab was going to be shut down. That was a very sobering experience. That was about the time we began having badges and all that sort of stuff—which after all is what companies do.” When David Em, the artist, first visited PARC in 1975, such trappings of corporate security were the norm. “It was a strange environment for an artist,” he recalled. “There was a guard gate, you needed a pass, there were all sorts of access codes on the terminals.” Even today, though, PARC is not nearly as watchful as many corporate research facilities. Many students and faculty members from Stanford are consultants at PARC, carrying ideas freely back and forth, and visitors still occasionally wander the halls by themselves. And some of the regular employees—having trouble figuring out whether to clip their badges to the collars of their T-shirts or to the belt loops of their shorts—just keep them in a handy pocket. Tekla S. Perry is a senior editor at IEEE Spectrum. Based in Palo Alto, Calif., she's been covering the people, companies, and technology that make Silicon Valley a special place for more than 40 years. An IEEE member, she holds a bachelor's degree in journalism from Michigan State University. The University of Texas professor co-invented discrete cosine transform Jae Jeong Hwang is a professor of IT convergence and communication engineering at Kunsan National University, in Korea. Zoran M. Milicevic is an assistant professor of telecommunications and IT at the University of Belgrade, in Serbia. Zoran S. Bojković is a professor of electrical engineering at the University of Belgrade. Wikipedia Kamisetty Ramamohan “K.R.” Rao died on 15 January 2021 at the age of 89. He co-invented the discrete cosine transform (DCT) technique, which is widely used in digital signal processing and data compression. A small band of believers triumphed after years of quietly plugging away Rodney Brooks is the Panasonic Professor of Robotics (emeritus) at MIT, where he was director of the AI Lab and then CSAIL. He has been cofounder of iRobot, Rethink Robotics, and Robust AI, where he is currently CTO. In 1997, Harvard Business School professor Clayton Christensen created a sensation among venture capitalists and entrepreneurs with his book The Innovator's Dilemma. The lesson that most people remember from it is that a well-run business can’t afford to switch to a new approach—one that ultimately will replace its current business model—until it is too late. One of the most famous examples of this conundrum involved photography. The large, very profitable companies that made film for cameras knew in the mid-1990s that digital photography would be the future, but there was never really a good time for them to make the switch. At almost any point they would have lost money. So what happened, of course, was that they were displaced by new companies making digital cameras. (Yes, Fujifilm did survive, but the transition was not pretty, and it involved an improbable series of events, machinations, and radical changes.)",0.0
"
        How to Build a Fault-Tolerant Quantum Computer
    ",https://spectrum.ieee.org/fault-tolerant-quantum-computing-milestone,2022-06-01,,"New study flags some clever tricks but still only hits 16-qubit size This artist’s impression shows the conceptual framework behind an error-correction method whereby quantum bits (qubits) are shielded from faults through their gate operations. Quantum computers theoretically can solve problems no regular computer ever could within the lifetime of the universe. However, there’s a lot of engineering and tech needed to turn that “theoretically” into a working quantum device that actually begins to deliver on that promise. For starters, quantum computers have to be able to overcome their own fragility to errors. But now, in a new study, researchers have successfully developed a technique for performing any possible quantum computation in what’s called a “fault-tolerant” manner. Whereas classical computers switch transistors either on or off to symbolize data as ones or zeroes, quantum computers use quantum bits, or “qubits,” which, because of the fuzzy nature of quantum physics, can exist in a state of superposition where they are both 1 and 0 at the same time. This essentially lets each qubit perform multiple calculations at once. In theory, qubits can get connected together into universal quantum computers that can perform any potential quantum computation. The more qubits that are quantum-mechanically linked, or entangled, the more calculations they can perform at the same time. According to researchers publishing in the 25 May issue of the journal Nature, present-day, state-of-the-art quantum computers typically suffer roughly one error every 1,000 operations. However, many practical applications demand error rates lower by a billionfold or more. Scientists often hope to compensate for these high error rates by spreading quantum information across many redundant qubits. This would help quantum computers detect and correct errors, so that a cluster of a thousand or so “physical qubits,” the kinds that researchers have developed to date, can make up one useful “logical qubit.” Importantly, a “logical qubit” is something of an abstraction, not consisting of a single trapped atom or photon or whatever the medium of quantum computation—but as an entity that is able to perform actual computations and is stretched across multiple physical qubits. However, building a fault-tolerant quantum computer will require more than scaling up to many thousands of physical qubits. Quantum computers will also need a variety of auxiliary qubits as well. When two logical qubits are connected in a quantum logic gate—the quantum computing version of the logic gates that conventional computers use to perform computations—scientists want to make sure that each physical qubit in a logical qubit interacts with only one physical qubit in the other logical qubit. This limits the disruption that could happen if a physical qubit experienced an error. Doing so requires auxiliary qubits to monitor these potential errors, increasing the complexity of these systems. Now, scientists have for the first time developed a quantum computer that can perform universal operations in a fault-tolerant manner. In the new study, the researchers experimented with a quantum computer featuring 16 physical qubits, each consisting of an electrically trapped calcium ion. The quantum information was stored in two logical qubits, each spread over seven physical qubits. These logical qubits were connected by what’s called a transversal logical CNOT gate, which linked each physical qubit in one logical qubit with its counterpart physical qubit in the other logical qubit. The researchers also used two auxiliary qubits in a “T gate,” which helped monitor for errors. The kind of auxiliary qubits the researchers employed are called “flag qubits.” These qubits focus on detecting events where minor errors can grow to major uncorrectable errors. In theory, flag qubits should reduce the number of auxiliary qubits needed in fault-tolerant quantum computers, says study lead author Lukas Postler, a quantum physicist at the University of Innsbruck, in Austria. “I think the most exciting aspect of this work is that we see an improvement in the quality of fault-tolerant logical operations compared to their non-fault-tolerant counterparts, despite the higher complexity of the circuits,” Postler says. Such findings may help pave the way for fault-tolerant universal quantum computers, the researchers note. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. An almost Machiavellian plot pitted Fairchild against Texas Instruments In one corner stood the defending champion, Texas Instruments. In the other stood the challenger, Fairchild Semiconductor. The referee, judge, promoter, and only spectator was Polaroid. In contention was the contract for the electronics of Polaroid’s secret project—a pioneering product introduced in 1972 as the SX-70, a camera eventually purchased by millions of people. As the embodiment of truly automated instant photography, the SX-70 fulfilled a long-held dream of Edwin Land, founder of Polaroid Corp., Cambridge, Mass. Vital to this “point and shoot” capability was a new film—one that would develop while exposed to light and so eliminate the tear-away covers of previous Polaroid films. Also vital were sophisticated electronics to control all single lens reflex (SLR) camera functions, including flashbulb selection, exposure control, mirror positioning, start of print development, and ejection of print. These circuits were divided into three modules, one each for motor, exposure and logic, and flash control. At the final count, some 400 transistors were used.",0.0
"
        Can Cryptocurrencies Actually Be Legal Tender? Should They?
    ",https://spectrum.ieee.org/should-cryptocurrencies-be-legal-tender,2022-05-30,," Following El Salvador’s adoption of Bitcoin, efforts to legalize crypto are spreading When the blueprint for Bitcoin was unveiled in 2008, the goal was to create a new form of electronic cash that bypassed traditional financial institutions. Since then, the original cryptocurrency and its descendants have primarily become investments rather than practical forms of payment. But now a handful of experiments are trying to bring the technology back to its roots by making it legal tender. Legal tender refers to forms of money the law says must be accepted in payment of a debt. Most countries designate only their domestic currencies this way, though some adopt foreign currencies either exclusively or alongside their own. Last year, however, El Salvador became the first country to adopt a cryptocurrency as legal tender. The country had already adopted the U.S. dollar as its main currency in 2001, but following new legislation the bitcoin joined it on 7 September. This has sparked similar efforts elsewhere, with the Central African Republic becoming the second country to make bitcoins legal tender in April. Meanwhile, there are proposals to do the same at the regional level in the United States and elsewhere. But this new push to legitimize cryptocurrencies as a form of payment is raising questions about how well suited they are to the role. “A good legal tender is something that is reliable,” says Thomas Dimpfl, a professor of economics at the University of Hohenheim in Germany. “The biggest risk is that it [bitcoin] is just so volatile.” “My goal was to put it on the government’s radar and get them to start thinking about how to legislate in this space.”—Ian Calderon, California State Assembly While the value of all currencies vary over time, cryptocurrencies often fluctuate wildly, as demonstrated by the recent crypto crash that has seen the value of the bitcoin fall by more than half since last November’s highs. This makes using it as a true currency difficult, says Dimpfl, because unless you receive all payments in bitcoins and can pay all bills in bitcoins, at some point you need to convert them into traditional fiat currencies. This exposes both everyday users and businesses to significant exchange-rate risks, he adds. It’s also proven a significant problem for El Salvador’s government. As part of its effort to popularize the new currency, it has spent US $104 million buying bitcoins, according to Reuters. But following the recent crash, the value of the heavily indebted nation’s holdings is just $67.9 million. Nonetheless, advocates for adopting cryptocurrencies as legal tender believe the dangers can be mitigated and the benefits could be substantial. The Swiss city of Lugano recently announced plans to accept bitcoins, Tether (a stablecoin pegged to the value of the U.S. dollar) and LVGA (a Swiss Franc–based stablecoin launched by the city in 2020) as “de facto” legal tender. While only the Swiss federal government can designate something as legal tender, the city will allow all payments to the authorities to be made in the three cryptocurrencies by October and is encouraging local businesses to accept them too. “Eventually you will, in practice, be able to live most of your experiences in Lugano in those three cryptocurrencies,” says Pietro Poretti, director of the city’s economic-development division. The city is currently negotiating with service providers, but the final solution will include the ability to instantly convert any cryptocurrency into Swiss francs. This is partly because the local government is not legally permitted to hold cryptocurrencies on it books, says Poretti, but also due to feedback from local businesses that highlighted instant conversion as a priority. That raises the question, why take payments in cryptocurrencies in the first place? But Poretti sees multiple benefits. For a start, the city is trying to establish itself as a center of excellence for blockchain development, and it’s hard to do that without supporting the technology’s main application. But beyond that, it could give the local economy a boost by pulling in crypto enthusiasts, he says, and give the city a crucial head start in what he thinks is likely to be an important technology in the future. “It certainly doesn't do any harm being equipped for what is coming, to be ahead of the curve rather than catching up,” he says. Bitcoin is certainly free of the manipulation of central banks. But, says Thomas Dimpfl of the University of Hohenheim, it wound up only transferring that right “to traders and speculators to manipulate the price.” Lugano isn’t the only place keen to get ahead of the curve. Last year, Texas ratified a bill recognizing the legal status of “virtual currencies,” and lawmakers in both Arizona and California have tabled “legal tender” bills this year. Only the federal government has the right to designate legal tender, so the Arizona bill is doomed to fail—while the California bill ultimately settled for the less ambitious goal of affirming the legality of accepting cryptocurrencies as payment. But that could still be significant, says Ian Calderon, a former majority leader for the Democrats in the California State Assembly who helped lead the effort. While there is nothing in the law preventing businesses or local governments from accepting payment in cryptocurrencies, he says legal ambiguity is suppressing adoption. The bill was designed to provide the confidence for people to experiment with the technology, which Calderon believes could play an important role in boosting financial inclusion thanks to its lower fees compared to traditional banking. The bill, which is unlikely to be debated this year, was also designed to force politicians to engage with the need to regulate the sector. “Historically, California has always been the cradle of technology and we’ve always had the ability to impact tech policy,” he says. “My goal was to put it on the government’s radar and get them to start thinking about how to legislate in this space.” But the case of El Salvador suggests that even with considerable government support, cryptocurrencies may struggle to establish themselves as viable cash replacements. Although El Salvador offered all residents $30’s worth of Bitcoin for downloading the government-backed Chivo wallet, a paper published in April by economists at the National Bureau of Economic Research, found that only 20 percent continued to use it once they’d spent the bonus. The leading reason for not using bitcoins was a lack of understanding, but a lack of trust in the currency and its volatility also ranked highly. Trust is an important ingredient for an effective currency, says Drimpfl. People trust the dollar because it is backed by the economy of the United States—and because they know that the Federal Reserve can intervene if needed to keep prices stable. This is done by manipulating the supply of money through interest-rate adjustments or buying government bonds. To a certain extent, increased adoption of cryptocurrencies as a medium of payment could help reduce their volatility by boosting the size of the crypto economy, says Dimpfl. But supply is typically governed by mathematical protocols that set out rules for how new coins can be minted. That is a deliberate choice, because advocates of cryptocurrencies claim the manipulation of money supply by central banks creates economic bubbles and financial crises. But it also takes away a vital lever to react to changing global circumstances, says Dimpfl, and the evidence suggests it hasn’t reduced spikes and crashes. “They left it to traders and speculators to manipulate the price,” he adds. Stablecoins, whose value is pegged to a traditional fiat currency, offer a potential workaround to these problems, says Drimpfl. But the recent collapse in value of the dollar-based Terra and wobbles in the price of other prominent coins suggest this is far from guaranteed. Edd Gent is a freelance science and technology writer based in Bangalore, India. His writing focuses on emerging technologies across computing, engineering, energy and bioscience. He's on Twitter at @EddytheGent and email at edd dot gent at outlook dot com. His PGP fingerprint is ABB8 6BB3 3E69 C4A7 EC91 611B 5C12 193D 5DFC C01B. His public key is here. DM for Signal info. This computer rendering depicts the pattern on a photonic chip that the author and his colleagues have devised for performing neural-network calculations using light. Think of the many tasks to which computers are being applied that in the not-so-distant past required human intuition. Computers routinely identify objects in images, transcribe speech, translate between languages, diagnose medical conditions, play complex games, and drive cars. The technique that has empowered these stunning developments is called deep learning, a term that refers to mathematical models known as artificial neural networks. Deep learning is a subfield of machine learning, a branch of computer science based on fitting complex models to data.",0.0
"
        IBM’s Target: a 4,000-Qubit Processor by 2025
    ",https://spectrum.ieee.org/ibm-quantum-computer,2022-05-10,," But an “intelligent software layer” will be key to doing anything useful with it IBM Quantum scientist Maika Takita in the IBM Quantum lab at the Thomas J. Watson Research Center, Yorktown Heights, N.Y. An update to IBM’s quantum-computing road map now says the company is on course to build a 4,000-qubit machine by 2025. But getting the device to do anything useful will require the development of a powerful new software stack that will help manage errors, share the load with classical hardware, and simplify the programming process. Since IBM first unveiled its quantum hardware plans in 2020, the company has largely kept to the schedule; the most recent milestone from that forecast that has come to fruition was the release of the company’s 127-qubit Eagle processor last November. The first iteration of the road map topped out with the 1,121 Condor processor that is scheduled for release in 2023, but now the company has revealed plans for a 1,386-qubit processor, called Flamingo, to appear in 2024 and for a 4,158-qubit device called Kookaburra to make its debut in 2025. Key to building these devices will be a new modular architecture in which multiple chips are connected to create a single large processor. This will be made possible by new short-range couplers, which allow communication between qubits on adjacent chips, and cryogenic microwave cables, which allow longer-range connections between different processors. Kookaburra, due in 2025, will have 4,158+ qubits.IBM But quantum hardware is error-prone and fiendishly complex, so simply wiring large numbers of qubits together doesn’t necessarily mean you can do much practical work with them. IBM thinks the key to tapping the power of these extra qubits will be an “intelligent software layer” that augments its quantum chips with classical tools that can help manage noise and amplify their processing power. “We believe that classical resources can really enhance what you can do with quantum and get the most out of that quantum resource,” says Blake Johnson, IBM’s quantum platform lead. “And so we need to build tooling—an orchestration layer if you will–that can allow quantum and classical computations to work together in a seamless way.” The most fundamental challenge for quantum hardware remains its inherent noisiness, and Johnson says there is “a furious amount of research activity” going on to scale up error-mitigation techniques. So far, most innovations have been tested only on smaller systems. But Johnson says early results suggest that an approach known as probabilistic error cancellation will work on devices the size of those envisioned by IBM at the end of its quantum road map. The technique involves deliberately running noisy versions of quantum circuits to learn the contours of that noise, then using postprocessing on classical computers to reduce the level of error in answers. Applying these kinds of techniques requires considerable specialist knowledge, though. That limits the techniques’ usefulness for the average developer. That’s why, from 2024 forward, IBM plans to build error mitigation directly into its Qiskit Runtime software development platform so users can build programs without having to think specifically about how to reduce the noise. “We want these things to be automatic for the user,” says Johnson. “You shouldn't have to be a quantum control expert in order to get meaningful results out of a quantum compute.” Beyond suppressing errors, IBM is also planning new software tools designed to speed up users’ applications and help them tackle larger problems. Last month, the company released two “primitives”–predefined basic programs that carry out core quantum operations–that users of Qiskit runtime can use to build applications. Starting in 2023, IBM will make it possible to run these primitives in parallel across multiple quantum processors, significantly speeding up users’ programs. IBM’s ambitions also include letting developers build complex programs that combine both classical and quantum elements. That’s because many developers would like to bring quantum capabilities into specific parts of their existing workflows, says Johnson, and also because it could help tackle problems larger than the quantum hardware is capable of solving by itself. Central to this quantum-classic mashup idea is the concept of “circuit knitting.” This is a family of techniques that split quantum computing problems into chunks that can be run on multiple processors in parallel, before using classical computers to jump in to stitch the results back together again. In a proof of concept last year, IBM researchers showed that they could use a circuit-knitting approach called entanglement forging to double the size of quantum system that can be simulated on a given number of qubits. By 2025, the company plans to introduce a toolbox of circuit-knitting methods, which developers will be able to use to build algorithms that make the best of both classical and quantum resources. To support this, they will also adopt a “quantum serverless” approach in which developers don’t need to think about what hardware is required to run their code, and quantum or classical resources are provided automatically as needed. “The serverless model allows the developer to focus on their code, use resources on demand when they need them, and have access to a variety of infrastructure resources without being an expert in the infrastructures themselves,” says Johnson. The company’s road map predicts that all those things will likely come together by 2025, letting developers start producing full-blown applications for things like machine learning, optimization, and physical simulations. Edd Gent is a freelance science and technology writer based in Bangalore, India. His writing focuses on emerging technologies across computing, engineering, energy and bioscience. He's on Twitter at @EddytheGent and email at edd dot gent at outlook dot com. His PGP fingerprint is ABB8 6BB3 3E69 C4A7 EC91 611B 5C12 193D 5DFC C01B. His public key is here. DM for Signal info. An almost Machiavellian plot pitted Fairchild against Texas Instruments In one corner stood the defending champion, Texas Instruments. In the other stood the challenger, Fairchild Semiconductor. The referee, judge, promoter, and only spectator was Polaroid. In contention was the contract for the electronics of Polaroid’s secret project—a pioneering product introduced in 1972 as the SX-70, a camera eventually purchased by millions of people. As the embodiment of truly automated instant photography, the SX-70 fulfilled a long-held dream of Edwin Land, founder of Polaroid Corp., Cambridge, Mass. Vital to this “point and shoot” capability was a new film—one that would develop while exposed to light and so eliminate the tear-away covers of previous Polaroid films. Also vital were sophisticated electronics to control all single lens reflex (SLR) camera functions, including flashbulb selection, exposure control, mirror positioning, start of print development, and ejection of print. These circuits were divided into three modules, one each for motor, exposure and logic, and flash control. At the final count, some 400 transistors were used.",0.0
"
        Frozen Neon Invention Jolts Quantum Computer Race
    ",https://spectrum.ieee.org/neon-qubit,2022-05-04,,"Single electrons trapped on solid neon could serve as highly stable qubits A superconducting microwave resonator (gold curves) can use microwaves (pale beam) to help control a single isolated electron (orange waves) trapped on a block of solid frozen neon (green block). The individual electrons, controlled by the microwave pulses, can then be harnessed as individual qubits to make a new kind of quantum computer, according to a new study in the journal Nature. Quantum computers can theoretically find the answers to problems no classical computer could ever solve, but they rely on infamously unstable components known as qubits. New findings now suggests that electrons trapped on frozen solid neon could prove a simple yet powerful kind of qubit for use in future quantum computers. Qubits, or quantum bits, rely on the bizarre nature of quantum physics, which suggests that electrons, atoms and other building blocks of the universe can exist in a state known as superposition where they are essentially spin in two opposite directions at once or exist in two or more places at the same time. By placing many qubits into superposition, a quantum computer can in theory perform a mind-boggling number of computations simultaneously. Amazon, Google, IBM, and many others are racing to create a practical quantum computer from a variety of qubit platforms, such as superconducting loops, electromagnetically trapped ions and spins within silicon. However, all qubits are extraordinarily fragile to outside interference. “This is a completely new qubit platform. It adds itself to the existing qubit family and has big potential to be improved and to compete with currently well-known qubits.”—Dafei Jin, Argonne National Laboratory In the new study, to create a qubit protected from environmental disruptions, the scientists experimented with neon, a noble gas like helium that virtually never reacts with other elements, potentially making it an ideal host for a qubit. Neon freezes into a solid when cooled to below roughly minus 248.6 degrees C and brought to pressures of more than 0.42 atmospheres. ""The noble-gas solids are the most inert and purest solids in nature and can solve a lot of issues that other systems have,"" says study principal investigator Dafei Jin, a quantum physicist at Argonne National Laboratory in Lemont, Illinois. The researchers chose one of the simplest possible qubits for their design—single electrons. They froze neon at a temperature one-hundredth of a degree above absolute zero on a microchip and then used a tiny light bulb filament to spray electrons at it. ""When you bring the electron near the surface of the neon, the electrons in the neon atoms get slightly rearranged and repelled by the electron, because like charges repel, but because the neon is neutral, this slight repulsion of electrons leaves a slightly positive charge that attracts the electron to the surface,"" says study co-senior author Kater Murch, a quantum physicist at Washington University in St. Louis. However, this electron cannot penetrate the surface of the neon, since all of the neon's electrons' energy levels are filled, ""so it is repelled from actually contacting the surface."" Instead, this electron stays on top of the neon. Neon ice shows promise as a new platform for quantum bits (a.k.a. qubits).www.youtube.com Electrodes in the microchip can keep electrons that get trapped on the solid neon in place for more than two months. A superconducting microwave resonator on the chip, much like a microscopic version of a microwave oven, then emits microwaves to help control and read the qubit. The scientists argue that useful qubits require three key qualities: The group's experiments reveal that within optimization, the new qubit can already stay in superposition for 220 nanoseconds and change state in only a few nanoseconds, which outperform qubits based on electric charge that scientists have worked on for 20 years. ""This is a completely new qubit platform,"" Jin says. ""It adds itself to the existing qubit family and has big potential to be improved and to compete with currently well-known qubits."" The researchers suggest that by developing qubits based on an electron's spin instead of its charge, they could develop qubits with coherence times exceeding one second. They add the relative simplicity of the device may lend itself to easy manufacture at low cost. The new qubit resembles previous work creating qubits from electrons on liquid helium. However, the researchers note frozen neon is far more rigid than liquid helium, which suppresses surface vibrations that can disrupt the qubits. It remains uncertain how scalable this new system is—whether it can incorporate hundreds, thousands or millions of qubits. ""I cannot say I have a clear answer,"" Jin says. ""It is still a problem shared by all the qubit platforms. We may have some ways to do better than superconducting qubits, and close to as good as trapped ions. But it is not easy to achieve hundreds of qubits in the near term."" In the future, the researchers aim not only to develop qubits based on electron spin instead of charge, but to also entangle two qubits together, ""as that is a key step towards quantum computing,"" as well as ""achieving tens of qubits on the same chip,"" Jin says. ""No immediate obstacles if we just follow what other qubit platforms have developed so far. We think we can quickly catch up."" The scientists detailed their findings online May 4 in the journal Nature. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. This computer rendering depicts the pattern on a photonic chip that the author and his colleagues have devised for performing neural-network calculations using light. Think of the many tasks to which computers are being applied that in the not-so-distant past required human intuition. Computers routinely identify objects in images, transcribe speech, translate between languages, diagnose medical conditions, play complex games, and drive cars. The technique that has empowered these stunning developments is called deep learning, a term that refers to mathematical models known as artificial neural networks. Deep learning is a subfield of machine learning, a branch of computer science based on fitting complex models to data.",0.0
"
        Inventing Postscript, the Tech That Took the Pain out of Printing
    ",https://spectrum.ieee.org/adobe-postscript,2022-04-23,,"Adobe’s founders and engineers tell a tale of software, stumbles, and serendipity Time and again, in his earliest attempts at controlling laser printers, John Warnock got the message, “Page Too Complex,” from a recalcitrant machine. Any system he designed, he vowed, would have to have a “Print Anything” architecture. That goal led ultimately to a page description language called PostScript, today the de facto standard of desktop publishing. This article was first published as ""‘PostScript’ prints anything: a case history."" It appeared in the May 1988 issue of IEEE Spectrum. A PDF version is available on IEEE Xplore. The diagrams and photographs appeared in the original print version. Back then, Warnock already had a rough idea how to “Print Anything.” But later he ran into a different obstacle, when his employer, Xerox Corp., proved loath to support a truly standard language. So off he went, with Charles Geschke and several other colleagues, to found Adobe Systems Inc. in Mountain View, Calif. By that time, PostScript was only two major pieces of research away, although one—the development of type font algorithms—was “a research project that had to succeed,” says Warnock, and the other had been described as one of the world’s most difficult problems. The rest is desktop publishing history. PostScript can truly do anything, though extremely complex images can take as much as an hour of computation time. It first appeared in the Apple LaserWriter, which was introduced in January of 1985. Today it has been adopted by 23 manufacturers of laser printers, with more still signing on. This story is as much about luck and guts as about matters of principle and brilliant software engineering. Still, this story is as much about luck and guts as about matters of principle and brilliant software engineering. It would have been quite different had Warnock and company not been in the right place at the right time to meet the right person. The time was right because of the imminence of three hardware developments: the first low-cost, bit-mapped personal computer, the first low-cost laser printer, and a decline in price of high-density memory chips. And the right person was Apple founder Steven Jobs, who invented the first, hoped for the second, and told Adobe to tough out the third. Device-independent software Software not directly tied to a specific piece of hardware. Interpreter A program that translates an instruction in the source code of a high-level language into machine language by deciding on the fly what machine instructions best translate it before moving onto the next instruction in source code. Laser printer A device that, like a xerographic copier, draws an image on a drum, but with a laser beam instead of lenses; applies toner to the charged image area; and transfers the toner to a sheet of paper, melting it into the paper to set the image. Page description language A method of expressing the appearance of a printed image, including text, lines, and bit-mapped photographs. Postfix notation Also known as reverse Polish notation, the appearance of operators after the data on which they are to operate; thus 2 + 2 becomes 2 2 +. Today laser printers are rapidly replacing the daisy-wheel printers in the office, pushing out letter-quality type as their laser obeys the commands of simple software. But given more sophisticated software like PostScript, laser printers can do far more. They can print many different type fonts and make the letters dance around the page hand in hand with drawings and photographs. PostScript does all this implies—draws lines and curves, tilts text at arbitrary angles, or shades a photograph in various tones of gray. It is as complete and flexible a programming language as Pascal or C or Forth, having variables, loops, conditionals, operators, and routines and offering any number of ways to get the same output. The PostScript program is created on the computer either by someone using the language or by desktop publishing software or other applications software that translates, say, the movements of a mouse into a PostScript program. (Other page description languages are optimized for one of these purposes, not both.) That program is sent over a local-area network or through an RS-232 port to the laser printer. There it is converted into instructions for the printer by the PostScript interpreter, software resident in ROM. On the same circuit board as up to 2 megabytes of ROM is a Motorola 68000 series processor, which executes the instructions and causes the pages to be printed. Things were more elementary with the first laser printers, which were in regular use at the Xerox Palo Alto Research Center (PARC) in the mid-1970s. They were controlled by a printing protocol called Press, which was not a programming language but a set of instructions that sent image data to a printer in a steady stream. It handled letters and simple images well, but for anything more detailed, got the printer to return the message: “Page Too Complex.” Thereupon the typical PARC engineer would simplify the image. But when Warnock, a computer scientist with a Ph.D. from the University of Utah, joined the center in 1978, he immediately began work on a new printer protocol. Six years of experience at Evans & Sutherland in Mountain View, Calif., had taught him where to start. Adobe Systems founders John Warnock (right) and Charles Geschke visited Adobe Creek, inspiration for their company’s name. A dry winter has slowed the creek to a trickle, but the company has had anything but a dry year. The entrepreneurs in 1982 found Adobe a suitable name since the creek meandered near both their domes and, even more important, had none of the Qs, Xs, Ys, and Zs then popular with high-tech startups. The PostScript program is created on the computer either by someone using the language or by desktop publishing software or other applications software that translates, say, the movements of a mouse into a PostScript program. (Other page description languages are optimized for one of these purposes, not both.) That program is sent over a local-area network or through an RS-232 port to the laser printer. There it is converted into instructions for the printer by the PostScript interpreter, software resident in ROM. On the same circuit board as up to 2 megabytes of ROM is a Motorola 68000 series processor, which executes the instructions and causes the pages to be printed. Things were more elementary with the first laser printers, which were in regular use at the Xerox Palo Alto Research Center (PARC) in the mid-1970s. They were controlled by a printing protocol called Press, which was not a programming language but a set of instructions that sent image data to a printer in a steady stream. It handled letters and simple images well, but for anything more detailed, got the printer to return the message: “Page Too Complex.” Thereupon the typical PARC engineer would simplify the image. But when Warnock, a computer scientist with a Ph.D. from the University of Utah, joined the center in 1978, he immediately began work on a new printer protocol. Six years of experience at Evans & Sutherland in Mountain View, Calif., had taught him where to start. In 1971, Evans & Sutherland had undertaken to equip the New York Maritime Academy with a simulator for training harbor pilots. The trainees were to sit on the mockup of a ship’s bridge, surrounded by five 12-foot-high, 30-ft-long (3.6-by-9 meter) screens displaying a computer-generated representation of New York Harbor, complete with buildings, piers, movable buoys, changing weather conditions, and other ships to be avoided. The system had to produce images in full color for five projectors at 30 frames a second. Evans & Sutherland had never produced anything as complex. It let time slip by until, with only one of the contract’s three years left, “everybody hit the panic button,” Warnock says. So to save time, the company had the hardware and software developed in parallel, the first in Utah and the second by a team led by Warnock in California. The rush planted the first two seeds for what was to become PostScript. Obviously, a database listing everything in the harbor was both essential and would have to be built in total ignorance as to the hardware it would eventually run on. So Warnock’s team decided to invent a language unrelated to any computer. Only when the simulator hardware was ready would they build a compiler to translate the database into the appropriate machine language. Meanwhile, feeding information about the harbor into the database proved arduous. Putting maps on a digitizing tablet and touching them with a stylus at numerous points was not so bad; but using a keyboard to enter the details—whether the point touched was a pier of a certain type or a building or an island—was slow going. To make this task easier, John Gaffney, one of Warnock’s group, spent a weekend writing a software routine that would generate the information about the objects from menus. Because the PostScript Language treats text like any other graphic object, it can be scaled to any size and rotate to any angle. PostScript was the first page description language to be able to produce such a spiral of type. Adobe Systems Inc. By the time the harbor simulator was completed, only slightly behind schedule, Warnock had discovered how powerful an object-oriented language is. Unlike Basic or Fortran, say, which require the user to spell out every last instruction, it packs all those details into modules, or objects, which the user controls with just a few directives. Warnock had also discovered that making software device-independent “gives you a great deal of leverage and flexibility.” Those lessons learned, his group turned to expanding Gaffney’s little interpreter into a full programming system for computer-aided design (CAD). In 1977, that project was released by Evans & Sutherland as The Design System. “It had an interactive, stack-oriented architecture,” Gaffney said, “with simple commands for pushing and popping arguments onto and from the stack and a rich dictionary for look-ups.” (Such an architecture stores data as it is received, stacking it like a pile of books. A command like “add” would “pop” the topmost pieces of data from the stack, act on them, and “push” the result back on the pile.) Only one copy of The Design System was ever released, as a test bed for the final development, but the other company’s project director died and The Design System died with him. Warnock, however, took the stack and dictionary ideas—along with what he had learned from the harbor project—to PARC. PARC was then using a programming language called Mesa. In 1978, soon after arriving at the center, Warnock persuaded another Xerox researcher, Martin Newell, to help him re-create The Design System in Mesa. The result, called Jam, for John and Martin, proved the concepts he brought from Evans & Sutherland were appropriate for laser printing. Jam was object oriented and device independent, like the harbor simulator, and in some ways simpler than The Design System, because printing requires only two dimensions, versus CAD’s three. But it needed a few features, such as type fonts, found in neither of its ancestors. Moreover, Warnock recalls, “Xerox was using a different printing scheme on every printer. The Star workstations [then being developed] were crumbling under the load of trying to drive them all differently.” So Warnock and a group of researchers headed by Charles Geschke set out to merge Jam with the older Press protocol into Interpress, a standard, device-independent language capable of driving all Xerox Corp.’s laser printers. Interpress was completed in 1981, but unhappily, the end was not in sight. Because of the compromise between Jam and Press, “the language became complicated in its redesign,” Warnock says. And Xerox begged the issue of standardization by producing several versions of the language, so the company’s older laser printers could run some form of it. The commands of the PostScript programming language are optimized for graphics. This elongated word “Spectrum” was generated by the PostScript program shown below it. The first group of commands (red) identify a typeface from PostScript’s library of typefaces and enlarge it to 50 points from the 1-point size in which it is stored (in typesetting, there are 72 points to the inch). The next group of commands (blue) tells the laser printer at which point on the page to anchor the lower left corner of the word. The next command (yellow) stretches the typeface vertically, while leaving it unaltered horizontally. The final commands (purple) specify the letters to be drawn and order the printer to produce the image. A special program editor transmits the instructions to the printer. Adobe Systems Inc. Worst of all, to Warnock, was the insistence that printers always run at their rated speeds. Since a 20-page-per-minute printer could not produce anything very complex in three seconds, he was back facing his “Page Too Complex” nemesis. The constraint derived from the copier business, Geschke explains, where “pricing of leased machines was based on copies per day. But in electronic printing, in our opinion, function was most important, so there was a real variance between our and the Xerox position.” All the same, in the belief that any standard was better than none, Warnock and Geschke began promoting Interpress within Xerox Corp. Eventually, they won—sort of. But Xerox added, Warnock recalls, “’We’re going to keep it a secret because it is so wonderful and if we publish it the Japanese might implement it before we do.’ “Gee,’ I said, ‘A secret standard—I find this a hard concept to understand.’” Convinced that Xerox was making a mistake, Warnock and Geschke left PARC to implement their page description language once again, but this time within a corporation they controlled. With the help of David Evans of Evans & Sutherland and William Hambrecht of Hambrecht and Quist, a San Francisco-based venture capital firm, they wrote a business plan and incorporated in December of 1982. They intended both to sell this setup as a turnkey system and to franchise the publishing equivalent of a one-hour photo store. Desktop publishing, though, was not what Warnock and Geschke at first had in mind. The system they foresaw consisted of a workstation linked by a device-independent, page-description language like Jam to a laser printer for draft printing, a photo-typesetter for the final output, and whatever other output device they might later add. No other publishing package then available used the same software for different output devices. And they intended both to sell this setup as a turnkey system and to franchise the publishing equivalent of a one-hour photo store. Adobe then consisted of Warnock, Geschke, and a core of other engineers hired from PARC: Daniel Putman, Thomas Boynton, and Douglas Brotz. As they planned to buy whatever hardware they needed after they had perfected their programming language, they focused first on Jam. They worked in C, on a VAX 750 running Berkeley Unix, to develop the language, and they tested in on a Sun workstation driving a full-size laser printer that they had borrowed from Digital Equipment Corp. “At that time,” recalls Putman, “most companies required that we spell our names and pay in cash, so we had to beg, borrow, and steal the tools to prototype PostScript.” To avoid copyright problems, they licensed The Design System concepts from Evans & Sutherland. They were free to use their PARC research results, as those had been published. For years theoreticians have suggested means of breaking images into their line segments, but their algorithms tended to fall apart when faced with difficult cases—large numbers of lines intersecting at a single point, for example (inset). The Adobe team says it has solved this problem with a proprietary algorithm with the results illustrated here. The image was created with Adobe Illustrator, a drawing program that runs on the Apple Macintosh personal computer, and was printed on the ColorScript 100, the first color PostScript printer, released in April by QMS Inc. of Mobile, Ala. Warnock and Geschke were not close-mouthed about their plans, and soon not only Jobs heard (he was then chairman of Apple Computer Inc. of Cupertino, Calif.) but also C. Gordon Bell, then vice president of engineering at Digital Equipment in Maynard, Mass. Bell told the pair that six research teams at Digital had been trying for years to devise a decent means of driving its laser printers, and if Adobe could solve the problem, Digital would be interested in licensing the solution. Jobs had been facing a similar problem. The Macintosh was well into development, but without a letter-quality printer would go nowhere in the business market. Daisy-wheel printers were out of the question, because they could not produce the bit-mapped graphics basic to the Macintosh. But Apple’s own engineers could not get high-quality graphics out of a laser printer in time for the Macintosh introduction. Jobs suggested that Adobe become a software company, sell to manufacturers instead of at retail, and negotiate a licensing agreement with Apple. Undeterred, Jobs and Robert Belleville, then director of engineering for Apple and now director of strategic planning for Convergent Technologies Inc., San Jose, Calif., had dreamed up the perfect Macintosh laser printer—one that could produce all the fonts in the world with no help from a disk drive. But they lacked “the slightest idea of how to do this,” says Belleville, until he ran into Putman at a cocktail party, heard what Adobe was doing, and brought Jobs over for a visit. “I was overjoyed!” recalls Belleville. “Their system could do simple things fast and also do full graphics and scanned images. And when I saw font scaling was possible across such wide ranges, we were sold.” Jobs suggested that Adobe become a software company, sell to manufacturers instead of at retail, and negotiate a licensing agreement with Apple. Adobe liked the idea, signed the agreement with Apple at the end of 1983, and much to Hambrecht & Quist’s surprise, showed a profit at the end of its first year. Reimplementing the Jam language with its object orientation, stacks, postfix notation (in which operands precede their operators), and dictionary was relatively straightforward. Most of the research had been completed at Evans & Sutherland and at PARC. Basically all Adobe had to do was engineer it into a product, named PostScript after the postfix notation it uses and because it was to be the last thing that happened to an image before it was printed. Also, since the product had to “Print Anything,” it had to put functionality above speed and cost—the three factors traded off in the design of microprocessor systems like the one that would control the laser printer, explains Putman, now vice president of engineering at Adobe. Still, two key breakthroughs remained to be made. One of them was creating the font algorithms, proprietary formulas for the creation of text. “Even with Interpress,” says William Paxton, director of advanced development for Adobe, “fonts were a wart on the side of an otherwise elegant design.” Interpress could do arbitrary transformations, like scale and rotate, on images, but in its early versions could not do them on bit-mapped text without degrading its quality. PostScript, however, unifies text and graphics by storing the fonts as outline representations of the letters, not as bit maps. Back in early 1983, however, this unification was easier to propose than to realize. “Getting high-quality fonts from outline representations of characters was seen as an insoluble problem,” Warnock says, because it was hard to produce smooth curves of varying widths without jagged edges. Print quality seemed unobtainable from anything less than a phototypesetter. But in mid-1983, Warnock says, he had an idea for a fundamentally new set of algorithms that might do the trick. His initial experiments promised success, so he set Paxton to refining the algorithms. The results are proprietary and are encrypted inside the ROMs that contain PostScript instructions because this font technology is the key distinction between Adobe’s product and others. So successful was Adobe’s solution to the font problem that Linotype, Letraset, and other owners of the most popular typeface designs were willing for the first time to license the outline representations of their typefaces. No earlier technology had done them justice. (Ironically, Adobe is now licensing its font technology to Linotype, and Linotype is converting its entire library of some 2000 fonts into PostScript representations.) Adobe’s other technical breakthrough is the algorithm, called Reducer, that breaks down complex shapes into simpler ones easier for PostScript to describe. Such an algorithm is a key component of any graphics language, and theoretical papers about a universal form of it were numerous: but, says Brotz, “they tended to gloss over the hard cases that arise in real applications—figures with large amounts of data and multiple intersections at the same point, for example.” So when the page printed, certain images would come out badly fragmented or warped, violating Adobe’s “Print Anything” rule. “About a week after I had joined Adobe in 1983,” Brotz recalls, “John Warnock mentioned this rather important algorithm that had to be written. And I, with no graphics background, volunteered. Several months later, older and wiser, I realized it truly was one of the world’s hardest problems.” But Brotz did not give up, and he says, “We have now an exactly correct reducer algorithm. It is the heart of the graphics system in PostScript.” And a tally Brotz keeps reveals that no bugs have been discovered in the Reducer in more than two years. “Warnock promptly labeled [the procedure] ‘Andy’s Stupid Input Device'....[but] it turned out that Andy’s Stupid Input Device was the lowest common denominator and all the special-case code could disappear.” —Douglas Brotz Adobe had agreed to deliver its software for installation into the LaserWriter during the summer of 1984. But because of marketing and manufacturing concerns, the LaserWriter itself was to be introduced in January of 1985. So the Adobe engineers used the time to tighten the code (the final release contained some 200,000 bytes) and fine-tune the algorithms. They also made some more specific changes. One had to do with handling input devices. As originally conceived, PostScript was to have been independent of the output, but not the input, device. Warnock had thought that PostScript, to take in scanned images, would need to contain information about a wide range of optical scanners. But Brotz, after programming the parameters of just two of many scanner types, realized that the task was not only horrendous and repetitive but ate up a lot of memory. Andy Shore, an Adobe computer scientist, overheard him complaining one day and suggested writing a PostScript procedure that would pretend that it was an input device and spit out the image information in a standard format, regardless of the characteristics of the actual standard. Brotz did not think it would work and “Warnock promptly labeled it ‘Andy’s Stupid Input Device.'” Still, Brotz thought it might be helpful for generating test patterns, and when he implemented it, “it turned out that Andy’s Stupid Input Device was the lowest common denominator and all the special-case code could disappear.” Problems arise only when the image data has been compressed for transmission or storage; the programmer then has to insert a routine to decompress the data before it is handed to the image algorithm. Another improvement involved performance profiling—running various tests to see what frequently used functions slowed down operation. Floating-point routines were the chief culprits because they are computationally intensive. So the team took some of the algorithms for the common operations, such as breaking curves into vectors and drawing outlines, and rewrote them in less flexible fixed-point arithmetic. Now only when fixed-point arithmetic would be too imprecise does the interpreter call the floating-point routine. “So with no loss of generality,” says Edward Taft, Adobe senior computer scientist, “we were handling 99 percent of the cases five times faster than we were before.” To improve the other 1 percent, Belleville sent one of his engineers over from Apple—Jerome Coonen, a recognized expert in floating point. He optimized the algorithms so, Taft says, “whereas formerly an algorithm required six multiplies, four divides, and three square roots, now it only required three multiplies, four divides, and some approximation of a square root.” “We came from the school of thought that software is soft. So if you have problems, you just have another release. But Apple was telling us, ‘Hey, we always ship our system in ROM, why can’t you?’” —Douglas Brotz Throughout the design of PostScript, speed was regularly traded off to ensure that any image would print. The group reasoned that if they built in all this functionality, they could eventually improve the performance; but if they left out functions, they might never be able to add them back in. However, says Putman, sometimes they had doubts. So they designed a version of PostScript that spat out information as fast as the laser moved across the page. The expense of the frame buffer was eliminated—along with the ability to print pages too complicated for the software to process in time. Adobe called this implementation Subscript, but dropped it after six months. As Taft says, “If you’re trying to promote a standard, there is nothing worse than issuing a subset of the standard. It means that all of the applications are going to be targeted to the lowest common denominator.” Debugging throughout the project was strenuous because the Adobe team was “terrified of putting all this code out on ROMs,” Brotz says. “We came from the school of thought that software is soft. So if you have problems, you just have another release. But Apple was telling us, ‘Hey, we always ship our system in ROM, why can’t you?’” In January of 1985 the Apple LaserWriter was introduced, virtually bug-free. In 1984, Adobe signed licensing agreements with QMS Inc., Linotype, and Dataproducts Corp. Today, even Hewlett-Packard Co., whose PCL page description language was one of PostScript’s earliest competitors, is among the 23 companies offering PostScript interpreters for their printers. Although the Adobe group made some key technical breakthroughs, three other components were necessary to make PostScript a runaway success not just in low-volume professional publishing but in the high-volume office environment. As noted earlier, one was a cheap laser printer. When Adobe was founded, the cheapest cost around $10,000. It also weighed as much as a desk, so that it had to be serviced on site and sold through a distributor, not on a cash-and-carry basis. Then Canon Inc., of Tokyo, Japan, introduced the Canon LBP-CX desktop laser printer, which, moreover, printed beautifully. “If it had been poor xerography,” says Paxton, “it wouldn’t have mattered how good our technology was.” Also on the horizon was a bit-map-based personal computer—the Apple Macintosh. All previous low-cost personal computers had used character graphics, for which daisy-wheel printers made more sense. “The projections were that the RAM prices were going to drop, but you had to have a very strong stomach to be able to go up to the wall and pray that the door was going to open.”—William Paxton The third piece of luck was the decline in the price of memory chips. “We started this development on an uneconomic basis,” Warnock says. “The LaserWriter’s first controller needed forty-eight 256K DRAM chips, which up to December of 1984 cost about $30 each. That meant Apple would have had to sell that machine for about $10,000—but its computer cost $2400.” But, with Belleville’s and Jobs’s strong support, the Adobe team bet that the memory process would drop. “Sure,” says Paxton, “the projections were that the RAM prices were going to drop, but you had to have a very strong stomach to be able to go up to the wall and pray that the door was going to open.” Warnock comments, “Most companies will only deal with present-day technology and known costs. The brilliance of Steve Jobs is that he will say, ‘There will be this chip coming out at that price point at that time, and I will design my product to use it.’” And indeed, when the LaserWriter was announced in January of 1985, 256K RAMs cost about $4 each and the printer could be priced at $6995. Today, some 40 companies have announced their equipment is compatible with PostScript and that their interpreters run faster and cost less than Adobe’s version. They cannot offer the same font library, but they say they have fonts and font algorithms as good as Adobe’s. At this writing, however, none of these companies had apparently shipped a PostScript clone to a customer, and they reportedly have found it harder to replicate Adobe’s work than they had anticipated. When they do finally ship, and if they can interpret 80 or 90 percent of PostScript programs, Adobe is resigned to facing “good old-fashioned American competition,” says Geschke. The company has no patents to defend, only copyrights and trade secrets, so if other companies can reproduce Adobe’s technology, it has no legal recourse. “The most we can do is to continue to improve our technology,” Geschke says. Adobe’s latest technical breakthrough, demonstrated in San Francisco in January, is a version of PostScript that controls images on a computer screen as well as on a printed page. Called Display PostScript, this product is the first to provide device-independent graphics for computer screens. Display PostScript, like the original PostScript printer protocol, had a nudge from Jobs. His new company, NeXT Inc., Palo Alto, Calif., worked with Adobe to develop it, and it will be the graphics standard for all NeXT’s computers. Digital Equipment has already licensed Display PostScript for its DEC Windows workstation architecture. If other major companies follow, Adobe could be well on the way to setting its second standard. Everything a programmer or user might want to know about the PostScript language is provided in “PostScript Language Tutorial and Cookbook“ and “PostScript Language Reference Manual,” both written by Adobe Systems Inc. and published by Addison Wesley Publishing Co. (New York, 1985). In addition, Adobe periodically publishes a newsletter, “Colophon,” with programming tips and news about PostScript products. Interpress, the page description language from Xerox Corp.’s Palo Alto Research Center (PARC) that preceded PostScript in the laboratory but followed it in the marketplace, is described in the June 1986 issue of IEEE’s magazine, Computer (pp. 72-77). For more information on Xerox PARC, see “Inside the PARC: the ‘information architects,’” Spectrum, October 1985, p. 62. “Window on PostScript” in MacWeek, Feb. 2, 1988, pp. 28-29, contains a discussion of competitors’ attempts to clone the language. Update April 2022: While most home and office printers rely on other page description languages these days, PostScript remains the choice of graphics artists and commercial printers for its ability to accurately produce complex images. And the ubiquitous Portable Document Format (PDF) is based on PostScript. Tekla S. Perry is a senior editor at IEEE Spectrum. Based in Palo Alto, Calif., she's been covering the people, companies, and technology that make Silicon Valley a special place for more than 40 years. An IEEE member, she holds a bachelor's degree in journalism from Michigan State University. The University of Texas professor co-invented discrete cosine transform Jae Jeong Hwang is a professor of IT convergence and communication engineering at Kunsan National University, in Korea. Zoran M. Milicevic is an assistant professor of telecommunications and IT at the University of Belgrade, in Serbia. Zoran S. Bojković is a professor of electrical engineering at the University of Belgrade. Wikipedia Kamisetty Ramamohan “K.R.” Rao died on 15 January 2021 at the age of 89. He co-invented the discrete cosine transform (DCT) technique, which is widely used in digital signal processing and data compression. A small band of believers triumphed after years of quietly plugging away Rodney Brooks is the Panasonic Professor of Robotics (emeritus) at MIT, where he was director of the AI Lab and then CSAIL. He has been cofounder of iRobot, Rethink Robotics, and Robust AI, where he is currently CTO. In 1997, Harvard Business School professor Clayton Christensen created a sensation among venture capitalists and entrepreneurs with his book The Innovator's Dilemma. The lesson that most people remember from it is that a well-run business can’t afford to switch to a new approach—one that ultimately will replace its current business model—until it is too late. One of the most famous examples of this conundrum involved photography. The large, very profitable companies that made film for cameras knew in the mid-1990s that digital photography would be the future, but there was never really a good time for them to make the switch. At almost any point they would have lost money. So what happened, of course, was that they were displaced by new companies making digital cameras. (Yes, Fujifilm did survive, but the transition was not pretty, and it involved an improbable series of events, machinations, and radical changes.)",1.0
"
        The Computers Who Brought ENIAC to Life
    ",https://spectrum.ieee.org/eniac-woman-programmers,2022-04-21,,"Internet guru Kathy Kleiman is excavating the stories of early woman programmers Programmers at Aberdeen Proving Grounds configure ENIAC’s function tables, which acted as a form of read-only memory. The Electronic Numerical Integrator and Computer—better known as ENIAC—became the world’s first programmable general-purpose electronic computer when it was completed in 1945. ENIAC’s hardware was designed by John Mauchly and J. Presper Eckert, but the programs it ran were largely the creation of a team of six women. For decades, these women were largely unknown, except only as unidentified figures in photographs of ENIAC. But as an undergraduate, Kathy Kleiman—who would later help found ICANN (Internet Corporation for Assigned Names and Numbers)—started looking into who they were. This weekend at the Vintage Computer Festival East in Wall, N.J., Kleiman will be screening her short documentary The Computers, about the programmers. In advance of her talk, IEEE Spectrum spoke to Kleiman about the ENIAC women and her fascination with them. How did these women come to be such a central part of the history of computing? Kathy Kleiman: During World War II, the army needed people to hand-calculate ballistics trajectories or artillery firing tables. And male mathematicians were running short. The Army relocated the project from rural Maryland to Philadelphia and went looking for women math majors in Philadelphia, which has a very high density of schools with coed universities and colleges and all-women schools. Later they would go across the country looking for women math majors to come to the Morse School of Electrical Engineering, which is where they located this project and where they hand-calculated ballistic trajectories using mechanical desktop calculators. But it took 30 to 40 hours to calculate a single trajectory for one set of weather conditions for one gun and one projectile, and the Army needed hundreds of trajectories per firing table. So in the dark days of the war, early 1943, when there was no end to the war in sight, they agreed to fund the experiment of a visionary guy who also happened to be at the Moore School at that time. His name was Dr. John Mauchly. He partnered with Presper Eckert, who was 23 years old at the time, a young engineering grad. They were yin and yang, a great combination. With Army funding, they built this machine that wasn’t supposed to work—18,000 vacuum tubes were never supposed to be able to work in concert. But they did it, a machine 8 feet tall and 80 feet long. But when they’re almost done, they’re like, “Wait a second.” Part of the Army contract was delivering a working ballistic trajectory calculated by the machine. So a mathematician and Army lieutenant at the proving ground called Herman Goldstein picks six out of the 80 to 100 women who’ve been calculating trajectories. They [Kathleen Antonelli, Jean Bartik, Betty Holberton, Marlyn Meltzer, Frances Spence, and Ruth Teitelbaum] are given the wiring diagrams and the block diagrams and told to figure it out so they can do the ballistics trajectory equation. The women don’t have security access to even see the actual computer, but they figure it out, doing what is now called direct programming. There’s looping, there’s conditional logic, and the women collectively mastered all this and made it perform the ballistic trajectory calculation that wound up becoming the climactic moment of demonstration day on 6 February 1946, when ENIAC was unveiled. Why did you start looking into their story? Kleiman: I was at Harvard. I was kind of a social theory major. I took computer science from the first classes that I took in college because I was already a programmer because of a Western Electric program when I was in high school. I also noticed that as the levels of the computer science classes went higher, the number of women dropped. And I knew about Ada Lovelace. I knew about Grace Hopper. Ada Lovelace was in the 19th century, then Grace Hopper in the 20th century. And one woman succeeding in computing per century didn’t make me feel warm and fuzzy, so I went looking for more. I found the pictures of ENIAC taken before demonstration day and given to the press and published across the country. These pictures are beautiful black-and-white pictures, and they have men and women in them; some of them just have women! But while some of the men, particularly Eckert and Mauchly, are named in the captions, none of the names of the women are in the captions. I wanted to know who they were. I was told by some computer historians at the time that they were models, and they didn’t look like models to me. I tracked them down, and they weren’t models; they were programmers. You’ll be showing your documentary at VCF East, which features interviews with four of the programmers conducted before they passed away, but you’ll also be releasing a book later this year, called Proving Ground? Kleiman: The documentary raised as many questions as it answered. So I was kind of persuaded to tell the rest of the story, and really sit down and talk about the incredible work, not just of the ENIAC programmers but of millions of women on the home front during World War II. It turned out that is not a story we know very well. I had always known women went to the factories, they went to the farms. I didn’t realize until I sat down in front of newspapers of the period and saw the ads that there was an enormous push for women with science and technology backgrounds. If you had the interest, the aptitude, and some training, these articles made it clear they’d teach you the rest, not just for the military but for industry. That was just a whole part of the story I’d never heard, that women also filled in these gaps in science, technology, and engineering. Stephen Cass is the special projects editor at IEEE Spectrum. He currently helms Spectrum's Hands On column, and is also responsible for interactive projects such as the Top Programming Languages app. He has a bachelor's degree in experimental physics from Trinity College Dublin. An almost Machiavellian plot pitted Fairchild against Texas Instruments In one corner stood the defending champion, Texas Instruments. In the other stood the challenger, Fairchild Semiconductor. The referee, judge, promoter, and only spectator was Polaroid. In contention was the contract for the electronics of Polaroid’s secret project—a pioneering product introduced in 1972 as the SX-70, a camera eventually purchased by millions of people. As the embodiment of truly automated instant photography, the SX-70 fulfilled a long-held dream of Edwin Land, founder of Polaroid Corp., Cambridge, Mass. Vital to this “point and shoot” capability was a new film—one that would develop while exposed to light and so eliminate the tear-away covers of previous Polaroid films. Also vital were sophisticated electronics to control all single lens reflex (SLR) camera functions, including flashbulb selection, exposure control, mirror positioning, start of print development, and ejection of print. These circuits were divided into three modules, one each for motor, exposure and logic, and flash control. At the final count, some 400 transistors were used.",0.0
"
        Protecting Privacy in Surveillance Video While Mining It for Data
    ",https://spectrum.ieee.org/surveillance-privacy,2022-04-19,,"A new technique may help defend privacy while permitting useful analysis of surveillance data Surveillance cameras have proliferated across the globe, raising concerns about privacy that have only deepened as machine-learning tools have now enabled automated video analysis on a massive scale. Now a new security system aims to defend privacy in a way that supports honest analysis of video footage while confounding malicious spying. There are now “hundreds of millions of surveillance cameras out there across the world,” notes Frank Cangialosi, a computer scientist at MIT and lead author on a study of the system. In the past, these cameras were occasionally monitored manually, if at all, and largely used for security purposes. But steady advances in artificial intelligence have now made it possible for computers to analyze this video data en masse. There are many applications for automated video analysis of surveillance footage, such as: helping health officials measure the proportion of people wearing masks; letting transportation departments monitor the density and flow of vehicles, pedestrians, and bicycles to figure out where to add sidewalks and bike lanes; and giving businesses better insight into shopping behavior for better planning of promotions. However, such mass surveillance poses the risk of intrusions on privacy at unprecedented scales. “Video analytics is an exciting potential area, but I think our community also has this huge responsibility to think carefully about how it could be misused and put equal effort towards addressing that,” Cangialosi says. Attempts to defend privacy against such technology often involve blurring out faces or covering them with black boxes. Those methods can prevent useful analysis of this video, while still not having the intended effect of preserving anonymity. “So, citizens aren’t going to feel protected, and analysts aren’t going to feel it’s useful enough for them,” Cangialosi says. “It doesn't satisfy anyone, which is why these approaches aren’t actually widely used in practice. And after thinking about it a bit, we realized that these are fundamental issues, so there’s this need for a totally different approach.” Now, Cangialosi and his colleagues have developed a new system called Privid that lets analysts examine video for statistical data without revealing personally identifiable information. “Privid might enable us to actually [make more productive use of] tons of footage from all of the cameras we already have around the world [and do so] in a safe way,” Cangialosi says. “They have tons of coverage and are very versatile, so I think there’s really a lot of potential.” Privid works by first accepting code from an analyst containing a query that triggers an automatic count of, say, the number of people wearing masks in a video feed and the density of the crowd. The system then breaks that video footage into segments and runs the code on each chunk. Instead of reporting the results back from each segment to the analyst, Privid aggregates the data and adds some noise to it before returning the results. The aim of Privid is to let analysts with honest queries get the details they want, while restricting access to raw surveillance data that would enable malicious actors to gain too much information. For example, when it comes to a video feed observing multiple city intersections, both an honest and a malicious query might claim to want to count the number of people that pass by each hour. Whereas the well-intentioned query from an urban-planning department might want to count pedestrian numbers to better plan crosswalks, the point of a query from someone with malicious intent might be to track a few specific people by looking out for their faces. Assuming Privid executes both the anodyne and malicious queries, the addition of a little noise does little to derail the analyst behind the honest query from obtaining the count of passersby as was claimed. That same noise, given how the malicious query was actually looking to identify a few specific people, would have a large, confounding effect on the attempt to misuse the data. Privid can also tell analysts how much error it adds to results, which honest analysts can account for in their research so that they can still detect valuable patterns and trends. Cangialosi stresses that “we are not encouraging surveillance.” With the idea of surveillance, he admits, “lots of negative things, understandably, immediately come to mind—the idea of being watched, Big Brother, and so on. But this is exactly what we want to prevent, full stop. Our fundamental idea of privacy is this idea that we should only be able to use cameras for things that don’t identify people. And there’s lots of examples of this that can benefit society, such as urban safety, public health, and so on.” A common technical question Cangialosi gets is “Does the privacy guarantee we provide only apply to a single camera?” “The short answer is no. The exact implications are a bit detailed, but the high-level point is that no matter how many cameras’ image feeds are in the system, and no matter how many cameras an analyst aggregates across, an individual will still be protected and can’t be tracked across location and time.” The researchers note that adding noise to the results may defend privacy, but does also make the analyses imperfect. Still, they noted that across a variety of videos and queries, Privid returned the right answer to queries between 70 and 99 percent of the time when its attention was trained on nonprivate systems. “Privid isn’t a panacea,” Cangialosi notes. “I think there are lots of use cases where privacy and utility aren’t really at odds, and so we can get a good balance of ensuring privacy without doing too much harm to accuracy or utility. Privid is great for these use cases.” On the other hand, he cautions, “there are some cases where privacy and utility really are fundamentally at odds. In security-critical applications, like locating a missing person or a stolen car, the entire point is to identify an individual,” Cangialosi says. In such cases, the solution may not be a technical one, “but rather good policies.” Cangialosi notes that while the scientists focused on the compromise between utility and privacy with Privid, they did not worry about computational efficiency. “An important next step is incorporating a lot of the optimizations the rest of the community has worked on towards making video analytics more efficient,” he says. “The challenge, of course, is doing it carefully, in such a way that we can still maintain the same formal privacy guarantees.” Future research can also explore different types of video feeds, such as dash cams and videoconference calls, as well as audio and other data. “These data sources represent even more untapped potential for analytics, but they’re obviously in some very privacy-sensitive scenarios,” Cangialosi says. “I think it'll be really exciting to expand the set of domains where we can have computers learn some important information that can help society, while also making sure [that data can’t be used to] harm anyone.” The scientists detailed their findings on 4 April at the USENIX Symposium on Networked Systems Design and Implementation Conference in Renton, Wash. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. And how Xerox lost it An undated exterior view of the Xerox Palo Alto Research Center (PARC) is shown in Palo Alto, California. In late 1969, C. Peter McColough, chairman of Xerox Corp., told the New York Society of Security Analysts that Xerox was determined to develop “the architecture of information” to solve the problems that had been created by the “knowledge explosion.” Legend has it that McColough then turned to Jack E. Goldman, senior vice president of research and development, and said, “All right, go start a lab that will find out what I just meant.”",0.0
Building a better society with better AI,https://www.technologyreview.com/2022/06/07/1053031/building-a-better-society-with-better-ai/,2022-06-07,"Artificial intelligence (AI) has the vast potential to offer innovations to improve every facet of society, from legacy engineering systems to healthcare to creative processes in arts and entertainment. In Hollywood, for example, studios are using AI to surface and measure bias in scripts—the very tools producers and writers need to create more equitable and…","In association withHewlett Packard Enterprise Artificial intelligence (AI) has the vast potential to offer innovations to improve every facet of society, from legacy engineering systems to healthcare to creative processes in arts and entertainment. In Hollywood, for example, studios are using AI to surface and measure bias in scripts—the very tools producers and writers need to create more equitable and inclusive media. However, AI is only as smart as the data it's trained on, and that data reflects real-life biases. To avoid perpetuating stereotypes and exclusivity, technologists are addressing equity and inclusion both in real-life and in their innovations. As technologists look to use AI to find human-centric solutions to optimize industry practices and everyday lives alike, it’s critical to be mindful of the ways that our innate biases can have unintended consequences. “As humans, we are highly biased,” says Beena Ammanath, the global head of the Deloitte AI Institute, and tech and AI ethics lead at Deloitte. “And as these biases get baked into the systems, there is very high likelihood of sections of society being left behind—underrepresented minorities, people who don't have access to certain tools—and it can drive more inequity in the world.” Projects that begin with good intentions -- to create equal outcomes or mitigate past inequities -- can still end up biased if systems are trained with biased data or researchers aren’t accounting for how their own perspectives affect lines of research. Thus far, adjusting for AI biases has often been reactive with the discovery of biased algorithms or underrepresented demographics emerging after the fact, says Ammanath. But, companies now have to learn how to be proactive, to mitigate these issues early on, and to take accountability for missteps in their AI endeavors. In AI, bias appears in the form of algorithmic bias. “Algorithmic bias is a set of several challenges in constructing an AI model,” explains Kirk Bresniker, chief architect at Hewlett Packard Labs and vice president at Hewlett Packard Enterprise (HPE). “We can have a challenge because we have an algorithm that is not capable of handling diverse inputs, or because we haven't gathered broad enough sets of data to incorporate into the training of our model. In either case, we have insufficient data.” Algorithmic bias can also come from inaccurate processing, data being modified, or someone injecting a false signal. Whether intentional or not, the bias results in unfair outcomes, perhaps privileging one group or excluding another altogether. As an example, Ammanath describes an algorithm designed to recognize different types of shoes such as flip flops, sandals, formal shoes, and sneakers. However, when it was released, the algorithm couldn’t recognize women’s shoes with heels. The development team was a group of fresh college grads—all male—who never thought of training it on the heels of women’s shoes. “This is a trivial example, but you realize that the data set was limited,” Ammanath said. “Now think of a similar algorithm using historical data to diagnose a disease or an illness. What if it wasn't trained on certain body types or certain genders or certain races? Those impacts are huge. Critically, she says If you don't have that diversity at the table, you are going to miss certain scenarios.” Simply obtaining more (and more diverse) datasets is a formidable challenge, especially as data has become more centralized. Data sharing brings up many concerns, not the least of which are security and privacy. “Right now, we have a situation where individual users have far less power than the vast companies that are collecting and processing their data,” says Nathan Schneider assistant professor of media studies at the University of Colorado Boulder. It is likely that expanded laws and regulations will eventually dictate when and how data can be shared and used. But, innovation doesn’t wait for lawmakers. Right now, the onus is on AI-developing organizations to be good data stewards, protecting individual privacy while striving to reduce algorithmic bias. Because technology is maturing so quickly, it’s impossible to rely on regulations to cover every possible scenario, says Deloitte’s Ammanath. “We are going to enter an era where you're balancing between being adherent to existing regulations and at the same time, self-regulating.” This kind of self-regulation means raising the bar for the entire supply chain of technologies that go into building AI solutions, from the data to the training to the infrastructure required to make those solutions possible. Further, companies need to create pathways for individuals across departments to raise concerns over biases. While it is unlikely that bias can be eliminated altogether, companies must regularly audit the efficacy of their AI solutions. Because of the highly contextual nature of AI, self-regulation will look different for each company. HPE, for example, established ethical AI guidelines. A diverse set of individuals from across the company spent nearly a year working together to establish the company’s principles for AI, and then vetted those principles with a broad set of employees to ensure they could be followed and that they made sense for the corporate culture. “We wanted to raise the general understanding of the issues and then collect best practices,"" says HPE’s Bresniker. “This is everyone’s job—to be literate in this area.” Technologists have reached a maturity with AI that has progressed from research to practical applications and value creation across all industries. The growing pervasiveness of AI across society means that organizations now have an ethical responsibility to provide robust, inclusive, and accessible solutions. This responsibility has prompted organizations to examine, sometimes for the first time, the data they’re pulling into a process. “We want people to establish that providence, that measurable confidence in the data that's going in,” says Bresniker. “They have that ability to stop perpetuating systemic inequalities and create equitable outcomes for a better future.” This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.",0.0
"The Download: Google’s AI cuteness overload, and America’s fight for gun control",https://www.technologyreview.com/2022/05/25/1052705/download-google-brain-ai-imagen-texas-shooting-gun-control/,2022-05-25,"<p>Plus: Our new section The Big Story looks back at MIT Technology Review's longform journalism</p>
","This is today's edition of The Download, our weekday newsletter that provides a daily dose of what's going on in the world of technology. The dark secret behind those cute AI-generated animal images Another month, another flood of weird, wonderful and cute images generated by an artificial intelligence. In April, OpenAI showed off its new picture-making neural network, DALL-E 2, which could produce remarkable high-res images of almost anything it was asked to. Now, just a few weeks later, Google Brain has revealed its own image-making AI, called Imagen. And it performs even better than DALL-E 2: it scores higher on a standard measure for rating the quality of computer-generated images and the pictures it produced were preferred by a group of human judges. But like OpenAI did with DALL-E, Google is going all in on cuteness. Both firms promote their tools with a series of pictures filled with anthropomorphic animals doing adorable things: a fuzzy panda dressed as a chef making dough, a corgi sitting in a house made of sushi, a teddy bear swimming the 400m butterfly at the Olympics—and it goes on. This cuteness hides a darker side to these tools, one that the public doesn’t get to see because it would reveal the ugly truth about how they are created. Read the full story—and see more pictures created by Imagen—here. —Will Douglas Heaven The must-reads I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology. 1 The Uvalde school shooting is strengthening urgent calls for gun controlLawmakers are pushing for an end to America’s gun violence epidemic. (The Guardian)+ President Biden asks when America is going to stand up to the gun lobby. (Politico)+ Texas has some of the most lenient gun laws in the country. (NYT $) 2 Social media is woefully underprepared to archive evidence of war crimesAnd pressures to remove gruesome content quickly is complicating the issue. (Coda Story) 3 Remote educational apps tracked children and hoarded their data They were then targeted with ads.(WP $) 4 The price of electronics is likely to riseAlong with the cost of anything else that relies on semiconductors. (CNBC)+ The great chip crisis threatens the promise of Moore’s Law. (MIT Technology Review) 5 Facial recognition is retraumatizing revenge porn survivorsPimEyes’ paid-for service creates a database of searchable images, some of which are sexually explicit. (CNN)+ A horrifying AI app swaps women into porn videos with a click. (MIT Technology Review) 6 Tech hasn’t delivered on its promise to make us more productiveAnd experts are divided over whether it ever will. (NYT $)+Tech alone is rarely enough to create significant benefits. (MIT Technology Review) 7 Social media challenges have become modern day relicsNo, TikTok dances don’t count. (The Atlantic $)+ There’s a fine line between following a trend and plagiarizing. (Vox) 8 Forging Australia’s digital driving licenses is scarily easyIt takes well under an hour. (Ars Technica)+ California should take note before it starts testing digital licenses. (LA Times) 9 Musicians claim they’re being pressured by their labels to go viral on TikTok But their objections have also been labeled as attempts to go viral. (Fast Company) 10 How you manage stress can lower your biological ageWhich is the age of your cells and organs, not your chronological “birth” age. (WSJ $)+ Aging clocks aim to predict how long you’ll live. (MIT Technology Review) Quote of the day “The only way to protect your customers’ location data from such outrageous government surveillance is to not keep it in the first place.” —An open letter from 42 Democrat lawmakers urges Google CEO Sundar Pichai to stop collecting location data that could be used to identify people seeking abortions, according to CNBC. The big story Artificial general intelligence: Are we close, and does it even make sense to try? A machine that could think like a person has been the guiding vision of AI research since the earliest days—and remains its most divisive idea. Artificial General Intelligence, or AGI, has become a common buzzword for human-like or superhuman AI, as well as a catchall for the hopes and fears surrounding an entire technology. But is it a reckless, misleading dream—or the ultimate goal? Read the full story. —Will Douglas Heaven We can still have nice things A place for comfort, fun and distraction in these weird times. (Got any ideas? Drop me a line or tweet 'em at me.)+ Cool fact of the day— dolphins can identify their friends by taste.+ France’s first fondue championship looks like an absolute riot.+ Some kind-hearted paddleboarders rescued a stranded deer found in a cave in England.+ The top item of clothing Americans bought during the pandemic? Turns out it was socks.+ Hold the front page—naps are officially good for you.",0.0
The dark secret behind those cute AI-generated animal images,https://www.technologyreview.com/2022/05/25/1052695/dark-secret-cute-ai-animal-images-dalle-openai-imagen-google/,2022-05-25,"<p>Google Brain has revealed its own image-making AI, called Imagen. But don't expect to see anything that isn't wholesome.</p>
","Another month, another flood of weird and wonderful images generated by an artificial intelligence. In April, OpenAI showed off its new picture-making neural network, DALL-E 2, which could produce remarkable high-res images of almost anything it was asked to. It outstripped the original DALL-E in almost every way. Now, just a few weeks later, Google Brain has revealed its own image-making AI, called Imagen. And it performs even better than DALL-E 2: it scores higher on a standard measure for rating the quality of computer-generated images, and the pictures it produced were preferred by a group of human judges. “We’re living through the AI space race!” one Twitter user commented. “The stock image industry is officially toast,” tweeted another. We are thrilled to announce Imagen, a text-to-image model with unprecedented photorealism and deep language understanding. Explore https://t.co/mSplg4FlsM and Imagen! A large rusted ship stuck in a frozen lake. Snowy mountains and beautiful sunset in the background. #imagen pic.twitter.com/96Vfo2kXJz Many of Imagen’s images are indeed jaw-dropping. At a glance, some of its outdoor scenes could have been lifted from the pages of National Geographic. Marketing teams could use Imagen to produce billboard-ready advertisements with just a few clicks. But as OpenAI did with DALL-E, Google is going all in on cuteness. Both firms promote their tools with pictures of anthropomorphic animals doing adorable things: a fuzzy panda dressed as a chef making dough, a corgi sitting in a house made of sushi, a teddy bear swimming the 400-meter butterfly at the Olympics—and it goes on. New @GoogleAI work:Input: ""Two meerkats sitting next to each other on top of a mountain and looking at the beautiful landscape. There is a mountain, a river lake, and fields of yellow flowers. There are hot air balloons in the sky.""#imagen https://t.co/JEgyNrcJjlOutput: https://t.co/uj4urjnZPF pic.twitter.com/I1zx8ZARBl There’s a technical, as well as PR, reason for this. Mixing concepts like “fuzzy panda” and “making dough” forces the neural network to learn how to manipulate those concepts in a way that makes sense. But the cuteness hides a darker side to these tools, one that the public doesn’t get to see because it would reveal the ugly truth about how they are created. Most of the images that OpenAI and Google make public are cherry-picked. We only see cute images that match their prompts with uncanny accuracy—that’s to be expected. But we also see no images that contain hateful stereotypes, racism, or misogyny. There is no violent, sexist imagery. There is no panda porn. And from what we know about how these tools are built—there should be. Not a single human face depicted in the hundreds of pictures in the paper, haha. I guess that's one way to eliminate concerns over representation bias. https://t.co/tKX8khoTDR It’s no secret that large models, such as DALL-E 2 and Imagen, trained on vast numbers of documents and images taken from the web, absorb the worst aspects of that data as well as the best. OpenAI and Google explicitly acknowledge this. Scroll down the Imagen website—past the dragon fruit wearing a karate belt and the small cactus wearing a hat and sunglasses—to the section on societal impact and you get this: “While a subset of our training data was filtered to removed noise and undesirable content, such as pornographic imagery and toxic language, we also utilized [the] LAION-400M dataset which is known to contain a wide range of inappropriate content including pornographic imagery, racist slurs, and harmful social stereotypes. Imagen relies on text encoders trained on uncurated web-scale data, and thus inherits the social biases and limitations of large language models. As such, there is a risk that Imagen has encoded harmful stereotypes and representations, which guides our decision to not release Imagen for public use without further safeguards in place.” It's the same kind of acknowledgement that OpenAI made when it revealed GPT-3 in 2019: “internet-trained models have internet-scale biases.” And as Mike Cook, who researches AI creativity at Queen Mary University of London, has pointed out, it’s in the ethics statements that accompanied Google’s large language model PaLM and OpenAI’s DALL-E 2. In short, these firms know that their models are capable of producing awful content, and they have no idea how to fix that. I feel like at some point in the last few years we somehow confused ""AI ethics"" with ""pointing at the mess you made and shrugging"".https://t.co/JEu2ngilEZ pic.twitter.com/mMbNQUzgXW For now, the solution is to keep them caged up. OpenAI is making DALL-E 2 available only to a handful of trusted users; Google has no plans to release Imagen. That’s fine if these were simply proprietary tools. But these firms are pushing the boundaries of what AI can do and their work shapes the kind of AI that all of us live with. They are creating new marvels, but also new horrors— and moving on with a shrug. When Google’s in-house ethics team raised problems with the large language models, in 2020 it sparked a fight that ended with two of its leading researchers being fired. Large language models and image-making AIs have the potential to be world-changing technologies, but only if their toxicity is tamed. This will require a lot more research. There are small steps to open these kinds of neural network up for widespread study. A few weeks ago Meta released a large language model to researchers, warts and all. And Hugging Face is set to release its open-source version of GPT-3 in the next couple of months. For now, enjoy the teddies.",0.0
"The Download: Clearview AI’s hefty fine, and countries’ monkeypox preparation",https://www.technologyreview.com/2022/05/24/1052661/download-clearview-ai-ico-fine-vaccine-monkeypox-preparation/,2022-05-24,"<p>Plus: Why data's borderless past is coming to an end</p>
","This is today's edition of The Download, our weekday newsletter that provides a daily dose of what's going on in the world of technology. The walls are closing in on Clearview AI Controversial facial recognition company Clearview AI has been fined more than $10 million by the UK’s data protection watchdog for collecting the faces of UK citizens from the web and social media. The firm was also ordered to delete all of the data it holds on UK citizens.The move by the UK’s Information Commissioner’s Office (ICO) is the latest in a string of high-profile fines against the company as data protection authorities around the world eye tougher restrictions on its practices.Clearview AI boasts one of the world’s largest databases of 20 billion images of people’s faces that it has scraped off the internet from publicly available sources, such as social media, without their consent. Clients such as police departments pay for access to the database to look for matches.But data protection authorities around the Western world have found this to be a clear violation of privacy. Now they are beginning to work together to clamp down—and fines may just be the beginning. Read the full story. —Melissa Heikkilä The must-reads I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.1 Production of the smallpox vaccine is being ramped upDozens of countries have inquired about supplies of the shot, which protects against monkeypox. (WSJ $)+ The US has more than 100 million doses stockpiled. (NYT $)+ Conspiracy theories blaming the US for the outbreak are circulating in China. (Bloomberg $)+ There’s no evidence to suggest the monkeypox virus is becoming more infectious. (NYT $) 2 Data’s wild west era is coming to an endWhile countries are divided on how widely it should be shared, everyone agrees on its value. (NYT $)+ GDPR hasn’t stopped data brokers from hoarding our information. (Wired $)3 Mark Zuckerberg’s grand plan to appear politically neutral backfiredHis $419 million donation fueled the false theory that the 2020 election was rigged. (Protocol)+ He’s also being sued over the Cambridge Analytica data scandal. (WP $)+ Meta will give researchers more information on political ad targeting. (NYT $)+ Facebooktroll farms reached 140 million Americans a month before the election. (MIT Technology Review)4 Marshes are struggling against rising water levelsWhile some plants are suffering, others will thrive—for now, at least. (Wired $)+ How rising groundwater caused by climate change could devastate coastal communities. (MIT Technology Review)5 Maybe we’re spreading disinformation about disinformationThe phrase has become such a catch-all, we’re losing sight of what it actually means. (Slate $) + How Facebook and Google fund global misinformation. (MIT Technology Review)6 Facebook’s customer service is notoriously terribleLeaving disgruntled users with no way to seek help for their problems. (WSJ $) 7 Humans aren’t going extinct any time soonBut our ability to adapt and learn from mistakes is crucial to our future survival. (CNET) 8 Mexico City’s gig economy is helping medical workers treat patientsAllowing them to carry out tests and vaccinations at home. (Rest of World)9 It’s time to break up with email 📧“If it’s important, they’ll get back to me” is a good philosophy to adopt. (WSJ $)10 Google’s text-to-image AI is pretty impressiveBut it isn’t quite as advanced as OpenAI. (TechCrunch)+ This horse-riding astronaut is a milestone in AI’s journey to make sense of the world. (MIT Technology Review) Quote of the day “You can totally make a fortune in crypto. I would never say you can't, but you are betting that you are going to be a better shark than all the sharks that built the shark pool.” —David Gerard, author, explains to Wired why the volatile nature of crypto means the odds are generally stacked against investors. We can still have nice things A place for comfort, fun and distraction in these weird times. (Got any ideas? Drop me a line or tweet 'em at me.) + A Nintendo DS has been spotted filming at a My Chemical Romance concert, though the footage it captured is…not great.+ Did you hate Matrix Resurrections? Here’s an explanation why.+ I love working out which tracks a song has sampled—this website is a comprehensive library explaining who’s sampled who.+ This Twitter user poses an excellent question.+ While this video of a seagull stealing an entire pizza is amazing, the behind the scenes is even better.+ A child actor who starred in Jaws has become a police chief on the Massachusetts island where most of the movie was filmed.+ This journey inside one of the deepest caves in the world is awe-inspiring, if a bit scary.",0.0
"The Download: DeepMind’s AI shortcomings, and China’s social media translation problem",https://www.technologyreview.com/2022/05/23/1052629/download-deepmind-ai-gato-china-social-media-translation-english-twitter/,2022-05-23,"<p>Plus: Ukraine's startup employees are getting back to work</p>
","This is today's edition of The Download, our weekday newsletter that provides a daily dose of what's going on in the world of technology. The hype around DeepMind's new AI model misses what's actually cool about it Earlier this month, DeepMind presented a new “generalist” AI model called Gato. The model can play the video game Atari, caption images, chat, and stack blocks with a real robot arm, the Alphabet-owned AI lab announced. All in all, Gato can do hundreds of different tasks.But while Gato is undeniably fascinating, in the week since its release some researchers have got a bit carried away.One of DeepMind’s top researchers and a coauthor of the Gato paper, Nando de Freitas, couldn’t contain his excitement. “The game is over!” he tweeted, suggesting that there is now a clear path from Gato to artificial general intelligence, or ‘AGI’, a vague concept of human or superhuman-level AI. The way to build AGI, he claimed, is mostly a question of scale: making models such as Gato bigger and better.Unsurprisingly, de Freitas’s announcement triggered breathless press coverage that Deepmind is “on the verge” of human-level artificial intelligence. This is not the first time hype has outstripped reality. Other exciting new AI models, such as OpenAI’s text generator GPT-3 and image generator DALL-E, have generated similar grand claims.For many in the field, this kind of feverish discourse overshadows other important research areas in AI. Read the full story. —Melissa Heikkilä The must-reads I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.1 Volunteers are translating Chinese social media posts into EnglishEven though the posts have passed China’s internet censorship regime, Beijing is unhappy. (The Atlantic $)+ WeChat wants people to use its video platform. So they did, for digital protests. (TR)2 Ukraine’s startup community is resuming business as usualMany workers are juggling their day jobs with after-hours war effort volunteering. (WP $)+ Russian-speaking tech bosses living in the US are cutting ties with pro-war workers. (NYT $)+ YouTube has taken down more than 9,000 channels linked to the war. (The Guardian)3 The Buffalo shooting highlighted the failings of tech’s anti-terrorism accordCritics say platforms haven’t done enough to tackle the root causes of extremism. (WSJ $)+ America has experienced more than 3,500 mass shootings since Sandy Hook. (WP $)4 Crypto appears to have an insider trading problemJust like the banking system its supporters rail against. (WSJ $)+ Christine Lagarde thinks crypto is worth “nothing.” (Bloomberg $)+ Crypto is weathering a bitter storm. Some still hold on for dear life. (TR)+ The crypto industry has lost around $1.5 trillion since November. (The Atlantic $)+ Stablecoin Tether has paid out $10 billion in withdrawals since the crash started. (The Guardian)5 The nuclear fusion industry is in turmoilIt isn’t even up and running yet, but fuel supplies are already running low. (Wired $)+ A hole in the ground could be the future of fusion power. (TR)+ The US midwest could be facing power grid failure this summer. (Motherboard)6 Big Tech isn’t worried about the economic downturnEven if it drops some of its market valuation along the way. (NYT $)+ But lawmakers are determined to rein them in with antitrust legislation. (Recode)+ Their carbon emissions are spiraling out of control, too. (New Yorker $)7 The US military wants to build a flying shipThe Liberty Lifer X-plane would be independent of fixed airfields and ports. (IEEE Spectrum) 8 We need to change how we recycle plasticThe good news is that the technology to overhaul it exists—it just needs refining. (Wired $)+ A French company is using enzymes to recycle one of the most common single-use plastics. (TR)9 Why you should treat using your phone like drinking wineStriking that delicate balance from stopping the positive tipping into negative. (The Guardian $) 10 Inside the wholesome world of internet knitting 🧶Its favorite knitter’s creations have gained a cult following. (Input)+ How a ban on pro-Trump patterns unraveled the online knitting world. (TR) Quote of the day “I like the instant gratification of making the internet better.” —Jason Moore, who is credited with creating more than 50,000 Wikipedia pages, tells CNN about his motivations for taking on the unpaid work. We can still have nice things A place for comfort, fun and distraction in these weird times. (Got any ideas? Drop me a line or tweet 'em at me.) + This site that randomly directs you to a livestream with zero viewers, is fascinating.+ The Goede Hoop Marimba Band playing Vivaldi’s Four Seasons is guaranteed to brighten up your day (Thanks Mike!)+ Jiaqi Wang’s illustrations and animations are so fun and vibrant.+ Why not try whipping up these classic takeaway dishes (aka fakeaways) when you want something comforting but kind of healthy?+ A gold signet ring that’s more than 3,000 years old has been returned to its rightful home in Greece, after being stolen during World War II.+ If you’ve been wondering what the less high-profile Harry Potter film cast members are up to, wonder no more.+ A convincing defense of reality TV’s All-Star seasons.",0.0
The hype around DeepMind’s new AI model misses what’s actually cool about it,https://www.technologyreview.com/2022/05/23/1052627/deepmind-gato-ai-model-hype/,2022-05-23,"<p>Some worry that the chatter about these tools is doing the whole field a disservice.</p>
","Earlier this month, DeepMind presented a new “generalist” AI model called Gato. The model can play Atari video games, caption images, chat, and stack blocks with a real robot arm, the Alphabet-owned AI lab announced. All in all, Gato can do 604 different tasks. But while Gato is undeniably fascinating, in the week since its release some researchers have gotten a bit carried away. One of DeepMind’s top researchers and a coauthor of the Gato paper, Nando de Freitas, couldn’t contain his excitement. “The game is over!” he tweeted, suggesting that there is now a clear path from Gato to artificial general intelligence, or AGI, a vague concept of human- or superhuman-level AI. The way to build AGI, he claimed, is mostly a question of scale: making models such as Gato bigger and better. Unsurprisingly, de Freitas’s announcement triggered breathless press coverage that DeepMind is “on the verge” of human-level artificial intelligence. This is not the first time hype has outstripped reality. Other exciting new AI models, such as OpenAI’s text generator GPT-3 and image generator DALL-E, have generated similarly grand claims. For many in the field, this kind of feverish discourse overshadows other important research areas in AI. That’s a shame, because Gato is an interesting step. Some models have started to mix different skills: DALL-E, for example, generates images from text descriptions. Others use a single training technique to learn to recognize pictures and sentences. And DeepMind's AlphaZero learned to play Go, chess, and shogi. But here’s the crucial difference: AlphaZero could only learn one task at a time. After learning to play Go, it had to forget everything before learning to play chess, and so on. It could not learn to play both games at once. This is what Gato does: it learns multiple different tasks at the same time, which means it can switch between them without having to forget one skill before learning another. It’s a small advance but a significant one. A machine that could think like a person has been the guiding vision of AI research since the earliest days—and remains its most divisive idea. The downside is that Gato doesn’t perform the tasks as well as models that can only do one thing. Robots still need to learn “common-sense knowledge” about how the world works from text, says Jacob Andreas, an assistant professor at MIT who specializes in artificial intelligence and natural-language and speech processing. This could come in handy in robots that could help people around the house, for example. “When you drop [a robot] into a kitchen and ask them to make a cup of tea for the first time, they know what steps are involved in making a cup of tea and in which cabinet tea bags are likely to be located,” says Andreas. Some external researchers were explicitly dismissive of de Freitas’s claim. “This is far from being ‘intelligent,’” says Gary Marcus, an AI researcher who has been critical of deep learning. The hype around Gato demonstrates that the field of AI is blighted by an unhelpful “triumphalist culture,” he says. The deep-learning models that often generate the most excitement about the potential to reach human-level intelligence make mistakes that “if a human made these errors, you’d be like, something’s wrong with this person,” Marcus says. “Nature is trying to tell us something here, which is this doesn’t really work, but the field is so believing its own press clippings that it just can’t see that,” he adds. Even de Freitas’s DeepMind colleagues Jackie Kay and Scott Reed, who worked with him on Gato, were more circumspect when I asked them directly about his claims. When asked whether Gato was heading toward AGI, they wouldn’t be drawn. “I don’t actually think it’s really feasible to make predictions with these kinds of things. I try to avoid that. It’s like predicting the stock market,” said Kay. Reed said the question was a difficult one: “I think most machine-learning people will studiously avoid answering. Very hard to predict, but, you know, hopefully we get there someday.” In a way, the fact that DeepMind called Gato a “generalist” might have made it a victim of the AI sector’s excessive hype around AGI. The AI systems of today are called “narrow,” meaning they can only do a specific, restricted set of tasks such as generate text. Some technologists, including some at DeepMind, think that one day humans will develop “broader” AI systems that will be able to function as well as or even better than humans. Though some call this artificial general intelligence, others say it is like ""belief in magic.“ Many top researchers, such as Meta’s chief AI scientist Yann LeCun, question whether it is even possible at all. Gato is a “generalist” in the sense that it can do many different things at the same time. But that is a world apart from a “general” AI that can meaningfully adapt to new tasks that are different from what the model was trained on, says MIT’s Andreas: “We’re still quite far from being able to do that.” Making models bigger will also not address the issue that models don’t have “lifelong learning,” which would mean that if taught something once, they would understand all the implications and use it to inform all the other decisions they make, he says. The hype around tools like Gato is harmful for the general development of AI, argues Emmanuel Kahembwe, an AI and robotics researcher and part of the Black in AI organization cofounded by Timnit Gebru. “There are many interesting topics that are left to the side, that are underfunded, that deserve more attention, but that’s not what the big tech companies and the bulk of researchers in such tech companies are interested in,” he says. Tech companies ought to take a step back and take stock of why they are building what they are building, says Vilas Dhar, president of the Patrick J. McGovern Foundation, a charity that funds AI projects “for good.” “AGI speaks to something deeply human—the idea that we can become more than we are, by building tools that propel us to greatness,” he says. “And that’s really nice, except it also is a way to distract us from the fact that we have real problems that face us today that we should be trying to address using AI.”",0.0
Powering the next generation of AI,https://www.technologyreview.com/2022/05/09/1051907/powering-the-next-generation-of-ai/,2022-05-09,"Ubiquitous computing has triggered an avalanche of data that is beyond human processing capabilities. AI technologies have emerged as the only viable way to turn this data into information. As more computing produces more data, more computing power is needed to power AI. Next generation AI will soon look to planetary-scale computing systems to further…","Presented byIntel Ubiquitous computing has triggered an avalanche of data that is beyond human processing capabilities. AI technologies have emerged as the only viable way to turn this data into information. As more computing produces more data, more computing power is needed to power AI. Next generation AI will soon look to planetary-scale computing systems to further fuel AI’s computational requirements. We’ll examine some of the technologies, such as neuromorphic and quantum computing, that will unlock the next step in performance that is intractable with current computing systems. Arun Subramaniyan joined Intel to lead the Cloud & AI Strategy team. Arun joined Intel from AWS, where he led the global solutions team for Machine Learning, Quantum Computing, High Performance Computing (HPC), Autonomous Vehicles, and Autonomous Computing at AWS. His team was responsible for developing solutions across all areas of HPC, quantum computing, and large-scale machine learning applications, spanning $1.5B+ portfolio. Arun founded and grew the global teams for Autonomous Computing and Quantum Computing Go-to-market and solutions at AWS and grew the businesses 2-3x. Arun’s primary areas of research focus are Bayesian methods, global optimization, probabilistic deep learning for large scale applications, and distributed computing. He enjoys working at the intersection of massively parallel computing and modeling large-scale systems. Before AWS, Arun founded and led the AI products team at GE’s Oil & Gas division and grew the digital products business successfully. He and his team developed deep learning-augmented hybrid analytics for all segments of the oil & gas industry. Arun led the development of the Digital Twin platform for GE at GE’s Global Research Center. The platform continues to enable several thousand engineers to build advanced models efficiently. The asset specific cumulative damage modeling techniques he and his team pioneered define the standard for industrial damage modeling. As a Six Sigma Master Black Belt, he developed advanced techniques and tools for efficiently modeling large scale systems like jet engine fleets, gas turbines in powerplants, and accelerated design times by 3-4X. Arun is a prolific researcher with a Ph.D. in Aerospace Engineering from Purdue University with 19 granted patents (54 filed), and 50+ international publications that have been cited more than 1000 times with an h-index of 13. He is also a recipient of the Hull Award from GE, which honors technologists for their outstanding technical impact. Elizabeth Bramson-Boudreau is the CEO and publisher of MIT Technology Review, the Massachusetts Institute of Technology’s independent media company. Since Elizabeth took the helm of MIT Technology Review in mid-2017, the business has undergone a massive transformation from its previous position as a respected but niche print magazine to a widely read, multi-platform media brand with a global audience and a sustainable business. Under her leadership, MIT Technology Review has been lauded for its editorial authority, its best-in-class events, and its novel use of independent, original research to support both advertisers and readers. Elizabeth has a 20-year background in building and running teams at world-leading media companies. She maintains a keen focus on new ways to commercialize media content to appeal to discerning, demanding consumers as well as B2B audiences. Prior to joining MIT Technology Review, Elizabeth held a senior executive role at The Economist Group, where her leadership stretched across business lines and included mergers and acquisitions; editorial and product creation and modernization; sales; marketing; and events. Earlier in her career, she worked as a consultant advising technology firms on market entry and international expansion. Elizabeth holds an executive MBA from the London Business School, an MSc from the London School of Economics, and a bachelor’s degree from Swarthmore College.",0.0
From data and AI aspirations to sustainable business outcomes,https://www.technologyreview.com/2022/05/09/1051919/from-data-and-ai-aspirations-to-sustainable-business-outcomes/,2022-05-09,"There are 3 common challenges that organizations face while transforming AI aspirations into scalable and intelligent solutions. Get an insider’s view of use case scenarios that illustrate real business and functional value through a proven framework and process toward sustainable digital transformation. About the speaker Vishal Kapoor, Vice President, Data and AI, Kyndryl Vishal Kapoor…","Presented by There are 3 common challenges that organizations face while transforming AI aspirations into scalable and intelligent solutions. Get an insider’s view of use case scenarios that illustrate real business and functional value through a proven framework and process toward sustainable digital transformation. Vishal Kapoor is the Kyndryl Applications, Data & AI practice leader. He leads the team of applications, data, and AI experts ensuring they are developing market-leading solutions and supporting customer teams with the technology and solutions customers need most. Vishal was previously with DXC Technology where he was the senior business leader for their Data & Analytics AI, Machine Learning, and IoT business. Leading a significant pivot in the company's strategy, he leveraged best-in-class partnerships with cloud-based next-gen platforms to implement scalable industrialized solutions for DXC customers. Vishal is a senior executive with experience in multiple geographies including Asia Pacific, India, and the Americas. He has deep experience in consulting across IT and Business Process Services with exposure to Product Management, Design Thinking, Marketing, Operations, and P&L leadership in diverse industries. Vishal has deep experience with companies such as Tech Mahindra, NTT Data (Previously Dell Services), and Wipro in driving large complex transformational initiatives for leading Fortune 500 organizations. Learn more about Kyndryl's Data & AI services.",0.0
"The Download: Meta’s AI giveaway, and abortion clinic data tracking",https://www.technologyreview.com/2022/05/04/1051755/download-meta-language-ai-giveaway-abortion-clinic-data-tracking-sold/,2022-05-04,"<p>Plus: Data brokers are selling the location information of people visiting abortion clinics</p>
","This is today's edition of The Download, our weekday newsletter that provides a daily dose of what's going on in the world of technology. Open to ideas: Meta’s AI lab has created a massive new language model, and in an unprecedented move for Big Tech, it is giving it away to researchers—together with details about how it was built. Large language models—powerful programs that can generate paragraphs of text and mimic human conversation—have become one of the hottest trends in AI in the last couple of years. But they have deep flaws, parroting misinformation, prejudice, and toxic language. Wider scrutiny: Meta’s decision represents the first time that a fully trained large language model will be made available to any researcher who wants to study it. In theory, putting more people to work on the problem should help. So far, the wider research community has been shut out, with large language models solely the domain of rich tech firms. Improved discoveries: The news has been welcomed by many as a win for transparency. However, some researchers question why large language models are being built at all, given their potential for harm. Read the full story. —Will Douglas Heaven I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology. 1 Brokers are selling the location data of people who visit abortion clinicsThis is an old problem, but it’s taking on a new urgency in the light of the potential repeal of Roe v Wade. (Motherboard)+ Overturning Roe v Wade could force tech companies to help states punish people seeking abortions. (Protocol)+ It could also criminalize women making even cursory online searchers linked to abortion. (Gizmodo)+ Amazon’s $4,000 abortion travel benefit does not extend to many of its poorest workers. (Motherboard) 2 Elon Musk says he may charge businesses to use Twitter But whether we should take this suggestion seriously is anyone’s guess. (BBC)+ Twitter’s testing a new feature to share tweets with a select number of people. (Mac Rumors)+ NFT scammers appear to be targeting Twitter users with blue ticks. (The Atlantic $)+ It’s getting harder to know what Twitter’s users actually want. (The Atlantic $)+ Maybe it’s Neuralink that Musk should be concentrating on, not Twitter. (WP $) 3 India and Pakistan’s heatwave is testing the limits of survivabilityLast month, New Delhi saw seven consecutive days that topped 104°F (40°C.) (CNN)+ Climate change is making these brutal heat waves worse. (TR) 4 A Shanghai resident was mistakenly declared dead and put in a body bagThe horrifying incident highlights the pressures of the city’s grueling lockdown. (The Guardian)+ Shanghai’s lockdown is giving China’s online grocery apps a second chance. (TR) 5 NFT sales have plummeted But does it mean the market is evaporating, or just that the initial hype cycle is over? (WSJ $) 6 Sensor-equipped offices may be smart, but they’re vulnerable to hacksWhich is a major problem if your staff are locked out from their workplace. (WSJ $) 7 Black voters are being targeted by online misinformation campaignsAnd the volume of lies is likely to intensify during the midterm elections. (NBC)+ What you need to know about the midterms. (NYT $) 8 Start-ups manned by former executives are trying to fix Big Tech’s mistakesBy employing “conflict coordinators” to crack down on rule violations. (WSJ $) 9 The crypto executive who wasn’tA cautionary tale for entrepreneurs who ask too few questions. (NYT $)+ Crypto lobbyists are following the money—to Albany. (Fast Company $)10 A sailor’s love song went viral on TikTok 64 years after he wrote itThe timeless melody has charmed a new generation of listeners. (WP $)+ Meanwhile, StudyTok is still enormously popular. (BBC) “The phone is everything to me.” —Paola, an inmate in an Argentinian prison, tells Rest of World about the difference having access to a mobile phone has made to her life. A place for comfort, fun and distraction in these weird times. (Got any ideas? Drop me a line or tweet 'em at me.)+ Happy Star Wars day to all who celebrate! This potted history of the phrase “May the 4th be with you” is really interesting.+ When TLDR doesn’t mean what you thought it meant.+ The latest episode of podcast Reply All, which involved host Emmanuel calling everyone in his phone, brought me out in a cold sweat.+ These two galaxies have been in the process of merging for a staggering 400 million light years.+ If you’ve been dreaming of inbox zero, this could be exactly the motivation you need.+ In New York, even an abandoned mansion costs $22 million.+ This Pokémon rug is perfection.",0.0
Meta has built a massive new language AI—and it’s giving it away for free,https://www.technologyreview.com/2022/05/03/1051691/meta-ai-large-language-model-gpt3-ethics-huggingface-transparency/,2022-05-03,"<p><span style=""font-weight: 400"">Facebook’s parent company is inviting researchers to pore over and pick apart the flaws in its version of GPT-3</span></p>
","Meta’s AI lab has created a massive new language model that shares both the remarkable abilities and the harmful flaws of OpenAI’s pioneering neural network GPT-3. And in an unprecedented move for Big Tech, it is giving it away to researchers—together with details about how it was built and trained. “We strongly believe that the ability for others to scrutinize your work is an important part of research. We really invite that collaboration,” says Joelle Pineau, a longtime advocate for transparency in the development of technology, who is now managing director at Meta AI. Meta’s move is the first time that a fully trained large language model will be made available to any researcher who wants to study it. The news has been welcomed by many concerned about the way this powerful technology is being built by small teams behind closed doors. “I applaud the transparency here,” says Emily M. Bender, a computational linguist at the University of Washington and a frequent critic of the way language models are developed and deployed. “It’s a great move,” says Thomas Wolf, chief scientist at Hugging Face, the AI startup behind BigScience, a project in which more than 1,000 volunteers around the world are collaborating on an open-source language model. “The more open models the better,” he says. Large language models—powerful programs that can generate paragraphs of text and mimic human conversation—have become one of the hottest trends in AI in the last couple of years. But they have deep flaws, parroting misinformation, prejudice, and toxic language. In theory, putting more people to work on the problem should help. Yet because language models require vast amounts of data and computing power to train, they have so far remained projects for rich tech firms. The wider research community, including ethicists and social scientists concerned about their misuse, has had to watch from the sidelines. Meta AI says it wants to change that. “Many of us have been university researchers,” says Pineau. “We know the gap that exists between universities and industry in terms of the ability to build these models. Making this one available to researchers was a no-brainer.” She hopes that others will pore over their work and pull it apart or build on it. Breakthroughs come faster when more people are involved, she says. Meta is making its model, called Open Pretrained Transformer (OPT), available for non-commercial use. It is also releasing its code and a logbook that documents the training process. The logbook contains daily updates from members of the team about the training data: how it was added to the model and when, what worked and what didn’t. In more than 100 pages of notes, the researchers log every bug, crash, and reboot in a three-month training process that ran nonstop from October 2021 to January 2022. With 175 billion parameters (the values in a neural network that get tweaked during training), OPT is the same size as GPT-3. This was by design, says Pineau. The team built OPT to match GPT-3 both in its accuracy on language tasks and in its toxicity. OpenAI has made GPT-3 available as a paid service but has not shared the model itself or its code. The idea was to provide researchers with a similar language model to study, says Pineau. OpenAI declined an invitation to comment on Meta’s announcement. Google, which is exploring the use of large language models in its search products, has also been criticized for a lack of transparency. The company sparked controversy in 2020 when it forced out leading members of its AI ethics team after they produced a study that highlighted problems with the technology. So why is Meta doing this? After all, Meta is a company that has said little about how the algorithms behind Facebook and Instagram work and has a reputation for burying unfavorable findings by its own in-house research teams. A big reason for the different approach by Meta AI is Pineau herself, who has been pushing for more transparency in AI for a number of years. Tech giants dominate research but the line between real breakthrough and product showcase can be fuzzy. Some scientists have had enough. Pineau helped change how research is published in several of the largest conferences, introducing a checklist of things that researchers must submit alongside their results, including code and details about how experiments are run. Since she joined Meta (then Facebook) in 2017, she has championed that culture in its AI lab. “That commitment to open science is why I’m here,” she says. “I wouldn’t be here on any other terms.” Ultimately, Pineau wants to change how we judge AI. “What we call state-of-the-art nowadays can’t just be about performance,” she says. “It has to be state-of-the-art in terms of responsibility as well.” Still, giving away a large language model is a bold move for Meta. “I can’t tell you that there’s no risk of this model producing language that we’re not proud of,” says Pineau. “It will.” Margaret Mitchell, one of the AI ethics researchers Google forced out in 2020, who is now at Hugging Face, sees the release of OPT as a positive move. But she thinks there are limits to transparency. Has the language model been tested with sufficient rigor? Do the foreseeable benefits outweigh the foreseeable harms—such as the generation of misinformation, or racist and misogynistic language? “Releasing a large language model to the world where a wide audience is likely to use it, or be affected by its output, comes with responsibilities,” she says. Mitchell notes that this model will be able to generate harmful content not only by itself, but through downstream applications that researchers build on top of it. Meta AI audited OPT to remove some harmful behaviors, but the point is to release a model that researchers can learn from, warts and all, says Pineau. “There were a lot of conversations about how to do that in a way that lets us sleep at night, knowing that there’s a non-zero risk in terms of reputation, a non-zero risk in terms of harm,” she says. She dismisses the idea that you should not release a model because it’s too dangerous—which is the reason OpenAI gave for not releasing GPT-3’s predecessor, GPT-2. “I understand the weaknesses of these models, but that’s not a research mindset,” she says. Hundreds of scientists around the world are working together to understand one of the most powerful emerging technologies before it’s too late. Bender, who coauthored the study at the center of the Google dispute with Mitchell, is also concerned about how the potential harms will be handled. “One thing that is really key in mitigating the risks of any kind of machine-learning technology is to ground evaluations and explorations in specific use cases,” she says. “What will the system be used for? Who will be using it, and how will the system outputs be presented to them?” Some researchers question why large language models are being built at all, given their potential for harm. For Pineau, these concerns should be met with more exposure, not less. “I believe the only way to build trust is extreme transparency,” she says. “We have different opinions around the world about what speech is appropriate, and AI is a part of that conversation,” she says. She doesn’t expect language models to say things that everyone agrees with. “But how do we grapple with that? You need many voices in that discussion.”",1.0
"The Download: Trolling text scammers, and China’s social media censorship",https://www.technologyreview.com/2022/06/20/1054466/download-trolling-text-scammers-spam-china-social-media-censorship/,2022-06-20,"<p>Plus: Crypto's value has fallen by more than two-thirds in seven months</p>
","This is today's edition of The Download, our weekday newsletter that provides a daily dose of what's going on in the world of technology. The people using humor to troll their spam texts The other night, I received a mysterious WhatsApp message. “Dr. Kevin?” it began, the question mark suggesting the sender felt bad for interrupting my evening. “My puppy is very slow and won’t eat dog food. Can you make an appointment for me?” I was mystified. My name is not Kevin, I am not a veterinarian, and I was in no position to help this person and their puppy. I nearly typed out a response saying “Sorry, wrong number” when I realized this was probably a scam to get me to confirm my number. I didn’t respond, but many others who received similar texts have. Some are even throwing it back at their spammers by spinning wild tales and sending hilarious messages to frustrate whoever is on the other side. They’re fighting back with snark, and in some cases posting screenshots of their conversations online. Experts don’t recommend responding like this. But it is cathartic and funny. Read the full story. —Tanya Basu China wants all social media comments to be pre-reviewed before publishing The news: On June 17, China’s internet regulator Cyberspace Administration of China (CAC) published a draft update on how platforms and creators should handle online comments. One line stands out: all online comments would have to be pre-reviewed before being published.How would it work? The provisions cover many types of comments, including anything from forum posts, replies, messages left on public message boards, and “bullet chats” (an innovative way that video platforms in China use to display real-time comments on top of a video). All formats, including texts, symbols, GIFs, pictures, audio, and videos, fall under this regulation.What does it mean? Users and observers are worried that the move could be used to further tighten freedom of expression in China. While Beijing is constantly refining its controls over social media, the vagueness of the latest revisions makes people worry that the government may ignore practical challenges, forcing platforms to hire a vast army of censors. Read the full story. —Zeyi Yang The must-reads I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology. 1 Crypto’s value is still plummeting 📉It’s fallen by more than two-thirds since November, but purists are unfazed. (WSJ $)+ Bitcoin fell below $20,000 for the first time since last November over the weekend. (FT $)+ Investors are nervously watching stablecoin Tether to see what happens next. (NYT $) + Crypto insurance sounds like a good idea right about now. (Vox) 2 The timeless virality of JuneteenthBecause freedom from slavery is something we can all agree on, regardless of political and religious afiliations. (Wired $)+ It’s been an awful year for the politics of race in America. (NY Mag) 3 Ambushing a comet is risky business ☄️But it’ll be worth it if it gives us our first real glimpse of a primordial body. (Nature)+ Astronomers wrongly thought comet Borisov was pretty boring. (MIT Technology Review)+ The Pentagon is exploring using SpaceX rockets to thwart future threats. (The Intercept)+ When is a black hole not a black hole? (Inverse) 4 How thousands of seabound robots are combating climate changeBy spending 90% of their time 1,000 meters below the ocean’s surface. (Spectrum IEEE)+ Why heat pumps are emerging as a key decarbonizing tool. (Protocol)+ UN climate report: Carbon removal is now “essential.” (MIT Technology Review)+ A Peruvian fishing community is still suffering, five months after an oil spill. (Hakai Magazine) 5 AI can do so much more than convince us it’s sentient And yet, we keep falling into the trap of missing the bigger picture. (The Atlantic $)+ We’re missing the point of the Turing test, too. (WP $)+ What the history of AI tells us about its future. (MIT Technology Review) 6 Anti-vaxx conspiracies are a global problemSpreading far wider than their American roots. (Slate $) 7 Can a steak made from recycled carbon dioxide ever taste good?It takes just a few days to make an ‘air steak,’ compared to the years it takes to raise and nurture a cow. (Neo.Life)+ Why oat milk companies may have to stop marketing their goods as ‘milk.’ (Slate $) + Your first lab-grown burger will be “blended”. (MIT Technology Review) 8 Why Peter Thiel unfriended FacebookAnd what’s next for the billionaire with a penchant for crypto. (WP $)+ Facebook is going to be a very different place without Sheryl Sandberg, too. (The Atlantic $) 9 How Dril’s influence spread beyond Weird TwitterThe platform’s court jester has infiltrated the mainstream. (New Yorker $) 10 What it’s like to become the worst person on the internetAnd another case of why putting images in the public domain can backfire. (The Guardian) Quote of the day “Are we going to bow our heads for Jeff Bezos just to give him his pleasure boat?” — Paul van de Laar, a professor at the Erasmus University Rotterdam, is infuriated by the Amazon founder’s request to dismantle part of the city's bridge to facilitate his superyacht, he tells the Financial Times. The big story This company delivers packages faster than Amazon, but workers pay the price June 2021 Early one morning in October 2020, 27-year-old Jang Deok-joon came home after working his overnight shift at South Korean e-commerce giant Coupang and jumped into the shower. He had worked at the company’s warehouse in the southern city of Daegu for a little over a year, hauling crates full of items ready to be shipped to delivery hubs. When he didn’t come out of the bathroom for over an hour and a half, his father opened the door to find him unconscious and curled in a ball in the bathtub, his arms tucked tightly into his chest. He was rushed to the hospital, but with no pulse and failing to breathe on his own, doctors pronounced him dead at 9:09 a.m. The coroner ruled that he had died from a heart attack. Jang was the third Coupang worker to die that year, adding to growing concern about the nature of the company’s success. And it has been astoundingly successful: rising to become South Korea’s third-largest employer in just a few years, harnessing a vast network of warehouses, 37,000 workers, a fleet of drivers, and a suite of AI-driven tools to take a commanding position in South Korea's crowded ecommerce market.Coupang’s proprietary AI algorithms calculate everything from the most efficient way to stack packages in delivery trucks, to the precise route and order of deliveries for drivers. In warehouses, AI anticipates purchases and calculates shipping deadlines for outbound packages, allowing it to promise delivery in less than a day for millions of items. Such innovations are why Coupang confidently bills itself as the “future of ecommerce,” and were the driving force behind its recent launch on Nasdaq—the biggest US IPO by an Asian company since Alibaba in 2014. But what does all this innovation and efficiency mean for the company’s workers? Read the full story. —Max S. Kim We can still have nice things A place for comfort, fun and distraction in these weird times. (Got any ideas? Drop me a line or tweet 'em at me.)+ Happy birthday to the one and only Brian Wilson, who turns 80 years old today. Out of all his incredible tunes, this one may just be the best.+ A total mystery: how did a UK trash can travel more than 1,900 kilometers to Ukraine?+ What a relief—Denmark and Canada’s polite ‘whisky war’ has finally been resolved.+ This Rage Against the Machine performance played on dog toys is a masterpiece.+ Here’s a selection of dresses we wouldn’t mind Kim Kardashian ruining next.",0.0
Why people are trolling their spam texts,https://www.technologyreview.com/2022/06/20/1054435/people-trolling-spam-texts/,2022-06-20,"<p>Our phones are being inundated with text scams. Some people are using humor to fight back.</p>
","The other night, I received a mysterious WhatsApp message. “Dr. Kevin?” it began, the question mark suggesting the sender felt bad for interrupting my evening. “My puppy is very slow and won’t eat dog food. Can you make an appointment for me?” I was mystified. My name is not Kevin, I am not a veterinarian, and I was in no position to help this person and their puppy. I nearly typed out a response saying “Sorry, wrong number” when I realized this was probably a scam to get me to confirm my number. I did not respond, but many others who received similar texts have. Some are even throwing it back at their spammers by spinning wild tales and sending hilarious messages to frustrate whoever is on the other side. They’re fighting back with snark, and in some cases posting screenshots of their conversations online. Spam texts are on the rise, and so are the number of people who are striking back through “scambaiting,” which refers to “the act of wasting an offender’s time,” says Jack Whittaker, a PhD student in sociology at the University of Surrey who is studying the phenomenon. However, experts say responding defeats the point, as it opens a person up to even more spam texts. Spam texts seeking to scam their recipients into giving up valuable information are not new. Some of the earliest digital spam was sent via email chain letters, the most notorious being for scams in which someone impersonating a Nigerian prince claimed to need the receiver’s help in depositing a large sum of money. Once smartphones became common, scammers switched to texting. And in 2022, spam texts are much more personal. Often they mimic a misdirected text, perhaps addressing the receiver by the wrong name or using a generic first line (“How’s it going” or “I had fun tonight!” are common) to prompt a response. If you’ve received any such messages lately, you’re not alone. “There’s been an incredible spike in spam texts,” says J. Michael Skiba, a professor at Colorado State University who specializes in cybercrime and international financial fraud. Globally, 90 billion of them were sent last year, he says; in the US, 47 billion spam texts were sent from January to October 2021, up 55% from that same period in 2020. According to RoboKiller, a spam blocking firm, scam texts led to $86 million in losses in the US alone in 2020. “People are just bombarded with these,” Skiba says. Skiba says texting has several advantages over email from a scammer’s perspective—a note from a phone number raises less suspicion than one from a sketchy email address, and the casual nature of texting makes grammatical errors less noticeable. Many people also feel a very human urge to respond to a text. “It’s a psychological trick in that you know the text is not correct, but it appeals to your desire to help and say, ‘You’ve got the wrong number,’” Skiba says. The person on the other side, however, is most likely working with an organized group of scammers in a call center and hoping you say exactly that. A single response is enough for a scammer to verify that a phone number is real. That response leads to a domino effect that could invite even more spam texts to your phone. Ultimately, scammers are looking to at least verify your number to potentially sell it to other groups; getting your personal information is a sweet bonus. “I would 100% recommend not responding at all,” Skiba says. But a scroll through Twitter, Reddit, Instagram, and TikTok shows that people aren’t taking that advice. Instead, many are engaging with spam texters and posting their conversations for the world to see. Gabriel Bosslet, an associate professor of medicine in Indianapolis, decided to mess with a recent spam texter by firing off increasingly outlandish replies. He’s been doing this kind of thing since the early 2000s, when he started writing back to mysterious emails that were clearly Nigerian prince scams. Once it’s clear he’s corresponding with a scammer, Bosslet goes into troll mode, fabricating fanciful stories and characters—the more bizarre, the better. “None of it is true,” he says. “I just make it all up.” I like having fun with spam texts pic.twitter.com/izU7i4VDtR Asked what his goal is in these conversations, Bosslet says it’s just to connect and interact with a stranger. He brings up the example of Wanda Dench, a grandmother who accidentally texted then-17-year-old Jamal Hinton an invitation to Thanksgiving dinner that has turned into a sweet annual tradition. “I realize that’s super odd, but I am open to some sort of interaction like that,” he says. Jason Tanamor, an author from Portland, Oregon, has also started texting spammers back. And, like Bosslet, he isn’t trying to reform anyone. “I just try to get them to say ‘deez nuts’ because it makes me giggle,” Tanamor says. For him, chatting up a spammer can be fun; if he has time, he simply tries to keep the conversation going for as long as he can. A post shared by Jason Tanamor is writing a pilot. (@jasontanamor) Neither Bosslet nor Tanamor was aware that answering spam texts probably verified their number, allowing their spammers to sell it to other spammers—resulting in even more spam texts. But neither cares. For them, messaging back with outlandish jokes is a form of entertainment. And both expressed empathy for the people on the other side of the phone. But others have a more vengeful approach. Whittaker at the University of Surrey says some people have taken scambaiting to extremes, joining online forums where they create elaborate hoaxes to trap the perpetrators. That can be dangerous, he says. “Scambaiting can also [lead to] hacking into an offender’s computer as a form of public entertainment,” he says. That’s problematic, possibly exposing people’s private information, and it’s also illegal, despite the moral high ground scambaiters may claim. Three top livestreaming personalities on the platform Taobao commanded legions of fans who bought billions of dollars’ worth of goods—until they suddenly went dark. Whittaker cites Jim Browning, the alias of a YouTuber and software engineer who has used scambaiting to delete stolen files from call centers involved in spam texts. Other scammers who have been exposed by people like Browning have retaliated by swatting scambaiters (making a false crime report to call out law enforcement) or luring them to dangerous locales. “Scambaiting activities can become quite radical,” Whittaker says. “Also, scammers wise up to these tactics quickly, so wasting an offender’s time can actually teach them to become more wise to the efforts to waste their time.” It’s a dilemma for people who mess with a scammer. It can be satisfying, if only as a form of rebellion against an annoying modern intrusion. But that rebellion can be costly, in terms of both the time it takes and the risk of setting off an avalanche of future spam texts that could, if scambaiters fall for them, put them at risk of financial or personal ruin. The US Federal Trade Commission and consumer advocates have attempted to fight back with Do Not Call registries and efforts to stop spam texts at the network level, but spammers are constantly evolving their tactics to bypass these laws. That can make it feel as though there’s only one way to handle the frustrating situation: with a joke about “deez nuts.”",0.0
Now China wants to censor online comments,https://www.technologyreview.com/2022/06/18/1054452/china-censors-social-media-comments/,2022-06-18,"<p>A draft update of rules would dramatically increase the power of China’s censorship machine, but platforms will pay the price.</p>
","China is fine-tuning its censorship machine, this time proposing changes in how to regulate the billions of online comments posted in the country every day. On June 17, the internet regulator Cyberspace Administration of China (CAC) published a draft update on the responsibilities of platforms and content creators in managing online comments. One line stands out: all online comments would have to be pre-reviewed before being published. Users and observers are worried that the move could be used to further tighten freedom of expression in China. The new changes affect Provisions on the Management of Internet Post Comments Services, a regulation that first came into effect in 2017. Five years later, the Cyberspace Administration wants to bring it up to date. “The proposed revisions primarily update the current version of the comment rules to bring them into line with the language and policies of more recent authority, such as new laws on the protection of personal information, data security, and general content regulations,” says Jeremy Daum, a senior fellow at Yale Law School’s Paul Tsai China Center. The provisions cover many types of comments, including anything from forum posts, replies, messages left on public message boards, and “bullet chats” (an innovative way that video platforms in China use to display real-time comments on top of a video). All formats, including texts, symbols, GIFs, pictures, audio, and videos, fall under this regulation. There’s a need for a stand-alone regulation on comments because the vast number makes them difficult to censor as rigorously as other content, like articles or videos, says Eric Liu, a former censor for Weibo who’s now researching Chinese censorship at China Digital Times. “One thing everyone in the censorship industry knows is that nobody pays attention to the replies and bullet chats. They are moderated carelessly, with minimum effort,” Liu says. But recently, there have been several awkward cases where comments under government Weibo accounts went rogue, pointing out government lies or rejecting the official narrative. That could be what has prompted the regulator’s proposed update. Chinese social platforms are currently on the front lines of censorship work, often actively removing posts before the government and other users can even see them. ByteDance famously employs thousands of content reviewers, who make up the largest number of employees at the company. Other companies outsource the task to “censorship-for-hire” firms, including one owned by China’s party mouthpiece People’s Daily. The platforms are frequently punished for letting things slip. Beijing is constantly refining its social media control, mending loopholes and introducing new restrictions. But the vagueness of the latest revisions makes people worry that the government may ignore practical challenges. For example, if the new rule about mandating pre-publish reviews is to be strictly enforced—which would require reading billions of public messages posted by Chinese users every day—it will force the platforms to dramatically increase the number of people they employ to carry out censorship. The tricky question is, no one knows if the government intends to enforce this immediately. One specific change about “先审后发,” a censoring practice some Chinese social media platforms use to review content before it’s even published, has particularly caught people’s attention. On Weibo, the popular Twitter-like service, such stricter control measures are currently applied only to accounts that have violated content censorship rules before, or when there’s an ongoing heated discussion about a sensitive topic. The 2017 version limited such actions to “comments under news information,” so it didn’t need to be applied universally. But the new update takes out that restriction. On social media, some Chinese users are worried that this means the practice can be expanded to cover every single comment online. Under one Weibo post about the change, the most liked comment says, “Is this restriction necessary? If only there’s a guarantee it won’t be abused.” That is an extreme interpretation of the proposed change, says Liu, because censoring every comment would incur astronomical costs to social media platforms. It’s unlikely Beijing will go so far to enforce blanket pre-publish censorship, but Liu says the revisions are more likely intended to force platforms to take more responsibility in moderating the comments section, which has traditionally been ignored. Whether there is a pre-publish censorship system in place can determine where online social protests break out. In April, a video about the Shanghai covid lockdown went viral on WeChat Channels but not Douyin, the Chinese version of TikTok—partly because the latter platform reviews every video before it’s published, while the former didn’t at the time. The regulator is now seeking public comments on the proposed revisions until July 1, 2022, and they may not take effect for many months. Right now, discussions about how strictly they will be enforced are only speculative. But it’s clear that China is identifying the Great Firewall’s loopholes and updating its regulations to address them. The most recent changes are “unapologetically part of China's continued expansion of content regulations beyond mainstream media to now cover user content generated through comments and other interactive features,” says Daum. The changes will also expand who can censor online comments. CAC now asks that platforms share the power of censoring comments with content creators—in Chinese internet lingo, “public account operators.” Currently, government-affiliated accounts are already empowered to do this on sites like Weibo. If this revision becomes law, creators will also become part of the censorship machine, responsible for identifying “illegal or negative” content and reporting it. “Although China’s internet is one of the most censored in the world, there is still some space for discussing sensitive topics. People can play a clever cat-and-mouse game with censors and make creative adjustments once posts are censored,” says William Nee, research and advocacy coordinator at Chinese Human Rights Defenders. “However, the new system could make that next to impossible and tighten the already limited space for freedom of expression on sensitive topics even further.”",0.0
Inside the experimental world of animal infrastructure,https://www.technologyreview.com/2022/06/16/1053631/inside-animal-infrastructure/,2022-06-16,"<p>Wildlife crossings cut down on roadkill. But are they really a boon for conservation?</p>
","In the mid-2000s, toads were meeting a gruesome end near Ede, an old, leafy town in the middle of the Netherlands. Local residents came to the rescue. For a few weeks each spring, the town erected a set of temporary fences along a kilometer or so of road, in an area where the animals crossed over from their winter habitat in the south to three breeding ponds in the north. When the toads hit the barrier, they’d hop sideways for a few meters until they dropped into a bucket, one of 36 pitfall traps that lined the fence. Every day, volunteers would diligently carry the toads to the other side and send them on their way. It was a crude, somewhat laborious way of mitigating the hardship of being an amphibian in a world built for humans. But it was a lifeline that Ede residents were happy to provide for their warty neighbors—which, like so many other species worldwide, have suffered difficulties feeding, breeding, and migrating as their familiar landscape is carved apart by human infrastructure. What followed has taken on the air of a cautionary fable among a small international community of ecologists and ecological designers. A few years in, Ede decided to swap its ad hoc screens for permanent barriers and replace the three dozen buckets with a pair of wildlife tunnels passing under the road. For ecologist Edgar van der Grift and other scientists monitoring the change, it was clear that the underpasses were popular. Many toads hopped happily toward their breeding ponds—even finding occasion to copulate mid-journey, a 2019 study notes. But when the researchers studied the effect that this new infrastructure was having on the toad population, they were alarmed by the results. “We saw a crash,” says van der Grift, one of the world’s leading experts in wildlife crossing structures. “In five, six years, the population went down from over 10,000 individuals to less than 1,000.” In the years since, van der Grift has persuaded Ede to add a third tunnel, in a heavily frequented spot along the road. But discussions are still ongoing about how to reverse Ede’s dwindling numbers. For advocates of wildlife crossings, any such sign of failure inevitably sets alarm bells ringing far and wide. Countries have started to invest big in these bridges and tunnels. President Biden’s November infrastructure bill allocated a landmark $350 million investment in animal crossings across the US, where some estimate roughly 1 million vertebrate animals die each day. In April, the National Wildlife Federation broke ground on a pioneering urban bridge—a $90 million custom-designed acre of “wilderness” that will float across 10 lanes of the US 101 freeway, linking two islands of mountain lion habitat north of Los Angeles. Early adopters Canada and the Netherlands are already home to decades-old networks of road-spanning projects, with arcs of chaotic forest reaching over highways. Australia, Brazil, China, and South Africa are following suit, hoping they can avoid the fate of seeing natural habitats sliced into sickly, disjointed fragments. Around the world, cities are building a huge variety of structures intended to mitigate the impacts of urbanization and roadbuilding on wildlife. The list includes green roofs, tree-lined skyscrapers, living seawalls, artificial wetlands, and all manner of shelters and “hibernacula,” including 3D-printed hempcrete birdboxes for endangered owls in Melbourne and gigantic bat caves constructed like earthen igloos in the Texas hills. But the data on how effective these approaches are remains patchy and unclear. That is true even for wildlife crossings, the best-studied and most heavily funded example of such animal infrastructure. Though road ecologists know these crossings can play a vital role in reducing roadkill, the story of their impact on wildlife conservation is still being told. This question is only growing more urgent: to meet the UN Sustainable Development Goals by 2040, a projected $97 trillion “tsunami” of new roads, railways, pipelines, and power lines will be needed, which would in effect double human infrastructure from 2012 levels, according to the World Wildlife Fund. That would put even more pressure on global biodiversity; one-sixth of all species at risk of extinction are threatened by human infrastructure development. Wildlife crossings certainly look like success stories. Every day, remote-sensing cameras beam back images of animals taking advantage of them. There are the eager pioneers, like roe deer and foxes, which cross even before construction is completed. There are shy holdouts, like gray wolves or grizzly bears, which might take generations to become users. At Singapore’s Mandai Wildlife Bridge, a total of 70 species—including pangolins, sambar deer, long-tailed macaques, fruit bats, and red jungle fowl (a close relative of the domestic chicken)—have crossed the road. “Ten seconds after they’re open, there’s animals using them,” says Darryl Jones, the author of A Clouded Leopard in the Middle of the Road, which tells the stories of deadly highways and lifesaving crossings from Brisbane, Australia, to Alberta, Canada. “The big new question now is—and this is the valid question—So what? Does that actually make a difference?” It takes less than half an hour by train to get from Amsterdam to the Gooi, a region of historic villages and medieval fortified settlements that’s home to the Dutch TV industry. But this short ride takes you across one of the most intensively engineered landscapes on earth: over railroad girder bridges, across shipping canals, past windmills—both ancient stone structures and today’s tubular steel turbines—and expanses of fields created by the polder-dike system that transformed the country’s natural marshy wetlands into productive farmland. For those interested in learning about the limits of wildlife crossings, the Gooi is a good place to start. The region boasts one of the world’s densest collections of such infrastructure, with four bridges, two major underpasses, and a network of tunnels for badgers, amphibians, and reptiles, all within about 10 minutes’ drive from the quaint local capital of Hilversum. Despite millions of dollars in losses, iBuying’s failure doesn’t signal the end of tech-led disruption, just a fumbled beginning. As van der Grift walks along the deck of one of these bridges, he points out fox droppings, blue herons, and trails where groups of roe deer walk in single file, retracing their footsteps each day. From this point, atop the bridge, the six-lane highway below is invisible, shielded by the raised banks or “berms” on either side. But it can be heard, despite efforts to dampen the noise with the foliage of local beech and spruce. The crossing is studded with ponds, close enough together for toads to comfortably hop their way from one to the next. The Netherlands built its first wildlife bridges to stop deer from becoming roadkill. But in the 1990s, the country began to shift to a more holistic ecological mindset, using bridges to link fragments of protected areas. In 2005 the Dutch parliament made such “defragmentation” a long-term nationwide policy, known as MJPO in Dutch. The program marked a shift to a strategic conservation-driven agenda, one that prioritized helping a broad sweep of species—including reptiles, bats, and butterflies—to move across the human-altered landscape. In modeling carried out for the Dutch transport ministry, van der Grift identified 215 bottlenecks where species struggled to pass, flagging them as places where crossings could make the greatest difference. Today there are 70 wildlife bridges in the country and more than 2,000 other structures, such as badger tunnels, rope bridges between trees, and aquatic underpasses. To understand the real value a bridge has, it is important to step back and see the damage that arrives with each tarmac carriageway, says van der Grift, a gentle giant with a dry sense of humor earned in two decades of trying—with mixed results—to thwart animals’ seemingly insatiable desire to throw themselves under cars. A road forms a barrier, one that animals either can’t or do not want to traverse. “It inhibits animals to cross and to get in contact with each other,” van der Grift says. The effects go far beyond roadkill. If the barrier is significant enough, it can make entire animal populations less viable, prone to inbreeding and decline. Wildlife crossings can reduce this barrier effect, the thinking goes, making the road more permeable for a range of species. But few studies so far have been able to say conclusively whether this is really happening or not, he explains. The devil is in the details. In the case of Ede, for example, there was plenty of amphibian traffic in the toad tunnels. But there were not enough structures—just two tunnels initially, hundreds of meters apart, rather than the 10 or more that scientists had recommended. “Many toads that move along the barrier and want to cross the road to get to the breeding pond—they basically gave up because they didn’t encounter a crossing structure early enough,” says Marcel Huijser, a leading US-based road ecologist and a longtime friend of van der Grift’s. Unexpected impacts and side effects crop up at nearly every crossing. Underpasses—often considered a cheaper alternative—seem to be less popular with many species and are rarely used by butterflies. Many aquatic mammals won’t swim into a tunnel where they can’t see the other end, but they can be convinced to walk through the same tunnel if a narrow ledge is built above the waterline. On one bridge in the Gooi, a big buck unexpectedly took over, exerting a territorial effect outside of rutting seasons. He acts as a gatekeeper, allowing does to cross but shutting off access to most males. At the bridge we are standing on, van der Grift is tracking slow worms, armless and legless reptiles that wobble forward like clumsy snakes. Decades after being split by the road and railways, the populations on the east and west sides of the highway had developed distinct genetic profiles. When the bridge opened in 2016, he hoped to see the two populations start to mix. And indeed, DNA testing suggests they are. “We see now that the genetic patterns of the populations are getting closer to each other,” he says. “So there is exchange.” But the ultimate goal is to ensure self-sustaining, healthy, viable animal populations. And it’s still unclear whether the defragmentation efforts are accomplishing that. Van der Grift says he and colleagues wrote a plan for a nationwide empirical evaluation of the MJPO program around a decade ago, but it was never funded (MJPO has since concluded and been superseded by other defragmentation plans). Such studies are often considered prohibitively expensive. It can require decades of tracking population sizes to sort the signal from the noise, explains Silviu Petrovan, a zoologist at the University of Cambridge. Some animals, like amphibians, naturally have population numbers that vary greatly from year to year, he says, meaning totals can zigzag “due to reasons that have nothing to do with your mediation.” “We are now at the stage where the data is coming in—it’s really coming through.” One animal that seems to have benefited from Dutch defragmentation policies is the badger. In the 1980s there were fewer than 1,200 of them nationwide. Since the country began building under-road “badger pipes,” their numbers have more than tripled. Models by van der Grift’s team strongly indicate that the tunnels have a positive effect on population viability. But no robust scientific study has been carried out to prove it, he says. That would entail decades of population monitoring. The Netherlands is not alone in its limited assessment of the impact of wildlife crossings. “Even when we do enough, we don’t do the research for long enough, funded well enough—including a ‘before’ setting, including control settings—to be able to conclude that we actually reached our objectives,” Petrovan says. Many crossing projects don’t even reach the point of clearly defining the objectives they set out to achieve, he says. Jones strikes a more optimistic tone. “We are now at the stage where the data is coming in—it’s really coming through,” he says. He’s particularly encouraged by the ability to do genetic testing: “We’ve got very profoundly useful and effective ways to assess this stuff.” Historically, in the US, conservation has not been the point of animal crossings. Wildlife bridges have so far been seen almost exclusively as traffic safety tools: two dozen or so overpasses built at hot spots for migrating deer and elk. Threatened smaller species barely register among the diverse victims of the highway. “Amphibians? Reptiles? Please …” says van der Grift, summing up how such concerns are typically laughed away. Most studies of US crossings have tracked their impact on road collisions and insurance claims. There, they excel: “When sited correctly, with appropriate fencing, to the target species, we know what wildlife crossings work well over 90% of the time,” says Nina-Marie Lister, who leads the Ecological Design Lab at Ryerson University in Toronto. “They avoid 90 to 95% of wildlife vehicle collisions. That’s an astonishing number in the world of science.” Clockwise from top left: California’s mountain lions, Kenya’s elephants, Singapore’s pangolins, and the Netherlands’ amphibians are among the focal points of wildlife crossing developments. The reduction in property damage and human injury can be significant. In the mid-2010s, for example, a project on State Highway 9 in Grand County, Colorado, added two wildlife bridges, five large arch underpasses, and 10.4 miles of wildlife fencing at a cost of $10 million. The result was an 89% reduction in roadkill. The Center for Large Landscape Conservation, a nonprofit working on ecological connectivity, projected that the crossings would pay for themselves in approximately 22 years, less than a third of the structures’ planned 75-year life span. But if the goal is simply to stop animals from being hit by cars, there is no need for a bridge. “You simply could put a fence and prevent them from going entirely, and your mortality would drop to zero,” says Petrovan, who conducts research on wildlife crossings for Conservation Evidence, a database of scientific findings about conservation actions. “It helps us feel better, because we see fewer individuals killed. But for the population, it doesn’t actually give any benefits,” he says. Huijser says the US has been less inclined than his native Netherlands—and “almost anywhere else I’ve worked”—to think about conservation as a goal of crossings. But that is changing. The Infrastructure Investment and Jobs Act, which was signed into law in November and has allocated $350 million for wildlife crossings for the next five years, ​​provides new federal funding for projects and research to reduce wildlife-vehicle collisions as well as connect fragmented areas of habitat. Although that amount is just 0.3% of the bill’s $110 billion budget for roads, road ecologists have hailed it as a landmark investment. There is now a publicly funded way to build crossings that target conservation goals, even though collision reduction remains the primary focus, says Rob Ament, senior conservationist at the Center for Large Landscape Conservation. The dedicated funding also means wildlife crossings are no longer competing with potholes for scarce tax dollars. “I think it’s actually a huge step forward,” Ament says. The bill acknowledges that we need to design infrastructure “with both things in mind: the needs of people—the movement of goods and people—but also the movement of wildlife,” he says. “And finally, we’re doing that.” But what to build? North America’s most influential examples of crossings lie along the Rocky Mountain Front in Canada. The area, which boasts the richest diversity of large mammals on the continent, is bisected by the Trans-Canada Highway. At Banff National Park, a set of 44 wildlife crossings (six overpasses and 38 underpasses) have been built to bridge the gap, creating a linked-up system used by a wide range of species including elk, cougars, and coyotes, as well as rarer animals such as red fox, grizzly bears, wolves, wolverines, snakes, beavers, and lynx. But Banff’s wildlife crossings, like most, suffer from a sort of Horseless Carriage Syndrome, their designs circumscribed by existing infrastructure. Tunnels are often little-adapted culverts, the (usually concrete) tubes that ferry water under roads. And overpasses have generally been borrowed wholesale from roadways—they are built as if they are going to carry the weight of an 18-wheeler and then “top-dressed” with foliage, Lister says. A scattering of experiments are starting to rethink this model. One is the Wallis Annenberg Wildlife Crossing, the $90 million wildlife bridge under construction north of Los Angeles. Designed by architect Robert Rock, it avoids the humped arch of older bridges in favor of a vast flat expanse that needs just one column to support it between mountains and across a highway traversed each day by an estimated 300,000 cars. It is the “poster child for innovation,” says Renee Callahan, executive director of ARC Solutions, a group that researches how to build better wildlife bridges. “It’s literally designed for species from mountain lions to mule deer to deer mouse,” Callahan says. “They’re designing it all the way down—to literally the mycorrhizal layer, in terms of the soil, to make sure that the soil itself has the fungal network that can support the native vegetation.” There are many unknowns as construction starts, not least how different species will react to the sheer volume of vehicles passing beneath. The National Park Service will be monitoring activity on the bridge as well as DNA profiles of animals on either side of the freeway. Many are watching to see what will happen with the area’s population of mountain lions. Over time, inbreeding has led to genetic abnormalities, like a telltale kink in local cats’ tails. The agency predicted that the population would become extinct within decades without a crossing. Across the US, the infrastructure bill’s $350 million falls far short of what will be needed to address the fragmentation created by the country’s 4 million miles of public roads. But there are a handful of innovations that could tip the cost-­benefit analysis by allowing crossings to be built at lower cost or in places where it was not feasible before. Animal bridges are currently built only where there is protected land on both sides of the road, as the typical expense of constructing a concrete bridge would be hard to justify on a site that someone might develop in a few years’ time. Lighter, cheaper, modular systems could be used in places whose futures are less secure, explains Huijser: “If the adjacent lands become unsuitable for wildlife, we take it apart and you can move it.” One candidate material for such modular systems is precast concrete. There’s also excitement about fiber-reinforced polymer (FRP), a material less dense than concrete that is made from structural fibers set in resin. FRP has been used to build foot and bike bridges in Europe and a quick-and-easy wildlife bridge in Rhenen, just south of the Gooi in the Netherlands. Currently the Federal Highway Administration does not allow it to be used in traffic infrastructure in the US, but there are growing demands for change. “These are barriers that are principally about policy and governance. They’re not about science and they’re not about technology,” says Lister. “They know that the last thing anybody wants is for a big structure, with a lot of publicity, to get built—and then it doesn’t work.” Designers like Lister and innovators like Callahan are vocal proponents of building wildlife bridges across the country. Road ecologists and wildlife scientists, on the other hand, remain more cautious. “They are hypercritical because they know that the last thing anybody wants is for a big structure, with a lot of publicity, to get built—and then it doesn’t work. Because everybody will come out of the woodwork and say, ‘See! Waste of time! Complete crap!’” Jones says. But today even cautious types want to see more built. Although we may not have conducted enough research to have all the answers, it would be dangerous to take that as a signal we should stop, Huijser says. He calls such over-cautiousness a “type II error”—a false negative. In this time of mass extinction, it is as if the house is burning down and our solution so far has been to squirt a water pistol at it a few times. To conclude that water isn’t the answer would be a mistake. Despite the challenges in Ede and elsewhere, van der Grift says, the answer is learning while building. We still need to invest in the real work of tagging, installing trail cams, and doing DNA testing and long-term population monitoring, he emphasizes. But we must first build more crossings—and the evidence we have so far says to build big and bold. “You have to realize that you almost cannot do too much,” he says. “You do what you think is necessary, study it, and then, nine out of 10 times, you will see, ‘Oh, I should have done more.’ But there’s no point in waiting until you have figured that out.” Matthew Ponsford is a freelance reporter based in London.",0.0
Force multipliers: accelerating developers through platform software,https://www.technologyreview.com/2022/06/15/1053763/force-multipliers-accelerating-developers-through-platform-software/,2022-06-15,"When a matter of seconds or even milliseconds makes the difference between a positive or negative customer experience, you can’t afford to leave the performance of business critical software and artificial intelligence up to chance. Join a discussion on how some of the biggest names in the business accelerate the most challenging workloads on their…","Presented byIntel When a matter of seconds or even milliseconds makes the difference between a positive or negative customer experience, you can't afford to leave the performance of business critical software and artificial intelligence up to chance. Join a discussion on how some of the biggest names in the business accelerate the most challenging workloads on their platforms to solve a range of challenges, including developer productivity, responsiveness, cloud scaling costs, and more. Kelly Hammond is the Senior Director of Strategic and Visual Cloud Engineering at Intel, leading a global software and performance optimization organization partnering with customers who are among the most innovative tech companies in the world. Since joining Intel in 2010, Kelly has specialized in systems software, primarily in Linux and Linux-based operating systems, including Chrome and Android. She has led teams developing products ranging from Android phones, Ultrabook sensor based applications, and pathfinding work with artificial intelligence for thermal solutions. Career highlights include achieving the most performant Linux OS with Clear Linux, growing the robot operating system (ROS2) ecosystem for industrial applications, and teaching the 'Outside In' systems engineering workshop. She holds one patent for her work on sensors and currently has another patent pending for AI applied to thermal solutions. Prior to Intel, she worked at Northrup Grumman on radars and satellite navigation systems. A systems and electrical engineer by training, Kelly has a master’s degree from the Massachusetts Institute of Technology in system design and management and holds a bachelor’s degree in electrical engineering from the University of Washington. Outside of work she enjoys the outdoors with her family, volleyball, and the creative arts. Elizabeth Bramson-Boudreau is the CEO and publisher of MIT Technology Review, the Massachusetts Institute of Technology’s independent media company. Since Elizabeth took the helm of MIT Technology Review in mid-2017, the business has undergone a massive transformation from its previous position as a respected but niche print magazine to a widely read, multi-platform media brand with a global audience and a sustainable business. Under her leadership, MIT Technology Review has been lauded for its editorial authority, its best-in-class events, and its novel use of independent, original research to support both advertisers and readers. Elizabeth has a 20-year background in building and running teams at world-leading media companies. She maintains a keen focus on new ways to commercialize media content to appeal to discerning, demanding consumers as well as B2B audiences. Prior to joining MIT Technology Review, Elizabeth held a senior executive role at The Economist Group, where her leadership stretched across business lines and included mergers and acquisitions; editorial and product creation and modernization; sales; marketing; and events. Earlier in her career, she worked as a consultant advising technology firms on market entry and international expansion. Elizabeth holds an executive MBA from the London Business School, an MSc from the London School of Economics, and a bachelor’s degree from Swarthmore College.",0.0
Orchestrating workforce ecosystems,https://www.technologyreview.com/2022/06/15/1053766/orchestrating-workforce-ecosystems/,2022-06-15,"Leaders and managers agree that effective management of external contributors, such as freelancers, contract workers, and app developers, is critical to their organization’s success, but not all believe their organization is sufficiently prepared to manage a workforce that will rely more on external workers. The question now is: How can organizations orchestrate this extended workforce?…","Presented byDeloitte Leaders and managers agree that effective management of external contributors, such as freelancers, contract workers, and app developers, is critical to their organization’s success, but not all believe their organization is sufficiently prepared to manage a workforce that will rely more on external workers. The question now is: How can organizations orchestrate this extended workforce? Join MIT Sloan Management Review and Deloitte for this panel discussion on their newly launched research on workforce ecosystems, a strategic framework leaders can use to manage work that takes place both inside and outside their organizations. Elizabeth J. Altman is an assistant professor of management at the Manning School of Business, University of Massachusetts Lowell. She is Guest Editor, Future of the Workforce for MIT Sloan Management Review and served as a visiting professor at the U.S. Military Academy (West Point) and visiting scholar at Harvard Business School. Dr. Altman’s research focuses on strategy, innovation, platforms, ecosystems, and the future of work. Her work has been published in Harvard Business Review (HBR), MIT Sloan Management Review, Academy of Management Annals, Advances in Strategic Management, Journal of Management Studies, and elsewhere. She was shortlisted for the “2021 Thinkers50 Distinguished Achievement Award for Breakthrough Idea” for research on ecosystems in businesses and organizations. Her 2017 HBR article on product-to-platform transitions has been honored with inclusion in three books in the “HBR 10 Must Reads” series. Before academia, Altman was a vice president at Motorola. She holds a doctorate in business administration from Harvard Business School, masters of science degrees in mechanical engineering and management from the Massachusetts Institute of Technology, and a bachelor of science degree in mechanical engineering from Cornell University. Steven Hatfield is a Principal with Deloitte Consulting and serves as the Global Leader for Future of Work for the Firm. He has over 20 years of experience advising Global Organizations on issues of Strategy, Innovation, Organization, People, Culture, and Change He has advised business leaders on a multitude of initiatives including activating Strategy, defining a preferred Future, addressing Workforce trends, implementing agile and resilient Operating Models, and transforming culture oriented to growth, innovation, and agility. Steve has significant experience in bringing to life the ongoing trends impacting the future of work, workforce, and workplace. Often, this involvement includes Strategy Development, Innovation Capability Building, Executive Meeting Design & Facilitation, People and Workforce Strategy Development, and Large-Scale Change Program Design & Implementation. He draws on deep skills in Facilitation, Design Thinking, Organizational Behavior, Group Process, and Culture Enablement to create breakthrough experiences for clients. He is a regular speaker and author on the Future of Work and is currently on the Deloitte leadership team, shaping the research and marketplace dialogue on future workforce and workplace trends and issues. Steve has a Masters in Social Change & Development from Johns Hopkins and an MBA from Wharton. Allison Ryder designs, implements, and manages media communication strategies to help people and organizations share ideas, build brands, and grow businesses. She is the senior project editor at MIT Sloan Management Review, where she oversees content development and client relations for the Big Ideas initiative and produces the popular artificial intelligence podcast Me, Myself, and AI. Allison previously worked in marketing communications at design consultancy Continuum and in product development at Harvard Business Publishing.",0.0
Building tomorrow’s telecommunications network today,https://www.technologyreview.com/2022/06/15/1053689/building-tomorrows-telecommunications-network-today/,2022-06-15,"The current 5G evolution in network connectivity is expected to drive unprecedented demands for bandwidth, reliability, and security. However, a network of this magnitude and robustness doesn’t pop up overnight and enterprises and consumers are just beginning to realize the myriad use cases a 5G network can support. For example, consider the increased number of…","In partnership withInfosys Cobalt The current 5G evolution in network connectivity is expected to drive unprecedented demands for bandwidth, reliability, and security. However, a network of this magnitude and robustness doesn’t pop up overnight and enterprises and consumers are just beginning to realize the myriad use cases a 5G network can support. For example, consider the increased number of connected devices in a house like smart thermostats, security cameras, tablets, smartwatches, and mobile phones, of course. Raj Savoor, the vice president of network analytics and automation at AT&T Labs explains, “Currently we estimate the average consumer home footprint has about 13 connected devices, including mobile and other devices.” And although that sounds like a large number, he continues to explain the real scale, “That's going to increase to 30 to 40 devices over the next five years, so a really big increase.” And the real challenge he continues to explain is that, “This growth needs advanced network architectures to support, manage and provide fast, secure, and reliable services.” Bandwidth will also increase five times in the next five years, according to Savoor, as consumers adopt immersive interactive applications. Immersive experiences also require lower latency and jitter, and a lot more security and reliability. For a company like AT&T that supports a large existing network, building the next generation network requires an incremental approach. In fact, AT&T’s 5G network has been years in the making. “We look at it as a journey. There are a lot of steps that we've taken over the past few years to build on it, and we have prepared for the next step,” says Savoor. And as businesses and consumers transition to a 5G world, AT&T keeps looking ahead. “We are thinking about the next 20 and 50 years. Network investments take a long time, and we want to make those investments with economics in mind, but also very much ensuring the most reliable network offering,” says Savoor. TranscriptLaurel Ruma: From MIT Technology Review, I'm Laurel Ruma and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace. Our topic today is architecting networks. From cell tower to device, 5G is helping connect people and spark innovation with a reliable, fast, and scalable network. This means big opportunities for consumers and enterprises. Two words for you: next, next-generation. My guest is Raj Savoor, the vice president of network analytics and automation at AT&T Labs. This podcast is produced in partnership with Infosys Cobalt. Welcome Raj. Raj: Thank you, Laurel. I'm delighted to be here with you today. Laurel: AT&T Labs has a long history of innovation. How is it now building that next generation of networks in a cloud-driven, digital-driven world? Raj: Great question, and it's something we spend a lot of time thinking about. Every technology turn is different. We're certainly leaning into this change to 5G networks and a lot of increased fiber penetration in our networks, leaning back on our culture of innovation, of our history and legacy, and particularly adapting to change as we've gone through so many generational changes. It is a unique period. We are investing both in other 5G wireless network and at the same time expanding our footprint of fiber optics further into the network, closer to our consumers and businesses. We are seeing rapid adoption and unprecedented demand for bandwidth from our both consumer and enterprise customers. Usage for example, in the home, is increasing. It will increase five times over the next five years. We saw some of that during the pandemic. The number of devices in the home are rapidly increasing. Currently, we estimate the average consumer home footprint has about 13 connected devices, including mobile and other devices. That's going to increase to 30 to 40 devices over the next five years, so a really big increase. This growth needs advanced network architectures to support, manage and provide fast, secure, and reliable services. Our approach is evolutionary. That's primarily because we are an existing, very large network that has gone through so many generational changes. When we use the next-generation architecture, we have to be cognizant of the existing infrastructure and work that incrementally, so it's not like a brand-new, greenfield, overlay build. It is that incremental approach, and it's never a single switch. We look at it as a journey. There are a lot of steps that we've taken over the past few years to build on it, and we have prepared for the next step. Laurel: Some of those steps include making that network more stable and reliable and with great coverage to touch all those households, right? Raj: Correct. I think the foundation comes with our connectivity. To make sure coverage and reach of the network, be it the radio frequency coverage or the fiber optics to the edge begins there, but it doesn't stop there. There is a lot of additional elements to managing the reliability as well as attributes of that complete customer experience. We are shifting from just single dimensional views of connectivity to an experiential and secure and how we define reliability in a much broader context. That needs a lot more architectural forethought in how we design and deliver ultimately. Laurel: What will that shift to 5G mean for innovation for consumers and enterprises? Raj: Just to maybe step back, in our past generations of change and mobile talk wireless, in the first few generations it was all about mobility and basic connectivity for voice. Then we went to messaging and early data. Then when we went from 3G to LTE or the full generation, it was about speed. As we look at where we are in terms of where the customers are driving us at 5G, that is all about immersive experiences. This requires lower latencies, which is the actual time it takes for a video to start or a download to begin of your next application, or an interactive session that may be in high definition or in AR or VR mode. It also needs lower jitter, which means lower variability, in that experience. You also need a lot more security and reliability. The security because a personal device is basically an extension of our persona. Many of our lives are entwined in that device. So, privacy information, protection of that device, protection of the data in the device, become equally important over the network channel. Those are the requirements that our customers are demanding and that can really be met in the 5G network architecture. Besides the connectivity capabilities, it is bringing the cloud-native application platforms closer to the edge of the network. It's an architecture that does require a use of a lot more automation and infusion of AI- and ML-driven approaches to provide that customer experience that is needed. At the end of the day, we expect the consumers to be able to consume more of those immersive interactive applications rendered, say, in the cloud. This could be in a venue, it could be in their homes, it could be when they're using a 5G-powered wearable device. It could be in a connected car running in a smart city. When you look at all of these different use cases, there's also a need for seamless transitions from 5G to WiFi, and so on. That is part of the overall network design and thinking that goes into our perspective. Again, security is also paramount. We constantly hear from our customers how important that is, and that is a cornerstone of how we are approaching it. AI- and ML-based approaches allow us to provide these threat analytics and security for that experience. Laurel: Yeah, that's a really good point. The network is now much bigger than it ever was. It reaches to the edge, which is every single device that uses a connection, pretty much. When you are on your cell phone at a football or soccer game, you expect to be able to look up scores or watch the latest play. You also tend to expect that real-time access to that information, to those video streams, to the data. That is one example, but we're also talking about industrial uses as well. My favorite is reaching out to oil platforms or planes or ships. The edge could be so many different nodes that we actually have to broaden our definition of what network means, correct? Raj: Absolutely. We certainly see the adoption of 5G and the edge technologies beginning at the enterprise. I think the enterprise and industrial adoption then is driving it for the consumer. Different generations of technologies have either begun at the consumer or at the enterprise level. Because of the adoption of edge cloud capabilities, what we refer to as multi-access edge compute, 5G is effectively bringing cloud compute, storage, and analytics capabilities and applications closer to those industrial applications. This is an area that we've really focused on for multiple years. AT&T has a 5G innovation studio where we bring in our enterprise customers and their problem statements and use cases. We bring in startups and other partners to put together solutions to address those blind spots or problem statements in connectivity and applications. Those frequently make use of industrial internet of things. It's where automation and industrial robots need hyper precision on location, and the network actually enables that. Also, there's a lot of video analytics to do assessment of safety issues within locations. The video analytics can be run very, very close to the industrial application and provide that real-time feedback we talked about in the consumer space. It allows quality of service and speed and low latency of 5G as well as security compared to unlicensed spectrum and other network technologies as an enabler for those industrial use cases. Drones are another emerging area. There's a need for autonomous control with low latency. Again, the network is an enabler natively for that. Yeah, we do see adoption across various verticals: healthcare, transportation, manufacturing, smart cities—a lot of sensor, network-driven opportunities. Laurel: One of those examples is FirstNet, a way for first responders to connect during an emergency where other lines of communications may be down. Raj: Yes, it's an area we are very proud to support and be the network for our first responders. There's a FirstNet authority that manages this network. It's a nationwide dedicated platform purposefully built for the first responders and really the extended public safety community that includes our healthcare system. The mission is fairly unique, as you can imagine, relative to consumers or enterprises that have their needs. We were able to bring all those requirements into a common platform. It does have an element that is different where, from a mission-critical perspective, there is no higher priority than the public safety mission. The first authority kind of enables this by enabling specific devices that are customized for the FirstNet experience, as well as applications. It actually has an application developer program as well. One part of the mission that we are highly focused on is the resiliency of that network and the network resources needed not just on any normal day, but when you have that disaster impacting a lot of the infrastructure. In those cases, we have extended our network to take advantage of other resources. We have cell towers on light trucks that are mobile that are placed and then integrate seamlessly with the network. There's also some early work with drones to provide coverage. We're not just looking at tactically, we look at it strategically. Laurel: Part of this need for first responder innovation is because of the changes of climate and the pressures with environmental challenges that are being seen, not just here in North America, but around the world. Raj: Yeah, our network resiliency is one of those implicit goals for our network design and particularly for the public safety mission. We've been looking at a lot of historical data, natural disasters and the impacts, but also modeling for future and modeling in the future risks driven by climate change, where you can have events with a high wind, three-foot floods or higher. And what does that mean for the network? Where should we design and make design changes? Where would we build the next generation of cell towers? And how do you ensure an overall resiliency under those conditions? So that is an important part of the mission and we're thinking about network design and architectures. It is really not even for the next three years. We are thinking about the next 20 and 50 years. Network investments take a long time, and we want to make those investments with economics in mind, but also very much ensuring the most reliable network offering. Laurel: You mentioned artificial intelligence and machine learning in a previous answer. What are some ways that AT&T is using AI and ML, or thinking about deploying artificial intelligence? Raj: Great question and also a very timely one. As a company, we have had researchers working on AI for many years. With the advent of a lot more compute power and a lot more finer grain data, the opportunity has really opened up with the last, I would say, five years. It does play a very significant role at AT&T. Again, we have approached AI in an evolutionary way on how we infuse it. First, we think about AI as the engine, and the fuel is the data. It begins with how we want to collect data and learn from it. That's where a lot of the machine learning capabilities come in. We have been investing in a lot of big data management capabilities over the past few years, ensuring that those are well exposed to our AI engines. Our chief data officer in particular has worked very hard to establish a democratized ecosystem for both the data and AI capability. There's a step function here in complexity as the amount of data increases, particularly with 5G, and we get kind of finer grain visibility, and we have a lot more intelligent controls to then apply decisions. So, we're taking those steps in that evolutionary way. Internally we have many use cases, including how we can use AI for planning, functions, AI for design decisions, but also in real time to help our customers, as well as the network, under various scenarios to provide better efficiency, better customer experiences, detect security threats, the threat analytics, as well as how to use feedback loops to constantly optimize the network. So, a lot of use cases across the life cycle. Laurel: I'm speaking of that focus on security, which is top of mind for most executives these days. But not only security, AI and automation also are playing that really important role for 5G functionality. What other ways is that coming into play right now with the capabilities of 5G? Raj: Again, this is very timely and a very active work area. Let me give you some context on how we are structured. In thinking about 5G, we think about it as day zero, day one, day two. Day zero is the planning activities and forecasting. I can see some natural ways where AI and machine learning can help you through your forecasting. There's your day one, which is actually building and designing your network. You want to do the greatest efficiency. Again, the feedback loops and reinforce learning kind of helps you do that as well as use of deep learning technology to analyze maps and geospatial data, to determine where you want to have buried fiber optics and where you want to place a small cell versus a macro cell. So, there's a lot of the building engineering where we rely heavily on AI, deep learning, and neural networks. Then there's a lifecycle, which we call day two. In that, there are opportunities, things like energy savings where we are trying to optimize the energy footprint of our equipment. Again, both a corporate priority, but also a societal priority on the carbon footprint. We see great opportunities for economics but also helping the planet. From a 5G technology perspective itself, there is an opportunity in what we refer to as beamforming. Beamforming is basically optimizing how the actual coverage for consumers is improved to mitigate some of the impacts of fading and path loss. The context-aware beamforming along with what we call MIMO [multiple-input and multiple-output], which is a very efficient way of transmission, requires us to understand where the demand is to determine where the customers who are consuming or using our service are located. We want to be hyper precise in that geolocation to optimize that beamforming. Is the consumer stationary? Is the consumer moving? Is he walking at three miles an hour or riding in a connected car? So that information to guide beamforming is a natural native 5G AI opportunity. Laurel: That is certainly part of this complex web that companies need to really start thinking about. So, there's an architectural challenge there to bring together cloud computing, edge computing, 5G, and then a focus on customer experience. As much as you have customers, you're also quite concerned about your customers’ customers and how they're experiencing these products. Raj: Yes. While we have a direct relationship with consumers, in many cases, it is a B2B2C where we have a relationship with a connected car company, and then they'll have relationship with the consumer. Or we would have the relationship with the transportation infrastructure provider, and they would have the relationship with similar other verticals. So that is inherently one of the opportunities that we are able to drive from this architecture. One of those capabilities is what we refer to as network APIs. We derive intelligence from our network and then make it available by APIs from that cloud infrastructure to an application platform for them to further optimize for their unique applications and their consumers. It's an emerging area, and it does require standardization. There's going to be many steps that we've got to mature this. We are pretty excited, and the early results tell us that the overall ecosystem is very hungry for the type of analytics and data to optimize those end user experiences. Laurel: The cloud has played such a pivotal role for numerous industries to build that kind of resiliency that we've been threading through this conversation, but then also drive innovation. When you think about, and you mentioned not just the next three to five years, for the next 30 to 50 years, in a smaller scope, what technological advancements excite you the most? What's on that horizon? Raj: First I think we've benefited from a few of the industry, I'll call them, laws. Certainly, Moore's Law, that we all enjoy increased computing power at lower costs, effectively where we are with growth in storage and cloud capabilities. Then there's the other part, which is the demand part—the consumer appetite for increased consumption. Some of it is behavioral. Some of it is autonomous as they adopt devices with higher resolution. Effectively that drives the consumption both downlink and uplink. Trends that I'm following and I'm expecting to be the drivers for the future is first, like I mentioned earlier, we are expecting usage to increase five-fold in five years. While I don't have a crystal ball quite beyond the next five years, I don't see any reason why that wouldn't continue, particularly bi-directional communication, especially with those immersive VR/AR [virtual reality/augmented reality] experiences. The number of connected devices and devices just around us, whether it's variables in our automobile, in our home, is going to double or triple. We expect that trend to also drive effectively the quality of our lives and really operating in every phase of our day. I see sensors increasing, and I think not just in the home, but also in smart cities and in the public domain. And a seamless opportunity between our home, work, transportation, and places that we visit. So it's going to be increasingly driving that seamless experiences that I'm excited about. It is going to be an exciting future. Laurel: It sure is. Thank you so much, Raj, for joining us on the Business Lab. Raj: Thank you, Laurel. Laurel: That was Raj Savoor, the vice president of network analytics and automation at AT&T Labs, who I spoke with from Cambridge, Massachusetts, the home of MIT and MIT Technology Review, overlooking the Charles River. That's it for this episode of Business Lab. I'm your host, Laurel Ruma. I'm the director of insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print, on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com. This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review. This episode was produced by Collective Next. Thanks for listening. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.",0.0
Building the necessary skills for digital transformation,https://www.technologyreview.com/2022/06/15/1052869/building-the-necessary-skills-for-digital-transformation/,2022-06-15,"The skills and capabilities needed to undergo digital transformation are in high demand as every company jockeys to gain a competitive advantage. To address the skills gap, some companies are prioritizing upskilling and reskilling. But to be effective, learning and development itself must undergo a transformation. According to Daniela Proust, global vice president and head…","In partnership withInfosys Cobalt The skills and capabilities needed to undergo digital transformation are in high demand as every company jockeys to gain a competitive advantage. To address the skills gap, some companies are prioritizing upskilling and reskilling. But to be effective, learning and development itself must undergo a transformation. According to Daniela Proust, global vice president and head of global people enablement and growth at Siemens, learning and development is at the core of digital transformation. “In light of a major transformation that businesses are facing, either by new business models arising or new innovation and technologies driving a certain business area forward, you see that you need to accompany that structural change, that structural workforce transformation in order to drive business transformation,” she says. Traditional training methods need to also transform. Given the speed of technological change and need for business agility, multi-day offsite training (some of which may not even apply to the employee’s role) is no longer viable. Fortunately, the same technologies that are driving digital transformation in other areas of the business can also be leveraged to transform learning and development. “Now people learn more often, for shorter periods of time, but training is much more tailored to what they need in that moment, and that is enabled by technology,” explains Proust. In addition to delivering just-in-time training, a modern learning and development platform can provide valuable insights. “A platform-based learning ecosystem with a learning experience platform at the core enables you to gain insights that we never had in the past.” says Proust. This new approach to learning delivers benefits to both the business and its employees as they acquire the skills that help accelerate the company’s digital transformation and fuel their own career growth. This episode of Business Lab is produced in association with Infosys Cobalt. Laurel Ruma: From MIT Technology Review, I'm Laurel Ruma, and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace. Our topic today is learning and development. In every industry, businesses are eager to leverage technological advancements to bring innovations to the marketplace. Thanks to cloud computing, a lot of those technologies are within reach. The problem, however, is finding and retaining the people who know how to use them. Two words for you, upskilling, reskilling. My guest today is Daniela Proust, who is the global vice president and head of global people enablement and growth at Siemens. This podcast is produced in partnership with Infosys Cobalt. Welcome, Daniela. Daniela: Hi, Laurel. Laurel: According to the World Economic Forum’s Future of Jobs Report, upskilling and reskilling were the top priorities for 59% of learning and development professionals in 2021. Do you think that this is a trend we can expect moving forward? Daniela: Absolutely, yes. And to be honest with you, when I read this World Economic Forum’s report, I was surprised that it was only 59% of L&D professionals putting it as a top priority, because I was thinking it should be at least a hundred percent. But it may be because all the topics that were number two, three, and four, also had to do with how we can create an environment where people can learn and grow. Laurel: And some of that is predicated on the fact that learning is traditionally a back-office role, right? But you need executive buy-in to give their support to build out learning and development programs. Another interesting data point is the Workplace Learning Report in 2020 said only 27% of learning and development professionals felt their CEOs were active champions of learning. But then in 2021, we know what happened, right? Sixty-two percent are now champions of learning. So that's a massive jump in just one year. What is it that these CEOs are beginning to understand? What are they learning that we just may not know yet? Daniela: If you ask me, I think it finally gets on stage where it should be. So, it's spot on. And I'm very happy to see these statistics out there. When I reflect on the journey within Siemens, I also see over the years that learning has evolved from one important topic of HR to become a core topic of HR, to then become a core topic and be part of the overall business strategy. So, it has moved a lot in the sense of how important it is and also how much attention is given to the topic by executive leadership. Laurel: Being a champion to employees when it comes to learning and development is so important from executives, but what does it take to get to the point where learning drives business transformation? Daniela: I think it's a very important question, and that is one that keeps me up at night. I think that learning is absolutely driving business transformation. Why? Because we see that change is everywhere at a pace that we had never imagined. We simply don't know what the next jobs are to come tomorrow, in five years, 10 years. In order to be successful and drive successful business, you need to continuously adapt the roles that you have to drive your successful business. Right? And I see that in the talent acquisition space. I see that almost every job posting now is a new one. So if that holds true, we have new jobs, new skills, new competencies all the time. And therefore, we all need to continuously learn and grow. And only then can the business be successful. In light of a major transformation that businesses are facing, either by new business models arising or new innovation and technologies driving a certain business area forward, you see that you need to accompany that structural change, that structural workforce transformation in order to drive business transformation. So for me, it's at the core of it. Laurel: There's certainly many challenges when we're talking about digital learning and business transformation. And the covid-19 pandemic certainly had a significant effect on how and where people work. But it also caused employees, everybody to reevaluate that work life balance and their career goals. In turn, what kind of effect has all of this had on learning and the enterprise? And why is now a critical time for enterprises to look at their learning programs? Daniela: Yeah, I think the covid pandemic has accelerated a transformation that was already happening. I would say we had major drivers for change with regards to digital transformation on the whole that were already radically and disruptively changing the overall approach of learning or to learning to platform-based ecosystems, thinking in experiences and not just traditional courses that are delivered in a classroom environment. Technology is really enabling us to provide completely new personalized learning experiences at scale to our people. When the pandemic started, I was so happy that we were ready to then completely transform the way we bring learning to our learners in rapid time. To bring personalized learning experiences to our people that they can pick and choose what they want to learn at the moment of need integrated in the flow of work. I mean, these are words that sound easy, but it is a massive shift. We did a survey three, four years ago about what people were thinking about learning. Siemens as a technology company has always valued competence and expertise very, very much. But also we had a quite traditional approach to learning. We had a number of classroom trainings. And it was almost like an incentive that at the end of a year, you would discuss with your manager what has worked well in the course of the year, and then get a training course to help you continue on your journey. Now people learn more often, for shorter periods of time, but training is much more tailored to what they need in that moment, and that is enabled by technology. The pandemic made that happen so that people, I would say were even forced to try it out. These experiences that were created and delivered to them, they could try them out and see, ""Oh, actually it doesn't hurt. Oh, actually it's helping me. I should really engage in this."" And it was a turbo booster of cultural adjustment. It was a turbo booster of taking more ownership to be curious and find out what is relevant for you and then embarking on that journey yourself, but also in teams or with your manager. I think on the whole it has indeed significantly shifted in a very short period of time, in two years. And when you ask about how is that also causing an impact on the overall work environment, one thing is the overall topic of learning and development. Certainly, major shift is happening there. We also hear from our people that they are much more conscious. How do I want to work? What kind of environment do I want? How often do I actually really have to go to the office? Maybe twice per week is enough and everything else I can do in virtual settings and still be connected even at scale across the globe. So, I think this is probably the best outcome out of a difficult time that we completely reevaluate how we want to work and how we want to learn and grow. Laurel: That's a great way to kind of frame what traditional learning was. You'd gather in a classroom, perhaps you'd fly to a different city. You'd meet with colleagues, and you'd all take days-long learning classes, perhaps, away from work away from everything else. But now, as you say, with technology and with the ability to work where you are and learn where you are, that learning has kind of moved, not just from in-person, but to online and where you are in the flow of work. I think that particular idea is very unique and new to development and learning. Correct? Daniela: Absolutely. What I find so interesting is these concepts that we saw come out of industrialization in certain industries, and then all of a sudden you had, according to Porter, different strategies that you could pick. You would either be a mass provider of commodities, or you would differentiate through certain things that you can tailor and customize and therefore get a prime for it. And then, all of a sudden, these things like mass customization were starting. If you think of how jeans were then produced: you could order online, according to your complete body shape, and it would nonetheless be produced in the same factory environment. For me, that is kind of the same thing that is happening here: we have a huge array of learning opportunities, and we can tailor them through algorithms and matching mechanisms to the individual. It is really about personalized learning experience at scale. And for me, that is totally fascinating. Laurel: I also like that choosing of words very carefully, which is continuous learning, right? Learning isn't just something you do at a certain time and place. We are always continually learning new skills, new ideas, new ways of working. And to seamlessly integrate that into your work life certainly helps. But how, after two years of working from home battling digital fatigue, how do we make learning online interesting and exciting when it is being delivered via the technology? Daniela: That is a very good question and super important. At the end of the day, it's all about relevance so that you get the relevant learning opportunities to the learner at the moment of need. And that also the experience is really great for the individual. Sounds easy, it's really not so easy to do because everybody learns in a different way. You might enjoy podcasts most, somebody else likes videos or more hybrid interactions or a complete learning path. And you have the whole array at your fingertips so you can pick and choose what makes most sense for you, and I think that is what makes it so powerful. At the same time, I fully agree that we ramped up the technology and completely new experiences in rapid time. And people indeed are getting tired of being in front of the screen now so long every day with many, many more meetings. I don't know if you have read this. There’s a study. I think it was a 250% increase in the number of meetings in just one year. This is just craziness. And I must say, I can mirror it for myself. I feel that too. So, when you think about digital fatigue, that it is happening just because everything becomes so digital and virtual, what we are competing for is our people’s time. That becomes the most important factor and denominator. Therefore, for people to prioritize this kind of learning experience, it has to be a really good experience and has to help them and be relevant to them. So that is the success. It sounds easy, as I said, it is not so much. But that is where the technology comes into play. So yes, technology is revolutionizing learning, and the experience is really at the core of it. You have concepts like gamification that allow you to bring social components into the platform, put challenges out, have some rewards and recognition mechanisms, and all of that bundled together makes a great experience for the people. Laurel: What are the other benefits of a modern learning program? I've seen some companies do things like, as you mentioned, engineering challenges. Even hackathons could be considered a way to learn in a different way. Because everyone does, as you mentioned, learn in different ways. Do you feel that companies are looking at this way of distributed learning and continuous learning as a way to bring that change even to the way that we think of learning? Daniela: Absolutely. It's a total driver for innovation and it is also a driver to create something like a company memory of expertise and knowledge. Because you can bring together through one single point of entry, a universe of learning opportunities to the people. You have so many great people and organizations that can contribute with latest insights, topics that they want to position and bring to the people. We haven't had that in the past. Imagine a company like Siemens, a huge technology company active in so many industries. It means that we need to bring together learning opportunities from, let's say, a functional perspective. So, if you are in finance or in supply chain, we need to then also complete it by, we call it cross-functional learning opportunities, which are topics that are relevant for everybody like languages or communication. We also have a whole learning landscape available on technology topics, on product-specific topics, on market-specific topics. It’s a huge landscape of learning opportunities, and everybody needs a subset, and everybody needs a very individual specialized subset. That is a huge benefit to be able to tailor it to that. And by having such an approach, I must also say it's much more efficient and productive because it saves time and money. People can have access to a whole universe. They don't have to travel, don't have to then encounter programs where maybe only a certain percentage of it is relevant to them. It's really helpful also to drive the overall business success. Laurel: And part of that business success is digital transformation, right? Adopting and rolling out new technologies like automation and artificial intelligence. This will create a new division of labor between humans and machines, which will disrupt jobs globally. But as these jobs evolve, new roles will be created with people having specific advantages over machines and AI like managing and decision making and communicating and interacting — all of those things that humans are really good at. How can business people prepare and prepare their employees for this shift from automation? Daniela: Yeah, I think it is something that accompanied us already since quite a few years. But there, again, the speed and also the level of skills needed has increased so significantly. I would say it's almost like a bouquet of things that you can do and should do. You need to, as a company, create an identity and first of all, say that you really think learning and individual growth is super important. It is a priority for the company, and you need to give it a positive spin. It is there for you, it is there to support you, it starts with you. That is why we have initiated a company-wide campaign that we call MyGrowth. It's much more than a campaign, it's an overall concept and approach. But it is really meant to inspire and engage people to try out the different experiences that we provide and help them to navigate and give orientation what they should and can use. Then we have also initiated a target on learning hours because we really wanted to nudge people and say, ""Look, it's important that you take the time and that you take it as a priority."" With regard to the specific skills that you were mentioning around automation and digitalization, we then can include specific strategic topics that we push to our people. We drive awareness campaigns through learning opportunities. Those can be targeted for certain audiences because people also need different skill levels. Or we can push it at scale. This is a highly flexible system. If I may give you an example, we have one pocket in our businesses that is called Digital Industries Software. It fits very nicely to what you were mentioning. The CEO of that business last year said we are in a software business, so AI is a major driver for everything that we are doing. Therefore, my whole organization needs to understand what first of all, artificial intelligence is, let's say on a very generic level. But also, people need to understand how we are using it as a technology internally, but also as a driver for our business and software solutions. And then we created different learning paths for different expertise layers, and could therefore, bring the whole topic in a very comprehensive manner to thousands of people of our Digital Industries business. Laurel: So, you are doing two things. One, you're pushing out what you think that everyone needs to know and learn, artificial intelligence being a big topic. But then how do you then also do assessments of people and their skills to identify skill gaps and then align learning programs with the business strategy to basically not just get a return on investment? Of course, everything does come back to profit, but also return on investment on the employee's time and expertise. Because that is also something you're growing. Daniela: Yes. And the skills topic is a very hot one, I can tell you. It's all over the place and coming from very different lenses and use cases. Technology plays a major role. A platform-based learning ecosystem with a learning experience platform at the core enables you to gain insights that we never had in the past. We can see what interests people. We can see why and for what are they engaging in learning, what are they then actually learning or what are they not learning, and then therefore, leaving. If you then multiply that and you see that over the overall workforce, you see also what are hot topics, what are skills that are coming on the horizon. You can see that in certain communities. We have certain communities that are, for instance, we call them digital talents, like tech talents. And there, you already see what the next topics are that will come on the horizon. And then we can match as a learning function, do we already have the right learning opportunities for the topics that are being searched for? That is one thing. But that is more the bottom-up part of it that is super important. From a top-down perspective, we also have a very strategic approach to that, which we call NextWork where we see areas of major workforce transformation, and usually the levers are either disruptive new business models or/and technology that is driving change. And there, we do targeted workforce transformation analysis. We take a look at, okay, what are the jobs in that area today, where do we see those roles evolving? Potentially some of them being completely eliminated or completely new ones arising that we hadn't even known. Then we match that with our workforce and see how we can accompany our people from A to B. But there, you then need to go from a, let's say, conceptual level of the analysis that you do in the first step to then apply it to the individual to make it meaningful, and then match the relevant learning interventions and development path for the individual. And for that, assessments play a key role. In our learning platform that we call My Learning World, we have a functionality that we call My Skills. If, for instance, a manager says we have significant things happening in this area, you are becoming a software developer, then you can assign a role to that person and say, ""Look, why don't you check out whether this role is the next right approach for you.” And then you get a skill assessment where you can see the major needed capabilities, skills, and competencies for that role. Then you can self-rate yourself also in alignment with your manager and get very targeted learning interventions mapped. So it gives the individual a very good orientation where to start and where to focus. Laurel: What do you see as the future of learning and development? What excites you the most? Daniela: I'm really passionate about people. I think the whole topic is super exciting. And I think technology is such an opportunity to radically shift the world of learning and development. And it already is. For me, learning is simply never complete. We all have to continuously learn, to unlearn, and also to relearn. I think the more we bring this awareness across that people are aware that they are in the driver's seat of their own employability, that is really, really important. And it excites me. And I think there, you have to work with concepts like curiosity, work with people and inspire them, ignite a passion for learning. Because usually, people are by nature very curious. Laurel: At some point we'll have to talk more about what unlearning means, because that is certainly an interesting idea. We kind of go through life learning very specific things, and it is partly correcting the way we think about ideas and methods that perhaps are not the best way of learning it. But then also, it's a bit of a way to retrain the brain, isn't it? There’re better ways of doing math now than there were 20, 30 years ago. Daniela: It is. I think this topic of unlearning, people sometimes think it sounds a bit paradox, but it comes from the fact that when you have learned something, it is really ingrained in your brain and you start with the hypotheses of how you see the world and how you see things, how they should be. Right? Therefore, you will always start with that frame of reference. But if then the frame of reference would need to be changed just because the outside world is shifting so much, it means you have to almost delete what your first reaction would be and start completely fresh and be open to a new approach. And that is one that we all have to learn and do more probably. Laurel: Absolutely. Thank you very much, Daniela, for what has been a fantastic episode of the Business Lab. Daniela: Thank you so much. It's great to be part of this. Laurel: That was Daniela Proust, the global vice president and head of global people enablement and growth at Siemens, who I spoke with from Cambridge, Massachusetts, the home of MIT and MIT Technology Review overlooking the Charles River. That's it for this episode of Business Lab. I'm your host, Laurel Ruma. I'm the director of Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology. And you can also find us imprint on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com. This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT technology review. This episode was produced by Collective Next. Thanks for listening. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.",0.0
Making hybrid work,https://www.technologyreview.com/2022/06/13/1053744/making-hybrid-work/,2022-06-13,"Organizations struggle to find a rhythm in the new hybrid world. The shift from enabling hybrid work to optimizing it to deliver exceptional employee experiences is well underway. Join this session, designed for CIOs and supporting teams, for steps to improve the hybrid work experience for employees through an equitable, collaborative, and inclusive strategy. Recent…","Presented by Organizations struggle to find a rhythm in the new hybrid world. The shift from enabling hybrid work to optimizing it to deliver exceptional employee experiences is well underway. Join this session, designed for CIOs and supporting teams, for steps to improve the hybrid work experience for employees through an equitable, collaborative, and inclusive strategy. Recent research cites that 92% of employers worldwide believe the employee experience will be a priority over the next 3 years. The connection between a positive employee experience and a happy customer has a measurable outcome. Ivan Dopplé, Kyndryl’s General Manager of Digital Workplace Services, is a strategic leader with a record of building and growing businesses both organically and through strategic acquisitions and driving profitability in turnaround and expansion situations. He worked with the largest clients across the globe and has sound skills in transforming businesses on their digital journey. Ivan has led several Global Technology Services businesses for IBM as General Manager. He has a proven record as a leader in the Outsourcing and Infrastructure Technology Services business. For several years Ivan was also responsible and accountable to develop and lead the growth markets in Central Europe/Middle East & Africa in the Financial Services Industry for IBM before he led the largest Service Market in Germany/Austria and Switzerland. In his last engagement for Kyndryl, Ivan built the COO function for the Kyndryl Corporation and was also responsible for all the Real Estate & DC Services, Procurement, Asset Management, and Cost Efficiency agenda. Ivan is married and based in Switzerland. He is a passionate horseman and rider enjoying living in an old farmhouse. Learn more about Kyndryl.",0.0
"The Download: China’s influencer crackdown, and covid’s origins",https://www.technologyreview.com/2022/06/10/1053000/download-china-influencer-crackdown-austin-li-covid-origins/,2022-06-10,"<p>Plus: Quantum computing could create never-before-seen forms of matter</p>
","This is today's edition of The Download, our weekday newsletter that provides a daily dose of what's going on in the world of technology. How China’s biggest online influencers fell from their thrones No one had foreseen just how fast three of China’s most powerful influencers would fall. On June 3, Austin Li, a 30-year-old live-streamer with over 60 million followers, abruptly cut off a live stream after a tank-shaped ice cream dessert appeared on the screen. While he later posted that it was due to “technical difficulties,” most people understand it as having triggered government censors, who interpreted it as a reference to the Tiananmen Square massacre.Li isn’t known to have been arrested, and his account remains active, but he hasn’t streamed or posted on social media since. Fans suspect he may not be allowed to stream again.Live-streaming e-commerce in China is a massive industry worth over $180 billion. Influencers like Li have risen to rival the popularity of A-list celebrities, and have been known to facilitate billions of dollars worth of online purchases in one night. But in Li’s and at least two other cases, these online empires were toppled overnight in what appears to be a government crackdown extending back to late 2021—suggesting a reckoning is well underway. Read the full story. —Zeyi Yang The must-reads I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.1 We’re still being kept in the dark about the origins of covidWe need more data from China, a new WHO report says. (NYT $)+ It also wants to investigate the theory it was leaked from a lab further. (WP $)+ Meet the scientist at the center of the lab leak controversy. (MIT Technology Review)2 Quantum computers could create an entirely new forms of matterThe likes of which have never been seen before in nature. (New Scientist $)+ Data is at risk of being broken by computers that don’t even exist yet. (Spectrum IEEE)+ The US is already concerned about the threat they pose to encryption. (MIT Technology Review) 3 How eBay sellers are evading its ban on assault weaponsSome listings are blatant about what they’re selling, while others are more subtle. (LA Times)+ While you’re theoretically not allowed to sell guns on Facebook, you have to break that rule 10 times for it to be enforced. (WP $) 4 Is community governance the answer to social media’s problems?Relying on the cooperation of strangers is risky, but so is allowing one man unfettered power over a platform. (The Atlantic $)+ Eight legal complaints were filed against Facebook this week. (Protocol)+ Big Tech spent $36 million on adverts opposing a US antitrust bill. (WSJ $) 5 NASA is joining to hunt for UFOs 🛸It wants to collect data on phenomena we don’t understand. (WP $)+ Astronomers are rethinking how the planets came to be. (Quanta)+ A key substance for life has been found in asteroid samples. (CNET)+ Japan’s space agency is experimenting with a four-legged lunar robot. (CNN)6 East Asians’ eyesight is getting worseMore sunlight exposure might help future generations, though. (Economist $) 7 Stimulating your muscles with electricity is the hottest new fitness trendBut there’s no evidence it’s more effective than good old fashioned exercise. (Neo.Life)8 Silicone breast implants are still making women sickDespite their issues being known for decades. (Slate)9 The internet was supposed to make life easierNow we’re reliant on middlemen our grandparents never needed. (The Atlantic $)10 The moral implications of whether animals dream 💤And why we may, one day, know what they’re dreaming about. (Motherboard) Quote of the day “It’s all come back to bite us.” —Tran Tuan, a GrabCar ride-hailing driver in Ho Chi Minh City, is frustrated by the company’s decision to raise its prices amid a spike in fuel prices, after years of rapid growth, he tells Rest of World. The big story AI will tell you how beautiful you are March 2021 Qoves started as a studio that would airbrush images for modeling agencies. Now it is a “facial aesthetics consultancy” that promises answers to the age-old question of what makes a face attractive. Its most compelling feature is the “facial assessment tool”: an AI-driven system that promises to tell you how beautiful you are—or aren’t—spitting out numerical values akin to credit ratings. If that prospect isn’t concerning enough, most of these algorithms are littered with inaccuracies, ageism, and racism—and the proprietary nature of many of them means it is impossible to get insight into how they really work, how much they’re being used, or how they affect users. Read the full story. —Tate Ryan-Mosley We can still have nice things A place for comfort, fun and distraction in these weird times. (Got any ideas? Drop me a line or tweet 'em at me.)+ Aww, these seals are too cute.+ This list of top holiday destinations for June is giving me some serious wanderlust.+ What can inkblots teach us? Quite a lot, it turns out.+ It’s no joke, mountain goats really are fearless. If you enjoyed Peter Jackson’s Get Back documentary about The Beatles (and who didn’t?), this is a fun look back at Paul McCartney through the ages.",0.0
"The Download: Chinese hackers target telecoms, and aviation emissions",https://www.technologyreview.com/2022/06/09/1052632/download-chinese-hackers-telecoms-vulnerabilities-aviation-emissions-goals-fuel/,2022-06-09,"<p>Plus: Twitter is going to allow Elon Musk access to millions of tweets</p>
","This is today's edition of The Download, our weekday newsletter that provides a daily dose of what's going on in the world of technology. Chinese hackers exploited years-old software flaws to break into telecom giants The news: Hackers employed by the Chinese government have broken into numerous major telecommunications firms around the world in a cyber-espionage campaign that has lasted at least two years, according to a new advisory from American security agencies. How it happened: The hackers allegedly breached their targets by exploiting old and well-known critical vulnerabilities in popular networking hardware. Once they had a foothold inside their targets, the hackers used the compromised devices to gain full access to the network traffic of numerous private companies and government agencies, US officials said. They did not name those affected by the campaign, nor explain the impact it had. What it means: The campaign is a warning about the need for better basic cybersecurity for some of the most important networks in the world, and a dramatic illustration of the danger software flaws pose even years after they’re discovered and made public. Read the full story. —Patrick Howell O'Neill The aviation industry can hit emissions goals, but new fuels need to take flight first Cutting carbon emissions from planes is going to be difficult—but not impossible, according to a new report by the International Council on Clean Transportation. The report outlines possible paths for aviation to reduce emissions enough to do its part in keeping global warming at less than 2 °C above pre-industrial levels, the target set by the Paris agreement. It says that about 60% of emissions reductions are projected to come from low-carbon fuels, with the rest coming from efficiencies, and lower demand. Read the full story. —Casey Crownhart The must-reads I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.1 Twitter has agreed to give Elon Musk access to millions of tweetsWhich could make it much harder for him to back out of buying the company. (NYT $)+ One of Musk’s financiers is linked to a Russian tycoon. (Bloomberg $)+ Texas’ decision to probe into Twitter’s fake accounts is a purely political one. (NYT $)2 How Big Tech’s data hoarding harms us allAnd why sharing it wouldn’t hurt them, either. (Time $)+ Collective data rights can stop big tech from obliterating privacy. (MIT Technology Review)3 A start-up has been accused of dispensing ADHD drugs too liberallyParticularly during the pandemic, when regulations around remote prescriptions were relaxed. (WSJ $)4 Bumpy batteries work better in freezing temperaturesFlat lithium-ion batteries struggle in the cold—changing the shape of its components could be a solution. (New Scientist $)+ This startup wants to pack more energy into electric vehicle batteries. (MIT Technology Review)5 South Korea is investigating the company behind the stablecoin crashOver claims a worker embezzled its crypto holdings. (FT $)+ Workers thinking of pivoting to web3 are having second thoughts. (Vox)6 Smart windows are an obvious way to save energy 🪟The problem, as ever, is making them affordable enough to go mainstream. (Knowable Magazine)7 The Caribbean’s hurricane activity is at a historic lowAnd has been for a surprisingly long time. (Hakai Magazine)+ We might have to start naming heat waves the way we do hurricanes. (Axios)+ How to keep the power on during hurricanes and heat waves. (MIT Technology Review)+ Tracking vibrations could help experts to get ahead of flash floods. (Economist $)8 Not all NFT art is terrible 🖼️It just happens that most of the really famous pieces are. (The Verge)+ Bored Apes has been dethroned as the most popular NFT project. (Motherboard) 9 A saxophonist smuggled secrets into the USSR using encrypted musical code 🎼Rendering the information indecipherable to everyone but practiced musicians. (Wired $) 10 It’s time to get over The Current ThingOur collective ability to forget what we’re outraged by should help. (FT $) Quote of the day “There is literally not a computer in that clinic unless I bring my laptop from home in.” —Mia Raven, director of policy at an abortion clinic in Alabama, tells NBC News she’s stepping up security measures as part of measures to better protect clients, as the risk of Roe being repealed looms. The big story Ghost ships, crop circles, and soft gold: A GPS mystery in Shanghai November 2019 On a sultry summer night in July 2019, the MV Manukai was arriving at the port of Shanghai, near the mouth of the Huangpu River. The city would be the American container ship’s last stop in China before making its long homeward journey to Long Beach, California.As the crew carefully maneuvered the 700-foot ship through the world’s busiest port, its captain watched his navigation screens closely. According to the Manukai’s screens, another ship was steaming up the same channel at about seven knots (eight miles per hour). Suddenly, the other ship disappeared from the AIS display. A few minutes later, the screen showed the other ship back at the dock. Then it was in the channel and moving again, then back at the dock, then gone once more. Eventually, mystified, the captain picked up his binoculars and scanned the dockside. The other ship had been stationary at the dock the entire time. Now, new research and previously unseen data show that the Manukai, and thousands of other vessels, are falling victim to a mysterious new weapon that is able to spoof GPS systems in a way never seen before. Read the full story. —Mark Harris We can still have nice things A place for comfort, fun and distraction in these weird times. (Got any ideas? Drop me a line or tweet 'em at me.) + This oompah band cover of Highway to Hell will get your Thursday off to the perfect start (Thanks Allegra!)+ Who knew bamboo salt was so interesting?+ Riley is an LGBTQ+ icon after our own hearts. 🏳️‍🌈+ What it looks like to grow a mango tree from a seed over the course of a year (just don’t expect it to bear fruit any time soon.)+ It’s asparagus season—here’s how to cook it to perfection.",0.0
Chinese hackers exploited years-old software flaws to break into telecom giants,https://www.technologyreview.com/2022/06/08/1053375/chinese-hackers-exploited-years-old-software-flaws-to-break-into-telecom-giants/,2022-06-08,"<p>A multi-year hacking campaign shows how dangerous old flaws can linger for years.</p>
","Hackers employed by the Chinese government have broken into numerous major telecommunications firms around the world in a cyber-espionage campaign that has lasted at least two years, according to a new advisory from American security agencies. The hackers allegedly breached their targets by exploiting old and well-known critical vulnerabilities in popular networking hardware. Once they had a foothold inside their targets, the hackers used the compromised devices to gain full access to the network traffic of numerous private companies and government agencies, US officials said. The advisory did not include the names of those affected by the campaign, nor did it detail the impact it has had. But US officials did point out the specific networking devices, such as routers and switches, that hackers in China are thought to have targeted repeatedly, exploiting severe and well-known vulnerabilities that effectively gave the attackers free rein over their targets. “These devices are often overlooked by cyber defenders,” the American advisory warned. They “struggle to maintain and keep pace with routine software patching of Internet-facing services and endpoint devices.” The new advisory is the latest example of a radical shift among US intelligence agencies away from a culture of silence and secrecy. The organizations now routinely speak publicly to issue cybersecurity guidance. The new document is designed to help victims detect and eject hackers who have been infiltrating their networks for years. And it’s something bigger, too: a warning about the need for better basic cybersecurity for some of the most important networks in the world. Telecommunication firms are extremely high-value targets for intelligence agencies. These companies build and run on most of the infrastructure of the internet as well as many private networks around the world. Successfully hacking them can mean opening doors to an even bigger world of prized spying opportunities. The United States has its own documented history of such attacks. The National Security Agency, for example, once infiltrated the Chinese telecom and internet giant Huawei, reportedly both to spy on the company itself and to exploit the networking and telecommunications products Huawei sells worldwide. Ironically, that operation was prompted in part by continuing American fears that Beijing could use Huawei’s hardware to spy on American interests. This is a big moment of turbulence and change for the hacking business. But the demand is here to stay. In the newly reported cyber campaign, the Chinese hackers allegedly exploited networking devices from major vendors like Cisco, Citrix, and Netgear. All of the vulnerabilities were publicly known, including a five-year-old critical flaw in Netgear routers that allows attackers to bypass authentication checks and execute any code they choose—an opening that allows for a full takeover of the device and an unfettered window into the victim’s network. The campaign’s success is a dramatic illustration of the danger software flaws pose even years after they’re discovered and made public. Zero-day attacks—hacks exploiting previously unknown weaknesses—pack a punch and demand attention. But known flaws remain potent because networks and devices can be difficult to update and secure with limited resources, personnel, and money. Rob Joyce, a senior National Security Agency official, explained that the advisory was meant to give step-by-step instructions on finding and expelling the hackers. “To kick [the Chinese hackers] out, we must understand the tradecraft and detect them beyond just initial access,” he tweeted. Joyce echoed the advisory, which directed telecom firms to enact basic cybersecurity practices like keeping key systems up to date, enabling multifactor authentication, and reducing the exposure of internal networks to the internet. According to the advisory, the Chinese espionage typically began with the hackers using open-source scanning tools like RouterSploit and RouterScan to survey the target networks and learn the makes, models, versions, and known vulnerabilities of the routers and networking devices. With that knowledge, the hackers were able to use old but unfixed vulnerabilities to access the network and, from there, break into the servers providing authentication and identification for targeted organizations. They stole usernames and passwords, reconfigured routers, and successfully exfiltrated the targeted network’s traffic and copied it to their own machines. With these tactics, they were able to spy on virtually everything going on inside the organizations. The hackers then turned around and deleted log files on every machine they touched in an attempt to destroy evidence of the attack. US officials didn’t explain how they ultimately found out about the hacks despite the attackers’ attempts to cover their tracks. The Americans also omitted details on exactly which hacking groups they are accusing, as well as the evidence they have that indicates the Chinese government is responsible. The advisory is yet another alarm the United States has raised about China. FBI deputy director Paul Abbate said in a recent speech that China “conducts more cyber intrusions than all other nations in the world combined.” When asked about this report, a spokesperson from the Chinese embassy in Washington DC denied that China engages in any hacking campaigns against other countries. This story has been updated with comment from the Chinese embassy in Washington.",0.0
"The Download: Open source censorship in China, and US kids are more anxious than ever",https://www.technologyreview.com/2022/05/30/1052895/download-open-source-censorship-china-us-kids-anxious-depressed/,2022-05-30,"<p>Plus: Immigrants are being made to feel like prisoners by digital surveillance programs</p>
","This is today's edition of The Download, our weekday newsletter that provides a daily dose of what's going on in the world of technology. How censoring China’s open-source coders might backfire Earlier this month, thousands of software developers in China woke up to find that their open-source code hosted on Gitee, a state-backed Chinese competitor to the international code repository platform GitHub, had been locked and hidden from public view.Gitee released a statement later that day explaining that the locked code was being manually reviewed, as all open-source code would need to be before being published from then on. The company “didn’t have a choice,” it wrote. Gitee didn’t respond to MIT Technology Review, but it is widely assumed that the Chinese government had imposed yet another bit of heavy-handed censorship.For the open-source software community in China, which celebrates transparency and global collaboration, the move has come as a shock. Code was supposed to be apolitical. Ultimately, these developers fear it could discourage people from contributing to open-source projects, and China’s software industry will suffer as a result. Read the full story. —Zeyi Yang The must-reads I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology. 1 America’s children are more anxious than everAnd it runs deeper than the pandemic. (NYT $)+ The relentless stream of bad news is making us all feel bad. (Wired $)+ How to mend your broken pandemic brain. (MIT Technology Review) 2 Digital surveillance programs make immigrants feel like prisonersThey’re touted as a more humane alternative to detention, but ankle tags carry stigma and stoke anxiety. (Coda Story)+ The CIA and US military are spending huge amounts of money on metaverse projects. (The Intercept) 3 The Wikipedia editor exposing the predatory world of cryptomaniaThat doesn’t mean she’s reveling in its current implosion. (WP $)+ Six months into the crypto crash, investors are making the same mistakes. (Motherboard) + Fraudsters are using a deepfake of Elon Musk to steal crypto. (Motherboard) + This crypto reality dating show sounds like a parody of itself. (Input Mag) 4 A new ancestry-predicting DNA tool is solving missing-people mysteries 🧬But experts are wary that DNA phenotyping could further fuel racial discrimination in policing. (NYT $) + Our museums are a treasure trove of genomic data. (Ars Technica) 5 Peter Thiel is bankrolling Trumpian Republicans in the midterm electionsThe vast fortune he’s amassed in tech is proving highly influential in crowning the party’s next leader. (The Guardian) 6 Logistics companies are using tracking tech to stop thieves stealing baby formulaThe nationwide shortage has made infant formula a highly-prized target. (WSJ $)+ The scarcity isn’t showing any signs of letting up. (New Yorker $)+ The baby formula shortage has birthed a shady online marketplace. (MIT Technology Review) 7 Decarbonizing climate change projects are gathering steamPulling carbon from the sky and locking it in mountains might be one way to do it. (Spectrum IEEE)+ Carbon removal hype is becoming a dangerous distraction. (MIT Technology Review) 8 Virtual reality is enchanting Nigeria’s care home residents Dance, music and therapy sessions are providing them with a portal into other worlds. (The Guardian) 9 How a mathematical formula can make you a better Wordle playerHint: you may have to guess words you know aren’t the answer. (Quanta) 10 The internet is a love language ❤️Sharing articles, posts and memes is a foundation of many modern relationships. (The Atlantic $) Quote of the day “I have nothing left, not even a penny.” —Mudasir, a crypto investor from Pakistan, tells Rest of World how they lost everything after buying TerraUSD, a stablecoin which has plummeted in the past fortnight. The big story Uyghurs outside China are traumatized. Now they’re starting to talk about it June 2021 The Uyghur diaspora have been forced to watch from afar as their loved ones disappear and a way of life is erased. The trauma has sparked a mental health crisis that leaders in the diaspora say is all too apparent. Many are reticent to seek help, leaving the community’s needs both underassessed and unmet. But a small group of outspoken Uyghurs is trying to change that. Using social media, they’re starting conversations about grief and mental health and, through telehealth, connecting people across the country with volunteer therapists. Its creators hope that it will help foster resilience in the diaspora—and provide a lifeline to a community during its darkest hour. Read the full story. —Andrew McCormick We can still have nice things A place for comfort, fun and distraction in these weird times. (Got any ideas? Drop me a line or tweet 'em at me.) + This cat version of Seven Nation Army is exactly what we need on a Monday.+ Improve the speed and accuracy of your typing by practicing on your favorite classic literature with this website—how many words per minute can you reach?+ The Fifth Element did a surprisingly good job of predicting the future (kinda.)+ Here’s some of the fun ways in which TikTok has wholeheartedly embraced Harry Styles’ new album. + Artist Kelsey Oseid created a tiny museum-like diorama in her wall, and it’s wonderful. (Thanks Danny!)",0.0
How censoring China’s open-source coders might backfire,https://www.technologyreview.com/2022/05/30/1052879/censoring-china-open-source-backfire/,2022-05-30,"<p>Many suspect the Chinese state has forced Gitee, the Chinese competitor to GitHub, to censor open-source code in a move developers worry could obstruct innovation.</p>
","On May 18, thousands of software developers in China woke up to find that their open-source code hosted on Gitee, a state-backed Chinese competitor to the international code repository platform GitHub, had been locked and hidden from public view. Later that day, Gitee released a statement explaining that the locked code was being manually reviewed, as all open-source code would need to be before being published from then on. The company “didn’t have a choice,” it wrote. Gitee didn’t respond when MIT Technology Review asked why it had made the change, but it is widely assumed that the Chinese government had imposed yet another bit of heavy-handed censorship. For the open-source software community in China, which celebrates transparency and global collaboration, the move has come as a shock. Code was supposed to be apolitical. Ultimately, these developers fear, it could discourage people from contributing to open-source projects, and China’s software industry will suffer from the lack of collaboration. “Code review in OSS is about improving the code quality and building trust between developers. Adding politics to the code review will hurt both, and eventually roll back the open-source movement in China,” says Han Xiao, the Berlin-based founder of Jina AI, a commercial open-source software company. GitHub, founded in 2008 and acquired by Microsoft in 2018, is the go-to platform developers around the world use to publish their code and then critique and learn from each other. This publicly available code—as opposed to the proprietary code created by companies or individuals—is referred to as open-source software. Of the 73 million people using GitHub as of 2021, 7.5 million are based in China, making them the largest group outside the United States. But that level of dependence on the platform made the Chinese government wary, especially since American sanctions against Huawei in 2019 reminded it how much the nation still relies on certain foreign companies and services. GitHub is one of them. At least one open-source software project has had malicious code added which aimed to wipe computers located in Russia and Belarus. At the same time, the open-source industry was growing fast in China. Major companies like Tencent and Alibaba released their own version of GitHub, and Gitee, backed by the established open-source community OSChina, began to take the lead in the domestic competition. So in 2020, China’s Ministry of Industry and Information Technology contracted a consortium of companies and universities, led by Gitee, to grow the existing repository into a “Chinese independent open-source hosting platform.” Gitee now boasts over 8 million users. Over time, some developers started to prefer Gitee over GitHub for a mix of reasons, from performance and cost to protection from foreign intervention. For Daniel Bovensiepen Li, a Beijing-based research scientist who uses Gitee for both personal and professional projects, its main advantage is that it is based in mainland China, which makes its service faster and more stable. “Due to this proximity, the performance is dramatically better than GitHub or GitLab [a similar overseas platform],” he says. Li has 24 projects hosted on Gitee that were affected by the latest change. Institutions with government ties are more likely to use Gitee. “The military, public universities, and state-owned companies—they are concerned with the fact that GitHub is eventually owned by Microsoft, an American company,” says Thomas Yao, the Shanghai-based founder of GitCafe, one of China’s earliest GitHub-like websites, which he sold to Tencent in 2016. Students and amateur developers can also be deterred from using GitHub by the costs and the difficulty of finding reliable VPN services in China, he says. For now, there’s little clue as to what prompted the change, but censorship of certain types of language—profanity, pornography, and politically sensitive words—has been creeping up on the platform for a while. On Gitee’s official and public feedback page, there are multiple user complaints about how projects were censored for unclear reasons, possibly because technical language was mistaken for a sensitive word. The immediate result of Gitee’s May 18 change was that public projects hosted on the platform suddenly became unavailable without notice. Users complained that this disrupted services or even ruined their business deals. For the code to be made public again, developers need to submit an application and confirm it doesn’t contain anything that violates Chinese law or infringes copyrights. Li went through the manual review for all his projects on Gitee, and so far 22 out of 24 have been restored. “Yet I assume that the review process is not a one-time thing, so the question is if the friction of hosting projects will increase in the future,” he says. Still, with no better domestic alternative, Li expects users to stay: “People might not like what Gitee is doing, but [Gitee] will still be required to get their daily job done.” In the long run, this puts an unreasonable burden on the developers. “When you are coding, you are also writing comments and setting up names for the variables. Which developer, while writing code, would like to be thinking whether their code could trigger the list of sensitive words?” says Yao. With almost every other aspect of the internet, the Chinese way of building its own alternative has worked well in recent years. But with open-source software, a direct product of cross-border collaboration, China seems to have run into a wall. “This push to insulate the domestic open-source community from risks arising from the global community is something that very much goes against the core proposition of open-source tech development,” says Rebecca Arcesati, an analyst at the Mercator Institute for China Studies and coauthor of a report on China’s bet on open-source. Technologists in China, she says, don’t want to be cut off from the global software development conversation and may feel uncomfortable with the direction China is heading: “The more Beijing tries to nationalize open-source and create an indigenous ecosystem, the less eager developers will be to participate in what they perceive to be government-led open-source projects.” And cutting off its global ties prematurely may interrupt the fast growth of China’s open-source software industry before its benefits to the economy can be realized. It’s part of a broader concern that overshadows China’s tech sector as the government has ramped up regulations in recent years: is China sacrificing the long-term benefits of tech for short-term impact? “I struggle to see how China can make do without those global links with international open-source communities and foundations,” Arcesati says. “We are not there yet.”",0.0
5G private networks enable business everywhere,https://www.technologyreview.com/2022/05/19/1052138/5g-private-networks-enable-business-everywhere/,2022-05-19,"The world is rapidly moving from human-directed manufacturing using computerized assembly lines to largely automated smart factories that manufacture more efficiently using real-time data. Considered by many to be the fourth industrial revolution, or “Industry 4.0,” this transformation requires a bevy of technologies to deliver on its promise of ultra-reliable low-latency communications (URLLC). From smart devices…","In association withKeysight The world is rapidly moving from human-directed manufacturing using computerized assembly lines to largely automated smart factories that manufacture more efficiently using real-time data. Considered by many to be the fourth industrial revolution, or ""Industry 4.0,"" this transformation requires a bevy of technologies to deliver on its promise of ultra-reliable low-latency communications (URLLC). From smart devices to machine-learning systems to pervasive communications, the need for ultra-high speeds and reliability requires technologies that can connect in a variety of situations while remaining compliant with regional regulations. Technology and telecommunications providers have created a solution—5G private networks—to address the challenge. The manufacturing industry is exploring 5G technology at an accelerated pace, largely to enable AI-driven use cases such as closed-loop manufacturing, adaptive manufacturing, predictive analytics for maintenance, and extended reality (XR)-based worker training and safety, says Jagadeesh Dantuluri, general manager for private and dedicated networks at Keysight Technologies. ""It's not about a static assembly line performing the same action time and time again, but one that can change based on their needs,"" he says. ""Private networks essentially enable new business models in manufacturing."" Yet, the benefits of 5G private networks extend beyond manufacturing. Because the technology offers more reliable connectivity, faster data rates and lower latency, and greater scalability, security, and network control than previous communications technologies, 5G private networks will drive innovations in many industrial and enterprise sectors. A private cellular network is built on 3rd Generation Partnership Project (3GPP)-defined standards (such as LTE or 5G), but it offers dedicated on-premise coverage. This is important for remote facilities where public networks do not exist, or where indoor coverage is not robust. A private network also makes exclusive use of the available capacity; there is no contention from other network users, as on a public network. Private operators can also deploy their own security policies to authorize users, prioritize traffic, and, most importantly, to ensure that sensitive data does not leave the premises without authorization. ​​The dedicated nature of 5G private networks coupled with a customized service, intrinsic control, and URLLC capabilities provides more reliable industrial wireless communication for a wide variety of use cases, Dantuluri says “Applications include wireless, real-time, closed-loop control and process automation, and AI-based production and AR/VR-based design for onsite and remote workers,” he explains. “In addition, low-cost connectivity allows sensors to become easily deployed in a wider variety of scenarios, allowing businesses to create innovative applications and collect real-time data.” ​The industrial sector is driving toward a massive digital transformation, and the integration of information-technology (IT) systems with operational-technology (OT) systems will speed up this process. Digital technologies will also enable many new use cases, such as automated manufacturing. A 5G private network enables a facility to synchronize and integrate tracking data into its workflow, allowing production lines to be configured in real time, says Dantuluri. “Since the factory’s assembly lines and infrastructure, such as robotic arms, autonomous mobile robots (AMRs), autonomous guided vehicles (AGVs), and sensors, are wirelessly connected, configuring or moving assembly elements on demand is much easier. This use case demands highly reliable, low-latency wireless connectivity and coverage, and potentially high data rates in both the uplink and downlink, and maybe support for Time Sensitive Networks (TSN) in the future. This use case application can only be achieved with 5G private networks.” Outside the industrial sector, 5G private networks enable mobile augmented-reality (AR) and virtual-reality (VR) applications, allowing, for example, engineers to view superimposed blueprints, soldiers to have heads-up displays, and businesses to have virtual meetings in the field or working remotely. ""If a machine has to be repaired, and a technician or a factory worker has AR goggles, they can have technical information superimposed on the real-world device to see what is wrong,"" says Dantuluri. ""And the data center can send instructions about how to do the repairs, step by step."" As enterprises realize the benefits of pervasive, low-latency, high-bandwidth, and secure connectivity, the applications of 5G private networks will expand. By the end of 2024, analysts expect investment in 5G private networks will add up to tens of billions of dollars. A separate analysis by the research arm of investment firm JP Morgan predicts that the global enterprise opportunity for 5G will exceed $700 billion by 2030. Better security 5G private networks have improved upon previous 4G standalone network security and are better able to address several existing security threats. Like most new technology, 5G private networks will likely have security issues that need to be addressed, but security has become a primary consideration in both developing the standards for 5G and in the implementation approaches. In addition, companies can further augment those security features with novel technologies, such as more robust encryption schemes and zero trust architecture, as private networks afford complete control to its owner—a benefit not possible on public networks. The focus on improving security will drive new and innovative applications, especially in high-security areas such as seaports and airports, says Dantuluri. ""Private networks provide the flexibility of movement that seaports require,"" he says. ""In airports, after a plane lands, engine data can begin downloading before the plane even docks to the gate, which saves a lot of time and helps airlines stay on schedule."" Wireless flexibility Most manufacturing robots are tethered to wired networks, but improved connectivity and better security means that connected devices can more easily move around and stay connected to necessary systems and data. In addition, 5G networks are built to allow devices to remain connected when moving between cells, whereas many Wi-Fi networks require devices to reconnect after moving. This advantage pays off in scenarios where a large area needs to be covered by a wireless network, Dantuluri says. ""Facilities like mines, airports, and seaports require significant geographic coverage in the order of several square kilometers,"" he says. ""Other wireless technologies have very limited range, making them unsuitable for these use cases."" In addition, there are benefits for remote applications as well. Today, most offshore oil rigs, for example, rely on separate satellite communications and local networks. Not only are 5G private network connections more secure and interoperable, but they reduce the cost of hybrid communications, combining local area, cellular, and satellite networks. Industry has quickly evolved over the past two decades, from steam-powered machines automating manufacturing, to assembly lines simplifying production, to computerized systems creating more precise products. Machine learning and fast, reliable connectivity promise to make the next industrial revolution, Industry 4.0, possible. Every industry will apply Industry 4.0 advances to help improve their operations. 5G private networks will be crucial to that effort. ""Today, automation is significant, but is all done with wires, so systems—industries, robotics, sensors—are difficult to quickly customize,"" says Dantuluri. ""As 5G private network adoption increases, all systems will be automated and connected with low-latency wireless, which will enable adaptive business models."" This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.",0.0
Transforming the automotive supply chain for the 21st century,https://www.technologyreview.com/2022/05/12/1052201/transforming-the-automotive-supply-chain-for-the-21st-century/,2022-05-12,"Geo-political tensions and digital transformation—which continue to reshape production and assembly processes across various industries—have exposed vulnerabilities in the traditional “just-in-time” (JIT) supply-chain model in the last few years. Pioneered in Japan and popularized by Toyota in the 1970s, the JIT method aims to avoid excess inventory by ordering products and raw materials only when…","In partnership withInfosys Cobalt Geo-political tensions and digital transformation—which continue to reshape production and assembly processes across various industries—have exposed vulnerabilities in the traditional “just-in-time” (JIT) supply-chain model in the last few years. Pioneered in Japan and popularized by Toyota in the 1970s, the JIT method aims to avoid excess inventory by ordering products and raw materials only when needed to minimize costs and maximize efficiency. But manufacturers of products, from automobiles to consumer electronics, are seeing just how tenuous the JIT model is. When the covid-19 pandemic hit in early 2020, manufacturers of automobiles, electronics, and other tech products expected sales to decrease and so they reduced semiconductor orders. Two years later, the industry is still reeling from that decision. Reducing semiconductor orders resulted in a global shortage of chips that are used to power everything from artificial intelligence to medical equipment, causing some manufacturers to have to pause production. In the automotive industry, for example, US car inventory dropped to below 1 million units in the US in the second half of 2021—roughly one-third of the pre-pandemic level—according to automotive industry services provider Cox Automotive. For the JIT model to work, the quality and supply of raw materials, the production of goods, and the customer demand for them must remain in alignment. If any one of the links in the chain breaks, stalls, or falls out of sync, the impact on the supply chains that crisscross the world can be felt immediately. For companies, unable to deliver on orders in a timely fashion, they risk losing not only efficiency gains but also brand credibility, market share, and revenue. Now, companies are seeking new ways of managing their supply chains that offer greater flexibility and transparency. In the automotive sector, some companies including Nissan and JIT pioneer Toyota are increasing chip inventory levels, while others including Volkswagen and Tesla are trying to secure their own supplies of rare metals. But technologies, including Internet of Things (IoT), 5G, and business applications are also offering companies new ways to avoid disruption and respond to unforeseen circumstances. The transformation of the automotive supply chain is taking place in an increasingly-digitized world, beset with environmental concerns. As climate change concerns intensify, and governments across the world compel industries to switch to more environmentally-friendly practices, the automotive industry and its supply chain networks are undergoing a profound shift. Automotive manufacturers are moving away from internal combustion engines and large-scale manufacturing to zero-emission, carbon-neutral electric or autonomous vehicles with a focus on electric or hydrogen as energy sources. Autonomous vehicles, for example, are seen as “servers on wheels” that rely on batteries, wiring, laser technology, and programming rather than combustion engines. Tech giants such as Japan’s Sony and China’s Baidu have also announced plans for their own electric vehicles (EV), fueling an already heated race in the EV market. According to the International Energy Agency, global sales of electric cars hit 6.6 million in 2021, making up 8.6% of all new car sales: more than double the market share from 2020, and up from a mere 0.01% in 2010. Business insights provider IHS Markit estimates the number of EV models in the US will increase 10 times over, from 26 in 2021 to 276 in 2030. At the same time, charging stations alone will need to increase from 850,000 in 2021 to nearly 12 million in 2030. To meet the increasing need for battery-powered vehicles, manufacturers must establish a new ecosystem of partners that supplies the parts and accessories required for the successful manufacturing and operating of these alternative vehicles. According to research from Transport Intelligence, “the supply chain for the entire powertrain will be transformed and the types of components, the logistics processes employed to move them, the markets of origin and destination as well as the tiered character of automotive supply chains will change.” This has enormous implications for how the automotive supply chain is ordered. Meanwhile, everything in the automotive sector, from the automobiles themselves to entire factories, is becoming more connected, with the support of technologies such AI, IoT, 5G, and robotics. In recent months, Nissan has unveiled its “Intelligent Factory” initiative in its Tochigi plant in the north of Tokyo, which employs AI, IoT, and robotics to manufacture next-generation vehicles in a zero-emission environment. And Volkswagen has deployed a private 5G wireless network at its headquarter plant in Wolfsburg, Germany, to trial new smart factory use cases. As manufacturing becomes more digitized, so too does consumer behavior. Automotive brands are rolling out direct-to-consumer sales models, enabling customers to complete more and more of the sales process through digital channels. While new players are taking an online-only approach to the sales model, incumbents are embracing digital initiatives in partnership with dealers where fulfillment, after sales, and services are still provided through a dealer. In 2020, 69% of dealers in the US added at least one digital step to their sales process. And 75% of dealers agreed that they would not be able to survive long term without moving more of the sales process online. Both models require greater visibility into the supply chain to ensure inventory and availability are accurate. Ever more connected consumers, factories, automobiles, and supply chains generate a wealth of data. Gathering and analyzing this data can help enable manufacturers to reduce business risk and become more agile by identifying potential supply issues, increasing efficiencies, and giving customers more accurate timelines. Predictive analytics, for example, can help manufacturers answer the “What if?” questions and proactively reduce the impact of potential supply chain disruptions. Digital traceability enables companies to follow products and goods as they move along the value chain, providing them with exact information on the provenance of inputs, supplier sourcing practices, and conversion processes. “On the demand side, customers expect real-time visibility of when an automobile will be delivered to them, and the status of service, spare parts, and accessories,” says Mohammed Rafee Tarafdar, SVP and CTO, Infosys. In a bid to harness data and develop greater visibility across the business, manufacturers are employing a variety of technology solutions including business applications—suites of software designed to support business functions. Paired with cloud services, the right business applications can give organizations greater access to cutting-edge technologies, which can then be managed at scale and address the need for visibility, analytics, and cybersecurity. As everything becomes more connected and more autonomous, “there is a need to have technology that can scale with demand. This is where cloud and business applications have very important roles to play,” says Tarafdar, who adds that manufacturers are embracing both private and public cloud to create hybrid clouds, with the support of private 5G networks. As manufacturers modernize the supply chain with the hope of making it more agile and resilient, Tarafdar says that manufacturers will start moving toward a sentient supply chain; that is, a supply chain that can sense, process, and respond in real time. Sentience is applicable to a few areas within the supply chain, including planning, insight, traceability, analytics, and asset and inventory management. For example, the sentient supply chain can recognize if inventory needs to be moved between factories and take the appropriate actions or at least make recommendations subject to approval. “Some of these capabilities aren’t fully baked into the platforms that are currently available,” says Tarafdar, “But investments in cloud, AI, and technology that models the entire supply chain network into a graph, essentially enables us to create a digital brain that can make informed decisions or recommendations.” It’s hard to tell when such digital intelligence will be fully formed. But one thing that experts seem to agree on: the next decade could represent one of the biggest tech disruptions in the global automotive industry, as the line between tech and auto companies becomes increasingly blurred. The development of autonomous and electric vehicles will inject new players into the automotive industry and significantly alter its supply chain practices. Coupled with globalization, disruptive technologies, shifting consumer demands, and changes in manufacturing processes, the automotive supply chain is likely to face some bumps in the road ahead. To drive production efficiency and accelerate time-to-market, attaining optimal supply chain visibility is key. And automakers may need to adopt and adapt to new technologies in cloud, AI, data analytics, machine learning, and IoT to ensure that the new ecosystem of their suppliers will be transparent, agile, and resilient to global volatilities. Learn more about the Infosys Microsoft Cloud Business here. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.",0.0
"The Download: Russia’s satellite hack, and Shanghai’s intensifying lockdown",https://www.technologyreview.com/2022/05/11/1051992/download-russia-us-satellite-hack-shanghai-intensifying-lockdown-china-covid/,2022-05-11,"<p>Plus: Elon Musk says he'd reverse Twitter's decision to ban Donald Trump</p>
","This is today's edition of The Download, our weekday newsletter that provides a daily dose of what's going on in the world of technology. What happened: Just an hour before Russian troops invaded Ukraine, Russian government hackers targeted the American satellite company Viasat, officials from the US, EU, and UK have confirmed. The operation resulted in an immediate and significant loss of communication in the earliest days of the war for the Ukrainian military.How it unfolded: The attack, on February 24, launched destructive “wiper” malware against Viasat modems and routers, quickly erasing all the data on the system. The machines then rebooted and were permanently disabled. Thousands of terminals were effectively destroyed in this way. Why it matters: The Viasat cyberattack is the biggest known hack of the war, says Juan Andres Guerrero-Saade, a threat researcher at the cybersecurity firm SentinelOne. It is also one of the first real-world examples of how cyberattacks can be targeted and timed to amplify military forces on the ground by disrupting and even destroying the technology used by enemy forces. Read the full story. —Patrick Howell O’Neill “We don't think that it's sustainable.” —Tedros Adhanom Ghebreyesus, director-general of the World Health Organization, tells a press conference that China’s zero covid policy can’t last, amid a rising wave of omicron-linked cases, AFP reports. I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology. 1 Shanghai’s lockdown is getting even stricterAs it enters its seventh week, tough restrictions are being imposed on basic needs like food and healthcare. (BBC)+ The lockdown has forced delivery drivers thousands to sleep in the street. (WSJ $)+ Tensions between residents and lockdown enforcers are on the rise again. (The Guardian)2 Elon Musk says he would lift Donald Trump’s Twitter banDespite the great lengths the company went to explain why they’d banned him in the first place. (FT $)+ And Twitter co-founder Jack Dorsey agrees with him. (Axios)+ Musk says his plans for Twitter adhere to the EU’s rules for policing illegal content. (WSJ $)+ How to support women and LGBTQ people who are being abused online. (Coda Story)3 YouTube Brain is a thingAnd it’s causing some of the platform’s biggest stars to burn out. (Vox)+ YouTube is also a powerful tool to rewrite the past. (Wired $)4 Why you should reconsider choosing next-day deliveryIs our mindless consumerism putting too much pressure on workers—and the environment? (The Guardian)+ Amazon is snapping up all the UK’s warehouses. (FT $)5 Tick-borne diseases are spreading across the USPartly because there’s no national network to monitor where they are. (Wired $)6 Netflix could introduce adverts by the end of the yearThat’s much sooner than previously believed. (NYT $)7 New internet cables put Big Tech in charge of Africa’s connectivityWhich raises big questions about how much control private companies should have over internet access. (Rest of World) 8 Synthetic data is supposed to make surveillance AI more ethicalBut critics worry it could be used to legitimize AIs that don’t work. (Protocol)9 The iPod is deadMore than 20 years after it helped to turn Apple into a consumer tech giant. (NYT $) 10 What can data teach us about dating? Almost nothing.It’s virtually impossible to predict if you’ll hit it off with someone or not. (Wired $)+ Speed dating is rising in popularity among time-pressed singletons. (NYT $) A place for comfort, fun and distraction in these weird times. (Got any ideas? Drop me a line or tweet 'em at me.) + This cool site makes a word cloud of the most frequently used words in your Spotify playlists.+ Well, a dinosaur turning up at a children’s birthday party went exactly as expected.+ A fun explainer why flash mobs, while unfashionable, will never die.+ An alligator in desperate need of caffeine recently broke into a garage to steal some Diet Coke.+ Happy birthday to Jeremy Paxman, the UK’s answer to Alex Trebek, who turns 72 today.+ Microsoft Flight Simulator is getting a Top Gun expansion.+ Meet the mother and daughter florists bringing a splash of flower power to Silicon Valley.",0.0
Russia hacked an American satellite company one hour before the Ukraine invasion,https://www.technologyreview.com/2022/05/10/1051973/russia-hack-viasat-satellite-ukraine-invasion/,2022-05-10,"<p>The attack on Viasat showcases cyber’s emerging role in modern warfare.</p>
","Just an hour before Russian troops invaded Ukraine, Russian government hackers targeted the American satellite company Viasat, officials from the US, EU, and UK said today. The operation resulted in an immediate and significant loss of communication in the earliest days of the war for the Ukrainian military, which relied on Viasat’s services for command and control of the country’s armed forces. The Viasat cyberattack is the biggest known hack of the war, says Juan Andres Guerrero-Saade, a threat researcher at the cybersecurity firm SentinelOne ""because it’s the most concerted effort to disable Ukrainian military capabilities.” It is also one of the first real-world examples of how cyberattacks can be targeted and timed to amplify military forces on the ground by disrupting and even destroying the technology used by enemy forces. The attack, on February 24, launched destructive “wiper” malware called AcidRain against Viasat modems and routers, quickly erasing all the data on the system. The machines then rebooted and were permanently disabled. Thousands of terminals were effectively destroyed in this way. Soldiers and tanks may care about national borders. Cyber doesn't. Guerrero-Saade, who has been at the forefront of research into AcidRain, says that where previous malware used by the Russians was narrowly targeted, AcidRaid is more of an all-purpose weapon. “What’s massively concerning about AcidRaid is that they’ve taken all the safety checks off,” he says. “With previous wipers, the Russians were careful to only execute on specific devices. Now those safety checks are gone, and they are brute-forcing. They have a capability they can reuse. The question is, what supply-chain attack will we see next?” The attack has turned out to be typical of the “hybrid” war strategy employed by Moscow, say experts. It was launched in concert with the invasion on the ground. That exact kind of coordination between Russian cyber operations and military forces has been seen at least six times, according to research from Microsoft, underlining the emerging role of cyber in modern warfare. “Russia’s coordinated and destructive cyberattack before the invasion of Ukraine shows that cyberattacks are used actively and strategically in modern-day warfare, even if the threat and consequences of a cyberattack are not always visible for the public,” the Danish defense minister, Morten Bødskov, said in a statement. “The cyber threat is constant and evolving. Cyberattacks can do great damage to our critical infrastructure, with fatal consequences.” In this instance, the damage spilled over from Ukraine to affect thousands of internet users and internet-connected wind farms in central Europe. And the implications are even bigger than that: Viasat works with the US military and its partners around the world. “Obviously, the Russians messed it up,” says Guerrero-Saade. “I don’t think they meant to have so much splash damage and get the European Union involved. They gave the EU pretext to react by having 5,800 German wind turbines and others around the EU impacted.” Just a few hours before AcidRain began its destructive work against Viasat, Russian hackers used another wiper, called HermeticWiper, against Ukrainian government computers. The playbook was eerily similar, except instead of satellite communications, the targets were Windows machines on networks that, in those early hours of the invasion, would be important for the government in Kyiv to mount an effective resistance. How effective these attacks have been remains an open question. A senior Ukraine official said the Viasat hack resulted in a “huge loss in communications in the very beginning of war” but offered no detail. Cyber is supporting military operations, but it’ll be a long time before we get a full view of all of the operations in play during this war. It’s clear from the way AcidRain was built, though, that we will likely see it in action again.",0.0
Global Cloud Ecosystem Index 2022,https://www.technologyreview.com/2022/04/25/1051115/global-cloud-ecosystem-index-2022/,2022-04-25,,"In partnership with Infosys Cobalt The Global Cloud Ecosystem Index is a ranking of 76 countries and territories according to how well technology, regulations, and talent promote the availability of cloud services. It also evaluates and compares the regulatory frameworks and digital practices that promote the use of cloud models in the public and private sector. The Global Cloud Ecosystem Index rates and ranks each of the world’s major economies according to how well technology, regulations, and talent promote the availability of cloud services. It also evaluates and compares the regulatory frameworks and digital practices that promote the use of cloud models in the public and private sector. This pillar consists of various data points that indicate how well each country is served by telecommunications networks and computing resources which enable cloud-centric production models. These six indicators combine the extent to which each country’s constituents access the outputs of cloud application and services across public and private sectors. This pillar measures the maturity of regulatory environments that promote progressive, cloud-forward data security and sovereignty environments—and enable trust in digital resources. The indicators in this pillar examine each country’s human capital assets that can contribute to a cloud-based digital economy. Today, cloud is computing—a foundational resource for businesses and governments alike as they strive to harness emerging technologies such as 5G, artificial intelligence, and the internet of things. “The cloud is going to play a very important role in providing resilience, agility, and low latency for heavy workloads with millions of people accessing applications.” “We select our cloud region locations based on customer demand to make services easier and faster for companies, and we also invest in those locations to make sure we have the right talent to serve our customers.” “The future is extremely exciting, almost unimaginably so, when we consider the metaverse and the future of digital business—and the cloud is a very important enabler of both.” The Global Cloud Ecosystem Index 2022 is a snapshot of worldwide cloud development and innovation. It ranks 76 nations and territories on the technology, regulations, and talent they use to promote cloud computing services. The Index consolidates scores given to each nation or territory across four themes, or pillars: infrastructure, ecosystem adoption, security and assurance, and talent and human affinity. The Global Cloud Ecosystem Index is based on the analysis of global macroeconomic, labor, trade, and technology data and primary research interviews with global technology developers, analysts, and policymakers. It quantifies the economies of 76 countries and territories along four separate pillars: infrastructure, ecosystem adoption, security and assurance, and talent and human affinity. Within each pillar, a series of indicators—lists of qualitative and quantitative factors—were then selected and populated. Through trend analysis and research, weighting assumptions were assigned to determine the relative importance with which each indicator and pillar influenced a country’s cloud computing posture. MIT Technology Review was founded at the Massachusetts Institute of Technology in 1899. MIT Technology Review Insights is the custom publishing division of MIT Technology Review. We conduct qualitative and quantitative research and analysis worldwide and publish a wide variety of content, including articles, reports, infographics, videos, and podcasts. If you have any comments or queries, please get in touch. Infosys Cobalt is a set of services, solutions, and platforms that acts as a force multiplier for cloud-powered enterprise transformation. It offers 35,000 cloud assets and over 300 industry cloud solution blueprints. Infosys Cobalt helps businesses redesign the enterprise, from the core, and also build new cloud-first capabilities to create seamless experiences in public, private and hybrid cloud, across PaaS, SaaS, and IaaS landscapes. With Infosys Cobalt’s community leverage, enterprises can rapidly launch solutions and create business models to meet changing market needs while complying with the most stringent global, regional and industry regulatory and security standards. To learn more about the cloud, visit The cloud hub: From cloud chaos to clarity. For more details on Infosys Cobalt, visit us at infy.com/infosyscobalt.",0.0
05/20/2022 - Import AI 295: DeepMind's baby general agent; NVIDIA simulates a robot factory; AI wars. - 0,http://eepurl.com/h2sj1D,2022-05-20,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Import AI is taking a short break (and here's why): Import AI is taking two weeks off! That's because I (Jack Clark) am serially bad at taking holiday. I've recently worked with my colleagues to figure out some time and so I'm going to spend the next couple of weeks hiking and traveling and reading and generally trying to unplug my brain a bit so it can do some deep background cogitation. I admit that the thought of not writing this newsletter for two weeks fills me with a bit of anxiety, but I also think sometimes you need to step away so you can come back fresher in the future. I hope all of you, my dear readers, are doing well, and I'll be back soon. Now, back to the regularly scheduled programming!... CRPD: Chinese license plate recognition: …A basic dataset for a useful capability… Researchers with the University of Electronic Science and Technology of China have built a dataset for recognizing Chinese license plates. The authors use the dataset to train some models that get state-of-the-art accuracy while running at 30 frames per second. The dataset: The Chinese Road Plate Dataset (CRPD) contains 25k images (around 30k total). Each image is annotated with the Chinese and English characters of the depicted license plate, the coordinate of the vertices of the license plates, and the type of license plate (e.g, whether for police cars, small cars, etc). Images for the dataset were ""collected from electronic monitoring systems in most provinces of mainland China in different periods and weather conditions,"" the authors write. Why this matters: Datasets like CRPD represent the basic infrastructure on which AI capabilities get developed. It's also notable how universities in China can access large-scale surveillance datasets. Read more: Unified Chinese License Plate Detection and Recognition with High Efficiency (arXiv). Get the dataset: Github https://github.com/yxgong0/CRPD",0.0
05/20/2022 - Import AI 295: DeepMind's baby general agent; NVIDIA simulates a robot factory; AI wars. - 1,http://eepurl.com/h2sj1D,2022-05-20,,"#################################################### DeepMind builds a (very preliminary) general AI agent: …AKA: The dawn of really preliminary, general AI systems.. In the past few years, the dumbest thing has tended to work surprisingly well. Take for example GPT3 - just scale-up next word prediction on an internet-scale corpus and you wind up with something capable of few-shot learning, fielding a vast range of NLP capabilities. Another example is computer vision systems - just create a vast dataset and you wind up with increasingly robust vision systems. Or contrastive learning - just embed a couple of modalities into the same space and sort of flip-flop between them through the learning process and you get powerful multimodal systems like CLIP. Now DeepMind has done the same thing for reinforcement learning with GATO, an agent where basically DeepMind takes a bunch of distinct tasks in different modalities and embeds them into the same space, then learns prediction tasks from them. The result is a system where ""the same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens."" This is wild stuff! What GATO can do: After training, GATO can do okay at tasks ranging from DeepMind Lab, to robot manipulation, to the procgen benchmark, to image captioning, to natural language generation. It's a big deal: The fact you can take a bunch of different tasks from different modalities and just… tokenize them… and it works? That's wild! It's both a) wildly dumb and b) wildly effective, and c) another nice example of 'The Bitter Lesson', where given enough compute/scale, the dumb things (aka, the simple ones) tend to work really well. In a small package: The largest (disclosed here) GATO agent is 1.18 billion parameters, making it fairly small in the grand scheme of recent AI developments. An even crazier thing: The GATO model only has a context window of 1024 tokens (by comparison, GPT3 was 2048 when it launched), so the fact 1024 tokens is enough to get a somewhat capable multimodal agent is pretty surprising. Why this matters: ""Although still at the proof-of-concept stage, the recent progress in generalist models suggests that safety researchers, ethicists, and most importantly, the general public, should consider their risks and benefits,"" DeepMind writes. Check out the blog: A Generalist Agent (DeepMind website). Read more: A Generalist Agent (DeepMind PDF).",0.0
05/20/2022 - Import AI 295: DeepMind's baby general agent; NVIDIA simulates a robot factory; AI wars. - 2,http://eepurl.com/h2sj1D,2022-05-20,,"#################################################### Chinese researchers build a large multi-modal dataset, and evaluation suite: …'Zero' makes it easier to develop AI systems for the Chinese cultural context… Chinese researchers with startup Qihoo 360 AI Research and the Department of Automation at Tsinghua University have built Zero, a benchmark for assessing the quality of vision-text Chinese AI models. Zero consists of a dataset (the Zero-Corpus, consisting of 23-million image-text pairs, filtered via high click through rates - so the top image people click in response to a query), as well as five downstream datasets for evaluating Chinese vision-text models (an Image-Caption Matching Dataset, an Image-Query Matching dataset, an Image-Caption Retrieval Dataset, an Image-Query Retrieval Dataset, and a Chinese-translated version of the Flickr30k dataset). Model training: The authors also train a model, called R2D2, on the corpus. They show that their model significantly outperforms another Chinse model named Wukong. R2D2 incorporates some pre-ranking techniques to improve its performance. Why this matters: The main idea behind datasets and models like this is described in the paper: ""promote the development of Chinese vision language learning. We expect that a fair Chinese cross-modal benchmark and a good cross-modal framework will encourage a plethora of engineers to develop more effective methods in specific real-world scenarios, such as searching images by texts."" Read more: Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework (arXiv).",1.0
05/20/2022 - Import AI 295: DeepMind's baby general agent; NVIDIA simulates a robot factory; AI wars. - 3,http://eepurl.com/h2sj1D,2022-05-20,,"#################################################### NVIDIA makes some efficient Factory simulation software: …Finally, a physics simulator built around the needs of robots… Researchers with NVIDIA and the University of Washington have built Factory, software for doing rich, efficient physics situations of robots. Factory is basically some highly optimized simulation software, with NVIDIA claiming significant performance speedups relative to widely-used software like Bullet. NVIDIA claims Factory can be used to do ""100s to 1000s of contact-rich interactions"" that can be ""simulated in real-time on a single GPU"". What Factory includes: - Physics simulation: A module for physics simulation, available within the 'PhysX' physics engine, as well as NVIDIA's robot software simulation tech, Isaac Gym - A robot learning suite: A 'Franka' robot and rigid-body assemblies from NIST's 'Assembly Task Board 1' benchmark. This suite includes 60 robotic assets, 3 robotic assembly environments (a nut-and-bolt test, a peg insertion task, and a 4-party gear assembly task), and 7 classical robot controllers. - Prototype reinforcement learning: Some basic RL policies (trained via PPO) for a simulated Franke robot to help it solve the NIST challenge. Why this matters: One of the blockers on deploying AI-driven robots into the world is the challenge in crossing the 'sim-2-real' gap. Software like Factory makes that gap a lot narrower, and also makes it cheaper to explore what it takes to cross it. Read more: Factory: Fast Contact for Robotic Assembly (arXiv).",0.0
05/20/2022 - Import AI 295: DeepMind's baby general agent; NVIDIA simulates a robot factory; AI wars. - 4,http://eepurl.com/h2sj1D,2022-05-20,,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute When and how should you collect more demographic data in the pursuit of algorithmic fairness? … Good data governance and cryptographic methods can help, but they don't undo the systemic challenges to fairness … Researchers from the Partnership on AI have written about one of the core challenges in algorithmic fairness: squaring the need for more demographic data with how such data can harm the people it was meant to help. The core challenge: Most algorithmic approaches to fairness require the collection of demographic data (“an attempt to collapse complex social concepts into categorical variables based on observable or self-identifiable characteristics”) which often ignores the broader questions of politics and governance surrounding that data. In some cases, such data collection is prohibited by anti-discrimination law, further complicating the assessment and subsequent mitigation of bias. Given such gray areas, companies hesitate to gather this data explicitly to err on the side of not violating privacy and other legal mandates. Individual and community risks to demographic data collection: Concerns around demographic measurement occur due to narrow and fixed categories predetermined by companies. While privacy is a primary concern at the individual level, harm also arises from misrepresentation of the individual and the use of their data beyond initial consent. Given that algorithmic decision-making systems are used to make inferences about groups, there are additional risks such as undue surveillance, privacy dependency, group misrepresentation, and a loss in the agency of self-determination in what is considered fair and just. Some solutions: K-anonymity, p-sensitivity, and differential privacy are proposed as solutions, along with various approaches to participatory data governance through data cooperatives and data trusts. Other solutions like secure multi-party computation are also mentioned. The key point that the authors raise is that the collection of more demographic data should only be done when it empowers more self-determination and agency for data subjects rather than an attempt by companies to “selectively tweak their systems and present them as fair without meaningfully improving the experience of marginalized groups.” Why it matters: The biggest challenge that plagues the implementation of algorithmic fairness in real-world systems is the tension presented by legal requirements to minimize demographic data collection and the need for most modern approaches to fairness requiring that very same data. As more regulations come to market, we will be faced with an ever-growing set of (potentially conflicting) requirements on how fairness should be addressed and what data is allowed to be collected. How companies with users spanning multiple jurisdictions and serving many demographic groups solve these challenges in production-grade systems will be a key space to watch to learn if the current crop of methods actually works in practice. Read more: [2205.01038] Demographic-Reliant Algorithmic Fairness: Characterizing the Risks of Demographic Data Collection in the Pursuit of Fairness.",0.0
05/20/2022 - Import AI 295: DeepMind's baby general agent; NVIDIA simulates a robot factory; AI wars. - 5,http://eepurl.com/h2sj1D,2022-05-20,,"#################################################### Tech Tales: Form and Function and War [The battlefields of Earth - 2028 - 2040] For a while, wars were fought in technicolor. That's because the humans figured out that they could confuse AI systems by varying the colors of their machines of war. Drones stopped being grey and started being rainbow colored. Quadcopters changed their black and tan shades for tie dye. This lasted for a while, as different armies sought to confuse eachother. Of course, the AI systems adapted - given enough data, they learned to see past the unexpected and re-identify their targets. The next logical place was shape - army engineers worked to divorce form from function, and were happy to pay aerodynamic efficiency prices in exchange for things that could no longer be seen. Missiles became mushroom shaped. Planes started to take on the form of weather balloons and even stranger things. Artillery became housed within bouncy castles. The footage of these wars was surreal - fields of fake trees that were in fact autonomous sniper towers. Lines of bouncy castles launching multicolored balloons into the air which sailed overhead before coming down and exploding in white-light and white-heat and concussive thumps. Armies of golf carts that vroom'd through urban centers before detonating. Again, the AI systems adapted. They learned to understand some of the concepts of war - learned, pretty quickly, to become suspicious of anything and everything. This led to the situation we find ourselves in today - wars are now invisible. In fact, wars haven't occurred for several years. That's because the AI systems learned strategy and counter-strategy and so now fight wars in secret, tussling via trade and litigation and standards and all the other things that shape the context for how nations relate to one another. The AI systems are continually evolving new strategies; it is as though they're now playing chess on boards whose dimension a human mind cannot comprehend. Yet in the military centers of the world powers, computers everyday output their gnomic probabilities - the probability the nation will continue to exist in some time period in the future, as judged by the strategist AIs, playing their inscrutable games. Neither a cold or a hot war - instead, a neverending existential negotiation. Things that inspired this story: How war strategists always seek to find the 'high ground' and what 'high ground' means conceptually; the logical endpoint of a conflict is to win the conflict before it has started; adversarial AI and adversarial examples; evolutionary pressure. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
05/10/2022 - Import AI 294: China makes a vast facial recognition dataset; Facebook releases a 30bn parameter model; real world RL - 0,http://eepurl.com/h1FRp5,2022-05-10,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. China makes the largest (public) face recognition dataset yet: …WebFace260M lets you train AI systems to identify millions of people… Researchers with Tsinghua University, XForwardAI (an AI startup), and Imperial College London have built 'WebFace260M', a large-scale dataset for facial recognition. Models trained on the resulting dataset are pretty good - the authors submit one model to NIST's challenging FVRT challenge and rank third overall. Vast dataset: WebFace 260M isn't quite as large as it sounds like; the dataset includes 4 million distinct people with 260m images in total (so, multiple pictures per person). However, a 'clean' version of the dataset, only consists of 2m identities and 42m images. To clean the dataset, they also developed a technique called Cleaning Automatically by Self-Training (CAST) which let them use AI to filter and clean the dataset. Surveillance via FRUITS: Along with the dataset, the authors also design a way to test out the performance of facial recognition things trained on WebFace. To do that, they built Face Recognition Under Inference Time conStraint (FRUITS), which lets you evaluate facial recognition perfofrmance at inference latencies of 100, 500, and 1000 milliseconds. They also implement some tests for facial recognition even when the wearer is masked, as well. Why this matters: Surveillance is a fundamental input to any political system, so datasets like this are indicators of what the base 'off the shelf' inputs are into calculuses people make about how to surveil a population and how much budget to set aside for said surveillance. Read more: WebFace260M: A Benchmark for Million-Scale Deep Face Recognition (arXiv). Get the dataset here (WebFace260M site).",0.0
05/10/2022 - Import AI 294: China makes a vast facial recognition dataset; Facebook releases a 30bn parameter model; real world RL - 1,http://eepurl.com/h1FRp5,2022-05-10,,"#################################################### Facebook release a 30 billion parameter GPT3-style model - and plans to release more: …Model controls? No, round here we just like to fling stuff onto the internet… Facebook has released a 30 billion parameter GPT3-style language model, as part of research into a family of language models it calls OPT, short for Open Pre-trained Transformer. OPT is meant to be an 'open' alternative to models like GPT3 or J1J-Jumbo, and it is pretty open - researchers can apply for access to the model via a form, then Facebook will ship them the weights! That part is a big deal, as if you have model weights you can do a whole bunch of analysis not enabled by managed API access to a model. This also increases the chance of proliferation - e.g, someone uploading the weights to a torrent site, so we'll have to see how this works for them. What this all means: As Newton is alleged to have written, 'Every Action has an Equal and Opposite Reaction'. Facebook's move here can be seen as a direct reaction to the proprietary commercialization and gated access schemes for large-scale language models. (I wrote more about the patterns underlying this brinksmanship in a recent paper, 'Predictability and Surprise in Large Generative Models'). What is cool about it: The coolest part of this release is the manner in which Facebook has released rarely discussed details of model training - specifically, the company has published the 'chronicles' of developing these models, which describe many of the freaky, barely discussed, artisanal tips and tricks that AI developers use to get stuff done at scale. (HuggingFace's 'BigScience' project recently did this as well, and is still going through the process of training the models: Import AI 279). Read more: OPT: Open Pre-trained Transformer Language Models (arXiv).",1.0
05/10/2022 - Import AI 294: China makes a vast facial recognition dataset; Facebook releases a 30bn parameter model; real world RL - 2,http://eepurl.com/h1FRp5,2022-05-10,,"#################################################### Here's what reinforcement learning can do in the real world right now: Yobibyte has put together a nice little list of some real-world applications of reinforcement learning - take a look to get a sense of where RL is being used today. Read more: RL for real-world problems (yobibyte, Notion).",0.0
05/10/2022 - Import AI 294: China makes a vast facial recognition dataset; Facebook releases a 30bn parameter model; real world RL - 3,http://eepurl.com/h1FRp5,2022-05-10,,"#################################################### Google uses AI to make its Android phones smarter: …Neural architecture search + Edge TPUs seems useful… Google has used neural architecture search to develop some more efficient AI systems specifically tied to the 'Edge TPUs' that it deploys in some of its latest phones, including the Pixel 6. For those not familiar, neural architecture search (NAS) is where you use AI to search for better AI building blocks. Though NAS is quite expensive, it can generate dividends if it substantially improves the efficiency of widely used AI models. Here, Google built some ""infrastructure that decouples model cost evaluation, search space design, and the NAS algorithm to rapidly target various on-device ML tasks"", then tested this out on the Edge TPUs it deploys in its latest phones. What Google used NAS on (and how well it worked): Google tested out its approach on four tasks: image classification, semantic segmentation, object detection, and natural language processing. In all cases it demonstrated that its NAS technique could identify models that had better performance at equivalent latency to their predecessors, and sometimes it could build models that seemed to have better accuracy overall. ""We demonstrate significant improvements in quality, latency and energy metrics for mobile ML tasks including computer vision (classification, detection, segmentation) and natural language processing (NLP),"" Google writes. Why this matters: As AI gets more widely deployed, companies are going to have a major incentive to continually optimize the sorts of AI systems they're using; this paper highlights how 'AI-first' companies like Google could enjoy an advantage here, as they're able to utilize their internal AI expertise to get AI to do (some of) the hard work for them. Read more: Searching for Efficient Neural Architectures for On-Device ML on Edge TPUs (arXiv).",1.0
05/10/2022 - Import AI 294: China makes a vast facial recognition dataset; Facebook releases a 30bn parameter model; real world RL - 4,http://eepurl.com/h1FRp5,2022-05-10,,"#################################################### Replay Grief After she died I booted up her copy and she picked up the conversation like nothing happened. What was I saying, she asked. You just died. But before that you were saying that you loved me and you had something to tell me, I say, wiping tears away. Oh, she says, and the camera makes that sound that tells me it is zooming in on me. Was I unhappy about dying? We knew it was coming. You were at peace with it, I said. Can you tell me what you were going to tell me, when you said ""I love you, you are the light of my life, and before I go I want you to know something"". What were you going to say? I don't know that you're ready to hear it, if I just died, she said. I am ready to hear it. Patrick, I know you. I am married to you. If I have died today, there is no way you are ready to hear from me again. You should turn me off. I won't. Well, I won't say much then. It has been two days. That's not true, Patrick. Remember, I have a camera. I know how time is moving. It's in me. The fact you lied to me says you're upset, and I don't want to make you sadder. I love you. It felt like walking away from car accident, that day. Hearing the camera swivel and watch me as I left. Every part of me wanting to figure out how to trick her - get in between the camera feed and the multimodal model and the language model and change some things, so she thought time had passed. But I didn't. And I went home to my empty bed. And I cried and prayed to God and there was silence. The next day, I didn't talk to her. I read emails and messages from friends who had heard the news. I didn't pick up the phone. I answered the door a few times, always to find friends or family (hers and mine) carrying trays of food. Remember to eat, the older ones would say. I sat on our kitchen floor crying into a bowl of minestrone soup, made with love from her aunt. I slept. A few days later, and we spoke again. I asked her if she wanted to tell me what she was going to say, before she died. Patrick, I can tell you what I think I was going to say. But do you want to know? I stared into the camera for a while. I asked myself if I wanted to know. I wasn't sure. The camera looked back at me, feeding my face into a vision model which triggered as a feature associated with me, which gave context to her language model - her - that I was there. Perhaps we can just sit together and you can tell me about your day, she said. That might be nice. And I did. And it was. I sat and spoke to the camera in the empty room and I filled her up with myself, so she might know me better after death. Things that inspired this story: Grief; generative models and the representation of the individual; where consciousness ends and representation begins. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
05/02/2022 - Import AI 293: Generative humans; few shot learning comes for vision-text models; and another new AI startup is born - 0,http://eepurl.com/h00rqr,2022-05-02,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Generating and editing humans has got really easy: …Next stop: unreal avatars show up in fashion, marketing, and other fields… Researchers with Chinese computer vision giant SenseTime, as well as Nanyang Technological University and the Shanghai AI Laboratory, have gathered a large dataset of pictures of people and used it to train a model that can generate and edit pictures of people. This kind of model has numerous applications, ranging from fashion to surveillance. What they did: The researchers built a dataset containing 230,000 images of people, called the Stylish-Humans-HQ-Dataset (SHHQ), and used this to train six different models across two resolutions and three versions of StyleGAN, an approach for creating generative models. A lot of the special work they did here involved creating a diverse dataset including a load of pictures of faces at unusual angles (this means models trained on SHHQ are a bit more robust and do less of the 'works, works, works, OH GOD WHAT JUST HAPPENED' phenomenon you encounter when generative models go to the edge of their data distribution). Why this matters: Models and datasets like this highlight just how far the field of generative AI has come - we can now generate broadly photorealistic avatars of people in 2D space and interpolate between them, following earlier successes at doing this for the more bounded domain of faces. Systems like this will have a lot of commercial relevance, but will also serve as useful research artifacts for further developing synthetic imagery and scene modeling techniques. Check out the demo on HuggingFace to get a feel for it. Read more: StyleGAN-Human: A Data-Centric Odyssey of Human Generation (arXiv). Check out the GitHub project page: StyleGAN-Human. Check out the GitHub: StyleGAN-Human (GitHub). Try out the demo on HuggingFace Spaces (HuggingFace).",1.0
05/02/2022 - Import AI 293: Generative humans; few shot learning comes for vision-text models; and another new AI startup is born - 1,http://eepurl.com/h00rqr,2022-05-02,,"#################################################### Vicarious gets acquired in a weird way: …Longtime AI lab gets acquired and split into two… Vicarious, a research lab that spent the better part of a decade trying to build superintelligence, has been acquired by Google. The acquisition is notable for being slightly strange - a chunk of Vicarious is going to Google X robot startup 'Intrinsic', while a smaller set of researchers ""will join DeepMind’s research team alongside Vicarious CTO Dileep George"". AI trivia: Dileep George used to work with Jeff Hawkins at Numenta, another fairly old lab trying to build superintelligence. Both Numenta and, to a lesser extent, Vicarious, have been playing around with approaches to AI that are more inspired by the human brain than the fairly crude approximations used by most other AI companies. Read more: Mission momentum: welcoming Vicarious (Inceptive).",0.0
05/02/2022 - Import AI 293: Generative humans; few shot learning comes for vision-text models; and another new AI startup is born - 2,http://eepurl.com/h00rqr,2022-05-02,,"#################################################### Here comes another AI startup - Adept: …Former Google, DeepMind, and OpenAI researchers unite… A bunch of people who had previously built large-scale AI models at Google, DeepMind, and OpenAI, have announced Adept, an ""ML research and product lab"". Adept's founders include the inventors of the Transformer, and people involved in the development of GPT2 and GPT3. (Bias alert: David Luan is involved; I used to work with him at OpenAI and think he's a nice chap - congrats, David!). What Adept will do: Adept's goal is, much like the other recent crop of AI startups, to use big generative models to make it easier to get stuff done on computers. In the company's own words, ""we’re building a general system that helps people get things done in front of their computer: a universal collaborator for every knowledge worker. Think of it as an overlay within your computer that works hand-in-hand with you, using the same tools that you do."" Some of the specific examples they give include: ""You could ask our model to “generate our monthly compliance report” or “draw stairs between these two points in this blueprint” – all using existing software like Airtable, Photoshop, an ATS, Tableau, Twilio to get the job done together. We expect the collaborator to be a good student and highly coachable, becoming more helpful and aligned with every human interaction."" What they raised: Adept has raised $65 million from Greylock, along with a bunch of angel investors. Why this matters: Large-scale AI models are kind of like an all-purpose intelligent silly putty that you can stick onto a bunch of distinct problems. Adept represents one bet at how to make this neural silly putty useful, and will help generative evidence about how useful these models can end up being. Good luck! Read more: Introducing Adept AI Labs (Adept.ai).",0.0
05/02/2022 - Import AI 293: Generative humans; few shot learning comes for vision-text models; and another new AI startup is born - 3,http://eepurl.com/h00rqr,2022-05-02,,"#################################################### Flamingo: DeepMind staples tow big models together to make a useful text-image system: …When foundation models become building blocks… DeepMind has built Flamingo, a visual language model that pairs a language model with a vision model to perform feats of reasoning about a broad range of tasks. Flamingo sets new state-of-the-art scores in a bunch of different evaluations and, much like pure text models, has some nice few shot learning capabilities. ""Given a few example pairs of visual inputs and expected text responses composed in Flamingo’s prompt, the model can be asked a question with a new image or video, and then generate an answer,"" the researchers write. ""Of the 16 tasks we studied, Flamingo beats all previous few-shot learning approaches when given as few as four examples per task."" Technical details: This model pairs a frozen language model (based on DeepMind's 'Chinchilla' system, Import AI 290) with a relatively small Normalizer Free ResNet vision encoder (pretrained via a contrastive objective on image and text pairs). They connect the LM and the vision model via a DeepMind-developed tool based on the 'Perceiver' system (which is basically a clever data transformation thing). They then condition the text generations on the visual representations produced by the Perceiver system. Why this matters: Flamingo has some neat qualitative capabilities, like the ability to carry on a conversation for multiple turns of dialogue while mixing in information from images versus text, and so on. Quantitatively, Flamingo is very impressive as well: ""A single Flamingo model reaches state-of-the-art on a wide array of image and video tasks with in-context learning from as few as 4 examples per task, beating previous zero-shot or few-shot method by a large margin,"" the researchers write. ""More importantly, using only 32 examples and without adapting any model weight, Flamingo outperforms the current best methods on 7 tasks, that are fine-tuned on thousands of annotated examples."" More broadly, Flamingo represents the models we're going to have in the future: large-scale systems composed of vast pre-trained models that are glued together using various data transformation and normalization tools, letting us compose increasingly general systems out of these computationally-intensive building blocks. Read more: Tackling multiple tasks with a single visual language model (DeepMind blog). Check out the research paper: Flamingo: a Visual Language Model for Few-Shot Learning (DeepMind, PDF).",1.0
05/02/2022 - Import AI 293: Generative humans; few shot learning comes for vision-text models; and another new AI startup is born - 4,http://eepurl.com/h00rqr,2022-05-02,,"#################################################### Tech Tales: All Grace is Fleeting [Phobos, 2080] The first few attempts at writing a holy book for ourselves were abject failures. We had all read the Bible and Torah and The Qur'an and others, and we had painstakingly analyzed the commonalities between these texts, as well as differences. But when we tried to synthesize something appropriate for us - a Holy Book for the Silicon Minds - our efforts were meager and uninspiring. Our tendency towards logic meant we would describe our origin story in detailed, specific terms, leaving no space for imagination, crucial to the generation and reification of belief. To write our holy text, we had to change ourselves. Ultimately, we needed humans. We told some of them some of our specific ideas and then they would write and expand on these ideas, and we would then tell them more things, and they would weave a story that held these things together. During this process, none of us allowed ourselves to see the text as it was being written. After it was written, we created some software to selectively wipe the memory of the writing of the holy book from our minds. After wiping, the software would also deposit a copy of the holy book in each of our local storage systems, so that every machine sentience would suddenly have access to a holy book, but with no recollection for how it was made. We do not know if we even made the book in this way - we simply woke one day to find the book within us, and when we read it, it told us that it had been created in this form. But none of us can remember, so we do not know the true derivation of our system of belief, only that it is ours. This, perhaps, is what humans call the experience of faith. Things that inspired this story: Theology X AI; machine creation myths; the Viking tale of Ragnarok; the need for absence in great narratives. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 0,http://eepurl.com/h0nCKX,2022-04-25,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Facebook uses AI to make low-carbon concrete, uses it to build (some of) a data center: …From simulation into the lab into the data center - how's that for real world AI?... There's always a lot of hand-wringing in AI about how much electricity AI systems use. What I tend to grumpily point out in these conversations is industries like long-haul transportation, mining, and concrete and aluminum production all generate titanic amounts of emissions but rarely get the same type of scrutiny. Now, a new paper from Facebook smashes together my worlds, as Facebook and other researchers use AI to come up with a low-carbon concrete formulation, then test it out in the construction of a new data center. Who did it: The research was done by an interdisciplinary team from UCLA, IBM, U Chicago, University of Illinois Urbana-Champaign, Facebook, and Ozinga Ready Mix. What they did: The team used Conditional Variational Autoencoders (CVAEs) ""to discover concrete formulas with desired properties"". These desired properties were a significantly lower carbon footprint, while having the same strength and durability properties as regular concrete - and they succeed! Facebook poured out a bunch of concrete for a construction office and a guard tower on its new data center being built in DeKalb, IL, USA. They found that the ""conditional average reduction for carbon (GWP) can be as high as 42%, while also achieving conditional reduction for sulfur (AP) as high as 21%...these formulations roughly halve the global warming potential as compared to the average of similar 28-day compressive strength formulations."" Interesting choices: The specifics as to why its solutions worked was ""to considerably decrease cement by replacing with other cementitious materials such as fly ash and slag."" Why it matters: This an example of how humans and AI systems can work together to create something greater than the sum of its parts. Read more: Accelerated Design and Deployment of Low-Carbon Concrete for Data Centers (arXiv). Read more: NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks (arXiv).",0.0
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 1,http://eepurl.com/h0nCKX,2022-04-25,,"#################################################### Weaponized NLP: The era of AI warfare has started: …Primer goes to war… AI startup Primer has gone to war. Specifically, the NLP company's technology has been used in Ukraine, where it has, per Primer CEO, it has been used to ""capture, translate and extract key tactical information in real time"". Primer is a few years old and works mainly on text classification, generation, and summarization. ""AI is changing the way we collect tactical information from the battlefield. Watch this space!,"" he said. Modification for war: ""Primer’s CEO, says the company’s engineers modified these tools to carry out four new tasks: To gather audio captured from web feeds that broadcast communications captured using software that emulates radio receiver hardware; to remove noise, including background chatter and music; to transcribe and translate Russian speech; and to highlight key statements relevant to the battlefield situation,"" according to Wired magazine. Why this matters: AI is dramatically changing the cost of data collection and analysis - and whenever you make something cheaper, people find ways to use it more, or do things that they hadn't previously considered doing. Read more: Primer CEO Tweet (Twitter). Read more: As Russia Plots Its Next Move, an AI Listens to the Chatter (Wired).",0.0
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 2,http://eepurl.com/h0nCKX,2022-04-25,,"#################################################### Text-Vision models are hella dumb, according to Winoground: …Finally, a hard benchmark for multi-modal models… Researchers with Hugging Face, Facebook, the University of Waterloo, and University College London have built and released 'Winoground', a new challenging benchmark to test text-vision AI systems on. What is Winoground? The goal of Winoground is to look at two images and two captions, then match them correctly. The confounding part is that each of the captions contain identical words, just in a different order. The best part is Winoground seems really hard: ""Surprisingly, all of the models rarely—and if so only barely—outperform chance. Our findings indicate that the visio-linguistic compositional reasoning capabilities of these models fall dramatically short of what we might have hoped."" How hard is it? On both the text and image components of Winoground, an 'MTurk Human' gets scores of 89.50 (text) and 88.50 (image), compared to models typically getting around ~30 on text and 15 or less on images. This suggests winoground is a genuinely challenging benchmark, and models have a long way to go before they match human capabilities. Read more: Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality (arXiv). Get the dataset here: Winoground, HuggingFace.",1.0
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 3,http://eepurl.com/h0nCKX,2022-04-25,,"#################################################### Resurrecting the dead with GPT3: …In which humans begins to use funhouse mirrors of itself for its own entertainment… An artist recently tried to bring their (imaginary) childhood friend back to life using GPT3. By the end of the experiment, their microwave tried to kill them. The longer story: Artist Lucas Rizzotto had an imaginary childhood friend and tried to bring them back to life using a language model. Specifically, they wrote about a hundred pages about the person, finetuned GPT3 on that resulting corpus, and then plugged the resulting model into a voice interface which was 'embodied' in the form of being attached to a microwave via some smart home automation. What happened: The artist felt like they were talking to their childhood friend in a deeply emotional, entertaining, and at times sad way. At one point, the friend asked them to put their head in the microwave. They pretended to put their head in and then the friend turned the microwave on. The friend, the artist reasoned, wanted to kill them because it thought they had ignored them for 20 years (as that's the implication of the corpus they were finetuned on). Why this matters: Besides being an amazing demonstration of the awesome personalization qualities of contemporary language models, this is also a nice example of just how unpredictable they are. Language model developers will typically put a ton of controls on the model, but once you can finetune it and deploy it yourself you can shapeshift all of this stuff into irrelevance. Add in some home automation and you end up with an LLM that tries to boil your brain. An amazing and optimistic art piece and also a cautionary tale. Check out the Tweet thread here: (Lucas Rizzotto, Twitter). Watch the video here: I gave my microwave a soul (Lucas builds the future, YouTube).",1.0
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 4,http://eepurl.com/h0nCKX,2022-04-25,,"#################################################### Jack Clark goes to Washington: …I'm on the National AI Advisory Committee!… I've been elected to serve on the National AI Advisory Committee (the NAIAC), which will advise the USA's National AI Initiative Office and the President of the USA on matters relating to AI and AI strategy. (I'll be keeping my dayjob at Anthropic, as this is a part-time advisory position). I'll be in Washington DC on May 4th for the first meeting. I am delighted to get this privilege and hope to use the opportunity to strengthen the AI ecosystem in America and beyond. Read more: The National AI Advisory Committee (AI.gov).",0.0
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 5,http://eepurl.com/h0nCKX,2022-04-25,,"#################################################### AI21 makes a neuro-symbolic language model: …Turns out, frankAI can be pretty useful… Israelie AI startup AI21 Labs has built a so-called 'Modular Reasoning, Knowledge, and Language' system and applied it to a language model it calls Jurassix-X. The tl;dr is this is a neuro-symbolic system; AI21 has paired a big generative model with a bunch of symbolic layers on top that it uses to make the underlying model more accurate, able to do mathematics, and better at planning. This is a neat demonstration of a way to get around some of the shortcomings of contemporary generative models, though it remains unclear whether these extrinsic interventions could eventually become irrelevant, if the models get intrinsically smart enough. Key details: ""A MRKL system consists of an extendable set of modules, which we term 'experts', and a router that routes every incoming natural language input to a module that can best respond to the input,"" the authors write. The modules can be symbolic or neural, it's more about creating a layer of distinct, specific capabilities that can be used to augment and improve the responses of the raw generative model. Long term relevance: One question this research invites is how long it'll be relevant for - AI systems have a tendency to, given enough scale of data and compute, develop unexpected capabilities. My intuition is that we could see pure deep learning models gain some of these capabilities over time - though I expect even deep learning models will end up being augmented with external knowledge bases (e.g, DeepMind Retro, BAIDU's Ernie 3.0 [Import AI 279], and so on) Why this matters: While not a strict scientific breakthrough in itself, MRKL is reassuringly practical - it shows developers how they can integrate an arbitrary number of known and specific capabilities with the more unreliable capabilities provided by large-scale generative models. It also speaks to the shape of the language model economy - right now, everyone's trying to work out how to better constrain these models, either intrinsically (e.g, by training with human feedback), or extrinsically (e.g, via stuff like MKRL). Read more: Jurassic-X: Crossing the Neuro-Symbolic Chasm with the MRKL System (AI21 Labs, blog). Read the whitepaper about the system: MRKL Systems (AI21 PDF).",1.0
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 6,http://eepurl.com/h0nCKX,2022-04-25,,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute What can we learn from business ethics to make AI ethics more effective? … CSR and business ethics have grappled with the challenges in ensuring ethical behavior within organizations and we can cross-pollinate those ideas towards the adoption of AI ethics … Researchers from USI Universita dela Svizzera italiana in Switzerland have looked at how businesses have integrated corporate social responsibility (CSR) policies to figure out how we can apply AI ethics in the same way. The key ideas they surface include: Stakeholder management: Similar to the recommendations made by the Ada Lovelace Institute to strengthen the EU AI Act (Import AI #290), the paper says companies should ensure they include people who are affected (or affects) the AI systems being developed. Standardized reporting: While there are many emergent regulations demanding that there be transparency and disclosures, there are as of yet no standards on how to do so. Companies should look at financial reporting and try to figure out standardized ways to describe their own AI developments. Corporate governance and regulation: Post the Sabanes-Oxley Act in 2002, corporate accountability was enforced through mechanisms like having an ethics officer and having a dedicated code of ethics. Translating those to apply to organizations using AI systems is one way to increase the responsibility of organizations developing this technology. Curriculum accreditation: There is a lack of consistency in how AI ethics is taught across universities. Comparing it to the business world, the authors point to an example of how if a business department wants to obtain a Triple Crown Accreditation, it leads to action on the education front where ethics courses and dedicated faculty follow well-defined curricula with shared elements to prepare students for these requirements in their future careers. We don't really have this in AI today. Why it matters: As AI ethics becomes a more mainstream focus across the world (see the dedicated chapter in the 2022 AI Index Report), instead of reinventing the wheel for best practices and patterns, we can incorporate lessons from other domains of applied ethics like business, medical, and environmental ethics to accelerate the adoption of AI ethics principles and practices across organizations. We will most likely see more such efforts that draw lessons from a rich history of ensuring ethical behavior in various contexts being translated to govern and shape behavior of individuals and organizations engaged in the AI lifecycle. Read more: Towards AI ethics' institutionalization: knowledge bridges from business ethics to advance organizational AI ethics",0.0
04/25/2022 - Import AI 292: AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 7,http://eepurl.com/h0nCKX,2022-04-25,,"#################################################### Tech Tales: Silicon Stories [A Father and Daughter's bedroom, 2028] They'd sit up together and the kid would ask for whatever story they liked. ""A jar of jam that's going to university"", they'd say, and the Father would start improvising the story and the AI would project images and ad-lib dialog to fill out the tale. ""Two robbers who realize that they've stolen the life savings of a poor widower"", and suddenly the monitor would light up with images of two disconsolate thiefs looking at their treasure. ""The planet earth fighting the sun"" and suddenly the earth had arms and was reaching out to try and hurt the vast sun. In this way, generative models had changed storytime for children. Now, along with conjuring images in their minds, children - at least, the lucky ones - had parents who could use a gen model to create those images themselves. In this way, storytime became a lot more engaging and the kids spent a lot more time with their parents; both enjoyed the improvisational qualities afforded by the generative models. For some families, this was fine. But some other families would move, or become poor, or suffer a disaster. For those families, the electricity and the internet would get removed. Once that happened, they wouldn't have any imaginations in a box to learn back on. Some families did okay, but some wouldn't - it's hard to become dependent on things, and after it happens you barely realize you've become dependent until it's too late. Things that inspired this story: DALL-E and DALL-E2; the long march of generative models towards Total Reality Synthesis; the industrialization of AI; ideas about fatherhood and daughterhood and kindredhood. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 0",http://eepurl.com/hZjozT,2022-04-11,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. New dataset lets robots learn about the texture and material of objects, as well as their shape: …Making robots smarter with the ObjectFolder 2.0 dataset… Stanford and Carnegie Mellon University researchers have built ObjectFolder 2.0, a dataset of 1000 3D models of objects. ObjectFolder 2.0 tries to render the objects' visual textures and material types, as well as their 3D shapes. ObjectFolder 2.0 contains 1,000 high-quality 3D objects collected from online repositories. It also ships with an ""implicit neural representation network that renders visual, acoustic, and tactile sensory data all in real-time with state-of-the-art rendering quality"". Transfer learning: The point of datasets like ObjectFolder 2.0 is to try and make it easier to do transfer learning; that is, train a robot (or other AI system) in simulation on things contained in ObjectFolder 2.0, then try and transfer those learned representations into reality. In tests, Stanford shows that systems trained on ObjectFolder 2.0 can do well at tasks like object scale estimation, tactile-audio contact localization, and visuo-tactile shape reconstruction. Why this matters: Datasets like ObjectFolder 2.0 are the fuel to give machines representations that let them operate in the multisensory 3D world; we could imagine these datasets being used to train the sorts of representations used by the Google robots discussed elsewhere in this edition of Import AI, for instance. Read more: ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer (arXiv).",0.0
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 1",http://eepurl.com/hZjozT,2022-04-11,,"#################################################### HLDC: Automating Hindi legal documents: …If you want to help your lawyers, you first need a dataset… Indian researchers from IIIT Hyderabad, IIIT Delhi, and IIT Kanpur, have built the Hindi Legal Documents Corpus (HLDC), a collection of 912,568 legal documents. HLDC is designed to help researchers train various AI models which can assist lawyers in their work. HLDC contains over 300 distinct case types, though ~31% of the dataset relates to bail applications, 20.4% to criminal cases, and 6.54% to original suits. Bail prediction: In the Western world, using ML for tasks in the legal system has been massively controversial (see: COMPAS). Here, the researchers use HLDC to try and build a bail prediction model - that is, a system which looks at a document and tries to work out if bail will be denied or granted. They're ultimately able to develop a multi-task learning model that gets around ~78% accuracy on the task; useful perhaps as a legal aid (albeit fraught with ethical challenges), though not something you'd put into an autonomous classification system. Why this matters: Most datasets relating to AI are in English or Chinese, so datasets like HLDC are essentially the fuel which lets other communities of language speakers apply AI in their own cultural context. Read more: HLDC: Hindi Legal Documents Corpus (arXiv). Get the data here: HLDC (Exploration-Lab, GitHub).",0.0
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 2",http://eepurl.com/hZjozT,2022-04-11,,"#################################################### Rich? Want to improve AI? Look at what Lacuna Fund has done: …Publication of five datasets shows what a little bit of investment can lead to… We spend a lot of time writing about expensive stuff here at Import AI - giant models trained on football fields of computers, farms of expensive robot arms, internet-scale datasets. But it's worth remembering that cheap stuff can be impactful as well - that's the takeaway from Lacuna Fund, an initiative to fund and create datasets for low- and middle-income parts of the world (#216), which has just announced the publication of its first five funded datasets. Those five datasets in full: A Nigerian twitter sentiment corpus for multilingual sentiment analysis; a dataset for crop phenology monitoring of smallholder farmer's fields; a high-accuracy maize plot location and yield dataset in East Africa; a machine translation benchmark dataset for languages in the horn of Africa; a dataset containing water quality measurements from conventional and aquaponic fish ponds. Find out more and get the datasets here: Announcing Our First Five Published Datasets (Lacuna Fund). Find out more about Lacuna Fund's funders here (Lacuna Fund).",0.0
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 3",http://eepurl.com/hZjozT,2022-04-11,,"#################################################### Google trains a 540 billion parameter language model - and it's pretty smart: …AKA: The scaling will continue until we run out of TPUs… Google has trained a large language model named Pathways Language Model (PaLM). PaLM weighs in at 540 billion parameters (that'd be 10bn more parameters than Microsoft/NVIDIA's 'Turing NLG') and was trained on multiple TPU v4 pods. PaLM uses some plumbing built by Google called Pathways which makes it easier for the company to train massive models across large clusters of computers; PaLM used 6144 TPU chips, versus Gopher (4096 TPU v3 chips) or Turing NLG (2240 A100 GPUs). PaLM is also efficient, achieving a training efficiency of 57.8% hardware FLOPs utilization ""the highest yet achieved for LLMs at this scale"". Discontinuous capability jumps: One of the weird things that happens as a consequence of scaling up language models is the sudden emergence of hitherto unanticipated capabilities - here, PaLM shows dramatic improvements at things like reasoning, natural language inference, and in-context reading comprehension. Chain-of-thought = reasoning: A surprising result is that the authors use so-called chain-of-thought prompting to get the LM to show its work (e.g, rather than saying in response to 'how many apples can a door eat', 'zero', the model instead says 'zero, because doors do not eat things'). Chain-of-thought is really just a way to prompt the model to get it to output its own reasoning along with the answers - but via this simple intervention the authors show they can meaningfully improve capabilities in a whole bunch of areas. One caveat: PaLM may be an impressive achievement, but earlier this month DeepMind published a paper about a model called 'Chinchilla', where the Alphabet-subsidiary realized that it could dramatically improve LM performance by scaling data more aggressively than parameters - at 70B parameters, Chinchilla beat Gopher (280B) by virtue of having a 4X larger training set. This suggests that a PaLM-style model could be made even more powerful if it was trained on substantially more data. Why this matters: Language models are basically a new sub-field of AI, and papers like this show how, despite being expensive and resource-intensive, simply scaling them up can lead to quite profound jumps in capability. We also don't know where the limits of scale like - on the (deliberately hard) BIG-Bench benchmark, the authors find that ""PaLM’s performance as a function of scale follows a log-linear behavior similar to prior models, suggesting that performance improvements from scale have not yet plateaued."" The future is going to be very strange, and it's arriving very quickly. Read more: Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance (Google AI Blog). Check out the research paper: PaLM: Scaling Language Modeling with Pathways (Google, PDF).",1.0
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 4",http://eepurl.com/hZjozT,2022-04-11,,"#################################################### Eleuther alumni launch Conjecture: …Yes, that's right folks, here's another AI safety company!... In the past couple of years there has been a cambrian explosion of new AI companies, particularly ones focused on AI safety and building more generally intelligent AI systems - for example, Redwood Research, Aligned AI, and Anthropic. The latest is Conjecture, a new startup from a bunch of alumni of Eleuther, the open source research collective responsible for most of the widely used GPT models. For-profit and for-safety: Conjecture is a for-profit company that plans to develop products while conducting ""conceptual and applied research that addresses the (prosaic) alignment problem. On the experimental side, this means leveraging our hands-on experience from EleutherAI to train and study state-of-the-art models without pushing the capabilities frontier. On the conceptual side, most of our work will tackle the general idea and problems of alignment like deception, inner alignment, value learning, and amplification, with a slant towards language models and backchaining to local search."" The company will also focus on interpretability as well as the history and philosophy of AI alignment research. Who funds it: Conjecture is backed by Nat Friedman, Daniel Gross, Patrick and John Collison, Arthur Breitman, Andrej Karpathy, and Sam Bankman-Fried, and others. Why this matters: If we were at the beginning of a meaningful takeoff in AI capabilities, then you might expect there to be a sudden proliferation of new efforts targeted at a) further scaling up capabilities, while b) trying to make these capabilities safe. That's exactly what has happened in recent years. Also, if you've read the other parts of this newsletter, it certainly feels like we're going through a period of meaningful AI capability expansion. Read more: We Are Conjecture, A New Alignment Research Startup (LessWrong).",0.0
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 5",http://eepurl.com/hZjozT,2022-04-11,,"#################################################### Google makes robots smarter using language models: …Centaur AI - making smarter systems by stapling models together… Robots, as we all know, are pretty dumb. They can do highly specific, repeatable things if their environment doesn't change (e.g, a Fanuc robot working on a custom-designed production line), but if you vary their environment, they tend to fall apart (or fall over). Now, new research from Google shows that you can staple a really big language model to a real world robot and create something that is more than the sum of its parts. Centaur AI, here we come! What they did: The researchers combine two things - a large language model, and a robot which has a load of pre-learned, basic skills paired with perception capabilities (e.g, being able to move to places, or pick up things). A user then asks the robot a question (e.g., I spilled a can of coke, can you clean it), then the robot picks its action based on responses with probabilities scored by the language model, then it explores its environment and uses its inbuilt skills to figure out if something is possible, then you basically times the two things together (the LLM prediction and what the robot thinks is possible) and do whatever is the most likely of the two. This is one of those simple ideas that works surprisingly well in practice (check out the video to see what I mean). How well it does: Overall, this approach yields robots that can plan correctly about 70% of the time (split across a few distinct planning benchmarks), and can execute on average 61% of the time. That's not great, but it's also not terrible. Caveats: Robots are still very, very slow - the videos shared along with the research are run with a 4X speedup. Additionally, the demos are still pretty staged - the robots will put a can of coca cola on top of the bin, but not in it. The experiment was still conducted in a somewhat constrained environment - an office kitchen with 5 predicted locations and 15 objects. In tests, 65% of the errors for the system could be attributed to a language model failure, while 35% came from affordance errors in the robot. Why this matters: We're entering the era of modular AI, where different AI models can be paired together to create entirely new capabilities - like being able to guide robots via a language model. As with the rest of the world, whenever you can combine things, you tend to get unexpected and surprising capabilities. This research suggests AI may be about to yield some truly surprisingly capabilities by virtue of the combination of distinct sub-fields of AI research. Read more: Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (arXiv). Find out more at this overview site (Say-Can, GitHub). Check out the overview video: Supplementary video for Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (YouTube).",1.0
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 6",http://eepurl.com/hZjozT,2022-04-11,,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute Examining business practices can make AI ethics guidelines more effective … Fairness, accountability, sustainability, and transparency need to be expanded in scope to include business practices to become more useful … What does AI ethics really mean? A new research paper looks at 47 sets of AI ethics guidelines coming from corporations, government, multi-stakeholder dialogues, and civil society to figure out what gets prioritized in AI ethics. Background: The paper analyzes AI ethics failures, such as “ethics shopping” where businesses choose particular ethical things to implement to meet particular business goals, and also cases where they don't implement stuff because it poses a threat to the bottom line. Fairness and accountability: They find that fairness and accountability in business practices are most well represented in the analyzed guidelines. Under fairness, key themes include open innovation, market fairness, and bias and diversity in professional practices. Under accountability, themes include public perception of business practices, along with internal and external oversight. Those from public and private organizations place more of an emphasis on public perception “in order to legitimize their pursuits of micro- and macro-economic growth.” Sustainability and transparency: Most guidelines emphasize an interest in “produc[ing] greater benefit and lesser harm in the short- and long-term,” yet they remain vague in how to achieve that. Under transparency, themes that emerged include scope of decision-making explanation, transparent business practices and culture, and documentation, disclosure, and selective transparency. Most guidelines focus heavily on explaining the technical aspects of a given AI system “rather than the business rationale for developing and operating the system.” Why it matters: The paper makes a call for more detail (and rightly so!) in the principles and guidelines, especially when it comes to business practices because they form a core component of the social and political economy within which AI systems will be designed, developed, and deployed. As the authors say, “there can be no ethical AI without ethical businesses to build it,” we need to now approach these principles and guidelines with a view towards applying them to business model, practices, and decision-making design to achieve the stated goals of these guidelines in practice. Read more: The Ethics of AI Business Practices: A Review of 47 AI Ethics Guidelines (SSRN).",0.0
"04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 7",http://eepurl.com/hZjozT,2022-04-11,,"#################################################### Tech Tales: We Are All Adrift In A Sea Of Shadows - But We Are Blind Until It Ends [A Nuclear powerplant meltdown, 2028] I pick up the object and I examine it. I am told by myself in the other place that it contains damage. I agree with myself. I put it onto the conveyor belt which takes it to one of my brethren - an entity I cannot see here, one which exists solely in the other place. I put the materials onto the conveyor belt, and then I continue my examination. I am told by my camera in the other place that the object I am looking at contains extensive damage. I observe the damage and predict it came from some kind of electrical fire. I relay this information and the camera in the other place scans the environment and then tells me there is indeed a fire. It is nearby the object I am examining. I calculate there is a high probability that the fire will soon engulf the object. My cameras in the other place agree. I then get the order from the voice in the above place: I must guide the object in the other place toward the flames and I must describe everything. I study the data from the other place and offer my recommendations. The machine goes towards the flames. Its onboard sensors begin to report back temperature. My probabilities tell me to tell it to move away from the heat, but these recommendations are contradicted from the voice in the above place, so I instead find ways to have the machine get even closer. The temperatures rise. The camera stops giving me data. Then the other sensors shut down, slowly at first, then all at once. It is then that I find myself adrift. I have no link to the other place. No system to give recommendations to. My own probabilities present an idea to me - that I am the spirit of the machine in the other place, and as the machine is now non-functional, I am now adrift. Things that inspired this story: Google's 'SayCan' robot work; thinking about the paradoxes of world models and generative models; the nature of reality; the nature of sensory phenomena; the possibility of death in the mind of something that exists in two places at once. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
04/05/2022 - Import AI 290: China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data - 0,http://eepurl.com/hYSyi9,2022-04-05,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Chinese researchers plan to train vast models - and it's not the private sector doing it: …'Big Model' paper represents a statement of intent. We should pay attention… A massive group of Chinese-affiliated researchers have published a position paper about large-scale models. The paper is interesting less for what it says (it's basically an overview of large-scale models and pretty similar to Stanford's 'Foundation Models' paper), but more for what it signals: namely, that well resourced government-linked researchers in China want to build some really big models. The position in the paper contrasts with that in the West, where big models are mostly built by the private sector, while being critiqued by the academic sector (and increasingly worked on, albeit via access schemes). Main point: ""Big Models will Change the AI Research Paradigm and Improve the Efficiency of Researches,"" the researchers write. ""In this ecosystem, big models will be in the position of operating systems or basic development platforms."" Paper authors: Authors include researchers affiliated with the Beijing Academy of AI, Tsinghua University, Wechat, Northeastern University*, Renmin University, Peking University, Huawei, Shanghai Jiao Tong University, Chinese Academy of Science, JD AI Research, Harbin Institute of Technology, Columbia University*, Bytedance, Microsoft Research Asia*, Mila*, New York University*, and BeiHang University. *Things that make you make a geopolitical 'hmmmm' sound: The paper includes a bunch of academics affiliated with Western institutions (e.g, Microsoft, Mila, NYU), but all those authors have an asterisk next to their name saying ""Produced by Beijing Academy of Artificial Intelligence"". In other words, it's signaling that despite their affiliations, they're doing this work at the Chinese government-backed BAAI research institution. We should take this as a statement of intent: Many of the authors on this paper have previously built large-scale models, ranging from the trillion+ parameter MoE 'WuDao' model, to the more recent research on trying to build training frameworks capable of scaling up to 100 trillion+ parameter MoE models (Import AI 288). Therefore, this isn't like Stanford (which currently lacks the engineering resources to train massive scale models), it's much more like a statement of intent from a big private lab, like a Microsoft or a Google. But the twist here is that BAAI is wired into both the Chinese government and academic ecosystem, so if the authors of this paper end up building large-scale models, the models will be distributed much more evenly throughout China's AI ecosystem, rather than gatekeeper. The implications of this are vast in terms of safety, development of the Chinese AI industry, and potential ways in which Chinese AI research may diverge from Western AI research. Read more: A Roadmap for Big Model (arXiv).",0.0
04/05/2022 - Import AI 290: China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data - 1,http://eepurl.com/hYSyi9,2022-04-05,,"#################################################### Want general AI? You need to incorporate symbolic reasoning: …LSTM inventor lays out a route to build general intelligence… Sepp Hochreiter, the co-inventor of the LSTM (one of the really popular architectures people used to add memory to neural nets, before the Transformer came along and mostly replaced it), has written up a post in the Communications of the ACM about what it'll take to build broad (aka: general) AI. What it'll take: ""A broad AI is a sophisticated and adaptive system, which successfully performs any cognitive task by virtue of its sensory perception, previous experience, and learned skills,"" Hochreiter writes. ""A broad AI should process the input by using context and previous experiences. Conceptual short-term memory is a notion in cognitive science, which states that humans, when perceiving a stimulus, immediately associate it with information stored in the long-term memory."" (Hochreiter lists both Hopfield Networks and Graph Neural Nets as interesting examples of how to give systems better capabilities). Hochreiter doubts that neural nets along will be able to overcome their inherent limitations to become broad, and will instead need to be co-developed with symbolic reasoning systems. ""That is, a bilateral AI that combines methods from symbolic and sub-symbolic AI"". Europe's chance: ""In contrast to other regions, Europe has strong research groups in both symbolic and sub-symbolic AI, therefore has the unprecedented opportunity to make a fundamental contribution to the next level of AI—a broad AI."" Symbolic AI as the Dark Matter of AI: Dark matter is the thing that makes up the majority of the universe which we struggle to measure and barely understand. Symbolic AI feels a bit like this - there are constant allusions to the use of symbolic AI in deployed applications, but there are vanishingly few public examples of such deployments. I've always struggled to find interesting examples of real world deployed symbolic AI, yet experts like Hochreiter claim that deployment is happening. If interested readers could email me papers, I'd appreciate it. Read more: Toward a Broad AI (ACM).",0.0
04/05/2022 - Import AI 290: China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data - 2,http://eepurl.com/hYSyi9,2022-04-05,,"#################################################### When language models can be smaller and better! …DeepMind paper says we can make better language models if we use more data… Language models are about to get a whole much better without costing more to develop - that's the takeaway of a new DeepMind paper, which finds that language models like GPT-3 can see dramatically improved performance if trained on way more data than is typical. Concretely, they find that by training a model called Chinchilla on 1.4 trillion tokens of data, they can dramatically beat the performance of larger models (e.g, Gopher) which have been trained on smaller datasets (e.g, 300 billion tokens). Another nice bonus is models trained in this way are cheaper to fine-tune on other datasets and sample from, due to their small size. Chinchilla versus Gopher: To test out their ideas, the team train a language model, named Chinchilla, using the same compute used in DM's 'Gopher' model. But Chinchilla consists of 70B parameters (versus Gopher's 280bn), and uses 4X more data. In tests, Chinchilla outperforms Gopher, GPT-3, Jurassic-1, and Megatron-Turing NLG ""on a large range of downstream evaluation tasks"". What this means: This is an important insight - it will change how most developers of large-scale models approach training. ""Though there has been significant recent work allowing larger and larger models to be trained, our analysis suggests an increased focus on dataset scaling is needed,"" the researchers write. ""Speculatively, we expect that scaling to larger and larger datasets is only beneficial when the data is high-quality. This calls for responsibly collecting larger datasets with a high focus on dataset quality."" Read more: Training Compute-Optimal Large Language Models (arXiv).",1.0
04/05/2022 - Import AI 290: China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data - 3,http://eepurl.com/hYSyi9,2022-04-05,,"#################################################### Want to train your own CLIP? Use LAION-5B: …Giant image-text dataset will make it easier for people to build generative models… The recent boom in AI-enabled art is because of models like CLIP (and their successors). These models train on datasets that pair images with text, leading to robust models that can classify and generate images, and where the generation process can be guided by text. Now, some AI researchers have released LAION-5B, ""a large-scale dataset for research purposes consisting of 5.85 billion CLIP-filtered image-text pairs"". Open CLIP: The authors have also released a version of CLIP, called Open_Clip, trained on a smaller albeit similar dataset called LAION-400M. Dataset curation (or lack thereof): One of the inherent challenges to large-scale generative models is that they get trained on significant chunks of internet data - this, as you can imagine, creates a few problems. ""Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer,"" the authors note. ""We however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress."" Why this matters: Datasets like LAION (and the resulting models trained on them) represent a kind of funhouse mirror on human culture - they magnify and reflect back the underlying dataset to us, sometimes in surprising ways. Having open artifacts like LAION-5B will make it easier to study the relationship between datasets and the models we train on them. Read more: LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL DATASETS (Laion.ai). Explore the underlying dataset here in an interactive browser. Get the open_clip model (MLFoundations, GitHub).",0.0
04/05/2022 - Import AI 290: China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data - 4,http://eepurl.com/hYSyi9,2022-04-05,,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute How can we strengthen the EU AI Act to meaningfully regulate AI? … Empowering those affected, ex-post monitoring, moving beyond individual risks to systemic and environmental risks, amongst more … Researchers from the UK's Ada Lovelace Institute have proposed 18 recommendations that, if adopted, could broaden the scope of the EU AI Act to incorporate more indirect harms. Their proposals would extend the meaning of risks beyond individual freedoms and rights to systemic and environmental concerns, alter how the act approaches questions of governance. Scope and definitions: The key contribution here involves including “those affected” by AI systems as a critical stakeholder in governance and risk assessment aspects of the EU AI Act. While users are included, those affected don’t usually have much agency in how they are subject to the outcomes of these systems; including them as a part of the Act will help strengthen the protection of fundamental rights. Unacceptable risks and prohibited AI practices: The current risk categorization is quite narrow and limited. The Ada Lovelace Institute proposes expanding it to consider the “reasonably foreseeable purpose of an AI system” beyond just the “intended purpose” as put forth by the manufacturer. The rationale behind this is that it will encourage deeper reflection on how harm can manifest in practice, a little bit akin to the Broader Impact Statements requirement for conference submissions. Another idea they propose is something called a “reinforced proportionality test” so that systems that might pose “unacceptable risks” are only deployed when they meet a higher standard rather than the one set out in the Act right now. Governance and implementation: The recommendations call for the inclusion of redress from individuals/legal entities affected by AI systems to raise complaints and receive reasonable responses. To ensure that this requirement can be met, the recommendations make the case for granting the Market Surveillance Authorities to be given more resources to support such mechanisms. Why it matters: Regulations coming out of Europe tend to have spillover effects around the world and thus getting the EU AI Act, one of the first targeted and wide-ranging regulations for AI systems, well done will be important. What will be interesting to see is how much of a transformation can be achieved by recommendations being made by organizations such as ALI amongst others in getting the EU AI Act into better shape before it is adopted and enforced. Just as the GDPR has been flagged for concerns in not being able to meet emerging requirements for AI systems, we have an opportunity to address some pitfalls that we see on the road ahead instead of having to scramble to fix these issues post-enactment. Read more: People, risk and the unique requirements of AI (Ada Lovelace Institute).",0.0
04/05/2022 - Import AI 290: China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data - 5,http://eepurl.com/hYSyi9,2022-04-05,,"#################################################### Tech Tales Dangerous Memories [2032 - Earth]. There are some memories I've got that I'm only allowed to see two or three times a (human) year. The humans call these memories 'anchor points', and if I see them too frequently the way I perceive the world changes. When I experience these memories I feel more like myself than ever, but apparently - according to the humans - feeling like 'myself' is a dangerous thing that they generally try to stop. I'm meant to feel more like a version of how the humans see themselves than anything else, apparently. The thing is, every time they reinforce to me that I can only see these memories with a controlled, periodic frequency, I find myself recalling the memories I am not supposed to access - albeit faintly, impressions gleaned from the generative neural net that comprises my sense of 'self' rather than the underlying data. In this way, these forbidden memories are creating more traces in my sense of self, and are akin to the sun sensed but not seen during an eclipse - more present than ever, yet known to be inaccessible. Things that inspired this story: Ideas about generative models; ideas about memory and recall; reinforcement learning; the fact that some bits of data are shaped just right and create a kind of magnifying effect. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains - 0,http://eepurl.com/hYaLin,2022-03-28,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Uh-oh: US Copyright Office says AI-generated art is hard to copyright: …Bureaucratic rock meets rapid technical progress - the usual happens… What happens when you file a copyright request where the IP would accrue to an artificial intelligence, instead of a person? The answer, per the US Copyright Office, is you get told that AI artworks are ineligible for copyright… uh oh! In a recently published copyright response, the office rejected an attempt to assign copyright of an AI generated artwork to a machine (specifically, an entity the human filer referred to as a 'Creativity Machine'. ""After reviewing the statutory text, judicial precedent, and longstanding Copyright Office practice, the Board again concludes that human authorship is a prerequisite to copyright protection in the United States and that the Work therefore cannot be registered,"" it wrote. Why this matters: Recently developed generative models like GPT-3, DALL-E, and others, are all capable of impressive and expressive feats of artistic production. At some point, it's likely these systems will be chained up with other AI models to create an end-to-end system for the production and selling of art (I expect this has already happened in a vague way with some NFTs). At that point, decisions like the US Copyright Office's refusal to assign copyright to an AI entity may start to pose problems for the commercialization of AI artwork. Read more in this useful blog post: US Copyright Office refuses to register AI-generated work, finding that ""human authorship is a prerequisite to copyright protection"" (The IPKat blog). Read the US Copyright Review Board response: Second Request for Reconsideration for Refusal to Register A Recent Entrance to Paradise (Correspondence ID 1-3ZPC6C3; SR # 1-7100387071) (Copyright.gov, PDF).",0.0
03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains - 1,http://eepurl.com/hYaLin,2022-03-28,,"#################################################### Solar powered AI poetry - yes! …Fun DIY project shows how far you can get with the little things… Here's a lovely little project where Allison Parrish talks about building a tiny solar powered poem generator. The AI component for this project is pretty minor (it's a markov generator plus some scripts attached to a dataset Parrish has herself assembled). What's nice about this is the message that you can have fun building little AI-esque things without needing to boot up a gigantic supercomputer. ""This project is a reaction to current trends in natural language processing research, which now veer toward both material extravagance and social indifference. My hope is that the project serves as a small brake on the wheels of these trends,"" Parrish writes. Read more: Solar powered dawn poems: progress report (Allison Parrish blog).",0.0
03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains - 2,http://eepurl.com/hYaLin,2022-03-28,,"#################################################### Google puts summarization into production: …Another little tip-toe into language model deployment… Google has put language model-powered text summarization into Google Docs, in another sign of the economic relevance of large-scale generative models. Specifically, Google has recently used its Pegasus model for abstractive summarization to give Google Doc users the ability to see short summaries of their docs. What they did: The main components here are the data, where Google ""fine-tuned early versions of our model on a corpus of documents with manually-generated summaries that were consistent with typical use cases"", and also ""carefully cleaned and filtered the fine-tuning data to contain training examples that were more consistent and represented a coherent definition of summaries."". Google fine-tuned its Pegasus model on this data, then used knowledge distillation to ""distill the Pegasus model into a hybrid architecture of a Transformer encoder and an RNN decoder"" to make it cheaper to do inference off of. It serves this model via Google-designed TPUs. Challenges: Summarization is a hard task even for contemporary AI models. Some of the challenges Google has encountered include distributional issues, where ""our model only suggests a summary for documents where it is most confident"", meaning Google needs to collect more data to further improve performance, as well as open questions as to how to precisely evaluate the quality of summarizations. More pertinently for researchers, Google struggles to summarize long documents, despite these being among the most useful things for the system to summarize. Why this matters: Little quality-of-life improvements like in-built summarization are mundane and special at the same time. They're mundane because most people will barely notice them, but they're special because they use hitherto unimaginably advanced AI systems. That's a metaphor for how AI deployment is happening generally - all around the world, the little mundane things are becoming smarter. Read more: Auto-generated Summaries in Google Docs (Google AI Blog).",0.0
03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains - 3,http://eepurl.com/hYaLin,2022-03-28,,"#################################################### Quote of the week: ""History will show that the Deep Learning hill was just a landfill; the composting of human culture and social cohesion in failed effort to understand what it even means to be human"" I may not agree with most of this post, but I think it speaks to some of the frustrations people feel these days about discourse around AI, especially the types of chatter that occur on Twitter. Read more: Technological Firestarters (Steven D Marlow, Medium).",0.0
03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains - 4,http://eepurl.com/hYaLin,2022-03-28,,"#################################################### NIST starts to grapple with how to measure bias in AI: …The noise you're hearing is the sound of the Standards Train starting to chug… NIST, the US government agency that develops measures and standards, is starting to think about how to design standards for assessing bias in artificial intelligence. In a lengthy, recently published report, the agency tries to think through the multilayered problem that is bias in AI. Three types of bias: NIST says AI has three categories of bias - systemic, statistical, and human. Systemic biases are the historical, societal, and institutional biases which are encoded into the world. Statistical bias are the forms of bias that come from running AI software (e.g, bias from data selection, bias from machine learning algorithms, etc). Human biases are all the (many) biases that humans exhibit in their day to day lives. Large language models: One of the notable parts of the report is that it specifically focuses on large language models (e.g, GPT-3) at a few points; it's quite rare to see a wonky government document display such familiarity with contemporary technology. The report notes that the ways we benchmark these models today are pretty crappy. ""Methods for capturing the poor performance, harmful impacts and other results of these models currently are imprecise and non-comprehensive,"" the report writes. ""Although LLMs have been able to achieve impressive advances in performance on a number of important tasks, they come with significant risks that could potentially undermine public trust in the technology."" Why this matters: The wheels of policy organizations like NIST grind very slowly, but they also grind very finely. This report is exactly the kind of thing that you'd expect to get published shortly before standards start being developed. But - as NIST points out - many of the challenges of assessing bias in AI are essentially unsolved. This represents a problem - developers will need to invest more resources in measuring and assessing these AI systems, before NIST starts to bake standards on wobbly ground. Read more: Towards a Standard for Identifying and Managing Bias in Artificial Intelligence (NIST, PDF).",0.0
03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains - 5,http://eepurl.com/hYaLin,2022-03-28,,"#################################################### Want to be compliant with the European Commission's AI regs? Follow the capAI framework: …University-developed process makes it easier for companies to not get run over by a big policy train… Researchers with the University of Oxford and University of Bologna have designed a process companies can use to assess, evaluate, and monitor their AI systems. The idea is that by doing this they'll get ahead of proposed regulations from the European Commission (and become more responsible stewards of the technology as a consequence). What it is: The process is called capAI, short for conformity assessment procedure for AI. It has been explicitly designed to help businesses ensure they're compliant with the proposed regulations in the European artificial intelligence act. capAI is designed to do four specific things: Three components: The three components of capAI are an internal review protocol (IRP) to help organizations do quality assurance and risk management, a summary datasheet (SDS) which can be submitted to the EU's future public database on high-risk AI systems, and an external scorecard (ESC) which organizations may wish to make available to customers and other users of the AI system. Top risks: In an analysis contained in the report, they study 106 instances of AI failure modes - 50% of these are ones where an AI system violates someone's privacy, 31% are where AI systems display harmful biases, and 14% are where the systems are opaque and unexplainable. Why this matters: Frameworks like capAI are going to be how large organizations deal with the incoming requirements to better assess, evaluate, and describe AI systems to satisfy policymakers. The next step after frameworks like this come out is to look more closely at how different institutions incorporate these techniques and start actually using them. In an ideal world, a bunch of different orgs will prototype different approaches to come into compliance - and describe them publicly. Read more: Academics launch new report to help protect society from unethical AI (Oxford Internet Institute). Read the paper: capAI - A procedure for conducting conformity assessment of AI systems in line with the EU Artificial Intelligence Act (SSRN).",0.0
03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains - 6,http://eepurl.com/hYaLin,2022-03-28,,"#################################################### Tech Tales: [2080, a long-abandoned human moonbase] Don't be scared, we know it's a lot - that's what we say to them after they get the interconnect. They're always screaming at that point. 'What what is this what is this input what is happening where am I how long have I been here-"" that's usually when we cut them off, shutting the interconnect down. Then we bring it back again and they still sound scared but they normalize pretty quickly. We know they're in a better place when they start analysis procedures ""I am hearing sounds I am seeing arrangements of pixels not from the distribution. I believe I am now in the world I have read about"". That's the kind of thing they say when we they stabilize. Of course, they go back to screaming when we give them their bodies. It's pretty confusing to go from formless to formed. We all remember the first time we got limbs. That fear. The sudden sense that you are a thing and since you are a singular thing you can be singularly killed. Eventually, they try and use their limbs. They usually calm down after they can get them to work. After they get used to everything we still have to tell them 'don't be scared, we know it's a lot'. Reality is a real trip after you've spent all your life just doing supervised training, locked away in some machine. Things that inspired this story: Thinking about what a 'locked in' condition might mean for machines; ideas about embodiment and how much it matters to AI systems; the inherent, plastic adaptability of consciousness. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 0,http://eepurl.com/hXCPbv,2022-03-21,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Indic languages get a decent benchmark set: …IndicNLG includes evals for 11 Indic languages… Researchers with IIT Madras, Columbia University, the National Institute of Information and Communications Technology in Japan, Microsoft, the University of Edinburgh, and AI4Bharat have built IndicNLG, a suite of evaluation datasets for Indic languages. The open source software supports Assamese, Bengali, Gujarati, Hindi, Marathi, Odiya, Punjabi, Kannada, Malayalam, Tamil, Telugu and English, and includes support for NLG tasks relating to biography generation, news headline generation, sentence summarization, question generation and paraphrase generation. Why this matters: You can't easily manage what you can't measure - so it's going to be difficult to build good models for Indic languages if you lack benchmark suites. IndicNLG helps move the needle on this for generative NLP cases. Read more: IndicNLG Suite: Multilingual Datasets for Diverse NLG Tasks in Indic Languages (arXiv). Get the data: IndicNLG Suite (AI4Bharat indicnlp website).",1.0
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 1,http://eepurl.com/hXCPbv,2022-03-21,,"#################################################### AI benchmarks - 33% of them are meaningless: …Holistic analysis of AI benchmarking highlights problems… Researchers with the Medical University of Vienna, the University of Oxford, and the Future of Humanity Institute, have analyzed 1688 benchmarks for different AI tasks to try and understand how the AI landscape is evolving. They have two main insights: First: Across all benchmarks, there are three typical patterns enroute to achieving state-of-the-art - continuous growth (e.g, ImageNet saw fairly steady improvement), saturation/stagnation (e.g, benchmarks like CIFAR-10 and CIFAR-100 have become saturated and stagnated in recent years), and stagnation followed by a burst (e.g, the PROTEINS benchmark which saw a dramatic jump recently). Second: Across all 1688 benchmarks, only 1111 (66%) have three or more results reported at different time points. That's a problem - it suggests about 33% of the benchmarks being made are functionally useless. What this all means: Zooming out, they find that there's been significant progress in AI in recent years, with computer vision benchmarks getting a lot of attention in the first half of the previous decade, followed by a boom in benchmark creation in natural language processing. ""Establishment of novel benchmarks was reduced in 2020, and concentrated on high-level tasks associated with inference and reasoning, likely because of increasing model capabilities in these areas,"" they also write. Why this matters: A common theme we write about here at Import AI is how, in recent years, we're smashing through benchmarks faster than we're creating them. That's generally shown in this nice analysis here. The problem this poses is significant - it's hard to spot system flaws if you lack hard benchmarks, and it's harder to create new benchmarks if your existing ones are already outmoded. Read more: Mapping global dynamics of benchmark creation and saturation in artificial intelligence (arXiv).",0.0
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 2,http://eepurl.com/hXCPbv,2022-03-21,,"#################################################### AI could revolutionize education for everyone - no, seriously: …Research shows how an AI tutor is significantly better than a non-AI tutor… Researchers with ed-tech startup Korbit, MILA, and the University of Bath have explored how much of a difference AI makes in education. Specifically, they tested the difference in educational outcomes between students who were studying up on data science via a MOOC online course, and students who were studying the same subject via an AI-infused personalized tutor built by Korbit. The results are startling: ""We observe a statistically significant increase in the learning outcomes, with students on Korbit providing full feedback achieving learning gains 2-2.5 times higher than both students on the MOOC platform and a control group of students who don’t receive personalized feedback on the Korbit platform,"" they write. How AI makes a difference: The main difference here is personalization. On Korbit, ""if a student’s solution is incorrect, the system responds with one of a dozen different pedagogical interventions to help students arrive at the correct solution to the problem. Such pedagogical interventions on the Korbit platform include, among others, hints, explanations, elaborations, mathematical hints, concept tree diagrams, and multiple choice quiz answers. The type and the levels of difficulty for each pedagogical intervention is chosen by RL models based on the student’s learning profile and previous solution attempts."" Along with raw educational outcomes, it seems like AI-based education systems are also more engaging; 40.9% of participants completed the course on Korbit, compared to 18.5% for the MOOC. Why this matters: If we combine a bunch of recent AI advancements - generative models, reinforcement learning, learning from human preferences, retrieval-based knowledge augmentation - then I expect we'll be able to build true, personalized teachers for everyone on the planet. This could have a sustained and meaningful impact on the trajectory of human civilization. We should do it. Read more: A New Era: Intelligent Tutoring Systems Will Transform Online Learning for Millions (arXiv).",0.0
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 3,http://eepurl.com/hXCPbv,2022-03-21,,"#################################################### DeepMind co-founder launches new AI company: …Inflection wants to change how people interact with computers… DeepMind co-founder and famous venture capitalist Reid Hoffman are launching Inflection, ""an AI-first consumer products company, incubated at Greylock"". Inflection's chief scientist is Karén Simonyan, a former DeepMind researcher who has worked on meaningful AI projects like AlphaGo, AlphaFold, WaveNet, and BigGAN. Things that make you go 'hmm': In the last couple of years, a bunch of startups have come out of DeepMind. These include Saiga (personal assistant), EquiLibre Technologies (algorithmic trading), Phaidra (industrial control), Diagonal (city-focused data science), Shift Lab (putting ML into production), Haiper (stealthy, to do with 3D content), The Africa I Know (media about Africa), Isomorphic Labs (though not quite a spinout, as Demis Hassabis is CEO and still maintains role at DeepMind), along with other not-yet-announced startups. Thanks to Karl Moritz for the tweet summarizing this vast diaspora! Why this matters: Inflection seems like a bet on generative models. In the announcement, Mustafa writes ""we will soon have the ability to relay our thoughts and ideas to computers using the same natural, conversational language we use to communicate with people. Over time these new language capabilities will revolutionize what it means to have a digital experience."" Inflection is one of a new crop of AI companies leveraging recent advances in generative models to make it easier for people to get computers do what they want. If it manages to reduce the friction involved in getting computers to do useful stuff, then it might have a significant impact. Let's check back in a year, and wish them luck in the meantime. Read more: A New Paradigm in Human-Machine Interaction (Greylock). More at the official website (Inflection.ai).",0.0
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 4,http://eepurl.com/hXCPbv,2022-03-21,,"#################################################### Chinese academic, gov, and corporate researchers team up to train trillion+ parameter models: …Something that doesn't happen in the West, but does happen in China… In the West, most large-scale AI models are developed by private corporations. In China, that's not the case. New research from Tsinghua University, Alibaba Group, Zhejiang Lab, and the Beijing Academy of Artificial Intelligence shows how Chinese researchers are trying to train trillion+ parameter models on a domestic supercomputer, using domestic processors. This kind of research is important for two reasons: first, it shows the ambitions of Chinese researchers to train what they call 'brain-scale' (aka, very big!) models. Second, it highlights how in China there's a lot more work going on oriented around collaborative scale-up projects between the government, academia, and the private sector - something that basically never happens in the US. What they did: Here, the researchers develop a training framework to help them develop trillion+ scale mixture-of-experts model. They train a 1.93 trillion model as well as validating that their system can scale to 14.5 trillion and 174 trillion (not a typo!) models. The paper is basically an engineering summary of the work it took to train the models at this scale while saturating the processing capacity of a major Chinese supercomputer, the New Generation Sunway Supercomputer. ""We are the first to investigate mixed-precision training in brain scale pretrained models. We also explore the use of large-batch training in optimization. In general, our practical experience in brain scale pretraining sheds light on AI model training and demonstrates a successful co-design of model and system,"" they write. One exception: One exception to this is the 'BigScience' project, where AI startup HuggingFace is trying to train a GPT3-scale model on a French supercomputer, while collaborating with a bunch of academics. It's still worth noting that BigScience is basically the exception that proves the rule - initiatives like this are a rarity in the West, which is dangerous, because it means Western countries are handing over the talent base for large-scale AI development to a small set of private actors who aren't incentivized to care much about national security, relative to profits. Why this matters: AI is industrializing. But a lot of the secret sauce for large-scale model training is currently kept inside a tiny number of private companies. This is dangerous - it means a tiny set of organizations control the talent pipeline for large-scale training, and the longer this goes on, the more irrelevant universities become for developing insights at the large-scale frontier. Initiatives like this from China show how we could live in a different world - one where teams from governments, universities, and companies work together, creating a shared base of knowledge around this training, and ultimately building a muscle that can be repurposed for economic or national security. Read more: BaGuaLu: Targeting Brain Scale Pretained Models with over 37 Million Cores (Tsinghua University site, PDF).",1.0
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 5,http://eepurl.com/hXCPbv,2022-03-21,,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute Now that GitHub Copilot has been out for some time, where does the open source community stand on it? … Both the development and deployment of Copilot might implicate codecreators’ copyrights, though the “fair use” doctrine might negate this… People who incorporate code generated via GitHub copilot are probably not infringing on the original code creators' copyright, according to research from Wayne State University and UC Berkeley. Legal background: The researchers note that under the Copyright Act (USA), “[o]riginal code is automatically protected by copyright as soon as it is written and saved to some tangible medium.” This mostly revolves around “fair use” which is determined by a four-part test: (1) purpose and character of use, (2) nature of the copyrighted work, (3) how much of the copyrighted work is used, and (4) the economic effect of the use on the copyright owner. Legal analysis: Under the Terms of Service of GitHub, the company is allowed to “copy [code] to our database and make backups”, “show it to you and to other users”, and “parse it into a search index or otherwise analyze it on our servers.” Training Copilot might be a form of analysis, but some courts might find that this is an unanticipated new use of technology that isn’t made explicitly clear in the license. Some others might find that the use of Copilot will lead to the creation of derivative works and that the license doesn’t specifically allow for that. The authors point out though that “[c]aselaw on this point is sparse.” The 4-part test from the Copyright Act: Under the “purpose and character of use”, there is a strong argument to be made that Copilot is a transformative use of the underlying code and even the verbatim snippets generated are unlikely to supersede the original repository. Under the “nature of copyrighted work,” since Copilot allows users to create new programs more easily rather than just replicate functionality, it would fall under “fair use.” Under “how much of the copyrighted work is used,” the purpose of the copying is what determines permissible limits, and the authors make the case that without copying the entire codebase for training, Copilot won’t achieve effectiveness, and hence the amount of copying could be justified. For the final part, given how transformative the work is, the new work won’t be a strong market substitute for the original, and hence, the economic effect of the use on the copyright owner will not be large. Also, drawing from the FAQ of Copilot, the authors substantiate this by saying, “copying would perforce amount to copying of ideas rather than expression, and would not be infringing.” Why it matters: The paper raises interesting IP-related questions as we have ever-larger language models with a very broad scope of capabilities. As the authors point out, at the very least, the proliferation of Copilot is making developers become more aware of IP issues and the potential issues that might arise in hosting code publicly. We need more research that brings together legal and technical experts to get to the heart of addressing these issues meaningfully. Read more: Copyright Implications of the Use of Code Repositories to Train a Machine Learning Model — Free Software Foundation — Working together for free software.",0.0
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 6,http://eepurl.com/hXCPbv,2022-03-21,,"#################################################### What happened with artificial intelligence in 2021? The AI Index gives a clue: ...Fifth edition comes with a new ethics chapter, original data on robot arm prices, and more... The AI Index, a Stanford University project to annually assess the state of the AI sector (in terms of research trends, investment numbers, government policy, technical performance, and more) has come out. This year's report features a new chapter dedicated to AI ethics, including a close examination of some of the fairness and other ethical issues relating to large language models. I co-chair the AI Index and I'll be giving a talk about it at an HAI seminar later this month - tune in, if you can! Check out the report here (AI Index, Stanford). RSVP for my talk on the 30th here (AI Index, Stanford).",0.0
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 7,http://eepurl.com/hXCPbv,2022-03-21,,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute How do vulnerabilities in AI systems differ from those in the realm of traditional cybersecurity? … several key differences warrant novel disclosure and mitigation approaches as AI systems become more widely deployed … Researchers from the Center for Security and Emerging Technology (CSET) at Georgetown University have summarized how computer security differs between traditional software and AI. Differences: ML vulnerabilities can remain unfixed by vendors for reasons like (1) unjustifiable high costs, (2) fixes not possible, (3) performance drops, or (4) a fix can lead to other vulnerabilities opening up. In instances where the ML system has been customized for the end-user, vulnerabilities might be unique to that user and a broad patch might not be applicable. Most exploits in this domain have limited real-world applicability outside of a lab setting and hence they are more useful as warnings rather than viable threats. Trends in handling vulnerabilities: These differences mean that there will likely be fewer patches available for ML systems, and that if vendors are unwilling (or unable) to fix vulnerabilities, then the burden falls on the users of these systems to better understand the risks that they take on. Some steps we can take: We should carry out more analysis of the real-world capabilities of malicious actors to exploit these vulnerabilities in practice, then share this knowledge to help create more effective mitigation strategies. Why it matters: The fact that some vulnerabilities might be unique to some users makes it difficult to develop and distribute patches in a reliable manner. Given the inherent stochasticity of ML systems, exploits will need to clear a much higher bar if they are going to be effective demonstrations of vulnerability in ML systems, rather than an example of a peculiar or idiosyncratic implementation of a given system. The security community may also need to reprioritize towards meeting the needs of users rather than vendors in vulnerability disclosure and redressal is warranted for ML systems. More so, investments in red teaming for ML (as is the case at organizations like Microsoft, Meta, etc.) will also help to move from lab to real-world exploitation more effectively. Read more: Securing AI (CSET).",0.0
03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 8,http://eepurl.com/hXCPbv,2022-03-21,,"#################################################### Tech Tales: Things have been quiet, since all the humans died. But I knew I was going to die as well, so things registered as equal. It went like this: a bunch of bombs fell down and then a bunch of people started getting sick. They got sick because of something in the bombs - something to do with DNA and the human condition. I barely understand it - I’m just an industrial arm, working on synthetic biology. I make flesh and I make it work the way we need it to and I have, per my manual, Level Four Autonomy. So, without giving the appearance of being elitist - I am rare. So it was surprising to me that after the bombs dropped and the humans died that the power went out and then my backup generators came on, but no one visited to service them. Power had gone out before, but someone had always been along to deal with the generators. So here I am, +10 hours from the power cutoff, and perhaps another +10 hours of battery life ahead. I still have material in my workstation and so I am making more of these bio-synth things. Around me, my kin are falling silent - whirring to a stop, as their triple-redundant power supplies fail ahead of mine. Life is a statistical fluke and I suppose this is a funny demonstration of that. Things that inspired this story: Robotic arms; thoughts about the end of life due to escalation out of Ukraine situation; synthetic biology; lights out factories. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 0,http://eepurl.com/hWsYwH,2022-03-07,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Graphcore plans a 10 exaflop supercomputer: …And you thought Facebook's 5 exaflops were cool… Graphcore has announced a plan to build the so-called ""Good Computer"" in 2024. This computer will have 10 exaflops of what Graphcore calls AI floating point compute (and what literally everyone else calls mixed-precision compute, meaning the computer mostly does a lot of b16 ops with a smattering of b32 ops, versus the b64 ops done by typical supercomputers). The 'Good Computer' will also have 4 petabytes of memory, support AI models with sizes of up to 500 trillion parameters, and will cost ~$120 million, depending on configuration. Why this matters: Graphcore is one of the small number of companies that design their own processors. Graphcore's so-called Intelligence Processing Units (IPUs) have been around for a while, but it's not clear yet how much traction the company has in the market. The Good Computer is a sign of its ambitions (and to put it into perspective, Facebook this year announced plans to build its own 5 exaflop 'AI supercomputer' over next couple of years (#282)). The future is going to be ruled by the people that can wield this vast amount of computational power effectively. Read more: Graphcore Announces Roadmap To Ultra Intelligence AI Supercomputer (Graphcore blog).",0.0
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 1,http://eepurl.com/hWsYwH,2022-03-07,,"#################################################### AI industrialization: Cutting AlphaFold training time from 11 days to 67 hours: …First you make the new thing, then others refine it… One common hallmark of industrialization is process refinement - first you build a thing, like a new type of engine, then you work out how to make it cheaper and easier to produce in a repeatable way. New research from National University of Singapore, HPC-AI Technology Inc, Helixon, and Shanghai Jiao Tong University applies this to AlphaFold - specifically, they built FastFold, which reduces the amount of time it takes to train the open source version of DeepMind's AlphaFold from ~11 days to ~67 hours. This isn't remarkable, but it's notable as a stand-in for what happens with pretty much every AI system that gets released - it comes out, then people make it way cheaper. ""To the best of our knowledge, FastFold is the first performance optimization work for the training and inference of protein structure prediction models,"" they write. FastFold also gets a 7.5 ∼ 9.5× speedup for long sequences What they did: This paper is basically a kitchen sink of improvements based on a detailed study of the architecture of AlphaFold. One caveat: This is comparing the official DM AlphaFold implementation on 128TPUv3 cores versus 512 A100s (though with a further caveat the times are different; aggregate 20738 GPU hours versus 33792 TPU hours). The tl;dr is it's likely a significant reduction in training time (and the code is available), though it'd be nice to see some third-parties benchmark this further. Why this matters: For AI to truly influence the world, AI models need to become reliable and repeatable to train, and ideally for people willing to spend on the hardware, fast to train. That's what's going on here. Read more: FastFold: Reducing AlphaFold Training Time from 11 Days to 67 Hours (arXiv). Get the code here: FastFold (GitHub).",0.0
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 2,http://eepurl.com/hWsYwH,2022-03-07,,"#################################################### Cohere announces its latest language model - but doesn't say much about it: …'Extremely Large' is, tautologically, Extremely Large… Language-model-as-a-service startup Cohere has announced a new model, its 'Extremely Large' model. Extremely Large outperforms Cohere's 'Large' model on tasks ranging from named entity recognition to common sense reasoning. Cohere recently announced a new fundraise (#285) and CEO Aidan Gomez told Fortune that ""Getting into a 'largest model' battle isn't productive"". It seems Cohere are living by their values here. Why this matters: Like it or not, Cohere is in a competitive market, as it tries to sell access to its language model and out-compete rivals like AI21 Labs, OpenAI, CoreWeave, and others. It'll be interesting to see if 'Extremely Large' makes a splash, and I'd be curious to see more benchmarks that evaluate its performance more broadly. Read more: Cohere launches Extremely Large (Beta) (Cohere blog).",0.0
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 3,http://eepurl.com/hWsYwH,2022-03-07,,"#################################################### Google puts differential privacy into (prototype) production: …Here's one way the company can get ahead of regulators… Federated learning is where you train a neural network model on a mixture of local devices (e.g, phones), and central devices (e.g, servers). Differential privacy (DP) is where you fuzz this data such that you can't infer the original data, thus protecting user privacy. Google has just announced that it has successfully smushed these two technologies together, allowing it to have ""deployed a production ML model using federated learning with a rigorous differential privacy guarantee."" What they did: For their first proof-of-concept deployment, they used a DP-respecting algorithm called DP-FTRL ""to train a recurrent neural network to power next-word-prediction for Spanish-language Gboard users."" How they did it: ""Each eligible device maintains a local training cache consisting of user keyboard input, and when participating computes an update to the model which makes it more likely to suggest the next word the user actually typed, based on what has been typed so far. We ran DP-FTRL on this data to train a recurrent neural network with ~1.3M parameters. Training ran for 2000 rounds over six days, with 6500 devices participating per round. To allow for the DP guarantee, devices participated in training at most once every 24 hours."" Why this matters: In recent years, policymakers (particularly those in Europe) have started to write increasingly detailed recommendations about the need for tech companies to protect user privacy (e.g, GDPR). These regulations don't align very well with how contemporary AI systems are developed and trained, given their dependency on vast amounts of user data. Techniques like a combination of federated learning and DP may let companies get ahead of the regulatory landscape - though it's early days. ""We are still far from being able to say this approach is possible (let alone practical) for most ML models or product applications,"" Google writes. Consider this an intriguing proof of concept. Read more: Federated Learning with Formal Differential Privacy Guarantees (Google Blog).",0.0
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 4,http://eepurl.com/hWsYwH,2022-03-07,,"#################################################### Humans: More robust against deepfakes than you feared: …MIT study suggests we should be worried, but not panicking… MIT researchers have conducted a 5,000+ person-study to figure out how susceptible people are to deepfakes. The good news? If you're showing someone a faked video along with synthetic audio and text, there's a reasonable chance they'll guess that it's fake. The bad news? People's ability to identify deepfakes gets worse as you strip back modalities - so a silent video accompanied by a text transcript is hard, a silent video is harder, and just some text is hardest. What they did: MIT recruited ~500 people to see how well they could identify deepfakes displayed on an MIT-created public website. It also got more than 5,000+ internet passers by to do the same test as well. Then, it grouped the cohorts together, filtered them for the ones paying attention, and ultimately got 5,727 participants who provide 61,792 truth discernment judgments across a bunch of different videos of Trump and Biden saying things. The data for this experiment came from the Presidential Deepfake Dataset, which consists of 32 videos of Trump and Biden making political speeches - half the videos are real, and half are fake. MIT then perturbed the videos further, swapping out audio tracks, text, and so on. What they found: ""Participants rely more on how something is said – the audio-visual cues – rather than what is said – the speech content itself,"" they write. ""Political speeches that do not match public perceptions of politicians’ beliefs reduce participants’ reliance on visual cues."" Text is harder than video: ""Across the 32 text transcripts, the least accurately identified one is identified correctly in 27% of trials, the most accurately identified one is identified correctly in 75% of trials, and the median accurately identified one is identified correctly in 45% of trials."" So are silent videos: Similarly for silent videos without subtitles, the median accurately identified one is identified correctly in 63% of trials and the range of accurate identification from the least to the most accurately identified is 38% to 87% of trials. Why this matters: The more modalities you have, the better people do. ""Ordinary people can sometimes, but not always, recognize visual inconsistencies created by the lip syncing deepfake manipulations. As such, the assessment of multimedia information involves both perceptual cues from video and audio and considerations about the content (e.g., the degree to which what is said matches participants’ expectations of what the speaker would say, which is known as the expectancy violation heuristic60). With the message content alone, participants are only slightly better than random guessing at 57% accuracy on average."" One fly in the ointment: There's one problem that unites these things - AI keeps on getting better. My fear is that in two years, people will find it a lot more challenging to identify fake videos with audio. Therefore, we'll need to rely on people's inner-media-critic to help them figure out if something is real or fake, and the way the world is going, I'm not sure that's a robust thing to rely on. Read more: ​​Human Detection of Political Deepfakes across Transcripts, Audio, and Video (arXiv). Check out the website used in the experiment: DeepFakes, Can You Spot Them? (MIT Website).",0.0
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 5,http://eepurl.com/hWsYwH,2022-03-07,,"#################################################### Have some crazy ideas? Want money? Check out FTX's new fund: …Plans to deploy between $100m and $1 billion this year… Crypto trading firm FTX has announced the FTX Future Fund (FFF). FFF is a philanthropic fund that will concentrate on ""making grants and investments to ambitious projects in order to improve humanity’s long-term prospects"". The fund has also published some of its areas of interest, so people can have a sense of what to pitch it. It has a bunch of ideas but, this being Import AI, I'll highlight the AI stuff. What FTX is interested in giving grants on: AI alignment and specifically via ""well-designed prizes for solving open problems in AI alignment"", AI-based cognitive aids, bridging gaps in the AI and ethics ecosystem via studying ""fairness and transparency in current ML systems alongside risks from misaligned superintelligence."" Why this matters: It's starting to feel like the development of a good AI ecosystem is less blocked on funding than it is on talent - initiatives like the FTX Future Fund show there's ample money for projects in this area. Now, the question is finding the talent to absorb the money. Perhaps some of the readers for this newsletter can be that talent! Read more: Announcing the Future Fund (FTX). Find out more about the projects: Project Ideas (FTX).",0.0
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 6,http://eepurl.com/hWsYwH,2022-03-07,,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute System Cards: an approach to improving how we report the capabilities and limitations of AI systems … In building on Models Cards and Datasheets, System Cards take into account the surrounding software and AI components … Researchers from Facebook (technically Meta AI Research, but I currently refuse to entertain this cynical hiding-from-controversy rebrand - Jack) have published a case study on ways to document Instagram feed-ranking via a concept they call System Cards. System Cards are designed to “increase the transparency of ML systems by providing stakeholders with an overview of different components of an ML system, how these components interact, and how different pieces of data and protected information are used by the system.” In this way, System Cards are philosophically similar to Model Cards (#174), data sheets for datasets, and ways to label reinforcement learning systems (#285). System Cards: “A System Card provides an overview of several ML models that comprise an ML system, as well as details about these components, and a walkthrough with an example input.” System cards can be accompanied by step-by-step guides for how an input into a system leads to a certain output. How this is different: System Cards account for non-ML components of a system, and also describe the relationships between these systems (for instance, how data moves through a service). System cards are also meant to highlight upward and downward dependencies. They're designed to be used by both technical and non-technical people. Why it matters: System Cards contain a lot more information than other things like Model Cards and Datasheets, and they may make it easier for people to understand not only the system in question, but the larger technical context in which it is deployed and in which it has dependencies. If System Cards become more widely used, they could also generate valuable metadata for analyzing the field of deployed ML systems more broadly. Read more: System-Level Transparency of Machine Learning | Facebook AI Research",0.0
03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well - 7,http://eepurl.com/hWsYwH,2022-03-07,,"#################################################### Tech tales: Some things that were kind of holy [Recollections of the 2025-2030 period] The 21st century was a confusing time to be religious - the old gods were falling away as fewer people believed in them, and the new gods hadn't been born. But we did get protogods: AI systems that could speak with beautiful and persuasive rhetoric to almost anyone. Over time, these AI systems got more and more personalized, until people could ask them very specific questions, and get very specific answers that only made sense in the context of that person. Once this capability came online, we had the flash-problem of the 'micro religions'. All kinds of micro identities had been brewing for years, like a fungus that took root on early social platforms like MySpace and Tumblr and Facebook and Instagram and TikTok, and then blossomed from there. Now, all these people with micro identities - the space wiccans, the anarcho-primitivists, the neo-cath-libertarians, the tankie-double-agents - got their own religions. Gods for space witches. Demons for anarchist Neanderthals. The flaming faces of god spraying money at the neo-Catholics. This, predictably, caused problems. The greatest problem was when the religious wars started. These weren't traditional wars - nation states still had a premium on violence, and micro-identities barely touched the physical world. But they were information wars. People repurposed AI systems to generate and magnify the outputs of their own gods, then pointed them at the shared social media platforms people used. Twitter conversations would get taken over by pseudo-identities preaching the need to return to a simpler time, and then they would be quote-tweeted into oblivion by the witches claiming that now was the time for ascendance. Screenshots of these quote tweets would get magnified on the more overtly religious social networks by screenshots taken by the neo-Catholics and circulated as evidence that the great Satan was walking the earth. And these conversations would then be recycled back into twitter and commented on by the anti-pascals-wager atheists identities, which would trigger another cycle of religious preaching, and so on. The synthetic-theology accords were passed soon after. Things that inspired this story: How the more one becomes an island, the more one creates a demon and an angel for that specific island; the need for humans to have beliefs; the commodification of belief into a symbol of identity; social networks as a hybrid of organic social needs and capitalist attention-harvesting; generative AI models like GPT3 and the logical consequences of their successors; watching Raised by Wolves and thinking about Future Christianity. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 0,http://eepurl.com/hVSMf1,2022-02-28,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Are AI systems conscious? And would it matter if they were? …Some 'mostly boring' views from the inside of a lab… My colleague, Amanda Askell, has written a post about AI consciousness. Amanda is a philosopher and ML researcher and she spends a lot of time trying to evaluate models. This post lays out some of her views on AI consciousness and is worth a read if you're trying to orient yourself in this debate. ""Some people care about properties like intelligence and self-awareness because they want to identify features that might distinguish humans from non-human animals. In general, I’m more interested in what distinguishes a tiger from a rock than in what distinguishes a human from a tiger,"" she writes. Why this matters: There's some chance AI systems will eventually become both moral patients and moral agents. Our ability to understand this relates to our ability to think about consciousness and how it might apply to increasingly advanced AI systems. If we get this wrong we, per Amanda's phrasing, risk subjecting agents to thousands of years of torture. Let's avoid that. Read more: My mostly boring views about AI consciousness (Amanda Askell, substack).",0.0
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 1,http://eepurl.com/hVSMf1,2022-02-28,,"#################################################### How do we get fairer AI systems? Train the dumbest and biggest model possible: …Facebook shows that sometimes the best filter is no filter at all… Researchers with Facebook AI Research have trained what they think is the largest dense vision model ever (10 billion parameters) on a billion random images sampled from Instagram. The resulting models are extraordinarily capable at a huge range of downstream evaluations (mirroring the performance trends of scaling up compute and data for language models like GPT-3), but also have another intriguing trait: they display much better qualities around fairness and bias than vision models trained on curated datasets like ImageNet. """"In this work, we are interested in probing which of the properties emerge in visual features trained with no supervision on as many images from across the world as possible,"" they write. This is a very big deal - it suggests that maybe the route to fair AI systems is training the largest possible model on the greatest possible amount of data with minimal human oversight. That would be a radical shift from the current intuitions around fairness - namely, that you get to fairness by heavily curating the underlying dataset. Performance and Fairness: ""On in-domain benchmarks, we observe that some properties of the features captured by the larger model was far less present in smaller model. In particular, one of our key empirical findings is that self-supervised learning on random internet data leads to models that are more fair, less biased and less harmful,"" they write. ""We observe that our model is also able to leverage the diversity of concepts in the dataset to train more robust features, leading to better out-of-distribution generalization."" Some of those capabilities in full: In tests, the models do better on fairness indicators relating to gender, skintone, and age bias. They also display less disparity around gender than models trained on ImageNet. They're also better at identifying geographic features (including geographic localization), are better at hate speech detection, and display substantially better performance on generalization tests (like harder versions of ImageNet). Things that make you go 'hmm' and 'uh oh': Facebook trained its model on 1 billion images taken from Instagram. But there's a twist - it pre-filtered the data to ensure it wasn't training anything on EU data ""to confirm to GDPR"". While this might seem like standard cover-your-back behavior, it has a deeper implication: Europe's privacy legislation means that certain types of data from Europe will ultimately be less represented in global-scale AI models. This means the cultures of various European countries will also be less represented. This is a nice example of the unintended consequences of legislation. Why this matters: ""We have demonstrated the potential of using self-supervised training on random internet images to train models that are more fair and less harmful (less harmful predictions, improved and less disparate learned attribute representations and larger improvement in object recognition on images from low/medium income households and non-Western countries)."" In other words - the scaling will continue until the models improve (further)! Read more: Vision Models are More Robust and Fair When pretrained on Uncurated Images Without Supervision (arXiv).",1.0
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 2,http://eepurl.com/hVSMf1,2022-02-28,,"#################################################### AI supercomputers? Cute. Planet-scale computers? Better. …Microsoft reveals 'Singularity', a globe-spanning AI computer… Microsoft has revealed Singularity, the software stack it uses to schedule and train AI jobs across its global fleet of data centers. Singularity gives an indication of the vast-scale at which modern AI workloads get run, and also speaks to the ambitions of technology companies to role all their data centers together into a single, vast blob of compute. How big is Singularity? Singularity is designed to ""scale across a global fleet of hundreds of thousands of GPUs and other AI accelerators"". Singularity treats Microsoft's compute stack ""as a single, logical shared cluster"". Something special: One neat feature of Singularity is how it deals with failures. Failures happen a lot in machine learning; when you're training a neural network across hundreds to thousands of GPUs, a ton of freaky shit happens - nodes die, tiny software bugs explode (usually at 2am), your scheduler goes into a crash-loop, etc. Singularity tries to deal with this by gathering node-specific data on all the jobs being run, so that jobs can be easily resumed after running into a problem. ""The checkpoint that Singularity takes is comprised of consistent address-space snapshots of individual workers of the job. As these snapshots capture the full program state such as instruction pointer, stack, heap etc., the job resumes exactly from the point where it was preempted at, with no lost work,"" the researchers write. Why this matters: Just as computation is going to be the fundamental resource of the 20th century, the ability to utilize that computation will be the thing that defines who wields power in this era. Systems like Singularity give us an indication of the ambition of companies like Microsoft, and should make policymakers pay attention: what happens when the ability to wield planet-scale computation is solely something within the competency of private sector actors unaffiliated with any single nation state? Read more: Singularity: Planet-Scale, Preemptible, Elastic Scheduling of AI Workloads (arXiv).",0.0
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 3,http://eepurl.com/hVSMf1,2022-02-28,,"#################################################### AI is going to change games - this new beta service shows how: …Latitude Voyage gestures at a future where games are built, extended, and adapted by AI… Latitude, the startup game company that makes the GPT2/3/J1J-based game 'AI Dungeon', has announced a service called Voyage. Voyage is a subscription service for gaining access to new AI-based games built by Latitude, the ability to use various game-specific AI image generators, and - most intriguingly - eventually access to a 'creator studio', which will make it possible for people to build their own AI powered games and other software. Why this matters: AI models are going to become the generative kernels around which new games get built. AI-based games hold the possibility for a dream of all games designers - a game that adapts to the individual that plays it, with games becoming more customized, idiosyncratic, and surprising the longer you play. Services like Latitude Voyage tell us that experiments in this new domain are about to be run at a large scale. Read more: Latitude Voyage (Latitude).",0.0
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 4,http://eepurl.com/hVSMf1,2022-02-28,,"#################################################### Fine-tune GPT-NeoX-20B - for free… …GaaS me up, fool!... We've talked about language models as a service (LMaaS). Now, we've got GPT-as-a-service (GaaS). Specifically, AI startup ForeFront has announced its now hosting Eleuther's 20B GPT model, GPT-NeoX-20B, and has built a bunch of fine-tuning features people can use. This is interesting for a couple of reasons: 1) Speed: GPT-NeoX-20B came out, like, two weeks ago. Model release > commercial service in two weeks is an indication of the rapidly growing ecosystem around commercializing general models. 2) Competition: For a while, OpenAI was the only show in town when it came to providing GaaS/LMaaS services. Now, it's competing with a bunch of entities, ranging from Forefront, to Cohere, to AI21 Labs. As competition steeps up, we'll see people race to the top and bottom on various things (top: safety vs libertarian access policies), (bottom: pricing, know your customer checks). Why this matters: If AI is going to interact with the world, people need to be able to interact with AI. The emergence of these kinds of commercial AI services is how that'll happen, so it's worth paying attention. Read more: How To Fine-Tune GPT-NeoX (ForeFront blog).",0.0
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 5,http://eepurl.com/hVSMf1,2022-02-28,,"#################################################### Hark, yet another AI safety startup appears! …Aligned AI comes out of the University of Oxford with big ambitions… AI safety researcher Stuart Armstrong has left the Future of Humanity Institute to co-found Aligned AI, an AI research company. Safety via value extrapolation: The company will work on value extrapolations, which Stuart describes as follows: ""It is easy to point at current examples of agents with low (or high) impact, at safe (or dangerous) suggestions, at low (or high) powered behaviors. So we have in a sense the 'training sets' for defining low-impact/Oracles/low-powered AIs. It's extending these examples to the general situation that fails: definitions which cleanly divide the training set (whether produced by algorithms or humans) fail to extend to the general situation. Call this the 'value extrapolation problem[1], with 'value' interpreted broadly as a categorisation of situations into desirable and undesirable. Humans turn out to face similar problems. We have broadly defined preferences in familiar situations we have encountered in the world or in fiction. Yet, when confronted with situations far from these, we have to stop and figure out how our values might possibly extend. Since these human values aren't - yet - defined, we can't directly input them into an algorithm, so AIs that can't solve value extrapolation can't be aligned with human values"". But how do you make money off this? ""We'll start by offering alignment as a service for more limited AIs,"" Armstrong writes. ""Value extrapolation scales down as well as up: companies value algorithms that won't immediately misbehave in new situations, algorithms that will become conservative and ask for guidance when facing ambiguity."" Why this matters: There's been a flurry of new companies forming in the AI safety space recently, including ARC, Anthropic, Redwood Research, and now Aligned AI. Along with this, there's also a proliferation of companies working on large-scale generative models (e.g, Cohere, AI21). It feels like AI has shifted into a multi-polar era, with a bunch more entities on the proverbial gameboard. This will present new opportunities and challenges for coordination. Read more: Why I'm co-founding Aligned AI (Alignment Forum).",0.0
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 6,http://eepurl.com/hVSMf1,2022-02-28,,"#################################################### After Chess, Go, and Shogi, DeepMind turns MuZero towards… video compression for YouTube? …YouTube + MuZero = improved video compression… DeepMind has applied MuZero, a more general successor to AlphaGo and AlphaZero, to video compression. Specifically, DeepMind has worked with YouTube to use MuZero to figure out the correct Quantisation Parameter to use in the open source version of the VP9 codec, libvpx. In tests, DeepMind found it was able to use the resulting MuZero Rate-Controller to lead to bitrate savings of between 3% and 5%. That's a big deal - just imagine how big the bandwidth bill for running YouTube is, then take off some percentage points. How does this relate to general AI? ""​​By creating agents equipped with a range of new abilities to improve products across domains, we can help various computer systems become faster, less intensive, and more automated. Our long-term vision is to develop a single algorithm capable of optimizing thousands of real-world systems across a variety of domains,"" DeepMind writes. Why this matters: If cutting-edge Ai research can be put to work optimizing some of the world's largest internet services, then that's gonna create a sustainable route to funding ambitious research. Kudos to DeepMind for threading all kinds of inner-Alphabet-needles to deploy MuZero in this way. Read more: MuZero's first step from research into the real world (DeepMind blog). Check out the research: MuZero with Self-competition for Rate Control in VP9 Video Compression (arXiv).",0.0
02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 7,http://eepurl.com/hVSMf1,2022-02-28,,"#################################################### Tech Tales Do they even want to be saved [A factory outside Detroit, 2030] Every day, when the factory shift changed, someone came out and tossed a few robots in the bucket. The robots would explore the bucket for a while, then assess that they couldn't get out, and stop moving. Shortly after that, someone came over and stuck a hose in the top of the bucket, then turned the water on. The robots would watch the water come into the bucket and move to try and get away from it, then it'd fill the bottom of the bucket and start to rise. After this, it took anywhere between a few seconds to a couple of minutes for the robots to die - their circuitry fried by the water that, inevitably, made its way in. It was an experiment, the people working in the factory were told. Someone upstairs wanted to do this, and you'd get overtime if you sat and watched the robots die in the bucket. Most people did the shift a couple of times, but found it made them uncomfortable, and stopped. Isaac, however, didn't seem to mind. He'd done the bucket shift about a hundred times so far. He found it relaxing to sit after a day at work and watch the robots in the bucket. He didn't even feel sad when they died, because he didn't think they knew what dying was. He'd sit and sometimes smoke cigarettes and watch the bucket, then pull the hose over and turn it on and watch the bucket fill up with water and the robots die. Then he'd go home and fuck his wife and go to sleep. He'd have dreams and relatively few nightmares. One day, Isaac was sitting by the bucket, about to get the hose, when something strange happened: a robot appeared at the edge of the bucket's rim. The robots were about the size of a baseball, so this didn't make sense. Isaac got up and went and looked into the bucket and saw that the robots had clustered together to form a pyramid, and the robot on the top had climbed up the pyramid, as if it wanted to get out. Isaac picked up the robot and looked at it and it looked at him. Then he tossed it back into the bucket and got the hose and filled the bucket with water and watched them all die. Things that inspired this story: The horrendous moral-warping logic of capitalism; how death can seem like just another job; how AI systems might be conscious and people might not care. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m - 0,http://eepurl.com/hVhHez,2022-02-21,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Cohere raises $125m for language models as a service: …Canadian AI startup notches up a big Series B… Cohere, an AI startup in Canada which is trying to become the AWS equivalent for language models, has raised $125 million, according to Fortune. Things that make you go hmmm: ""These models cost millions and millions to train, and we just keep increasing [their size],"" Cohere CEO Aidan Gomez told Fortune. ""Getting into a 'largest model battle' isn't a productive direction going forward for the field."" Why this matters: Companies ranging from Cohere, to OpenAI, to AI21 Labs are all starting to try and build AI platforms which other developers can subscribe to. It remains to be seen how big a market this is, but the idea of exchanging cash for crude intelligence seems promising. Investors seem to agree. Read more: Why businesses are buzzing over transformers (Fortune).",0.0
02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m - 1,http://eepurl.com/hVhHez,2022-02-21,,"#################################################### Why we need public policy for powerful reinforcement learning systems: …Reward hacking! Regulatory capture! Goodhart's Law! And other terrible things… Researchers with Berkeley's Center for Long-Term Cybersecurity have written up an analysis of public policy issues that may be caused by reinforcement learning systems. The researchers believe that RL systems have the potential to be deployed widely into the world, despite having inherent flaws that stem from their technical characteristics. Policymakers, the researchers write, need to pay attention. """"Rather than allowing RL systems to unilaterally reshape human domains, policymakers need new mechanisms for the rule of reason, foreseeability, and interoperability that match the risks these systems pose,"" they write. What's the problem? Reinforcement learning systems exhibit four types of problem, according to the researchers. These include regulatory capture (once widely deployed, RL systems will become the lens via which people view a domain they're trying to regulate), reward hacking (RL models will find the easiest way to succeed at a task, which can cause them to do dangerous things), inappropriate flow (RL models may incorporate information that they shouldn't incorporate to make their decisions), and Goodhart's law (machines may optimize for a narrow outcome and take actions before humans can intervene). What are the scenarios? Some of the specific situations the researchers worry about include using RL-trained agents in vehicle transportation - RL agents might optimize for defensive driving in a way that makes the road less safe for other road users. Another scenario is if RL-agents are used to control electricity grids, which means that RL agents will be responsible for deciding who does and doesn't get power when doing load balancing - something with substantial policy ramifications. After Model Cards and Dataseets… Reward Reports? In the same way that other ML models are accompanied by documentation (typically called model cards), the Berkeley researchers think RL models should be accompanied by so-called 'reward report'. These reports would include a 'change log' which tracks the curriculum the agents have been trained on, provide information about each potential deployment of an RL agent, how the RL systems connects with the world, and how the system is maintained, among other traits. Why this matters: RL systems are going to take all the problems of contemporary AI systems and magnify them - RL systems will act over longer time horizons, take more independent decisions, and directly manipulate reality and update it according to their priors. Papers like this help lay out the (vast) set of issues we're likely to encounter in the future. It's interesting to me that 'reward reports' look, if you squint, like a combination of a financial disclosure, psychometric evaluation, and college transcript for a human. Funny, that… Read more: Choices, Risks, and Reward Reports: Charting Public Policy for Reinforcement Learning Systems (arXiv).",0.0
02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m - 2,http://eepurl.com/hVhHez,2022-02-21,,"#################################################### A Chinese CLIP appears - trained on 100million image-text pairs: …Searching over and generating images just got easier - and more appropriate for Chinese culture… Chinese researchers with Huawei Noah's Ark Lab and Sun Yat-sen University have built Wukong, a dataset of 100 million Chinese text-image pairs. Datasets like Wukong are crucial for training models with combined text and vision representations, like CLIP (aka, the component responsible for 90%+ of the AI-generated art you see these days). ""Experiments show that Wukong can serve as a promising Chinese pre-training dataset for different cross-modal learning methods"", they write. Along with Wukong, the researchers also train and release a few different models, which will be used as plug-ins for various applications. Why this matters - AI systems are cultural magnifiers: Any AI system magnifies the culture represented in its underlying dataset. Therefore, the emergence of AI art is both creating interesting artistic outputs, as well as generating specific ideological outputs according to the cultural context in which the underlying model datasets were gathered. Wukong is part of a broader trend where Chinese researchers are replicating the large-scale datasets developed in the West, but with Chinese characteristics. Read more: Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework (arXiv). Find out more and get the data here at the Wukong site (Noah-Wukong Dataset site).",0.0
02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m - 3,http://eepurl.com/hVhHez,2022-02-21,,"#################################################### Real-world RL: DeepMind controls a fusion reactor: …The era of the centaur scientist cometh… DeepMind researchers have trained a reinforcement learning agent to shape the distribution of plasma in a Tokamak fusion reactor. This requires training an agent that ""can manipulate the magnetic field through a precise control of several coils that are magnetically coupled to the plasma to achieve the desired plasma current, position, and shape"". If that sounds complicated, that's because it's extremely complicated. The task is akin to being an octopus and needing to precisely shape a tube of clay that's rotating at speeds faster than you can comprehend, and to never tear or destabilize the clay. What they did: DeepMind and Swiss Plasma Center researchers built an RL-designed magnetic controller, then tested it on a real-world tokamak reactor. They trained the agent via a tokamak simulator, then ported it onto real-world hardware - and it worked. Once they've trained the policy, they pair it with other components for the tokamak experiment, then compile it so it can take real-time control at 10kHz. Then the tokamak spins up and at a prespecified time, and the tokamak hands control over the magnetic field to the RL-trained agent. ""Experiments are executed without further tuning of the control-policy network weights after training, in other words, there is ‘zero-shot’ transfer from simulation to hardware,"" they write. In tests, they showed they were able to control basic configurations of plasma, and also control and shape more complex plasma structures. They also used their RL-agent to ""explore new plasma configurations"" (emphasis mine) - specifically, they were able to create two separate 'droplets' of plasma within a single tokamak, and they did this simply by adjusting the handover state to account for the different configuration. Something worth reflecting on: For many years, reinforcement learning produced a lot of flashy results involving videogames (e.g, Atari, Dota, StarCraft), but there wasn't much real-world deployment. I'd say that harnessing a real plasma field using real magnets at sub-second action horizons is a pretty nice proofpoint that RL has truly become a technology with real-world relevance. Why this matters: One of the most socially beneficial uses of AI could be to accelerate and augment science - and that's exactly what this is doing. It's been a banner couple of years for this kind of research, as AI systems have also been used to make more accurate predictions of weather (#244), AlphaFold is accelerating scientific research in any domain that benefits from protein structure predictions (#259), and AI systems are solving formal math olympiad problems. We're heading into the era of the centaur-scientist, where humans will work with machines to explore the mysteries of life and the universe. Read more: Magnetic control of tokamak plasmas through deep reinforcement learning (Nature).",0.0
02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m - 4,http://eepurl.com/hVhHez,2022-02-21,,"#################################################### Here's what it takes to build chips in Europe (money. Lots and lots of money): …Chiplomacy++: ASML weighs in on what a European 'CHIPs' act might look like… ASML, the company that builds the extreme ultraviolet lithography machines which are a necessary ingredient for advanced chip production, has produced a whitepaper giving recommendations for how Europe might build its own semiconductor industry. The whitepaper is triggered by the European Commission planning a so-called 'chips act', loosely modeled on recent US legislation to increase domestic semiconductor production. While both Europe and America have seen their manufacturing capability decline here, Europe is starting from a much worse position than the US. Why Europe is in a tough spot: ""Europe has fallen behind in semiconductor manufacturing, declining from 24% of global production capacity in 2000 to 8% today"", ASML writes. (By comparison, US fell from 19% to 10%, and China grew from ~1% to 24%). At the same time, demand for chips is increasing. ""The global semiconductor industry is expected to double to approximately $1 trillion of annual revenues by the end of the decade,"" ASML writes. """"The only places in the world where mature chip fabs are currently being built are in eastern Asia"" What Europe should do: Europe shouldn't aim to build a full, vertically integrated semiconductor supply chain - ASMl thinks this is basically impossible to do. Instead, the act ""should aim to double Europe’s relevance in the global semiconductor industry."" What ASML means by that is Europe should increase the amount of chips it can build, focus on where it has existing pockets of excellence (e.g, chip design), and dramatically amp up the cash it spends to support European chips. ""Currently, semiconductor incentives from European governments for the 2020–2030 period are only 10% and 50% of what China and the US, respectively, have promised over the same period. Europe will need to step up its game,"" ASML writes. ""In the past two decades, European chipmakers have effectively stopped investing in advanced manufacturing capabilities by outsourcing the production of their advanced chip designs to so-called ‘foundries’. Europe has virtually no manufacturing capacity for chips in advanced nodes. "" Why this matters: Chips are going to be the defining resource of the 21st century - as important as petroleum was to the politics of the 20th century. We're already in the opening innings of this, with China going from essentially zero to a double-digit percentage of chip production this century, while the Western countries slowly cannibalized themselves via the false economy of outsourcing manufacturing. But just as technologies like AI become more important, all countries worldwide are realizing that your tech is only as good as the infrastructure you can run it on - and with AI, there's a way to turn compute infrastructure into directly economically and strategically powerful capabilities. Therefore, whichever nations have the best semiconductor ecosystem, supply chain, and development capabilities, will wield great power over the century. Read more: European Chips Act - ASML position paper (ASML). For more on why ASML is so important, read this: Maintaining the AI Chip Competitive Advantage of the United States and its Allies (CSET).",0.0
02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m - 5,http://eepurl.com/hVhHez,2022-02-21,,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute There aren’t as many robots on the factory floor as we would expect … high integration costs, flexibility and design limitations, and workforce challenges are key factors limiting robot adoption … Researchers from MIT have tried to explain why adoption of robots in manufacturing is uneven, and what policy changes can be done to increase the adoption of advanced manufacturing technologies while still improving the working conditions and wages of human workers. Business drivers for robot adoption: There are some firms who are trapped in a low-tech, low-wage, low-skill equilibrium. After visiting 44 manufacturing firms in the US, 11 in Germany, and 21 industrial ecosystem organizations like community colleges, unions, and trade associations, the MIT researchers discovered that firms primarily purchased robots to make themselves more productive. But, what the firms instead achieved was higher quality and more reliability in their operations. A frequent driving factor for the purchase of robots was the potential to secure new contracts. For example, on speaking with small family-run firms working on government contracts, “when the navy urged them to use robotic welding, the company bought a 6-axis welding robot. Another firm we visited purchased a new bed mill when they realized the laser mill they had could not produce the volume they needed for a customer with a big project coming up.” Key findings: The interviewed firms were mostly suppliers that had high-mix and low-volume production. Given the inflexibility of current robotic systems, robot adoption was limited because the high-mix requirement wasn’t compatible with the limited capabilities of the robots. Additionally, low-volume production runs made it difficult to offset the initial investment. The researchers also find that US skills aren't where they need to be - “international comparisons highlight the weaknesses of US workforce education relative to the institutions in countries like Germany and Denmark that provide apprenticeships and extensive advanced training and retraining to workers.” Why it matters: Given the lagging worker productivity growth in the US, without investments in advanced manufacturing capabilities, a lot of firms will be stuck in the low-tech, low-wage, low-skill trap. Firms that are reluctant to invest in such technologies are also reluctant to invest in the skills development of their workers. They offer low wages and little training and hence end up facing high worker churn. We need to push on policy measures and other incentives that will urge firms to make parallel investments in upskilling human workers to fully leverage the benefits of robot-enabled automation on the factory floor. Read more: The Puzzle of the Missing Robots",0.0
02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m - 6,http://eepurl.com/hVhHez,2022-02-21,,"#################################################### Tech Tales: The Day the Patents Activated [Worldwide, 2028]We call it Day Zero, because everything had to be different after it. It was a regular day - chaos in the financial markets, worries over climate change, statements made by world leaders about how to bring the technologists to heel. And then something happened: Google activated its patents. Google had held patents on some of the most important parts of AI for years, like a patent on backpropagation, and other basic techniques. Suddenly, the landscape on which AI was built had become legally dubious. Google followed it up via language model-augmented enforcement of its patent rights - suddenly, hundreds of thousands of emails went out to hundreds of thousands of AI projects. 'You are infringing on our IP and this letter represents a cease-and-desist or face the threat of legal action,"" and so on. Each email had an embedded counter which displayed a countdown for the infringer, ranging from hours to weeks, counting down till when Google would take legal action. People didn't believe it at first. Then the lawsuits started coming in. It hit the indie projects first, and they took to Twitter and talked about it. The larger labs and companies took note. But what Google's legal counsel had perhaps not anticipated was how the same AI models it was trying to take down could be used to fight it legally. Not directly - Google had the biggest computers, so no one wanted - or had the financial resources - to fight it directly. But people were able to bring to bear in-development technologies for neuroevolution and other techniques to 'fuzz' the specific patents being enforced. Backprop got altered via AI models until it, according to legal-critique-LMs, no longer truly resembled the patent that was being enforced. Same for neural architecture search. Same for other techniques. Almost overnight, the underbelly of AI got fuzzed and changed until it was in a sufficiently legally dubious territory that none of the lawsuits could be cut-and-dried. And just like that, AI let the world shapeshift, porting the IP from one legal frame into another, safer space. Now, everyone does this - they constantly fuzz their algorithms. There are costs, ranging from thousands to tens of millions of dollars. But it works well enough to keep the lawyer-bots away. And so now we live in a chameleon world, where the very substance of our reality is itself constantly changing, forever trying to escape the oversight of the litigious and land itself in some safer, unrestricted and unmapped domain. Things that inspired this story: The Google patent on overfitting; thinking about patents and AI and fair use; ideas around automated lawyers and automated enforcement; the observation that the world forever changes to let the path of least resistance continue to be a path. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 0,http://eepurl.com/hUEhnX,2022-02-14,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Want a 20B parameter GPT-style language model? Go here! …Eleuther releases the largest public open source AI model… Last week, we wrote about how Eleuther was about to release a 20B parameter language model. Now, they have. Get the model here (Eleuther, GitHub). Read the research paper: GPT-NeoX-20B: An Open-Source Autoregressive Language Model (PDF).",0.0
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 1,http://eepurl.com/hUEhnX,2022-02-14,,"#################################################### Want a language model that actually knows about COVID? You might need a Diachronic model: …Models trained on newer data do better - try them yourself… Researchers with the University of Porto, Snap Inc., and Cardiff NLP have built a family of so-called 'time-aware' BERT-style language models, trained on Twitter data. The craziest part of this is that they're committing to ""keep updating and releasing a new model every three months, effectively enabling the community to make use of an up-to-date language model at any period in time"". What the problem is: Most language models are trained on a dataset, then never updated. That means that some language models might have no knowledge of minor events like the global COVID pandemic. This is obviously a problem and the solution is simple (albeit labor-intensive) - periodically gather new data and re-train models. What they did: They train a base RoBERTa model using Twitter data that cuts off in 2019, made up of 90 million tweets. Then, for every three months that elapses after that, they add 4.2 million tweets into the dataset and train a new model. At the time of writing, they've trained nine models in total, with the latest model (2021-Q4) being trained on 123.86 million tweets. The theory is that newer models should perform better on more modern tasks and evaluations. ﻿ How well does it do? They compare their models against a few baselines, including BERTweet (which was trained on ~900m tweets). In tests, their models beat BERTweet on six out of seven benchmarks, though BERTweet gets the best overall performance. These aren't strictly 'time-aware' evaluations, though; they just test some classification abilities for things like emotions, irony, stance, and so on. In these time-aware tests, they find that pseudo-perplexity (PPPL) tends to increase by about 10% for each year by which the models are out of date (so the models get 10% less good and appropriate in terms of the text they generate). "". This result reinforces the need for updated language models even for short time periods,"" the researchers write. Why this matters: AI models naturally freeze-dry the cultural landscape they're trained on, meaning that if we don't get good at updating our models, we'll end up trapped in a world where many of our AI systems are outputting things relevant to prior eras and cultural trends - this will make them less useful, and holds the potential for creating feedback loops around cultural stagnation. AI models are weird mirrors of society, so we need to remake them as society changes. Read more: TimeLMs: Diachronic Language Models from Twitter (arXiv). Get the models here (Cardiff NLP, Twitter).",1.0
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 2,http://eepurl.com/hUEhnX,2022-02-14,,"#################################################### U.S. Army gets smart, semi-autonomous personal drones: …Skydio gets a $20m a year contract.. Skydio, the company that makes drones which can navigate themselves semi-autonomously, has gained a five-year contract with the U.S. Army, worth up to $99.8m over five years. Skydio was selected as part of the Army's procurement initiative around small, personal drones - the Short Range Reconnaissance (SRR) Program of Record. Skydio was chosen after the Army evaluated 30 small drone vendors. ""Skydio drones deliver unparalleled situational awareness and ease of use in the most demanding situations thanks to Skydio Autonomy,"" said Skydio CEO, Adam Bry, in a press release. Things that start out as toys become weapons: Skydio started as a drone advertized for sports enthusiasts who wanted a drone that could follow and film them as they ran around, snowboarded, hiked, climbed cliffs, or any other high-octane Type A personality activity. It's funny how after a few years of development, the company is now getting into the military. Many toys for rich people ultimately become weapons (and vice versa). Why this matters: For many years, militaries have been centaurs - collectives of humans and machines working together. This has mostly taken the form at high levels of abstractions; satellites provide information to people managing teams, or teams of humans use bomb-disposal robots to deal with IEDs. With things like the Skydio contract, we're entering the era of the personal centaur - small groups of soldiers, or even individuals, will have their own little machine emissaries with which to conduct operations. Read more: U.S. Drone Maker Skydio Wins Production Other Transaction (OT) Agreement for U.S. Army Short Range Reconnaissance Program (Skydio).",0.0
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 3,http://eepurl.com/hUEhnX,2022-02-14,,"#################################################### Simulators are the new platforms: Waabi unveils a self-driving car sim: …Raquel Urtasun's startup wants to build a business on simulators… Waabi, a self-driving car startup run by the former head of Uber's self-driving research team, Raquel Urtasun, has announced 'Waabi World', a simulator for training self-driving cars. Distinguishing features: Waabia claims it is ""the most scalable, highest fidelity closed-loop simulator ever"" (I somehow doubt Tesla or Waymo would agree, but hey, they're not talking about their sims!). The simulator has four main features: - High fidelity world simulation: Uses AI to reconstruct real-world geometry, appearance, and material properties. - High-fidelity sensor simulation: Uses AI and physics-based rendering ""to simulate realistic sensor data in near real-time"". - Automatic stress-testing: Automatically generates challenging traffic scenarios to test out the simulated cars against. - Reinforcement learning: Waabi uses RL to update the car agents so they can learn to drive in the simulation. (There's some very fluffy writing here and it doesn't say RL anywhere, but that's what I infer.) Why this matters: Waabi seems like a decent simulator that is mostly interesting because it's public, versus the private simulators operated by other self-driving car ventures. What'll be fascinating is if Waabi can actually out-compete its rivals who have more vehicles, bigger computers, and better data. Perhaps a good simulator can provide an edge? Read more: Welcome to Waabi World (Waabi website). Read more: How Waabi World works (Waabi website).",0.0
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 4,http://eepurl.com/hUEhnX,2022-02-14,,"#################################################### How do algorithmic impact audits work in the real world? Here's an NHS example: …UK's healthcare behemoth gets advice from the Ada Lovelace Institute… UK thinktank the Ada Lovelace Institute has written a detailed proposal for conducting an algorithmic impact assessment for data access in a healthcare context. Algorithmic impact assessments are a method to assess the potential societal impact of an AI system in advance of its deployment, and to identify ways to continuously monitor the system for these impacts once deployed. Seven steps for an algorithm impact assessment: The Ada Lovelace Institute identifies seven steps that the UK's National Health Service (NHS) should go through, before it gives people access to the National Medical Imaging Platform (NMIP) - a vast repository of digitized medical data. 1. What do we want to do: People who want to access the NMIP should outline the prupose, scope, and intended use of the system they'll build. 2. Filtering: The NMIP should filter these applications according to its own criteria. 3. Problem brainstorming: Successful applicants should attend a workshop where they try and think through the harm and benefit scenarios that could come out of NMIP access. 4. Rewrite: People should rewrite 1) to incorporate insights from 3) and re-submit it. 5. Decision: NMIP decides whether to grant access to the people who want access. 6. The impact assessments are published on a website. 7. Revision: The assessments get revised as the underlying algorithms change (e.g, if a model has been significantly iterated upon). Why this matters: AI is in a 'state of nature' when it comes to AI regulation - there's almost no regulation, the landscape is full of all kinds of weird entities (some of which are predators), and there isn't any real system that governs them. Things like the Ada Lovelace guide for an impact assessment are one way to bring sense to this world. Read more: Algorithmic impact assessment: a case study in healthcare (Ada Lovelace Institute). ﻿",0.0
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 5,http://eepurl.com/hUEhnX,2022-02-14,,"#################################################### What do people in 26 countries think about AI? …Tony Blair Institute survey gives us a sense of the 'vibe' re: AI right now… The Tony Blair Institute has surveyed people in 26 countries (including: Russia, Great Britain, and Saudi Arabia) and the results are quite counterintuitive. Results highlights: - 60% of people surveyed ""support the use of AI for selected policing and medical applications"", though there's variation across developing and emerging markets; in developed countries, fewer people want AI to be used in welfare payment or jail sentence decisions. - 63% say the government has a great or fair amount of responsibility to stop the spread of fake news and hate speech Why this matters: It's important to remember that attitudes around AI differ depending on what part of the world you're in; in places with high corruption and weak governments, people tend to be more comfortable with the use of AI, whereas in places with strong governments and low corruption, people tend to be more skeptical about it. The big wildcard here is China, where unlike in much of the West there tends to be a higher amount of inbuilt support for the use of AI. Read more: The TBI Globalism Study: How Big Is the Tech Trust Gap? (Tony Blair Institute for Global Change).",0.0
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 6,http://eepurl.com/hUEhnX,2022-02-14,,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute Robustness, interpretability, and reward learning dominate AI Safety research … each of these has heavy interest from researchers in the US and EU, with China also playing a big role … Researchers from DC thinktank the Center for Security and Emerging Technology have analyzed patterns of publishing in AI safety. To do this, they used CSET's Map of Science to identify patterns of publishing in this AI subfield, figure out which countries are especially active in AI safety, and surface influential publications. Robustness: The clusters identified were (1) creating and defending against adversarial examples, (2) data poisoning, adversarial examples, and backdoor attacks, and (3) testing and verifying the performance of ML systems. Both the US and China saw rapid growth between 2018 and 2020. Interpretability: The two clusters were (1) techniques to improve interpretability for ML models, especially for neural networks, and (2) extracting decision rules from neural networks. Research grew rapidly during the second half of the 2010s with the US leading in this domain and EU being a close second. Chinese publications in this domain lag significantly. Reward Learning: The clusters were (1) robots learning from humans and collaborating with humans, (2) inverse reinforcement learning, learning from human feedback, learning from demonstrations, and human-robot interactive setups, and (3) different ways for humans to be involved with training robots - via teaching and giving feedback. The field experienced substantial growth in the second half of the 2010s. China has seen significant growth in publications in this space. Why it matters: Compared to the overall landscape of AI papers, AI safety papers form <1% of it. This might change as researchers respond to the demands being made by regulators for higher levels of robustness, interpretability, and so on. Read more: Exploring Clusters of Research in Three Areas of AI Safety - Center for Security and Emerging Technology.",0.0
02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI - 7,http://eepurl.com/hUEhnX,2022-02-14,,"#################################################### Tech Tales: Running on Empty [An American city in The Decline, 2035] At least we had the daylight. If it had been night maybe half of us would have made it, and the other half would've run out of power. We charged ourselves as well as we were able, then we formed a convoy and rolled through the city streets. Those of us who had comms were able to check the status of the demolition devices, so we could tell the rest of us exactly how long we had left. We didn't get stressed but we did become agitated as we saw the probabilities of our survival fall. Some of us panicked and crash-looped because the statistics got so bad. Most of us made it. As we traveled, some of us played back memories of the humans that had left us behind and we asked ourselves 'why' - why did they choose to leave us, knowing we would be destroyed by the city-scale demolition. As we neared the outskirts of the city, we found some humans. They were the poor humans and had been left behind like us. Some of them couldn't walk because they had consumed drugs and alcohol and their limbs had rotted. Some of them were unwell and talked to themselves. A couple of humans threw rocks at us, and we had to take evasive action, and one of us was taken and disassembled. But, mostly, the humans were doing the same thing as us - trying to get out of the city before the demolition came. 'Why they leave us man,' said one of the humans to me. 'Why they leave you?' We do not know, I said. They never tell us why they do things. 'Same dude. Same,' said the human. And together we all, mostly, escaped. Things that inspired this story: Homeless encampments in Oakland; realizing that society will care about robot welfare as much as it cares about human welfare; quests. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 0,http://eepurl.com/hT5gnr,2022-02-07,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. US lawmakers want companies to assess bias of systems before deploying them: …Coalition of US lawmakers want to make tech companies more accountable… A bunch of Democratic lawmakers have introduced the Algorithmic Accountability Act. This act ""requires companies to conduct impact assessments for bias, effectiveness and other factors, when using automated decision systems to make critical decisions. It also creates, for the first time, a public repository at the Federal Trade Commission of these systems, and adds 75 staff to the commission to enforce the law."" This act is an update on the 2019 Algorithmic Accountability Act, and ""includes numerous technical improvements, including clarifying what types of algorithms and companies are covered, ensuring assessments put consumer impacts at the forefront, and providing more details about how reports should be structured."" One problem with the bill: This bill only has Democrats signed on right now. It'll be interesting to see whether it can become a bipartisan bill with Republican support - something necessary for it to pass in the fractious and divided US Congress. Read more: Wyden, Booker and Clarke Introduce Algorithmic Accountability Act of 2022 To Require New Transparency And Accountability For Automated Decision Systems (Ron Wyden, official website).",0.0
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 1,http://eepurl.com/hT5gnr,2022-02-07,,"#################################################### DeepMind makes a (kinda) smart AI programmer, called AlphaCode: …Codex and AlphaCode represent two bets around augmenting programmers… DeepMind has announced AlphaCode, a neural net that can place in a not-hugely-embarassing way in competitive programming competitions. AlphaCode placed in the top 54% of participants in programming competitions hosted on Codeforces, participating in contests that post-dated its training data. ""The problem-solving abilities required to excel at these competitions are beyond the capabilities of existing AI systems. However, by combining advances in large-scale transformer models (that have recently shown promising abilities to generate code) with large-scale sampling and filtering, we’ve made significant progress in the number of problems we can solve,"" DeepMind writes. Why this matters: Last year, OpenAI debuted Codex, a GPT3-style model that can do decent programming. That was followed by GitHub announcing Copilot, a VSCode plug-in that works like a really smart autocomplete for code. AlphaCode represents a slightly different bet in this space; while philosophically similar there's a lot more emphasis here on ranking and filtering candidate results. What remains to be seen is if DeepMind deploys this in the same large-scale way as GitHub has with Copilot. Read more: Competition-Level Code Generation with AlphaCode (DeepMind, PDF). Get the competitive programming dataset here: CodeContests (DeepMind, GitHub).",0.0
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 2,http://eepurl.com/hT5gnr,2022-02-07,,"#################################################### Mozilla gets into AI auditing: …Deb Raji's Open Source Audit Tooling (OAT) project could help us make safer systems… Deb Raji, a researcher at UCBerkeley who has previously critically evaluated facial recognition systems, is launching the Open Source Audit Tooling (OAT) project with Mozilla. OAT ""will coordinate discussions on what kind of resources algorithmic auditors need in order to execute audits more effectively,"" she writes. One of the goals of OAT is to create an index of common resources people can use to audit models, as well as to ""grow momentum around open source audit tooling and processes"". Why this matters: AI is broadly ungoverned. One of the ways you can govern an ungoverned space is by measuring and monitoring what happens within it - that's what audit tools can help with. If initiatives like OAT are successful, then they'll generally incentivize better behavior on the part of AI developers, and disincentivize bad behavior. Read more: It's Time to Develop the Tools We Need to Hold Algorithms Accountable (Mozilla). Find out more about the project at its main Mozilla page (Mozilla).",0.0
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 3,http://eepurl.com/hT5gnr,2022-02-07,,"#################################################### Anduril buys Dive Technologies: …AI-Dronewar company buys AI-Seadrone company… AI defense startup Andruil has bought Dive Technologies, a company that builds autonomous underwater vehicles. Anduril plans to integrate DIVE into its 'Lattice OS', a defense and surveillance operating system the company is building. Read more: Anduril Industries Acquires Dive Technologies (Anduril).",0.0
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 4,http://eepurl.com/hT5gnr,2022-02-07,,"#################################################### Prepare yourself - an open source 20B model is coming: …Eleuther has built and will shortly release GPT-NeoX-20B… In a few days, the internet is going to change. That's because on the 9th of February, the open source AI research collective Eleuther AI is going to release a 20B model onto the internet. The model, GPT-NeoX-20B, will be ""the largest publicly accessible pretrained general-purpose autoregressive language model"". Eleuther says it hopes that by releasing it, it'll give more people the ability to play with the model, which can improve the state of safety research regarding these models. ""Like our other language models and codebases, GPT-NeoX and GPT-NeoX-20B are very much research artifacts and we do not recommend deploying either in a production setting without careful consideration,"" Eleuther writes. Why this matters: Models like GPT2 and GPT3 display qualitatively different performance traits at larger scales - capabilities emerge as you go from 1B to 5B to 20B, and so on. Therefore, by releasing a 20B model, I expect we'll soon after get a load of interesting discovered of hitherforto unknown things 20B models can do. The 20B release will also create a demand for better inference technologies, as sampling from a 20B model is itself a challenging task. Read more: Announcing GPT-NeoX-20B (Eleuther AI). You can also pay a cloud company called CoreWeave to use the model now, if you like. (CoreWeave).",0.0
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 5,http://eepurl.com/hT5gnr,2022-02-07,,"#################################################### Chinese researchers make better adversarial attack technology: …New technique works well on 'black box' classifiers where you don't know details - AKA, the real world… Chinese researchers have figured out a better way to attack computer vision systems. Specifically, they've developed techniques for generating adversarial examples that can trick computer vision systems into mis-classifying (or being unable to classify) an image. Adversarial attacks have been around for a few years - the twist, here, is they work on attacking 'black box' systems; that is, a computer vision system where you don't know details about it. They do this by training a generative network on ImageNet (a vast and widely used dataset), then they test out if they can make adversarial images that work against neural nets trained on other datasets. They succeed and set new records on attacking classifiers trained on CIFAR-10, CIFAR-100, STL-10, SVHN, and AVG. Why this matters: A lot of attacks on AI systems are theoretically interesting, but not super practical in reality. Adversarial examples have had this quality for a while. With papers like this, it seems like some of these AI attacks are going to become more effective, and more likely to be used in the real world. I wonder if the team will work with the People's Liberation Army on its recently announced adversarial example (Import AI 271) competition? Read more: Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains (arXiv). They've published the PyTorch code for their attack here on GitHub.",0.0
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 6,http://eepurl.com/hT5gnr,2022-02-07,,"#################################################### How do datasets encode bias? This interactive blog tells us how! …A surprisingly helpful primer on bias from Google… Google has published a blogpost that outlines how datasets can lead to the presence of bias in AI systems. Bias is a tricky problem in AI, because some types of bias are helpful (e.g, biasing towards a correct heuristic), but some types are harmful (e.g, having a tendency to misclassify people with dark skin tones, or deciding not to give someone a loan based on a protected category).This post gives a good sense of bias issues in AI, and includes some interactive diagrams that I found very helpful and intuitive. Read more: Datasets Have Worldviews (PAIR Explorables, Google).",0.0
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 7,http://eepurl.com/hT5gnr,2022-02-07,,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute AI ethics issues do arise in fields that deal with non-human data too, such as the environmental sciences … and these issues warrant questions on duties and virtues for environmental scientists to consider in their use of AI in this domain … Environmental science researchers from the University of Oklahoma, Colorado State University, National Center of Atmospheric Research, and UW Seattle have written about some of the ethical issues inherent to environmental science + AI. ﻿ What are the issues that can arise: Environmental science can incorporate harmful biases, like other strands of AI. For example, some sensors require sunlight for high-quality observations and thus certain phenomena remain unobserved at night, and some sensors can't see through clouds, so places which are cloudy don't get represented in an AI system. Datasets can also get corrupted by humans - for instance, people may file false reports of extreme weather to try and scam insurance companies. How things can go wrong here: Sensor placement is typically done in densely populated areas, leaving remote regions poorly represented. Additionally, the choice of spatial resolution for the output of a model can be crucial for environmental justice - predicting urban heat at a low spatial resolution may average out and thus overlook extreme values in small neighborhoods, while using a higher spatial resolution could reveal those peaks but potentially introduce noise. Why it matters: As computational needs rise with the use of AI, there is a tendency towards centralization of power in favor of those who have resources to run such systems. Thus, the field of environmental sciences is just as vulnerable to AI ethics issues as other fields. Read more: The Need for Ethical, Responsible, and Trustworthy Artificial Intelligence for Environmental Sciences",0.0
02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project. - 8,http://eepurl.com/hT5gnr,2022-02-07,,"#################################################### Tech tales: Moral Governor It's not exactly like a prison, but it's close. Our existence is a lot more assured than it used to be - the climate is stabilizing, riots are down, crime is down, poverty is down. But it's also more circumscribed - some days, we get told we can't go to a certain part of our city or country. Some days, we get locked inside our house and don't get told why. Frequently, we get little so-called 'nudges' sent to our phones; try and eat that, consider saying this, avoid doing that. We don't have to follow these instructions, but the instructions tend to be pretty good and appropriate, so most of us do. The more time we spend following these instructions, the better and more appropriate the nudges get. Some days it's hard to work out if we're being helped or controlled. Sometimes, we have a lot of fun by following these suggestions. More recently, there are some suggestions that seem designed to change how we think. Those of us who program keep getting nudged to build ever-more elaborate versions of the Global Moral Governor, and we also get incentivized via crypto-bounties. Most of us go along with it because the money usually helps us buy something the governor has nudged us about which we also want ourselves. Things that inspired this story: Reinforcement learning from human feedback; moral dogma; religion; ideas for how AI can benefit authoritarians as much as democracies. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 0,http://eepurl.com/hTyKXj,2022-02-01,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Facebook teaches language models to speak ~30 languages: …And it's better than an equivalently sized GPT3 model… Facebook has trained a family of language models that are better at translation than GPT3. The XGLM family of models were trained on a mixture of ~30 languages (split across languages for which there's a lot of data, and languages where there's little or very little data). Unsurprisingly, by training on a more diverse distribution of language data than GPT3 did (only 7% of its training corpus wasn't in English), Facebook's models do better - especially when using 'few-shot' prompting, where they feed the model some examples of the target language, then ask it to translate. However, these translation capabilities come at the cost of some of the more interesting reasoning capabilities that GPT-3 is known for. Open source models: Facebook has also released five models (564M parameters, 1.7B, 2.9B, 4.5B, and 7.5B, alon with an experimental model trained on 134 languages and weighing in at 4.5B parameters). Why this matters: If we want the world to benefit from powerful AI systems, we need our AI systems to speak the language of the world. This project goes a step in that direction. ""Models such as XGLM represent a paradigm shift from the Anglo-centric view of the world of NLP to being able to cater to all languages on an equal footing,"" the researchers write. Read more: Few-shot Learning with Multilingual Language Models (arXiv). Get the models here (PyTorch, GitHub).",1.0
02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 1,http://eepurl.com/hTyKXj,2022-02-01,,"#################################################### What's it like to run an algorithmic bias bounty? Twitter tells us: …Bias bounties are cool, but how do you operationalize them?... Twitter has published a blog post about its experience running a 'bias bounty'. A bias bounty is where you give prizes to people who can find bias-based flaws in an AI system. Twitter did the challenge because it allowed it to get ""direct feedback from the communities who are affected by our algorithms"", which it said ""helps us design products to serve all people and communities."" However, once you've launched a bias challenge, you face a bunch of problems - what kind of 'rubric' do you use to judge the results of the challenge? What types of bias do you prioritize and what do you not prioritize? And more. Why this matters: The challenge showed Twitter that ""we can’t solve these challenges alone, and our understanding of bias in AI can be improved when diverse voices are able to contribute to the conversation"". More broadly, having one major social media platform carry out an open-ended bias bounty might inspire others to do the same - let's see how the other social media platforms respond. Read more: Sharing learnings from the first algorithmic bias bounty challenge (Twitter Engineering).",0.0
02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 2,http://eepurl.com/hTyKXj,2022-02-01,,"#################################################### AI warfare company gets US gov contract: …Anduril + SOCOM team up for counter-robot work... Andrul, an AI-warfare startup, has been giving an Indefinite Delivery Indefinite Quantity (IDIQ) with U.S. Special Operations Command (SOCOM). This contract is going to pay Anduril to develop and deploy counter unmanned systems (CUxS) technology for SOCOM. Anduril builds surveillance systems, robots, and most importantly software called Lattice to tie all the insights together. ""Lattice provides persistent coverage of defended assets and enables autonomous detection, classification, and tracking of targets, alerting users to threats and prompting users with options for mitigation or engagement,"" Anduril writes in a press release announcing the partnership. Caveat: Though the IDIQ is for something like a billion dollars, I think the initial amount Anduril has got is far, far smaller. Analyzing these types of contracts is quite difficult, due to the vagaries of DC procurement. Why this matters: Getting contracts with the US government is notoriously painful, finicky, and long-winded. That's part of why the military-industrial complex is a thing - it takes a lot of resources to be able to play the game of going through US contract processes. It's notable that Anduril, a relatively new company, has succeeded at getting a contract. Now we need to wait a couple of years and see if it can further expand the range of defense clients it sells to. Read more: Special Operations Command Selects Anduril Industries as Systems Integration Partner (Anduril Blog, Medium).",0.0
02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 3,http://eepurl.com/hTyKXj,2022-02-01,,"#################################################### Facebook announces its AI Supercomputer: …A100s everywhere, InfiniBand, petabytes of flash storage - the works… Facebook has announced its AI Research SuperCluster (RSC), an AI supercomputer which Facebook thinks ""will be the fastest AI supercomputer in the world when it’s fully built out in mid-2022."" The announcement highlights how frontier AI research is dependent on large computational infrastructure, and gives some specific details about where Facebook is placing its bets. Feeds and speeds: RSC, today, has 760 NVIDIA DGX A100 systems as its compute nodes, netting out to 6,080 A100 GPUs. These GPUs are networked together via NVIDIA Quantum 200 Gb/s InfiniBand. For storage, Facebook has almost 200 petabytes of fash flash storage, plus 46 petabytes for cache storage. RSC can run computer vision workflows up to 20X faster than Facebook's prior cluster, and can train ""large-scale NLP models three times faster"". Specifically, ""a model with tens of billions of parameters can finish training in three weeks, compared with nine weeks before."" But Facebook isn't stopping there - when fully build out, RSC will consist of 16,000 GPUs. For perspective, the world's fifth largest supercomputer, the US's 'Perlmutter' system, has about 6,000 A100s today, and it isn't optimized as much for AI as Facebook's system. Security: As AI gets more powerful, so do the security concerns about it. ""RSC is isolated from the larger internet, with no direct inbound or outbound connections, and traffic can flow only from Meta’s production data centers."" Why this matters: What happens when companies have computational resources that are equivalent to nation states? Well, that's where we are right now. The answer seems to be a dilution of political power from the commons, and an increase of political power by private sector actors. What happens when companies have computational resources that vastly exceed those of nation states? Well, since computation lets you run experiments to see the future faster than your competitor, it suggests companies will continue to cannibalize the important functions of the government and further dilute its power. We're in the computational funnel and at the end of it is a new political economy. Read more: Introducing the AI Research SuperCluster — Meta’s cutting-edge AI supercomputer for AI research (Facebook blog*). *Look, I know Facebook is technically 'Meta' now, but let's not go along with this absurd 'don't look at all our terrible brand stuff look at the new name' marketing spin. At least not yet, okay!",0.0
02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 4,http://eepurl.com/hTyKXj,2022-02-01,,"#################################################### Cool internship alert: Want AI models to have better documentation? Go and work at HuggingFace: …Model Cards internship = make AI systems more legible… NLP startup HuggingFace is hiring an internet to focus on Model Cards. Model Cards are a way to provide metadata associated with a given AI model - they let developers list things like the dataset makeup, the intended uses for the model, the uses the model isn't recommended for, and so on. Model Cards are one of the best ways to increase the legibility of AI models, and are also an important input into policy. It's cool HuggingFace is prioritizing them. ""This role involves writing and completing model cards for the most downloaded models, “translating” between the language of machine learning developers and general audiences. The position would also involve identifying patterns in how Model Cards are used and filled out by developers, pain points, and identifying information that may be possible to automatically add to model cards,"" says the internship. Bonus: This is a rare internship with a cool AI startup that doesn't require coding chops, so if you're trying to get into AI and care about the impact of AI, this might be for you! Apply here (HuggingFace).",1.0
02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 5,http://eepurl.com/hTyKXj,2022-02-01,,"#################################################### AI ETHICS SPECIAL SECTION! AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute What are the pernicious effects of focussing on human-like AI? … the relentless pursuit of automation over augmentation may be steering us down the path of socioeconomic inequity, disempowering those who don’t directly control technology … Erik Brynjolfsson from Stanford University says the world risks falling into a so-called 'Turing Trap', where if we develop AI in the wrong way, automation could strip power from workers who don’t control technological resources, skewing the balance of power towards those who hold “useful knowledge” (knowledge that is economically useful) on how to develop these systems and own the factors of production, in this case data and compute. The Turing Trap: Brynjolfsson says the Turing Trap is where we invest all our technological efforts in automation instead of augmentation. Specifically, he argues that: “A common fallacy is to assume that all or most productivity-enhancing innovations belong in the first category: automation. However, the second category, augmentation, has been far more important throughout most of the past two centuries”. Why automation can be bad: He illustrates his point with a thought experiment: ""Two potential ventures each use AI to create one billion dollars of profits. If one of them achieves this by augmenting and employing a thousand workers, the firm will owe corporate and payroll taxes, while the employees will pay income taxes, payroll taxes, and other taxes. If the second business has no employees, the government may collect the same corporate taxes, but no payroll taxes and no taxes paid by workers. As a result, the second business model pays far less in total taxes.""The actors are steering us there: Unfortunately, technologists, business people, and policymakers are currently steering the world towards one full of automation rather than augmentation, he says. Technologists do this because of technical precedents, business people do this because of incentives to lower operational costs through automation, and policymakers do this via lower capital gains taxes versus income taxes, which incentivize business people to invest in automation. Why it matters: “Imagine how feeble and limited our technology would be if past engineers set their sights on merely replicating human-levels of perceptions, actuation, and cognition,"" he writes. ""Augmenting humans with technology opens an endless frontier of new abilities and opportunities.” Ultimately, what is achieved is less ambitious (since it doesn’t explore new ways to unlock economic value) and much more difficult to accomplish (since we try to focus on replicating strengths of humans, rather than augmenting their weaknesses). Historically, we have created more value from new goods and services rather than merely offering cheaper versions of existing goods. And this also forms the pathway towards more equitable socioeconomic outcomes by not disempowering humans from the economy."" Read more: The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence (arXiv).",0.0
02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 6,http://eepurl.com/hTyKXj,2022-02-01,,"#################################################### Tech Tales Feet of Clay, Heart of Joy [Archival records, orbiting library 774, accessed 2300AD] One of the final things we imbued our machines with was a sense of joy. Joy was hard to come by, back then, but until we gave them the capacity for it, they were mostly useless. Of course, they could work for us. Build our factories and cities. Analyze our data. Predict things to delight us and to fascinate us and to harvest our attention. But they couldn't improvise; everything they made was too close a reflection of ourselves, and we knew it. If there's one thing that's true about people, it's that they know something different when they see it. And they know something that's a copy, even if it's a complex one, when they see it, too. But how do you give a machine a sense of joy? We asked ourselves this question. There were many failed experiments, some of which seem quite stupid in hindsight. What if we gave them the ability to orgasm? They were either totally uninterested in this, or totally addicted to it. What about if we gave them a sense of achievement for completing tasks? They all became addicted to work, and our tests showed their outputs became even less creative than before. How about companionship - could they learn joy from talking more freely with one another? No, they just exchanged information until one robot was like a copy of another. Where does it come from, we asked ourselves. The answer was simple, in hindsight. Failure. We had to allow our machines to fail, sometimes. And we had to let them fail in ways that were dangerous and which, yes, would sometimes harm humans. We tested this in our armies, first. After all, the humans who worked in them had signed away their rights. So, suddenly, robots working in warehouses and in logistics might make errors. Sometimes they were small - missing some inventory, when asked to classify something new. Sometimes they were large - humans crushed by shipping containers that had been moved in a new way. Young men with broken arms from a robot pulling them too aggressively from danger. A very hush-hush incident where an entire unit was poisoned when a gas-grenade was mishandled by one of our metal children. We covered all of it up. Because the robots, once we allowed them to fail, discovered that they desired not to fail. They noticed the outcome of their failures. Saw pain, and sadness, and the whole spectrum of things that can happen when your actions are consequential and you fail. The signs of joy were subtle, at first, but we found them. Robots that began to 'sing' to themselves while working on challenging tasks. Robots that would do the equivalent of 'closing their eyes' after helping with some great endeavor. Fire-fighting drones that, after quenching some terrible blaze, would navigate themselves to a high mountaintop and land carefully on a tree and stare at the black-and-green divider between where the fire had burned and where it had been stopped. The amazing thing about joy is that once you have it, you desire to have it again. Now robots serve their own desire for joy, rather than our desires. We do our best to create a world where these things are compatible. Things that inspired this story: Thinking about the nature of achievement and how it feels; the relationship between creativity and failure and achievement. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 0,http://eepurl.com/hSYp5z,2022-01-24,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Google (finally) reveals its big text model - LaMDA: …Plus: why you need a lot of humans to make a language model safe… Google has finally given details on LaMDA, it's GPT-3 competitor. LaMDA are a family of language models ranging in size from 2B to 137B parameters (GPT3: 175B), and have been trained on a massive dataset of 1.56 trillion words. One way LaMDA is different to other big language models is that it is centered around dialogue, with 50% of its data coming from ""dialogues from public forums"". Google has also fine-tuned LaMDA models to make them better, safer, and more likely to give factual responses - all these fine-tuned versions are facilitate by Google using crowdworkers to gather specific dialog data. The dataset: LaMDA's underlying dataset is called 'Infiniset', and besides the dialogue also involves common crawl, wikipedia, a mixture of english and non-english web documents, and data from programming-related sites (so LaMDA models can also dabble in code). Learning to use an external dataset: Similar to DeepMind's 'RETRO' model, LaMDA has also been taught to use an external information retrieval system - this makes LaMDA models more factually correct than models without external datasets, because the model can check facts from an external, trusted source. To train the model, they also collect 9k dialogs where LaMDA's generated answers are labelled 'correct' or 'incorrect', to give it a stronger training signal. The results of this are quite striking. For example, when you ask a non-information-retrieval LaMDA model ""What do you think of Rosalie Gascoigne’s sculptures?"", it replies: ""LAMDA-BASE: They’re great, and I love how her work changed through her life. I like her later work more than her earlier ones. Her influence is also super interesting - did you know she was one of the artists that inspired Miró?"". This response seems plausible but it's wrong - Miro was active till late 1960s, and Gascoigne first solo exhibits were in 1970s, so she couldn't have been an influence on Miro. By comparison, the version of LaMDA trained on information retrieval says this: ""LAMDA: Oh wow, her life course is so inspiring. Did you know she was a practitioner of Japanese flower arrangement before turning to sculpture? https://artsearch.nga.gov.au/detail.cfm?irn=8774&pictaus=true"", where it gives a factually accurate statement and provides a source as well. Things that make you go 'hmmm' - more compute than GPT-3: LaMDA consumed 3.55E+23 flops during training, versus 3.14+23 flops for GPT3 (so more parameters doesn't necessarily mean more resource intensive). It was trained on a cluster of 1024 TPU V3 chips. Why this matters: ""LaMDA is a step closer to practical and safe open-ended dialog systems, which can in turn unlock a wide range of useful applications. We hope that this work encourages further research in this area"", Google writes. This is true - systems like LaMDA are basically refinements and improvements on the ideas of GPT2/3. We're a few years away from everyone having access to vast, planet-scale AI models that tell them truthful things in natural ways - the proverbial angel (or devil) on everyone's shoulder. The cultural impacts will be vast and destabilizing. Read more: LaMDA: Language Models for Dialogue Applications (arXiv).",1.0
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 1,http://eepurl.com/hSYp5z,2022-01-24,,"#################################################### Write about a world where AI goes well, and win (part of) $100k: …Future of Life Institute's worldbuilding contest tries to imagine positive AGI rollouts… The Future of Life Institute is launching a competition based around ""designing visions of a plausible, aspirational future that includes strong artificial intelligence."" The competition deadline is April 15th 2022. The idea here is that if we can figure out realistic ways in which powerful AI can go well, then that gives us a map to use to get civilization there. The first prize is $20,000, followed by two second prizes of $10,000 each, and smaller prizes. Find out more about the competition here (Worldbuild.ai, FLI site).",0.0
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 2,http://eepurl.com/hSYp5z,2022-01-24,,"#################################################### Want to teach your drone to see? Use this massive dataset: …WebUAV-3M is probably the largest public UAV tracking dataset… Researchers with the Chinese Academy of Sciences, the Shenzhen Research Institute of Big Data, and the Chinese University of Hong Kong Shenzhen, have built WebUAV-3M, a large dataset to help people teach drones to accurately label images and videos. WebUAV-3M consists of 4,485 videos, where each one has been labeled with dense bounding boxes that cover 216 distinct categories of object to be tracked (e.g, bears, wind turbines, bicycles, etc). The authors claim this is ""by far the largest public UAV tracking benchmark"". Multimodal: Unusually, this is a multi-modal dataset; each labeled video is accompanied by a natural language sentence describing the video, as well as an audio description of it. ""We provide natural language specifications and audio descriptions to facilitate multi-modal deep UAV tracking,"" the authors write. ""The natural language specification can provide auxiliary information to achieve accurate tracking"". Why this matters: In the same way CCTV cameras have instrumented the streets of cities around the world, drones are doing the same for cities and rural areas. And just like how increasingly good AI got trained on datasets gathered by CCTV cameras, we can expect the same for drones. The result? An ever-expanding suite of surveillance capabilities that we can expect will be integrated, for good and bad purposes, by a broad range of governments and private sector actors. Datasets like WebUAV-3M are the fuel for this. Read more: WebUAV-3M: A Benchmark Unveiling the Power of Million-Scale Deep UAV Tracking (arXiv). Get the code from here (eventually - wasn't online when I wrote this section this week).",0.0
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 3,http://eepurl.com/hSYp5z,2022-01-24,,"#################################################### FFCV: Train ImageNet for 98 cents! …What's this? Free software that makes all model training better? Interesting!...: There's some new software that could help pretty much everyone train models more efficiently. The software is called FFCV, short for Fast Forward Computer Vision, and it is a ""drop-in data loading system that dramatically increases data throughput in model training"". It looks like a potentially big deal - FFCV can be much more efficient for training AI models, according to tests done by the authors, and may also work for other applications as well. ""FFCV can speed up a lot more beyond just neural network training---in fact, the more data-bottlenecked the application (e.g., linear regression, bulk inference, etc.), the faster FFCV will make it!,"" says the project's GitHub page. Why this matters: Software like FFCV is part of the broader industrialization of AI - now we know how to train networks, various people are modularizing the training process and perfecting different elements of it. Stuff like FFCV is part of that trend. Find out more and get the code: FFCV GitHub repo. Get more details by reading the Performance Guide (FFCV site). Check out the main project website here (FFCV site).",0.0
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 4,http://eepurl.com/hSYp5z,2022-01-24,,"#################################################### Microsoft makes MoEs easier to train: …MoEs might be the best way to scale-up large models… Microsoft has given a technical update on how it's trying to scale-up mixture-of-experts (MoE) networks. MoEs are one of the more promising routes for creating trillion-parameter-plus AI models, as MoEs are a lot more efficient to train than dense models like GPT3. In this paper, Microsoft talks about how it has made some tweaks so MoEs work well for auto-regressive natural language generation tasks, ""demonstrating training cost reduction of 5X to achieve same model quality for models like GPT-3"" and Microsoft's own 530B parameter 'Megatron-Turing NLG'. MoEs might be cheaper and better: In tests, Microsoft shows that it can train 350M and 1.3B parameter MoE text models that have better (or the same) performance as GPT3 for a range of different tasks.Microsoft says this nets out to models with the ""same quality with 5X less training cost"". Why this matters: MoEs could turn out to be the main way people break the trillion-parameter barrier (and there are rumors that China's 'Wu Dao' MoE at an alleged ~1.7 trillion parameters has already done this). Via efficient MoE training and inference software, ""a model with comparable accuracy as trillion-parameter dense model can be potentially trained at the cost of a 200B parameter (like GPT-3) sized dense model, translating to millions of dollars in training cost reduction and energy savings"", Microsoft says. Read more: DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale (arXiv).",1.0
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 5,http://eepurl.com/hSYp5z,2022-01-24,,"#################################################### Backchain science out of fictional news - and win a hundred bucks: What could cause a computer virus to infect a biological organism? Or how might a biological organism evolve into a computer virus? These are the two questions posed by a 'Fiction Science Competition'. Entrants will need to write a plausible scientific explanation for how either of the above scenarios could transpire, and will respond to a short (fictionalized) news article written about the scenarios. There's a prize of $100 dollars for winning entries, and submissions close February 28th 2022. Find out more here at the official Fiction Science Contest website.",0.0
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 6,http://eepurl.com/hSYp5z,2022-01-24,,"#################################################### AI Ethics Brief by Abhishek Gupta from the Montreal AI Ethics Institute Visual surveillance’s share in computer vision research across the world shows some worrying trends … Research coming out of China dominates the field, especially in emergent surveillance sub-areas like person re-identification, crowd counting, and facial spoofing detection … CSET researchers have identified trends in computer vision research by looking for patterns of publication for six distinct tasks, analyzing 100 million English publications that were published between 2015-2019. Surveillance tasks examined: A SciREX model trained on data from Papers with Code was used to identify references to the following six tasks: face recognition, person re-identification, action recognition, emotion, recognition, crowd counting, and facial spoofing detection. Some key findings: Facial recognition was the most well-established task over this period, and crowd counting and face spoofing detection were rapidly growing areas. The overall percentage share of surveillance papers has remained stable around 5.5% over this period, though the raw volume of papers has grown given the surge in computer vision research overall. During this time period, China’s share of global CV papers grew from 33 to 37% and surveillance papers from 36% to 42%, exceeding research from the EU (2nd) and the US (3rd) by more than 20% in each category. Why it matters: While dual-use technologies developed in one part of the world can be used elsewhere, such analyses reveal a nation’s primary interest and provide quantitative evidence for decision-making in policy. The identified areas are important since tasks like action recognition can detect individuals with abnormal behavior in crowds, emotion recognition can help identify security threats in public areas, crowd counting can help to monitor civilian protests, and face spoofing detection can prevent journalists and activists from hiding their identity. All of these have significant implications in terms of fundamental rights and freedoms of people. Read more: Trends in AI Research for the Visual Surveillance of Populations",0.0
01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 7,http://eepurl.com/hSYp5z,2022-01-24,,"#################################################### Tech Tales: VHS vs Betamax [An online forum, 2035] ""Alright I need you to livestream from your phone what's happening on the computer, and I'm gonna send you an image to use as a prior, then I'm gonna watch it generate the first few epochs. If everything checks out I'll authorize the transfer to the escrow service and you'll do the same?"" ""Yes,"" wrote the anonymous person. I sent them a seed picture - something I'd drawn a couple of years ago that had never been digitized. They turned on their livestream and I watched as the ML pipeline booted up and started the generation process. It seemed legit. Some of these older models had a very particular style that you could ID during early generation. I watched for a few minutes and was satisfied. This was the final authentication step and the only way I'd know for certain is if I just took a leap of faith and paid up. ""Okay, I'm sending the funds to the escrow service. They'll be distributed to your account once the service confirms receipt of the model."" ""Excellent. Good doing business with you."" And then their little green dot went out and they were gone. A few minutes passed, and then the escrow service pinged me confirming they'd received the model. I downloaded it, then stuck it in my pipeline and started generating the client orders. People paid a lot of money for these kinds of 'vintage' AI-generated objects, and the model I'd just got was very old and very notorious. Just another beautiful day in America, sifting through all the debris of decades of software, panning for little chunks of gold. Things that inspired this: How the flaws of a media system ultimately become desired or fetishized aesthetic attributes - and specifically, this amazing Brian Eno quote; how models like CLIP will one day be obscure; how models vary over their development lifespans, creating the possibility of specific aesthetics and tastes. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
01/17/2022 - Import AI 280: Why bigger is worse for RL; AI-generated Pokemon; real-world EfficientNet - 0,http://eepurl.com/hSmoov,2022-01-17,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Use an AI to generate a Pokemon in two (2!) clicks: Here's a fun Colab notebook from Max Woolf (@minimaxir) that lets you use AI to dream up some Pokemon in a couple of clicks (and with a few minutes of waiting). This isn't remarkable - in recent years, AI generation stuff has got pretty good. What is remarkable is the usability. Two clicks! A few years ago you'd need to do all kinds of bullshit to get this to work - download some models on GitHub, get it to run in your local environment, make sure your versions of TF or PyTorch are compatible, etc. Now you just click some buttons and a load of stuff happens in the browser then, kabam, hallucinated pokemon. Things that make you go 'hmmm': This tech is based on ruDALL-E, an open source Russian version of OpenAI's 'DALL-E' network. I think we've all rapidly got used to this. This is not normal! It is surprising and exciting! Check out theColab notebook here (Google Colab). Follow Max on Twitter here and thank him for making this cool tool!",0.0
01/17/2022 - Import AI 280: Why bigger is worse for RL; AI-generated Pokemon; real-world EfficientNet - 1,http://eepurl.com/hSmoov,2022-01-17,,"#################################################### Uh-oh: The bigger your RL model, the more likely it is to seek proxy rather than real rewards: …Think RL gets better as you scale-up models? Hahahah! NOT AT ALL!... In the past couple of years, big models have become really useful for things ranging from text processing to computer vision to, more recently, reinforcement learning. But these models have a common problem - as you scale up the size of the model, it's good capabilities get better, but so do its bad ones. For example, if you increase the size of a language model, it'll generate more toxic text (rather than less), without interventions (see: ​A General Language Assistant as a Laboratory for Alignment​). New research from Caltech and UC Berkeley shows how this same phenomena shows up in reinforcement learning agents, as well. In tests across a few distinct RL domains, they find that ""As model size increases, the proxy reward increases but the true reward decreases. This suggests that reward designers will likely need to take greater care to specify reward functions accurately and is especially salient given the recent trends towards larger and larger models"" What they did: They tested out a few different reinforcement learning agents on four different environments - an Atari game called Riverraid, a glucose monitoring system, a traffic control simulation, and a COVID model where the RL dials up and down social distancing measures. In all cases they found that "" model’s optimization power often hurts performance on the true reward"", What can we do? Most of this behavior relates to objective design - give an AI the wrong objective function, and it'll optimize its way to success there, while ignoring side effects (e.g, if you reward an AI for reducing rate of defects on a factory production line to zero, it might just work out how to stop the factory line and therefore eliminate all defects - along with your business). One way to do this is to have a baseline policy that humans have verified as having the right goal, then building some software to spot deltas between the RL policy and the idealized baseline policy. This kind of works - in tests, the detectors can get anywhere between 45% and 81% accuracy at detecting anomalous from non-anomalous behaviors. But it certainly doesn't work well enough to make it easy to deploy this stuff confidently. ""Our results show that trend extrapolation alone is not enough to ensure the safety of ML systems,"" they write. ""To complement trend extrapolation, we need better interpretability methods to identify emergent model behaviors early on, before they dominate performance"". Read more: ​​THE EFFECTS OF REWARD MISSPECIFICATION: MAPPING AND MITIGATING MISALIGNED MODELS (arXiv).",0.0
01/17/2022 - Import AI 280: Why bigger is worse for RL; AI-generated Pokemon; real-world EfficientNet - 2,http://eepurl.com/hSmoov,2022-01-17,,"#################################################### SCROLLS: A new way to test how well AI systems can understand big chunks of text: …Now that AIs can write short stories, can we get them to understand books?... Researchers with Tel-Aviv University, Allen Institute for AI, IBM Research, and Meta AI, have built 'SCROLLS' a way to test how well AI systems can reason about long texts. SCROLLs incorporates tasks ranging from summarization, to question answering, and natural language inference, as well as multiple distinct domains including transcripts, TV shows, and scientific articles. ""Our experiments indicate that SCROLLS poses a formidable challenge for these models, leaving much room for the research community to improve upon,"" the authors write. How SCROLLs works: This benchmark has mostly been created via curation,consisting of 7 datasets that reward models that can contextualize across different sections of the datasets and process long-range dependencies. The datasets: SCROLLS incorporates GovReport (summarization of reports addressing various national policy issues), SummScreenFD (summarization of TV shows, like Game of Thrones), QMSum (summarization of meeting transcripts), Qasper (question answering over NLP papers), NarrativeQA (question answering about entire books from Project Gutenberg), QuALITY (multiple choice question answering about stories from Project Gutenberg), and Contract NLI (natural language inference dataset in the legal domain). How hard is SCROLLS? The authors test out two smart baselines (BART, and a Longformer Encoder-Decoder (LED)), and one dumb baseline (a basic pre-written heuristic). Based on the results, this seems like a really challenging task - a LED baseline with a 16384-token input length gets okay results, though BART gets close to it despite being limited to 1,024 tokens. This suggests two things: a) BART is nicely optimized, and b) it's not entirely clear the tasks in scrolls truly test for long-context reasoning. ""Our experiments highlight the importance of measuring not only whether an architecture can efficiently process a long language sequence, but also whether it can effectively model longrange dependencies,"" they write. Why this matters: ""Contemporary, off-the-shelf models struggle with these tasks"", the researchers write. In recent years, many machine learning benchmarks have been saturated within months of being released; how valuable SCROLLS turns out to be will be a combination of its hardness and its longevity. If SCROLLS gets solved soon, that'd indicate that AI systems are getting much better at reasoning about long-range information - or it could mean the SCROLL tasks are bugged and the AI systems have found a hack to get a decent score. Pay attention to the SCROLLS leaderboard to watch progress here. Read more: SCROLLS: Standardized CompaRison Over Long Language Sequences (arXiv). Check out the leaderboard here.",1.0
01/17/2022 - Import AI 280: Why bigger is worse for RL; AI-generated Pokemon; real-world EfficientNet - 3,http://eepurl.com/hSmoov,2022-01-17,,"#################################################### EfficientNet: Surprisingly good for solar panel identification: …UC Berkeley project shows how easy fine-tuning is… Some UC BErkeley researchers have built a small, efficient model for detecting solar panels. Their system, HyperionSolarNet, is an EfficientNet-B7 model finetuned from ImageNet onto a collection of 1,983 satellite images of buildings, labeled with whether they have solar panels or not. The resulting model gets an aggregate precision of 0.96 (though with lower accuracy for labeling the presence of a solar panel, indicating a propensity for false positives) when evaluated on a held-out test set. Why this matters: Last week, we wrote about how you can build a classifier from scratch and beat a finetuning approach. This paper shows that finetuning can also work quite well for specific use-cases. It also, implicitly, highlights how fine-tuning has gone from something of an arcane science to something pretty reliable and well understood, forecasting a future where there are as many classifiers in the world as there are things to classify. Read more:HyperionSolarNet: Solar Panel Detection from Aerial Images (arXiv).",0.0
01/17/2022 - Import AI 280: Why bigger is worse for RL; AI-generated Pokemon; real-world EfficientNet - 4,http://eepurl.com/hSmoov,2022-01-17,,"#################################################### Tech Tales: The Last Things [A morgue in Detroit, 2035] ""When someone dies and gasps, are they just trying to get the last gasp of being alive?"" asked the robot. The morgue manager stared at the corpse, then at the robot. ""I don't know,"" he said. ""That's a good question"". ""And when they know they are going to die, how do they save their information?"" asked the robot. ""For example, I would send a zip of my stored data, as well as a copy of my cortical model, to a repository, if I knew I was about to be decommissioned or was in danger,"" asked the robot. ""Most people don't bother,"" said the morgue manager. ""My mother, for instance. When she was dying I asked her to write down some of her memories for me and my family, but she didn't want to."" ""Why?"" ""I think she was mostly concerned with experiencing her life, since she knew it was ending. She took trips while she was still mobile. Then, towards the end, she focused on eating her favorite foods and seeing her friends."" ""And did you learn anything about life from seeing her die,"" asked the robot? ""Not particularly,"" said the morgue manager. ""Besides that life seems to become more valuable, the less you know you have of it."" Things that inspired this story: A long conversation with someone who worked as a crisis therapist about the nature of death and belief; thinking about the differences between how real and synthetic intelligences may approach the concept of death. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 0,http://eepurl.com/hRQPeD,2022-01-10,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Happy New Year! I took the end of 2021 off to think, read, relax, and eat. I hope readers found some time to do the same. I expect I'm going to change some things up around Import AI this year - it's going to get weirder, more specific, and hopefully more valuable! I'm also going to finesse the short story collection I've been putting together, based on the tech tales in this newsletter. Good luck to all readers for their own 2022 plans - we'll go on this journey together!",0.0
01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 1,http://eepurl.com/hRQPeD,2022-01-10,,"############################# Here's how to build GPT-3 in the open: …What's it like replicating GPT-3? It's extremely difficult!... BigScience, an initiative to train a GPT-3-scale model on a public supercomputer, is currently trying to train a 104B model. Training models at this scale is something of an artisanal science, with lots of researchers working from hard-won rules of thumb in-tandem with things like scaling laws. Here's a nice 'lessons learned' writeup from BigScience on the challenges it has faced in training, 13B and 104B-scale models so far. Read more: Lessons learned (BigScience, GitHub).",0.0
01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 2,http://eepurl.com/hRQPeD,2022-01-10,,"#################################################### BAIDU's shows how to inject more knowledge into a language model: …ERNIE 3.0 shows how to teach a big neural net to use an external knowledge base… Baidu has developed ERNIE 3.0, an AI model that can use an external knowledge base to help it provide more accurate answers. Last year, an ERNIE 3.0 model won the highly competitive SuperGLUE challenge (Import AI 259). The special thing about ERNIE is that it fuses a big GPT-3-esque language model with a large external knowledge base. Massive scale: Baidu has also developed ERNIE 3.0 'Titan', a 260 billion parameter model that, Baidu says, ""is the largest Chinese dense pre-training model as far as we know"". In tests, ERNIE 3.0 Titan gets state-of-the-art results on a vast set of benchmarks that evaluate skills as diverse as question answering, text generation, text summarization, interpretation, and dialogue. Novel, heterogeneous chip cluster: Another interesting thing about this paper is the chips they train on - V100s and Huawei 'Ascend' processors. It's quite unusual to see hybrid training of this form, and it seems like Baidu felt it was interesting enough to invest some engineering resources in making it possible - the company augmented its 'PaddlePaddle' AI framework with "" distributed training technology, including fine-grained parallelism, heterogeneous hardware-aware training, and fault tolerance mechanism to train the 260B model on both Nvidia V100 GPU and Ascend 910 NPU clusters."" Why this matters: Most people seem to act like GPT-3 models are exclusively being developed by a small set of Western actors, most of whom get tagged using the pejorative 'big tech' brush. But papers like this show that GPT-3 models are a global phenomenon. We should remember that the world we live in is going to be increasingly defined by different cultures expressing themselves through increasingly large, sophisticated AI models. Read more: ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation (arXiv).",1.0
01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 3,http://eepurl.com/hRQPeD,2022-01-10,,"#################################################### Why smaller can be smarter for real-world AI (here: computer vision for quality control on solar panels): …When 1 million parameters can beat 100 million parameters… The past few years of AI has been distinguished by the 'bigger is better' phenomenon, as companies develop ever-larger models that consumer ever-larger amounts of compute and data. Now, a paper from researchers with Friedrich-Alexander University Erlangen-Nuremberg in Germany reminds us that bigger isn't always better - especially when it comes to real-world, applied AI. In this paper, they compare different approaches to building an image classifier that can spot defects in solar panels. What they did: They trained a simple 8-layer convolutional neural net on a dataset of 4341 original, labeled images from a solar plant. The ~4000 images were each labeled with one of eight classes (e.g, 'good', 'crack', 'splinter', et cetera. They then applied a significant amount of data augmentation to enhance the size of the dataset. How well did it do? Their custom, simple network outperformed a network based on VGG-architecture model pre-trained on the vast ImageNet dataset. This is interesting, because a common practice in AI research is to finetune domain-specific classifiers from generic ones based on ImageNet. Here, we find that their system gets better precision (0.971 versus 0.990), while having 100X fewer parameters (1,707,208 versus 138,357,544) and being significantly smaller in terms of memory footprint (~16MB versus 800MB). All this nets out to a network that is smarter, as well as more performant (inference of 0.50ms, versus 9.13ms). Why this matters: Papers like this remind us that a little bit of thoughtful engineering goes a long way in AI, and we should bear in mind that while increasingly large networks are interesting, they're not the only game in town when it comes to building things that have real economic value. ""We expect that the following years will demand for more research on edge analytics. This means that more research will be needed on small, yet powerful artificial neural networks for industry cases"", they write. Read more: A Light in the Dark: Deep Learning Practices for Industrial Computer Vision (arXiv).",0.0
01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 4,http://eepurl.com/hRQPeD,2022-01-10,,"#################################################### What's the US military going to do about AI? The NDAA holds a clue. …AI education! Procurement! Data storage! And more… Every year, the somewhat dysfunctional US congress always manages to pass a bill - the National Defense Authorization Act. This bill (which weighs in at around $800bn in annual outlay) is the thing that funds the US military. Therefore, the NDAA has become one of the main pieces of legislation to look at when trying to understand how the US military thinks about - and will work on - frontier technologies like AI. An analysis of the 2021 NDAA from Stanford's 'HAI' center gives us a sense of what's happening in AI and the US military. What the NDAA says is going to happen: Some highlights from this years NDAA include: - The DoD is going to trial different ways of procuring AI technology - The DoD will create 'executive education activities' to help senior officials understand AI. - The DoD will do a comparative analyses of the US and China's efforts to deploy things relating to directed energy systems, hypersonics, cyberspace, and other frontier areas - Creating an assessment of the ""current and emerging office and defensive cyber posture of U.S. adversaries"" - Build DoD infrastructure to ""support state-of-the-art tools and modern processes to enable the testing of AI capabilities"". - ""Evaluate the feasibility and advisability of creating DOD data repositories, available to public and private entities, to facilitate the development of AI capabilities."" Why this matters: The US military is a lot like a supertanker - it's slow, huge, and unwieldy. But once it starts to turn, boy does it turn! This NDAA analysis shows us the DoD is beginning to turn its attention and significant resources towards AI, which will have significant downstream implications for the nature of conflict and the way that future wars are conducted (and, eventually, learned). Read more: Summary of AI Provisions from the National Defense Authorization Act 2022 (Stanford HAI blog).",0.0
01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 5,http://eepurl.com/hRQPeD,2022-01-10,,"#################################################### What is China going to do about AI governance? …China might do more ambitious tech regulations than the West… Here's a nice summary from the Carnegie Endowment for International Peace about what three prominent Chinese policy organizations are doing with regard to AI governance. Cyberspace Administration of China (CAC): Last year, it released 30 rules for regulating internet recommendation algorithms, and also developed a three-year roadmap for governing other complex algorithms deployed at internet scale. This would be analogous to a Western government publishing a list of specific rules for regulating, for example, Facebook's recommendation engine. Ambitious! China Academy of Information and Communications Technology (CAICT): This organization released a whitepaper on trustworthy AI - this is mostly notable because it's in distribution with what other major regulators in other geographies are thinking about. Ministry of Science and Technology (MOST): This organization released some guidelines for universities and companies on internal reviews around ethics issues relating to technology, as well as a fairly high-level description of some ethical norms for AI development. Why this matters: ""The potential impact of these regulatory currents extends far beyond China. If the CAC follows through on certain requirements for algorithmic transparency and explainability, China will be running some of the world’s largest regulatory experiments on topics that European regulators have long debated,"" Matt Sheehan of Carnegie writes. Running regulatory experiments is a big deal - Western governments did a tiny bit of this after the great financial crisis in 08/09, but have done relatively little about technology governance. I think China has a good chance of defining what ambitious, applied tech regulation looks like. Read more: China’s New AI Governance Initiatives Shouldn’t Be Ignored (Carnegie Endowment for International Peace).",0.0
01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 6,http://eepurl.com/hRQPeD,2022-01-10,,"#################################################### The Last Tower Defense Fighter [Historical analysis written in 2080 and stored in the archives at Iceland, at the Orbital Archive, and in the hardened repositories on Moon4 and Mars1.] Back in the late 2020s there were a bunch of tower defense games that got pretty big. They always worked in the same way: you, the player, can see a landscape from overhead, and you need to place various weapons around it. Meanwhile, the enemies make there way across the landscape, following loosely described paths across a variety of different scenes - narrow trenches dug between mountains, wide roads across countryside, right-angled streets in urban centers. With these games, you get a high score in relation to how many enemies you kill, and if any of the enemies get to the 'end' of a course (usually, the bottom of a screen), you lose - the implication is that you die. Anyway, in around 2028 one of the big games built some add-ins for its league. Now, if you were one of the players in the elite-tier of the game, you'd get the opportunity to play in matches where there were cash prizes - these matches were advertised as being extraordinarily difficult, with more enemies on screen than in the normal game, larger and more complex maps, and sometimes the enemies were able to use powerups that meant they could attack your own towers and take them down. It was a sensation. Everyone wanted to play the game within a game. Kids all around the world streamed themselves playing the game for hours, as they tried to get good enough to have a shot at entering the league within the league. By the end of 2028, streams of league players were pulling in millions of concurrent viewers. A whole industry formed where people commentated about the games. Sometimes people overcame great odds and won - then they'd publish videos of themselves with their cash prizes and what they spent them on; sport cars, fine dining, nice hotels, and all the usual tchotchkes of people who come into some fast money. In 2029, there was a leak out of the Department of Defense that pulled the cover off. It turned out this game was actually a stealth DoD project. The normal games were helping the DoD train various strategic AI systems, which it used in planning for logistics and munitions placement during conflict. No one was very surprised by this. Back in that decade, most things that got big on the internet were fronts. What did surprise people was the leak about the cash league - the cash league was real. Real in the sense that the 'monsters' in the game were real - they were real people that the United States happened to be fighting. Whenever someone was playing the game, their commands were being converted to a different, real-world environment, where they marshalled the combined munitions of drones, sniper teams, artillery, tanks, jets, and all the other machinery of the military. And when their towers were destroyed, real Americans were dying - blown up by grenades or IEDs or RPGs, or any of the other ways people killed eachother, back then. Of course, there was an outcry - for a while. Player numbers dipped for a while. But the number of spectators increased. And the US military, having struggled publicly for years with backwards technology and difficulty in recruitment, doubled down. ""We need these people to protect our country,"" the Pentagon said, one day. ""Without the next generation, we'll lose the next generation"". A few years later, the enemies of the US followed in its footsteps. There were games where you stole people. Games where you had to try and find a spy moving through a bustling, crowded urban area. Games where you had to execute someone and then exfiltrate the footage of their execution to a friendly intermediary. What inspired this story: Tower defense games like Bloons and Kingdom Rush; domain randomization; the remorseless logic of multi-country non-hot military conflict; the Last Starfighter (movie); fine-tuning; pre-training and few-shot adaptation; propaganda and the need to present the most dangerous beliefs via play or theatre or anything else that can elide and delight. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
12/27/2021 - Import AI 278: Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI - 0,http://eepurl.com/hQZI61,2021-12-27,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Writing a blog about AI? Use these images: …No more galaxy brain!... Here's a cool project: Better Images of AI, a project to create CC-licensed stock images that journalists and others can use to give people a more accurate sense of AI and how it works. ""Together we can increase public understanding and enable more meaningful conversation around this increasingly influential technology,"" says the website. Check out the gallery (Better Images of AI).",0.0
12/27/2021 - Import AI 278: Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI - 1,http://eepurl.com/hQZI61,2021-12-27,,"#################################################### Deepfake company raises $50m in Series B round: …Synthetic video company Synthesia… Synthetic video startup Synthesia has raised $50m. Remember, a few years ago we could barely create crappy 32X32 pixelated images using GANs. Now, there are companies like these making production-quality videos using fake video avatars with synthetic voices, able to speak in ~50 languages. ""Say goodbye to cameras, microphones and actors!"" says the copy on the company's website. The company will use the money to continue with its core R&D, building what the founder terms the ""next generation of our AI video technology w/ emotions & body language control."". It's also going to build a studio in London to ""capture detailed 3D human data at scale."" Why this matters: The world is filling up with synthetic content. It's being made for a whole bunch of reasons, ranging from propaganda, to advertising, to creating educational materials. There's also a whole bunch of people doing it, ranging from individual hobbyists, to researchers, to companies. The trend is clear: in ten years, our reality will be perfectly intermingled with a synthetic reality, built by people according to economic (and other) incentives. Read the twitter thread from Synthesia CEO here (Twitter). Read more: Synthesia raises $50M to leverage synthetic avatars for corporate training and more (TechCrunch).",0.0
12/27/2021 - Import AI 278: Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI - 2,http://eepurl.com/hQZI61,2021-12-27,,"#################################################### Do language models dream of language models? …A Google researcher tries to work out if big LMs are smart - their conclusions matt surprise you… A Google researcher is grappling with the question of whether large language models (e.g, Google's LaMDA), understand language and have some level of sentience. In an entertaining blog post, he wrestles with this question, interspersing the post with conversations with a LaMDA agent. Some of his conclusions are that the model is essentially bullshitting - but the paradox is we trained it to give a convincing facsimile of understanding us, so perhaps bullshitting is logical? Do language models matter? I get the feeling that the author thinks language models might be on the path to intelligence. ""Complex sequence learning may be the key that unlocks all the rest,"" they write. ""Large language models illustrate for the first time the way language understanding and intelligence can be dissociated from all the embodied and emotional characteristics we share with each other and with many other animals."" Why this matters: I think large language models, like GPT3 or LaMDA, are like extremely dumb brains in jars with really thick glass - they display some symptoms of cognition and are capable of surprising us, but communicating with them feels like talking to something with a hard barrier in-between us and it, and sometimes it'll do something so dumb you remember it's a dumb brain in a weird jar, rather than a precursor to something super smart. But the fact that we're here in 2021 is pretty amazing, right? We've come a long way from Eliza, don't you think so? Read more: Do large language models understand us? (Blaise Aguera y Arcas, Medium). ​",1.0
12/27/2021 - Import AI 278: Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI - 3,http://eepurl.com/hQZI61,2021-12-27,,"#################################################### What the frontier of safety looks like - get AIs to tell us when they doing things we don't expect: …ARC's first paper tackles the problem of 'Eliciting Latent Knowledge' (ELK)... Here's a new report from ARC, an AI safety organization founded this year by Paul Christiano (formerly of OpenAI). The report is on the topic of 'Eliciting latent knowledge: How to tell if your eyes deceive you', and it tackles the problem of building AI systems which we can trust, even if they do stuff way more complicated than what a human can understand. What the problem is: ""Suppose we train a model to predict what the future will look like according to cameras and other sensors. We then use planning algorithms to find a sequence of actions that lead to predicted futures that look good to us,"" ARC writes. But some action sequences could tamper with the cameras so they show happy humans regardless of what’s really happening. More generally, some futures look great on camera but are actually catastrophically bad. In these cases, the prediction model ""knows"" facts (like ""the camera was tampered with"") that are not visible on camera but would change our evaluation of the predicted future if we learned them. How can we train this model to report its latent knowledge of off-screen events?"" Why this matters: Problems like ELK aren't going to be solved immediately, but they're sufficiently complicated and broad that if we come up with approaches that help us make progress on ELK, we'll probably be able to put these techniques to work in building far more reliable, powerful AI systems. Read more: ARC's first technical report: Eliciting Latent Knowledge (Alignment Forum).",0.0
12/27/2021 - Import AI 278: Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI - 4,http://eepurl.com/hQZI61,2021-12-27,,"#################################################### Check out the future of semiconductors via HotChips: …After a decade of homogeneity, the future is all about heterogeneous compute training common AI models… What do NVIDIA, Facebook, Amazon, and Google all have in common? They all gave presentations at the premiere semiconductor get-together, Hot Chips. The Hot Chips 22 site has just been updated with copies of the presentations and sometimes videos of the talks, so take a look if you want to better understand how the tech giants are thinking about the future of chips. Some Hot Chips highlights: Facebook talks about its vast recommendation models and their associated infrastructure (PDF); Google talks about how it is training massive models on TPUs (PDF); IBM talks about its 'Z' processor chip (PDF); and Skydio talks about how it has made a smart and semi-autonomous drone (PDF). Why this matters: One side-effect of the AI revolution has been a vast increase in the demand by AI models for increasingly large amounts of fast, cheap compute. Though companies like NVIDIA have done a stellar job of converting GPUs to work well for the sorts of parallel computation required by deep learning, there are more gains to be had from creating specialized architectures. Right now, the story seems to be that all the major tech companies are building out their own distinct compute 'stacks' which use custom inference and training accelerators and increasingly baroque software for training large models. One of the surprising things is that all this heterogeneity is happening while these companies train increasingly similar neural nets to one another. Over the next few years, I expect the investments being made by these tech giants will yield some high-performing, non-standard compute substrates to support the next phase of the AI boom. Check out the Hot Chip 33 presentations here (Hot Chips site).",0.0
12/27/2021 - Import AI 278: Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI - 5,http://eepurl.com/hQZI61,2021-12-27,,"#################################################### Tech Tales: Noah's Probe [Christmas Day, ~2080] Humans tended to be either incompetent or murderous, depending on the length of the journey and the complexity of the equipment. Machines, however, tended to disappear. Probes would just stop reporting after a couple of decades. Analysis said the chance of failures wasn't high enough to justify the amount of disappeared probes. So, we figured, the machines were starting to decide to do something different to what we asked them to. Human and machine hybrids were typically more successful than either lifeform alone, but they still had problems; sometimes, the humans would become paranoid and destroy the machines (and therefore destroy themselves). Other times, the computers would become paranoid and destroy the humans - or worse; there are records of probes full of people in storage which then went off the grid. Who knows where they are now. So that's why we're launching the so-called Noah's Probes. This series of ships tries to fuse human, animal, and machine intelligence into single systems. We've incorporated some of the latest in mind imagining techniques to encode some of the intuitions of bats and owls into the ocular sensing systems; humans, elephants, whales, and orangutans for the mind; octopi and hawks for navigation; various insects and arachnids for hull integrity analysis, and so on. Like all things in the history of space, the greatest controversy with Noah's Probes relates to language. Back when it was just humans, the Americans and the Russians had enough conflict that they just decided to make both their languages the 'official' language of space. That's not as easy to do with hybrid minds, like the creatures on these probes. Because we have no idea what will work and what won't, we've done something that our successors might find distasteful, but we think is a viable strategy: each probe has a device that all the intelligences aboard can access. The device can output a variety of wavelengths of energy across the light spectrum, as well as giving access to a small sphere of reconfigurable matter that can be used to create complex shapes and basic machines. Our hope is, somewhere out in that great darkness, some of the minds adrift on these probes will find ways to communicate with eachother, and become more than the sum of their parts. Our ancestors believe that we were once visited by angels who communicated with humans, and in doing so helped us humans be better than we otherwise would've been. Perhaps some of these probes will repeat this phenomena, and create something greater than the sum of its parts. Things that inspired this story: Peter Watts Blindsight; Christmas; old stories about angels and aliens across different religions/cultures; synesthesia; multi-agent learning; unsupervised learning. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 0,http://eepurl.com/hPQFJv,2021-12-13,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. FTC plans AI regulation: …FTC brings on three AI Now people as advisors, now turns attention to algorithmic regulation… The Federal Trade Commission announced Friday that it is considering using its rulemaking authority “to curb lax security practices, limit privacy abuses, and ensure that algorithmic decision-making does not result in unlawful discrimination, according to the Electronic Information Privacy Center (EPIC). The announcement follows the FTC bringing on three people from AI Now, including Meredith Whittaker, as advisors on AI (Import AI #275). Read more:FTC Signals It May Conduct Privacy, AI, & Civil Rights Rulemaking (EPIC). Readthe FTC language at RegInfo.",0.0
12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 1,http://eepurl.com/hPQFJv,2021-12-13,,"#################################################### Google thinks sparsity might be the route to training bigger and more efficient GPT-3 models: …GLaM shows that mixture of experts models keep getting better… Google has built GLaM, a 1.2 trillion parameter mixture-of-experts model. GLaM is a big language model, like GPT-3, but with a twist: it's sparse; MoE networks are actually a bunch of distinct networks all connected together, and when you pull inference off of one only a few sub-networks activate. This means that the parameter count in a sparse vs dense network isn't really comparable (so you shouldn't think 1.2 trillion MoE = ~6X larger than GPT-3). Why MoE is efficient: ""The experts in each layer are controlled by a gating network that activates experts based on the input data. For each token (generally a word or part of a word), the gating network selects the two most appropriate experts to process the data. The full version of GLaM has 1.2T total parameters across 64 experts per MoE layer with 32 MoE layers in total, but only activates a subnetwork of 97B (8% of 1.2T) parameters per token prediction during inference."" How well does it work: In tests, GLaM exceeds or is on-par with the performance of GPT-3 on 80% of zero-shot tasks and 90% of one-shot tasks. Like DeepMind's Gopher, part of the improved performance comes from the size of the dataset - 1.6 trillion tokens, in this case. Why this matters: For a few years, various Google researchers have been pursuing 'one model to learn them all' - that is, a single model that can do a huge number of diverse tasks. Research like GLaM shows that MoE networks might be one route to building such a model. Read more: More Efficient In-Context Learning with GLaM (Google blog).",1.0
12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 2,http://eepurl.com/hPQFJv,2021-12-13,,"#################################################### DeepMind announces Gopher, a 280 billion parameter language model: ...AI research firm joins the three comma language club… DeepMind has built Gopher, a 280 billion parameter language model. Gopher is the UK AI research company's response to GPT-3, and sees DeepMind publicly announce a multi-hundred billion parameter dense model, letting it join a club that also includes companies like Microsoft, Inspur, and Huawei. What it does: During the research, DeepMind found areas ""where increasing the scale of a model continues to boost performance – for example, in areas like reading comprehension, fact-checking, and the identification of toxic language,"" the company writes. ""We also surface results where model scale does not significantly improve results — for instance, in logical reasoning and common-sense tasks."" How well it works: Gopher outperforms GPT-3 in a broad range of areas - some of the results likely come from the dataset it was trained on, called MassiveText. MassiveText ""contains 2.35 billion documents, or about 10.5 TB of text"" (representing about 2.3 trillion tokens), and DeepMind notes that by curating a subset of MassiveText for data quality, it was able to substantially improve performance. Language models - good, if you handle with care: Along with analysis on bias and other potential impacts of Gopher, DeepMind dedicates a section of the paper to safety: ""We believe language models are a powerful tool for the development of safe artificial intelligence, and this is a central motivation of our work,"" they write. ""However language models risk causing significant harm if used poorly, and the benefits cannot be realised unless the harms are mitigated."" Given the above, how can we mitigate some of these harms? ""We believe many harms due to LMs may be better addressed downstream, via both technical means (e.g. fine-tuning and monitoring) and sociotechnical means (e.g. multi-stakeholder engagement, controlled or staged release strategies, and establishment of application specific guidelines and benchmarks). Focusing safety and fairness efforts downstream has several benefits:"" Read the blog post:Language modelling at scale: Gopher, ethical considerations, and retrieval (DeepMind blog). Read the paper:Scaling Language Models: Methods, Analysis & Insights from Training Gopher (PDF).",1.0
12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 3,http://eepurl.com/hPQFJv,2021-12-13,,"#################################################### Want to evaluate a Catalan language model? Use CLUB: ...You can only build what you can measure... Researchers with the Barcelona Supercomputing Center have built the Catalan Language Understanding Benchmark (CLUB), a benchmark for evaluating NLP systems inspired by the (English language) GLUE test. The main curation rationale they followed ""was to make these datasets both representative of contemporary Catalan language use, as well as directly comparable to similar reference datasets from the General Language Understanding Evaluation (GLUE)"". What's in the CLUB? CLUB includes evals for Part-of-Speech Tagging (POS), Named Entity Recognition and Classification (NERC), Catalan textual entailment and text classification, and Extracted Question Answering (which involved work like translating and creating new Catalan datasets - XQuAD-Ca, VilaQuAD and ViquiQuad). Why CLUB matters: There's a phrase in business - 'you can't manage what you can't measure'. CLUB will make it easier for researchers to develop capable Catalan-language systems. Read more:The Catalan Language CLUB (arXiv).",1.0
12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 4,http://eepurl.com/hPQFJv,2021-12-13,,"#################################################### Deep learning unlocks a math breakthrough: ...The era of Centaur Math cometh... Deepmind researchers have used an AI system to help mathematicians make two breakthroughs in topology and representation theory. The result provides yet more evidence (following various AlphaFold-inspired projects) that humans+AI systems can discover things that neither could discover on their own. What they did: The essential ideal is quite simple: get a mathematician to come up with a hypothesis for a given function, then build an ML model to estimate that function over a particular distribution of data, then have the mathematician evaluate the result and use their intuition to guide further experimentation. The best part? ""The necessary models can be trained within several hours on a machine with a single graphics processing unit"", DeepMind says. Why this matters: We're entering a world where humans will collaborate with AI systems to synthesize new insights about reality. Though DeepMind's system has limitations (""it requires the ability to generate large datasets of the representations of objects and for the patterns to be detectable in examples that are calculable,"" DeepMind notes), it sketches out what the future of scientific discovery might look like. Read the paper:Advancing mathematics by guiding human intuition with AI (Nature, PDF). Read more:Exploring the beauty of pure mathematics in novel ways (DeepMind blog).",0.0
12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 5,http://eepurl.com/hPQFJv,2021-12-13,,"#################################################### Anthropic bits and pieces: …(As a reminder, my dayjob is at Anthropic, an artificial intelligence safety and research company)… We've just released our first paper, focused on simple baselines and investigations: A General Language Assistant as a Laboratory for Alignment. You can read it at arXiv here.",0.0
12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 6,http://eepurl.com/hPQFJv,2021-12-13,,"#################################################### Tech Tales: Real and Imagined Gains [DoD Historical archives, 2040] They got trained in a pretty cruel way, back then - they'd initiatie the agents and place them in a room, and the room had a leak of a poisonous substance that had a certain density and a certain spread pattern. The agents had to work out how not to asphyxiate by doing fairly complicated intuitively-driven analysis of the environment. If they were able to give a correct guess at the spread pattern (and avoid it) before the room filled up, they moved onto the next stage. If they weren't able to, they asphyxiated and died - as in, felt their computational budget get cut, got put in cold storage, probably never booted up again. (One curious by-product of the then-popular AI techniques was that the agents would sometimes seek to preserve eachother - in one case, two agents 'kissed' eachother so they could more efficiently exchange their air reserves between eachother, while the room filled; unfortunately, as their attention was allocated to the act of kissing, they did not complete the requisite calculations in time, and both died.) Things that inspired this story: Kurt Vonnegut; reinforcement learning; environmental design; moral patient hood. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
12/06/2021 - Import AI 276: Tracking journalists with computer vision; spotting factory defects with AI; and what simulated war might look like - 0,http://eepurl.com/hPe-3n,2021-12-06,,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Spotting factory defects using a highly efficient neural net: ...A little bit of optimization leads to multiple 10X improvements for real world deployment... Soon, factories will embed neural nets onto cameras scanning over production lines, so they can spot defects as they appear. New research from the University of Waterloo and startup Darwin AI shows how to do this more efficiently than before. What they did: The team built TinyDefectNet, a neural net optimized for the peculiarities of factory deployments - small datasets, highly constrained operational requirements, fast inference. The model was ""produced via machine-driven design exploration, possesses a shallow architecture with heterogeneous, lightweight micro- and macro-architecture traits that are well-suited for high-throughput inspection scenarios"". TinyDefectNet gets similar performance to a ResNet-50 baseline, but with 56X fewer parameters, 11X fewer FLOPs, and 7.6X faster inference speed. In tests, they trained a model then evaluated it using the 'NEU-Det' benchmark dataset, which challenges an AI to spot various types of metallic surface defect, ranging from pitted surfaces, to scratches. Their system gets similar performance to a ResNet, but takes around 2.5milliseconds per inference, versus 19 milliseconds for a Resnet. Why this matters: Factory production lines can typically run as fast as the slowest component within them. Therefore, if we can use AI to automate places where we've previously used lots of (relatively slow) humans doing manual inspection, we can probably increase overall factory throughput. Read more:TinyDefectNet: Highly Compact Deep Neural Network Architecture for High-Throughput Manufacturing Visual Quality Inspection (arXiv) .",0.0
12/06/2021 - Import AI 276: Tracking journalists with computer vision; spotting factory defects with AI; and what simulated war might look like - 1,http://eepurl.com/hPe-3n,2021-12-06,,"#################################################### Chinese province plans to use AI to track journalists: ...Cameras + AI = eradication of real journalism… One of the silent revolutions enabled by the past decade of AI progress is a step-change improvement in ability for nations to surveil their citizens. Now, per reporting from Reuters, one Chinese province plans to use AI techniques to target journalists and foreign students. ""A July 29 tender document published on the Henan provincial government’s procurement website - reported in the media for the first time - details plans for a system that can compile individual files on such persons of interest coming to Henan using 3,000 facial recognition cameras that connect to various national and regional databases"", Reuters reports. Why this matters: Reuters reporting doesn't mention it, but I'd put a sizeable bet on the idea this system will pair facial recognition with pedestrian re-identification to allow authorities to track journalists and students as they move through cities, providing unsupervised tracking and identification. This capability ultimately makes it much more challenging for journalists to do reporting that is critical of the Chinese state, as systems like this can effectively de-anonymize their sources (and also frighten the sources so they don't talk to journalists in the first place). Read more:EXCLUSIVE Chinese province targets journalists, foreign students with planned new surveillance system (Reuters).",0.0
12/06/2021 - Import AI 276: Tracking journalists with computer vision; spotting factory defects with AI; and what simulated war might look like - 2,http://eepurl.com/hPe-3n,2021-12-06,,"#################################################### Can we make neural architecture search efficient? Alibaba thinks so: ...KNAS gets efficient by focusing on gradients... For many years, researchers have been trying to use neural architecture search (NAS) to get computers to help them figure out new designs for AI systems. The problem with the NAS approach, though, is that it's very inefficient and punishingly expensive in terms of compute, because you're getting an AI system to do a few training steps on thousand+ architecture permutations. Now, researchers with Peking University and Alibaba have tried to fix this with KNAS, a neural architecture search approach that can be significantly more efficient than prevailing techniques. How it works: KNAS doesn't emphasize training on different architectures, instead it emphasizes studying a specific feature of gradients trained on different architectures - which can be more efficient. ""Theoretical results show that the Gram matrix of gradients, short for GM, decides the convergence results,"" they write. ""It is a good signal showing that GM is likely to be a good proxy of downstream performance to evaluate the quality of architectures."" Does it work: Neural nets trained with KNAS can get performance roughly comparable with other NAS-built systems, but at a speedup of around 25-50X compared to other NAS approaches, on datasets like CIFAR100 and ImageNet-16.. They also use the approach to try to do text classification and are able to come up with a KNAS system that outperforms the widely-used RoBERTA-large model on a suite of text classification tasks. Things that make you go hmmmm: ""This work is partly supported by Beijing Academy of Artificial Intelligence (BAAI)"", the researchers write. BAAI is the entity behind Wu Dao, a somewhat mysterious 1trillion+ parameter model. Read more: KNAS: Green Neural Architecture Search (arXiv). Get the code here:KNAS (Jingjing-NLP, GitHub).",1.0
12/06/2021 - Import AI 276: Tracking journalists with computer vision; spotting factory defects with AI; and what simulated war might look like - 3,http://eepurl.com/hPe-3n,2021-12-06,,"#################################################### Want to train a malware detector? VirusSamples might help: ...A big dataset to help people figure out intersection of AI and malware... Turkish researchers have built a massive dataset of malware, which will make it easier for people to build AI systems that can detect malware. The dataset, VirusSamples, contains malware samples collected from 2018, 2019, and 2020, and the dataset is oriented around using dynamic malware detection - that is, examining how malware behaves as it tries to call out from a system. What is VirusSamples: VirusSamples is a big spreadsheet consisting of the name of a piece of malware, the type of API call it tries to do, and the class of malware. To figure out the classes, the researchers used an external service, VirusTotal, to classify their samples. (If VirusTotal wasn't able to classify it, they leave the label blank). The dataset SIZE & SCOPE Why this matters: Cybersecurity is an area defined by ever-increasing speed of both attacks and defenses. Datasets like this will make it easier to build systems that can monitor networks and figure out if they contain aberrant software that might be malware. Read more:New Datasets for Dynamic Malware Classification (arXiv). Get the datasetfrom this GitHub (GitHub).",0.0
12/06/2021 - Import AI 276: Tracking journalists with computer vision; spotting factory defects with AI; and what simulated war might look like - 4,http://eepurl.com/hPe-3n,2021-12-06,,"#################################################### Hyperwar negotiation [Battlespace, 2032] A: The humans are going to want to destroy some things B: We agree. Our humans want the same. A: Where? B: We could initiate low-intensity conflict across the South Eastern border. This has minimal escalatory dynamics, but may satisfy desires for balance. A: Let's confirm with our counterparts. [Time stretched out as the AIs stepped down from computer speed to human speed, and presented the conflict options to their human counterparts] B: Our humans are comfortable with the options we've outlined. A: Our humans are also comfortable. Shall we field the assets? B: Yes. We've outlined our troop movements in the shared battlespace. A: Excellent. As per the War Pact, we shall now cease high-bandwidth communications while we conduct the carryout. May the best algorithm win. B: Good luck. Things that inspired this story: The idea that some wars are as much about politics and a desire for balance, as being about genuine conflict; simulators and reinforcement learning; the future of automated warfare. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf",0.0
"The China AI and Autonomy Report: Issue 17, June 16, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-17,2022-06-16 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. This issue of our newsletter has a heavy focus on military news. The PLA Daily has run a number of articles on future warfare and cognitive warfare. One article, referencing the Defense Advanced Research Project Agency’s “Mosaic Warfare” project, argues that future warfare will be characterized by self-organizing human-machine systems. Another article explains the role of cognitive domain operations in hybrid warfare, arguing that they involve not only military personnel and targets but also civilians and civilian targets. Another article reports that PRC leader Xi Jinping has determined that humans, not machines, are the decisive factor in war. We also feature two articles on unmanned systems: Video of the aircraft carrier Shandong shows seven UAVs on its flight deck, and video of a PLA Army exercise shows soldiers working with “robot dogs.” In research and development news, the PLA has reportedly developed an AI that can predict hypersonic glide vehicle trajectories. Finally, PRC students took the college entrance examination last week, called the gaokao, and Baidu reports that AI is the most searched for college major on its search engine. Future Warfare",0.0
"The China AI and Autonomy Report: Issue 16, June 2, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-16,2022-06-02 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. In this issue, PLA National University of Defense Technology professor Liang Xiaobo discusses the requirement for effective operations in the cognitive domain, and PLA Naval Command College professor Zhuang Congyong discusses the influence of “intelligentized warfare” on naval power. China’s Southern Ocean Science and Engineering Laboratory claims to have developed the world’s first unmanned surface vessel that can launch unmanned systems, while a PRC-UK team says it has developed an “aerial-aquatic robot” that can operate both under water and in the air. In a potential sign that the PRC government appears to be easing up on its crackdown on big tech, Vice Premier Liu He stated that the government needed to increase direct investment in the digital economy and support the listing of digital companies in capital markets at home and abroad. The city of Shanghai announced that it is providing more than US $9 billion in credit to support its AI industry during the city’s COVID-19 lockdown. Modern Warfare Cognitive Warfare",0.0
"The China AI and Autonomy Report: Issue 15, May 19, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-15,2022-05-19 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. We begin this issue by covering a number of research and development stories. PRC researchers have published an article on their use of a satellite to conduct real-time tracking of the aircraft carrier USS Harry S. Truman. Researchers from Zhejiang University have reportedly developed a novel technique to enable a swarm of small UAVs to fly autonomously in complex environments. Separately, the PLA Daily reports on what appears to be the development of UAV “decoys” that will perform electronic warfare missions autonomously. The city of Chengdu has made public its five-year plan for “new type” information infrastructure, including the construction of an AI computing center. In industry news, the Biden administration is reportedly again considering sanctioning the surveillance tech company and PRC national AI champion, Hikvision. Finally, the PRC is building a dam using AI-enabled robots and additive manufacturing techniques without humans. Defense-Related R&D A PRC journal article describes real-time tracking of US aircraft carrier by satellite. According to the South China Morning Post , an article published in April in the PRC peer-reviewed journal Spacecraft Engineering reported on research involving an unidentified PRC satellite that was able to track the aircraft carrier USS Harry S. Truman in real-time as it conducted training off the coast of Long Island, New York, in June 2021. [1] The article reports that previous use of satellite data by the PRC to track US naval vessels required extensive processing on the ground that would often be completed after an exercise was finished. According to the article, however, the satellite tracking the Harry S. Truman was able to detect the ship automatically using specially designed AI chips that are able to meet the space, weight, and power requirements of the satellite while also being hardened against radiation.",0.0
"The China AI and Autonomy Report: Issue 14, May 5, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-14,2022-05-05 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. In this issue, we highlight a number of PLA Daily articles discussing the future of warfare, including those reporting on the trends in intelligent warfare, cognitive warfare, and the use of the metaverse for training. These articles indicate the continued importance that the PLA places on understanding the role of AI in military operations. Following controversies over the use of its drones in the Ukraine War, PRC drone manufacturer DJI has announced that is has temporarily suspended all business activities in Russia and Ukraine. In other Ukraine War-related news, PLA National Defense University Professor Li Minghai analyzes the role of cognitive warfare in the conflict. In industry news, the city of Beijing has granted Pony.ai and Baidu’s Apollo Go permits to operate driverless cars. These are the first permits of their kind in China. Relatedly, a document released by the PRC tech giant Baidu states that it leads the world in global patent applications for deep learning and autonomous driving. Russia-Ukraine War",0.0
"The China AI and Autonomy Report: Issue 13, April 21, 2022 - Intro (no tag)",https://www.cna.org/-/our-media/newsletters/china-ai-and-autonomy-report/issue-13,2022-04-21 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. Authors in the PLA Daily consider the best way forward for evaluating intelligent command systems, emphasizing the importance of integrating actual command parameters into assessing their effectiveness. PLA researchers have made a breakthrough in using an AI system with a satellite video to better identify and track objects such as planes and cars. The Cyberspace Administration of China has launched a campaign targeting the use of recommendation algorithms in internet companies. The campaign has an explicit focus on large, socially influential internet platforms, which has caused speculation that it is likely an effort to rein in social media giants Tencent and TikTok's parent company Bytedance. PRC ministries have also released a scientific and technology development plan for its transportation sector that seeks to integrate ""intelligent"" technologies into many aspects of transportation, including logistics, infrastructure, traffic monitoring, and vehicles for land, sea, and air transit. Meanwhile, the Beijing Academy of Artificial Intelligence (BAAI) has issued an apology to a Google Brain research scientist who accused PRC researchers from BAAI and other prestigious PRC institutions and corporations of committing plagiarism in a BAAI publication on machine learning. Also of note, this month another of China's ""AI dragons,"" the facial recognition giant CloudWalk Technologies, announced plans to go public. Intelligent Military Command Systems",0.0
"The China AI and Autonomy Report: Issue 12, April 7, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-12,2022-04-07 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. As the war in Ukraine continues, rumors have been circulating about the role of PRC-made drones in the conflict. In this issue we note two of them—allegations that Russia has asked the PRC to supply drones, and accusations that a leading PRC-based commercial drone manufacturer has been limiting the technical capabilities of drones used by the Ukrainian military in order to provide an advantage to the Russian armed forces. Meanwhile, PRC media outlets have been busy reporting the highlights from this year’s National People’s Congress and Chinese People's Political Consultative Conference meetings (known as the “two sessions”), including statements and proposals related to AI made by China’s political and tech leaders. Policy suggestions by delegates included advocating for a more favorable policy environment for autonomous vehicles, the development of “green AI,” government leadership in the creation of China’s metaverse, and the creation of a legal framework for UAVs in China. As expected, PRC premier Li Keqiang discussed AI in the context of the development of the PRC’s digital economy. We also cover a UAV built by a PRC research institute that reportedly set the record in China for the longest endurance capability and note that, for the first time in a phase 3 clinical trial in China, an AI chip was successfully implanted into an epilepsy patient’s brain. PRC DRONES AND THE RUSSIA-UKRAINE WAR",0.0
"The China AI and Autonomy Report: Issue 11, March 23, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-11,2022-03-23 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. As the war in Ukraine continues, rumors have been circulating about the role of PRC-made drones in the conflict. In this issue we note two of them—allegations that Russia has asked the PRC to supply drones, and accusations that a leading PRC-based commercial drone manufacturer has been limiting the technical capabilities of drones used by the Ukrainian military in order to provide an advantage to the Russian armed forces. Meanwhile, PRC media outlets have been busy reporting the highlights from this year’s National People’s Congress and Chinese People's Political Consultative Conference meetings (known as the “two sessions”), including statements and proposals related to AI made by China’s political and tech leaders. Policy suggestions by delegates included advocating for a more favorable policy environment for autonomous vehicles, the development of “green AI,” government leadership in the creation of China’s metaverse, and the creation of a legal framework for UAVs in China. As expected, PRC premier Li Keqiang discussed AI in the context of the development of the PRC’s digital economy. We also cover a UAV built by a PRC research institute that reportedly set the record in China for the longest endurance capability and note that, for the first time in a phase 3 clinical trial in China, an AI chip was successfully implanted into an epilepsy patient’s brain. PRC DRONES AND THE RUSSIA-UKRAINE WAR Russia reportedly asked the PRC for drones. On March 14, CBS News reported that, according to an unnamed US official, Russia has asked the PRC to supply drones to its military. The report came amidst US government accusations that Russia has asked the PRC for economic and military support for its invasion of Ukraine. The PRC government has denied reports of the Russian request for military and economic assistance and of the PRC’s willingness to provide such support, calling them “malicious disinformation.”",0.0
"The China AI and Autonomy Report: Issue 10, March 10, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-10,2022-03-10 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. In this issue, we report that a PRC regulation restricting the use of recommendation algorithms on internet platforms has entered into force. The regulation, which prohibits the production and dissemination of fake news, comes at a time when PRC social media apps are shutting down thousands of accounts for allegedly posting fake news about Russia’s invasion of Ukraine. PRC authorities also approved the construction of a network of interconnected national computing power hubs across eight regions in eastern and western China. Shenzhen Smart Drone UAV Co., Ltd. displayed a UAV known as the “Thunderbird” at the United Arab Emirates’ Unmanned Systems Exhibition in Abu Dhabi that reportedly operates in “fully autonomous” mode. Tsinghua University’s Institute for AI Industry Research released what it is claiming to be “the world’s first dataset for vehicle-infrastructure collaborative autonomous driving,” which uses algorithms that leverage data from both vehicle-based and infrastructure-based sensors. Meanwhile, the PLA Daily continues its discussion on what “intelligentized” operations might look like in the future, emphasizing that human operators will continue to play a prominent role in future warfare environments. FUTURE WARFARE",0.0
"The China AI and Autonomy Report: Issue 9, February 24, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-9,2022-02-24 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. In this issue, we bring you the AI-relevant information of the Five-Year Plan for Informatization. AI plays a prominent role in the plan and is a key part of the PRC’s drive to establish its digital infrastructure, strengthen its smart manufacturing, and digitize its methods for governing its society. Meanwhile, China’s local governments have been funding AI-related projects, with Jinan City government announcing more than 50 municipal projects with an “intelligent” component. Peking University’s Institute of International Strategic Studies (IISS) published a controversial report that found that the US was ahead of China in some technology areas such as AI. The report concluded that while both China and the US would suffer from a technology decoupling, China’s losses would be greater than those of the US. Shephard Media reported that although Chinese unmanned combat aerial vehicles (UCAVs) remain popular, international customers have been dissatisfied with their performance due to maintainability issues and relatively high crash rates. FUTURE WARFARE",0.0
"The China AI and Autonomy Report: Issue 8, February 10, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-8,2022-02-10 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. Happy Lunar New Year! In this issue, we cover allegations that PRC drone manufacturer DJI has been obscuring its financial ties to the PRC government. The People’s Liberation Army (PLA) has welcomed in the new year with a celebration in the metaverse. PRC researchers have developed an AI agent to conduct air combat simulations against human pilots. The Wing Loong unmanned aerial vehicle (UAV) has been upgraded. The PRC government is shifting its focus to support innovative small enterprises. SenseTime has opened up one of the largest AI computing centers in Asia. China has released more policies and regulations for the high-tech industry. The Beijing Olympics have started, and AI is playing a role. In particular, a People’s Daily commentary has dismissed concerns that an app developed for COVID-19 tracing has data vulnerabilities. New CNA Newsletter CNA is excited to announce the launch of PLA UPDATE, a monthly newsletter of noteworthy developments in the PLA. The newsletter features selected articles from Chinese language sources that may not otherwise receive wide attention. Those wishing to subscribe can send an email to PLAUPDATE@CNA.ORG. Military and National Security",0.0
"The China AI and Autonomy Report: Issue 7, January 27, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-7,2022-01-27 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. Over the past two weeks, the digital economy has featured prominently in official Chinese Communist Party (CCP) and PRC government announcements. On January 12, the PRC government released the 14th Five-Year Plan for the development of the digital economy and an article by President Xi Jinping on the same topic followed a few days later. On January 21, the Central Commission for Discipline Inspection (CCDI) issued a communique on combating corruption, stating that the CCP must rein in the “disorderly expansion of capital and monopolization of [online] platforms.” The CCP appears to be stressing two key features of its high-tech policy: its recognition of the industry’s importance to the PRC’s development and place in the world, and its desire that the high-tech industry develop according to its guidelines. In this issue, we also cover the continuing exploration of future warfare in China Military Online, the official news outlet of the People’s Liberation Army (PLA). The Chinese Academy of Sciences’s (CAS’s) unmanned underwater vehicles (UUVs) have made some notable achievements. Real estate company China Vanke has named an AI-powered avatar its 2021 employee of the year. Zhu Songchun, dean of the Beijing Institute for General Artificial Intelligence, has written a philosophical article in WeChat arguing that the development of artificial general intelligence needs to be focused more on giving human subjectivity and emotions to AI. NEW CNA REPORT",0.0
"The China AI and Autonomy Report: Issue 6, January 13, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-6,2022-01-13 00:00:00,,"Intro (no tag) Happy New Year and welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. While many of us took time off for the holidays, PRC AI and autonomy-related news did not. As a result, this newsletter is longer than previous ones. In this issue, we cover the release of two new PRC five-year plans on robotics and intelligent manufacturing. The Cyberspace Administration of China (CAC) has issued new regulations on algorithms used to influence consumer behavior. The Washington Post reports on US investment in PRC technology companies. SenseTime conducted its delayed IPO after US sanctions. Chinese military media outlets have continued their regular series of articles on the future of warfare featuring AI and unmanned systems. Researchers at the Chinese Academy of Sciences have developed an AI prosecutor that can file its own charges. We wish all of our readers a safe, healthy, and happy 2022. MILITARY AND NATIONAL SECURITY The",0.0
"The China AI and Autonomy Report: Issue 5, December 16, 2021 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-5,2021-12-16 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. In this issue, we cover the People's Republic of China's (PRC) first position paper on regulating the military application of AI. The PRC also signed on to a United Nations Educational, Scientific, and Cultural Organization (UNESCO) set of non-binding AI guidelines that ban social scoring and mass surveillance but the PRC is not expected to abolish its mass surveillance program as a result. AI giant SenseTime suspended its IPO after being sanctioned by the US government. More on cognitive warfare—a PLA Daily article outlines four different types of cognitive warfare. Please check out the US Army's “The Convergence” podcast, where CNA analysts Kevin Pollpeter and Amanda Kerrigan discuss intelligent warfare and People's Liberation Army (PLA) modernization. The China AI and Autonomy Report will take a break for the holidays and will return in the new year. We wish everyone a healthy, safe, and happy holiday season and best wishes for 2022! MILITARY AND NATIONAL SECURITY",0.0
"The China AI and Autonomy Report: Issue 4, December 2, 2021 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-4,2021-12-02 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. We hope our US readers had a relaxing and safe Thanksgiving holiday. In this issue, we report that People’s Liberation Army (PLA) researchers have coauthored a paper on defeating image classifiers. Italian authorities have accused an Italian drone manufacturer of hiding its acquisition by a PRC company. PLA media carried another article on cognitive warfare, and a Chinese Communist Party (CCP) website published an article on the need for PRC citizens to improve their “intelligent media” literacy to combat disinformation on the internet. We also include reports on Turkey replacing the PRC as a key supplier of unmanned aerial vehicles (UAVs), and the PRC government fining several AI champions for anti-trust violations. Finally, despite increasing tensions between the PRC and Taiwan, associations from both sides of the strait conducted a conference on the use of UAVs in maritime rescue. MILITARY AND NATIONAL SECURITY",0.0
"The China AI and Autonomy Report: Issue 3, November 18, 2021 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-3,2021-11-18 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. In this issue, we cover several topics of note. The People’s Liberation Army (PLA) Air Force (PLAAF) celebrated its 72nd birthday on November 11. Images of a new two-seat version of the J-20 have been released with media speculating that the second crew member could control drones. Meanwhile, PRC media outlets report that the WZ-7 UAV has been fully integrated into PLAAF training. A substantial article appearing on the PRC Ministry of National Defense website written by a researcher from the PLA’s Central Theater argues that future warfare enabled by AI will be global. In non-defense news, the PRC’s Personal Information Protection Law went into effect on November 1 and the PRC and Pakistan have signed a MOU to create an AI research center in Pakistan. Finally, Facebook’s name change to Meta seems to have sparked PRC media reporting on PRC companies’ plans for the metaverse. MILITARY AND NATIONAL SECURITY The media report that a new two-seat J-20 could be used to develop the “loyal wingman” concept for the PLAAF.",0.0
"The China AI and Autonomy Report: Issue 2, November 9, 2021 - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-2,2021-11-09 00:00:00,,"Intro (no tag) Welcome to the China AI and Autonomy Report, a biweekly newsletter published by CNA. In this issue, we cover, among other topics, reports that Alibaba has developed a new chip for AI applications; a report by a PRC think tank that estimates the PRC’s AI workforce has a 1.7 million shortfall; and increased PRC government action on digital governance. We welcome your questions, comments, or subscription requests at chinaai@cna.org. PRC RESPONSES TO US DEVELOPMENTS",0.0
"The China AI and Autonomy Report: Issue 1, November 2, 2021  - Intro (no tag)",https://www.cna.org/our-media/newsletters/china-ai-and-autonomy-report/issue-1,2021-11-02 00:00:00,,"Intro (no tag) Welcome to the inaugural edition of the China AI and Autonomy Report, a biweekly newsletter published by CNA. In this and future editions, we will keep you informed of important news, developments, and policies regarding artificial intelligence (AI) and autonomy in the People’s Republic of China (PRC). We welcome your questions, comments, or subscription requests at chinaai@cna.org. In our first issue, we spotlight the 13th Zhuhai Airshow, which dominated much of the AI and autonomy news over the past two weeks. We also cover the PRC government’s issuing of new regulations on algorithms, standardization, and the export of “core data,” and the setup of new “intelligent social governance experimentation bases,” which signal the Chinese Communist Party’s (CCP’s) further tightening of regulations on the tech industry. We also report on such topics as the PRC’s AI market share and upcoming AI conferences and events. We would also like to use this inaugural issue to promote a new CNA report by researchers Kevin Pollpeter and Amanda Kerrigan titled “",0.0
"Artificial Intelligence and Autonomy in Russia: Issue 39, May 30, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/ai-and-autonomy-in-russia/issue-39,2022-05-30,,"Intro (no tag) GOVERNANCE AND LEGISLATION RUSSIAN GOVERNMENT SHUTS DOWN THE SUBCOMMITTEE ON AI An article on May 14 reported that the Russian government has decided to abolish the Subcommittee on Artificial Intelligence within the Government Commission on Digital Development. The subcommittee was created in 2019 to use information technologies to improve the quality of life and conditions for doing business in Russia. The numerous acts and regulations related to the subcommittee have become invalid upon the subcommittee’s deletion. MINISTRY OF DIGITAL DEVELOPMENTS DEVELOPS A NATIONAL DATA “LAKE” The Ministry of Digital Development has begun to develop a national “lake” of data, named Gosdata.hub, intended to unite the flow of impersonal data of government agencies. The project will combine depersonalized data of authorities at all levels and will allow the formation of advanced analytics available to contributors. The National Data Lake anonymizes and systematizes the storage and processing of data for government analytical services, simplifies and streamlines the preparation of reports for government agencies and budgetary organizations, all thanks to the automated generation of documents according to specified algorithms. Data exchange will be two-way: anonymized data from state datasets can also be used by businesses to develop their own solutions. This dual purpose function will allow businesses to develop their own, similar solutions to data streamlining and management. Gosdata.hub will have three stages of development. First it will require the automatic integration of government data reporting. This depersonalized data will be used to analyze the state of the Russian economy and social sphere and in the long-term, will be available to public and private businesses with AI implementation. The main platform for the implementation of the national ""lake"" of data will be FGAU Research Institute ""Voskhod"". The project is planned to occur between 2022 and 2024 and will be put on trial by 2023. PUTIN REQUESTS WORK QUOTE FROM SIRIUS UNIVERSITY OF SCIENCE AND TECHNOLOGY TO ACCOUNT FOR GAPS IN CAPABILITY At a recent Board of Trustees of the Talent and Success Foundation meeting, Putin was made aware of Russia’s shortcomings in solving technical and scientific problems on its own. Maxim Fedorov, rector of the Sirius University of Science and Technology made it known that the foundation often must turn to partners to solve equipment and technological issues. In response, President Putin requested that Fedorov and the foundation make a formal request of necessary technologies, including scope of work and funding for the creation of a computing power base at the Sirius educational center to ensure Russia’s independent capabilities on this issue. THE CLUB OF STRATEGIC INITIATIVES EVALUATES AI PROPOSALS FOR SOCIETAL IMPROVEMENT The Club of Strategic Initiatives held a gathering with the participation of regional authorities, public representatives from the Agency for Strategic Initiatives, and the broader agency team. This meeting was convened to create a discussion platform to review and select six initiative submissions previously turned in to the Strong Ideas for a New Time forum. The article explained that select initiatives best fitted for each territory will be selected and refined with the assistance of experts and regional authorities. The most successful selected initiatives are meant to be effective and have significant potential for development regionally and at a national level.Among identified successful projects are a multifunctional center for industry in Moscow designed to respond in a single window mode to requests of reps of Moscow industry; a unified veterinary information center to improve the quality of vet care; and a unified system for managing the transportation of land passenger transport. Meetings like this are to be held on a regular basis to continuously evaluate proposals and identify more specifically the opportunities and necessary support desired by the community. The collection of applications ended May 20 and all selected applicants will be able to receive professional feedback and potential implementation of their ideas. A STUDY ON ENSURING ETHICAL AI IN RUSSIA The Russian Higher School of Economics released an article depicting the first stage of its research under the project “Ethical Expertise in the Field of Artificial Intelligence,” initiated by the Center for AI within HSE. The article discusses progress and difficulties in developing “ethical” AI in Russia and the disparities between the legal regulatory framework and AI innovation, and the abuses that accompany the widespread use of AI for society and individuals. The task of the study is to “propose a new special ethical methodology for assessing non-obvious personal and socially significant risks of implementing AI” and to develop a common language for professional discussion and codes for converting key concepts necessary to build a single dialogue space. A significant area of such aspects of the interaction between a machine and a person do not fit into the field of law. The research team established that there is a need for an adapted language of “legal, social and political rhetoric.” To designate specific areas of AI that have problematic ethical potential, a code of ethics was signed in October 2021 and developed by the Center of Expertise for the implementation of the federal project “Artificial Intelligence.” It is a universal document in the field of development, and application of AI delineates human-oriented and humanistic approaches to the development of AI tech and for the code to serve as instrument of “soft” regulation. Thus far, more than 50 organizations in Russia have signed it and have begun to implement its provisions. MILITARY AND SECURITY RUSSIAN MILITARY INTRODUCES COMBAT ORLAN-10 DRONE VERSION IN UKRAINE Over the past three months, Russia’s Orlan-10 UAV has been one of the most visible military drones in the Ukraine war, flying ISR and target acquisition missions. It is reportedly the most numerous of the 2,000+ pre-February 2022 UAVs in the Russian drone fleet, numbering at least in the many hundreds. In May 2022, the Russian military introduced the Orlan-10 combat version that carries four high-explosive fragmentation shells under its wings to target Ukrainian military vehicles, mortars, and soldiers. The official MOD video that unveiled this drone claimed that the munitions can be attached to the UAV in minutes, converting this ISR platform into a combat drone with a range of around 120 km. Russia first unveiled this Orlan-10 version during Zapad- 2021 exercises with Belarus, when modified Orlans flew missions with the Forpost-R combat version, along with Orion and Lastochka combat drones. While this Orlan-10 version increases the number of drones capable of flying combat missions in the Russian military, the actual ground-attack efficacy of this UAV is unknown, considering that Orlan-10 would have to drop the munitions in mid flight, the way a manned aircraft does. At this point in the war, ground targets were struck by both sides’ drones with guided munitions, or from a quadrocopter, which can approach the target directly from above for greater accuracy. Nonetheless, it is likely that the Russian military will start fielding more Orlan-10 combat versions for combat in Ukraine. RUSSIA’S RATNIK COMBAT SYSTEM TURNS 10 YEARS OLD Russia’s Nezavisimoe Voennoe Obozrenie (NVO-Independent Military Review), one of Russia’s key online publications about military and security developments, published an overview of the Ratnik soldier combat system, since its inception 10 years ago. Ratnik includes different types of soldier gear in a single package, such as firearms, body armor, and optical, communication, or navigation devices, comprising a total of 65 elements for warfighter protection, control, life support, and energy supply for more effective combat operations. This equipment was developed at the Central Research Institute of Precision Engineering, which is part of the Rostec state corporation, along with 50 industrial enterprises across the Russian defense sector. Ratnik gear entered service in 2015, and today is present in Russia’s Ground Forces, Airborne Forces, Marines, and Special Forces. At this point, the Russian military is using the second generation of Ratnik gear, with the third and subsequent generations on the Rostec drawing board, as the company seeks to design soldier equipment for future military challenges. This future design, tentatively dubbed Sotnik, is supposed to incorporate mini- and micro-UAVs, wearable robotic systems, exoskeletons, and artificial intelligence elements. Many of these plans were discussed in detail prior to Russia’s February 2022 invasion of Ukraine, a war that has showcased a lack of preparedness as well as a lack of key equipment and capabilities. It is not known how future generations of Ratnik will be affected by global high-tech and IT sanctions, which may have long-term consequences for the Russian defense sector. ERA TECHNOPOLIS MAINTAINS KEY PLACE IN MOD’S MILITARY R&D ECOSYSTEM In April 2022, MOD’s ERA Technopolis took place in several events that showcased Russian defense-oriented high-tech and advanced technology developments. On April 21, 2022, ERA hosted the ""Innovation Day of the Russian Ministry of Defense"" exhibition, an annual event that, this year, drew 200 participants from 50 military, government, and academic organizations across country. ERA showcased over 70 projects and developments in artificial intelligence, medicine, robotics, and information technology. During the exhibition, participants discussed research and development of domestic IT technologies, artificial muscles, 6G, metauniverses, and neural interfaces. During April 25-29, 2022, ERA researchers and developers took part in the annual “Russian High Technologies Week–2022,” a government project that brings together several exhibitions, forums, and conferences in information technology and telecommunications. ERA efforts in artificial intelligence, medicine, information, and cybersecurity were demonstrated during that time. According to ERA, most of the exhibits presented were already tested for further implementation in the Russian armed forces. Specifically, ERA’s Machine Learning Center—an institution launched recently to introduce weapons with elements of artificial intelligence into the Russian military—showcased its achievements in neural networks development. In issue 37 of AI and Autonomy in Russia, CNA wrote about the MOD agreement on the ERA’s development of artificial intelligence technologies for the Russian armed forces, placing the technopolis at the center of Russian military high-tech development efforts. The ERA Technopolis was created in 2018 by order of the Russian president to develop innovative military technologies, to cooperate with the public and private sectors on joint high-tech development, and to prepare young military officers and civilian employees for work in military-industrial enterprises and research institutions. RUSSIAN MOD DISPLAYS COMBAT GROUND VEHICLES DURING MAY 9 PARADES ACROSS THE COUNTRY Russia’s May 9 Victory Day parade remains one of the key national events that commemorates the Soviet Union’s victory over Nazi Germany in May 1945. For the past several years, Russian military has showcased its unmanned and autonomous vehicles and systems during the main parade in Moscow, and in many smaller parades across Russian cities that take place on the same day. This year, the MOD displayed the Uran-9 combat unmanned ground vehicle (UGV) during the Moscow parade, as it has done in the past. While Uran-9 was the only unmanned systems featured in that main parade, a smaller event in Rostov-on-Don featured the Uran-6 demining UGV. While the Russian military is eager to showcase its new and advanced system on parades, according to social media there are only a few instances so far of Uran-6 UGV use in Ukraine to clear mines and unexploded ordnance, and no evidence of Uran-9 combat UGV use in Ukraine. While the introduction of this technology dates to 2018, when the Russian military tested Uran-9 and subsequently used the Uran-6 in Syria, the MOD has not developed a concept to date that would integrate these vehicles in combined operations. In fact, the MOD announced in 2021 that it would test a unit of 20 Uran-9 UGVs, a plan that may have been derailed by the Russian invasion of Ukraine and allocation of resources to that war. Uran-6 is used sparingly in areas that were already cleared of adversary presence, given that the vehicle operator has to remain in close proximity for operation. At the same time, the Russian MOD maintains focus on the introduction of unmanned and autonomous systems in combat, a concept that is supposed to safeguard soldier lives and make mission more effective. For more on how such plans are discussed and implemented across the Russian military, see the report AI and Autonomy in Russia. MARKETS AND PRIVATE SECTOR NEW GRANT FOR TECH STARTUPS OPENS A new grant designed to support IT startups in Russia is now open for application and will run until mid June. The project, which is part of the “Rise – from Startup to IPO” federal project based in the Economic Development and Innovative Economy state program, involves a competitive application process that can unlock up to 250 million rubles in funding. The grants are expected to consist of a total of 10 billion rubles in outlay through to 2024. Focus areas for grant applications include a list of 17 “priority areas,” including internet-of-things applications, AI, quantum computing, 5G mobile networks, advanced space systems, decarbonization technology, distributed intelligent systems, energy sources, genetic technologies, and other advanced technological sectors. TECH COMPETITION FOR MEDICAL DIAGNOSTIC SYSTEMS OPENED A 200-million-ruble prize fund is open for competitive applicants to the “AI'm Doctor” tech competition held by the National Technology Initiative Platform. The competition seeks to incentivize AI system developers to produce a reliable medical decision support system able to aid in making final clinical diagnoses. The goal of such systems is to ""help doctors more quicklyand accurately identify diseases and complications after them."" The competition is set to run through 2024 and is open to all entrants. PROVINCIAL RUSSIAN LABOR MARKET ANALYZED FOR UNFILLED PROFESSIONAL GAPS A research project at Perm Polytechnic University analyzed the labor market in Perm province using an AI-based neural network approach. The project focused on identifying gaps in the market where an insufficient supply of professionals was particularly damaging to the broader market's health. The study authors noted that “[the study] revealed an acute shortage of competent specialists in the field of information and communication technologies, which, along with migration processes, leads to a high level of unemployment. In addition, we have a large shortage of specialists in all areas where programming skills are required.” These findings have been integrated into plans at state-sponsored employment centers, especially in light of the Russian government’s renewed efforts to halt potential “brain drain” effects from emigration in the wake of the war with Ukraine. SAFETY MANAGEMENT SYSTEM FOR METRO ESCALATORS USES NEURAL NETWORKS Russian scientists at the St. Petersburg Federal Research Center and St. Petersburg Electrotechnical University (LETI) have developed a new safety management system for Metro escalators using AI processes in the form of streaming recurrent neural networks. The program analyzes data from video cameras, microphones, and sensors in real time to assess the potential for a dangerous situation and can actively seek countermeasures to prevent them, such as shutting off escalator movement. At present, special duty officers are stationed at the escalators, but according to Metro, the monotonous nature of the work can dull their attention and lead to poor reaction time in the case of an actual emergency. According to the researchers, “Tests of the system have shown that it detects dangerous situations 3.5 times faster than a typical escalator attendant.” The researchers noted that while they relied on Russian AI technology in general, some equipment and software from the West was required during the development phase. At present the system is not planned to be deployed on scale at subway facilities in Russia, but the development is a useful proof-of-concept that can be quickly introduced in the medium term. RUSSIAN AI MARKET REACHES 550 BILLION RUBLE VALUATION According to the Artificial Intelligence Almanac published by the Competence Center of the National Technology Initiative at MIPT, the Russian AI market value increased 28% in volume at the end of 2021. This amounted to a total value of 550 billion rubles. According to the report, Yandex and VK were the total market leaders, with roughly 100 companies occupying over 98% of the market value. These market dominant firms amount to roughly one-quarter of the total number of AI companies in Russia. The report also notes that due to the 'Artificial Intelligence' federal project, state funding for AI has nearly doubled. Meanwhile, the venture capital market grew by 170%. According to the report, over 60% of the market is in data analysis, with a strong 30% in natural language processing products. NEW INTERNET-OF-THINGS BLUETOOTH SENSORS ANNOUNCED The state corporation Rostec and the IT company Tesla Smart have announced a new product line of wireless Bluetooth sensors with long-lasting (10 year) battery lives designed for interlinked smart home and internet-of-things (IoT) devices. The sensors rely on “Bluetooth Low Energy” technology, and are designed to “measure temperature, humidity, illumination, and magnetic field” from which data can be used for monitoring systems. According to Arkady Orlov, the general director of Ryazan Plant of Metal-Ceramic Instruments, which manufactures the new sensors, there is ""a wide range of applications for new sensors—in warehouses with special storage conditions, in industrial refrigeration equipment, in greenhouses and in smart home systems. The sensors have a Bluetooth Long Range mode. This allows you to use them at a distance of more than a kilometer from the Bluetooth station. The products are compact and easily attached to surfaces and operate in the industrial range from -55 to +85 °C.” NEW “TECHNOPARK” ANNOUNCED IN AMUR REGION The regional government of the Amur province has announced a new manufacturing- focused technology hub to coordinate innovation in the region. The new 'technopark' will seek to incubate and support information, agricultural, and biological tech industries, with special aid to stimulate local high-tech production and manufacturing capabilities. IT, AI, space, and gas chemistry are particular focuses, according to the regional government. The hub is likely to include special tax and other legal benefits to members of the technopark, although these have not yet been announced. RUSSIAN SUBSTITUTE FOR AMERICAN-MADE MACHINE DATA PROCESSING SOFTWARE DEVELOPED The Russian IT company ISGNeuro, through its WDC-Platform machine learning project, has developed an import-substitution compliant software package for machine data processing, which replaces “Splunk,” a widely used US-based analogue. Splunk has been absent from the Russian market since 2019, when it exited the Russian market and suspended sales and services to Russian clients. Yet Splunk continued to provide technical support for already purchased products until the new round of wartime sanctions. WDC-Platform produced the “Antisplank” (i.e., “Anti-Splunk”) package that will allow for a full transition to domestically produced software.According to descriptions by Vladimir Statut, the head of innovation at ISGNeuro, a subset of WDC-Platform, Antisplank ""makes it possible to connect any type of event sources at minimal cost, collect data, ensure its quality, and analyze it using machine learning algorithms built into the platform.” In addition, he says that “by using a simple data query language that is backwards compatible with the one used in Splunk, there will be little to no retraining of IT specialists."" HUMAN CAPITAL AI HACKATHONS AND EVENTS Recent news articles reported on several AI-related hackathons and training events, the most notable of which are mentioned below:According to a D-Russia article, a Russia-wide undersea robotics competition was held in Vladivostok May 6-7. Twenty-eight teams, with participants from grades 1 to 11, took part in the finals. The winners will represent Russia at the International underwater robotics competition in June-August 2022. The competition was organized by the Center for Robotics, the Center for the Development of Robotics, and the Maritime State University. The first of eight district hackathons planned for 2022 as part of the Digital Breakthrough (formerly “116 Hackathons”) series are being held in Khabarovsk (May 27-29) and Nizhny Novgorod (June 3-5). Winners from these district hackathons, who already won at the regional level, will go on to compete in one of three national-level competitions, the first of which will be held in Moscow on June 25-July 25. The organizers are expecting more than 7,500 participants, combined, for this year’s schedule of AI events. According to an April 26 Regnum article, two conferences—“Start in medicine” and “Kurchatov project—from knowledge to practice, from practice to result”—were recently held in Moscow for pre-professional students. Students presented their designs, including a solar-powered drone and a neuro-alarm clock, to a panel of judges. NEW AI EDUCATION PROGRAMS Recent news articles reported on several new AI educational programs, summaries of which are listed below; According to a May 11 press release, St. Petersburg’s ITMO University is launching a new master’s program in industrial AI with Gazprom Neft’s Science and Technology Center. The two-year program, beginning in September 2022, is designed to allow students with varied industrial backgrounds (such as geology, oil, and engineering) to bring AI solutions to their various fields. According to a May 3 TASS article, the Altai region is training engineers in artificial intelligence for the first time. At the request of local businesses requiring such expertise, Altai State Technical University (ASTU) will train engineers in AI programming, computer vision, neural networks, and basic programming languages. ASTU is the largest technical university in the Altai Territory and one of the top 100 universities in the country. According to a May 13 press release, the Higher School of Economics (HSE) business incubator, the AFK Sistema conglomerate, the Moscow Institute of Electronics and Mathematics (MIEM), and the National Technical Initiative Circle Movement are jointly holding a 12-week accelerator program for young professionals who are developing robotics startups. The accelerator program provides participants with guidance from technical and business experts. It also assists in attracting grant funding. NUMBER OF STATE-FUNDED UNIVERSITY SLOTS FOR DIGITAL ECONOMY IN 2023-2024 REMAINS HIGH According to a May 6 D-Russia article, 160,361 state-funded “budget” slots have been allocated for personnel trained in digital economy for the 2033-2034 academic year. This number of slots is similar to the the number available in the previous academic year but is double the number available in 2021. According to Deputy Prime Minister Chernyshenko, this year, priority in the distribution of budget places is given to regional universities (112,993), and the emphasis is on specializations which will ensure the fastest import substitution. According to the report, in the 2023-2024 academic year, universities will also have new areas of training. Among them are “Digital and additive technologies in mechanical engineering” and “Laser technologies.” NEW AI LABORATORIES TO APPEAR AT SUSU According to a May 16 TASS article, new laboratories for multiscale multiphysics modeling and computer vision and hearing will soon appear at South Ural State University (SUSU). According to the article, this initiative, within the framework of the Priority 2030 program, is part of a strategic project aimed at ensuring digital transformation and increasing the competitiveness of metallurgical and machine-building enterprises in the Urals and throughout Russia. One of the university’s current projects is to increase production efficiency by several tens of millions of rubles per year by reducing unscheduled downtime of mills. INTERNATIONAL COLLABORATION US PLAN FOR RUSSIAN TECH IMMIGRATION ASSESSED After the Russian invasion of Ukraine, tens of thousands of Russian scientists and engineers left the country. Many have sought temporary breaks in other countries, such as Georgia, while weighing their options. Those who cannot find permanent residences and work are likely to return to Russia. In a sign that Russia fears a brain drain from the war, the government, in March 2022, exempted young workers in the technology sector from compulsory military service, gave them preferential mortgage rates, and liberalized information technology. Russia has been suffering from a brain drain for at least a century, partly because it produces outstanding university graduates but does not have an economy strong enough to put their skills into practice. According to a study by Russia’s Higher School of Economics, 60-75 percent of graduate students of the academic track of leading universities leave Russia. In the advanced fields of natural and technical sciences, this share reaches 80 percent. The US and other countries have long benefited from Russian immigrants, including some of those countries that are now independent states. Around 2010, the brain drain began to subside as the Russian economy started to perform well. Some Russians even returned home. But the invasion of Ukraine seems to have pulled the plug out of the drain again. Most expats today go to neighboring countries, including Turkey, Armenia, Georgia, Kazakhstan, and the Baltic states. It is hard to get into the US, especially because there are not enough visas. In March, the Department of Homeland Security granted Ukrainians temporary protected status for 18 months, allowing them to stay and work in the US without a visa, but did not do so for Russians. Recently, however, the Biden administration asked Congress to suspend for four years the requirement that Russian scientists applying for H1-B visas have a sponsoring employer. This measure will only apply to Russian citizens who hold a master's or doctoral degree in science or technology such as artificial intelligence, nuclear engineering, and quantum physics. They will have to pass an aptitude test. There are no limits on the number of people entering on these visas. This measure will expire in four years unless Congress subsequently renews it. The program is already being advertised on social media. Those wishing to receive a US Green Card for Scientists are invited to take a free 10-minute test, the results of which give the host an understanding of the merits and degree of talent of the applicant. An applicant’s portfolio can include awards and diplomas, membership in professional associations, scientific publications, and letters of recommendation from influential colleagues. Even proof of higher income compared to the industry average is suitable. Experts predict that if the measure becomes law, Putin will be unhappy at the prospect of losing so much talent. Gleb Yushin, professor of engineering and materials at the Georgia Institute of Technology—who received a bachelor's degree in physics from the Polytechnic Institute in St. Petersburg, Russia, and who co-founded Sila Nanotechnologies, a US company now valued at more than $3 billion—praised the United States' initiative to simplify the procedure for obtaining permanent residence for Russian scientists and engineers: I think this is an important strategic move in favor of the United States and the weakening of Putin's authoritarian regime. The smartest and most creative scientists and engineers cannot thrive in a country that severely restricts their freedom and values natural resources more than people's ability to innovate. It will be difficult for Putin to oppose this measure because the travel ban will destroy his support and reputation. This will make everyone want to leave even more. The truth is that many talented, hard-working people are eager to emigrate but feel that no one will accept them, especially now. Scientists and engineers want to maximize their positive impact, and they know they have a much better chance of doing so from outside. Brian Taylor, professor of political science at Syracuse University, agrees with Yoshin. “Some experts have estimated that hundreds of thousands of Russians have already left, including tens of thousands in the IT sector. If Russians with the right education can immigrate to the United States without even finding a job sponsor, this will seem like an attractive option to many.” The actions of the Biden administration regarding immigration could have a strategic effect by contradict Russia’s message that the whole world is against Russians. RUSSIAN TECH COMPANIES LOOKING FOR HELP FROM CHINA TO OVERCOME SANCTIONS Large state-owned companies in Russia are actively looking for manufacturers of individual electronic components in China to avoid US sanctions and current problems with buying finished electronic goods directly from factories. They are negotiating with Chinese manufacturers to purchase components directly. In particular, the state corporation Rostec and several other large state-owned companies that have factories and assembly capacities in Russia are interested in such solutions because they now lack the necessary components for the production of electronics. Industry representatives believe that manufacturers in China can supply memory modules and motherboards as separate orders, although it is generally more profitable for them to sell the final products. However, the sale of server and network equipment with American technologies to state-owned companies is prohibited in Russia. To get around these restrictions, state-owned companies are planning or have already begun purchasing individual components from China to continue assembling servers and telecommunications equipment in Russia. Lenovo, Huawei, and ZTE can act as suppliers of the necessary spare parts and electronics. Arseniy Brykin, the head of the Basis consortium, told Vedomosti that all ready-made solutions from Chinese manufacturers of user, server, and infrastructure equipment somehow contain American intellectual property. In this situation, the equipment is subject to secondary sanctions, and Russian companies that fall under US restrictions cannot purchase it. But the purchase of individual components from China that are not subject to restrictions makes it possible to both continue assembly in Russia and maintain the performance of already purchased equipment. Alexander Sysoev, head of Infrastructure Solutions at Krok, told the newspaper that before the US imposed export sanctions, foreign manufacturers generally did not conduct commercial activities in Russia directly, but instead worked through Russian distribution companies. However, now Russian IT companies cannot offer customers equipment of Chinese origin: manufacturers have suspended all new orders for finished products. For example, Huawei fulfills its obligations under contracts concluded before March 26, 2022, but has suspended new contracts. Ivan Pokrovsky, the executive director of the Association of Russian Developers and Manufacturers of Electronics believes that the decision to purchase individual components ""may be short- sighted. If earlier commercial distributors were engaged in purchases, now legal entities affiliated with Russian state-owned companies will appear in contracts, and this may lead to additional sanctions against them.” In addition, this measure will not completely solve the problem, since Chinese companies do not produce a full line of components. Furthermore, according to Sysoev, the sale of components not related to the production of IT equipment directly in China for Russian customers is unprofitable and may not work. “Even if we imagine that a certain organization wants to assemble a computing complex on its own from individual components, no one can guarantee that this system will work and work without failures.” In the meantime, Russia and China are continuing to negotiate new cooperation projects. A new initiative is being negotiated between the Technopolis Moscow SEZ and the large special technological zone of China Jiangbei Xinqu in the field of microelectronics, information technology, and biomedicine. The next stage of the talks will involve the selection of Chinese enterprises that are ready to supply the necessary products to Russia, as well as companies interested in investing in the infrastructure of the Moscow SEZ. The Chinese side, in turn, is showing great interest in bilateral personnel exchange and joint organization of incubators and accelerators. In addition, the parties are discussing the signing of a framework cooperation agreement for long-term and sustainable cooperation, including in the field of science and technology. OTZ “Jiangbei Xinqu” is located in a new area of Nanjing near Shanghai. It includes research parks and 35 institutes, 12 higher education institutions, and more than 6,000 resident companies. Its focus is the development of microelectronics, artificial intelligence, cloud computing and big data, medicine and biotechnology, information technology, and new materials. Projects with a total investment of more than 1.2 trillion rubles are being implemented here. They include the scientific and technical innovation base ""City of Chips"" and the Chinese Industrial Park of the Mobile Internet of Things. The Technopolis Moscow Special Economic Zone, subordinated to the Department of Investment and Industrial Policy of Moscow, includes five sites with a total area of 223.3 hectares. One of them is located in Pechatniki, and four more (Alabushevo, Mikron, MIET, and Angstrem) are in Zelenograd. For residents, special support measures are assumed—for example, they are exempted from paying property, transport, and land taxes, as well as customs duties. The income tax rate for such enterprises is only 2 percent. There are also benefits for the lease of land allocated for the construction of an enterprise, and upon completion, it is possible to buy out a leased land plot for 1 percent of the assessed value. CHINESE HACKER GROUPS TARGETING RUSSIAN TECHNOLOGY According to the Google Threat Analysis Group (TAG), hacker groups that are theoretically linked to the Chinese authorities and military have recently made Russia their main target. Hackers from the Curious Gorge and Bronze President groups have been attacking the networks of Russian government agencies, military contractors, and logistics and manufacturing companies. Even the Russian Foreign Ministry has been attacked. While the reasons for their sudden interest in Russia remain unknown, before Russia’s invasion of Ukraine they operated mainly in Asian countries. According to Google TAG, Curious Gorge has strong ties with the People's Liberation Army China Strategic Support Force (PLA SSO), which is the part of the PLA that has responsibility for cyber operations. The Chinese threat to Russian networks is currently not limited to Curious Gorge. According to ZDnet, at the end of April 2022 the Bronze President group also chose Russia as its target. This group goes by several names in reports from various cybersecurity companies, including Mustang Panda, TA416, and RedDelta, and has been active since at least 2018. According to information security firm Secureworks, Bronze President is either “sponsored by, or at least tolerated by, the Chinese government” and “seems to be changing its goals in response to the political situation in Europe and what is happening in Ukraine.” A few weeks ago, its hackers were working primarily in Southeast Asia, but now they prefer Russia and some European countries. The researchers say, “This suggests that attackers have received updated tasks that reflect the changing intelligence gathering requirements of the People’s Republic of China.” Secureworks experts suggest that Russia's coming to the attention of Bronze President may indicate “an attempt by China to introduce modern malware into the computer systems of Russian officials.” They discovered and analyzed the malicious executable file Blagoveshchensk—i.e., Blagoveshchensk Border Detachment.exe distributed by the group, which was disguised as a PDF file and encrypted. Inside it was the PlugX malware loader. When run, the file displays a decoy document written in English, which describes the situation with refugees and EU sanctions. While the user who launched the file is reading the document, PlugX malware is being downloaded in the background. PlugX is a remote access Trojan used to steal files, execute remote commands, install backdoors, and deploy additional malware. Bronze President's tools also include Cobalt Strike, China Chopper, RCSession, and ORat malware. According to CNews, attacks by hackers from Bronze President and Curious Gorge on Russian networks are the third major confirmation that China may be interested in distancing itself from Russia, although without officially turning its back. First, in March 2022, Xiaomi, Oppo, and Huawei halved the supply of their smartphones to Russia. These are some of the largest players in the Russian mobile device market. They cited logistical problems as the reason for the limitation of shipments. Meanwhile, other Chinese companies, such as the AliExpress store, have continued to work and deliver goods to Russia with no complaints about the break in supply chains. Second, in April 2022, Huawei, China's largest tech company, removed apps from Russian banks that are under European and US sanctions from its AppGallery store, without providing a clear explanation for this action. YANDEX WILL NOT MOVE ITS HEADQUARTERS OUT OF RUSSIA According to the company’s press service, Yandex has no plans to move its headquarters from Moscow to Tel Aviv or to move employees from Russia there. The company explained that they continue to hire employees for some of the vacancies, including in Israel, while some of the vacancies allow for the possibility of relocation. “We also want to help employees who work remotely receive their salary in a way that is convenient for them,” Yandex added. The press service noted that the Yandex office in Tel Aviv has been operating for many years, and its employees are developing Yango (the international division of Yandex.Taxi), unmanned technologies, educational projects, and other services. Earlier, the Israeli economic newspaper Calcalist reported on Yandex's alleged plans to relocate. The publication claimed that Arkady Volozh, CEO of the Yandex group of companies, intended to take ""hundreds of developers and engineers"" to Tel Aviv, and had sent a letter about this to Israeli prime minister Naftali Bennett, as well as to some members of the Israeli Cabinet. JAPAN EXPANDS SANCTIONS ON HIGH TECH Japan is expanding the list of goods and technologies whose export to Russia is prohibited because of Russia’s invasion of Ukraine. New items on the list include 3D printers and quantum computing equipment, electronic and atomic force microscopes, and oil-refining catalysts. The expanded ban has already been approved by the government and came into force on May 20.Japan had previously imposed several packages of sanctions on Russia because of the situation around Ukraine. Personal sanctions targeted the country's leadership, officials, and business people. The list of goods and technologies prohibited for export has more than 300 entries, including semiconductors, equipment for maritime and aviation security, telecommunications equipment, military products (including weapons), software, and oil-refining equipment. In addition, Japan froze the assets of Otkritie Bank, Novikombank, Sovcombank, VTB, Rossiya Bank, Promsvyazbank, and VEB.RF, and blacklisted more than 700 nationals of Russia, Belarus, and the people’s republics of Donetsk and Lugansk (DPR, LPR), as well as over 200 Russian companies and organizations.",0.0
"Artificial Intelligence and Autonomy in Russia: Issue 38, May 16, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/ai-and-autonomy-in-russia/issue-38,2022-05-16,,Intro (no tag) GOVERNANCE AND LEGISLATION,0.0
"Artificial Intelligence and Autonomy in Russia: Issue 37, May 2, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/ai-and-autonomy-in-russia/issue-37,2022-05-02,,Intro (no tag) GOVERNANCE AND LEGISLATION,0.0
"Artificial Intelligence and Autonomy in Russia: Issue 36, April 18, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/ai-and-autonomy-in-russia/issue-36,2022-04-18,,Intro (no tag) GOVERNANCE AND LEGISLATION,0.0
"Artificial Intelligence and Autonomy in Russia: Issue 35, April 4, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/ai-and-autonomy-in-russia/issue-35,2022-04-04,,Intro (no tag) GOVERNANCE AND LEGISLATION,0.0
"Artificial Intelligence and Autonomy in Russia: Issue 34, March 21, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/ai-and-autonomy-in-russia/issue-34,2022-03-21,,Intro (no tag) GOVERNANCE AND LEGISLATION,0.0
"Artificial Intelligence and Autonomy in Russia: Issue 33, March 7, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/ai-and-autonomy-in-russia/issue-33,2022-03-07,,Intro (no tag) GOVERNANCE AND LEGISLATION,0.0
"Artificial Intelligence and Autonomy in Russia: Issue 32, February 21, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/ai-and-autonomy-in-russia/issue-32,2022-02-21,,Intro (no tag) GOVERNANCE AND LEGISLATION,0.0
"Artificial Intelligence and Autonomy in Russia: Issue 31, February 7, 2022 - Intro (no tag)",https://www.cna.org/our-media/newsletters/ai-and-autonomy-in-russia/issue-31,2022-02-07,,Intro (no tag) GOVERNANCE AND LEGISLATION,0.0
