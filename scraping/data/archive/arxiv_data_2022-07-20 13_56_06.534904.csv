title,url,date,text,cleaning,tokens
Learning Bellman Complete Representations for Offline Policy Evaluation,"[{'href': 'http://arxiv.org/abs/2207.05837v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.05837v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-12 21:02:02,"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

1

An Empirical Evaluation of Four Off-the-Shelf
Proprietary Visual-Inertial Odometry Systems
Jungha Kim, Minkyeong Song, Yeoeun Lee, Moonkyeong Jung, and Pyojin Kim∗

2
2
0
2

l
u
J

4
1

]

O
R
.
s
c
[

1
v
0
8
7
6
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—Commercial visual-inertial odometry (VIO) systems
have been gaining attention as cost-effective, off-the-shelf six
degrees of freedom (6-DoF) ego-motion tracking methods for
estimating accurate and consistent camera pose data, in addition
localization from
to their ability to operate without external
is unclear
motion capture or global positioning systems. It
from existing results, however, which commercial VIO platforms
are the most stable, consistent, and accurate in terms of state
estimation for indoor and outdoor robotic applications. We assess
four popular proprietary VIO systems (Apple ARKit, Google
ARCore, Intel RealSense T265, and Stereolabs ZED 2) through
a series of both indoor and outdoor experiments where we show
their positioning stability, consistency, and accuracy. We present
our complete results as a benchmark comparison for the research
community.

Index Terms—Commercial visual-inertial odometry, Apple

ARKit, Google ARCore, Intel T265, Stereolabs ZED 2

I. INTRODUCTION

T HIS article presents a benchmark comparison of off-the-

shelf proprietary visual-inertial odometry (VIO) systems
used for autonomous navigation of robotic applications, which
are the process of determining the position and orientation of
a camera-inertial measurement unit (IMU)-rig in 3D space by
analyzing the associated camera images and IMU data. As
the VIO research has reached a level of maturity, there exist
several open published VIO methods such as MSCKF [1],
OKVIS [2], VINS-Mono [3], and many commercial prod-
ucts utilize closed proprietary VIO algorithms such as Apple
ARKit [4], Google ARCore [5] that offer off-the-shelf VIO
pipelines which can be employed on an end-user’s system of
choice.

The current research studies provide some comparative
experiments on the performance of the popular VIO ap-
proaches, however, they consider only a subset of the existing
open-source and proprietary VIO algorithms, and conduct
insufﬁcient performance evaluation only on publicly-available
datasets rather than indoor and outdoor challenging real-world
environments. In particular, although commercial VIO systems
(Intel T265, Stereolabs ZED 2) play an important role in
the several DARPA challenges [6], [7] and many commercial
products or apps (Pok´emon GO, IKEA Place AR), there is a
lack of research for benchmarking the positioning accuracy of
these closed proprietary VIO platforms.

The motivation of this paper is to address this deﬁciency
by performing a comprehensive evaluation of off-the-shelf

All

authors

are with Department

Systems
Engineering, Sookmyung Women’s University, Seoul, South Korea.
{alice3071,smk615,snwfry,jmk7791,pjinkim}@
sookmyung.ac.kr (∗ Corresponding author: Pyojin Kim)

of Mechanical

Fig. 1. The custom-built capture rig for benchmarking 6-DoF motion tracking
performance of Apple ARKit (iPhone 12 Pro Max), Google ARCore (LG V60
ThinQ), Intel RealSense T265, and Stereolabs ZED 2.

commercially-available VIO systems in challenging indoor
and outdoor environments as shown in Fig. 2. This is the ﬁrst
comparative study on four popular proprietary VIO systems
in six challenging real-world environments, both indoors and
outdoors. Especially, we select the following four proprietary
VIO systems that are frequently used in autonomous driving
robotic applications:

• Apple ARKit [4] - Apple’s augmented reality (AR) plat-
form, which includes ﬁltering-based VIO algorithms [8]
to enable iOS devices to sense how they move in 3D
space.

• Google ARCore [5] - Google’s AR platform utilizing
a multi-state constraint Kalman ﬁlter (MSCKF) style
VIO algorithm [1], [9], called concurrent odometry and
mapping (COM) [10].

• Intel RealSense T265 [11] - a stand-alone VIO and
simultaneous localization and mapping (SLAM) tracking
device developed for use in robotics, drones, and more,
with all position computations performed on the device.
• Stereolabs ZED 2 [12] - a hand-held stereo camera with
built-in IMU for neural depth sensing and visual-inertial
stereo, requiring an external NVIDIA GPU to obtain the
6-DoF camera poses.

We do not consider open-source published VIO methods
and non-inertial visual simultaneous localization and map-
ping (SLAM) algorithms, for example ROVIO [13], VINS-
Mono [3], ORB-SLAM [14], and DSO [15]. We focus on the
off-the-shelf commercial VIO/SLAM products that might be of
interest to more researchers and engineers because open pub-
lished VIO algorithms are relatively difﬁcult to understand and
operate, and their comparisons are made in the literature [16],
[17] to some extent.

figure_capture_rig.pdf

Google ARCore
(LG V60 ThinQ)

Apple ARKit
(iPhone 12 Pro Max)

Intel RealSense T265

Stereolabs ZED 2

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

2

Fig. 2. Accumulated 3D point cloud (middle) with the estimated 6-DoF trajectory (red) from Apple ARKit in multi-ﬂoor environments. We capture the
6-DoF camera poses and 3D points while climbing the multi-story stairs (left). Among the four proprietary VIO systems, Apple ARKit shows the most
consistent and accurate 6-DoF motion tracking results, consistently reconstructing the 3D geometry of stairs and hallways. The Apple ARKit track (red) and
3D reconstruction results have a similar shape as the ground-truth blueprint of a building (right).

Our experiments are conducted in six challenging indoor
and outdoor environments with the custom-built
rig
equipped with the four VIO devices as illustrated in Fig. 1. Our
test sequences contain long and narrow corridors, large open
spaces, repetitive stairways, an underground parking lot with
insufﬁcient lighting, and about 3.1 kilometers of a vehicular
test in complex urban trafﬁc environments.

test

Our goal is to provide a thorough benchmark of closed
proprietary VIO systems, in order to provide a reference for
researchers on VIO products, as well as readers who require
an off-the-shelf 6-DoF state estimation solution that is suitable
for their robotic platforms and autonomous vehicles.

II. RELATED WORK

Despite proprietary VIO systems being utilized in many
products and areas for industrial usage (e.g., for building an
accurate indoor map, as a precise positioning system, etc.),
there is no benchmark study that satisﬁes our proposed goals.
While comprehensive comparisons of open-source published
VIO methods exist [16], they focus only on evaluating the
popular academic VIO algorithms on the EuRoC micro aerial
vehicle dataset [18], and do not cover off-the-shelf proprietary
VIO systems and various indoor and outdoor environments.
Although ADVIO [19] presents a VIO comparison including
three proprietary platforms and two academic approaches,
its main contribution is to develop a set of RGB and IMU
smartphone datasets, not a performance evaluation between the
proprietary VIO platforms. In [20], [21], some comparative
studies of proprietary VIO systems have been performed,
they consider only a few proprietary VIO platforms.
but
Performance evaluation is only conducted in a simple 2D
indoor environment with a short camera moving distance.

Since we focus on the 6-DoF positioning accuracy of the
proprietary VIO systems, we can instead consider the existing
results relevant to this problem. The proposed VIO approach
in [22] compares to Google ARCore and VINS-Mono [3],
but only on a few indoor sequences with very little camera
movement. The evaluation framework in [23] assesses the 6-
DoF motion tracking performance of ARCore with the ground

truth under several circumstances, but they lack comparative
results for other proprietary VIO systems such as ARKit and
T265, and detailed analyses are performed only for ARCore.
Most important is that no existing work considers an in-
door/outdoor performance evaluation for four popular propri-
etary VIO systems that are frequently deployed on robotic
applications, AR/VR apps, and industrial usages. Our test se-
quences are authentic and illustrate realistic use cases, contain-
ing challenging environments with scarce or repetitive visual
features, both indoors and outdoors, and varying motions from
walking to driving camera movements. They also include rapid
rotations without translation as they are problematic motions
for many VIO/SLAM algorithms.

III. VISUAL-INERTIAL ODOMETRY SYSTEMS

We brieﬂy summarize the primary features of four off-
the-shelf proprietary VIO systems based on data published
on the relevant ofﬁcial websites, papers, GitHub, and patent
documents, and how to collect 6-DoF pose estimates from
each VIO mobile device. Since most proprietary VIO/SLAM
platforms are all closed-source, we do not cover the detailed
VIO academic backgrounds and implementations.

A. Apple ARKit

Apple ARKit [4] is Apple’s augmented reality (AR) soft-
ware framework, which includes a tightly-coupled ﬁltering-
based VIO algorithm similar to the MSCKF [1] to enable
iOS devices to sense how they move in 3D space. It contains
a sliding window ﬁlter, bundle adjustment, motion/structure
marginalization modules [8], and is expected to be applied to
various robotic applications such as Apple Glasses and Car
in the future, not just for iPhone and iPad, which is why we
conduct vehicle tests in this benchmark. We develop a custom
iOS data collection app1 for capturing ARKit 6-DoF camera
poses, RGB image sequences, and IMU measurements using
an iPhone 12 Pro Max running iOS 14.7.1. It saves the pose
estimates as a translation vector and a unit quaternion at 60 Hz,

1https://github.com/PyojinKim/ios logger

figure_front_page.pdf

Floor 5

Floor 4

Floor 3

Floor 2

Floor 1

19.2 m

13.2 m

9.2 m

4.6 m

0.0 m

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

3

and each pose is expressed in a global coordinate frame created
by the phone when starting iOS data collection. Although there
are various iPhone and iPad models, the core VIO algorithm
in ARKit is the same, thus we empirically conﬁrm that there
is little difference in the VIO performance of each device.

B. Google ARCore

ARCore [5] is Google’s platform for building AR ex-
periences utilizing the multi-state constraint Kalman ﬁlter
(MSCKF) style VIO/SLAM algorithms [9], [24] with many
subsequent variations, called concurrent odometry and map-
ping (COM) [10]. ARCore is a successor to Google Project
Tango [25], and is currently applied only to Android OS
smartphones, but
it would be extended to various robotic
platforms such as Google Wing, Maps, and Waymo, which
is why we evaluate ARCore in a large-scale outdoor sequence
of about 3.1 kilometers of a vehicular test. We build a custom
Android OS app based on Google’s ARCore example2 to
acquire ARCore 6-DoF camera poses and IMU measurements
at 30 Hz with an LG V60 ThinQ running Android 10.0.0 and
ARCore 1.29. Although there are various Android OS devices
such as Samsung Galaxy and Google Pixel, smartphones
on the list3 certiﬁed by Google demonstrate similar motion
tracking performance regardless of device model.

C. Intel RealSense T265

a

hassle-free

Intel RealSense T265

stand-alone
is
VIO/SLAM device to track its own position and orientation
in 3D space. The embedded processor, vision processing unit
(VPU), runs the entire VIO algorithm onboard, analyzes the
image sequences from stereo ﬁsheye cameras and fuses all
sensor information together. Since the T265 VIO algorithm
runs on the device itself without using the resource of the
host computer,
is widely used as a 6-DoF positioning
sensor in 3D space for various robotic applications such as
DARPA challenges [6] and autonomous ﬂying drones [26].
We collect the 6-DoF motion tracking results at 200 Hz using
Intel RealSense SDK 2.04, and save the T265 6-DoF camera
poses by connecting it to an Intel NUC mini PC.

it

D. Stereolabs ZED 2

Stereolabs ZED 2 is a hand-held stereo camera with built-in
IMU for neural depth sensing, 6-DoF VIO/SLAM, and real-
time 3D mapping. Stereolabs has not made their VIO/SLAM
algorithm public, and the description of the VIO algorithm is
relatively vague compared to other proprietary VIO systems.
It is one of the popular stereo camera sensors for various
robotic applications such as drone inspection [27], but has
the disadvantage of requiring an external NVIDIA GPU to
perform positional
tracking and neural depth sensing. We
develop a program to collect the ZED 2 6-DoF camera poses
at 30 Hz based on ZED SDK 3.5.25 on an NVIDIA Jetson
Nano onboard computer.

2https://github.com/rfbr/IMU and pose Android Recorder
3https://developers.google.com/ar/devices
4https://github.com/IntelRealSense/librealsense
5https://www.stereolabs.com/developers/release/

Fig. 3. We carry the capture rig by hand, and store the onboard computers
and batteries to collect the motion data indoors (left). In the outdoor vehicular
tests, we ﬁx the capture rig to the front passenger seat (right).

IV. EXPERIMENTS

We evaluate four proprietary VIO systems with the four
devices (iPhone 12 Pro Max, LG V60 ThinQ, Intel T265,
ZED2) attached to the custom-built capture rig as shown in
Fig. 1 and Fig. 3 on the large-scale challenging indoor and
outdoor environments, both qualitatively and quantitatively.
Indoors, we record the motion data by a walking person, and
outdoors, the data is collected by rigidly attaching the capture
rig to a car in Fig. 3. We save the 6-DoF pose estimates
of ARKit and ARCore through the custom apps on each
smartphone device, and record the moving trajectories of T265
and ZED2 in the Intel NUC and NVIDIA Jetson Nano onboard
computers. We maintain the default parameter settings of each
VIO platform, and deactivate all capabilities related to SLAM
(e.g., loop closure) for a fair comparison between each VIO
system. Furthermore, in order to interpret the motion tracking
results in the same reference coordinate frame, we calibrate the
intrinsic and extrinsic parameters of all cameras by capturing
multiple views of a checkerboard.

Our benchmark dataset contains various indoor and outdoor
sequences in six different locations, and the total length of
each sequence ranges from 83 m to 3051 m, which are
primarily designed for benchmarking medium and long-range
VIO performance. There are three indoor and three outdoor
sequences, and all indoor sequences are captured in a 7-story
building in the university campus including long corridors,
open hallway spaces, and stair climbs as shown in the top row
of Fig. 4. The indoor cases are as realistic as possible contain-
ing repetitive motion in stairs, temporary occlusions, and areas
lacking visual features. The bottom row of Fig. 4 illustrates
example frames from three outdoor sequences acquired in the
outdoor university campus, underground parking lot, and urban
outdoor roads. In order to evaluate the performance of each
VIO system quantitatively without an external motion capture
system, we coincide the start and end points of the movement
trajectories in all experiments, and measure the ﬁnal drift
error (FDE) metric, which is the end point position error in
meter. We report the quantitative evaluation results of four VIO
systems in Table I. The smallest end point position error for
each sequence is indicated in bold. The ideal FDE value (the
ground-truth path) should be 0, and a large FDE value denotes
an inaccurate position estimate since we deﬁne the starting
point of movement as the origin. In addition, by overlaying
the estimated VIO trajectories on the ﬂoorplan of the building

figure_external_view.pdf

Google 
ARCore

Apple 
ARKit

Intel
T265

ZED 2

Laptop

Google
ARCore

Apple ARKit

Battery

Intel NUC
& NVIDIA
Jetson Nano

Stereolabs
ZED 2

Intel RealSense T265

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

4

Fig. 4. Example image frames from indoor and outdoor benchmark datasets. The top row represents three indoor sequences by foot including long corridors
(a), open hallway spaces (b), and repetitive stairs (c) from a university building. We acquire the camera motion data in the outdoor campus on foot (d), and
through a car in the underground parking lot (e) and urban outdoor roads (f).

TABLE I
EVALUATION RESULTS (FDE) OF FOUR PROPRIETARY VIO SYSTEMS

Experiment

ARKit

ARCore

T265

ZED 2

Length (m)

Indoor Corridor
Indoor Hallway
Indoor Stairs

Outdoor Campus
Parking Lot
Outdoor Roads

0.79
0.14
0.19

2.01
0.26
2.68

0.12
0.09
3.98

0.07
1.14
140.08

1.88
0.61
1.49

4.08
9.01
×

1.44
4.58
4.76

206.38
10.85
409.25

145.21
83.98
114.13

513.81
446.26
3051.61

or Google Maps, we evaluate the consistency, stability, and
reliability of each VIO system qualitatively.

A. Indoor Long Corridors and Open Hallway Sequences

We evaluate four VIO systems in a long U-shape corridor
and open hallway spaces easily found in typical ofﬁce and uni-
versity buildings as shown in Fig. 5. Fig. 4 (a) and (b) illustrate
example frames from both locations. The trajectories of these
sequences are approximately 145 and 84 meters, and include
5 and 11 pure rotational movements and difﬁcult textures. In
a long U-shape corridor and open hallway sequences, the start
and end points of ARKit (red) meet at the black circle without
a severe rotational drift while maintaining the orthogonality
and scale of the estimated trajectory well compared with the
ﬂoorplan. Although ARCore (green) shows the most accurate
results in terms of the FDE metric in Table I, the estimated
VIO trajectory does not match the ﬂoorplan well. Intel T265
(blue) estimates accurate 3-DoF rotational motion well, but
there is a problem with the scale of the moving trajectory
compared to the ﬂoorplan, showing a little larger trajectory
than the actual movements. ZED2 (magenta) presents the most
inaccurate and inconsistent positioning performance among
the four VIO methods as the rotational motion drift error
gradually accumulates over time. Overall, the estimated VIO
trajectories by ARKit (red) are the most similar and consistent
motion tracking results to the actual movements following the
shape of the corridor on the ﬂoorplan.

B. Indoor Multistory Stairs Sequence

We perform a comparative experiment

in a multi-ﬂoor
staircase environment with the 114 m trajectory going up
the stairs from the 2nd basement ﬂoor (B2) to the 5th ﬂoor
(5F) of a building in Fig. 6. The repetitive rotational motion
included in the 3D trajectory of climbing the stairs makes VIO
positioning challenging. Fig. 4 (c) shows example frames from
multistory stair sequence. In the top view (xy-plane), we start
and end at the same points marked in the black circle to check
loop closing in the estimated VIO trajectories. ARKit (red) has
the best performance; the top and side views of ARKit (red)
show the overlapped, consistent 6-DoF motion tracking results
while other VIO systems gradually diverge from the initially
estimated loop. With ARKit (red), the starting and ending
points in xy-plane (top view) nearly match; for the others,
they do not. The ﬁnal drift error (FDE) of ARKit in xy-plane
is 0.19 m, while ARCore, T265, and ZED2 are 3.98 m, 1.49
m, and 4.76 m, respectively. In particular, ZED2 (magenta)
has the most severe trajectory distortion in the z-axis direction
(height) among the four VIO systems. Fig. 6 illustrates the side
and front views of the stairway with the paths from four VIO
devices, showing high consistency of ARKit (red) compared
to other VIO platforms. It is noteworthy that the height of each
ﬂoor estimated by ARKit and the actual height (the ground-
truth) from the building blueprint are approximately identical.

C. Outdoor University Campus Sequence

We choose an outdoor location in the university campus
approximately 513 m to determine which VIO system works
well in the environment of the rapid change of the topogra-
phy, and the narrow road returning as shown in the left of
Fig. 7. Example frames are shown in Fig. 4 (d). It shows the
resulting 6-DoF trajectories from four VIO platforms overlaid
on Google Maps, demonstrating that the start and end points
of ARKit (red) and ARCore (green) meet while matching
well with the shape of the roads shown on Google Maps.
The shape of the estimated trajectory of T265 (blue) is very

figure_example_frames.pdf

(a) Indoor Corridor

(b) Indoor Hallway

(c) Indoor Stairs

(d) Outdoor Campus

(e) Underground Parking Lot

(f) Urban Outdoor Roads

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

5

Fig. 5. Estimated trajectories with four proprietary VIO systems in a long U-shape corridor (left) and open hallway space (right) sequences. We start and
end at the same point marked in the black circle to evaluate the loop closing performance of tested commercial VIO systems. The estimated paths for ARKit
(red) match the building ﬂoorplan most consistently, and only the starting and ending points of ARKit nearly meet; for the others, they do not.

due to its inaccurate rotation estimation, showing the most
severe distortion of the actual movement trajectory among the
four VIO systems as shown in the left of Fig. 7.

D. Outdoor Urban Roads and Parking Lot Sequences

We perform an outdoor vehicle driving experiment with a
mileage of approximately 3 km by attaching the capture rig to
the vehicle as shown in the right of Fig. 7. Fig. 4 (e) and (f)
show example frames from the underground parking lot and
urban outdoor roads. We acquire the motion data while driving
on public automobile roads near Seoul Station in Seoul, and
there are plenty of moving people, cars, and occasional large
vehicles visible in the outdoor environments, which makes mo-
tion tracking of VIO challenging. Even in high-speed driving
conditions, sometimes exceeding 60 km/h, ARKit (red) shows
surprisingly accurate and consistent 6-DoF motion tracking
results overlaid on Google Maps as shown in the right of
Fig. 7. The start and end points of ARKit (red) accurately
meet in the black circle, and the ﬁnal drift error (FDE) is only
2.68 m in Table I. ARCore (green) occasionally fails when the
speed of the car increases or the light variations occur abruptly.
In T265 (blue), if the car stops temporarily due to a stop signal
or is driving too fast, the VIO algorithm diverges and fails to
estimate the location. ZED2 (magenta) accumulates rotational
drift error over time, resulting in inaccurate motion estimation
results. While four VIO systems perform relatively well in the
previous walking sequences, this is not the case in the more
challenging vehicular test, which is not ofﬁcially supported by
any of the tested VIO devices. Only ARKit is able to produce
stable motion tracking results even in a vehicular test.

We conduct an additional vehicular test driving the same
trajectory repeatedly in a dark underground parking lot with
poor visual conditions as shown in Fig. 8. The total traveling
distance is about 450 m, and we drive the car at a low speed
from 5 to 15 km/h. Although ARKit does not restore the actual
movements perfectly in the parking lot, ARKit (red) shows the

Fig. 6. Comparison of four VIO systems on multi-story stairs from the 2nd
basement ﬂoor (B2) to the 5th ﬂoor (5F). It shows the side (left), front (right-
bottom), and top (right-top) views of the estimated VIO trajectories. ARKit
has the most consistent camera motions along with the shape of the stairs,
and only matches the start and end points marked in the black circle.

similar to ARKit and ARcore’s results, however, the scale
of the estimated path of T265 is smaller than the actual
movements. T265 suffers from a scale inconsistency problem,
which is generally observed in monocular visual odometry
conﬁguration. The orthogonality of ZED2 (magenta) is broken

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

6

Fig. 7. Estimated motion trajectories of four proprietary VIO systems in an outdoor campus (left) and urban outdoor roads (right) sequences overlaid on
Google Maps. We start and end at the same point marked in the black circle to check loop closing performance. ARKit (red) tracks the 6-DoF camera
poses well following the shape of the roads on Google Maps most consistently and accurately. Only ARKit (red) is able to produce stable motion tracking
performance even when driving a vehicle over 60 km/h (right).

in realistic use cases where people and vehicles are very
crowded not only indoors but also outdoors.

Google ARCore exhibits accurate and consistent motion
tracking performance next to ARKit. ARCore works well for
indoor sequences and the motion data collected by a walking
person, but
it diverges or the VIO algorithm deteriorates
sharply when moving rapidly or in poor lighting conditions.
Intel RealSense T265 shows good positioning performance
just behind Google ARCore. T265 operates 6-DoF motion
tracking indoors not badly, however, it has a problem of scale
inconsistency issue when estimating the moving path larger or
smaller than the scale of the actual movements. Also, T265’s
motion tracking sometimes fails if the moving speed is too
slow or fast.

The motion tracking performance of Stereolabs ZED 2 is
the most inconsistent and inaccurate among the four VIO de-
vices for indoors and outdoors. As the 6-DoF motion tracking
progresses, the rotational error occurs most severely, and this
rotation error accumulates over time, resulting in an incorrect
path in which the starting and ending points are very different.
In particular, ZED2 exhibits a tendency that it cannot track a
straight path correctly when we actually move in a straight
line outdoors, and rotational drift error is more severe when
moving fast.

VI. CONCLUSION

Fig. 8. Example paths in the underground parking lot overlaid on the ﬂoorplan
to evaluate the consistency and accuracy. The trajectories of ARKit (red)
overlap signiﬁcantly, but the paths of the other VIO devices suffer from a
rotational drift, showing inaccurate and inconsistent positioning results.

overlapped, consistent motion estimation results while other
VIO systems gradually diverge from the initially estimated
loop. Since we perform the evaluation at a relatively low
speed (10 km/h) compared to the previous vehicle test (60
km/h), other VIO systems do not diverge or fail at all. Among
four VIO methods, the ZED2 positioning results are the most
deviating from the actual movements in the underground
parking lot.

V. DISCUSSION

Overall, Apple ARKit demonstrates the most consistent,
accurate, reliable, and stable motion tracking results among the
four VIO systems across both indoor and outdoor uses. ARKit
performs well and robustly in various real-world challeng-
ing environments such as sudden camera movements, abrupt
changes in illumination, and high-speed movements with very
rare cases where tracking failure or motion jump occurs.
ARKit achieves accurate and robust positioning performance

We have conducted a survey of the 6-DoF ego-motion
tracking performance of four off-the-shelf proprietary VIO
platforms in challenging indoor and outdoor environments.
To the best of our knowledge, this is the ﬁrst back-to-back
comparison of ARKit, ARCore, T265, and ZED2, demon-
strating that Apple ARKit performs well and robustly in most
indoor and outdoor scenarios. We hope that the results and
conclusions presented in this paper may help members of the
research community in ﬁnding appropriate VIO platforms for
their robotic systems and applications.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

7

REFERENCES

[1] A. I. Mourikis and S. I. Roumeliotis, “A multi-state constraint kalman

ﬁlter for vision-aided inertial navigation,” in IEEE ICRA, 2007.

[2] S. Leutenegger, P. Furgale, V. Rabaud, M. Chli, K. Konolige, and
R. Siegwart, “Keyframe-based visual-inertial slam using nonlinear op-
timization,” Proceedings of Robotis Science and Systems (RSS) 2013,
2013.

[3] T. Qin, P. Li, and S. Shen, “Vins-mono: A robust and versatile monocular

visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.

[4] “Apple ARKit,” https://developer.apple.com/documentation/arkit/, Ac-

cessed: 2022-02-22.

[5] “Google ARCore,” https://developers.google.com/ar, Accessed: 2022-

02-22.

[6] T. Rouˇcek, M. Pecka, P.

ˇC´ıˇzek, T. Petˇr´ıˇcek, J. Bayer, V. ˇSalansk`y,
D. Heˇrt, M. Petrl´ık, T. B´aˇca, V. Spurn`y, et al., “Darpa subterranean
challenge: Multi-robotic exploration of underground environments,” in
International Conference on Modelling and Simulation for Autonomous
Systems. Springer, 2019.

[7] P. Root, “Fast lightweight autonomy (ﬂa),” Defense Advanced Research
Projects Agency, https://www. darpa. mil/program/fast-lightweight-
autonomy [retrieved 31 Dec. 2018], 2021.

[8] A. Flint, O. Naroditsky, C. P. Broaddus, A. Grygorenko, S. Roumeliotis,
and O. Bergig, “Visual-based inertial navigation,” Dec. 11 2018, US
Patent 10,152,795.

[9] A. I. Mourikis, N. Trawny, S. I. Roumeliotis, A. E. Johnson, A. Ansar,
and L. Matthies, “Vision-aided inertial navigation for spacecraft entry,
descent, and landing,” IEEE Transactions on Robotics, 2009.

[10] E. Nerurkar, S. Lynen, and S. Zhao, “System and method for concurrent

odometry and mapping,” Oct. 13 2020, US Patent 10,802,147.

[11] “Intel RealSense Tracking Camera T265,” https://www.intelrealsense.

com/tracking-camera-t265/, Accessed: 2022-02-22.

[12] “Stereolabs ZED 2 Stereo Camera,” https://www.stereolabs.com/zed-2/,

Accessed: 2022-02-22.

[13] M. Bloesch, S. Omari, M. Hutter, and R. Siegwart, “Robust visual
inertial odometry using a direct ekf-based approach,” in 2015 IEEE/RSJ
international conference on intelligent robots and systems (IROS), 2015.
[14] R. Mur-Artal and J. D. Tard´os, “Orb-slam2: An open-source slam
system for monocular, stereo, and rgb-d cameras,” IEEE transactions
on robotics, 2017.

[15] J. Engel, V. Koltun, and D. Cremers, “Direct sparse odometry,” IEEE
transactions on pattern analysis and machine intelligence, 2017.
[16] J. Delmerico and D. Scaramuzza, “A benchmark comparison of monoc-

ular VIO algorithms for ﬂying robots,” in IEEE ICRA, 2018.

[17] C. Campos, R. Elvira, J. J. G. Rodr´ıguez, J. M. Montiel, and J. D. Tard´os,
“Orb-slam3: An accurate open-source library for visual, visual–inertial,
and multimap slam,” IEEE Transactions on Robotics, 2021.

[18] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W.
Achtelik, and R. Siegwart, “The euroc micro aerial vehicle datasets,”
The International Journal of Robotics Research, vol. 35, no. 10, pp.
1157–1163, 2016.

[19] S. Cort´es, A. Solin, E. Rahtu, and J. Kannala, “ADVIO: An authentic

dataset for visual-inertial odometry,” in ECCV, 2018.

[20] A. Alapetite, Z. Wang, and M. Patalan, “Comparison of three off-the-

shelf visual odometry systems,” Robotics, 2020.

[21] S. Ouerghi, N. Ragot, and X. Savatier, “Comparative study of a com-
mercial tracking camera and ORB-SLAM2 for person localization,” in
VISAPP, 2020.

[22] Y. Ling, L. Bao, Z. Jie, F. Zhu, Z. Li, S. Tang, Y. Liu, W. Liu, and
T. Zhang, “Modeling varying camera-imu time offset in optimization-
based visual-inertial odometry,” in Proceedings of the European Con-
ference on Computer Vision (ECCV), 2018.

[23] H. G¨umg¨umc¨u, “Evaluation framework for proprietary slam systems
exempliﬁed on google arcore,” Master’s thesis, ETH Zurich, 2019.
[24] E. D. Nerurkar, K. J. Wu, and S. I. Roumeliotis, “C-klam: Constrained
keyframe-based localization and mapping,” in 2014 IEEE international
conference on robotics and automation (ICRA).

[25] E. Marder-Eppstein, “Project tango,” in ACM SIGGRAPH 2016 Real-

Time Live!, 2016, pp. 25–25.

[26] R. Bonatti, R. Madaan, V. Vineet, S. Scherer, and A. Kapoor, “Learning
visuomotor policies for aerial navigation using cross-modal representa-
tions,” in 2020 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS).

[27] R. Fan, J. Jiao, J. Pan, H. Huang, S. Shen, and M. Liu, “Real-time
dense stereo embedded in a uav for road inspection,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
Workshops, 2019.

","JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 An Empirical Evaluation of Four Off-the-Shelf Proprietary Visual-Inertial Odometry Systems Jungha Kim, Minkyeong Song, Yeoeun Lee, Moonkyeong Jung, and Pyojin Kim∗ 2 2 0 2 l u J 4 1 ] O R . s c [ 1 v 0 8 7 6 0 . 7 0 2 2 : v i X r a Abstract—Commercial visual-inertial odometry (VIO) systems have been gaining attention as cost-effective, off-the-shelf six degrees of freedom (6-DoF) ego-motion tracking methods for estimating accurate and consistent camera pose data, in addition localization from to their ability to operate without external is unclear motion capture or global positioning systems. It from existing results, however, which commercial VIO platforms are the most stable, consistent, and accurate in terms of state estimation for indoor and outdoor robotic applications. We assess four popular proprietary VIO systems (Apple ARKit, Google ARCore, Intel RealSense T265, and Stereolabs ZED 2) through a series of both indoor and outdoor experiments where we show their positioning stability, consistency, and accuracy. We present our complete results as a benchmark comparison for the research community. Index Terms—Commercial visual-inertial odometry, Apple ARKit, Google ARCore, Intel T265, Stereolabs ZED 2 I. INTRODUCTION T HIS article presents a benchmark comparison of off-the- shelf proprietary visual-inertial odometry (VIO) systems used for autonomous navigation of robotic applications, which are the process of determining the position and orientation of a camera-inertial measurement unit (IMU)-rig in 3D space by analyzing the associated camera images and IMU data. As the VIO research has reached a level of maturity, there exist several open published VIO methods such as MSCKF [1], OKVIS [2], VINS-Mono [3], and many commercial prod- ucts utilize closed proprietary VIO algorithms such as Apple ARKit [4], Google ARCore [5] that offer off-the-shelf VIO pipelines which can be employed on an end-user’s system of choice. The current research studies provide some comparative experiments on the performance of the popular VIO ap- proaches, however, they consider only a subset of the existing open-source and proprietary VIO algorithms, and conduct insufﬁcient performance evaluation only on publicly-available datasets rather than indoor and outdoor challenging real-world environments. In particular, although commercial VIO systems (Intel T265, Stereolabs ZED 2) play an important role in the several DARPA challenges [6], [7] and many commercial products or apps (Pok´emon GO, IKEA Place AR), there is a lack of research for benchmarking the positioning accuracy of these closed proprietary VIO platforms. The motivation of this paper is to address this deﬁciency by performing a comprehensive evaluation of off-the-shelf All authors are with Department Systems Engineering, Sookmyung Women’s University, Seoul, South Korea. {alice3071,smk615,snwfry,jmk7791,pjinkim}@ sookmyung.ac.kr (∗ Corresponding author: Pyojin Kim) of Mechanical Fig. 1. The custom-built capture rig for benchmarking 6-DoF motion tracking performance of Apple ARKit (iPhone 12 Pro Max), Google ARCore (LG V60 ThinQ), Intel RealSense T265, and Stereolabs ZED 2. commercially-available VIO systems in challenging indoor and outdoor environments as shown in Fig. 2. This is the ﬁrst comparative study on four popular proprietary VIO systems in six challenging real-world environments, both indoors and outdoors. Especially, we select the following four proprietary VIO systems that are frequently used in autonomous driving robotic applications: • Apple ARKit [4] - Apple’s augmented reality (AR) plat- form, which includes ﬁltering-based VIO algorithms [8] to enable iOS devices to sense how they move in 3D space. • Google ARCore [5] - Google’s AR platform utilizing a multi-state constraint Kalman ﬁlter (MSCKF) style VIO algorithm [1], [9], called concurrent odometry and mapping (COM) [10]. • Intel RealSense T265 [11] - a stand-alone VIO and simultaneous localization and mapping (SLAM) tracking device developed for use in robotics, drones, and more, with all position computations performed on the device. • Stereolabs ZED 2 [12] - a hand-held stereo camera with built-in IMU for neural depth sensing and visual-inertial stereo, requiring an external NVIDIA GPU to obtain the 6-DoF camera poses. We do not consider open-source published VIO methods and non-inertial visual simultaneous localization and map- ping (SLAM) algorithms, for example ROVIO [13], VINS- Mono [3], ORB-SLAM [14], and DSO [15]. We focus on the off-the-shelf commercial VIO/SLAM products that might be of interest to more researchers and engineers because open pub- lished VIO algorithms are relatively difﬁcult to understand and operate, and their comparisons are made in the literature [16], [17] to some extent. figure_capture_rig.pdf Google ARCore (LG V60 ThinQ) Apple ARKit (iPhone 12 Pro Max) Intel RealSense T265 Stereolabs ZED 2 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2 Fig. 2. Accumulated 3D point cloud (middle) with the estimated 6-DoF trajectory (red) from Apple ARKit in multi-ﬂoor environments. We capture the 6-DoF camera poses and 3D points while climbing the multi-story stairs (left). Among the four proprietary VIO systems, Apple ARKit shows the most consistent and accurate 6-DoF motion tracking results, consistently reconstructing the 3D geometry of stairs and hallways. The Apple ARKit track (red) and 3D reconstruction results have a similar shape as the ground-truth blueprint of a building (right). Our experiments are conducted in six challenging indoor and outdoor environments with the custom-built rig equipped with the four VIO devices as illustrated in Fig. 1. Our test sequences contain long and narrow corridors, large open spaces, repetitive stairways, an underground parking lot with insufﬁcient lighting, and about 3.1 kilometers of a vehicular test in complex urban trafﬁc environments. test Our goal is to provide a thorough benchmark of closed proprietary VIO systems, in order to provide a reference for researchers on VIO products, as well as readers who require an off-the-shelf 6-DoF state estimation solution that is suitable for their robotic platforms and autonomous vehicles. II. RELATED WORK Despite proprietary VIO systems being utilized in many products and areas for industrial usage (e.g., for building an accurate indoor map, as a precise positioning system, etc.), there is no benchmark study that satisﬁes our proposed goals. While comprehensive comparisons of open-source published VIO methods exist [16], they focus only on evaluating the popular academic VIO algorithms on the EuRoC micro aerial vehicle dataset [18], and do not cover off-the-shelf proprietary VIO systems and various indoor and outdoor environments. Although ADVIO [19] presents a VIO comparison including three proprietary platforms and two academic approaches, its main contribution is to develop a set of RGB and IMU smartphone datasets, not a performance evaluation between the proprietary VIO platforms. In [20], [21], some comparative studies of proprietary VIO systems have been performed, they consider only a few proprietary VIO platforms. but Performance evaluation is only conducted in a simple 2D indoor environment with a short camera moving distance. Since we focus on the 6-DoF positioning accuracy of the proprietary VIO systems, we can instead consider the existing results relevant to this problem. The proposed VIO approach in [22] compares to Google ARCore and VINS-Mono [3], but only on a few indoor sequences with very little camera movement. The evaluation framework in [23] assesses the 6- DoF motion tracking performance of ARCore with the ground truth under several circumstances, but they lack comparative results for other proprietary VIO systems such as ARKit and T265, and detailed analyses are performed only for ARCore. Most important is that no existing work considers an in- door/outdoor performance evaluation for four popular propri- etary VIO systems that are frequently deployed on robotic applications, AR/VR apps, and industrial usages. Our test se- quences are authentic and illustrate realistic use cases, contain- ing challenging environments with scarce or repetitive visual features, both indoors and outdoors, and varying motions from walking to driving camera movements. They also include rapid rotations without translation as they are problematic motions for many VIO/SLAM algorithms. III. VISUAL-INERTIAL ODOMETRY SYSTEMS We brieﬂy summarize the primary features of four off- the-shelf proprietary VIO systems based on data published on the relevant ofﬁcial websites, papers, GitHub, and patent documents, and how to collect 6-DoF pose estimates from each VIO mobile device. Since most proprietary VIO/SLAM platforms are all closed-source, we do not cover the detailed VIO academic backgrounds and implementations. A. Apple ARKit Apple ARKit [4] is Apple’s augmented reality (AR) soft- ware framework, which includes a tightly-coupled ﬁltering- based VIO algorithm similar to the MSCKF [1] to enable iOS devices to sense how they move in 3D space. It contains a sliding window ﬁlter, bundle adjustment, motion/structure marginalization modules [8], and is expected to be applied to various robotic applications such as Apple Glasses and Car in the future, not just for iPhone and iPad, which is why we conduct vehicle tests in this benchmark. We develop a custom iOS data collection app1 for capturing ARKit 6-DoF camera poses, RGB image sequences, and IMU measurements using an iPhone 12 Pro Max running iOS 14.7.1. It saves the pose estimates as a translation vector and a unit quaternion at 60 Hz, 1https://github.com/PyojinKim/ios logger figure_front_page.pdf Floor 5 Floor 4 Floor 3 Floor 2 Floor 1 19.2 m 13.2 m 9.2 m 4.6 m 0.0 m JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3 and each pose is expressed in a global coordinate frame created by the phone when starting iOS data collection. Although there are various iPhone and iPad models, the core VIO algorithm in ARKit is the same, thus we empirically conﬁrm that there is little difference in the VIO performance of each device. B. Google ARCore ARCore [5] is Google’s platform for building AR ex- periences utilizing the multi-state constraint Kalman ﬁlter (MSCKF) style VIO/SLAM algorithms [9], [24] with many subsequent variations, called concurrent odometry and map- ping (COM) [10]. ARCore is a successor to Google Project Tango [25], and is currently applied only to Android OS smartphones, but it would be extended to various robotic platforms such as Google Wing, Maps, and Waymo, which is why we evaluate ARCore in a large-scale outdoor sequence of about 3.1 kilometers of a vehicular test. We build a custom Android OS app based on Google’s ARCore example2 to acquire ARCore 6-DoF camera poses and IMU measurements at 30 Hz with an LG V60 ThinQ running Android 10.0.0 and ARCore 1.29. Although there are various Android OS devices such as Samsung Galaxy and Google Pixel, smartphones on the list3 certiﬁed by Google demonstrate similar motion tracking performance regardless of device model. C. Intel RealSense T265 a hassle-free Intel RealSense T265 stand-alone is VIO/SLAM device to track its own position and orientation in 3D space. The embedded processor, vision processing unit (VPU), runs the entire VIO algorithm onboard, analyzes the image sequences from stereo ﬁsheye cameras and fuses all sensor information together. Since the T265 VIO algorithm runs on the device itself without using the resource of the host computer, is widely used as a 6-DoF positioning sensor in 3D space for various robotic applications such as DARPA challenges [6] and autonomous ﬂying drones [26]. We collect the 6-DoF motion tracking results at 200 Hz using Intel RealSense SDK 2.04, and save the T265 6-DoF camera poses by connecting it to an Intel NUC mini PC. it D. Stereolabs ZED 2 Stereolabs ZED 2 is a hand-held stereo camera with built-in IMU for neural depth sensing, 6-DoF VIO/SLAM, and real- time 3D mapping. Stereolabs has not made their VIO/SLAM algorithm public, and the description of the VIO algorithm is relatively vague compared to other proprietary VIO systems. It is one of the popular stereo camera sensors for various robotic applications such as drone inspection [27], but has the disadvantage of requiring an external NVIDIA GPU to perform positional tracking and neural depth sensing. We develop a program to collect the ZED 2 6-DoF camera poses at 30 Hz based on ZED SDK 3.5.25 on an NVIDIA Jetson Nano onboard computer. 2https://github.com/rfbr/IMU and pose Android Recorder 3https://developers.google.com/ar/devices 4https://github.com/IntelRealSense/librealsense 5https://www.stereolabs.com/developers/release/ Fig. 3. We carry the capture rig by hand, and store the onboard computers and batteries to collect the motion data indoors (left). In the outdoor vehicular tests, we ﬁx the capture rig to the front passenger seat (right). IV. EXPERIMENTS We evaluate four proprietary VIO systems with the four devices (iPhone 12 Pro Max, LG V60 ThinQ, Intel T265, ZED2) attached to the custom-built capture rig as shown in Fig. 1 and Fig. 3 on the large-scale challenging indoor and outdoor environments, both qualitatively and quantitatively. Indoors, we record the motion data by a walking person, and outdoors, the data is collected by rigidly attaching the capture rig to a car in Fig. 3. We save the 6-DoF pose estimates of ARKit and ARCore through the custom apps on each smartphone device, and record the moving trajectories of T265 and ZED2 in the Intel NUC and NVIDIA Jetson Nano onboard computers. We maintain the default parameter settings of each VIO platform, and deactivate all capabilities related to SLAM (e.g., loop closure) for a fair comparison between each VIO system. Furthermore, in order to interpret the motion tracking results in the same reference coordinate frame, we calibrate the intrinsic and extrinsic parameters of all cameras by capturing multiple views of a checkerboard. Our benchmark dataset contains various indoor and outdoor sequences in six different locations, and the total length of each sequence ranges from 83 m to 3051 m, which are primarily designed for benchmarking medium and long-range VIO performance. There are three indoor and three outdoor sequences, and all indoor sequences are captured in a 7-story building in the university campus including long corridors, open hallway spaces, and stair climbs as shown in the top row of Fig. 4. The indoor cases are as realistic as possible contain- ing repetitive motion in stairs, temporary occlusions, and areas lacking visual features. The bottom row of Fig. 4 illustrates example frames from three outdoor sequences acquired in the outdoor university campus, underground parking lot, and urban outdoor roads. In order to evaluate the performance of each VIO system quantitatively without an external motion capture system, we coincide the start and end points of the movement trajectories in all experiments, and measure the ﬁnal drift error (FDE) metric, which is the end point position error in meter. We report the quantitative evaluation results of four VIO systems in Table I. The smallest end point position error for each sequence is indicated in bold. The ideal FDE value (the ground-truth path) should be 0, and a large FDE value denotes an inaccurate position estimate since we deﬁne the starting point of movement as the origin. In addition, by overlaying the estimated VIO trajectories on the ﬂoorplan of the building figure_external_view.pdf Google ARCore Apple ARKit Intel T265 ZED 2 Laptop Google ARCore Apple ARKit Battery Intel NUC & NVIDIA Jetson Nano Stereolabs ZED 2 Intel RealSense T265 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 Fig. 4. Example image frames from indoor and outdoor benchmark datasets. The top row represents three indoor sequences by foot including long corridors (a), open hallway spaces (b), and repetitive stairs (c) from a university building. We acquire the camera motion data in the outdoor campus on foot (d), and through a car in the underground parking lot (e) and urban outdoor roads (f). TABLE I EVALUATION RESULTS (FDE) OF FOUR PROPRIETARY VIO SYSTEMS Experiment ARKit ARCore T265 ZED 2 Length (m) Indoor Corridor Indoor Hallway Indoor Stairs Outdoor Campus Parking Lot Outdoor Roads 0.79 0.14 0.19 2.01 0.26 2.68 0.12 0.09 3.98 0.07 1.14 140.08 1.88 0.61 1.49 4.08 9.01 × 1.44 4.58 4.76 206.38 10.85 409.25 145.21 83.98 114.13 513.81 446.26 3051.61 or Google Maps, we evaluate the consistency, stability, and reliability of each VIO system qualitatively. A. Indoor Long Corridors and Open Hallway Sequences We evaluate four VIO systems in a long U-shape corridor and open hallway spaces easily found in typical ofﬁce and uni- versity buildings as shown in Fig. 5. Fig. 4 (a) and (b) illustrate example frames from both locations. The trajectories of these sequences are approximately 145 and 84 meters, and include 5 and 11 pure rotational movements and difﬁcult textures. In a long U-shape corridor and open hallway sequences, the start and end points of ARKit (red) meet at the black circle without a severe rotational drift while maintaining the orthogonality and scale of the estimated trajectory well compared with the ﬂoorplan. Although ARCore (green) shows the most accurate results in terms of the FDE metric in Table I, the estimated VIO trajectory does not match the ﬂoorplan well. Intel T265 (blue) estimates accurate 3-DoF rotational motion well, but there is a problem with the scale of the moving trajectory compared to the ﬂoorplan, showing a little larger trajectory than the actual movements. ZED2 (magenta) presents the most inaccurate and inconsistent positioning performance among the four VIO methods as the rotational motion drift error gradually accumulates over time. Overall, the estimated VIO trajectories by ARKit (red) are the most similar and consistent motion tracking results to the actual movements following the shape of the corridor on the ﬂoorplan. B. Indoor Multistory Stairs Sequence We perform a comparative experiment in a multi-ﬂoor staircase environment with the 114 m trajectory going up the stairs from the 2nd basement ﬂoor (B2) to the 5th ﬂoor (5F) of a building in Fig. 6. The repetitive rotational motion included in the 3D trajectory of climbing the stairs makes VIO positioning challenging. Fig. 4 (c) shows example frames from multistory stair sequence. In the top view (xy-plane), we start and end at the same points marked in the black circle to check loop closing in the estimated VIO trajectories. ARKit (red) has the best performance; the top and side views of ARKit (red) show the overlapped, consistent 6-DoF motion tracking results while other VIO systems gradually diverge from the initially estimated loop. With ARKit (red), the starting and ending points in xy-plane (top view) nearly match; for the others, they do not. The ﬁnal drift error (FDE) of ARKit in xy-plane is 0.19 m, while ARCore, T265, and ZED2 are 3.98 m, 1.49 m, and 4.76 m, respectively. In particular, ZED2 (magenta) has the most severe trajectory distortion in the z-axis direction (height) among the four VIO systems. Fig. 6 illustrates the side and front views of the stairway with the paths from four VIO devices, showing high consistency of ARKit (red) compared to other VIO platforms. It is noteworthy that the height of each ﬂoor estimated by ARKit and the actual height (the ground- truth) from the building blueprint are approximately identical. C. Outdoor University Campus Sequence We choose an outdoor location in the university campus approximately 513 m to determine which VIO system works well in the environment of the rapid change of the topogra- phy, and the narrow road returning as shown in the left of Fig. 7. Example frames are shown in Fig. 4 (d). It shows the resulting 6-DoF trajectories from four VIO platforms overlaid on Google Maps, demonstrating that the start and end points of ARKit (red) and ARCore (green) meet while matching well with the shape of the roads shown on Google Maps. The shape of the estimated trajectory of T265 (blue) is very figure_example_frames.pdf (a) Indoor Corridor (b) Indoor Hallway (c) Indoor Stairs (d) Outdoor Campus (e) Underground Parking Lot (f) Urban Outdoor Roads JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5 Fig. 5. Estimated trajectories with four proprietary VIO systems in a long U-shape corridor (left) and open hallway space (right) sequences. We start and end at the same point marked in the black circle to evaluate the loop closing performance of tested commercial VIO systems. The estimated paths for ARKit (red) match the building ﬂoorplan most consistently, and only the starting and ending points of ARKit nearly meet; for the others, they do not. due to its inaccurate rotation estimation, showing the most severe distortion of the actual movement trajectory among the four VIO systems as shown in the left of Fig. 7. D. Outdoor Urban Roads and Parking Lot Sequences We perform an outdoor vehicle driving experiment with a mileage of approximately 3 km by attaching the capture rig to the vehicle as shown in the right of Fig. 7. Fig. 4 (e) and (f) show example frames from the underground parking lot and urban outdoor roads. We acquire the motion data while driving on public automobile roads near Seoul Station in Seoul, and there are plenty of moving people, cars, and occasional large vehicles visible in the outdoor environments, which makes mo- tion tracking of VIO challenging. Even in high-speed driving conditions, sometimes exceeding 60 km/h, ARKit (red) shows surprisingly accurate and consistent 6-DoF motion tracking results overlaid on Google Maps as shown in the right of Fig. 7. The start and end points of ARKit (red) accurately meet in the black circle, and the ﬁnal drift error (FDE) is only 2.68 m in Table I. ARCore (green) occasionally fails when the speed of the car increases or the light variations occur abruptly. In T265 (blue), if the car stops temporarily due to a stop signal or is driving too fast, the VIO algorithm diverges and fails to estimate the location. ZED2 (magenta) accumulates rotational drift error over time, resulting in inaccurate motion estimation results. While four VIO systems perform relatively well in the previous walking sequences, this is not the case in the more challenging vehicular test, which is not ofﬁcially supported by any of the tested VIO devices. Only ARKit is able to produce stable motion tracking results even in a vehicular test. We conduct an additional vehicular test driving the same trajectory repeatedly in a dark underground parking lot with poor visual conditions as shown in Fig. 8. The total traveling distance is about 450 m, and we drive the car at a low speed from 5 to 15 km/h. Although ARKit does not restore the actual movements perfectly in the parking lot, ARKit (red) shows the Fig. 6. Comparison of four VIO systems on multi-story stairs from the 2nd basement ﬂoor (B2) to the 5th ﬂoor (5F). It shows the side (left), front (right- bottom), and top (right-top) views of the estimated VIO trajectories. ARKit has the most consistent camera motions along with the shape of the stairs, and only matches the start and end points marked in the black circle. similar to ARKit and ARcore’s results, however, the scale of the estimated path of T265 is smaller than the actual movements. T265 suffers from a scale inconsistency problem, which is generally observed in monocular visual odometry conﬁguration. The orthogonality of ZED2 (magenta) is broken JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6 Fig. 7. Estimated motion trajectories of four proprietary VIO systems in an outdoor campus (left) and urban outdoor roads (right) sequences overlaid on Google Maps. We start and end at the same point marked in the black circle to check loop closing performance. ARKit (red) tracks the 6-DoF camera poses well following the shape of the roads on Google Maps most consistently and accurately. Only ARKit (red) is able to produce stable motion tracking performance even when driving a vehicle over 60 km/h (right). in realistic use cases where people and vehicles are very crowded not only indoors but also outdoors. Google ARCore exhibits accurate and consistent motion tracking performance next to ARKit. ARCore works well for indoor sequences and the motion data collected by a walking person, but it diverges or the VIO algorithm deteriorates sharply when moving rapidly or in poor lighting conditions. Intel RealSense T265 shows good positioning performance just behind Google ARCore. T265 operates 6-DoF motion tracking indoors not badly, however, it has a problem of scale inconsistency issue when estimating the moving path larger or smaller than the scale of the actual movements. Also, T265’s motion tracking sometimes fails if the moving speed is too slow or fast. The motion tracking performance of Stereolabs ZED 2 is the most inconsistent and inaccurate among the four VIO de- vices for indoors and outdoors. As the 6-DoF motion tracking progresses, the rotational error occurs most severely, and this rotation error accumulates over time, resulting in an incorrect path in which the starting and ending points are very different. In particular, ZED2 exhibits a tendency that it cannot track a straight path correctly when we actually move in a straight line outdoors, and rotational drift error is more severe when moving fast. VI. CONCLUSION Fig. 8. Example paths in the underground parking lot overlaid on the ﬂoorplan to evaluate the consistency and accuracy. The trajectories of ARKit (red) overlap signiﬁcantly, but the paths of the other VIO devices suffer from a rotational drift, showing inaccurate and inconsistent positioning results. overlapped, consistent motion estimation results while other VIO systems gradually diverge from the initially estimated loop. Since we perform the evaluation at a relatively low speed (10 km/h) compared to the previous vehicle test (60 km/h), other VIO systems do not diverge or fail at all. Among four VIO methods, the ZED2 positioning results are the most deviating from the actual movements in the underground parking lot. V. DISCUSSION Overall, Apple ARKit demonstrates the most consistent, accurate, reliable, and stable motion tracking results among the four VIO systems across both indoor and outdoor uses. ARKit performs well and robustly in various real-world challeng- ing environments such as sudden camera movements, abrupt changes in illumination, and high-speed movements with very rare cases where tracking failure or motion jump occurs. ARKit achieves accurate and robust positioning performance We have conducted a survey of the 6-DoF ego-motion tracking performance of four off-the-shelf proprietary VIO platforms in challenging indoor and outdoor environments. To the best of our knowledge, this is the ﬁrst back-to-back comparison of ARKit, ARCore, T265, and ZED2, demon- strating that Apple ARKit performs well and robustly in most indoor and outdoor scenarios. We hope that the results and conclusions presented in this paper may help members of the research community in ﬁnding appropriate VIO platforms for their robotic systems and applications. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7 REFERENCES [1] A. I. Mourikis and S. I. Roumeliotis, “A multi-state constraint kalman ﬁlter for vision-aided inertial navigation,” in IEEE ICRA, 2007. [2] S. Leutenegger, P. Furgale, V. Rabaud, M. Chli, K. Konolige, and R. Siegwart, “Keyframe-based visual-inertial slam using nonlinear op- timization,” Proceedings of Robotis Science and Systems (RSS) 2013, 2013. [3] T. Qin, P. Li, and S. Shen, “Vins-mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018. [4] “Apple ARKit,” https://developer.apple.com/documentation/arkit/, Ac- cessed: 2022-02-22. [5] “Google ARCore,” https://developers.google.com/ar, Accessed: 2022- 02-22. [6] T. Rouˇcek, M. Pecka, P. ˇC´ıˇzek, T. Petˇr´ıˇcek, J. Bayer, V. ˇSalansk`y, D. Heˇrt, M. Petrl´ık, T. B´aˇca, V. Spurn`y, et al., “Darpa subterranean challenge: Multi-robotic exploration of underground environments,” in International Conference on Modelling and Simulation for Autonomous Systems. Springer, 2019. [7] P. Root, “Fast lightweight autonomy (ﬂa),” Defense Advanced Research Projects Agency, https://www. darpa. mil/program/fast-lightweight- autonomy [retrieved 31 Dec. 2018], 2021. [8] A. Flint, O. Naroditsky, C. P. Broaddus, A. Grygorenko, S. Roumeliotis, and O. Bergig, “Visual-based inertial navigation,” Dec. 11 2018, US Patent 10,152,795. [9] A. I. Mourikis, N. Trawny, S. I. Roumeliotis, A. E. Johnson, A. Ansar, and L. Matthies, “Vision-aided inertial navigation for spacecraft entry, descent, and landing,” IEEE Transactions on Robotics, 2009. [10] E. Nerurkar, S. Lynen, and S. Zhao, “System and method for concurrent odometry and mapping,” Oct. 13 2020, US Patent 10,802,147. [11] “Intel RealSense Tracking Camera T265,” https://www.intelrealsense. com/tracking-camera-t265/, Accessed: 2022-02-22. [12] “Stereolabs ZED 2 Stereo Camera,” https://www.stereolabs.com/zed-2/, Accessed: 2022-02-22. [13] M. Bloesch, S. Omari, M. Hutter, and R. Siegwart, “Robust visual inertial odometry using a direct ekf-based approach,” in 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS), 2015. [14] R. Mur-Artal and J. D. Tard´os, “Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras,” IEEE transactions on robotics, 2017. [15] J. Engel, V. Koltun, and D. Cremers, “Direct sparse odometry,” IEEE transactions on pattern analysis and machine intelligence, 2017. [16] J. Delmerico and D. Scaramuzza, “A benchmark comparison of monoc- ular VIO algorithms for ﬂying robots,” in IEEE ICRA, 2018. [17] C. Campos, R. Elvira, J. J. G. Rodr´ıguez, J. M. Montiel, and J. D. Tard´os, “Orb-slam3: An accurate open-source library for visual, visual–inertial, and multimap slam,” IEEE Transactions on Robotics, 2021. [18] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W. Achtelik, and R. Siegwart, “The euroc micro aerial vehicle datasets,” The International Journal of Robotics Research, vol. 35, no. 10, pp. 1157–1163, 2016. [19] S. Cort´es, A. Solin, E. Rahtu, and J. Kannala, “ADVIO: An authentic dataset for visual-inertial odometry,” in ECCV, 2018. [20] A. Alapetite, Z. Wang, and M. Patalan, “Comparison of three off-the- shelf visual odometry systems,” Robotics, 2020. [21] S. Ouerghi, N. Ragot, and X. Savatier, “Comparative study of a com- mercial tracking camera and ORB-SLAM2 for person localization,” in VISAPP, 2020. [22] Y. Ling, L. Bao, Z. Jie, F. Zhu, Z. Li, S. Tang, Y. Liu, W. Liu, and T. Zhang, “Modeling varying camera-imu time offset in optimization- based visual-inertial odometry,” in Proceedings of the European Con- ference on Computer Vision (ECCV), 2018. [23] H. G¨umg¨umc¨u, “Evaluation framework for proprietary slam systems exempliﬁed on google arcore,” Master’s thesis, ETH Zurich, 2019. [24] E. D. Nerurkar, K. J. Wu, and S. I. Roumeliotis, “C-klam: Constrained keyframe-based localization and mapping,” in 2014 IEEE international conference on robotics and automation (ICRA). [25] E. Marder-Eppstein, “Project tango,” in ACM SIGGRAPH 2016 Real- Time Live!, 2016, pp. 25–25. [26] R. Bonatti, R. Madaan, V. Vineet, S. Scherer, and A. Kapoor, “Learning visuomotor policies for aerial navigation using cross-modal representa- tions,” in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). [27] R. Fan, J. Jiao, J. Pan, H. Huang, S. Shen, and M. Liu, “Real-time dense stereo embedded in a uav for road inspection,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2019.","['class', 'file', 'vol', 'empirical', 'evaluation', 'proprietary', 'visualinertial', 'odometry', 'system', 'l', 'r', 'x', 'abstract', 'commercial', 'visualinertial', 'odometry', 'vio', 'system', 'gain', 'attention', 'costeffective', 'degree', 'freedom', '6dof', 'egomotion', 'tracking', 'method', 'estimate', 'accurate', 'consistent', 'camera', 'pose', 'datum', 'addition', 'localization', 'ability', 'operate', 'external', 'unclear', 'motion', 'capture', 'global', 'positioning', 'system', 'exist', 'result', 'however', 'commercial', 'vio', 'platform', 'stable', 'consistent', 'accurate', 'term', 'state', 'estimation', 'indoor', 'outdoor', 'robotic', 'application', 'assess', 'popular', 'proprietary', 'vio', 'system', 'apple', 'arcore', 'stereolab', 'ze', 'series', 'indoor', 'outdoor', 'experiment', 'show', 'positioning', 'stability', 'consistency', 'accuracy', 'present', 'complete', 'result', 'benchmark', 'comparison', 'research', 'community', 'index', 'term', 'commercial', 'visualinertial', 'odometry', 'apple', 'arkit', 'stereolab', 'ze', 'introduction', 'article', 'present', 'benchmark', 'comparison', 'offthe', 'shelf', 'proprietary', 'visualinertial', 'odometry', 'vio', 'system', 'use', 'autonomous', 'navigation', 'robotic', 'application', 'process', 'determine', 'position', 'orientation', 'camerainertial', 'measurement', 'unit', 'imurig', 'space', 'analyze', 'associated', 'camera', 'image', 'imu', 'datum', 'vio', 'research', 'reach', 'level', 'maturity', 'exist', 'several', 'open', 'publish', 'vio', 'method', 'okvis', 'vinsmono', 'many', 'commercial', 'prod', 'uct', 'utilize', 'close', 'proprietary', 'vio', 'algorithm', 'apple', 'arkit', 'arcore', 'offer', 'pipeline', 'employ', 'enduser', '’s', 'system', 'choice', 'current', 'research', 'study', 'provide', 'comparative', 'experiment', 'performance', 'popular', 'vio', 'proache', 'however', 'consider', 'subset', 'exist', 'opensource', 'proprietary', 'vio', 'algorithm', 'conduct', 'insufﬁcient', 'performance', 'evaluation', 'publiclyavailable', 'dataset', 'rather', 'indoor', 'outdoor', 'challenge', 'realworld', 'environment', 'particular', 'commercial', 'stereolab', 'ze', 'play', 'important', 'role', 'several', 'darpa', 'challenge', 'many', 'commercial', 'product', 'app', 'go', 'lack', 'research', 'benchmarke', 'positioning', 'accuracy', 'close', 'proprietary', 'vio', 'platform', 'motivation', 'paper', 'address', 'deﬁciency', 'perform', 'comprehensive', 'evaluation', 'author', 'department', 'system', 'engineering', 'woman', 'correspond', 'author', 'fig', 'custombuilt', 'capture', 'rig', 'benchmarke', '6dof', 'motion', 'track', 'performance', 'apple', 'arkit', 'iphone', 'max', 'arcore', 'lg', 'v60', 'thinq', 'realsense', 'stereolab', 'ze', 'commerciallyavailable', 'vio', 'system', 'challenge', 'indoor', 'outdoor', 'environment', 'show', 'fig', 'ﬁrst', 'comparative', 'study', 'popular', 'proprietary', 'vio', 'system', 'challenging', 'realworld', 'environment', 'indoor', 'outdoors', 'especially', 'select', 'follow', 'proprietary', 'vio', 'system', 'frequently', 'use', 'autonomous', 'drive', 'robotic', 'application', 'apple', 'arkit', 'apple', 'augment', 'reality', 'plat', 'form', 'include', 'ﬁlteringbase', 'vio', 'algorithm', 'enable', 'io', 'device', 'sense', 'move', 'space', 'arcore', 'platform', 'utilize', 'multistate', 'constraint', 'vio', 'call', 'concurrent', 'odometry', 'mapping', 'com', 'standalone', 'vio', 'simultaneous', 'localization', 'mapping', 'slam', 'tracking', 'device', 'develop', 'use', 'robotic', 'drone', 'position', 'computation', 'perform', 'device', 'stereolab', 'ze', 'handheld', 'stereo', 'camera', 'builtin', 'imu', 'neural', 'depth', 'sensing', 'visualinertial', 'stereo', 'require', 'external', 'nvidia', 'gpu', 'obtain', '6dof', 'camera', 'pose', 'consider', 'opensource', 'publish', 'vio', 'method', 'noninertial', 'visual', 'simultaneous', 'localization', 'map', 'ping', 'slam', 'algorithm', 'example', 'rovio', 'vin', 'orbslam', 'dso', 'focus', 'commercial', 'vioslam', 'product', 'interest', 'researcher', 'engineer', 'open', 'pub', 'lishe', 'vio', 'algorithm', 'relatively', 'difﬁcult', 'understand', 'operate', 'comparison', 'make', 'literature', 'extent', 'figurecapturerigpdf', 'google', 'arcore', 'lg', 'v60', 'thinq', 'apple', 'arkit', 'iphone', 'stereolab', 'ze', 'journal', 'class', 'file', 'vol', 'fig', 'accumulate', 'point', 'cloud', 'middle', 'estimate', '6dof', 'trajectory', 'red', 'apple', 'arkit', 'multiﬂoor', 'environment', 'capture', '6dof', 'camera', 'pose', 'point', 'climb', 'multistory', 'stair', 'leave', 'proprietary', 'vio', 'system', 'show', 'consistent', 'accurate', 'motion', 'track', 'result', 'consistently', 'reconstruct', '3d', 'geometry', 'stair', 'hallway', 'apple', 'arkit', 'track', 'red', '3d', 'reconstruction', 'result', 'similar', 'shape', 'groundtruth', 'blueprint', 'building', 'right', 'experiment', 'conduct', 'challenge', 'indoor', 'outdoor', 'environment', 'custombuilt', 'rig', 'equip', 'vio', 'device', 'illustrate', 'fig', 'test', 'sequence', 'contain', 'long', 'narrow', 'corridor', 'large', 'open', 'space', 'repetitive', 'stairway', 'underground', 'parking', 'lot', 'insufﬁcient', 'lighting', 'kilometer', 'vehicular', 'test', 'complex', 'urban', 'trafﬁc', 'environment', 'test', 'goal', 'provide', 'thorough', 'benchmark', 'close', 'proprietary', 'vio', 'system', 'order', 'provide', 'reference', 'researcher', 'vio', 'product', 'well', 'reader', 'require', '6dof', 'state', 'estimation', 'solution', 'suitable', 'robotic', 'platform', 'autonomous', 'vehicle', 'relate', 'work', 'proprietary', 'vio', 'system', 'utilize', 'many', 'product', 'area', 'industrial', 'usage', 'eg', 'build', 'accurate', 'indoor', 'map', 'precise', 'positioning', 'system', 'benchmark', 'study', 'satisﬁes', 'propose', 'goal', 'comprehensive', 'comparison', 'opensource', 'publish', 'vio', 'method', 'exist', 'focus', 'evaluate', 'popular', 'academic', 'vio', 'algorithm', 'euroc', 'micro', 'aerial', 'vehicle', 'dataset', 'cover', 'proprietary', 'vio', 'system', 'various', 'indoor', 'outdoor', 'environment', 'present', 'vio', 'comparison', 'include', 'proprietary', 'platform', 'academic', 'approach', 'main', 'contribution', 'develop', 'set', 'imu', 'smartphone', 'dataset', 'performance', 'evaluation', 'proprietary', 'vio', 'platform', 'comparative', 'study', 'proprietary', 'vio', 'system', 'perform', 'consider', 'proprietary', 'vio', 'platform', 'performance', 'evaluation', 'conduct', 'simple', 'indoor', 'environment', 'short', 'camera', 'move', 'distance', 'focus', 'positioning', 'accuracy', 'proprietary', 'vio', 'system', 'instead', 'consider', 'exist', 'result', 'relevant', 'problem', 'propose', 'vio', 'approach', 'compare', 'google', 'arcore', 'indoor', 'sequence', 'little', 'camera', 'movement', 'evaluation', 'framework', 'assess', 'dof', 'motion', 'track', 'performance', 'arcore', 'ground', 'truth', 'several', 'circumstance', 'lack', 'comparative', 'result', 'proprietary', 'vio', 'system', 'arkit', 'detailed', 'analysis', 'perform', 'arcore', 'important', 'exist', 'work', 'consider', 'dooroutdoor', 'performance', 'evaluation', 'popular', 'propri', 'etary', 'vio', 'system', 'frequently', 'deploy', 'robotic', 'application', 'arvr', 'app', 'industrial', 'usage', 'test', 'quence', 'authentic', 'illustrate', 'realistic', 'use', 'case', 'contain', 'e', 'challenge', 'environment', 'scarce', 'repetitive', 'visual', 'feature', 'indoor', 'outdoors', 'vary', 'motion', 'walk', 'drive', 'camera', 'movement', 'also', 'include', 'rapid', 'rotation', 'translation', 'problematic', 'motion', 'many', 'vioslam', 'algorithm', 'visualinertial', 'odometry', 'system', 'brieﬂy', 'summarize', 'primary', 'feature', 'proprietary', 'vio', 'system', 'base', 'datum', 'publish', 'relevant', 'ofﬁcial', 'website', 'paper', 'github', 'patent', 'document', 'collect', '6dof', 'pose', 'estimate', 'vio', 'mobile', 'device', 'proprietary', 'vioslam', 'platform', 'closedsource', 'cover', 'detailed', 'vio', 'academic', 'background', 'implementation', 'apple', 'arkit', 'apple', 'arkit', 'augment', 'reality', 'soft', 'ware', 'framework', 'include', 'tightlycouple', 'ﬁltering', 'base', 'similar', 'msckf', 'enable', 'io', 'device', 'sense', 'move', 'space', 'contain', 'slide', 'window', 'ﬁlter', 'bundle', 'adjustment', 'motionstructure', 'marginalization', 'module', 'expect', 'apply', 'various', 'robotic', 'application', 'apple', 'glass', 'car', 'future', 'iphone', 'ipad', 'conduct', 'vehicle', 'test', 'benchmark', 'develop', 'custom', 'io', 'datum', 'collection', 'app1', 'capture', '6dof', 'camera', 'pose', 'rgb', 'image', 'sequence', 'imu', 'measurement', 'use', 'iphone', 'max', 'run', 'io', 'save', 'pose', 'estimate', 'translation', 'vector', 'unit', 'quaternion', 'logger', 'figurefrontpagepdf', 'floor', 'floor', 'floor', 'floor', 'floor', 'class', 'file', 'vol', 'pose', 'express', 'global', 'coordinate', 'frame', 'create', 'phone', 'start', 'io', 'datum', 'collection', 'various', 'iphone', 'ipad', 'model', 'core', 'thus', 'empirically', 'conﬁrm', 'little', 'difference', 'vio', 'performance', 'device', 'arcore', 'arcore', 'platform', 'build', 'ex', 'perience', 'utilize', 'multistate', 'style', 'many', 'subsequent', 'variation', 'call', 'concurrent', 'odometry', 'map', 'ping', 'com', 'arcore', 'successor', 'currently', 'apply', 'android', 'smartphone', 'extend', 'various', 'robotic', 'platform', 'wing', 'map', 'waymo', 'evaluate', 'arcore', 'largescale', 'outdoor', 'sequence', 'kilometer', 'vehicular', 'test', 'build', 'custom', 'android', 'base', 'arcore', 'example2', 'acquire', 'arcore', '6dof', 'camera', 'pose', 'imu', 'measurement', 'hz', 'lg', 'v60', 'thinq', 'run', 'arcore', 'various', 'android', 'device', 'smartphone', 'list3', 'certiﬁed', 'demonstrate', 'similar', 'motion', 'tracking', 'performance', 'regardless', 'device', 'model', 'realsense', 'hasslefree', 'intel', 'realsense', 'standalone', 'vioslam', 'device', 'track', 'position', 'orientation', 'space', 'embed', 'processor', 'vision', 'processing', 'unit', 'vpu', 'run', 'entire', 'vio', 'onboard', 'analyze', 'image', 'sequence', 'stereo', 'ﬁsheye', 'camera', 'fuse', 'sensor', 'information', 'together', 'run', 'device', 'use', 'resource', 'host', 'computer', 'widely', 'use', 'positioning', 'sensor', 'space', 'various', 'robotic', 'application', 'darpa', 'challenge', 'autonomous', 'ﬂying', 'drone', 'collect', 'motion', 'track', 'result', 'hz', 'use', 'intel', 'realsense', 'save', '6dof', 'camera', 'pose', 'connect', 'intel', 'nuc', 'mini', 'stereolab', 'ze', 'stereolab', 'ze', 'handheld', 'stereo', 'camera', 'builtin', 'imu', 'neural', 'depth', 'sense', '6dof', 'vioslam', 'real', 'time', 'mapping', 'stereolab', 'make', 'vioslam', 'public', 'description', 'vio', 'relatively', 'vague', 'compare', 'proprietary', 'vio', 'system', 'popular', 'stereo', 'camera', 'sensor', 'various', 'robotic', 'application', 'drone', 'inspection', 'disadvantage', 'require', 'external', 'nvidia', 'gpu', 'perform', 'positional', 'tracking', 'neural', 'depth', 'sense', 'develop', 'program', 'collect', 'zed', '6dof', 'camera', 'pose', 'hz', 'base', 'pose', 'android', 'recorder', 'fig', 'carry', 'capture', 'rig', 'hand', 'store', 'onboard', 'computer', 'battery', 'collect', 'motion', 'datum', 'indoor', 'leave', 'outdoor', 'vehicular', 'test', 'ﬁx', 'capture', 'rig', 'front', 'passenger', 'seat', 'right', 'iv', 'experiment', 'evaluate', 'proprietary', 'vio', 'system', 'device', 'iphone', 'max', 'lg', 'v60', 'zed2', 'attach', 'custombuilt', 'capture', 'rig', 'show', 'fig', 'fig', 'largescale', 'challenge', 'indoor', 'outdoor', 'environment', 'qualitatively', 'quantitatively', 'indoor', 'record', 'motion', 'datum', 'walk', 'person', 'outdoors', 'datum', 'collect', 'rigidly', 'attach', 'capture', 'rig', 'car', 'fig', 'save', 'pose', 'estimate', 'arcore', 'custom', 'app', 'smartphone', 'device', 'record', 'move', 'trajectory', 'intel', 'nuc', 'computer', 'maintain', 'default', 'parameter', 'setting', 'vio', 'platform', 'deactivate', 'capability', 'relate', 'slam', 'loop', 'closure', 'fair', 'comparison', 'vio', 'system', 'furthermore', 'order', 'interpret', 'motion', 'track', 'result', 'reference', 'coordinate', 'frame', 'calibrate', 'intrinsic', 'extrinsic', 'parameter', 'camera', 'capture', 'multiple', 'view', 'checkerboard', 'benchmark', 'dataset', 'contain', 'various', 'indoor', 'outdoor', 'sequence', 'different', 'location', 'total', 'length', 'sequence', 'range', 'primarily', 'design', 'benchmarke', 'medium', 'longrange', 'vio', 'performance', 'indoor', 'outdoor', 'sequence', 'indoor', 'sequence', 'capture', 'building', 'university', 'campus', 'include', 'long', 'corridor', 'open', 'hallway', 'space', 'stair', 'climb', 'show', 'top', 'row', 'fig', 'indoor', 'case', 'realistic', 'possible', 'contain', 'e', 'repetitive', 'motion', 'stair', 'temporary', 'occlusion', 'area', 'lack', 'visual', 'feature', 'bottom', 'row', 'fig', 'illustrate', 'example', 'frame', 'outdoor', 'sequence', 'acquire', 'outdoor', 'university', 'campus', 'underground', 'parking', 'lot', 'urban', 'outdoor', 'road', 'order', 'evaluate', 'performance', 'vio', 'system', 'quantitatively', 'external', 'motion', 'capture', 'system', 'coincide', 'start', 'end', 'point', 'movement', 'trajectory', 'experiment', 'measure', 'ﬁnal', 'drift', 'error', 'fde', 'metric', 'end', 'point', 'position', 'error', 'meter', 'report', 'quantitative', 'evaluation', 'result', 'vio', 'system', 'table', 'small', 'end', 'point', 'position', 'error', 'sequence', 'indicate', 'bold', 'ideal', 'fde', 'value', 'groundtruth', 'path', 'large', 'fde', 'value', 'denote', 'inaccurate', 'position', 'estimate', 'deﬁne', 'starting', 'point', 'movement', 'origin', 'addition', 'overlay', 'estimate', 'vio', 'trajectory', 'ﬂoorplan', 'building', 'figureexternalviewpdf', 'arcore', 'apple', 'ze', 'laptop', 'google', 'arcore', 'apple', 'stereolab', 'ze', 'realsense', 'journal', 'latex', 'class', 'file', 'vol', 'fig', 'example', 'image', 'frame', 'indoor', 'outdoor', 'benchmark', 'dataset', 'top', 'row', 'represent', 'indoor', 'sequence', 'foot', 'include', 'long', 'corridor', 'open', 'hallway', 'space', 'b', 'repetitive', 'stair', 'c', 'university', 'building', 'acquire', 'camera', 'motion', 'datum', 'outdoor', 'campus', 'foot', 'car', 'underground', 'parking', 'lot', 'e', 'urban', 'outdoor', 'road', 'table', 'evaluation', 'result', 'fde', 'proprietary', 'vio', 'system', 'experiment', 'arkit', 'arcore', 'ze', 'length', 'indoor', 'corridor', 'indoor', 'hallway', 'indoor', 'stair', 'outdoor', 'campus', 'parking', 'lot', 'outdoor', 'road', '×', 'map', 'evaluate', 'consistency', 'stability', 'reliability', 'vio', 'system', 'qualitatively', 'indoor', 'long', 'corridor', 'open', 'hallway', 'sequence', 'evaluate', 'vio', 'system', 'long', 'ushape', 'corridor', 'open', 'hallway', 'space', 'easily', 'find', 'typical', 'ofﬁce', 'uni', 'versity', 'building', 'show', 'fig', 'fig', 'illustrate', 'example', 'frame', 'location', 'trajectory', 'sequence', 'approximately', 'meter', 'include', 'pure', 'rotational', 'movement', 'difﬁcult', 'texture', 'long', 'ushape', 'corridor', 'open', 'hallway', 'sequence', 'start', 'end', 'point', 'red', 'meet', 'black', 'circle', 'severe', 'rotational', 'drift', 'maintain', 'orthogonality', 'scale', 'estimate', 'trajectory', 'well', 'compare', 'ﬂoorplan', 'arcore', 'green', 'show', 'accurate', 'result', 'term', 'fde', 'metric', 'table', 'estimate', 'vio', 'trajectory', 'match', 'ﬂoorplan', 'well', 'intel', 'blue', 'estimate', 'accurate', 'rotational', 'motion', 'problem', 'scale', 'move', 'trajectory', 'compare', 'ﬂoorplan', 'show', 'little', 'large', 'trajectory', 'actual', 'movement', 'zed2', 'magenta', 'present', 'inaccurate', 'inconsistent', 'positioning', 'performance', 'vio', 'method', 'rotational', 'motion', 'drift', 'error', 'gradually', 'accumulate', 'time', 'overall', 'estimate', 'vio', 'trajectory', 'similar', 'consistent', 'motion', 'track', 'result', 'actual', 'movement', 'follow', 'shape', 'corridor', 'ﬂoorplan', 'indoor', 'multistory', 'stair', 'sequence', 'perform', 'comparative', 'experiment', 'multiﬂoor', 'staircase', 'environment', 'trajectory', 'go', 'stair', '2nd', 'basement', 'ﬂoor', '5th', 'ﬂoor', 'building', 'fig', 'repetitive', 'rotational', 'motion', 'include', '3d', 'trajectory', 'climb', 'stair', 'make', 'vio', 'position', 'challenge', 'fig', 'c', 'show', 'example', 'frame', 'multistory', 'stair', 'sequence', 'top', 'view', 'xyplane', 'start', 'end', 'point', 'mark', 'black', 'circle', 'check', 'loop', 'close', 'estimate', 'vio', 'trajectory', 'arkit', 'red', 'good', 'performance', 'top', 'side', 'view', 'red', 'show', 'overlapped', 'consistent', 'motion', 'track', 'result', 'vio', 'system', 'gradually', 'diverge', 'initially', 'estimate', 'loop', 'red', 'starting', 'end', 'point', 'top', 'view', 'nearly', 'match', 'ﬁnal', 'drift', 'error', 'fde', 'arcore', 'respectively', 'particular', 'zed2', 'magenta', 'severe', 'trajectory', 'distortion', 'zaxis', 'direction', 'height', 'vio', 'system', 'fig', 'illustrate', 'side', 'front', 'view', 'stairway', 'path', 'vio', 'device', 'show', 'high', 'consistency', 'red', 'compare', 'vio', 'platform', 'noteworthy', 'height', 'ﬂoor', 'estimate', 'actual', 'height', 'ground', 'truth', 'building', 'blueprint', 'approximately', 'identical', 'c', 'outdoor', 'university', 'campus', 'sequence', 'choose', 'outdoor', 'location', 'university', 'campus', 'approximately', 'determine', 'vio', 'system', 'work', 'well', 'environment', 'rapid', 'change', 'topogra', 'phy', 'narrow', 'road', 'return', 'show', 'left', 'fig', 'example', 'frame', 'show', 'fig', 'show', 'result', '6dof', 'trajectory', 'vio', 'platform', 'overlay', 'map', 'demonstrate', 'start', 'end', 'point', 'arkit', 'red', 'arcore', 'green', 'meet', 'match', 'well', 'shape', 'road', 'show', 'map', 'shape', 'estimate', 'trajectory', 'blue', 'figureexampleframespdf', 'indoor', 'corridor', 'b', 'indoor', 'hallway', 'c', 'indoor', 'stair', 'outdoor', 'campus', 'e', 'underground', 'parking', 'lot', 'urban', 'outdoor', 'road', 'class', 'file', 'vol', 'fig', 'estimate', 'trajectory', 'proprietary', 'vio', 'system', 'long', 'ushape', 'corridor', 'leave', 'open', 'hallway', 'space', 'right', 'sequence', 'start', 'end', 'point', 'mark', 'black', 'circle', 'evaluate', 'loop', 'closing', 'performance', 'test', 'commercial', 'vio', 'system', 'estimate', 'path', 'arkit', 'red', 'match', 'building', 'ﬂoorplan', 'consistently', 'starting', 'end', 'point', 'nearly', 'meet', 'due', 'inaccurate', 'rotation', 'estimation', 'show', 'severe', 'distortion', 'actual', 'movement', 'trajectory', 'vio', 'system', 'show', 'left', 'fig', 'outdoor', 'urban', 'road', 'parking', 'lot', 'sequence', 'perform', 'outdoor', 'vehicle', 'drive', 'experiment', 'mileage', 'approximately', 'km', 'attach', 'capture', 'rig', 'vehicle', 'show', 'right', 'fig', 'fig', 'e', 'show', 'example', 'frame', 'underground', 'parking', 'lot', 'urban', 'outdoor', 'road', 'acquire', 'motion', 'datum', 'drive', 'public', 'automobile', 'road', 'station', 'plenty', 'move', 'people', 'car', 'occasional', 'large', 'vehicle', 'visible', 'outdoor', 'environment', 'make', 'mo', 'tion', 'tracking', 'vio', 'challenge', 'even', 'highspeed', 'driving', 'condition', 'sometimes', 'exceed', 'kmh', 'red', 'show', 'surprisingly', 'accurate', 'consistent', '6dof', 'motion', 'track', 'result', 'overlaid', 'map', 'show', 'right', 'fig', 'start', 'end', 'point', 'accurately', 'meet', 'black', 'circle', 'ﬁnal', 'drift', 'error', 'fde', 'table', 'arcore', 'green', 'occasionally', 'fail', 'speed', 'car', 'increase', 'light', 'variation', 'occur', 'abruptly', 'blue', 'car', 'stop', 'temporarily', 'stop', 'signal', 'drive', 'fast', 'vio', 'algorithm', 'diverge', 'fail', 'estimate', 'location', 'zed2', 'magenta', 'accumulate', 'rotational', 'drift', 'error', 'time', 'result', 'inaccurate', 'motion', 'estimation', 'result', 'vio', 'system', 'perform', 'relatively', 'well', 'previous', 'walking', 'sequence', 'case', 'challenging', 'vehicular', 'test', 'ofﬁcially', 'support', 'test', 'vio', 'device', 'able', 'produce', 'stable', 'motion', 'track', 'result', 'even', 'vehicular', 'test', 'conduct', 'additional', 'vehicular', 'test', 'drive', 'trajectory', 'repeatedly', 'dark', 'underground', 'parking', 'lot', 'poor', 'visual', 'condition', 'show', 'fig', 'total', 'travel', 'distance', 'drive', 'car', 'low', 'speed', 'kmh', 'restore', 'actual', 'movement', 'perfectly', 'parking', 'lot', 'arkit', 'red', 'show', 'fig', 'comparison', 'vio', 'system', 'multistory', 'stair', '2nd', 'basement', 'ﬂoor', '5th', 'ﬂoor', 'show', 'side', 'leave', 'front', 'right', 'bottom', 'top', 'righttop', 'view', 'estimate', 'vio', 'trajectory', 'consistent', 'camera', 'motion', 'shape', 'stair', 'match', 'start', 'end', 'point', 'mark', 'black', 'circle', 'similar', 'arkit', 'arcore', 'result', 'however', 'scale', 'estimate', 'path', 'small', 'actual', 'movement', 'suffer', 'scale', 'inconsistency', 'problem', 'generally', 'observe', 'monocular', 'visual', 'odometry', 'conﬁguration', 'orthogonality', 'zed2', 'magenta', 'break', 'journal', 'class', 'file', 'vol', 'fig', 'estimate', 'motion', 'trajectory', 'proprietary', 'vio', 'system', 'outdoor', 'campus', 'leave', 'urban', 'outdoor', 'road', 'right', 'sequence', 'overlaid', 'map', 'start', 'end', 'point', 'mark', 'black', 'circle', 'check', 'loop', 'closing', 'performance', 'arkit', 'red', 'track', '6dof', 'camera', 'pose', 'well', 'follow', 'shape', 'road', 'map', 'consistently', 'accurately', 'arkit', 'red', 'able', 'produce', 'stable', 'motion', 'tracking', 'performance', 'even', 'drive', 'vehicle', 'kmh', 'right', 'realistic', 'use', 'case', 'people', 'vehicle', 'crowded', 'indoor', 'also', 'outdoors', 'arcore', 'exhibit', 'accurate', 'consistent', 'motion', 'tracking', 'performance', 'next', 'arkit', 'arcore', 'work', 'well', 'indoor', 'sequence', 'motion', 'datum', 'collect', 'walk', 'person', 'diverge', 'deteriorate', 'sharply', 'move', 'rapidly', 'poor', 'lighting', 'condition', 'intel', 'realsense', 'show', 'good', 'positioning', 'performance', 'operate', '6dof', 'motion', 'track', 'indoor', 'badly', 'problem', 'scale', 'inconsistency', 'issue', 'estimate', 'move', 'path', 'large', 'small', 'scale', 'actual', 'movement', 'also', 'motion', 'tracking', 'sometimes', 'fail', 'move', 'speed', 'slow', 'fast', 'motion', 'track', 'performance', 'stereolab', 'ze', 'inconsistent', 'inaccurate', 'vio', 'vice', 'indoor', 'outdoors', 'motion', 'tracking', 'progress', 'rotational', 'error', 'occur', 'severely', 'rotation', 'error', 'accumulate', 'time', 'result', 'incorrect', 'path', 'starting', 'end', 'point', 'different', 'particular', 'zed2', 'exhibit', 'tendency', 'track', 'straight', 'path', 'correctly', 'actually', 'move', 'straight', 'line', 'outdoors', 'rotational', 'drift', 'error', 'severe', 'move', 'fast', 'vi', 'conclusion', 'fig', 'example', 'path', 'underground', 'parking', 'lot', 'overlaid', 'ﬂoorplan', 'evaluate', 'consistency', 'accuracy', 'trajectory', 'arkit', 'red', 'overlap', 'signiﬁcantly', 'path', 'vio', 'device', 'suffer', 'rotational', 'drift', 'show', 'inaccurate', 'inconsistent', 'positioning', 'result', 'overlap', 'consistent', 'motion', 'estimation', 'result', 'vio', 'system', 'gradually', 'diverge', 'initially', 'estimate', 'loop', 'perform', 'evaluation', 'relatively', 'low', 'speed', 'kmh', 'compare', 'previous', 'vehicle', 'test', 'kmh', 'vio', 'system', 'diverge', 'fail', 'vio', 'method', 'zed2', 'positioning', 'result', 'deviate', 'actual', 'movement', 'underground', 'parking', 'lot', 'discussion', 'overall', 'apple', 'demonstrate', 'consistent', 'accurate', 'reliable', 'stable', 'motion', 'track', 'result', 'vio', 'system', 'indoor', 'outdoor', 'use', 'arkit', 'perform', 'well', 'robustly', 'various', 'e', 'environment', 'sudden', 'camera', 'movement', 'abrupt', 'change', 'illumination', 'highspeed', 'movement', 'rare', 'case', 'track', 'failure', 'motion', 'jump', 'occur', 'achieve', 'accurate', 'robust', 'positioning', 'performance', 'conduct', 'survey', 'egomotion', 'tracking', 'performance', 'proprietary', 'vio', 'platform', 'challenge', 'indoor', 'outdoor', 'environment', 'good', 'knowledge', 'ﬁrst', 'backtoback', 'comparison', 'demon', 'strate', 'apple', 'perform', 'well', 'robustly', 'indoor', 'outdoor', 'scenario', 'hope', 'result', 'conclusion', 'present', 'paper', 'help', 'member', 'research', 'community', 'ﬁnde', 'appropriate', 'vio', 'platform', 'robotic', 'system', 'application', 'journal', 'class', 'file', 'vol', 'reference', 'mouriki', 'roumeliotis', 'multistate', 'constraint', 'kalman', 'ﬁlter', 'visionaide', 'inertial', 'navigation', 'ieee', 'leutenegger', 'p', 'furgale', 'v', 'rabaud', 'r', 'siegwart', 'keyframebase', 'visualinertial', 'slam', 'use', 'nonlinear', 'timization', 'proceeding', 'robotis', 'science', 'system', 'rss', 'robust', 'versatile', 'monocular', 'visualinertial', 'state', 'estimator', 'ieee', 'transaction', 'robotic', 'apple', 'arkit', 'ac', 'cesse', 'arcore', 'access', 'rouˇcek', 'j', 'bayer', 'ˇsalansky', 'b´aˇca', 'v', 'spurny', 'multirobotic', 'exploration', 'underground', 'environment', 'international', 'conference', 'modelling', 'simulation', 'autonomous', 'springer', 'p', 'root', 'fast', 'lightweight', 'advanced', 'research', 'project', 'agency', 'darpa', 'milprogramfastlightweight', 'autonomy', 'retrieve', 'flint', 'naroditsky', 'p', 'broaddus', 'grygorenko', 'roumeliotis', 'bergig', 'visualbase', 'inertial', 'navigation', 'patent', 'mourikis', 'trawny', 'roumeliotis', 'e', 'johnson', 'ansar', 'l', 'matthie', 'visionaide', 'inertial', 'navigation', 'spacecraft', 'entry', 'descent', 'land', 'ieee', 'transaction', 'robotic', 'system', 'method', 'concurrent', 'odometry', 'mapping', 'patent', 'intel', 'realsense', 'tracking', 'camera', 'comtrackingcamerat265', 'access', 'stereolab', 'ze', 'stereo', 'camera', 'access', 'hutter', 'r', 'siegwart', 'robust', 'visual', 'inertial', 'odometry', 'use', 'direct', 'ekfbase', 'approach', 'ieeersj', 'international', 'conference', 'intelligent', 'robot', 'system', 'iro', 'r', 'murartal', 'orbslam2', 'opensource', 'slam', 'system', 'monocular', 'stereo', 'rgbd', 'camera', 'ieee', 'transaction', 'robotic', 'v', 'cremer', 'direct', 'sparse', 'odometry', 'ieee', 'transaction', 'pattern', 'analysis', 'machine', 'intelligence', 'j', 'delmerico', 'scaramuzza', 'benchmark', 'comparison', 'monoc', 'ular', 'vio', 'algorithm', 'ﬂye', 'robot', 'ieee', 'c', 'montiel', 'accurate', 'opensource', 'library', 'visual', 'visual', 'inertial', 'multimap', 'slam', 'ieee', 'transaction', 'robotic', 'gohl', 'schneider', 'rehder', 'r', 'siegwart', 'euroc', 'micro', 'aerial', 'vehicle', 'dataset', 'international', 'journal', 'research', 'vol', 'pp', 'cort´e', 'solin', 'e', 'rahtu', 'kannala', 'advio', 'authentic', 'dataset', 'visualinertial', 'odometry', 'eccv', 'alapetite', 'wang', 'patalan', 'comparison', 'offthe', 'shelf', 'visual', 'odometry', 'system', 'robotic', 'ouerghi', 'ragot', 'savati', 'comparative', 'study', 'com', 'mercial', 'tracking', 'camera', 'orbslam2', 'person', 'localization', 'modeling', 'vary', 'cameraimu', 'time', 'offset', 'optimization', 'base', 'visualinertial', 'odometry', 'proceeding', 'european', 'con', 'ference', 'computer', 'vision', 'eccv', 'g¨umg¨umc¨u', 'evaluation', 'framework', 'proprietary', 'slam', 'system', 'e', 'constrain', 'keyframebase', 'localization', 'mapping', 'ieee', 'international', 'conference', 'robotic', 'automation', 'siggraph', 'real', 'time', 'live', 'r', 'vineet', 'scherer', 'kapoor', 'learn', 'visuomotor', 'policy', 'aerial', 'navigation', 'use', 'crossmodal', 'representa', 'tion', 'ieeersj', 'international', 'conference', 'intelligent', 'robot', 'system', 'iro', 'r', 'fan', 'realtime', 'dense', 'stereo', 'embed', 'uav', 'road', 'inspection', 'proceeding', 'ieeecvf', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'workshop']"
An Empirical Study of Implicit Regularization in Deep Offline RL,"[{'href': 'http://arxiv.org/abs/2207.02099v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.02099v2', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-05 15:07:31,"A Comparison of Source Distribution and Result 
Overlap in Web Search Engines 

AUTHORS SECTION 
Yagci, Nurce 

Sünkler, Sebastian 

Häußler, Helena 

Lewandowski, Dirk 

HAW Hamburg, Germany | nurce.yagci@haw-hamburg.de 

HAW Hamburg, Germany | sebastian.suenkler@haw-hamburg.de 

HAW Hamburg, Germany | helena.haeuessler@haw-hamburg.de 

HAW Hamburg, Germany | dirk.lewandowski@haw-hamburg.de 

ABSTRACT 
When it comes to search engines, users generally prefer Google. Our study aims to find the differences between the 
results found in Google compared to other search engines. We compared the top 10 results from Google, Bing, 
DuckDuckGo, and Metager, using 3,537 queries generated from Google Trends from Germany and the US. Google 
displays more unique domains in the top results than its competitors. Wikipedia and news websites are the most 
popular sources overall. With some top sources dominating search results, the distribution of domains is also 
consistent across all search engines. The overlap between Google and Bing is always under 32%, while Metager has 
a higher overlap with Bing than DuckDuckGo, going up to 78%. This study shows that the use of another search 
engine, especially in addition to Google, provides a wider variety in sources and might lead the user to find new 
perspectives. 

KEYWORDS 
Web search; search engine; web scraping; Google; source comparison 

INTRODUCTION 
Why should there be more than one search engine? While users may prefer one search engine over others for its 
usability, specialized features, or a more convenient integration into their technical environment, the question that 
interests us in this research is whether a user will benefit from using another search engine than Google when it 
comes to finding results from different sources. Our starting point is the fact that Google is the most-used search 
engine by far (StatCounter, 2022), that user to a large degree trust search engines to provide them relevant and 
useful results (European Commission, 2016; Purcell et al., 2012), and that only some users use another search 
engine in addition to Google (Schultheiß & Lewandowski, 2021). 

Users place great trust in search engines. This is reflected by the 91% of US users who said they find what they are 
looking for always or most of the time, and the 66% who believe search engines are a fair and unbiased source of 
information (Purcell et al., 2012). Furthermore, 78% of European internet and online platform users said they trust 
that their search engine results are the most relevant results (European Commission, 2016). Globally, users trust 
search engines more than any other source (including traditional news outlets) when it comes to news (Edelman 
Trust Institute, 2022) and users trust news found via search significantly more than news found on social media 
(Newman et al., 2021). 

As the Web is enormous and different search engines might prefer different sources, it is interesting to see whether 
the top sources shown in search results differ from one search engine to the other. It might be that an alternative 
search engine prefers results from ""alternative"" sources, e.g., in terms of political leaning or preferring non-
commercial content providers. This all comes down to whether alternative search engines are actually alternatives in 
regards to the results they display. If they were, possible benefits of using a search engine other than Google include 
finding different results, finding additional results, and finding more relevant results. No matter which of these goals 
a user aims to achieve, they will need other results than Google's. Therefore, it is interesting to see whether other 
search engines provide users with such results. 

There has been an ongoing discussion on alternative search engines and how Google's dominance in the search 
engine market can be broken. Approaches range from establishing single alternative search engines to building 
infrastructures for such alternatives (e.g., Lewandowski, 2019); also see Mager, 2014). With Google dominating the 
search engine market (StatCounter, 2022), it often seems that there are no alternatives at all. On the other hand, the 
number of alternative (or simply ""other"") search engines is often overestimated. Many seem-to-be search engines are 
merely search portals displaying results from a partner instead of generating the results from their own index. For 
instance, Yahoo and Ecosia get their results from Bing and can therefore not be considered search engines in their 
own right. But still, there may be other reasons for using a search engine without its own index. Some of the unique 
benefits alternative search engines advertise are privacy (e.g., Startpage and DuckDuckGo) or being a company 
investing its profits in environmental projects (e.g., Ecosia). Another type of search engine is the meta search engine 

 
(e.g., Metager). Such an engine sends the queries to several other search engines, then aggregates and re-ranks the 
top results. We deem it especially interesting whether such an approach will lead to a wider variety of search results, 
i.e., results from a more diverse set of sources. So, in the context of our research, we will consider any search engine 
that either has its own index or provides a unique selection and re-ranking of results from one or more indexes as an 
alternative search engine. We are especially interested in the differences in the source distribution; the relevance of 
the results is out of the scope of our research. 

More than 20 years ago, Introna & Nissenbaum (2000) argued that search engines as commercial operations tend to 
prefer big websites and, therefore, a portion of the Web, i.e., the smaller sites, remain hidden from view. Studies 
measuring what users select seem to confirm this: Goel et al. (2010) found that within Yahoo, only 10,000 websites 
account for approximately 80% of result clicks. It is important to note that this does not merely result from user 
preference for particular sources but that users predominantly select from the top results shown by the search engine. 
What is out of the immediate view of users will not be chosen (Lewandowski & Kammerer, 2021). 

It is striking that few studies have compared the results between different search engines in recent years. Older 
studies (see literature review section) overall found that top results from different search engines did not overlap too 
much. In this paper, we address how the top results of Google differ from alternatives and, therefore, whether it is 
worthwhile for a user to consider these alternatives. If a search engine other than Google produced very similar 
results to Google, a user would not benefit much from using that search engine when source variety is considered. 

LITERATURE REVIEW 

Search result overlap 
By the mid-1990s, researching the overlap of search results between different search engines sparked interest for the 
purpose of estimating the size of the Web (Bharat, 1998; Ding & Marchionini, 1996; Lawrence & Giles, 1999). The 
generally small degrees of overlap indicated diverse underlying databases, each of limited size. Therefore, meta-
search engines that combined the results of several search engines were meant to provide an additional value 
(Chignell et al., 1999) and attracted further research (i.e., Meng et al., 2002). Consequently, Spink et al. (Spink et 
al., 2006) included the meta search engine Dogpile in their extensive study on the overlap of search results. For a 
collection of 12,570 queries, the results found on the first search engine result page (SERP) of Ask Jeeves, Google, 
MSN Search, and Yahoo were captured. 84.9% of the results were unique to one search engine, while only 1.1% 
were shared by all search engines. Additionally, the low overlap between search engines also manifests in the 
ranking since only 7% of the top results were similar. This is consistent with previous studies that reported low 
overlap in ranking (Bar-Ilan, 2005; Bar‐Ilan et al., 2006). 

Subsequent studies report an increase of overlap in search results, while ranking algorithms appear to be the leading 
cause for differences in the presentation of the results. Bilal and Ellis (2011) compared major web search engines 
(Google, Bing, Yahoo) with search engines specifically designed for children (Ask Kids, Yahoo Kids) for queries of 
various lengths. Bing and Yahoo shared the same overlap with Google for nearly all queries, ranging between 22% 
and 40%. In contrast, Yahoo Kids had mostly unique results. However, the disparate relevance ranking between 
Bing and Yahoo suggests different ranking algorithms. Cardoso and Magalhães (2011) measured the overlap of 
search result rankings based on URLs and website contents. Search results for 40,000 queries were retrieved from 
Google and Bing. The overlap in domains was about 29%, and Google had more exclusive domains. When looking 
at the result sets without considering the positions, the similarity between the result sets increased. This indicates 
that Google and Bing have different ranking preferences but index mostly the same sources.  

Similarly, Agrawal et al. (2016) based their overlap analysis on content but factored in the description of the result 
snippets. The top 10 results of 67 informational queries were collected from Google and Bing. The results indicate a 
high overlap between Google and Bing, with a slightly higher similarity among the top 5 results. Most recently, a 
study about information on Covid-19 was conducted by Makhortykh et al. (2020). The first 50 results for queries in 
different languages were collected from Baidu, Bing, DuckDuckGo, Google, Yandex, and Yahoo. DuckDuckGo and 
Yahoo shared nearly 50% of their results, whereas the other pairs remained under 25%. Google had an overlap of 
around 10% with Bing and a negligible one with DuckDuckGo, Yandex, Yahoo, and Baidu. 

Source diversity 
Various studies took a closer look at search results and evaluated the domains, the types of sources, as well as their 
diversity across result sets and search engines. Comparing Google, Live Search, and Yahoo, Thelwall (2008) 
reported that Yahoo returned the highest amount of different domains to a query and also returned around 10% more 
top-level domains than other search engines. Höchstötter & Lewandowski (2009) showed that while the sources in 
top results of various search engines differ, there is a concentration on some popular sources in top results. More 
recently, Lewandowski & Sünkler (2019) collected search results on queries related to insurance providers and 
identified the distribution of top-level domains. Only ten different domains were found in the first result across all 

 
queries. The five most popular domains in the top 5 results are price comparison websites, which make up 88.4% of 
all first results. The share of popular domains decreases according to the ranking but remains at 42.9% at position 
ten. 

Unkel and Haim (2019) observed Google search results prior to the German parliamentary elections in 2017. The 
study found that result lists for candidates and parties exhibit a high share of self-administered websites. In contrast, 
results about election facts, guidance, and issues are mainly from general interest news, government information, 
and privately run websites. Among the top ten domains across all search results are seven news websites which 
account for a quarter of all search results. By contrast, Wikipedia made up 5.4% of all search results. The high share 
of news and Wikipedia articles is confirmed by Steiner et al. (2022) who compared the first results for queries on 
debated topics in Germany across the search engines Ask, Bing, DuckDuckGo, Google, and Ixquick. All search 
engines positioned Wikipedia in the first rank for queries on climate change and the Transatlantic Trade and 
Investment Partnership (TTIP); Ask did so for almost every query. In the other search engines, news sites were 
placed in the first rank most of the time. However, for the topic Covid-19, the sources are more diverse. As 
Makhortykh et al. (2020) observed, only Yahoo incorporates recent information from legacy media, whereas Bing 
strongly relied on healthcare-related sources and Google highlighted government-related websites. Yandex was the 
only search engine that included alternative media in its top 20 search results. Especially for recent topics that lack 
an established knowledge basis, the choice of search engine may considerably impact what information a user gets 
to see. 

OBJECTIVES AND RESEARCH QUESTIONS 
Our study addresses the following research questions: 

1. Do top results from alternative search engines differ from Google's in regard to the number of unique sources? 

2. Do top results from alternative search engines differ from Google's in regard to top sources? 

3. Do top results from alternative search engines differ from Google's in regard to source concentration, i.e., are 

results distributed over more or fewer sources in different search engines? 

To answer these questions, we selected three alternative search engines to compare to Google. Aside from Bing, 
which is the biggest competitor to Google, we chose DuckDuckGo and Metager. DuckDuckGo uses results from 
Bing, Yahoo, and Yandex, and has the added benefit of advanced privacy settings such as non-personalized results. 
Metager, a German meta-search engine, aggregates results from several search engines, including Bing and Yandex. 
As such, either search engine might provide a more varied set of sources. Since the alternative search engines each 
have an index different from Google's, the comparison will give insights into what domains are favored by Google 
and which domains are excluded. Also, we will gain insights into the differences in ranking between them. 

METHODS 
As illustrated in Figure 1, this study was conducted in five steps. In the first step, the query sets were generated from 
previously collected Google Trends data. The daily trends for both Germany and the US were collected daily at 3 
am CET from November 10th, 2021, until March 31st, 2022. Since the daily number of trending queries can vary, the 
query sets for Germany and the US are not the same size. Two thousand nine hundred sixteen trending queries for 
Germany and 2,819 for the US were initially collected. However, since some topics of interest are longer lived than 
a single day, the same topics can be trending on multiple days. Therefore, after removing duplicates, our query sets 
consist of 1,821 queries for Germany and 2,126 queries for the US. The queries have a wide range, including topics 
like celebrities (e.g., ""Sandra Bullock""), sports (e.g., ""NFL,"" ""Liverpool vs. Southampton""), seasonal events (e.g., 
""Macy's Thanksgiving Day Parade,"" ""Restaurants open on Thanksgiving""), headline news (e.g., ""TikTok class-
action lawsuit settlement""), and more. 

 
Figure 1. Methodology of the current study 

In step 2, for each query, the top 10 results from all four search engines were collected. We made sure to request 
results for the appropriate language and location by using URL parameters. Furthermore, we decided to limit the 
comparison to the top 10 results because users usually only consider these. Again, for our study, only the 
implications for the user perception are relevant. The Web scraping component of the  Relevance Assessment Tool 
(RAT; Sünkler et al., 2022) was used to collect the data. The scraping took place between April 6th and 9th, 2022, 
with 230,315 results (Germany: 109,604; US: 120,711) gathered. It is important to note that only organic search 
results were considered. A fair comparison can be ensured when ranked result lists instead of vertical inserts of 
universal search results (e.g., Google News and Bing News) are considered. We also ignored ads since they are not 
part of the search engine's web index. 

During step 3, the python library urllib was used to get the domain of each search result. To ensure a fair comparison 
between all search engines, in step 4, we removed results for any query that had less than ten results in any of the 
search engines. Additionally, the data was checked and stripped of any errors that might have occurred during the 
collection, like duplicate results. This resulted in a further reduced dataset consisting of 1,672 queries and 66,880 
results for Germany and 1,865 queries and 74,600 results for the US. Lastly, the extraction of top-level domains was 
further refined in this step. We used basic string matching to unify URLs that pointed to the same domains. For 
example, the following three URLs would have been counted as separate domains without the string matching: 
https://www.facebook.com, ""http://www.facebook.com"", and ""http://www.facebook.com/"". 

Finally, in step five, we used different methods of comparing the data collected. An initial comparison is made by 
using simple descriptive statistics. To measure source concentration, we adapt the Gini coefficient, a measure of 
statistical dispersion used to measure income or wealth inequality (Gini, 1936). The Gini coefficient is a single 
number ranging from 0 to 1, where 0 represents perfect equality, and 1 is the maximum inequality. As Ortega et al. 
(2008) demonstrated, adapting this measure to the distribution of values in different domains is appropriate. In the 
case of search result sets, this allows us to compare the various search engines easily. The lower the Gini coefficient, 
the more equally the results are distributed over all domains contributing to a particular result set. 

Following that, we calculate the Jaccard similarity index (González et al., 2008). Puschmann (2019) demonstrates 
the usage of the Jaccard index to measure the similarity between two result sets. The Jaccard index is a single 
number ranging from 0 to 1, where 0 means that the two sets are entirely dissimilar, while 1 means that they are 
identical. We calculate the Jaccard index for every two-way combination of our four search engines (e.g., Google 
and Bing, Bing and Metager, etc.) for all stages between the top 1 and top 10 results (e.g., Google and Bing position 
1 to 5). 

RESULTS 

Classification of domains 
Since we are working with sources from two different countries and the type of content behind some domains might 
not be immediately recognizable, we manually created a general set of categories based on the top 50 domains found 
across all results. Following this, we manually classified the 50 most popular domains for both the German and US 
results (see Table 1). 

When comparing the top 50 domains in the German results, News service dominates with 54%, while Movies & 
Entertainment and Sports make up 18% and 14%, respectively. In the US, 34% of the top 50 domains are Sports 
websites, with News services following close behind at 30%. Movies & Entertainment only make up 12%. These are 

 
 
the overall distributions of domain classes across all search engines; below, we look at the differences between the 
search engines.  

Example 

Top 50 Germany (share) 

Top 50 US (share) 

Class 

Celebrities 

E-Commerce 

variety.com 

amazon.com 

Government website 

bundesregierung.de 

Information service 

wikipedia.org 

Movies & Entertainment 

News service 

Sports 

imdb.com 

cnn.com 

espn.com 

Social media 

instagram.com 

0.06 

0.02 

0 

0.02 

0.18 

0.54 

0.14 

0.04 

0.10 

0.06 

0 

0.02 

0.12 

0.30 

0.34 

0.06 

Table 1. Classes of domains and their frequency in the top 50 domains 

Variety of domains 
A comparison of the number of root domains in the search results of Google, Bing, DuckDuckGo, and Metager in 
Germany shows that Google has the greatest diversity by a small margin (see Figure 2). Overall, the values are very 
similar. However, it is noticeable that Google has the greatest variety of domains, especially in the first three 
positions. Interestingly, the greater diversity of Google's sources is even more pronounced in the US results. To 
examine the differences more closely, we look at the numbers below. 

Figure 2. Cumulative number of unique domains 

Table 2 shows the cumulative frequencies for the German results. The number of unique root domains converges at 
the fifth position. We found 2,841 unique domains in Google's results, 2,783 unique domains for Bing, 2,707 unique 
domains for DuckDuckGo, and 2,683 unique domains for Metager in Germany. 

Position 

1 

2 

3 

4 

5 

Google 

Bing 

DuckDuckGo 

Metager 

# 

609 

959 

1,216 

1,480 

1,703 

share 

0.21 

0.34 

0.43 

0.52 

0.60 

# 

389 

778 

1,072 

1,325 

1,602 

share 

0.14 

0.28 

0.39 

0.48 

0.58 

# 

302 

544 

900 

1,232 

1,530 

share 

0.11 

0.20 

0.33 

0.46 

0.57 

# 

372 

735 

1,027 

1,298 

1,531 

share 

0.14 

0.27 

0.38 

0.48 

0.57 

 
 
Position 

6 

7 

8 

9 

10 

Google 

Bing 

DuckDuckGo 

Metager 

# 

share 

# 

share 

# 

share 

# 

share 

1,891 

2,092 

2,329 

2,577 

2,841 

0.67 

0.74 

0.82 

0.91 

1.00 

1,853 

2,071 

2,292 

2,528 

2,783 

0.67 

0.74 

0.82 

0.91 

1.00 

1,773 

2,047 

2,272 

2,497 

2,707 

0.65 

0.76 

0.84 

0.92 

1.00 

1,785 

2,008 

2,222 

2,453 

2,693 

0.66 

0.75 

0.83 

0.91 

1.00 

Table 2. Cumulative number of unique domains (Germany; #: cumulative number, %: percentage of total) 

There are differences when comparing the search results in Germany and the US. It is notable that, in contrast to 
Germany, in the US results, there are only minor differences in the first positions. However, Google also shows the 
greatest variety of root domains in the search results in the US. This is more evident than in the German results (see 
Table 3). Overall, we found 4,085 unique domains in Google, 3,602 in Bing, 3,579 in DuckDuckGo, and 3,500 in 
Metager. 

Position 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

Google 

Bing 

DuckDuckGo 

Metager 

# 

508 

961 

1,398 

1,828 

2,188 

2,531 

2,896 

3,289 

3,682 

4,085 

share 

0.12 

0.24 

0.34 

0.45 

0.54 

0.62 

0.71 

0.81 

0.90 

1.00 

# 

521 

851 

1,164 

1,491 

1,822 

2,168 

2,521 

2,870 

3,231 

3,602 

share 

0.14 

0.24 

0.32 

0.41 

0.51 

0.60 

0.70 

0.80 

0.90 

1.00 

# 

493 

849 

1,210 

1,547 

1,860 

2,208 

2,546 

2,888 

3,207 

3,579 

share 

0.14 

0.24 

0.34 

0.43 

0.52 

0.62 

0.71 

0.81 

0.90 

1.00 

# 

485 

820 

1,130 

1,445 

1,772 

2,110 

2,442 

2,785 

3,133 

3,500 

share 

0.14 

0.23 

0.32 

0.41 

0.51 

0.60 

0.70 

0.80 

0.90 

1.00 

Table 3. Cumulative number of unique domains (USA; #: cumulative number, %: percentage of total) 

Popular domains 
When comparing the top domains for each search engine across all German search results collected, we find a clear 
preference for Wikipedia in all of them (see Table 4). While Wikipedia is the most popular domain in all search 
engines, the frequency in the Google results is significantly lower, with only 658 compared to the number of 
occurrences in Bing being 1,948, in Metager 1,878, and in DuckDuckGo 1,752. 

The other domains in the top 10 across the search engines are News services (Google: 7, Bing: 4, DuckDuckGo: 3, 
Metager: 4). Sports make up 2 out of 10 for all but Google, with only one sports website. Surprisingly, Instagram is 
in the top 10 for DuckDuckGo and Metager, even though in Table 1, we showed that domains of the class Social 
Media only make up 4% of top domains in the German results. The same goes for Amazon, which is the second 
most frequent domain in DuckDuckGo, even though in the overall results, E-Commerce websites only make up 2%. 

No. 

Google 

Bing 

DuckDuckGo 

Metager 

1 

2 

3 

wikipedia.org (658) 

wikipedia.org (1,948) 

wikipedia.org (1,752) 

wikipedia.org (1,878) 

spiegel.de (414) 

stern.de (375) 

zdf.de (324) 

bild.de (298) 

amazon.de (735) 

zdf.de (344) 

zdf.de (298) 

bunte.de (314) 

 
No. 

Google 

Bing 

DuckDuckGo 

Metager 

4 

5 

6 

7 

8 

9 

gala.de (300) 

bild.de (288) 

bunte.de (296) 

kicker.de (261) 

bild.de (290) 

kicker.de (294) 

bild.de (255) 

kicker.de (285) 

t-online.de (279) 

gala.de (279) 

tagesschau.de (251) 

gala.de (282) 

tagesschau.de (271) 

tagesschau.de (274) 

wunderweib.de (248) 

tagesschau.de (268) 

kicker.de (222) 

sportschau.de (261) 

bunte.de (245) 

sportschau.de (256) 

zdf.de (219) 

web.de (252) 

instagram.com (243) 

web.de (238) 

10 

sueddeutsche.de (219) 

instagram.com (245) 

sportschau.de (242) 

instagram.com (232) 

Table 4. Top 10 domains shown in different search engines (Germany) 

Contrarily, in the US results, Wikipedia has the highest number of occurrences in Google with 1,892 compared to 
Bing's 1,388, Metager's 1,304, and DuckDuckGo's 1,287 (see Table 5). Still, what remains the same is that 
Wikipedia is the most popular domain across all search engines. 

Another difference in the US results is that Social Media sources are much more prevalent across all search engines, 
especially in Google, with Instagram as the second most frequent domain, Facebook as the third, and Twitter as the 
sixth. This is surprising when considering that Social Media domains only make up 6% of the top domains for the 
US results (see Table 1). Another finding is that YouTube, a subsidiary of Google, is the top 8th domain for all three 
alternatives but not for Google. Again, 30% of the top domains for Google, DuckDuckGo, and Metager are News 
services. For Bing, it is 40%. For each search engine, Sports websites make up 20% of domains. 

No. 

Google 

Bing 

DuckDuckGo 

Metager 

1 

2 

3 

4 

5 

6 

7 

8 

9 

wikipedia.org (1,892) 

wikipedia.org (1,388) 

wikipedia.org (1,287) 

wikipedia.org (1,304) 

instagram.com (726) 

imdb.com (669) 

imdb.com (644) 

yahoo.com (714) 

facebook.com (503) 

yahoo.com (634) 

yahoo.com (642) 

imdb.com (680) 

espn.com (377) 

espn.com (610) 

espn.com (601) 

espn.com (614) 

cbssports.com (363) 

nypost.com (518) 

nypost.com (536) 

nypost.com (514) 

twitter.com (328) 

cbssports.com (511) 

cbssports.com (513) 

cnn.com (504) 

imdb.com (244) 

cnn.com (495) 

cnn.com (506) 

cbssports.com (502) 

britannica.com (226) 

youtube.com (414) 

youtube.com (352) 

youtube.com (417) 

usatoday.com (222) 

msn.com (322) 

go.com (320) 

msn.com (324) 

10 

nytimes.com (220) 

instagram.com (318) 

instagram.com (281) 

instagram.com (288) 

Table 5. Top 10 domains for each search engine (USA) 

Exclusivity of domains 
This section compares the top 50 domains for each search engine to determine what source Google and its 
alternatives might exclusively offer to the user. Table 6 shows the domains found solely in Google's top 50 domains 
list for the German queries. Out of these 20 domains, eight are Sports websites, six are News services, and four 
belong to the Entertainment & Movies class, with one each for E-Commerce and Government websites. 

ran.de 

amazon.de 

dazn.com 

sky.de 

eurosport.de 

zeit.de 

dfb.de 

kino.de 

fr.de 

fernsehserien.de 

tagesspiegel.de 

filmstarts.de 

bundesregierung.de 

tz.de 

weltfussball.de 

spox.com 

sportbuzzer.de 

deutschlandfunk.de 

rnd.de 

dw.com 

 
Table 6. Domains only found in Google's top 50 (Germany) 

On the contrary, table 7 shows the list of domains found in the top 50 domains of all three alternatives but not in 
Google. These would be the domains missed by users who only use Google. Out of the 13 domains, four are News 
services, followed by three Movies & Entertainment and two Celebrities sources. Interestingly, Instagram and 
YouTube are missing from Google, as well. 

sport.de 

instagram.com 

wunderweib.de 

fussballdaten.de 

promipool.de 

n-tv.de 

abendzeitung-muenchen.de 

finanzen.net 

youtube.com 

imdb.com 

ardmediathek.de 

express.de 

daserste.de 

Table 7. Domains found in all search engines but Google's top 50 (Germany) 

When implementing the same evaluation for the US results, we find that 17 domains are exclusively found in 
Google's top 50 domains, with five domains classified as Sports, four as Movies & Entertainment, another four as 
News services, three as Social Media and one as Celebrities (see Table 8). Notable is that social media giants 
Facebook and TikTok are only found in Google and not in the alternatives. 

marca.com 

reuters.com 

rotowire.com 

facebook.com 

tiktok.com 

usmagazine.com 

npr.org 

discogs.com 

spotify.com 

theathletic.com 

spotrac.com 

linkedin.com 

deadline.com 

forbes.com 

variety.com 

washingtonpost.com 

sports-reference.com 

Table 8. Domains only found in Google's top 50 (US) 

On the opposite end, 13 domains are not found in the US results on Google (see Table 9). Six of them are News 
services, three are websites about Celebrities, two are about Movies & Entertainment, and one each are Sports and 
E-Commerce websites. Interestingly, Amazon is missing in Google's results, as well as msn.com, a service from 
Microsoft, like Bing itself. 

amazon.com 

yardbarker.com 

msn.com 

cnn.com 

pagesix.com 

screenrant.com 

heavy.com 

foxnews.com 

decider.com 

tmz.com 

newsweek.com 

huffpost.com 

biography.com 

Table 9. Domains found in all search engines but Google's top 50 (US) 

Distribution of domains 
Next, we look at the Gini coefficient to measure the concentration and statistical dispersion of root domains in 
search engines. The distributions across all search engines in both German and US results show very similar values. 
The results range from 0.77 to 0.79 and 0.73 to 0.76 for German and US results, respectively (see Figure 3). Overall, 
this means that there is an imbalance in the distribution of results from root domains and that some top sources 
dominate the search results for both countries.  

 
 
 
 
Figure 3. Source distribution of domains (Germany) 

Similarity of domains 
Comparing the similarities of the top 10 German results, we find that Bing and Metager are most similar in terms of 
their top results, with 70%, followed by 64% overlap between DuckDuckGo and Metager and 63% between Bing 
and DuckDuckGo. However, when comparing the alternative search engines to Google, we find that the highest 
overlap is between Google and Bing with 28% and slightly lower ones with DuckDuckGo and Metager at 27%. 

Interestingly, the top 1 result overlap is higher between Google and Bing (31%) than the overlap between Bing and 
DuckDuckGo (15%).  

The results show a mean overlap of 30% of top 1, 40% of top 3, and 47% of top 10 results (table 10). This indicates 
that when looking at the entire first search results page, nearly half of the domains are the same in all search engines 
considered. For search engine users, this means that in addition to Google, this would also allow them to see search 
results that they would otherwise miss if they used the alternative search engine 

Rank 

Top 1 

Top 3 

Top 10 

Google 
Bing 

Google 
DDG 

Google 
Metager 

0.31 

0.30 

0.28 

0.10 

0.21 

0.27 

0.29 

0.30 

0.27 

Bing 
DDG 

0.15 

0.39 

0.63 

Bing 
Metager 

DDG 
Metager 

0.78 

0.78 

0.70 

0.15 

0.39 

0.64 

Mean 

0.30 

0.40 

0.47 

Table 10. Jaccard similarity of domains (Germany; DDG: DuckDuckGo) 

The same evaluation for the US yields similar results. Again, the overlap between Bing and Metager is the highest, 
but the margin between this pair and the other alternative pairings has been reduced. When looking at the top 10 
results, Bing and Metager overlap by 65%, Bing and DuckDuckGo by 64%, and DuckDuckGo and Metager by 62%. 
While Google's overlap with the other search engines is lower in the US results, again, the pair of Google and Bing 
is slightly higher (25%) than the 24% overlap between Google and the other two alternatives (see Table 11). 

Narrowing the results down to the top 3, the overlap between Bing and DuckDuckGo increases minimally to 65% 
and remains the same for DuckDuckGo and Metager at 62%. Here, the overlap between Google and Bing is lower 
than in the German results, with only 26%. Furthermore, the top 1 result overlap between Google and Bing is at 
29%, which is also lower than in the German results. Generally, compared to the German results, there is a higher 
overlap in the top 1 position for the pairs Google and DuckDuckGo (20%), Bing and DuckDuckGo (50%), and 
DuckDuckGo and Metager (47%). The averaged results make this trend clearer because the overlaps for the top 1, 
top 3, and top 10 results are similar, with 41%, 46%, and 44%, respectively. 

 
 
Rank 

Top 1 

Top 3 

Top 10 

Google 
Bing 

Google 
DDG 

Google 
Metager 

0.29 

0.26 

0.25 

0.20 

0.24 

0.24 

0.28 

0.25 

0.24 

Bing 
DDG 

0.50 

0.65 

0.64 

Bing 
Metager 

DDG 
Metager 

0.74 

0.71 

0.65 

0.47 

0.62 

0.62 

Mean 

0.41 

0.46 

0.44 

Table 11. Jaccard similarity of domains (US; DDG: DuckDuckGo) 

DISCUSSION 
Our study examined whether there are differences in the sources of the top search results between Google and 
alternative search engines based on popular search queries. An evaluation of root domains of the most popular 
sources shows only a small overlap between Google and the alternative search engines (RQ1). Overall, we found an 
overlap of 27% to 28% between Google and the alternatives in German results, and in the US, the overlap ranges 
from 24% to 25%. There is a significantly higher overlap between the alternative search engines of about 63% to 
70% in German results and 62% to 65% in US results. This may be explained by all three alternative search engines 
using Bing's index at least in part. However, our findings show that Metager consistently has the highest overlap 
with Bing, going up to 78% and only overlapping as much as 64% with DuckDuckGo. The lower overlap of Google 
with the alternative search engines had already been shown in the studies of Agrawal et al. (2016) and Makhortykh 
et al. (2020). Our study provides further evidence for this. 

When looking at the uniqueness of sources across all German queries, we found that the search engines returned a 
similar total number of sources, ranging from 2,693 to 2,841. However, for the US queries, the variety in Google 
was noticeably higher, with over 4,000 total sources compared to around 3,500 of the alternatives. 

The most popular domain was Wikipedia, followed by sources we classified as News services (RQ2). This is likely 
exacerbated by selecting Google Trends as the source of our search queries, which usually includes queries related 
to popular news stories, sports, and celebrities. Still, it is consistent with previous studies' findings (Steiner et al., 
2022) which already showed that Wikipedia and news made up the majority of search results. Furthermore, in our 
research, Wikipedia and news sources were the top domains across all results and the most frequent sources for each 
search engine individually. 

When comparing the result sets of the top 10 results from Google, Bing, DuckDuckGo, and Metager, using 3,537 
queries generated from Google Trends from Germany and the US, it is interesting to see that news sources are far 
more prevalent in German results, with social media being very infrequent. On the other hand, the US results had 
more social media websites. Interestingly, in the US results, YouTube was in the top 10 most popular domains in all 
search engines but Google. The same was the case for the top 50 domains in German results. This is unexpected 
because YouTube is a subsidiary of Google. However, this finding may be explained by the fact that we only 
collected organic search results, and Google might be using universal search results to display YouTube results. 
Another interesting difference is the greater preference of Wikipedia in Google in US results (1,892, 10,2% of all 
domains) compared to German results (658, 4% of all domains). Furthermore, the second and third most popular 
sources on Google are Instagram and Facebook for the US results, while they are not even in the top 10 of German 
Google sources. Finally, in terms of what is missing from Google in the US results, it is notable that Fox News is 
not found in the top 50 sources, while it is present in all of the alternatives. 

The concentration of sources and source diversity showed a tendency for only a few root domains to make up a large 
share of search results. The Gini Index values of 0.73 and 0.79 in Germany and the United States, respectively, are a 
clear indicator (RQ3). This is consistent with findings from previous studies (Höchstötter & Lewandowski, 2009) 
that showed that only a few top sources dominate the search results in search engines. 

Of course, our study is not without limitations. First, the selection of search queries is the most significant factor in 
compiling the data that was evaluated. Even though the number of queries is high and there is some diversity in the 
queries, the topics are almost always focused on news, celebrities, and sports, which inevitably leads to many news 
sources in the search results. A refined approach to selecting search queries would be appropriate for a more 
accurate evaluation of source diversity. For example, this could be achieved by focusing on socially controversial 
topics and choosing the queries accordingly. 

Further limitations arise in the evaluation of source types. The classification we used in this study is very broad. A 
more precise classification would help make statements about the actual kinds of sources and thus to determine even 
more precisely which preferences and biases exist in different search engines. For example, grouping sources 
according to seriousness and reliability could serve as an explanation for the selection of sources to be displayed in 

 
search results. Regarding the search results collected in this study, another limitation is that only organic search 
results were considered. We did not consider advertisements or universal search results, although these have a strong 
influence on what users see on the SERP. The relevance of the search results was also not considered. 

CONCLUSION 
This study provides important insights into whether, although Google is by far the most popular search engine, the 
use of alternatives could benefit users. Our results show that using another or more than one search engine leads to 
seeing more diverse search results, allowing users to inform themselves more comprehensively. It should be noted 
that within each search engine's results, the concentration of sources shows that only a few top sources dominate the 
results, meaning whichever search engine a user chooses to use will shape what sources the information they get to 
see comes from. 

RESEARCH DATA 
Research data is available at: https://osf.io/nt3wv/  

ACKNOWLEDGMENTS 
This work is funded by the German Research Foundation (DFG – Deutsche Forschungsgemeinschaft; Grant No. 
460676551).  

REFERENCES 
Agrawal, R. (2016). Overlap in the Web Search Results of Google and Bing. Journal of Web Science, 2(1), 17–30. 

https://doi.org/10.1561/106.00000005 

Bar-Ilan, J. (2005). Comparing rankings of search results on the Web. Information Processing & Management, 

41(6), 1511–1519. https://doi.org/10.1016/J.IPM.2005.03.008 

Bar‐Ilan, J., Levene, M., & Mat‐Hassan, M. (2006). Methods for evaluating dynamic changes in search engine 

rankings: a case study. Journal of Documentation, 62(6), 708–729. 
https://doi.org/10.1108/00220410610714930 

Bharat, K. (1998). A technique for measuring the relative size and overlap of public Web search engines. Computer 

Networks and ISDN Systems, 30(1–7), 379–388. https://doi.org/10.1016/S0169-7552(98)00127-5 

Bilal, D., & Ellis, R. (2011). Evaluating Leading Web Search Engines on Children’s Queries. In Lecture Notes in 
Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in 
Bioinformatics): Vol. 6764 LNCS (Issue PART 4, pp. 549–558). https://doi.org/10.1007/978-3-642-21619-
0_67 

Cardoso, B., & Magalhães, J. (2011). Google, bing and a new perspective on ranking similarity. Proceedings of the 

20th ACM International Conference on Information and Knowledge Management - CIKM ’11, 1933–1936. 
https://doi.org/10.1145/2063576.2063858 

Chignell, M. H., Gwizdka, J., & Bodner, R. C. (1999). Discriminating meta-search: a framework for evaluation. 
Information Processing & Management, 35(3), 337–362. https://doi.org/10.1016/S0306-4573(98)00065-X 

Ding, W., & Marchionini, G. (1996). A comparative study of web search service performance. Proceedings of the 

ASIS Annual Meeting, 33, 136–142. https://eric.ed.gov/?id=EJ557172 
Edelman Trust Institute. (2022). Edelman Trust Barometer 2022 - Global Report. 

https://www.edelman.com/sites/g/files/aatuss191/files/2022-01/2022 Edelman Trust Barometer 
FINAL_Jan25.pdf 

European Commission. (2016). Special Eurobarometer 447 – Online Platforms. European Commission. 

https://doi.org/10.2759/937517 

Gini, C. (1936). On the measure of concentration with special reference to income and statistics. Colorado College 

Publication, General Series, 208(1), 73–79. 

Goel, S., Broder, A., Gabrilovich, E., & Pang, B. (2010). Anatomy of the long tail. Proceedings of the Third ACM 

International Conference on Web Search and Data Mining - WSDM ’10, 201. 
https://doi.org/10.1145/1718487.1718513 

González, C. G., Bonventi, W., & Rodrigues, A. L. V. (2008). Density of Closed Balls in Real-Valued and 

Autometrized Boolean Spaces for Clustering Applications. In Lecture Notes in Computer Science (including 
subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Vol. 5249 LNAI (pp. 
8–22). Springer Verlag. https://doi.org/10.1007/978-3-540-88190-2_7 

Höchstötter, N., & Lewandowski, D. (2009). What users see – Structures in search engine results pages. Information 

Sciences, 179(12), 1796–1812. https://doi.org/10.1016/j.ins.2009.01.028 

Introna, L. D., & Nissenbaum, H. (2000). Shaping the Web: Why the Politics of Search Engines Matters. The 

Information Society, 16(3), 169–185. https://doi.org/10.1080/01972240050133634 

Lawrence, S., & Giles, C. L. (1999). Accessibility of information on the web. Nature, 400(6740), 107–107. 

https://doi.org/10.1038/21987 

Lewandowski, D. (2019). The web is missing an essential part of infrastructure. Communications of the ACM, 62(4), 

 
24–24. https://doi.org/10.1145/3312479 

Lewandowski, D., & Kammerer, Y. (2021). Factors influencing viewing behaviour on search engine results pages: a 

review of eye-tracking research. Behaviour & Information Technology, 40(14), 1485–1515. 
https://doi.org/10.1080/0144929X.2020.1761450 

Lewandowski, D., & Sünkler, S. (2019). What does Google recommend when you want to compare insurance 

offerings? Aslib Journal of Information Management, 71(3), 310–324. https://doi.org/10.1108/AJIM-07-2018-
0172 

Mager, A. (2014). Is Small Really Beautiful? Big Search and Its Alternatives. In R. König & M. Rasch (Eds.), 

Society of the Query Reader (pp. 59–72). Institute of Network Cultures. 

Makhortykh, M., Urman, A., & Ulloa, R. (2020). How search engines disseminate information about COVID-19 

and why they should do better. Harvard Kennedy School Misinformation Review, 1(May), 1–12. 
https://doi.org/10.37016/mr-2020-017 

Meng, W., Yu, C., & Liu, K.-L. (2002). Building efficient and effective metasearch engines. ACM Computing 

Surveys, 34(1), 48–89. https://doi.org/10.1145/505282.505284 

Newman, N., Fletcher, R., Schulz, A., Andı, S., Robertson, C., & Kleis Nielsen, R. (2021). The Reuters Institute 

Digital News Report 2021. https://reutersinstitute.politics.ox.ac.uk/sites/default/files/2021-
06/Digital_News_Report_2021_FINAL.pdf 

Ortega, F., Gonzalez-Barahona, J. M., & Robles, G. (2008). On the Inequality of Contributions to Wikipedia. 

Proceedings of the 41st Annual Hawaii International Conference on System Sciences (HICSS 2008), 304–304. 
https://doi.org/10.1109/HICSS.2008.333 

Purcell, K., Brenner, J., & Rainie, L. (2012). Search Engine Use 2012. https://www.pewresearch.org/internet/wp-

content/uploads/sites/9/media/Files/Reports/2012/PIP_Search_Engine_Use_2012.pdf 

Puschmann, C. (2019). Beyond the Bubble: Assessing the Diversity of Political Search Results. Digital Journalism, 

7(6), 824–843. https://doi.org/10.1080/21670811.2018.1539626 

Schultheiß, S., & Lewandowski, D. (2021). A representative online survey among German search engine users with 
a focus on questions regarding search engine optimization (SEO): a study within the SEO Effect project -
Working Paper 2. https://osf.io/wzhxs 

Spink, A., Jansen, B. J., Blakely, C., & Koshman, S. (2006). A study of results overlap and uniqueness among major 

Web search engines. Information Processing and Management, 42(5), 1379–1391. 
https://doi.org/10.1016/j.ipm.2005.11.001 

StatCounter. (2022). Search Engine Market Share. https://gs.statcounter.com/search-engine-market-share/ 
Steiner, M., Magin, M., Stark, B., & Geiß, S. (2022). Seek and you shall find? A content analysis on the diversity of 
five search engines’ results on political queries. Information, Communication & Society, 25(2), 217–241. 
https://doi.org/10.1080/1369118X.2020.1776367 

Sünkler, S., Lewandowski, D., Schultheiß, S., Yagci, N., Sygulla, D., & von Mach, S. (2022). Relevance Assessment 

Tool. osf.io/t3hg9 

Thelwall, M. (2008). Quantitative comparisons of search engine results. Journal of the American Society for 

Information Science and Technology, 59(11), 1702–1710. https://doi.org/10.1002/asi.20834 

Unkel, J., & Haim, M. (2021). Googling Politics: Parties, Sources, and Issue Ownerships on Google in the 2017 

German Federal Election Campaign. Social Science Computer Review, 39(5), 844–861. 
https://doi.org/10.1177/0894439319881634 

 
 
","A Comparison of Source Distribution and Result Overlap in Web Search Engines AUTHORS SECTION Yagci, Nurce Sünkler, Sebastian Häußler, Helena Lewandowski, Dirk HAW Hamburg, Germany | nurce.yagci@haw-hamburg.de HAW Hamburg, Germany | sebastian.suenkler@haw-hamburg.de HAW Hamburg, Germany | helena.haeuessler@haw-hamburg.de HAW Hamburg, Germany | dirk.lewandowski@haw-hamburg.de ABSTRACT When it comes to search engines, users generally prefer Google. Our study aims to find the differences between the results found in Google compared to other search engines. We compared the top 10 results from Google, Bing, DuckDuckGo, and Metager, using 3,537 queries generated from Google Trends from Germany and the US. Google displays more unique domains in the top results than its competitors. Wikipedia and news websites are the most popular sources overall. With some top sources dominating search results, the distribution of domains is also consistent across all search engines. The overlap between Google and Bing is always under 32%, while Metager has a higher overlap with Bing than DuckDuckGo, going up to 78%. This study shows that the use of another search engine, especially in addition to Google, provides a wider variety in sources and might lead the user to find new perspectives. KEYWORDS Web search; search engine; web scraping; Google; source comparison INTRODUCTION Why should there be more than one search engine? While users may prefer one search engine over others for its usability, specialized features, or a more convenient integration into their technical environment, the question that interests us in this research is whether a user will benefit from using another search engine than Google when it comes to finding results from different sources. Our starting point is the fact that Google is the most-used search engine by far (StatCounter, 2022), that user to a large degree trust search engines to provide them relevant and useful results (European Commission, 2016; Purcell et al., 2012), and that only some users use another search engine in addition to Google (Schultheiß & Lewandowski, 2021). Users place great trust in search engines. This is reflected by the 91% of US users who said they find what they are looking for always or most of the time, and the 66% who believe search engines are a fair and unbiased source of information (Purcell et al., 2012). Furthermore, 78% of European internet and online platform users said they trust that their search engine results are the most relevant results (European Commission, 2016). Globally, users trust search engines more than any other source (including traditional news outlets) when it comes to news (Edelman Trust Institute, 2022) and users trust news found via search significantly more than news found on social media (Newman et al., 2021). As the Web is enormous and different search engines might prefer different sources, it is interesting to see whether the top sources shown in search results differ from one search engine to the other. It might be that an alternative search engine prefers results from ""alternative"" sources, e.g., in terms of political leaning or preferring non- commercial content providers. This all comes down to whether alternative search engines are actually alternatives in regards to the results they display. If they were, possible benefits of using a search engine other than Google include finding different results, finding additional results, and finding more relevant results. No matter which of these goals a user aims to achieve, they will need other results than Google's. Therefore, it is interesting to see whether other search engines provide users with such results. There has been an ongoing discussion on alternative search engines and how Google's dominance in the search engine market can be broken. Approaches range from establishing single alternative search engines to building infrastructures for such alternatives (e.g., Lewandowski, 2019); also see Mager, 2014). With Google dominating the search engine market (StatCounter, 2022), it often seems that there are no alternatives at all. On the other hand, the number of alternative (or simply ""other"") search engines is often overestimated. Many seem-to-be search engines are merely search portals displaying results from a partner instead of generating the results from their own index. For instance, Yahoo and Ecosia get their results from Bing and can therefore not be considered search engines in their own right. But still, there may be other reasons for using a search engine without its own index. Some of the unique benefits alternative search engines advertise are privacy (e.g., Startpage and DuckDuckGo) or being a company investing its profits in environmental projects (e.g., Ecosia). Another type of search engine is the meta search engine (e.g., Metager). Such an engine sends the queries to several other search engines, then aggregates and re-ranks the top results. We deem it especially interesting whether such an approach will lead to a wider variety of search results, i.e., results from a more diverse set of sources. So, in the context of our research, we will consider any search engine that either has its own index or provides a unique selection and re-ranking of results from one or more indexes as an alternative search engine. We are especially interested in the differences in the source distribution; the relevance of the results is out of the scope of our research. More than 20 years ago, Introna & Nissenbaum (2000) argued that search engines as commercial operations tend to prefer big websites and, therefore, a portion of the Web, i.e., the smaller sites, remain hidden from view. Studies measuring what users select seem to confirm this: Goel et al. (2010) found that within Yahoo, only 10,000 websites account for approximately 80% of result clicks. It is important to note that this does not merely result from user preference for particular sources but that users predominantly select from the top results shown by the search engine. What is out of the immediate view of users will not be chosen (Lewandowski & Kammerer, 2021). It is striking that few studies have compared the results between different search engines in recent years. Older studies (see literature review section) overall found that top results from different search engines did not overlap too much. In this paper, we address how the top results of Google differ from alternatives and, therefore, whether it is worthwhile for a user to consider these alternatives. If a search engine other than Google produced very similar results to Google, a user would not benefit much from using that search engine when source variety is considered. LITERATURE REVIEW Search result overlap By the mid-1990s, researching the overlap of search results between different search engines sparked interest for the purpose of estimating the size of the Web (Bharat, 1998; Ding & Marchionini, 1996; Lawrence & Giles, 1999). The generally small degrees of overlap indicated diverse underlying databases, each of limited size. Therefore, meta- search engines that combined the results of several search engines were meant to provide an additional value (Chignell et al., 1999) and attracted further research (i.e., Meng et al., 2002). Consequently, Spink et al. (Spink et al., 2006) included the meta search engine Dogpile in their extensive study on the overlap of search results. For a collection of 12,570 queries, the results found on the first search engine result page (SERP) of Ask Jeeves, Google, MSN Search, and Yahoo were captured. 84.9% of the results were unique to one search engine, while only 1.1% were shared by all search engines. Additionally, the low overlap between search engines also manifests in the ranking since only 7% of the top results were similar. This is consistent with previous studies that reported low overlap in ranking (Bar-Ilan, 2005; Bar‐Ilan et al., 2006). Subsequent studies report an increase of overlap in search results, while ranking algorithms appear to be the leading cause for differences in the presentation of the results. Bilal and Ellis (2011) compared major web search engines (Google, Bing, Yahoo) with search engines specifically designed for children (Ask Kids, Yahoo Kids) for queries of various lengths. Bing and Yahoo shared the same overlap with Google for nearly all queries, ranging between 22% and 40%. In contrast, Yahoo Kids had mostly unique results. However, the disparate relevance ranking between Bing and Yahoo suggests different ranking algorithms. Cardoso and Magalhães (2011) measured the overlap of search result rankings based on URLs and website contents. Search results for 40,000 queries were retrieved from Google and Bing. The overlap in domains was about 29%, and Google had more exclusive domains. When looking at the result sets without considering the positions, the similarity between the result sets increased. This indicates that Google and Bing have different ranking preferences but index mostly the same sources. Similarly, Agrawal et al. (2016) based their overlap analysis on content but factored in the description of the result snippets. The top 10 results of 67 informational queries were collected from Google and Bing. The results indicate a high overlap between Google and Bing, with a slightly higher similarity among the top 5 results. Most recently, a study about information on Covid-19 was conducted by Makhortykh et al. (2020). The first 50 results for queries in different languages were collected from Baidu, Bing, DuckDuckGo, Google, Yandex, and Yahoo. DuckDuckGo and Yahoo shared nearly 50% of their results, whereas the other pairs remained under 25%. Google had an overlap of around 10% with Bing and a negligible one with DuckDuckGo, Yandex, Yahoo, and Baidu. Source diversity Various studies took a closer look at search results and evaluated the domains, the types of sources, as well as their diversity across result sets and search engines. Comparing Google, Live Search, and Yahoo, Thelwall (2008) reported that Yahoo returned the highest amount of different domains to a query and also returned around 10% more top-level domains than other search engines. Höchstötter & Lewandowski (2009) showed that while the sources in top results of various search engines differ, there is a concentration on some popular sources in top results. More recently, Lewandowski & Sünkler (2019) collected search results on queries related to insurance providers and identified the distribution of top-level domains. Only ten different domains were found in the first result across all queries. The five most popular domains in the top 5 results are price comparison websites, which make up 88.4% of all first results. The share of popular domains decreases according to the ranking but remains at 42.9% at position ten. Unkel and Haim (2019) observed Google search results prior to the German parliamentary elections in 2017. The study found that result lists for candidates and parties exhibit a high share of self-administered websites. In contrast, results about election facts, guidance, and issues are mainly from general interest news, government information, and privately run websites. Among the top ten domains across all search results are seven news websites which account for a quarter of all search results. By contrast, Wikipedia made up 5.4% of all search results. The high share of news and Wikipedia articles is confirmed by Steiner et al. (2022) who compared the first results for queries on debated topics in Germany across the search engines Ask, Bing, DuckDuckGo, Google, and Ixquick. All search engines positioned Wikipedia in the first rank for queries on climate change and the Transatlantic Trade and Investment Partnership (TTIP); Ask did so for almost every query. In the other search engines, news sites were placed in the first rank most of the time. However, for the topic Covid-19, the sources are more diverse. As Makhortykh et al. (2020) observed, only Yahoo incorporates recent information from legacy media, whereas Bing strongly relied on healthcare-related sources and Google highlighted government-related websites. Yandex was the only search engine that included alternative media in its top 20 search results. Especially for recent topics that lack an established knowledge basis, the choice of search engine may considerably impact what information a user gets to see. OBJECTIVES AND RESEARCH QUESTIONS Our study addresses the following research questions: 1. Do top results from alternative search engines differ from Google's in regard to the number of unique sources? 2. Do top results from alternative search engines differ from Google's in regard to top sources? 3. Do top results from alternative search engines differ from Google's in regard to source concentration, i.e., are results distributed over more or fewer sources in different search engines? To answer these questions, we selected three alternative search engines to compare to Google. Aside from Bing, which is the biggest competitor to Google, we chose DuckDuckGo and Metager. DuckDuckGo uses results from Bing, Yahoo, and Yandex, and has the added benefit of advanced privacy settings such as non-personalized results. Metager, a German meta-search engine, aggregates results from several search engines, including Bing and Yandex. As such, either search engine might provide a more varied set of sources. Since the alternative search engines each have an index different from Google's, the comparison will give insights into what domains are favored by Google and which domains are excluded. Also, we will gain insights into the differences in ranking between them. METHODS As illustrated in Figure 1, this study was conducted in five steps. In the first step, the query sets were generated from previously collected Google Trends data. The daily trends for both Germany and the US were collected daily at 3 am CET from November 10th, 2021, until March 31st, 2022. Since the daily number of trending queries can vary, the query sets for Germany and the US are not the same size. Two thousand nine hundred sixteen trending queries for Germany and 2,819 for the US were initially collected. However, since some topics of interest are longer lived than a single day, the same topics can be trending on multiple days. Therefore, after removing duplicates, our query sets consist of 1,821 queries for Germany and 2,126 queries for the US. The queries have a wide range, including topics like celebrities (e.g., ""Sandra Bullock""), sports (e.g., ""NFL,"" ""Liverpool vs. Southampton""), seasonal events (e.g., ""Macy's Thanksgiving Day Parade,"" ""Restaurants open on Thanksgiving""), headline news (e.g., ""TikTok class- action lawsuit settlement""), and more. Figure 1. Methodology of the current study In step 2, for each query, the top 10 results from all four search engines were collected. We made sure to request results for the appropriate language and location by using URL parameters. Furthermore, we decided to limit the comparison to the top 10 results because users usually only consider these. Again, for our study, only the implications for the user perception are relevant. The Web scraping component of the Relevance Assessment Tool (RAT; Sünkler et al., 2022) was used to collect the data. The scraping took place between April 6th and 9th, 2022, with 230,315 results (Germany: 109,604; US: 120,711) gathered. It is important to note that only organic search results were considered. A fair comparison can be ensured when ranked result lists instead of vertical inserts of universal search results (e.g., Google News and Bing News) are considered. We also ignored ads since they are not part of the search engine's web index. During step 3, the python library urllib was used to get the domain of each search result. To ensure a fair comparison between all search engines, in step 4, we removed results for any query that had less than ten results in any of the search engines. Additionally, the data was checked and stripped of any errors that might have occurred during the collection, like duplicate results. This resulted in a further reduced dataset consisting of 1,672 queries and 66,880 results for Germany and 1,865 queries and 74,600 results for the US. Lastly, the extraction of top-level domains was further refined in this step. We used basic string matching to unify URLs that pointed to the same domains. For example, the following three URLs would have been counted as separate domains without the string matching: https://www.facebook.com, ""http://www.facebook.com"", and ""http://www.facebook.com/"". Finally, in step five, we used different methods of comparing the data collected. An initial comparison is made by using simple descriptive statistics. To measure source concentration, we adapt the Gini coefficient, a measure of statistical dispersion used to measure income or wealth inequality (Gini, 1936). The Gini coefficient is a single number ranging from 0 to 1, where 0 represents perfect equality, and 1 is the maximum inequality. As Ortega et al. (2008) demonstrated, adapting this measure to the distribution of values in different domains is appropriate. In the case of search result sets, this allows us to compare the various search engines easily. The lower the Gini coefficient, the more equally the results are distributed over all domains contributing to a particular result set. Following that, we calculate the Jaccard similarity index (González et al., 2008). Puschmann (2019) demonstrates the usage of the Jaccard index to measure the similarity between two result sets. The Jaccard index is a single number ranging from 0 to 1, where 0 means that the two sets are entirely dissimilar, while 1 means that they are identical. We calculate the Jaccard index for every two-way combination of our four search engines (e.g., Google and Bing, Bing and Metager, etc.) for all stages between the top 1 and top 10 results (e.g., Google and Bing position 1 to 5). RESULTS Classification of domains Since we are working with sources from two different countries and the type of content behind some domains might not be immediately recognizable, we manually created a general set of categories based on the top 50 domains found across all results. Following this, we manually classified the 50 most popular domains for both the German and US results (see Table 1). When comparing the top 50 domains in the German results, News service dominates with 54%, while Movies & Entertainment and Sports make up 18% and 14%, respectively. In the US, 34% of the top 50 domains are Sports websites, with News services following close behind at 30%. Movies & Entertainment only make up 12%. These are the overall distributions of domain classes across all search engines; below, we look at the differences between the search engines. Example Top 50 Germany (share) Top 50 US (share) Class Celebrities E-Commerce variety.com amazon.com Government website bundesregierung.de Information service wikipedia.org Movies & Entertainment News service Sports imdb.com cnn.com espn.com Social media instagram.com 0.06 0.02 0 0.02 0.18 0.54 0.14 0.04 0.10 0.06 0 0.02 0.12 0.30 0.34 0.06 Table 1. Classes of domains and their frequency in the top 50 domains Variety of domains A comparison of the number of root domains in the search results of Google, Bing, DuckDuckGo, and Metager in Germany shows that Google has the greatest diversity by a small margin (see Figure 2). Overall, the values are very similar. However, it is noticeable that Google has the greatest variety of domains, especially in the first three positions. Interestingly, the greater diversity of Google's sources is even more pronounced in the US results. To examine the differences more closely, we look at the numbers below. Figure 2. Cumulative number of unique domains Table 2 shows the cumulative frequencies for the German results. The number of unique root domains converges at the fifth position. We found 2,841 unique domains in Google's results, 2,783 unique domains for Bing, 2,707 unique domains for DuckDuckGo, and 2,683 unique domains for Metager in Germany. Position 1 2 3 4 5 Google Bing DuckDuckGo Metager # 609 959 1,216 1,480 1,703 share 0.21 0.34 0.43 0.52 0.60 # 389 778 1,072 1,325 1,602 share 0.14 0.28 0.39 0.48 0.58 # 302 544 900 1,232 1,530 share 0.11 0.20 0.33 0.46 0.57 # 372 735 1,027 1,298 1,531 share 0.14 0.27 0.38 0.48 0.57 Position 6 7 8 9 10 Google Bing DuckDuckGo Metager # share # share # share # share 1,891 2,092 2,329 2,577 2,841 0.67 0.74 0.82 0.91 1.00 1,853 2,071 2,292 2,528 2,783 0.67 0.74 0.82 0.91 1.00 1,773 2,047 2,272 2,497 2,707 0.65 0.76 0.84 0.92 1.00 1,785 2,008 2,222 2,453 2,693 0.66 0.75 0.83 0.91 1.00 Table 2. Cumulative number of unique domains (Germany; #: cumulative number, %: percentage of total) There are differences when comparing the search results in Germany and the US. It is notable that, in contrast to Germany, in the US results, there are only minor differences in the first positions. However, Google also shows the greatest variety of root domains in the search results in the US. This is more evident than in the German results (see Table 3). Overall, we found 4,085 unique domains in Google, 3,602 in Bing, 3,579 in DuckDuckGo, and 3,500 in Metager. Position 1 2 3 4 5 6 7 8 9 10 Google Bing DuckDuckGo Metager # 508 961 1,398 1,828 2,188 2,531 2,896 3,289 3,682 4,085 share 0.12 0.24 0.34 0.45 0.54 0.62 0.71 0.81 0.90 1.00 # 521 851 1,164 1,491 1,822 2,168 2,521 2,870 3,231 3,602 share 0.14 0.24 0.32 0.41 0.51 0.60 0.70 0.80 0.90 1.00 # 493 849 1,210 1,547 1,860 2,208 2,546 2,888 3,207 3,579 share 0.14 0.24 0.34 0.43 0.52 0.62 0.71 0.81 0.90 1.00 # 485 820 1,130 1,445 1,772 2,110 2,442 2,785 3,133 3,500 share 0.14 0.23 0.32 0.41 0.51 0.60 0.70 0.80 0.90 1.00 Table 3. Cumulative number of unique domains (USA; #: cumulative number, %: percentage of total) Popular domains When comparing the top domains for each search engine across all German search results collected, we find a clear preference for Wikipedia in all of them (see Table 4). While Wikipedia is the most popular domain in all search engines, the frequency in the Google results is significantly lower, with only 658 compared to the number of occurrences in Bing being 1,948, in Metager 1,878, and in DuckDuckGo 1,752. The other domains in the top 10 across the search engines are News services (Google: 7, Bing: 4, DuckDuckGo: 3, Metager: 4). Sports make up 2 out of 10 for all but Google, with only one sports website. Surprisingly, Instagram is in the top 10 for DuckDuckGo and Metager, even though in Table 1, we showed that domains of the class Social Media only make up 4% of top domains in the German results. The same goes for Amazon, which is the second most frequent domain in DuckDuckGo, even though in the overall results, E-Commerce websites only make up 2%. No. Google Bing DuckDuckGo Metager 1 2 3 wikipedia.org (658) wikipedia.org (1,948) wikipedia.org (1,752) wikipedia.org (1,878) spiegel.de (414) stern.de (375) zdf.de (324) bild.de (298) amazon.de (735) zdf.de (344) zdf.de (298) bunte.de (314) No. Google Bing DuckDuckGo Metager 4 5 6 7 8 9 gala.de (300) bild.de (288) bunte.de (296) kicker.de (261) bild.de (290) kicker.de (294) bild.de (255) kicker.de (285) t-online.de (279) gala.de (279) tagesschau.de (251) gala.de (282) tagesschau.de (271) tagesschau.de (274) wunderweib.de (248) tagesschau.de (268) kicker.de (222) sportschau.de (261) bunte.de (245) sportschau.de (256) zdf.de (219) web.de (252) instagram.com (243) web.de (238) 10 sueddeutsche.de (219) instagram.com (245) sportschau.de (242) instagram.com (232) Table 4. Top 10 domains shown in different search engines (Germany) Contrarily, in the US results, Wikipedia has the highest number of occurrences in Google with 1,892 compared to Bing's 1,388, Metager's 1,304, and DuckDuckGo's 1,287 (see Table 5). Still, what remains the same is that Wikipedia is the most popular domain across all search engines. Another difference in the US results is that Social Media sources are much more prevalent across all search engines, especially in Google, with Instagram as the second most frequent domain, Facebook as the third, and Twitter as the sixth. This is surprising when considering that Social Media domains only make up 6% of the top domains for the US results (see Table 1). Another finding is that YouTube, a subsidiary of Google, is the top 8th domain for all three alternatives but not for Google. Again, 30% of the top domains for Google, DuckDuckGo, and Metager are News services. For Bing, it is 40%. For each search engine, Sports websites make up 20% of domains. No. Google Bing DuckDuckGo Metager 1 2 3 4 5 6 7 8 9 wikipedia.org (1,892) wikipedia.org (1,388) wikipedia.org (1,287) wikipedia.org (1,304) instagram.com (726) imdb.com (669) imdb.com (644) yahoo.com (714) facebook.com (503) yahoo.com (634) yahoo.com (642) imdb.com (680) espn.com (377) espn.com (610) espn.com (601) espn.com (614) cbssports.com (363) nypost.com (518) nypost.com (536) nypost.com (514) twitter.com (328) cbssports.com (511) cbssports.com (513) cnn.com (504) imdb.com (244) cnn.com (495) cnn.com (506) cbssports.com (502) britannica.com (226) youtube.com (414) youtube.com (352) youtube.com (417) usatoday.com (222) msn.com (322) go.com (320) msn.com (324) 10 nytimes.com (220) instagram.com (318) instagram.com (281) instagram.com (288) Table 5. Top 10 domains for each search engine (USA) Exclusivity of domains This section compares the top 50 domains for each search engine to determine what source Google and its alternatives might exclusively offer to the user. Table 6 shows the domains found solely in Google's top 50 domains list for the German queries. Out of these 20 domains, eight are Sports websites, six are News services, and four belong to the Entertainment & Movies class, with one each for E-Commerce and Government websites. ran.de amazon.de dazn.com sky.de eurosport.de zeit.de dfb.de kino.de fr.de fernsehserien.de tagesspiegel.de filmstarts.de bundesregierung.de tz.de weltfussball.de spox.com sportbuzzer.de deutschlandfunk.de rnd.de dw.com Table 6. Domains only found in Google's top 50 (Germany) On the contrary, table 7 shows the list of domains found in the top 50 domains of all three alternatives but not in Google. These would be the domains missed by users who only use Google. Out of the 13 domains, four are News services, followed by three Movies & Entertainment and two Celebrities sources. Interestingly, Instagram and YouTube are missing from Google, as well. sport.de instagram.com wunderweib.de fussballdaten.de promipool.de n-tv.de abendzeitung-muenchen.de finanzen.net youtube.com imdb.com ardmediathek.de express.de daserste.de Table 7. Domains found in all search engines but Google's top 50 (Germany) When implementing the same evaluation for the US results, we find that 17 domains are exclusively found in Google's top 50 domains, with five domains classified as Sports, four as Movies & Entertainment, another four as News services, three as Social Media and one as Celebrities (see Table 8). Notable is that social media giants Facebook and TikTok are only found in Google and not in the alternatives. marca.com reuters.com rotowire.com facebook.com tiktok.com usmagazine.com npr.org discogs.com spotify.com theathletic.com spotrac.com linkedin.com deadline.com forbes.com variety.com washingtonpost.com sports-reference.com Table 8. Domains only found in Google's top 50 (US) On the opposite end, 13 domains are not found in the US results on Google (see Table 9). Six of them are News services, three are websites about Celebrities, two are about Movies & Entertainment, and one each are Sports and E-Commerce websites. Interestingly, Amazon is missing in Google's results, as well as msn.com, a service from Microsoft, like Bing itself. amazon.com yardbarker.com msn.com cnn.com pagesix.com screenrant.com heavy.com foxnews.com decider.com tmz.com newsweek.com huffpost.com biography.com Table 9. Domains found in all search engines but Google's top 50 (US) Distribution of domains Next, we look at the Gini coefficient to measure the concentration and statistical dispersion of root domains in search engines. The distributions across all search engines in both German and US results show very similar values. The results range from 0.77 to 0.79 and 0.73 to 0.76 for German and US results, respectively (see Figure 3). Overall, this means that there is an imbalance in the distribution of results from root domains and that some top sources dominate the search results for both countries. Figure 3. Source distribution of domains (Germany) Similarity of domains Comparing the similarities of the top 10 German results, we find that Bing and Metager are most similar in terms of their top results, with 70%, followed by 64% overlap between DuckDuckGo and Metager and 63% between Bing and DuckDuckGo. However, when comparing the alternative search engines to Google, we find that the highest overlap is between Google and Bing with 28% and slightly lower ones with DuckDuckGo and Metager at 27%. Interestingly, the top 1 result overlap is higher between Google and Bing (31%) than the overlap between Bing and DuckDuckGo (15%). The results show a mean overlap of 30% of top 1, 40% of top 3, and 47% of top 10 results (table 10). This indicates that when looking at the entire first search results page, nearly half of the domains are the same in all search engines considered. For search engine users, this means that in addition to Google, this would also allow them to see search results that they would otherwise miss if they used the alternative search engine Rank Top 1 Top 3 Top 10 Google Bing Google DDG Google Metager 0.31 0.30 0.28 0.10 0.21 0.27 0.29 0.30 0.27 Bing DDG 0.15 0.39 0.63 Bing Metager DDG Metager 0.78 0.78 0.70 0.15 0.39 0.64 Mean 0.30 0.40 0.47 Table 10. Jaccard similarity of domains (Germany; DDG: DuckDuckGo) The same evaluation for the US yields similar results. Again, the overlap between Bing and Metager is the highest, but the margin between this pair and the other alternative pairings has been reduced. When looking at the top 10 results, Bing and Metager overlap by 65%, Bing and DuckDuckGo by 64%, and DuckDuckGo and Metager by 62%. While Google's overlap with the other search engines is lower in the US results, again, the pair of Google and Bing is slightly higher (25%) than the 24% overlap between Google and the other two alternatives (see Table 11). Narrowing the results down to the top 3, the overlap between Bing and DuckDuckGo increases minimally to 65% and remains the same for DuckDuckGo and Metager at 62%. Here, the overlap between Google and Bing is lower than in the German results, with only 26%. Furthermore, the top 1 result overlap between Google and Bing is at 29%, which is also lower than in the German results. Generally, compared to the German results, there is a higher overlap in the top 1 position for the pairs Google and DuckDuckGo (20%), Bing and DuckDuckGo (50%), and DuckDuckGo and Metager (47%). The averaged results make this trend clearer because the overlaps for the top 1, top 3, and top 10 results are similar, with 41%, 46%, and 44%, respectively. Rank Top 1 Top 3 Top 10 Google Bing Google DDG Google Metager 0.29 0.26 0.25 0.20 0.24 0.24 0.28 0.25 0.24 Bing DDG 0.50 0.65 0.64 Bing Metager DDG Metager 0.74 0.71 0.65 0.47 0.62 0.62 Mean 0.41 0.46 0.44 Table 11. Jaccard similarity of domains (US; DDG: DuckDuckGo) DISCUSSION Our study examined whether there are differences in the sources of the top search results between Google and alternative search engines based on popular search queries. An evaluation of root domains of the most popular sources shows only a small overlap between Google and the alternative search engines (RQ1). Overall, we found an overlap of 27% to 28% between Google and the alternatives in German results, and in the US, the overlap ranges from 24% to 25%. There is a significantly higher overlap between the alternative search engines of about 63% to 70% in German results and 62% to 65% in US results. This may be explained by all three alternative search engines using Bing's index at least in part. However, our findings show that Metager consistently has the highest overlap with Bing, going up to 78% and only overlapping as much as 64% with DuckDuckGo. The lower overlap of Google with the alternative search engines had already been shown in the studies of Agrawal et al. (2016) and Makhortykh et al. (2020). Our study provides further evidence for this. When looking at the uniqueness of sources across all German queries, we found that the search engines returned a similar total number of sources, ranging from 2,693 to 2,841. However, for the US queries, the variety in Google was noticeably higher, with over 4,000 total sources compared to around 3,500 of the alternatives. The most popular domain was Wikipedia, followed by sources we classified as News services (RQ2). This is likely exacerbated by selecting Google Trends as the source of our search queries, which usually includes queries related to popular news stories, sports, and celebrities. Still, it is consistent with previous studies' findings (Steiner et al., 2022) which already showed that Wikipedia and news made up the majority of search results. Furthermore, in our research, Wikipedia and news sources were the top domains across all results and the most frequent sources for each search engine individually. When comparing the result sets of the top 10 results from Google, Bing, DuckDuckGo, and Metager, using 3,537 queries generated from Google Trends from Germany and the US, it is interesting to see that news sources are far more prevalent in German results, with social media being very infrequent. On the other hand, the US results had more social media websites. Interestingly, in the US results, YouTube was in the top 10 most popular domains in all search engines but Google. The same was the case for the top 50 domains in German results. This is unexpected because YouTube is a subsidiary of Google. However, this finding may be explained by the fact that we only collected organic search results, and Google might be using universal search results to display YouTube results. Another interesting difference is the greater preference of Wikipedia in Google in US results (1,892, 10,2% of all domains) compared to German results (658, 4% of all domains). Furthermore, the second and third most popular sources on Google are Instagram and Facebook for the US results, while they are not even in the top 10 of German Google sources. Finally, in terms of what is missing from Google in the US results, it is notable that Fox News is not found in the top 50 sources, while it is present in all of the alternatives. The concentration of sources and source diversity showed a tendency for only a few root domains to make up a large share of search results. The Gini Index values of 0.73 and 0.79 in Germany and the United States, respectively, are a clear indicator (RQ3). This is consistent with findings from previous studies (Höchstötter & Lewandowski, 2009) that showed that only a few top sources dominate the search results in search engines. Of course, our study is not without limitations. First, the selection of search queries is the most significant factor in compiling the data that was evaluated. Even though the number of queries is high and there is some diversity in the queries, the topics are almost always focused on news, celebrities, and sports, which inevitably leads to many news sources in the search results. A refined approach to selecting search queries would be appropriate for a more accurate evaluation of source diversity. For example, this could be achieved by focusing on socially controversial topics and choosing the queries accordingly. Further limitations arise in the evaluation of source types. The classification we used in this study is very broad. A more precise classification would help make statements about the actual kinds of sources and thus to determine even more precisely which preferences and biases exist in different search engines. For example, grouping sources according to seriousness and reliability could serve as an explanation for the selection of sources to be displayed in search results. Regarding the search results collected in this study, another limitation is that only organic search results were considered. We did not consider advertisements or universal search results, although these have a strong influence on what users see on the SERP. The relevance of the search results was also not considered. CONCLUSION This study provides important insights into whether, although Google is by far the most popular search engine, the use of alternatives could benefit users. Our results show that using another or more than one search engine leads to seeing more diverse search results, allowing users to inform themselves more comprehensively. It should be noted that within each search engine's results, the concentration of sources shows that only a few top sources dominate the results, meaning whichever search engine a user chooses to use will shape what sources the information they get to see comes from. RESEARCH DATA Research data is available at: https://osf.io/nt3wv/ ACKNOWLEDGMENTS This work is funded by the German Research Foundation (DFG – Deutsche Forschungsgemeinschaft; Grant No. 460676551). REFERENCES Agrawal, R. (2016). Overlap in the Web Search Results of Google and Bing. Journal of Web Science, 2(1), 17–30. https://doi.org/10.1561/106.00000005 Bar-Ilan, J. (2005). Comparing rankings of search results on the Web. Information Processing & Management, 41(6), 1511–1519. https://doi.org/10.1016/J.IPM.2005.03.008 Bar‐Ilan, J., Levene, M., & Mat‐Hassan, M. (2006). Methods for evaluating dynamic changes in search engine rankings: a case study. Journal of Documentation, 62(6), 708–729. https://doi.org/10.1108/00220410610714930 Bharat, K. (1998). A technique for measuring the relative size and overlap of public Web search engines. Computer Networks and ISDN Systems, 30(1–7), 379–388. https://doi.org/10.1016/S0169-7552(98)00127-5 Bilal, D., & Ellis, R. (2011). Evaluating Leading Web Search Engines on Children’s Queries. In Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Vol. 6764 LNCS (Issue PART 4, pp. 549–558). https://doi.org/10.1007/978-3-642-21619- 0_67 Cardoso, B., & Magalhães, J. (2011). Google, bing and a new perspective on ranking similarity. Proceedings of the 20th ACM International Conference on Information and Knowledge Management - CIKM ’11, 1933–1936. https://doi.org/10.1145/2063576.2063858 Chignell, M. H., Gwizdka, J., & Bodner, R. C. (1999). Discriminating meta-search: a framework for evaluation. Information Processing & Management, 35(3), 337–362. https://doi.org/10.1016/S0306-4573(98)00065-X Ding, W., & Marchionini, G. (1996). A comparative study of web search service performance. Proceedings of the ASIS Annual Meeting, 33, 136–142. https://eric.ed.gov/?id=EJ557172 Edelman Trust Institute. (2022). Edelman Trust Barometer 2022 - Global Report. https://www.edelman.com/sites/g/files/aatuss191/files/2022-01/2022 Edelman Trust Barometer FINAL_Jan25.pdf European Commission. (2016). Special Eurobarometer 447 – Online Platforms. European Commission. https://doi.org/10.2759/937517 Gini, C. (1936). On the measure of concentration with special reference to income and statistics. Colorado College Publication, General Series, 208(1), 73–79. Goel, S., Broder, A., Gabrilovich, E., & Pang, B. (2010). Anatomy of the long tail. Proceedings of the Third ACM International Conference on Web Search and Data Mining - WSDM ’10, 201. https://doi.org/10.1145/1718487.1718513 González, C. G., Bonventi, W., & Rodrigues, A. L. V. (2008). Density of Closed Balls in Real-Valued and Autometrized Boolean Spaces for Clustering Applications. In Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Vol. 5249 LNAI (pp. 8–22). Springer Verlag. https://doi.org/10.1007/978-3-540-88190-2_7 Höchstötter, N., & Lewandowski, D. (2009). What users see – Structures in search engine results pages. Information Sciences, 179(12), 1796–1812. https://doi.org/10.1016/j.ins.2009.01.028 Introna, L. D., & Nissenbaum, H. (2000). Shaping the Web: Why the Politics of Search Engines Matters. The Information Society, 16(3), 169–185. https://doi.org/10.1080/01972240050133634 Lawrence, S., & Giles, C. L. (1999). Accessibility of information on the web. Nature, 400(6740), 107–107. https://doi.org/10.1038/21987 Lewandowski, D. (2019). The web is missing an essential part of infrastructure. Communications of the ACM, 62(4), 24–24. https://doi.org/10.1145/3312479 Lewandowski, D., & Kammerer, Y. (2021). Factors influencing viewing behaviour on search engine results pages: a review of eye-tracking research. Behaviour & Information Technology, 40(14), 1485–1515. https://doi.org/10.1080/0144929X.2020.1761450 Lewandowski, D., & Sünkler, S. (2019). What does Google recommend when you want to compare insurance offerings? Aslib Journal of Information Management, 71(3), 310–324. https://doi.org/10.1108/AJIM-07-2018- 0172 Mager, A. (2014). Is Small Really Beautiful? Big Search and Its Alternatives. In R. König & M. Rasch (Eds.), Society of the Query Reader (pp. 59–72). Institute of Network Cultures. Makhortykh, M., Urman, A., & Ulloa, R. (2020). How search engines disseminate information about COVID-19 and why they should do better. Harvard Kennedy School Misinformation Review, 1(May), 1–12. https://doi.org/10.37016/mr-2020-017 Meng, W., Yu, C., & Liu, K.-L. (2002). Building efficient and effective metasearch engines. ACM Computing Surveys, 34(1), 48–89. https://doi.org/10.1145/505282.505284 Newman, N., Fletcher, R., Schulz, A., Andı, S., Robertson, C., & Kleis Nielsen, R. (2021). The Reuters Institute Digital News Report 2021. https://reutersinstitute.politics.ox.ac.uk/sites/default/files/2021- 06/Digital_News_Report_2021_FINAL.pdf Ortega, F., Gonzalez-Barahona, J. M., & Robles, G. (2008). On the Inequality of Contributions to Wikipedia. Proceedings of the 41st Annual Hawaii International Conference on System Sciences (HICSS 2008), 304–304. https://doi.org/10.1109/HICSS.2008.333 Purcell, K., Brenner, J., & Rainie, L. (2012). Search Engine Use 2012. https://www.pewresearch.org/internet/wp- content/uploads/sites/9/media/Files/Reports/2012/PIP_Search_Engine_Use_2012.pdf Puschmann, C. (2019). Beyond the Bubble: Assessing the Diversity of Political Search Results. Digital Journalism, 7(6), 824–843. https://doi.org/10.1080/21670811.2018.1539626 Schultheiß, S., & Lewandowski, D. (2021). A representative online survey among German search engine users with a focus on questions regarding search engine optimization (SEO): a study within the SEO Effect project - Working Paper 2. https://osf.io/wzhxs Spink, A., Jansen, B. J., Blakely, C., & Koshman, S. (2006). A study of results overlap and uniqueness among major Web search engines. Information Processing and Management, 42(5), 1379–1391. https://doi.org/10.1016/j.ipm.2005.11.001 StatCounter. (2022). Search Engine Market Share. https://gs.statcounter.com/search-engine-market-share/ Steiner, M., Magin, M., Stark, B., & Geiß, S. (2022). Seek and you shall find? A content analysis on the diversity of five search engines’ results on political queries. Information, Communication & Society, 25(2), 217–241. https://doi.org/10.1080/1369118X.2020.1776367 Sünkler, S., Lewandowski, D., Schultheiß, S., Yagci, N., Sygulla, D., & von Mach, S. (2022). Relevance Assessment Tool. osf.io/t3hg9 Thelwall, M. (2008). Quantitative comparisons of search engine results. Journal of the American Society for Information Science and Technology, 59(11), 1702–1710. https://doi.org/10.1002/asi.20834 Unkel, J., & Haim, M. (2021). Googling Politics: Parties, Sources, and Issue Ownerships on Google in the 2017 German Federal Election Campaign. Social Science Computer Review, 39(5), 844–861. https://doi.org/10.1177/0894439319881634","['comparison', 'source', 'distribution', 'result', 'overlap', 'web', 'search', 'engine', 'author', 'section', 'yagci', 'nurce', 'sünkler', 'sebastian', 'häußler', 'helena', 'lewandowski', 'dirk', 'helenahaeuesslerhawhamburgde', 'abstract', 'come', 'search', 'engine', 'user', 'generally', 'prefer', 'study', 'aim', 'find', 'difference', 'result', 'find', 'compare', 'search', 'engine', 'compare', 'top', 'result', 'bing', 'duckduckgo', 'metager', 'use', 'query', 'generate', 'trend', 'display', 'unique', 'domain', 'top', 'result', 'competitor', 'wikipedia', 'news', 'website', 'popular', 'source', 'overall', 'top', 'source', 'dominate', 'search', 'result', 'distribution', 'domain', 'also', 'consistent', 'search', 'engine', 'overlap', 'bing', 'always', 'metager', 'high', 'overlap', 'bing', 'duckduckgo', 'go', 'study', 'show', 'use', 'search', 'engine', 'especially', 'addition', 'provide', 'wide', 'variety', 'source', 'lead', 'user', 'find', 'new', 'perspective', 'keyword', 'web', 'search', 'search', 'engine', 'web', 'scrape', 'source', 'comparison', 'introduction', 'search', 'engine', 'user', 'prefer', 'search', 'engine', 'usability', 'specialize', 'feature', 'convenient', 'integration', 'technical', 'environment', 'question', 'interest', 'research', 'user', 'benefit', 'use', 'search', 'engine', 'come', 'find', 'result', 'different', 'source', 'starting', 'point', 'fact', 'mostuse', 'search', 'engine', 'far', 'statcounter', 'user', 'large', 'degree', 'trust', 'search', 'engine', 'provide', 'relevant', 'useful', 'result', 'purcell', 'et', 'user', 'use', 'search', 'engine', 'addition', 'lewandowski', 'user', 'place', 'great', 'trust', 'search', 'engine', 'reflect', 'user', 'say', 'find', 'look', 'always', 'time', 'believe', 'search', 'engine', 'fair', 'unbiased', 'source', 'information', 'purcell', 'et', 'furthermore', 'european', 'internet', 'online', 'platform', 'user', 'say', 'trust', 'search', 'engine', 'result', 'relevant', 'result', 'globally', 'user', 'trust', 'search', 'engine', 'source', 'include', 'traditional', 'news', 'outlet', 'come', 'news', 'user', 'trust', 'news', 'find', 'search', 'significantly', 'news', 'find', 'social', 'medium', 'newman', 'web', 'enormous', 'different', 'search', 'engine', 'prefer', 'different', 'source', 'interesting', 'see', 'top', 'source', 'show', 'search', 'result', 'differ', 'search', 'engine', 'alternative', 'search', 'engine', 'prefer', 'result', 'alternative', 'source', 'eg', 'term', 'political', 'lean', 'prefer', 'non', 'commercial', 'content', 'provider', 'come', 'alternative', 'search', 'engine', 'actually', 'alternative', 'regard', 'result', 'display', 'possible', 'benefit', 'use', 'search', 'engine', 'include', 'find', 'different', 'result', 'find', 'additional', 'result', 'find', 'relevant', 'result', 'matter', 'goal', 'user', 'aim', 'achieve', 'need', 'result', 'google', 'therefore', 'interesting', 'see', 'search', 'engine', 'provide', 'user', 'result', 'ongoing', 'discussion', 'alternative', 'search', 'engine', 'google', 'dominance', 'search', 'engine', 'market', 'break', 'approach', 'range', 'establish', 'single', 'alternative', 'search', 'engine', 'build', 'infrastructure', 'alternative', 'eg', 'lewandowski', 'also', 'see', 'mager', 'dominate', 'search', 'engine', 'market', 'statcounter', 'often', 'seem', 'alternative', 'hand', 'number', 'alternative', 'simply', 'search', 'engine', 'often', 'overestimate', 'many', 'seemtobe', 'search', 'engine', 'merely', 'search', 'portal', 'display', 'result', 'partner', 'instead', 'generate', 'result', 'index', 'instance', 'yahoo', 'ecosia', 'get', 'result', 'bing', 'therefore', 'consider', 'search', 'engine', 'right', 'still', 'reason', 'use', 'search', 'engine', 'index', 'unique', 'benefit', 'alternative', 'search', 'engine', 'advertise', 'privacy', 'eg', 'startpage', 'duckduckgo', 'company', 'invest', 'profit', 'environmental', 'project', 'eg', 'ecosia', 'type', 'search', 'engine', 'meta', 'search', 'engine', 'eg', 'metager', 'engine', 'send', 'query', 'several', 'search', 'engine', 'aggregate', 'rerank', 'top', 'result', 'deem', 'especially', 'interesting', 'approach', 'lead', 'wide', 'variety', 'search', 'result', 'result', 'diverse', 'set', 'source', 'context', 'research', 'consider', 'search', 'engine', 'index', 'provide', 'unique', 'selection', 'reranking', 'result', 'index', 'alternative', 'search', 'engine', 'especially', 'interested', 'difference', 'source', 'distribution', 'relevance', 'result', 'scope', 'research', 'year', 'ago', 'argue', 'search', 'engine', 'commercial', 'operation', 'tend', 'prefer', 'big', 'website', 'therefore', 'portion', 'web', 'small', 'site', 'remain', 'hidden', 'view', 'study', 'measure', 'user', 'select', 'seem', 'confirm', 'goel', 'find', 'website', 'account', 'approximately', 'result', 'click', 'important', 'note', 'merely', 'result', 'user', 'preference', 'particular', 'source', 'user', 'predominantly', 'select', 'top', 'result', 'show', 'search', 'engine', 'immediate', 'view', 'user', 'choose', 'lewandowski', 'kammerer', 'strike', 'study', 'compare', 'result', 'different', 'search', 'engine', 'recent', 'year', 'old', 'study', 'see', 'literature', 'review', 'section', 'overall', 'find', 'top', 'result', 'different', 'search', 'engine', 'overlap', 'much', 'paper', 'address', 'top', 'result', 'differ', 'alternative', 'therefore', 'worthwhile', 'user', 'consider', 'alternative', 'search', 'engine', 'produce', 'similar', 'result', 'google', 'user', 'benefit', 'much', 'use', 'search', 'engine', 'source', 'variety', 'consider', 'literature', 'review', 'search', 'result', 'overlap', 'mid1990s', 'research', 'overlap', 'search', 'result', 'different', 'search', 'engine', 'spark', 'interest', 'purpose', 'estimate', 'size', 'web', 'bharat', 'marchionini', 'lawrence', 'gile', 'generally', 'small', 'degree', 'overlap', 'indicate', 'diverse', 'underlie', 'database', 'limited', 'size', 'therefore', 'meta', 'search', 'engine', 'combine', 'result', 'several', 'search', 'engine', 'mean', 'provide', 'additional', 'value', 'chignell', 'attract', 'research', 'consequently', 'spink', 'include', 'meta', 'search', 'engine', 'dogpile', 'extensive', 'study', 'overlap', 'search', 'result', 'collection', 'query', 'result', 'find', 'first', 'search', 'engine', 'result', 'page', 'serp', 'search', 'capture', 'result', 'unique', 'search', 'engine', 'share', 'search', 'engine', 'additionally', 'low', 'overlap', 'search', 'engine', 'also', 'manifest', 'ranking', 'top', 'result', 'similar', 'consistent', 'previous', 'study', 'report', 'low', 'overlap', 'rank', 'barilan', 'bar‐ilan', 'subsequent', 'study', 'report', 'increase', 'overlap', 'search', 'result', 'rank', 'algorithm', 'appear', 'lead', 'cause', 'difference', 'presentation', 'result', 'bilal', 'compare', 'major', 'web', 'search', 'engine', 'google', 'bing', 'yahoo', 'search', 'engine', 'specifically', 'design', 'child', 'ask', 'kid', 'yahoo', 'kid', 'query', 'various', 'length', 'bing', 'share', 'overlap', 'query', 'range', 'contrast', 'yahoo', 'kid', 'mostly', 'unique', 'result', 'however', 'disparate', 'relevance', 'rank', 'bing', 'suggest', 'different', 'rank', 'algorithm', 'cardoso', 'magalhães', 'measure', 'overlap', 'search', 'result', 'ranking', 'base', 'url', 'website', 'content', 'search', 'result', 'query', 'retrieve', 'bing', 'overlap', 'domain', 'exclusive', 'domain', 'look', 'result', 'set', 'consider', 'position', 'similarity', 'result', 'set', 'increase', 'indicate', 'bing', 'different', 'ranking', 'preference', 'index', 'mostly', 'source', 'similarly', 'agrawal', 'base', 'overlap', 'analysis', 'content', 'factor', 'description', 'result', 'snippet', 'top', 'result', 'informational', 'query', 'collect', 'bing', 'result', 'indicate', 'high', 'overlap', 'bing', 'slightly', 'high', 'similarity', 'top', 'result', 'recently', 'study', 'information', 'covid19', 'conduct', 'makhortykh', 'first', 'result', 'query', 'different', 'language', 'collect', 'baidu', 'bing', 'duckduckgo', 'yahoo', 'duckduckgo', 'share', 'nearly', 'result', 'pair', 'remain', 'google', 'overlap', 'bing', 'negligible', 'one', 'duckduckgo', 'yandex', 'yahoo', 'baidu', 'source', 'diversity', 'various', 'study', 'take', 'close', 'look', 'search', 'result', 'evaluate', 'domain', 'type', 'source', 'well', 'diversity', 'result', 'set', 'search', 'engine', 'compare', 'live', 'search', 'thelwall', 'report', 'return', 'high', 'amount', 'different', 'domain', 'query', 'also', 'return', 'around', 'toplevel', 'domain', 'search', 'engine', 'höchstötter', 'lewandowski', 'show', 'source', 'top', 'result', 'various', 'search', 'engine', 'differ', 'concentration', 'popular', 'source', 'top', 'result', 'recently', 'lewandowski', 'sünkler', 'collect', 'search', 'result', 'query', 'relate', 'insurance', 'provider', 'identify', 'distribution', 'toplevel', 'domain', 'different', 'domain', 'find', 'first', 'result', 'query', 'popular', 'domain', 'top', 'result', 'price', 'comparison', 'website', 'make', 'first', 'result', 'share', 'popular', 'domain', 'decrease', 'accord', 'ranking', 'remain', 'position', 'unkel', 'haim', 'observed', 'search', 'result', 'prior', 'german', 'parliamentary', 'election', 'study', 'find', 'result', 'list', 'candidate', 'party', 'exhibit', 'high', 'share', 'selfadministered', 'website', 'contrast', 'result', 'election', 'fact', 'guidance', 'issue', 'mainly', 'general', 'interest', 'news', 'government', 'information', 'privately', 'run', 'website', 'top', 'domain', 'search', 'result', 'news', 'website', 'account', 'quarter', 'search', 'result', 'contrast', 'wikipedia', 'make', 'search', 'result', 'high', 'share', 'news', 'article', 'confirm', 'compare', 'first', 'result', 'query', 'debate', 'topic', 'search', 'engine', 'ask', 'bing', 'duckduckgo', 'google', 'ixquick', 'search', 'engine', 'position', 'wikipedia', 'first', 'rank', 'query', 'climate', 'change', 'transatlantic', 'trade', 'investment', 'partnership', 'ttip', 'ask', 'almost', 'query', 'search', 'engine', 'news', 'site', 'place', 'first', 'rank', 'time', 'however', 'topic', 'covid19', 'source', 'diverse', 'makhortykh', 'et', 'observe', 'incorporate', 'recent', 'information', 'legacy', 'medium', 'bing', 'strongly', 'rely', 'healthcarerelated', 'source', 'highlight', 'governmentrelate', 'website', 'yandex', 'search', 'engine', 'include', 'alternative', 'medium', 'top', 'search', 'result', 'especially', 'recent', 'topic', 'lack', 'established', 'knowledge', 'basis', 'choice', 'search', 'engine', 'considerably', 'impact', 'information', 'user', 'get', 'see', 'objective', 'research', 'question', 'study', 'address', 'follow', 'research', 'question', 'top', 'result', 'alternative', 'search', 'engine', 'differ', 'google', 'regard', 'number', 'unique', 'source', 'top', 'result', 'alternative', 'search', 'engine', 'differ', 'google', 'regard', 'top', 'source', 'top', 'result', 'alternative', 'search', 'engine', 'differ', 'google', 'regard', 'source', 'concentration', 'result', 'distribute', 'source', 'different', 'search', 'engine', 'answer', 'question', 'select', 'alternative', 'search', 'engine', 'compare', 'google', 'aside', 'bing', 'big', 'competitor', 'google', 'choose', 'duckduckgo', 'metager', 'duckduckgo', 'use', 'result', 'bing', 'yahoo', 'yandex', 'add', 'benefit', 'advanced', 'privacy', 'setting', 'nonpersonalized', 'result', 'metager', 'german', 'metasearch', 'engine', 'aggregate', 'result', 'several', 'search', 'engine', 'include', 'bing', 'yandex', 'search', 'engine', 'provide', 'varied', 'set', 'source', 'alternative', 'search', 'engine', 'index', 'different', 'google', 'comparison', 'give', 'insight', 'domain', 'favor', 'domain', 'exclude', 'also', 'gain', 'insight', 'difference', 'rank', 'method', 'illustrate', 'figure', 'study', 'conduct', 'step', 'first', 'step', 'query', 'set', 'generate', 'previously', 'collect', 'trend', 'datum', 'daily', 'trend', 'collect', 'daily', '31st', 'daily', 'number', 'trend', 'query', 'vary', 'query', 'set', 'size', 'trend', 'query', 'initially', 'collect', 'however', 'topic', 'interest', 'long', 'live', 'single', 'day', 'topic', 'trend', 'multiple', 'day', 'therefore', 'remove', 'duplicate', 'query', 'set', 'consist', 'query', 'query', 'query', 'wide', 'range', 'include', 'topic', 'celebrity', 'eg', 'sport', 'nfl', 'liverpool', 'seasonal', 'event', 'day', 'parade', 'restaurant', 'open', 'headline', 'news', 'eg', 'tiktok', 'class', 'action', 'lawsuit', 'settlement', 'figure', 'methodology', 'current', 'study', 'step', 'query', 'top', 'result', 'search', 'engine', 'collect', 'make', 'sure', 'request', 'result', 'appropriate', 'language', 'location', 'use', 'url', 'parameter', 'furthermore', 'decide', 'limit', 'comparison', 'top', 'result', 'user', 'usually', 'consider', 'study', 'implication', 'user', 'perception', 'relevant', 'web', 'scrape', 'component', 'relevance', 'assessment', 'rat', 'sünkler', 'use', 'collect', 'datum', 'scraping', 'take', 'place', '6th', '9th', 'result', 'gather', 'important', 'note', 'organic', 'search', 'result', 'consider', 'fair', 'comparison', 'ensure', 'rank', 'result', 'list', 'instead', 'vertical', 'insert', 'universal', 'search', 'result', 'bing', 'news', 'consider', 'also', 'ignore', 'ad', 'part', 'search', 'engine', 'web', 'index', 'step', 'use', 'get', 'domain', 'search', 'result', 'ensure', 'fair', 'comparison', 'search', 'engine', 'step', 'remove', 'result', 'query', 'less', 'result', 'search', 'engine', 'additionally', 'datum', 'check', 'strip', 'error', 'occur', 'collection', 'duplicate', 'result', 'result', 'far', 'reduce', 'dataset', 'consist', 'query', 'result', 'query', 'result', 'lastly', 'extraction', 'toplevel', 'domain', 'far', 'refine', 'step', 'use', 'basic', 'string', 'match', 'unify', 'url', 'point', 'domain', 'example', 'follow', 'url', 'count', 'separate', 'domain', 'string', 'match', 'finally', 'step', 'use', 'different', 'method', 'compare', 'datum', 'collect', 'initial', 'comparison', 'make', 'use', 'simple', 'descriptive', 'statistic', 'measure', 'source', 'concentration', 'adapt', 'gini', 'coefficient', 'measure', 'statistical', 'dispersion', 'use', 'measure', 'income', 'wealth', 'inequality', 'gini', 'coefficient', 'single', 'number', 'range', 'represent', 'perfect', 'equality', 'maximum', 'inequality', 'ortega', 'demonstrate', 'adapt', 'measure', 'distribution', 'value', 'different', 'domain', 'appropriate', 'case', 'search', 'result', 'set', 'allow', 'compare', 'various', 'search', 'engine', 'easily', 'low', 'gini', 'coefficient', 'equally', 'result', 'distribute', 'domain', 'contribute', 'particular', 'result', 'set', 'follow', 'calculate', 'jaccard', 'similarity', 'index', 'demonstrate', 'usage', 'jaccard', 'index', 'measure', 'similarity', 'result', 'set', 'jaccard', 'index', 'single', 'number', 'range', 'mean', 'set', 'entirely', 'dissimilar', 'mean', 'identical', 'calculate', 'jaccard', 'index', 'twoway', 'combination', 'search', 'engine', 'eg', 'bing', 'bing', 'metag', 'stage', 'top', 'top', 'result', 'bing', 'position', 'result', 'classification', 'domain', 'work', 'source', 'different', 'country', 'type', 'content', 'domain', 'immediately', 'recognizable', 'manually', 'create', 'general', 'set', 'category', 'base', 'top', 'domain', 'find', 'result', 'follow', 'manually', 'classify', 'popular', 'domain', 'german', 'result', 'see', 'table', 'compare', 'top', 'domain', 'german', 'result', 'news', 'service', 'dominate', 'movie', 'entertainment', 'sport', 'make', 'respectively', 'top', 'domain', 'sport', 'website', 'news', 'service', 'follow', 'close', 'behind', 'movie', 'entertainment', 'make', 'overall', 'distribution', 'domain', 'class', 'search', 'engine', 'look', 'difference', 'search', 'engine', 'example', 'top', 'share', 'top', 'share', 'class', 'celebrity', 'varietycom', 'government', 'website', 'bundesregierungde', 'information', 'service', 'movie', 'entertainment', 'news', 'service', 'sport', 'social', 'medium', 'table', 'class', 'domain', 'frequency', 'top', 'domain', 'variety', 'domain', 'comparison', 'number', 'root', 'domain', 'search', 'result', 'google', 'bing', 'duckduckgo', 'metager', 'show', 'great', 'diversity', 'small', 'margin', 'see', 'figure', 'overall', 'value', 'similar', 'however', 'noticeable', 'great', 'variety', 'domain', 'especially', 'first', 'position', 'interestingly', 'great', 'diversity', 'google', 'source', 'even', 'pronounce', 'result', 'examine', 'difference', 'closely', 'look', 'number', 'figure', 'cumulative', 'number', 'unique', 'domain', 'table', 'show', 'cumulative', 'frequency', 'german', 'result', 'number', 'unique', 'root', 'domain', 'converge', 'fifth', 'position', 'find', 'unique', 'domain', 'google', 'result', 'unique', 'domain', 'bing', 'unique', 'domain', 'duckduckgo', 'unique', 'domain', 'metag', 'position', 'google', 'bing', 'duckduckgo', 'metager', 'share', 'share', 'share', 'share', 'position', 'google', 'bing', 'duckduckgo', 'metager', 'share', 'share', 'share', 'share', 'table', 'cumulative', 'number', 'unique', 'domain', 'cumulative', 'number', 'percentage', 'total', 'difference', 'compare', 'search', 'result', 'notable', 'contrast', 'result', 'minor', 'difference', 'first', 'position', 'however', 'also', 'show', 'great', 'variety', 'root', 'domain', 'search', 'result', 'evident', 'german', 'result', 'see', 'table', 'overall', 'find', 'unique', 'domain', 'bing', 'duckduckgo', 'metager', 'position', 'google', 'bing', 'duckduckgo', 'metager', 'share', 'share', 'share', 'share', 'table', 'cumulative', 'number', 'unique', 'domain', 'cumulative', 'number', 'percentage', 'total', 'popular', 'domain', 'compare', 'top', 'domain', 'search', 'engine', 'german', 'search', 'result', 'collect', 'find', 'clear', 'preference', 'wikipedia', 'see', 'table', 'popular', 'domain', 'search', 'engine', 'frequency', 'result', 'significantly', 'low', 'compare', 'number', 'occurrence', 'bing', 'metager', 'duckduckgo', 'domain', 'top', 'search', 'engine', 'news', 'service', 'bing', 'duckduckgo', 'metager', 'sport', 'make', 'sport', 'website', 'surprisingly', 'top', 'duckduckgo', 'metag', 'even', 'table', 'show', 'domain', 'class', 'social', 'medium', 'make', 'top', 'domain', 'german', 'result', 'go', 'amazon', 'second', 'frequent', 'domain', 'duckduckgo', 'even', 'overall', 'result', 'ecommerce', 'website', 'make', 'bing', 'duckduckgo', 'metager', 'wikipediaorg', 'wikipediaorg', 'spiegelde', 'sternde', 'zdfde', 'bildde', 'amazonde', 'zdfde', 'zdfde', 'buntede', 'bing', 'duckduckgo', 'metager', 'galade', 'bildde', 'buntede', 'bildde', 'bildde', 'tonlinede', 'galade', 'tagesschaude', 'galade', 'tagesschaude', 'tagesschaude', 'tagesschaude', 'sportschaude', 'buntede', 'sportschaude', 'zdfde', 'webde', 'sueddeutschede', 'instagramcom', 'sportschaude', 'instagramcom', 'table', 'top', 'domain', 'show', 'different', 'search', 'engine', 'germany', 'contrarily', 'result', 'high', 'number', 'occurrence', 'compare', 'bing', 'metager', 'see', 'table', 'still', 'remain', 'wikipedia', 'popular', 'domain', 'search', 'engine', 'difference', 'result', 'social', 'medium', 'source', 'much', 'prevalent', 'search', 'engine', 'especially', 'instagram', 'second', 'frequent', 'domain', 'facebook', 'third', 'twitter', 'sixth', 'surprising', 'consider', 'social', 'medium', 'domain', 'make', 'top', 'domain', 'result', 'see', 'table', 'finding', 'youtube', 'subsidiary', 'top', '8th', 'domain', 'alternative', 'top', 'domain', 'metager', 'news', 'service', 'bing', 'search', 'engine', 'sport', 'website', 'make', 'domain', 'bing', 'duckduckgo', 'metager', 'wikipediaorg', 'imdbcom', 'yahoocom', 'imdbcom', 'nypostcom', 'nypostcom', 'nypostcom', 'twittercom', 'britannicacom', 'youtubecom', 'youtubecom', 'youtubecom', 'gocom', 'nytimescom', 'instagramcom', 'instagramcom', 'table', 'top', 'domain', 'search', 'engine', 'usa', 'exclusivity', 'domain', 'section', 'compare', 'top', 'domain', 'search', 'engine', 'determine', 'source', 'alternative', 'exclusively', 'offer', 'user', 'table', 'show', 'domain', 'find', 'solely', 'google', 'top', 'domain', 'list', 'german', 'query', 'domain', 'sport', 'website', 'news', 'service', 'belong', 'entertainment', 'movie', 'class', 'ecommerce', 'government', 'website', 'rande', 'amazonde', 'dazncom', 'skyde', 'eurosportde', 'zeitde', 'rndde', 'dwcom', 'table', 'domain', 'find', 'google', 'top', 'germany', 'contrary', 'table', 'show', 'list', 'domain', 'find', 'top', 'domain', 'alternative', 'domain', 'miss', 'user', 'use', 'domain', 'news', 'service', 'follow', 'movie', 'entertainment', 'celebrity', 'source', 'interestingly', 'instagram', 'youtube', 'miss', 'well', 'sportde', 'instagramcom', 'daserstede', 'table', 'domain', 'find', 'search', 'engine', 'google', 'top', 'germany', 'implement', 'evaluation', 'result', 'find', 'domain', 'exclusively', 'find', 'google', 'top', 'domain', 'domain', 'classify', 'sport', 'movie', 'entertainment', 'news', 'service', 'social', 'medium', 'one', 'celebrity', 'see', 'table', 'notable', 'social', 'medium', 'giant', 'facebook', 'tiktok', 'find', 'alternative', 'marcacom', 'varietycom', 'table', 'domain', 'find', 'google', 'top', 'opposite', 'end', 'domain', 'find', 'result', 'see', 'table', 'news', 'service', 'website', 'celebrity', 'movie', 'entertainment', 'one', 'sport', 'ecommerce', 'website', 'interestingly', 'amazon', 'miss', 'google', 'result', 'well', 'service', 'bing', 'screenrantcom', 'table', 'domain', 'find', 'search', 'engine', 'google', 'top', 'distribution', 'domain', 'next', 'look', 'gini', 'coefficient', 'measure', 'concentration', 'statistical', 'dispersion', 'root', 'domain', 'search', 'engine', 'distribution', 'search', 'engine', 'german', 'result', 'show', 'similar', 'value', 'result', 'range', 'german', 'result', 'respectively', 'see', 'figure', 'overall', 'mean', 'imbalance', 'distribution', 'result', 'root', 'domain', 'top', 'source', 'dominate', 'search', 'result', 'country', 'figure', 'source', 'distribution', 'domain', 'similarity', 'domain', 'compare', 'similarity', 'top', 'german', 'result', 'find', 'bing', 'metager', 'similar', 'term', 'top', 'result', 'follow', 'overlap', 'duckduckgo', 'metager', 'bing', 'duckduckgo', 'however', 'compare', 'alternative', 'search', 'engine', 'google', 'find', 'high', 'overlap', 'bing', 'slightly', 'low', 'one', 'duckduckgo', 'metag', 'interestingly', 'top', 'result', 'overlap', 'high', 'bing', 'overlap', 'bing', 'duckduckgo', 'result', 'show', 'mean', 'overlap', 'top', 'top', 'top', 'result', 'table', 'indicate', 'look', 'entire', 'first', 'search', 'result', 'page', 'nearly', 'half', 'domain', 'search', 'engine', 'consider', 'search', 'engine', 'user', 'mean', 'addition', 'also', 'allow', 'see', 'search', 'result', 'otherwise', 'miss', 'use', 'alternative', 'search', 'engine', 'rank', 'top', 'top', 'top', 'google', 'bing', 'bing', 'ddg', 'bing', 'metager', 'ddg', 'metager', 'mean', 'table', 'jaccard', 'similarity', 'domain', 'ddg', 'duckduckgo', 'evaluation', 'yield', 'similar', 'result', 'overlap', 'bing', 'metager', 'high', 'margin', 'pair', 'alternative', 'pairing', 'reduce', 'look', 'top', 'result', 'bing', 'metager', 'overlap', 'bing', 'duckduckgo', 'duckduckgo', 'metag', 'google', 'overlap', 'search', 'engine', 'low', 'result', 'pair', 'bing', 'slightly', 'high', 'overlap', 'alternative', 'see', 'table', 'narrow', 'result', 'top', 'overlap', 'bing', 'duckduckgo', 'increase', 'minimally', 'remain', 'duckduckgo', 'metag', 'overlap', 'bing', 'low', 'german', 'result', 'furthermore', 'top', 'result', 'overlap', 'bing', 'also', 'low', 'german', 'result', 'generally', 'compare', 'german', 'result', 'high', 'overlap', 'top', 'position', 'pair', 'duckduckgo', 'bing', 'duckduckgo', 'duckduckgo', 'metag', 'average', 'result', 'make', 'trend', 'clear', 'overlap', 'top', 'top', 'top', 'result', 'similar', 'respectively', 'rank', 'top', 'top', 'top', 'google', 'bing', 'ddg', 'bing', 'ddg', 'bing', 'metager', 'ddg', 'metag', 'mean', 'table', 'jaccard', 'similarity', 'domain', 'ddg', 'duckduckgo', 'discussion', 'study', 'examine', 'difference', 'source', 'top', 'search', 'result', 'alternative', 'search', 'engine', 'base', 'popular', 'search', 'query', 'evaluation', 'root', 'domain', 'popular', 'source', 'show', 'small', 'overlap', 'alternative', 'search', 'engine', 'rq1', 'overall', 'find', 'overlap', 'alternative', 'german', 'result', 'overlap', 'range', 'significantly', 'high', 'overlap', 'alternative', 'search', 'engine', 'german', 'result', 'result', 'explain', 'alternative', 'search', 'engine', 'use', 'bing', 'index', 'least', 'part', 'however', 'finding', 'show', 'metager', 'consistently', 'high', 'overlap', 'bing', 'go', 'overlap', 'much', 'duckduckgo', 'low', 'overlap', 'alternative', 'search', 'engine', 'already', 'show', 'study', 'makhortykh', 'study', 'provide', 'evidence', 'look', 'uniqueness', 'source', 'german', 'query', 'find', 'search', 'engine', 'return', 'similar', 'total', 'number', 'source', 'range', 'however', 'query', 'variety', 'noticeably', 'high', 'total', 'source', 'compare', 'alternative', 'popular', 'domain', 'wikipedia', 'follow', 'source', 'classify', 'news', 'service', 'rq2', 'likely', 'exacerbate', 'select', 'google', 'trend', 'source', 'search', 'query', 'usually', 'include', 'query', 'relate', 'popular', 'news', 'story', 'sport', 'celebrity', 'still', 'consistent', 'previous', 'study', 'finding', 'already', 'show', 'wikipedia', 'news', 'make', 'majority', 'search', 'result', 'furthermore', 'research', 'wikipedia', 'news', 'source', 'top', 'domain', 'result', 'frequent', 'source', 'search', 'engine', 'individually', 'compare', 'result', 'set', 'top', 'result', 'bing', 'duckduckgo', 'metager', 'use', 'query', 'generate', 'trend', 'interesting', 'see', 'news', 'source', 'far', 'prevalent', 'german', 'result', 'social', 'medium', 'infrequent', 'hand', 'result', 'social', 'medium', 'website', 'interestingly', 'result', 'youtube', 'top', 'popular', 'domain', 'search', 'engine', 'case', 'top', 'domain', 'german', 'result', 'unexpected', 'youtube', 'subsidiary', 'finding', 'explain', 'fact', 'collect', 'organic', 'search', 'result', 'use', 'universal', 'search', 'result', 'display', 'youtube', 'result', 'interesting', 'difference', 'great', 'preference', 'wikipedia', 'result', 'domain', 'compare', 'german', 'result', 'domain', 'furthermore', 'second', 'third', 'popular', 'source', 'instagram', 'facebook', 'result', 'even', 'top', 'german', 'google', 'source', 'finally', 'term', 'miss', 'result', 'notable', 'news', 'find', 'top', 'source', 'present', 'alternative', 'concentration', 'source', 'source', 'diversity', 'show', 'tendency', 'root', 'domain', 'make', 'large', 'share', 'search', 'result', 'gini', 'index', 'value', 'respectively', 'clear', 'indicator', 'rq3', 'consistent', 'finding', 'previous', 'study', 'höchstötter', 'lewandowski', 'show', 'top', 'source', 'dominate', 'search', 'result', 'search', 'engine', 'course', 'study', 'limitation', 'first', 'selection', 'search', 'query', 'significant', 'factor', 'compile', 'datum', 'evaluate', 'even', 'number', 'query', 'high', 'diversity', 'query', 'topic', 'almost', 'always', 'focus', 'news', 'celebrity', 'sport', 'inevitably', 'lead', 'many', 'news', 'source', 'search', 'result', 'refined', 'approach', 'select', 'search', 'query', 'appropriate', 'accurate', 'evaluation', 'source', 'diversity', 'example', 'achieve', 'focus', 'socially', 'controversial', 'topic', 'choose', 'query', 'accordingly', 'limitation', 'arise', 'evaluation', 'source', 'type', 'classification', 'use', 'study', 'broad', 'precise', 'classification', 'help', 'make', 'statement', 'actual', 'kind', 'source', 'thus', 'determine', 'even', 'precisely', 'preference', 'bias', 'exist', 'different', 'search', 'engine', 'example', 'grouping', 'source', 'accord', 'seriousness', 'reliability', 'serve', 'explanation', 'selection', 'source', 'display', 'search', 'result', 'regard', 'search', 'result', 'collect', 'study', 'limitation', 'organic', 'search', 'result', 'consider', 'consider', 'advertisement', 'universal', 'search', 'result', 'strong', 'influence', 'user', 'see', 'serp', 'relevance', 'search', 'result', 'also', 'consider', 'conclusion', 'study', 'provide', 'important', 'insight', 'far', 'popular', 'search', 'engine', 'use', 'alternative', 'benefit', 'user', 'result', 'show', 'use', 'search', 'engine', 'lead', 'see', 'diverse', 'search', 'result', 'allow', 'user', 'inform', 'comprehensively', 'note', 'search', 'engine', 'result', 'concentration', 'source', 'show', 'top', 'source', 'dominate', 'result', 'mean', 'search', 'engine', 'user', 'choose', 'use', 'shape', 'source', 'information', 'get', 'see', 'come', 'research', 'datum', 'research', 'datum', 'available', 'acknowledgment', 'work', 'fund', 'german', 'research', 'foundation', 'dfg', 'forschungsgemeinschaft', 'grant', 'reference', 'agrawal', 'r', 'overlap', 'web', 'search', 'result', 'bing', 'journal', 'web', 'science', 'barilan', 'compare', 'ranking', 'search', 'result', 'web', 'information', 'processing', 'management', 'mat‐hassan', 'method', 'evaluate', 'dynamic', 'change', 'search', 'engine', 'ranking', 'case', 'study', 'journal', 'documentation', 'technique', 'measure', 'relative', 'size', 'overlap', 'public', 'web', 'search', 'engine', 'computer', 'network', 'isdn', 'system', 'bilal', 'evaluate', 'lead', 'web', 'search', 'engine', 'child', 'query', 'lecture', 'note', 'computer', 'science', 'include', 'subserie', 'lecture', 'note', 'artificial', 'intelligence', 'lecture', 'note', 'bioinformatics', 'vol', 'lnc', 'issue', 'part', 'magalhães', 'bing', 'new', 'perspective', 'rank', 'similarity', 'proceeding', '20th', 'acm', 'international', 'conference', 'information', 'knowledge', 'management', 'cikm', 'gwizdka', 'bodner', 'r', 'discriminate', 'metasearch', 'framework', 'evaluation', 'information', 'processing', 'management', 'ding', 'marchionini', 'comparative', 'study', 'web', 'search', 'service', 'performance', 'proceeding', 'asis', 'annual', 'meeting', 'trust', 'barometer', 'global', 'report', 'trust', 'barometer', 'special', 'eurobarometer', 'gini', 'c', 'measure', 'concentration', 'special', 'reference', 'income', 'statistic', 'college', 'publication', 'general', 'goel', 'broder', 'gabrilovich', 'e', 'pang', 'b', 'anatomy', 'long', 'tail', 'proceeding', 'third', 'acm', 'international', 'conference', 'web', 'search', 'datum', 'mining', 'rodrigue', 'l', 'density', 'close', 'ball', 'realvalue', 'autometrize', 'boolean', 'space', 'cluster', 'application', 'lecture', 'note', 'computer', 'science', 'include', 'subserie', 'lecture', 'note', 'artificial', 'intelligence', 'lecture', 'note', 'bioinformatics', 'vol', 'höchstötter', 'lewandowski', 'user', 'see', 'structure', 'search', 'engine', 'result', 'page', 'information', 'science', 'nissenbaum', 'shape', 'web', 'politic', 'search', 'engine', 'matter', 'information', 'society', 'gile', 'accessibility', 'information', 'web', 'nature', 'lewandowski', 'web', 'miss', 'essential', 'part', 'infrastructure', 'communication', 'acm', 'lewandowski', 'kammerer', 'factor', 'influence', 'view', 'behaviour', 'search', 'engine', 'result', 'page', 'review', 'eyetracke', 'research', 'behaviour', 'information', 'technology', 'lewandowski', 'sünkler', 'recommend', 'want', 'compare', 'insurance', 'offering', 'aslib', 'information', 'management', 'mager', 'small', 'really', 'beautiful', 'big', 'search', 'alternative', 'r', 'könig', 'ed', 'society', 'query', 'reader', 'network', 'culture', 'makhortykh', 'urman', 'ulloa', 'r', 'search', 'engine', 'disseminate', 'information', 'covid19', 'misinformation', 'review', 'build', 'efficient', 'effective', 'metasearch', 'engine', 'acm', 'computing', 'survey', 'newman', 'r', 'schulz', 'kleis', 'report', 'ortega', 'roble', 'inequality', 'contribution', 'proceeding', '41st', 'annual', 'conference', 'system', 'science', 'search', 'engine', 'use', 'contentuploadssites9mediafilesreports2012pipsearchengineuse2012pdf', 'bubble', 'assess', 'diversity', 'political', 'search', 'result', 'digital', 'journalism', 'schultheiß', 'lewandowski', 'representative', 'online', 'survey', 'german', 'search', 'engine', 'user', 'focus', 'question', 'regard', 'search', 'engine', 'optimization', 'seo', 'study', 'seo', 'effect', 'project', 'work', 'paper', 'spink', 'blakely', 'study', 'result', 'overlap', 'uniqueness', 'major', 'web', 'search', 'engine', 'information', 'processing', 'management', 'statcounter', 'search', 'engine', 'market', 'share', 'seek', 'find', 'content', 'analysis', 'diversity', 'search', 'engine', 'result', 'political', 'query', 'information', 'communication', 'society', 'sünkler', 'lewandowski', 'schultheiß', 'relevance', 'assessment', 'tool', 'thelwall', 'quantitative', 'comparison', 'search', 'engine', 'result', 'journal', 'american', 'society', 'information', 'science', 'technology', 'unkel', 'haim', 'google', 'politic', 'party', 'source', 'issue', 'ownership', 'german', 'federal', 'election', 'campaign', 'social', 'science', 'computer', 'review']"
A Security & Privacy Analysis of US-based Contact Tracing Apps,"[{'href': 'http://arxiv.org/abs/2207.08978v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.08978v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-18 23:14:49,"Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Jonathan D. Chang * 1 Kaiwen Wang * 1 Nathan Kallus 2 Wen Sun 1

Abstract

1. Introduction

2
2
0
2

l
u
J

2
1

]

G
L
.
s
c
[

1
v
7
3
8
5
0
.
7
0
2
2
:
v
i
X
r
a

We study representation learning for Ofﬂine Re-
inforcement Learning (RL), focusing on the im-
portant task of Ofﬂine Policy Evaluation (OPE).
Recent work shows that, in contrast to supervised
learning, realizability of the Q-function is not
enough for learning it. Two sufﬁcient conditions
for sample-efﬁcient OPE are Bellman complete-
ness and coverage. Prior work often assumes
that representations satisfying these conditions
are given, with results being mostly theoretical in
nature. In this work, we propose BCRL, which
directly learns from data an approximately lin-
ear Bellman complete representation with good
coverage. With this learned representation, we
perform OPE using Least Square Policy Evalua-
tion (LSPE) with linear functions in our learned
representation. We present an end-to-end theoreti-
cal analysis, showing that our two-stage algorithm
enjoys polynomial sample complexity provided
some representation in the rich class considered
is linear Bellman complete. Empirically, we ex-
tensively evaluate our algorithm on challenging,
image-based continuous control tasks from the
Deepmind Control Suite. We show our represen-
tation enables better OPE compared to previous
representation learning methods developed for
off-policy RL (e.g., CURL, SPR). BCRL achieves
competitive OPE error with the state-of-the-art
method Fitted Q-Evaluation (FQE), and beats
FQE when evaluating beyond the initial state dis-
tribution. Our ablations show that both linear
Bellman complete and coverage components of
our method are crucial.

*Equal contribution 1Computer Science, Cornell University,
Ithaca, NY, USA 2Operations Research and Information Engi-
neering, Cornell Tech, New York, NY, USA. Correspondence
to: Jonathan D. Chang <https://jdchang1.github.io>,
Kaiwen Wang <https://kaiwenw.github.io>.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

Deep Reinforcement Learning (RL) has developed agents
that solve complex sequential decision making tasks, achiev-
ing new state-of-the-art results and surpassing expert human
performance. Despite these impressive results, these algo-
rithms often require a prohibitively large number of online
interactions to scale to higher dimensional inputs.

To address these sample complexity demands, a recent line
of work (van den Oord et al., 2018; Anand et al., 2019;
Mazoure et al., 2020; Yang & Nachum, 2021) has incor-
porated advances in unsupervised representation learning
from the supervised learning literature into developing RL
agents. For example, CURL (Laskin et al., 2020) and SPR
(Schwarzer et al., 2021) utilize contrastive representation ob-
jectives as auxiliary losses within an existing RL framework.
These efforts are motivated by the tremendous success such
self-supervised techniques have offered in computer vision,
natural language processing, speech processing, and beyond.
While these formulations have shown sample complexity
improvements empirically, it remains an open question
whether these approaches successfully address the unique
challenges from RL that usually do not appear in supervised
learning, such as exploration and exploitation, credit assign-
ments, long horizon prediction, and distribution shift. In
particular, recent work (Wang et al., 2021b; Amortila et al.,
2020; Foster et al., 2021) has shown that realizability of the
learning target in RL (namely, the Q-function) is insufﬁcient
to avoid exponential dependence on problem parameters.

In this paper, we study representation learning for an
important subtask of off-policy RL: ofﬂine policy evaluation
(OPE). OPE is a critical component for any off-policy
policy optimization approach (e.g., off-policy actor critic
such as SAC, Haarnoja et al., 2018, and DDPG, Lillicrap
et al., 2016). Moreover, OPE allows us to focus on issues
arising from distribution shift and long horizon prediction.
Speciﬁcally, we propose a new approach that leverages
rich function approximation (e.g., deep neural networks)
to learn a representation that is both Bellman complete
and exploratory. A linear Bellman complete representation
means that linear functions in the representation have zero
inherent Bellman error (Antos et al., 2008), i.e., applying
the Bellman operator on a linear function results in a new
linear function. An exploratory representation means that

 
 
 
 
 
 
Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

the resulting feature covariance matrix formed by the ofﬂine
dataset is well-conditioned. These two representational
properties ensure that, under linear function approximation
(i.e., the linear evaluation protocol, Grill et al., 2020),
classic least squares policy evaluation (LSPE) (Nedi´c &
Bertsekas, 2003; Duan et al., 2020; Wang et al., 2021a)
can achieve accurate policy evaluation. We provide
an end-to-end analysis showing that our representation
learning approach together with LSPE ensures near-optimal
policy evaluation with polynomial sample complexity.

Empirically, we extensively evaluate our method on image-
based continuous control tasks from the Deepmind Control
Suite. First, we compare against two representation learning
approaches developed for off-policy RL: CURL (Laskin
et al., 2020) and SPR (Schwarzer et al., 2021). These bear
many similarities to contrastive learning techiniques for
unsupervised representation learning: SimCLR (Chen et al.,
2020) and Bootstrap your own latent (BYOL, Grill et al.,
2020), respectively. Under the linear evaluation protocol
(i.e., LSPE with a linear function on top of the learned
representation), our approach consistently outperforms
these baselines. We observe that representations learned
by CURL and SPR sometimes even exhibit instability
when evaluated using LSPE (prediction error blows up
when more iterations of LSPE is applied), while our
approach is more stable. This comparison demonstrates
that representation learning in ofﬂine RL is more subtle and
using representation techniques developed from supervised
learning settings may not result in the best performance
for ofﬂine RL. Our ablations show that both linear Bellman
completeness and coverage are crucial, as our method
also blows up if one ingredient is missing. Finally, BCRL
achieves state-of-the-art OPE error when compared with
other OPE methods, and improves the state-of-the-art when
evaluating beyond the initial state distribution.1

1.1. Related Work

Representation Learning in Ofﬂine RL:

From the theoretical side, Hao et al. (2021) considers of-
ﬂine RL in sparse linear MDPs (Jin et al., 2020). Learning
with sparsity can be understood as feature selection. Their
work has a much stronger coverage condition than ours:
namely, given a representation class Φ, they assume that any
feature φ ∈ Φ has global coverage under the ofﬂine data
distribution, i.e., Es,a∼νφ(s, a)φ(s, a)(cid:62) is well conditioned
where ν is ofﬂine data distribution. In our work, we only
assume that there exists one φ(cid:63) that has global coverage,
thus strictly generalizing their coverage condition. Uehara
& Sun (2021) propose a general model-based ofﬂine RL
approach that can perform representation learning for linear
MDPs in the ofﬂine setting without global coverage. How-

1

Code available at https://github.com/CausalML/bcrl.

ever, their algorithm is a version space approach and is not
computationally efﬁcient. Also, our linear Bellman com-
pleteness condition strictly generalizes linear MDPs. (Ni
et al., 2021) consider learning state-action embeddings from
a known RKHS. We use general function approximation that
can be more powerful than RKHS. Finally, Parr et al. (2008)
identiﬁes bellman completeness as a desirable condition for
feature selection in RL when analyzing an equivalence be-
tween linear value-function approximation and linear model
approximation. In our work, we investigate how to learn
bellman completeness representation, and also the role of
coverage in our feature selection.

From the empirical side, Yang & Nachum (2021) evalu-
ated a broad range of unsupervised objectives for learning
pretrained representations from ofﬂine datasets for down-
stream Imitation Learning (IL), online RL, and ofﬂine RL.
They found that the use of pretrained representations dra-
matically improved the downstream performance for policy
learning algorithms. In this work, we aim to identify such
an unsupervised representation objective and to extend the
empirical evaluation to ofﬂine image-based continuous con-
trol tasks. Nachum & Yang (2021) presents a provable
contrastive representation learning objective for IL (derived
from maximum likelihood loss), learning state representa-
tions from expert data to do imitation with behavior cloning.
Our approach instead focuses on learning state-action rep-
resentations for OPE. Finally, both Song et al. (2016) and
Chung et al. (2019) present algorithms to learn representa-
tions suitable for linear value function approximation. Song
et al. (2016) is most relevant to our work where they aim
to learn bellman complete features. Both works, however,
work in the online setting and do not consider coverage
induced from the representation which we identify is impor-
tant for accurate OPE.

Representation Learning in Online RL:

Theoretical works on representation learning in online RL
focus on learning representations to facilitate exploration
from scratch. OLIVE (Jiang et al., 2017), BLin-UCB (Du
et al., 2021), FLAMBE (Agarwal et al., 2020), Mofﬂe (Modi
et al., 2021), and Rep-UCB (Uehara et al., 2022) propose ap-
proaches for representation learning in linear MDPs where
they assume that there exists a feature φ(cid:63) ∈ Φ that ad-
mits the linear MDP structure for the ground truth transi-
tion. Zhang et al. (2021) posits an even stronger assumption
where every feature φ ∈ Φ admits the linear MDP struc-
ture for the true transition. Note that we only assume that
there exists one φ(cid:63) that admits linear Bellman complete-
ness, which strictly generalizes linear MDPs. Hence, our
representation learning setting is more general than prior
theoretical works. Note that we study the ofﬂine setting
while prior works operate in the online setting which has
additional challenges from online exploration.

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

On the empirical side, there is a large body of works that
adapt existing self-supervised learning techniques devel-
oped from computer vision and NLP to RL. For learning
representations from image-based RL tasks, CPC (van den
Oord et al., 2018), ST-DIM (Anand et al., 2019), DRIML
(Mazoure et al., 2020), CURL (Laskin et al., 2020), and
SPR (Schwarzer et al., 2021) learn representations by op-
timizing various temporal contrastive losses. In particular,
Laskin et al. (2020) proposes to interleave minimizing an In-
foNCE objective with similarities to MoCo (He et al., 2020)
and SimCLR (Chen et al., 2020) while learning a policy
with SAC (Haarnoja et al., 2018). Moreover, Schwarzer
et al. (2021) proposes learning a representation similar to
BYOL (Grill et al., 2020) alongside a Q-function for Deep
Q-Learning (Mnih et al., 2013). We compare our repre-
sentational objective against CURL and SPR in Section 6,
and demonstrate that under linear evaluation protocol, ours
outperform both CURL and SPR. Note, we refer the readers
to Schwarzer et al. (2021) for comparisons between SPR
and CPC, ST-DIM, and DRIML.

2. Preliminaries

In this paper we consider the inﬁnite horizon discounted
MDP (cid:104)S, A, γ, P, r, d0(cid:105). where S, A are state and action
spaces which could contain a large number of states and ac-
tions or could even be inﬁnite, γ ∈ (0, 1) is a discount
factor, P is the transition kernel, r : S × A → R is
the reward function, and d0 ∈ ∆(S) is the initial state
distribution. We assume rewards are bounded by 1, i.e.
|r(s, a)| ≤ 1. Given a policy π : S (cid:55)→ ∆(A), we denote
V π(s) = E (cid:2)(cid:80)∞
h=0 γhrh|π, s0 := s(cid:3) as the expected dis-
counted total reward of policy π starting at state s. We
= Es∼d0 V π(s) as the expected discounted total
denote V π
d0
reward of the policy π starting at the initial state distribu-
tion d0. We also denote average state-action distribution
dπ
h=0 γhdπ
h(s, a) is
d0
the probability of π visiting the (s, a) pair at time step h,
starting from d0.

(s, a) = (1 − γ) (cid:80)∞

h(s, a) where dπ

In OPE, we seek to evaluate the expected discounted to-
tal reward V πe of a target policy πe : S (cid:55)→ ∆(A) given
data drawn from an ofﬂine state-action distribution ν ∈
∆(S × A). The latter can, for example, be the state-action
distribution under a behavior policy πb. Namely, the ofﬂine
dataset D = {si, ai, ri, s(cid:48)
i}N
i=1 consists of N i.i.d tuples
generated as (s, a) ∼ ν, r = r(s, a), s(cid:48) ∼ P (·|s, a).

We deﬁne Bellman operator T πassociated with π as follows:
T πf (s, a) =∆ r(s, a) + γEs(cid:48)∼P (s,a),a(cid:48)∼π(s(cid:48)) [f (s(cid:48), a(cid:48))]
We may drop the superscript π when it is clear from context,
in particular when π = πe is the ﬁxed target policy.
A representation, or feature, φ : S × A → Rd is
an embedding of state-action pairs into d-dimensional

(cid:80)N

space. We let Σ(φ) = Eν [φ(s, a)φ(s, a)(cid:124)] , (cid:98)Σ(φ) =
i=1 φ(si, ai)φ(si, ai)(cid:124). We consider learning a rep-
1
N
resentation from a feature hypothesis class Φ ⊂ [S × A (cid:55)→
Rd]. We assume features have bounded norm: (cid:107)φ(s, a)(cid:107)2 ≤
1, ∀s, a, ∀φ ∈ Φ. In our experiments, Φ is convolutional
neural nets with d outputs.

Notation We denote BW := {x ∈ Rd : (cid:107)x(cid:107)2 ≤ W } as
the Euclidean Ball in Rd with radius W . Given a distribution
ν ∈ ∆(S × A) and a function f : S × A (cid:55)→ R, we denote
ν = Es,a∼νf 2(s, a). Given a positive
L2(ν) norm as (cid:107)f (cid:107)2
xT Σx and let λmin(Σ)
deﬁnite matrix Σ, let (cid:107)x(cid:107)Σ =
denote the minimum eigenvalue. When ν (cid:28) µ we let
dν
dµ denote the Radon-Nikodym derivative. We use ◦ to
denote composition, so s(cid:48), a(cid:48) ∼ P (s, a) ◦ π is short-form
for s(cid:48) ∼ P (s, a), a(cid:48) ∼ π(s(cid:48)). For any function f (s, a) and
a policy π, we denote f (s, π) = Ea∼π(s) [f (s, a)].

√

3. Linear Bellman Completeness

Before we introduce our representation learning approach,
in this section, we ﬁrst consider OPE with linear function
approximation with a given representation φ. Lessons from
supervised learning or linear bandit may suggest that OPE
should be possible with polynomially many ofﬂine samples,
as long as (1) ground truth target Qπe is linear in φ (i.e.
∃w ∈ Rd, such that for all s, a, Qπe (s, a) = w(cid:62)φ(s, a)),
and (2) the ofﬂine data provides sufﬁcient coverage over
πe (i.e., λmin(Σ(φ)) > 0). Unfortunately, under these two
assumptions, there are lower bounds indicating that for
any OPE algorithm, there exists an MDP where one will
need at least exponentially (exponential in horizon or d)
many ofﬂine samples to provide an accurate estimate of
V πe (Wang et al., 2021b; Foster et al., 2021; Amortila et al.,
2020). This lower bound indicates that one needs additional
structural conditions on the representation.

The additional condition the prior work has consider is
Bellman completeness (BC). Since we seek to learn a rep-
resentation rather than assume theoretical conditions on a
given one, we will focus on an approximate version of BC.

Deﬁnition 3.1 (Approximate Linear BC). A representation
φ is εν-approximately linear Bellman complete if,

max
w1∈BW

min
w2∈BW

(cid:107)w

(cid:124)
2 φ − T πe (w

(cid:124)
1 φ)(cid:107)ν ≤ εν.

Note the dependence on ν, πe, and W .

(cid:124)
1 φ(s, a), T π(w

Intuitively the above condition is saying that for any linear
(cid:124)
1 φ(s, a)) itself can be approxi-
function w
mated by another linear function under the distribution ν.
Remark 3.2. Low-rank MDPs (Jiang et al., 2017; Agarwal
et al., 2020) are subsumed in the exact linear BC model, i.e.,
εν = 0 under any distribution ν.

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Algorithm 1 Least Squares Policy Evaluation (LSPE)
1: Input: Target policy πe, features φ, dataset D
2: Initialize (cid:98)θ0 = 0 ∈ BW .
3: for k = 1, 2, ..., K do
(cid:124)
k−1φ(s, a),
Set (cid:98)fk−1(s, a) = (cid:98)θ
4:

(cid:98)Vk−1(s) = Ea∼πe(s)[ (cid:98)fk−1(s, a)]

5:

Perform linear regression:

1
N

N
(cid:88)

i=1

(cid:98)θk ∈ arg min

θ∈BW

6: end for
7: Return (cid:98)fK.

(cid:0)θ(cid:124)φ(si, ai) − ri − γ (cid:98)Vk−1(s(cid:48)

i)(cid:1)2

Note that the Bellman completeness condition is more sub-
tle than the common realizability condition: for any ﬁxed φ,
increasing its expressiveness (e.g., add more features) does
not imply a monotonic decrease in εν. Thus common tricks
such as lifting features to higher order polynomial kernel
space or more general reproducing kernel Hilbert space does
not imply the linear Bellman complete condition, nor does it
improve the coverage condition. We next show approximate
Linear BC together with coverage imply sample-efﬁcient
OPE via Least Square Policy Evaluation (LSPE) (Algo-
rithm 1). We present our result using the relative condition
number, deﬁned for any initial state distribution p0 as
x(cid:124)Es,a∼dπe

φ(s, a)φ(s, a)(cid:124)x

κ(p0) := sup
x∈Rd

p0
x(cid:124)Σ(φ)x

.

(1)

Theorem 3.3 (Sample Complexity of LSPE). Assume fea-
ture φ satisﬁes approximate Linear BC with parameter εν.
For any δ ∈ (0, 0.1), w.p. at least 1 − δ, we have for any
initial state distribution p0

(cid:12)
(cid:12)V πe
(cid:12)

p0

(cid:104)

− Es∼p0

(cid:98)fK(s, πe)

(cid:105)(cid:12)
(cid:12)
(cid:12) ≤

γK/2
1 − γ

+

(cid:114)(cid:13)
(cid:13)
4
(cid:13)

ddπe
p0
dν

(cid:13)
(cid:13)
(cid:13)∞

(1 − γ)2

εν

+

480(cid:112)κ(p0)(1 + W )d log(N )(cid:112)log(10/δ)

√

(1 − γ)2

N

,

where (cid:98)fK is the output of Algorithm 1.

Please see Appendix C.1 for proof. The above result holds
simultaneously over all initial state distributions p0 covered
by the data distribution ν. If ν has full coverage, i.e.
if
Σ (cid:23) βI, as is commonly assumed in the literature, one
can show that κ(p0) ≤ β−1 for any initial state distribution
(cid:13)
(cid:13)
p0. Also note that the concentrability coefﬁcient
(cid:13)∞
shows up as T πe (cid:98)fk−1 can be nonlinear. In the exact Linear
BC case, where εν = 0 (i.e., there is a linear function
that perfectly approximates T πe (cid:98)fk−1 under ν), the term
involving the concentrability coefﬁcient will be 0 and we
can even avoid its ﬁniteness.

ddπe
p0
dν

(cid:13)
(cid:13)
(cid:13)

4. Representation Learning

The previous section indicates sufﬁcient conditions on the
representation for efﬁcient and accurate OPE with linear
function approximation. However, a representation that is
Bellman complete and also provides coverage is not avail-
able a priori, especially for high-dimensional settings (e.g.,
image-based control tasks as we consider in our experi-
ments). Existing theoretically work often assume such rep-
resentation is given. We propose to learn a representation φ
that is approximately Bellman complete and also provides
good coverage, via rich function approximation (e.g., a deep
neural network). More formally, we want to learn a repre-
sentation φ such that (1) it ensures approximate Bellman
complete, i.e., εν is small, and (2) has a good coverage, i.e.,
λmin(Σ(φ)) is as large as possible, which are the two key
properties to guarantee accurate and efﬁcient OPE indicated
by Theorem 3.3. To formulate the representation learning
question, our key assumption is that our representation hy-
pothesis class Φ is rich enough such that it contains at least
one representation φ(cid:63) that is linear Bellman complete (i.e.,
εν = 0) and has a good coverage.

Assumption 4.1 (Representational power of Φ). There ex-
ists φ(cid:63) ∈ Φ, such that (1) φ(cid:63) achieves exact Linear BC
(deﬁnition 3.1 with εν = 0), and (2) φ(cid:63) induces coverage,
i.e., λmin(Σ(φ(cid:63))) ≥ β > 0.

Our goal is to learn such a representation from the hypothe-
sis class Φ. Note that unlike prior RL representation learning
works (Hao et al., 2021; Zhang et al., 2021), here we only
assume that there exists a φ(cid:63) has linear BC and induces
good coverage, other candidate φ ∈ Φ could have terrible
coverage and does not necessarily have linear BC.

Before proceeding to the learning algorithm, we ﬁrst present
an equivalent condition for linear BC, which does not rely
on the complicated min-max expression of Deﬁnition 3.1.

Proposition 4.2. Consider a feature φ with full rank covari-
ance Σ(φ). Given any W > 0, the feature φ being linear BC
(under BW ) implies that there exist (ρ, M ) ∈ BW × Rd×d
with (cid:107)M (cid:107)2 ≤

1 − (cid:107)ρ(cid:107)2

(cid:113)

W 2 , and

Eν

(cid:21)

(cid:13)
(cid:20)M
(cid:13)
(cid:13)
ρ(cid:124)
(cid:13)

φ(s, a) −

(cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)
r(s, a)

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

= 0.

(2)

On the other hand, if there exists (ρ, M ) ∈ BW × Rd×d
with (cid:107)M (cid:107)2 < 1 such that the above equality holds, then φ
must satisfy exact linear BC with W ≥ (cid:107)ρ(cid:107)2

.

1−(cid:107)M (cid:107)2

Please see Appendix D for proof. The above shows that lin-
ear BC is equivalent to a simple linear relationship between
the feature and the expected next step’s feature and reward.
This motivates our feature learning algorithm: if we are ca-
pable of learning a representation φ such that the transition

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

from the current feature φ(s, a) to the expected next fea-
ture Es(cid:48)∼P (s,a)[φ(s(cid:48), πe)] and reward r(s, a) is linear, then
we’ve found a feature φ that is linear BC.

4.1. Algorithm

To learn the representation that achieves linear BC, we use
Proposition 4.2 to design a bilevel optimization program.
We start with deterministic transition as a warm up.

Deterministic transition Due to determinism in the tran-
sition, we do not have an expectation with respect to s(cid:48)
anymore. So, we design the bilevel optimization as follows:

However, we cannot directly optimize the above objective
since we do not have access to P (s, a) to compute the ex-
pected next step feature Es(cid:48)∼P (s,a)φ(s(cid:48), πe). Also note that
the expectation Es(cid:48) is inside the square which means that
we cannot even get an unbiased estimate of the gradient of
(φ, M ) with one sample s(cid:48) ∼ P (s, a). This phenomenon
is related to the double sampling issue on ofﬂine policy
evaluation literature which forbids one to directly optimize
Bellman residuals. Algorithms such as TD are designed
to overcome the double sampling issue. Here, we use a
different technique to tackle this issue (Chen & Jiang, 2019).
We introduce a function class G ⊂ S × A (cid:55)→ Rd which is
rich enough to contain the expected next feature.

(cid:104)

(cid:21)

min
φ∈Φ

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2
Θ = (cid:8)(ρ, M ) ∈ B(cid:107)ρ(cid:63)(cid:107) × Rd×d :, (cid:107)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2

(cid:20)γφ(s(cid:48), πe)
r(s, a)

(cid:13)
(cid:20)M
(cid:13)
(cid:13)
ρ(cid:124)
(cid:13)

min
(ρ,M )∈Θ

φ(s, a) −

ED

(cid:9) ,

where ρ(cid:63), M (cid:63) are the optimal ρ, M for the linear BC φ(cid:63)
(in Assumption 4.1). Namely, we aim to search for a repre-
sentation φ ∈ Φ, such that the relationship between φ(s, a)
and the combination of the next time step’s feature φ(s(cid:48), πe)
and the reward, is linear. The spectral norm constraint in
Θ is justiﬁed by Proposition 4.2. Solving the above bilevel
moment condition ﬁnds a representation that achieves ap-
proximate linear BC.

However, there is no guarantee that such representation can
provide a good coverage over πe’s traces. We introduce
regularizations for φ using the ideas from optimal designs,
particularly the E-optimal design. Deﬁne the minimum
eigenvalue regularization

(3)

Assumption 4.3. For any φ ∈ Φ, we have that the mapping
(s, a) (cid:55)→ γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)] is in G.

We form the optimization problem as follows:

(cid:104)

min
φ∈Φ

min
(ρ,M )∈Θ

(cid:21)

ED

(cid:13)
(cid:20)M
(cid:13)
(cid:13)
ρ(cid:124)
(cid:13)
ED (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2

φ(s, a) −

(cid:20)γφ(s(cid:48), πe)
r(s, a)
(cid:105)

.

2

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

(5)

− min
g∈G

Note that under Assumption 4.3, the min over G will approx-
imate γ2ED(cid:107)Es(cid:48)∼P (s,a)φ(s(cid:48), πe) − φ(s(cid:48), πe)(cid:107)2
2, i.e., the av-
erage variance induced by the stochastic transition. Thus,
for a ﬁxed φ and M , we can see that the following

ED (cid:107)M φ(s, a) − γφ(s(cid:48), πe)(cid:107)2
− γ2ED(cid:107)Es(cid:48)∼P (s,a)φ(s(cid:48), πe) − φ(s(cid:48), πe)(cid:107)2
2

2

RE(φ) := λmin (ED [φ(s, a)φ(s, a)(cid:124)]) ,

is indeed an unbiased estimate of:

as the smallest eigenvalue of the empirical feature covari-
ance matrix under the representation φ. Maximizing this
quantity ensures that our feature provides good coverage,
i.e. λmin(Σ(φ)) is as large as possible.

Thus, to learn a representation that is approximately linear
Bellman complete and also provides sufﬁcient coverage, we
formulate the following constrained bilevel optimization:

(cid:104)

min
φ∈Φ

(cid:13)
(cid:20)M
(cid:13)
(cid:13)
ρ(cid:124)
(cid:13)
s.t., RE(φ) ≥ β/2.

min
(ρ,M )∈Θ

ED

(cid:21)

φ(s, a) −

(cid:20)γφ(s(cid:48), πe)
r(s, a)

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

To extend this to stochastic transitions, there is an additional
double sampling issue, which we discuss and address now.

Stochastic transition Ideally, we would solve the follow-
ing bilevel optimization problem,

(cid:104)

min
φ∈Φ

min
(ρ,M )∈Θ

ED

(cid:21)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:20)M
ρ(cid:124)

φ(s, a) −

(cid:20)γEs(cid:48)∼P (s,a)[φ(s(cid:48), πe)]
r(s, a)

(cid:105)

.

2

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(4)

Es,a∼ν

(cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13)
(cid:13)
2
(cid:13)
2

which matches to the ideal objective in Eq. 4. Thus solv-
ing for φ based Eq. 5 allows us to optimize Eq. 4, which
allows us to learn an approximate linear Bellman complete
representation. Similarly, we incorporate the E-optimal de-
sign here by adding a constraint that forcing the smallest
eigenvalue of the empirical feature covariance matrix, i.e.,
RE(φ), to be lower bounded.

Once we learn a representation that is approximately linear
BC, and also induces sufﬁcient coverage, we simply call the
LSPE to estimate V πe . The whole procedure is summarized
in Algorithm 2. Note in Alg 2 we put constraints to the
objective function using Lagrangian multiplier.

There are other design choices that also encourage coverage.
One particular choice we study empirically is motivated
by the idea of D-optimal design. Here we aim to ﬁnd a
representation that maximizes the following log-determinant

RD(φ) := ln det (ED [φ(s, a)φ(s, a)(cid:124)]) .

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Algorithm 2 OPE with Bellman Complete and exploratory
Representation Learning (BCRL)
1: Input: Representation class Φ, dataset D of size 2N ,
design regularization R, function class G, policy πe.

2: Randomly split D into two sets D1, D2 of size N .
3: If the system is stochastic, learn representation (cid:98)φ as,

(cid:104)

arg min
φ∈Φ

− min
g∈G

(cid:21)

ED1

min
(ρ,M )∈Θ

(cid:13)
(cid:20)M
(cid:13)
(cid:13)
ρ(cid:124)
(cid:13)
ED1 (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2

φ(s, a) −

(cid:105)

2

(cid:20)γφ(s(cid:48), πe)
r(s, a)

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

− λR(φ)

4: Otherwise, for deterministic system, learn (cid:98)φ as,

(cid:104)

arg min
φ∈Φ

min
(ρ,M )∈Θ

ED1

(cid:21)

(cid:13)
(cid:20)M
(cid:13)
(cid:13)
ρ(cid:124)
(cid:13)

φ(s, a) −

(cid:20)γφ(s(cid:48), πe)
r(s, a)

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

− λR(φ)

5: Return (cid:98)V := LSPE(πe, (cid:98)φ, D2).

When D is large, then the regularization RD(φ) approx-
imates (cid:80)
i ln (σi (Eν [φ(s, a)φ(s, a)(cid:124)])). Maximizing
RD(φ) then intuitively maximizes coverage over all
directions. D-optimal design is widely used for bandits
(Lattimore & Szepesvári, 2020) and RL (Wang et al., 2021b;
Agarwal et al., 2019) to design exploration distributions
with global coverage. Note that, in contrast to these contexts
where the feature is ﬁxed and the distribution is optimized,
we optimize the feature, given the data distribution ν.

4.2. Sample Complexity Analysis

We now prove a ﬁnite sample utility guarantee for the em-
pirical constrained bilevel optimization problem,

(cid:98)φ ∈ arg min

φ∈Φ

(cid:104)

min
(ρ,M )∈Θ

ED

(cid:21)

(cid:13)
(cid:20)M
(cid:13)
(cid:13)
ρ
(cid:13)

(cid:124)

φ(s, a) −

ED (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2

2

(cid:105)
.

− min
g∈G

(cid:20)γφ(s(cid:48), πe)
r(s, a)

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

(6)

s.t., RE(φ) ≥ β/2.

For simplicity, we state our results for discrete function class
Φ and G. Note that our sample complexity only scales with
respect ln(|Φ|) and ln(|G|), which are the standard com-
plexity measures for discrete function classes. We extend
our analysis to inﬁnite function classes under metric entropy
assumptions (Wainwright, 2019; van der Vaart & Wellner,
1996) in the Appendix; see Assumption E.5.

The following theorem shows that Algorithm 2 learns a
representation (cid:98)φ that is O(N −1/2) approximately Linear
BC and has coverage.

Theorem 4.4. Assume Assumption 4.1 (and Assump-
:=
Let C2
tion 4.3 if
96 log1/2(|Φ|)+4
2 , then for any

the system is stochastic).
√

. If N ≥ C 2

d+4 log1/2(1/δ)

β/4

δ ∈ (0, 1), w.p. at least 1 − δ, we have

1. (cid:98)φ satisﬁes (cid:98)ε-approximate Linear BC with

(cid:98)ε ≤

13d(1 + W )2 log1/2(4W |Φ|N/δ)
√
N

+

7γ(1 + W ) log1/2(2|G|/δ)
√
N

,

2. λmin(Σ( (cid:98)φ)) ≥ β/4.

If transitions are deterministic, treat log(|G|) = 0.

Proof Sketch. First, we use Weyl’s Perturbation Theorem
and chaining to show that the eigenvalues of Σ(φ) are
close to (cid:98)Σ(φ), uniformly over φ. This implies that (a)
λmin((cid:98)Σ(φ(cid:63))) ≥ β/2, and hence is feasible in the empirical
bilevel optimization Equation (6), and (b) λmin(Σ( (cid:98)φ)) ≥
β/4. Since φ(cid:63) is feasible, we apply uniform concentration
arguments to argue that (cid:98)φ has low population loss (Equa-
tion (5)), which implies approximate Linear BC.

The error term in (cid:98)ε is comprised of the statistical errors of
ﬁtting Φ and of ﬁtting G for the double sampling correc-
tion. In the contextual bandit setting, i.e. γ = 0, there
is no transition, so the second term becomes 0. Chaining
together with Theorem 3.3 gives the following end-to-end
(cid:101)O(N −1/2) evaluation error guarantee for LSPE with the
learned features:

Theorem 4.5. Under Assumption 4.1 (and Assumption 4.3
if P (s, a) is stochastic). Let C2, (cid:98)ε be as deﬁned in Theo-
rem 4.4. If N ≥ C 2
2 , then we have for any δ ∈ (0, 1/2), w.p.
at least 1 − 2δ, for all distributions p0,

(cid:12)
(cid:12)V πe
(cid:12)

− Es∼p0

(cid:104)
(cid:98)fK(s, πe)

p0
960β−1/2(1 + W )d log(N )(cid:112)log(10/δ)

(cid:105)(cid:12)
(cid:12)
(cid:12) ≤

γK/2
1 − γ

+

√

(1 − γ)2

N

.

+

(cid:114)(cid:13)
(cid:13)
(cid:13)

4

ddπe
p0
dν

(cid:13)
(cid:13)
(cid:13)∞

(1 − γ)2

(cid:98)ε

Comparison to FQE What if one ignores the representa-
tion learning and just runs the Fitted Q-Evaluation (FQE)
which directly performs least square ﬁtting with the nonlin-
ear function class F := {w(cid:62)φ(s, a) : φ ∈ Φ, (cid:107)w(cid:107)2 ≤ W }?
As N → ∞, FQE will suffer the following worst-case Bell-
man error (also called inherent Bellman error):

εibe := max
f ∈F

min
g∈F

(cid:107)g − T πe f (cid:107)2
ν .

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Algorithm 3 Practical Instantiation of BCRL
1: Input: Ofﬂine dataset D = {s, a, s(cid:48)}, target policy πe
2: Initialize parameters for φ, M , ρ, and φ
3: for t = 0, 1, . . . , T do
4: Mt+1 ← Mt − η∇M J(φt, Mt, ρt, φt)
ρt+1 ← ρt − η∇ρJ(φt, Mt, ρt, φt)
5:
φt+1 ← φt − η∇φJ(φt, Mt+1, ρt+1, φt)
6:
φt+1 ← τ φt+1 + (1 − τ )φt
7:
8: end for
9: Linear evaluation: (cid:98)V = LSPE(πe, φT , D).

Note that our assumption that there exists a linear BC rep-
resentation φ(cid:63) does not imply that the worst-case Bellman
error εibe is small. In contrast, when N → ∞, our approach
will accurately estimate V πe
p0 .

5. A Practical Implementation

In this section we instantiate a practical implementation to
learn our representation using deep neural networks for our
representation function class Φ. Based on Equation (3), we
ﬁrst formalize our bilevel optimization objective:

J(φ, M, ρ, φ) = ED

(cid:21)

(cid:13)
(cid:20)M
(cid:13)
(cid:13)
ρ
(cid:13)
− λ log det ED

φ(s, a) −

(cid:20)γφ(s(cid:48), πe)
r(s, a)
(cid:2)φ(s, a)φ(s, a)(cid:62)(cid:3) .

(cid:124)

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

In our implementation, we replace the hard constraint pre-
sented in Section 4.1 with a Lagrange multiplier, i.e. we use
the optimal design constraint as a regularization term when
learning φ. Speciﬁcally, we maximize the log det of the
covariance matrix induced by the feature, which maximizes
all eigenvalues since log det is the sum of the log eigen-
values. Our experiment results in Section 6.4 demonstrate
that it indeed improves the condition number of Σ(φ). In
some of our experiments, we use a target network in our
implementation. Namely, the updates make use of a target
network φ where the weights can be an exponential moving
average of the representation network’s weights. This idea
of using a slow moving target network has been shown to
stabilize training in both the RL (Mnih et al., 2013) and the
representation learning literature (Grill et al., 2020).

As summarized in Algorithm 3, given our ofﬂine behavior
dataset D and target policy πe, we iteratively update M

Figure 1. Representative frames from DeepMind Control Suite
tasks. From left to right, Finger Turn Hard, Cheetah
Run, Quadruped Walk, and Humanoid Stand.

and φ and then use the resulting learned representation to
perform OPE. Please see Appendix F for implementation
and hyperparameter details. As we will show in our
experiments, our update procedures for φ signiﬁcantly
minimizes the Bellman Completeness loss and also improve
the condition number of Σ(φ), which are the two key
quantities to ensure good performance of LSPE with linear
regression as shown in Theorem 3.3.

6. Experiments

Our goal is to answer the following questions. (1) How
do our representations perform on downstream LSPE com-
pared to other popular unsupervised representation learning
techniques? (2) How important are both the linear bellman
completeness and optimal design components for learning
representations? (3) How competitive is BCRL with other
OPE methods, especially for evaluating beyond the initial
state distribution?

Following the standards in evaluating representations in su-
pervised learning, we used a linear evaluation protocol,
i.e., linear regression in LSPE on top of a given representa-
tion. This allows us to focus on evaluating the quality of the
representation. We compared our method to prior techniques
on a range of challenging, image-based continuous control
tasks from the DeepMind Control Suite benchmark (Tassa
et al., 2018): Finger Turn Hard, Cheetah Run,
Quadruped Walk, and Humanoid Stand. Frames
from the tasks are shown in Figure 1. To investigate our
learned representation, we benchmark our representation
against two state-of-the-art self-supervised representation
learning objectives adopted for RL: (1) CURL uses the In-
foNCE objective to contrastively learn state-representations;
and (2) SPR adopts the BYOL contrastive learning frame-
work to learn representations with latent dynamics. Note,
we modiﬁed CURL for OPE by optimizing the contrastive
loss between state-action pairs rather than just states. For
SPR, we did not include the Q prediction head and used
their state representation plus latent dynamics as the state-
action representation for downstream linear evaluation. We
used the same architecture for the respective representa-
tions across all evaluated algorithms. To compare against
other OPE methods, we additionally compared against Fit-
ted Q-Evaluation (Munos & Szepesvári, 2008; Kostrikov
& Nachum, 2020) (FQE), weighted doubly robust policy
evaluation (Jiang & Li, 2016; Thomas & Brunskill, 2016)
(DR), Dreamer-v2 (Hafner et al., 2021) model based eval-
uation (MB), and DICE (Yang et al., 2020). We modiﬁed
implementations from the benchmark library released by Fu
et al. (2021) for FQE and DR and used the authors’ released
codebases for Dreamer-v2 and BestDICE.2

2https://github.com/google-research/dice_rl.

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Figure 2. OPE curves across ﬁve seeds using representations trained with BCRL, SPR, and CURL on the ofﬂine datasets (Table 1).

mators beyond the original initial state distribution d0. The
ﬁrst setting ensures that baselines and our algorithm all sat-
isfy the coverage condition, thus demonstrating the unique
beneﬁt of learning a Bellman complete representation. The
second setting evaluates the robustness of our algorithm, i.e.,
the ability to estimate beyond the original d0.

Evaluation on On-Policy + Off-Policy Data: To further
investigate the importance of learning linear BC features,
we experiment with learning representations from an ofﬂine
dataset that also contains state-action pairs from the target
policy. More speciﬁcally, we train our representations on a
dataset containing 100K behavior policy and 100K target
policy samples. Note, only the experiment in this paragraph
uses this mixture dataset. With the addition of on-policy
data from the target policy, we can focus on just the role of
linear BC for OPE performance because the density ratio
ddπe
po
dν and the relative condition number (Eq. 1) is at most
2, i.e. we omit the design regularization and focused on
minimizing the Bellman completeness loss. Figure 4 (Left)
shows that BCRL outperforms baselines in this setting, even

Figure 3. (Left) Root mean squared evaluation error across all tasks.
(Right) Mean spearman ranking correlation across all tasks.

Our target policies were trained using the authors’ imple-
mentation of DRQ-v2 (Yarats et al., 2021a), a state-of-the-
art, off-policy actor critic algorithm for vision-based contin-
uous control. With high-quality target policies, we collected
200 rollouts and did a linear evaluation protocol to predict
the discounted return. For our ofﬂine behavior datasets, we
collected 100K samples from a trained policy with mean
performance roughly a quarter of that of the target policy
(Table 1). All results are aggregated over ﬁve random seeds.
See Appendix F for details on hyperparameters, environ-
ments, and dataset composition.

6.1. OPE via LSPE with Learned Representations

Figure 2 compares the OPE performance of BCRL against
SPR and CURL. Representations learned by BCRL outper-
form those learned by SPR and CURL. On some tasks, SPR
and CURL both exhibited an exponential error ampliﬁca-
tion with respect to the number of iterations of LSPE, while
BCRL did not suffer from any blowup.

6.2. OPE Performance

Figure 3 compares the OPE performance of BCRL against
multiple benchmarks from the OPE literature. BCRL is
competitive with FQE and evaluates better than other bench-
marks across the tasks that we tested on.

We also evaluated how well estimated values from
BCRL rank policies. Following (Fu et al., 2021), we use
the spearman ranking correlation metric to compute the
correlation between ordinal rankings according to the OPE
estimates and the ground truth values. For ranking, we
evaluated three additional target policies with mean perfor-
mances roughly 75%, 50%, and 10% of the target policy.
Figure 3 presents the mean correlation of each evaluation
algorithm across all tasks. BCRL is competitive with FQE
and consistently better than other benchmarks at ranking
policies.

6.3. Further Investigation of Different Settings

In this section, we consider two additional settings: (1) the
ofﬂine dataset contains some on-policy data, which ensures
that the ofﬂine data provides coverage over the evaluation
policy’s state action distribution; (2) we evaluate all esti-

Figure 4. (Left) Evaluation on a mixture dataset with on-policy
and off-policy data. Note the addition of target policy data bounds
the relative condition number (Eq. 1). (Right) Evaluation beyond
the initial state distribution (given just the ofﬂine data).

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Figure 5. (Left) BCRL’s OPE curves for Cheetah Run with (blue) and without (red) the D-optimal design based regularization. (Center)
Bar plot of the singular values of the feature covariance matrices for the left plot. (Right) OPE curves for Finger Turn Hard with
(blue) and without (red) optimizing for linear BC.

matching FQE performance across tasks. Our experiments
corroborate our analysis that explicitly enforcing our learned
representations to be linear BC improves OPE. Note that the
fact that LSPE with SPR and CURL features still blows up
under this mixture data means that the other representations’
failures are not just due to coverage.

Evaluation Beyond Initial State Distribution: To further
investigate the beneﬁts of optimizing the D-optimal design
to improve coverage, we investigate doing OPE beyond the
initial state distribution d0, which is supported by Theo-
rem 4.5. Note that if our representation is exactly Bellman
complete and also the corresponding feature covariance ma-
trix is well-conditioned, we should be able to evaluate well
on any states. Speciﬁcally, we experiment on evaluating at
all timesteps in a target policy rollout, not just at the initial
state distribution. Figure 4 (Right) shows that BCRL is able
to more robustly evaluate out-of-distribution than all other
benchmarks.

6.4. Ablation Studies

Impact of Optimal Design Regularization: To investi-
gate the impact of maximizing the D-optimal design, we
ablate the design regularization term from our objective
and analyze the downstream evaluation performance and
the respective feature covariance matrices on the ofﬂine
dataset. Figure 5 (Center) presents a bar plot of the sin-
gular values of the feature covariance matrix (Σ(φ) :=
Es,a∼νφ(s, a)φ(s, a)(cid:62)). Figure 5 (Left) shows the down-
stream OPE performance for features trained with and with-
out the design regularization on the Cheetah Run task.
Note that without the regularization, we ﬁnd that that the
feature covariance matrix has much worse condition number,
i.e. feature is less exploratory. As our analysis suggests, we
also observe a deterioration in evaluation performance with-
out the design regularization to explicitly learn exploratory
features.

Impact of Linear Bellman Completeness:
Figure 5
(Right) presents an ablation study where we only opti-
mize for the design term in our objective. We ﬁnd that

downstream OPE performance degrades without directly op-
timizing for linear BC, suggesting that a feature with good
coverage alone is not enough to avoid error ampliﬁcation.

7. Conclusion

We present BCRL which leverages rich function approxi-
mation to learn Bellman complete and exploratory repre-
sentations for stable and accurate ofﬂine policy evaluation.
We provide a mathematical framework of representation
learning in ofﬂine RL, which generalizes all existing repre-
sentation learning frameworks from the RL theory literature.
We provide an end-to-end theoretical analysis of our ap-
proach for OPE and demonstrate that BCRL can accurately
estimate policy values with polynomial sample complex-
ity. Notably, the complexity has no explicit dependence
on the size of the state and action space, instead, it only
depends on the statistical complexity of the representation
hypothesis class. Experimentally, we extensively evaluate
our approach on the DeepMind Control Suite, a set of image-
based, continuous control, robotic tasks. First, we show that
under the linear evaluation protocol – using linear regres-
sion on top of the representations inside the classic LSPE
framework – our approach outperforms prior RL representa-
tion techniques CURL and SPR which leverage contrastive
representation learning techniques SimCLR and BYOL re-
spectively. We also show that BCRL achieves competitive
OPE performance with the state-of-the-art FQE, and notice-
ably improves upon it when evaluating beyond the initial
state distribution. Finally, our ablations show that approxi-
mate Linear Bellman Completeness and coverage are crucial
ingredients to the success of our algorithm. Future work
includes extending BCRL to ofﬂine policy optimization.

ACKNOWLEDGEMENTS

This material is based upon work supported by the National
Science Foundation under Grant No. 1846210 and by a Cor-
nell University Fellowship. We thank Rahul Kidambi, Ban
Kawas, and the anonymous reviewers for useful discussions
and feedback.

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

References

Agarwal, A., Jiang, N., Kakade, S. M., and Sun, W. Rein-
forcement learning: Theory and algorithms. CS Dept.,
UW Seattle, Seattle, WA, USA, Tech. Rep, 2019.

Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W.
Flambe: Structural complexity and representation learn-
ing of low rank mdps. NeurIPS, 33:20095–20107, 2020.

Amortila, P., Jiang, N., and Xie, T. A variant of the wang-
foster-kakade lower bound for the discounted setting.
arXiv preprint arXiv:2011.01075, 2020.

Anand, A., Racah, E., Ozair, S., Bengio, Y., Côté,
Unsupervised state rep-
M., and Hjelm, R. D.
resentation learning in atari.
In Wallach, H. M.,
Larochelle, H., Beygelzimer, A., d’Alché-Buc, F.,
Fox, E. B., and Garnett, R. (eds.), NeurIPS, pp. 8766–
URL https://proceedings.
8779,
neurips.cc/paper/2019/hash/
6fb52e71b837628ac16539c1ff911667-Abstract.
html.

2019.

Antos, A., Szepesvári, C., and Munos, R.

Fitted
In
and Roweis,
(eds.), NIPS, volume 20. Curran Associates,
URL https://proceedings.

q-iteration in continuous action-space mdps.
Platt,
J., Koller, D., Singer, Y.,
S.
Inc.,
neurips.cc/paper/2007/file/
da0d1111d2dc5d489242e60ebcbaf988-Paper.
pdf.

2008.

Bhatia, R. Matrix analysis, volume 169. Springer Science

& Business Media, 2013.

Boucheron, S., Lugosi, G., and Massart, P. Concentration
inequalities: A nonasymptotic theory of independence.
Oxford university press, 2013.

Chen, J. and Jiang, N. Information-theoretic considerations
in batch reinforcement learning. In ICML, pp. 1042–1051.
PMLR, 2019.

Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. E.
A simple framework for contrastive learning of visual
representations. ICML, 2020. URL https://arxiv.
org/abs/2002.05709.

Chung, W., Nath, S., Joseph, A., and White, M. Two-
timescale networks for nonlinear value function approxi-
mation. In ICLR, 2019. URL https://openreview.
net/forum?id=rJleN20qK7.

Duan, Y., Jia, Z., and Wang, M. Minimax-optimal off-policy
evaluation with linear function approximation. In ICML,
pp. 2701–2709. PMLR, 2020.

Foster, D. J., Krishnamurthy, A., Simchi-Levi, D., and Xu,
Y. Ofﬂine reinforcement learning: Fundamental barri-
ers for value function approximation. arXiv preprint
arXiv:2111.10919, 2021.

Fu, J., Norouzi, M., Nachum, O., Tucker, G., Wang, Z.,
Novikov, A., Yang, M., Zhang, M. R., Chen, Y., Kumar,
A., Paduraru, C., Levine, S., and Paine, T. L. Benchmarks
for deep off-policy evaluation. ICLR, 2021. URL https:
//arxiv.org/abs/2103.16596.

Grill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond, P.,
Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z.,
Gheshlaghi Azar, M., Piot, B., kavukcuoglu, k., Munos,
R., and Valko, M. Bootstrap your own latent - a new
approach to self-supervised learning. In Larochelle, H.,
Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H.
(eds.), NeurIPS, volume 33, pp. 21271–21284. Curran As-
sociates, Inc., 2020. URL https://proceedings.
neurips.cc/paper/2020/file/
f3ada80d5c4ee70142b17b8192b2958e-Paper.
pdf.

Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-
critic: Off-policy maximum entropy deep reinforcement
learning with a stochastic actor. In ICML, pp. 1861–1870.
PMLR, 2018.

Hafner, D., Lillicrap, T. P., Norouzi, M., and Ba, J.
In In-
Mastering atari with discrete world models.
ternational Conference on Learning Representations,
2021. URL https://openreview.net/forum?
id=0oabwyZbOu.

Hao, B., Duan, Y., Lattimore, T., Szepesvári, C., and Wang,
M. Sparse feature selection makes batch reinforcement
learning more sample efﬁcient. In ICML, pp. 4063–4073.
PMLR, 2021.

He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. B. Mo-
mentum contrast for unsupervised visual representation
learning. CVPR, 2020. URL http://arxiv.org/
abs/1911.05722.

Jiang, N. and Li, L. Doubly robust off-policy value evalua-
tion for reinforcement learning. In ICML, pp. 652–661.
PMLR, 2016.

Du, S. S., Kakade, S. M., Lee, J. D., Lovett, S., Mahajan,
G., Sun, W., and Wang, R. Bilinear classes: A struc-
tural framework for provable generalization in rl. arXiv
preprint arXiv:2103.10897, 2021.

Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and
Schapire, R. E. Contextual decision processes with low
bellman rank are pac-learnable. In ICML, pp. 1704–1713.
PMLR, 2017.

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. Provably
efﬁcient reinforcement learning with linear function ap-
proximation. In COLT, pp. 2137–2143. PMLR, 2020.

Kostrikov, I. and Nachum, O. Statistical bootstrapping
for uncertainty estimation in off-policy evaluation, 2020.
URL https://arxiv.org/abs/2007.13609.

Laskin, M., Srinivas, A., and Abbeel, P. Curl: Contrastive
unsupervised representations for reinforcement learning.
In ICML, pp. 5639–5650. PMLR, 2020.

Lattimore, T. and Szepesvári, C. Bandit algorithms. Cam-

bridge University Press, 2020.

Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T.,
Tassa, Y., Silver, D., and Wierstra, D. Continuous control
with deep reinforcement learning. In ICLR, 2016.

Parr, R., Li, L., Taylor, G., Painter-Wakeﬁeld, C., and
Littman, M. L. An analysis of linear models, lin-
ear value-function approximation, and feature selection
In ICML, pp. 752–759,
for reinforcement learning.
New York, NY, USA, 2008. Association for Comput-
doi: 10.
ing Machinery.
1145/1390156.1390251. URL https://doi.org/
10.1145/1390156.1390251.

ISBN 9781605582054.

Pollard, D. Empirical processes: theory and applications.
In NSF-CBMS regional conference series in probability
and statistics, pp. i–86. JSTOR, 1990.

Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D.,
Courville, A. C., and Bachman, P. Data-efﬁcient rein-
forcement learning with momentum predictive represen-
tations. ICLR, 2021. URL https://arxiv.org/
abs/2007.05929.

Mazoure, B., Tachet des Combes, R., Doan, T. L.,
Bachman, P., and Hjelm, R. D. Deep reinforcement
and infomax learning.
In Larochelle, H., Ranzato,
M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.),
NeurIPS, volume 33, pp. 3686–3698. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.
neurips.cc/paper/2020/file/
26588e932c7ccfa1df309280702fe1b5-Paper.
pdf.

Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,
Antonoglou, I., Wierstra, D., and Riedmiller, M. A.
Playing atari with deep reinforcement learning. NIPS
Deep Learning Workshop, 2013. URL http://arxiv.
org/abs/1312.5602.

Modi, A., Chen, J., Krishnamurthy, A., Jiang, N.,
and Agarwal, A. Model-free representation learning
arXiv preprint
and exploration in low-rank mdps.
arXiv:2102.07035, 2021.

Munos, R. and Szepesvári, C. Finite-time bounds for ﬁtted

value iteration. JMLR, 9(5), 2008.

Nachum, O. and Yang, M.

Provable representation
learning for imitation with contrastive fourier features.
NeurIPS, 2021. URL https://arxiv.org/abs/
2105.12272.

Nedi´c, A. and Bertsekas, D. P. Least squares policy evalua-
tion algorithms with linear function approximation. Dis-
crete Event Dynamic Systems, 13(1):79–110, 2003.

Ni, C., Zhang, A. R., Duan, Y., and Wang, M. Learning good
state and action representations via tensor decomposition.
In 2021 IEEE International Symposium on Information
Theory (ISIT), pp. 1682–1687. IEEE, 2021.

learning.

reinforcement

Song, Z., Parr, R. E., Liao, X., and Carin, L. Linear
feature encoding for
In
Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and
Garnett, R. (eds.), NIPS, volume 29. Curran Asso-
ciates, Inc., 2016. URL https://proceedings.
neurips.cc/paper/2016/file/
8232e119d8f59aa83050a741631803a6-Paper.
pdf.

Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D.
d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq,
arXiv preprint
A., et al. Deepmind control suite.
arXiv:1801.00690, 2018.

Thomas, P. S. and Brunskill, E. Data-efﬁcient off-policy
policy evaluation for reinforcement learning, 2016. URL
https://arxiv.org/abs/1604.00923.

Uehara, M. and Sun, W. Pessimistic model-based ofﬂine
reinforcement learning under partial coverage. arXiv
preprint arXiv:2107.06226, 2021.

Uehara, M., Zhang, X., and Sun, W. Representation learning
for online and ofﬂine RL in low-rank MDPs. In ICLR,
2022. URL https://openreview.net/forum?
id=J4iSIR9fhY0.

van den Oord, A., Li, Y., and Vinyals, O. Representation
learning with contrastive predictive coding. arXiv, 2018.
URL http://arxiv.org/abs/1807.03748.

van der Vaart, A. W. and Wellner, J. A. Weak Con-
vergence and Empirical Processes.
Springer Se-
ISBN
ries in Statistics. Springer New York, 1996.
9781475725476.
doi: 10.1007/978-1-4757-2545-2.
URL http://link.springer.com/10.1007/
978-1-4757-2545-2.

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Wainwright, M. J. High-dimensional statistics: A non-
asymptotic viewpoint, volume 48. Cambridge University
Press, 2019.

Wang, R., Foster, D. P., and Kakade, S. M. What are the
statistical limits of ofﬂine RL with linear function approx-
imation? ICLR, 2021a. URL https://arxiv.org/
abs/2010.11895.

Wang, Y., Wang, R., and Kakade, S. M. An exponential
lower bound for linearly-realizable mdps with constant
suboptimality gap. arXiv preprint arXiv:2103.12690,
2021b.

Yang, M. and Nachum, O. Representation matters: Ofﬂine
pretraining for sequential decision making. In Meila, M.
and Zhang, T. (eds.), ICML, volume 139 of Proceedings
of Machine Learning Research, pp. 11784–11794. PMLR,
2021. URL http://proceedings.mlr.press/
v139/yang21h.html.

Yang, M., Nachum, O., Dai, B., Li, L., and Schuurmans,
D. Off-policy evaluation via the regularized lagrangian.
NeurIPS, 33:6551–6561, 2020.

Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. Mastering
visual continuous control: Improved data-augmented re-
inforcement learning. arXiv preprint arXiv:2107.09645,
2021a.

Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J.,
and Fergus, R. Improving sample efﬁciency in model-
free reinforcement learning from images. AAAI, 2021b.
URL http://arxiv.org/abs/1910.01741.

Zhang, W., He, J., Zhou, D., Zhang, A., and Gu, Q. Prov-
ably efﬁcient representation learning in low-rank markov
decision processes. arXiv preprint arXiv:2106.11935,
2021.

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Appendices

A. Metric Entropy and Entropy Integral

We recall the standard notions of entropy integrals here, based on the following distance on Φ,

dΦ(φ, (cid:101)φ) =∆ ED

(cid:104)(cid:13)
(cid:13)
(cid:13)φ(s, a) − (cid:101)φ(s, a)

(cid:105)

(cid:13)
(cid:13)
(cid:13)2

Let N (t, Φ) denote the t-covering number under dΦ.
Deﬁnition A.1. Deﬁne the entropy integral, which we assume to be ﬁnite as,

κ(Φ) =∆

(cid:90) 4

0

log1/2 N (t, Φ)dt

When Φ is ﬁnite, N (t) ≤ |Φ|, so κ(Φ) ≤ O(log1/2(|Φ|)).

B. Technical Lemmas

Lemma B.1. Let Xi be i.i.d. random variables s.t. |Xi| ≤ c and E (cid:2)X 2
least 1 − δ,

i

(cid:3) ≤ ν, then for any δ ∈ (0, 1), we have w.p. at

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

N
(cid:88)

i=1

Xi − E [X]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ inf
a>0

ν
2a

+

(c + a) log(2/δ)
N

,

and if ν ≤ LE [X] for some positive L, then in particular,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

N
(cid:88)

i=1

(cid:12)
(cid:12)
Xi − E [X]
(cid:12)
(cid:12)
(cid:12)

≤

1
2

E [X] +

(c + L) log(2/δ)
N

.

Proof. First, by Bernstein’s inequality (Boucheron et al., 2013, Theorem 2.10), we have w.p. 1 − δ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

N
(cid:88)

i=1

(cid:12)
(cid:12)
Xi − E [X]
(cid:12)
(cid:12)
(cid:12)

(cid:114)

≤

2ν log(2/δ)
N

+

c log(2/δ)
N

Using the fact that 2xy ≤ x2/a + ay2 for any a > 0, split the square root term,

(c + a) log(2/δ)
N
which yields the ﬁrst part. If ν ≤ LE [X], picking a = L concludes the proof.

≤ inf
a>0

ν
2a

+

We now state several results of Orlicz norms (mostly from Pollard, 1990) for completeness. For an increasing, convex,
positive function Φ : R+ (cid:55)→ R+ , such that Φ(x) ∈ [0, 1), deﬁne the Orlicz norm as
(cid:107)Z(cid:107)Φ := inf {C > 0 | E [Φ(|Z|/C)] ≤ 1} .
It is indeed a norm on the Φ-Orlicz space of random variables LΦ(ν), since it can be interpreted as the Minkowski functional
of the convex set K = {X : E [Φ(|X|)] ≤ 1}.

Let x1, x2, . . . , xN be i.i.d. datapoints drawn from some underlying distribution. Let ω denote the randomness of the
N sampled datapoints, and let Fω = (cid:8)(f (xi(ω)))N
i=1 : f ∈ F(cid:9) ⊂ RN denote the (random) set of vectors from the data
corresponding to ω. Let σ denote a vector of N i.i.d. Rademacher random variables (±1 equi-probably), independent of all
else.

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Lemma B.2 (Symmetrization). For any increasing, convex, positive function Φ, we have

(cid:34)

(cid:32)

Eω

Φ

sup
f ∈Fω

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N
(cid:88)

i=1

fi − Eωfi

(cid:33)(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:34)

(cid:32)

(cid:33)(cid:35)

≤ Eω,σ

Φ

2 sup
f ∈Fω

|σ · f |

.

Proof. See Theorem 2.2 of (Pollard, 1990).

Lemma B.3 (Contraction). Let F ⊂ RN , and suppose λ : RN (cid:55)→ RN s.t. each component λi : R (cid:55)→ R is L-Lipschitz.
Then, for any increasing, convex, positive function Φ, we have

(cid:34)

(cid:32)

Eσ

Φ

sup
f ∈F

(cid:33)(cid:35)

|σ · λ(f )|

≤

(cid:34)

(cid:32)

(cid:33)(cid:35)

Eσ

Φ

2L sup
f ∈F

|σ · f |

3
2

Proof. Apply Theorem 5.7 of (Pollard, 1990) to the functions λi/L, which are contractions.

We now focus on the speciﬁc Orlicz space of sub-Gaussian random variables, with the function Ψ(x) = 1
Ψ-Orlicz norm of the maximum of random variables can be bounded by the maximum of the Ψ-Orlicz norms.

5 exp(x2). The

Lemma B.4. For any random variables Z1, ..., Zm, we have

(cid:107) max
i≤m

|Zi|(cid:107)Ψ ≤ (cid:112)2 + log(m) max

i≤m

(cid:107)Zi(cid:107)Ψ

Proof. See Lemma 3.2 of (Pollard, 1990).

Lemma B.5. For each f ∈ RN , we have (cid:107)σ · f (cid:107)Ψ ≤ 2(cid:107)f (cid:107)2.

Proof. See Lemma 3.1 of (Pollard, 1990).

The following is a truncated chaining result for Orlicz norms. This result is not new, but many sources state and prove it in
terms of covering and Rademacher complexity, rather than for Orlicz norms. In particular, it generalizes Theorem 3.5 of
(Pollard, 1990) – consider a sequence of α’s converging to zero.
Lemma B.6 (Chaining). Let F ⊂ RN such that 0 ∈ F. Then,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

sup
f ∈F

|σ · f |

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ

(cid:40)

≤ inf
α≥0

3α

√

N + 9

(cid:90) b

α

(cid:112)log(D(δ/2, F))dδ

(cid:41)

,

where b = supf ∈F (cid:107)f (cid:107)2 and D(δ, F) is the Euclidean δ-packing number for F.

Proof. Suppose b and all the packing numbers are ﬁnite, otherwise the right hand side is inﬁnite and there is nothing to
show. For an arbitrary K > 1, construct a sequence of K ﬁner and ﬁner approximations to F,

{0} = F0 ⊂ F1 ⊂ · · · ⊂ FK ⊂ FK+1 = F

where for any k ∈ [K], Fk satisﬁes the property that for any f ∈ F, there exist nk(f ) ∈ Fk s.t. (cid:107)nk(f ) − f (cid:107)2 ≤ b2−k.
Indeed this can be done iteratively: for any Fk, we can construct Fk+1 by adding elements to construct a maximal b2−k
packing (maximality ensures the distance requirement, since the existence of any vector which has larger distance can be
added to the packing). By deﬁnition of D(·, F), we have |Fk| ≤ D(b2−k, F).

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

For any k ∈ [K], we have by triangle inequality,

sup
f ∈Fk+1

|σ · f | ≤ sup

f ∈Fk+1

|σ · nk(f ))| + sup

|σ · (nk(f ) − f )|

= sup
f ∈Fk

|σ · f | + sup

f ∈Fk+1

f ∈Fk+1
|σ · (nk(f ) − f )|

If k = K, we can loosely bound the right-most term by Cauchy-Schwarz, since for any f ∈ F, we have |σ · (nk(f ) − f )| ≤
√

√

N (cid:107)nk(f ) − f (cid:107)2 ≤

N b2−K. So, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

sup
f ∈F

|σ · f |

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

max
f ∈FK

|σ · f |

(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ

+

√

N b2−K
√
log 5

,

since for any non-negative constant c, (cid:107)c(cid:107)Ψ = inf (cid:8)C > 0 : 1
log 5 . If k < K, the suprema are
taken over ﬁnite sets, so the maximum is attained. Hence, we can apply a special property of the Ψ-Orlicz norm (Lemma B.4),
to get,

5 exp((c/C)2) ≤ 1(cid:9) = c√

(cid:13)
(cid:13)
(cid:13)
(cid:13)

max
f ∈Fk+1

|σ · f |

(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ

By Lemma B.5,

≤

≤

≤

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

max
f ∈Fk

|σ · f |

max
f ∈Fk

|σ · f |

max
f ∈Fk

|σ · f |

max
f ∈Fk

|σ · f |

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

max
f ∈Fk+1

|σ · (nk(f ) − f ))|

(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ

+ (cid:112)2 + log(|Fk+1|) max

f ∈Fk+1

(cid:107)σ · (nk(f ) − f ))(cid:107)Ψ

+ 2(cid:112)2 + log(|Fk+1|) max

f ∈Fk+1

(cid:107)nk(f ) − f )(cid:107)2

+ 2(cid:112)2 + log(|Fk+1|) · b2−k.

Also, note that since F0 = {0} by construction, we have (cid:107)maxf ∈F0 |σ · f |(cid:107)Ψ = 0. Unrolling this, we have

(cid:107) sup
f ⊂F

|σ · f |(cid:107)Ψ ≤

√

N b2−K
√
log 5

K−1
(cid:88)

+

2−k2b

(cid:113)

2 + log(D(b2−(k+1), F))

(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ
(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ

(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ
(cid:13)
(cid:13)
(cid:13)
(cid:13)Ψ

k=0
√

Since for any D ≥ 2, we have (cid:112)2 + log(1 + D)/

D ≤ 2.2,

√

N b2−K
√
log 5

√

N b2−K
√
log 5

≤

=

Since D(·, F) is a monotone decreasing,

√

√

N b2−K
√
log 5
N b2−K
√
log 5

≤

=

+ 4.4b

K−1
(cid:88)

k=0

2−k

(cid:113)

log(D(b2−(k+1), F))

K−1
(cid:88)

(cid:113)

+ 17.6b

(2−(k+1) − 2−(k+2))

log(D(b2−(k+1), F))

k=0

(cid:90) b/2

+ 17.6

b2−(K+1)

(cid:112)log(D(δ, F))dδ

(cid:90) b

+ 8.8

b2−K

(cid:112)log(D(δ/2, F))dδ.

Now consider any α > 0. Pick K such that

b

2K+1 ≤ α ≤ b

2K , then we have

(cid:107) sup
f ⊂F

|σ · f |(cid:107)Ψ ≤

√

2α
√

N
log 5

+ 8.8

(cid:90) b

α

(cid:112)log(D(δ/2, F))dδ.

Since α was arbitrary, the above bound holds when we take an inﬁmum over α.

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

C. Proofs for LSPE

We ﬁrst show a generalization of the performance difference lemma (PDL).
Lemma C.1 (Generalized PDL). For any policies π, π(cid:48), any function f : S × A (cid:55)→ R, and any initial state distribution µ,
we have

µ − Es∼µ [f (s, π(cid:48))] =
V π

1
1 − γ

Es,a∼dπ

µ

(cid:104)

T π(cid:48)

(cid:105)
f (s, a) − f (s, π(cid:48))

(7)

Proof. Let T π be the distribution of trajectories τ = (s0, a0, s1, a1, s2, a2, ...) from rolling out π. So, we have,

µ − Es∼µ [f (s, π(cid:48))] = Eτ ∼T π
V π

= Eτ ∼T π

= Eτ ∼T π

(cid:35)

γtr(st, at)

− Es∼µ [f (s, π(cid:48))]

(cid:35)
γt (r(st, at) + f (st, π(cid:48)) − f (st, π(cid:48))) − f (s0, π(cid:48))

γt (r(st, at) + γf (st+1, π(cid:48)) − f (st, π(cid:48)))

(cid:35)

(cid:34) ∞
(cid:88)

t=0
(cid:34) ∞
(cid:88)

t=0
(cid:34) ∞
(cid:88)

t=0

=

=

1
1 − γ
1
1 − γ

Es,a∼dπ

µ

(cid:2)r(s, a) + γEs(cid:48)∼P (s,a) [f (s(cid:48), π(cid:48))] − f (s, π(cid:48))(cid:3)

Es,a∼dπ

µ

(cid:104)

T π(cid:48)

(cid:105)
f (s, a) − f (s, π(cid:48))

.

This generalizes the PDL, which we can get by setting f (s, a) = Qπ(cid:48)

(s, a):

µ − V π(cid:48)
V π

µ =

=

=

1
1 − γ
1
1 − γ
1
1 − γ

Es,a∼dπ

µ

Es,a∼dπ

µ

(cid:104)
T π(cid:48)

(cid:104)
Qπ(cid:48)

Qπ(cid:48)

(s, a) − Qπ(cid:48)

(s, π(cid:48))

(cid:105)

(s, a) − V π(cid:48)

(s)

(cid:105)

Es,a∼dπ

µ

(cid:104)
Aπ(cid:48)

(cid:105)
(s, a)

.

To prove our LSPE guarantee, we’ll instantiate f (s, a) to be the estimated (cid:98)f (s, a) from LSPE, and set π = π(cid:48). This gives us
an expression for the prediction error of LSPE,

(cid:12)
(cid:12)V π
(cid:12)

µ − Es∼µ

(cid:104)

(cid:98)fk(s, π)

(cid:105)(cid:12)
(cid:12)
(cid:12) =

1
1 − γ

(cid:12)
(cid:12)
(cid:12)

Edπ

µ

(cid:104)

T π (cid:98)fk(s, a) − fk(s, a)

(cid:105)(cid:12)
(cid:12)
(cid:12) .

We then upper bound the right hand side by its L2(dπ
of running LSPE by the regression losses at each step.
Lemma C.2. Consider any policy π and functions f1, . . . , fK : S × A (cid:55)→ R that satisfy maxk=1,...,K (cid:107)fk −
T πfk−1(cid:107)L2(dπ

) ≤ η, and f0(s, a) = 0. Then, for all k = 1, . . . , K, we have (cid:107)fk − T πfk(cid:107)L2(dπ

µ) norm, which is the Bellman error. Next, we bound the Bellman error

) ≤ 4

1−γ η + γk/2.

p0

p0

Proof. For any k = 1, . . . , K,

(cid:107)fk − T πfk(cid:107)L2(dπ

p0

) ≤ (cid:107)fk − T πfk−1(cid:107)L2(dπ
≤ η + γ(Es,a∼dπ
p0
≤ η + γ(Es,a∼dπ

p0

) + (cid:107)T πfk−1 − T πfk(cid:107)L2(dπ

p0

)

p0

[(Es(cid:48),a(cid:48)∼P (s,a)◦π [fk−1(s(cid:48), a(cid:48)) − fk(s(cid:48), a(cid:48))])2])1/2
,s(cid:48),a(cid:48)∼P (s,a)◦π[(fk−1(s(cid:48), a(cid:48)) − fk(s(cid:48), a(cid:48)))2])1/2

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Since dπ
p0
thus non-negative,

(s, a) = γE˜s,˜a∼dπ
P0

P (s|˜s, ˜a)π(a|s) + (1 − γ)p0(s)π(a|s), and the quantity inside the expectation is a square, and

≤ η + γ(γ−1Es,a∼dπ
p0

[(fk−1(s, a) − fk(s, a))2])1/2

√

= η +

γ(cid:107)fk−1 − fk(cid:107)L2(dπ
p0

)

√

≤ η +

γ

(cid:16)

η + (cid:107)fk−1 − T πfk−1(cid:107)L2(dπ

p0

(cid:17)

.

)

Unrolling the recursion and using (cid:107)f0 − T πf0(cid:107)L2(dπ

p0

) ≤ 1, we have

(cid:107)fK − T πfK(cid:107)L2(dπ

p0

) ≤ η +

√

γ(η +

√

γ(η + . . .

√

γ(η + 1) . . . ))

√
γK
√
γ

1 −
1 −

=

η + γK/2,

which gives the claim since

1
√
1−

γ ≤ 2(1 − γ)−1 and 1 −

√

γK ≤ 1.

The following lemma is a “fast rates”-like result for norms. It shows that the norm induced by the empirical covariance
matrix (cid:98)Σ can be bounded by the norm induced by two times the population covariance matrix Σ, up to some (cid:101)O(N −1/2)
terms.

Lemma C.3 (Fast Rates for Σ-norm). For any δ ∈ (0, 1), we have with probability at least 1 − δ, for any x ∈ BW , we have

and

(cid:107)x(cid:107)

(cid:98)Σ ≤ 2(cid:107)x(cid:107)Σ + 5W

(cid:114)

d log(N/δ)
N

,

(cid:107)x(cid:107)Σ ≤ 2(cid:107)x(cid:107)

(cid:98)Σ + 5W

(cid:114)

d log(N/δ)
N

.

Proof. First, ﬁx any x ∈ BW . Since (x(cid:124)φ(s, a))2 ≤ W 2 almost surely, we have E (cid:2)(x(cid:124)φ(s, a))4(cid:3) ≤ W 2E (cid:2)(x(cid:124)φ(s, a))2(cid:3).
So by Lemma B.1, w.p. at least 1 − δ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

N
(cid:88)

i=1

(cid:12)
(cid:12)
(x(cid:124)φ(si, ai))2 − E (cid:2)(x(cid:124)φ(s, a))2(cid:3)
(cid:12)
(cid:12)
(cid:12)

≤

1
2

E (cid:2)(x(cid:124)φ(s, a))2(cid:3) +

2W 2 log(2/δ)
N

.

In particular, this means (cid:107)x(cid:107)2
(cid:98)Σ
W/

N
√
N -net of BW , which can be done with (1 + 2

2 (cid:107)x(cid:107)2

≤ 3

Σ + 2W 2 log(2/δ)

√

and (cid:107)x(cid:107)2

Σ ≤ 2(cid:107)x(cid:107)2
(cid:98)Σ

+ 4W 2 log(2/δ)
N

. Now union bound over a

N )d elements. The approximation error from this cover is

(cid:107)x(cid:107)

≤

(cid:98)Σ ≤ (cid:107)n(x)(cid:107)
(cid:114) 3
2
(cid:114) 3
2
(cid:114) 3
2

≤

≤

(cid:98)Σ + (cid:107)n(x) − x(cid:107)
(cid:98)Σ

(cid:115)

(cid:107)n(x)(cid:107)Σ +

2W 2 log(2(1 + 2

(cid:114) 3
2
(cid:114)

(cid:107)x(cid:107)Σ +

(cid:107)n(x) − x(cid:107)Σ +

(cid:107)x(cid:107)Σ + 4W

d log(N/δ)
N

,

√

N )d/δ)

+

W
√
N

N

(cid:115)

2W 2 log(2(1 + 2

√

N )d/δ)

N

+

W
√
N

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

where n(x) is the closest element in the net to x. Similarly,

(cid:107)x(cid:107)Σ ≤ (cid:107)n(x)(cid:107)Σ + (cid:107)n(x) − x(cid:107)Σ

≤ 2(cid:107)n(x)(cid:107)

(cid:98)Σ +

(cid:115)

4W 2 log(2(1 + 2

√

N )d/δ)

+

N
(cid:115)

W
√
N
√

≤ 2(cid:107)x(cid:107)

(cid:98)Σ + 2(cid:107)n(x) − x(cid:107)
(cid:114)

(cid:98)Σ + +

≤ 2(cid:107)x(cid:107)

(cid:98)Σ + 5W

d log(N/δ)
N

.

4W 2 log(2(1 + 2

N )d/δ)

+

W
√
N

N

Now deﬁne the following notation for analyzing Least Squares Policy Evaluation (LSPE). For every target vector (cid:107)ϑ(cid:107)2 ≤ W ,
deﬁne

yϑ = r + γϑ(cid:124)φ(s(cid:48), π),

θϑ = arg min
(cid:107)θ(cid:107)≤W

Es,a∼ν,s(cid:48)∼P (s,a)[(yϑ − θ(cid:124)φ(s, a))2] := (cid:96)(θ, ϑ),

i = ri + γϑ(cid:124)φ(s(cid:48)
yϑ

(cid:98)θϑ = arg min
(cid:107)θ(cid:107)≤W

1
N

i, π),
N
(cid:88)

i=1

(yϑ

i − θ(cid:124)φ(si, ai))2 := (cid:98)(cid:96)(θ, ϑ).

The following lemma are useful facts about the optimal θϑ and (cid:98)θϑ.
Lemma C.4. For any ϑ1, ϑ2, we have

For any θ, ϑ, we have

(cid:107)θϑ1 − θϑ2 (cid:107)Σ ≤ (cid:112)2γW (cid:107)ϑ1 − ϑ2(cid:107)2,
(cid:98)Σ ≤ (cid:112)2γW (cid:107)ϑ1 − ϑ2(cid:107)2.
(cid:107)(cid:98)θϑ1 − (cid:98)θϑ2 (cid:107)

(cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) ≥ (cid:107)θ − θϑ(cid:107)2
Σ.

recall

that θϑ minimizes f (θ) = E (cid:2)(yϑ − θ(cid:124)φ(s, a))2(cid:3), which has the Jacobian ∇f (θ) =
Proof. First,
2E (cid:2)(θ(cid:124)φ(s, a) − yϑ)φ(s, a)(cid:3). Since θϑ is optimal over BW , the necessary optimality condition is that −∇f (θϑ) ∈
ϑφ(s, a))(cid:3) , θ − θϑ(cid:105) ≤ 0. In
NBW (θϑ), the normal cone of BW at θϑ, i.e. for any θ ∈ BW , we have (cid:104)E (cid:2)φ(s, a)(yϑ − θ
particular, we have

(cid:124)

adding the two we get

(cid:104)E (cid:2)φ(s, a)(yϑ1 − θ
(cid:104)E (cid:2)φ(s, a)(yϑ2 − θ

(cid:124)
ϑ1
(cid:124)
ϑ2

φ(s, a))(cid:3) , θϑ2 − θϑ1(cid:105) ≤ 0
φ(s, a))(cid:3) , θϑ1 − θϑ2(cid:105) ≤ 0

(cid:107)θϑ1 − θϑ2 (cid:107)2

Σ = (cid:104)E [φ(s, a)(θϑ1 − θϑ2 )(cid:124)φ(s, a))] , θϑ1 − θϑ2(cid:105)

≤ (cid:104)E (cid:2)(yϑ1 − yϑ2)φ(s, a)(cid:3) , θϑ1 − θϑ2(cid:105)
= γ(cid:104)E [φ(s, a)φ(s(cid:48), π)(cid:124)] (ϑ1 − ϑ2), θϑ1 − θϑ2(cid:105)
≤ γ(cid:107)E [φ(s, a)φ(s(cid:48), π)(cid:124)] (cid:107)2(cid:107)ϑ1 − ϑ2(cid:107)2(cid:107)θϑ1 − θϑ2 (cid:107)2
≤ 2γW (cid:107)ϑ1 − ϑ2(cid:107)2.

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

The claim with (cid:98)θϑ1, (cid:98)θϑ2 follows by the same argument.

For the second claim, we ﬁrst apply the Parallelogram law, followed by the ﬁrst-order optimality of θϑ,
(cid:96)(θ, ϑ) = (cid:96)(θϑ, ϑ) + Eν((θϑ − θ)(cid:124)φ(s, a))2 + 2Eν[(yϑ − θ(cid:124)φ(s, a))φ(s, a)(cid:124)(θϑ − θ)]

≥ (cid:96)(θϑ, ϑ) + Eν((θϑ − θ)(cid:124)φ(s, a))2 + 0
= (cid:96)(θϑ, ϑ) + (cid:107)θ − θϑ(cid:107)2
Σ.

Now we show our key lemma about the concentration of least squares, uniformly over all targets generated by ϑ ∈ BW .
Lemma C.5 (Concentration for Least Squares). Let F = {(s, a) (cid:55)→ θ(cid:124)φ(s, a) : (cid:107)θ(cid:107)2 ≤ W }, with (cid:107)φ(s, a)(cid:107)2 ≤ 1. Then,
for any δ ∈ (0, 1) w.p. at least 1 − δ, we have

(cid:107)ˆθϑ − θϑ(cid:107)Σ < 120d(1 + W )

sup
ϑ∈BW

log(N )(cid:112)log(10/δ)
√
N

.

Proof. By Lemma C.3, we have that w.p. at least 1 − δ, we can bound the random (cid:107) · (cid:107)
simultaneously over all vectors in a ball,

(cid:98)Σ norm by (cid:107) · (cid:107)Σ, and vice versa,

E = {∀x ∈ B2W , (cid:107)x(cid:107)

(cid:98)Σ ≤ 2(cid:107)x(cid:107)Σ + t and (cid:107)x(cid:107)Σ ≤ 2(cid:107)x(cid:107)

(cid:98)Σ + t},

provided t ≥ 5W
probability and expectations will be implicitly conditioned on E.

(cid:113) d log(N/δ)
N

. For the remainder of this proof, we condition on this high-probability event. That is, any

First, we’ll show that for an arbitrary and ﬁxed ϑ ∈ BW , w.p. at least 1 − δ, we have

(cid:107)(cid:98)θϑ − θϑ(cid:107)Σ < t.

To do so, we will bound the probability of the complement, which in turn can be simpliﬁed by the following chain of
arguments,

(cid:107)ˆθϑ − θϑ(cid:107)Σ ≥ t

By (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) ≥ (cid:107)θ − θϑ(cid:107)Σ from Lemma C.4,

=⇒ (cid:96)(ˆθϑ, ϑ) − (cid:96)(θϑ, ϑ) ≥ t2
=⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) ≥ t2, ˆ(cid:96)(θ, ϑ) − ˆ(cid:96)(θϑ, ϑ) ≤ 0

By convexity and continuity of (cid:96)(·, ϑ), we can make this strict equality.
Indeed, given this, by Intermediate Value
Theorem, there exists λ ∈ [0, 1] such that θ(cid:48) = (1 − λ)θ + λθϑ has (cid:96)(θ(cid:48), ϑ) − (cid:96)(θϑ, ϑ) = ν. Then by convexity,
ˆ(cid:96)(θ(cid:48), ϑ) − ˆ(cid:96)(θϑ, ϑ) ≤ (1 − λ)ˆ(cid:96)(θ, ϑ) − (1 − λ)ˆ(cid:96)(θϑ, ϑ) ≤ 0.

=⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) = t2, ˆ(cid:96)(θ, ϑ) − ˆ(cid:96)(θϑ, ϑ) ≤ 0
=⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:107)θ − θϑ(cid:107)Σ ≤ t, ((cid:96)(θ, ϑ) − ˆ(cid:96)(θ, ϑ)) − ((cid:96)(θϑ, ϑ) − ˆ(cid:96)(θϑ, ϑ)) ≥ t2

By conditioning on E,

=⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:107)θ − θϑ(cid:107)

(cid:98)Σ ≤ 3t, ((cid:96)(θ, ϑ) − ˆ(cid:96)(θ, ϑ)) − ((cid:96)(θϑ, ϑ) − ˆ(cid:96)(θϑ, ϑ)) ≥ t2

Hence, we now focus on bounding

(cid:32)

P

sup
θ∈Θ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N
(cid:88)

i=1

(cid:12)
(cid:12)
ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)]
(cid:12)
(cid:12)
(cid:12)

(cid:33)

≥ N t2

≤ δ,

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

where we deﬁne

ζi(θ, ϑ) = (yϑ

i − θ(cid:124)φ(si, ai))2,
Θ = {θ : (cid:107)θ(cid:107) ≤ W, (cid:107)θ − θϑ(cid:107)

Since ζi(θ, ϑ) − ζi(θϑ, θ) = λi((θ − θϑ)(cid:124)φ(si, ai)) where λi(x) = x(2yϑ
and λi(0) = 0, the contraction lemma (Lemma B.3) tells us that it sufﬁces to consider a simpler class. Deﬁne

(cid:98)Σ ≤ 3t}.
i − (θ + θϑ)(cid:124)φ(si, ai)) is 2(1 + W )-Lipschitz

(8)

ωi(θ, ϑ) = (θ − θϑ)(cid:124)φ(si, ai)

and ω(θ, ϑ) = (ωi(θ, ϑ))N

i=1. By Chaining (Lemma B.6), we have
(cid:18) 1
J
(cid:40)

|σ · ω(θ, ϑ)|

Eσ

Ψ

(cid:20)

sup
Θ
√

(cid:90) b

where J = inf
α≥0

3α

N + 9

α

(cid:19)(cid:21)

≤ 1

(cid:112)log(D(δ/2, ω(Θ)))dδ

(cid:41)

,

D(δ, F) is the Euclidean packing number of F ⊂ RN , and b = supΘ (cid:107)ω(θ, ϑ)(cid:107)2 is the envelope.

ω(Θ) = {ω(θ, ϑ) : θ ∈ Θ} ,

Now we’ll bound the truncated entropy integral, J. First notice that b ≤ 3t
which localizes in (cid:107) · (cid:107)

(cid:98)Σ,

√

N , based on the deﬁnition of Θ (Equation (8)),

b
√
N

= sup
Θ

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
N

N
(cid:88)

i=1

(θ − θϑ)(cid:124)φ(si, ai)φ(si, ai)(cid:124)(θ − θϑ) = sup
Θ

(cid:107)θ − θϑ(cid:107)

(cid:98)Σ ≤ 3t.

Now, we bound the packing number D(·, ω(Θ)). Let θ1, θ2 ∈ BW be arbitrary,

(cid:107)ω(θ1, ϑ) − ω(θ2, ϑ)(cid:107)2
√
N

= (cid:107)θ1 − θ2(cid:107)

(cid:98)Σ ≤

(cid:113)

σmax((cid:98)Σ)(cid:107)θ1 − θ2(cid:107)2 ≤ (cid:107)θ1 − θ2(cid:107)2.

So for any ε, we can construct an ε-cover by setting (cid:107)θ1 − θ2(cid:107)2 ≤ ε/
N (ε, F) denote the Euclidean covering number of F ⊂ RN . Then,

√

N , which requires (W (1 + 2

√

N /ε))d points. Let

log D(ε, ω(Θ)) ≤ log N (ε/2, ω(Θ)) ≤ d log W (1 + 4

√

N /ε)

So,

√

N

(cid:90) 3t

(cid:112)log(D(ε/2, ω(Θ)))dε

0
(cid:90) 3t

√

N

(cid:113)

d log(W (1 + 8

√

N /ε))dε

J ≤

≤

0
√

≤ 3t

√

≤ 3t

dN ((cid:112)log W +

(cid:90) 1

(cid:112)log(1 + 3/(εt)))dε

dN ((cid:112)log W + (cid:112)log(4/t)),

0

since (cid:82) 1

0

(cid:112)log(1 + c/ε)dε ≤ (cid:112)log(1 + c) for any c > 0, and assuming t ≤ 1.

Now we put everything together. Let c denote a positive constant,

(cid:32)

P

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N
(cid:88)

i=1

(cid:12)
(cid:12)
ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)]
(cid:12)
(cid:12)
(cid:12)

sup
Θ

(cid:33)

≥ N t2

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Since Ψ is increasing,

(cid:32)

(cid:32)

= P

Ψ

c sup
Θ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N
(cid:88)

i=1

(cid:12)
(cid:12)
ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)]
(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:33)

≥ Ψ (cid:0)cN t2(cid:1)

By Markov’s inequality,
(cid:104)

E

Ψ

≤

(cid:16)

c supΘ

(cid:12)
(cid:12)
(cid:12)

(cid:80)N

(cid:12)
i=1 ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)]
(cid:12)
(cid:12)

(cid:17)(cid:105)

Ψ (cN t2)

By Symmetrization (Lemma B.2),

≤

E [Eσ [Ψ (2c supΘ |σ · (ζ(θ, ϑ) − ζ(θϑ, ϑ))|)]]
Ψ (cN t2)

By Contraction (Lemma B.3) and that ζi(θ, ϑ) − ζi(θϑ, ϑ) = λi(ωi(θ, ϑ)) where λi is L = 2(1 + W ) Lipschitz,

≤

3
2

·

E [Eσ [Ψ (4cL supΘ |σ · ω(θ, ϑ)|)]]
Ψ (cN t2)

Setting c = 1

4LJ and applying Chaining (Lemma B.6), the numerator is bounded by 1,

(cid:32)

≤ 8 exp

−

(cid:19)2(cid:33)

(cid:18) N t2
4LJ

Applying upper bound on J,


(cid:32)

≤ 8 exp

−

8(1 + W ) · 3t

√

N t2
√

dN (

log W + (cid:112)log(4/t))
(cid:19)

(cid:33)2


(cid:18)

≤ 8 exp

−

N t2
242(1 + W )2d(log W + log(4/t))

Now, we set t = 24(1 + W )

(cid:113) d log(N ) log(1/δ)
N

,

(cid:18)

≤ 8 exp

−

log(N ) log(1/δ)
log(N/d log(N ))

(cid:19)

Since log(N ) ≥ log(N/d log(N )),

≤ 8δ.

Hence, we have shown that for an arbitrary and ﬁxed ϑ ∈ BW , w.p. 1 − 9δ, we have

(cid:107)(cid:98)θϑ − θϑ(cid:107)Σ < t = 24(1 + W )

(cid:114)

d log(N ) log(1/δ)
N

.

We ﬁnally apply a union bound over ϑ ∈ BW . Consider a W/N -cover of ϑ ∈ BW , which requires (1 + 2N )d points. Then,
for any ϑ ∈ BW , we have

(cid:107)(cid:98)θϑ − θϑ(cid:107)Σ ≤ (cid:107)(cid:98)θϑ − (cid:98)θn(ϑ)(cid:107)Σ + (cid:107)(cid:98)θn(ϑ) − θn(ϑ)(cid:107)Σ + (cid:107)θn(ϑ) − θϑ(cid:107)Σ

(cid:98)Σ + t) + t + (cid:112)2γW (cid:107)n(ϑ) − ϑ(cid:107)

≤ (2(cid:107)(cid:98)θϑ − (cid:98)θn(ϑ)(cid:107)
≤ 2t + 3(cid:112)2γW (cid:107)n(ϑ) − ϑ(cid:107)
≤ 2t + 3(cid:112)2γW 2/N
≤ 5t.

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Thus, we have shown that w.p. 1 − δ,

sup
ϑ∈BW

(cid:107)(cid:98)θϑ − θϑ(cid:107)Σ < 120d(1 + W )

log(N )(cid:112)log(10/δ)
√
N

.

A nice corollary is that when Σ provides full coverage, i.e. Σ (cid:23) βI for some positive β, then we can bound

sup
ϑ∈BW

(cid:107)ˆθϑ − θϑ(cid:107)2 ≤ σmin(Σ)−1/2 sup
ϑ∈BW

(cid:107)(cid:98)θϑ − θϑ(cid:107)Σ ≤ β−1/2 · sup
ϑ∈BW

(cid:107)(cid:98)θϑ − θϑ(cid:107)Σ.

C.1. Main LSPE theorem

We now prove our LSPE sample complexity guarantee Theorem 3.3.

Theorem 3.3 (Sample Complexity of LSPE). Assume feature φ satisﬁes approximate Linear BC with parameter εν. For
any δ ∈ (0, 0.1), w.p. at least 1 − δ, we have for any initial state distribution p0

(cid:12)
(cid:12)V πe
(cid:12)

p0

(cid:104)

− Es∼p0

(cid:98)fK(s, πe)

(cid:105)(cid:12)
(cid:12)
(cid:12) ≤

γK/2
1 − γ

+

(cid:114)(cid:13)
(cid:13)
4
(cid:13)

ddπe
p0
dν

(cid:13)
(cid:13)
(cid:13)∞

(1 − γ)2

εν

+

480(cid:112)κ(p0)(1 + W )d log(N )(cid:112)log(10/δ)

√

(1 − γ)2

N

,

where (cid:98)fK is the output of Algorithm 1.

Proof of Theorem 3.3. By Lemmas C.1 and C.2,

(cid:12)
(cid:12)V π
(cid:12)

p0

(cid:104)

− Es∼p0

(cid:98)fk(s, π)

(cid:105)(cid:12)
(cid:12)
(cid:12) ≤

4

(1 − γ)2 max

k=1,2,...

(cid:107) ˆfk − T π ˆfk−1(cid:107)L2(dπ

p0

) +

γk/2
1 − γ

.

Next, we bound the maximum regression error. Consider any initial state distribution p0, then we have

max
k=1,2,...

(cid:107) ˆfk − T π ˆfk−1(cid:107)L2(dπ

p0

) ≤ sup
(cid:107)ϑ(cid:107)≤W

(cid:124)
(cid:107)ˆθ

ϑφ − T π(ϑ(cid:124)φ)(cid:107)L2(dπ

p0

)

≤ sup

(cid:107)ϑ(cid:107)≤W

(cid:124)
(cid:107)ˆθ
ϑφ − θ

(cid:124)
ϑφ(cid:107)L2(dπ
p0

(cid:107)θ

(cid:124)

ϑφ − T π(ϑ(cid:124)φ)(cid:107)L2(dπ

p0

)

≤ (cid:112)κ(p0)

sup
(cid:107)ϑ(cid:107)≤W

(cid:107)ˆθϑ − θϑ(cid:107)Σ +

sup
(cid:107)ϑ(cid:107)≤W

(cid:107)θ

(cid:124)

ϑφ − T π(ϑ(cid:124)φ)(cid:107)L2(ν)

) + sup
(cid:107)ϑ(cid:107)≤W
(cid:115)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:115)(cid:13)
(cid:13)
(cid:13)
(cid:13)

ddπ
p0
dν

ddπ
p0
dν

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

εν.

≤ (cid:112)κ(p0) sup

ϑ∈BW

(cid:107)ˆθϑ − θϑ(cid:107)Σ +

The quantity supϑ∈BW (cid:107)ˆθϑ − θϑ(cid:107)Σ can be directly bounded by Lemma C.5 w.p. at least 1 − δ. Thus, we have shown the
desired result: for any initial state distribution p0,

(cid:12)
(cid:12)V π
(cid:12)

p0

− Es,a∼p0◦π

(cid:104)

(cid:98)fk(s, a)

(cid:105)(cid:12)
(cid:12)
(cid:12) ≤

4
(1 − γ)2

(cid:32)
(cid:112)κ(p0)120d(1 + W )

log(N )(cid:112)log(10/δ)
√
N

+

(cid:33)

(cid:115)(cid:13)
(cid:13)
(cid:13)
(cid:13)

ddπ
p0
dν

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

εν +

γk/2
1 − γ

.

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

D. Proofs for Linear BC Equivalence

Proposition 4.2. Consider a feature φ with full rank covariance Σ(φ). Given any W > 0, the feature φ being linear BC
(under BW ) implies that there exist (ρ, M ) ∈ BW × Rd×d with (cid:107)M (cid:107)2 ≤

1 − (cid:107)ρ(cid:107)2

(cid:113)

W 2 , and

Eν

(cid:21)

(cid:13)
(cid:20)M
(cid:13)
(cid:13)
ρ(cid:124)
(cid:13)

φ(s, a) −

(cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)
r(s, a)

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

= 0.

(2)

On the other hand, if there exists (ρ, M ) ∈ BW × Rd×d with (cid:107)M (cid:107)2 < 1 such that the above equality holds, then φ must
satisfy exact linear BC with W ≥ (cid:107)ρ(cid:107)2

.

1−(cid:107)M (cid:107)2

Proof. (⇐=) Suppose that (cid:107)M (cid:107)2 < 1 and ρ ∈ BW satisfy,

Eν

(cid:21)

(cid:13)
(cid:20)M
(cid:13)
(cid:13)
ρ(cid:124)
(cid:13)

φ(s, a) −

(cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)
r(s, a)

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

= 0.

Then, for any w1 ∈ BW , setting w2 = ρ + M (cid:124)w1 satisﬁes,

2 φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w

(cid:124)

1 φ(s(cid:48), πe)](cid:1)2

(cid:107)w

(cid:124)
2 φ − T πe (w

(cid:124)
1 φ)(cid:107)2

ν = Eν
= Eν
= Eν
= 0.

(cid:124)

(cid:0)w
(cid:0)ρ(cid:124)φ(s, a) − r(s, a) + w
(cid:0)w

(cid:124)
1

(cid:0)M φ(s, a) − γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)](cid:1)(cid:1)2

(cid:124)

1 M φ(s, a) − γEs(cid:48)∼P (s,a) [w

(cid:124)

1 φ(s(cid:48), πe)](cid:1)2

Also, we have (cid:107)w2(cid:107)2 ≤ (cid:107)ρ(cid:107)2 + (cid:107)M (cid:107)2(cid:107)w1(cid:107)2 ≤ W since W ≥ (cid:107)ρ(cid:107)2

1−(cid:107)M (cid:107)2

. Thus, φ satisﬁes exact Linear BC.

(=⇒) Suppose φ satisﬁes exact Linear BC, that is

max
w1∈BW

min
w2∈BW

(cid:107)w

(cid:124)
2 φ − T π(w

(cid:124)
1 φ)(cid:107)2

ν = 0.

To see that there exists ρ ∈ BW that linearizes the reward w.r.t φ under ν, set w1 = 0, and we have:

min
w2∈BW

Es,a∼ν

(cid:13)
(cid:13)w(cid:62)

2 φ(s, a) − r(s, a)(cid:13)
2
2 = 0.
(cid:13)

Let ρ to be the minimizer of the above objective.

Now we need to show that there exists a M ∈ Rd×d with (cid:107)M (cid:107)2 <

(cid:113)

1 − (cid:107)ρ(cid:107)2

W 2 that satisﬁes

Es,a∼ν

(cid:13)
(cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13)
2
2 = 0.
(cid:13)

To extract the i-th row of M , plug in wi = W ei (note that wi ∈ BW ). By exact Linear BC, we know that there exists a
vector vi ∈ BW , such that:

(cid:13)
(cid:13)v

(cid:124)

i φ(s, a) − ρ(cid:124)φ(s, a) − γW Es(cid:48)∼P (s,a)e

Repeating this for every i ∈ [d], we can construct M as follows,

M =

1
W



(v1 − ρ)(cid:124)
...


(vd − ρ)(cid:124)




 ,

(cid:124)

i φ(s(cid:48), πe)(cid:13)

(cid:13)ν = 0.

which satisﬁes,

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Es,a∼ν
d
(cid:88)

=

i=1

d
(cid:88)

i=1

=

(cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13)
(cid:13)
2
(cid:13)
2

Es,a∼ν

(cid:13)
(cid:13)e

(cid:124)
i

(cid:0)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:1)(cid:13)
2
(cid:13)
2

Es,a∼ν

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
W

(vi − ρ)(cid:124)φ(s, a) − γEs(cid:48)∼P (s,a)e

(cid:13)
2
(cid:13)
(cid:124)
i φ(s(cid:48), πe)
(cid:13)
(cid:13)
2

= 0.

Hence, we have (cid:13)

(cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13)
2
ν = 0.
(cid:13)

(cid:113)

1 − (cid:107)ρ(cid:107)2

W 2 . First we show that (cid:107)M (cid:107)2 ≤ 1. For any w1 ∈ BW , by exact linear
Finally, we must show that (cid:107)M (cid:107)2 <
BC, there exists w2 ∈ BW s.t. (cid:13)
1 φ(s(cid:48), πe)](cid:13)
2
ν = 0, and by the construction of M ,
(cid:13)w
(cid:13)
φ(s, a)(cid:107)2
satisﬁes (cid:107)(w2 − ρ − M (cid:124)w1)
Σ(φ) = 0. Since Σ(φ) is positive deﬁnite, we have that
w2 = ρ + M (cid:124)w1 is the unique choice of w2, which by exact linear BC is in BW . Hence, we’ve shown that for any
w1 ∈ BW , we also have that ρ + M (cid:124)w1 ∈ BW . Now take w1 and −w1, subtracting the two expressions yields that
2M (cid:63)(cid:124)w1 ∈ B2W . Since this is true for arbitrary w1, taking supremum over w1 ∈ BW shows that (cid:107)M (cid:107)2 ≤ 1.

2 φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w
ν = (cid:107)w2 − ρ − M (cid:124)w1(cid:107)2

(cid:124)

(cid:124)

(cid:124)

(cid:124)
Now we show that the inequality must be strict. Consider the singular value decomposition: M = (cid:80)d
i where
{ui}i∈[d] and {vi}i∈[d] are each an orthonormal basis of Rd, and σi is the i-th largest singular value. Without loss of
generality, suppose ρ(cid:124)u1 ≥ 0, since we can always ﬂip the sign of u1. If we pick x = W v1 ∈ BW , we have M x = W σ1u1.
By the argument in the previous paragraph, since x ∈ BW , we have ρ + M x ∈ BW , implying that

i=1 σiuiv

W 2 ≥ (cid:107)M x + ρ(cid:107)2

2 = (cid:107)W σ1u1 + (ρ(cid:124)u1)u1 + (ρ − (ρ(cid:124)u1)u1)(cid:107)2

2

Since u1 and (ρ − (ρ(cid:124)u1)u1) are orthogonal, by Pythagoras, we have

= |W σ1 + ρ(cid:124)u1|2 + (cid:107)(ρ − (ρ(cid:124)u1)u1)(cid:107)2
2
= (W σ1)2 + 2W σ1ρ(cid:124)u1 + (ρ(cid:124)u1)2 + (cid:107)(ρ − (ρ(cid:124)u1)u1)(cid:107)2
2
= (W σ1)2 + 2W σ1ρ(cid:124)u1 + (cid:107)(ρ(cid:124)u1)u1(cid:107)2

2 + (cid:107)(ρ − (ρ(cid:124)u1)u1)(cid:107)2

2

By Pythagoras,

= (W σ1)2 + 2W σ1ρ(cid:124)u1 + (cid:107)ρ(cid:107)2
2 .

Hence, we get the following inequality:

Solve for σ1 and using the fact that ρ(cid:124)u1 ≥ 0, we have that,

W 2σ2

1 + 2W (ρ(cid:124)u1)σ1 + (cid:107)ρ2(cid:107)2 − W 2 ≤ 0.

σ1 ≤

−ρ(cid:124)u1 + (cid:112)(ρ(cid:124)u1)2 + (W 2 − (cid:107)ρ(cid:107)2)
W

(cid:112)W 2 − (cid:107)ρ(cid:107)2
W

≤

(cid:114)

≤

1 −

(cid:107)ρ(cid:107)2
W 2 .

We ﬁnally show that (cid:107)ρ(cid:107)2 < W unless M = 0. We prove this by contradiction. Assume (cid:107)ρ(cid:107)2 ≥ W . Following the above
argument, take any w1 ∈ BW , we must have w2 := ρ + M (cid:124)w1 ∈ BW . We discuss two cases.

First if ρ (cid:54)∈ range(M (cid:124)). In this case, we must have (cid:107)w2(cid:107)2
has non-zero entries. Thus, this case leads to contradiction .

2 = (cid:107)ρ(cid:107)2

2 + (cid:107)M (cid:124)w1(cid:107)2

2 = W 2 + (cid:107)M (cid:124)w1(cid:107)2

2 > W 2, as long as M

Second, if ρ ∈ range(M (cid:124)). In this case, there must exist a vector x (cid:54)= 0 such that M (cid:124)x = ρ. Consider ¯x := W x
(cid:107)x(cid:107)2
We have w2 := ρ + M (cid:124) ¯x = ρ

, which means that (cid:107)w2(cid:107)2 > W , which causes contradiction again.

(cid:17)

(cid:16)

1 + W
(cid:107)x(cid:107)2

∈ BW .

So unless M = 0, which only happens when γ = 0 (i.e. horizon is 1), we have (cid:107)ρ(cid:107)2 < W .

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

(cid:21)

Eν

(ρ,M )∈Θ

Llbc(φ) = min

We now show an approximate version to the equivalence of Proposition 4.2. First, recall the bilevel loss from Equation (3).
We use Llbc and (cid:98)Llbc to denote the population and empirical versions as follows,
(cid:13)
(cid:20)M
(cid:20)γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)]
(cid:13)
(cid:13)
ρ(cid:124)
r(s, a)
(cid:13)
(cid:21)(cid:13)
(cid:13)
(cid:20)M
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
ρ(cid:124)
(cid:13)
(cid:13)
2
where Θ = (cid:8)(ρ, M ) ∈ BW × Rd×d : (cid:107)ρ(cid:107) ≤ (cid:107)ρ(cid:63)(cid:107), (cid:107)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2
(cid:9) and (ρ(cid:63), M (cid:63)) are corresponding to the linear BC φ(cid:63).
Lemma D.1. Suppose a feature (cid:98)φ satisﬁes Llbc( (cid:98)φ) ≤ ε2. Then, (cid:98)φ is ε(1 + W )-approximately Linear BC, provided
W ≥ (cid:107)ρ(cid:63)(cid:107)2

ED (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2
2 ,

(cid:20)γφ(s(cid:48), πe)
r(s, a)

(cid:98)Llbc(φ) = min

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

− min
g∈G

φ(s, a) −

φ(s, a) −

(ρ,M )∈Θ

(10)

ED

(9)

(cid:21)

.

1−(cid:107)M (cid:63)(cid:107)2

Proof. Suppose Llbc( (cid:98)φ) ≤ ε2, so there exists (cid:99)M (s.t. (cid:107) (cid:99)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2 < 1) and (cid:98)ρ ∈ BW s.t.
(cid:20)
(cid:21)
(cid:99)M
(cid:98)ρ(cid:124)

(cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)
r(s, a)

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

φ(s, a) −

Es,a∼ν

≤ ε2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

For any w1 ∈ BW , we can take w2 = (cid:98)ρ + (cid:99)M (cid:124)w1. Then, (cid:107)w2(cid:107)2 ≤ (cid:107)(cid:98)ρ(cid:107)2 + (cid:107) (cid:99)M (cid:107)2W ≤ (cid:107)ρ(cid:63)(cid:107)2 + (cid:107)M (cid:63)(cid:107)2W ≤ W by our
assumption on W . Hence,

max
w1∈BW

≤ max
w1∈BW

(cid:124)

(cid:13)
(cid:13)w

2 φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w
min
w2∈BW
(cid:13)
(cid:13)((cid:98)ρ + (cid:99)M (cid:124)w1)(cid:124)φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w
(cid:13)
(cid:115)

(cid:124)

1 φ(s(cid:48), πe)](cid:13)
(cid:13)ν

(cid:13)
(cid:124)
1 φ(s(cid:48), πe)]
(cid:13)
(cid:13)ν

((cid:98)ρ + (cid:99)M (cid:124)w1)(cid:124)φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w
(cid:115)

(cid:20)(cid:16)

(cid:124)

((cid:98)ρ(cid:124)φ(s, a) − r(s, a))2(cid:105)

+

Es,a∼ν

w

1 ( (cid:99)M φ(s, a) − γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)])

(cid:17)2(cid:21)

(cid:17)2(cid:21)

(cid:124)
1 φ(s(cid:48), πe)]

= max
w1∈BW

Es,a∼ν

(cid:20)(cid:16)

(cid:114)

Es,a∼ν

(cid:104)

≤ max
w1∈BW

≤ ε(1 + W ),

as desired.

E. Proofs for Representation Learning

To simplify analysis, assume that the functions in G have bounded (cid:96)2 norm, i.e. ∀g ∈ G, s, a ∈ S × A, (cid:107)g(s, a)(cid:107)2 ≤ γ.
This is reasonable, and can always be achieved by clipping without loss of accuracy, since the target for g(s, a) is
γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)] and (cid:107)φ(s, a)(cid:107)2 ≤ 1 for any s, a ∈ S × A.

E.1. Lemmas

Recall that λk(A) denotes the k-th largest eigenvalue of a matrix A, i.e. λ1(A), λn(A) give the largest and smallest
eigenvalues respectively.
Lemma E.1 (Weyl’s Perturbation Theorem). Let A, B ∈ Cn×n be Hermitian matrices. Then

max
k

|λk(A) − λk(B)| ≤ (cid:107)A − B(cid:107)2 .

Proof. Please see (Bhatia, 2013, Corollary III.2.6).

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

We extend this to be uniform over all Φ.

Lemma E.2 (Uniform spectrum concentration). For any δ ∈ (0, 1), w.p. 1 − δ,

sup
φ∈Φ,k∈[d]

(cid:12)
(cid:12)
(cid:12)λk(Σ(φ)) − λk((cid:98)Σ(φ))

(cid:12)
(cid:12)
(cid:12) ≤

1
√
N

(cid:16)

(cid:17)
96κ(Φ) + 4d + 4 log1/2(1/δ)

Proof. First observe that,

sup
φ∈Φ

sup
k

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ sup
(cid:12)λk(Σ(φ)) − λk((cid:98)Σ(φ))

φ∈Φ

(cid:13)
(cid:13)
(cid:13)Σ(φ) − (cid:98)Σ(φ)

(cid:13)
(cid:13)
(cid:13)2
x(cid:124)(Σ(φ) − (cid:98)Σ(φ))x

(Eν − ED)(x(cid:124)φ(s, a))2

=

=

sup
φ∈Φ,(cid:107)x(cid:107)2≤1
sup
φ∈Φ,(cid:107)x(cid:107)2≤1

Now we want to bound the Rademacher complexity of the class

F = (cid:8)(s, a) (cid:55)→ (x(cid:124)φ(s, a))2, φ ∈ Φ, (cid:107)x(cid:107)2 ≤ 1(cid:9)

First, to bound the envelope, we have (x(cid:124)φ(s, a))2 ≤ 1. To cover, consider any φ ∈ Φ and x ∈ Rd s.t. (cid:107)x(cid:107)2 ≤ 1. Pick (cid:101)φ, (cid:101)x
close to φ, x, so that

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
N

N
(cid:88)

(cid:16)

i=1

(x(cid:124)φ(si, ai))2 − ((cid:101)x(cid:124) (cid:101)φ(si, ai))2

(cid:17)2

(cid:118)
(cid:117)
(cid:117)
(cid:116)

≤ 2

1
N

N
(cid:88)

(cid:16)

i=1

(x(cid:124)φ(si, ai) − (cid:101)x(cid:124) (cid:101)φ(si, ai))

(cid:17)2



(cid:118)
(cid:117)
(cid:117)
(cid:116)

≤ 2



1
N

N
(cid:88)

(cid:16)

i=1

x(cid:124)(φ(si, ai) − (cid:101)φ(si, ai))

(cid:17)2

+

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
N

N
(cid:88)

(cid:16)

i=1

(x − (cid:101)x)(cid:124) (cid:101)φ(si, ai)

(cid:17)2





(cid:16)

≤ 2

dΦ(φ, (cid:101)φ) + (cid:107)x − (cid:101)x(cid:107)2

(cid:17)

So it sufﬁces to take dΦ(φ, (cid:101)φ), (cid:107)x − (cid:101)x(cid:107)2 ≤ t/4 to t-cover F in L2(D). Note the t/4-covering number for x in the unit ball
is (1 + 8/t)d. Thus, by Dudley’s entropy bound ((5.48) of (Wainwright, 2019)),

RN (F) ≤

≤

24
√
N
96
√
N

(cid:90) 1

log1/2(N (t/4, Φ) · (1 + 8/t)d)dt

0
(cid:16)

κ(Φ) + 4

√

(cid:17)

d

Thus, by Theorem 4.10 of (Wainwright, 2019), w.p. 1 − δ,

sup
φ∈Φ,(cid:107)x(cid:107)2≤1

(Eν − ED)(x(cid:124)φ(s, a))2 ≤ 2RN (F) +

4 log1/2(1/δ)
√
N
√

≤

(cid:16)

1
√
N

96κ(Φ) + 4

d + 4 log1/2(1/δ)

(cid:17)

We now prove the double sampling lemma, i.e. modiﬁed Bellman Residual Minimization (Chen & Jiang, 2019). This will
help deal with the double sampling issue when transitions are stochastic. Recall that G is a function class of functions
g : X (cid:55)→ Rd. Let ν be a distribution over X ⊂ Rd and, for any x ∈ X , let P (x) be a distribution over Y ⊂ Rd.

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Lemma E.3 (Double Sampling). Suppose x (cid:55)→ Ey∼P (x) [y] ∈ G. Then,

Ex∼ν

(cid:104)(cid:13)
(cid:13)x − Ey∼P (x) [y](cid:13)
2
(cid:13)
2

(cid:105)

= Ex∼ν,y∼P (x)

(cid:104)

(cid:107)x − y(cid:107)2
2

(cid:105)

− inf
g∈G

Ex∼ν,y∼P (x)

(cid:104)

(cid:107)g(x) − y(cid:107)2
2

(cid:105)

Proof.

Ex∼ν,y∼P (x)

(cid:104)

(cid:105)

− Ex∼ν

(cid:107)x − y(cid:107)2
2
(cid:104)

(cid:105)

(cid:104)(cid:13)
(cid:13)x − Ey∼P (x) [y](cid:13)
2
(cid:13)
2
(cid:105)
2 − 2 (cid:10)x, Ey∼P (x) [y](cid:11) + (cid:13)
(cid:107)x(cid:107)2

(cid:16)

2

(cid:13)Ey∼P (x) [y](cid:13)
2
(cid:13)
2

(cid:17)(cid:105)

(cid:104)

(cid:104)

(cid:104)

= Ex∼ν

= Ex∼ν

= Ex∼ν

Ey∼P (x)

Ey∼P (x)

Ey∼P (x)

(cid:104)

(cid:107)x(cid:107)2

2 − 2 (cid:104)x, y(cid:105) + (cid:107)y(cid:107)2
(cid:105)
(cid:13)Ey∼P (x) [y](cid:13)
2
(cid:13)
2
(cid:105)(cid:105)

− (cid:13)
(cid:107)y(cid:107)2
2
(cid:104)(cid:13)
(cid:13)y − Ey∼P (x) [y](cid:13)
2
(cid:13)
2

−
(cid:105)

where the last step uses the fact that (cid:13)
observe that, assuming g(cid:63)(x) (cid:55)→ Ey∼P (x) [y] ∈ G, we have that it is the minimizer of,

(cid:13)Ey∼P (x) [y](cid:13)
2
2 = Ey∼P (x)
(cid:13)

(cid:2)(cid:10)y, Ey∼P (x) [y](cid:11)(cid:3), and completing the square. Now,

g(cid:63) ∈ arg min

g∈G

Ex∼ν,y∼P (x)

(cid:104)

(cid:107)g(x) − y(cid:107)2
2

(cid:105)

(11)

which completes the proof.

E.2. Concentration lemmas

For any φ, deﬁne the optimal ρ, M, g for our losses as follows:

ρφ ∈ arg min
(ρ,_)∈Θ

Eν

(cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3)

Mφ ∈ arg min
(_,M )∈Θ

Eν◦P

(cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), πe)(cid:107)2

2

(cid:3)

gφ ∈ arg min

g∈G

Eν◦P

(cid:2)(cid:107)g(s, a) − γφ(s(cid:48), π)(cid:107)2

2

(cid:3)

Similarly, deﬁne (cid:98)ρφ, (cid:99)Mφ, (cid:98)gφ to be minimizers of the above losses when expectation is taken over the empirical distribution
D, instead of the population distribution ν. Observe that the unconstrained minimization yields a closed form solution for
gφ as gφ(s, a) = γEs(cid:48)∼P (s,a) [φ(s(cid:48), π)] – Assumption 4.3 posits that G is rich enough to capture this.

The key property of our squared losses is that the second moment can be upper bounded by the expectation, which allows us
to invoke the second part of the above Lemma B.1. We now combine this with covering to get uniform convergence results.
Lemma E.4. For any δ ∈ (0, 1), w.p. at least 1 − δ, for any φ ∈ Φ, ρ ∈ BW and M ∈ Rd×d with (cid:107)M (cid:107)2 ≤ 1, we have

(cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3) − Eν
(cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2

(cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3)(cid:12)

2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2
2

1
2

Eν
(cid:12) ≤
(cid:3) − Eν◦P

(cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3) + ερ,
(cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2

2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2
2

(cid:3)(cid:12)
(cid:12)

Eν◦P

(cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2

2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2
2

(cid:3) + εM ,

(cid:12)
(cid:12)ED
(cid:12)
(cid:12)ED
1
2

≤

and, assuming realizability (Assumption 4.3), for every g ∈ G, we have

(cid:12)
(cid:12)ED
1
2

≤

(cid:2)(cid:107)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2

2 − (cid:107)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2
2

(cid:3) − Eν

(cid:2)(cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2

2

(cid:3)(cid:12)
(cid:12)

Eν

(cid:2)(cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2

2

(cid:3) + εg,

where ερ, εM , ρg are deﬁned below.

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Finite function classes Assuming Φ and G are ﬁnite, we have

ερ ≤

6d(1 + W )2 log(4W |Φ| N/δ)
N

εM ≤

32d2 log(4 |Φ| N/δ)
N

εg ≤

20γ2 log(2 |G| /δ)
N

.

Proof for ερ. For a ﬁxed φ ∈ Φ, ρ ∈ BW , apply Lemma B.1 to Xi = (ρ(cid:124)φ(si, ai) − r(si, ai))2. The envelope is
(cid:3) ≤ (1 + W )2E [Xi]. So, the error from the lemma is
|Xi| ≤ (1 + W )2 and the second moment is bounded E (cid:2)X 2
(cid:12)(ρ(cid:124)φ(s, a) − r(s, a))2 − ((cid:101)ρ(cid:124)φ(s, a) − r(s, a))2(cid:12)
2(1+W )2 log(2/δ)
(cid:12) =
N
|(ρ − (cid:101)ρ)(cid:124)φ(s, a)((ρ + (cid:101)ρ)(cid:124)φ(s, a) + 2r(s, a))| ≤ 2(1 + W )(cid:107)(cid:101)ρ − ρ(cid:107)2, we consider a 1
N -net of BW which requires
(1 + 2W N )d points. The error from this ε-net approximation is at most 4(1+W )

. Now union bound over an ε-net of BW . Since (cid:12)

. Finally, union bound over Φ.

i

N

Proof for εM . For a ﬁxed φ ∈ Φ and M ∈ Rd×d s.t. (cid:107)M (cid:107)2 < 1, apply Lemma B.1 to Xi = (cid:107)a(cid:107)2
a = M φ(si, ai)−γφ(s(cid:48)
Further, observe that (cid:107)a(cid:107)2
E (cid:2)(cid:107)a + b(cid:107)2

2 − (cid:107)b(cid:107)2
2 where
2 ≤ (1+γ)2+(2γ)2 ≤ 8.
(cid:3) ≤
(cid:3) ≤ 16E [Xi], where we used Lemma E.3 to give us

2 = (cid:104)a + b, a − b(cid:105) ≤ (cid:107)a + b(cid:107)2(cid:107)a − b(cid:107)2. So, the second moment is bounded E (cid:2)X 2

(cid:3) ≤ (1 + 3γ)2E (cid:2)(cid:107)M φ(si, ai) − γgφ(si, ai)(cid:107)2

i, π). The envelope is |Xi| = |Xi| ≤ (cid:107)a(cid:107)2

i, π), b = gφ(si, ai)−γφ(s(cid:48)

2 − (cid:107)b(cid:107)2

2+(cid:107)b(cid:107)2

2(cid:107)a − b(cid:107)2
2

2

i

E (cid:2)(cid:107)M φ(si, ai) − γgφ(si, ai)(cid:107)2

2

(cid:3) = E (cid:2)(cid:107)M φ(si, ai) − γφ(s(cid:48)

i, π)(cid:107)2

2 − (cid:107)gφ(si, ai) − γφ(s(cid:48)

i, π)(cid:107)2
2

(cid:3) .

So, the error from the lemma is 24 log(2/δ)

N

. Now union bound over an ε-net of (cid:8)M ∈ Rd×d : (cid:107)M (cid:107)2 ≤ 1(cid:9). Observe that

(cid:12)
(cid:0)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2
2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2
(cid:12)
(cid:12)
2
(cid:13)
(cid:13)M φ(si, ai) + (cid:102)M φ(si, ai) − 2γφ(s(cid:48)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:13)
(cid:13)
(cid:13)(M − (cid:102)M )φ(si, ai)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(M − (cid:102)M )
(cid:13)2

(cid:13)
(cid:13)
(cid:13)(M − (cid:102)M )

· (2 + 2γ) ≤ 4

(cid:13)
(cid:13)
(cid:13)2

(cid:1) −

≤

≤

(cid:16)

.

(cid:13)
(cid:13)
i, π)
(cid:13)2

(cid:107) (cid:102)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2

2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2
2

(cid:17)(cid:12)
(cid:12)
(cid:12)

N -net (under (cid:107)(cid:107)F ) for {M ∈ Rd×d : (cid:107)M (cid:107)F ≤

Consider a 1
like the (cid:96)2 for a d2-dimensional vector. This is a 1
(cid:107)M (cid:107)2 ≤ (cid:107)M (cid:107)F ≤

√

d(cid:107)M (cid:107)2. The error from this ε-net approximation is at most 8

points since it is
N -net (under (cid:107)(cid:107)2) for the subset (cid:8)M ∈ Rd×d : (cid:107)M (cid:107)2 ≤ 1(cid:9) since

d}, which requires (1 + 2N

N . Finally, union bound over Φ.

d)d2

√

√

Proof for εg. For a ﬁxed g ∈ G, apply Lemma B.1 to Xi = (cid:107)g(si, ai) − γφ(cid:63)(s(cid:48)
excess regression loss. Under realizability Assumption 4.3, we have E [Xi] = (cid:107)g − gφ(cid:63) (cid:107)2

i, π)(cid:107)2

2, since

2 − (cid:107)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2

2, the

E [Xi] = E

= E

(cid:104)
(cid:107)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2
(cid:104)
(cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2

2 − (cid:107)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2

2

(cid:105)

2 + 2(cid:104)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π), g(s, a) − gφ(cid:63) (s, a)(cid:105)

(cid:105)

By deﬁnition of gφ(cid:63) , we have Es(cid:48)∼P (s,a) [gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)] = 0, so,

= E

(cid:104)
(cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2
2

(cid:105)

is

envelope

|Xi|
The
E (cid:2)(cid:107)g(s, a) + gφ(cid:63) (s, a) − 2γEa(cid:48)∼π(s(cid:48)) [φ(cid:63)(s(cid:48), a(cid:48))] (cid:107)2
lemma is 20γ2 log(2/δ)

. Now union bound over G.

≤

(2γ)2

N

the

and
2(cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2
2

second moment

≤
bounded
(cid:3) ≤ (4γ)2E [Xi]. So the error term from the

is

i

E (cid:2)X 2

(cid:3)

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Inﬁnite function classes When Φ and G are inﬁnite, we need to assume some metric entropy conditions. Then in the ﬁnal
step of the ﬁnite-class proofs above, we union bound on a well-chosen ε-net and collect an additional approximation error
which is on the order of O(1/N ).
Assumption E.5. For F ∈ {Φ, G}, we assume there exists p ∈ R++ such that N (t, F) (cid:46) t−p, where the net is under the
following distances,

dΦ(φ, (cid:101)φ) =∆ ED

+ ED

(cid:105)

+ Eν
(cid:105)

(cid:13)
(cid:104)(cid:13)
(cid:13)
(cid:13)
(cid:13)φ(s, a) − (cid:101)φ(s, a)
(cid:13)2
(cid:13)
(cid:104)(cid:13)
+ Es,a∼D,s(cid:48)∼P (s,a)
(cid:13)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π)
(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:104)(cid:13)
(cid:13)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π)
(cid:13)
(cid:13)
(cid:13)2
(cid:114)

(cid:105)

(cid:105)

(cid:104)(cid:13)
(cid:13)
(cid:13)φ(s, a) − (cid:101)φ(s, a)
(cid:104)(cid:13)
(cid:13)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π)
(cid:13)

(cid:13)
(cid:13)
(cid:13)2

(cid:107)g(si, ai) − (cid:101)g(si, ai)(cid:107)2

2

(cid:105)

+

(cid:104)

Eν

(cid:107)g(s, a) − (cid:101)g(s, a)(cid:107)2

2

(cid:105)

(cid:105)

(cid:13)
(cid:13)
(cid:13)2

+ Es,a∼ν,s(cid:48)∼P (s,a)

(cid:114)

dG(g, (cid:101)g) =∆

(cid:104)

ED

Note that this assumption is automatically satisﬁed for any p > 0 by VC classes (van der Vaart & Wellner, 1996, Theorem
2.6.4). Under this assumption, we have

N

ερ (cid:46) d(1 + W )2(1 + p) log(W N/δ)
εM (cid:46) d2(1 + p) log(N/δ)
N
εg (cid:46) γ2p log(N/δ)

.

N

Proof for ερ. From before, we showed for a ﬁxed φ ∈ Φ, we have ερ (cid:46) d(1+W )2 log(W N/δ)
. Observe that
(cid:12)
(cid:12)
(cid:101)φ(s, a) − r(s, a))2(cid:12)
(cid:12)
(cid:12)ρ(cid:124)(φ(s, a) − (cid:101)φ(s, a))(ρ(cid:124)(φ(s, a) + (cid:101)φ(s, a)) + 2r(s, a))
(cid:12)(ρ(cid:124)φ(s, a) − r(s, a))2 − (ρ(cid:124)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ 2W (1+
(cid:12) =
W )(cid:107)φ(s, a) − (cid:101)φ(s, a)(cid:107)2, and so the difference of the loss with φ and the loss with (cid:101)φ is bounded by 2W (1 + W )dΦ(φ, (cid:101)φ).
Now union bound over a 1
N -net, which requires O(N p) points by Assumption E.5. The approximation error using the net is
2W (1+W )
N

N

.

Proof for εM . From before, we showed for a ﬁxed φ ∈ Φ, we have εM (cid:46) d2 log(N/δ)

N

. Observe that

(cid:12)
(cid:0)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2
(cid:12)
(cid:12)
≤ (cid:107)M (φ(s, a) − (cid:101)φ(s, a)) − γ(φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π))(cid:107)(cid:107)M (φ(s, a) + (cid:101)φ(s, a)) − γ(φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π))(cid:107)
+ (cid:107)gφ(s, a) − g

2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2
2

(cid:107)M (cid:101)φ(s, a) − γ (cid:101)φ(s(cid:48), π)(cid:107)2

(cid:101)φ(s, a) − γ(φ(s(cid:48), π) + (cid:101)φ(s(cid:48), π))(cid:107)

2 − (cid:107)g

(cid:1) −

(cid:16)

(cid:101)φ(s, a) − γ (cid:101)φ(s(cid:48), π)(cid:107)2

2

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:17)
(cid:107)φ(s, a) − (cid:101)φ(s, a)(cid:107) + γ(cid:107)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π)(cid:107)

(cid:101)φ(s, a) − γ(φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π))(cid:107)(cid:107)gφ(s, a) + g
· (2 + 2γ)
(cid:17)
(cid:101)φ(s, a)(cid:107) + γ(cid:107)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π)(cid:107)

(cid:107)gφ(s, a) − g

· (2 + 2γ)

(cid:16)

(cid:16)

≤

+

Using closed form solution for gφ,

≤ 16dΦ(φ, (cid:101)φ).

Now union bound over a 1
at most 16
N .

N -net, which requires O(N p) points by Assumption E.5. The approximation error using the net is

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Proof for εg. From before, we showed for a ﬁxed g ∈ G, we have εg (cid:46) γ2 log(1/δ)

N

. Observe that

(cid:12)(cid:107)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 − (cid:107)(cid:101)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2(cid:12)
(cid:12)
(cid:12)
≤ (cid:107)g(s, a) − (cid:101)g(s, a)(cid:107)(cid:107)g(s, a) + (cid:101)g(s, a) − 2γφ(cid:63)(s(cid:48), π)(cid:107)
≤ (2 + 2γ)dG(g, (cid:101)g).

Now union bound over a 1
at most 4
N .

N -net, which requires O(N p) points by Assumption E.5. The approximation error using the net is

E.3. Main Results

Lemma E.6. Suppose φ(cid:63) ∈ Φ is Linear BC. Suppose Assumption 4.3 if transitions are stochastic. Moreover, suppose φ(cid:63) is
feasible in the bilevel optimization (and so (cid:98)Llbc( (cid:98)φ) ≤ (cid:98)Llbc(φ(cid:63))). Then, w.p. 1 − 5δ,

Llbc( (cid:98)φ) ≤

24d(1 + W )2 log(4W |Φ|N/δ) + 128d2 log(4|Φ|N/δ) + 40γ2 log(2|G|/δ)
N

Proof.

Llbc( (cid:98)φ)
(cid:104)
= Eν

Since ρ

≤ Eν

+ Eν◦P

(cid:124)

(ρ

(cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105)
(cid:98)φ are minimizers under ν,
(cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105)
((cid:98)ρ

(cid:98)φ, M
(cid:104)

(cid:124)

+ Eν◦P

(cid:104)
(cid:107)M

(cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2

2

(cid:104)
(cid:107) (cid:99)M

(cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2

2

(cid:105)

(cid:105)

− Eν◦P

− Eν◦P

By the ρ, M parts of Lemma E.4,

≤ 2ED

(cid:104)

(cid:124)

(cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105)
((cid:98)ρ

+ 2ED

Since (cid:98)g

(cid:98)φ minimizes under D,

≤ 2ED

(cid:104)

(cid:124)

(cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105)
((cid:98)ρ

+ 2ED

By optimality of (cid:98)φ under (cid:98)Llbc,

(cid:104)
(cid:107) (cid:99)M

(cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2

2

(cid:104)
(cid:107) (cid:99)M

(cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2

2

(cid:105)

(cid:105)

− 2ED

− 2ED

(cid:104)
(cid:107)g

(cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2

2

(cid:104)
(cid:107)g

(cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2

2

(cid:104)
(cid:107)g

(cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2

2

(cid:104)
(cid:107)(cid:98)g

(cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2

2

(cid:105)

(cid:105)

(cid:105)

(cid:105)

+ 2ερ + 2εM

+ 2ερ + 2εM

≤ 2ED

(cid:104)

(cid:124)

φ(cid:63) φ(cid:63)(s, a) − r(s, a))2(cid:105)
((cid:98)ρ

+ 2ED

(cid:104)
(cid:107) (cid:99)Mφ(cid:63) φ(cid:63)(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2
2

(cid:105)

− 2ED

(cid:2)(cid:107)(cid:98)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2

2

(cid:3)

+ 2ερ + 2εM

By
− 1
2

the G part
(cid:2)(cid:107)gφ(cid:63) − (cid:98)gφ(cid:63) (cid:107)2

2

Eν

have ED

of Lemma E.4, we
(cid:3) + εg ≤ εg. Then, using the optimality of (cid:98)ρ and (cid:99)M under D,
(cid:3) − 2ED

(cid:2)(cid:107)Mφ(cid:63) φ(cid:63)(s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2

+ 2ED

(cid:2)(cid:107)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2

2

≤ 2ED

(cid:104)

(ρ

(cid:124)

φ(cid:63) φ(cid:63)(s, a) − r(s, a))2(cid:105)

(cid:2)(cid:107)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2

2

(cid:3)

2 − (cid:107)(cid:98)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2

2

(cid:3)

≤

+ 2ερ + 2εM + 2εg

By the ρ, M parts of Lemma E.4,

≤ 3Eν

(cid:104)
(ρ

(cid:124)

φ(cid:63) φ(cid:63)(s, a) − r(s, a))2(cid:105)

+ 3Eν◦P

(cid:2)(cid:107)Mφ(cid:63) φ(cid:63)(s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2

2

(cid:3) − 3Eν

(cid:2)(cid:107)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2

2

(cid:3)

+ 4ερ + 4εM + 2εg

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

By Assumption 4.3 and Lemma E.3,

= 3Llbc(φ(cid:63)) + 4ερ + 4εM + 2εg

By assumption that φ(cid:63) is Linear BC and Proposition 4.2,

= 4ερ + 4εM + 2εg.

We now prove Theorem 4.4, in the general stochastic case. The deterministic transitions case is subsumed by ignoring the
minimization over G, i.e. setting the complexity term of G to zero.

Theorem 4.4. Assume Assumption 4.1 (and Assumption 4.3 if
96 log1/2(|Φ|)+4
. If N ≥ C 2

d+4 log1/2(1/δ)

√

2 , then for any δ ∈ (0, 1), w.p. at least 1 − δ, we have

β/4

the system is stochastic).

Let C2

:=

1. (cid:98)φ satisﬁes (cid:98)ε-approximate Linear BC with

(cid:98)ε ≤

13d(1 + W )2 log1/2(4W |Φ|N/δ)
√
N

+

7γ(1 + W ) log1/2(2|G|/δ)
√
N

,

2. λmin(Σ( (cid:98)φ)) ≥ β/4.

If transitions are deterministic, treat log(|G|) = 0.

Proof of Theorem 4.4. First, by our assumption that
w.p. at least 1 − δ, we have supφ∈Φ
important consequences:

(cid:12)
(cid:12)
(cid:12)λmin(Σ(φ)) − λmin((cid:98)Σ(φ))

N ≥ 4(96κ(Φ) + 4

d + 4 log1/2(1/δ))/β, Lemma E.2 implies that
(cid:12)
(cid:12)
(cid:12) ≤ β/4. Under this high probability event, we have two

√

√

1. φ(cid:63) is feasible in Equation (6), since λmin((cid:98)Σ(φ(cid:63))) ≥ λmin(Σ(φ(cid:63))) − β/4 ≥ β(1 − 1/4) ≥ β/2. In particular, this

means (cid:98)Llbc( (cid:98)φ) ≤ (cid:98)Llbc(φ(cid:63)).

2. The covariance of (cid:98)φ has lower-bounded eigenvalues, since λmin(Σ( (cid:98)φ)) ≥ λmin((cid:98)Σ( (cid:98)φ)) − β/4 ≥ β(1/2 − 1/4) ≥ β/4.

Now, apply Lemma E.6 to bound Llbc( (cid:98)φ), so w.p. 1 − 5δ,

Llbc( (cid:98)φ) ≤

24d(1 + W )2 log(4W |Φ|N/δ) + 128d2 log(4|Φ|N/δ) + 40γ2 log(2|G|/δ)
N

By Lemma D.1, we have that (cid:98)φ is (cid:98)ε-approximately Linear BC, with parameter

(cid:114)

(cid:98)ε ≤ (1 + W ) ·

24d(1 + W )2 log(4W |Φ|N/δ) + 128d2 log(4|Φ|N/δ) + 40γ2 log(2|G|/δ)
N
7γ(1 + W ) log1/2(2|G|/δ)
√
N

13d(1 + W )2 log1/2(4W |Φ|N/δ)
√
N

+

≤

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Finally, we remark that the (cid:99)W for (cid:98)φ (in the approximately Linear BC case) is upper bounded by a polynomial in W (cid:63) in the
assumed exact Linear BC of φ(cid:63). Consider our assumption that φ(cid:63) is exactly Linear BC with W (cid:63) = W (use (cid:63) to highlight
that it is the W in the assumption, which we now show matches the W in the result). Then, by Proposition 4.2, ∃M (cid:63) with
(cid:107)M (cid:63)(cid:107)2 ≤
W (cid:63)2 . Hence, it sufﬁces to minimize over this smaller ball for (cid:99)M , so that (cid:107) (cid:99)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2. Now, take the
smallest possible W in Lemma D.1, so that

1 − (cid:107)ρ(cid:63)(cid:107)2

(cid:113)

2

(cid:99)W =

≤

=

≤

(cid:107)ρ(cid:63)(cid:107)2
1 − (cid:107)M (cid:63)(cid:107)2
(cid:107)ρ(cid:63)(cid:107)2
(cid:113)
1 − (cid:107)ρ(cid:63)(cid:107)2
2
W (cid:63)2
(cid:113)

1 −

(cid:107)ρ(cid:63)(cid:107)2 (1 +

1 − (cid:107)ρ(cid:63)(cid:107)2
W (cid:63)2 )

2

(cid:107)ρ(cid:63)(cid:107)2
2
W (cid:63)2

2W (cid:63)2
(cid:107)ρ(cid:63)(cid:107)2

,

which is a polynomial in W (cid:63).

Our end-to-end result is deduced by chaining our LSPE theorem and the above theorem together.
Theorem 4.5. Under Assumption 4.1 (and Assumption 4.3 if P (s, a) is stochastic). Let C2, (cid:98)ε be as deﬁned in Theorem 4.4.
If N ≥ C 2

2 , then we have for any δ ∈ (0, 1/2), w.p. at least 1 − 2δ, for all distributions p0,

(cid:12)
(cid:12)V πe
(cid:12)

− Es∼p0

(cid:104)
(cid:98)fK(s, πe)

p0
960β−1/2(1 + W )d log(N )(cid:112)log(10/δ)

(cid:105)(cid:12)
(cid:12)
(cid:12) ≤

γK/2
1 − γ

+

√

(1 − γ)2

N

.

+

(cid:114)(cid:13)
(cid:13)
(cid:13)

4

ddπe
p0
dν

(cid:13)
(cid:13)
(cid:13)∞

(1 − γ)2

(cid:98)ε

Proof of Theorem 4.5. We ﬁrst apply Theorem 4.4 to see that (cid:98)φ satisﬁes the two properties needed for LSPE. It is indeed
approximately Linear BC, with (cid:98)ε speciﬁed in the theorem, and also has coverage (i.e. λmin(Σ( (cid:98)φ)) ≥ β/4). Using these two
facts, and on a separate independent dataset D2 (needs to be a separate dataset since (cid:98)φ is data-dependent), we run LSPE and
directly apply Theorem 3.3 for the result.

F. Implementation Details

Here we detail all environment speciﬁcations and hyperparameters used in the main text.

F.1. Dataset Details

Using the publicly released implementation for DrQ-v2, we trained high quality target policies and saved checkpoints for
ofﬂine behavior datasets. We refer the readers to Yarats et al. (2021a) for exact hyperparameters.

F.2. Environment Details

Following the standards used by DrQ-v2 (Yarats et al., 2021a), all environments have a maximum horizon length of 500
timesteps. This is achieved by the behavior/target policy having an action repeat of 2 frames. Furthermore, each state is 3
stacked frames that are each 84 × 84 dimensional RGB images (thus 9 × 84 × 84).

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

Task

Target
Performance

Behavior
Performance

Finger Turn Hard
Cheetah Run
Quadruped Walk
Humanoid Stand

927
758
873
827

226 (24%)
192 (25%)
236 (27%)
277 (33%)

Table 1. Performance for target and behavior policies used to collect evaluation and ofﬂine datasets respectively.

Task

Action Space Dimension

Task Traits Reward Type

Finger Turn Hard
Cheetah Run
Quadruped Walk
Humanoid Stand

2
6
12
21

turn
locomotion
locomotion
stand

sparse
dense
dense
dense

Table 2. Task descriptions, action space dimension, and reward type for each tested environment.

F.3. Representation Architecture and Hyperparameter Details

We adopt the same network architecture as DrQ-v2’s critic, ﬁrst introduced in SAC-AE (Yarats et al., 2021b). More
speciﬁcally, to process pixel input, we have a 5 layer ConvNet with 3 × 3 kernels and 32 channels with ReLU activations.
The ﬁrst convolutional layer has a stride of 2 while the rest has stride 1. The output is fed through a single fully connected
layer normalized by LayerNorm. Finally, there is a tanh nonlinearity on the outputted 50 dimensional state-representation.
The action is then concatenated with this output and fed into a 4-layer MLP all with ReLU activations.

Hyperparameter

Value

Feature Dimension
Weight Initialization
Optimizer
Learning Rate
Batch Size
Training Epochs
τ (target)
λDesign

512
orthogonal init.
Adam
1 × 10−5
2048
200
0.005
5 × 10−6

Table 3. Hyperparameters used for BCRL

Learning Bellman Complete Representations for Ofﬂine Policy Evaluation

F.4. Benchmarks and Metrics

Modiﬁcations to CURL: Originally, CURL only does contrastive learning between the image states with data augmentation.
For OPE, apply the same CURL objective to the state-action feature detailed in the previous section. Note we also train
CURL with the same random cropping image augmentations presented by the authors. Finally, since we are not interleaving
the representation learning with SAC, we do not have a Q prediction head.

Modiﬁcations to SPR: We use the same image encoder as our features for SPR. The main difference is in the architecture
of the projection layers where we implement as 3-layer mlp with ReLU activations. Note that these are additional parameters
that neither CURL nor BCRL require. Finally, similarly to CURL, we do not have an additional Q-prediction head.

Spearman Ranking Correlation Metric: This rank correlation measures the correlation between the ordinal rankings of
the value estimates and the ground truth returns. As deﬁned in Fu et al. (2021), we have for policies 1, 2, . . . , N , true returns
V1:N , and estimated returns ˆV1:N :

Ranking Correlation =

(cid:17)

(cid:16)

V1:N , ˆV1:N
Cov
σ (V1:N ) σ( ˆV1:N )

","Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Jonathan D. Chang * 1 Kaiwen Wang * 1 Nathan Kallus 2 Wen Sun 1 Abstract 1. Introduction 2 2 0 2 l u J 2 1 ] G L . s c [ 1 v 7 3 8 5 0 . 7 0 2 2 : v i X r a We study representation learning for Ofﬂine Re- inforcement Learning (RL), focusing on the im- portant task of Ofﬂine Policy Evaluation (OPE). Recent work shows that, in contrast to supervised learning, realizability of the Q-function is not enough for learning it. Two sufﬁcient conditions for sample-efﬁcient OPE are Bellman complete- ness and coverage. Prior work often assumes that representations satisfying these conditions are given, with results being mostly theoretical in nature. In this work, we propose BCRL, which directly learns from data an approximately lin- ear Bellman complete representation with good coverage. With this learned representation, we perform OPE using Least Square Policy Evalua- tion (LSPE) with linear functions in our learned representation. We present an end-to-end theoreti- cal analysis, showing that our two-stage algorithm enjoys polynomial sample complexity provided some representation in the rich class considered is linear Bellman complete. Empirically, we ex- tensively evaluate our algorithm on challenging, image-based continuous control tasks from the Deepmind Control Suite. We show our represen- tation enables better OPE compared to previous representation learning methods developed for off-policy RL (e.g., CURL, SPR). BCRL achieves competitive OPE error with the state-of-the-art method Fitted Q-Evaluation (FQE), and beats FQE when evaluating beyond the initial state dis- tribution. Our ablations show that both linear Bellman complete and coverage components of our method are crucial. *Equal contribution 1Computer Science, Cornell University, Ithaca, NY, USA 2Operations Research and Information Engi- neering, Cornell Tech, New York, NY, USA. Correspondence to: Jonathan D. Chang <https://jdchang1.github.io>, Kaiwen Wang <https://kaiwenw.github.io>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). Deep Reinforcement Learning (RL) has developed agents that solve complex sequential decision making tasks, achiev- ing new state-of-the-art results and surpassing expert human performance. Despite these impressive results, these algo- rithms often require a prohibitively large number of online interactions to scale to higher dimensional inputs. To address these sample complexity demands, a recent line of work (van den Oord et al., 2018; Anand et al., 2019; Mazoure et al., 2020; Yang & Nachum, 2021) has incor- porated advances in unsupervised representation learning from the supervised learning literature into developing RL agents. For example, CURL (Laskin et al., 2020) and SPR (Schwarzer et al., 2021) utilize contrastive representation ob- jectives as auxiliary losses within an existing RL framework. These efforts are motivated by the tremendous success such self-supervised techniques have offered in computer vision, natural language processing, speech processing, and beyond. While these formulations have shown sample complexity improvements empirically, it remains an open question whether these approaches successfully address the unique challenges from RL that usually do not appear in supervised learning, such as exploration and exploitation, credit assign- ments, long horizon prediction, and distribution shift. In particular, recent work (Wang et al., 2021b; Amortila et al., 2020; Foster et al., 2021) has shown that realizability of the learning target in RL (namely, the Q-function) is insufﬁcient to avoid exponential dependence on problem parameters. In this paper, we study representation learning for an important subtask of off-policy RL: ofﬂine policy evaluation (OPE). OPE is a critical component for any off-policy policy optimization approach (e.g., off-policy actor critic such as SAC, Haarnoja et al., 2018, and DDPG, Lillicrap et al., 2016). Moreover, OPE allows us to focus on issues arising from distribution shift and long horizon prediction. Speciﬁcally, we propose a new approach that leverages rich function approximation (e.g., deep neural networks) to learn a representation that is both Bellman complete and exploratory. A linear Bellman complete representation means that linear functions in the representation have zero inherent Bellman error (Antos et al., 2008), i.e., applying the Bellman operator on a linear function results in a new linear function. An exploratory representation means that Learning Bellman Complete Representations for Ofﬂine Policy Evaluation the resulting feature covariance matrix formed by the ofﬂine dataset is well-conditioned. These two representational properties ensure that, under linear function approximation (i.e., the linear evaluation protocol, Grill et al., 2020), classic least squares policy evaluation (LSPE) (Nedi´c & Bertsekas, 2003; Duan et al., 2020; Wang et al., 2021a) can achieve accurate policy evaluation. We provide an end-to-end analysis showing that our representation learning approach together with LSPE ensures near-optimal policy evaluation with polynomial sample complexity. Empirically, we extensively evaluate our method on image- based continuous control tasks from the Deepmind Control Suite. First, we compare against two representation learning approaches developed for off-policy RL: CURL (Laskin et al., 2020) and SPR (Schwarzer et al., 2021). These bear many similarities to contrastive learning techiniques for unsupervised representation learning: SimCLR (Chen et al., 2020) and Bootstrap your own latent (BYOL, Grill et al., 2020), respectively. Under the linear evaluation protocol (i.e., LSPE with a linear function on top of the learned representation), our approach consistently outperforms these baselines. We observe that representations learned by CURL and SPR sometimes even exhibit instability when evaluated using LSPE (prediction error blows up when more iterations of LSPE is applied), while our approach is more stable. This comparison demonstrates that representation learning in ofﬂine RL is more subtle and using representation techniques developed from supervised learning settings may not result in the best performance for ofﬂine RL. Our ablations show that both linear Bellman completeness and coverage are crucial, as our method also blows up if one ingredient is missing. Finally, BCRL achieves state-of-the-art OPE error when compared with other OPE methods, and improves the state-of-the-art when evaluating beyond the initial state distribution.1 1.1. Related Work Representation Learning in Ofﬂine RL: From the theoretical side, Hao et al. (2021) considers of- ﬂine RL in sparse linear MDPs (Jin et al., 2020). Learning with sparsity can be understood as feature selection. Their work has a much stronger coverage condition than ours: namely, given a representation class Φ, they assume that any feature φ ∈ Φ has global coverage under the ofﬂine data distribution, i.e., Es,a∼νφ(s, a)φ(s, a)(cid:62) is well conditioned where ν is ofﬂine data distribution. In our work, we only assume that there exists one φ(cid:63) that has global coverage, thus strictly generalizing their coverage condition. Uehara & Sun (2021) propose a general model-based ofﬂine RL approach that can perform representation learning for linear MDPs in the ofﬂine setting without global coverage. How- 1 Code available at https://github.com/CausalML/bcrl. ever, their algorithm is a version space approach and is not computationally efﬁcient. Also, our linear Bellman com- pleteness condition strictly generalizes linear MDPs. (Ni et al., 2021) consider learning state-action embeddings from a known RKHS. We use general function approximation that can be more powerful than RKHS. Finally, Parr et al. (2008) identiﬁes bellman completeness as a desirable condition for feature selection in RL when analyzing an equivalence be- tween linear value-function approximation and linear model approximation. In our work, we investigate how to learn bellman completeness representation, and also the role of coverage in our feature selection. From the empirical side, Yang & Nachum (2021) evalu- ated a broad range of unsupervised objectives for learning pretrained representations from ofﬂine datasets for down- stream Imitation Learning (IL), online RL, and ofﬂine RL. They found that the use of pretrained representations dra- matically improved the downstream performance for policy learning algorithms. In this work, we aim to identify such an unsupervised representation objective and to extend the empirical evaluation to ofﬂine image-based continuous con- trol tasks. Nachum & Yang (2021) presents a provable contrastive representation learning objective for IL (derived from maximum likelihood loss), learning state representa- tions from expert data to do imitation with behavior cloning. Our approach instead focuses on learning state-action rep- resentations for OPE. Finally, both Song et al. (2016) and Chung et al. (2019) present algorithms to learn representa- tions suitable for linear value function approximation. Song et al. (2016) is most relevant to our work where they aim to learn bellman complete features. Both works, however, work in the online setting and do not consider coverage induced from the representation which we identify is impor- tant for accurate OPE. Representation Learning in Online RL: Theoretical works on representation learning in online RL focus on learning representations to facilitate exploration from scratch. OLIVE (Jiang et al., 2017), BLin-UCB (Du et al., 2021), FLAMBE (Agarwal et al., 2020), Mofﬂe (Modi et al., 2021), and Rep-UCB (Uehara et al., 2022) propose ap- proaches for representation learning in linear MDPs where they assume that there exists a feature φ(cid:63) ∈ Φ that ad- mits the linear MDP structure for the ground truth transi- tion. Zhang et al. (2021) posits an even stronger assumption where every feature φ ∈ Φ admits the linear MDP struc- ture for the true transition. Note that we only assume that there exists one φ(cid:63) that admits linear Bellman complete- ness, which strictly generalizes linear MDPs. Hence, our representation learning setting is more general than prior theoretical works. Note that we study the ofﬂine setting while prior works operate in the online setting which has additional challenges from online exploration. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation On the empirical side, there is a large body of works that adapt existing self-supervised learning techniques devel- oped from computer vision and NLP to RL. For learning representations from image-based RL tasks, CPC (van den Oord et al., 2018), ST-DIM (Anand et al., 2019), DRIML (Mazoure et al., 2020), CURL (Laskin et al., 2020), and SPR (Schwarzer et al., 2021) learn representations by op- timizing various temporal contrastive losses. In particular, Laskin et al. (2020) proposes to interleave minimizing an In- foNCE objective with similarities to MoCo (He et al., 2020) and SimCLR (Chen et al., 2020) while learning a policy with SAC (Haarnoja et al., 2018). Moreover, Schwarzer et al. (2021) proposes learning a representation similar to BYOL (Grill et al., 2020) alongside a Q-function for Deep Q-Learning (Mnih et al., 2013). We compare our repre- sentational objective against CURL and SPR in Section 6, and demonstrate that under linear evaluation protocol, ours outperform both CURL and SPR. Note, we refer the readers to Schwarzer et al. (2021) for comparisons between SPR and CPC, ST-DIM, and DRIML. 2. Preliminaries In this paper we consider the inﬁnite horizon discounted MDP (cid:104)S, A, γ, P, r, d0(cid:105). where S, A are state and action spaces which could contain a large number of states and ac- tions or could even be inﬁnite, γ ∈ (0, 1) is a discount factor, P is the transition kernel, r : S × A → R is the reward function, and d0 ∈ ∆(S) is the initial state distribution. We assume rewards are bounded by 1, i.e. |r(s, a)| ≤ 1. Given a policy π : S (cid:55)→ ∆(A), we denote V π(s) = E (cid:2)(cid:80)∞ h=0 γhrh|π, s0 := s(cid:3) as the expected dis- counted total reward of policy π starting at state s. We = Es∼d0 V π(s) as the expected discounted total denote V π d0 reward of the policy π starting at the initial state distribu- tion d0. We also denote average state-action distribution dπ h=0 γhdπ h(s, a) is d0 the probability of π visiting the (s, a) pair at time step h, starting from d0. (s, a) = (1 − γ) (cid:80)∞ h(s, a) where dπ In OPE, we seek to evaluate the expected discounted to- tal reward V πe of a target policy πe : S (cid:55)→ ∆(A) given data drawn from an ofﬂine state-action distribution ν ∈ ∆(S × A). The latter can, for example, be the state-action distribution under a behavior policy πb. Namely, the ofﬂine dataset D = {si, ai, ri, s(cid:48) i}N i=1 consists of N i.i.d tuples generated as (s, a) ∼ ν, r = r(s, a), s(cid:48) ∼ P (·|s, a). We deﬁne Bellman operator T πassociated with π as follows: T πf (s, a) =∆ r(s, a) + γEs(cid:48)∼P (s,a),a(cid:48)∼π(s(cid:48)) [f (s(cid:48), a(cid:48))] We may drop the superscript π when it is clear from context, in particular when π = πe is the ﬁxed target policy. A representation, or feature, φ : S × A → Rd is an embedding of state-action pairs into d-dimensional (cid:80)N space. We let Σ(φ) = Eν [φ(s, a)φ(s, a)(cid:124)] , (cid:98)Σ(φ) = i=1 φ(si, ai)φ(si, ai)(cid:124). We consider learning a rep- 1 N resentation from a feature hypothesis class Φ ⊂ [S × A (cid:55)→ Rd]. We assume features have bounded norm: (cid:107)φ(s, a)(cid:107)2 ≤ 1, ∀s, a, ∀φ ∈ Φ. In our experiments, Φ is convolutional neural nets with d outputs. Notation We denote BW := {x ∈ Rd : (cid:107)x(cid:107)2 ≤ W } as the Euclidean Ball in Rd with radius W . Given a distribution ν ∈ ∆(S × A) and a function f : S × A (cid:55)→ R, we denote ν = Es,a∼νf 2(s, a). Given a positive L2(ν) norm as (cid:107)f (cid:107)2 xT Σx and let λmin(Σ) deﬁnite matrix Σ, let (cid:107)x(cid:107)Σ = denote the minimum eigenvalue. When ν (cid:28) µ we let dν dµ denote the Radon-Nikodym derivative. We use ◦ to denote composition, so s(cid:48), a(cid:48) ∼ P (s, a) ◦ π is short-form for s(cid:48) ∼ P (s, a), a(cid:48) ∼ π(s(cid:48)). For any function f (s, a) and a policy π, we denote f (s, π) = Ea∼π(s) [f (s, a)]. √ 3. Linear Bellman Completeness Before we introduce our representation learning approach, in this section, we ﬁrst consider OPE with linear function approximation with a given representation φ. Lessons from supervised learning or linear bandit may suggest that OPE should be possible with polynomially many ofﬂine samples, as long as (1) ground truth target Qπe is linear in φ (i.e. ∃w ∈ Rd, such that for all s, a, Qπe (s, a) = w(cid:62)φ(s, a)), and (2) the ofﬂine data provides sufﬁcient coverage over πe (i.e., λmin(Σ(φ)) > 0). Unfortunately, under these two assumptions, there are lower bounds indicating that for any OPE algorithm, there exists an MDP where one will need at least exponentially (exponential in horizon or d) many ofﬂine samples to provide an accurate estimate of V πe (Wang et al., 2021b; Foster et al., 2021; Amortila et al., 2020). This lower bound indicates that one needs additional structural conditions on the representation. The additional condition the prior work has consider is Bellman completeness (BC). Since we seek to learn a rep- resentation rather than assume theoretical conditions on a given one, we will focus on an approximate version of BC. Deﬁnition 3.1 (Approximate Linear BC). A representation φ is εν-approximately linear Bellman complete if, max w1∈BW min w2∈BW (cid:107)w (cid:124) 2 φ − T πe (w (cid:124) 1 φ)(cid:107)ν ≤ εν. Note the dependence on ν, πe, and W . (cid:124) 1 φ(s, a), T π(w Intuitively the above condition is saying that for any linear (cid:124) 1 φ(s, a)) itself can be approxi- function w mated by another linear function under the distribution ν. Remark 3.2. Low-rank MDPs (Jiang et al., 2017; Agarwal et al., 2020) are subsumed in the exact linear BC model, i.e., εν = 0 under any distribution ν. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Algorithm 1 Least Squares Policy Evaluation (LSPE) 1: Input: Target policy πe, features φ, dataset D 2: Initialize (cid:98)θ0 = 0 ∈ BW . 3: for k = 1, 2, ..., K do (cid:124) k−1φ(s, a), Set (cid:98)fk−1(s, a) = (cid:98)θ 4: (cid:98)Vk−1(s) = Ea∼πe(s)[ (cid:98)fk−1(s, a)] 5: Perform linear regression: 1 N N (cid:88) i=1 (cid:98)θk ∈ arg min θ∈BW 6: end for 7: Return (cid:98)fK. (cid:0)θ(cid:124)φ(si, ai) − ri − γ (cid:98)Vk−1(s(cid:48) i)(cid:1)2 Note that the Bellman completeness condition is more sub- tle than the common realizability condition: for any ﬁxed φ, increasing its expressiveness (e.g., add more features) does not imply a monotonic decrease in εν. Thus common tricks such as lifting features to higher order polynomial kernel space or more general reproducing kernel Hilbert space does not imply the linear Bellman complete condition, nor does it improve the coverage condition. We next show approximate Linear BC together with coverage imply sample-efﬁcient OPE via Least Square Policy Evaluation (LSPE) (Algo- rithm 1). We present our result using the relative condition number, deﬁned for any initial state distribution p0 as x(cid:124)Es,a∼dπe φ(s, a)φ(s, a)(cid:124)x κ(p0) := sup x∈Rd p0 x(cid:124)Σ(φ)x . (1) Theorem 3.3 (Sample Complexity of LSPE). Assume fea- ture φ satisﬁes approximate Linear BC with parameter εν. For any δ ∈ (0, 0.1), w.p. at least 1 − δ, we have for any initial state distribution p0 (cid:12) (cid:12)V πe (cid:12) p0 (cid:104) − Es∼p0 (cid:98)fK(s, πe) (cid:105)(cid:12) (cid:12) (cid:12) ≤ γK/2 1 − γ + (cid:114)(cid:13) (cid:13) 4 (cid:13) ddπe p0 dν (cid:13) (cid:13) (cid:13)∞ (1 − γ)2 εν + 480(cid:112)κ(p0)(1 + W )d log(N )(cid:112)log(10/δ) √ (1 − γ)2 N , where (cid:98)fK is the output of Algorithm 1. Please see Appendix C.1 for proof. The above result holds simultaneously over all initial state distributions p0 covered by the data distribution ν. If ν has full coverage, i.e. if Σ (cid:23) βI, as is commonly assumed in the literature, one can show that κ(p0) ≤ β−1 for any initial state distribution (cid:13) (cid:13) p0. Also note that the concentrability coefﬁcient (cid:13)∞ shows up as T πe (cid:98)fk−1 can be nonlinear. In the exact Linear BC case, where εν = 0 (i.e., there is a linear function that perfectly approximates T πe (cid:98)fk−1 under ν), the term involving the concentrability coefﬁcient will be 0 and we can even avoid its ﬁniteness. ddπe p0 dν (cid:13) (cid:13) (cid:13) 4. Representation Learning The previous section indicates sufﬁcient conditions on the representation for efﬁcient and accurate OPE with linear function approximation. However, a representation that is Bellman complete and also provides coverage is not avail- able a priori, especially for high-dimensional settings (e.g., image-based control tasks as we consider in our experi- ments). Existing theoretically work often assume such rep- resentation is given. We propose to learn a representation φ that is approximately Bellman complete and also provides good coverage, via rich function approximation (e.g., a deep neural network). More formally, we want to learn a repre- sentation φ such that (1) it ensures approximate Bellman complete, i.e., εν is small, and (2) has a good coverage, i.e., λmin(Σ(φ)) is as large as possible, which are the two key properties to guarantee accurate and efﬁcient OPE indicated by Theorem 3.3. To formulate the representation learning question, our key assumption is that our representation hy- pothesis class Φ is rich enough such that it contains at least one representation φ(cid:63) that is linear Bellman complete (i.e., εν = 0) and has a good coverage. Assumption 4.1 (Representational power of Φ). There ex- ists φ(cid:63) ∈ Φ, such that (1) φ(cid:63) achieves exact Linear BC (deﬁnition 3.1 with εν = 0), and (2) φ(cid:63) induces coverage, i.e., λmin(Σ(φ(cid:63))) ≥ β > 0. Our goal is to learn such a representation from the hypothe- sis class Φ. Note that unlike prior RL representation learning works (Hao et al., 2021; Zhang et al., 2021), here we only assume that there exists a φ(cid:63) has linear BC and induces good coverage, other candidate φ ∈ Φ could have terrible coverage and does not necessarily have linear BC. Before proceeding to the learning algorithm, we ﬁrst present an equivalent condition for linear BC, which does not rely on the complicated min-max expression of Deﬁnition 3.1. Proposition 4.2. Consider a feature φ with full rank covari- ance Σ(φ). Given any W > 0, the feature φ being linear BC (under BW ) implies that there exist (ρ, M ) ∈ BW × Rd×d with (cid:107)M (cid:107)2 ≤ 1 − (cid:107)ρ(cid:107)2 (cid:113) W 2 , and Eν (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) φ(s, a) − (cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 = 0. (2) On the other hand, if there exists (ρ, M ) ∈ BW × Rd×d with (cid:107)M (cid:107)2 < 1 such that the above equality holds, then φ must satisfy exact linear BC with W ≥ (cid:107)ρ(cid:107)2 . 1−(cid:107)M (cid:107)2 Please see Appendix D for proof. The above shows that lin- ear BC is equivalent to a simple linear relationship between the feature and the expected next step’s feature and reward. This motivates our feature learning algorithm: if we are ca- pable of learning a representation φ such that the transition Learning Bellman Complete Representations for Ofﬂine Policy Evaluation from the current feature φ(s, a) to the expected next fea- ture Es(cid:48)∼P (s,a)[φ(s(cid:48), πe)] and reward r(s, a) is linear, then we’ve found a feature φ that is linear BC. 4.1. Algorithm To learn the representation that achieves linear BC, we use Proposition 4.2 to design a bilevel optimization program. We start with deterministic transition as a warm up. Deterministic transition Due to determinism in the tran- sition, we do not have an expectation with respect to s(cid:48) anymore. So, we design the bilevel optimization as follows: However, we cannot directly optimize the above objective since we do not have access to P (s, a) to compute the ex- pected next step feature Es(cid:48)∼P (s,a)φ(s(cid:48), πe). Also note that the expectation Es(cid:48) is inside the square which means that we cannot even get an unbiased estimate of the gradient of (φ, M ) with one sample s(cid:48) ∼ P (s, a). This phenomenon is related to the double sampling issue on ofﬂine policy evaluation literature which forbids one to directly optimize Bellman residuals. Algorithms such as TD are designed to overcome the double sampling issue. Here, we use a different technique to tackle this issue (Chen & Jiang, 2019). We introduce a function class G ⊂ S × A (cid:55)→ Rd which is rich enough to contain the expected next feature. (cid:104) (cid:21) min φ∈Φ (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 Θ = (cid:8)(ρ, M ) ∈ B(cid:107)ρ(cid:63)(cid:107) × Rd×d :, (cid:107)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2 (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) min (ρ,M )∈Θ φ(s, a) − ED (cid:9) , where ρ(cid:63), M (cid:63) are the optimal ρ, M for the linear BC φ(cid:63) (in Assumption 4.1). Namely, we aim to search for a repre- sentation φ ∈ Φ, such that the relationship between φ(s, a) and the combination of the next time step’s feature φ(s(cid:48), πe) and the reward, is linear. The spectral norm constraint in Θ is justiﬁed by Proposition 4.2. Solving the above bilevel moment condition ﬁnds a representation that achieves ap- proximate linear BC. However, there is no guarantee that such representation can provide a good coverage over πe’s traces. We introduce regularizations for φ using the ideas from optimal designs, particularly the E-optimal design. Deﬁne the minimum eigenvalue regularization (3) Assumption 4.3. For any φ ∈ Φ, we have that the mapping (s, a) (cid:55)→ γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)] is in G. We form the optimization problem as follows: (cid:104) min φ∈Φ min (ρ,M )∈Θ (cid:21) ED (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) ED (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2 φ(s, a) − (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:105) . 2 (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (5) − min g∈G Note that under Assumption 4.3, the min over G will approx- imate γ2ED(cid:107)Es(cid:48)∼P (s,a)φ(s(cid:48), πe) − φ(s(cid:48), πe)(cid:107)2 2, i.e., the av- erage variance induced by the stochastic transition. Thus, for a ﬁxed φ and M , we can see that the following ED (cid:107)M φ(s, a) − γφ(s(cid:48), πe)(cid:107)2 − γ2ED(cid:107)Es(cid:48)∼P (s,a)φ(s(cid:48), πe) − φ(s(cid:48), πe)(cid:107)2 2 2 RE(φ) := λmin (ED [φ(s, a)φ(s, a)(cid:124)]) , is indeed an unbiased estimate of: as the smallest eigenvalue of the empirical feature covari- ance matrix under the representation φ. Maximizing this quantity ensures that our feature provides good coverage, i.e. λmin(Σ(φ)) is as large as possible. Thus, to learn a representation that is approximately linear Bellman complete and also provides sufﬁcient coverage, we formulate the following constrained bilevel optimization: (cid:104) min φ∈Φ (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) s.t., RE(φ) ≥ β/2. min (ρ,M )∈Θ ED (cid:21) φ(s, a) − (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 To extend this to stochastic transitions, there is an additional double sampling issue, which we discuss and address now. Stochastic transition Ideally, we would solve the follow- ing bilevel optimization problem, (cid:104) min φ∈Φ min (ρ,M )∈Θ ED (cid:21) (cid:13) (cid:13) (cid:13) (cid:13) (cid:20)M ρ(cid:124) φ(s, a) − (cid:20)γEs(cid:48)∼P (s,a)[φ(s(cid:48), πe)] r(s, a) (cid:105) . 2 (cid:21)(cid:13) (cid:13) (cid:13) (cid:13) 2 (4) Es,a∼ν (cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13) (cid:13) 2 (cid:13) 2 which matches to the ideal objective in Eq. 4. Thus solv- ing for φ based Eq. 5 allows us to optimize Eq. 4, which allows us to learn an approximate linear Bellman complete representation. Similarly, we incorporate the E-optimal de- sign here by adding a constraint that forcing the smallest eigenvalue of the empirical feature covariance matrix, i.e., RE(φ), to be lower bounded. Once we learn a representation that is approximately linear BC, and also induces sufﬁcient coverage, we simply call the LSPE to estimate V πe . The whole procedure is summarized in Algorithm 2. Note in Alg 2 we put constraints to the objective function using Lagrangian multiplier. There are other design choices that also encourage coverage. One particular choice we study empirically is motivated by the idea of D-optimal design. Here we aim to ﬁnd a representation that maximizes the following log-determinant RD(φ) := ln det (ED [φ(s, a)φ(s, a)(cid:124)]) . Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Algorithm 2 OPE with Bellman Complete and exploratory Representation Learning (BCRL) 1: Input: Representation class Φ, dataset D of size 2N , design regularization R, function class G, policy πe. 2: Randomly split D into two sets D1, D2 of size N . 3: If the system is stochastic, learn representation (cid:98)φ as, (cid:104) arg min φ∈Φ − min g∈G (cid:21) ED1 min (ρ,M )∈Θ (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) ED1 (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2 φ(s, a) − (cid:105) 2 (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 − λR(φ) 4: Otherwise, for deterministic system, learn (cid:98)φ as, (cid:104) arg min φ∈Φ min (ρ,M )∈Θ ED1 (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) φ(s, a) − (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 − λR(φ) 5: Return (cid:98)V := LSPE(πe, (cid:98)φ, D2). When D is large, then the regularization RD(φ) approx- imates (cid:80) i ln (σi (Eν [φ(s, a)φ(s, a)(cid:124)])). Maximizing RD(φ) then intuitively maximizes coverage over all directions. D-optimal design is widely used for bandits (Lattimore & Szepesvári, 2020) and RL (Wang et al., 2021b; Agarwal et al., 2019) to design exploration distributions with global coverage. Note that, in contrast to these contexts where the feature is ﬁxed and the distribution is optimized, we optimize the feature, given the data distribution ν. 4.2. Sample Complexity Analysis We now prove a ﬁnite sample utility guarantee for the em- pirical constrained bilevel optimization problem, (cid:98)φ ∈ arg min φ∈Φ (cid:104) min (ρ,M )∈Θ ED (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ (cid:13) (cid:124) φ(s, a) − ED (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2 2 (cid:105) . − min g∈G (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (6) s.t., RE(φ) ≥ β/2. For simplicity, we state our results for discrete function class Φ and G. Note that our sample complexity only scales with respect ln(|Φ|) and ln(|G|), which are the standard com- plexity measures for discrete function classes. We extend our analysis to inﬁnite function classes under metric entropy assumptions (Wainwright, 2019; van der Vaart & Wellner, 1996) in the Appendix; see Assumption E.5. The following theorem shows that Algorithm 2 learns a representation (cid:98)φ that is O(N −1/2) approximately Linear BC and has coverage. Theorem 4.4. Assume Assumption 4.1 (and Assump- := Let C2 tion 4.3 if 96 log1/2(|Φ|)+4 2 , then for any the system is stochastic). √ . If N ≥ C 2 d+4 log1/2(1/δ) β/4 δ ∈ (0, 1), w.p. at least 1 − δ, we have 1. (cid:98)φ satisﬁes (cid:98)ε-approximate Linear BC with (cid:98)ε ≤ 13d(1 + W )2 log1/2(4W |Φ|N/δ) √ N + 7γ(1 + W ) log1/2(2|G|/δ) √ N , 2. λmin(Σ( (cid:98)φ)) ≥ β/4. If transitions are deterministic, treat log(|G|) = 0. Proof Sketch. First, we use Weyl’s Perturbation Theorem and chaining to show that the eigenvalues of Σ(φ) are close to (cid:98)Σ(φ), uniformly over φ. This implies that (a) λmin((cid:98)Σ(φ(cid:63))) ≥ β/2, and hence is feasible in the empirical bilevel optimization Equation (6), and (b) λmin(Σ( (cid:98)φ)) ≥ β/4. Since φ(cid:63) is feasible, we apply uniform concentration arguments to argue that (cid:98)φ has low population loss (Equa- tion (5)), which implies approximate Linear BC. The error term in (cid:98)ε is comprised of the statistical errors of ﬁtting Φ and of ﬁtting G for the double sampling correc- tion. In the contextual bandit setting, i.e. γ = 0, there is no transition, so the second term becomes 0. Chaining together with Theorem 3.3 gives the following end-to-end (cid:101)O(N −1/2) evaluation error guarantee for LSPE with the learned features: Theorem 4.5. Under Assumption 4.1 (and Assumption 4.3 if P (s, a) is stochastic). Let C2, (cid:98)ε be as deﬁned in Theo- rem 4.4. If N ≥ C 2 2 , then we have for any δ ∈ (0, 1/2), w.p. at least 1 − 2δ, for all distributions p0, (cid:12) (cid:12)V πe (cid:12) − Es∼p0 (cid:104) (cid:98)fK(s, πe) p0 960β−1/2(1 + W )d log(N )(cid:112)log(10/δ) (cid:105)(cid:12) (cid:12) (cid:12) ≤ γK/2 1 − γ + √ (1 − γ)2 N . + (cid:114)(cid:13) (cid:13) (cid:13) 4 ddπe p0 dν (cid:13) (cid:13) (cid:13)∞ (1 − γ)2 (cid:98)ε Comparison to FQE What if one ignores the representa- tion learning and just runs the Fitted Q-Evaluation (FQE) which directly performs least square ﬁtting with the nonlin- ear function class F := {w(cid:62)φ(s, a) : φ ∈ Φ, (cid:107)w(cid:107)2 ≤ W }? As N → ∞, FQE will suffer the following worst-case Bell- man error (also called inherent Bellman error): εibe := max f ∈F min g∈F (cid:107)g − T πe f (cid:107)2 ν . Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Algorithm 3 Practical Instantiation of BCRL 1: Input: Ofﬂine dataset D = {s, a, s(cid:48)}, target policy πe 2: Initialize parameters for φ, M , ρ, and φ 3: for t = 0, 1, . . . , T do 4: Mt+1 ← Mt − η∇M J(φt, Mt, ρt, φt) ρt+1 ← ρt − η∇ρJ(φt, Mt, ρt, φt) 5: φt+1 ← φt − η∇φJ(φt, Mt+1, ρt+1, φt) 6: φt+1 ← τ φt+1 + (1 − τ )φt 7: 8: end for 9: Linear evaluation: (cid:98)V = LSPE(πe, φT , D). Note that our assumption that there exists a linear BC rep- resentation φ(cid:63) does not imply that the worst-case Bellman error εibe is small. In contrast, when N → ∞, our approach will accurately estimate V πe p0 . 5. A Practical Implementation In this section we instantiate a practical implementation to learn our representation using deep neural networks for our representation function class Φ. Based on Equation (3), we ﬁrst formalize our bilevel optimization objective: J(φ, M, ρ, φ) = ED (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ (cid:13) − λ log det ED φ(s, a) − (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:2)φ(s, a)φ(s, a)(cid:62)(cid:3) . (cid:124) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 In our implementation, we replace the hard constraint pre- sented in Section 4.1 with a Lagrange multiplier, i.e. we use the optimal design constraint as a regularization term when learning φ. Speciﬁcally, we maximize the log det of the covariance matrix induced by the feature, which maximizes all eigenvalues since log det is the sum of the log eigen- values. Our experiment results in Section 6.4 demonstrate that it indeed improves the condition number of Σ(φ). In some of our experiments, we use a target network in our implementation. Namely, the updates make use of a target network φ where the weights can be an exponential moving average of the representation network’s weights. This idea of using a slow moving target network has been shown to stabilize training in both the RL (Mnih et al., 2013) and the representation learning literature (Grill et al., 2020). As summarized in Algorithm 3, given our ofﬂine behavior dataset D and target policy πe, we iteratively update M Figure 1. Representative frames from DeepMind Control Suite tasks. From left to right, Finger Turn Hard, Cheetah Run, Quadruped Walk, and Humanoid Stand. and φ and then use the resulting learned representation to perform OPE. Please see Appendix F for implementation and hyperparameter details. As we will show in our experiments, our update procedures for φ signiﬁcantly minimizes the Bellman Completeness loss and also improve the condition number of Σ(φ), which are the two key quantities to ensure good performance of LSPE with linear regression as shown in Theorem 3.3. 6. Experiments Our goal is to answer the following questions. (1) How do our representations perform on downstream LSPE com- pared to other popular unsupervised representation learning techniques? (2) How important are both the linear bellman completeness and optimal design components for learning representations? (3) How competitive is BCRL with other OPE methods, especially for evaluating beyond the initial state distribution? Following the standards in evaluating representations in su- pervised learning, we used a linear evaluation protocol, i.e., linear regression in LSPE on top of a given representa- tion. This allows us to focus on evaluating the quality of the representation. We compared our method to prior techniques on a range of challenging, image-based continuous control tasks from the DeepMind Control Suite benchmark (Tassa et al., 2018): Finger Turn Hard, Cheetah Run, Quadruped Walk, and Humanoid Stand. Frames from the tasks are shown in Figure 1. To investigate our learned representation, we benchmark our representation against two state-of-the-art self-supervised representation learning objectives adopted for RL: (1) CURL uses the In- foNCE objective to contrastively learn state-representations; and (2) SPR adopts the BYOL contrastive learning frame- work to learn representations with latent dynamics. Note, we modiﬁed CURL for OPE by optimizing the contrastive loss between state-action pairs rather than just states. For SPR, we did not include the Q prediction head and used their state representation plus latent dynamics as the state- action representation for downstream linear evaluation. We used the same architecture for the respective representa- tions across all evaluated algorithms. To compare against other OPE methods, we additionally compared against Fit- ted Q-Evaluation (Munos & Szepesvári, 2008; Kostrikov & Nachum, 2020) (FQE), weighted doubly robust policy evaluation (Jiang & Li, 2016; Thomas & Brunskill, 2016) (DR), Dreamer-v2 (Hafner et al., 2021) model based eval- uation (MB), and DICE (Yang et al., 2020). We modiﬁed implementations from the benchmark library released by Fu et al. (2021) for FQE and DR and used the authors’ released codebases for Dreamer-v2 and BestDICE.2 2https://github.com/google-research/dice_rl. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Figure 2. OPE curves across ﬁve seeds using representations trained with BCRL, SPR, and CURL on the ofﬂine datasets (Table 1). mators beyond the original initial state distribution d0. The ﬁrst setting ensures that baselines and our algorithm all sat- isfy the coverage condition, thus demonstrating the unique beneﬁt of learning a Bellman complete representation. The second setting evaluates the robustness of our algorithm, i.e., the ability to estimate beyond the original d0. Evaluation on On-Policy + Off-Policy Data: To further investigate the importance of learning linear BC features, we experiment with learning representations from an ofﬂine dataset that also contains state-action pairs from the target policy. More speciﬁcally, we train our representations on a dataset containing 100K behavior policy and 100K target policy samples. Note, only the experiment in this paragraph uses this mixture dataset. With the addition of on-policy data from the target policy, we can focus on just the role of linear BC for OPE performance because the density ratio ddπe po dν and the relative condition number (Eq. 1) is at most 2, i.e. we omit the design regularization and focused on minimizing the Bellman completeness loss. Figure 4 (Left) shows that BCRL outperforms baselines in this setting, even Figure 3. (Left) Root mean squared evaluation error across all tasks. (Right) Mean spearman ranking correlation across all tasks. Our target policies were trained using the authors’ imple- mentation of DRQ-v2 (Yarats et al., 2021a), a state-of-the- art, off-policy actor critic algorithm for vision-based contin- uous control. With high-quality target policies, we collected 200 rollouts and did a linear evaluation protocol to predict the discounted return. For our ofﬂine behavior datasets, we collected 100K samples from a trained policy with mean performance roughly a quarter of that of the target policy (Table 1). All results are aggregated over ﬁve random seeds. See Appendix F for details on hyperparameters, environ- ments, and dataset composition. 6.1. OPE via LSPE with Learned Representations Figure 2 compares the OPE performance of BCRL against SPR and CURL. Representations learned by BCRL outper- form those learned by SPR and CURL. On some tasks, SPR and CURL both exhibited an exponential error ampliﬁca- tion with respect to the number of iterations of LSPE, while BCRL did not suffer from any blowup. 6.2. OPE Performance Figure 3 compares the OPE performance of BCRL against multiple benchmarks from the OPE literature. BCRL is competitive with FQE and evaluates better than other bench- marks across the tasks that we tested on. We also evaluated how well estimated values from BCRL rank policies. Following (Fu et al., 2021), we use the spearman ranking correlation metric to compute the correlation between ordinal rankings according to the OPE estimates and the ground truth values. For ranking, we evaluated three additional target policies with mean perfor- mances roughly 75%, 50%, and 10% of the target policy. Figure 3 presents the mean correlation of each evaluation algorithm across all tasks. BCRL is competitive with FQE and consistently better than other benchmarks at ranking policies. 6.3. Further Investigation of Different Settings In this section, we consider two additional settings: (1) the ofﬂine dataset contains some on-policy data, which ensures that the ofﬂine data provides coverage over the evaluation policy’s state action distribution; (2) we evaluate all esti- Figure 4. (Left) Evaluation on a mixture dataset with on-policy and off-policy data. Note the addition of target policy data bounds the relative condition number (Eq. 1). (Right) Evaluation beyond the initial state distribution (given just the ofﬂine data). Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Figure 5. (Left) BCRL’s OPE curves for Cheetah Run with (blue) and without (red) the D-optimal design based regularization. (Center) Bar plot of the singular values of the feature covariance matrices for the left plot. (Right) OPE curves for Finger Turn Hard with (blue) and without (red) optimizing for linear BC. matching FQE performance across tasks. Our experiments corroborate our analysis that explicitly enforcing our learned representations to be linear BC improves OPE. Note that the fact that LSPE with SPR and CURL features still blows up under this mixture data means that the other representations’ failures are not just due to coverage. Evaluation Beyond Initial State Distribution: To further investigate the beneﬁts of optimizing the D-optimal design to improve coverage, we investigate doing OPE beyond the initial state distribution d0, which is supported by Theo- rem 4.5. Note that if our representation is exactly Bellman complete and also the corresponding feature covariance ma- trix is well-conditioned, we should be able to evaluate well on any states. Speciﬁcally, we experiment on evaluating at all timesteps in a target policy rollout, not just at the initial state distribution. Figure 4 (Right) shows that BCRL is able to more robustly evaluate out-of-distribution than all other benchmarks. 6.4. Ablation Studies Impact of Optimal Design Regularization: To investi- gate the impact of maximizing the D-optimal design, we ablate the design regularization term from our objective and analyze the downstream evaluation performance and the respective feature covariance matrices on the ofﬂine dataset. Figure 5 (Center) presents a bar plot of the sin- gular values of the feature covariance matrix (Σ(φ) := Es,a∼νφ(s, a)φ(s, a)(cid:62)). Figure 5 (Left) shows the down- stream OPE performance for features trained with and with- out the design regularization on the Cheetah Run task. Note that without the regularization, we ﬁnd that that the feature covariance matrix has much worse condition number, i.e. feature is less exploratory. As our analysis suggests, we also observe a deterioration in evaluation performance with- out the design regularization to explicitly learn exploratory features. Impact of Linear Bellman Completeness: Figure 5 (Right) presents an ablation study where we only opti- mize for the design term in our objective. We ﬁnd that downstream OPE performance degrades without directly op- timizing for linear BC, suggesting that a feature with good coverage alone is not enough to avoid error ampliﬁcation. 7. Conclusion We present BCRL which leverages rich function approxi- mation to learn Bellman complete and exploratory repre- sentations for stable and accurate ofﬂine policy evaluation. We provide a mathematical framework of representation learning in ofﬂine RL, which generalizes all existing repre- sentation learning frameworks from the RL theory literature. We provide an end-to-end theoretical analysis of our ap- proach for OPE and demonstrate that BCRL can accurately estimate policy values with polynomial sample complex- ity. Notably, the complexity has no explicit dependence on the size of the state and action space, instead, it only depends on the statistical complexity of the representation hypothesis class. Experimentally, we extensively evaluate our approach on the DeepMind Control Suite, a set of image- based, continuous control, robotic tasks. First, we show that under the linear evaluation protocol – using linear regres- sion on top of the representations inside the classic LSPE framework – our approach outperforms prior RL representa- tion techniques CURL and SPR which leverage contrastive representation learning techniques SimCLR and BYOL re- spectively. We also show that BCRL achieves competitive OPE performance with the state-of-the-art FQE, and notice- ably improves upon it when evaluating beyond the initial state distribution. Finally, our ablations show that approxi- mate Linear Bellman Completeness and coverage are crucial ingredients to the success of our algorithm. Future work includes extending BCRL to ofﬂine policy optimization. ACKNOWLEDGEMENTS This material is based upon work supported by the National Science Foundation under Grant No. 1846210 and by a Cor- nell University Fellowship. We thank Rahul Kidambi, Ban Kawas, and the anonymous reviewers for useful discussions and feedback. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation References Agarwal, A., Jiang, N., Kakade, S. M., and Sun, W. Rein- forcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, 2019. Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W. Flambe: Structural complexity and representation learn- ing of low rank mdps. NeurIPS, 33:20095–20107, 2020. Amortila, P., Jiang, N., and Xie, T. A variant of the wang- foster-kakade lower bound for the discounted setting. arXiv preprint arXiv:2011.01075, 2020. Anand, A., Racah, E., Ozair, S., Bengio, Y., Côté, Unsupervised state rep- M., and Hjelm, R. D. resentation learning in atari. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox, E. B., and Garnett, R. (eds.), NeurIPS, pp. 8766– URL https://proceedings. 8779, neurips.cc/paper/2019/hash/ 6fb52e71b837628ac16539c1ff911667-Abstract. html. 2019. Antos, A., Szepesvári, C., and Munos, R. Fitted In and Roweis, (eds.), NIPS, volume 20. Curran Associates, URL https://proceedings. q-iteration in continuous action-space mdps. Platt, J., Koller, D., Singer, Y., S. Inc., neurips.cc/paper/2007/file/ da0d1111d2dc5d489242e60ebcbaf988-Paper. pdf. 2008. Bhatia, R. Matrix analysis, volume 169. Springer Science & Business Media, 2013. Boucheron, S., Lugosi, G., and Massart, P. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013. Chen, J. and Jiang, N. Information-theoretic considerations in batch reinforcement learning. In ICML, pp. 1042–1051. PMLR, 2019. Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. E. A simple framework for contrastive learning of visual representations. ICML, 2020. URL https://arxiv. org/abs/2002.05709. Chung, W., Nath, S., Joseph, A., and White, M. Two- timescale networks for nonlinear value function approxi- mation. In ICLR, 2019. URL https://openreview. net/forum?id=rJleN20qK7. Duan, Y., Jia, Z., and Wang, M. Minimax-optimal off-policy evaluation with linear function approximation. In ICML, pp. 2701–2709. PMLR, 2020. Foster, D. J., Krishnamurthy, A., Simchi-Levi, D., and Xu, Y. Ofﬂine reinforcement learning: Fundamental barri- ers for value function approximation. arXiv preprint arXiv:2111.10919, 2021. Fu, J., Norouzi, M., Nachum, O., Tucker, G., Wang, Z., Novikov, A., Yang, M., Zhang, M. R., Chen, Y., Kumar, A., Paduraru, C., Levine, S., and Paine, T. L. Benchmarks for deep off-policy evaluation. ICLR, 2021. URL https: //arxiv.org/abs/2103.16596. Grill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., Piot, B., kavukcuoglu, k., Munos, R., and Valko, M. Bootstrap your own latent - a new approach to self-supervised learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), NeurIPS, volume 33, pp. 21271–21284. Curran As- sociates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ f3ada80d5c4ee70142b17b8192b2958e-Paper. pdf. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor- critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In ICML, pp. 1861–1870. PMLR, 2018. Hafner, D., Lillicrap, T. P., Norouzi, M., and Ba, J. In In- Mastering atari with discrete world models. ternational Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=0oabwyZbOu. Hao, B., Duan, Y., Lattimore, T., Szepesvári, C., and Wang, M. Sparse feature selection makes batch reinforcement learning more sample efﬁcient. In ICML, pp. 4063–4073. PMLR, 2021. He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. B. Mo- mentum contrast for unsupervised visual representation learning. CVPR, 2020. URL http://arxiv.org/ abs/1911.05722. Jiang, N. and Li, L. Doubly robust off-policy value evalua- tion for reinforcement learning. In ICML, pp. 652–661. PMLR, 2016. Du, S. S., Kakade, S. M., Lee, J. D., Lovett, S., Mahajan, G., Sun, W., and Wang, R. Bilinear classes: A struc- tural framework for provable generalization in rl. arXiv preprint arXiv:2103.10897, 2021. Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R. E. Contextual decision processes with low bellman rank are pac-learnable. In ICML, pp. 1704–1713. PMLR, 2017. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. Provably efﬁcient reinforcement learning with linear function ap- proximation. In COLT, pp. 2137–2143. PMLR, 2020. Kostrikov, I. and Nachum, O. Statistical bootstrapping for uncertainty estimation in off-policy evaluation, 2020. URL https://arxiv.org/abs/2007.13609. Laskin, M., Srinivas, A., and Abbeel, P. Curl: Contrastive unsupervised representations for reinforcement learning. In ICML, pp. 5639–5650. PMLR, 2020. Lattimore, T. and Szepesvári, C. Bandit algorithms. Cam- bridge University Press, 2020. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. In ICLR, 2016. Parr, R., Li, L., Taylor, G., Painter-Wakeﬁeld, C., and Littman, M. L. An analysis of linear models, lin- ear value-function approximation, and feature selection In ICML, pp. 752–759, for reinforcement learning. New York, NY, USA, 2008. Association for Comput- doi: 10. ing Machinery. 1145/1390156.1390251. URL https://doi.org/ 10.1145/1390156.1390251. ISBN 9781605582054. Pollard, D. Empirical processes: theory and applications. In NSF-CBMS regional conference series in probability and statistics, pp. i–86. JSTOR, 1990. Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville, A. C., and Bachman, P. Data-efﬁcient rein- forcement learning with momentum predictive represen- tations. ICLR, 2021. URL https://arxiv.org/ abs/2007.05929. Mazoure, B., Tachet des Combes, R., Doan, T. L., Bachman, P., and Hjelm, R. D. Deep reinforcement and infomax learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), NeurIPS, volume 33, pp. 3686–3698. Curran Asso- ciates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ 26588e932c7ccfa1df309280702fe1b5-Paper. pdf. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. A. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop, 2013. URL http://arxiv. org/abs/1312.5602. Modi, A., Chen, J., Krishnamurthy, A., Jiang, N., and Agarwal, A. Model-free representation learning arXiv preprint and exploration in low-rank mdps. arXiv:2102.07035, 2021. Munos, R. and Szepesvári, C. Finite-time bounds for ﬁtted value iteration. JMLR, 9(5), 2008. Nachum, O. and Yang, M. Provable representation learning for imitation with contrastive fourier features. NeurIPS, 2021. URL https://arxiv.org/abs/ 2105.12272. Nedi´c, A. and Bertsekas, D. P. Least squares policy evalua- tion algorithms with linear function approximation. Dis- crete Event Dynamic Systems, 13(1):79–110, 2003. Ni, C., Zhang, A. R., Duan, Y., and Wang, M. Learning good state and action representations via tensor decomposition. In 2021 IEEE International Symposium on Information Theory (ISIT), pp. 1682–1687. IEEE, 2021. learning. reinforcement Song, Z., Parr, R. E., Liao, X., and Carin, L. Linear feature encoding for In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), NIPS, volume 29. Curran Asso- ciates, Inc., 2016. URL https://proceedings. neurips.cc/paper/2016/file/ 8232e119d8f59aa83050a741631803a6-Paper. pdf. Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, arXiv preprint A., et al. Deepmind control suite. arXiv:1801.00690, 2018. Thomas, P. S. and Brunskill, E. Data-efﬁcient off-policy policy evaluation for reinforcement learning, 2016. URL https://arxiv.org/abs/1604.00923. Uehara, M. and Sun, W. Pessimistic model-based ofﬂine reinforcement learning under partial coverage. arXiv preprint arXiv:2107.06226, 2021. Uehara, M., Zhang, X., and Sun, W. Representation learning for online and ofﬂine RL in low-rank MDPs. In ICLR, 2022. URL https://openreview.net/forum? id=J4iSIR9fhY0. van den Oord, A., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv, 2018. URL http://arxiv.org/abs/1807.03748. van der Vaart, A. W. and Wellner, J. A. Weak Con- vergence and Empirical Processes. Springer Se- ISBN ries in Statistics. Springer New York, 1996. 9781475725476. doi: 10.1007/978-1-4757-2545-2. URL http://link.springer.com/10.1007/ 978-1-4757-2545-2. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Wainwright, M. J. High-dimensional statistics: A non- asymptotic viewpoint, volume 48. Cambridge University Press, 2019. Wang, R., Foster, D. P., and Kakade, S. M. What are the statistical limits of ofﬂine RL with linear function approx- imation? ICLR, 2021a. URL https://arxiv.org/ abs/2010.11895. Wang, Y., Wang, R., and Kakade, S. M. An exponential lower bound for linearly-realizable mdps with constant suboptimality gap. arXiv preprint arXiv:2103.12690, 2021b. Yang, M. and Nachum, O. Representation matters: Ofﬂine pretraining for sequential decision making. In Meila, M. and Zhang, T. (eds.), ICML, volume 139 of Proceedings of Machine Learning Research, pp. 11784–11794. PMLR, 2021. URL http://proceedings.mlr.press/ v139/yang21h.html. Yang, M., Nachum, O., Dai, B., Li, L., and Schuurmans, D. Off-policy evaluation via the regularized lagrangian. NeurIPS, 33:6551–6561, 2020. Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. Mastering visual continuous control: Improved data-augmented re- inforcement learning. arXiv preprint arXiv:2107.09645, 2021a. Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R. Improving sample efﬁciency in model- free reinforcement learning from images. AAAI, 2021b. URL http://arxiv.org/abs/1910.01741. Zhang, W., He, J., Zhou, D., Zhang, A., and Gu, Q. Prov- ably efﬁcient representation learning in low-rank markov decision processes. arXiv preprint arXiv:2106.11935, 2021. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Appendices A. Metric Entropy and Entropy Integral We recall the standard notions of entropy integrals here, based on the following distance on Φ, dΦ(φ, (cid:101)φ) =∆ ED (cid:104)(cid:13) (cid:13) (cid:13)φ(s, a) − (cid:101)φ(s, a) (cid:105) (cid:13) (cid:13) (cid:13)2 Let N (t, Φ) denote the t-covering number under dΦ. Deﬁnition A.1. Deﬁne the entropy integral, which we assume to be ﬁnite as, κ(Φ) =∆ (cid:90) 4 0 log1/2 N (t, Φ)dt When Φ is ﬁnite, N (t) ≤ |Φ|, so κ(Φ) ≤ O(log1/2(|Φ|)). B. Technical Lemmas Lemma B.1. Let Xi be i.i.d. random variables s.t. |Xi| ≤ c and E (cid:2)X 2 least 1 − δ, i (cid:3) ≤ ν, then for any δ ∈ (0, 1), we have w.p. at (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 N N (cid:88) i=1 Xi − E [X] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ inf a>0 ν 2a + (c + a) log(2/δ) N , and if ν ≤ LE [X] for some positive L, then in particular, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 N N (cid:88) i=1 (cid:12) (cid:12) Xi − E [X] (cid:12) (cid:12) (cid:12) ≤ 1 2 E [X] + (c + L) log(2/δ) N . Proof. First, by Bernstein’s inequality (Boucheron et al., 2013, Theorem 2.10), we have w.p. 1 − δ, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 N N (cid:88) i=1 (cid:12) (cid:12) Xi − E [X] (cid:12) (cid:12) (cid:12) (cid:114) ≤ 2ν log(2/δ) N + c log(2/δ) N Using the fact that 2xy ≤ x2/a + ay2 for any a > 0, split the square root term, (c + a) log(2/δ) N which yields the ﬁrst part. If ν ≤ LE [X], picking a = L concludes the proof. ≤ inf a>0 ν 2a + We now state several results of Orlicz norms (mostly from Pollard, 1990) for completeness. For an increasing, convex, positive function Φ : R+ (cid:55)→ R+ , such that Φ(x) ∈ [0, 1), deﬁne the Orlicz norm as (cid:107)Z(cid:107)Φ := inf {C > 0 | E [Φ(|Z|/C)] ≤ 1} . It is indeed a norm on the Φ-Orlicz space of random variables LΦ(ν), since it can be interpreted as the Minkowski functional of the convex set K = {X : E [Φ(|X|)] ≤ 1}. Let x1, x2, . . . , xN be i.i.d. datapoints drawn from some underlying distribution. Let ω denote the randomness of the N sampled datapoints, and let Fω = (cid:8)(f (xi(ω)))N i=1 : f ∈ F(cid:9) ⊂ RN denote the (random) set of vectors from the data corresponding to ω. Let σ denote a vector of N i.i.d. Rademacher random variables (±1 equi-probably), independent of all else. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Lemma B.2 (Symmetrization). For any increasing, convex, positive function Φ, we have (cid:34) (cid:32) Eω Φ sup f ∈Fω (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) N (cid:88) i=1 fi − Eωfi (cid:33)(cid:35) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:34) (cid:32) (cid:33)(cid:35) ≤ Eω,σ Φ 2 sup f ∈Fω |σ · f | . Proof. See Theorem 2.2 of (Pollard, 1990). Lemma B.3 (Contraction). Let F ⊂ RN , and suppose λ : RN (cid:55)→ RN s.t. each component λi : R (cid:55)→ R is L-Lipschitz. Then, for any increasing, convex, positive function Φ, we have (cid:34) (cid:32) Eσ Φ sup f ∈F (cid:33)(cid:35) |σ · λ(f )| ≤ (cid:34) (cid:32) (cid:33)(cid:35) Eσ Φ 2L sup f ∈F |σ · f | 3 2 Proof. Apply Theorem 5.7 of (Pollard, 1990) to the functions λi/L, which are contractions. We now focus on the speciﬁc Orlicz space of sub-Gaussian random variables, with the function Ψ(x) = 1 Ψ-Orlicz norm of the maximum of random variables can be bounded by the maximum of the Ψ-Orlicz norms. 5 exp(x2). The Lemma B.4. For any random variables Z1, ..., Zm, we have (cid:107) max i≤m |Zi|(cid:107)Ψ ≤ (cid:112)2 + log(m) max i≤m (cid:107)Zi(cid:107)Ψ Proof. See Lemma 3.2 of (Pollard, 1990). Lemma B.5. For each f ∈ RN , we have (cid:107)σ · f (cid:107)Ψ ≤ 2(cid:107)f (cid:107)2. Proof. See Lemma 3.1 of (Pollard, 1990). The following is a truncated chaining result for Orlicz norms. This result is not new, but many sources state and prove it in terms of covering and Rademacher complexity, rather than for Orlicz norms. In particular, it generalizes Theorem 3.5 of (Pollard, 1990) – consider a sequence of α’s converging to zero. Lemma B.6 (Chaining). Let F ⊂ RN such that 0 ∈ F. Then, (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) sup f ∈F |σ · f | (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Ψ (cid:40) ≤ inf α≥0 3α √ N + 9 (cid:90) b α (cid:112)log(D(δ/2, F))dδ (cid:41) , where b = supf ∈F (cid:107)f (cid:107)2 and D(δ, F) is the Euclidean δ-packing number for F. Proof. Suppose b and all the packing numbers are ﬁnite, otherwise the right hand side is inﬁnite and there is nothing to show. For an arbitrary K > 1, construct a sequence of K ﬁner and ﬁner approximations to F, {0} = F0 ⊂ F1 ⊂ · · · ⊂ FK ⊂ FK+1 = F where for any k ∈ [K], Fk satisﬁes the property that for any f ∈ F, there exist nk(f ) ∈ Fk s.t. (cid:107)nk(f ) − f (cid:107)2 ≤ b2−k. Indeed this can be done iteratively: for any Fk, we can construct Fk+1 by adding elements to construct a maximal b2−k packing (maximality ensures the distance requirement, since the existence of any vector which has larger distance can be added to the packing). By deﬁnition of D(·, F), we have |Fk| ≤ D(b2−k, F). Learning Bellman Complete Representations for Ofﬂine Policy Evaluation For any k ∈ [K], we have by triangle inequality, sup f ∈Fk+1 |σ · f | ≤ sup f ∈Fk+1 |σ · nk(f ))| + sup |σ · (nk(f ) − f )| = sup f ∈Fk |σ · f | + sup f ∈Fk+1 f ∈Fk+1 |σ · (nk(f ) − f )| If k = K, we can loosely bound the right-most term by Cauchy-Schwarz, since for any f ∈ F, we have |σ · (nk(f ) − f )| ≤ √ √ N (cid:107)nk(f ) − f (cid:107)2 ≤ N b2−K. So, we have (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) sup f ∈F |σ · f | (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Ψ ≤ (cid:13) (cid:13) (cid:13) (cid:13) max f ∈FK |σ · f | (cid:13) (cid:13) (cid:13) (cid:13)Ψ + √ N b2−K √ log 5 , since for any non-negative constant c, (cid:107)c(cid:107)Ψ = inf (cid:8)C > 0 : 1 log 5 . If k < K, the suprema are taken over ﬁnite sets, so the maximum is attained. Hence, we can apply a special property of the Ψ-Orlicz norm (Lemma B.4), to get, 5 exp((c/C)2) ≤ 1(cid:9) = c√ (cid:13) (cid:13) (cid:13) (cid:13) max f ∈Fk+1 |σ · f | (cid:13) (cid:13) (cid:13) (cid:13)Ψ By Lemma B.5, ≤ ≤ ≤ ≤ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) max f ∈Fk |σ · f | max f ∈Fk |σ · f | max f ∈Fk |σ · f | max f ∈Fk |σ · f | + (cid:13) (cid:13) (cid:13) (cid:13) max f ∈Fk+1 |σ · (nk(f ) − f ))| (cid:13) (cid:13) (cid:13) (cid:13)Ψ + (cid:112)2 + log(|Fk+1|) max f ∈Fk+1 (cid:107)σ · (nk(f ) − f ))(cid:107)Ψ + 2(cid:112)2 + log(|Fk+1|) max f ∈Fk+1 (cid:107)nk(f ) − f )(cid:107)2 + 2(cid:112)2 + log(|Fk+1|) · b2−k. Also, note that since F0 = {0} by construction, we have (cid:107)maxf ∈F0 |σ · f |(cid:107)Ψ = 0. Unrolling this, we have (cid:107) sup f ⊂F |σ · f |(cid:107)Ψ ≤ √ N b2−K √ log 5 K−1 (cid:88) + 2−k2b (cid:113) 2 + log(D(b2−(k+1), F)) (cid:13) (cid:13) (cid:13) (cid:13)Ψ (cid:13) (cid:13) (cid:13) (cid:13)Ψ (cid:13) (cid:13) (cid:13) (cid:13)Ψ (cid:13) (cid:13) (cid:13) (cid:13)Ψ k=0 √ Since for any D ≥ 2, we have (cid:112)2 + log(1 + D)/ D ≤ 2.2, √ N b2−K √ log 5 √ N b2−K √ log 5 ≤ = Since D(·, F) is a monotone decreasing, √ √ N b2−K √ log 5 N b2−K √ log 5 ≤ = + 4.4b K−1 (cid:88) k=0 2−k (cid:113) log(D(b2−(k+1), F)) K−1 (cid:88) (cid:113) + 17.6b (2−(k+1) − 2−(k+2)) log(D(b2−(k+1), F)) k=0 (cid:90) b/2 + 17.6 b2−(K+1) (cid:112)log(D(δ, F))dδ (cid:90) b + 8.8 b2−K (cid:112)log(D(δ/2, F))dδ. Now consider any α > 0. Pick K such that b 2K+1 ≤ α ≤ b 2K , then we have (cid:107) sup f ⊂F |σ · f |(cid:107)Ψ ≤ √ 2α √ N log 5 + 8.8 (cid:90) b α (cid:112)log(D(δ/2, F))dδ. Since α was arbitrary, the above bound holds when we take an inﬁmum over α. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation C. Proofs for LSPE We ﬁrst show a generalization of the performance difference lemma (PDL). Lemma C.1 (Generalized PDL). For any policies π, π(cid:48), any function f : S × A (cid:55)→ R, and any initial state distribution µ, we have µ − Es∼µ [f (s, π(cid:48))] = V π 1 1 − γ Es,a∼dπ µ (cid:104) T π(cid:48) (cid:105) f (s, a) − f (s, π(cid:48)) (7) Proof. Let T π be the distribution of trajectories τ = (s0, a0, s1, a1, s2, a2, ...) from rolling out π. So, we have, µ − Es∼µ [f (s, π(cid:48))] = Eτ ∼T π V π = Eτ ∼T π = Eτ ∼T π (cid:35) γtr(st, at) − Es∼µ [f (s, π(cid:48))] (cid:35) γt (r(st, at) + f (st, π(cid:48)) − f (st, π(cid:48))) − f (s0, π(cid:48)) γt (r(st, at) + γf (st+1, π(cid:48)) − f (st, π(cid:48))) (cid:35) (cid:34) ∞ (cid:88) t=0 (cid:34) ∞ (cid:88) t=0 (cid:34) ∞ (cid:88) t=0 = = 1 1 − γ 1 1 − γ Es,a∼dπ µ (cid:2)r(s, a) + γEs(cid:48)∼P (s,a) [f (s(cid:48), π(cid:48))] − f (s, π(cid:48))(cid:3) Es,a∼dπ µ (cid:104) T π(cid:48) (cid:105) f (s, a) − f (s, π(cid:48)) . This generalizes the PDL, which we can get by setting f (s, a) = Qπ(cid:48) (s, a): µ − V π(cid:48) V π µ = = = 1 1 − γ 1 1 − γ 1 1 − γ Es,a∼dπ µ Es,a∼dπ µ (cid:104) T π(cid:48) (cid:104) Qπ(cid:48) Qπ(cid:48) (s, a) − Qπ(cid:48) (s, π(cid:48)) (cid:105) (s, a) − V π(cid:48) (s) (cid:105) Es,a∼dπ µ (cid:104) Aπ(cid:48) (cid:105) (s, a) . To prove our LSPE guarantee, we’ll instantiate f (s, a) to be the estimated (cid:98)f (s, a) from LSPE, and set π = π(cid:48). This gives us an expression for the prediction error of LSPE, (cid:12) (cid:12)V π (cid:12) µ − Es∼µ (cid:104) (cid:98)fk(s, π) (cid:105)(cid:12) (cid:12) (cid:12) = 1 1 − γ (cid:12) (cid:12) (cid:12) Edπ µ (cid:104) T π (cid:98)fk(s, a) − fk(s, a) (cid:105)(cid:12) (cid:12) (cid:12) . We then upper bound the right hand side by its L2(dπ of running LSPE by the regression losses at each step. Lemma C.2. Consider any policy π and functions f1, . . . , fK : S × A (cid:55)→ R that satisfy maxk=1,...,K (cid:107)fk − T πfk−1(cid:107)L2(dπ ) ≤ η, and f0(s, a) = 0. Then, for all k = 1, . . . , K, we have (cid:107)fk − T πfk(cid:107)L2(dπ µ) norm, which is the Bellman error. Next, we bound the Bellman error ) ≤ 4 1−γ η + γk/2. p0 p0 Proof. For any k = 1, . . . , K, (cid:107)fk − T πfk(cid:107)L2(dπ p0 ) ≤ (cid:107)fk − T πfk−1(cid:107)L2(dπ ≤ η + γ(Es,a∼dπ p0 ≤ η + γ(Es,a∼dπ p0 ) + (cid:107)T πfk−1 − T πfk(cid:107)L2(dπ p0 ) p0 [(Es(cid:48),a(cid:48)∼P (s,a)◦π [fk−1(s(cid:48), a(cid:48)) − fk(s(cid:48), a(cid:48))])2])1/2 ,s(cid:48),a(cid:48)∼P (s,a)◦π[(fk−1(s(cid:48), a(cid:48)) − fk(s(cid:48), a(cid:48)))2])1/2 Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Since dπ p0 thus non-negative, (s, a) = γE˜s,˜a∼dπ P0 P (s|˜s, ˜a)π(a|s) + (1 − γ)p0(s)π(a|s), and the quantity inside the expectation is a square, and ≤ η + γ(γ−1Es,a∼dπ p0 [(fk−1(s, a) − fk(s, a))2])1/2 √ = η + γ(cid:107)fk−1 − fk(cid:107)L2(dπ p0 ) √ ≤ η + γ (cid:16) η + (cid:107)fk−1 − T πfk−1(cid:107)L2(dπ p0 (cid:17) . ) Unrolling the recursion and using (cid:107)f0 − T πf0(cid:107)L2(dπ p0 ) ≤ 1, we have (cid:107)fK − T πfK(cid:107)L2(dπ p0 ) ≤ η + √ γ(η + √ γ(η + . . . √ γ(η + 1) . . . )) √ γK √ γ 1 − 1 − = η + γK/2, which gives the claim since 1 √ 1− γ ≤ 2(1 − γ)−1 and 1 − √ γK ≤ 1. The following lemma is a “fast rates”-like result for norms. It shows that the norm induced by the empirical covariance matrix (cid:98)Σ can be bounded by the norm induced by two times the population covariance matrix Σ, up to some (cid:101)O(N −1/2) terms. Lemma C.3 (Fast Rates for Σ-norm). For any δ ∈ (0, 1), we have with probability at least 1 − δ, for any x ∈ BW , we have and (cid:107)x(cid:107) (cid:98)Σ ≤ 2(cid:107)x(cid:107)Σ + 5W (cid:114) d log(N/δ) N , (cid:107)x(cid:107)Σ ≤ 2(cid:107)x(cid:107) (cid:98)Σ + 5W (cid:114) d log(N/δ) N . Proof. First, ﬁx any x ∈ BW . Since (x(cid:124)φ(s, a))2 ≤ W 2 almost surely, we have E (cid:2)(x(cid:124)φ(s, a))4(cid:3) ≤ W 2E (cid:2)(x(cid:124)φ(s, a))2(cid:3). So by Lemma B.1, w.p. at least 1 − δ, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 N N (cid:88) i=1 (cid:12) (cid:12) (x(cid:124)φ(si, ai))2 − E (cid:2)(x(cid:124)φ(s, a))2(cid:3) (cid:12) (cid:12) (cid:12) ≤ 1 2 E (cid:2)(x(cid:124)φ(s, a))2(cid:3) + 2W 2 log(2/δ) N . In particular, this means (cid:107)x(cid:107)2 (cid:98)Σ W/ N √ N -net of BW , which can be done with (1 + 2 2 (cid:107)x(cid:107)2 ≤ 3 Σ + 2W 2 log(2/δ) √ and (cid:107)x(cid:107)2 Σ ≤ 2(cid:107)x(cid:107)2 (cid:98)Σ + 4W 2 log(2/δ) N . Now union bound over a N )d elements. The approximation error from this cover is (cid:107)x(cid:107) ≤ (cid:98)Σ ≤ (cid:107)n(x)(cid:107) (cid:114) 3 2 (cid:114) 3 2 (cid:114) 3 2 ≤ ≤ (cid:98)Σ + (cid:107)n(x) − x(cid:107) (cid:98)Σ (cid:115) (cid:107)n(x)(cid:107)Σ + 2W 2 log(2(1 + 2 (cid:114) 3 2 (cid:114) (cid:107)x(cid:107)Σ + (cid:107)n(x) − x(cid:107)Σ + (cid:107)x(cid:107)Σ + 4W d log(N/δ) N , √ N )d/δ) + W √ N N (cid:115) 2W 2 log(2(1 + 2 √ N )d/δ) N + W √ N Learning Bellman Complete Representations for Ofﬂine Policy Evaluation where n(x) is the closest element in the net to x. Similarly, (cid:107)x(cid:107)Σ ≤ (cid:107)n(x)(cid:107)Σ + (cid:107)n(x) − x(cid:107)Σ ≤ 2(cid:107)n(x)(cid:107) (cid:98)Σ + (cid:115) 4W 2 log(2(1 + 2 √ N )d/δ) + N (cid:115) W √ N √ ≤ 2(cid:107)x(cid:107) (cid:98)Σ + 2(cid:107)n(x) − x(cid:107) (cid:114) (cid:98)Σ + + ≤ 2(cid:107)x(cid:107) (cid:98)Σ + 5W d log(N/δ) N . 4W 2 log(2(1 + 2 N )d/δ) + W √ N N Now deﬁne the following notation for analyzing Least Squares Policy Evaluation (LSPE). For every target vector (cid:107)ϑ(cid:107)2 ≤ W , deﬁne yϑ = r + γϑ(cid:124)φ(s(cid:48), π), θϑ = arg min (cid:107)θ(cid:107)≤W Es,a∼ν,s(cid:48)∼P (s,a)[(yϑ − θ(cid:124)φ(s, a))2] := (cid:96)(θ, ϑ), i = ri + γϑ(cid:124)φ(s(cid:48) yϑ (cid:98)θϑ = arg min (cid:107)θ(cid:107)≤W 1 N i, π), N (cid:88) i=1 (yϑ i − θ(cid:124)φ(si, ai))2 := (cid:98)(cid:96)(θ, ϑ). The following lemma are useful facts about the optimal θϑ and (cid:98)θϑ. Lemma C.4. For any ϑ1, ϑ2, we have For any θ, ϑ, we have (cid:107)θϑ1 − θϑ2 (cid:107)Σ ≤ (cid:112)2γW (cid:107)ϑ1 − ϑ2(cid:107)2, (cid:98)Σ ≤ (cid:112)2γW (cid:107)ϑ1 − ϑ2(cid:107)2. (cid:107)(cid:98)θϑ1 − (cid:98)θϑ2 (cid:107) (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) ≥ (cid:107)θ − θϑ(cid:107)2 Σ. recall that θϑ minimizes f (θ) = E (cid:2)(yϑ − θ(cid:124)φ(s, a))2(cid:3), which has the Jacobian ∇f (θ) = Proof. First, 2E (cid:2)(θ(cid:124)φ(s, a) − yϑ)φ(s, a)(cid:3). Since θϑ is optimal over BW , the necessary optimality condition is that −∇f (θϑ) ∈ ϑφ(s, a))(cid:3) , θ − θϑ(cid:105) ≤ 0. In NBW (θϑ), the normal cone of BW at θϑ, i.e. for any θ ∈ BW , we have (cid:104)E (cid:2)φ(s, a)(yϑ − θ particular, we have (cid:124) adding the two we get (cid:104)E (cid:2)φ(s, a)(yϑ1 − θ (cid:104)E (cid:2)φ(s, a)(yϑ2 − θ (cid:124) ϑ1 (cid:124) ϑ2 φ(s, a))(cid:3) , θϑ2 − θϑ1(cid:105) ≤ 0 φ(s, a))(cid:3) , θϑ1 − θϑ2(cid:105) ≤ 0 (cid:107)θϑ1 − θϑ2 (cid:107)2 Σ = (cid:104)E [φ(s, a)(θϑ1 − θϑ2 )(cid:124)φ(s, a))] , θϑ1 − θϑ2(cid:105) ≤ (cid:104)E (cid:2)(yϑ1 − yϑ2)φ(s, a)(cid:3) , θϑ1 − θϑ2(cid:105) = γ(cid:104)E [φ(s, a)φ(s(cid:48), π)(cid:124)] (ϑ1 − ϑ2), θϑ1 − θϑ2(cid:105) ≤ γ(cid:107)E [φ(s, a)φ(s(cid:48), π)(cid:124)] (cid:107)2(cid:107)ϑ1 − ϑ2(cid:107)2(cid:107)θϑ1 − θϑ2 (cid:107)2 ≤ 2γW (cid:107)ϑ1 − ϑ2(cid:107)2. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation The claim with (cid:98)θϑ1, (cid:98)θϑ2 follows by the same argument. For the second claim, we ﬁrst apply the Parallelogram law, followed by the ﬁrst-order optimality of θϑ, (cid:96)(θ, ϑ) = (cid:96)(θϑ, ϑ) + Eν((θϑ − θ)(cid:124)φ(s, a))2 + 2Eν[(yϑ − θ(cid:124)φ(s, a))φ(s, a)(cid:124)(θϑ − θ)] ≥ (cid:96)(θϑ, ϑ) + Eν((θϑ − θ)(cid:124)φ(s, a))2 + 0 = (cid:96)(θϑ, ϑ) + (cid:107)θ − θϑ(cid:107)2 Σ. Now we show our key lemma about the concentration of least squares, uniformly over all targets generated by ϑ ∈ BW . Lemma C.5 (Concentration for Least Squares). Let F = {(s, a) (cid:55)→ θ(cid:124)φ(s, a) : (cid:107)θ(cid:107)2 ≤ W }, with (cid:107)φ(s, a)(cid:107)2 ≤ 1. Then, for any δ ∈ (0, 1) w.p. at least 1 − δ, we have (cid:107)ˆθϑ − θϑ(cid:107)Σ < 120d(1 + W ) sup ϑ∈BW log(N )(cid:112)log(10/δ) √ N . Proof. By Lemma C.3, we have that w.p. at least 1 − δ, we can bound the random (cid:107) · (cid:107) simultaneously over all vectors in a ball, (cid:98)Σ norm by (cid:107) · (cid:107)Σ, and vice versa, E = {∀x ∈ B2W , (cid:107)x(cid:107) (cid:98)Σ ≤ 2(cid:107)x(cid:107)Σ + t and (cid:107)x(cid:107)Σ ≤ 2(cid:107)x(cid:107) (cid:98)Σ + t}, provided t ≥ 5W probability and expectations will be implicitly conditioned on E. (cid:113) d log(N/δ) N . For the remainder of this proof, we condition on this high-probability event. That is, any First, we’ll show that for an arbitrary and ﬁxed ϑ ∈ BW , w.p. at least 1 − δ, we have (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ < t. To do so, we will bound the probability of the complement, which in turn can be simpliﬁed by the following chain of arguments, (cid:107)ˆθϑ − θϑ(cid:107)Σ ≥ t By (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) ≥ (cid:107)θ − θϑ(cid:107)Σ from Lemma C.4, =⇒ (cid:96)(ˆθϑ, ϑ) − (cid:96)(θϑ, ϑ) ≥ t2 =⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) ≥ t2, ˆ(cid:96)(θ, ϑ) − ˆ(cid:96)(θϑ, ϑ) ≤ 0 By convexity and continuity of (cid:96)(·, ϑ), we can make this strict equality. Indeed, given this, by Intermediate Value Theorem, there exists λ ∈ [0, 1] such that θ(cid:48) = (1 − λ)θ + λθϑ has (cid:96)(θ(cid:48), ϑ) − (cid:96)(θϑ, ϑ) = ν. Then by convexity, ˆ(cid:96)(θ(cid:48), ϑ) − ˆ(cid:96)(θϑ, ϑ) ≤ (1 − λ)ˆ(cid:96)(θ, ϑ) − (1 − λ)ˆ(cid:96)(θϑ, ϑ) ≤ 0. =⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) = t2, ˆ(cid:96)(θ, ϑ) − ˆ(cid:96)(θϑ, ϑ) ≤ 0 =⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:107)θ − θϑ(cid:107)Σ ≤ t, ((cid:96)(θ, ϑ) − ˆ(cid:96)(θ, ϑ)) − ((cid:96)(θϑ, ϑ) − ˆ(cid:96)(θϑ, ϑ)) ≥ t2 By conditioning on E, =⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:107)θ − θϑ(cid:107) (cid:98)Σ ≤ 3t, ((cid:96)(θ, ϑ) − ˆ(cid:96)(θ, ϑ)) − ((cid:96)(θϑ, ϑ) − ˆ(cid:96)(θϑ, ϑ)) ≥ t2 Hence, we now focus on bounding (cid:32) P sup θ∈Θ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) N (cid:88) i=1 (cid:12) (cid:12) ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)] (cid:12) (cid:12) (cid:12) (cid:33) ≥ N t2 ≤ δ, Learning Bellman Complete Representations for Ofﬂine Policy Evaluation where we deﬁne ζi(θ, ϑ) = (yϑ i − θ(cid:124)φ(si, ai))2, Θ = {θ : (cid:107)θ(cid:107) ≤ W, (cid:107)θ − θϑ(cid:107) Since ζi(θ, ϑ) − ζi(θϑ, θ) = λi((θ − θϑ)(cid:124)φ(si, ai)) where λi(x) = x(2yϑ and λi(0) = 0, the contraction lemma (Lemma B.3) tells us that it sufﬁces to consider a simpler class. Deﬁne (cid:98)Σ ≤ 3t}. i − (θ + θϑ)(cid:124)φ(si, ai)) is 2(1 + W )-Lipschitz (8) ωi(θ, ϑ) = (θ − θϑ)(cid:124)φ(si, ai) and ω(θ, ϑ) = (ωi(θ, ϑ))N i=1. By Chaining (Lemma B.6), we have (cid:18) 1 J (cid:40) |σ · ω(θ, ϑ)| Eσ Ψ (cid:20) sup Θ √ (cid:90) b where J = inf α≥0 3α N + 9 α (cid:19)(cid:21) ≤ 1 (cid:112)log(D(δ/2, ω(Θ)))dδ (cid:41) , D(δ, F) is the Euclidean packing number of F ⊂ RN , and b = supΘ (cid:107)ω(θ, ϑ)(cid:107)2 is the envelope. ω(Θ) = {ω(θ, ϑ) : θ ∈ Θ} , Now we’ll bound the truncated entropy integral, J. First notice that b ≤ 3t which localizes in (cid:107) · (cid:107) (cid:98)Σ, √ N , based on the deﬁnition of Θ (Equation (8)), b √ N = sup Θ (cid:118) (cid:117) (cid:117) (cid:116) 1 N N (cid:88) i=1 (θ − θϑ)(cid:124)φ(si, ai)φ(si, ai)(cid:124)(θ − θϑ) = sup Θ (cid:107)θ − θϑ(cid:107) (cid:98)Σ ≤ 3t. Now, we bound the packing number D(·, ω(Θ)). Let θ1, θ2 ∈ BW be arbitrary, (cid:107)ω(θ1, ϑ) − ω(θ2, ϑ)(cid:107)2 √ N = (cid:107)θ1 − θ2(cid:107) (cid:98)Σ ≤ (cid:113) σmax((cid:98)Σ)(cid:107)θ1 − θ2(cid:107)2 ≤ (cid:107)θ1 − θ2(cid:107)2. So for any ε, we can construct an ε-cover by setting (cid:107)θ1 − θ2(cid:107)2 ≤ ε/ N (ε, F) denote the Euclidean covering number of F ⊂ RN . Then, √ N , which requires (W (1 + 2 √ N /ε))d points. Let log D(ε, ω(Θ)) ≤ log N (ε/2, ω(Θ)) ≤ d log W (1 + 4 √ N /ε) So, √ N (cid:90) 3t (cid:112)log(D(ε/2, ω(Θ)))dε 0 (cid:90) 3t √ N (cid:113) d log(W (1 + 8 √ N /ε))dε J ≤ ≤ 0 √ ≤ 3t √ ≤ 3t dN ((cid:112)log W + (cid:90) 1 (cid:112)log(1 + 3/(εt)))dε dN ((cid:112)log W + (cid:112)log(4/t)), 0 since (cid:82) 1 0 (cid:112)log(1 + c/ε)dε ≤ (cid:112)log(1 + c) for any c > 0, and assuming t ≤ 1. Now we put everything together. Let c denote a positive constant, (cid:32) P (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) N (cid:88) i=1 (cid:12) (cid:12) ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)] (cid:12) (cid:12) (cid:12) sup Θ (cid:33) ≥ N t2 Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Since Ψ is increasing, (cid:32) (cid:32) = P Ψ c sup Θ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) N (cid:88) i=1 (cid:12) (cid:12) ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)] (cid:12) (cid:12) (cid:12) (cid:33) (cid:33) ≥ Ψ (cid:0)cN t2(cid:1) By Markov’s inequality, (cid:104) E Ψ ≤ (cid:16) c supΘ (cid:12) (cid:12) (cid:12) (cid:80)N (cid:12) i=1 ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)] (cid:12) (cid:12) (cid:17)(cid:105) Ψ (cN t2) By Symmetrization (Lemma B.2), ≤ E [Eσ [Ψ (2c supΘ |σ · (ζ(θ, ϑ) − ζ(θϑ, ϑ))|)]] Ψ (cN t2) By Contraction (Lemma B.3) and that ζi(θ, ϑ) − ζi(θϑ, ϑ) = λi(ωi(θ, ϑ)) where λi is L = 2(1 + W ) Lipschitz, ≤ 3 2 · E [Eσ [Ψ (4cL supΘ |σ · ω(θ, ϑ)|)]] Ψ (cN t2) Setting c = 1 4LJ and applying Chaining (Lemma B.6), the numerator is bounded by 1, (cid:32) ≤ 8 exp − (cid:19)2(cid:33) (cid:18) N t2 4LJ Applying upper bound on J,  (cid:32) ≤ 8 exp − 8(1 + W ) · 3t √ N t2 √ dN ( log W + (cid:112)log(4/t)) (cid:19) (cid:33)2  (cid:18) ≤ 8 exp − N t2 242(1 + W )2d(log W + log(4/t)) Now, we set t = 24(1 + W ) (cid:113) d log(N ) log(1/δ) N , (cid:18) ≤ 8 exp − log(N ) log(1/δ) log(N/d log(N )) (cid:19) Since log(N ) ≥ log(N/d log(N )), ≤ 8δ. Hence, we have shown that for an arbitrary and ﬁxed ϑ ∈ BW , w.p. 1 − 9δ, we have (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ < t = 24(1 + W ) (cid:114) d log(N ) log(1/δ) N . We ﬁnally apply a union bound over ϑ ∈ BW . Consider a W/N -cover of ϑ ∈ BW , which requires (1 + 2N )d points. Then, for any ϑ ∈ BW , we have (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ ≤ (cid:107)(cid:98)θϑ − (cid:98)θn(ϑ)(cid:107)Σ + (cid:107)(cid:98)θn(ϑ) − θn(ϑ)(cid:107)Σ + (cid:107)θn(ϑ) − θϑ(cid:107)Σ (cid:98)Σ + t) + t + (cid:112)2γW (cid:107)n(ϑ) − ϑ(cid:107) ≤ (2(cid:107)(cid:98)θϑ − (cid:98)θn(ϑ)(cid:107) ≤ 2t + 3(cid:112)2γW (cid:107)n(ϑ) − ϑ(cid:107) ≤ 2t + 3(cid:112)2γW 2/N ≤ 5t. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Thus, we have shown that w.p. 1 − δ, sup ϑ∈BW (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ < 120d(1 + W ) log(N )(cid:112)log(10/δ) √ N . A nice corollary is that when Σ provides full coverage, i.e. Σ (cid:23) βI for some positive β, then we can bound sup ϑ∈BW (cid:107)ˆθϑ − θϑ(cid:107)2 ≤ σmin(Σ)−1/2 sup ϑ∈BW (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ ≤ β−1/2 · sup ϑ∈BW (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ. C.1. Main LSPE theorem We now prove our LSPE sample complexity guarantee Theorem 3.3. Theorem 3.3 (Sample Complexity of LSPE). Assume feature φ satisﬁes approximate Linear BC with parameter εν. For any δ ∈ (0, 0.1), w.p. at least 1 − δ, we have for any initial state distribution p0 (cid:12) (cid:12)V πe (cid:12) p0 (cid:104) − Es∼p0 (cid:98)fK(s, πe) (cid:105)(cid:12) (cid:12) (cid:12) ≤ γK/2 1 − γ + (cid:114)(cid:13) (cid:13) 4 (cid:13) ddπe p0 dν (cid:13) (cid:13) (cid:13)∞ (1 − γ)2 εν + 480(cid:112)κ(p0)(1 + W )d log(N )(cid:112)log(10/δ) √ (1 − γ)2 N , where (cid:98)fK is the output of Algorithm 1. Proof of Theorem 3.3. By Lemmas C.1 and C.2, (cid:12) (cid:12)V π (cid:12) p0 (cid:104) − Es∼p0 (cid:98)fk(s, π) (cid:105)(cid:12) (cid:12) (cid:12) ≤ 4 (1 − γ)2 max k=1,2,... (cid:107) ˆfk − T π ˆfk−1(cid:107)L2(dπ p0 ) + γk/2 1 − γ . Next, we bound the maximum regression error. Consider any initial state distribution p0, then we have max k=1,2,... (cid:107) ˆfk − T π ˆfk−1(cid:107)L2(dπ p0 ) ≤ sup (cid:107)ϑ(cid:107)≤W (cid:124) (cid:107)ˆθ ϑφ − T π(ϑ(cid:124)φ)(cid:107)L2(dπ p0 ) ≤ sup (cid:107)ϑ(cid:107)≤W (cid:124) (cid:107)ˆθ ϑφ − θ (cid:124) ϑφ(cid:107)L2(dπ p0 (cid:107)θ (cid:124) ϑφ − T π(ϑ(cid:124)φ)(cid:107)L2(dπ p0 ) ≤ (cid:112)κ(p0) sup (cid:107)ϑ(cid:107)≤W (cid:107)ˆθϑ − θϑ(cid:107)Σ + sup (cid:107)ϑ(cid:107)≤W (cid:107)θ (cid:124) ϑφ − T π(ϑ(cid:124)φ)(cid:107)L2(ν) ) + sup (cid:107)ϑ(cid:107)≤W (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) ddπ p0 dν ddπ p0 dν (cid:13) (cid:13) (cid:13) (cid:13)∞ (cid:13) (cid:13) (cid:13) (cid:13)∞ εν. ≤ (cid:112)κ(p0) sup ϑ∈BW (cid:107)ˆθϑ − θϑ(cid:107)Σ + The quantity supϑ∈BW (cid:107)ˆθϑ − θϑ(cid:107)Σ can be directly bounded by Lemma C.5 w.p. at least 1 − δ. Thus, we have shown the desired result: for any initial state distribution p0, (cid:12) (cid:12)V π (cid:12) p0 − Es,a∼p0◦π (cid:104) (cid:98)fk(s, a) (cid:105)(cid:12) (cid:12) (cid:12) ≤ 4 (1 − γ)2 (cid:32) (cid:112)κ(p0)120d(1 + W ) log(N )(cid:112)log(10/δ) √ N + (cid:33) (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) ddπ p0 dν (cid:13) (cid:13) (cid:13) (cid:13)∞ εν + γk/2 1 − γ . Learning Bellman Complete Representations for Ofﬂine Policy Evaluation D. Proofs for Linear BC Equivalence Proposition 4.2. Consider a feature φ with full rank covariance Σ(φ). Given any W > 0, the feature φ being linear BC (under BW ) implies that there exist (ρ, M ) ∈ BW × Rd×d with (cid:107)M (cid:107)2 ≤ 1 − (cid:107)ρ(cid:107)2 (cid:113) W 2 , and Eν (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) φ(s, a) − (cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 = 0. (2) On the other hand, if there exists (ρ, M ) ∈ BW × Rd×d with (cid:107)M (cid:107)2 < 1 such that the above equality holds, then φ must satisfy exact linear BC with W ≥ (cid:107)ρ(cid:107)2 . 1−(cid:107)M (cid:107)2 Proof. (⇐=) Suppose that (cid:107)M (cid:107)2 < 1 and ρ ∈ BW satisfy, Eν (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) φ(s, a) − (cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 = 0. Then, for any w1 ∈ BW , setting w2 = ρ + M (cid:124)w1 satisﬁes, 2 φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w (cid:124) 1 φ(s(cid:48), πe)](cid:1)2 (cid:107)w (cid:124) 2 φ − T πe (w (cid:124) 1 φ)(cid:107)2 ν = Eν = Eν = Eν = 0. (cid:124) (cid:0)w (cid:0)ρ(cid:124)φ(s, a) − r(s, a) + w (cid:0)w (cid:124) 1 (cid:0)M φ(s, a) − γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)](cid:1)(cid:1)2 (cid:124) 1 M φ(s, a) − γEs(cid:48)∼P (s,a) [w (cid:124) 1 φ(s(cid:48), πe)](cid:1)2 Also, we have (cid:107)w2(cid:107)2 ≤ (cid:107)ρ(cid:107)2 + (cid:107)M (cid:107)2(cid:107)w1(cid:107)2 ≤ W since W ≥ (cid:107)ρ(cid:107)2 1−(cid:107)M (cid:107)2 . Thus, φ satisﬁes exact Linear BC. (=⇒) Suppose φ satisﬁes exact Linear BC, that is max w1∈BW min w2∈BW (cid:107)w (cid:124) 2 φ − T π(w (cid:124) 1 φ)(cid:107)2 ν = 0. To see that there exists ρ ∈ BW that linearizes the reward w.r.t φ under ν, set w1 = 0, and we have: min w2∈BW Es,a∼ν (cid:13) (cid:13)w(cid:62) 2 φ(s, a) − r(s, a)(cid:13) 2 2 = 0. (cid:13) Let ρ to be the minimizer of the above objective. Now we need to show that there exists a M ∈ Rd×d with (cid:107)M (cid:107)2 < (cid:113) 1 − (cid:107)ρ(cid:107)2 W 2 that satisﬁes Es,a∼ν (cid:13) (cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13) 2 2 = 0. (cid:13) To extract the i-th row of M , plug in wi = W ei (note that wi ∈ BW ). By exact Linear BC, we know that there exists a vector vi ∈ BW , such that: (cid:13) (cid:13)v (cid:124) i φ(s, a) − ρ(cid:124)φ(s, a) − γW Es(cid:48)∼P (s,a)e Repeating this for every i ∈ [d], we can construct M as follows, M = 1 W  (v1 − ρ)(cid:124) ...   (vd − ρ)(cid:124)    , (cid:124) i φ(s(cid:48), πe)(cid:13) (cid:13)ν = 0. which satisﬁes, Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Es,a∼ν d (cid:88) = i=1 d (cid:88) i=1 = (cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13) (cid:13) 2 (cid:13) 2 Es,a∼ν (cid:13) (cid:13)e (cid:124) i (cid:0)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:1)(cid:13) 2 (cid:13) 2 Es,a∼ν (cid:13) (cid:13) (cid:13) (cid:13) 1 W (vi − ρ)(cid:124)φ(s, a) − γEs(cid:48)∼P (s,a)e (cid:13) 2 (cid:13) (cid:124) i φ(s(cid:48), πe) (cid:13) (cid:13) 2 = 0. Hence, we have (cid:13) (cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13) 2 ν = 0. (cid:13) (cid:113) 1 − (cid:107)ρ(cid:107)2 W 2 . First we show that (cid:107)M (cid:107)2 ≤ 1. For any w1 ∈ BW , by exact linear Finally, we must show that (cid:107)M (cid:107)2 < BC, there exists w2 ∈ BW s.t. (cid:13) 1 φ(s(cid:48), πe)](cid:13) 2 ν = 0, and by the construction of M , (cid:13)w (cid:13) φ(s, a)(cid:107)2 satisﬁes (cid:107)(w2 − ρ − M (cid:124)w1) Σ(φ) = 0. Since Σ(φ) is positive deﬁnite, we have that w2 = ρ + M (cid:124)w1 is the unique choice of w2, which by exact linear BC is in BW . Hence, we’ve shown that for any w1 ∈ BW , we also have that ρ + M (cid:124)w1 ∈ BW . Now take w1 and −w1, subtracting the two expressions yields that 2M (cid:63)(cid:124)w1 ∈ B2W . Since this is true for arbitrary w1, taking supremum over w1 ∈ BW shows that (cid:107)M (cid:107)2 ≤ 1. 2 φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w ν = (cid:107)w2 − ρ − M (cid:124)w1(cid:107)2 (cid:124) (cid:124) (cid:124) (cid:124) Now we show that the inequality must be strict. Consider the singular value decomposition: M = (cid:80)d i where {ui}i∈[d] and {vi}i∈[d] are each an orthonormal basis of Rd, and σi is the i-th largest singular value. Without loss of generality, suppose ρ(cid:124)u1 ≥ 0, since we can always ﬂip the sign of u1. If we pick x = W v1 ∈ BW , we have M x = W σ1u1. By the argument in the previous paragraph, since x ∈ BW , we have ρ + M x ∈ BW , implying that i=1 σiuiv W 2 ≥ (cid:107)M x + ρ(cid:107)2 2 = (cid:107)W σ1u1 + (ρ(cid:124)u1)u1 + (ρ − (ρ(cid:124)u1)u1)(cid:107)2 2 Since u1 and (ρ − (ρ(cid:124)u1)u1) are orthogonal, by Pythagoras, we have = |W σ1 + ρ(cid:124)u1|2 + (cid:107)(ρ − (ρ(cid:124)u1)u1)(cid:107)2 2 = (W σ1)2 + 2W σ1ρ(cid:124)u1 + (ρ(cid:124)u1)2 + (cid:107)(ρ − (ρ(cid:124)u1)u1)(cid:107)2 2 = (W σ1)2 + 2W σ1ρ(cid:124)u1 + (cid:107)(ρ(cid:124)u1)u1(cid:107)2 2 + (cid:107)(ρ − (ρ(cid:124)u1)u1)(cid:107)2 2 By Pythagoras, = (W σ1)2 + 2W σ1ρ(cid:124)u1 + (cid:107)ρ(cid:107)2 2 . Hence, we get the following inequality: Solve for σ1 and using the fact that ρ(cid:124)u1 ≥ 0, we have that, W 2σ2 1 + 2W (ρ(cid:124)u1)σ1 + (cid:107)ρ2(cid:107)2 − W 2 ≤ 0. σ1 ≤ −ρ(cid:124)u1 + (cid:112)(ρ(cid:124)u1)2 + (W 2 − (cid:107)ρ(cid:107)2) W (cid:112)W 2 − (cid:107)ρ(cid:107)2 W ≤ (cid:114) ≤ 1 − (cid:107)ρ(cid:107)2 W 2 . We ﬁnally show that (cid:107)ρ(cid:107)2 < W unless M = 0. We prove this by contradiction. Assume (cid:107)ρ(cid:107)2 ≥ W . Following the above argument, take any w1 ∈ BW , we must have w2 := ρ + M (cid:124)w1 ∈ BW . We discuss two cases. First if ρ (cid:54)∈ range(M (cid:124)). In this case, we must have (cid:107)w2(cid:107)2 has non-zero entries. Thus, this case leads to contradiction . 2 = (cid:107)ρ(cid:107)2 2 + (cid:107)M (cid:124)w1(cid:107)2 2 = W 2 + (cid:107)M (cid:124)w1(cid:107)2 2 > W 2, as long as M Second, if ρ ∈ range(M (cid:124)). In this case, there must exist a vector x (cid:54)= 0 such that M (cid:124)x = ρ. Consider ¯x := W x (cid:107)x(cid:107)2 We have w2 := ρ + M (cid:124) ¯x = ρ , which means that (cid:107)w2(cid:107)2 > W , which causes contradiction again. (cid:17) (cid:16) 1 + W (cid:107)x(cid:107)2 ∈ BW . So unless M = 0, which only happens when γ = 0 (i.e. horizon is 1), we have (cid:107)ρ(cid:107)2 < W . Learning Bellman Complete Representations for Ofﬂine Policy Evaluation (cid:21) Eν (ρ,M )∈Θ Llbc(φ) = min We now show an approximate version to the equivalence of Proposition 4.2. First, recall the bilevel loss from Equation (3). We use Llbc and (cid:98)Llbc to denote the population and empirical versions as follows, (cid:13) (cid:20)M (cid:20)γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)] (cid:13) (cid:13) ρ(cid:124) r(s, a) (cid:13) (cid:21)(cid:13) (cid:13) (cid:20)M 2 (cid:13) (cid:13) (cid:13) (cid:13) ρ(cid:124) (cid:13) (cid:13) 2 where Θ = (cid:8)(ρ, M ) ∈ BW × Rd×d : (cid:107)ρ(cid:107) ≤ (cid:107)ρ(cid:63)(cid:107), (cid:107)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2 (cid:9) and (ρ(cid:63), M (cid:63)) are corresponding to the linear BC φ(cid:63). Lemma D.1. Suppose a feature (cid:98)φ satisﬁes Llbc( (cid:98)φ) ≤ ε2. Then, (cid:98)φ is ε(1 + W )-approximately Linear BC, provided W ≥ (cid:107)ρ(cid:63)(cid:107)2 ED (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2 2 , (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:98)Llbc(φ) = min (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 − min g∈G φ(s, a) − φ(s, a) − (ρ,M )∈Θ (10) ED (9) (cid:21) . 1−(cid:107)M (cid:63)(cid:107)2 Proof. Suppose Llbc( (cid:98)φ) ≤ ε2, so there exists (cid:99)M (s.t. (cid:107) (cid:99)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2 < 1) and (cid:98)ρ ∈ BW s.t. (cid:20) (cid:21) (cid:99)M (cid:98)ρ(cid:124) (cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 φ(s, a) − Es,a∼ν ≤ ε2 (cid:13) (cid:13) (cid:13) (cid:13) For any w1 ∈ BW , we can take w2 = (cid:98)ρ + (cid:99)M (cid:124)w1. Then, (cid:107)w2(cid:107)2 ≤ (cid:107)(cid:98)ρ(cid:107)2 + (cid:107) (cid:99)M (cid:107)2W ≤ (cid:107)ρ(cid:63)(cid:107)2 + (cid:107)M (cid:63)(cid:107)2W ≤ W by our assumption on W . Hence, max w1∈BW ≤ max w1∈BW (cid:124) (cid:13) (cid:13)w 2 φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w min w2∈BW (cid:13) (cid:13)((cid:98)ρ + (cid:99)M (cid:124)w1)(cid:124)φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w (cid:13) (cid:115) (cid:124) 1 φ(s(cid:48), πe)](cid:13) (cid:13)ν (cid:13) (cid:124) 1 φ(s(cid:48), πe)] (cid:13) (cid:13)ν ((cid:98)ρ + (cid:99)M (cid:124)w1)(cid:124)φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w (cid:115) (cid:20)(cid:16) (cid:124) ((cid:98)ρ(cid:124)φ(s, a) − r(s, a))2(cid:105) + Es,a∼ν w 1 ( (cid:99)M φ(s, a) − γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)]) (cid:17)2(cid:21) (cid:17)2(cid:21) (cid:124) 1 φ(s(cid:48), πe)] = max w1∈BW Es,a∼ν (cid:20)(cid:16) (cid:114) Es,a∼ν (cid:104) ≤ max w1∈BW ≤ ε(1 + W ), as desired. E. Proofs for Representation Learning To simplify analysis, assume that the functions in G have bounded (cid:96)2 norm, i.e. ∀g ∈ G, s, a ∈ S × A, (cid:107)g(s, a)(cid:107)2 ≤ γ. This is reasonable, and can always be achieved by clipping without loss of accuracy, since the target for g(s, a) is γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)] and (cid:107)φ(s, a)(cid:107)2 ≤ 1 for any s, a ∈ S × A. E.1. Lemmas Recall that λk(A) denotes the k-th largest eigenvalue of a matrix A, i.e. λ1(A), λn(A) give the largest and smallest eigenvalues respectively. Lemma E.1 (Weyl’s Perturbation Theorem). Let A, B ∈ Cn×n be Hermitian matrices. Then max k |λk(A) − λk(B)| ≤ (cid:107)A − B(cid:107)2 . Proof. Please see (Bhatia, 2013, Corollary III.2.6). Learning Bellman Complete Representations for Ofﬂine Policy Evaluation We extend this to be uniform over all Φ. Lemma E.2 (Uniform spectrum concentration). For any δ ∈ (0, 1), w.p. 1 − δ, sup φ∈Φ,k∈[d] (cid:12) (cid:12) (cid:12)λk(Σ(φ)) − λk((cid:98)Σ(φ)) (cid:12) (cid:12) (cid:12) ≤ 1 √ N (cid:16) (cid:17) 96κ(Φ) + 4d + 4 log1/2(1/δ) Proof. First observe that, sup φ∈Φ sup k (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ sup (cid:12)λk(Σ(φ)) − λk((cid:98)Σ(φ)) φ∈Φ (cid:13) (cid:13) (cid:13)Σ(φ) − (cid:98)Σ(φ) (cid:13) (cid:13) (cid:13)2 x(cid:124)(Σ(φ) − (cid:98)Σ(φ))x (Eν − ED)(x(cid:124)φ(s, a))2 = = sup φ∈Φ,(cid:107)x(cid:107)2≤1 sup φ∈Φ,(cid:107)x(cid:107)2≤1 Now we want to bound the Rademacher complexity of the class F = (cid:8)(s, a) (cid:55)→ (x(cid:124)φ(s, a))2, φ ∈ Φ, (cid:107)x(cid:107)2 ≤ 1(cid:9) First, to bound the envelope, we have (x(cid:124)φ(s, a))2 ≤ 1. To cover, consider any φ ∈ Φ and x ∈ Rd s.t. (cid:107)x(cid:107)2 ≤ 1. Pick (cid:101)φ, (cid:101)x close to φ, x, so that (cid:118) (cid:117) (cid:117) (cid:116) 1 N N (cid:88) (cid:16) i=1 (x(cid:124)φ(si, ai))2 − ((cid:101)x(cid:124) (cid:101)φ(si, ai))2 (cid:17)2 (cid:118) (cid:117) (cid:117) (cid:116) ≤ 2 1 N N (cid:88) (cid:16) i=1 (x(cid:124)φ(si, ai) − (cid:101)x(cid:124) (cid:101)φ(si, ai)) (cid:17)2  (cid:118) (cid:117) (cid:117) (cid:116) ≤ 2  1 N N (cid:88) (cid:16) i=1 x(cid:124)(φ(si, ai) − (cid:101)φ(si, ai)) (cid:17)2 + (cid:118) (cid:117) (cid:117) (cid:116) 1 N N (cid:88) (cid:16) i=1 (x − (cid:101)x)(cid:124) (cid:101)φ(si, ai) (cid:17)2   (cid:16) ≤ 2 dΦ(φ, (cid:101)φ) + (cid:107)x − (cid:101)x(cid:107)2 (cid:17) So it sufﬁces to take dΦ(φ, (cid:101)φ), (cid:107)x − (cid:101)x(cid:107)2 ≤ t/4 to t-cover F in L2(D). Note the t/4-covering number for x in the unit ball is (1 + 8/t)d. Thus, by Dudley’s entropy bound ((5.48) of (Wainwright, 2019)), RN (F) ≤ ≤ 24 √ N 96 √ N (cid:90) 1 log1/2(N (t/4, Φ) · (1 + 8/t)d)dt 0 (cid:16) κ(Φ) + 4 √ (cid:17) d Thus, by Theorem 4.10 of (Wainwright, 2019), w.p. 1 − δ, sup φ∈Φ,(cid:107)x(cid:107)2≤1 (Eν − ED)(x(cid:124)φ(s, a))2 ≤ 2RN (F) + 4 log1/2(1/δ) √ N √ ≤ (cid:16) 1 √ N 96κ(Φ) + 4 d + 4 log1/2(1/δ) (cid:17) We now prove the double sampling lemma, i.e. modiﬁed Bellman Residual Minimization (Chen & Jiang, 2019). This will help deal with the double sampling issue when transitions are stochastic. Recall that G is a function class of functions g : X (cid:55)→ Rd. Let ν be a distribution over X ⊂ Rd and, for any x ∈ X , let P (x) be a distribution over Y ⊂ Rd. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Lemma E.3 (Double Sampling). Suppose x (cid:55)→ Ey∼P (x) [y] ∈ G. Then, Ex∼ν (cid:104)(cid:13) (cid:13)x − Ey∼P (x) [y](cid:13) 2 (cid:13) 2 (cid:105) = Ex∼ν,y∼P (x) (cid:104) (cid:107)x − y(cid:107)2 2 (cid:105) − inf g∈G Ex∼ν,y∼P (x) (cid:104) (cid:107)g(x) − y(cid:107)2 2 (cid:105) Proof. Ex∼ν,y∼P (x) (cid:104) (cid:105) − Ex∼ν (cid:107)x − y(cid:107)2 2 (cid:104) (cid:105) (cid:104)(cid:13) (cid:13)x − Ey∼P (x) [y](cid:13) 2 (cid:13) 2 (cid:105) 2 − 2 (cid:10)x, Ey∼P (x) [y](cid:11) + (cid:13) (cid:107)x(cid:107)2 (cid:16) 2 (cid:13)Ey∼P (x) [y](cid:13) 2 (cid:13) 2 (cid:17)(cid:105) (cid:104) (cid:104) (cid:104) = Ex∼ν = Ex∼ν = Ex∼ν Ey∼P (x) Ey∼P (x) Ey∼P (x) (cid:104) (cid:107)x(cid:107)2 2 − 2 (cid:104)x, y(cid:105) + (cid:107)y(cid:107)2 (cid:105) (cid:13)Ey∼P (x) [y](cid:13) 2 (cid:13) 2 (cid:105)(cid:105) − (cid:13) (cid:107)y(cid:107)2 2 (cid:104)(cid:13) (cid:13)y − Ey∼P (x) [y](cid:13) 2 (cid:13) 2 − (cid:105) where the last step uses the fact that (cid:13) observe that, assuming g(cid:63)(x) (cid:55)→ Ey∼P (x) [y] ∈ G, we have that it is the minimizer of, (cid:13)Ey∼P (x) [y](cid:13) 2 2 = Ey∼P (x) (cid:13) (cid:2)(cid:10)y, Ey∼P (x) [y](cid:11)(cid:3), and completing the square. Now, g(cid:63) ∈ arg min g∈G Ex∼ν,y∼P (x) (cid:104) (cid:107)g(x) − y(cid:107)2 2 (cid:105) (11) which completes the proof. E.2. Concentration lemmas For any φ, deﬁne the optimal ρ, M, g for our losses as follows: ρφ ∈ arg min (ρ,_)∈Θ Eν (cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3) Mφ ∈ arg min (_,M )∈Θ Eν◦P (cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), πe)(cid:107)2 2 (cid:3) gφ ∈ arg min g∈G Eν◦P (cid:2)(cid:107)g(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:3) Similarly, deﬁne (cid:98)ρφ, (cid:99)Mφ, (cid:98)gφ to be minimizers of the above losses when expectation is taken over the empirical distribution D, instead of the population distribution ν. Observe that the unconstrained minimization yields a closed form solution for gφ as gφ(s, a) = γEs(cid:48)∼P (s,a) [φ(s(cid:48), π)] – Assumption 4.3 posits that G is rich enough to capture this. The key property of our squared losses is that the second moment can be upper bounded by the expectation, which allows us to invoke the second part of the above Lemma B.1. We now combine this with covering to get uniform convergence results. Lemma E.4. For any δ ∈ (0, 1), w.p. at least 1 − δ, for any φ ∈ Φ, ρ ∈ BW and M ∈ Rd×d with (cid:107)M (cid:107)2 ≤ 1, we have (cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3) − Eν (cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 (cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3)(cid:12) 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 1 2 Eν (cid:12) ≤ (cid:3) − Eν◦P (cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3) + ερ, (cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:3)(cid:12) (cid:12) Eν◦P (cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:3) + εM , (cid:12) (cid:12)ED (cid:12) (cid:12)ED 1 2 ≤ and, assuming realizability (Assumption 4.3), for every g ∈ G, we have (cid:12) (cid:12)ED 1 2 ≤ (cid:2)(cid:107)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) − Eν (cid:2)(cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 (cid:3)(cid:12) (cid:12) Eν (cid:2)(cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 (cid:3) + εg, where ερ, εM , ρg are deﬁned below. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Finite function classes Assuming Φ and G are ﬁnite, we have ερ ≤ 6d(1 + W )2 log(4W |Φ| N/δ) N εM ≤ 32d2 log(4 |Φ| N/δ) N εg ≤ 20γ2 log(2 |G| /δ) N . Proof for ερ. For a ﬁxed φ ∈ Φ, ρ ∈ BW , apply Lemma B.1 to Xi = (ρ(cid:124)φ(si, ai) − r(si, ai))2. The envelope is (cid:3) ≤ (1 + W )2E [Xi]. So, the error from the lemma is |Xi| ≤ (1 + W )2 and the second moment is bounded E (cid:2)X 2 (cid:12)(ρ(cid:124)φ(s, a) − r(s, a))2 − ((cid:101)ρ(cid:124)φ(s, a) − r(s, a))2(cid:12) 2(1+W )2 log(2/δ) (cid:12) = N |(ρ − (cid:101)ρ)(cid:124)φ(s, a)((ρ + (cid:101)ρ)(cid:124)φ(s, a) + 2r(s, a))| ≤ 2(1 + W )(cid:107)(cid:101)ρ − ρ(cid:107)2, we consider a 1 N -net of BW which requires (1 + 2W N )d points. The error from this ε-net approximation is at most 4(1+W ) . Now union bound over an ε-net of BW . Since (cid:12) . Finally, union bound over Φ. i N Proof for εM . For a ﬁxed φ ∈ Φ and M ∈ Rd×d s.t. (cid:107)M (cid:107)2 < 1, apply Lemma B.1 to Xi = (cid:107)a(cid:107)2 a = M φ(si, ai)−γφ(s(cid:48) Further, observe that (cid:107)a(cid:107)2 E (cid:2)(cid:107)a + b(cid:107)2 2 − (cid:107)b(cid:107)2 2 where 2 ≤ (1+γ)2+(2γ)2 ≤ 8. (cid:3) ≤ (cid:3) ≤ 16E [Xi], where we used Lemma E.3 to give us 2 = (cid:104)a + b, a − b(cid:105) ≤ (cid:107)a + b(cid:107)2(cid:107)a − b(cid:107)2. So, the second moment is bounded E (cid:2)X 2 (cid:3) ≤ (1 + 3γ)2E (cid:2)(cid:107)M φ(si, ai) − γgφ(si, ai)(cid:107)2 i, π). The envelope is |Xi| = |Xi| ≤ (cid:107)a(cid:107)2 i, π), b = gφ(si, ai)−γφ(s(cid:48) 2 − (cid:107)b(cid:107)2 2+(cid:107)b(cid:107)2 2(cid:107)a − b(cid:107)2 2 2 i E (cid:2)(cid:107)M φ(si, ai) − γgφ(si, ai)(cid:107)2 2 (cid:3) = E (cid:2)(cid:107)M φ(si, ai) − γφ(s(cid:48) i, π)(cid:107)2 2 − (cid:107)gφ(si, ai) − γφ(s(cid:48) i, π)(cid:107)2 2 (cid:3) . So, the error from the lemma is 24 log(2/δ) N . Now union bound over an ε-net of (cid:8)M ∈ Rd×d : (cid:107)M (cid:107)2 ≤ 1(cid:9). Observe that (cid:12) (cid:0)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 (cid:12) (cid:12) 2 (cid:13) (cid:13)M φ(si, ai) + (cid:102)M φ(si, ai) − 2γφ(s(cid:48) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13)(M − (cid:102)M )φ(si, ai) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)(M − (cid:102)M ) (cid:13)2 (cid:13) (cid:13) (cid:13)(M − (cid:102)M ) · (2 + 2γ) ≤ 4 (cid:13) (cid:13) (cid:13)2 (cid:1) − ≤ ≤ (cid:16) . (cid:13) (cid:13) i, π) (cid:13)2 (cid:107) (cid:102)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:17)(cid:12) (cid:12) (cid:12) N -net (under (cid:107)(cid:107)F ) for {M ∈ Rd×d : (cid:107)M (cid:107)F ≤ Consider a 1 like the (cid:96)2 for a d2-dimensional vector. This is a 1 (cid:107)M (cid:107)2 ≤ (cid:107)M (cid:107)F ≤ √ d(cid:107)M (cid:107)2. The error from this ε-net approximation is at most 8 points since it is N -net (under (cid:107)(cid:107)2) for the subset (cid:8)M ∈ Rd×d : (cid:107)M (cid:107)2 ≤ 1(cid:9) since d}, which requires (1 + 2N N . Finally, union bound over Φ. d)d2 √ √ Proof for εg. For a ﬁxed g ∈ G, apply Lemma B.1 to Xi = (cid:107)g(si, ai) − γφ(cid:63)(s(cid:48) excess regression loss. Under realizability Assumption 4.3, we have E [Xi] = (cid:107)g − gφ(cid:63) (cid:107)2 i, π)(cid:107)2 2, since 2 − (cid:107)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2, the E [Xi] = E = E (cid:104) (cid:107)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 (cid:104) (cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 − (cid:107)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:105) 2 + 2(cid:104)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π), g(s, a) − gφ(cid:63) (s, a)(cid:105) (cid:105) By deﬁnition of gφ(cid:63) , we have Es(cid:48)∼P (s,a) [gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)] = 0, so, = E (cid:104) (cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 (cid:105) is envelope |Xi| The E (cid:2)(cid:107)g(s, a) + gφ(cid:63) (s, a) − 2γEa(cid:48)∼π(s(cid:48)) [φ(cid:63)(s(cid:48), a(cid:48))] (cid:107)2 lemma is 20γ2 log(2/δ) . Now union bound over G. ≤ (2γ)2 N the and 2(cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 second moment ≤ bounded (cid:3) ≤ (4γ)2E [Xi]. So the error term from the is i E (cid:2)X 2 (cid:3) Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Inﬁnite function classes When Φ and G are inﬁnite, we need to assume some metric entropy conditions. Then in the ﬁnal step of the ﬁnite-class proofs above, we union bound on a well-chosen ε-net and collect an additional approximation error which is on the order of O(1/N ). Assumption E.5. For F ∈ {Φ, G}, we assume there exists p ∈ R++ such that N (t, F) (cid:46) t−p, where the net is under the following distances, dΦ(φ, (cid:101)φ) =∆ ED + ED (cid:105) + Eν (cid:105) (cid:13) (cid:104)(cid:13) (cid:13) (cid:13) (cid:13)φ(s, a) − (cid:101)φ(s, a) (cid:13)2 (cid:13) (cid:104)(cid:13) + Es,a∼D,s(cid:48)∼P (s,a) (cid:13)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π) (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:104)(cid:13) (cid:13)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π) (cid:13) (cid:13) (cid:13)2 (cid:114) (cid:105) (cid:105) (cid:104)(cid:13) (cid:13) (cid:13)φ(s, a) − (cid:101)φ(s, a) (cid:104)(cid:13) (cid:13)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:107)g(si, ai) − (cid:101)g(si, ai)(cid:107)2 2 (cid:105) + (cid:104) Eν (cid:107)g(s, a) − (cid:101)g(s, a)(cid:107)2 2 (cid:105) (cid:105) (cid:13) (cid:13) (cid:13)2 + Es,a∼ν,s(cid:48)∼P (s,a) (cid:114) dG(g, (cid:101)g) =∆ (cid:104) ED Note that this assumption is automatically satisﬁed for any p > 0 by VC classes (van der Vaart & Wellner, 1996, Theorem 2.6.4). Under this assumption, we have N ερ (cid:46) d(1 + W )2(1 + p) log(W N/δ) εM (cid:46) d2(1 + p) log(N/δ) N εg (cid:46) γ2p log(N/δ) . N Proof for ερ. From before, we showed for a ﬁxed φ ∈ Φ, we have ερ (cid:46) d(1+W )2 log(W N/δ) . Observe that (cid:12) (cid:12) (cid:101)φ(s, a) − r(s, a))2(cid:12) (cid:12) (cid:12)ρ(cid:124)(φ(s, a) − (cid:101)φ(s, a))(ρ(cid:124)(φ(s, a) + (cid:101)φ(s, a)) + 2r(s, a)) (cid:12)(ρ(cid:124)φ(s, a) − r(s, a))2 − (ρ(cid:124) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ 2W (1+ (cid:12) = W )(cid:107)φ(s, a) − (cid:101)φ(s, a)(cid:107)2, and so the difference of the loss with φ and the loss with (cid:101)φ is bounded by 2W (1 + W )dΦ(φ, (cid:101)φ). Now union bound over a 1 N -net, which requires O(N p) points by Assumption E.5. The approximation error using the net is 2W (1+W ) N N . Proof for εM . From before, we showed for a ﬁxed φ ∈ Φ, we have εM (cid:46) d2 log(N/δ) N . Observe that (cid:12) (cid:0)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 (cid:12) (cid:12) ≤ (cid:107)M (φ(s, a) − (cid:101)φ(s, a)) − γ(φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π))(cid:107)(cid:107)M (φ(s, a) + (cid:101)φ(s, a)) − γ(φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π))(cid:107) + (cid:107)gφ(s, a) − g 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:107)M (cid:101)φ(s, a) − γ (cid:101)φ(s(cid:48), π)(cid:107)2 (cid:101)φ(s, a) − γ(φ(s(cid:48), π) + (cid:101)φ(s(cid:48), π))(cid:107) 2 − (cid:107)g (cid:1) − (cid:16) (cid:101)φ(s, a) − γ (cid:101)φ(s(cid:48), π)(cid:107)2 2 (cid:17)(cid:12) (cid:12) (cid:12) (cid:17) (cid:107)φ(s, a) − (cid:101)φ(s, a)(cid:107) + γ(cid:107)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π)(cid:107) (cid:101)φ(s, a) − γ(φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π))(cid:107)(cid:107)gφ(s, a) + g · (2 + 2γ) (cid:17) (cid:101)φ(s, a)(cid:107) + γ(cid:107)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π)(cid:107) (cid:107)gφ(s, a) − g · (2 + 2γ) (cid:16) (cid:16) ≤ + Using closed form solution for gφ, ≤ 16dΦ(φ, (cid:101)φ). Now union bound over a 1 at most 16 N . N -net, which requires O(N p) points by Assumption E.5. The approximation error using the net is Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Proof for εg. From before, we showed for a ﬁxed g ∈ G, we have εg (cid:46) γ2 log(1/δ) N . Observe that (cid:12)(cid:107)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 − (cid:107)(cid:101)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2(cid:12) (cid:12) (cid:12) ≤ (cid:107)g(s, a) − (cid:101)g(s, a)(cid:107)(cid:107)g(s, a) + (cid:101)g(s, a) − 2γφ(cid:63)(s(cid:48), π)(cid:107) ≤ (2 + 2γ)dG(g, (cid:101)g). Now union bound over a 1 at most 4 N . N -net, which requires O(N p) points by Assumption E.5. The approximation error using the net is E.3. Main Results Lemma E.6. Suppose φ(cid:63) ∈ Φ is Linear BC. Suppose Assumption 4.3 if transitions are stochastic. Moreover, suppose φ(cid:63) is feasible in the bilevel optimization (and so (cid:98)Llbc( (cid:98)φ) ≤ (cid:98)Llbc(φ(cid:63))). Then, w.p. 1 − 5δ, Llbc( (cid:98)φ) ≤ 24d(1 + W )2 log(4W |Φ|N/δ) + 128d2 log(4|Φ|N/δ) + 40γ2 log(2|G|/δ) N Proof. Llbc( (cid:98)φ) (cid:104) = Eν Since ρ ≤ Eν + Eν◦P (cid:124) (ρ (cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105) (cid:98)φ are minimizers under ν, (cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105) ((cid:98)ρ (cid:98)φ, M (cid:104) (cid:124) + Eν◦P (cid:104) (cid:107)M (cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107) (cid:99)M (cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:105) (cid:105) − Eν◦P − Eν◦P By the ρ, M parts of Lemma E.4, ≤ 2ED (cid:104) (cid:124) (cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105) ((cid:98)ρ + 2ED Since (cid:98)g (cid:98)φ minimizes under D, ≤ 2ED (cid:104) (cid:124) (cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105) ((cid:98)ρ + 2ED By optimality of (cid:98)φ under (cid:98)Llbc, (cid:104) (cid:107) (cid:99)M (cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107) (cid:99)M (cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:105) (cid:105) − 2ED − 2ED (cid:104) (cid:107)g (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107)g (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107)g (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107)(cid:98)g (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:105) (cid:105) (cid:105) (cid:105) + 2ερ + 2εM + 2ερ + 2εM ≤ 2ED (cid:104) (cid:124) φ(cid:63) φ(cid:63)(s, a) − r(s, a))2(cid:105) ((cid:98)ρ + 2ED (cid:104) (cid:107) (cid:99)Mφ(cid:63) φ(cid:63)(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:105) − 2ED (cid:2)(cid:107)(cid:98)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) + 2ερ + 2εM By − 1 2 the G part (cid:2)(cid:107)gφ(cid:63) − (cid:98)gφ(cid:63) (cid:107)2 2 Eν have ED of Lemma E.4, we (cid:3) + εg ≤ εg. Then, using the optimality of (cid:98)ρ and (cid:99)M under D, (cid:3) − 2ED (cid:2)(cid:107)Mφ(cid:63) φ(cid:63)(s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 + 2ED (cid:2)(cid:107)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 ≤ 2ED (cid:104) (ρ (cid:124) φ(cid:63) φ(cid:63)(s, a) − r(s, a))2(cid:105) (cid:2)(cid:107)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) 2 − (cid:107)(cid:98)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) ≤ + 2ερ + 2εM + 2εg By the ρ, M parts of Lemma E.4, ≤ 3Eν (cid:104) (ρ (cid:124) φ(cid:63) φ(cid:63)(s, a) − r(s, a))2(cid:105) + 3Eν◦P (cid:2)(cid:107)Mφ(cid:63) φ(cid:63)(s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) − 3Eν (cid:2)(cid:107)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) + 4ερ + 4εM + 2εg Learning Bellman Complete Representations for Ofﬂine Policy Evaluation By Assumption 4.3 and Lemma E.3, = 3Llbc(φ(cid:63)) + 4ερ + 4εM + 2εg By assumption that φ(cid:63) is Linear BC and Proposition 4.2, = 4ερ + 4εM + 2εg. We now prove Theorem 4.4, in the general stochastic case. The deterministic transitions case is subsumed by ignoring the minimization over G, i.e. setting the complexity term of G to zero. Theorem 4.4. Assume Assumption 4.1 (and Assumption 4.3 if 96 log1/2(|Φ|)+4 . If N ≥ C 2 d+4 log1/2(1/δ) √ 2 , then for any δ ∈ (0, 1), w.p. at least 1 − δ, we have β/4 the system is stochastic). Let C2 := 1. (cid:98)φ satisﬁes (cid:98)ε-approximate Linear BC with (cid:98)ε ≤ 13d(1 + W )2 log1/2(4W |Φ|N/δ) √ N + 7γ(1 + W ) log1/2(2|G|/δ) √ N , 2. λmin(Σ( (cid:98)φ)) ≥ β/4. If transitions are deterministic, treat log(|G|) = 0. Proof of Theorem 4.4. First, by our assumption that w.p. at least 1 − δ, we have supφ∈Φ important consequences: (cid:12) (cid:12) (cid:12)λmin(Σ(φ)) − λmin((cid:98)Σ(φ)) N ≥ 4(96κ(Φ) + 4 d + 4 log1/2(1/δ))/β, Lemma E.2 implies that (cid:12) (cid:12) (cid:12) ≤ β/4. Under this high probability event, we have two √ √ 1. φ(cid:63) is feasible in Equation (6), since λmin((cid:98)Σ(φ(cid:63))) ≥ λmin(Σ(φ(cid:63))) − β/4 ≥ β(1 − 1/4) ≥ β/2. In particular, this means (cid:98)Llbc( (cid:98)φ) ≤ (cid:98)Llbc(φ(cid:63)). 2. The covariance of (cid:98)φ has lower-bounded eigenvalues, since λmin(Σ( (cid:98)φ)) ≥ λmin((cid:98)Σ( (cid:98)φ)) − β/4 ≥ β(1/2 − 1/4) ≥ β/4. Now, apply Lemma E.6 to bound Llbc( (cid:98)φ), so w.p. 1 − 5δ, Llbc( (cid:98)φ) ≤ 24d(1 + W )2 log(4W |Φ|N/δ) + 128d2 log(4|Φ|N/δ) + 40γ2 log(2|G|/δ) N By Lemma D.1, we have that (cid:98)φ is (cid:98)ε-approximately Linear BC, with parameter (cid:114) (cid:98)ε ≤ (1 + W ) · 24d(1 + W )2 log(4W |Φ|N/δ) + 128d2 log(4|Φ|N/δ) + 40γ2 log(2|G|/δ) N 7γ(1 + W ) log1/2(2|G|/δ) √ N 13d(1 + W )2 log1/2(4W |Φ|N/δ) √ N + ≤ Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Finally, we remark that the (cid:99)W for (cid:98)φ (in the approximately Linear BC case) is upper bounded by a polynomial in W (cid:63) in the assumed exact Linear BC of φ(cid:63). Consider our assumption that φ(cid:63) is exactly Linear BC with W (cid:63) = W (use (cid:63) to highlight that it is the W in the assumption, which we now show matches the W in the result). Then, by Proposition 4.2, ∃M (cid:63) with (cid:107)M (cid:63)(cid:107)2 ≤ W (cid:63)2 . Hence, it sufﬁces to minimize over this smaller ball for (cid:99)M , so that (cid:107) (cid:99)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2. Now, take the smallest possible W in Lemma D.1, so that 1 − (cid:107)ρ(cid:63)(cid:107)2 (cid:113) 2 (cid:99)W = ≤ = ≤ (cid:107)ρ(cid:63)(cid:107)2 1 − (cid:107)M (cid:63)(cid:107)2 (cid:107)ρ(cid:63)(cid:107)2 (cid:113) 1 − (cid:107)ρ(cid:63)(cid:107)2 2 W (cid:63)2 (cid:113) 1 − (cid:107)ρ(cid:63)(cid:107)2 (1 + 1 − (cid:107)ρ(cid:63)(cid:107)2 W (cid:63)2 ) 2 (cid:107)ρ(cid:63)(cid:107)2 2 W (cid:63)2 2W (cid:63)2 (cid:107)ρ(cid:63)(cid:107)2 , which is a polynomial in W (cid:63). Our end-to-end result is deduced by chaining our LSPE theorem and the above theorem together. Theorem 4.5. Under Assumption 4.1 (and Assumption 4.3 if P (s, a) is stochastic). Let C2, (cid:98)ε be as deﬁned in Theorem 4.4. If N ≥ C 2 2 , then we have for any δ ∈ (0, 1/2), w.p. at least 1 − 2δ, for all distributions p0, (cid:12) (cid:12)V πe (cid:12) − Es∼p0 (cid:104) (cid:98)fK(s, πe) p0 960β−1/2(1 + W )d log(N )(cid:112)log(10/δ) (cid:105)(cid:12) (cid:12) (cid:12) ≤ γK/2 1 − γ + √ (1 − γ)2 N . + (cid:114)(cid:13) (cid:13) (cid:13) 4 ddπe p0 dν (cid:13) (cid:13) (cid:13)∞ (1 − γ)2 (cid:98)ε Proof of Theorem 4.5. We ﬁrst apply Theorem 4.4 to see that (cid:98)φ satisﬁes the two properties needed for LSPE. It is indeed approximately Linear BC, with (cid:98)ε speciﬁed in the theorem, and also has coverage (i.e. λmin(Σ( (cid:98)φ)) ≥ β/4). Using these two facts, and on a separate independent dataset D2 (needs to be a separate dataset since (cid:98)φ is data-dependent), we run LSPE and directly apply Theorem 3.3 for the result. F. Implementation Details Here we detail all environment speciﬁcations and hyperparameters used in the main text. F.1. Dataset Details Using the publicly released implementation for DrQ-v2, we trained high quality target policies and saved checkpoints for ofﬂine behavior datasets. We refer the readers to Yarats et al. (2021a) for exact hyperparameters. F.2. Environment Details Following the standards used by DrQ-v2 (Yarats et al., 2021a), all environments have a maximum horizon length of 500 timesteps. This is achieved by the behavior/target policy having an action repeat of 2 frames. Furthermore, each state is 3 stacked frames that are each 84 × 84 dimensional RGB images (thus 9 × 84 × 84). Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Task Target Performance Behavior Performance Finger Turn Hard Cheetah Run Quadruped Walk Humanoid Stand 927 758 873 827 226 (24%) 192 (25%) 236 (27%) 277 (33%) Table 1. Performance for target and behavior policies used to collect evaluation and ofﬂine datasets respectively. Task Action Space Dimension Task Traits Reward Type Finger Turn Hard Cheetah Run Quadruped Walk Humanoid Stand 2 6 12 21 turn locomotion locomotion stand sparse dense dense dense Table 2. Task descriptions, action space dimension, and reward type for each tested environment. F.3. Representation Architecture and Hyperparameter Details We adopt the same network architecture as DrQ-v2’s critic, ﬁrst introduced in SAC-AE (Yarats et al., 2021b). More speciﬁcally, to process pixel input, we have a 5 layer ConvNet with 3 × 3 kernels and 32 channels with ReLU activations. The ﬁrst convolutional layer has a stride of 2 while the rest has stride 1. The output is fed through a single fully connected layer normalized by LayerNorm. Finally, there is a tanh nonlinearity on the outputted 50 dimensional state-representation. The action is then concatenated with this output and fed into a 4-layer MLP all with ReLU activations. Hyperparameter Value Feature Dimension Weight Initialization Optimizer Learning Rate Batch Size Training Epochs τ (target) λDesign 512 orthogonal init. Adam 1 × 10−5 2048 200 0.005 5 × 10−6 Table 3. Hyperparameters used for BCRL Learning Bellman Complete Representations for Ofﬂine Policy Evaluation F.4. Benchmarks and Metrics Modiﬁcations to CURL: Originally, CURL only does contrastive learning between the image states with data augmentation. For OPE, apply the same CURL objective to the state-action feature detailed in the previous section. Note we also train CURL with the same random cropping image augmentations presented by the authors. Finally, since we are not interleaving the representation learning with SAC, we do not have a Q prediction head. Modiﬁcations to SPR: We use the same image encoder as our features for SPR. The main difference is in the architecture of the projection layers where we implement as 3-layer mlp with ReLU activations. Note that these are additional parameters that neither CURL nor BCRL require. Finally, similarly to CURL, we do not have an additional Q-prediction head. Spearman Ranking Correlation Metric: This rank correlation measures the correlation between the ordinal rankings of the value estimates and the ground truth returns. As deﬁned in Fu et al. (2021), we have for policies 1, 2, . . . , N , true returns V1:N , and estimated returns ˆV1:N : Ranking Correlation = (cid:17) (cid:16) V1:N , ˆV1:N Cov σ (V1:N ) σ( ˆV1:N )","['learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'sun', 'abstract', 'introduction', 'l', 'u', 'x', 'study', 'representation', 'learn', 'ofﬂine', 'inforcement', 'learning', 'focus', 'portant', 'task', 'ofﬂine', 'policy', 'evaluation', 'ope', 'recent', 'work', 'show', 'contrast', 'supervised', 'learning', 'realizability', 'qfunction', 'enough', 'learn', 'sufﬁcient', 'condition', 'sampleefﬁcient', 'ope', 'bellman', 'complete', 'ness', 'coverage', 'prior', 'work', 'often', 'assume', 'representation', 'satisfy', 'condition', 'give', 'result', 'mostly', 'theoretical', 'nature', 'work', 'propose', 'directly', 'learn', 'datum', 'approximately', 'complete', 'representation', 'good', 'coverage', 'learn', 'representation', 'perform', 'ope', 'use', 'least', 'square', 'policy', 'evalua', 'tion', 'lspe', 'linear', 'function', 'learn', 'representation', 'present', 'endtoend', 'theoreti', 'cal', 'analysis', 'show', 'twostage', 'algorithm', 'enjoy', 'polynomial', 'sample', 'complexity', 'provide', 'representation', 'rich', 'class', 'consider', 'linear', 'bellman', 'complete', 'empirically', 'tensively', 'evaluate', 'algorithm', 'challenge', 'imagebase', 'continuous', 'control', 'task', 'deepmind', 'control', 'suite', 'show', 'represen', 'tation', 'enable', 'well', 'compare', 'previous', 'representation', 'learning', 'method', 'develop', 'offpolicy', 'achieve', 'competitive', 'ope', 'error', 'stateoftheart', 'method', 'fit', 'fqe', 'beat', 'fqe', 'evaluate', 'initial', 'state', 'tribution', 'ablation', 'show', 'linear', 'bellman', 'complete', 'coverage', 'component', 'method', 'crucial', 'equal', 'contribution', 'science', 'university', 'ithaca', 'research', 'information', 'neere', 'proceeding', 'international', 'conference', 'machine', 'learn', 'copy', 'right', 'author', 'deep', 'reinforcement', 'learn', 'develop', 'agent', 'solve', 'complex', 'sequential', 'decision', 'make', 'task', 'e', 'new', 'stateoftheart', 'result', 'surpass', 'expert', 'human', 'performance', 'impressive', 'result', 'algo', 'rithm', 'often', 'require', 'prohibitively', 'large', 'number', 'online', 'interaction', 'scale', 'high', 'dimensional', 'input', 'address', 'sample', 'complexity', 'demand', 'recent', 'line', 'work', 'anand', 'mazoure', 'incor', 'porate', 'advance', 'unsupervised', 'representation', 'learn', 'supervised', 'learning', 'literature', 'develop', 'agent', 'example', 'curl', 'laskin', 'spr', 'schwarzer', 'utilize', 'contrastive', 'representation', 'jective', 'auxiliary', 'loss', 'exist', 'rl', 'framework', 'effort', 'motivate', 'tremendous', 'success', 'selfsupervise', 'technique', 'offer', 'computer', 'vision', 'natural', 'language', 'processing', 'speech', 'processing', 'formulation', 'show', 'sample', 'complexity', 'improvement', 'empirically', 'remain', 'open', 'question', 'approach', 'successfully', 'address', 'unique', 'challenge', 'usually', 'appear', 'supervised', 'learning', 'exploration', 'exploitation', 'credit', 'assign', 'ment', 'long', 'horizon', 'prediction', 'distribution', 'shift', 'particular', 'recent', 'work', 'show', 'realizability', 'learning', 'target', 'namely', 'qfunction', 'insufﬁcient', 'avoid', 'exponential', 'dependence', 'problem', 'parameter', 'paper', 'study', 'representation', 'learn', 'important', 'subtask', 'offpolicy', 'ofﬂine', 'policy', 'evaluation', 'ope', 'ope', 'critical', 'component', 'offpolicy', 'policy', 'optimization', 'approach', 'eg', 'offpolicy', 'actor', 'critic', 'sac', 'haarnoja', 'ddpg', 'lillicrap', 'et', 'moreover', 'allow', 'focus', 'issue', 'arise', 'distribution', 'shift', 'long', 'horizon', 'prediction', 'speciﬁcally', 'propose', 'new', 'approach', 'leverage', 'rich', 'function', 'approximation', 'eg', 'deep', 'neural', 'network', 'learn', 'representation', 'bellman', 'complete', 'exploratory', 'linear', 'bellman', 'complete', 'representation', 'mean', 'linear', 'function', 'representation', 'inherent', 'bellman', 'error', 'anto', 'apply', 'bellman', 'operator', 'linear', 'function', 'result', 'new', 'linear', 'function', 'exploratory', 'representation', 'mean', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'result', 'feature', 'covariance', 'matrix', 'form', 'ofﬂine', 'dataset', 'wellconditione', 'representational', 'property', 'ensure', 'linear', 'function', 'approximation', 'linear', 'evaluation', 'protocol', 'grill', 'least', 'square', 'policy', 'evaluation', 'lspe', 'duan', 'achieve', 'accurate', 'policy', 'evaluation', 'provide', 'endtoend', 'analysis', 'show', 'representation', 'learn', 'approach', 'together', 'lspe', 'ensure', 'nearoptimal', 'policy', 'evaluation', 'polynomial', 'sample', 'complexity', 'empirically', 'extensively', 'evaluate', 'method', 'image', 'base', 'continuous', 'control', 'task', 'deepmind', 'control', 'first', 'compare', 'representation', 'learning', 'approach', 'develop', 'offpolicy', 'curl', 'laskin', 'spr', 'schwarzer', 'bear', 'many', 'similarity', 'contrastive', 'learning', 'techinique', 'unsupervised', 'representation', 'learn', 'simclr', 'bootstrap', 'latent', 'byol', 'grill', 'respectively', 'linear', 'evaluation', 'lspe', 'linear', 'function', 'top', 'learn', 'representation', 'approach', 'consistently', 'outperform', 'baseline', 'observe', 'representation', 'learn', 'curl', 'spr', 'sometimes', 'even', 'exhibit', 'instability', 'evaluate', 'use', 'lspe', 'prediction', 'error', 'blow', 'iteration', 'lspe', 'apply', 'approach', 'stable', 'comparison', 'demonstrate', 'representation', 'learn', 'ofﬂine', 'subtle', 'use', 'representation', 'technique', 'develop', 'supervised', 'learning', 'setting', 'result', 'good', 'performance', 'ofﬂine', 'ablation', 'show', 'linear', 'coverage', 'crucial', 'method', 'also', 'blow', 'ingredient', 'miss', 'finally', 'achieve', 'stateoftheart', 'ope', 'error', 'compare', 'ope', 'method', 'improve', 'stateoftheart', 'evaluate', 'initial', 'state', 'distribution1', 'relate', 'work', 'representation', 'learn', 'ofﬂine', 'theoretical', 'side', 'consider', 'ﬂine', 'sparse', 'linear', 'jin', 'learn', 'sparsity', 'understand', 'feature', 'selection', 'work', 'much', 'strong', 'coverage', 'condition', 'namely', 'give', 'representation', 'class', 'assume', 'feature', 'global', 'coverage', 'ofﬂine', 'data', 'distribution', 'acid62', 'well', 'condition', 'ν', 'ofﬂine', 'datum', 'distribution', 'work', 'assume', 'exist', 'φcid63', 'global', 'coverage', 'thus', 'strictly', 'generalize', 'coverage', 'condition', 'propose', 'general', 'modelbase', 'ofﬂine', 'rl', 'approach', 'perform', 'representation', 'learn', 'linear', 'mdps', 'ofﬂine', 'setting', 'global', 'coverage', 'code', 'available', 'ever', 'algorithm', 'version', 'space', 'approach', 'computationally', 'efﬁcient', 'also', 'linear', 'bellman', 'com', 'pleteness', 'condition', 'strictly', 'generalize', 'linear', 'consider', 'learn', 'stateaction', 'embedding', 'know', 'rkh', 'use', 'general', 'function', 'approximation', 'powerful', 'rkh', 'finally', 'parr', 'bellman', 'completeness', 'desirable', 'condition', 'feature', 'selection', 'analyze', 'equivalence', 'tween', 'linear', 'valuefunction', 'approximation', 'linear', 'model', 'approximation', 'work', 'investigate', 'learn', 'completeness', 'representation', 'also', 'role', 'coverage', 'feature', 'selection', 'empirical', 'side', 'evalu', 'ate', 'broad', 'range', 'unsupervised', 'objective', 'learn', 'pretraine', 'representation', 'ofﬂine', 'dataset', 'stream', 'imitation', 'learn', 'online', 'ofﬂine', 'find', 'use', 'pretraine', 'representation', 'matically', 'improve', 'downstream', 'performance', 'policy', 'learning', 'algorithm', 'work', 'aim', 'identify', 'unsupervised', 'representation', 'objective', 'extend', 'empirical', 'evaluation', 'ofﬂine', 'imagebase', 'continuous', 'present', 'provable', 'contrastive', 'representation', 'learn', 'objective', 'derive', 'maximum', 'likelihood', 'loss', 'learn', 'state', 'representa', 'tion', 'expert', 'datum', 'imitation', 'behavior', 'clone', 'approach', 'instead', 'focus', 'learn', 'stateaction', 'rep', 'resentation', 'ope', 'finally', 'song', 'present', 'algorithm', 'learn', 'representa', 'tion', 'suitable', 'linear', 'value', 'function', 'approximation', 'song', 'relevant', 'work', 'aim', 'learn', 'complete', 'feature', 'work', 'however', 'work', 'online', 'setting', 'consider', 'coverage', 'induce', 'representation', 'identify', 'impor', 'tant', 'accurate', 'ope', 'representation', 'learn', 'online', 'theoretical', 'work', 'representation', 'learn', 'online', 'rl', 'focus', 'learn', 'representation', 'facilitate', 'exploration', 'scratch', 'olive', 'blinucb', 'flambe', 'agarwal', 'et', 'modi', 'proache', 'representation', 'learn', 'linear', 'mdps', 'assume', 'exist', 'feature', 'φcid63', 'ad', 'mit', 'linear', 'mdp', 'structure', 'ground', 'truth', 'posit', 'even', 'strong', 'assumption', 'feature', 'admit', 'linear', 'mdp', 'ture', 'true', 'transition', 'note', 'assume', 'exist', 'φcid63', 'admit', 'linear', 'bellman', 'complete', 'ness', 'strictly', 'generalize', 'linear', 'mdps', 'hence', 'representation', 'learn', 'setting', 'general', 'prior', 'theoretical', 'work', 'note', 'study', 'ofﬂine', 'setting', 'prior', 'work', 'operate', 'online', 'setting', 'additional', 'challenge', 'online', 'exploration', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'empirical', 'side', 'large', 'body', 'work', 'adapt', 'exist', 'selfsupervised', 'learning', 'technique', 'devel', 'ope', 'computer', 'vision', 'nlp', 'rl', 'learn', 'representation', 'imagebase', 'task', 'anand', 'driml', 'mazoure', 'laskin', 'spr', 'schwarzer', 'learn', 'representation', 'op', 'timize', 'various', 'temporal', 'contrastive', 'loss', 'particular', 'laskin', 'propose', 'interleave', 'minimize', 'fonce', 'objective', 'similarity', 'moco', 'simclr', 'learn', 'policy', 'sac', 'haarnoja', 'moreover', 'schwarzer', 'propose', 'learn', 'representation', 'similar', 'byol', 'grill', 'qfunction', 'deep', 'qlearning', 'compare', 'repre', 'sentational', 'objective', 'curl', 'spr', 'section', 'demonstrate', 'linear', 'evaluation', 'protocol', 'outperform', 'curl', 'spr', 'note', 'refer', 'reader', 'schwarzer', 'comparison', 'spr', 'driml', 'preliminary', 'paper', 'consider', 'inﬁnite', 'horizon', 'discount', 'mdp', 'cid104s', 'γ', 'p', 'r', 'd0cid105', 'state', 'action', 'space', 'contain', 'large', 'number', 'state', 'tion', 'even', 'inﬁnite', 'discount', 'factor', 'p', 'transition', 'kernel', 'r', '×', 'r', 'reward', 'function', '∆s', 'initial', 'state', 'distribution', 'assume', 'reward', 'bound', 'give', 'policy', 'denote', 'e', 'scid3', 'expect', 'count', 'total', 'reward', 'policy', 'π', 'start', 'state', 'expect', 'discount', 'total', 'denote', 'reward', 'policy', 'π', 'start', 'initial', 'state', 'distribu', 'tion', 'd0', 'also', 'denote', 'average', 'stateaction', 'distribution', 'probability', 'visit', 'pair', 'time', 'step', 'h', 'start', '−', 'γ', 'cid80∞', 'ope', 'seek', 'evaluate', 'expect', 'discount', 'πe', 'target', 'policy', 'give', 'datum', 'draw', 'ofﬂine', 'stateaction', 'distribution', 'ν', '×', 'latter', 'example', 'stateaction', 'distribution', 'behavior', 'policy', 'namely', 'ofﬂine', 'dataset', 'ri', 'scid48', 'i1', 'consist', 'iid', 'tuple', 'generate', '∼', 'ν', 'r', 'scid48', '∼', 'p', 'deﬁne', 'bellman', 'operator', 'πassociate', 'follow', '∆', 'rs', 'saacid48∼πscid48', 'scid48', 'acid48', 'drop', 'superscript', 'π', 'clear', 'context', 'particular', 'ﬁxed', 'target', 'policy', 'representation', 'feature', '×', 'rd', 'embedding', 'stateaction', 'pair', 'ddimensional', 'space', 'let', 'eν', 'φsi', 'consider', 'learn', 'n', 'resentation', 'feature', 'hypothesis', 'class', '×', 'assume', 'feature', 'bound', 'norm', '∈', 'experiment', 'φ', 'convolutional', 'neural', 'net', 'output', 'notation', 'denote', 'give', 'distribution', 'ν', '×', 'function', '×', 'cid55→', 'r', 'denote', 'ν', 'esa∼νf', 'give', 'positive', 'norm', 'cid1072', 'σx', 'let', 'λminσ', 'deﬁnite', 'matrix', 'σ', 'let', 'cid107xcid107σ', 'denote', 'minimum', 'eigenvalue', 'µ', 'let', 'denote', 'radonnikodym', 'derivative', 'use', '◦', 'denote', 'composition', 'scid48', 'acid48', '∼', 'p', '◦', 'shortform', 'scid48', '∼', 'p', 'acid48', '∼', 'πscid48', 'function', 'policy', 'denote', 'linear', 'bellman', 'completeness', 'introduce', 'representation', 'learn', 'approach', 'section', 'ﬁrst', 'consider', 'ope', 'linear', 'function', 'approximation', 'give', 'representation', 'lesson', 'supervised', 'learning', 'linear', 'bandit', 'suggest', 'ope', 'possible', 'polynomially', 'many', 'ofﬂine', 'sample', 'long', 'ground', 'truth', 'target', 'linear', 'qπe', 'wcid62φs', 'ofﬂine', 'datum', 'provide', 'sufﬁcient', 'coverage', 'unfortunately', 'assumption', 'low', 'bound', 'indicate', 'ope', 'exist', 'mdp', 'need', 'least', 'exponentially', 'exponential', 'horizon', 'many', 'ofﬂine', 'sample', 'provide', 'accurate', 'estimate', 'v', 'amortila', 'lower', 'bind', 'indicate', 'need', 'additional', 'structural', 'condition', 'representation', 'additional', 'condition', 'prior', 'work', 'consider', 'bellman', 'completeness', 'seek', 'learn', 'rep', 'resentation', 'rather', 'assume', 'theoretical', 'condition', 'give', 'one', 'focus', 'approximate', 'version', 'approximate', 'linear', 'representation', 'ενapproximately', 'linear', 'bellman', 'complete', 'max', 'min', '−', 'πe', 'εν', 'note', 'dependence', 'ν', 'cid124', 'φs', 'πw', 'intuitively', 'condition', 'say', 'linear', 'φs', 'approxi', 'function', 'mate', 'linear', 'function', 'distribution', 'ν', 'remark', 'lowrank', 'mdps', 'agarwal', 'subsume', 'exact', 'linear', 'distribution', 'ν', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'least', 'square', 'policy', 'evaluation', 'lspe', 'input', 'target', 'policy', 'feature', 'initialize', 'k−1φs', 'set', 'cid98vk−1', 'perform', 'linear', 'regression', 'i1', 'end', 'return', 'ai', '−', '−', 'γ', 'note', 'completeness', 'condition', 'sub', 'tle', 'common', 'realizability', 'condition', 'ﬁxed', 'increase', 'expressiveness', 'eg', 'add', 'feature', 'imply', 'monotonic', 'decrease', 'εν', 'thus', 'common', 'trick', 'lift', 'feature', 'high', 'order', 'polynomial', 'kernel', 'space', 'general', 'reproduce', 'kernel', 'space', 'imply', 'linear', 'bellman', 'complete', 'condition', 'improve', 'coverage', 'condition', 'next', 'show', 'approximate', 'linear', 'together', 'coverage', 'imply', 'sampleefﬁcient', 'ope', 'least', 'square', 'policy', 'evaluation', 'lspe', 'present', 'result', 'use', 'relative', 'condition', 'number', 'deﬁne', 'initial', 'state', 'distribution', 'p0', 'aφ', 'theorem', 'sample', 'complexity', 'lspe', 'assume', 'fea', 'ture', 'satisﬁes', 'approximate', 'linear', 'parameter', 'εν', 'wp', 'least', '−', 'δ', 'initial', 'state', 'distribution', 'cid12v', 'πe', 'cid104', 'cid98fks', 'πe', '−', 'ddπe', 'dν', 'cid13∞', '−', 'γ2', 'logn', 'γ2', 'cid98fk', 'output', 'see', 'c1', 'proof', 'result', 'hold', 'simultaneously', 'initial', 'state', 'distribution', 'cover', 'datum', 'distribution', 'ν', 'ν', 'full', 'coverage', 'ie', 'commonly', 'assume', 'literature', 'show', 'initial', 'state', 'distribution', 'also', 'note', 'concentrability', 'coefﬁcient', 'cid13∞', 'show', 'nonlinear', 'exact', 'linear', 'case', 'linear', 'function', 'perfectly', 'approximate', 'ν', 'term', 'involve', 'concentrability', 'coefﬁcient', 'even', 'avoid', 'ﬁniteness', 'ddπe', 'dν', 'representation', 'learn', 'previous', 'section', 'indicate', 'sufﬁcient', 'condition', 'representation', 'efﬁcient', 'accurate', 'ope', 'linear', 'function', 'approximation', 'however', 'representation', 'bellman', 'complete', 'also', 'provide', 'coverage', 'avail', 'able', 'priori', 'especially', 'highdimensional', 'setting', 'eg', 'imagebase', 'control', 'task', 'consider', 'experi', 'ment', 'exist', 'theoretically', 'work', 'often', 'assume', 'rep', 'resentation', 'give', 'propose', 'learn', 'representation', 'approximately', 'bellman', 'complete', 'also', 'provide', 'good', 'coverage', 'rich', 'function', 'approximation', 'eg', 'deep', 'neural', 'network', 'formally', 'want', 'learn', 'repre', 'sentation', 'ensure', 'approximate', 'bellman', 'complete', 'small', 'good', 'coverage', 'large', 'possible', 'key', 'property', 'guarantee', 'accurate', 'efﬁcient', 'ope', 'indicate', 'formulate', 'representation', 'learn', 'question', 'key', 'assumption', 'representation', 'pothesis', 'class', 'φ', 'rich', 'enough', 'contain', 'least', 'representation', 'φcid63', 'linear', 'bellman', 'complete', 'good', 'coverage', 'assumption', 'representational', 'power', 'ex', 'ist', 'φcid63', '∈', 'φcid63', 'achieve', 'exact', 'linear', 'φcid63', 'induce', 'coverage', 'goal', 'learn', 'representation', 'hypothe', 'sis', 'class', 'note', 'prior', 'rl', 'representation', 'learning', 'work', 'assume', 'exist', 'φcid63', 'linear', 'induce', 'good', 'coverage', 'candidate', 'terrible', 'coverage', 'necessarily', 'linear', 'proceed', 'learn', 'ﬁrst', 'present', 'equivalent', 'condition', 'linear', 'rely', 'complicated', 'minmax', 'expression', 'deﬁnition', 'proposition', 'consider', 'feature', 'φ', 'full', 'rank', 'ance', 'σφ', 'give', 'feature', 'linear', 'imply', 'exist', '×', 'rd×d', 'cid1072', '−', 'eν', 'φs', '−', 'cid20γescid48∼p', 'saφscid48', 'πe', 'hand', 'exist', '×', 'rd×d', 'cid1072', 'equality', 'hold', 'satisfy', 'exact', 'linear', 'cid1072', 'see', 'proof', 'show', 'equivalent', 'simple', 'linear', 'relationship', 'feature', 'expect', 'next', 'step', 'feature', 'reward', 'motivate', 'feature', 'learn', 'pable', 'learn', 'representation', 'transition', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'current', 'feature', 'expect', 'next', 'fea', 'ture', 'saφscid48', 'πe', 'reward', 'linear', 'find', 'feature', 'linear', 'learn', 'representation', 'achieve', 'linear', 'use', 'proposition', 'design', 'bilevel', 'optimization', 'program', 'start', 'deterministic', 'transition', 'warm', 'deterministic', 'transition', 'determinism', 'tran', 'sition', 'expectation', 'respect', 'scid48', 'anymore', 'design', 'bilevel', 'optimization', 'follow', 'however', 'directly', 'optimize', 'objective', 'access', 'p', 'compute', 'ex', 'pecte', 'next', 'step', 'feature', 'saφscid48', 'πe', 'also', 'note', 'expectation', 'escid48', 'square', 'mean', 'even', 'get', 'unbiased', 'estimate', 'gradient', 'sample', 'scid48', '∼', 'p', 'phenomenon', 'relate', 'double', 'sampling', 'issue', 'ofﬂine', 'policy', 'evaluation', 'literature', 'forbid', 'directly', 'optimize', 'bellman', 'residual', 'algorithm', 'td', 'design', 'overcome', 'double', 'sampling', 'issue', 'use', 'different', 'technique', 'tackle', 'issue', 'introduce', 'function', 'class', '×', 'rich', 'enough', 'contain', 'expect', 'next', 'feature', 'cid104', '×', 'rd×d', 'cid107', 'cid1072', 'cid63cid1072', 'cid20γφscid48', 'ρcid63', 'optimal', 'ρ', 'linear', 'assumption', 'namely', 'aim', 'search', 'repre', 'sentation', 'relationship', 'combination', 'next', 'time', 'step', '’s', 'feature', 'φscid48', 'reward', 'linear', 'spectral', 'norm', 'constraint', 'justiﬁe', 'proposition', 'solve', 'moment', 'condition', 'ﬁnd', 'representation', 'achieve', 'proximate', 'linear', 'however', 'guarantee', 'representation', 'provide', 'good', 'coverage', '’s', 'trace', 'introduce', 'regularization', 'use', 'idea', 'optimal', 'design', 'particularly', 'eoptimal', 'design', 'deﬁne', 'minimum', 'eigenvalue', 'regularization', 'assumption', 'mapping', 'cid55→', 'γescid48∼p', 'sa', 'φscid48', 'πe', 'g', 'form', 'optimization', 'problem', 'follow', 'ed', '−', 'γφscid48', '−', 'cid20γφscid48', 'cid21cid13', 'min', 'note', 'assumption', 'min', 'approx', 'imate', 'saφscid48', 'πe', '−', 'φscid48', 'erage', 'variance', 'induce', 'stochastic', 'transition', 'thus', 'ﬁxed', 'φ', 'see', 'follow', 'ed', 'φs', '−', 'γφscid48', '−', 'saφscid48', 'πe', '−', 'φscid48', 'reφ', 'λmin', 'indeed', 'unbiased', 'estimate', 'small', 'eigenvalue', 'empirical', 'feature', 'covari', 'ance', 'matrix', 'representation', 'maximize', 'quantity', 'ensure', 'feature', 'provide', 'good', 'coverage', 'large', 'possible', 'thus', 'learn', 'representation', 'approximately', 'linear', 'bellman', 'complete', 'also', 'provide', 'sufﬁcient', 'coverage', 'formulate', 'following', 'constrain', 'bilevel', 'optimization', 'cid104', 'min', '−', 'cid20γφscid48', 'πe', 'extend', 'stochastic', 'transition', 'additional', 'double', 'sampling', 'issue', 'discuss', 'address', 'stochastic', 'transition', 'ideally', 'solve', 'follow', 'e', 'bilevel', 'optimization', 'problem', 'cid104', '−', 'cid20γescid48∼p', 'saφscid48', 'πe', 'cid21cid13', 'esa∼ν', 'φs', '−', 'saφscid48', 'πecid13', 'match', 'ideal', 'objective', 'eq', 'thus', 'solv', 'e', 'base', 'eq', 'allow', 'optimize', 'eq', 'allow', 'learn', 'approximate', 'linear', 'bellman', 'complete', 'representation', 'similarly', 'incorporate', 'eoptimal', 'sign', 'add', 'constraint', 'force', 'small', 'eigenvalue', 'empirical', 'feature', 'covariance', 'matrix', 'lower', 'bound', 'learn', 'representation', 'approximately', 'linear', 'also', 'induce', 'sufﬁcient', 'coverage', 'simply', 'call', 'lspe', 'estimate', 'whole', 'procedure', 'summarize', 'note', 'put', 'constraint', 'objective', 'function', 'use', 'lagrangian', 'multiplier', 'design', 'choice', 'also', 'encourage', 'coverage', 'particular', 'choice', 'study', 'empirically', 'motivate', 'idea', 'doptimal', 'design', 'aim', 'representation', 'maximize', 'follow', 'logdeterminant', 'ln', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'ope', 'bellman', 'complete', 'exploratory', 'representation', 'learn', 'input', 'representation', 'class', 'φ', 'dataset', 'size', 'design', 'regularization', 'r', 'function', 'class', 'g', 'policy', 'πe', 'randomly', 'split', 'set', 'd1', 'size', 'system', 'stochastic', 'learn', 'representation', 'cid98φ', 'min', '−', 'γφscid48', '−', 'cid105', 'cid20γφscid48', 'πe', '−', 'λrφ', 'otherwise', 'deterministic', 'system', 'learn', 'φs', '−', 'cid20γφscid48', 'πe', '−', 'λrφ', 'return', 'large', 'imate', 'eν', 'acid124', 'maximize', 'intuitively', 'maximize', 'coverage', 'direction', 'doptimal', 'design', 'widely', 'use', 'bandit', 'lattimore', 'szepesvári', 'agarwal', 'design', 'exploration', 'distribution', 'global', 'coverage', 'note', 'contrast', 'context', 'feature', 'ﬁxe', 'distribution', 'optimize', 'optimize', 'feature', 'give', 'data', 'distribution', 'ν', 'sample', 'complexity', 'analysis', 'prove', 'ﬁnite', 'sample', 'utility', 'guarantee', 'pirical', 'constrain', 'bilevel', 'optimization', 'problem', 'φs', '−', 'ed', '−', 'γφscid48', 'min', 'cid20γφscid48', 'πe', 'simplicity', 'state', 'result', 'discrete', 'function', 'class', 'φ', 'g', 'note', 'sample', 'complexity', 'scale', 'respect', 'lnφ', 'lng', 'standard', 'com', 'plexity', 'measure', 'discrete', 'function', 'class', 'extend', 'analysis', 'inﬁnite', 'function', 'class', 'metric', 'assumption', 'wainwright', 'der', 'wellner', 'appendix', 'see', 'assumption', 'e5', 'follow', 'theorem', 'show', 'learn', 'representation', 'cid98φ', 'approximately', 'linear', 'coverage', 'assume', 'assumption', 'assump', 'let', 'log12φ4', 'system', 'stochastic', 'wp', 'least', '−', 'δ', 'cid98φ', 'satisﬁes', 'cid98εapproximate', 'linear', 'λminσ', '≥', 'β4', 'transition', 'deterministic', 'treat', 'proof', 'sketch', 'first', 'use', 'perturbation', 'theorem', 'chain', 'show', 'eigenvalue', 'σφ', 'close', 'cid98σφ', 'uniformly', 'imply', '≥', 'β2', 'hence', 'feasible', 'empirical', 'bilevel', 'optimization', 'equation', 'λminσ', '≥', 'φcid63', 'feasible', 'apply', 'uniform', 'concentration', 'argument', 'argue', 'low', 'population', 'loss', 'imply', 'approximate', 'linear', 'error', 'term', 'comprise', 'statistical', 'error', 'ﬁtte', 'ﬁtte', 'g', 'double', 'sampling', 'correc', 'tion', 'contextual', 'bandit', 'set', 'transition', 'second', 'term', 'become', 'chain', 'together', 'give', 'following', 'endtoend', '−12', 'evaluation', 'error', 'guarantee', 'lspe', 'learn', 'feature', 'theorem', 'assumption', 'assumption', 'stochastic', 'let', 'cid98ε', 'deﬁne', 'wp', 'least', 'distribution', 'p0', 'cid12v', 'πe', 'cid104', 'cid98fks', 'πe', 'logn', 'γ2', 'ddπe', 'dν', 'cid13∞', 'γ2', 'comparison', 'fqe', 'ignore', 'representa', 'tion', 'learning', 'run', 'fit', 'fqe', 'directly', 'perform', 'least', 'square', 'ﬁtte', 'ear', 'function', 'class', 'fqe', 'suffer', 'follow', 'error', 'also', 'call', 'inherent', 'error', 'g∈f', 'g', '−', 'πe', 'cid1072', 'ν', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'practical', 'instantiation', 'input', 'ofﬂine', 'dataset', 'scid48', 'target', 'policy', 'πe', 'initialize', 'parameter', 'ρ', 'η∇ρjφt', 'η∇φjφt', 'τ', 'end', 'linear', 'evaluation', 'note', 'assumption', 'exist', 'linear', 'φcid63', 'imply', 'error', 'small', 'contrast', 'n', '∞', 'approach', 'accurately', 'estimate', 'πe', 'practical', 'implementation', 'section', 'instantiate', 'practical', 'implementation', 'learn', 'representation', 'use', 'deep', 'neural', 'network', 'representation', 'function', 'class', 'φ', 'base', 'equation', 'ﬁrst', 'formalize', 'bilevel', 'optimization', 'objective', 'ed', '−', '−', 'cid20γφscid48', 'πe', 'aφs', 'acid62cid3', 'cid21cid13', 'implementation', 'replace', 'hard', 'constraint', 'pre', 'sente', 'section', 'lagrange', 'multipli', 'use', 'optimal', 'design', 'constraint', 'regularization', 'term', 'learn', 'φ', 'speciﬁcally', 'maximize', 'log', 'det', 'covariance', 'matrix', 'induce', 'feature', 'maximize', 'eigenvalue', 'log', 'det', 'sum', 'log', 'eigen', 'value', 'experiment', 'result', 'section', 'demonstrate', 'indeed', 'improve', 'condition', 'number', 'experiment', 'use', 'target', 'network', 'implementation', 'namely', 'update', 'make', 'use', 'target', 'network', 'weight', 'exponential', 'move', 'average', 'representation', 'network', 'weight', 'idea', 'use', 'slow', 'move', 'target', 'network', 'show', 'stabilize', 'training', 'rl', 'et', 'representation', 'learn', 'literature', 'grill', 'summarize', 'give', 'ofﬂine', 'behavior', 'dataset', 'target', 'policy', 'iteratively', 'update', 'figure', 'representative', 'frame', 'control', 'suite', 'task', 'left', 'right', 'finger', 'turn', 'hard', 'run', 'quadrupe', 'walk', 'humanoid', 'stand', 'φ', 'use', 'result', 'learn', 'representation', 'perform', 'ope', 'see', 'implementation', 'hyperparameter', 'detail', 'show', 'experiment', 'update', 'procedure', 'signiﬁcantly', 'minimize', 'bellman', 'completeness', 'loss', 'also', 'improve', 'condition', 'number', 'key', 'quantity', 'ensure', 'good', 'performance', 'lspe', 'linear', 'regression', 'show', 'experiment', 'goal', 'answer', 'follow', 'question', 'representation', 'perform', 'downstream', 'lspe', 'com', 'pare', 'popular', 'unsupervised', 'representation', 'learn', 'technique', 'important', 'linear', 'bellman', 'completeness', 'optimal', 'design', 'component', 'learn', 'representation', 'competitive', 'ope', 'method', 'especially', 'evaluate', 'initial', 'state', 'distribution', 'follow', 'standard', 'evaluate', 'representation', 'pervise', 'learn', 'use', 'linear', 'evaluation', 'protocol', 'linear', 'regression', 'lspe', 'top', 'give', 'representa', 'tion', 'allow', 'focus', 'evaluate', 'quality', 'representation', 'compare', 'method', 'prior', 'technique', 'range', 'challenge', 'imagebase', 'continuous', 'control', 'task', 'benchmark', 'finger', 'turn', 'hard', 'run', 'quadrupe', 'walk', 'humanoid', 'stand', 'frame', 'task', 'show', 'figure', 'investigate', 'learn', 'representation', 'benchmark', 'representation', 'stateoftheart', 'selfsupervise', 'representation', 'learn', 'objective', 'adopt', 'curl', 'use', 'fonce', 'objective', 'contrastively', 'learn', 'staterepresentation', 'spr', 'adopt', 'byol', 'contrastive', 'learning', 'frame', 'work', 'learn', 'representation', 'latent', 'dynamic', 'note', 'modiﬁe', 'curl', 'ope', 'optimize', 'contrastive', 'loss', 'stateaction', 'pair', 'rather', 'state', 'spr', 'include', 'q', 'prediction', 'head', 'use', 'state', 'representation', 'latent', 'dynamic', 'state', 'action', 'representation', 'downstream', 'linear', 'evaluation', 'use', 'architecture', 'respective', 'representa', 'tion', 'evaluate', 'algorithm', 'compare', 'ope', 'method', 'additionally', 'compare', 'fit', 'muno', 'szepesvári', 'nachum', 'fqe', 'weight', 'doubly', 'robust', 'policy', 'evaluation', 'brunskill', 'model', 'base', 'eval', 'uation', 'modiﬁe', 'implementation', 'benchmark', 'library', 'release', 'fu', 'fqe', 'use', 'author', 'release', 'codebase', 'dreamerv2', 'bestdice2', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'figure', 'ope', 'curve', 'ﬁve', 'seed', 'use', 'representation', 'train', 'spr', 'curl', 'ofﬂine', 'dataset', 'table', 'mator', 'original', 'initial', 'state', 'distribution', 'd0', 'ﬁrst', 'setting', 'ensure', 'baseline', 'algorithm', 'sit', 'isfy', 'coverage', 'condition', 'thus', 'demonstrate', 'unique', 'beneﬁt', 'learn', 'bellman', 'complete', 'representation', 'second', 'setting', 'evaluate', 'robustness', 'ability', 'estimate', 'original', 'd0', 'evaluation', 'onpolicy', 'offpolicy', 'datum', 'far', 'investigate', 'importance', 'learn', 'linear', 'feature', 'experiment', 'learn', 'representation', 'ofﬂine', 'dataset', 'also', 'contain', 'stateaction', 'pair', 'target', 'policy', 'speciﬁcally', 'train', 'representation', 'dataset', 'contain', 'behavior', 'policy', 'target', 'policy', 'sample', 'note', 'experiment', 'paragraph', 'use', 'mixture', 'dataset', 'addition', 'onpolicy', 'datum', 'target', 'policy', 'focus', 'role', 'linear', 'ope', 'performance', 'density', 'ratio', 'relative', 'condition', 'number', 'eq', 'ie', 'omit', 'design', 'regularization', 'focus', 'minimize', 'bellman', 'completeness', 'loss', 'figure', 'leave', 'show', 'outperform', 'baseline', 'setting', 'even', 'figure', 'left', 'root', 'mean', 'squared', 'evaluation', 'error', 'task', 'right', 'mean', 'spearman', 'rank', 'correlation', 'task', 'target', 'policy', 'train', 'use', 'author', 'imple', 'mentation', 'drqv2', 'yarat', 'stateofthe', 'art', 'offpolicy', 'actor', 'critic', 'visionbase', 'contin', 'uous', 'control', 'highquality', 'target', 'policy', 'collect', 'rollout', 'linear', 'evaluation', 'protocol', 'predict', 'discounted', 'return', 'ofﬂine', 'behavior', 'dataset', 'collect', 'sample', 'train', 'policy', 'mean', 'performance', 'roughly', 'quarter', 'target', 'policy', 'table', 'result', 'aggregate', 'ﬁve', 'random', 'seed', 'see', 'detail', 'hyperparameter', 'environ', 'ment', 'dataset', 'composition', 'ope', 'lspe', 'learn', 'representation', 'figure', 'compare', 'ope', 'performance', 'spr', 'curl', 'representation', 'learn', 'outper', 'form', 'learn', 'spr', 'curl', 'task', 'spr', 'curl', 'exhibit', 'exponential', 'error', 'ampliﬁca', 'tion', 'respect', 'number', 'iteration', 'lspe', 'suffer', 'blowup', 'ope', 'performance', 'figure', 'compare', 'ope', 'performance', 'multiple', 'benchmark', 'literature', 'competitive', 'fqe', 'evaluate', 'well', 'bench', 'mark', 'task', 'test', 'also', 'evaluate', 'well', 'estimate', 'value', 'rank', 'policy', 'follow', 'fu', 'use', 'spearman', 'rank', 'correlation', 'metric', 'compute', 'correlation', 'ordinal', 'ranking', 'accord', 'ope', 'estimate', 'ground', 'truth', 'value', 'rank', 'evaluate', 'additional', 'target', 'policy', 'mean', 'perfor', 'mance', 'roughly', 'target', 'policy', 'figure', 'present', 'mean', 'correlation', 'evaluation', 'task', 'competitive', 'fqe', 'consistently', 'well', 'benchmark', 'rank', 'policy', 'investigation', 'different', 'setting', 'section', 'consider', 'additional', 'setting', 'ofﬂine', 'dataset', 'contain', 'onpolicy', 'datum', 'ensure', 'ofﬂine', 'datum', 'provide', 'coverage', 'evaluation', 'policy', 'state', 'action', 'distribution', 'evaluate', 'figure', 'left', 'evaluation', 'mixture', 'dataset', 'onpolicy', 'offpolicy', 'datum', 'note', 'addition', 'target', 'policy', 'datum', 'bound', 'relative', 'condition', 'number', 'eq', 'right', 'evaluation', 'initial', 'state', 'distribution', 'give', 'ofﬂine', 'datum', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'figure', 'ope', 'curve', 'run', 'blue', 'red', 'doptimal', 'design', 'base', 'center', 'bar', 'plot', 'singular', 'value', 'feature', 'covariance', 'matrix', 'left', 'plot', 'right', 'ope', 'curve', 'finger', 'turn', 'hard', 'blue', 'red', 'optimize', 'linear', 'match', 'fqe', 'performance', 'task', 'experiment', 'corroborate', 'analysis', 'explicitly', 'enforce', 'learned', 'representation', 'linear', 'improve', 'ope', 'note', 'fact', 'lspe', 'spr', 'curl', 'feature', 'still', 'blow', 'mixture', 'datum', 'mean', 'representation', 'failure', 'due', 'coverage', 'evaluation', 'initial', 'state', 'distribution', 'far', 'investigate', 'beneﬁts', 'optimize', 'doptimal', 'design', 'improve', 'coverage', 'investigate', 'ope', 'initial', 'state', 'distribution', 'support', 'rem', 'note', 'representation', 'exactly', 'bellman', 'complete', 'also', 'correspond', 'feature', 'trix', 'wellconditione', 'able', 'evaluate', 'well', 'state', 'speciﬁcally', 'experiment', 'evaluate', 'timestep', 'target', 'policy', 'rollout', 'initial', 'state', 'distribution', 'figure', 'right', 'show', 'able', 'robustly', 'evaluate', 'outofdistribution', 'benchmark', 'ablation', 'study', 'impact', 'optimal', 'design', 'regularization', 'gate', 'impact', 'maximize', 'doptimal', 'design', 'ablate', 'design', 'regularization', 'term', 'objective', 'analyze', 'downstream', 'evaluation', 'performance', 'respective', 'feature', 'covariance', 'matrix', 'ofﬂine', 'dataset', 'figure', 'center', 'present', 'bar', 'plot', 'sin', 'gular', 'value', 'feature', 'covariance', 'matrix', 'figure', 'leave', 'show', 'stream', 'ope', 'performance', 'feature', 'train', 'design', 'regularization', 'run', 'task', 'note', 'regularization', 'ﬁnd', 'feature', 'covariance', 'matrix', 'much', 'bad', 'condition', 'number', 'feature', 'less', 'exploratory', 'analysis', 'suggest', 'also', 'observe', 'deterioration', 'evaluation', 'performance', 'design', 'regularization', 'explicitly', 'learn', 'exploratory', 'feature', 'impact', 'linear', 'completeness', 'figure', 'right', 'present', 'ablation', 'study', 'design', 'term', 'objective', 'ﬁnd', 'downstream', 'ope', 'performance', 'degrade', 'directly', 'op', 'timize', 'linear', 'suggest', 'feature', 'good', 'coverage', 'alone', 'enough', 'avoid', 'error', 'ampliﬁcation', 'conclusion', 'present', 'leverage', 'rich', 'function', 'approxi', 'mation', 'learn', 'bellman', 'complete', 'exploratory', 'repre', 'sentation', 'stable', 'accurate', 'ofﬂine', 'policy', 'evaluation', 'provide', 'mathematical', 'framework', 'representation', 'learn', 'ofﬂine', 'generalize', 'exist', 'repre', 'sentation', 'learn', 'framework', 'theory', 'literature', 'provide', 'endtoend', 'theoretical', 'analysis', 'ap', 'proach', 'ope', 'demonstrate', 'accurately', 'estimate', 'policy', 'value', 'polynomial', 'sample', 'complex', 'ity', 'notably', 'complexity', 'explicit', 'dependence', 'size', 'state', 'action', 'space', 'instead', 'depend', 'statistical', 'complexity', 'representation', 'hypothesis', 'class', 'experimentally', 'extensively', 'evaluate', 'approach', 'deepmind', 'control', 'suite', 'set', 'image', 'base', 'continuous', 'control', 'robotic', 'task', 'first', 'show', 'linear', 'evaluation', 'protocol', 'use', 'linear', 'regre', 'sion', 'top', 'representation', 'classic', 'lspe', 'framework', 'approach', 'outperform', 'prior', 'representa', 'tion', 'technique', 'curl', 'spr', 'leverage', 'contrastive', 'representation', 'learn', 'technique', 'simclr', 'byol', 'spectively', 'also', 'show', 'achieve', 'competitive', 'ope', 'performance', 'stateoftheart', 'fqe', 'notice', 'ably', 'improve', 'evaluate', 'initial', 'state', 'distribution', 'finally', 'ablation', 'show', 'mate', 'linear', 'coverage', 'crucial', 'ingredient', 'success', 'future', 'work', 'include', 'extend', 'ofﬂine', 'policy', 'optimization', 'acknowledgement', 'material', 'base', 'work', 'support', 'national', 'science', 'foundation', 'grant', 'cor', 'nell', 'university', 'fellowship', 'thank', 'kidambi', 'ban', 'anonymous', 'reviewer', 'useful', 'discussion', 'feedback', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'reference', 'agarwal', 'forcement', 'learning', 'theory', 'uw', 'seattle', 'seattle', 'agarwal', 'kakade', 'krishnamurthy', 'structural', 'complexity', 'representation', 'learn', 'ing', 'low', 'rank', 'mdps', 'neurip', 'amortila', 'variant', 'lower', 'bind', 'discount', 'set', 'arxiv', 'preprint', 'anand', 'racah', 'e', 'ozair', 'bengio', 'state', 'rep', 'hjelm', 'r', 'resentation', 'learn', 'atari', 'beygelzimer', 'e', 'b', 'garnett', 'r', 'ed', 'neurip', 'neuripsccpaper2019hash', 'anto', 'szepesvári', 'c', 'muno', 'r', 'fit', 'roweis', 'ed', 'nip', 'volume', 'curran', 'qiteration', 'continuous', 'actionspace', 'mdps', 'platt', 'koll', 'singer', 'pdf', 'r', 'matrix', 'analysis', 'volume', 'springer', 'science', 'business', 'medium', 'lugosi', 'massart', 'p', 'concentration', 'inequalitie', 'nonasymptotic', 'theory', 'consideration', 'batch', 'reinforcement', 'learning', 'icml', 'pmlr', 'e', 'simple', 'framework', 'contrastive', 'learning', 'visual', 'representation', 'icml', 'chung', 'white', 'timescale', 'network', 'nonlinear', 'value', 'function', 'approxi', 'mation', 'evaluation', 'linear', 'function', 'approximation', 'icml', 'pmlr', 'foster', 'krishnamurthy', 'simchilevi', 'reinforcement', 'learn', 'fundamental', 'barri', 'er', 'value', 'function', 'approximation', 'arxiv', 'preprint', 'fu', 'tucker', 'paine', 'benchmark', 'deep', 'offpolicy', 'evaluation', 'iclr', 'altché', 'richemond', 'p', 'buchatskaya', 'e', 'pire', 'azar', 'muno', 'bootstrap', 'latent', 'new', 'approach', 'selfsupervise', 'learning', 'r', 'ed', 'neurip', 'volume', 'curran', 'neuripsccpaper2020file', 'f3ada80d5c4ee70142b17b8192b2958epap', 'pdf', 'abbeel', 'p', 'soft', 'actor', 'critic', 'offpolicy', 'maximum', 'deep', 'reinforcement', 'learn', 'stochastic', 'actor', 'icml', 'pmlr', 'hafner', 'p', 'master', 'atari', 'discrete', 'world', 'model', 'ternational', 'conference', 'learn', 'representation', 'lattimore', 'sparse', 'feature', 'selection', 'make', 'batch', 'reinforcement', 'learn', 'sample', 'efﬁcient', 'icml', 'pmlr', 'contrast', 'unsupervised', 'visual', 'representation', 'learn', 'cvpr', 'doubly', 'robust', 'offpolicy', 'value', 'evalua', 'tion', 'reinforcement', 'learning', 'icml', 'pmlr', 'class', 'tural', 'framework', 'provable', 'generalization', 'preprint', 'agarwal', 'langford', 'schapire', 'r', 'e', 'contextual', 'decision', 'process', 'low', 'bellman', 'rank', 'paclearnable', 'icml', 'pmlr', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'provably', 'efﬁcient', 'reinforcement', 'learn', 'linear', 'function', 'proximation', 'colt', '2137–2143', 'pmlr', 'nachum', 'statistical', 'bootstrapping', 'uncertainty', 'estimation', 'offpolicy', 'evaluation', 'abbeel', 'p', 'curl', 'contrastive', 'unsupervised', 'representation', 'reinforcement', 'learning', 'icml', 'pmlr', 'lattimore', 'szepesvári', 'c', 'bandit', 'algorithm', 'cam', 'bridge', 'university', 'press', 'lillicrap', 'heess', 'erez', 'tassa', 'wierstra', 'continuous', 'control', 'deep', 'reinforcement', 'learning', 'iclr', 'painterwakeﬁeld', 'littman', 'analysis', 'linear', 'model', 'approximation', 'feature', 'selection', 'icml', 'reinforcement', 'learn', 'association', 'e', 'machinery', 'url', 'isbn', 'pollard', 'empirical', 'process', 'theory', 'application', 'nsfcbms', 'regional', 'conference', 'series', 'probability', 'statistic', 'i–86', 'jstor', 'schwarzer', 'anand', 'goel', 'r', 'hjelm', 'r', 'c', 'bachman', 'p', 'dataefﬁcient', 'rein', 'forcement', 'learn', 'momentum', 'predictive', 'represen', 'tation', 'iclr', 'url', 'abs200705929', 'mazoure', 'b', 'combe', 'r', 'doan', 'bachman', 'p', 'hjelm', 'r', 'deep', 'reinforcement', 'infomax', 'learn', 'r', 'ed', 'neurip', 'volume', 'neuripsccpaper2020file', 'pdf', 'mnih', 'grave', 'antonoglou', 'wierstra', 'riedmiller', 'playing', 'atari', 'deep', 'reinforcement', 'learning', 'nip', 'deep', 'learning', 'workshop', 'modi', 'agarwal', 'modelfree', 'representation', 'learn', 'preprint', 'exploration', 'lowrank', 'mdps', 'muno', 'r', 'szepesvári', 'c', 'finitetime', 'bound', 'ﬁtte', 'value', 'iteration', 'jmlr', 'nachum', 'provable', 'representation', 'learn', 'imitation', 'contrastive', 'fouri', 'feature', 'neurip', 'bertsekas', 'p', 'least', 'square', 'policy', 'evalua', 'tion', 'algorithm', 'linear', 'function', 'approximation', 'crete', 'event', 'dynamic', 'system', 'r', 'learn', 'good', 'state', 'action', 'representation', 'tensor', 'decomposition', 'ieee', 'international', 'symposium', 'information', 'theory', 'isit', 'ieee', 'learn', 'reinforcement', 'song', 'parr', 'r', 'linear', 'feature', 'encoding', 'garnett', 'r', 'ed', 'nip', 'volume', 'url', 'neuripsccpaper2016file', 'pdf', 'muldal', 'erez', 'merel', 'preprint', 'brunskill', 'dataefﬁcient', 'offpolicy', 'policy', 'evaluation', 'reinforcement', 'learn', 'url', 'pessimistic', 'modelbase', 'ofﬂine', 'reinforcement', 'learning', 'partial', 'coverage', 'preprint', 'uehara', 'representation', 'learn', 'online', 'ofﬂine', 'rl', 'lowrank', 'mdps', 'vinyal', 'representation', 'learn', 'contrastive', 'predictive', 'code', 'wellner', 'weak', 'con', 'vergence', 'empirical', 'process', 'springer', 'isbn', 'rie', 'statistic', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'wainwright', 'statistic', 'non', 'asymptotic', 'viewpoint', 'volume', 'press', 'p', 'kakade', 'statistical', 'limit', 'ofﬂine', 'linear', 'exponential', 'lower', 'bind', 'linearlyrealizable', 'mdps', 'constant', 'suboptimality', 'gap', 'preprint', 'nachum', 'representation', 'matter', 'ofﬂine', 'pretraine', 'sequential', 'decision', 'making', 'ed', 'icml', 'volume', 'proceeding', 'machine', 'learn', 'research', 'pmlr', 'schuurman', 'offpolicy', 'evaluation', 'regularize', 'lagrangian', 'yarat', 'fergus', 'r', 'lazaric', 'l', 'master', 'visual', 'continuous', 'control', 'improve', 'dataaugmente', 'inforcement', 'learn', 'preprint', 'yarat', 'fergus', 'r', 'improve', 'sample', 'efﬁciency', 'model', 'free', 'reinforcement', 'learn', 'image', 'q', 'ably', 'efﬁcient', 'representation', 'learn', 'lowrank', 'markov', 'decision', 'process', 'arxiv', 'preprint', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'appendice', 'metric', 'entropy', 'integral', 'recall', 'standard', 'notion', 'entropy', 'integral', 'base', 'follow', 'distance', '−', 'let', 'φ', 'denote', 'tcovering', 'number', 'deﬁnition', 'deﬁne', 'integral', 'assume', 'ﬁnite', 'κφ', 'n', 'ﬁnite', 'φ', 'olog12φ', 'technical', 'let', 'iid', 'random', 'variable', 'e', 'least', '−', 'δ', 'cid3', 'ν', 'wp', 'i1', 'e', 'inf', 'positive', 'l', 'particular', 'i1', 'xi', '−', 'e', 'e', 'c', 'n', 'proof', 'first', 'theorem', '−', 'δ', 'i1', 'xi', '−', 'e', 'use', 'fact', 'ay2', 'split', 'square', 'root', 'term', 'yield', 'ﬁrst', 'part', 'pick', 'l', 'conclude', 'proof', 'inf', 'state', 'several', 'result', 'orlicz', 'norm', 'mostly', 'pollard', 'completeness', 'increase', 'positive', 'function', 'r', 'r', 'deﬁne', 'norm', 'inf', 'indeed', 'norm', 'φorlicz', 'space', 'random', 'variable', 'lφν', 'interpret', 'minkowski', 'functional', 'set', 'e', 'let', 'iid', 'datapoint', 'draw', 'underlie', 'distribution', 'let', 'ω', 'denote', 'randomness', 'n', 'sample', 'datapoint', 'let', 'fcid9', 'denote', 'random', 'set', 'vector', 'datum', 'correspond', 'ω', 'let', 'σ', 'denote', 'vector', 'iid', 'rademach', 'random', 'variable', '±1', 'equiprobably', 'independent', 'else', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'symmetrization', 'increase', 'positive', 'function', 'i1', 'fi', 'eωfi', 'sup', '∈fω', 'proof', 'see', 'theorem', 'pollard', 'let', 'suppose', 'component', 'r', 'llipschitz', 'increase', 'positive', 'function', 'cid33cid35', 'σ', 'sup', 'σ', 'proof', 'apply', 'theorem', 'pollard', 'function', 'λil', 'contraction', 'focus', 'speciﬁc', 'orlicz', 'space', 'variable', 'function', 'ψorlicz', 'norm', 'maximum', 'random', 'variable', 'bound', 'maximum', 'norm', 'expx2', 'b4', 'random', 'variable', 'cid107', 'i≤m', 'proof', 'see', 'pollard', 'cid1072', 'proof', 'see', 'follow', 'truncated', 'chaining', 'result', 'orlicz', 'norm', 'result', 'new', 'many', 'source', 'state', 'prove', 'term', 'covering', 'rademach', 'complexity', 'rather', 'orlicz', 'norm', 'particular', 'generalize', 'theorem', 'pollard', 'consider', 'sequence', '’s', 'converge', 'b6', 'chaining', 'let', 'sup', 'σ', 'inf', 'fdδ', 'cid107f', 'cid1072', 'dδ', 'δpacke', 'number', 'proof', 'suppose', 'packing', 'number', 'ﬁnite', 'otherwise', 'right', 'hand', 'side', 'inﬁnite', 'show', 'arbitrary', 'construct', 'sequence', 'ﬁner', 'ﬁner', 'approximation', 'f1', 'satisﬁes', 'property', 'exist', 'cid1072', 'b2−k', 'indeed', 'iteratively', 'construct', 'fk1', 'add', 'element', 'construct', 'maximal', 'b2−k', 'packing', 'maximality', 'ensure', 'distance', 'requirement', 'existence', 'vector', 'large', 'distance', 'add', 'packing', 'deﬁnition', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'triangle', 'inequality', 'σ', 'sup', 'loosely', 'bind', 'rightmost', 'term', 'cauchyschwarz', 'σ', 'cid1072', 'b2−k', 'sup', 'σ', 'max', 'log', 'nonnegative', 'constant', 'log', 'suprema', 'take', 'ﬁnite', 'set', 'maximum', 'attain', 'hence', 'apply', 'special', 'property', 'get', 'expcc2', '≤', 'c√', 'max', 'σ', 'cid13ψ', 'max', 'max', 'max', 'logfk1', 'max', 'cid1072', 'logfk1', 'b2−k', 'also', 'note', 'construction', 'cid107maxf', '∈f0', 'σ', 'unroll', 'cid107', '≥', 'log1', 'monotone', 'decrease', 'log', '44b', 'k0', '−', 'logdb2−k1', 'b2−k1', 'cid112logdδ', 'fdδ', 'b', 'b2−k', 'fdδ', 'consider', 'pick', 'cid107', 'fdδ', 'arbitrary', 'bound', 'hold', 'take', 'inﬁmum', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'c', 'proof', 'lspe', 'ﬁrst', 'show', 'generalization', 'performance', 'difference', 'generalize', 'pdl', 'policy', 'π', 'πcid48', 'function', '×', 'cid55→', 'r', 'initial', 'state', 'distribution', 'µ', '−', 'γ', 'esa∼dπ', 'µ', 'cid104', 'πcid48', 'πcid48', 'proof', 'let', 'π', 'distribution', 'trajectory', 's0', 'roll', 'π', 'π', 'γtrst', 'πcid48', 'πcid48', 'πcid48', '∞', '∞', '∞', '−', 'γ', '−', 'γ', 'esa∼dπ', 'µ', 'cid2rs', 'scid48', 'πcid48', 'πcid48cid3', 'esa∼dπ', 'µ', 'cid104', 'πcid48', 'generalize', 'pdl', 'get', 'set', 'µ', '−', 'πcid48', '−', 'γ', '−', 'γ', '−', 'γ', 'esa∼dπ', 'esa∼dπ', 'µ', '−', 'qπcid48', '−', 'πcid48', 'esa∼dπ', 'µ', 'prove', 'lspe', 'guarantee', 'instantiate', 'estimated', 'lspe', 'set', 'πcid48', 'give', 'expression', 'prediction', 'error', 'lspe', 'π', 'µ', 'cid104', 'cid98fks', 'π', '−', 'γ', 'edπ', 'µ', 'cid104', 'cid98fks', '−', 'fks', 'upper', 'bind', 'right', 'hand', 'side', 'l2dπ', 'run', 'lspe', 'regression', 'loss', 'step', 'consider', 'policy', 'π', 'function', '×', 'cid55→', 'r', 'satisfy', 'maxk1k', '−', 'η', 'k', '−', 'norm', 'bellman', 'error', 'next', 'bind', 'bellman', 'error', 'proof', '−', 'γesa∼dπ', 'cid107', 'πfk−1', '−', 'πfkcid107l2dπ', '◦', 'fk−1scid48', 'acid48', '−', 'fkscid48', 'scid48acid48∼p', 'sa', '◦', 'πfk−1scid48', 'acid48', '−', 'fkscid48', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'thus', 'γe˜s˜a∼dπ', 'p0', 'p', 'quantity', 'expectation', 'square', 'fk−1s', '−', 'fks', 'γcid107fk−1', '−', 'fkcid107l2dπ', 'γ', 'cid107fk−1', '−', 'cid17', 'unroll', 'recursion', 'use', '−', '−', 'γk', '−', 'give', 'claim', '−', 'γk', 'follow', 'fast', 'rateslike', 'result', 'norm', 'show', 'norm', 'induce', 'empirical', 'covariance', 'matrix', 'bound', 'norm', 'induce', 'time', 'population', 'covariance', 'matrix', 'σ', 'cid101on', '−12', 'term', 'lemma', 'fast', 'rate', 'σnorm', 'probability', 'least', '−', 'δ', 'proof', 'first', 'almost', 'surely', 'e', 'cid2xcid124φ', 'a4cid3', 'cid2xcid124φ', 'a2cid3', 'least', '−', 'δ', 'i1', 'ai2', '−', 'e', 'a2cid3', 'e', 'cid2xcid124φ', 'a2cid3', 'particular', 'mean', 'net', 'union', 'bind', 'element', 'approximation', 'error', 'cover', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'close', 'element', 'net', 'x', 'similarly', 'cid107xcid107σ', 'dδ', 'dδ', 'deﬁne', 'follow', 'notation', 'analyze', 'least', 'square', 'policy', 'evaluation', 'lspe', 'target', 'vector', 'deﬁne', 'sayϑ', 'π', 'i1', 'yϑ', '−', 'ai2', 'cid98cid96θ', 'following', 'lemma', 'useful', 'fact', 'optimal', 'θϑ', 'c4', 'θ', '−', 'cid98θϑ2', 'ϑ', '−', 'cid96θϑ', 'σ', 'recall', 'minimize', 'e', '−', 'θcid124φs', 'jacobian', 'proof', 'first', 'cid2θcid124φ', '−', 'yϑφs', 'acid3', 'optimal', 'necessary', 'optimality', 'condition', 'acid3', 'normal', 'cone', 'θ', 'particular', 'add', 'get', 'ayϑ1', 'θ', 'ϑ1', 'acid3', 'φs', 'acid3', 'θϑ1', '−', '−', 'cid1072', 'φs', 'aθϑ1', 'cid124φ', 'θϑ1', '−', 'cid2yϑ1', 'acid3', 'θϑ1', '−', 'aφscid48', 'aφscid48', 'πcid124', 'cid1072', '2γw', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'claim', 'cid98θϑ1', 'cid98θϑ2', 'follow', 'argument', 'second', 'claim', 'ﬁrst', 'apply', 'parallelogram', 'law', 'follow', 'ﬁrstorder', 'optimality', 'θϑ', 'eνθϑ', '−', '−', 'aφ', 'acid124θϑ', 'θ', 'eνθϑ', '−', 'σ', 'show', 'key', 'lemma', 'concentration', 'least', 'square', 'uniformly', 'target', 'generate', 'c5', 'concentration', 'least', 'square', 'let', 'cid55→', 'wp', 'least', '−', 'δ', 'cid107ˆθϑ', 'θϑcid107σ', 'sup', 'proof', 'c3', 'wp', 'least', '−', 'δ', 'bind', 'random', 'cid107', 'cid107', 'simultaneously', 'vector', 'ball', 'norm', 'vice', 'versa', 'e', 'cid107xcid107σ', 'cid98σ', 'provide', 'probability', 'expectation', 'implicitly', 'condition', 'e', 'remainder', 'proof', 'condition', 'highprobability', 'event', 'first', 'show', 'arbitrary', 'ﬁxe', 'ϑ', '∈', 'least', '−', 'δ', 'cid107cid98θϑ', 'θϑcid107σ', 'bind', 'probability', 'complement', 'turn', 'simpliﬁe', 'follow', 'chain', 'argument', 'cid107ˆθϑ', 'θϑcid107σ', '≥', 'ϑ', '−', 'cid96θϑ', 'cid96ˆθϑ', 'cid96θ', 'ϑ', '−', 'ˆcid96θ', 'convexity', 'continuity', 'make', 'strict', 'equality', 'indeed', 'give', 'intermediate', 'theorem', 'exist', 'θcid48', '−', 'λθϑ', 'cid96θcid48', '−', 'cid96θϑ', 'ν', 'convexity', 'ˆcid96θcid48', '−', 'λˆcid96θ', 'ϑ', '−', '−', 'cid96θ', 'ϑ', '−', 'cid96θϑ', 't2', 'ˆcid96θ', '−', 'ˆcid96θ', 'ϑ', '−', 'cid96θϑ', 'conditioning', 'ϑ', '−', 'ˆcid96θ', 'ϑ', '−', 'cid96θϑ', 'hence', 'focus', 'bound', 'p', 'sup', 'θ∈θ', 'i1', 'ζiθ', 'ϑ', '−', 'ζiθϑ', 'eν', 'ϑ', '−', 'ζiθϑ', '≥', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'deﬁne', 'ζiθ', 'ai2', 'ζiθ', 'ϑ', '−', 'ζiθϑ', '−', 'ai', 'tell', 'sufﬁce', 'consider', 'simple', 'class', 'deﬁne', '−', 'ai', 'ωiθ', 'ϑn', 'chain', 'b6', 'σ', 'b', 'inf', '3α', 'n', 'cid112logdδ2', 'ωθdδ', 'dδ', 'euclidean', 'packing', 'number', 'supθ', 'envelope', 'bind', 'truncated', 'first', 'notice', 'localize', 'base', 'deﬁnition', 'sup', 'i1', 'bind', 'packing', 'number', 'let', 'arbitrary', 'cid107ωθ1', 'ωθ2', 'θ2cid107', 'σmaxcid98σcid107θ1', '−', 'θ2cid1072', 'ε', 'construct', 'εcover', 'set', 'ε', 'denote', 'euclidean', 'cover', 'number', 'require', 'point', 'let', 'log', 'dε', 'log', 'ωθdε', 'logw', 'cεdε', 'c', 'assume', 'put', 'together', 'let', 'c', 'denote', 'positive', 'constant', 'p', 'i1', 'ζiθ', 'ϑ', '−', 'ζiθϑ', 'eν', 'ϑ', '−', 'ζiθϑ', 'sup', 'θ', '≥', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'increase', 'i1', 'ζiθ', 'ϑ', '−', 'ζiθϑ', 'eν', 'ϑ', '−', 'ζiθϑ', '≥', 't2cid1', 'inequality', 'supθ', 'i1', 'ζiθ', 'ϑ', '−', 'ζiθϑ', 'eν', 'ϑ', '−', 'ζiθϑ', 'cid17cid105', 'cn', 't2', 'e', 'supθ', 'σ', 'ζθ', '−', 'ζθϑ', 'ψ', 'cn', 't2', 'contraction', 'ζiθ', 'ϑ', '−', 'ζiθϑ', 'λiωiθ', 'ϑ', 'lipschitz', 'e', '4cl', 'supθ', 'σ', 't2', 'set', '4lj', 'apply', 'chain', 'numerator', 'bound', 'cid192cid33', 't2', '4lj', 'apply', 'upper', 'bind', 'dn', 'log', 'cid332\uf8f6', '\uf8f8', 'log4', 'set', 'logn', 'logn', 'lognd', '8δ', 'hence', 'show', 'arbitrary', 'ﬁxe', 'ϑ', '∈', 'wp', '−', 'cid107cid98θϑ', 'θϑcid107σ', 'logn', 'ﬁnally', 'apply', 'union', 'bind', 'consider', 'wn', 'cover', 'require', 'point', 'ϑ', 'cid107cid98θϑ', 'θϑcid107σ', 'cid107cid98θϑ', '−', 'cid98θnϑcid107σ', '−', 'θnϑcid107σ', 'θϑcid107σ', 'cid98θnϑcid107', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'thus', 'show', 'sup', 'cid107cid98θϑ', 'θϑcid107σ', 'nice', 'corollary', 'σ', 'provide', 'full', 'coverage', 'positive', 'β', 'bind', 'sup', 'cid107cid98θϑ', 'cid107cid98θϑ', 'θϑcid107σ', 'main', 'lspe', 'theorem', 'prove', 'lspe', 'sample', 'complexity', 'guarantee', 'theorem', 'theorem', 'sample', 'complexity', 'lspe', 'assume', 'feature', 'satisﬁes', 'approximate', 'linear', 'parameter', 'εν', 'wp', 'least', '−', 'δ', 'initial', 'state', 'distribution', 'cid12v', 'πe', 'cid104', 'cid98fks', 'πe', '−', 'ddπe', 'dν', 'cid13∞', '−', 'γ2', 'logn', 'γ2', 'cid98fk', 'output', 'proof', 'π', 'cid104', 'cid98fks', 'π', 'γ2', 'max', 'ˆfk', '−', 'next', 'bind', 'maximum', 'regression', 'error', 'consider', 'initial', 'state', 'distribution', 'p0', 'max', 'k12', 'ˆfk', '−', 'sup', 'ϑφ', '−', 'sup', '−', 'θ', 'ϑφ', '−', 'sup', 'θϑcid107σ', 'ϑφ', '−', 'cid115cid13', 'ddπ', 'dν', 'ddπ', 'p0', 'dν', 'cid13∞', 'cid13∞', 'sup', 'θϑcid107σ', 'quantity', 'cid107ˆθϑ', 'θϑcid107σ', 'directly', 'bound', 'least', '−', 'thus', 'show', 'desire', 'result', 'initial', 'state', 'distribution', 'π', '−', 'π', 'cid104', 'cid98fks', 'γ2', 'cid115cid13', 'ddπ', 'dν', 'cid13∞', 'εν', '−', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'proof', 'linear', 'consider', 'feature', 'φ', 'full', 'rank', 'covariance', 'give', 'feature', 'linear', 'imply', 'exist', '×', 'rd×d', 'cid1072', '−', 'eν', 'φs', '−', 'cid20γescid48∼p', 'saφscid48', 'πe', 'hand', 'exist', '×', 'rd×d', 'cid1072', 'equality', 'hold', 'satisfy', 'exact', 'linear', 'cid1072', 'proof', 'suppose', 'cid107', 'cid1072', 'satisfy', 'φs', '−', 'cid20γescid48∼p', 'saφscid48', 'πe', 'w1', 'set', 'w2', 'satisﬁes', 'φs', '−', 'rs', '−', 'φscid48', 'πecid12', '−', 'πe', 'ν', '−', 'rs', 'w', 'cid0', 'φs', '−', 'sa', 'φscid48', '−', 'φscid48', 'πecid12', 'also', 'cid107', 'cid107ρcid1072', 'cid1072', 'thus', 'satisﬁes', 'exact', 'linear', 'suppose', 'satisﬁes', 'exact', 'linear', 'min', '−', 'ν', 'see', 'exist', '∈', 'linearize', 'reward', 'ν', 'set', 'esa∼ν', 'cid13wcid62', 'φs', 'rs', 'acid13', 'let', 'minimizer', 'objective', 'need', 'show', 'exist', '∈', 'rd×d', 'cid1072', 'satisﬁes', 'esa∼ν', 'φs', '−', 'saφscid48', 'πecid13', 'extract', 'row', 'plug', 'note', 'exact', 'linear', 'know', 'exist', 'vector', 'φs', '−', '−', 'sae', 'repeat', 'construct', 'follow', 'v1', '\uf8ef', '\uf8f0', '\uf8fb', 'φscid48', 'πecid13', 'satisﬁes', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'esa∼ν', 'φs', '−', 'saφscid48', 'πecid13', 'esa∼ν', 'cid0', '−', 'saφscid48', 'πecid1cid13', 'esa∼ν', 'w', '−', 'sae', 'φscid48', 'πe', 'hence', 'φs', '−', 'saφscid48', 'πecid13', 'ν', 'first', 'show', 'cid1072', 'w1', 'exact', 'linear', 'finally', 'show', 'cid1072', 'exist', 'φscid48', 'πecid13', 'ν', 'construction', 'φs', 'satisﬁes', '−', '−', 'positive', 'deﬁnite', 'unique', 'choice', 'w2', 'exact', 'linear', 'hence', 'show', 'w1', 'also', 'take', 'w1', 'subtract', 'expression', 'yield', 'true', 'arbitrary', 'w1', 'take', 'supremum', 'w1', 'show', 'cid1072', 'φs', '−', 'rs', '−', 'w', '−', 'show', 'inequality', 'strict', 'consider', 'singular', 'value', 'decomposition', 'orthonormal', 'basis', 'rd', 'ith', 'large', 'singular', 'value', 'loss', 'generality', 'suppose', 'always', 'ﬂip', 'sign', 'u1', 'pick', 'argument', 'previous', 'paragraph', '∈', 'imply', 'σiuiv', '≥', 'cid107', 'ρcid1072', 'σ1u1', 'u1', '−', 'ρcid124u1u1', 'orthogonal', 'pythagora', 'cid107ρ', 'σ1ρcid124u1', 'cid107ρ', 'σ1ρcid124u1', 'cid107ρ', 'σ1ρcid124u1', 'hence', 'get', 'follow', 'inequality', 'solve', 'use', 'fact', 'cid107ρ2cid1072', '−', '−', 'ﬁnally', 'show', 'prove', 'contradiction', 'assume', '≥', 'follow', 'argument', 'take', 'w1', 'discuss', 'case', 'first', 'rangem', 'case', 'nonzero', 'entry', 'thus', 'case', 'lead', 'contradiction', 'cid107', 'cid107', 'w', 'long', 'second', 'rangem', 'case', 'exist', 'vector', 'x', 'consider', '¯x', '¯x', 'mean', 'cause', 'contradiction', 'happen', 'horizon', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'llbcφ', 'min', 'show', 'approximate', 'version', 'equivalence', 'proposition', 'first', 'recall', 'bilevel', 'loss', 'equation', 'use', 'denote', 'population', 'empirical', 'version', 'follow', 'cid20', 'cid20γescid48∼p', 'sa', 'φscid48', 'πe', 'rs', 'cid21cid13', '×', 'cid107', 'cid1072', 'ρcid63', 'correspond', 'linear', 'd1', 'suppose', 'feature', 'cid98φ', 'satisﬁes', 'ε2', 'approximately', 'linear', 'provide', '−', 'γφscid48', 'cid20γφscid48', 'πe', 'cid98llbcφ', 'cid21cid13', '−', 'φs', '−', 'ρm', 'ed', 'cid63cid1072', 'proof', 'suppose', 'ε2', 'exist', 'cid1072', 'cid98ρ', 'cid98ρcid124', 'cid20γescid48∼p', 'saφscid48', 'πe', 'φs', 'ε2', 'w1', 'take', 'cid1072w', '≤', 'cid107ρcid63cid1072', 'cid107', 'cid63cid1072w', 'assumption', 'hence', 'max', 'φs', '−', 'rs', '−', 'min', 'cid124w1cid124φs', '−', 'rs', '−', 'φscid48', 'πecid13', 'φscid48', 'cid124w1cid124φs', '−', 'rs', '−', '−', 'rs', 'a2cid105', 'φs', '−', 'sa', 'φscid48', 'πe', 'cid172cid21', 'cid172cid21', 'φscid48', 'πe', 'max', 'desire', 'e', 'proof', 'representation', 'learn', 'simplify', 'analysis', 'assume', 'function', 'bound', 'norm', '×', 'γ', 'reasonable', 'always', 'achieve', 'clip', 'loss', 'accuracy', 'target', 'φscid48', '×', 'lemma', 'recall', 'λka', 'denote', 'kth', 'large', 'eigenvalue', 'matrix', 'λ1a', 'λna', 'give', 'large', 'small', 'eigenvalue', 'respectively', 'perturbation', 'theorem', 'let', 'b', '∈', 'hermitian', 'matrix', 'λkb', 'cid107a', '−', 'proof', 'see', 'corollary', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'extend', 'uniform', 'spectrum', 'concentration', 'wp', '−', 'δ', 'sup', '−', '96κφ', 'log121δ', 'proof', 'first', 'observe', 'cid13σφ', '−', 'cid98σφ', '−', 'φ∈φcid107xcid1072≤1', 'want', 'bound', 'rademacher', 'complexity', 'class', '≤', 'first', 'bound', 'envelope', 'cover', 'consider', '∈', 'φ', 'pick', 'cid101φ', 'close', 'φ', 'i1', 'cid101φsi', 'ai2', 'i1', 'cid101φsi', 'ai', '\uf8ed', 'i1', '−', 'cid101φsi', 'ai', 'i1', 'cid101φsi', 'ai', '\uf8f6', 'dφφ', 'cid101φ', '−', 'sufﬁce', 'take', 'cid101φ', 'tcover', 'l2d', 'note', 't4covering', 'number', 'x', 'unit', 'ball', '8td', 'thus', 'bind', 'wainwright', 'φ', 'thus', 'theorem', 'wainwright', 'wp', 'sup', '96κφ', 'log121δ', 'prove', 'double', 'sample', 'help', 'deal', 'double', 'sampling', 'issue', 'transition', 'stochastic', 'recall', 'g', 'function', 'class', 'function', 'let', 'ν', 'distribution', 'rd', 'let', 'p', 'x', 'distribution', 'learn', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'double', 'sampling', 'suppose', 'ey∼p', 'x', 'g', 'ey∼p', 'ycid13', 'ex∼νy∼p', 'cid104', 'inf', 'cid105', 'proof', 'ex∼νy∼p', 'cid104', 'ey∼p', 'ycid13', 'cid105', 'cid16', 'ycid13', 'cid17cid105', 'ey∼p', 'ey∼p', 'ey∼p', 'cid107xcid1072', '−', 'cid104x', 'ycid105', 'ycid13', 'cid105cid105', '−', 'cid107ycid1072', '−', 'ey∼p', 'ycid13', '−', 'cid105', 'last', 'step', 'use', 'fact', 'observe', 'assume', 'ey∼p', 'x', 'g', 'minimizer', 'ycid13', 'ey∼p', 'ey∼p', 'ycid11cid3', 'complete', 'square', 'gcid63', 'complete', 'proof', 'concentration', 'lemma', 'deﬁne', 'optimal', 'g', 'loss', 'follow', '−', 'rs', '◦', 'p', 'cid2cid107', '−', 'γφscid48', 'cid3', 'p', 'cid2cid107gs', '−', 'γφscid48', 'cid3', 'similarly', 'deﬁne', 'cid98ρφ', 'cid98gφ', 'minimizer', 'loss', 'expectation', 'take', 'empirical', 'distribution', 'instead', 'population', 'distribution', 'ν', 'observe', 'unconstrained', 'minimization', 'yield', 'close', 'form', 'solution', 'φscid48', 'π', 'assumption', 'posit', 'g', 'rich', 'enough', 'capture', 'key', 'property', 'squared', 'loss', 'second', 'moment', 'upper', 'bound', 'expectation', 'allow', 'invoke', 'second', 'part', 'b1', 'combine', 'cover', 'get', 'uniform', 'convergence', 'result', 'wp', 'least', '−', 'δ', '∈', 'cid1072', 'cid2ρcid124φ', '−', 'rs', '−', 'eν', 'cid2cid107', '−', 'γφscid48', 'cid2ρcid124φ', '−', 'rs', '−', 'cid107gφ', '−', 'γφscid48', 'eν', 'cid3', '−', 'eν', '◦', 'p', 'cid2ρcid124φ', '−', 'rs', '−', 'γφscid48', 'πcid1072', '−', 'cid107gφ', '−', 'γφscid48', 'πcid1072', '◦', 'p', 'cid2cid107', '−', 'γφscid48', 'πcid1072', '−', 'cid107gφ', '−', 'γφscid48', 'πcid1072', 'cid3', 'cid12ed', 'cid12ed', '≤', 'assume', 'realizability', 'assumption', 'g', 'cid12ed', 'cid2cid107gs', '−', 'γφcid63scid48', 'πcid1072', '−', '−', 'γφcid63scid48', 'cid3', '−', 'eν', 'cid2cid107gs', '−', 'gφcid63', 'eν', 'cid2cid107gs', '−', 'gφcid63', 'cid3', 'deﬁne', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'finite', 'function', 'class', 'assume', 'g', 'ﬁnite', 'log4', 'nδ', 'log2', 'proof', 'ﬁxed', 'apply', 'ai', '−', 'rsi', 'envelope', 'cid3', 'xi', 'error', 'lemma', 'second', 'moment', 'bound', 'e', 'cid12ρcid124φ', '−', 'rs', '−', 'rs', '21w', 'n', '−', 'cid101ρcid124φs', '2rs', 'consider', 'n', 'net', 'require', 'point', 'error', 'εnet', 'approximation', 'union', 'bind', 'εnet', 'finally', 'union', 'bind', 'proof', 'ﬁxed', 'cid1072', 'apply', 'far', 'observe', 'cid107acid1072', 'e', 'cid2cid107a', '−', 'cid3', 'cid3', '16e', 'use', 'give', 'b', '−', 'cid107a', '−', 'second', 'moment', 'bound', 'e', 'cid3', '3γ2e', 'φsi', 'ai', 'γgφsi', 'π', 'envelope', 'π', 'gφsi', '−', '−', 'e', 'φsi', 'ai', 'γgφsi', 'cid3', 'φsi', 'ai', '−', 'γφscid48', '−', 'ai', '−', 'γφscid48', 'cid3', 'error', 'lemma', 'union', 'bind', 'εnet', 'cid107', 'observe', 'cid0cid107', 'φs', '−', 'γφscid48', 'πcid1072', '−', 'cid107gφ', '−', 'γφscid48', 'φsi', 'ai', 'φsi', 'ai', '−', '−', 'cid102', 'φsi', 'ai', '−', 'cid102', '−', 'cid102', 'cid1', 'π', 'cid132', 'cid107', 'cid102', 'φs', '−', 'γφscid48', 'πcid1072', '−', 'cid107gφ', '−', 'γφscid48', 'n', 'net', 'cid107', 'consider', 'd2dimensional', 'vector', 'cid107', 'cid1072', 'cid1072', 'error', 'εnet', 'approximation', 'point', 'n', 'net', 'subset', 'cid107', 'require', 'finally', 'union', 'bind', 'proof', 'ﬁxed', 'g', 'apply', 'ai', 'excess', 'regression', 'loss', 'realizability', 'assumption', 'g', '−', 'gφcid63', 'cid1072', '−', '−', 'γφcid63scid48', 'πcid1072', '−', 'γφcid63scid48', 'cid104', '−', 'gφcid63', '−', '−', 'γφcid63scid48', '2cid104gφcid63', '−', 'π', '−', 'gφcid63', 'deﬁnition', 'gφcid63', 'gφcid63', 'e', '−', 'gφcid63', 'e', 'gφcid63', '−', 'acid48', 'cid1072', 'union', 'bind', '2γ2', '−', 'gφcid63', 'second', 'moment', 'bound', 'cid3', '4γ2e', 'error', 'term', 'e', 'cid3', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'inﬁnite', 'function', 'class', 'g', 'inﬁnite', 'need', 'assume', 'metric', 'entropy', 'condition', 'ﬁnal', 'step', 'ﬁniteclass', 'proof', 'union', 'bind', 'wellchosen', 'εnet', 'collect', 'additional', 'approximation', 'error', 'order', 'assumption', 'e5', 'g', 'assume', 'exist', '∈', 'r', 'n', 'net', 'follow', '−', 'cid101φs', 'cid13φscid48', 'π', '−', 'cid101φscid48', 'π', 'cid13φscid48', '−', 'cid101φscid48', 'π', '−', 'cid13φscid48', '−', 'cid101φscid48', 'π', 'cid107gsi', 'cid104', 'eν', '−', 'cid101g', 'dgg', 'note', 'assumption', 'automatically', 'satisﬁed', 'class', 'wellner', 'theorem', 'assumption', 'p', 'logw', 'cid46', 'd21', 'p', 'γ2p', 'n', 'proof', 'ερ', 'show', 'ﬁxed', 'logw', 'observe', '−', 'rs', 'cid101φs', '2rs', '−', 'rs', '−', 'cid101φs', 'difference', 'loss', 'φ', 'loss', 'cid101φ', 'bound', 'union', 'bind', 'n', 'net', 'require', 'p', 'point', 'assumption', 'e5', 'approximation', 'error', 'use', 'net', 'proof', 'show', 'ﬁxed', 'observe', 'cid0cid107', 'φs', '−', 'γφscid48', 'cid107', '−', '−', 'γφscid48', 'π', '−', 'cid101φscid48', 'πcid107cid107', 'φs', 'cid101φs', '−', 'γφscid48', 'π', '−', 'cid107gφ', '−', 'g', '−', 'cid107gφ', '−', 'γφscid48', 'cid107', '−', 'γ', 'cid101φscid48', '−', 'γφscid48', 'π', 'πcid107', '−', 'cid107', 'g', '−', 'γ', 'cid101φscid48', '−', 'cid101φs', 'γcid107φscid48', '−', 'πcid107', '−', 'γφscid48', 'π', '−', 'πcid107cid107gφ', 'g', 'γcid107φscid48', '−', 'cid101φscid48', 'πcid107', 'cid107gφ', '−', 'g', '2γ', 'use', 'closed', 'form', 'solution', '16dφφ', 'cid101φ', 'union', 'bind', 'n', 'net', 'require', 'p', 'point', 'assumption', 'e5', 'approximation', 'error', 'use', 'net', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'proof', 'show', 'ﬁxed', 'g', 'n', 'observe', '−', 'γφcid63scid48', '−', '−', 'cid101g', 'acid107cid107gs', '−', '2γφcid63scid48', 'πcid107', 'union', 'bind', 'n', 'net', 'require', 'p', 'point', 'assumption', 'e5', 'approximation', 'error', 'use', 'net', 'main', 'result', 'suppose', 'φcid63', 'linear', 'suppose', 'assumption', 'transition', 'stochastic', 'moreover', 'suppose', 'φcid63', 'feasible', 'bilevel', 'optimization', 'cid98llbcφcid63', 'wp', 'log2gδ', 'proof', 'llbc', 'eν', 'cid98φ', 'cid98φs', '−', 'rs', 'a2cid105', 'cid98φ', 'minimizer', 'ν', 'cid98φs', '−', 'rs', 'a2cid105', 'cid98ρ', 'eν', '◦', 'p', 'cid104', 'cid107', '−', 'γ', 'cid98φscid48', 'πcid1072', 'cid104', 'cid98φ', '−', 'γ', 'cid98φscid48', 'πcid1072', 'eν', '◦', 'p', '−', 'eν', '◦', 'p', 'ρ', 'part', 'cid104', '−', 'rs', 'a2cid105', 'cid98ρ', '2ed', 'minimize', '2ed', 'cid104', '−', 'rs', 'a2cid105', 'cid98ρ', '2ed', 'optimality', 'cid98φ', 'cid104', 'cid98φ', '−', 'γ', 'cid98φscid48', 'πcid1072', 'cid104', 'cid98φ', '−', 'γ', 'cid98φscid48', 'πcid1072', '2ed', '−', '2ed', 'cid104', 'cid107', 'g', '−', 'γ', 'cid98φscid48', 'πcid1072', 'cid104', 'cid107', 'g', '−', 'γ', 'cid98φscid48', 'πcid1072', 'cid104', 'cid107', 'g', '−', 'γ', 'cid98φscid48', 'πcid1072', 'cid104', '−', 'γ', 'cid98φscid48', 'πcid1072', '2ερ', '2εm', '2ερ', '2εm', '2ed', 'φcid63', 'φcid63s', '−', 'rs', 'a2cid105', 'cid98ρ', '2ed', 'cid104', 'cid99mφcid63', 'φcid63s', '−', 'γφcid63scid48', '−', '2ed', '−', 'γφcid63scid48', 'cid3', '2ερ', '2εm', '−', 'g', 'part', '−', 'cid98gφcid63', 'cid1072', 'eν', 'cid3', 'use', 'optimality', 'cid99', 'cid3', '−', '2ed', 'cid2cid107mφcid63', 'φcid63s', '−', 'φcid63scid48', '2ed', '−', 'φcid63scid48', 'πcid1072', '2ed', 'cid104', 'φcid63', 'φcid63s', '−', 'rs', 'a2cid105', '−', 'φcid63scid48', 'πcid1072', 'cid3', '−', 'cid107cid98gφcid63', '−', 'φcid63scid48', 'πcid1072', 'cid3', '2ερ', '2εm', '2εg', 'ρ', 'part', 'φcid63', 'φcid63s', '−', 'rs', 'a2cid105', '3eν', '◦', 'φcid63s', '−', 'φcid63scid48', 'πcid1072', 'cid3', '−', '3eν', '−', 'φcid63scid48', 'πcid1072', 'cid3', '4ερ', '4εm', '2εg', 'learn', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'assumption', '4ερ', '4εm', '2εg', 'assumption', 'φcid63', 'linear', 'proposition', '4ερ', '4εm', '2εg', 'prove', 'theorem', 'general', 'stochastic', 'case', 'deterministic', 'transition', 'case', 'subsume', 'ignore', 'minimization', 'set', 'complexity', 'term', 'assume', 'assumption', 'assumption', 'wp', 'least', '−', 'δ', 'β4', 'system', 'stochastic', 'let', 'cid98φ', 'satisﬁes', 'cid98εapproximate', 'linear', 'λminσ', '≥', 'β4', 'transition', 'deterministic', 'treat', 'proof', 'first', 'assumption', 'wp', 'least', '−', 'δ', 'important', 'consequence', '−', 'λmincid98σφ', '≥', 'imply', 'β4', 'high', 'probability', 'event', 'φcid63', 'feasible', 'equation', '≥', 'particular', 'mean', 'cid98llbcφcid63', 'covariance', 'cid98φ', 'lowerbounde', 'eigenvalue', 'λminσ', 'β4', '≥', 'β4', 'apply', 'e6', 'bind', 'cid98φ', 'log2gδ', 'd1', 'cid98εapproximately', 'linear', 'log2gδ', '7γ1', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'finally', 'remark', 'cid98φ', 'approximately', 'linear', 'case', 'upper', 'bound', 'polynomial', 'assume', 'exact', 'linear', 'φcid63', 'consider', 'assumption', 'φcid63', 'exactly', 'linear', 'w', 'use', 'highlight', 'w', 'assumption', 'show', 'match', 'w', 'result', 'proposition', 'hence', 'sufﬁce', 'minimize', 'small', 'ball', 'cid1072', 'take', 'small', 'possible', 'w', 'd1', '−', 'cid99w', '−', 'cid107', 'cid63cid1072', '−', 'cid107ρcid63cid1072', 'cid632', '−', 'cid107ρcid63cid1072', '−', 'cid107ρcid63cid1072', 'cid632', 'cid632', 'polynomial', 'endtoend', 'result', 'deduce', 'chain', 'lspe', 'theorem', 'theorem', 'together', 'theorem', 'assumption', 'assumption', 'stochastic', 'let', 'cid98ε', 'deﬁne', 'wp', 'least', 'distribution', 'p0', 'cid12v', 'πe', 'cid104', 'cid98fks', 'πe', 'logn', 'γ2', 'ddπe', 'dν', 'cid13∞', '−', 'γ2', 'cid98ε', 'proof', 'theorem', 'ﬁrst', 'apply', 'theorem', 'see', 'cid98φ', 'satisﬁes', 'property', 'need', 'lspe', 'indeed', 'approximately', 'linear', 'cid98ε', 'speciﬁed', 'theorem', 'also', 'coverage', 'λminσ', '≥', 'β4', 'use', 'fact', 'separate', 'independent', 'dataset', 'need', 'separate', 'dataset', 'datadependent', 'run', 'lspe', 'directly', 'apply', 'theorem', 'result', 'implementation', 'detail', 'detail', 'environment', 'speciﬁcation', 'hyperparameter', 'use', 'main', 'text', 'f1', 'dataset', 'detail', 'use', 'publicly', 'release', 'implementation', 'drqv2', 'train', 'high', 'quality', 'target', 'policy', 'save', 'checkpoint', 'ofﬂine', 'behavior', 'dataset', 'refer', 'reader', 'yarat', 'exact', 'hyperparameter', 'environment', 'detail', 'follow', 'standard', 'use', 'drqv2', 'yarat', 'environment', 'maximum', 'horizon', 'length', 'timestep', 'achieve', 'behaviortarget', 'policy', 'action', 'repeat', 'frame', 'furthermore', 'state', 'stack', 'frame', '×', 'dimensional', 'rgb', 'image', 'thus', '×', '×', 'learn', 'bellman', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'task', 'target', 'performance', 'behavior', 'performance', 'finger', 'turn', 'hard', 'run', 'quadrupe', 'walk', 'humanoid', 'stand', 'table', 'performance', 'target', 'behavior', 'policy', 'use', 'collect', 'evaluation', 'ofﬂine', 'dataset', 'respectively', 'task', 'action', 'space', 'dimension', 'task', 'trait', 'reward', 'type', 'finger', 'turn', 'hard', 'run', 'quadrupe', 'walk', 'humanoid', 'stand', 'turn', 'locomotion', 'locomotion', 'stand', 'sparse', 'dense', 'dense', 'dense', 'table', 'task', 'description', 'action', 'space', 'dimension', 'reward', 'type', 'test', 'environment', 'f3', 'representation', 'architecture', 'hyperparameter', 'detail', 'adopt', 'network', 'architecture', 'ﬁrst', 'introduce', 'yarat', '2021b', 'speciﬁcally', 'process', 'pixel', 'input', 'layer', 'convnet', '×', 'kernel', 'channel', 'relu', 'activation', 'convolutional', 'layer', 'stride', 'rest', 'stride', 'output', 'feed', 'single', 'fully', 'connected', 'layer', 'normalize', 'layernorm', 'finally', 'tanh', 'nonlinearity', 'outputted', 'dimensional', 'staterepresentation', 'action', 'concatenate', 'output', 'feed', 'mlp', 'relu', 'activation', 'hyperparameter', 'value', 'feature', 'dimension', 'weight', 'initialization', 'optimizer', 'learning', 'rate', 'batch', 'size', 'training', 'epoch', 'τ', 'target', 'λdesign', 'orthogonal', 'init', '×', '×', 'table', 'hyperparameter', 'use', 'learn', 'complete', 'representation', 'ofﬂine', 'policy', 'evaluation', 'benchmark', 'metric', 'modiﬁcation', 'curl', 'originally', 'curl', 'contrastive', 'learning', 'image', 'state', 'datum', 'augmentation', 'ope', 'apply', 'curl', 'objective', 'stateaction', 'feature', 'detail', 'previous', 'section', 'note', 'also', 'train', 'curl', 'random', 'cropping', 'image', 'augmentation', 'present', 'author', 'finally', 'interleave', 'representation', 'learn', 'sac', 'q', 'prediction', 'head', 'modiﬁcation', 'spr', 'use', 'image', 'encoder', 'feature', 'main', 'difference', 'architecture', 'projection', 'layer', 'implement', 'mlp', 'relu', 'activation', 'note', 'additional', 'parameter', 'curl', 'require', 'finally', 'similarly', 'curl', 'additional', 'qprediction', 'head', 'spearman', 'rank', 'correlation', 'metric', 'rank', 'correlation', 'measure', 'correlation', 'ordinal', 'ranking', 'value', 'estimate', 'ground', 'truth', 'return', 'deﬁne', 'fu', 'policy', 'n', 'true', 'return', 'v1n', 'estimate', 'return', 'ˆv1n', 'rank', 'correlation', 'v1n', 'ˆv1n', 'cov', 'σ', 'v1n', 'σ', 'ˆv1n']"
"A Comparison of Source Distribution and Result Overlap in Web Search
  Engines","[{'href': 'http://arxiv.org/abs/2207.07330v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.07330v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-15 07:58:49,"2
2
0
2

l
u
J

8
1

]

R
C
.
s
c
[

1
v
8
7
9
8
0
.
7
0
2
2
:
v
i
X
r
a

A Security & Privacy Analysis
of US-based Contact Tracing Apps

Joydeep Mitra
Department of Computer Science
Stony Brook University
joydeep.mitra@stonybrook.edu

July 20, 2022

Abstract

With the onset of COVID-19, governments worldwide planned to develop and deploy contact
tracing apps to help speed up the contact tracing process. However, experts raised concerns
about the long-term privacy and security implications of using these apps. Consequently,
several proposals were made to design privacy-preserving contact tracing apps. To this end,
Google and Apple developed the Google/Apple Exposure Notiﬁcation (GAEN) framework to
help public health authorities develop privacy-preserving contact tracing apps. In the United
States, 26 states used the GAEN framework to develop their contact tracing apps.
In this
paper, we empirically evaluate the US-based apps to determine 1) the privileges these apps
have, 2) if the apps comply with their deﬁned privacy policies, and 3) if they contain known
vulnerabilities that can be exploited to compromise privacy. The results show that all apps
violate their privacy policies and contain several known vulnerabilities.

1 Introduction

Contact tracing is one of many methods health authorities use to contain the rapid spread of
COVID-19 [1]. However, manual contact tracing to keep track of the growing pandemic has been
a challenge for health authorities due to the lack of human resources and the limitations of human
memory to remember all possible contacts accurately [2]. Consequently, there have been several
eﬀorts to automate the contact tracing process by developing mobile apps that use tracking tech-
nology in mobile phones, such as GPS and Bluetooth [3, 4]. In addition, since mobile phones are
ubiquitous, they can be used to eﬀectively and quickly identify a person’s contacts when they test
positive for COVID.

1.1 Background

1.1.1 How do Contact Tracing Apps Work?

Contact tracing apps work by estimating if two mobile phones, X and Y, are close to each other
based on a metric deﬁned by local health authorities (e.g., 6 feet). Y will be notiﬁed of a potential
infection if X tests positive and conﬁrms the result. An app detects proximity using Bluetooth

1

 
 
 
 
 
 
Low Energy (BLE), Global Positioning System (GPS) technology, or a combination of the two. If
two phones are nearby, then the apps exchange encounter messages, which contain, among other
things, random identiﬁers and signal strength. Apps may also include location data in the encounter
messages. Each app keeps a record of all the encounter messages it has exchanged with other apps.
If one of these messages is from a phone whose user has tested positive, then the app analyzes the
encounter message to calculate the level of risk based on factors the local health authorities decided.
This calculation is performed either locally in the device or by a central server.

While contact tracing apps can be useful to help expedite the process of contact tracing, experts
have warned that such technologies have long-term consequences for user privacy and security since
they can be easily exploited and used in malicious contexts such as mass surveillance [5, 6, 7].
Therefore, contact tracing apps’ design and implementation must be carefully scrutinized before
being deployed and adopted.

1.1.2 Contact Tracing App Architectures

The need to ensure privacy in contact tracing apps has led to several proposals of app architectures
that will best protect user privacy – centralized, decentralized, and hybrid [8, 9]. In a centralized
architecture, a central server collects and stores personally identiﬁable information (PII), which is
further used to generate random identiﬁers for the encounter messages. Further, the central server
is responsible for determining the users that have been potentially exposed. Government health
authorities have access to the data stored in the central server and can use it to perform advanced
aggregate analysis, which can be useful for understanding trends to help inform mitigation eﬀorts.
However, this approach negatively aﬀects user privacy since the central server is assumed to be
trusted and has access to vast amounts of personal information, which unauthorized or hostile
entities can potentially misuse.

On the other hand, a decentralized server has minimal data. Most of the data is maintained
locally on the user’s device. Each app periodically communicates with the server to download
a set of random identiﬁers that have tested positive. The app then performs a risk analysis of
its encounter messages locally to determine if it had come close to a device that tested positive.
This architecture helps preserve privacy better than a centralized architecture. However, it limits
access to crucial data that could be used to analyze the spread of the pandemic or to understand
the eﬀectiveness of the app. The hybrid architecture combines features of the centralized and
decentralized architecture, where random identiﬁer generation is left to the local device, whereas
the server manages the risk analysis and exposure notiﬁcation.

Countries have developed and deployed apps based on a centralized architecture (e.g., Singa-
pore’s TraceTogether [10]) and decentralized architecture (e.g., the apps based on the Google/Apple
framework). Further, many apps also used location data to enable automated contact tracing (e.g.,
India’s Arogya Setu [11]). However, there is no consensus among researchers about which approach
is the most feasible for eﬀective contact tracing while minimizing privacy risks [12, 13].

1.1.3 The Google Apple Exposure Notiﬁcation System

In May 2020, Google and Apple collaborated to develop the Google Apple Exposure Notiﬁcation
(GAEN) framework based on the decentralized architecture to help public health authorities develop
contact tracing apps [14, 15].

A GAEN app works by generating a temporary key, which changes periodically. The key is used
to encrypt locally stored data and to generate a random identiﬁer. The app embeds the identiﬁer

2

in encounter messages and exchanges them with other apps using BLE. When a user tests positive,
a public health oﬃcial uses a veriﬁcation server to send the user a conﬁrmation code. The user
can use the conﬁrmation code to upload all their recent keys (e.g., last 14 days) to a key server.
Each app downloads keys periodically from the key server and compares them with a set of keys
exchanged in the last few days. If there is a match, the app determines the risk of the potential
exposure based on a formula pre-determined by public health authorities.

1.2 Motivation

In the absence of an oﬃcial national contact tracing app in the United States, individual US states
used the GAEN framework to develop their own contact tracing apps. To date, 26 US states
have developed a GAEN app. Despite the security and privacy guarantees built into the GAEN
framework, people in the US lack conﬁdence in the apps’ ability and intentions to protect their
privacy [16]. The Center for Disease Control and Prevention (CDC) in the US recommends that
healthcare authorities should conduct a third-party assessment of contact tracing apps and make
the results publicly available [9]. However, according to the technology assessment conducted by
the United States Government Accountability Oﬃce (GAO) at the behest of the US Congress,
most states with a contact tracing apps have not conducted a third-party assessment. Moreover,
the states that have conducted an evaluation, have not made the results publicly available [9].
Motivated by the CDC’s recommendations and the lack of assessments of the US-based apps, in
this paper we are analyzing the privacy and security of GAEN-based Android apps for contact
tracing in the US. Speciﬁcally, we are asking the following research questions:

• RQ1: What degree of privilege do the GAEN-based Android apps for contact tracing have?
Android apps by default have least privilege. They need to request the system or users
for permission to perform privileged operations (e.g., use Bluetooth). The purpose of this
question is to understand the permissions used by these apps and their privacy implications.

• RQ2: Do GAEN-based Android apps for contact tracing violate their own privacy policies?
Contact tracing apps are required to publish a privacy policy to inform users of the app’s
capabilities and its data sharing, storage and retention policies. The purpose of this question
is to determine if the privacy policy is consistent with the behavior encoded in the app’s
source code.

RQ3: Do GAEN-based Android apps for contact tracing contain known Android app vulner-
abilities? Android apps can have vulnerabilities that can be exploited by malicious apps
available locally or remotely to compromise users’ privacy. The purpose of this question is to
identify if GAEN-based contact tracing apps have similar vulnerabilities.

2 Methodology

In this section, we describe the apps selected, the tools chosen, and the factors we considered to
answer our research questions.

2.1 App Selection

We selected the oﬃcial contact tracing apps in Android of all US states developed using the exposure
notiﬁcation APIs by Google and Apple (GAEN). We considered these apps since the focus of our

3

App
State
Alabama
Arizona
California
Colorado
Connecticut
Delaware
DC
Guam
Hawaii
Louisiana
Maryland
Michigan
Minnesota
Nevada
New Jersey
New Mexico
New York
North Carolina
North Dakota
& Wyoming
Pennsylvania
Utah
Virginia
Washington
Wisconsin

App
Name
GuideSafe
Covid Watch Arizona
CA Notify
CO Exposure Notiﬁcations
COVID Alert CT
COVID Alert DE
DC CAN
Guam Covid Alert
AlohaSafe Alert
COVID Defense
MD COVID Alert
MI COVID Alert
COVIDaware MN
COVID Trace Nevada
COVID Alert NJ
NM Notify
COVID Alert NY
SlowCOVIDNC
Care19 Alert

Version
Name
1.10.0
2.1.11

minted1000003
minted141006
1.0.1

Package
Name
gov.adph.exposurenotiﬁcations
gov.azdhs.covidwatch.android
gov.ca.covid19.exposurenotiﬁcations minted14020
gov.co.cdphe.exposurenotiﬁcations
gov.ct.covid19.exposurenotiﬁcations
gov.de.covidtracker
gov.dc.covid19.exposurenotiﬁcations minted1100019
org.pathcheck.guam.bt
org.alohasafe.alert
org.pathcheck.la.bt
gov.md.covid19.exposurenotiﬁcations minted151008
gov.michigan.MiCovidExposure
org.pathcheck.covidsafepathsBt.mn
gov.nv.dhhs.en
com.nj.gov.covidalert
gov.nm.covid19.exposurenotiﬁcations minted1200004
gov.ny.health.proximity
gov.nc.dhhs.exposurenotiﬁcation
com.proudcrowd.exposure

1.4
1.17.12
minted1200005
1.0.1

1.0.10
1.0.15
1.9.1

1.1.5
1.6
1.2

Version
Code
2764
201011
minted14020
10000032
141006
15
11000192
1947
41
2661
151008
255
3503
12000052
20
12000042
81
205
10

COVID Alert PA
UT Exposure Notiﬁcations
COVIDWISE
WA Notify
WI Exposure Notiﬁcation

gov.pa.covidtracker
gov.ut.covid19.exposurenotiﬁcations minted1100011
1.5
giv.vdh.exposurenotiﬁcation
gov.wa.doh.exposurenotiﬁcations
minted142004
gov.wi.covid19.exposurenotiﬁcations minted141003

2.0.0

46
11000112
160
142004
141003

Table 1: US-based contact tracing GAEN apps.

App size Download
(in MB) Date
6.70
3.56
10.07
3.38
9.94
105.55
11.8
64.52
64.64
7.37
10.19
3.19
3.19
12.12
105.62
3.9
105.90
3.1
7.23

Nov 12, 2021
Nov 19, 2021
Oct 15, 2021
Nov 19, 2021
Nov 12, 2021
Dec 3, 2021
Dec 3, 2021
Jan 19, 2022
Nov 5, 2021
Feb 4, 2022
Mar 11, 2022
Oct 15, 2021
Mar 11, 2022
Mar 25, 2022
Nov 5, 2021
Mar 4, 2022
Oct 8, 2021
Nov 5, 2021
Nov 12, 2021

105.68
11.82
9.32
10.36
9.86

Oct 14, 2021
Dec 3, 2021
Mar 24, 2022
Nov 5, 2021
Mar 19, 2022

study is to examine GAEN apps based in the US.

We found the apps from the Android Developer’s oﬃcial page [17]. Each US-based app has a
link to Google Play. We used the links to download the corresponding APK ﬁle from Google Play
in an Emulator running an Android version supported by the app. We transferred the apk ﬁles
from the emulator to a computer where we could reverse engineer and statically analyze the apps.
All US states did not develop a GAEN app. Further, North Dakota and Wyoming use a single
app. The Massachusetts app is built into the device and can only be installed from the device’s
settings, not from Google Play. We could not obtain this app since we were using an emulator to
install the apps. Consequently, we did not consider the Massachusetts app in our assessment. In
total, we ended up with 24 apps. Table 1 lists all the selected apps along with their name, package,
version information, size of the APK ﬁles, and when we downloaded them.

2.2 Tool Selection

Research in mobile app security and privacy has led to the development of several tools and tech-
niques to detect vulnerabilities and malicious behavior [18, 19]. The tools are based on static and
dynamic analysis. Most use static analysis to either ﬂag vulnerabilities or malicious behavior or
guide subsequent dynamic analysis. Few tools use only dynamic analysis. Prior research eﬀorts
studying the eﬃcacy of such tools have observed that for freely available tools, static analysis tools
detect more known Android app vulnerabilities than dynamic analysis tools [20]. Furthermore,
among the static analysis tools, MobSF [21] detects the most known vulnerabilities. Based on this
observation, we used MobSF as the primary tool for analysis.

MobSF has a static and dynamic analyzer. The static analyzer statically analyzes the app’s
source code to determine if the app uses APIs and features that are known to cause Android app

4

Figure 1: Snippet from a report generated by MobSF. Each issue has a severity, security standards
it is associated with, and the source ﬁle/s in which it was detected

vulnerabilities (see Figure 1). We failed to run the dynamic analyzer on the contact tracing apps
that we had selected. Hence, we considered only the static analyzer.

We used Androguard [22] to generate the control ﬂow graph of an app (see Figure 2), which was
used to analyze the data ﬂow through the app. We tracked the data ﬂow in the control ﬂow graph to
determine if data ﬂowing out of the app is sensitive or if the data being used by the app is potentially
malicious. This was necessary to verify the potential data leak and data injection vulnerabilities
reported by MobSF’s static analyzer, as it is known to report false positives. Speciﬁcally, we used
the following strategies:

• We conﬁrmed a potential data leak vulnerability if there was a path in the control ﬂow
graph from a pre-identiﬁed sensitive data source node to a target node with shared storage,
network, or inter-app communication APIs. Sensitive data sources include APIs used to
collect user input (e.g., biometric), read from app’s private ﬁles and communication channels
(e.g., Bluetooth), and strings hard coded with personal information (e.g., IP address).

• We conﬁrmed a data injection vulnerability if there was a path in the control ﬂow graph from
a pre-identiﬁed source node of potentially malicious data such as shared storage, network,
or inter-app communication APIs to a target node and the target node was using the data
without sanitizing it. Examples of a target node using potentially malicious data after sani-
tization include an exported broadcast receiver that uses input data only if it was sent via an

5

intent-ﬁlter with a system-deﬁned action or a function in a target node that uses input data
from a trusted remote server.

Figure 2: Snippet of the list of edges in a control ﬂow graph generated by the AndroGuard tool for
the California app. The source and target columns indicate the nodes in the graph. Each row is a
directed edge connecting the source node to the target node. Each node has an ID and an API call
contained in the source code.

2.3 Policy Analysis

A focus of our study is to determine if an app’s source code is consistent with the app’s privacy
policy. To this end, we downloaded each app’s privacy policy and examined them. We identiﬁed
the features that the privacy policy of an app claims the app does not use (e.g., does not collect
location). All selected apps in their privacy policies claim the following:

1. does not collect, store, or transmit any personally identiﬁable data.

2. stores exposure data (e.g., random IDs and exposure date) locally in the users’ device.

3. prevents unauthorized access to locally stored data.

4. encrypts locally stored data.

5. communicates with trusted servers through encrypted networks in the United States.

We looked for these features in the apps using MobSF and Androguard. If at least one such
feature was found in the app, then we deemed that the app violates its own privacy policy. For
example, using MobSF, we determined instances in an app’s source code where data was being
stored in external storage. We then used the app’s control ﬂow graph (generated by Androguard)
to determine the source of the data stored in external storage and whether the source is sensitive.
If sensitive data was being stored in external storage, then we deemed it as a violation of the app’s
privacy policy due to bullet three listed above.

6

2.4 Known Vulnerabilities Analysis

Android apps have vulnerabilities, which malicious apps exploit to cause harm to the user [23, 24,
25, 26]. Therefore, it is necessary to ensure such vulnerabilities do not occur in apps, especially
contact tracing apps, which deal with sensitive personal information and can perform privileged
operations on the phone. Hence, we analyzed these apps for known Android app vulnerabilities.

We used MobSF, and the Ghera repository [25] for our analysis. MobSF provides a list of
potential vulnerabilities in its static analysis report. We investigated each of them to determine their
veracity as MobSF is known to report false positives. Moreover, MobSF does not detect all known
vulnerabilities. Therefore, we used Ghera, a repository of 60 known vulnerability benchmarks, to
further guide our analysis. Each benchmark in Ghera is well-documented and contains only the
features related to the vulnerability captured in the benchmark. We statically analyzed the 24 apps
in our set using MobSF to determine the features/APIs used in them. We then considered only
those features also used in the Ghera benchmarks. We investigated each such feature to determine if
it resulted in a potentially exploitable vulnerability, that is, it could be exploited by a malicious app
on the device or remotely. For example, an app can give unrestricted access to another app using
the pending intent feature in Android. If MobSF found an app with a component using pending
intent, we investigated the component to conﬁrm if it performed privileged operations, which, if
true, could result in a potential privilege escalation attack.

3 Results

In this section, we report the ﬁndings of our study in terms of the permissions requested and used
by the apps, their potential privacy violations, and the potential vulnerabilities in them that can
be exploited to cause harm to the user.

3.1 RQ1: What degree of privilege do the GAEN-based Android apps

for contact tracing have?

A total of eight permissions are used across all 24 contact tracing apps. An app uses approximately
seven permissions on average. The least number of permissions used by an app is six, and the
highest is eight. These statistics are higher than the average permissions used by apps in general,
which is ﬁve [27], with nearly 100k of one million apps using zero permissions as of June 2014.
Furthermore, ﬁve of the eight permissions are used by all 24 apps, which suggests that only ﬁve
of them are necessary for contact tracing. The other three permissions are most likely extraneous.
Therefore, the GAEN apps in Android are over-privileged, which is concerning since these apps have
access to vast amounts of personal data. Over-privileged apps increase the risk of being exploited
since they expose a larger attack surface due to having more privileges than needed [28]. Therefore,
developers of these apps should carefully consider the required permissions and ensure that only
the necessary ones are used by their apps.

All permissions, except two, are normal permissions [29], that is, granted by Android when the
app is installed. Therefore, the apps do not need to ask the user for permission at runtime; they
always have them. In the context of contact tracing, where many users install these apps to help
prevent the spread of the pandemic, this permission model compromises privacy by not giving users
control of granting or denying permission to the apps. Furthermore, it assumes that the apps are
benign and that users should trust them during installation. This assumption hinders an app’s

7

Permissions

Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y

Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y

Y
N
N
N
N
Y
N
Y
Y
Y
N
N
Y
N
Y
N
Y
N
N

Y
N
N
N
N
Y
N
Y
Y
Y
N
N
Y
N
Y
N
N
N
N

Y
Y
Y
Y
Y
N
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y

Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y

Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y

P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 # Permissions per app
App
8
Y
Alabama
8
Y
Arizona
6
Y
California
6
Y
Colorado
6
Y
Connecticut
7
Y
Delaware
6
Y
DC
8
Y
Guam
8
Y
Hawaii
8
Y
Louisiana
6
Y
Maryland
Michigan
6
Y
8
Y
Minnesota
8
Y
Nevada
8
Y
New Jersey
8
Y
New Mexico
New York
7
Y
6
North Carolina Y
North Dakota
8
Y
& Wyoming
Penn
Utah
Virginia
Washington
Wisconsin
# Apps per
permission

N
N
N
N
N
N
N
N
N
N
N
N
N
Y
N
Y
N
N
Y

N
Y
N
N
N
N
N
N
N
N
N
N
N
N
N
N
N
N
N

N
Y
N
N
N
N
N
N
N
N
N
N
N
N
N
N
N
N
N

N
N
N
N
N
N
N
N
N
N
N
N
N
Y
N
Y
N
N
Y

Y
Y
Y
Y
Y
24

Y
Y
Y
Y
Y
24

Y
Y
Y
Y
Y
24

Y
Y
Y
Y
Y
24

Y
Y
Y
Y
Y
24

Y
Y
Y
Y
Y
23

Y
N
N
N
N
9

N
N
N
N
N
2

N
N
N
N
N
2

N
N
N
N
N
7

N
N
N
N
N
2

N
N
N
N
N
2

7
6
6
6
6

Table 2: Permissions used by the US-based GAEN apps as declared in their manifest ﬁle. The
permissions names are encoded as PN due to lack of space. The exact permission names are listed
in Table 3

Permission Code Permission Name
P1
P2
P3
P4
P5
P6
P7
P8
P9
P10
P11
P12

android.permission.INTERNET
android.permission.VIBRATE
android.permission.RECEIVE BOOT COMPLETED
android.permission.BLUETOOTH
android.permission.ACCESS NETWORK STATE
android.permission.ACCESS WIFI STATE
android.permission.WAKE LOCK
android.permission.FOREGROUND SERVICE
com.google.android.c2dm.permission.RECEIVE
com.google.android.ﬁnsky.permission.BIND GET INSTALL REFERRER SERVICE
android.permission.USE BIOMETRIC
android.permission.USE FINGERPRINT

Table 3: Name of permissions used in all US-based GAEN apps.

8

adoption in an environment where the role of government-deployed contact tracing apps is being
seen with suspicion [30]. However, app developers cannot resolve this issue as Android, not the
apps, deﬁne these permissions. Therefore, platform developers should either consider changing the
way these permissions are granted or deﬁne custom permissions that will be used in the context of
contact tracing.

Google-based third-party analytics libraries, not the core Android system, deﬁne two permis-
sions. Only two of the 24 considered apps use these permissions. Therefore, this raises the question
if these permissions are necessary for contact tracing. Further, such apps are designed to have
the necessary permissions to access privileged operations in the device (e.g., Bluetooth). In this
context, should contact tracing apps further increase security and privacy risks by using third-party
libraries that are not directly related to the task of contact tracing?

The USE FINGERPRINT permission is deprecated [31]. Two of 24 apps use it. While using
deprecated permissions is not recommended, this is not a major concern as both the apps use
the permission in conjunction with the USE BIOMETRIC permission, which is the recommended
permission to use instead of USE FINGERPRINT. Nevertheless, apps should not use deprecated
permissions since they may have unknown and unexpected security and privacy implications.

Two of the 24 apps use biometric permissions to access the device capabilities to use and collect
biometric information. While this is not a violation of privacy by itself, it increases the risk of
exposing private sensitive user info to unauthorized entities. Furthermore, since a majority of the
GAEN apps are not using biometric permissions, it raises questions about the necessity of using
such capabilities to collect and transmit biometric-related data.

The ACCESS WIFI STATE permission is used by 17 of the 24 selected apps. This suggests
that not all apps need this permission for contact tracing. Apps use this permission when they
connect to a remote server through WiFi. Using WiFi is not always secure since WiFi networks
may not be protected and may be susceptible to Man-In-The-Middle attacks. Therefore, apps that
collect and transmit sensitive information over the internet should avoid WiFi communication.

The VIBRATE permission is used by nine of 24 apps to control the device’s vibration. This
permission is not necessarily benign.
If used incorrectly or maliciously, it may damage a user’s
phone [27]. Therefore, it is best to avoid using such permissions if not absolutely necessary. Since
a majority of the selected apps do not use the VIBRATE permission, this permission is not likely
necessary for GAEN apps.

Only one of the 24 apps declares a query element in its manifest ﬁle to indicate the list of
apps it can communicate with. Speciﬁcally, the Arizona app declares that the query elements
android.intent.action.DIAL and android.intent.action.SEND in its manifest ﬁles. This implies that
the Arizona app has capabilities to place phone calls and share data with any app with the SEND
intent. In the context of contact tracing, these capabilities are unnecessary and pose additional risk
to the security and privacy of the app’s users.

3.2 RQ2: Do GAEN-based Android apps for contact tracing violate their

own privacy policies?

All US-based GAEN apps violate their privacy policy since their behavior is inconsistent with at
least one of the claims in their policy. This is concerning since contact tracing apps collect and
store vast amounts of private data, which can be potentially misused. Therefore, they should take
additional care to guarantee their users’ privacy or at least be consistent with their own policies.

The GAEN apps are designed not to collect, store, or track location. However, 20 of the 24

9

GAEN apps collect users’ locations despite claiming otherwise in their privacy policy. On further
analysis, we discovered that these apps are not explicitly collecting location. However, they use
a library called TwilightManager that collects user location to determine the local time. Apps
that have conﬁgured dark themes automatically import and use this library. Therefore, these apps
violate their privacy policy due to using a library that collects location. Consequently, this raises
the question if app developers are aware of the privacy implications of the libraries they used in
their apps. Vetting the libraries before using them is especially crucial for contact tracing apps,
which malicious actors can potentially misuse to compromise user privacy.

The Nevada and New Mexico apps collect Biometrics for authentication or to encrypt locally
stored data. However, the privacy policy of these apps does not explicitly state that they col-
lect biometric information. Moreover, they mention that the apps do not collect any personally
identiﬁable information. Hence, we deem these apps as violating their own privacy policy.

All apps claim in their privacy policy that they store exposure-related data in local storage in
a way that prevents unauthorized access. However, nine of the 24 apps store their data in external
storage. Any app installed on the device (including malicious apps) can access this data if they
have the necessary permission to access external storage. Therefore, all other apps can potentially
access the exposure-related data stored by these nine apps. Consequently, this leads to a violation
of privacy as deﬁned in the apps’ privacy policy. This issue could have been addressed by using the
app’s internal storage instead of the external storage because, in Android, the internal storage of
an app can only be accessed by the app. Moreover, Android recommends that an app’s data should
be stored in internal storage unless it needs to be shared with other apps. Considering more than
a third of the selected apps used external storage instead of internal storage suggests that several
developers of these apps are not aware of the diﬀerence between the two. This lack of knowledge is
concerning since the apps collect and store vast amounts of personal information. If access to the
data is not minimized, then they can be potential targets for cyberattacks by malicious actors.

The privacy policy of all the selected apps mentions that data stored locally in the device is
protected by encryption. However, six of the 24 apps use the AES block cipher in ECB mode,
which is a weak cipher. Weak encryption is a violation of the apps’ privacy policy of encrypting
local data since it leads to a potential leak of sensitive data. This result implies that a signiﬁcant
number of developers are not aware that AES in ECB mode should be avoided despite several security
guidelines, such as OWASP, recommending not to use it. One likely reason that developers end up
using the ECB mode is that this is the default mode for AES encryption in Android. As a result,
developers must explicitly change the mode. However, this oversight is surprising since the focus
of the GAEN apps was to ensure user privacy by storing personally identiﬁable information locally
and protecting them via strong encryption. Failure to use strong encryption despite claiming to
do so in their privacy policies raises questions about their diligence in protecting users’ data from
misuse.

Fourteen of the 24 apps use HTTP to communicate with remote servers. Consequently, their
communication can be potentially hijacked by Man-In-The-Middle attacks. Furthermore, this is
inconsistent with the apps’ stated privacy policy that mentions that communication with remote
servers is encrypted end-to-end. Considering that HTTP is used in more than half of the selected
apps implies that the developers of these apps are either unaware of the implications of using HTTP
or did not do due diligence to verify that all communication with remote web servers uses HTTPS.
The use of HTTP is concerning, especially since using HTTPS is an essential requirement of apps
that transmit sensitive personal information to remote web servers. Moreover, the developers of
apps promoted for large-scale use and with access to vast amounts of personal information must be

10

more perceptive of such issues and take extra care to avoid them.

Exposure notiﬁcation apps transmit exposure-related data (e.g., random IDs and exposure date)
to remote servers when a user consents to share their data in the event of an exposure to COVID-19.
The apps’ privacy policy does not mention the location of the servers. Therefore, users reasonably
assume that the servers are in the United States. However, 13 of the 24 apps communicate with
exposure notiﬁcation servers outside the United States. Furthermore, the privacy policy of two of
the 13 apps explicitly states that the apps only communicate with servers in the United States.
Therefore, this raises the question if the apps are sending exposure-related data of US-based residents
outside the US. The apps should be more transparent and explicitly state in their policies the location
of the exposure notiﬁcation servers so users can make informed decisions.

3.3 RQ3: Do GAEN-based Android apps for contact tracing contain

known Android app vulnerabilities?

We discovered a total of six known vulnerabilities across all 24 apps, and each app had approximately
two vulnerabilities on average. All 24 apps had at least one of the six vulnerabilities. Table 5 shows
the breakdown of the vulnerabilities found in each app. The vulnerabilities are brieﬂy described as
follows:

• Unprotected Component. Android apps consist of components. Apps can export their com-
ponents to share operations or data with other apps. However, if components are exported
without restrictions, all apps, including malicious apps, can access them. Consequently, it
can lead to denial-of-service, data leak, and data injection attacks.

• Insecure PRNG. Apps that use the Random package in Java to generate pseudo-random
numbers can be more easily predicted than apps that use the SecureRandom package. Since
contact tracing apps based on GAEN rely on random identiﬁers, they should use a random
number generator that makes it harder to predict the random numbers. Therefore, apps
should use the SecureRandom package instead of the Random package to generate hard-to-
predict random identiﬁers.

• Weak hashing. Hashing algorithms such as MD5 and SHA-1 are considered weak. Attackers
can use a hash collision to forge a duplicate hash. Therefore, apps should avoid using them
to prevent forgery attacks.

• Data Backup. Android allows users to create backups of all data in an app without having
root privileges. Consequently, malicious users with access to the device will be able to create
a backup of all the app’s data using a USB. Apps can be conﬁgured to protect against
this potential attack by setting the allowBackup attribute in the app’s manifest ﬁle to false.
Android recommends apps to disable this feature to prevent malicious users from accessing
an app’s local data.

• Insecure TLS/SSL Implementation. Apps using TLS/SSL protocol to communicate with
remote servers must verify the trustworthiness of the servers. The established way to verify
trust is for the app to maintain a list of trusted certiﬁcate authorities (CAs). The server
is conﬁgured with a certiﬁcate containing a public key and a matching private key. The
certiﬁcate must be signed by a certiﬁcate authority (CA) trusted by the app. Generally, the
list of trusted CAs is pre-installed in the device on which the app is installed. However, the

11

connection may fail if the certiﬁcate used to conﬁgure the server (1) is signed by a CA, not
in the list of trusted CAs, (2) or is self-signed, (3) or is signed by an intermediate certiﬁcate
missing from the server conﬁguration. While the third reason is addressed at the server-side,
the ﬁrst two reasons are addressed by implementing a custom TrustManager, an Android API,
in the app. Implementation mistakes in the custom TrustManager lead to vulnerabilities (e.g.,
trust all CAs) that can lead to Man-In-The-Middle attacks.

• Unpinned Certiﬁcates. Apps installed in a device trust all CAs that are pre-conﬁgured with
the device. App developers can further restrict the CAs that the app will trust by pinning a
set of trusted CAs to the app. The app then trusts only the pinned CAs and not any other
CA, including the ones trusted by the device. Although not mandatory, certiﬁcate pinning is
good security practice. However, they should be used with care as they can hamper usability
due to communication failure because of outdated certiﬁcates as a result of changes to the
server conﬁguration.

Twelve of the 24 apps had at least one unprotected component vulnerability. We found sev-
eral manifestations of this vulnerability. For example, apps used a third-party library that had
an unprotected broadcast receiver that could write to shared preferences. Malicious apps could
potentially exploit this vulnerability to execute data injection attacks.
In other instances, apps
deﬁned an unprotected activity that could access Bluetooth and location. As a result, malicious
apps could potentially exploit this activity to get access to privileged features without having the
necessary permissions. Furthermore, few apps deﬁned an activity that could share exposure-related
diagnostic information with a remote server. Since this activity was exported without restriction,
malicious apps could potentially exploit this vulnerability to communicate with the remote server.
Apps must protect these components by making them private to the app. If these apps need to share
these components with the underlying system, then they must protect the components with system
permissions.

All apps, except two, chose to use an insecure PRNG to generate the random identiﬁers used to
identify devices where the app is installed anonymously. Choosing an insecure PRNG over a more
secure PRNG allows malicious actors to potentially predict the random identiﬁer more easily, which
could be used to create duplicate identiﬁers and hence create erroneous or fake exposure-related data
entries. Therefore, all the selected apps must use the SecureRandom package to generate random
identiﬁers.

Seven of the 24 apps use MD5 or SHA-1, which are weak hashing algorithms. Choosing a weak
hashing algorithm for contact tracing apps is problematic since these apps are expected to provide
strong privacy and security guarantees. A strong hashing algorithm is the most basic requirement
with which the app can ensure the integrity of the information it stores.

A sixth of the 24 apps, that is, four apps allow users to back up the app’s data without rooting
the device. This is insecure for GAEN apps since they store a user’s and their contacts’ exposure-
related data locally in the app. A malicious user with access to the device but without root access
can get access to this data and misuse it. Apps that have this feature should disable it by setting
the allowbackup attribute in their manifest ﬁle to false.

Five of the 24 apps chose to use certiﬁcate pinning instead of relying on the device’s list of trusted
CAs. While certiﬁcate pinning provides additional protection against MITM attacks, most apps
choose not to use it, possibly because of potential connection failures due to the pinned certiﬁcates
becoming outdated. In such situations, the only way to restore the app is by pushing a software
update, which the users must install. Temporary connection failures are not ideal and could render

12

an app useless. However, in the context of the GAEN framework, where communication with remote
servers is minimal1 and security is paramount, it is advisable for the apps to use certiﬁcate pinning
to reduce the risk of an MITM attack. Further, apps could pin backup certiﬁcates to prevent relying
on only one pinned certiﬁcate. If one of them is outdated, the app can use the backups to connect
to the server.

MobSF reported an insecure SSL implementation vulnerability in exactly one app (the Arizona
app) because the app was using a pinned self-signed certiﬁcate. Apps pinning self-signed certiﬁcates
have beneﬁts and limitations in terms of preventing MITM attacks. Consider a scenario where an
app has pinned a certiﬁcate signed by a CA that has been compromised. In this situation, the app
will need to be updated via a software update with a newly issued certiﬁcate. On the other hand,
if the app had pinned a self-signed certiﬁcate, then the app only trusts that certiﬁcate and will not
be aﬀected if any other CA’s certiﬁcate is compromised. However, using self-signed certiﬁcates are
secure only if they are continuously monitored. In its absence, app developers may not know about
a compromised certiﬁcate, and the app’s communication with the compromised server will continue
unknowingly. However, actively maintaining self-signed certiﬁcates is more cumbersome than using
certiﬁcates from a trusted CA. Trusted CAs can revoke compromised certiﬁcates used by a server
to stop communication between the app with the pinned certiﬁcate and the server, protecting the
user from further harm. Therefore, in the general case, it is more secure to pin certiﬁcates signed
by trusted CAs instead of self-signing them.

4 Discussion

4.1 Observations on the apps

The results show that contact tracing apps in Android based on the GAEN framework are over-
privileged. Further, in Android, the apps are granted permission to use privileged system features at
install time. As a result, users have less control over granting permission to these apps at runtime.
This is concerning since these apps could potentially be used for tasks other than contact tracing,
such as mass surveillance. Therefore, the apps must collect minimal information and use the least
privileges.

The GAEN framework was developed to help create apps that preserve user privacy. How-
ever, our analysis shows that all apps violate their own privacy policy due to several potential
reasons, such as developer oversight and developers’ lacking domain knowledge and awareness of
the underlying platform.

Oversight is concerning but understandable since contact tracing apps were developed hurriedly
to tackle the challenges of a growing pandemic. However, oversight in this context leads to a lack
of transparency and credibility. It exacerbates the skepticism that the general public has towards
contact tracing apps and hampers their widespread adoption. Weak adoption is not desirable since
contact tracing apps if used eﬀectively, are a vital tool to contain the pandemic.
If developer
oversight is the reason for privacy violations, then there is a need to develop tools and techniques
that help developers write privacy policies that are consistent with their app’s behavior and vice
versa.

Few apps in our study missed mentioning in their privacy policy all the personally identiﬁable
information that they collected in their apps. One possible reason for this is a lack of domain

1Users only connect with the remote server to upload a positive test result. Also, apps periodically connect with

the remote server to check for positive cases.

13

knowledge in developers. In this context, privacy research eﬀorts should focus on developing meth-
ods to help identify domain-speciﬁc personally identiﬁable information. Contact tracing apps have
features that could be misused to violate users’ privacy. Therefore, it is crucial to accurately iden-
tify the information that the apps collect so users can make informed decisions about the privacy
implications of using the apps.

The GAEN framework made it easier for healthcare providers to create apps for eﬀective contact
tracing without the need to know the details of the underlying platform [9]. This is also evident
from certain privacy violations, which could have been avoided if developers had known about the
underlying platform behavior. Therefore, existing research in app security and privacy should focus
on developing methods and tools to assist less experienced developers gain the necessary knowledge
to avoid privacy violations.

The selected apps had vulnerabilities that are well known in the app development community.
In fact, all four vulnerabilities that were discovered are part of the OWASP top 10 [32], a popular
set of guidelines for developing secure mobile and web apps. The apps had these vulnerabilities
despite the focus on ensuring that the apps are secure and preserve user privacy. This suggests
that the app developers did not have the experience to avoid these mistakes, or they did not have
access to tools to help them prevent these vulnerabilities eﬀectively. The latter is less likely since
app development IDEs such as Android Studio have support for detecting and preventing such
vulnerabilities during development. Therefore, it is more likely that the states did not allocate the
resources to recruit developers with suﬃcient experience in mobile app development. While it is
understandable that states need to prioritize their resources to tackle a pandemic, they should have
planned better before developing and deploying apps with long-term consequences for user privacy
and security.

4.2 Observations on MobSF

While investigating the potential vulnerabilities MobSF reported, we discovered that a few of them
were false positives, that is, falsely reported as vulnerabilities. We report and discuss them to help
tool developers like MobSF improve their tools.

Brieﬂy, MobSF reported a total of ﬁve false positives across the 24 apps. Further, MobSF
reported ﬁve false positives for all apps except the Virginia app and the Arizona app, which had
three and four false positives, respectively. We explain the false positives, their likely reasons, and
suggestions on how to avoid them as follows:

• The SQLInjection vulnerability was falsely reported in 23 of the 24 apps. MobSF reported
the vulnerability in apps using the execSQL API to execute SQL queries. However, using
execSQL leads to SQLInjection only if the query string has user-supplied input with potentially
malicious SQL. None of the queries reported as being potentially vulnerable to SQLInjection
relied on user input. Hence, they were not vulnerable to SQLInjection.

Suggestion: MobSF should ﬂag queries in execSQL only if they rely on user-input and are not
parameterized.

• MobSF falsely reported an unprotected component vulnerability in all 24 apps because it
found components in these apps that were exported. However, these components were also
protected by system permissions, that is, only the system could be granted access to these
components. Furthermore, these permissions were deﬁned as part of the GAEN framework
and were made available only for the purpose of contact tracing to government healthcare

14

authorities. Therefore, apps without necessary authorization cannot request these permissions
and get access to the exported components.

Suggestion: MobSF should consider the system permissions deﬁned in the GAEN framework
during analysis to improve the detection of unprotected components.

• MobSF reported that 22 of the 24 apps saved sensitive data such as usernames, passwords,
and secret keys in clear text. Furthermore, MobSF claimed that 22 of the 24 apps logged
sensitive data. On further inspection, we found the claims to be false. MobSF reported the
apps because they were saving or logging string constants in ﬁles and the strings contained
words such as ”key” and ”password,” but these constants were not sensitive data.

Suggestion: Considering that none of the string constants with words like key and password
were sensitive data in our sample of apps, MobSF should consider using other heuristics to
identify sensitive data.

• MobSF falsely reported a Janus signature vulnerability in all 24 apps. Janus is a system
vulnerability that allows attackers to inject a DEX ﬁle into an APK ﬁle signed with the v1
signature scheme without aﬀecting the signatures. The vulnerability can be exploited because
an Android package can be a valid DEX ﬁle and APK ﬁle at the same time. However, this
vulnerability is exploitable only if the app runs on an Android version lower than 7.0. Android
developers ﬁxed this vulnerability in version 7.0 and above. As a result, Android APKs or
packages have to be signed with the v2 and v3 signature schemes. All 24 contact tracing apps
we considered used the v2 and v3 signature schemes to sign their APKs. MobSF ﬂagged the
apps as potentially vulnerable to Janus because the apps were also signed with the v1 signature
scheme to enable backward compatibility. We categorize this as a false positive since the app
developers have no choice but to sign their apps with the v1 signature scheme to support
Android versions less than 7.0. Moreover, the app developers are doing due diligence by
signing the apps with v2 and v3 signature schemes along with the v1 scheme (for backward
compatibility), which is the best they can do under the circumstance.

Suggestion: MobSF should ﬂag apps as potentially vulnerable to Janus if they are signed only
with the v1 signature scheme. For apps signed with v1,v2, and v3 signatures, MobSF should
consider reporting a diﬀerent label like an information label to inform the developers that while
this cannot be ﬁxed at the app stage, one should be aware that the v1 signature is vulnerable on
Android versions less than 7. Therefore, apps might want to consider supporting only Android
version 7.0 or more.

In conclusion, MobSF reported more false positives than potential true positives w.r.t potential
vulnerabilities in each app (see Tables 5 vs. 6), which shows that MobSF has a high false positive
rate. This observation is consistent with prior research eﬀorts to evaluate the eﬀectiveness of security
analysis tools for Android apps [20]. In general, static analysis tools like MobSF have a high false
positive rate, which hampers their adoption and reduces their eﬀectiveness. Due to a high false
positive rate, MobSF users have to manually verify the warnings and issues reported, which reduces
trust in the tool verdicts. Consequently, this hampers tool adoption and the overall eﬀectiveness of
the tool.

15

5 Related Work

Since the onset of the pandemic, numerous proposals have been made to automate the contact
In this context, many contact tracing apps have been implemented and
tracing process [4, 33].
deployed worldwide. This has led to a plethora of eﬀorts to survey the characteristics and evaluate
the eﬀectiveness of contact tracing apps [34, 35, 36]. Several agencies worldwide have called for an
independent assessment of the security and privacy risks posed by contact tracing apps for greater
transparency and accountability [9]. In this context, researchers have evaluated the security and
privacy of the contact tracing apps to understand if they pose any privacy risks to the users [37, 38].
In this section, we discuss the eﬀorts that are most closely related to our work and how they are
diﬀerent.

Wen et al. [39] performed a systematic study of 41 contact tracing apps deployed on Android and
iOS. They used program analysis to determine the APIs relevant for contact tracing and identify the
information collected by the apps. Additionally, they performed a cross-platform comparison of apps
available on Android and iOS. Their results show that some apps expose identiﬁable information
that can enable ﬁngerprinting of apps and tracking of speciﬁc users. Moreover, they observed that
some apps exhibited inconsistencies across platforms, which led to diﬀerent privacy implications
across the platforms. Their eﬀort was one of the ﬁrst attempts to understand the privacy and
security implications of contact tracing apps. Although their eﬀort is related to our evaluation, we
focus on diﬀerent aspects. For example, in addition to vulnerability analysis, we also analyze the
privacy policy of the apps, which their eﬀort does not consider.

Samhi et al. [34] conducted an empirical evaluation of Android apps in Google Play related to
COVID-19. The aim of their study was to broadly characterize the apps in terms of their purpose,
intended users, complexity, development process, and potential security risks. While their focus was
not on security and privacy, they observed that none of the apps they considered leaked sensitive
data based on static analysis of the apps using tools FlowDroid and IccTA. However, recent eﬀorts
to measure the eﬀectiveness of security analysis tools in Android have raised questions on the
eﬀectiveness of static analysis tools like FlowDroid and IccTA in detecting sensitive data leaks.

Hatmian et al.

[40] analyzed the privacy and security performance of 28 contact tracing apps
available on the Android platform in May-June 2020. They analyzed the permissions used by the
apps and the potential vulnerabilities in them. Further, they measured the coverage of the privacy
policy of the apps w.r.t the privacy principles outlined in the General Data Protection Regulation
(GDPR) laws [41]. Our work is diﬀerent from theirs in a number of ways. First, we focus on oﬃcial
apps developed by the US states based on the GAEN framework. None of the four US apps in
the study conducted by Hatmian et al. are based on the GAEN framework. Second, instead of
measuring the coverage of the apps’ privacy policies w.r.t the privacy principles, we analyze if the
apps’ privacy policies are consistent with their source code, that is, apps are not violating their own
privacy policies. Third, we critically analyze the verdicts reported by tools such as MobSF instead
of reporting them as is. For example, Hatmian et al. report, based on MobSF’s analysis, that all
apps they considered log sensitive information. In our evaluation, MobSF also ﬂagged all apps as
logging sensitive information. However, after manually verifying the verdict, we discovered this was
a false positive for all apps.

In November 2020, Baumgartner et al. [37] demonstrated that the GAEN framework’s design is
vulnerable to proﬁling and de-anonymizing infected persons and relay-based wormhole attacks that
are capable of generating fake contacts to derail the contact tracing process. They claimed that
if the vulnerabilities are not addressed, then all apps based on the framework will be vulnerable.

16

Instead of analyzing the GAEN framework’s design, in this paper, we are analyzing the apps based
on the GAEN framework from the perspective of whether the apps comply with their own privacy
policy and if they contain vulnerabilities that manifest due to known implementation bugs and
incorrect conﬁgurations.

Ang et al. [42] reviewed the security and privacy of 70 contact tracing apps one year after the
pandemic. They statically analyzed the apps using MobSF for vulnerabilities based on threat sce-
narios they identiﬁed for contact tracing apps. Additionally, they reported data trackers embedded
in apps that can potentially violate privacy since data collected by the trackers can be used with-
out the users’ consent. However, their privacy analysis does not include any analysis of an app’s
privacy policy. The set of apps in the evaluation by Ang et al. includes 20 apps that we considered
in our assessment. However, the results of our analysis diﬀer signiﬁcantly. For example, 80% of the
apps they considered stored sensitive information in cleartext. On the other hand, we found this
to be a false positive for 20 of the 24 apps we considered. Similarly, we discovered vulnerabilities
in our analysis (e.g., data backup) that are not reported in Ang et al. Further, Ang et al. used
dynamic analyzers such as VirusTotal [43] to detect the presence of malware in the contact tracing
apps. However, malware detection tools such as VirusTotal do not accurately detect the presence
of malware as they often falsely identify Potentially Unwanted Programs (PUPs) as malware [20].
Kouliaridis et al. [44] investigated all oﬃcial contact tracing apps deployed by European coun-
tries as of Feb 2, 2021. They analyzed the apps both statically and dynamically. Static analysis
included sensitive permissions and API calls, third-party trackers, and known vulnerabilities and
conﬁgurations that aﬀect app security based on the Common Weakness Enumerations (CWEs) [45]
and Common Vulnerabilities and Exposures (CVEs) [46]. Dynamic analysis involved instrumenting
the app’s source code, verifying if the app uses location and Bluetooth services at runtime, and
in three
monitoring network traﬃc. The evaluation in our paper diﬀers from Kouliaridis et al.
distinct ways. First, we considered a diﬀerent set of apps. Second, we analyzed the apps’ privacy
policies to determine if they are consistent with their encoded behavior. Third, we did not dynam-
ically analyze the apps. Further, there are notable diﬀerences in our static analysis observations.
For example, Kouliaridis et al. report that two-thirds of their apps, which included GAEN apps,
had a potential SQL injection vulnerability based on MobSF’s analysis. However, we observed that
MobSF falsely reported SQL injection as a vulnerability in 23 of the 24 GAEN apps we considered.

6 Caveats

In vulnerability analysis, we considered vulnerabilities in Ghera and reported by MobSF. Since
we did not cover vulnerabilities outside these sources, it is possible that they existed in the apps
but went unreported. Consequently, our vulnerability analysis may not be comprehensive. App
developers reading this report must take steps to ﬁx the reported vulnerabilities and perform further
analysis to ensure that other vulnerabilities not reported here do not exist in their apps.

We conﬁrmed the potential vulnerabilities reported by static analysis tools by manually examin-
ing them. However, we did not build malicious applications to exploit the vulnerabilities. Therefore,
we do not know to what extent the vulnerabilities are exploitable.

The results reported in this study are limited to the set of apps considered or the GAEN apps in
general. Therefore, they should not be generalized for other contact tracing apps, especially apps
not based on the GAEN framework.

17

7 Conclusion

In this paper, we conducted a systematic investigation of 24 contact tracing apps based on the GAEN
framework in the US. All the apps were implemented and deployed by the oﬃcial health departments
of the respective US states. We discovered that the considered apps are over-privileged, they violate
their own privacy policies, and contain vulnerabilities that can be exploited by malicious users to
cause harm to the app’s users. While there have been previous eﬀorts at evaluating the contact
tracing apps for privacy violations and vulnerabilities, none of them have focused on the consistency
of the apps’ privacy policies w.r.t to their encoded behavior. Although there are similarities between
our eﬀort and existing eﬀorts in terms of vulnerability analysis of contact tracing apps, the results
diﬀer markedly. Our results show that few vulnerabilities reported as potential vulnerabilities in
related evaluations are false positives. For example, several existing research eﬀorts have reported
the Janus vulnerability, which we reported as a false positive, as a true positive in their results.
Therefore, this raises the question if eﬀorts to study the privacy and security of contact tracing
apps are reporting vulnerabilities that may not manifest in reality. Reporting false positives as
potential true positives may erode the public’s trust in contact tracing apps and eventually lead to
reduced adoption, which may ultimately weaken eﬀorts to contain the pandemic. Therefore, there
is a need for researchers to continuously evaluate the security and privacy of contact tracing apps
to reproduce and verify the results.

8 Ackowledgements

We wish to thank Minqi Shi, Taylor Giles, Soroush Semerkant, Mihir Madhira, Jeﬀrey Jiminez,
Colin Ruan, and Patrick Wszeborowski, undergraduate students in the Department of Computer
Science at Stony Brook University for assisting with data collection.

References

[1] D. Klinkenberg, C. Fraser, and H. Heesterbeek, “The eﬀectiveness of contact tracing in emerg-

ing epidemics,” PloS one, vol. 1, no. 1, p. e12, 2006.

[2] T. Jiang, Y. Zhang, M. Zhang, T. Yu, Y. Chen, C. Lu, J. Zhang, Z. Li, J. Gao, and S. Zhou,
“A survey on contact tracing: the latest advancements and challenges,” ACM Transactions on
Spatial Algorithms and Systems (TSAS), vol. 8, no. 2, pp. 1–35, 2022.

[3] A. Anglemyer, T. H. Moore, L. Parker, T. Chambers, A. Grady, K. Chiu, M. Parry, M. Wilczyn-
ska, E. Flemyng, and L. Bero, “Digital contact tracing technologies in epidemics: a rapid
review,” Cochrane Database of Systematic Reviews, no. 8, 2020.

[4] N. Ahmed, R. A. Michelin, W. Xue, S. Ruj, R. Malaney, S. S. Kanhere, A. Seneviratne, W. Hu,
H. Janicke, and S. K. Jha, “A survey of covid-19 contact tracing apps,” IEEE access, vol. 8,
pp. 134 577–134 601, 2020.

[5] A. Akinbi, M. Forshaw, and V. Blinkhorn, “Contact tracing apps for the covid-19 pandemic: a
systematic literature review of challenges and future directions for neo-liberal societies,” Health
Information Science and Systems, vol. 9, no. 1, pp. 1–15, 2021.

18

[6] F. Rowe, “Contact tracing apps and values dilemmas: A privacy paradox in a neo-liberal

world,” International Journal of Information Management, vol. 55, p. 102178, 2020.

[7] F. Hassandoust, S. Akhlaghpour, and A. C. Johnston, “Individuals’ privacy concerns and
adoption of contact tracing mobile applications in a pandemic: A situational privacy calculus
perspective,” Journal of the American Medical Informatics Association, vol. 28, no. 3, pp.
463–471, 2021.

[8] T. Martin, G. Karopoulos, J. L. Hern´andez-Ramos, G. Kambourakis, and I. Nai Fovino, “De-
mystifying covid-19 digital contact tracing: A survey on frameworks and mobile apps,” Wireless
Communications and Mobile Computing, vol. 2020, 2020.

[9] U. S. G. A. Oﬃce, “Beneﬁts and challenges of smartphone applications to augment contact

tracing,” https://www.gao.gov/products/gao-21-104622, Sept 2021.

[10] T. TraceTogether, “How does tracetogether work,” 2020.

[11] R. Gupta, M. Bedi, P. Goyal, S. Wadhera, and V. Verma, “Analysis of covid-19 tracking tool
in india: case study of aarogya setu mobile application,” Digital Government: Research and
Practice, vol. 1, no. 4, pp. 1–8, 2020.

[12] S. Vaudenay, “Centralized or decentralized? the contact tracing dilemma,” Cryptology ePrint

Archive, 2020.

[13] T. Li, C. Faklaris, J. King, Y. Agarwal, L. Dabbish, J. I. Hong et al., “Decentralized is not risk-
free: Understanding public perceptions of privacy-utility trade-oﬀs in covid-19 contact-tracing
apps,” arXiv preprint arXiv:2005.11957, 2020.

[14] Google, “Exposure notiﬁcations implementation guide,” https://developers.google.com/

android/exposure-notiﬁcations/implementation-guide, Feb. 2022.

[15] Apple,

“Enexposureconﬁguration,”
exposurenotiﬁcation/enexposureconﬁguration, Feb. 2022.

https://developer.apple.com/documentation/

[16] S. Altmann, L. Milsom, H. Zillessen, R. Blasone, F. Gerdon, R. Bach, F. Kreuter, D. Nosenzo,
S. Toussaert, J. Abeler et al., “Acceptability of app-based contact tracing for covid-19: Cross-
country survey study,” JMIR mHealth and uHealth, vol. 8, no. 8, p. e19857, 2020.

[17] Google, “Publicly-available exposure notiﬁcations apps,” https://developers.google.com/

android/exposure-notiﬁcations/apps, Feb. 2022.

[18] Sufatrio, D. J. J. Tan, T.-W. Chua, and V. L. L. Thing, “Securing android: A survey, taxonomy,

and challenges,” ACM Comput. Surv., pp. 58:1–58:45, 2015.

[19] L. Li, T. F. Bissyand´e, M. Papadakis, S. Rasthofer, A. Bartel, D. Octeau, J. Klein, and
L. Traon, “Static analysis of android apps: A systematic literature review,” Information and
Software Technology, vol. 88, pp. 67–95, 2017.

[20] V.-P. Ranganath and J. Mitra, “Are free android app security analysis tools eﬀective in de-
tecting known vulnerabilities?” Empirical Software Engineering, vol. 25, no. 1, pp. 178–219,
2020.

19

[21] A. Abraham, Magaofei, M. Dobrushin, and V. Nadal, “Mobsf github,” https://github.com/

MobSF/Mobile-Security-Framework-MobSF, Feb. 2022.

[22] A. Desnos, G. Gueguen, and S. Bachmann, “Androguard,” https://androguard.readthedocs.

io/en/latest/, Feb. 2022.

[23] L. Lu, Z. Li, Z. Wu, W. Lee, and G. Jiang, “Chex: statically vetting android apps for compo-
nent hijacking vulnerabilities,” in Proceedings of the 2012 ACM conference on Computer and
communications security, 2012, pp. 229–240.

[24] T. Watanabe, M. Akiyama, F. Kanei, E. Shioji, Y. Takata, B. Sun, Y. Ishi, T. Shibahara,
T. Yagi, and T. Mori, “Understanding the origins of mobile app vulnerabilities: A large-scale
measurement study of free and paid apps,” in 2017 IEEE/ACM 14th International Conference
on Mining Software Repositories (MSR).

IEEE, 2017, pp. 14–24.

[25] J. Mitra and V.-P. Ranganath, “Ghera: A repository of android app vulnerability benchmarks,”
in Proceedings of the 13th International Conference on Predictive Models and Data Analytics
in Software Engineering, 2017, pp. 43–52.

[26] M. Ghafari, P. Gadient, and O. Nierstrasz, “Security smells in android,” in 2017 IEEE 17th
IEEE,

international working conference on source code analysis and manipulation (SCAM).
2017, pp. 121–130.

[27] P. R. Center, “An analysis of android app permissions,” https://www.pewresearch.org/

internet/2015/11/10/an-analysis-of-android-app-permissions/, Nov 2015.

[28] B. P. Sarma, N. Li, C. Gates, R. Potharaju, C. Nita-Rotaru, and I. Molloy, “Android permis-
sions: a perspective combining risks and beneﬁts,” in Proceedings of the 17th ACM symposium
on Access Control Models and Technologies, 2012, pp. 13–22.

[29] Google,

“Android
permissions/overview, May 2022.

app

permissions,”

https://developer.android.com/guide/topics/

[30] A. V. Prakash and S. Das, “Explaining citizens’ resistance to use digital contact tracing apps: A
mixed-methods study,” International Journal of Information Management, vol. 63, p. 102468,
2022.

[31] Google, “Manifest permissions in android,” https://developer.android.com/reference/android/

Manifest.permission#USE FINGERPRINT, Jul. 2022.

[32] Owasp, “Owasp top 10,” https://owasp.org/www-project-mobile-top-10/, Jul. 2022.

[33] J. Bell, D. Butler, C. Hicks, and J. Crowcroft, “Tracesecure: Towards privacy preserving

contact tracing,” arXiv preprint arXiv:2004.04059, 2020.

[34] J. Samhi, K. Allix, T. F. Bissyand´e, and J. Klein, “A ﬁrst look at android applications in google
play related to covid-19,” Empirical Software Engineering, vol. 26, no. 4, pp. 1–49, 2021.

[35] H. Cho, D. Ippolito, and Y. W. Yu, “Contact tracing mobile apps for covid-19: Privacy

considerations and related trade-oﬀs,” arXiv preprint arXiv:2003.11511, 2020.

20

[36] M. Lanzing, “Contact tracing apps: an ethical roadmap,” Ethics and information technology,

vol. 23, no. 1, pp. 87–90, 2021.

[37] L. Baumg¨artner, A. Dmitrienko, B. Freisleben, A. Gruler, J. H¨ochst, J. K¨uhlberg, M. Mezini,
R. Mitev, M. Miettinen, A. Muhamedagic et al., “Mind the gap: Security & privacy risks
of contact tracing apps,” in 2020 IEEE 19th international conference on trust, security and
privacy in computing and communications (TrustCom).

IEEE, 2020, pp. 458–467.

[38] Y. Gvili, “Security analysis of the covid-19 contact tracing speciﬁcations by apple inc. and

google inc.” Cryptology ePrint Archive, 2020.

[39] H. Wen, Q. Zhao, Z. Lin, D. Xuan, and N. Shroﬀ, “A study of the privacy of covid-19 con-
tact tracing apps,” in International Conference on Security and Privacy in Communication
Systems. Springer, 2020, pp. 297–317.

[40] M. Hatamian, S. Wairimu, N. Momen, and L. Fritsch, “A privacy and security analysis of early-
deployed covid-19 contact tracing android apps,” Empirical software engineering, vol. 26, no. 3,
pp. 1–51, 2021.

[41] E. Union, “General data protection regulation,” https://gdpr.eu/what-is-gdpr/, 2022.

[42] V. Ang and L. K. Shar, “Covid-19 one year on–security and privacy review of contact tracing

mobile apps,” IEEE Pervasive Computing, vol. 20, no. 4, pp. 61–70, 2021.

[43] H. Systemas, “Virustotal,” https://www.virustotal.com/gui/home/upload, 2022.

[44] V. Kouliaridis, G. Kambourakis, E. Chatzoglou, D. Geneiatakis, and H. Wang, “Dissecting
contact tracing apps in the android platform,” Plos one, vol. 16, no. 5, p. e0251867, 2021.

[45] Mitre, “Common weakness enumeration,” https://cwe.mitre.org/, 2022.

[46] “Common vulnerabilities and exposures,” https://cve.mitre.org/, 2022.

21

App

Collects Location Uses Insecure Uses Weak Uses HTTP Communicates With Non-US # Violations

Privacy Policy Violation

Y
Alabama
Y
Arizona
Y
California
Y
Colorado
Y
Connecticut
Y
Delaware
Y
DC
N
Guam
Hawaii
Y
Y
Louisiana
Maryland
Y
Y
Michigan
N
Minnesota
Nevada
Y
New Jersey
N
New Mexico
Y
New York
N
North Carolina Y
Y
North Dakota
Penn
Y
Y
Utah
Virginia
Y
Y
Washington
Y
Wisconsin
# Apps per
20
violation

Storage
Y
N
N
N
N
Y
N
N
Y
N
N
N
Y
N
Y
Y
Y
N
Y
Y
N
N
N
N
9

Encryption
Y
N
N
N
N
N
N
Y
Y
Y
N
N
Y
N
N
N
N
Y
N
N
N
N
N
N
6

N
N
Y
Y
N
Y
Y
N
N
N
Y
Y
Y
Y
Y
Y
N
Y
N
N
Y
N
Y
Y
14

server domain
Y
N
Y
Y
Y
N
Y
N
N
N
Y
N
Y
Y
N
Y
N
Y
Y
N
N
N
Y
Y
13

per app
4
1
3
3
2
3
3
1
3
2
3
2
4
3
2
4
1
4
3
2
2
1
3
3

Table 4: Privacy Violations by each US-based GAEN app. Columns 2-6 indicate a feature or an
action that an app claims it does not use or do in its privacy policy. The cells with Y/N denote
Yes if an app performs the action in the corresponding column and No if it does not. Y implies a
privacy violation and N implies otherwise.

22

No Certiﬁcate # Vulns.

per app
3
3
2
2
3
2
3
4
3
3
3
1
3
4
2
4
2
2
2
2
3
2
3
3

App

Unprotected
Component
Y
Alabama
N
Arizona
N
California
N
Colorado
Y
Connecticut
N
Delaware
N
DC
Y
Guam
Y
Hawaii
Y
Louisiana
Maryland
Y
N
Michigan
Y
Minnesota
Y
Nevada
N
New Jersey
New Mexico
Y
N
New York
North Carolina
N
North Dakota/Wyoming N
N
Penn
N
Utah
Virginia
Y
Y
Washington
Y
Wisconsin
# Apps per
12
Vuln

Known Vulnerabilities
Allows

Insecure Weak
PRNG
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
N
Y
Y
Y
Y
Y
Y
Y
Y
Y
N
Y
Y
22

hashing Data Backup
N
N
N
Y
N
N
N
N
N
N
N
Y
N
N
N
Y
N
N
N
N
N
N
N
N
N
N
Y
N
N
Y
Y
N
N
Y
N
N
N
Y
N
Y
Y
N
N
N
N
N
N
N
4
7

Insecure
SSL Impl. Pinning
N
Y
N
N
N
N
Y
N
N
N
N
N
N
N
N
N
N
N
N
N
N
N
N
N
1

Y
N
Y
Y
Y
N
Y
Y
Y
Y
Y
Y
Y
Y
N
Y
N
Y
N
N
Y
Y
Y
Y
18

Table 5: Known Vulnerabilities in each US-based GAEN app. Columns 2-5 indicate a known
vulnerability. The cells with Y/N denote Yes if an app contains the vulnerability and No otherwise.

23

App

SQL Injection Unprotected Component Cleartext Storage Log Sensitive

False Positive Vulnerability

Y
Alabama
Y
Arizona
Y
California
Y
Colorado
Y
Connecticut
Y
Delaware
Y
DC
Y
Guam
Hawaii
Y
Y
Louisiana
Y
Maryland
Y
Michigan
Y
Minnesota
Y
Nevada
New Jersey
Y
Y
New Mexico
Y
New York
North Carolina
Y
North Dakota/Wyoming Y
Y
Penn
Utah
Y
N
Virginia
Y
Washington
Y
Wisconsin
# Apps per
23
false positive

Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
24

Y
N
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
N
Y
Y
22

Data
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
24

Table 6: False Positives reported by MobSF in every US-based GAEN app. The cells with Y indicate
that the vulnerability in the corresponding column was falsely reported as a potential vulnerability
by MobSF and N denotes MobSF did not report the vulnerability in the corresponding column.

5

5

5

5

5

5

5

4

5

5

per app

Janus Signature # False Positives
Vulnerability
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
24

5

5

5

5

5

3

5

5

5

5

5

5

5

5

24

","2 2 0 2 l u J 8 1 ] R C . s c [ 1 v 8 7 9 8 0 . 7 0 2 2 : v i X r a A Security & Privacy Analysis of US-based Contact Tracing Apps Joydeep Mitra Department of Computer Science Stony Brook University joydeep.mitra@stonybrook.edu July 20, 2022 Abstract With the onset of COVID-19, governments worldwide planned to develop and deploy contact tracing apps to help speed up the contact tracing process. However, experts raised concerns about the long-term privacy and security implications of using these apps. Consequently, several proposals were made to design privacy-preserving contact tracing apps. To this end, Google and Apple developed the Google/Apple Exposure Notiﬁcation (GAEN) framework to help public health authorities develop privacy-preserving contact tracing apps. In the United States, 26 states used the GAEN framework to develop their contact tracing apps. In this paper, we empirically evaluate the US-based apps to determine 1) the privileges these apps have, 2) if the apps comply with their deﬁned privacy policies, and 3) if they contain known vulnerabilities that can be exploited to compromise privacy. The results show that all apps violate their privacy policies and contain several known vulnerabilities. 1 Introduction Contact tracing is one of many methods health authorities use to contain the rapid spread of COVID-19 [1]. However, manual contact tracing to keep track of the growing pandemic has been a challenge for health authorities due to the lack of human resources and the limitations of human memory to remember all possible contacts accurately [2]. Consequently, there have been several eﬀorts to automate the contact tracing process by developing mobile apps that use tracking tech- nology in mobile phones, such as GPS and Bluetooth [3, 4]. In addition, since mobile phones are ubiquitous, they can be used to eﬀectively and quickly identify a person’s contacts when they test positive for COVID. 1.1 Background 1.1.1 How do Contact Tracing Apps Work? Contact tracing apps work by estimating if two mobile phones, X and Y, are close to each other based on a metric deﬁned by local health authorities (e.g., 6 feet). Y will be notiﬁed of a potential infection if X tests positive and conﬁrms the result. An app detects proximity using Bluetooth 1 Low Energy (BLE), Global Positioning System (GPS) technology, or a combination of the two. If two phones are nearby, then the apps exchange encounter messages, which contain, among other things, random identiﬁers and signal strength. Apps may also include location data in the encounter messages. Each app keeps a record of all the encounter messages it has exchanged with other apps. If one of these messages is from a phone whose user has tested positive, then the app analyzes the encounter message to calculate the level of risk based on factors the local health authorities decided. This calculation is performed either locally in the device or by a central server. While contact tracing apps can be useful to help expedite the process of contact tracing, experts have warned that such technologies have long-term consequences for user privacy and security since they can be easily exploited and used in malicious contexts such as mass surveillance [5, 6, 7]. Therefore, contact tracing apps’ design and implementation must be carefully scrutinized before being deployed and adopted. 1.1.2 Contact Tracing App Architectures The need to ensure privacy in contact tracing apps has led to several proposals of app architectures that will best protect user privacy – centralized, decentralized, and hybrid [8, 9]. In a centralized architecture, a central server collects and stores personally identiﬁable information (PII), which is further used to generate random identiﬁers for the encounter messages. Further, the central server is responsible for determining the users that have been potentially exposed. Government health authorities have access to the data stored in the central server and can use it to perform advanced aggregate analysis, which can be useful for understanding trends to help inform mitigation eﬀorts. However, this approach negatively aﬀects user privacy since the central server is assumed to be trusted and has access to vast amounts of personal information, which unauthorized or hostile entities can potentially misuse. On the other hand, a decentralized server has minimal data. Most of the data is maintained locally on the user’s device. Each app periodically communicates with the server to download a set of random identiﬁers that have tested positive. The app then performs a risk analysis of its encounter messages locally to determine if it had come close to a device that tested positive. This architecture helps preserve privacy better than a centralized architecture. However, it limits access to crucial data that could be used to analyze the spread of the pandemic or to understand the eﬀectiveness of the app. The hybrid architecture combines features of the centralized and decentralized architecture, where random identiﬁer generation is left to the local device, whereas the server manages the risk analysis and exposure notiﬁcation. Countries have developed and deployed apps based on a centralized architecture (e.g., Singa- pore’s TraceTogether [10]) and decentralized architecture (e.g., the apps based on the Google/Apple framework). Further, many apps also used location data to enable automated contact tracing (e.g., India’s Arogya Setu [11]). However, there is no consensus among researchers about which approach is the most feasible for eﬀective contact tracing while minimizing privacy risks [12, 13]. 1.1.3 The Google Apple Exposure Notiﬁcation System In May 2020, Google and Apple collaborated to develop the Google Apple Exposure Notiﬁcation (GAEN) framework based on the decentralized architecture to help public health authorities develop contact tracing apps [14, 15]. A GAEN app works by generating a temporary key, which changes periodically. The key is used to encrypt locally stored data and to generate a random identiﬁer. The app embeds the identiﬁer 2 in encounter messages and exchanges them with other apps using BLE. When a user tests positive, a public health oﬃcial uses a veriﬁcation server to send the user a conﬁrmation code. The user can use the conﬁrmation code to upload all their recent keys (e.g., last 14 days) to a key server. Each app downloads keys periodically from the key server and compares them with a set of keys exchanged in the last few days. If there is a match, the app determines the risk of the potential exposure based on a formula pre-determined by public health authorities. 1.2 Motivation In the absence of an oﬃcial national contact tracing app in the United States, individual US states used the GAEN framework to develop their own contact tracing apps. To date, 26 US states have developed a GAEN app. Despite the security and privacy guarantees built into the GAEN framework, people in the US lack conﬁdence in the apps’ ability and intentions to protect their privacy [16]. The Center for Disease Control and Prevention (CDC) in the US recommends that healthcare authorities should conduct a third-party assessment of contact tracing apps and make the results publicly available [9]. However, according to the technology assessment conducted by the United States Government Accountability Oﬃce (GAO) at the behest of the US Congress, most states with a contact tracing apps have not conducted a third-party assessment. Moreover, the states that have conducted an evaluation, have not made the results publicly available [9]. Motivated by the CDC’s recommendations and the lack of assessments of the US-based apps, in this paper we are analyzing the privacy and security of GAEN-based Android apps for contact tracing in the US. Speciﬁcally, we are asking the following research questions: • RQ1: What degree of privilege do the GAEN-based Android apps for contact tracing have? Android apps by default have least privilege. They need to request the system or users for permission to perform privileged operations (e.g., use Bluetooth). The purpose of this question is to understand the permissions used by these apps and their privacy implications. • RQ2: Do GAEN-based Android apps for contact tracing violate their own privacy policies? Contact tracing apps are required to publish a privacy policy to inform users of the app’s capabilities and its data sharing, storage and retention policies. The purpose of this question is to determine if the privacy policy is consistent with the behavior encoded in the app’s source code. RQ3: Do GAEN-based Android apps for contact tracing contain known Android app vulner- abilities? Android apps can have vulnerabilities that can be exploited by malicious apps available locally or remotely to compromise users’ privacy. The purpose of this question is to identify if GAEN-based contact tracing apps have similar vulnerabilities. 2 Methodology In this section, we describe the apps selected, the tools chosen, and the factors we considered to answer our research questions. 2.1 App Selection We selected the oﬃcial contact tracing apps in Android of all US states developed using the exposure notiﬁcation APIs by Google and Apple (GAEN). We considered these apps since the focus of our 3 App State Alabama Arizona California Colorado Connecticut Delaware DC Guam Hawaii Louisiana Maryland Michigan Minnesota Nevada New Jersey New Mexico New York North Carolina North Dakota & Wyoming Pennsylvania Utah Virginia Washington Wisconsin App Name GuideSafe Covid Watch Arizona CA Notify CO Exposure Notiﬁcations COVID Alert CT COVID Alert DE DC CAN Guam Covid Alert AlohaSafe Alert COVID Defense MD COVID Alert MI COVID Alert COVIDaware MN COVID Trace Nevada COVID Alert NJ NM Notify COVID Alert NY SlowCOVIDNC Care19 Alert Version Name 1.10.0 2.1.11 minted1000003 minted141006 1.0.1 Package Name gov.adph.exposurenotiﬁcations gov.azdhs.covidwatch.android gov.ca.covid19.exposurenotiﬁcations minted14020 gov.co.cdphe.exposurenotiﬁcations gov.ct.covid19.exposurenotiﬁcations gov.de.covidtracker gov.dc.covid19.exposurenotiﬁcations minted1100019 org.pathcheck.guam.bt org.alohasafe.alert org.pathcheck.la.bt gov.md.covid19.exposurenotiﬁcations minted151008 gov.michigan.MiCovidExposure org.pathcheck.covidsafepathsBt.mn gov.nv.dhhs.en com.nj.gov.covidalert gov.nm.covid19.exposurenotiﬁcations minted1200004 gov.ny.health.proximity gov.nc.dhhs.exposurenotiﬁcation com.proudcrowd.exposure 1.4 1.17.12 minted1200005 1.0.1 1.0.10 1.0.15 1.9.1 1.1.5 1.6 1.2 Version Code 2764 201011 minted14020 10000032 141006 15 11000192 1947 41 2661 151008 255 3503 12000052 20 12000042 81 205 10 COVID Alert PA UT Exposure Notiﬁcations COVIDWISE WA Notify WI Exposure Notiﬁcation gov.pa.covidtracker gov.ut.covid19.exposurenotiﬁcations minted1100011 1.5 giv.vdh.exposurenotiﬁcation gov.wa.doh.exposurenotiﬁcations minted142004 gov.wi.covid19.exposurenotiﬁcations minted141003 2.0.0 46 11000112 160 142004 141003 Table 1: US-based contact tracing GAEN apps. App size Download (in MB) Date 6.70 3.56 10.07 3.38 9.94 105.55 11.8 64.52 64.64 7.37 10.19 3.19 3.19 12.12 105.62 3.9 105.90 3.1 7.23 Nov 12, 2021 Nov 19, 2021 Oct 15, 2021 Nov 19, 2021 Nov 12, 2021 Dec 3, 2021 Dec 3, 2021 Jan 19, 2022 Nov 5, 2021 Feb 4, 2022 Mar 11, 2022 Oct 15, 2021 Mar 11, 2022 Mar 25, 2022 Nov 5, 2021 Mar 4, 2022 Oct 8, 2021 Nov 5, 2021 Nov 12, 2021 105.68 11.82 9.32 10.36 9.86 Oct 14, 2021 Dec 3, 2021 Mar 24, 2022 Nov 5, 2021 Mar 19, 2022 study is to examine GAEN apps based in the US. We found the apps from the Android Developer’s oﬃcial page [17]. Each US-based app has a link to Google Play. We used the links to download the corresponding APK ﬁle from Google Play in an Emulator running an Android version supported by the app. We transferred the apk ﬁles from the emulator to a computer where we could reverse engineer and statically analyze the apps. All US states did not develop a GAEN app. Further, North Dakota and Wyoming use a single app. The Massachusetts app is built into the device and can only be installed from the device’s settings, not from Google Play. We could not obtain this app since we were using an emulator to install the apps. Consequently, we did not consider the Massachusetts app in our assessment. In total, we ended up with 24 apps. Table 1 lists all the selected apps along with their name, package, version information, size of the APK ﬁles, and when we downloaded them. 2.2 Tool Selection Research in mobile app security and privacy has led to the development of several tools and tech- niques to detect vulnerabilities and malicious behavior [18, 19]. The tools are based on static and dynamic analysis. Most use static analysis to either ﬂag vulnerabilities or malicious behavior or guide subsequent dynamic analysis. Few tools use only dynamic analysis. Prior research eﬀorts studying the eﬃcacy of such tools have observed that for freely available tools, static analysis tools detect more known Android app vulnerabilities than dynamic analysis tools [20]. Furthermore, among the static analysis tools, MobSF [21] detects the most known vulnerabilities. Based on this observation, we used MobSF as the primary tool for analysis. MobSF has a static and dynamic analyzer. The static analyzer statically analyzes the app’s source code to determine if the app uses APIs and features that are known to cause Android app 4 Figure 1: Snippet from a report generated by MobSF. Each issue has a severity, security standards it is associated with, and the source ﬁle/s in which it was detected vulnerabilities (see Figure 1). We failed to run the dynamic analyzer on the contact tracing apps that we had selected. Hence, we considered only the static analyzer. We used Androguard [22] to generate the control ﬂow graph of an app (see Figure 2), which was used to analyze the data ﬂow through the app. We tracked the data ﬂow in the control ﬂow graph to determine if data ﬂowing out of the app is sensitive or if the data being used by the app is potentially malicious. This was necessary to verify the potential data leak and data injection vulnerabilities reported by MobSF’s static analyzer, as it is known to report false positives. Speciﬁcally, we used the following strategies: • We conﬁrmed a potential data leak vulnerability if there was a path in the control ﬂow graph from a pre-identiﬁed sensitive data source node to a target node with shared storage, network, or inter-app communication APIs. Sensitive data sources include APIs used to collect user input (e.g., biometric), read from app’s private ﬁles and communication channels (e.g., Bluetooth), and strings hard coded with personal information (e.g., IP address). • We conﬁrmed a data injection vulnerability if there was a path in the control ﬂow graph from a pre-identiﬁed source node of potentially malicious data such as shared storage, network, or inter-app communication APIs to a target node and the target node was using the data without sanitizing it. Examples of a target node using potentially malicious data after sani- tization include an exported broadcast receiver that uses input data only if it was sent via an 5 intent-ﬁlter with a system-deﬁned action or a function in a target node that uses input data from a trusted remote server. Figure 2: Snippet of the list of edges in a control ﬂow graph generated by the AndroGuard tool for the California app. The source and target columns indicate the nodes in the graph. Each row is a directed edge connecting the source node to the target node. Each node has an ID and an API call contained in the source code. 2.3 Policy Analysis A focus of our study is to determine if an app’s source code is consistent with the app’s privacy policy. To this end, we downloaded each app’s privacy policy and examined them. We identiﬁed the features that the privacy policy of an app claims the app does not use (e.g., does not collect location). All selected apps in their privacy policies claim the following: 1. does not collect, store, or transmit any personally identiﬁable data. 2. stores exposure data (e.g., random IDs and exposure date) locally in the users’ device. 3. prevents unauthorized access to locally stored data. 4. encrypts locally stored data. 5. communicates with trusted servers through encrypted networks in the United States. We looked for these features in the apps using MobSF and Androguard. If at least one such feature was found in the app, then we deemed that the app violates its own privacy policy. For example, using MobSF, we determined instances in an app’s source code where data was being stored in external storage. We then used the app’s control ﬂow graph (generated by Androguard) to determine the source of the data stored in external storage and whether the source is sensitive. If sensitive data was being stored in external storage, then we deemed it as a violation of the app’s privacy policy due to bullet three listed above. 6 2.4 Known Vulnerabilities Analysis Android apps have vulnerabilities, which malicious apps exploit to cause harm to the user [23, 24, 25, 26]. Therefore, it is necessary to ensure such vulnerabilities do not occur in apps, especially contact tracing apps, which deal with sensitive personal information and can perform privileged operations on the phone. Hence, we analyzed these apps for known Android app vulnerabilities. We used MobSF, and the Ghera repository [25] for our analysis. MobSF provides a list of potential vulnerabilities in its static analysis report. We investigated each of them to determine their veracity as MobSF is known to report false positives. Moreover, MobSF does not detect all known vulnerabilities. Therefore, we used Ghera, a repository of 60 known vulnerability benchmarks, to further guide our analysis. Each benchmark in Ghera is well-documented and contains only the features related to the vulnerability captured in the benchmark. We statically analyzed the 24 apps in our set using MobSF to determine the features/APIs used in them. We then considered only those features also used in the Ghera benchmarks. We investigated each such feature to determine if it resulted in a potentially exploitable vulnerability, that is, it could be exploited by a malicious app on the device or remotely. For example, an app can give unrestricted access to another app using the pending intent feature in Android. If MobSF found an app with a component using pending intent, we investigated the component to conﬁrm if it performed privileged operations, which, if true, could result in a potential privilege escalation attack. 3 Results In this section, we report the ﬁndings of our study in terms of the permissions requested and used by the apps, their potential privacy violations, and the potential vulnerabilities in them that can be exploited to cause harm to the user. 3.1 RQ1: What degree of privilege do the GAEN-based Android apps for contact tracing have? A total of eight permissions are used across all 24 contact tracing apps. An app uses approximately seven permissions on average. The least number of permissions used by an app is six, and the highest is eight. These statistics are higher than the average permissions used by apps in general, which is ﬁve [27], with nearly 100k of one million apps using zero permissions as of June 2014. Furthermore, ﬁve of the eight permissions are used by all 24 apps, which suggests that only ﬁve of them are necessary for contact tracing. The other three permissions are most likely extraneous. Therefore, the GAEN apps in Android are over-privileged, which is concerning since these apps have access to vast amounts of personal data. Over-privileged apps increase the risk of being exploited since they expose a larger attack surface due to having more privileges than needed [28]. Therefore, developers of these apps should carefully consider the required permissions and ensure that only the necessary ones are used by their apps. All permissions, except two, are normal permissions [29], that is, granted by Android when the app is installed. Therefore, the apps do not need to ask the user for permission at runtime; they always have them. In the context of contact tracing, where many users install these apps to help prevent the spread of the pandemic, this permission model compromises privacy by not giving users control of granting or denying permission to the apps. Furthermore, it assumes that the apps are benign and that users should trust them during installation. This assumption hinders an app’s 7 Permissions Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y N N N N Y N Y Y Y N N Y N Y N Y N N Y N N N N Y N Y Y Y N N Y N Y N N N N Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 # Permissions per app App 8 Y Alabama 8 Y Arizona 6 Y California 6 Y Colorado 6 Y Connecticut 7 Y Delaware 6 Y DC 8 Y Guam 8 Y Hawaii 8 Y Louisiana 6 Y Maryland Michigan 6 Y 8 Y Minnesota 8 Y Nevada 8 Y New Jersey 8 Y New Mexico New York 7 Y 6 North Carolina Y North Dakota 8 Y & Wyoming Penn Utah Virginia Washington Wisconsin # Apps per permission N N N N N N N N N N N N N Y N Y N N Y N Y N N N N N N N N N N N N N N N N N N Y N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N Y N Y N N Y Y Y Y Y Y 24 Y Y Y Y Y 24 Y Y Y Y Y 24 Y Y Y Y Y 24 Y Y Y Y Y 24 Y Y Y Y Y 23 Y N N N N 9 N N N N N 2 N N N N N 2 N N N N N 7 N N N N N 2 N N N N N 2 7 6 6 6 6 Table 2: Permissions used by the US-based GAEN apps as declared in their manifest ﬁle. The permissions names are encoded as PN due to lack of space. The exact permission names are listed in Table 3 Permission Code Permission Name P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 android.permission.INTERNET android.permission.VIBRATE android.permission.RECEIVE BOOT COMPLETED android.permission.BLUETOOTH android.permission.ACCESS NETWORK STATE android.permission.ACCESS WIFI STATE android.permission.WAKE LOCK android.permission.FOREGROUND SERVICE com.google.android.c2dm.permission.RECEIVE com.google.android.ﬁnsky.permission.BIND GET INSTALL REFERRER SERVICE android.permission.USE BIOMETRIC android.permission.USE FINGERPRINT Table 3: Name of permissions used in all US-based GAEN apps. 8 adoption in an environment where the role of government-deployed contact tracing apps is being seen with suspicion [30]. However, app developers cannot resolve this issue as Android, not the apps, deﬁne these permissions. Therefore, platform developers should either consider changing the way these permissions are granted or deﬁne custom permissions that will be used in the context of contact tracing. Google-based third-party analytics libraries, not the core Android system, deﬁne two permis- sions. Only two of the 24 considered apps use these permissions. Therefore, this raises the question if these permissions are necessary for contact tracing. Further, such apps are designed to have the necessary permissions to access privileged operations in the device (e.g., Bluetooth). In this context, should contact tracing apps further increase security and privacy risks by using third-party libraries that are not directly related to the task of contact tracing? The USE FINGERPRINT permission is deprecated [31]. Two of 24 apps use it. While using deprecated permissions is not recommended, this is not a major concern as both the apps use the permission in conjunction with the USE BIOMETRIC permission, which is the recommended permission to use instead of USE FINGERPRINT. Nevertheless, apps should not use deprecated permissions since they may have unknown and unexpected security and privacy implications. Two of the 24 apps use biometric permissions to access the device capabilities to use and collect biometric information. While this is not a violation of privacy by itself, it increases the risk of exposing private sensitive user info to unauthorized entities. Furthermore, since a majority of the GAEN apps are not using biometric permissions, it raises questions about the necessity of using such capabilities to collect and transmit biometric-related data. The ACCESS WIFI STATE permission is used by 17 of the 24 selected apps. This suggests that not all apps need this permission for contact tracing. Apps use this permission when they connect to a remote server through WiFi. Using WiFi is not always secure since WiFi networks may not be protected and may be susceptible to Man-In-The-Middle attacks. Therefore, apps that collect and transmit sensitive information over the internet should avoid WiFi communication. The VIBRATE permission is used by nine of 24 apps to control the device’s vibration. This permission is not necessarily benign. If used incorrectly or maliciously, it may damage a user’s phone [27]. Therefore, it is best to avoid using such permissions if not absolutely necessary. Since a majority of the selected apps do not use the VIBRATE permission, this permission is not likely necessary for GAEN apps. Only one of the 24 apps declares a query element in its manifest ﬁle to indicate the list of apps it can communicate with. Speciﬁcally, the Arizona app declares that the query elements android.intent.action.DIAL and android.intent.action.SEND in its manifest ﬁles. This implies that the Arizona app has capabilities to place phone calls and share data with any app with the SEND intent. In the context of contact tracing, these capabilities are unnecessary and pose additional risk to the security and privacy of the app’s users. 3.2 RQ2: Do GAEN-based Android apps for contact tracing violate their own privacy policies? All US-based GAEN apps violate their privacy policy since their behavior is inconsistent with at least one of the claims in their policy. This is concerning since contact tracing apps collect and store vast amounts of private data, which can be potentially misused. Therefore, they should take additional care to guarantee their users’ privacy or at least be consistent with their own policies. The GAEN apps are designed not to collect, store, or track location. However, 20 of the 24 9 GAEN apps collect users’ locations despite claiming otherwise in their privacy policy. On further analysis, we discovered that these apps are not explicitly collecting location. However, they use a library called TwilightManager that collects user location to determine the local time. Apps that have conﬁgured dark themes automatically import and use this library. Therefore, these apps violate their privacy policy due to using a library that collects location. Consequently, this raises the question if app developers are aware of the privacy implications of the libraries they used in their apps. Vetting the libraries before using them is especially crucial for contact tracing apps, which malicious actors can potentially misuse to compromise user privacy. The Nevada and New Mexico apps collect Biometrics for authentication or to encrypt locally stored data. However, the privacy policy of these apps does not explicitly state that they col- lect biometric information. Moreover, they mention that the apps do not collect any personally identiﬁable information. Hence, we deem these apps as violating their own privacy policy. All apps claim in their privacy policy that they store exposure-related data in local storage in a way that prevents unauthorized access. However, nine of the 24 apps store their data in external storage. Any app installed on the device (including malicious apps) can access this data if they have the necessary permission to access external storage. Therefore, all other apps can potentially access the exposure-related data stored by these nine apps. Consequently, this leads to a violation of privacy as deﬁned in the apps’ privacy policy. This issue could have been addressed by using the app’s internal storage instead of the external storage because, in Android, the internal storage of an app can only be accessed by the app. Moreover, Android recommends that an app’s data should be stored in internal storage unless it needs to be shared with other apps. Considering more than a third of the selected apps used external storage instead of internal storage suggests that several developers of these apps are not aware of the diﬀerence between the two. This lack of knowledge is concerning since the apps collect and store vast amounts of personal information. If access to the data is not minimized, then they can be potential targets for cyberattacks by malicious actors. The privacy policy of all the selected apps mentions that data stored locally in the device is protected by encryption. However, six of the 24 apps use the AES block cipher in ECB mode, which is a weak cipher. Weak encryption is a violation of the apps’ privacy policy of encrypting local data since it leads to a potential leak of sensitive data. This result implies that a signiﬁcant number of developers are not aware that AES in ECB mode should be avoided despite several security guidelines, such as OWASP, recommending not to use it. One likely reason that developers end up using the ECB mode is that this is the default mode for AES encryption in Android. As a result, developers must explicitly change the mode. However, this oversight is surprising since the focus of the GAEN apps was to ensure user privacy by storing personally identiﬁable information locally and protecting them via strong encryption. Failure to use strong encryption despite claiming to do so in their privacy policies raises questions about their diligence in protecting users’ data from misuse. Fourteen of the 24 apps use HTTP to communicate with remote servers. Consequently, their communication can be potentially hijacked by Man-In-The-Middle attacks. Furthermore, this is inconsistent with the apps’ stated privacy policy that mentions that communication with remote servers is encrypted end-to-end. Considering that HTTP is used in more than half of the selected apps implies that the developers of these apps are either unaware of the implications of using HTTP or did not do due diligence to verify that all communication with remote web servers uses HTTPS. The use of HTTP is concerning, especially since using HTTPS is an essential requirement of apps that transmit sensitive personal information to remote web servers. Moreover, the developers of apps promoted for large-scale use and with access to vast amounts of personal information must be 10 more perceptive of such issues and take extra care to avoid them. Exposure notiﬁcation apps transmit exposure-related data (e.g., random IDs and exposure date) to remote servers when a user consents to share their data in the event of an exposure to COVID-19. The apps’ privacy policy does not mention the location of the servers. Therefore, users reasonably assume that the servers are in the United States. However, 13 of the 24 apps communicate with exposure notiﬁcation servers outside the United States. Furthermore, the privacy policy of two of the 13 apps explicitly states that the apps only communicate with servers in the United States. Therefore, this raises the question if the apps are sending exposure-related data of US-based residents outside the US. The apps should be more transparent and explicitly state in their policies the location of the exposure notiﬁcation servers so users can make informed decisions. 3.3 RQ3: Do GAEN-based Android apps for contact tracing contain known Android app vulnerabilities? We discovered a total of six known vulnerabilities across all 24 apps, and each app had approximately two vulnerabilities on average. All 24 apps had at least one of the six vulnerabilities. Table 5 shows the breakdown of the vulnerabilities found in each app. The vulnerabilities are brieﬂy described as follows: • Unprotected Component. Android apps consist of components. Apps can export their com- ponents to share operations or data with other apps. However, if components are exported without restrictions, all apps, including malicious apps, can access them. Consequently, it can lead to denial-of-service, data leak, and data injection attacks. • Insecure PRNG. Apps that use the Random package in Java to generate pseudo-random numbers can be more easily predicted than apps that use the SecureRandom package. Since contact tracing apps based on GAEN rely on random identiﬁers, they should use a random number generator that makes it harder to predict the random numbers. Therefore, apps should use the SecureRandom package instead of the Random package to generate hard-to- predict random identiﬁers. • Weak hashing. Hashing algorithms such as MD5 and SHA-1 are considered weak. Attackers can use a hash collision to forge a duplicate hash. Therefore, apps should avoid using them to prevent forgery attacks. • Data Backup. Android allows users to create backups of all data in an app without having root privileges. Consequently, malicious users with access to the device will be able to create a backup of all the app’s data using a USB. Apps can be conﬁgured to protect against this potential attack by setting the allowBackup attribute in the app’s manifest ﬁle to false. Android recommends apps to disable this feature to prevent malicious users from accessing an app’s local data. • Insecure TLS/SSL Implementation. Apps using TLS/SSL protocol to communicate with remote servers must verify the trustworthiness of the servers. The established way to verify trust is for the app to maintain a list of trusted certiﬁcate authorities (CAs). The server is conﬁgured with a certiﬁcate containing a public key and a matching private key. The certiﬁcate must be signed by a certiﬁcate authority (CA) trusted by the app. Generally, the list of trusted CAs is pre-installed in the device on which the app is installed. However, the 11 connection may fail if the certiﬁcate used to conﬁgure the server (1) is signed by a CA, not in the list of trusted CAs, (2) or is self-signed, (3) or is signed by an intermediate certiﬁcate missing from the server conﬁguration. While the third reason is addressed at the server-side, the ﬁrst two reasons are addressed by implementing a custom TrustManager, an Android API, in the app. Implementation mistakes in the custom TrustManager lead to vulnerabilities (e.g., trust all CAs) that can lead to Man-In-The-Middle attacks. • Unpinned Certiﬁcates. Apps installed in a device trust all CAs that are pre-conﬁgured with the device. App developers can further restrict the CAs that the app will trust by pinning a set of trusted CAs to the app. The app then trusts only the pinned CAs and not any other CA, including the ones trusted by the device. Although not mandatory, certiﬁcate pinning is good security practice. However, they should be used with care as they can hamper usability due to communication failure because of outdated certiﬁcates as a result of changes to the server conﬁguration. Twelve of the 24 apps had at least one unprotected component vulnerability. We found sev- eral manifestations of this vulnerability. For example, apps used a third-party library that had an unprotected broadcast receiver that could write to shared preferences. Malicious apps could potentially exploit this vulnerability to execute data injection attacks. In other instances, apps deﬁned an unprotected activity that could access Bluetooth and location. As a result, malicious apps could potentially exploit this activity to get access to privileged features without having the necessary permissions. Furthermore, few apps deﬁned an activity that could share exposure-related diagnostic information with a remote server. Since this activity was exported without restriction, malicious apps could potentially exploit this vulnerability to communicate with the remote server. Apps must protect these components by making them private to the app. If these apps need to share these components with the underlying system, then they must protect the components with system permissions. All apps, except two, chose to use an insecure PRNG to generate the random identiﬁers used to identify devices where the app is installed anonymously. Choosing an insecure PRNG over a more secure PRNG allows malicious actors to potentially predict the random identiﬁer more easily, which could be used to create duplicate identiﬁers and hence create erroneous or fake exposure-related data entries. Therefore, all the selected apps must use the SecureRandom package to generate random identiﬁers. Seven of the 24 apps use MD5 or SHA-1, which are weak hashing algorithms. Choosing a weak hashing algorithm for contact tracing apps is problematic since these apps are expected to provide strong privacy and security guarantees. A strong hashing algorithm is the most basic requirement with which the app can ensure the integrity of the information it stores. A sixth of the 24 apps, that is, four apps allow users to back up the app’s data without rooting the device. This is insecure for GAEN apps since they store a user’s and their contacts’ exposure- related data locally in the app. A malicious user with access to the device but without root access can get access to this data and misuse it. Apps that have this feature should disable it by setting the allowbackup attribute in their manifest ﬁle to false. Five of the 24 apps chose to use certiﬁcate pinning instead of relying on the device’s list of trusted CAs. While certiﬁcate pinning provides additional protection against MITM attacks, most apps choose not to use it, possibly because of potential connection failures due to the pinned certiﬁcates becoming outdated. In such situations, the only way to restore the app is by pushing a software update, which the users must install. Temporary connection failures are not ideal and could render 12 an app useless. However, in the context of the GAEN framework, where communication with remote servers is minimal1 and security is paramount, it is advisable for the apps to use certiﬁcate pinning to reduce the risk of an MITM attack. Further, apps could pin backup certiﬁcates to prevent relying on only one pinned certiﬁcate. If one of them is outdated, the app can use the backups to connect to the server. MobSF reported an insecure SSL implementation vulnerability in exactly one app (the Arizona app) because the app was using a pinned self-signed certiﬁcate. Apps pinning self-signed certiﬁcates have beneﬁts and limitations in terms of preventing MITM attacks. Consider a scenario where an app has pinned a certiﬁcate signed by a CA that has been compromised. In this situation, the app will need to be updated via a software update with a newly issued certiﬁcate. On the other hand, if the app had pinned a self-signed certiﬁcate, then the app only trusts that certiﬁcate and will not be aﬀected if any other CA’s certiﬁcate is compromised. However, using self-signed certiﬁcates are secure only if they are continuously monitored. In its absence, app developers may not know about a compromised certiﬁcate, and the app’s communication with the compromised server will continue unknowingly. However, actively maintaining self-signed certiﬁcates is more cumbersome than using certiﬁcates from a trusted CA. Trusted CAs can revoke compromised certiﬁcates used by a server to stop communication between the app with the pinned certiﬁcate and the server, protecting the user from further harm. Therefore, in the general case, it is more secure to pin certiﬁcates signed by trusted CAs instead of self-signing them. 4 Discussion 4.1 Observations on the apps The results show that contact tracing apps in Android based on the GAEN framework are over- privileged. Further, in Android, the apps are granted permission to use privileged system features at install time. As a result, users have less control over granting permission to these apps at runtime. This is concerning since these apps could potentially be used for tasks other than contact tracing, such as mass surveillance. Therefore, the apps must collect minimal information and use the least privileges. The GAEN framework was developed to help create apps that preserve user privacy. How- ever, our analysis shows that all apps violate their own privacy policy due to several potential reasons, such as developer oversight and developers’ lacking domain knowledge and awareness of the underlying platform. Oversight is concerning but understandable since contact tracing apps were developed hurriedly to tackle the challenges of a growing pandemic. However, oversight in this context leads to a lack of transparency and credibility. It exacerbates the skepticism that the general public has towards contact tracing apps and hampers their widespread adoption. Weak adoption is not desirable since contact tracing apps if used eﬀectively, are a vital tool to contain the pandemic. If developer oversight is the reason for privacy violations, then there is a need to develop tools and techniques that help developers write privacy policies that are consistent with their app’s behavior and vice versa. Few apps in our study missed mentioning in their privacy policy all the personally identiﬁable information that they collected in their apps. One possible reason for this is a lack of domain 1Users only connect with the remote server to upload a positive test result. Also, apps periodically connect with the remote server to check for positive cases. 13 knowledge in developers. In this context, privacy research eﬀorts should focus on developing meth- ods to help identify domain-speciﬁc personally identiﬁable information. Contact tracing apps have features that could be misused to violate users’ privacy. Therefore, it is crucial to accurately iden- tify the information that the apps collect so users can make informed decisions about the privacy implications of using the apps. The GAEN framework made it easier for healthcare providers to create apps for eﬀective contact tracing without the need to know the details of the underlying platform [9]. This is also evident from certain privacy violations, which could have been avoided if developers had known about the underlying platform behavior. Therefore, existing research in app security and privacy should focus on developing methods and tools to assist less experienced developers gain the necessary knowledge to avoid privacy violations. The selected apps had vulnerabilities that are well known in the app development community. In fact, all four vulnerabilities that were discovered are part of the OWASP top 10 [32], a popular set of guidelines for developing secure mobile and web apps. The apps had these vulnerabilities despite the focus on ensuring that the apps are secure and preserve user privacy. This suggests that the app developers did not have the experience to avoid these mistakes, or they did not have access to tools to help them prevent these vulnerabilities eﬀectively. The latter is less likely since app development IDEs such as Android Studio have support for detecting and preventing such vulnerabilities during development. Therefore, it is more likely that the states did not allocate the resources to recruit developers with suﬃcient experience in mobile app development. While it is understandable that states need to prioritize their resources to tackle a pandemic, they should have planned better before developing and deploying apps with long-term consequences for user privacy and security. 4.2 Observations on MobSF While investigating the potential vulnerabilities MobSF reported, we discovered that a few of them were false positives, that is, falsely reported as vulnerabilities. We report and discuss them to help tool developers like MobSF improve their tools. Brieﬂy, MobSF reported a total of ﬁve false positives across the 24 apps. Further, MobSF reported ﬁve false positives for all apps except the Virginia app and the Arizona app, which had three and four false positives, respectively. We explain the false positives, their likely reasons, and suggestions on how to avoid them as follows: • The SQLInjection vulnerability was falsely reported in 23 of the 24 apps. MobSF reported the vulnerability in apps using the execSQL API to execute SQL queries. However, using execSQL leads to SQLInjection only if the query string has user-supplied input with potentially malicious SQL. None of the queries reported as being potentially vulnerable to SQLInjection relied on user input. Hence, they were not vulnerable to SQLInjection. Suggestion: MobSF should ﬂag queries in execSQL only if they rely on user-input and are not parameterized. • MobSF falsely reported an unprotected component vulnerability in all 24 apps because it found components in these apps that were exported. However, these components were also protected by system permissions, that is, only the system could be granted access to these components. Furthermore, these permissions were deﬁned as part of the GAEN framework and were made available only for the purpose of contact tracing to government healthcare 14 authorities. Therefore, apps without necessary authorization cannot request these permissions and get access to the exported components. Suggestion: MobSF should consider the system permissions deﬁned in the GAEN framework during analysis to improve the detection of unprotected components. • MobSF reported that 22 of the 24 apps saved sensitive data such as usernames, passwords, and secret keys in clear text. Furthermore, MobSF claimed that 22 of the 24 apps logged sensitive data. On further inspection, we found the claims to be false. MobSF reported the apps because they were saving or logging string constants in ﬁles and the strings contained words such as ”key” and ”password,” but these constants were not sensitive data. Suggestion: Considering that none of the string constants with words like key and password were sensitive data in our sample of apps, MobSF should consider using other heuristics to identify sensitive data. • MobSF falsely reported a Janus signature vulnerability in all 24 apps. Janus is a system vulnerability that allows attackers to inject a DEX ﬁle into an APK ﬁle signed with the v1 signature scheme without aﬀecting the signatures. The vulnerability can be exploited because an Android package can be a valid DEX ﬁle and APK ﬁle at the same time. However, this vulnerability is exploitable only if the app runs on an Android version lower than 7.0. Android developers ﬁxed this vulnerability in version 7.0 and above. As a result, Android APKs or packages have to be signed with the v2 and v3 signature schemes. All 24 contact tracing apps we considered used the v2 and v3 signature schemes to sign their APKs. MobSF ﬂagged the apps as potentially vulnerable to Janus because the apps were also signed with the v1 signature scheme to enable backward compatibility. We categorize this as a false positive since the app developers have no choice but to sign their apps with the v1 signature scheme to support Android versions less than 7.0. Moreover, the app developers are doing due diligence by signing the apps with v2 and v3 signature schemes along with the v1 scheme (for backward compatibility), which is the best they can do under the circumstance. Suggestion: MobSF should ﬂag apps as potentially vulnerable to Janus if they are signed only with the v1 signature scheme. For apps signed with v1,v2, and v3 signatures, MobSF should consider reporting a diﬀerent label like an information label to inform the developers that while this cannot be ﬁxed at the app stage, one should be aware that the v1 signature is vulnerable on Android versions less than 7. Therefore, apps might want to consider supporting only Android version 7.0 or more. In conclusion, MobSF reported more false positives than potential true positives w.r.t potential vulnerabilities in each app (see Tables 5 vs. 6), which shows that MobSF has a high false positive rate. This observation is consistent with prior research eﬀorts to evaluate the eﬀectiveness of security analysis tools for Android apps [20]. In general, static analysis tools like MobSF have a high false positive rate, which hampers their adoption and reduces their eﬀectiveness. Due to a high false positive rate, MobSF users have to manually verify the warnings and issues reported, which reduces trust in the tool verdicts. Consequently, this hampers tool adoption and the overall eﬀectiveness of the tool. 15 5 Related Work Since the onset of the pandemic, numerous proposals have been made to automate the contact In this context, many contact tracing apps have been implemented and tracing process [4, 33]. deployed worldwide. This has led to a plethora of eﬀorts to survey the characteristics and evaluate the eﬀectiveness of contact tracing apps [34, 35, 36]. Several agencies worldwide have called for an independent assessment of the security and privacy risks posed by contact tracing apps for greater transparency and accountability [9]. In this context, researchers have evaluated the security and privacy of the contact tracing apps to understand if they pose any privacy risks to the users [37, 38]. In this section, we discuss the eﬀorts that are most closely related to our work and how they are diﬀerent. Wen et al. [39] performed a systematic study of 41 contact tracing apps deployed on Android and iOS. They used program analysis to determine the APIs relevant for contact tracing and identify the information collected by the apps. Additionally, they performed a cross-platform comparison of apps available on Android and iOS. Their results show that some apps expose identiﬁable information that can enable ﬁngerprinting of apps and tracking of speciﬁc users. Moreover, they observed that some apps exhibited inconsistencies across platforms, which led to diﬀerent privacy implications across the platforms. Their eﬀort was one of the ﬁrst attempts to understand the privacy and security implications of contact tracing apps. Although their eﬀort is related to our evaluation, we focus on diﬀerent aspects. For example, in addition to vulnerability analysis, we also analyze the privacy policy of the apps, which their eﬀort does not consider. Samhi et al. [34] conducted an empirical evaluation of Android apps in Google Play related to COVID-19. The aim of their study was to broadly characterize the apps in terms of their purpose, intended users, complexity, development process, and potential security risks. While their focus was not on security and privacy, they observed that none of the apps they considered leaked sensitive data based on static analysis of the apps using tools FlowDroid and IccTA. However, recent eﬀorts to measure the eﬀectiveness of security analysis tools in Android have raised questions on the eﬀectiveness of static analysis tools like FlowDroid and IccTA in detecting sensitive data leaks. Hatmian et al. [40] analyzed the privacy and security performance of 28 contact tracing apps available on the Android platform in May-June 2020. They analyzed the permissions used by the apps and the potential vulnerabilities in them. Further, they measured the coverage of the privacy policy of the apps w.r.t the privacy principles outlined in the General Data Protection Regulation (GDPR) laws [41]. Our work is diﬀerent from theirs in a number of ways. First, we focus on oﬃcial apps developed by the US states based on the GAEN framework. None of the four US apps in the study conducted by Hatmian et al. are based on the GAEN framework. Second, instead of measuring the coverage of the apps’ privacy policies w.r.t the privacy principles, we analyze if the apps’ privacy policies are consistent with their source code, that is, apps are not violating their own privacy policies. Third, we critically analyze the verdicts reported by tools such as MobSF instead of reporting them as is. For example, Hatmian et al. report, based on MobSF’s analysis, that all apps they considered log sensitive information. In our evaluation, MobSF also ﬂagged all apps as logging sensitive information. However, after manually verifying the verdict, we discovered this was a false positive for all apps. In November 2020, Baumgartner et al. [37] demonstrated that the GAEN framework’s design is vulnerable to proﬁling and de-anonymizing infected persons and relay-based wormhole attacks that are capable of generating fake contacts to derail the contact tracing process. They claimed that if the vulnerabilities are not addressed, then all apps based on the framework will be vulnerable. 16 Instead of analyzing the GAEN framework’s design, in this paper, we are analyzing the apps based on the GAEN framework from the perspective of whether the apps comply with their own privacy policy and if they contain vulnerabilities that manifest due to known implementation bugs and incorrect conﬁgurations. Ang et al. [42] reviewed the security and privacy of 70 contact tracing apps one year after the pandemic. They statically analyzed the apps using MobSF for vulnerabilities based on threat sce- narios they identiﬁed for contact tracing apps. Additionally, they reported data trackers embedded in apps that can potentially violate privacy since data collected by the trackers can be used with- out the users’ consent. However, their privacy analysis does not include any analysis of an app’s privacy policy. The set of apps in the evaluation by Ang et al. includes 20 apps that we considered in our assessment. However, the results of our analysis diﬀer signiﬁcantly. For example, 80% of the apps they considered stored sensitive information in cleartext. On the other hand, we found this to be a false positive for 20 of the 24 apps we considered. Similarly, we discovered vulnerabilities in our analysis (e.g., data backup) that are not reported in Ang et al. Further, Ang et al. used dynamic analyzers such as VirusTotal [43] to detect the presence of malware in the contact tracing apps. However, malware detection tools such as VirusTotal do not accurately detect the presence of malware as they often falsely identify Potentially Unwanted Programs (PUPs) as malware [20]. Kouliaridis et al. [44] investigated all oﬃcial contact tracing apps deployed by European coun- tries as of Feb 2, 2021. They analyzed the apps both statically and dynamically. Static analysis included sensitive permissions and API calls, third-party trackers, and known vulnerabilities and conﬁgurations that aﬀect app security based on the Common Weakness Enumerations (CWEs) [45] and Common Vulnerabilities and Exposures (CVEs) [46]. Dynamic analysis involved instrumenting the app’s source code, verifying if the app uses location and Bluetooth services at runtime, and in three monitoring network traﬃc. The evaluation in our paper diﬀers from Kouliaridis et al. distinct ways. First, we considered a diﬀerent set of apps. Second, we analyzed the apps’ privacy policies to determine if they are consistent with their encoded behavior. Third, we did not dynam- ically analyze the apps. Further, there are notable diﬀerences in our static analysis observations. For example, Kouliaridis et al. report that two-thirds of their apps, which included GAEN apps, had a potential SQL injection vulnerability based on MobSF’s analysis. However, we observed that MobSF falsely reported SQL injection as a vulnerability in 23 of the 24 GAEN apps we considered. 6 Caveats In vulnerability analysis, we considered vulnerabilities in Ghera and reported by MobSF. Since we did not cover vulnerabilities outside these sources, it is possible that they existed in the apps but went unreported. Consequently, our vulnerability analysis may not be comprehensive. App developers reading this report must take steps to ﬁx the reported vulnerabilities and perform further analysis to ensure that other vulnerabilities not reported here do not exist in their apps. We conﬁrmed the potential vulnerabilities reported by static analysis tools by manually examin- ing them. However, we did not build malicious applications to exploit the vulnerabilities. Therefore, we do not know to what extent the vulnerabilities are exploitable. The results reported in this study are limited to the set of apps considered or the GAEN apps in general. Therefore, they should not be generalized for other contact tracing apps, especially apps not based on the GAEN framework. 17 7 Conclusion In this paper, we conducted a systematic investigation of 24 contact tracing apps based on the GAEN framework in the US. All the apps were implemented and deployed by the oﬃcial health departments of the respective US states. We discovered that the considered apps are over-privileged, they violate their own privacy policies, and contain vulnerabilities that can be exploited by malicious users to cause harm to the app’s users. While there have been previous eﬀorts at evaluating the contact tracing apps for privacy violations and vulnerabilities, none of them have focused on the consistency of the apps’ privacy policies w.r.t to their encoded behavior. Although there are similarities between our eﬀort and existing eﬀorts in terms of vulnerability analysis of contact tracing apps, the results diﬀer markedly. Our results show that few vulnerabilities reported as potential vulnerabilities in related evaluations are false positives. For example, several existing research eﬀorts have reported the Janus vulnerability, which we reported as a false positive, as a true positive in their results. Therefore, this raises the question if eﬀorts to study the privacy and security of contact tracing apps are reporting vulnerabilities that may not manifest in reality. Reporting false positives as potential true positives may erode the public’s trust in contact tracing apps and eventually lead to reduced adoption, which may ultimately weaken eﬀorts to contain the pandemic. Therefore, there is a need for researchers to continuously evaluate the security and privacy of contact tracing apps to reproduce and verify the results. 8 Ackowledgements We wish to thank Minqi Shi, Taylor Giles, Soroush Semerkant, Mihir Madhira, Jeﬀrey Jiminez, Colin Ruan, and Patrick Wszeborowski, undergraduate students in the Department of Computer Science at Stony Brook University for assisting with data collection. References [1] D. Klinkenberg, C. Fraser, and H. Heesterbeek, “The eﬀectiveness of contact tracing in emerg- ing epidemics,” PloS one, vol. 1, no. 1, p. e12, 2006. [2] T. Jiang, Y. Zhang, M. Zhang, T. Yu, Y. Chen, C. Lu, J. Zhang, Z. Li, J. Gao, and S. Zhou, “A survey on contact tracing: the latest advancements and challenges,” ACM Transactions on Spatial Algorithms and Systems (TSAS), vol. 8, no. 2, pp. 1–35, 2022. [3] A. Anglemyer, T. H. Moore, L. Parker, T. Chambers, A. Grady, K. Chiu, M. Parry, M. Wilczyn- ska, E. Flemyng, and L. Bero, “Digital contact tracing technologies in epidemics: a rapid review,” Cochrane Database of Systematic Reviews, no. 8, 2020. [4] N. Ahmed, R. A. Michelin, W. Xue, S. Ruj, R. Malaney, S. S. Kanhere, A. Seneviratne, W. Hu, H. Janicke, and S. K. Jha, “A survey of covid-19 contact tracing apps,” IEEE access, vol. 8, pp. 134 577–134 601, 2020. [5] A. Akinbi, M. Forshaw, and V. Blinkhorn, “Contact tracing apps for the covid-19 pandemic: a systematic literature review of challenges and future directions for neo-liberal societies,” Health Information Science and Systems, vol. 9, no. 1, pp. 1–15, 2021. 18 [6] F. Rowe, “Contact tracing apps and values dilemmas: A privacy paradox in a neo-liberal world,” International Journal of Information Management, vol. 55, p. 102178, 2020. [7] F. Hassandoust, S. Akhlaghpour, and A. C. Johnston, “Individuals’ privacy concerns and adoption of contact tracing mobile applications in a pandemic: A situational privacy calculus perspective,” Journal of the American Medical Informatics Association, vol. 28, no. 3, pp. 463–471, 2021. [8] T. Martin, G. Karopoulos, J. L. Hern´andez-Ramos, G. Kambourakis, and I. Nai Fovino, “De- mystifying covid-19 digital contact tracing: A survey on frameworks and mobile apps,” Wireless Communications and Mobile Computing, vol. 2020, 2020. [9] U. S. G. A. Oﬃce, “Beneﬁts and challenges of smartphone applications to augment contact tracing,” https://www.gao.gov/products/gao-21-104622, Sept 2021. [10] T. TraceTogether, “How does tracetogether work,” 2020. [11] R. Gupta, M. Bedi, P. Goyal, S. Wadhera, and V. Verma, “Analysis of covid-19 tracking tool in india: case study of aarogya setu mobile application,” Digital Government: Research and Practice, vol. 1, no. 4, pp. 1–8, 2020. [12] S. Vaudenay, “Centralized or decentralized? the contact tracing dilemma,” Cryptology ePrint Archive, 2020. [13] T. Li, C. Faklaris, J. King, Y. Agarwal, L. Dabbish, J. I. Hong et al., “Decentralized is not risk- free: Understanding public perceptions of privacy-utility trade-oﬀs in covid-19 contact-tracing apps,” arXiv preprint arXiv:2005.11957, 2020. [14] Google, “Exposure notiﬁcations implementation guide,” https://developers.google.com/ android/exposure-notiﬁcations/implementation-guide, Feb. 2022. [15] Apple, “Enexposureconﬁguration,” exposurenotiﬁcation/enexposureconﬁguration, Feb. 2022. https://developer.apple.com/documentation/ [16] S. Altmann, L. Milsom, H. Zillessen, R. Blasone, F. Gerdon, R. Bach, F. Kreuter, D. Nosenzo, S. Toussaert, J. Abeler et al., “Acceptability of app-based contact tracing for covid-19: Cross- country survey study,” JMIR mHealth and uHealth, vol. 8, no. 8, p. e19857, 2020. [17] Google, “Publicly-available exposure notiﬁcations apps,” https://developers.google.com/ android/exposure-notiﬁcations/apps, Feb. 2022. [18] Sufatrio, D. J. J. Tan, T.-W. Chua, and V. L. L. Thing, “Securing android: A survey, taxonomy, and challenges,” ACM Comput. Surv., pp. 58:1–58:45, 2015. [19] L. Li, T. F. Bissyand´e, M. Papadakis, S. Rasthofer, A. Bartel, D. Octeau, J. Klein, and L. Traon, “Static analysis of android apps: A systematic literature review,” Information and Software Technology, vol. 88, pp. 67–95, 2017. [20] V.-P. Ranganath and J. Mitra, “Are free android app security analysis tools eﬀective in de- tecting known vulnerabilities?” Empirical Software Engineering, vol. 25, no. 1, pp. 178–219, 2020. 19 [21] A. Abraham, Magaofei, M. Dobrushin, and V. Nadal, “Mobsf github,” https://github.com/ MobSF/Mobile-Security-Framework-MobSF, Feb. 2022. [22] A. Desnos, G. Gueguen, and S. Bachmann, “Androguard,” https://androguard.readthedocs. io/en/latest/, Feb. 2022. [23] L. Lu, Z. Li, Z. Wu, W. Lee, and G. Jiang, “Chex: statically vetting android apps for compo- nent hijacking vulnerabilities,” in Proceedings of the 2012 ACM conference on Computer and communications security, 2012, pp. 229–240. [24] T. Watanabe, M. Akiyama, F. Kanei, E. Shioji, Y. Takata, B. Sun, Y. Ishi, T. Shibahara, T. Yagi, and T. Mori, “Understanding the origins of mobile app vulnerabilities: A large-scale measurement study of free and paid apps,” in 2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR). IEEE, 2017, pp. 14–24. [25] J. Mitra and V.-P. Ranganath, “Ghera: A repository of android app vulnerability benchmarks,” in Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering, 2017, pp. 43–52. [26] M. Ghafari, P. Gadient, and O. Nierstrasz, “Security smells in android,” in 2017 IEEE 17th IEEE, international working conference on source code analysis and manipulation (SCAM). 2017, pp. 121–130. [27] P. R. Center, “An analysis of android app permissions,” https://www.pewresearch.org/ internet/2015/11/10/an-analysis-of-android-app-permissions/, Nov 2015. [28] B. P. Sarma, N. Li, C. Gates, R. Potharaju, C. Nita-Rotaru, and I. Molloy, “Android permis- sions: a perspective combining risks and beneﬁts,” in Proceedings of the 17th ACM symposium on Access Control Models and Technologies, 2012, pp. 13–22. [29] Google, “Android permissions/overview, May 2022. app permissions,” https://developer.android.com/guide/topics/ [30] A. V. Prakash and S. Das, “Explaining citizens’ resistance to use digital contact tracing apps: A mixed-methods study,” International Journal of Information Management, vol. 63, p. 102468, 2022. [31] Google, “Manifest permissions in android,” https://developer.android.com/reference/android/ Manifest.permission#USE FINGERPRINT, Jul. 2022. [32] Owasp, “Owasp top 10,” https://owasp.org/www-project-mobile-top-10/, Jul. 2022. [33] J. Bell, D. Butler, C. Hicks, and J. Crowcroft, “Tracesecure: Towards privacy preserving contact tracing,” arXiv preprint arXiv:2004.04059, 2020. [34] J. Samhi, K. Allix, T. F. Bissyand´e, and J. Klein, “A ﬁrst look at android applications in google play related to covid-19,” Empirical Software Engineering, vol. 26, no. 4, pp. 1–49, 2021. [35] H. Cho, D. Ippolito, and Y. W. Yu, “Contact tracing mobile apps for covid-19: Privacy considerations and related trade-oﬀs,” arXiv preprint arXiv:2003.11511, 2020. 20 [36] M. Lanzing, “Contact tracing apps: an ethical roadmap,” Ethics and information technology, vol. 23, no. 1, pp. 87–90, 2021. [37] L. Baumg¨artner, A. Dmitrienko, B. Freisleben, A. Gruler, J. H¨ochst, J. K¨uhlberg, M. Mezini, R. Mitev, M. Miettinen, A. Muhamedagic et al., “Mind the gap: Security & privacy risks of contact tracing apps,” in 2020 IEEE 19th international conference on trust, security and privacy in computing and communications (TrustCom). IEEE, 2020, pp. 458–467. [38] Y. Gvili, “Security analysis of the covid-19 contact tracing speciﬁcations by apple inc. and google inc.” Cryptology ePrint Archive, 2020. [39] H. Wen, Q. Zhao, Z. Lin, D. Xuan, and N. Shroﬀ, “A study of the privacy of covid-19 con- tact tracing apps,” in International Conference on Security and Privacy in Communication Systems. Springer, 2020, pp. 297–317. [40] M. Hatamian, S. Wairimu, N. Momen, and L. Fritsch, “A privacy and security analysis of early- deployed covid-19 contact tracing android apps,” Empirical software engineering, vol. 26, no. 3, pp. 1–51, 2021. [41] E. Union, “General data protection regulation,” https://gdpr.eu/what-is-gdpr/, 2022. [42] V. Ang and L. K. Shar, “Covid-19 one year on–security and privacy review of contact tracing mobile apps,” IEEE Pervasive Computing, vol. 20, no. 4, pp. 61–70, 2021. [43] H. Systemas, “Virustotal,” https://www.virustotal.com/gui/home/upload, 2022. [44] V. Kouliaridis, G. Kambourakis, E. Chatzoglou, D. Geneiatakis, and H. Wang, “Dissecting contact tracing apps in the android platform,” Plos one, vol. 16, no. 5, p. e0251867, 2021. [45] Mitre, “Common weakness enumeration,” https://cwe.mitre.org/, 2022. [46] “Common vulnerabilities and exposures,” https://cve.mitre.org/, 2022. 21 App Collects Location Uses Insecure Uses Weak Uses HTTP Communicates With Non-US # Violations Privacy Policy Violation Y Alabama Y Arizona Y California Y Colorado Y Connecticut Y Delaware Y DC N Guam Hawaii Y Y Louisiana Maryland Y Y Michigan N Minnesota Nevada Y New Jersey N New Mexico Y New York N North Carolina Y Y North Dakota Penn Y Y Utah Virginia Y Y Washington Y Wisconsin # Apps per 20 violation Storage Y N N N N Y N N Y N N N Y N Y Y Y N Y Y N N N N 9 Encryption Y N N N N N N Y Y Y N N Y N N N N Y N N N N N N 6 N N Y Y N Y Y N N N Y Y Y Y Y Y N Y N N Y N Y Y 14 server domain Y N Y Y Y N Y N N N Y N Y Y N Y N Y Y N N N Y Y 13 per app 4 1 3 3 2 3 3 1 3 2 3 2 4 3 2 4 1 4 3 2 2 1 3 3 Table 4: Privacy Violations by each US-based GAEN app. Columns 2-6 indicate a feature or an action that an app claims it does not use or do in its privacy policy. The cells with Y/N denote Yes if an app performs the action in the corresponding column and No if it does not. Y implies a privacy violation and N implies otherwise. 22 No Certiﬁcate # Vulns. per app 3 3 2 2 3 2 3 4 3 3 3 1 3 4 2 4 2 2 2 2 3 2 3 3 App Unprotected Component Y Alabama N Arizona N California N Colorado Y Connecticut N Delaware N DC Y Guam Y Hawaii Y Louisiana Maryland Y N Michigan Y Minnesota Y Nevada N New Jersey New Mexico Y N New York North Carolina N North Dakota/Wyoming N N Penn N Utah Virginia Y Y Washington Y Wisconsin # Apps per 12 Vuln Known Vulnerabilities Allows Insecure Weak PRNG Y Y Y Y Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y N Y Y 22 hashing Data Backup N N N Y N N N N N N N Y N N N Y N N N N N N N N N N Y N N Y Y N N Y N N N Y N Y Y N N N N N N N 4 7 Insecure SSL Impl. Pinning N Y N N N N Y N N N N N N N N N N N N N N N N N 1 Y N Y Y Y N Y Y Y Y Y Y Y Y N Y N Y N N Y Y Y Y 18 Table 5: Known Vulnerabilities in each US-based GAEN app. Columns 2-5 indicate a known vulnerability. The cells with Y/N denote Yes if an app contains the vulnerability and No otherwise. 23 App SQL Injection Unprotected Component Cleartext Storage Log Sensitive False Positive Vulnerability Y Alabama Y Arizona Y California Y Colorado Y Connecticut Y Delaware Y DC Y Guam Hawaii Y Y Louisiana Y Maryland Y Michigan Y Minnesota Y Nevada New Jersey Y Y New Mexico Y New York North Carolina Y North Dakota/Wyoming Y Y Penn Utah Y N Virginia Y Washington Y Wisconsin # Apps per 23 false positive Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y 24 Y N Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y N Y Y 22 Data Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y 24 Table 6: False Positives reported by MobSF in every US-based GAEN app. The cells with Y indicate that the vulnerability in the corresponding column was falsely reported as a potential vulnerability by MobSF and N denotes MobSF did not report the vulnerability in the corresponding column. 5 5 5 5 5 5 5 4 5 5 per app Janus Signature # False Positives Vulnerability Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y 24 5 5 5 5 5 3 5 5 5 5 5 5 5 5 24","['l', 'x', 'security', 'privacy', 'analysis', 'usbased', 'contact', 'trace', 'app', 'computer', 'science', 'abstract', 'onset', 'covid19', 'government', 'worldwide', 'plan', 'develop', 'deploy', 'contact', 'trace', 'app', 'help', 'speed', 'contact', 'tracing', 'process', 'expert', 'raise', 'concern', 'longterm', 'privacy', 'security', 'implication', 'use', 'app', 'consequently', 'several', 'proposal', 'make', 'design', 'privacypreserving', 'contact', 'trace', 'app', 'end', 'google', 'apple', 'develop', 'googleapple', 'exposure', 'notiﬁcation', 'gaen', 'framework', 'help', 'public', 'health', 'authority', 'develop', 'privacypreserve', 'contact', 'trace', 'app', 'state', 'use', 'framework', 'develop', 'contact', 'trace', 'app', 'paper', 'empirically', 'evaluate', 'usbased', 'app', 'determine', 'privilege', 'app', 'app', 'comply', 'deﬁned', 'privacy', 'policy', 'contain', 'know', 'vulnerability', 'exploit', 'compromise', 'privacy', 'result', 'show', 'app', 'violate', 'privacy', 'policy', 'contain', 'several', 'know', 'vulnerability', 'introduction', 'contact', 'tracing', 'many', 'method', 'health', 'authority', 'use', 'contain', 'rapid', 'spread', 'covid19', 'however', 'manual', 'contact', 'trace', 'keep', 'track', 'grow', 'pandemic', 'challenge', 'health', 'authority', 'lack', 'human', 'resource', 'limitation', 'human', 'memory', 'remember', 'possible', 'contact', 'accurately', 'consequently', 'several', 'eﬀort', 'automate', 'contact', 'tracing', 'process', 'develop', 'mobile', 'app', 'use', 'track', 'tech', 'nology', 'mobile', 'phone', 'gps', 'bluetooth', 'addition', 'mobile', 'phone', 'ubiquitous', 'use', 'eﬀectively', 'quickly', 'identify', 'person', 'contact', 'test', 'positive', 'covid', 'background', 'contact', 'trace', 'app', 'work', 'contact', 'trace', 'app', 'work', 'estimate', 'mobile', 'phone', 'x', 'close', 'base', 'metric', 'deﬁne', 'local', 'health', 'authority', 'eg', 'foot', 'potential', 'infection', 'test', 'positive', 'conﬁrm', 'result', 'app', 'detect', 'proximity', 'use', 'bluetooth', 'low', 'energy', 'ble', 'global', 'positioning', 'system', 'gp', 'technology', 'combination', 'phone', 'nearby', 'app', 'exchange', 'encounter', 'message', 'contain', 'thing', 'random', 'identiﬁer', 'signal', 'strength', 'app', 'also', 'include', 'location', 'datum', 'encounter', 'message', 'app', 'keep', 'record', 'encounter', 'message', 'exchange', 'app', 'message', 'phone', 'user', 'test', 'positive', 'app', 'analyze', 'encounter', 'message', 'calculate', 'level', 'risk', 'base', 'factor', 'local', 'health', 'authority', 'decide', 'calculation', 'perform', 'locally', 'device', 'central', 'server', 'contact', 'trace', 'app', 'useful', 'help', 'expedite', 'process', 'contact', 'trace', 'expert', 'warn', 'technology', 'longterm', 'consequence', 'user', 'privacy', 'security', 'easily', 'exploit', 'use', 'malicious', 'contexts', 'mass', 'surveillance', 'therefore', 'contact', 'trace', 'app', 'design', 'implementation', 'carefully', 'scrutinize', 'deploy', 'adopt', 'contact', 'trace', 'app', 'architecture', 'need', 'ensure', 'privacy', 'contact', 'trace', 'app', 'lead', 'several', 'proposal', 'app', 'architecture', 'well', 'protect', 'user', 'privacy', 'centralize', 'decentralized', 'hybrid', 'centralized', 'architecture', 'central', 'server', 'collect', 'store', 'personally', 'identiﬁable', 'information', 'pii', 'far', 'use', 'generate', 'random', 'identiﬁer', 'encounter', 'message', 'far', 'central', 'server', 'responsible', 'determine', 'user', 'potentially', 'expose', 'government', 'health', 'authority', 'access', 'datum', 'store', 'central', 'server', 'use', 'perform', 'advanced', 'aggregate', 'analysis', 'useful', 'understand', 'trend', 'help', 'inform', 'mitigation', 'eﬀort', 'however', 'approach', 'negatively', 'aﬀect', 'user', 'privacy', 'central', 'server', 'assume', 'trust', 'access', 'vast', 'amount', 'personal', 'information', 'unauthorized', 'hostile', 'entity', 'potentially', 'misuse', 'hand', 'decentralize', 'server', 'minimal', 'datum', 'datum', 'maintain', 'locally', 'user', 'device', 'app', 'periodically', 'communicate', 'server', 'download', 'set', 'random', 'identiﬁer', 'test', 'positive', 'app', 'perform', 'risk', 'analysis', 'encounter', 'message', 'locally', 'determine', 'come', 'close', 'device', 'test', 'positive', 'architecture', 'help', 'preserve', 'privacy', 'well', 'centralized', 'architecture', 'however', 'limit', 'access', 'crucial', 'datum', 'use', 'analyze', 'spread', 'pandemic', 'understand', 'eﬀectiveness', 'app', 'hybrid', 'architecture', 'combine', 'feature', 'centralized', 'decentralized', 'architecture', 'random', 'identiﬁer', 'generation', 'leave', 'local', 'device', 'server', 'manage', 'risk', 'analysis', 'exposure', 'notiﬁcation', 'country', 'develop', 'deploy', 'app', 'base', 'centralized', 'architecture', 'decentralized', 'architecture', 'eg', 'app', 'base', 'googleapple', 'framework', 'far', 'many', 'app', 'also', 'use', 'location', 'datum', 'enable', 'automate', 'contact', 'trace', 'however', 'consensus', 'researcher', 'approach', 'feasible', 'eﬀective', 'contact', 'trace', 'minimize', 'privacy', 'risk', 'apple', 'exposure', 'notiﬁcation', 'system', 'apple', 'collaborate', 'develop', 'apple', 'exposure', 'notiﬁcation', 'framework', 'base', 'decentralized', 'architecture', 'help', 'public', 'health', 'authority', 'develop', 'contact', 'trace', 'app', 'gaen', 'work', 'generate', 'temporary', 'key', 'change', 'periodically', 'key', 'use', 'encrypt', 'locally', 'store', 'datum', 'generate', 'random', 'embed', 'identiﬁer', 'encounter', 'message', 'exchange', 'app', 'use', 'ble', 'user', 'test', 'positive', 'public', 'health', 'oﬃcial', 'use', 'veriﬁcation', 'server', 'send', 'user', 'conﬁrmation', 'code', 'user', 'use', 'conﬁrmation', 'code', 'upload', 'recent', 'key', 'eg', 'last', 'day', 'key', 'server', 'app', 'download', 'key', 'periodically', 'key', 'server', 'compare', 'set', 'key', 'exchange', 'last', 'day', 'match', 'app', 'determine', 'risk', 'potential', 'exposure', 'base', 'formula', 'predetermine', 'public', 'health', 'authority', 'motivation', 'absence', 'oﬃcial', 'national', 'contact', 'trace', 'app', 'individual', 'state', 'use', 'framework', 'develop', 'contact', 'trace', 'app', 'date', 'state', 'develop', 'gaen', 'app', 'security', 'privacy', 'guarantee', 'build', 'people', 'lack', 'conﬁdence', 'app', 'ability', 'intention', 'protect', 'privacy', 'center', 'prevention', 'cdc', 'recommend', 'healthcare', 'authority', 'conduct', 'thirdparty', 'assessment', 'contact', 'trace', 'app', 'make', 'result', 'publicly', 'available', 'however', 'accord', 'technology', 'assessment', 'conduct', 'government', 'accountability', 'oﬃce', 'gao', 'behest', 'state', 'contact', 'trace', 'app', 'conduct', 'thirdparty', 'assessment', 'moreover', 'state', 'conduct', 'evaluation', 'make', 'result', 'publicly', 'available', 'motivate', 'cdc', 'recommendation', 'lack', 'assessment', 'usbased', 'app', 'paper', 'analyze', 'privacy', 'security', 'gaenbase', 'android', 'app', 'contact', 'trace', 'speciﬁcally', 'ask', 'follow', 'research', 'question', 'rq1', 'degree', 'privilege', 'gaenbase', 'android', 'app', 'contact', 'tracing', 'android', 'app', 'default', 'least', 'privilege', 'need', 'request', 'system', 'user', 'permission', 'perform', 'privileged', 'operation', 'use', 'bluetooth', 'purpose', 'question', 'understand', 'permission', 'use', 'app', 'privacy', 'implication', 'gaenbase', 'android', 'app', 'contact', 'tracing', 'violate', 'privacy', 'policy', 'contact', 'trace', 'app', 'require', 'publish', 'privacy', 'policy', 'inform', 'user', 'capability', 'datum', 'share', 'storage', 'retention', 'policy', 'purpose', 'question', 'determine', 'privacy', 'policy', 'consistent', 'behavior', 'encode', 'source', 'code', 'rq3', 'gaenbase', 'android', 'app', 'contact', 'trace', 'contain', 'know', 'android', 'vulner', 'ability', 'android', 'app', 'vulnerability', 'exploit', 'malicious', 'app', 'available', 'locally', 'remotely', 'compromise', 'user', 'privacy', 'purpose', 'question', 'identify', 'gaenbase', 'contact', 'trace', 'app', 'similar', 'vulnerability', 'methodology', 'section', 'describe', 'app', 'select', 'tool', 'choose', 'factor', 'consider', 'answer', 'research', 'question', 'app', 'selection', 'select', 'oﬃcial', 'contact', 'trace', 'app', 'android', 'state', 'develop', 'use', 'exposure', 'notiﬁcation', 'apis', 'apple', 'gaen', 'consider', 'app', 'focus', 'app', 'state', 'guidesafe', 'covid', 'watch', 'notify', 'co', 'exposure', 'notiﬁcation', 'covid', 'alert', 'ct', 'covid', 'alert', 'alohasafe', 'alert', 'covid', 'alert', 'nm', 'notify', 'covid', 'alert', 'care19', 'alert', 'version', 'name', 'package', 'name', 'govadphexposurenotiﬁcation', 'govazdhscovidwatchandroid', 'govcacovid19exposurenotiﬁcation', 'govcocdpheexposurenotiﬁcation', 'govctcovid19exposurenotiﬁcation', 'govdecovidtracker', 'govdccovid19exposurenotiﬁcation', 'orgpathcheckguambt', 'orgalohasafealert', 'orgpathchecklabt', 'govmdcovid19exposurenotiﬁcation', 'govmichiganmicovidexposure', 'orgpathcheckcovidsafepathsbtmn', 'govnvdhhsen', 'comnjgovcovidalert', 'govnmcovid19exposurenotiﬁcation', 'govnyhealthproximity', 'govncdhhsexposurenotiﬁcation', 'comproudcrowdexposure', 'version', 'code', 'covid', 'alert', 'exposure', 'notiﬁcation', 'covidwise', 'wa', 'notify', 'exposure', 'notiﬁcation', 'govpacovidtracker', 'govutcovid19exposurenotiﬁcation', 'givvdhexposurenotiﬁcation', 'govwadohexposurenotiﬁcation', 'table', 'usbased', 'contact', 'trace', 'app', 'app', 'size', 'download', 'date', 'nov', 'nov', 'oct', 'nov', 'nov', 'nov', 'feb', 'nov', 'oct', 'nov', 'nov', 'nov', 'study', 'examine', 'gaen', 'app', 'base', 'find', 'app', 'android', 'developer', 'oﬃcial', 'page', 'usbased', 'app', 'link', 'play', 'use', 'link', 'download', 'correspond', 'play', 'emulator', 'run', 'android', 'version', 'support', 'app', 'transfer', 'ﬁle', 'emulator', 'computer', 'reverse', 'engineer', 'statically', 'analyze', 'app', 'state', 'develop', 'gaen', 'wyoming', 'use', 'single', 'app', 'build', 'device', 'instal', 'device', 'setting', 'play', 'obtain', 'app', 'use', 'emulator', 'install', 'app', 'consequently', 'consider', 'assessment', 'total', 'end', 'app', 'table', 'list', 'select', 'app', 'name', 'package', 'version', 'information', 'size', 'ﬁle', 'download', 'tool', 'selection', 'research', 'mobile', 'app', 'security', 'privacy', 'lead', 'development', 'several', 'tool', 'tech', 'nique', 'detect', 'vulnerability', 'malicious', 'behavior', 'tool', 'base', 'static', 'dynamic', 'analysis', 'use', 'static', 'analysis', 'vulnerability', 'malicious', 'behavior', 'guide', 'subsequent', 'dynamic', 'analysis', 'tool', 'use', 'dynamic', 'analysis', 'prior', 'research', 'eﬀort', 'study', 'eﬃcacy', 'tool', 'observe', 'freely', 'available', 'tool', 'static', 'analysis', 'tool', 'detect', 'know', 'android', 'app', 'vulnerability', 'dynamic', 'analysis', 'tool', 'furthermore', 'static', 'analysis', 'tool', 'mobsf', 'detect', 'know', 'vulnerability', 'base', 'observation', 'use', 'mobsf', 'primary', 'tool', 'analysis', 'static', 'dynamic', 'analyzer', 'static', 'analyzer', 'statically', 'analyze', 'app', 'source', 'code', 'determine', 'app', 'use', 'apis', 'feature', 'know', 'cause', 'android', 'app', 'figure', 'snippet', 'report', 'generate', 'issue', 'severity', 'security', 'standard', 'associate', 'source', 'ﬁle', 'detect', 'vulnerability', 'see', 'figure', 'fail', 'run', 'dynamic', 'analyzer', 'contact', 'trace', 'app', 'select', 'hence', 'consider', 'static', 'analyzer', 'use', 'generate', 'control', 'ﬂow', 'graph', 'app', 'see', 'figure', 'use', 'analyze', 'datum', 'ﬂow', 'app', 'track', 'datum', 'ﬂow', 'control', 'ﬂow', 'graph', 'determine', 'datum', 'ﬂowe', 'app', 'sensitive', 'datum', 'use', 'app', 'potentially', 'malicious', 'necessary', 'verify', 'potential', 'data', 'leak', 'datum', 'injection', 'vulnerability', 'report', 'static', 'analyzer', 'know', 'report', 'false', 'positive', 'speciﬁcally', 'use', 'follow', 'strategy', 'conﬁrme', 'potential', 'data', 'leak', 'vulnerability', 'path', 'control', 'ﬂow', 'graph', 'preidentiﬁed', 'sensitive', 'datum', 'source', 'node', 'target', 'node', 'share', 'storage', 'network', 'communication', 'apis', 'sensitive', 'datum', 'source', 'include', 'use', 'collect', 'user', 'input', 'biometric', 'read', 'private', 'ﬁle', 'communication', 'channel', 'eg', 'bluetooth', 'string', 'hard', 'code', 'personal', 'information', 'eg', 'ip', 'address', 'conﬁrme', 'data', 'injection', 'vulnerability', 'path', 'control', 'ﬂow', 'graph', 'preidentiﬁed', 'source', 'node', 'potentially', 'malicious', 'datum', 'share', 'storage', 'network', 'interapp', 'communication', 'apis', 'target', 'node', 'target', 'node', 'use', 'datum', 'sanitize', 'example', 'target', 'node', 'use', 'potentially', 'malicious', 'datum', 'tization', 'include', 'export', 'broadcast', 'receiver', 'use', 'input', 'datum', 'send', 'intentﬁlter', 'systemdeﬁned', 'action', 'function', 'target', 'node', 'use', 'input', 'datum', 'trust', 'remote', 'server', 'figure', 'snippet', 'list', 'edge', 'control', 'ﬂow', 'generate', 'androguard', 'tool', 'source', 'target', 'column', 'indicate', 'node', 'graph', 'row', 'direct', 'edge', 'connect', 'source', 'node', 'target', 'node', 'node', 'api', 'call', 'contain', 'source', 'code', 'policy', 'analysis', 'focus', 'study', 'determine', 'source', 'code', 'consistent', 'privacy', 'policy', 'end', 'download', 'app', 'privacy', 'policy', 'examine', 'identiﬁe', 'feature', 'privacy', 'policy', 'app', 'claim', 'app', 'use', 'eg', 'collect', 'location', 'select', 'app', 'privacy', 'policy', 'claim', 'follow', 'collect', 'store', 'transmit', 'personally', 'identiﬁable', 'datum', 'store', 'exposure', 'datum', 'eg', 'random', 'id', 'exposure', 'date', 'locally', 'user', 'device', 'prevent', 'unauthorized', 'access', 'locally', 'store', 'datum', 'encrypt', 'locally', 'store', 'datum', 'communicate', 'trust', 'server', 'encrypted', 'network', 'look', 'feature', 'app', 'use', 'androguard', 'least', 'feature', 'find', 'app', 'deem', 'app', 'violate', 'privacy', 'policy', 'example', 'use', 'mobsf', 'determine', 'instance', 'app', 'source', 'code', 'datum', 'store', 'external', 'storage', 'use', 'control', 'ﬂow', 'generate', 'androguard', 'determine', 'source', 'datum', 'store', 'external', 'storage', 'source', 'sensitive', 'sensitive', 'datum', 'store', 'external', 'storage', 'deem', 'violation', 'privacy', 'policy', 'bullet', 'list', 'know', 'vulnerability', 'analysis', 'android', 'app', 'vulnerability', 'malicious', 'app', 'exploit', 'cause', 'harm', 'user', 'therefore', 'necessary', 'ensure', 'vulnerability', 'occur', 'app', 'especially', 'contact', 'trace', 'app', 'deal', 'sensitive', 'personal', 'information', 'perform', 'privileged', 'operation', 'phone', 'hence', 'analyze', 'app', 'know', 'android', 'app', 'vulnerability', 'use', 'mobsf', 'ghera', 'analysis', 'mobsf', 'provide', 'list', 'potential', 'vulnerability', 'static', 'analysis', 'report', 'investigate', 'determine', 'veracity', 'know', 'report', 'false', 'positive', 'moreover', 'detect', 'know', 'vulnerability', 'therefore', 'use', 'repository', 'know', 'vulnerability', 'benchmark', 'far', 'guide', 'analysis', 'benchmark', 'welldocumente', 'contain', 'feature', 'relate', 'vulnerability', 'capture', 'benchmark', 'statically', 'analyze', 'app', 'set', 'use', 'mobsf', 'determine', 'featuresapis', 'use', 'consider', 'feature', 'also', 'use', 'ghera', 'benchmark', 'investigate', 'feature', 'determine', 'result', 'potentially', 'exploitable', 'vulnerability', 'exploit', 'malicious', 'app', 'device', 'remotely', 'example', 'app', 'give', 'unrestricted', 'access', 'app', 'use', 'pende', 'intent', 'feature', 'android', 'find', 'app', 'component', 'use', 'pending', 'intent', 'investigate', 'component', 'conﬁrm', 'perform', 'privileged', 'operation', 'true', 'result', 'potential', 'privilege', 'escalation', 'attack', 'result', 'section', 'report', 'ﬁnding', 'study', 'term', 'permission', 'request', 'use', 'app', 'potential', 'privacy', 'violation', 'potential', 'vulnerability', 'exploit', 'cause', 'harm', 'user', 'rq1', 'degree', 'privilege', 'gaenbase', 'android', 'app', 'contact', 'tracing', 'total', 'permission', 'use', 'contact', 'trace', 'app', 'app', 'use', 'approximately', 'permission', 'average', 'least', 'number', 'permission', 'use', 'app', 'high', 'statistic', 'high', 'average', 'permission', 'use', 'app', 'general', 'ﬁve', 'nearly', 'app', 'use', 'permission', 'furthermore', 'ﬁve', 'permission', 'use', 'app', 'suggest', 'ﬁve', 'necessary', 'contact', 'trace', 'permission', 'likely', 'extraneous', 'therefore', 'app', 'android', 'overprivilege', 'concern', 'app', 'access', 'vast', 'amount', 'personal', 'datum', 'overprivilege', 'app', 'increase', 'risk', 'exploit', 'expose', 'large', 'attack', 'surface', 'privilege', 'need', 'therefore', 'developer', 'app', 'carefully', 'consider', 'require', 'permission', 'ensure', 'necessary', 'one', 'use', 'app', 'permission', 'normal', 'permission', 'grant', 'android', 'app', 'instal', 'therefore', 'app', 'need', 'ask', 'user', 'permission', 'runtime', 'always', 'context', 'contact', 'trace', 'many', 'user', 'install', 'app', 'help', 'prevent', 'spread', 'pandemic', 'permission', 'model', 'compromise', 'privacy', 'give', 'user', 'control', 'grant', 'deny', 'permission', 'app', 'furthermore', 'assume', 'app', 'benign', 'user', 'trust', 'installation', 'assumption', 'hinder', 'app', '’s', 'permission', 'p12', 'permission', 'app', 'app', 'permission', 'table', 'permission', 'use', 'usbased', 'gaen', 'app', 'declare', 'manifest', 'ﬁle', 'permission', 'name', 'encode', 'pn', 'lack', 'space', 'exact', 'permission', 'name', 'list', 'table', 'permission', 'code', 'permission', 'name', 'androidpermissionreceive', 'boot', 'complete', 'androidpermissionbluetooth', 'androidpermissionaccess', 'network', 'state', 'androidpermissionaccess', 'lock', 'androidpermissionforeground', 'service', 'comgoogleandroidc2dmpermissionreceive', 'comgoogleandroidﬁnskypermissionbind', 'get', 'install', 'referrer', 'service', 'androidpermissionuse', 'biometric', 'androidpermissionuse', 'fingerprint', 'table', 'name', 'permission', 'use', 'usbased', 'gaen', 'app', 'adoption', 'environment', 'role', 'governmentdeploye', 'contact', 'trace', 'app', 'see', 'suspicion', 'however', 'app', 'developer', 'resolve', 'issue', 'android', 'app', 'deﬁne', 'permission', 'therefore', 'platform', 'developer', 'consider', 'change', 'way', 'permission', 'grant', 'deﬁne', 'custom', 'permission', 'use', 'context', 'contact', 'trace', 'googlebase', 'thirdparty', 'analytic', 'library', 'core', 'android', 'system', 'deﬁne', 'permis', 'sion', 'consider', 'app', 'use', 'permission', 'therefore', 'raise', 'question', 'permission', 'necessary', 'contact', 'trace', 'far', 'app', 'design', 'necessary', 'permission', 'access', 'privileged', 'operation', 'device', 'bluetooth', 'context', 'contact', 'trace', 'app', 'far', 'increase', 'security', 'privacy', 'risk', 'use', 'thirdparty', 'library', 'directly', 'relate', 'task', 'contact', 'trace', 'use', 'fingerprint', 'permission', 'deprecate', 'app', 'use', 'use', 'deprecate', 'permission', 'recommend', 'major', 'concern', 'app', 'use', 'permission', 'conjunction', 'use', 'biometric', 'permission', 'recommend', 'permission', 'use', 'instead', 'use', 'fingerprint', 'nevertheless', 'app', 'use', 'deprecate', 'permission', 'unknown', 'unexpected', 'security', 'privacy', 'implication', 'app', 'use', 'biometric', 'permission', 'access', 'device', 'capability', 'use', 'collect', 'biometric', 'information', 'violation', 'privacy', 'increase', 'risk', 'expose', 'private', 'sensitive', 'user', 'info', 'unauthorized', 'entity', 'furthermore', 'majority', 'gaen', 'app', 'use', 'biometric', 'permission', 'raise', 'question', 'necessity', 'use', 'capability', 'collect', 'transmit', 'biometricrelated', 'datum', 'access', 'state', 'permission', 'use', 'select', 'app', 'suggest', 'app', 'need', 'permission', 'contact', 'trace', 'app', 'use', 'permission', 'connect', 'remote', 'server', 'use', 'always', 'secure', 'network', 'protect', 'susceptible', 'maninthemiddle', 'attack', 'therefore', 'app', 'collect', 'transmit', 'sensitive', 'information', 'internet', 'avoid', 'communication', 'vibrate', 'permission', 'use', 'app', 'control', 'device', 'vibration', 'permission', 'necessarily', 'benign', 'use', 'incorrectly', 'maliciously', 'damage', 'user', 'phone', 'therefore', 'good', 'avoid', 'use', 'permission', 'absolutely', 'necessary', 'majority', 'select', 'app', 'use', 'vibrate', 'permission', 'permission', 'likely', 'necessary', 'gaen', 'app', 'app', 'declare', 'query', 'element', 'manifest', 'ﬁle', 'indicate', 'list', 'app', 'communicate', 'speciﬁcally', 'declare', 'query', 'element', 'androidintentactiondial', 'androidintentactionsend', 'manifest', 'ﬁle', 'imply', 'capability', 'place', 'phone', 'call', 'share', 'datum', 'app', 'send', 'intent', 'context', 'contact', 'trace', 'capability', 'unnecessary', 'pose', 'additional', 'risk', 'security', 'privacy', 'user', 'rq2', 'gaenbase', 'android', 'app', 'contact', 'tracing', 'violate', 'privacy', 'policy', 'usbased', 'gaen', 'app', 'violate', 'privacy', 'policy', 'behavior', 'inconsistent', 'least', 'claim', 'policy', 'concern', 'contact', 'trace', 'app', 'collect', 'store', 'vast', 'amount', 'private', 'datum', 'potentially', 'misuse', 'therefore', 'take', 'additional', 'care', 'guarantee', 'user', 'privacy', 'least', 'consistent', 'policy', 'gaen', 'app', 'design', 'collect', 'store', 'track', 'location', 'however', 'gaen', 'app', 'collect', 'user', 'location', 'claim', 'otherwise', 'privacy', 'policy', 'analysis', 'discover', 'app', 'explicitly', 'collect', 'location', 'however', 'use', 'library', 'call', 'twilightmanager', 'collect', 'user', 'location', 'determine', 'local', 'time', 'app', 'conﬁgure', 'dark', 'theme', 'automatically', 'import', 'use', 'library', 'therefore', 'app', 'violate', 'privacy', 'policy', 'due', 'use', 'library', 'collect', 'location', 'consequently', 'raise', 'question', 'app', 'developer', 'aware', 'privacy', 'implication', 'library', 'use', 'app', 'vet', 'library', 'use', 'especially', 'crucial', 'contact', 'trace', 'app', 'malicious', 'actor', 'potentially', 'misuse', 'compromise', 'user', 'privacy', 'app', 'collect', 'biometric', 'authentication', 'encrypt', 'locally', 'store', 'datum', 'however', 'privacy', 'policy', 'app', 'explicitly', 'state', 'lect', 'biometric', 'information', 'moreover', 'mention', 'app', 'collect', 'personally', 'identiﬁable', 'information', 'hence', 'deem', 'app', 'violate', 'privacy', 'app', 'claim', 'privacy', 'policy', 'store', 'exposurerelated', 'datum', 'local', 'storage', 'way', 'prevent', 'unauthorized', 'access', 'however', 'app', 'store', 'datum', 'external', 'storage', 'app', 'instal', 'device', 'include', 'malicious', 'app', 'access', 'datum', 'necessary', 'permission', 'access', 'external', 'storage', 'therefore', 'app', 'potentially', 'access', 'exposurerelated', 'datum', 'store', 'app', 'consequently', 'lead', 'violation', 'privacy', 'deﬁne', 'app', 'privacy', 'policy', 'issue', 'address', 'use', 'internal', 'storage', 'instead', 'external', 'storage', 'android', 'internal', 'storage', 'app', 'access', 'app', 'moreover', 'android', 'recommend', 'datum', 'store', 'internal', 'storage', 'need', 'share', 'app', 'consider', 'third', 'select', 'app', 'use', 'external', 'storage', 'instead', 'internal', 'storage', 'suggest', 'several', 'developer', 'app', 'aware', 'diﬀerence', 'lack', 'knowledge', 'concern', 'app', 'collect', 'store', 'vast', 'amount', 'personal', 'information', 'access', 'datum', 'minimize', 'potential', 'target', 'cyberattack', 'malicious', 'actor', 'privacy', 'policy', 'select', 'app', 'mention', 'datum', 'store', 'locally', 'device', 'protect', 'encryption', 'however', 'app', 'use', 'aes', 'block', 'cipher', 'mode', 'weak', 'cipher', 'weak', 'encryption', 'violation', 'app', 'privacy', 'policy', 'encrypt', 'local', 'datum', 'lead', 'potential', 'leak', 'sensitive', 'datum', 'result', 'imply', 'signiﬁcant', 'number', 'developer', 'aware', 'mode', 'avoid', 'several', 'security', 'guideline', 'owasp', 'recommend', 'use', 'likely', 'reason', 'developer', 'end', 'use', 'ecb', 'mode', 'default', 'mode', 'encryption', 'result', 'developer', 'explicitly', 'change', 'mode', 'however', 'oversight', 'surprising', 'focus', 'gaen', 'app', 'ensure', 'user', 'privacy', 'store', 'personally', 'identiﬁable', 'information', 'locally', 'protect', 'strong', 'encryption', 'failure', 'use', 'strong', 'encryption', 'claim', 'privacy', 'policy', 'raise', 'question', 'diligence', 'protect', 'user', 'datum', 'misuse', 'app', 'use', 'http', 'communicate', 'remote', 'server', 'consequently', 'communication', 'potentially', 'hijack', 'maninthemiddle', 'attack', 'furthermore', 'inconsistent', 'app', 'state', 'privacy', 'policy', 'mention', 'communication', 'remote', 'server', 'encrypt', 'endtoend', 'consider', 'http', 'use', 'half', 'select', 'app', 'imply', 'developer', 'app', 'either', 'unaware', 'implication', 'use', 'http', 'due', 'diligence', 'verify', 'communication', 'remote', 'web', 'server', 'use', 'use', 'http', 'concern', 'especially', 'use', 'essential', 'requirement', 'app', 'transmit', 'sensitive', 'personal', 'information', 'remote', 'web', 'server', 'moreover', 'developer', 'app', 'promote', 'largescale', 'use', 'access', 'vast', 'amount', 'personal', 'information', 'perceptive', 'issue', 'take', 'extra', 'care', 'avoid', 'exposure', 'notiﬁcation', 'app', 'transmit', 'exposurerelated', 'datum', 'eg', 'random', 'id', 'exposure', 'date', 'remote', 'server', 'user', 'consent', 'share', 'datum', 'event', 'exposure', 'covid19', 'app', 'privacy', 'policy', 'mention', 'location', 'server', 'therefore', 'user', 'reasonably', 'assume', 'server', 'however', 'app', 'communicate', 'exposure', 'notiﬁcation', 'server', 'furthermore', 'privacy', 'policy', 'app', 'explicitly', 'state', 'app', 'communicate', 'server', 'therefore', 'raise', 'question', 'app', 'send', 'exposurerelated', 'datum', 'usbased', 'resident', 'app', 'transparent', 'explicitly', 'state', 'policy', 'location', 'exposure', 'notiﬁcation', 'server', 'user', 'make', 'informed', 'decision', 'rq3', 'gaenbase', 'android', 'app', 'contact', 'trace', 'contain', 'know', 'android', 'app', 'vulnerability', 'discover', 'total', 'know', 'vulnerability', 'app', 'app', 'approximately', 'vulnerability', 'average', 'app', 'least', 'vulnerability', 'table', 'show', 'breakdown', 'vulnerability', 'find', 'app', 'vulnerability', 'brieﬂy', 'describe', 'follow', '•', 'unprotected', 'component', 'android', 'app', 'consist', 'component', 'app', 'export', 'com', 'ponent', 'share', 'operation', 'datum', 'app', 'however', 'component', 'export', 'restriction', 'app', 'include', 'malicious', 'app', 'access', 'consequently', 'lead', 'denialofservice', 'datum', 'leak', 'datum', 'injection', 'attack', 'insecure', 'prng', 'app', 'use', 'random', 'package', 'generate', 'pseudorandom', 'number', 'easily', 'predict', 'app', 'use', 'securerandom', 'package', 'contact', 'trace', 'app', 'base', 'rely', 'random', 'identiﬁer', 'use', 'random', 'number', 'generator', 'make', 'hard', 'predict', 'random', 'number', 'therefore', 'app', 'use', 'securerandom', 'package', 'instead', 'random', 'package', 'generate', 'hardto', 'predict', 'random', 'identiﬁer', 'weak', 'hash', 'hash', 'algorithm', 'consider', 'weak', 'attacker', 'use', 'hash', 'collision', 'forge', 'duplicate', 'hash', 'therefore', 'app', 'avoid', 'use', 'prevent', 'forgery', 'attack', 'data', 'backup', 'android', 'allow', 'user', 'create', 'backup', 'datum', 'app', 'root', 'privilege', 'consequently', 'malicious', 'user', 'access', 'device', 'able', 'create', 'backup', 'app', 'datum', 'use', 'app', 'conﬁgure', 'protect', 'potential', 'attack', 'set', 'allowbackup', 'attribute', 'false', 'android', 'recommend', 'app', 'disable', 'feature', 'prevent', 'malicious', 'user', 'access', 'local', 'datum', 'insecure', 'tlsssl', 'implementation', 'app', 'use', 'tlsssl', 'protocol', 'communicate', 'remote', 'server', 'verify', 'trustworthiness', 'server', 'established', 'way', 'verify', 'trust', 'app', 'maintain', 'list', 'trust', 'certiﬁcate', 'authority', 'cas', 'server', 'conﬁgure', 'certiﬁcate', 'contain', 'public', 'key', 'match', 'private', 'key', 'certiﬁcate', 'sign', 'certiﬁcate', 'authority', 'trust', 'app', 'generally', 'list', 'trust', 'cas', 'preinstalle', 'device', 'app', 'instal', 'however', 'connection', 'fail', 'certiﬁcate', 'use', 'conﬁgure', 'server', 'sign', 'ca', 'list', 'trust', 'cas', 'selfsigne', 'sign', 'intermediate', 'certiﬁcate', 'miss', 'server', 'conﬁguration', 'third', 'reason', 'address', 'serverside', 'ﬁrst', 'reason', 'address', 'implement', 'custom', 'trustmanager', 'android', 'api', 'app', 'implementation', 'mistake', 'custom', 'trustmanager', 'lead', 'vulnerability', 'trust', 'cas', 'lead', 'maninthemiddle', 'attack', 'unpinned', 'certiﬁcate', 'app', 'instal', 'device', 'trust', 'cas', 'preconﬁgure', 'device', 'app', 'developer', 'far', 'restrict', 'cas', 'app', 'trust', 'pin', 'set', 'trusted', 'cas', 'app', 'app', 'trust', 'pin', 'cas', 'ca', 'include', 'one', 'trust', 'device', 'mandatory', 'certiﬁcate', 'pin', 'good', 'security', 'practice', 'however', 'use', 'care', 'hamper', 'usability', 'communication', 'failure', 'outdated', 'certiﬁcate', 'result', 'change', 'server', 'conﬁguration', 'app', 'least', 'unprotected', 'component', 'vulnerability', 'find', 'sev', 'eral', 'manifestation', 'vulnerability', 'example', 'app', 'use', 'thirdparty', 'library', 'unprotected', 'broadcast', 'receiver', 'write', 'share', 'preference', 'malicious', 'app', 'potentially', 'exploit', 'vulnerability', 'execute', 'data', 'injection', 'attack', 'instance', 'app', 'deﬁne', 'unprotected', 'activity', 'access', 'bluetooth', 'location', 'result', 'malicious', 'app', 'potentially', 'exploit', 'activity', 'get', 'access', 'privileged', 'feature', 'necessary', 'permission', 'furthermore', 'app', 'deﬁne', 'activity', 'share', 'exposurerelated', 'diagnostic', 'information', 'remote', 'server', 'activity', 'export', 'restriction', 'malicious', 'app', 'potentially', 'exploit', 'vulnerability', 'communicate', 'remote', 'server', 'app', 'protect', 'component', 'make', 'private', 'app', 'app', 'need', 'share', 'component', 'underlie', 'system', 'protect', 'component', 'system', 'permission', 'app', 'choose', 'use', 'insecure', 'prng', 'generate', 'random', 'identiﬁer', 'use', 'identify', 'device', 'app', 'instal', 'anonymously', 'choose', 'insecure', 'prng', 'secure', 'prng', 'allow', 'malicious', 'actor', 'potentially', 'predict', 'random', 'identiﬁer', 'easily', 'use', 'create', 'duplicate', 'identiﬁer', 'hence', 'create', 'erroneous', 'fake', 'exposurerelated', 'datum', 'entry', 'therefore', 'select', 'app', 'use', 'securerandom', 'package', 'generate', 'random', 'identiﬁer', 'app', 'use', 'md5', 'sha1', 'weak', 'hash', 'algorithm', 'choose', 'weak', 'hash', 'contact', 'trace', 'app', 'problematic', 'app', 'expect', 'provide', 'strong', 'privacy', 'security', 'guarantee', 'strong', 'hash', 'basic', 'requirement', 'app', 'ensure', 'integrity', 'information', 'store', 'sixth', 'app', 'app', 'allow', 'user', 'back', 'datum', 'root', 'device', 'insecure', 'gaen', 'app', 'store', 'user', 'contact', 'exposure', 'relate', 'datum', 'locally', 'app', 'malicious', 'user', 'access', 'device', 'root', 'access', 'get', 'access', 'datum', 'misuse', 'app', 'feature', 'disable', 'set', 'allowbackup', 'attribute', 'manifest', 'ﬁle', 'false', 'app', 'choose', 'use', 'certiﬁcate', 'pin', 'instead', 'rely', 'device', 'list', 'trust', 'cas', 'certiﬁcate', 'pinning', 'provide', 'additional', 'protection', 'mitm', 'attack', 'app', 'choose', 'use', 'possibly', 'potential', 'connection', 'failure', 'pin', 'certiﬁcate', 'outdate', 'situation', 'way', 'restore', 'app', 'push', 'software', 'update', 'user', 'install', 'temporary', 'connection', 'failure', 'ideal', 'render', 'app', 'useless', 'however', 'context', 'framework', 'communication', 'remote', 'server', 'minimal1', 'security', 'paramount', 'advisable', 'app', 'use', 'certiﬁcate', 'pin', 'reduce', 'risk', 'mitm', 'attack', 'app', 'pin', 'backup', 'certiﬁcate', 'prevent', 'rely', 'pin', 'certiﬁcate', 'outdate', 'app', 'use', 'backup', 'connect', 'server', 'report', 'insecure', 'ssl', 'implementation', 'vulnerability', 'exactly', 'app', 'app', 'use', 'pin', 'selfsigned', 'certiﬁcate', 'app', 'pin', 'selfsigned', 'certiﬁcate', 'beneﬁts', 'limitation', 'term', 'prevent', 'mitm', 'attack', 'consider', 'scenario', 'app', 'pin', 'certiﬁcate', 'sign', 'ca', 'compromise', 'situation', 'app', 'need', 'update', 'software', 'update', 'newly', 'issue', 'certiﬁcate', 'hand', 'app', 'pin', 'selfsigne', 'certiﬁcate', 'app', 'trust', 'certiﬁcate', 'aﬀecte', '’s', 'certiﬁcate', 'compromise', 'however', 'use', 'selfsigned', 'certiﬁcate', 'secure', 'continuously', 'monitor', 'absence', 'app', 'developer', 'know', 'compromise', 'certiﬁcate', 'communication', 'compromise', 'server', 'continue', 'unknowingly', 'however', 'actively', 'maintain', 'selfsigned', 'certiﬁcate', 'cumbersome', 'use', 'certiﬁcate', 'trust', 'trust', 'cas', 'revoke', 'compromise', 'certiﬁcate', 'use', 'server', 'stop', 'communication', 'app', 'pin', 'certiﬁcate', 'server', 'protect', 'user', 'harm', 'therefore', 'general', 'case', 'secure', 'pin', 'certiﬁcate', 'sign', 'trust', 'cas', 'instead', 'selfsigne', 'discussion', 'observation', 'app', 'result', 'show', 'contact', 'trace', 'app', 'android', 'base', 'framework', 'privileged', 'far', 'android', 'app', 'grant', 'permission', 'use', 'privileged', 'system', 'feature', 'install', 'time', 'result', 'user', 'less', 'control', 'grant', 'permission', 'app', 'runtime', 'concern', 'app', 'potentially', 'use', 'task', 'contact', 'trace', 'mass', 'surveillance', 'therefore', 'app', 'collect', 'minimal', 'information', 'use', 'least', 'privilege', 'framework', 'develop', 'help', 'create', 'app', 'preserve', 'user', 'privacy', 'ever', 'analysis', 'show', 'app', 'violate', 'privacy', 'policy', 'several', 'potential', 'reason', 'developer', 'oversight', 'developer', 'lack', 'domain', 'knowledge', 'awareness', 'underlying', 'platform', 'oversight', 'concern', 'understandable', 'contact', 'trace', 'app', 'develop', 'hurriedly', 'tackle', 'challenge', 'grow', 'pandemic', 'however', 'oversight', 'context', 'lead', 'lack', 'transparency', 'credibility', 'exacerbate', 'skepticism', 'general', 'public', 'contact', 'trace', 'app', 'hamper', 'widespread', 'adoption', 'weak', 'adoption', 'desirable', 'contact', 'trace', 'app', 'use', 'eﬀectively', 'vital', 'tool', 'contain', 'pandemic', 'developer', 'oversight', 'reason', 'privacy', 'violation', 'need', 'develop', 'tool', 'technique', 'help', 'developer', 'write', 'privacy', 'policy', 'consistent', 'app', 'behavior', 'vice', 'versa', 'app', 'study', 'miss', 'mention', 'privacy', 'personally', 'identiﬁable', 'information', 'collect', 'app', 'possible', 'reason', 'lack', 'domain', '1user', 'connect', 'remote', 'server', 'upload', 'positive', 'test', 'result', 'also', 'app', 'periodically', 'connect', 'remote', 'server', 'check', 'positive', 'case', 'knowledge', 'developer', 'context', 'privacy', 'research', 'eﬀort', 'focus', 'develop', 'meth', 'od', 'identify', 'domainspeciﬁc', 'personally', 'identiﬁable', 'information', 'contact', 'trace', 'app', 'feature', 'misuse', 'violate', 'user', 'privacy', 'therefore', 'crucial', 'accurately', 'iden', 'tify', 'information', 'app', 'collect', 'user', 'make', 'informed', 'decision', 'privacy', 'implication', 'use', 'app', 'framework', 'make', 'easy', 'healthcare', 'provider', 'create', 'app', 'eﬀective', 'contact', 'trace', 'need', 'know', 'detail', 'underlie', 'platform', 'also', 'evident', 'certain', 'privacy', 'violation', 'avoid', 'developer', 'know', 'underlying', 'platform', 'behavior', 'therefore', 'exist', 'research', 'app', 'security', 'privacy', 'focus', 'develop', 'method', 'tool', 'assist', 'less', 'experienced', 'developer', 'gain', 'necessary', 'knowledge', 'avoid', 'privacy', 'violation', 'select', 'app', 'vulnerability', 'well', 'known', 'development', 'community', 'fact', 'vulnerability', 'discover', 'part', 'owasp', 'top', 'popular', 'set', 'guideline', 'develop', 'secure', 'mobile', 'web', 'app', 'app', 'vulnerability', 'focus', 'ensure', 'app', 'secure', 'preserve', 'user', 'privacy', 'suggest', 'app', 'developer', 'experience', 'avoid', 'mistake', 'access', 'tool', 'help', 'prevent', 'vulnerability', 'eﬀectively', 'latter', 'less', 'likely', 'app', 'development', 'ide', 'android', 'studio', 'support', 'detect', 'prevent', 'vulnerability', 'development', 'therefore', 'likely', 'state', 'allocate', 'resource', 'recruit', 'developer', 'suﬃcient', 'experience', 'mobile', 'app', 'development', 'understandable', 'state', 'need', 'prioritize', 'resource', 'tackle', 'pandemic', 'plan', 'well', 'develop', 'deploying', 'app', 'longterm', 'consequence', 'user', 'privacy', 'security', 'observation', 'mobsf', 'investigate', 'potential', 'vulnerability', 'report', 'discover', 'false', 'positive', 'falsely', 'report', 'vulnerability', 'report', 'discuss', 'help', 'tool', 'developer', 'mobsf', 'improve', 'tool', 'brieﬂy', 'report', 'total', 'ﬁve', 'false', 'positive', 'app', 'report', 'ﬁve', 'false', 'positive', 'app', 'false', 'positive', 'respectively', 'explain', 'false', 'positive', 'likely', 'reason', 'suggestion', 'avoid', 'follow', 'sqlinjection', 'vulnerability', 'falsely', 'report', 'app', 'mobsf', 'report', 'vulnerability', 'app', 'use', 'api', 'execute', 'sql', 'query', 'however', 'use', 'lead', 'sqlinjection', 'query', 'string', 'usersupplie', 'input', 'potentially', 'malicious', 'sql', 'none', 'query', 'report', 'potentially', 'vulnerable', 'sqlinjection', 'rely', 'user', 'input', 'hence', 'vulnerable', 'sqlinjection', 'suggestion', 'ﬂag', 'query', 'rely', 'userinput', 'parameterize', 'falsely', 'report', 'unprotected', 'component', 'vulnerability', 'app', 'find', 'component', 'app', 'export', 'however', 'component', 'also', 'protect', 'system', 'permission', 'system', 'grant', 'access', 'component', 'furthermore', 'permission', 'deﬁne', 'part', 'framework', 'make', 'available', 'purpose', 'contact', 'trace', 'government', 'healthcare', 'authority', 'therefore', 'app', 'necessary', 'authorization', 'request', 'permission', 'get', 'access', 'export', 'component', 'suggestion', 'mobsf', 'consider', 'system', 'permission', 'deﬁne', 'framework', 'analysis', 'improve', 'detection', 'unprotected', 'component', 'report', 'app', 'save', 'sensitive', 'datum', 'username', 'password', 'secret', 'key', 'clear', 'text', 'furthermore', 'claim', 'app', 'log', 'sensitive', 'datum', 'inspection', 'find', 'claim', 'false', 'mobsf', 'report', 'app', 'save', 'log', 'string', 'constant', 'ﬁle', 'string', 'contain', 'word', 'key', 'password', 'constant', 'sensitive', 'datum', 'suggestion', 'consider', 'none', 'string', 'constant', 'word', 'key', 'password', 'sensitive', 'datum', 'sample', 'app', 'mobsf', 'consider', 'use', 'heuristic', 'identify', 'sensitive', 'datum', 'falsely', 'report', 'janu', 'signature', 'vulnerability', 'app', 'janu', 'system', 'vulnerability', 'allow', 'attacker', 'inject', 'dex', 'sign', 'v1', 'signature', 'scheme', 'aﬀecte', 'signature', 'vulnerability', 'exploit', 'android', 'package', 'valid', 'time', 'however', 'vulnerability', 'exploitable', 'app', 'run', 'android', 'version', 'low', 'android', 'developer', 'ﬁxe', 'vulnerability', 'version', 'result', 'android', 'apks', 'package', 'sign', 'scheme', 'contact', 'trace', 'app', 'consider', 'use', 'scheme', 'sign', 'apks', 'mobsf', 'ﬂagge', 'app', 'potentially', 'vulnerable', 'janu', 'app', 'also', 'sign', 'v1', 'signature', 'scheme', 'enable', 'backward', 'compatibility', 'categorize', 'false', 'positive', 'app', 'developer', 'choice', 'sign', 'app', 'v1', 'signature', 'scheme', 'support', 'android', 'version', 'less', 'moreover', 'app', 'developer', 'due', 'diligence', 'sign', 'app', 'scheme', 'v1', 'scheme', 'backward', 'compatibility', 'good', 'circumstance', 'suggestion', 'ﬂag', 'app', 'potentially', 'vulnerable', 'janus', 'sign', 'v1', 'signature', 'scheme', 'app', 'sign', 'consider', 'report', 'diﬀerent', 'label', 'information', 'label', 'inform', 'developer', 'ﬁxe', 'stage', 'aware', 'v1', 'signature', 'vulnerable', 'android', 'version', 'less', 'therefore', 'app', 'want', 'consider', 'support', 'android', 'version', 'conclusion', 'report', 'false', 'positive', 'potential', 'true', 'positive', 'wrt', 'potential', 'vulnerability', 'app', 'see', 'table', 'show', 'high', 'false', 'positive', 'rate', 'observation', 'consistent', 'prior', 'research', 'eﬀort', 'evaluate', 'eﬀectiveness', 'security', 'analysis', 'tool', 'android', 'app', 'general', 'static', 'analysis', 'tool', 'mobsf', 'high', 'false', 'positive', 'rate', 'hamper', 'adoption', 'reduce', 'eﬀectiveness', 'high', 'false', 'positive', 'rate', 'mobsf', 'user', 'manually', 'verify', 'warning', 'issue', 'report', 'reduce', 'trust', 'tool', 'verdict', 'consequently', 'hamper', 'tool', 'adoption', 'overall', 'eﬀectiveness', 'tool', 'relate', 'work', 'onset', 'pandemic', 'numerous', 'proposal', 'make', 'automate', 'contact', 'context', 'many', 'contact', 'trace', 'app', 'implement', 'tracing', 'process', 'deploy', 'worldwide', 'lead', 'plethora', 'eﬀort', 'survey', 'characteristic', 'evaluate', 'eﬀectiveness', 'contact', 'trace', 'app', 'several', 'agency', 'worldwide', 'call', 'independent', 'assessment', 'security', 'privacy', 'risk', 'pose', 'contact', 'trace', 'app', 'great', 'transparency', 'accountability', 'context', 'researcher', 'evaluate', 'security', 'privacy', 'contact', 'trace', 'app', 'understand', 'pose', 'privacy', 'risk', 'user', 'section', 'discuss', 'eﬀort', 'closely', 'related', 'work', 'diﬀerent', 'wen', 'perform', 'systematic', 'study', 'contact', 'trace', 'app', 'deploy', 'android', 'io', 'use', 'program', 'analysis', 'determine', 'apis', 'relevant', 'contact', 'trace', 'identify', 'information', 'collect', 'app', 'additionally', 'perform', 'crossplatform', 'comparison', 'app', 'available', 'android', 'io', 'result', 'show', 'app', 'expose', 'identiﬁable', 'information', 'enable', 'ﬁngerprinting', 'app', 'tracking', 'speciﬁc', 'user', 'moreover', 'observe', 'app', 'exhibit', 'inconsistency', 'platform', 'lead', 'diﬀerent', 'privacy', 'implication', 'platform', 'eﬀort', 'ﬁrst', 'attempt', 'understand', 'privacy', 'security', 'implication', 'contact', 'trace', 'app', 'eﬀort', 'relate', 'evaluation', 'focus', 'diﬀerent', 'aspect', 'example', 'addition', 'vulnerability', 'analysis', 'also', 'analyze', 'privacy', 'policy', 'app', 'eﬀort', 'consider', 'samhi', 'conduct', 'empirical', 'evaluation', 'android', 'app', 'play', 'relate', 'covid19', 'aim', 'study', 'broadly', 'characterize', 'app', 'term', 'purpose', 'intend', 'user', 'complexity', 'development', 'process', 'potential', 'security', 'risk', 'focus', 'security', 'privacy', 'observe', 'none', 'app', 'consider', 'leak', 'sensitive', 'datum', 'base', 'static', 'analysis', 'app', 'use', 'tool', 'flowdroid', 'iccta', 'however', 'recent', 'eﬀort', 'measure', 'eﬀectiveness', 'security', 'analysis', 'tool', 'raise', 'question', 'eﬀectiveness', 'static', 'analysis', 'tool', 'like', 'flowdroid', 'iccta', 'detect', 'sensitive', 'datum', 'leak', 'hatmian', 'analyze', 'privacy', 'security', 'performance', 'contact', 'trace', 'app', 'available', 'android', 'platform', 'mayjune', 'analyze', 'permission', 'use', 'app', 'potential', 'vulnerability', 'far', 'measure', 'coverage', 'privacy', 'policy', 'app', 'privacy', 'principle', 'outline', 'general', 'regulation', 'gdpr', 'law', 'work', 'diﬀerent', 'number', 'way', 'first', 'focus', 'oﬃcial', 'app', 'develop', 'state', 'base', 'none', 'app', 'study', 'conduct', 'base', 'framework', 'second', 'instead', 'measure', 'coverage', 'app', 'privacy', 'policy', 'wrt', 'privacy', 'principle', 'analyze', 'app', 'privacy', 'policy', 'consistent', 'source', 'code', 'app', 'violate', 'privacy', 'policy', 'third', 'critically', 'analyze', 'verdict', 'report', 'tool', 'mobsf', 'instead', 'report', 'example', 'hatmian', 'base', 'analysis', 'app', 'consider', 'log', 'sensitive', 'information', 'evaluation', 'mobsf', 'also', 'ﬂagge', 'app', 'log', 'sensitive', 'information', 'however', 'manually', 'verify', 'verdict', 'discover', 'false', 'positive', 'app', 'baumgartner', 'demonstrate', 'design', 'vulnerable', 'proﬁle', 'deanonymize', 'infected', 'person', 'relaybase', 'wormhole', 'attack', 'capable', 'generate', 'fake', 'contact', 'derail', 'contact', 'tracing', 'process', 'claim', 'vulnerability', 'address', 'app', 'base', 'framework', 'vulnerable', 'instead', 'analyze', 'design', 'paper', 'analyze', 'app', 'base', 'framework', 'perspective', 'app', 'comply', 'privacy', 'policy', 'contain', 'vulnerability', 'manifest', 'know', 'implementation', 'bug', 'incorrect', 'conﬁguration', 'review', 'security', 'privacy', 'contact', 'trace', 'app', 'year', 'pandemic', 'statically', 'analyze', 'app', 'use', 'mobsf', 'vulnerability', 'base', 'threat', 'sce', 'nario', 'identiﬁe', 'contact', 'trace', 'app', 'additionally', 'report', 'datum', 'tracker', 'embed', 'app', 'potentially', 'violate', 'privacy', 'datum', 'collect', 'tracker', 'use', 'user', 'consent', 'however', 'privacy', 'analysis', 'include', 'analysis', 'privacy', 'policy', 'set', 'app', 'evaluation', 'include', 'app', 'consider', 'assessment', 'however', 'result', 'analysis', 'diﬀer', 'signiﬁcantly', 'example', 'app', 'consider', 'store', 'sensitive', 'information', 'cleartext', 'hand', 'find', 'false', 'positive', 'app', 'consider', 'similarly', 'discover', 'vulnerability', 'analysis', 'eg', 'datum', 'backup', 'report', 'use', 'dynamic', 'analyzer', 'virustotal', 'detect', 'presence', 'malware', 'contact', 'trace', 'app', 'however', 'malware', 'detection', 'tool', 'virustotal', 'accurately', 'detect', 'presence', 'malware', 'often', 'falsely', 'identify', 'potentially', 'unwanted', 'program', 'pup', 'kouliaridi', 'investigate', 'oﬃcial', 'contact', 'trace', 'app', 'deploy', 'european', 'try', 'analyze', 'app', 'statically', 'dynamically', 'static', 'analysis', 'include', 'sensitive', 'permission', 'api', 'call', 'tracker', 'known', 'vulnerability', 'conﬁguration', 'aﬀect', 'app', 'security', 'base', 'common', 'weakness', 'enumeration', 'cwes', 'common', 'vulnerability', 'exposure', 'cf', 'dynamic', 'analysis', 'involve', 'instrument', 'source', 'code', 'verifying', 'app', 'use', 'location', 'bluetooth', 'service', 'runtime', 'monitoring', 'network', 'traﬃc', 'evaluation', 'paper', 'diﬀer', 'distinct', 'way', 'first', 'consider', 'diﬀerent', 'set', 'app', 'second', 'analyze', 'app', 'privacy', 'policy', 'determine', 'consistent', 'encode', 'behavior', 'third', 'dynam', 'ically', 'analyze', 'app', 'far', 'notable', 'diﬀerence', 'static', 'analysis', 'observation', 'example', 'kouliaridis', 'twothird', 'app', 'include', 'app', 'potential', 'sql', 'injection', 'vulnerability', 'base', 'analysis', 'however', 'observe', 'falsely', 'report', 'sql', 'injection', 'vulnerability', 'gaen', 'app', 'consider', 'caveat', 'vulnerability', 'analysis', 'consider', 'vulnerability', 'report', 'cover', 'vulnerability', 'source', 'possible', 'exist', 'app', 'go', 'unreported', 'consequently', 'vulnerability', 'analysis', 'comprehensive', 'app', 'developer', 'read', 'report', 'take', 'step', 'ﬁx', 'reported', 'vulnerability', 'perform', 'analysis', 'ensure', 'vulnerability', 'report', 'exist', 'app', 'conﬁrme', 'potential', 'vulnerability', 'report', 'static', 'analysis', 'tool', 'manually', 'examin', 'e', 'however', 'build', 'malicious', 'application', 'exploit', 'vulnerability', 'therefore', 'know', 'extent', 'vulnerability', 'exploitable', 'result', 'report', 'study', 'limit', 'set', 'app', 'consider', 'gaen', 'app', 'general', 'therefore', 'generalize', 'contact', 'trace', 'app', 'especially', 'app', 'base', 'conclusion', 'paper', 'conduct', 'systematic', 'investigation', 'contact', 'trace', 'app', 'base', 'framework', 'app', 'implement', 'deploy', 'oﬃcial', 'health', 'department', 'respective', 'state', 'discover', 'considered', 'app', 'overprivilege', 'violate', 'privacy', 'policy', 'contain', 'vulnerability', 'exploit', 'malicious', 'user', 'cause', 'harm', 'app', 'user', 'previous', 'eﬀort', 'evaluate', 'contact', 'trace', 'app', 'privacy', 'violation', 'vulnerability', 'none', 'focus', 'consistency', 'app', 'privacy', 'policy', 'wrt', 'encode', 'behavior', 'similarity', 'eﬀort', 'exist', 'eﬀort', 'term', 'vulnerability', 'analysis', 'contact', 'trace', 'app', 'result', 'diﬀer', 'markedly', 'result', 'show', 'vulnerability', 'report', 'potential', 'vulnerability', 'relate', 'evaluation', 'false', 'positive', 'example', 'several', 'exist', 'research', 'eﬀort', 'report', 'janu', 'vulnerability', 'report', 'false', 'positive', 'true', 'positive', 'result', 'therefore', 'raise', 'question', 'eﬀort', 'study', 'privacy', 'security', 'contact', 'trace', 'app', 'report', 'vulnerability', 'manifest', 'reality', 'report', 'false', 'positive', 'potential', 'true', 'positive', 'erode', 'public', 'trust', 'contact', 'trace', 'app', 'eventually', 'lead', 'reduce', 'adoption', 'ultimately', 'weaken', 'eﬀort', 'contain', 'pandemic', 'therefore', 'need', 'researcher', 'continuously', 'evaluate', 'security', 'privacy', 'contact', 'trace', 'app', 'reproduce', 'verify', 'result', 'ackowledgement', 'wish', 'thank', 'patrick', 'wszeborowski', 'undergraduate', 'student', 'department', 'computer', 'science', 'assist', 'datum', 'collection', 'reference', 'fraser', 'heesterbeek', 'eﬀectiveness', 'contact', 'trace', 'e', 'epidemic', 'plo', 'vol', 'p', 'e12', 'survey', 'contact', 'trace', 'late', 'advancement', 'challenge', 'acm', 'transaction', 'spatial', 'algorithm', 'system', 'vol', 'pp', 'anglemyer', 'moore', 'parker', 'chamber', 'grady', 'flemyng', 'bero', 'digital', 'contact', 'trace', 'technology', 'epidemic', 'rapid', 'review', 'cochrane', 'database', 'systematic', 'review', 'ahme', 'r', 'ruj', 'r', 'malaney', 'kanhere', 'seneviratne', 'jha', 'survey', 'covid19', 'contact', 'trace', 'app', 'ieee', 'access', 'vol', 'pp', 'akinbi', 'forshaw', 'blinkhorn', 'contact', 'trace', 'app', 'covid19', 'pandemic', 'systematic', 'literature', 'review', 'challenge', 'future', 'direction', 'neoliberal', 'society', 'health', 'information', 'science', 'system', 'vol', 'contact', 'trace', 'app', 'value', 'dilemmas', 'privacy', 'paradox', 'neoliberal', 'world', 'international', 'journal', 'information', 'management', 'vol', 'p', 'hassandoust', 'akhlaghpour', 'individual', 'privacy', 'concern', 'adoption', 'contact', 'trace', 'mobile', 'application', 'pandemic', 'situational', 'privacy', 'calculus', 'journal', 'vol', 'pp', 'g', 'kambouraki', 'nai', 'mystify', 'covid19', 'digital', 'contact', 'trace', 'survey', 'framework', 'mobile', 'app', 'wireless', 'communication', 'mobile', 'computing', 'vol', 'u', 'oﬃce', 'beneﬁts', 'challenge', 'smartphone', 'application', 'augment', 'contact', 'trace', 'tracetogether', 'tracetogether', 'work', 'r', 'gupta', 'bedi', 'p', 'wadhera', 'analysis', 'covid19', 'track', 'tool', 'case', 'study', 'mobile', 'application', 'digital', 'government', 'research', 'practice', 'vol', 'pp', 'vaudenay', 'centralize', 'decentralize', 'contact', 'trace', 'dilemma', 'cryptology', 'eprint', 'archive', 'faklaris', 'king', 'hong', 'decentralize', 'risk', 'free', 'understand', 'public', 'perception', 'privacyutility', 'tradeoﬀs', 'covid19', 'contacttrace', 'app', 'arxiv', 'preprint', 'exposure', 'notiﬁcation', 'implementation', 'guide', 'androidexposurenotiﬁcationsimplementationguide', 'apple', 'enexposureconﬁguration', 'exposurenotiﬁcationenexposureconﬁguration', 'r', 'kreuter', 'nosenzo', 'abeler', 'acceptability', 'appbase', 'contact', 'trace', 'survey', 'study', 'vol', 'p', 'publiclyavailable', 'exposure', 'notiﬁcation', 'app', 'sufatrio', 'l', 'thing', 'secure', 'survey', 'taxonomy', 'challenge', 'acm', 'comput', 'surv', 'rasthofer', 'bartel', 'octeau', 'static', 'analysis', 'android', 'app', 'systematic', 'literature', 'review', 'information', 'software', 'technology', 'vol', 'vp', 'ranganath', 'mitra', 'free', 'android', 'app', 'security', 'analysis', 'tool', 'eﬀective', 'tecte', 'know', 'vulnerability', 'empirical', 'software', 'engineering', 'vol', 'pp', 'nadal', 'desnos', 'l', 'chex', 'statically', 'vet', 'android', 'app', 'compo', 'nent', 'hijacking', 'vulnerability', 'proceeding', 'acm', 'conference', 'computer', 'communication', 'security', 'e', 'shioji', 'takata', 'mori', 'understand', 'origin', 'mobile', 'app', 'vulnerability', 'largescale', 'measurement', 'study', 'free', 'pay', 'app', 'ieeeacm', '14th', 'international', 'conference', 'mining', 'software', 'repository', 'ieee', 'repository', 'android', 'app', 'vulnerability', 'benchmark', 'proceeding', '13th', 'international', 'conference', 'predictive', 'model', 'datum', 'analytic', 'software', 'engineering', 'gadient', 'nierstrasz', 'security', 'smell', 'android', 'ieee', '17th', 'ieee', 'international', 'work', 'conference', 'source', 'code', 'analysis', 'manipulation', 'scam', 'p', 'r', 'center', 'analysis', 'android', 'app', 'permission', 'internet20151110ananalysisofandroidapppermission', 'nov', 'b', 'gate', 'r', 'potharaju', 'c', 'nitarotaru', 'molloy', 'permis', 'sion', 'perspective', 'combine', 'risk', 'beneﬁts', 'proceeding', '17th', 'acm', 'symposium', 'access', 'control', 'model', 'technology', 'app', 'permission', 'v', 'prakash', 'da', 'explain', 'citizen', 'resistance', 'use', 'digital', 'contact', 'trace', 'app', 'mixedmethod', 'study', 'international', 'journal', 'information', 'management', 'vol', 'p', 'manifest', 'permission', 'android', 'manifestpermissionuse', 'fingerprint', 'owasp', 'owasp', 'top', 'bell', 'butler', 'c', 'hick', 'crowcroft', 'tracesecure', 'privacy', 'preserve', 'contact', 'trace', 'arxiv', 'preprint', 'allix', 'ﬁrst', 'look', 'android', 'application', 'play', 'relate', 'covid19', 'empirical', 'software', 'engineering', 'vol', 'pp', 'h', 'cho', 'contact', 'trace', 'mobile', 'app', 'covid19', 'privacy', 'consideration', 'related', 'preprint', 'lanze', 'contact', 'trace', 'app', 'ethical', 'roadmap', 'ethic', 'information', 'technology', 'vol', 'l', 'baumg¨artner', 'dmitrienko', 'b', 'freisleben', 'gruler', 'r', 'mitev', 'muhamedagic', 'mind', 'gap', 'security', 'privacy', 'risk', 'contact', 'trace', 'app', 'ieee', '19th', 'international', 'conference', 'trust', 'security', 'privacy', 'computing', 'communication', 'trustcom', 'ieee', 'gvili', 'security', 'analysis', 'covid19', 'contact', 'trace', 'speciﬁcation', 'cryptology', 'eprint', 'archive', 'h', 'wen', 'shroﬀ', 'study', 'privacy', 'covid19', 'tact', 'trace', 'app', 'international', 'conference', 'security', 'privacy', 'communication', 'system', 'springer', 'l', 'fritsch', 'privacy', 'security', 'analysis', 'early', 'deploy', 'covid19', 'contact', 'trace', 'android', 'app', 'empirical', 'software', 'engineering', 'vol', 'e', 'union', 'general', 'datum', 'protection', 'regulation', 'v', 'year', 'security', 'privacy', 'review', 'contact', 'trace', 'mobile', 'app', 'ieee', 'pervasive', 'computing', 'vol', 'h', 'systemas', 'virustotal', 'v', 'kouliaridis', 'kambouraki', 'e', 'geneiataki', 'dissecting', 'contact', 'trace', 'app', 'android', 'platform', 'plo', 'vol', 'p', 'mitre', 'common', 'weakness', 'enumeration', 'common', 'vulnerability', 'exposure', 'app', 'collect', 'location', 'use', 'insecure', 'use', 'weak', 'use', 'http', 'communicate', 'nonus', 'violation', 'privacy', 'policy', 'violation', 'app', 'violation', 'storage', 'encryption', 'n', 'server', 'domain', 'app', 'table', 'privacy', 'violation', 'usbased', 'column', 'indicate', 'feature', 'action', 'app', 'claim', 'use', 'privacy', 'policy', 'cell', 'denote', 'app', 'perform', 'action', 'correspond', 'column', 'imply', 'privacy', 'violation', 'imply', 'otherwise', 'certiﬁcate', 'vulns', 'app', 'app', 'unprotected', 'component', 'dakotawyome', 'app', 'vuln', 'know', 'vulnerability', 'allow', 'insecure', 'weak', 'prng', 'hash', 'datum', 'backup', 'insecure', 'ssl', 'impl', 'pin', 'table', 'know', 'vulnerability', 'usbased', 'column', 'indicate', 'know', 'vulnerability', 'cell', 'denote', 'app', 'contain', 'vulnerability', 'otherwise', 'app', 'sql', 'injection', 'unprotected', 'component', 'cleartext', 'storage', 'log', 'sensitive', 'false', 'positive', 'vulnerability', 'dakotawyome', 'app', 'false', 'positive', 'datum', 'table', 'false', 'positive', 'report', 'mobsf', 'usbased', 'cell', 'indicate', 'vulnerability', 'correspond', 'column', 'falsely', 'report', 'potential', 'vulnerability', 'denote', 'mobsf', 'report', 'vulnerability', 'correspond', 'column', 'app', 'janu', 'signature', 'false', 'positive', 'vulnerability']"
"An Empirical Evaluation of Four Off-the-Shelf Proprietary
  Visual-Inertial Odometry Systems","[{'href': 'http://arxiv.org/abs/2207.06780v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.06780v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-14 09:40:34,"2
2
0
2

l
u
J

7

]

G
L
.
s
c
[

2
v
9
9
0
2
0
.
7
0
2
2
:
v
i
X
r
a

Preprint

An empirical study of implicit regularization in deep oﬄine
RL

Caglar Gulcehre∗, Srivatsan Srinivasan∗, Jakub Sygnowski, Georg Ostrovski,

Mehrdad Farajtabar, Matt Hoﬀman, Razvan Pascanu, Arnaud Doucet
DeepMind

Abstract

Deep neural networks are the most commonly used function approximators in oﬄine re-
inforcement learning. Prior works have shown that neural nets trained with TD-learning
and gradient descent can exhibit implicit regularization that can be characterized by under-
parameterization of these networks. Speciﬁcally, the rank of the penultimate feature layer,
also called eﬀective rank, has been observed to drastically collapse during the training. In
turn, this collapse has been argued to reduce the model’s ability to further adapt in later
stages of learning, leading to the diminished ﬁnal performance. Such an association between
the eﬀective rank and performance makes eﬀective rank compelling for oﬄine RL, primarily
for oﬄine policy evaluation. In this work, we conduct a careful empirical study on the
relation between eﬀective rank and performance on three oﬄine RL datasets : bsuite, Atari,
and DeepMind lab. We observe that a direct association exists only in restricted settings
and disappears in the more extensive hyperparameter sweeps. Also, we empirically identify
three phases of learning that explain the impact of implicit regularization on the learning
dynamics and found that bootstrapping alone is insuﬃcient to explain the collapse of the
eﬀective rank. Further, we show that several other factors could confound the relationship
between eﬀective rank and performance and conclude that studying this association under
simplistic assumptions could be highly misleading.

Contents

1 Introduction

2 Background

2.1 Eﬀective rank and implicit under-regularization . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 Auxiliary losses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 Deep oﬄine RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Experimental setup

4 Eﬀective rank and performance

4.1 Lifespan of learning with deep Q-networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

5

5

6

6

7

8

9

4.2 The eﬀect of dataset size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

∗Indicates joint ﬁrst authors.

1

 
 
 
 
 
 
Preprint

5 Interactions between rank and performance

6 The eﬀect of activation functions

7 Optimization

8 The eﬀect of loss function

8.1 Q-learning and behavior cloning

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.2 CURL: The eﬀect of self-supervision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9 Tandem RL

10 Oﬄine policy selection

11 Robustness to input perturbations

12 Discussion

A Appendix

A.1 The impact of depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.2 Spectral density of hessian for oﬄine RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.3 DeepMind lab: performance vs the eﬀective ranks . . . . . . . . . . . . . . . . . . . . . . . . .

A.4 SAM optimizer: does smoother loss landscape aﬀect the behavior of the rank collapse? . . . .

A.5 Eﬀective rank and the value error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.6 CURL on DeepMind Lab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.7 Learning rate evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.8 Learning curves long training regime and the diﬀerent phases of learning . . . . . . . . . . . .

A.9 Eﬀective rank and the performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.10 Interventions to eﬀective rank and randomized controlled trials (RCT) . . . . . . . . . . . . .

A.11 Computation of feature ranks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.12 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.13 bsuite phase transitions and bottleneck capacity . . . . . . . . . . . . . . . . . . . . . . . . .

A.14 Activation sparsity on bsuite

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

12

15

15

17

17

18

19

22

23

25

30

30

30

32

32

33

34

35

35

35

35

35

38

38

38

1 Introduction

The use of deep networks as function approximators in reinforcement learning (RL), referred to as Deep
Reinforcement Learning (DRL), has become the dominant paradigm in solving complex tasks. Until recently,
most DRL literature focused on online-RL paradigm, where agents must interact with the environment to
explore and learn. This led to remarkable results on Atari (Mnih et al., 2015), Go (Silver et al., 2017),
StarCraft II (Vinyals et al., 2019), Dota 2 (Berner et al., 2019), and robotics (Andrychowicz et al., 2020).
Unfortunately, the need to interact with the environment makes these algorithms unsuitable and unsafe for
many real-world applications, where any action taken can have serious ethical or harmful consequences or be

2

Preprint

costly. In contrast, in the oﬄine RL paradigm (Fu et al., 2020; Fujimoto et al., 2018; Gulcehre et al., 2020;
Levine et al., 2020), also known as batch RL (Ernst et al., 2005; Lange et al., 2012), agents learn from a ﬁxed
dataset previously logged by other (possibly unknown) agents. This ability makes oﬄine RL more applicable
to the real world.

Recently, Kumar et al. (2020a) showed that oﬄine RL methods coupled with TD learning losses could suﬀer
from a eﬀective rank collapse of the penultimate layer’s activations, which renders the network to become
under-parameterized. They further demonstrated a signiﬁcant fraction of Atari games, where the collapse
of eﬀective rank collapse corresponded to performance degradation. Subsequently, Kumar et al. (2020a)
explained the rank collapse phenomenon by analyzing a TD-learning loss with bootstrapped targets in the
kernel and linear regression setups. In these simpliﬁed scenarios, bootstrapping leads to self-distillation,
causing severe under-parametrization and poor performance, as also observed and analyzed by Mobahi et al.
(2020). Nevertheless, Huh et al. (2021) studied the rank of the representations in a supervised learning
setting (image classiﬁcation tasks) and argued that low rank leads to better performance. Thus the low-rank
representations could act as an implicit regularizer.

Figure 1: [Atari] The rank and the performance on broad vs narrow hyperparameter sweep:
Correlation between eﬀective rank and agent’s performance towards the end of training in diﬀerent Atari
games. We report the regression lines for the narrow sweep, which covers only a single oﬄine RL algorithm,
with a small minibatch size (32) and a learning rate sweep similar to the hyperparameter sweep deﬁned in
RL Unplugged (Gulcehre et al., 2020), whereas in the broad setting, we included more data from diﬀerent
models and a larger hyperparameter sweep. In the narrow setup, there is a positive relationship between the
eﬀective rank and the agent’s performance, but that relationship disappears in the broad data setup and
almost reverses.

Typically, in machine learning, we rely on empirical evidence to extrapolate the rules or behaviors of our
learned system from the experimental data. Often those extrapolations are done based on a limited number
of experiments due to constraints on computation and time. Unfortunately, while extremely useful, these
extrapolations might not always generalize well across all settings. While Kumar et al. (2020a) do not
concretely propose a causal link between the rank and performance of the system, one might be tempted to
extrapolate the results (agents performing poorly when their rank collapsed) to the existence of such a causal

3

Preprint

link, which herein we refer to as rank collapse hypothesis. In this work, we do a careful large-scale empirical
analysis of this potential causal link using the oﬄine RL setting (also used by Kumar et al. (2020a)) and
also the Tandem RL (Ostrovski et al., 2021) setting. The existence of this causal link would be beneﬁcial
for oﬄine RL, as controlling the rank of the model could improve the performance (see the regularization
term explored in Kumar et al. (2020a)) or as we investigate here, the eﬀective rank could be used for model
selection in settings where oﬄine evaluation proves to be elusive.

Key Observation 1: The rank and the performance are correlated in restricted settings, but that
correlation disappears when we increase the range of hyperparameters and the models (Figure 1) a.

aThis is because other factors like hyperparameters and architecture can confound the rank of the penultimate
layer; unless the those factors are controlled carefully, the conclusions drawn from the experiments based on the rank
can be misleading.

Instead, we show that diﬀerent factors aﬀect a network’s rank without aﬀecting its performance. This ﬁnding
indicates that unless all of these factors of variations are controlled – many of which we might still be unaware
of – the rank alone might be a misleading indicator of performance.

A deep Q network exhibits three phases during training. We show that rank can be used to identify diﬀerent
stages of learning in Q-learning if the other factors are controlled carefully. We believe that our study, similar
to others (e.g. Dinh et al., 2017), re-emphasizes the importance of critically judging our understanding of the
behavior of neural networks based on simpliﬁed mathematical models or empirical evidence from a limited
set of experiments.

Key Observation 2: Deep Q-learning approaches goes through three phases of learning: i) simple
behaviors, ii) complex behaviors, iii) under-parameterization (Figure 2). These phases can be
identiﬁed by the eﬀective rank and performance on a given task a.

aThe ﬁrst two phases of learning happen during the training of all models we tested. At times, the third phase of
the learning could potentially lead to the agent losing its representation capacity and the ensuing poor performance.

Figure 2: Lifespan of learning in deep Q-learning: The plot on the left-hand side illustrates the evolution
of the eﬀective rank, and the plot on the right-hand side demonstrates the evolution of the performance
during training. In the ﬁrst phase, the model learns easy-to-learn behaviors that are simplistic by nature
and ignore many factors of environmental variations. The eﬀective rank collapses to a minimal value in the
ﬁrst phase since the model does not need a large capacity to learn the simple behaviors. In phase 2, the
model learns more complex behaviors that we identify as those that obtain large returns when the policy is
evaluated in the environment. Typically, in supervised learning, phase 2 is followed by overﬁtting. However,
in oﬄine RL (speciﬁcally the TD-learning approaches that we tried here), we observed that it is often followed
by underﬁtting/under-parameterization in phase 3.

4

k
n
a
R
e
v

i
t
c
e
f
f

E

s
n
r
u
t
e
R
e
d
o
s
i
p
E

Training Steps

Training Steps

Phases of  Learning

Phase 1: Simple 
Behaviors

Phase 2: Complex 
Behaviors

Phase 3:
Underparametrization

 
 
Preprint

We organize the rest of the paper and our contributions in the order we introduce in the paper as follows:

• Section 2 presents the related work, and Section 3 explains the experimental design that we rely on.

• In Sections 4, 8, 6, 7 and 9, we study the extent of impact of diﬀerent interventions such as architectures,
loss functions and optimization on the causal link between the rank and agent performance. Some
interventions in the model, such as introducing an auxiliary loss (e.g. CURL (Laskin et al., 2020),
SAM (Foret et al., 2021) or the activation function) can increase the eﬀective rank but does not
necessarily improve the performance. This ﬁnding indicates that the rank of the penultimate layer is
not enough to explain an agent’s performance. We also identify the settings where the rank strongly
correlates with the performance, such as DQN with ReLU activation and many learning steps over a
ﬁxed dataset.

• In Section 4.1, we show that a deep Q network goes through three stages of learning, and those
stages can be identiﬁed by using rank if the hyperparameters of the model are controlled carefully.

• Section 5 describes the main outcomes of our investigations. Particularly, we analyze the impact
of the interventions described earlier and provide counter-examples that help contradict the rank
collapse hypothesis, establishing that the link between rank and performance can be aﬀected by
several other confounding factors of variation.

• In Section 11, we ablate and compare the robustness of BC and DQN models with respect to random
perturbation introduced only when evaluating the agent in the environment. We found out that the
oﬄine DQN agent is more robust than the behavior cloning agent which has higher eﬀective rank.

• Section 12 presents the summary of our ﬁndings and its implications for oﬄine RL, along with

potential future research directions.

2 Background

2.1 Eﬀective rank and implicit under-regularization

The choice of architecture and optimizer can impose speciﬁc implicit biases that prefer certain solutions over
others. The study of the impact of these implicit biases on the generalization of the neural networks is often
referred to as implicit regularization. There is a plethora of literature studying diﬀerent sources of implicit
regularization such as initialization of parameters (Glorot and Bengio, 2010; Li and Liang, 2018; He et al.,
2015), architecture (Li et al., 2017; Huang et al., 2020), stochasticity (Keskar et al., 2016; Sagun et al., 2017),
and optimization (Smith et al., 2021; Barrett and Dherin, 2020). The rank of the feature matrix of a neural
network as a source of implicit regularization has been an active area of study, speciﬁcally in the context of
supervised learning (Arora et al., 2019; Huh et al., 2021; Pennington and Worah, 2017; Sanyal et al., 2018;
Daneshmand et al., 2020; Martin and Mahoney, 2021). In this work, we study the phenomenon of implicit
regularization through the eﬀective rank of the last hidden layer of the network, which we formally deﬁne
below.

Deﬁnition: Eﬀective Rank

Recently, Kumar et al. (2020a) studied the impact of the eﬀective rank on generalization in the general
RL context. With a batch size of N and D units in the feature layer, they proposed the eﬀective
rank formulation presented in Equation 1 for a feature matrix Φ ∈ RN ×D where N ≥ D that uses a
threshold value of δ with the singular values σi(Φ) in descending order i.e. σ1(Φ) ≥ σ2(Φ) ≥ · · · . We
provide the speciﬁc implementation of the eﬀective rank we used throughout this paper in Appendix
A.11.

eﬀective rankδ(Φ) = min

k

(

k :

5

Pk

PD

i=1 σi(Φ)
j=1 σj(Φ)

)

≥ 1 − δ

.

(1)

Preprint

Terminology and Assumptions. Note that the threshold value δ throughout this work has been ﬁxed
to 0.01 similar to Kumar et al. (2020a). Throughout this paper, we use the term eﬀective rank to describe
the rank of the last hidden layer’s features only, unless stated otherwise. This choice is consistent with prior
work (Kumar et al., 2020a) as the last layer acts as a representation bottleneck to the output layer.

Kumar et al. (2020a) suggested that the eﬀective rank of the Deep RL models trained with TD-learning
objectives collapses because of i) implicit regularization, happening due to repeated iterations over the dataset
with gradient descent; ii) self-distillation eﬀect emerging due to bootstrapping losses. They supported their
hypothesis with both theoretical analysis and empirical results. The theoretical analysis provided in their
paper assume a simpliﬁed setting, with inﬁnitesimally small learning rates, batch gradient descent and linear
networks, in line with most theory papers on the topic.

2.2 Auxiliary losses

Additionally, we ablate the eﬀect of using an auxiliary loss on an agent’s performance and rank. Speciﬁcally,
we chose the ""Contrastive Unsupervised Representations for Reinforcement Learning"" (CURL) (Laskin et al.,
2020) loss that is designed for use in a contrastive self-supervised learning setting within standard RL
algorithms:

L(s, a, r, θ) = LQ(s, a, r, θ) + λ ∗ LCU RL(s, ˆs, θ).

(2)

Here, LQ refers to standard RL losses and the CURL loss LCU RL(s, ˆs, θ) is the contrastive loss between
the features of the current observation s and a randomly augmented observation ˆs as described in Laskin
et al. (2020). Besides this, we also ablate with Sharpness-Aware Minimization (SAM) (Foret et al., 2021), an
approach that seeks parameters that have a uniformly low loss in its neighborhood, which leads to ﬂatter
local minima. We chose SAM as a means to better understand whether the geometry of loss landscapes
help inform the correlation between the eﬀective rank and agent performance in oﬄine RL algorithms. In
our experiments, we focus on analyzing mostly the deep Q-learning (DQN) algorithm (Mnih et al., 2015) to
simplify the experiments and facilitate deeper investigation into the rank collapse hypothesis.

2.3 Deep oﬄine RL

Online RL requires interactions with an environment to learn using random exploration. However, online
interactions with an environment can be unsafe and unethical in the real-world (Dulac-Arnold et al., 2019).
Oﬄine RL methods do not suﬀer from this problem because they can leverage oﬄine data to learn policies
that enable the application of RL in the real world (Menick et al., 2022; Shi et al., 2021; Konyushkova et al.,
2021). Here, we focused on oﬄine RL due to its importance in real-world applications, and the previous works
showed that the implicit regularization eﬀect is more pronounced in the oﬄine RL (Kumar et al., 2020a;
2021a).

Some of the early examples of oﬄine RL algorithms are least-squares temporal diﬀerence methods (Bradtke
and Barto, 1996; Lagoudakis and Parr, 2003) and ﬁtted Q-iteration (Ernst et al., 2005; Riedmiller, 2005).
Recently, value-based approaches to oﬄine RL have been quite popular.

Value-based approaches typically lower the value estimates for unseen state-action pairs, either through
regularization (Kumar et al., 2020b) or uncertainty (Agarwal et al., 2020). One could also include R-BVE
(Gulcehre et al., 2021) in this category, although it regularizes the Q function only on the rewarding transitions
to prevent learning suboptimal policies. Similar to R-BVE, Mathieu et al. (2021) have also shown that
the methods using single-step of policy improvement work well on tasks with very large action space and
low state-action coverage. In this paper, due to their simplicity and popularity, we mainly study action
value-based methods: oﬄine DQN (Agarwal et al., 2020) and oﬄine R2D2 (Gulcehre et al., 2021). Moreover,
we also sparingly use Batched Constrained Deep Q-learning (BCQ) algorithm (Fujimoto et al., 2019), another
popular oﬄine RL algorithm that uses the behavior policy to constrain the actions taken by the target
network.

Most oﬄine RL approaches we explained here rely on pessimistic value estimates (Jin et al., 2021; Xie
et al., 2021; Gulcehre et al., 2021; Kumar et al., 2020b). Mainly because oﬄine RL datasets lack exhaustive
exploration, and extrapolating the values to the states and actions not seen in the training set can result in

6

Preprint

extrapolation errors which can be catastrophic with TD-learning (Fujimoto et al., 2018; Kumar et al., 2019).
On the other hand, in online RL, it is a common practice to have inductive biases to keep optimistic value
functions to encourage exploration (Machado et al., 2015).

We also experiment with the tandem RL setting proposed by Ostrovski et al. (2021), which employs two
independently initialized online (active) and oﬄine (passive) networks in a training loop where only the online
agent explores and drives the data generation process. Both agents perform identical learning updates on the
identical sequence of training batches in the same order. Tandem RL is a form of oﬄine RL. Still, unlike the
traditional oﬄine RL setting on ﬁxed datasets, in the tandem RL, the behavior policy can change over time,
which can make the learning non-stationary. We are interested in this setting because the agent does not
necessarily reuse the same data repeatedly, which was pointed in Lyle et al. (2021) as a potential cause for
the rank collapse.

3 Experimental setup

To test and verify diﬀerent aspects of the rank collapse hypothesis and its potential impact on the agent
performance, we ran a large number of experiments on bsuite (Osband et al., 2019), Atari (Bellemare et al.,
2013) and DeepMind lab (Beattie et al., 2016) environments. In all these experiments, we use the experimental
protocol, datasets and hyperparameters from Gulcehre et al. (2020) unless stated otherwise. We provide the
details of architectures and their default hyperparameters in Appendix A.12.

• bsuite – We run ablation experiments on bsuite in a fully oﬄine setting with the same oﬄine dataset
as the one used in Gulcehre et al. (2021). We use a DQN agent (as a representative TD Learning
algorithm) with multi-layer feed-forward networks to represent the value function. bsuite provides us
with a small playground environment which lets us test certain hypotheses which are computationally
prohibitive in other domains (for e.g.: computing Hessians) with respect to terminal features and an
agent’s generalization performance.

• Atari – To test whether some of our observations are also true with higher-dimensional input features
such as images, we run experiments on the Atari dataset. Once again, we use an oﬄine DQN agent as a
representative TD-learning algorithm with a convolutional network as a function approximator. On Atari,
we conducted large-scale experiments on diﬀerent conﬁgurations:

(a) Small-scale experiments:

– DQN-256-2M: Oﬄine DQN on Atari with minibatch size 256 trained for 2M gradient steps with
four diﬀerent learning rates: [3e − 5, 1e − 4, 3e − 4, 5e − 4]. We ran these experiments to observe if
our observations hold in the default training scenario identiﬁed in RL Unplugged (Gulcehre et al.,
2020).

– DQN-32-100M: Oﬄine DQN on Atari with minibatch size of 32 trained for 100M gradient steps
with three diﬀerent learning rates: (cid:2)3 × 10−5, 1 × 10−4, 3 × 10−4(cid:3). We ran those experiments to
explore the eﬀects of reducing the minibatch size.

(b) Large-scale experiments:

– Long run learning rate sweep (DQN-256-20M): Oﬄine DQN trained for 20M gradients steps
with 12 diﬀerent learning rates evenly spaced in log-space between 10−2 and 10−5 trained on
minibatches of size 256. The purpose of these experiments is to explore the eﬀect of wide range of
learning rates and longer training on the eﬀective rank.

– Long run interventions (DQN-interventions): Oﬄine DQN trained for 20M gradient steps
on minibatches of size 256 with 128 diﬀerent hyperparameter interventions on activation functions,
dataset size, auxiliary losses etc. The purpose of these experiments is to understand the impact of
such interventions on the eﬀective rank in the course of long training.

• DeepMind Lab – While bsuite and Atari present relatively simple fully observable tasks that require
no memory, DeepMind lab tasks (Gulcehre et al., 2021) are more complex, partially observable tasks
where it is very diﬃcult to obtain good coverage in the dataset even after collecting billions of transitions.
We speciﬁcally conduct our observational studies on the eﬀective ranks on the DeepMind Lab dataset,

7

Preprint

which has data collected from a well-trained agent on the SeekAvoid level. The dataset is collected by
adding diﬀerent levels of action exploration noise to a well-trained agent in order to get datasets with a
larger coverage of the state-action space.

Figure 3: Structural causal model (SCM) of diﬀerent factors that we test: M represents the model
selection method that we use to determine the h which denotes the observed confounders that are chosen at
the beginning of the training, including the task, the model architecture (including depth and number of
units), learning rate and the number of gradient steps to train. β is the eﬀective rank of the penultimate
layer. λ is the agent’s performance, measured as episodic returns the agent attains after evaluating in the
environment. A represents the unobserved confounders that change during training but may aﬀect the
performance, such as the number of dead units, the parameter norms, and the other underlying factors that
can inﬂuence learning dynamics. We test the eﬀect of each factor by interventions.

We illustrate the structural causal model (SCM) of the interactions between diﬀerent factors that we would
like to test in Figure 3. To explore the relationship between the rank and the performance, we intervene on
h, which represents potential exogenous sources of implicit regularization, such as architecture, dataset
size, and the loss function, including the auxiliary losses. The interventions on h will result in a randomized
controlled trial (RCT.) A represents the unobserved factors that might aﬀect performance denoted by λ an
the eﬀective rank β such as activation norms and number of dead units. It is easy to justify the relationship
between M, h, A and β. We argue that β is also confounded by A and h. We show the confounding eﬀect of
A on β with our interventions to beta via auxiliary losses or architectural changes that increase the rank but
do not aﬀect the performance. We aim to understand the nature of the relationship between these terms and
whether SCM in the ﬁgure describes what we notice in our empirical exploration.

We overload the term performance of the agent to refer to episodic returns attained by the agent when
it is evaluated online in the environment. In stochastic environments and datasets with limited coverage,
an oﬄine RL algorithm’s online evaluation performance and generalization abilities would correlate in most
settings (Gulcehre et al., 2020; Kumar et al., 2021c). The oﬄine RL agents will need to generalize when they
are evaluated in the environment because of:

• Stochasticity in the initial conditions and transitions of the environment. For example, in the Atari
case, the stochasticity arises from sticky actions and on DeepMind lab, it arises from the randomization of
the initial positions of the lemons and apples.

• Limited coverage: The coverage of the environment by the dataset is often limited. Thus, an agent is

very likely to encounter states and actions that it has never seen during training.

4 Eﬀective rank and performance

Based on the results of Kumar et al. (2020a), one might be tempted to extrapolate a positive causal link
between the eﬀective rank of the last hidden layer and the agent’s performance measured as episodic returns

8

 β: Eﬀective rank  h: Observed confounders  λ: Performance   
A: Hidden confounders   M: Model selection method

M

h

   λ

A

β

Preprint

attained when evaluated in the environment. We explore this potentially interesting relationship on a larger
scale by adopting a proof by contradiction approach. We evaluated the agents with the hyperparameter
setup deﬁned for the Atari datasets in RL Unplugged (Gulcehre et al., 2020) and the hyperparameter sweep
deﬁned for DeepMind Lab (Gulcehre et al., 2021). For a narrow set of hyperparameters this correlation
exists as observed in Figure 1. However, in both cases, we notice that a broad hyperparameter sweep
makes the correlation between performance and rank disappear (see Figures 1 and DeepMind lab Figure
in Appendix A.3). In particular, we ﬁnd hyperparameter settings that lead to low (collapsed) ranks with
high performance (on par with the best performance reported in the restricted hyperparameter range) and
settings that lead to high ranks but poor performance. This shows that the correlation between eﬀective rank
and performance can not be trusted for oﬄine policy selection. In the following sections, we further present
speciﬁc ablations that help us understand the dependence of the eﬀective rank vs. performance correlation
on speciﬁc hyperparameter interventions.

4.1 Lifespan of learning with deep Q-networks

Empirically, we found eﬀective rank suﬃcient to identify three phases when training an oﬄine DQN agent
with a ReLU activation function (Figure 2). Although the eﬀective rank may be suﬃcient to identify those
stages, it still does not imply a direct causal link between the eﬀective rank and the performance as discussed
in the following sections. Several other factors can confound eﬀective rank, making it less reliable as a guiding
metric for oﬄine RL unless those confounders are carefully controlled.

• Phase 1 (Simple behaviors): The eﬀective rank of the model ﬁrst collapses to a small value, in some
cases to a single digit value, and then gradually starts to increase. In this phase, the model learns easy to
learn behaviors that would have low performance when evaluated in the environment. We hypothesized
that this could be due to the implicit bias of the SGD to learn functions of increasing complexity over
training iterations (Kalimeris et al., 2019); therefore early in training, the network relies on simple
behaviours that are myopic to most of the variation in the data. Hence the model has a low rank. The
rank collapse early in the beginning of the training happens very abruptly, just in a handful of gradient
updates the rank collapses to a single digit number. However, this early rank collapse does not degrade
the performance of the agent. We call this rank collapse in the early in the training as self-pruning eﬀect.

• Phase 2 (Complex behaviors): In this phase, the eﬀective rank of the model increases and then
usually ﬂattens. The model starts to learn more complex behaviors that would achieve high returns when
evaluated in the environment.

• Phase 3 (Underﬁtting/Underparameterization): In this phase, the eﬀective rank collapses to a
small value (often to 1) again, and the performance of the agent often collapses too. The third phase
is called underﬁtting since the agent’s performance usually drops, and the eﬀective rank also collapses,
which causes the agent to lose part of its capacity. This phase is not always observed (or the performance
does not collapse with eﬀective ran towards the end of the training) in all settings as we demonstrate
in our diﬀerent ablations. Typically in supervised learning, Phase 2 is followed by over-ﬁtting, but with
oﬄine TD-learning, we could not ﬁnd any evidence of over-ﬁtting. We believe that this phase is primarily
due to the target Q-network needing to extrapolate over the actions not seen during the training and
causing extrapolation errors as described by (Kumar et al., 2019). A piece of evidence to support this
hypothesis is presented in Figure 29 in Appendix A.5, which suggests that the eﬀective rank and the value
error of the agent correlate well. In this phase, the low eﬀective rank and poor performance are caused by
a large number of dead ReLU units. Shin and Karniadakis (2020) also show that as the network has an
increasing number of dead units, it becomes under-parameterized and this could negatively inﬂuence the
agent’s performance.

It is possible to identify those three phases in many of the learning curves we provide in this paper, and our
ﬁrst two phases agree with the works on SGD’s implicit bias of learning functions of increasing complexity
(Kalimeris et al., 2019). Given a ﬁxed model and architecture, whether it is possible to observe all these three
phases during training fundamentally depends on:

9

Preprint

Figure 4: The three phases of learning: On IceHockey and MsPacman RL Unplugged Atari games we
illustrate the diﬀerent phases of learning with the oﬄine DQN agent using the learning rate of 0.0004. The
blue region in the plots identiﬁes the Phase 1, green region is the Phase 2, and red region is the Phase 3
of the learning. IceHockey is one of the games where the expert that generated the dataset performs quite
poorly on it, thus the majority of the data is just random exploration data. It is easy to see that in this
phase, the Oﬄine DQN performs very poorly and never manages to get out of the Phase 1. The performance
of the agent on IceHockey is poor and the eﬀective rank of the network is low throughout the training. On
MsPacman, we can observe all the three phases. The model transition into Phase 2 from Phase 1 quickly,
and then followed by the under-ﬁtting regime where the eﬀective rank collapses the agent performs poorly.

1. Hyperparameters: The phases that an oﬄine RL algorithm would go through during training
depends on hyperparameters such as learning rate and the early stopping or training budget. For
example, due to early stopping the model may just stop in the second phase, if the learning rate is
too small, since the parameters will move much slower the model may never get out of Phase 1. If
the model is not large enough, it may never learn to transition from Phase 1 into Phase 2.

2. Data distribution: The data distribution has a very big inﬂuence on the phases the agent goes
thorough, for example, if the dataset only contains random exploratory data and ORL method may
fail to learn complex behaviors from that data and as a result, will never transition into Phase 2
from Phase 1.

3. Learning Paradigm: The learning algorithm, including optimizers and the loss function, can
inﬂuence the phases the agent would go through during the training. For example, we observed that
Phase 3 only happens with the oﬄine TD-learning approaches.

It is possible to avoid phase three (underﬁtting) by ﬁnding the correct hyperparameters. We believe the
third phase we observe might be due to non-stationarity in RL losses (Igl et al., 2020) due to bootstrapping
or errors propagating through bootstrapped targets (Kumar et al., 2021b). The underﬁtting regime only
appears if the network is trained long enough. The quality and the complexity of the data that an agent
learns from also plays a key role in deciding which learning phases are observed during training. In Figure 4,
we demonstrate the diﬀerent phases of learning on IceHockey and MsPacman games. On IceHockey, since the
expert that generated the dataset has a poor performance on that game, the oﬄine DQN is stuck in Phase 1,
and did not managed to learn complex behaviors that would push it to Phase 2 but on MsPacMan, all the
three phases are present. We provide learning curves for all the online policy selection Atari games across
twelve learning rates in Appendix A.8.

10

k
n
a
R
e
v
i
t
c
e
f
f

E

6

4

2

−2.5

−5

−7.5

−10

−12.5

n
r
u
t
e
R
e
d
o
s
p
E

i

IceHockey

MsPacman

60

40

20

0

5000

10000
IceHockey

15000

20000

0

0

Learner Steps (x1000)

2500

5000

10000
MsPacman

15000

20000

2000

1500

1000

500

0

0

5000

10000

15000

20000

0

5000

10000

15000

20000

Learner Steps (x1000)

Phases of  Learning

Phase 1: Simple 
Behaviors

Phase 2: Complex 
Behaviors

Phase 3:
Underparametrization

 
 
Preprint

Figure 5 shows the relationship between the eﬀective rank and twelve learning rates. In this ﬁgure, the eﬀect
of learning rate on the diﬀerent phases of learning is distinguishable. For low learning rates, the ranks are
low because the agent can never transition from Phase 1 to Phase 2, and for large learning rates, the eﬀective
ranks are low because the agent is in Phase 3. Therefore, the distribution of eﬀective ranks and learning rates
has a Gaussian-like shape, as depicted in the ﬁgure. The distribution of rank shifts towards low learning
rates as we train the agents longer because slower models with low learning rates start entering Phase 2, and
the models trained with large learning rates enter Phase 3.

Figure 5: [Atari]: Bar charts of of eﬀective ranks with respect to the learning rates after 1M and 20M
learning steps. After 1M gradient steps, the ranks are distributed almost like a Gaussian. After 20M learning
steps the mode of the distribution of the ranks shifts towards left where the mode goes down for most games
as well. Namely as we train the network longer the rank goes down and in particular for the large learning
rates. For the low learning rates the rank is low because the model is stuck in Phase 1, the large learning
rates get into the Phase 3 quickly and thus the low ranks.

4.2 The eﬀect of dataset size

We can use the size of the dataset as a possible proxy metric for the coverage that the agent observes in the
oﬄine data. We uniformly sampled diﬀerent proportions (from 5% of the transitions to the entire dataset)
from the transitions in the RLUnplugged Atari benchmark dataset (Gulcehre et al., 2020) to understand how
the agent behaves with diﬀerent amounts of training data and whether this is a factor aﬀecting the rank of
the network.

Figure 6 shows the evolution of rank and returns over the course of training. The eﬀective rank and
performance collapse severely with low data proportions, such as when learning only on 5% of the entire

11

Preprint

dataset subsampled. Those networks can never transition from phase 1 to phase 2. However, as the proportion
of the dataset subsampled increases, the agents could learn more complex behaviors to get into phase 2. The
eﬀective rank collapses less severely for the larger proportions of the dataset, and the agents tend to perform
considerably better. In particular, we can see that in phase 1, an initial decrease of the rank correlates with an
increase in performance, which we can speculate is due to the network reducing its reliance on spurious parts
of the observations, leading to representations that generalize better across states. It is worth noting that the
ordering of the policies obtained by using the agents’ performance does not correspond to the ordering of the
policies with respect to the eﬀective rank throughout training. For example, oﬄine DQN trained on the full
dataset performs better than 50% of the dataset, while the agent trained using 50% of the data sometimes
has a higher rank. A similar observation can be made for Zaxxon, at the end of the training, the network
trained on the full dataset underperforms compared to the one trained on 50% of the data, even if the rank is
the same or higher.

Figure 6: [Atari] Dataset size: Evolution of ranks and returns as we vary the fraction of data available
for the agent to train on. We see that the agent which sees very little data collapses both in terms of rank
and performance. The agent which sees more of the data has good performance even while allowing some
shrinkage of rank during the training.

5 Interactions between rank and performance

To understand the interactions and eﬀects of diﬀerent factors on the rank and performance of oﬄine DQN,
we did several experiments and tested the eﬀect of diﬀerent hyperparameters on eﬀective rank and the
agent’s performance. Figure 3 shows the causal graph of the eﬀective rank, its confounders, and the agent’s
performance. Ideally, we would like to intervene in each node on this graph to measure its eﬀect. As the
rank is a continuous random variable, it is not possible to directly intervene on eﬀective rank. Instead, we
emulate interventions on the eﬀective rank by conditioning to threshold β with τ . In the control case (λ(1)),
we assume β > τ and for λ(0), we would have β ≤ τ .

We can write the average treatment eﬀect (ATE) for the eﬀect of setting the eﬀective rank to a large value on
the performance as:

ATE(λ, β, τ ) = E[λ|λ(1)] − E[λ|λ(0)].

(3)

Let us note that this ATE(λ, β, τ ) quantity doesn’t necessarily measure the causal eﬀect of β on λ, since we
know that the λ can be confounded by A.

We study the impact of factors such as activation function, learning rate, data split, target update steps, and
CURL and SAM losses on the Asterix and Gravitar levels. We chose these two games since Asterix is an

12

Preprint

i) ReLU

ii) tanh

iii) both

Figure 7: [Atari]: The correlation plot between the eﬀective rank and the performance (measured in terms
of episode returns by evaluating the agent in the environment) of oﬄine DQN on Asterix and Gravitar games
over 256 diﬀerent hyper-parameter conﬁgurations trained for 2M learning steps. There is a strong correlation
with the ReLU function, but the correlation disappears for the network with tanh activation function. There
is no signiﬁcant correlation between eﬀective rank and the performance on the Asterix game with the complete
data. Still, a positive correlation exists on the Gravitar game. These results are not aﬀected by the Simpson’s
paradox since the subgroups of the data when split into groups concerning activation functions, do not show
a consistent correlation.

easy-to-explore Atari game with relatively dense rewards, while Gravitar is a hard-to-explore and a sparse
reward setting (Bellemare et al., 2016). In Table 1, we present the results of intervening to β thresholded with
diﬀerent quantiles of the eﬀective rank. Choosing a network with a high eﬀective rank for a ReLU network
has a statistically signiﬁcant positive eﬀect on the agent’s performance concerning diﬀerent quantiles on
both Asterix and Gravitar. The agent’s performance is measured in terms of normalized scores as described
in Gulcehre et al. (2020). However, the results with the tanh activation function are mixed, and the eﬀect
of changing the rank does not have a statistically signiﬁcant impact on the performance in most cases. In
Figure 7, we show the correlations between the rank and performance of the agent. The network with ReLU
activation strongly correlates rank and performance, whereas tanh does not. The experimental data can
be prone to Simpson’s paradox which implies that a trend (correlation in this case) may appear in several
subgroups of the data, but it disappears or reverses when the groups are aggregated (Simpson, 1951). This can
lead to misleading conclusions. We divided our data into equal-sized subgroups for hyperparameters and
model variants, but we could not ﬁnd a consistent trend that disappeared when combined, as in Figure 7.
Thus, our experiments do not appear to be aﬀected by Simpson’s paradox.

13

Preprint

Table 1: Atari: The average treatment eﬀect of having a network with a higher rank than concerning
diﬀerent quantiles. We report the average treatment eﬀect (ATE), its uncertainty (using 95% conﬁdence
intervals using standard errors), and p-values for Asterix and Gravitar games. Higher eﬀective rank seems
to have a smaller eﬀect on Asterix than Gravitar. Let us note that, Gravitar is a sparse reward problem,
and Asterix is a dense-reward one. Overall the eﬀect is more prominent for ReLU than tanh, and with tanh
networks, it is not statistically signiﬁcant. We boldfaced the ATEs where the eﬀect is statistically signiﬁcant
(p < 0.05.) The type column of the table indicates the activation functions used in the experiments we did
the intervention. The “Combined” type corresponds to a combination of tanh and ReLU experiments.

Level

Type

Asterix

Gravitar

Combined
ReLU
Tanh
Combined
ReLU
Tanh

quantile=0.25
ATE
0.019 ± 0.010
0.079 ± 0.018
-0.014 ± 0.004
0.954 ± 0.077
1.654 ± 0.150
0.028 ± 0.090

quantile=0.5

quantile=0.75

quantile=0.95

p-value ATE
0.001
0.000
0.999
0.000
0.000
0.303

0.007 ± 0.013
0.089 ± 0.025
0.009 ± 0.005
0.569 ± 0.098
1.146 ± 0.163
0.185 ± 0.118

p-value ATE
0.207
0.000
0.996
0.000
0.000
0.005

0.005 ± 0.019
0.112 ± 0.040
-0.005 ± 0.006
0.496 ± 0.141
1.105 ± 0.256
0.242 ± 0.167

p-value ATE
0.324
0.000
0.886
0.000
0.000
0.009

-0.031 ± 0.011
0.110 ± 0.085
0.020 ± 0.017
0.412 ± 0.206
1.688 ± 0.564
0.054 ± 0.265

p-value
0.999
0.017
0.023
0.001
0.000
0.367

Table 2: [Atari] Average Treatment Eﬀect of diﬀerent interventions on Asterix and Gravitar: Quantity
of interest Y is the terminal rank of the features. Activation function and learning rate have the most
considerable eﬀect on the terminal feature ranks in our setup. We indicate them with boldface; changing the
activation function from ReLU to tanh improves the eﬀective rank, whereas changing the learning rate from
3 × 10−4 to 3 × 10−5 reduces the rank.

u
Activation Function
Learning Rate
Target Update Steps
SAM Loss Weight
Level
CURL Loss Weight
Data Split

Control (u)
ReLU
3 × 10−4
2500
0
Asterix
0
100%

Treatment T(u)
tanh
3 × 10−5
200
0.05
Gravitar
0.001
5%

Yt(u) − Yc(u)
66.97 ± 7.13
-54.15 ± 7.54
-15.23 ± 8.21
-10.05 ± 8.26
-9.41 ± 8.25
8.09 ± 8.27
5.11 ± 8.27

In this analysis, we set our control setting to be
trained with ReLU activations, the learning rate of
3e − 4, without any auxiliary losses, with training
done on the full dataset in the level Asterix. We
present the Average Treatment Eﬀect (ATE) (the
diﬀerence in ranks between the intervention being
present and absent in an experiment) of changing
each of these variables in Table 3. We ﬁnd that
changing the activation function from ReLU to tanh
drastically aﬀects the eﬀective ranks of the features,
which is in sync with our earlier observations on
other levels.
In Figure 8, we present a heatmap
of ATEs where we demonstrate the eﬀective rank
change when two variables are changed from original
control simultaneously. Once again, the activation
functions and learning rate signiﬁcantly aﬀect the
terminal ranks. We also observe some interesting
combinations that lead the model to converge to
a lower rank – for example, using SAM loss with
dropout. These observations further reinforce our
belief that diﬀerent factors aﬀect the phenomenon.

Figure 8: [Atari] ablations: Eﬀects of diﬀerent pairs
of interventions on the control set. The control sam-
ple is on level Asterix with no dropout, curl and loss
smoothing with the full dataset and a target update
period of 2500 steps. Overall, changing the activation
function and learning rate has the largest eﬀect on the
rank.

14

Activation(tanh)

186.1

Learning Rate(3e-5)

94.9 -74.9

SAM Weight(0.05)

71.5

77.3

91.8

CURL Weight(0.001)

165.1 -78.8 -27.1 -50.1

Dropout Rate(0.5)

123.9 -67.0 109.3 -64.6 70.8

Dataset Size(100%)

138.3 -80.1 55.7 -47.1 99.3 -49.5

Target Update Steps(200)

70.2 -78.1 55.3 -70.5 57.5

-2.4 -40.3

Level Name(Gravitar)

30.3 -68.6 -64.0 46.7 -35.9 -61.5 -78.3 -59.7

150

100

50

0

−50

)
h
n
a
t
(
n
o
i
t
a
v
i
t
c
A

)
5
-
e
3
(
e
t
a
R
g
n
n
r
a
e
L

i

)
5
0
.
0
(
t
h
g
e
W
M
A
S

i

)
1
0
0
.
0
(
t
h
g
e
W
L
R
U
C

i

)
5
.
0
(
e
t
a
R

t
u
o
p
o
r
D

)

%
0
0
1
(
e
z
S

i

t
e
s
a
t
a
D

)
r
a
t
i
v
a
r
G
(
e
m
a
N

l

e
v
e
L

)
0
0
2
(
s
p
e
t
S
e
t
a
d
p
U

t
e
g
r
a
T

 
 
 
 
 
 
 
 
Preprint

The extent of rank collapse and mitigating rank collapse alone may never fully ﬁx the agent’s learning ability
in diﬀerent environments.

6 The eﬀect of activation functions

The severe rank collapse of phase 3 is apparent in our Atari models, which have simple convolutional neural
networks with ReLU activation functions. When we study the same phenomenon on the SeekAvoid dataset,
rank collapse does not seem to happen similarly. It is important to note here that to solve those tasks
eﬀectively, the agent needs memory; hence, all networks have a recurrent core and LSTM. Since standard
LSTMs use tanh(·) activations, investigating in this setting would help us understand the role of the choice
of architecture on the behavior of the model’s rank.

Figure 10 shows that the output features of the LSTM network on the DeepMind lab dataset do not experience
any detrimental eﬀective rank collapse with diﬀerent exploration noise when we use tanh(·) activation function
for the cell. However, if we replace the tanh(·) activation function of the LSTM cell with ReLU or if we
replace the LSTM with a feed-forward MLP using ReLU activations (as seen in Figure 10) the eﬀective
rank in both cases, collapses to a small value at the end of training. This behavior shows that the choice of
activation function has a considerable eﬀect on whether the model’s rank collapses throughout training and,
subsequently, its ability to learn expressive value functions and policies in the environment as it is susceptible
to enter phase 3 of training.

Observation: Agents that have networks with ReLU units tend to have dead units which causes
the eﬀective rank to collapse in phase 3 of learning while other activations like tanh do not suﬀer a
similar collapse.

The activation functions inﬂuence both the network’s learning dynamics and performance. As noted by
Pennington and Worah (2017), the activation function can inﬂuence the rank of each layer at initialization.
Figure 9 presents our ﬁndings on bsuite levels. In general, the eﬀective rank of the penultimate layer with
ReLU activations collapses faster, while ELU and tanh(·) tend to maintain a relatively higher rank than
ReLU. As the eﬀective rank goes down for the catch environment, the activations become sparser, and the
units die. We illustrate the sparsity of the activations with Gram matrices over the features of the last hidden
layer of a feedforward network trained on bsuite in Figure 37 in Appendix A.14.

Figure 9: [bsuite] Eﬀective ranks of diﬀerent activation functions: The magnitude of drop in eﬀective
rank is more severe for ReLU and sigmoid activation functions than tanh.

7 Optimization

The inﬂuence of minibatch size and the learning rate on the learning dynamics is a well-studied phenomenon.
Smith and Le (2017); Smith et al. (2021) argued that the ratio between the minibatch size and learning rate
relates to implicit regularization of SGD and also aﬀects the learning dynamics. Thus, it is evident that
those factors would impact the eﬀective rank and performance. Here, we focus on a setup where we see the
correlation between eﬀective rank and performance and investigate how it emerges.

15

Preprint

Figure 10: [DeepMind lab SeekAvoid] Activation functions: Evolution of ranks in a typical LSTM
network with tanh activations at the cell of LSTM on DeepMind lab. We see that a typical LSTM network
does not get into Phase 3. However, when we change the activation function of the cell gate from tanh
to ReLU the eﬀective rank collapses to very small values. The eﬀective rank collapses when the LSTM is
replaced with a feedforward network too in cases of ReLU activation.

Our analysis and ablations in Table 3 and Figure 8 illustrate that the learning rate is one of the most
prominent factors on the performance of the agent. In general, there is no strong and consistent correlation
between the eﬀective rank and the performance across diﬀerent models and hyperparameter settings. However,
in Figure 1, we showed that the correlation exists in a speciﬁc setting with a particular minibatch size and
learning rate. Further, in Figure 7, we narrowed down that the correlation between the eﬀective rank and
the performance exists for the oﬄine DQN with ReLU activation functions. Thus in this section, we focus
on a regime where this correlation exists with the oﬄine DQN with ReLU and investigate how the learning
rate and minibatches aﬀect the rank. We ran several experiments to explore the relationship between the
minibatch size, learning rates, and rank. In this section, we report results on Atari in few diﬀerent settings –
Atari-DQN-256-2M, Atari-DQN-32-100M, Atari-DQN-256-20M and Atari-BC-256-2M.

The dying ReLU problem is a well-studied issue in supervised learning (Glorot et al., 2011; Gulcehre et al.,
2016) where due to the high learning rates or unstable learning dynamics, the ReLU units can get stuck in the
zero regime of the ReLU activation function. We compute the number of dead ReLU units of the penultimate
layer of a network as the number of units with zero activation value for all inputs. Increasing the learning
rate increases the number of dead units as can be seen in Figures 11 and 12. Even in BC, we observed that
using large learning rates can cause dead ReLU units, rank collapse, and poor performance, as can be seen in
Figure 13, and hence this behavior is not unique to oﬄine RL losses. Nevertheless, models with TD-learning
losses have catastrophic rank collapses and many dead units with lower learning rates than BC. Let us note
that the eﬀective rank depends on the number of units (D) and the number of dead units (η) at a layer. It is
easy to see that eﬀective rank is upper-bounded by D − η. In Figure 14, we observe a strong correlation
between the number of dead ReLU units of the penultimate layer of the ReLU network and its eﬀective rank.

Observation: The pace of learning inﬂuences the number of dead units and the eﬀective rank: the
larger the learning rates and the smaller minibatches are, the number of dead units increases, and the
eﬀective rank decreases. However, the performance is only poor when the eﬀective rank is severely
low.

Finally, we look into how eﬀective ranks shape up towards the end of training. We test 12 diﬀerent
learning rates, trying to understand the interaction between the learning rates and the eﬀective rank of
the representations. We summarize our main results in Figure 5 where training oﬄine DQN decreases the

16

Preprint

Figure 11: [Atari DQN-32-100M] Learning curves of oﬄine DQN for 100M gradient steps of training
with minibatch size of 32. Increasing the learning rate increases the number of dead units in the ReLU
network. As a result, the increased learning rate also causes more severe collapse as well, which aligns very
well with the number of dead units. We observed that this behavior can also happen with networks using
saturating activation functions such as sigmoid.

eﬀective rank. The eﬀective rank is low for both small and large learning rates. For higher learning rates, as
we have seen earlier, training for longer leads to many dead ReLU units, which in turn causes the eﬀective
rank to diminish, as seen in Figures 11 and 12. Moreover, as seen in those ﬁgures, in Phase 1, the eﬀective
rank and the number of dead units are low. Thus, the rank collapse in Phase 1 is not caused by the number
of dead units. In Phase 3, the number of dead units is high, but the eﬀective rank is drastically low. The
drastically low eﬀective rank is caused by the network’s large number of dead units in Phase 3. In Phase 3,
we believe that both the under-parameterization and the poor performance is caused by the number of dead
units in ReLU DQN, which was shown to be the case by (Shin and Karniadakis, 2020) in supervised learning.

Overall, we could only observe a high correlation between the eﬀective rank and the agent’s performance,
when we use ReLU activations in the network after a long training. We also present more analysis on how
controlling the loss landscape aﬀects the rank vs performance in Appendix A.2 and A.4.

8 The eﬀect of loss function

8.1 Q-learning and behavior cloning

Behavior cloning (BC) is a method to learn the behavior policy from an oﬄine dataset using supervised
learning approaches (Pomerleau, 1989). Policies learned by BC will learn to mimic the behavior policy, and
thus the performance of the learned BC agent is highly limited by the quality of the data the agent is trained
on. We compare BC and Q-learning in Figure 15. We conﬁrm that with default hyperparameters, the BC
agent’s eﬀective rank does not collapse at the end of training. In contrast, as shown by Kumar et al. (2020a),

17

Preprint

Figure 12: [Atari DQN-256-3M] Learning curves of oﬄine DQN for 3M gradient steps of training
with minibatch size of 256. Increasing the minibatch size improves the performance of the network with
larger learning rates.

DQN’s eﬀective rank collapses. DQN outperforms BC even if its rank is considerably lower, and during
learning, the rank is not predictive of the performance (Figure 15). This behavior indicates the unreliability
of eﬀective rank for oﬄine model selection.

8.2 CURL: The eﬀect of self-supervision

We study whether adding an auxiliary loss term proposed for CURL (Laskin et al., 2020) (Equation 2) during
training helps the model mitigate rank collapse. In all our Atari experiments, we use the CURL loss described
in (Laskin et al., 2020) without any modiﬁcations. Since the DeepMind lab tasks require memory, we apply a
similar CURL loss to the features aggregated with a mean of the states over all timesteps. In all experiments,
we also sweep over the weight of the CURL loss λ.

Figure 16 shows the ranks and returns on Atari (for DeepMind lab results see Appendix A.6.) In Atari games,
using very large weights for an auxiliary loss (≈ 1) prevents rank collapse, but they simultaneously deteriorate
the agent performance. We speculate, borrowing on intuitions from the supervised learning literature on
the role of the rank as implicit regularizer Huh et al. (2021), that in such scenarios large rank prevents the
network from ignoring spurious parts of the observations, which aﬀects its ability to generalize. On the other
hand, moderate weights of CURL auxiliary loss do not signiﬁcantly change the rank and performance of the
agent. Previously Agarwal et al. (2021) showed that the CURL loss does not improve the performance of an
RL agent on Atari in a statistically meaningful way.

On DeepMind lab games, we do not observe any rank collapse. In none of our DeepMind lab experiments
agents enter into Phase 3 after Phase 1 and 2. This is due to the use of the tanh activation function for
LSTM based on our investigation of the role of the activation functions in Section 6.

18

Preprint

Figure 13: [Atari BC-256-2M] Learning Curves of BC After 2M gradient steps with mini-batch size
of 256. For large enough learning rates the rank of BC agent also collapses. We hypothesize that the rank
collapse is a side eﬀect of learning in general and not only due to TD-learning based losses.

Figure 14: [Atari] Scatter plot for the correlation between the number of dead units and eﬀective rank at
the end of training and we observe that the eﬀective rank strongly correlates with the number of dead units.
Also, using larger learning rate increases the number of dead units in the network.

9 Tandem RL

Kumar et al. (2020a) propose one possible hypothesis for the observed rank collapse as the re-use of the same
transition samples multiple times, particularly prevalent in the oﬄine RL setting. A setting in which this
hypothesis can be tested directly is the ‘Tandem RL’ proposed in Ostrovski et al. (2021): here a secondary
(‘passive’) agent is trained from the data stream generated by an architecturally equivalent, independently
initialized baseline agent, which itself is trained in a regular, online RL fashion. The passive agent tends to
under-perform the active agent, despite identical architecture, learning algorithm, and data stream. This

19

Preprint

Figure 15: [Atari] Oﬄine DQN and Behavior Cloning (BC): We compare BC and DQN agents on
the Atari dataset. We used the same architecture, dataset, and training protocols for both baselines. We
used the hyperparameters deﬁned in (Gulcehre et al., 2020) for comparisons. The rank of the DQN agent is
signiﬁcantly lower and achieves higher returns than BC.

Figure 16: [Atari] Auxiliary losses: Evolution of ranks as we increase the weight of auxiliary losses. We
see that a strong weight for auxiliary loss helps mitigate the rank collapse but prevents the model from
learning useful representations.

setup presents a clean ablation setting in which both agents use data in the same way (in particular, not
diﬀering in their re-use of data samples), and so any diﬀerence in performance or rank of their representation
cannot be directly attributable to the reuse of data.

In Figure 17, we summarize the results of a Tandem-DQN using Adam (Kingma and Ba, 2014) and RMSProp
(Tieleman et al., 2012) optimizers. Despite the passive agent reusing data in a similar fashion as the online
agent, we observe that it collapses to a lower rank. Besides, the passive agent’s performance tends to
be signiﬁcantly (in most cases, catastrophically) worse than the online agent that could not satisfactorily
explained just by the extent of diﬀerence in their eﬀective ranks alone. We think that the Q-learning

20

Preprint

Figure 17: Atari, tandem RL: We investigate the eﬀect of the choice of optimizers between Adam and
RMSProp on rank and the performance of the models, both for the active (solid lines) and passive agents
(dashed lines). We observe the rank of the passive agent is lower than the active agent both for Adam and
RMSProp.

Figure 18: Atari, Forked tandem RL: We evaluate diﬀerent loss functions in the forked tandem setting,
where the passive agent is forked from the online agent, and the online agent’s parameters are frozen. Still,
the parameters of the online agent are updated. We forked the passive agent after it had seen 50M frames
during training which is denoted with dashed lines in the ﬁgure. We observe that using both BVE and MC
return losses improves the agent’s rank, but the agent’s performance is still poor.

algorithm seems to be not eﬃcient enough to exploit the data generated by the active agent to learn complex
behaviors that would put the passive agent into Phase 2. We also noticed that, although Adam achieves
better performance than RMSProp, the rank of the model trained with the Adam optimizer tends to be lower
than the models trained with RMSProp.

In Figure 18, we investigate the eﬀect of diﬀerent learning algorithms on the rank and the performance of an
agent in the forked tandem RL setting. In the forked tandem setting, an agent is ﬁrstly trained for a fraction
of its total training time. Then, the agent is ‘forked’ into active and passive agents, and they both start with
the same network weights where the active agent is ‘frozen’ and not trained but continues to generate data
from its policy to train the passive agent on this generated data for the remaining of the training time. Here,
we see that the rank ﬂat-lines when we fork the Q-learning agent, but the performance collapses dramatically.
In contrast, despite the rank of BVE and an agent trained with on-policy Monte Carlo returns going up, the

21

breakout

pong

i

)
e
v
s
s
a
p
(

s
k
n
a
R

l

e
n
r
e
K
e
r
u
t
a
e
F

200

150

100

50

0

200

100

0

0

0

50

100

150

200

200

150

100

50

0

seaquest

space_invaders

200

100

Algorithm

BVE
Monte-Carlo
Q-learning

50

100

150

200

0

0

50

100

150

200

50

100

150

200

0
 Frames (x 1M)

 
 
 
breakout

pong

seaquest

space_invaders

i

)
e
v
s
s
a
p
(

n
r
u
t
e
R
e
d
o
s
p
E

i

400

300

200

100

0

20

10

0

−10

−20

20000

15000

10000

5000

0

0

50

100

150

200

0

50

100

150

200

0
Frames (x 1M)

1500

1000

500

Algorithm

BVE
Monte-Carlo
Q-learning

50

100

150

200

0

0

50

100

150

200

 
 
Preprint

performance still drops. Nevertheless, the decline of performance for BVE and the agent trained with Monte
Carlo returns is not as bad as the Q-learning agent on most Atari games.

10 Oﬄine policy selection

In Section 7, we found that with the large learning rate sweep setting rank and number of dead units have
high correlation with the performance. Oﬄine policy selection (Paine et al., 2020) aims to choose the best
policy without any online interactions purely from oﬄine data. The apparent correlation between the rank
and performance raises the natural question of how well rank performs as an oﬄine policy selection approach.
We did this analysis on DQN-256-20M experiments where we previously observed strong correlation between
the rank and performance. We ran the DQN network described in DQN-256-20M experimental setting until
the end of the training and performed oﬄine policy selection using the eﬀective rank to select the best learning
rate based on the learning rate that yields to maximum eﬀective rank or minimum number of dead units.

Figure 19: Atari DQN-256-20M: This plot is depicting the simple regret of a DQN agent with ReLU
activations using eﬀective rank and the number of dead units. On the left, we show the the simple regret to
select the best learning rate using the eﬀective rank and on the right hand side, we show the simple regret
achieved based on the number of dead units.

Figure 19 illustrates the simple regret on each Atari oﬄine policy selection game. The simple regret between
the recommended policy measures how close an agent is to the best policy, and it is computed as described
in Konyushkova et al. (2021). A simple regret of 1 would mean that our oﬄine policy selection mechanism
successfully chooses the best policy, and 0 would mean that it would choose the worst policy. After 2M
training steps, the simple regret with the number of dead units as a policy selection method is poor. In
contrast, the simple regret achieved by selecting the agent with the highest eﬀective rank is good. The mean
simple regret achieved by using the number of dead units as an oﬄine policy selection (OPS) method is
0.45 ± 0.11, where the uncertainty ±0.11 is computed as standard error across ﬁve seeds. In contrast, simple
regret achieved by using eﬀective rank as an OPS method is 0.24 ± 0.12. The eﬀective ranks for most learning
rates collapse as we train longer since more models enter Phase 3 of learning—the number of dead units
increases. After 20M of learning steps, the mean simple regret computed using eﬀective rank as the OPS
method becomes 0.40 ± 0.07, and the mean simple regret is 0.25 ± 0.12 with the number of dead units for the
OPS. Let us note that the 2M learning step is more typical in training agents on the RL Unplugged Atari
dataset. The number of dead units becomes a good metric to do OPS when the network is trained for long
(20M steps in our experiment), where the rank becomes drastically small for most learning rate conﬁgurations.
However, eﬀective rank seems to be a good metric for the OPS earlier in training. Nevertheless, without
prior information about the task and agent, it is challenging to conclude whether the number of dead units
or eﬀective rank would be an appropriate OPS metric. Other factors such as the number of training steps,
activation functions, and other hyperparameters confound the eﬀective rank and number of dead units. Thus,
we believe those two metrics we tested are unreliable in general OPS settings without controlling those other
extraneous factors.

22

t
e
r
g
e
R
e
p
m
S

l

i

Effective rank

Number of dead units

1.000

20M steps
2M steps

0.791

0.838

0.665

0.588

0.545

0.503

0.504

0.457

0.569

0.524

0.359

0.309

0.665

0.539

0.393

0.411

0.423

0.621

0.542

0.446

0.190

0.079

k
c
a

t
t

A
n
o
m
e
D

0.000

y
e
k
c
o
H
e
c
I

r
e
n
n
u
R
d
a
o
R

0.021

n
o
x
x
a
Z

i

r
e
d
R
m
a
e
B

n
a
m
c
a
P
s
M

0.009
n
a
y
o
o
P

0.035

0.000

k
n
a

t

o
b
o
R

k
n
u
D
e
b
u
o
D

l

0.000

0.000

0.026

k
c
a

t
t

A
n
o
m
e
D

y
e
k
c
o
H
e
c
I

r
e
n
n
u
R
d
a
o
R

0.000
0.000
n
o
x
x
a
Z

0.000

i

r
e
d
R
m
a
e
B

0.075

n
a
m
c
a
P
s
M

n
a
y
o
o
P

0.000

k
n
a

t

o
b
o
R

k
n
u
D
e
b
u
o
D

l

 
Preprint

In Figure 20, we compare eﬀective rank as an OPS
method with respect to its percentage improvement
over the online policy selection to maximize the me-
dian reward achieved by the each agent across nine
Atari games. The eﬀective rank on Zaxxon, Beam-
Rider, MsPacman, Pooyan, Robotank, DoubleDunk
perform competitively to the online policy selection.
Eﬀective rank may be a complementary tool for net-
works with ReLU activation function and we believe
it can be a useful metric to monitor during the train-
ing in addition to number of dead units to have a
better picture about the performance of the agent.

11 Robustness to input perturbations

Figure 20: Atari DQN-256-20M: Here, we are de-
picting the percentage of improvement by doing oﬄine
policy selection using rank individually vs online policy
selection using median reward across nine Atari games
to select the best learning rate. Using eﬀective rank as
the oﬄine policy selection method performs relatively
well when compared to doing online policy selection
based on median normalized score across Atari games.

Several works, such as Sanyal et al. (2018) suggested
that low-rank models can be more robust to input
perturbations (speciﬁcally adversarial ones). It is
diﬃcult to just measure the eﬀect of low-rank rep-
resentations on the agent’s performance since rank
itself is not a robust measure, as it depends on diﬀer-
ent factors such as activation function and learning
which in turn can eﬀect the generalization of the
algorithm independently from rank. However, it is
easier to validate the antithesis, namely “Do more
robust agents need to have higher eﬀective rank than
less robust agents?”. We can easily test this hypothesis by comparing DQN and BC agents.

Robustness metric: We deﬁne the robustness metric ρ(p) based on three variables: i) the noise level p,
ii) the noise distribution d(p), and iii) the score obtained by agent when evaluated in the environment with
noise level p: score[d(p)] for which score[d(0)] represents the average score achieved by the agent without
any perturbation applied on it. Then we can deﬁne ρ(p) as follows:

ρ(p) = 1 −

score[d(0)] − score[d(p)]
score[d(0)]

=

score[d(p)]
score[d(0)]

.

(4)

In our experiments, we compare DQN and BC agents since we already know that BC has much larger ranks
across all Atari games than DQN. We trained these agents on datasets without any data augmentation. The
data augmentations are only applied on the inputs when the agent is evaluated in the environment. We
evaluate our agents on BeamRider, IceHockey, MsPacman, DemonAttack, Robotank and RoadRunner games
from RL Unplugged Atari online policy selection games. We excluded DoubleDunk, Zaxxon and Pooyan
games since BC agent’s performance on those levels was poor and very close to the performance of random
agent, so the robustness metrics on those games would not be very meaningful. On IceHockey, the results
can be negative, thus we shifted the scores by 20 to make sure that they are non-negative. In our robustness
experiments, we used the hyperparameters that were used in Gulcehre et al. (2020) on Atari for both for
DQN and BC.

In Figure 21, we investigate the Robustness of DQN and BC agents with
Robustness to random shifts:
respect to random translation image perturbations applied to the observations as described by Yarats et al.
(2020). We evaluate the agent on the Atari environment with varying degrees of input scaling. We noticed
that BC’s performance deteriorates abruptly, whereas the performance of DQN, while also decreasing, is
better than the BC one.

23

0
8

0
6

74.074

t
n
e
m
e
v
o
r
p
m

I

f
o

e
g
a
t
n
e
c
r
e
P

0
4

0
2

0

0
2
−

4.159

4.450

-3.332

-7.086

-18.605

-17.358

0
4
−

0
6
−

-54.037

-49.712

k
c
a

t
t

A
n
o
m
e
D

y
e
k
c
o
H
e
c
I

r
e
n
n
u
R
d
a
o
R

n
o
x
x
a
Z

i

r
e
d
R
m
a
e
B

n
a
m
c
a
P
s
M

n
a
y
o
o
P

k
n
a

t

o
b
o
R

k
n
u
D
e
b
u
o
D

l

 
 
Preprint

Figure 21: [Atari] Robustness to random shifts during evaluation: We measure the robustness of
BC and DQN to random shifts on six Atari games from RL Unplugged. The DQN and BC agents are
trained oﬄine without any perturbations on the inputs, only on the RL Unplugged datasets. However, during
evaluation time we perturbed the input images with ""random shift"" data augmentation. Overall, DQN is
more robust than BC to the evaluation-time random-scaling image perturbations that are not seen during
training. The diﬀerence is more pronounced on IceHockey and BeamRider games. DQN achieved mean AUC
(over diﬀerent games) of 2.042 and BC got 1.26.

Figure 22: [Atari] Robustness to random scaling during evaluation: We measure the robustness of
BC and DQN to random scaling over inputs on six Atari games from RL Unplugged. The DQN and BC agents
are trained oﬄine without data augmentations over the RL Unplugged datasets. However, we perturbed
the observations during the evaluation with ""random scale"" data augmentation. We observed that DQN
is more robust than BC to the evaluation-time random-scaling data augmentation. The diﬀerence is more
pronounced in IceHockey, BeamRider, and Robotank games. DQN achieved a mean AUC (over diﬀerent
games) of 1.60, and BC achieved 0.87.

24

e
r
o
c
s

s
s
e
n
t
s
u
b
o
R

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

MsPacman

DemonAttack

IceHockey

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14

0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14

0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14

Robotank

BeamRider

RoadRunner

1.0

0.8

0.6

0.4

0.2

0.0

0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14

DQN
BC

1.0

0.8

0.6

0.4

0.2

0.0

0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14

The random shift rate

0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14

 
e
r
o
c
s

s
s
e
n
t
s
u
b
o
R

MsPacman

DemonAttack

IceHockey

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.00 0.02 0.04 0.06 0.08 0.10

0.00 0.02 0.04 0.06 0.08 0.10

0.00 0.02 0.04 0.06 0.08 0.10

Robotank

BeamRider

RoadRunner

1.0

0.8

0.6

0.4

0.2

1.0

0.8

0.6

0.4

0.2

0.0

0.00 0.02 0.04 0.06 0.08 0.10

0.0

0.00 0.02 0.04 0.06 0.08 0.10

The random scale value

DQN
BC

1.0

0.8

0.6

0.4

0.2

0.0

0.00 0.02 0.04 0.06 0.08 0.10

 
Preprint

Robustness to random scaling:
In Figure 22, we explore the Robustness of DQN and BC agents with
respect to random scaling as image perturbation method applied to the observations that are feed into our
deep Q-network. We randomly scale the image inputs as described in Chen et al. (2017). When evaluating
the agent in an online environment, we test it with varying levels of image scaling. The results are again very
similar to the random shift experiments where the DQN agent is more robust to random perturbations in the
online environment.

According to the empirical results obtained from those two experiments, it is possible to see that DQN is
more robust to the evaluation-time input perturbations, speciﬁcally random shifts and scaling than the BC
agent. Thus, more robust representations do not necessarily require a higher eﬀective rank.

12 Discussion

In this work, we empirically investigated the previously hypothesized connection between low eﬀective rank
and poor performance. We found that the relationship between eﬀective rank and performance is not as
simple as previously conjectured. We discovered that an oﬄine RL agent trained with Q-learning during
training goes through three phases. The eﬀective rank collapses to severely low values in the ﬁrst phase –we
call this as the self-pruning phase– and the agent starts to learn basic behaviors from the dataset. Then in
the second phase, the eﬀective rank starts going up, and in the third phase, the eﬀective rank collapses again.
Several factors such as learning rate, activation functions and the number of training steps, inﬂuence the
occurrence, persistence and the extent of severity of the three phases of learning.

In general, a low rank is not always indicative of poor performance. Besides strong empirical evidence, we
propose a hypothesis trying to explain the underlying phenomenon: not all features are useful for the task
the neural network is trying to solve, and low rank might correlate with more robust internal representations
that can lead to better generalization. Unfortunately, reasoning about what it means for the rank to be too
low is hard in general, as the rank is agnostic to which direction of variations in the data are being ignored or
to higher-order terms that hint towards a more compact representation of the data with fewer dimensions.

Our results indicate that an agent’s eﬀective rank and performance correlate in restricted settings, such as
ReLU activation functions, Q-learning, and a ﬁxed architecture. However, as we showed in our experiments,
this correlation is primarily spurious in other settings since it disappears with simple modiﬁcations such
as changing the activation function and the learning rate. We found several ways to improve the eﬀective
rank of the agent without improving the performance, such as using tanh instead of ReLU, an auxiliary
loss (e.g., CURL), and the optimizer. These methods address the rank collapse but not the underlying
learning deﬁciency that causes the collapse and the poor performance. Nevertheless, our results show that
the dynamics of the rank and agent performance through learning are still poorly understood; we need more
theoretical investigation to understand the relationship between those two factors. We also observed in
Tandem and oﬄine RL settings that the rank collapses to a minimal value early in training. Then there is
unexplained variance between agents in the later stages of learning. Overall, the cause and role of this early
rank collapse remain unknown, and we believe understanding its potential eﬀects is essential in understanding
large-scale agents’ practical learning dynamics. The existence of low-rank but high-performing policies suggest
that our networks can be over-parameterized for the tasks and parsimonious representations emerge naturally
with TD-learning-based bootstrapping losses and ReLU networks in the oﬄine RL setting. Discarding the
dead ReLU units might achieve a more eﬃcient inference. We believe this ﬁnding can give inspiration to a
new family of pruning algorithms.

Acknowledgements: We would like to thank Clare Lyle, Will Dabney, Aviral Kumar, Rishabh Agarwal,
Tom le Paine, Mark Rowland and Yutian Chen for the discussions. We want to thank Mark Rowland, Rishabh
Agarwal and Aviral Kumar for the feedback on the early draft version of the paper. We want to thank Sergio
Gomez and Bjorn Winckler for their help with the infrastructure and the codebase at the inception of this
project. We would like to thank the developers of Acme (Hoﬀman et al., 2020), Jax (Bradbury et al., 2018),
Optax (Hessel et al., 2020) and Haiku (Hennigan et al., 2020).

25

Preprint

References

Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on oﬄine reinforce-

ment learning. In International Conference on Machine Learning, pages 104–114. PMLR, 2020.

Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep
reinforcement learning at the edge of the statistical precipice. Advances in neural information processing
systems, 34:29304–29320, 2021.

OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub
Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand
manipulation. The International Journal of Robotics Research, 39(1):3–20, 2020.

Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization.

Advances in Neural Information Processing Systems, 32:7413–7424, 2019.

David Barrett and Benoit Dherin. Implicit gradient regularization. In International Conference on Learning

Representations, 2020.

Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew
Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801,
2016.

Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying
count-based exploration and intrinsic motivation. Advances in Neural Information Processing Systems, 29,
2016.

Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An

evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013.

Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison,
David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. DotA 2 with large scale deep reinforcement
learning. arXiv preprint arXiv:1912.06680, 2019.

James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,
George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable
transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.

Steven Bradtke and Andrew Barto. Linear least-squares algorithms for temporal diﬀerence learning. Machine

Learning, 22:33–57, 03 1996.

Liang-Chieh Chen, George Papandreou, Florian Schroﬀ, and Hartwig Adam. Rethinking atrous convolution

for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.

Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi. Batch normalization
provably avoids ranks collapse for randomly initialised deep networks. Advances in Neural Information
Processing Systems, 2020.

Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio.
Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. arXiv
preprint arXiv:1406.2572, 2014.

Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep

nets. In International Conference on Machine Learning, volume 70, pages 1019–1028. PMLR, 2017.

Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement learning.

arXiv preprint arXiv:1904.12901, 2019.

Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal

of Machine Learning Research, 6:503–556, 2005.

26

Preprint

Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for

eﬃciently improving generalization. arXiv preprint arXiv:2103.09575, 2021.

Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep

data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.

Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in actor-critic

methods. arXiv preprint arXiv:1802.09477, 2018.

Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch deep

reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019.

Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization via

Hessian eigenvalue density. arXiv preprint arXiv:1901.10159, 2019.

Xavier Glorot and Yoshua Bengio. Understanding the diﬃculty of training deep feedforward neural networks.
In International Conference on Artiﬁcial Intelligence and Statistics, pages 249–256. JMLR Workshop and
Conference Proceedings, 2010.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In International
Conference on Artiﬁcial Intelligence and Statistics, pages 315–323. JMLR Workshop and Conference
Proceedings, 2011.

Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions.

In

International Conference on Machine Learning, pages 3059–3068. PMLR, 2016.

Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gómez Colmenarejo, Konrad Zolna,
Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, et al. RL unplugged: Benchmarks for
oﬄine reinforcement learning. arXiv preprint arXiv:2006.13888, 2020.

Caglar Gulcehre, Sergio Gómez Colmenarejo, Ziyu Wang, Jakub Sygnowski, Thomas Paine, Konrad Zolna,
Yutian Chen, Matthew Hoﬀman, Razvan Pascanu, and Nando de Freitas. Regularized behavior value
estimation. arXiv preprint arXiv:2103.09575, 2021.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-
level performance on imagenet classiﬁcation. In Proceedings of the IEEE International Conference on
Computer Vision, pages 1026–1034, 2015.

Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin. Haiku: Sonnet for JAX, 2020. URL

http://github.com/deepmind/dm-haiku.

Matteo Hessel, David Budden, Fabio Viola, Mihaela Rosca, Eren Sezener, and Tom Hennigan. Optax:
composable gradient transformation and optimisation, in jax!, 2020. URL http://github.com/deepmind/
optax.

Matt Hoﬀman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara Norman,
Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Alex Novikov, Sergio Gómez
Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Andrew Cowie, Ziyu Wang, Bilal Piot, and
Nando de Freitas. Acme: A research framework for distributed reinforcement learning. arXiv preprint
arXiv:2006.00979, 2020.

Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why do deep residual networks generalize better
than deep feedforward networks?–a neural tangent kernel perspective. arXiv preprint arXiv:2002.06262,
2020.

Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The

low-rank simplicity bias in deep networks. arXiv preprint arXiv:2103.10427, 2021.

Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson. Transient
non-stationarity and generalisation in deep reinforcement learning. arXiv preprint arXiv:2006.05826, 2020.

27

Preprint

Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably eﬃcient for oﬄine RL? In International

Conference on Machine Learning, pages 5084–5096. PMLR, 2021.

Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and
Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances in neural
information processing systems, 32, 2019.

Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint
arXiv:1609.04836, 2016.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference

on Learning Representations, 2014.

Ksenia Konyushkova, Yutian Chen, Thomas Paine, Caglar Gulcehre, Cosmin Paduraru, Daniel J Mankowitz,
Misha Denil, and Nando de Freitas. Active oﬄine policy selection. Advances in Neural Information
Processing Systems, 34, 2021.

Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing oﬀ-policy Q-learning
In Conference on Neural Information Processing Systems, pages

via bootstrapping error reduction.
11761–11771, 2019.

Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit under-parameterization inhibits

data-eﬃcient deep reinforcement learning. arXiv preprint arXiv:2010.14498, 2020a.

Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for oﬄine reinforce-

ment learning. arXiv preprint arXiv:2006.04779, 2020b.

Aviral Kumar, Rishabh Agarwal, Aaron Courville, Tengyu Ma, George Tucker, and Sergey Levine. Value-
based deep reinforcement learning requires explicit regularization.
In RL for Real Life Workshop &
Overparameterization: Pitfalls and Opportunities Workshop, ICML, 2021a. URL https://drive.google.
com/file/d/1Fg43H5oagQp-ksjpWBf_aDYEzAFMVJm6/view.

Aviral Kumar, Rishabh Agarwal, Tengyu Ma, Aaron Courville, George Tucker, and Sergey Levine. Dr3:
Value-based deep reinforcement learning requires explicit regularization. arXiv preprint arXiv:2112.04716,
2021b.

Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workﬂow for oﬄine

model-free robotic reinforcement learning. arXiv preprint arXiv:2109.10813, 2021c.

Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine Learning

Research, 4:1107–1149, 2003.

Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Marco Wiering
and Martijn van Otterlo, editors, Reinforcement Learning: State-of-the-Art, pages 45–73. Springer Berlin
Heidelberg, 2012.

Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for
reinforcement learning. In International Conference on Machine Learning, pages 5639–5650. PMLR, 2020.

Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Oﬄine reinforcement learning: Tutorial, review,

and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.

Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of

neural nets. arXiv preprint arXiv:1712.09913, 2017.

Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent

on structured data. arXiv preprint arXiv:1808.01204, 2018.

28

Preprint

Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dabney. On the eﬀect of auxiliary tasks on representation
dynamics. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1–9. PMLR, 2021.

Marlos C Machado, Sriram Srinivasan, and Michael Bowling. Domain-independent optimistic initialization
for reinforcement learning. In Workshops at the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence,
2015.

Charles H Martin and Michael W Mahoney. Implicit self-regularization in deep neural networks: Evidence
from random matrix theory and implications for learning. Journal of Machine Learning Research, 22(165):
1–73, 2021.

Michael Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom
Le Paine, Konrad Zolna, Richard Powell, Julian Schrittwieser, et al. Starcraft ii unplugged: Large scale
oﬄine reinforcement learning. In Deep RL Workshop NeurIPS 2021, 2021.

Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese,
Susannah Young, Lucy Campbell-Gillingham, Geoﬀrey Irving, and Nat McAleese. Teaching language
models to support answers with veriﬁed quotes. arXiv preprint arXiv:2203.11147, 2022.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex
Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir
Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis
Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.

Hossein Mobahi, Mehrdad Farajtabar, and Peter L Bartlett. Self-distillation ampliﬁes regularization in

Hilbert space. arXiv preprint arXiv:2002.05715, 2020.

Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney,
Tor Lattimore, Csaba Szepezvari, Satinder Singh, et al. Behaviour suite for reinforcement learning. arXiv
preprint arXiv:1908.03568, 2019.

Georg Ostrovski, Pablo Samuel Castro, and Will Dabney. The diﬃculty of passive learning in deep reinforce-

ment learning. Advances in Neural Information Processing Systems, 34, 2021.

Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu
Wang, and Nando de Freitas. Hyperparameter selection for oﬄine reinforcement learning. arXiv preprint
arXiv:2007.09055, 2020.

Jeﬀrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. Advances in

Neural Information Processing Systems, 2017.

Dean A Pomerleau. ALVINN: An autonomous land vehicle in a neural network. In Conference on Neural

Information Processing Systems, pages 305–313, 1989.

Martin Riedmiller. Neural ﬁtted Q iteration – ﬁrst experiences with a data eﬃcient neural reinforcement
learning method. In João Gama, Rui Camacho, Pavel B. Brazdil, Alípio Mário Jorge, and Luís Torgo,
editors, European Conference on Machine Learning, pages 317–328, 2005.

Levent Sagun, Léon Bottou, and Yann LeCun. Singularity of the hessian in deep learning. arXiv preprint

arXiv:1611.07476, 2016.

Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian

of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.

Amartya Sanyal, Varun Kanade, Philip HS Torr, and Puneet K Dokania. Robustness via deep low-rank

representations. arXiv preprint arXiv:1804.07090, 2018.

Tianyu Shi, Dong Chen, Kaian Chen, and Zhaojian Li. Oﬄine reinforcement learning for autonomous driving

with safety and exploration enhancement. arXiv preprint arXiv:2110.07067, 2021.

29

Preprint

Yeonjong Shin and George Em Karniadakis. Trainability of relu networks and data-dependent initialization.

Journal of Machine Learning for Modeling and Computing, 1(1), 2020.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas
Hubert, Lucas Baker, Matthew Lai, and Adrian Bolton. Mastering the game of go without human knowledge.
Nature, 550(7676):354–359, 2017.

Edward H Simpson. The interpretation of interaction in contingency tables. Journal of the Royal Statistical

Society: Series B (Methodological), 13(2):238–241, 1951.

Samuel L Smith and Quoc V Le. A Bayesian perspective on generalization and stochastic gradient descent.

arXiv preprint arXiv:1710.06451, 2017.

Samuel L Smith, Benoit Dherin, David GT Barrett, and Soham De. On the origin of implicit regularization

in stochastic gradient descent. arXiv preprint arXiv:2101.12176, 2021.

Tijmen Tieleman, Geoﬀrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of

its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012.

Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-learning. In

Thirtieth AAAI conference on artiﬁcial intelligence, 2016.

Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,
David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using
multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.

Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism

for oﬄine reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021.

Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep

reinforcement learning from pixels. In International Conference on Learning Representations, 2020.

A Appendix

A.1 The impact of depth

Pennington and Worah (2017) explored the relationship between the rank and the depth of a neural network
at initialization and found that the rank of each layer’s feature matrix decreases proportionally to the index of
a layer in a deep network. Here, we test the performance of a feedforward network on the bsuite task trained
using Double Q-learning (Van Hasselt et al., 2016) with 2, 8, and 16 layers to see the eﬀect of the number
of layers on the eﬀective rank. All our networks use ReLU activation functions and use He initialization
(He et al., 2015). Figure 23 illustrates that the rank collapses as one progresses from lower layers to higher
layers proportionally at the end of training as well. The network’s eﬀective rank (rank of the penultimate
layer) drops to a minimal value on all three tasks regardless of the network’s number of layers. The last layer
of a network will act as a bottleneck; thus, a collapse of the eﬀective rank would reduce the expressivity.
Nevertheless, a deeper network with the same rank as a shallower one can learn to represent a larger class of
functions (be less under-parametrized).

The agents exhibit poor performance when the eﬀective rank collapses to 1. At that point, all the ReLU units
die or become zero irrespective of input. Thus on bsuite, deeper networks –four and eight layered networks–
performed worse than two layered MLP.

A.2 Spectral density of hessian for oﬄine RL

Analyzing the eigenvalue spectrum of the Hessian is a common way to investigate the learning dynamics
and the loss surface of deep learning methods (Ghorbani et al., 2019; Dauphin et al., 2014). Understanding
Hessian’s loss landscape and eigenvalue spectrum can help us design better optimization algorithms. Here,

30

Preprint

(L)

(C)

(R)

Figure 23: [bsuite] The ranks and depths of the networks: The evolution of the ranks across diﬀerent
layers of deep neural networks. The ﬁgure on the left (L) is for the ranks of catch across diﬀerent layers. The
ﬁgure at the center (C) is for the ranks of mountain_car across diﬀerent layers. The ﬁgure on the right (R)
is for cartpole.

we analyze the eigenvalue spectrum of a single hidden layer feedforward network trained on the bsuite Catch
dataset from RL Unplugged (Gulcehre et al., 2020) to understand the loss-landscape of a network with a low
eﬀective rank compared to a model with higher rank at the end of the training. As established in Figure
24, ELU activation function network has a signiﬁcantly higher eﬀective rank than the ReLU network. By
comparing those two networks, we also look into the diﬀerences in the eigenvalue spectrum of a network with
high and low rank. Since the network and the inputs are relatively low-dimensional, we computed the full
Hessian over the dataset rather than a low-rank approximation. The eigenvalue spectrum of the Hessian
with ReLU and the ELU activation functions is shown in Figure 24. The rank collapse is faster for ReLU
than ELU. After 900k gradient updates, the ReLU network concentrates 92% of the eigenvalues of Hessian
around zero; this is due to the dead units in ReLU network (Glorot et al., 2011). On the other hand, the
ELU network has a few very large eigenvalues after the same number of gradient updates. In Figure 25, we
summarize the distribution of the eigenvalues of the Hessian matrices of the ELU and ReLU networks. As a
result, the Hessian and the feature matrix of the penultimate layer of the network will be both low-rank.
Moreover, in this case, the landscape of the ReLU network might be ﬂatter than the ELU beyond the notion
of wide basins (Sagun et al., 2016). This might mean that the ReLU network ﬁnds a simpler solution. Thus,
the ﬂatter landscape is conﬁrmed by the simpler function learned and less capacity used at the end of the
training, which is induced by the lower rank representations.

Figure 24: [bsuite Catch] spectral density of Hessian: The visualization of the spectral density of the
full Hessian of a network trained with 64 units using ReLU (left) and ELU (right) activation functions. The
eigenvalues of the Hessian of the oﬄine DQN with ReLU activation function are concentrated around 0 and
most of them are less than 1. The eigenvalues of the ELU network also concentrate around 0 with a few large
outlier eigenvalues.

31

k
n
a
R

35

30

25

20

15

10

5

0

2 Layer Net
8 Layer Net
16 Layer Net

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

Network's Depth

k
n
a
R
e
v
i
t
c
e
f
f

E

6

5

4

3

2

1

0

2 Layer Net
8 Layer Net
16 Layer Net

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Network's Depth

 
k
n
a
R
e
v
i
t
c
e
f
f

E

6

5

4

3

2

1

0

2 Layer Net
8 Layer Net
16 Layer Net

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Network's Depth

 
y
t
i
s
n
e
D

2

10

0

10

−2

10

−4

10

−6

10

−8

10

−10

10

Learner steps = 0
Learner steps = 900k

−0.1

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

Eigenvalue

y
t
i
s
n
e
D

2

10

0

10

−2

10

−4

10

−6

10

−8

10

−10

10

Learner steps = 0
Learner Steps = 900k

0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

Eigenvalue

Preprint

Figure 25: [bsuite Catch] Hessian eigenvalues (evs) for oﬄine DQN: We visualize the percentages of
positive, negative, and near-zero eigenvalues of the Hessian for oﬄine DQN with ELU and ReLU activation
functions on the catch dataset from bsuite. If the absolute value of an eigenvalue is less than 1e-7, we consider
it as a near-zero eigenvalue. We can see that for ELU network, near-zero, positive and negative eigenvalues
are almost evenly distributed. However, with ReLU network majority of eigenvalues are near-zero (90% of
the evs are exactly zero), very few negative (2 %) and some positive eigenvalues (7.1 %).

A.3 DeepMind lab: performance vs the eﬀective ranks

In Figure 26, we show the correlation between the eﬀective rank and the performance of R2D2, R2D2 trained
with CURL and R2D2 trained with SAM. When we look at each variant separately or as an aggregate, we
don’t see a strong correlation between the performance and the rank of an agent.

Figure 26: [DeepMind lab] The eﬀective rank and the performance: Correlations between feature
ranks and episode returns for diﬀerent exploration noises on DeepMind lab dataset. We include data from
three models: R2D2, R2D2 trained with CURL, and R2D2 trained with SAM. We do not observe a strong
correlation between the eﬀective rank and performance across diﬀerent noise exploration levels in the data.

A.4 SAM optimizer: does smoother loss landscape aﬀect the behavior of the rank collapse?

We study the relationship between smoother loss landscapes and rank collapse. We use Sharpness Aware
Minimization (SAM) (Foret et al., 2021) loss for potentially creating ﬂatter loss surfaces in order to see if
smoother loss landscapes aﬀect the rank and performance dynamics diﬀerently. Figures 27 and 28 show the
evolution of feature ranks and generalization performance in Atari and DeepMind lab respectively. We do not
observe a very clear relation between the extent of smoothing the loss and the feature ranks or generalization
performance.

32

% near-zero evs

100

80

60

40

20

0

% positive evs

% negative evs

ReLU DQN
ELU DQN

Eps: 0.0, Kendall τ:0.25, Pearson ρ:0.30

Eps: 0.01, Kendall τ:0.09, Pearson ρ:0.05

Eps: 0.1, Kendall τ:0.32, Pearson ρ:0.41

Eps: 0.25, Kendall τ:0.14, Pearson ρ:0.13

Variant
CURL
R2D2
SAM

s
k
n
a
R
e
v
i
t
c
e
f
f

E

l

i

a
n
m
r
e
T

200

100

0

0

2.5

5

7.5

10

0

5

10

15

0

10

20

30

0

10

20

30

40

Terminal Episode Return

 
 
Preprint

Figure 27: [Atari] Sharpness Aware Minimization (SAM) Loss: Evolution of ranks as we increase the
weight of auxiliary losses. We see that some amount of weight on the SAM loss helps mitigate the extent of
rank collapse but we observe no clear relationship with the agent performance.

Figure 28: [DeepMind lab-SeekAvoid Snapshot] Sharpness Aware Minimization (SAM) Loss We
do not observe any rank collapse as we continue training with the LSTM networks (because of the tanh
activations discussed in Section 6) used in DeepMind lab dataset over a spectrum of diﬀerent weights for the
SAM loss.

A.5 Eﬀective rank and the value error

A curious relationship is between the eﬀective rank and the value error, because a potential for the rank
collapse or Phase 3 with the TD-learning algorithms can be the errors propagating thorough the bootstrapped
targets. Figure 29 shows the correlation between the value error and the eﬀective ranks. There is a strong
anti-correlation between the eﬀective ranks and the performance of the agents except on the levels where the
expert agent that generated the dataset performs poorly at the end of the training (e.g. IceHockey.) This

33

Preprint

Figure 29: [Atari]: These plots shows the correlation between the value error and the eﬀective rank for
oﬄine DQN agent trained for 20M steps on online policy selection games for Atari. There is an apparent
anti-correlation between the eﬀective rank and the value error. Namely, as the value error of an agent when
evaluated in the environment increases the eﬀective rank decreases. The correlation is signiﬁcant on most
Atari levels except IceHockey where the expert agent that generated the dataset performs poorly.

makes the hypothesis that the extrapolation error can cause rank collapse (or push the agent to Phase 3)
more plausible.

A.6 CURL on DeepMind Lab

In Figure 30 shows the CURL results on DeepMind lab dataset. We couldn’t ﬁnd any consistent pattern
across various CURL loss weights.

Figure 30: [DeepMind lab-SeekAvoid:] auxiliary losses Evolution of ranks as we increase the weight of
auxiliary loss. While some auxiliary loss helps the model perform well, there is no clear correlation between
rank and performance.

34

Preprint

A.7 Learning rate evolution

In Figure 31, we also perform a hyperparameter selection by evaluating the model in the environment at
various stages during the training. As the oﬄine DQN is trained longer, the optimal learning rate for the
best agent performance when evaluated online goes down. As one increases the number of training steps of
an agent, we need to change the learning rate accordingly since the number of training steps aﬀects the best
learning rate.

Figure 31: [Atari]: Evolution of the optimal learning rate found by online evaluations in the environment.
As the model is trained longer the optimal learning rate found by online evaluations goes down.

A.8 Learning curves long training regime and the diﬀerent phases of learning

In this subsection, we investigate the eﬀect of changing learning rates on the eﬀective rank and the performance
of the agent on RL Unplugged Atari online policy selection games. We train the oﬄine DQN for 20M learning
steps which is ten times longer than typical oﬄine Atari agents (Gulcehre et al., 2020). We evaluated twelve
learning rates in (cid:2)10−2, 10−5(cid:3) equally spaced in logarithmic space. We show the eﬀective rank learning and
performance curves in Figure 32. It is easy to identify diﬀerent phases of learning in most of those curves.

A.9 Eﬀective rank and the performance

Figure 33 shows the correlation between the eﬀective rank and the performance of an agent trained with
minibatch of size 32. On most Atari online policy selection games it is possible to see a very strong correlation
but on some games the correlation is not there. It seems like even

Figure 34 depicts the correlation between the eﬀective rank and the performance of a DQN agent with
ReLU activation function. There is a signiﬁcant correlation on most Atari games. As we discussed earlier,
long training setting with ReLU activation functions where the eﬀect of the rank is the strongest on the
performance.

A.10

Interventions to eﬀective rank and randomized controlled trials (RCT)

As the rank is a continuous variable, we can ﬁlter out the experiments with certain ranks with a threshold τ .
In the control case (λ(1)), we assume β > τ and for λ(0), we would have β ≤ τ .

We can also write this as:

E[λ|do(β > τ )] − E[λ|do(β ≤ τ )].

(5)

A.11 Computation of feature ranks

Here, we present the Python code-stub that we used across our experiments (similar to Kumar et al. (2020a))
to compute the feature ranks of the pre-output layer’s features:

import numpy as np
def compute_rank_from_features(feature_matrix, rank_delta=0.01):

35

0.00030

0.00025

0.000231

e
t
a
R
g
n
n
r
a
e
L

i

0.00020

0.00015

0.00010

0.00005

0.000066

0.000019

1e+05 1.2e+06 2.4e+06 3.6e+06 4.8e+06 6e+06 7.2e+06 8.4e+06 9.6e+061.08e+071.2e+071.32e+071.44e+07
Learning Steps

 
Preprint

Figure 32: [Atari] Eﬀective rank curves: Eﬀective rank and performance oﬄine DQN agent across nine
Atari online policy selection games. We train the oﬄine DQN agent for 20M learning steps and evaluated 12
learning rates in (cid:2)10−2, 10−5(cid:3) equally spaced in logarithmic space. Let us note that in all games rank goes
down early in the training (Phase 1), then goes up (phase 2) and for some learning rate the eﬀective rank
collapses (Phase 3). Correspondingly, the performance is low in the beginning of the training (Phase 1), goes
up and stays high for a while (Phase 2), and sometimes it the performance collapses (Phase 3).

36

Preprint

Figure 33: [Atari] The correlation between ranks and returns for DQN trained with minibatch size of 32. We
ran each network with three learning rates and ﬁve diﬀerent seeds but we only show the mean across those
ﬁve seeds here. We can see very strong correlations on some games, but that correlation is not consistent.

Figure 34: [Atari] The correlation between ranks and returns for DQN trained for a network trained for 20M
learning steps and 12 diﬀerent learning rates with minibatch size of 256. We can see that there is signiﬁcant
positive correlation between the rank and the learning rates for most online policy selection games.

37

BeamRider, Spearman corr:      0.75

DemonAttack, Spearman corr:      0.87

200

DoubleDunk, Spearman corr:      0.44

IceHockey, Spearman corr:     -0.06

1000

1500

2000

100

50

0

1.8

1.4

1

s
k
n
a
R
e
v
i
t
c
e
f
f

E

l

i

a
n
m
r
e
T

−16
−8
RoadRunner, Spearman corr:     -0.48

−12

−4

12

9

6

3

0

0
MsPacman, Spearman corr:      0.59

10000

5000

0
Robotank, Spearman corr:      0.46

1000

2000

3000

150

100

50

0

75

50

25

0

75

50

25

0

1.15

1.10

1.05

1

100

90

80

70

60

50

90

60

30

0

−24

−22
Pooyan, Spearman corr:     -0.33

−22.5

−23.5

−23

Learning Rates

3e-05
0.0001
0.0003

0

Zaxxon, Spearman corr:      0.80

100

300

200

0

250

500

750

0

20

40
Episode return

60

0

2000

4000

6000

8000

 
 
Preprint

Hyper-parameters
Training batch size
Rank calculation batch size
Num training steps
Learning rate
Optimizer
Feedforward hidden layer size
Num hidden layers
Activation

Memory
Discount

bsuite
32
512
1e6
3e-4
Adam
64
2
ReLU

None
0.99

Atari
256
512
2e6
3e-5
Adam
512
1
ReLU

None
0.99

DeepMind lab
4 (episodes)
512
2e4
1e-3
Adam
256
1
ReLU
LSTM gates)
LSTM
0.997

(tanh

for

Table 3: The default hyper-parameters used in our work across diﬀerent domains.

""""""Computes rank of the features based on how many singular values are significant.""""""

sing_values = np.linalg.svd(feature_matrix, compute_uv=False)
cumsum = np.cumsum(sing_values)
nuclear_norm = np.sum(sing_values)
approximate_rank_threshold = 1.0 - rank_delta
threshold_crossed = (

cumsum >= approximate_rank_threshold * nuclear_norm)

effective_rank = sing_values.shape[0] - np.sum(threshold_crossed) + 1
return effective_rank

A.12 Hyperparameters

Here, we list the standard set of hyper-parameters that were used in diﬀerent domains: bsuite, Atari, and
DeepMind Lab respectively. These are the default hyper-parameters, which may diﬀer when stated so in our
speciﬁc ablation studies. For the DMLLAB task, we use the same network that was used in Gulcehre et al.
(2021). For all the Atari tasks, we use the same convolution torso that was used in Gulcehre et al. (2020)
which involves three layers of convolution with ReLU activations in between.

• Layer 1 - Conv2d(channels=32, kernel_shape=[8, 8], stride=[4, 4])

• Layer 2 - Conv2d(channels=64, kernel_shape=[4, 4], stride=[2, 2])

• Layer 3 - Conv2d(channels=64, kernel_shape=[3, 3], stride=[1, 1])

A.13 bsuite phase transitions and bottleneck capacity

We illustrate the phase transition of a simple MLPs with ReLU activations. In Figure 35, we have a network
of size (64, bottleneck units, 64) where we vary the number of bottleneck units. In Figure 36, we have a
network of size (64, bottleneck units) where we vary the number of bottleneck units. In both cases, having
smaller number of bottleneck units reduces the performance of the mode and agents were able to solve the
problem even when the penultimate layer’s eﬀective rank was small. With the larger learning rate the right
handside ﬁgures (b), the eﬀective ranks tend to be lower.

A.14 Activation sparsity on bsuite

In Figure 37, we show that the activations of the ReLU network becomes very sparse during the course of
training. The sparsity of the ReLU units seems to be signiﬁcantly higher than the ELU units at the end of
training.

38

Preprint

a)

b)

Figure 35:
[bsuite] - Catch dataset: Phase transition plots of a network with hidden layers of size
(64, bottleneck units, 64). On the y-axis, we show the log2(bottleneck units), and x-axis is the number of
gradient steps. The ﬁgures on the left hand-side (a) is trained with the learning rate with 8e-5 and right
hand-side (b) experiments are trained with learning rate of 3e-4. The ﬁrst row shows the episodic returns.
The second row shows the eﬀective rank of the second layer (bottleneck units) and the third row is showing
the penultimate layer’s eﬀective rank. The eﬀective rank collapses much faster with the learning rate of 4e-3
than 5e-8. The low ranks can still have good performance. The low bottleneck units causes the eﬀective rank
of the last layer to collapse faster. The performance of the network with the small number of bottleneck units
is poor. The eﬀective rank of the small number of bottleneck units is smaller.

39

Preprint

Figure 36:
[bsuite] - Catch dataset: Phase transition plots of a network with hidden layers of size
(64, bottleneck units). On the y-axis, we show the log2(bottleneck units), and x-axis is the number of gradient
steps. The ﬁgures on the left hand-side (a) is trained with the learning rate with 8e-5 and right hand-side (b)
experiments are trained with learning rate of 3e-4. The ﬁrst row shows the episodic returns. The second row
shows the eﬀective rank of the second layer (bottleneck units). The eﬀective rank collapses much faster with
the learning rate of 4e-3 than 5e-8. The low ranks can still have good performance. The small number of
bottleneck units causes the eﬀective rank of the layer to collapse faster. The performance of the network with
the small number of bottleneck units is poor. The eﬀective rank of the small number of bottleneck units is
smaller.

Figure 37: [bsuite Catch] Gram matrices of activations: Gram matrices of activations of a two-layer
MLP with ReLU and ELU activation functions. The activations of the ReLU units become sparser when
compared to ELU units at the end of the training due to dead ReLU units.

40

Initialization (step 0)

Convergence (step 900k)

ReLU

ELU

","2 2 0 2 l u J 7 ] G L . s c [ 2 v 9 9 0 2 0 . 7 0 2 2 : v i X r a Preprint An empirical study of implicit regularization in deep oﬄine RL Caglar Gulcehre∗, Srivatsan Srinivasan∗, Jakub Sygnowski, Georg Ostrovski, Mehrdad Farajtabar, Matt Hoﬀman, Razvan Pascanu, Arnaud Doucet DeepMind Abstract Deep neural networks are the most commonly used function approximators in oﬄine re- inforcement learning. Prior works have shown that neural nets trained with TD-learning and gradient descent can exhibit implicit regularization that can be characterized by under- parameterization of these networks. Speciﬁcally, the rank of the penultimate feature layer, also called eﬀective rank, has been observed to drastically collapse during the training. In turn, this collapse has been argued to reduce the model’s ability to further adapt in later stages of learning, leading to the diminished ﬁnal performance. Such an association between the eﬀective rank and performance makes eﬀective rank compelling for oﬄine RL, primarily for oﬄine policy evaluation. In this work, we conduct a careful empirical study on the relation between eﬀective rank and performance on three oﬄine RL datasets : bsuite, Atari, and DeepMind lab. We observe that a direct association exists only in restricted settings and disappears in the more extensive hyperparameter sweeps. Also, we empirically identify three phases of learning that explain the impact of implicit regularization on the learning dynamics and found that bootstrapping alone is insuﬃcient to explain the collapse of the eﬀective rank. Further, we show that several other factors could confound the relationship between eﬀective rank and performance and conclude that studying this association under simplistic assumptions could be highly misleading. Contents 1 Introduction 2 Background 2.1 Eﬀective rank and implicit under-regularization . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Auxiliary losses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Deep oﬄine RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Experimental setup 4 Eﬀective rank and performance 4.1 Lifespan of learning with deep Q-networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 5 5 6 6 7 8 9 4.2 The eﬀect of dataset size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 ∗Indicates joint ﬁrst authors. 1 Preprint 5 Interactions between rank and performance 6 The eﬀect of activation functions 7 Optimization 8 The eﬀect of loss function 8.1 Q-learning and behavior cloning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2 CURL: The eﬀect of self-supervision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Tandem RL 10 Oﬄine policy selection 11 Robustness to input perturbations 12 Discussion A Appendix A.1 The impact of depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Spectral density of hessian for oﬄine RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 DeepMind lab: performance vs the eﬀective ranks . . . . . . . . . . . . . . . . . . . . . . . . . A.4 SAM optimizer: does smoother loss landscape aﬀect the behavior of the rank collapse? . . . . A.5 Eﬀective rank and the value error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 CURL on DeepMind Lab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.7 Learning rate evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.8 Learning curves long training regime and the diﬀerent phases of learning . . . . . . . . . . . . A.9 Eﬀective rank and the performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.10 Interventions to eﬀective rank and randomized controlled trials (RCT) . . . . . . . . . . . . . A.11 Computation of feature ranks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.12 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.13 bsuite phase transitions and bottleneck capacity . . . . . . . . . . . . . . . . . . . . . . . . . A.14 Activation sparsity on bsuite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 15 15 17 17 18 19 22 23 25 30 30 30 32 32 33 34 35 35 35 35 35 38 38 38 1 Introduction The use of deep networks as function approximators in reinforcement learning (RL), referred to as Deep Reinforcement Learning (DRL), has become the dominant paradigm in solving complex tasks. Until recently, most DRL literature focused on online-RL paradigm, where agents must interact with the environment to explore and learn. This led to remarkable results on Atari (Mnih et al., 2015), Go (Silver et al., 2017), StarCraft II (Vinyals et al., 2019), Dota 2 (Berner et al., 2019), and robotics (Andrychowicz et al., 2020). Unfortunately, the need to interact with the environment makes these algorithms unsuitable and unsafe for many real-world applications, where any action taken can have serious ethical or harmful consequences or be 2 Preprint costly. In contrast, in the oﬄine RL paradigm (Fu et al., 2020; Fujimoto et al., 2018; Gulcehre et al., 2020; Levine et al., 2020), also known as batch RL (Ernst et al., 2005; Lange et al., 2012), agents learn from a ﬁxed dataset previously logged by other (possibly unknown) agents. This ability makes oﬄine RL more applicable to the real world. Recently, Kumar et al. (2020a) showed that oﬄine RL methods coupled with TD learning losses could suﬀer from a eﬀective rank collapse of the penultimate layer’s activations, which renders the network to become under-parameterized. They further demonstrated a signiﬁcant fraction of Atari games, where the collapse of eﬀective rank collapse corresponded to performance degradation. Subsequently, Kumar et al. (2020a) explained the rank collapse phenomenon by analyzing a TD-learning loss with bootstrapped targets in the kernel and linear regression setups. In these simpliﬁed scenarios, bootstrapping leads to self-distillation, causing severe under-parametrization and poor performance, as also observed and analyzed by Mobahi et al. (2020). Nevertheless, Huh et al. (2021) studied the rank of the representations in a supervised learning setting (image classiﬁcation tasks) and argued that low rank leads to better performance. Thus the low-rank representations could act as an implicit regularizer. Figure 1: [Atari] The rank and the performance on broad vs narrow hyperparameter sweep: Correlation between eﬀective rank and agent’s performance towards the end of training in diﬀerent Atari games. We report the regression lines for the narrow sweep, which covers only a single oﬄine RL algorithm, with a small minibatch size (32) and a learning rate sweep similar to the hyperparameter sweep deﬁned in RL Unplugged (Gulcehre et al., 2020), whereas in the broad setting, we included more data from diﬀerent models and a larger hyperparameter sweep. In the narrow setup, there is a positive relationship between the eﬀective rank and the agent’s performance, but that relationship disappears in the broad data setup and almost reverses. Typically, in machine learning, we rely on empirical evidence to extrapolate the rules or behaviors of our learned system from the experimental data. Often those extrapolations are done based on a limited number of experiments due to constraints on computation and time. Unfortunately, while extremely useful, these extrapolations might not always generalize well across all settings. While Kumar et al. (2020a) do not concretely propose a causal link between the rank and performance of the system, one might be tempted to extrapolate the results (agents performing poorly when their rank collapsed) to the existence of such a causal 3 Preprint link, which herein we refer to as rank collapse hypothesis. In this work, we do a careful large-scale empirical analysis of this potential causal link using the oﬄine RL setting (also used by Kumar et al. (2020a)) and also the Tandem RL (Ostrovski et al., 2021) setting. The existence of this causal link would be beneﬁcial for oﬄine RL, as controlling the rank of the model could improve the performance (see the regularization term explored in Kumar et al. (2020a)) or as we investigate here, the eﬀective rank could be used for model selection in settings where oﬄine evaluation proves to be elusive. Key Observation 1: The rank and the performance are correlated in restricted settings, but that correlation disappears when we increase the range of hyperparameters and the models (Figure 1) a. aThis is because other factors like hyperparameters and architecture can confound the rank of the penultimate layer; unless the those factors are controlled carefully, the conclusions drawn from the experiments based on the rank can be misleading. Instead, we show that diﬀerent factors aﬀect a network’s rank without aﬀecting its performance. This ﬁnding indicates that unless all of these factors of variations are controlled – many of which we might still be unaware of – the rank alone might be a misleading indicator of performance. A deep Q network exhibits three phases during training. We show that rank can be used to identify diﬀerent stages of learning in Q-learning if the other factors are controlled carefully. We believe that our study, similar to others (e.g. Dinh et al., 2017), re-emphasizes the importance of critically judging our understanding of the behavior of neural networks based on simpliﬁed mathematical models or empirical evidence from a limited set of experiments. Key Observation 2: Deep Q-learning approaches goes through three phases of learning: i) simple behaviors, ii) complex behaviors, iii) under-parameterization (Figure 2). These phases can be identiﬁed by the eﬀective rank and performance on a given task a. aThe ﬁrst two phases of learning happen during the training of all models we tested. At times, the third phase of the learning could potentially lead to the agent losing its representation capacity and the ensuing poor performance. Figure 2: Lifespan of learning in deep Q-learning: The plot on the left-hand side illustrates the evolution of the eﬀective rank, and the plot on the right-hand side demonstrates the evolution of the performance during training. In the ﬁrst phase, the model learns easy-to-learn behaviors that are simplistic by nature and ignore many factors of environmental variations. The eﬀective rank collapses to a minimal value in the ﬁrst phase since the model does not need a large capacity to learn the simple behaviors. In phase 2, the model learns more complex behaviors that we identify as those that obtain large returns when the policy is evaluated in the environment. Typically, in supervised learning, phase 2 is followed by overﬁtting. However, in oﬄine RL (speciﬁcally the TD-learning approaches that we tried here), we observed that it is often followed by underﬁtting/under-parameterization in phase 3. 4 k n a R e v i t c e f f E s n r u t e R e d o s i p E Training Steps Training Steps Phases of Learning Phase 1: Simple Behaviors Phase 2: Complex Behaviors Phase 3: Underparametrization Preprint We organize the rest of the paper and our contributions in the order we introduce in the paper as follows: • Section 2 presents the related work, and Section 3 explains the experimental design that we rely on. • In Sections 4, 8, 6, 7 and 9, we study the extent of impact of diﬀerent interventions such as architectures, loss functions and optimization on the causal link between the rank and agent performance. Some interventions in the model, such as introducing an auxiliary loss (e.g. CURL (Laskin et al., 2020), SAM (Foret et al., 2021) or the activation function) can increase the eﬀective rank but does not necessarily improve the performance. This ﬁnding indicates that the rank of the penultimate layer is not enough to explain an agent’s performance. We also identify the settings where the rank strongly correlates with the performance, such as DQN with ReLU activation and many learning steps over a ﬁxed dataset. • In Section 4.1, we show that a deep Q network goes through three stages of learning, and those stages can be identiﬁed by using rank if the hyperparameters of the model are controlled carefully. • Section 5 describes the main outcomes of our investigations. Particularly, we analyze the impact of the interventions described earlier and provide counter-examples that help contradict the rank collapse hypothesis, establishing that the link between rank and performance can be aﬀected by several other confounding factors of variation. • In Section 11, we ablate and compare the robustness of BC and DQN models with respect to random perturbation introduced only when evaluating the agent in the environment. We found out that the oﬄine DQN agent is more robust than the behavior cloning agent which has higher eﬀective rank. • Section 12 presents the summary of our ﬁndings and its implications for oﬄine RL, along with potential future research directions. 2 Background 2.1 Eﬀective rank and implicit under-regularization The choice of architecture and optimizer can impose speciﬁc implicit biases that prefer certain solutions over others. The study of the impact of these implicit biases on the generalization of the neural networks is often referred to as implicit regularization. There is a plethora of literature studying diﬀerent sources of implicit regularization such as initialization of parameters (Glorot and Bengio, 2010; Li and Liang, 2018; He et al., 2015), architecture (Li et al., 2017; Huang et al., 2020), stochasticity (Keskar et al., 2016; Sagun et al., 2017), and optimization (Smith et al., 2021; Barrett and Dherin, 2020). The rank of the feature matrix of a neural network as a source of implicit regularization has been an active area of study, speciﬁcally in the context of supervised learning (Arora et al., 2019; Huh et al., 2021; Pennington and Worah, 2017; Sanyal et al., 2018; Daneshmand et al., 2020; Martin and Mahoney, 2021). In this work, we study the phenomenon of implicit regularization through the eﬀective rank of the last hidden layer of the network, which we formally deﬁne below. Deﬁnition: Eﬀective Rank Recently, Kumar et al. (2020a) studied the impact of the eﬀective rank on generalization in the general RL context. With a batch size of N and D units in the feature layer, they proposed the eﬀective rank formulation presented in Equation 1 for a feature matrix Φ ∈ RN ×D where N ≥ D that uses a threshold value of δ with the singular values σi(Φ) in descending order i.e. σ1(Φ) ≥ σ2(Φ) ≥ · · · . We provide the speciﬁc implementation of the eﬀective rank we used throughout this paper in Appendix A.11. eﬀective rankδ(Φ) = min k ( k : 5 Pk PD i=1 σi(Φ) j=1 σj(Φ) ) ≥ 1 − δ . (1) Preprint Terminology and Assumptions. Note that the threshold value δ throughout this work has been ﬁxed to 0.01 similar to Kumar et al. (2020a). Throughout this paper, we use the term eﬀective rank to describe the rank of the last hidden layer’s features only, unless stated otherwise. This choice is consistent with prior work (Kumar et al., 2020a) as the last layer acts as a representation bottleneck to the output layer. Kumar et al. (2020a) suggested that the eﬀective rank of the Deep RL models trained with TD-learning objectives collapses because of i) implicit regularization, happening due to repeated iterations over the dataset with gradient descent; ii) self-distillation eﬀect emerging due to bootstrapping losses. They supported their hypothesis with both theoretical analysis and empirical results. The theoretical analysis provided in their paper assume a simpliﬁed setting, with inﬁnitesimally small learning rates, batch gradient descent and linear networks, in line with most theory papers on the topic. 2.2 Auxiliary losses Additionally, we ablate the eﬀect of using an auxiliary loss on an agent’s performance and rank. Speciﬁcally, we chose the ""Contrastive Unsupervised Representations for Reinforcement Learning"" (CURL) (Laskin et al., 2020) loss that is designed for use in a contrastive self-supervised learning setting within standard RL algorithms: L(s, a, r, θ) = LQ(s, a, r, θ) + λ ∗ LCU RL(s, ˆs, θ). (2) Here, LQ refers to standard RL losses and the CURL loss LCU RL(s, ˆs, θ) is the contrastive loss between the features of the current observation s and a randomly augmented observation ˆs as described in Laskin et al. (2020). Besides this, we also ablate with Sharpness-Aware Minimization (SAM) (Foret et al., 2021), an approach that seeks parameters that have a uniformly low loss in its neighborhood, which leads to ﬂatter local minima. We chose SAM as a means to better understand whether the geometry of loss landscapes help inform the correlation between the eﬀective rank and agent performance in oﬄine RL algorithms. In our experiments, we focus on analyzing mostly the deep Q-learning (DQN) algorithm (Mnih et al., 2015) to simplify the experiments and facilitate deeper investigation into the rank collapse hypothesis. 2.3 Deep oﬄine RL Online RL requires interactions with an environment to learn using random exploration. However, online interactions with an environment can be unsafe and unethical in the real-world (Dulac-Arnold et al., 2019). Oﬄine RL methods do not suﬀer from this problem because they can leverage oﬄine data to learn policies that enable the application of RL in the real world (Menick et al., 2022; Shi et al., 2021; Konyushkova et al., 2021). Here, we focused on oﬄine RL due to its importance in real-world applications, and the previous works showed that the implicit regularization eﬀect is more pronounced in the oﬄine RL (Kumar et al., 2020a; 2021a). Some of the early examples of oﬄine RL algorithms are least-squares temporal diﬀerence methods (Bradtke and Barto, 1996; Lagoudakis and Parr, 2003) and ﬁtted Q-iteration (Ernst et al., 2005; Riedmiller, 2005). Recently, value-based approaches to oﬄine RL have been quite popular. Value-based approaches typically lower the value estimates for unseen state-action pairs, either through regularization (Kumar et al., 2020b) or uncertainty (Agarwal et al., 2020). One could also include R-BVE (Gulcehre et al., 2021) in this category, although it regularizes the Q function only on the rewarding transitions to prevent learning suboptimal policies. Similar to R-BVE, Mathieu et al. (2021) have also shown that the methods using single-step of policy improvement work well on tasks with very large action space and low state-action coverage. In this paper, due to their simplicity and popularity, we mainly study action value-based methods: oﬄine DQN (Agarwal et al., 2020) and oﬄine R2D2 (Gulcehre et al., 2021). Moreover, we also sparingly use Batched Constrained Deep Q-learning (BCQ) algorithm (Fujimoto et al., 2019), another popular oﬄine RL algorithm that uses the behavior policy to constrain the actions taken by the target network. Most oﬄine RL approaches we explained here rely on pessimistic value estimates (Jin et al., 2021; Xie et al., 2021; Gulcehre et al., 2021; Kumar et al., 2020b). Mainly because oﬄine RL datasets lack exhaustive exploration, and extrapolating the values to the states and actions not seen in the training set can result in 6 Preprint extrapolation errors which can be catastrophic with TD-learning (Fujimoto et al., 2018; Kumar et al., 2019). On the other hand, in online RL, it is a common practice to have inductive biases to keep optimistic value functions to encourage exploration (Machado et al., 2015). We also experiment with the tandem RL setting proposed by Ostrovski et al. (2021), which employs two independently initialized online (active) and oﬄine (passive) networks in a training loop where only the online agent explores and drives the data generation process. Both agents perform identical learning updates on the identical sequence of training batches in the same order. Tandem RL is a form of oﬄine RL. Still, unlike the traditional oﬄine RL setting on ﬁxed datasets, in the tandem RL, the behavior policy can change over time, which can make the learning non-stationary. We are interested in this setting because the agent does not necessarily reuse the same data repeatedly, which was pointed in Lyle et al. (2021) as a potential cause for the rank collapse. 3 Experimental setup To test and verify diﬀerent aspects of the rank collapse hypothesis and its potential impact on the agent performance, we ran a large number of experiments on bsuite (Osband et al., 2019), Atari (Bellemare et al., 2013) and DeepMind lab (Beattie et al., 2016) environments. In all these experiments, we use the experimental protocol, datasets and hyperparameters from Gulcehre et al. (2020) unless stated otherwise. We provide the details of architectures and their default hyperparameters in Appendix A.12. • bsuite – We run ablation experiments on bsuite in a fully oﬄine setting with the same oﬄine dataset as the one used in Gulcehre et al. (2021). We use a DQN agent (as a representative TD Learning algorithm) with multi-layer feed-forward networks to represent the value function. bsuite provides us with a small playground environment which lets us test certain hypotheses which are computationally prohibitive in other domains (for e.g.: computing Hessians) with respect to terminal features and an agent’s generalization performance. • Atari – To test whether some of our observations are also true with higher-dimensional input features such as images, we run experiments on the Atari dataset. Once again, we use an oﬄine DQN agent as a representative TD-learning algorithm with a convolutional network as a function approximator. On Atari, we conducted large-scale experiments on diﬀerent conﬁgurations: (a) Small-scale experiments: – DQN-256-2M: Oﬄine DQN on Atari with minibatch size 256 trained for 2M gradient steps with four diﬀerent learning rates: [3e − 5, 1e − 4, 3e − 4, 5e − 4]. We ran these experiments to observe if our observations hold in the default training scenario identiﬁed in RL Unplugged (Gulcehre et al., 2020). – DQN-32-100M: Oﬄine DQN on Atari with minibatch size of 32 trained for 100M gradient steps with three diﬀerent learning rates: (cid:2)3 × 10−5, 1 × 10−4, 3 × 10−4(cid:3). We ran those experiments to explore the eﬀects of reducing the minibatch size. (b) Large-scale experiments: – Long run learning rate sweep (DQN-256-20M): Oﬄine DQN trained for 20M gradients steps with 12 diﬀerent learning rates evenly spaced in log-space between 10−2 and 10−5 trained on minibatches of size 256. The purpose of these experiments is to explore the eﬀect of wide range of learning rates and longer training on the eﬀective rank. – Long run interventions (DQN-interventions): Oﬄine DQN trained for 20M gradient steps on minibatches of size 256 with 128 diﬀerent hyperparameter interventions on activation functions, dataset size, auxiliary losses etc. The purpose of these experiments is to understand the impact of such interventions on the eﬀective rank in the course of long training. • DeepMind Lab – While bsuite and Atari present relatively simple fully observable tasks that require no memory, DeepMind lab tasks (Gulcehre et al., 2021) are more complex, partially observable tasks where it is very diﬃcult to obtain good coverage in the dataset even after collecting billions of transitions. We speciﬁcally conduct our observational studies on the eﬀective ranks on the DeepMind Lab dataset, 7 Preprint which has data collected from a well-trained agent on the SeekAvoid level. The dataset is collected by adding diﬀerent levels of action exploration noise to a well-trained agent in order to get datasets with a larger coverage of the state-action space. Figure 3: Structural causal model (SCM) of diﬀerent factors that we test: M represents the model selection method that we use to determine the h which denotes the observed confounders that are chosen at the beginning of the training, including the task, the model architecture (including depth and number of units), learning rate and the number of gradient steps to train. β is the eﬀective rank of the penultimate layer. λ is the agent’s performance, measured as episodic returns the agent attains after evaluating in the environment. A represents the unobserved confounders that change during training but may aﬀect the performance, such as the number of dead units, the parameter norms, and the other underlying factors that can inﬂuence learning dynamics. We test the eﬀect of each factor by interventions. We illustrate the structural causal model (SCM) of the interactions between diﬀerent factors that we would like to test in Figure 3. To explore the relationship between the rank and the performance, we intervene on h, which represents potential exogenous sources of implicit regularization, such as architecture, dataset size, and the loss function, including the auxiliary losses. The interventions on h will result in a randomized controlled trial (RCT.) A represents the unobserved factors that might aﬀect performance denoted by λ an the eﬀective rank β such as activation norms and number of dead units. It is easy to justify the relationship between M, h, A and β. We argue that β is also confounded by A and h. We show the confounding eﬀect of A on β with our interventions to beta via auxiliary losses or architectural changes that increase the rank but do not aﬀect the performance. We aim to understand the nature of the relationship between these terms and whether SCM in the ﬁgure describes what we notice in our empirical exploration. We overload the term performance of the agent to refer to episodic returns attained by the agent when it is evaluated online in the environment. In stochastic environments and datasets with limited coverage, an oﬄine RL algorithm’s online evaluation performance and generalization abilities would correlate in most settings (Gulcehre et al., 2020; Kumar et al., 2021c). The oﬄine RL agents will need to generalize when they are evaluated in the environment because of: • Stochasticity in the initial conditions and transitions of the environment. For example, in the Atari case, the stochasticity arises from sticky actions and on DeepMind lab, it arises from the randomization of the initial positions of the lemons and apples. • Limited coverage: The coverage of the environment by the dataset is often limited. Thus, an agent is very likely to encounter states and actions that it has never seen during training. 4 Eﬀective rank and performance Based on the results of Kumar et al. (2020a), one might be tempted to extrapolate a positive causal link between the eﬀective rank of the last hidden layer and the agent’s performance measured as episodic returns 8 β: Eﬀective rank h: Observed confounders λ: Performance A: Hidden confounders M: Model selection method M h λ A β Preprint attained when evaluated in the environment. We explore this potentially interesting relationship on a larger scale by adopting a proof by contradiction approach. We evaluated the agents with the hyperparameter setup deﬁned for the Atari datasets in RL Unplugged (Gulcehre et al., 2020) and the hyperparameter sweep deﬁned for DeepMind Lab (Gulcehre et al., 2021). For a narrow set of hyperparameters this correlation exists as observed in Figure 1. However, in both cases, we notice that a broad hyperparameter sweep makes the correlation between performance and rank disappear (see Figures 1 and DeepMind lab Figure in Appendix A.3). In particular, we ﬁnd hyperparameter settings that lead to low (collapsed) ranks with high performance (on par with the best performance reported in the restricted hyperparameter range) and settings that lead to high ranks but poor performance. This shows that the correlation between eﬀective rank and performance can not be trusted for oﬄine policy selection. In the following sections, we further present speciﬁc ablations that help us understand the dependence of the eﬀective rank vs. performance correlation on speciﬁc hyperparameter interventions. 4.1 Lifespan of learning with deep Q-networks Empirically, we found eﬀective rank suﬃcient to identify three phases when training an oﬄine DQN agent with a ReLU activation function (Figure 2). Although the eﬀective rank may be suﬃcient to identify those stages, it still does not imply a direct causal link between the eﬀective rank and the performance as discussed in the following sections. Several other factors can confound eﬀective rank, making it less reliable as a guiding metric for oﬄine RL unless those confounders are carefully controlled. • Phase 1 (Simple behaviors): The eﬀective rank of the model ﬁrst collapses to a small value, in some cases to a single digit value, and then gradually starts to increase. In this phase, the model learns easy to learn behaviors that would have low performance when evaluated in the environment. We hypothesized that this could be due to the implicit bias of the SGD to learn functions of increasing complexity over training iterations (Kalimeris et al., 2019); therefore early in training, the network relies on simple behaviours that are myopic to most of the variation in the data. Hence the model has a low rank. The rank collapse early in the beginning of the training happens very abruptly, just in a handful of gradient updates the rank collapses to a single digit number. However, this early rank collapse does not degrade the performance of the agent. We call this rank collapse in the early in the training as self-pruning eﬀect. • Phase 2 (Complex behaviors): In this phase, the eﬀective rank of the model increases and then usually ﬂattens. The model starts to learn more complex behaviors that would achieve high returns when evaluated in the environment. • Phase 3 (Underﬁtting/Underparameterization): In this phase, the eﬀective rank collapses to a small value (often to 1) again, and the performance of the agent often collapses too. The third phase is called underﬁtting since the agent’s performance usually drops, and the eﬀective rank also collapses, which causes the agent to lose part of its capacity. This phase is not always observed (or the performance does not collapse with eﬀective ran towards the end of the training) in all settings as we demonstrate in our diﬀerent ablations. Typically in supervised learning, Phase 2 is followed by over-ﬁtting, but with oﬄine TD-learning, we could not ﬁnd any evidence of over-ﬁtting. We believe that this phase is primarily due to the target Q-network needing to extrapolate over the actions not seen during the training and causing extrapolation errors as described by (Kumar et al., 2019). A piece of evidence to support this hypothesis is presented in Figure 29 in Appendix A.5, which suggests that the eﬀective rank and the value error of the agent correlate well. In this phase, the low eﬀective rank and poor performance are caused by a large number of dead ReLU units. Shin and Karniadakis (2020) also show that as the network has an increasing number of dead units, it becomes under-parameterized and this could negatively inﬂuence the agent’s performance. It is possible to identify those three phases in many of the learning curves we provide in this paper, and our ﬁrst two phases agree with the works on SGD’s implicit bias of learning functions of increasing complexity (Kalimeris et al., 2019). Given a ﬁxed model and architecture, whether it is possible to observe all these three phases during training fundamentally depends on: 9 Preprint Figure 4: The three phases of learning: On IceHockey and MsPacman RL Unplugged Atari games we illustrate the diﬀerent phases of learning with the oﬄine DQN agent using the learning rate of 0.0004. The blue region in the plots identiﬁes the Phase 1, green region is the Phase 2, and red region is the Phase 3 of the learning. IceHockey is one of the games where the expert that generated the dataset performs quite poorly on it, thus the majority of the data is just random exploration data. It is easy to see that in this phase, the Oﬄine DQN performs very poorly and never manages to get out of the Phase 1. The performance of the agent on IceHockey is poor and the eﬀective rank of the network is low throughout the training. On MsPacman, we can observe all the three phases. The model transition into Phase 2 from Phase 1 quickly, and then followed by the under-ﬁtting regime where the eﬀective rank collapses the agent performs poorly. 1. Hyperparameters: The phases that an oﬄine RL algorithm would go through during training depends on hyperparameters such as learning rate and the early stopping or training budget. For example, due to early stopping the model may just stop in the second phase, if the learning rate is too small, since the parameters will move much slower the model may never get out of Phase 1. If the model is not large enough, it may never learn to transition from Phase 1 into Phase 2. 2. Data distribution: The data distribution has a very big inﬂuence on the phases the agent goes thorough, for example, if the dataset only contains random exploratory data and ORL method may fail to learn complex behaviors from that data and as a result, will never transition into Phase 2 from Phase 1. 3. Learning Paradigm: The learning algorithm, including optimizers and the loss function, can inﬂuence the phases the agent would go through during the training. For example, we observed that Phase 3 only happens with the oﬄine TD-learning approaches. It is possible to avoid phase three (underﬁtting) by ﬁnding the correct hyperparameters. We believe the third phase we observe might be due to non-stationarity in RL losses (Igl et al., 2020) due to bootstrapping or errors propagating through bootstrapped targets (Kumar et al., 2021b). The underﬁtting regime only appears if the network is trained long enough. The quality and the complexity of the data that an agent learns from also plays a key role in deciding which learning phases are observed during training. In Figure 4, we demonstrate the diﬀerent phases of learning on IceHockey and MsPacman games. On IceHockey, since the expert that generated the dataset has a poor performance on that game, the oﬄine DQN is stuck in Phase 1, and did not managed to learn complex behaviors that would push it to Phase 2 but on MsPacMan, all the three phases are present. We provide learning curves for all the online policy selection Atari games across twelve learning rates in Appendix A.8. 10 k n a R e v i t c e f f E 6 4 2 −2.5 −5 −7.5 −10 −12.5 n r u t e R e d o s p E i IceHockey MsPacman 60 40 20 0 5000 10000 IceHockey 15000 20000 0 0 Learner Steps (x1000) 2500 5000 10000 MsPacman 15000 20000 2000 1500 1000 500 0 0 5000 10000 15000 20000 0 5000 10000 15000 20000 Learner Steps (x1000) Phases of Learning Phase 1: Simple Behaviors Phase 2: Complex Behaviors Phase 3: Underparametrization Preprint Figure 5 shows the relationship between the eﬀective rank and twelve learning rates. In this ﬁgure, the eﬀect of learning rate on the diﬀerent phases of learning is distinguishable. For low learning rates, the ranks are low because the agent can never transition from Phase 1 to Phase 2, and for large learning rates, the eﬀective ranks are low because the agent is in Phase 3. Therefore, the distribution of eﬀective ranks and learning rates has a Gaussian-like shape, as depicted in the ﬁgure. The distribution of rank shifts towards low learning rates as we train the agents longer because slower models with low learning rates start entering Phase 2, and the models trained with large learning rates enter Phase 3. Figure 5: [Atari]: Bar charts of of eﬀective ranks with respect to the learning rates after 1M and 20M learning steps. After 1M gradient steps, the ranks are distributed almost like a Gaussian. After 20M learning steps the mode of the distribution of the ranks shifts towards left where the mode goes down for most games as well. Namely as we train the network longer the rank goes down and in particular for the large learning rates. For the low learning rates the rank is low because the model is stuck in Phase 1, the large learning rates get into the Phase 3 quickly and thus the low ranks. 4.2 The eﬀect of dataset size We can use the size of the dataset as a possible proxy metric for the coverage that the agent observes in the oﬄine data. We uniformly sampled diﬀerent proportions (from 5% of the transitions to the entire dataset) from the transitions in the RLUnplugged Atari benchmark dataset (Gulcehre et al., 2020) to understand how the agent behaves with diﬀerent amounts of training data and whether this is a factor aﬀecting the rank of the network. Figure 6 shows the evolution of rank and returns over the course of training. The eﬀective rank and performance collapse severely with low data proportions, such as when learning only on 5% of the entire 11 Preprint dataset subsampled. Those networks can never transition from phase 1 to phase 2. However, as the proportion of the dataset subsampled increases, the agents could learn more complex behaviors to get into phase 2. The eﬀective rank collapses less severely for the larger proportions of the dataset, and the agents tend to perform considerably better. In particular, we can see that in phase 1, an initial decrease of the rank correlates with an increase in performance, which we can speculate is due to the network reducing its reliance on spurious parts of the observations, leading to representations that generalize better across states. It is worth noting that the ordering of the policies obtained by using the agents’ performance does not correspond to the ordering of the policies with respect to the eﬀective rank throughout training. For example, oﬄine DQN trained on the full dataset performs better than 50% of the dataset, while the agent trained using 50% of the data sometimes has a higher rank. A similar observation can be made for Zaxxon, at the end of the training, the network trained on the full dataset underperforms compared to the one trained on 50% of the data, even if the rank is the same or higher. Figure 6: [Atari] Dataset size: Evolution of ranks and returns as we vary the fraction of data available for the agent to train on. We see that the agent which sees very little data collapses both in terms of rank and performance. The agent which sees more of the data has good performance even while allowing some shrinkage of rank during the training. 5 Interactions between rank and performance To understand the interactions and eﬀects of diﬀerent factors on the rank and performance of oﬄine DQN, we did several experiments and tested the eﬀect of diﬀerent hyperparameters on eﬀective rank and the agent’s performance. Figure 3 shows the causal graph of the eﬀective rank, its confounders, and the agent’s performance. Ideally, we would like to intervene in each node on this graph to measure its eﬀect. As the rank is a continuous random variable, it is not possible to directly intervene on eﬀective rank. Instead, we emulate interventions on the eﬀective rank by conditioning to threshold β with τ . In the control case (λ(1)), we assume β > τ and for λ(0), we would have β ≤ τ . We can write the average treatment eﬀect (ATE) for the eﬀect of setting the eﬀective rank to a large value on the performance as: ATE(λ, β, τ ) = E[λ|λ(1)] − E[λ|λ(0)]. (3) Let us note that this ATE(λ, β, τ ) quantity doesn’t necessarily measure the causal eﬀect of β on λ, since we know that the λ can be confounded by A. We study the impact of factors such as activation function, learning rate, data split, target update steps, and CURL and SAM losses on the Asterix and Gravitar levels. We chose these two games since Asterix is an 12 Preprint i) ReLU ii) tanh iii) both Figure 7: [Atari]: The correlation plot between the eﬀective rank and the performance (measured in terms of episode returns by evaluating the agent in the environment) of oﬄine DQN on Asterix and Gravitar games over 256 diﬀerent hyper-parameter conﬁgurations trained for 2M learning steps. There is a strong correlation with the ReLU function, but the correlation disappears for the network with tanh activation function. There is no signiﬁcant correlation between eﬀective rank and the performance on the Asterix game with the complete data. Still, a positive correlation exists on the Gravitar game. These results are not aﬀected by the Simpson’s paradox since the subgroups of the data when split into groups concerning activation functions, do not show a consistent correlation. easy-to-explore Atari game with relatively dense rewards, while Gravitar is a hard-to-explore and a sparse reward setting (Bellemare et al., 2016). In Table 1, we present the results of intervening to β thresholded with diﬀerent quantiles of the eﬀective rank. Choosing a network with a high eﬀective rank for a ReLU network has a statistically signiﬁcant positive eﬀect on the agent’s performance concerning diﬀerent quantiles on both Asterix and Gravitar. The agent’s performance is measured in terms of normalized scores as described in Gulcehre et al. (2020). However, the results with the tanh activation function are mixed, and the eﬀect of changing the rank does not have a statistically signiﬁcant impact on the performance in most cases. In Figure 7, we show the correlations between the rank and performance of the agent. The network with ReLU activation strongly correlates rank and performance, whereas tanh does not. The experimental data can be prone to Simpson’s paradox which implies that a trend (correlation in this case) may appear in several subgroups of the data, but it disappears or reverses when the groups are aggregated (Simpson, 1951). This can lead to misleading conclusions. We divided our data into equal-sized subgroups for hyperparameters and model variants, but we could not ﬁnd a consistent trend that disappeared when combined, as in Figure 7. Thus, our experiments do not appear to be aﬀected by Simpson’s paradox. 13 Preprint Table 1: Atari: The average treatment eﬀect of having a network with a higher rank than concerning diﬀerent quantiles. We report the average treatment eﬀect (ATE), its uncertainty (using 95% conﬁdence intervals using standard errors), and p-values for Asterix and Gravitar games. Higher eﬀective rank seems to have a smaller eﬀect on Asterix than Gravitar. Let us note that, Gravitar is a sparse reward problem, and Asterix is a dense-reward one. Overall the eﬀect is more prominent for ReLU than tanh, and with tanh networks, it is not statistically signiﬁcant. We boldfaced the ATEs where the eﬀect is statistically signiﬁcant (p < 0.05.) The type column of the table indicates the activation functions used in the experiments we did the intervention. The “Combined” type corresponds to a combination of tanh and ReLU experiments. Level Type Asterix Gravitar Combined ReLU Tanh Combined ReLU Tanh quantile=0.25 ATE 0.019 ± 0.010 0.079 ± 0.018 -0.014 ± 0.004 0.954 ± 0.077 1.654 ± 0.150 0.028 ± 0.090 quantile=0.5 quantile=0.75 quantile=0.95 p-value ATE 0.001 0.000 0.999 0.000 0.000 0.303 0.007 ± 0.013 0.089 ± 0.025 0.009 ± 0.005 0.569 ± 0.098 1.146 ± 0.163 0.185 ± 0.118 p-value ATE 0.207 0.000 0.996 0.000 0.000 0.005 0.005 ± 0.019 0.112 ± 0.040 -0.005 ± 0.006 0.496 ± 0.141 1.105 ± 0.256 0.242 ± 0.167 p-value ATE 0.324 0.000 0.886 0.000 0.000 0.009 -0.031 ± 0.011 0.110 ± 0.085 0.020 ± 0.017 0.412 ± 0.206 1.688 ± 0.564 0.054 ± 0.265 p-value 0.999 0.017 0.023 0.001 0.000 0.367 Table 2: [Atari] Average Treatment Eﬀect of diﬀerent interventions on Asterix and Gravitar: Quantity of interest Y is the terminal rank of the features. Activation function and learning rate have the most considerable eﬀect on the terminal feature ranks in our setup. We indicate them with boldface; changing the activation function from ReLU to tanh improves the eﬀective rank, whereas changing the learning rate from 3 × 10−4 to 3 × 10−5 reduces the rank. u Activation Function Learning Rate Target Update Steps SAM Loss Weight Level CURL Loss Weight Data Split Control (u) ReLU 3 × 10−4 2500 0 Asterix 0 100% Treatment T(u) tanh 3 × 10−5 200 0.05 Gravitar 0.001 5% Yt(u) − Yc(u) 66.97 ± 7.13 -54.15 ± 7.54 -15.23 ± 8.21 -10.05 ± 8.26 -9.41 ± 8.25 8.09 ± 8.27 5.11 ± 8.27 In this analysis, we set our control setting to be trained with ReLU activations, the learning rate of 3e − 4, without any auxiliary losses, with training done on the full dataset in the level Asterix. We present the Average Treatment Eﬀect (ATE) (the diﬀerence in ranks between the intervention being present and absent in an experiment) of changing each of these variables in Table 3. We ﬁnd that changing the activation function from ReLU to tanh drastically aﬀects the eﬀective ranks of the features, which is in sync with our earlier observations on other levels. In Figure 8, we present a heatmap of ATEs where we demonstrate the eﬀective rank change when two variables are changed from original control simultaneously. Once again, the activation functions and learning rate signiﬁcantly aﬀect the terminal ranks. We also observe some interesting combinations that lead the model to converge to a lower rank – for example, using SAM loss with dropout. These observations further reinforce our belief that diﬀerent factors aﬀect the phenomenon. Figure 8: [Atari] ablations: Eﬀects of diﬀerent pairs of interventions on the control set. The control sam- ple is on level Asterix with no dropout, curl and loss smoothing with the full dataset and a target update period of 2500 steps. Overall, changing the activation function and learning rate has the largest eﬀect on the rank. 14 Activation(tanh) 186.1 Learning Rate(3e-5) 94.9 -74.9 SAM Weight(0.05) 71.5 77.3 91.8 CURL Weight(0.001) 165.1 -78.8 -27.1 -50.1 Dropout Rate(0.5) 123.9 -67.0 109.3 -64.6 70.8 Dataset Size(100%) 138.3 -80.1 55.7 -47.1 99.3 -49.5 Target Update Steps(200) 70.2 -78.1 55.3 -70.5 57.5 -2.4 -40.3 Level Name(Gravitar) 30.3 -68.6 -64.0 46.7 -35.9 -61.5 -78.3 -59.7 150 100 50 0 −50 ) h n a t ( n o i t a v i t c A ) 5 - e 3 ( e t a R g n n r a e L i ) 5 0 . 0 ( t h g e W M A S i ) 1 0 0 . 0 ( t h g e W L R U C i ) 5 . 0 ( e t a R t u o p o r D ) % 0 0 1 ( e z S i t e s a t a D ) r a t i v a r G ( e m a N l e v e L ) 0 0 2 ( s p e t S e t a d p U t e g r a T Preprint The extent of rank collapse and mitigating rank collapse alone may never fully ﬁx the agent’s learning ability in diﬀerent environments. 6 The eﬀect of activation functions The severe rank collapse of phase 3 is apparent in our Atari models, which have simple convolutional neural networks with ReLU activation functions. When we study the same phenomenon on the SeekAvoid dataset, rank collapse does not seem to happen similarly. It is important to note here that to solve those tasks eﬀectively, the agent needs memory; hence, all networks have a recurrent core and LSTM. Since standard LSTMs use tanh(·) activations, investigating in this setting would help us understand the role of the choice of architecture on the behavior of the model’s rank. Figure 10 shows that the output features of the LSTM network on the DeepMind lab dataset do not experience any detrimental eﬀective rank collapse with diﬀerent exploration noise when we use tanh(·) activation function for the cell. However, if we replace the tanh(·) activation function of the LSTM cell with ReLU or if we replace the LSTM with a feed-forward MLP using ReLU activations (as seen in Figure 10) the eﬀective rank in both cases, collapses to a small value at the end of training. This behavior shows that the choice of activation function has a considerable eﬀect on whether the model’s rank collapses throughout training and, subsequently, its ability to learn expressive value functions and policies in the environment as it is susceptible to enter phase 3 of training. Observation: Agents that have networks with ReLU units tend to have dead units which causes the eﬀective rank to collapse in phase 3 of learning while other activations like tanh do not suﬀer a similar collapse. The activation functions inﬂuence both the network’s learning dynamics and performance. As noted by Pennington and Worah (2017), the activation function can inﬂuence the rank of each layer at initialization. Figure 9 presents our ﬁndings on bsuite levels. In general, the eﬀective rank of the penultimate layer with ReLU activations collapses faster, while ELU and tanh(·) tend to maintain a relatively higher rank than ReLU. As the eﬀective rank goes down for the catch environment, the activations become sparser, and the units die. We illustrate the sparsity of the activations with Gram matrices over the features of the last hidden layer of a feedforward network trained on bsuite in Figure 37 in Appendix A.14. Figure 9: [bsuite] Eﬀective ranks of diﬀerent activation functions: The magnitude of drop in eﬀective rank is more severe for ReLU and sigmoid activation functions than tanh. 7 Optimization The inﬂuence of minibatch size and the learning rate on the learning dynamics is a well-studied phenomenon. Smith and Le (2017); Smith et al. (2021) argued that the ratio between the minibatch size and learning rate relates to implicit regularization of SGD and also aﬀects the learning dynamics. Thus, it is evident that those factors would impact the eﬀective rank and performance. Here, we focus on a setup where we see the correlation between eﬀective rank and performance and investigate how it emerges. 15 Preprint Figure 10: [DeepMind lab SeekAvoid] Activation functions: Evolution of ranks in a typical LSTM network with tanh activations at the cell of LSTM on DeepMind lab. We see that a typical LSTM network does not get into Phase 3. However, when we change the activation function of the cell gate from tanh to ReLU the eﬀective rank collapses to very small values. The eﬀective rank collapses when the LSTM is replaced with a feedforward network too in cases of ReLU activation. Our analysis and ablations in Table 3 and Figure 8 illustrate that the learning rate is one of the most prominent factors on the performance of the agent. In general, there is no strong and consistent correlation between the eﬀective rank and the performance across diﬀerent models and hyperparameter settings. However, in Figure 1, we showed that the correlation exists in a speciﬁc setting with a particular minibatch size and learning rate. Further, in Figure 7, we narrowed down that the correlation between the eﬀective rank and the performance exists for the oﬄine DQN with ReLU activation functions. Thus in this section, we focus on a regime where this correlation exists with the oﬄine DQN with ReLU and investigate how the learning rate and minibatches aﬀect the rank. We ran several experiments to explore the relationship between the minibatch size, learning rates, and rank. In this section, we report results on Atari in few diﬀerent settings – Atari-DQN-256-2M, Atari-DQN-32-100M, Atari-DQN-256-20M and Atari-BC-256-2M. The dying ReLU problem is a well-studied issue in supervised learning (Glorot et al., 2011; Gulcehre et al., 2016) where due to the high learning rates or unstable learning dynamics, the ReLU units can get stuck in the zero regime of the ReLU activation function. We compute the number of dead ReLU units of the penultimate layer of a network as the number of units with zero activation value for all inputs. Increasing the learning rate increases the number of dead units as can be seen in Figures 11 and 12. Even in BC, we observed that using large learning rates can cause dead ReLU units, rank collapse, and poor performance, as can be seen in Figure 13, and hence this behavior is not unique to oﬄine RL losses. Nevertheless, models with TD-learning losses have catastrophic rank collapses and many dead units with lower learning rates than BC. Let us note that the eﬀective rank depends on the number of units (D) and the number of dead units (η) at a layer. It is easy to see that eﬀective rank is upper-bounded by D − η. In Figure 14, we observe a strong correlation between the number of dead ReLU units of the penultimate layer of the ReLU network and its eﬀective rank. Observation: The pace of learning inﬂuences the number of dead units and the eﬀective rank: the larger the learning rates and the smaller minibatches are, the number of dead units increases, and the eﬀective rank decreases. However, the performance is only poor when the eﬀective rank is severely low. Finally, we look into how eﬀective ranks shape up towards the end of training. We test 12 diﬀerent learning rates, trying to understand the interaction between the learning rates and the eﬀective rank of the representations. We summarize our main results in Figure 5 where training oﬄine DQN decreases the 16 Preprint Figure 11: [Atari DQN-32-100M] Learning curves of oﬄine DQN for 100M gradient steps of training with minibatch size of 32. Increasing the learning rate increases the number of dead units in the ReLU network. As a result, the increased learning rate also causes more severe collapse as well, which aligns very well with the number of dead units. We observed that this behavior can also happen with networks using saturating activation functions such as sigmoid. eﬀective rank. The eﬀective rank is low for both small and large learning rates. For higher learning rates, as we have seen earlier, training for longer leads to many dead ReLU units, which in turn causes the eﬀective rank to diminish, as seen in Figures 11 and 12. Moreover, as seen in those ﬁgures, in Phase 1, the eﬀective rank and the number of dead units are low. Thus, the rank collapse in Phase 1 is not caused by the number of dead units. In Phase 3, the number of dead units is high, but the eﬀective rank is drastically low. The drastically low eﬀective rank is caused by the network’s large number of dead units in Phase 3. In Phase 3, we believe that both the under-parameterization and the poor performance is caused by the number of dead units in ReLU DQN, which was shown to be the case by (Shin and Karniadakis, 2020) in supervised learning. Overall, we could only observe a high correlation between the eﬀective rank and the agent’s performance, when we use ReLU activations in the network after a long training. We also present more analysis on how controlling the loss landscape aﬀects the rank vs performance in Appendix A.2 and A.4. 8 The eﬀect of loss function 8.1 Q-learning and behavior cloning Behavior cloning (BC) is a method to learn the behavior policy from an oﬄine dataset using supervised learning approaches (Pomerleau, 1989). Policies learned by BC will learn to mimic the behavior policy, and thus the performance of the learned BC agent is highly limited by the quality of the data the agent is trained on. We compare BC and Q-learning in Figure 15. We conﬁrm that with default hyperparameters, the BC agent’s eﬀective rank does not collapse at the end of training. In contrast, as shown by Kumar et al. (2020a), 17 Preprint Figure 12: [Atari DQN-256-3M] Learning curves of oﬄine DQN for 3M gradient steps of training with minibatch size of 256. Increasing the minibatch size improves the performance of the network with larger learning rates. DQN’s eﬀective rank collapses. DQN outperforms BC even if its rank is considerably lower, and during learning, the rank is not predictive of the performance (Figure 15). This behavior indicates the unreliability of eﬀective rank for oﬄine model selection. 8.2 CURL: The eﬀect of self-supervision We study whether adding an auxiliary loss term proposed for CURL (Laskin et al., 2020) (Equation 2) during training helps the model mitigate rank collapse. In all our Atari experiments, we use the CURL loss described in (Laskin et al., 2020) without any modiﬁcations. Since the DeepMind lab tasks require memory, we apply a similar CURL loss to the features aggregated with a mean of the states over all timesteps. In all experiments, we also sweep over the weight of the CURL loss λ. Figure 16 shows the ranks and returns on Atari (for DeepMind lab results see Appendix A.6.) In Atari games, using very large weights for an auxiliary loss (≈ 1) prevents rank collapse, but they simultaneously deteriorate the agent performance. We speculate, borrowing on intuitions from the supervised learning literature on the role of the rank as implicit regularizer Huh et al. (2021), that in such scenarios large rank prevents the network from ignoring spurious parts of the observations, which aﬀects its ability to generalize. On the other hand, moderate weights of CURL auxiliary loss do not signiﬁcantly change the rank and performance of the agent. Previously Agarwal et al. (2021) showed that the CURL loss does not improve the performance of an RL agent on Atari in a statistically meaningful way. On DeepMind lab games, we do not observe any rank collapse. In none of our DeepMind lab experiments agents enter into Phase 3 after Phase 1 and 2. This is due to the use of the tanh activation function for LSTM based on our investigation of the role of the activation functions in Section 6. 18 Preprint Figure 13: [Atari BC-256-2M] Learning Curves of BC After 2M gradient steps with mini-batch size of 256. For large enough learning rates the rank of BC agent also collapses. We hypothesize that the rank collapse is a side eﬀect of learning in general and not only due to TD-learning based losses. Figure 14: [Atari] Scatter plot for the correlation between the number of dead units and eﬀective rank at the end of training and we observe that the eﬀective rank strongly correlates with the number of dead units. Also, using larger learning rate increases the number of dead units in the network. 9 Tandem RL Kumar et al. (2020a) propose one possible hypothesis for the observed rank collapse as the re-use of the same transition samples multiple times, particularly prevalent in the oﬄine RL setting. A setting in which this hypothesis can be tested directly is the ‘Tandem RL’ proposed in Ostrovski et al. (2021): here a secondary (‘passive’) agent is trained from the data stream generated by an architecturally equivalent, independently initialized baseline agent, which itself is trained in a regular, online RL fashion. The passive agent tends to under-perform the active agent, despite identical architecture, learning algorithm, and data stream. This 19 Preprint Figure 15: [Atari] Oﬄine DQN and Behavior Cloning (BC): We compare BC and DQN agents on the Atari dataset. We used the same architecture, dataset, and training protocols for both baselines. We used the hyperparameters deﬁned in (Gulcehre et al., 2020) for comparisons. The rank of the DQN agent is signiﬁcantly lower and achieves higher returns than BC. Figure 16: [Atari] Auxiliary losses: Evolution of ranks as we increase the weight of auxiliary losses. We see that a strong weight for auxiliary loss helps mitigate the rank collapse but prevents the model from learning useful representations. setup presents a clean ablation setting in which both agents use data in the same way (in particular, not diﬀering in their re-use of data samples), and so any diﬀerence in performance or rank of their representation cannot be directly attributable to the reuse of data. In Figure 17, we summarize the results of a Tandem-DQN using Adam (Kingma and Ba, 2014) and RMSProp (Tieleman et al., 2012) optimizers. Despite the passive agent reusing data in a similar fashion as the online agent, we observe that it collapses to a lower rank. Besides, the passive agent’s performance tends to be signiﬁcantly (in most cases, catastrophically) worse than the online agent that could not satisfactorily explained just by the extent of diﬀerence in their eﬀective ranks alone. We think that the Q-learning 20 Preprint Figure 17: Atari, tandem RL: We investigate the eﬀect of the choice of optimizers between Adam and RMSProp on rank and the performance of the models, both for the active (solid lines) and passive agents (dashed lines). We observe the rank of the passive agent is lower than the active agent both for Adam and RMSProp. Figure 18: Atari, Forked tandem RL: We evaluate diﬀerent loss functions in the forked tandem setting, where the passive agent is forked from the online agent, and the online agent’s parameters are frozen. Still, the parameters of the online agent are updated. We forked the passive agent after it had seen 50M frames during training which is denoted with dashed lines in the ﬁgure. We observe that using both BVE and MC return losses improves the agent’s rank, but the agent’s performance is still poor. algorithm seems to be not eﬃcient enough to exploit the data generated by the active agent to learn complex behaviors that would put the passive agent into Phase 2. We also noticed that, although Adam achieves better performance than RMSProp, the rank of the model trained with the Adam optimizer tends to be lower than the models trained with RMSProp. In Figure 18, we investigate the eﬀect of diﬀerent learning algorithms on the rank and the performance of an agent in the forked tandem RL setting. In the forked tandem setting, an agent is ﬁrstly trained for a fraction of its total training time. Then, the agent is ‘forked’ into active and passive agents, and they both start with the same network weights where the active agent is ‘frozen’ and not trained but continues to generate data from its policy to train the passive agent on this generated data for the remaining of the training time. Here, we see that the rank ﬂat-lines when we fork the Q-learning agent, but the performance collapses dramatically. In contrast, despite the rank of BVE and an agent trained with on-policy Monte Carlo returns going up, the 21 breakout pong i ) e v s s a p ( s k n a R l e n r e K e r u t a e F 200 150 100 50 0 200 100 0 0 0 50 100 150 200 200 150 100 50 0 seaquest space_invaders 200 100 Algorithm BVE Monte-Carlo Q-learning 50 100 150 200 0 0 50 100 150 200 50 100 150 200 0 Frames (x 1M) breakout pong seaquest space_invaders i ) e v s s a p ( n r u t e R e d o s p E i 400 300 200 100 0 20 10 0 −10 −20 20000 15000 10000 5000 0 0 50 100 150 200 0 50 100 150 200 0 Frames (x 1M) 1500 1000 500 Algorithm BVE Monte-Carlo Q-learning 50 100 150 200 0 0 50 100 150 200 Preprint performance still drops. Nevertheless, the decline of performance for BVE and the agent trained with Monte Carlo returns is not as bad as the Q-learning agent on most Atari games. 10 Oﬄine policy selection In Section 7, we found that with the large learning rate sweep setting rank and number of dead units have high correlation with the performance. Oﬄine policy selection (Paine et al., 2020) aims to choose the best policy without any online interactions purely from oﬄine data. The apparent correlation between the rank and performance raises the natural question of how well rank performs as an oﬄine policy selection approach. We did this analysis on DQN-256-20M experiments where we previously observed strong correlation between the rank and performance. We ran the DQN network described in DQN-256-20M experimental setting until the end of the training and performed oﬄine policy selection using the eﬀective rank to select the best learning rate based on the learning rate that yields to maximum eﬀective rank or minimum number of dead units. Figure 19: Atari DQN-256-20M: This plot is depicting the simple regret of a DQN agent with ReLU activations using eﬀective rank and the number of dead units. On the left, we show the the simple regret to select the best learning rate using the eﬀective rank and on the right hand side, we show the simple regret achieved based on the number of dead units. Figure 19 illustrates the simple regret on each Atari oﬄine policy selection game. The simple regret between the recommended policy measures how close an agent is to the best policy, and it is computed as described in Konyushkova et al. (2021). A simple regret of 1 would mean that our oﬄine policy selection mechanism successfully chooses the best policy, and 0 would mean that it would choose the worst policy. After 2M training steps, the simple regret with the number of dead units as a policy selection method is poor. In contrast, the simple regret achieved by selecting the agent with the highest eﬀective rank is good. The mean simple regret achieved by using the number of dead units as an oﬄine policy selection (OPS) method is 0.45 ± 0.11, where the uncertainty ±0.11 is computed as standard error across ﬁve seeds. In contrast, simple regret achieved by using eﬀective rank as an OPS method is 0.24 ± 0.12. The eﬀective ranks for most learning rates collapse as we train longer since more models enter Phase 3 of learning—the number of dead units increases. After 20M of learning steps, the mean simple regret computed using eﬀective rank as the OPS method becomes 0.40 ± 0.07, and the mean simple regret is 0.25 ± 0.12 with the number of dead units for the OPS. Let us note that the 2M learning step is more typical in training agents on the RL Unplugged Atari dataset. The number of dead units becomes a good metric to do OPS when the network is trained for long (20M steps in our experiment), where the rank becomes drastically small for most learning rate conﬁgurations. However, eﬀective rank seems to be a good metric for the OPS earlier in training. Nevertheless, without prior information about the task and agent, it is challenging to conclude whether the number of dead units or eﬀective rank would be an appropriate OPS metric. Other factors such as the number of training steps, activation functions, and other hyperparameters confound the eﬀective rank and number of dead units. Thus, we believe those two metrics we tested are unreliable in general OPS settings without controlling those other extraneous factors. 22 t e r g e R e p m S l i Effective rank Number of dead units 1.000 20M steps 2M steps 0.791 0.838 0.665 0.588 0.545 0.503 0.504 0.457 0.569 0.524 0.359 0.309 0.665 0.539 0.393 0.411 0.423 0.621 0.542 0.446 0.190 0.079 k c a t t A n o m e D 0.000 y e k c o H e c I r e n n u R d a o R 0.021 n o x x a Z i r e d R m a e B n a m c a P s M 0.009 n a y o o P 0.035 0.000 k n a t o b o R k n u D e b u o D l 0.000 0.000 0.026 k c a t t A n o m e D y e k c o H e c I r e n n u R d a o R 0.000 0.000 n o x x a Z 0.000 i r e d R m a e B 0.075 n a m c a P s M n a y o o P 0.000 k n a t o b o R k n u D e b u o D l Preprint In Figure 20, we compare eﬀective rank as an OPS method with respect to its percentage improvement over the online policy selection to maximize the me- dian reward achieved by the each agent across nine Atari games. The eﬀective rank on Zaxxon, Beam- Rider, MsPacman, Pooyan, Robotank, DoubleDunk perform competitively to the online policy selection. Eﬀective rank may be a complementary tool for net- works with ReLU activation function and we believe it can be a useful metric to monitor during the train- ing in addition to number of dead units to have a better picture about the performance of the agent. 11 Robustness to input perturbations Figure 20: Atari DQN-256-20M: Here, we are de- picting the percentage of improvement by doing oﬄine policy selection using rank individually vs online policy selection using median reward across nine Atari games to select the best learning rate. Using eﬀective rank as the oﬄine policy selection method performs relatively well when compared to doing online policy selection based on median normalized score across Atari games. Several works, such as Sanyal et al. (2018) suggested that low-rank models can be more robust to input perturbations (speciﬁcally adversarial ones). It is diﬃcult to just measure the eﬀect of low-rank rep- resentations on the agent’s performance since rank itself is not a robust measure, as it depends on diﬀer- ent factors such as activation function and learning which in turn can eﬀect the generalization of the algorithm independently from rank. However, it is easier to validate the antithesis, namely “Do more robust agents need to have higher eﬀective rank than less robust agents?”. We can easily test this hypothesis by comparing DQN and BC agents. Robustness metric: We deﬁne the robustness metric ρ(p) based on three variables: i) the noise level p, ii) the noise distribution d(p), and iii) the score obtained by agent when evaluated in the environment with noise level p: score[d(p)] for which score[d(0)] represents the average score achieved by the agent without any perturbation applied on it. Then we can deﬁne ρ(p) as follows: ρ(p) = 1 − score[d(0)] − score[d(p)] score[d(0)] = score[d(p)] score[d(0)] . (4) In our experiments, we compare DQN and BC agents since we already know that BC has much larger ranks across all Atari games than DQN. We trained these agents on datasets without any data augmentation. The data augmentations are only applied on the inputs when the agent is evaluated in the environment. We evaluate our agents on BeamRider, IceHockey, MsPacman, DemonAttack, Robotank and RoadRunner games from RL Unplugged Atari online policy selection games. We excluded DoubleDunk, Zaxxon and Pooyan games since BC agent’s performance on those levels was poor and very close to the performance of random agent, so the robustness metrics on those games would not be very meaningful. On IceHockey, the results can be negative, thus we shifted the scores by 20 to make sure that they are non-negative. In our robustness experiments, we used the hyperparameters that were used in Gulcehre et al. (2020) on Atari for both for DQN and BC. In Figure 21, we investigate the Robustness of DQN and BC agents with Robustness to random shifts: respect to random translation image perturbations applied to the observations as described by Yarats et al. (2020). We evaluate the agent on the Atari environment with varying degrees of input scaling. We noticed that BC’s performance deteriorates abruptly, whereas the performance of DQN, while also decreasing, is better than the BC one. 23 0 8 0 6 74.074 t n e m e v o r p m I f o e g a t n e c r e P 0 4 0 2 0 0 2 − 4.159 4.450 -3.332 -7.086 -18.605 -17.358 0 4 − 0 6 − -54.037 -49.712 k c a t t A n o m e D y e k c o H e c I r e n n u R d a o R n o x x a Z i r e d R m a e B n a m c a P s M n a y o o P k n a t o b o R k n u D e b u o D l Preprint Figure 21: [Atari] Robustness to random shifts during evaluation: We measure the robustness of BC and DQN to random shifts on six Atari games from RL Unplugged. The DQN and BC agents are trained oﬄine without any perturbations on the inputs, only on the RL Unplugged datasets. However, during evaluation time we perturbed the input images with ""random shift"" data augmentation. Overall, DQN is more robust than BC to the evaluation-time random-scaling image perturbations that are not seen during training. The diﬀerence is more pronounced on IceHockey and BeamRider games. DQN achieved mean AUC (over diﬀerent games) of 2.042 and BC got 1.26. Figure 22: [Atari] Robustness to random scaling during evaluation: We measure the robustness of BC and DQN to random scaling over inputs on six Atari games from RL Unplugged. The DQN and BC agents are trained oﬄine without data augmentations over the RL Unplugged datasets. However, we perturbed the observations during the evaluation with ""random scale"" data augmentation. We observed that DQN is more robust than BC to the evaluation-time random-scaling data augmentation. The diﬀerence is more pronounced in IceHockey, BeamRider, and Robotank games. DQN achieved a mean AUC (over diﬀerent games) of 1.60, and BC achieved 0.87. 24 e r o c s s s e n t s u b o R 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 MsPacman DemonAttack IceHockey 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 Robotank BeamRider RoadRunner 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 DQN BC 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 The random shift rate 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 e r o c s s s e n t s u b o R MsPacman DemonAttack IceHockey 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.00 0.02 0.04 0.06 0.08 0.10 0.00 0.02 0.04 0.06 0.08 0.10 0.00 0.02 0.04 0.06 0.08 0.10 Robotank BeamRider RoadRunner 1.0 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.02 0.04 0.06 0.08 0.10 0.0 0.00 0.02 0.04 0.06 0.08 0.10 The random scale value DQN BC 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.02 0.04 0.06 0.08 0.10 Preprint Robustness to random scaling: In Figure 22, we explore the Robustness of DQN and BC agents with respect to random scaling as image perturbation method applied to the observations that are feed into our deep Q-network. We randomly scale the image inputs as described in Chen et al. (2017). When evaluating the agent in an online environment, we test it with varying levels of image scaling. The results are again very similar to the random shift experiments where the DQN agent is more robust to random perturbations in the online environment. According to the empirical results obtained from those two experiments, it is possible to see that DQN is more robust to the evaluation-time input perturbations, speciﬁcally random shifts and scaling than the BC agent. Thus, more robust representations do not necessarily require a higher eﬀective rank. 12 Discussion In this work, we empirically investigated the previously hypothesized connection between low eﬀective rank and poor performance. We found that the relationship between eﬀective rank and performance is not as simple as previously conjectured. We discovered that an oﬄine RL agent trained with Q-learning during training goes through three phases. The eﬀective rank collapses to severely low values in the ﬁrst phase –we call this as the self-pruning phase– and the agent starts to learn basic behaviors from the dataset. Then in the second phase, the eﬀective rank starts going up, and in the third phase, the eﬀective rank collapses again. Several factors such as learning rate, activation functions and the number of training steps, inﬂuence the occurrence, persistence and the extent of severity of the three phases of learning. In general, a low rank is not always indicative of poor performance. Besides strong empirical evidence, we propose a hypothesis trying to explain the underlying phenomenon: not all features are useful for the task the neural network is trying to solve, and low rank might correlate with more robust internal representations that can lead to better generalization. Unfortunately, reasoning about what it means for the rank to be too low is hard in general, as the rank is agnostic to which direction of variations in the data are being ignored or to higher-order terms that hint towards a more compact representation of the data with fewer dimensions. Our results indicate that an agent’s eﬀective rank and performance correlate in restricted settings, such as ReLU activation functions, Q-learning, and a ﬁxed architecture. However, as we showed in our experiments, this correlation is primarily spurious in other settings since it disappears with simple modiﬁcations such as changing the activation function and the learning rate. We found several ways to improve the eﬀective rank of the agent without improving the performance, such as using tanh instead of ReLU, an auxiliary loss (e.g., CURL), and the optimizer. These methods address the rank collapse but not the underlying learning deﬁciency that causes the collapse and the poor performance. Nevertheless, our results show that the dynamics of the rank and agent performance through learning are still poorly understood; we need more theoretical investigation to understand the relationship between those two factors. We also observed in Tandem and oﬄine RL settings that the rank collapses to a minimal value early in training. Then there is unexplained variance between agents in the later stages of learning. Overall, the cause and role of this early rank collapse remain unknown, and we believe understanding its potential eﬀects is essential in understanding large-scale agents’ practical learning dynamics. The existence of low-rank but high-performing policies suggest that our networks can be over-parameterized for the tasks and parsimonious representations emerge naturally with TD-learning-based bootstrapping losses and ReLU networks in the oﬄine RL setting. Discarding the dead ReLU units might achieve a more eﬃcient inference. We believe this ﬁnding can give inspiration to a new family of pruning algorithms. Acknowledgements: We would like to thank Clare Lyle, Will Dabney, Aviral Kumar, Rishabh Agarwal, Tom le Paine, Mark Rowland and Yutian Chen for the discussions. We want to thank Mark Rowland, Rishabh Agarwal and Aviral Kumar for the feedback on the early draft version of the paper. We want to thank Sergio Gomez and Bjorn Winckler for their help with the infrastructure and the codebase at the inception of this project. We would like to thank the developers of Acme (Hoﬀman et al., 2020), Jax (Bradbury et al., 2018), Optax (Hessel et al., 2020) and Haiku (Hennigan et al., 2020). 25 Preprint References Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on oﬄine reinforce- ment learning. In International Conference on Machine Learning, pages 104–114. PMLR, 2020. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:29304–29320, 2021. OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3–20, 2020. Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32:7413–7424, 2019. David Barrett and Benoit Dherin. Implicit gradient regularization. In International Conference on Learning Representations, 2020. Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Advances in Neural Information Processing Systems, 29, 2016. Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. DotA 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. Steven Bradtke and Andrew Barto. Linear least-squares algorithms for temporal diﬀerence learning. Machine Learning, 22:33–57, 03 1996. Liang-Chieh Chen, George Papandreou, Florian Schroﬀ, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017. Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi. Batch normalization provably avoids ranks collapse for randomly initialised deep networks. Advances in Neural Information Processing Systems, 2020. Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. arXiv preprint arXiv:1406.2572, 2014. Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In International Conference on Machine Learning, volume 70, pages 1019–1028. PMLR, 2017. Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement learning. arXiv preprint arXiv:1904.12901, 2019. Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6:503–556, 2005. 26 Preprint Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for eﬃciently improving generalization. arXiv preprint arXiv:2103.09575, 2021. Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020. Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018. Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch deep reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019. Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization via Hessian eigenvalue density. arXiv preprint arXiv:1901.10159, 2019. Xavier Glorot and Yoshua Bengio. Understanding the diﬃculty of training deep feedforward neural networks. In International Conference on Artiﬁcial Intelligence and Statistics, pages 249–256. JMLR Workshop and Conference Proceedings, 2010. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In International Conference on Artiﬁcial Intelligence and Statistics, pages 315–323. JMLR Workshop and Conference Proceedings, 2011. Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions. In International Conference on Machine Learning, pages 3059–3068. PMLR, 2016. Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gómez Colmenarejo, Konrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, et al. RL unplugged: Benchmarks for oﬄine reinforcement learning. arXiv preprint arXiv:2006.13888, 2020. Caglar Gulcehre, Sergio Gómez Colmenarejo, Ziyu Wang, Jakub Sygnowski, Thomas Paine, Konrad Zolna, Yutian Chen, Matthew Hoﬀman, Razvan Pascanu, and Nando de Freitas. Regularized behavior value estimation. arXiv preprint arXiv:2103.09575, 2021. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human- level performance on imagenet classiﬁcation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1026–1034, 2015. Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin. Haiku: Sonnet for JAX, 2020. URL http://github.com/deepmind/dm-haiku. Matteo Hessel, David Budden, Fabio Viola, Mihaela Rosca, Eren Sezener, and Tom Hennigan. Optax: composable gradient transformation and optimisation, in jax!, 2020. URL http://github.com/deepmind/ optax. Matt Hoﬀman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Alex Novikov, Sergio Gómez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Andrew Cowie, Ziyu Wang, Bilal Piot, and Nando de Freitas. Acme: A research framework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979, 2020. Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why do deep residual networks generalize better than deep feedforward networks?–a neural tangent kernel perspective. arXiv preprint arXiv:2002.06262, 2020. Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks. arXiv preprint arXiv:2103.10427, 2021. Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson. Transient non-stationarity and generalisation in deep reinforcement learning. arXiv preprint arXiv:2006.05826, 2020. 27 Preprint Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably eﬃcient for oﬄine RL? In International Conference on Machine Learning, pages 5084–5096. PMLR, 2021. Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances in neural information processing systems, 32, 2019. Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2014. Ksenia Konyushkova, Yutian Chen, Thomas Paine, Caglar Gulcehre, Cosmin Paduraru, Daniel J Mankowitz, Misha Denil, and Nando de Freitas. Active oﬄine policy selection. Advances in Neural Information Processing Systems, 34, 2021. Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing oﬀ-policy Q-learning In Conference on Neural Information Processing Systems, pages via bootstrapping error reduction. 11761–11771, 2019. Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit under-parameterization inhibits data-eﬃcient deep reinforcement learning. arXiv preprint arXiv:2010.14498, 2020a. Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for oﬄine reinforce- ment learning. arXiv preprint arXiv:2006.04779, 2020b. Aviral Kumar, Rishabh Agarwal, Aaron Courville, Tengyu Ma, George Tucker, and Sergey Levine. Value- based deep reinforcement learning requires explicit regularization. In RL for Real Life Workshop & Overparameterization: Pitfalls and Opportunities Workshop, ICML, 2021a. URL https://drive.google. com/file/d/1Fg43H5oagQp-ksjpWBf_aDYEzAFMVJm6/view. Aviral Kumar, Rishabh Agarwal, Tengyu Ma, Aaron Courville, George Tucker, and Sergey Levine. Dr3: Value-based deep reinforcement learning requires explicit regularization. arXiv preprint arXiv:2112.04716, 2021b. Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workﬂow for oﬄine model-free robotic reinforcement learning. arXiv preprint arXiv:2109.10813, 2021c. Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107–1149, 2003. Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Marco Wiering and Martijn van Otterlo, editors, Reinforcement Learning: State-of-the-Art, pages 45–73. Springer Berlin Heidelberg, 2012. Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, pages 5639–5650. PMLR, 2020. Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Oﬄine reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. arXiv preprint arXiv:1712.09913, 2017. Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. arXiv preprint arXiv:1808.01204, 2018. 28 Preprint Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dabney. On the eﬀect of auxiliary tasks on representation dynamics. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1–9. PMLR, 2021. Marlos C Machado, Sriram Srinivasan, and Michael Bowling. Domain-independent optimistic initialization for reinforcement learning. In Workshops at the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015. Charles H Martin and Michael W Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning. Journal of Machine Learning Research, 22(165): 1–73, 2021. Michael Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Konrad Zolna, Richard Powell, Julian Schrittwieser, et al. Starcraft ii unplugged: Large scale oﬄine reinforcement learning. In Deep RL Workshop NeurIPS 2021, 2021. Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoﬀrey Irving, and Nat McAleese. Teaching language models to support answers with veriﬁed quotes. arXiv preprint arXiv:2203.11147, 2022. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015. Hossein Mobahi, Mehrdad Farajtabar, and Peter L Bartlett. Self-distillation ampliﬁes regularization in Hilbert space. arXiv preprint arXiv:2002.05715, 2020. Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepezvari, Satinder Singh, et al. Behaviour suite for reinforcement learning. arXiv preprint arXiv:1908.03568, 2019. Georg Ostrovski, Pablo Samuel Castro, and Will Dabney. The diﬃculty of passive learning in deep reinforce- ment learning. Advances in Neural Information Processing Systems, 34, 2021. Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter selection for oﬄine reinforcement learning. arXiv preprint arXiv:2007.09055, 2020. Jeﬀrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. Advances in Neural Information Processing Systems, 2017. Dean A Pomerleau. ALVINN: An autonomous land vehicle in a neural network. In Conference on Neural Information Processing Systems, pages 305–313, 1989. Martin Riedmiller. Neural ﬁtted Q iteration – ﬁrst experiences with a data eﬃcient neural reinforcement learning method. In João Gama, Rui Camacho, Pavel B. Brazdil, Alípio Mário Jorge, and Luís Torgo, editors, European Conference on Machine Learning, pages 317–328, 2005. Levent Sagun, Léon Bottou, and Yann LeCun. Singularity of the hessian in deep learning. arXiv preprint arXiv:1611.07476, 2016. Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017. Amartya Sanyal, Varun Kanade, Philip HS Torr, and Puneet K Dokania. Robustness via deep low-rank representations. arXiv preprint arXiv:1804.07090, 2018. Tianyu Shi, Dong Chen, Kaian Chen, and Zhaojian Li. Oﬄine reinforcement learning for autonomous driving with safety and exploration enhancement. arXiv preprint arXiv:2110.07067, 2021. 29 Preprint Yeonjong Shin and George Em Karniadakis. Trainability of relu networks and data-dependent initialization. Journal of Machine Learning for Modeling and Computing, 1(1), 2020. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, and Adrian Bolton. Mastering the game of go without human knowledge. Nature, 550(7676):354–359, 2017. Edward H Simpson. The interpretation of interaction in contingency tables. Journal of the Royal Statistical Society: Series B (Methodological), 13(2):238–241, 1951. Samuel L Smith and Quoc V Le. A Bayesian perspective on generalization and stochastic gradient descent. arXiv preprint arXiv:1710.06451, 2017. Samuel L Smith, Benoit Dherin, David GT Barrett, and Soham De. On the origin of implicit regularization in stochastic gradient descent. arXiv preprint arXiv:2101.12176, 2021. Tijmen Tieleman, Geoﬀrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012. Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-learning. In Thirtieth AAAI conference on artiﬁcial intelligence, 2016. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019. Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for oﬄine reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021. Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations, 2020. A Appendix A.1 The impact of depth Pennington and Worah (2017) explored the relationship between the rank and the depth of a neural network at initialization and found that the rank of each layer’s feature matrix decreases proportionally to the index of a layer in a deep network. Here, we test the performance of a feedforward network on the bsuite task trained using Double Q-learning (Van Hasselt et al., 2016) with 2, 8, and 16 layers to see the eﬀect of the number of layers on the eﬀective rank. All our networks use ReLU activation functions and use He initialization (He et al., 2015). Figure 23 illustrates that the rank collapses as one progresses from lower layers to higher layers proportionally at the end of training as well. The network’s eﬀective rank (rank of the penultimate layer) drops to a minimal value on all three tasks regardless of the network’s number of layers. The last layer of a network will act as a bottleneck; thus, a collapse of the eﬀective rank would reduce the expressivity. Nevertheless, a deeper network with the same rank as a shallower one can learn to represent a larger class of functions (be less under-parametrized). The agents exhibit poor performance when the eﬀective rank collapses to 1. At that point, all the ReLU units die or become zero irrespective of input. Thus on bsuite, deeper networks –four and eight layered networks– performed worse than two layered MLP. A.2 Spectral density of hessian for oﬄine RL Analyzing the eigenvalue spectrum of the Hessian is a common way to investigate the learning dynamics and the loss surface of deep learning methods (Ghorbani et al., 2019; Dauphin et al., 2014). Understanding Hessian’s loss landscape and eigenvalue spectrum can help us design better optimization algorithms. Here, 30 Preprint (L) (C) (R) Figure 23: [bsuite] The ranks and depths of the networks: The evolution of the ranks across diﬀerent layers of deep neural networks. The ﬁgure on the left (L) is for the ranks of catch across diﬀerent layers. The ﬁgure at the center (C) is for the ranks of mountain_car across diﬀerent layers. The ﬁgure on the right (R) is for cartpole. we analyze the eigenvalue spectrum of a single hidden layer feedforward network trained on the bsuite Catch dataset from RL Unplugged (Gulcehre et al., 2020) to understand the loss-landscape of a network with a low eﬀective rank compared to a model with higher rank at the end of the training. As established in Figure 24, ELU activation function network has a signiﬁcantly higher eﬀective rank than the ReLU network. By comparing those two networks, we also look into the diﬀerences in the eigenvalue spectrum of a network with high and low rank. Since the network and the inputs are relatively low-dimensional, we computed the full Hessian over the dataset rather than a low-rank approximation. The eigenvalue spectrum of the Hessian with ReLU and the ELU activation functions is shown in Figure 24. The rank collapse is faster for ReLU than ELU. After 900k gradient updates, the ReLU network concentrates 92% of the eigenvalues of Hessian around zero; this is due to the dead units in ReLU network (Glorot et al., 2011). On the other hand, the ELU network has a few very large eigenvalues after the same number of gradient updates. In Figure 25, we summarize the distribution of the eigenvalues of the Hessian matrices of the ELU and ReLU networks. As a result, the Hessian and the feature matrix of the penultimate layer of the network will be both low-rank. Moreover, in this case, the landscape of the ReLU network might be ﬂatter than the ELU beyond the notion of wide basins (Sagun et al., 2016). This might mean that the ReLU network ﬁnds a simpler solution. Thus, the ﬂatter landscape is conﬁrmed by the simpler function learned and less capacity used at the end of the training, which is induced by the lower rank representations. Figure 24: [bsuite Catch] spectral density of Hessian: The visualization of the spectral density of the full Hessian of a network trained with 64 units using ReLU (left) and ELU (right) activation functions. The eigenvalues of the Hessian of the oﬄine DQN with ReLU activation function are concentrated around 0 and most of them are less than 1. The eigenvalues of the ELU network also concentrate around 0 with a few large outlier eigenvalues. 31 k n a R 35 30 25 20 15 10 5 0 2 Layer Net 8 Layer Net 16 Layer Net 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Network's Depth k n a R e v i t c e f f E 6 5 4 3 2 1 0 2 Layer Net 8 Layer Net 16 Layer Net 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Network's Depth k n a R e v i t c e f f E 6 5 4 3 2 1 0 2 Layer Net 8 Layer Net 16 Layer Net 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Network's Depth y t i s n e D 2 10 0 10 −2 10 −4 10 −6 10 −8 10 −10 10 Learner steps = 0 Learner steps = 900k −0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Eigenvalue y t i s n e D 2 10 0 10 −2 10 −4 10 −6 10 −8 10 −10 10 Learner steps = 0 Learner Steps = 900k 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Eigenvalue Preprint Figure 25: [bsuite Catch] Hessian eigenvalues (evs) for oﬄine DQN: We visualize the percentages of positive, negative, and near-zero eigenvalues of the Hessian for oﬄine DQN with ELU and ReLU activation functions on the catch dataset from bsuite. If the absolute value of an eigenvalue is less than 1e-7, we consider it as a near-zero eigenvalue. We can see that for ELU network, near-zero, positive and negative eigenvalues are almost evenly distributed. However, with ReLU network majority of eigenvalues are near-zero (90% of the evs are exactly zero), very few negative (2 %) and some positive eigenvalues (7.1 %). A.3 DeepMind lab: performance vs the eﬀective ranks In Figure 26, we show the correlation between the eﬀective rank and the performance of R2D2, R2D2 trained with CURL and R2D2 trained with SAM. When we look at each variant separately or as an aggregate, we don’t see a strong correlation between the performance and the rank of an agent. Figure 26: [DeepMind lab] The eﬀective rank and the performance: Correlations between feature ranks and episode returns for diﬀerent exploration noises on DeepMind lab dataset. We include data from three models: R2D2, R2D2 trained with CURL, and R2D2 trained with SAM. We do not observe a strong correlation between the eﬀective rank and performance across diﬀerent noise exploration levels in the data. A.4 SAM optimizer: does smoother loss landscape aﬀect the behavior of the rank collapse? We study the relationship between smoother loss landscapes and rank collapse. We use Sharpness Aware Minimization (SAM) (Foret et al., 2021) loss for potentially creating ﬂatter loss surfaces in order to see if smoother loss landscapes aﬀect the rank and performance dynamics diﬀerently. Figures 27 and 28 show the evolution of feature ranks and generalization performance in Atari and DeepMind lab respectively. We do not observe a very clear relation between the extent of smoothing the loss and the feature ranks or generalization performance. 32 % near-zero evs 100 80 60 40 20 0 % positive evs % negative evs ReLU DQN ELU DQN Eps: 0.0, Kendall τ:0.25, Pearson ρ:0.30 Eps: 0.01, Kendall τ:0.09, Pearson ρ:0.05 Eps: 0.1, Kendall τ:0.32, Pearson ρ:0.41 Eps: 0.25, Kendall τ:0.14, Pearson ρ:0.13 Variant CURL R2D2 SAM s k n a R e v i t c e f f E l i a n m r e T 200 100 0 0 2.5 5 7.5 10 0 5 10 15 0 10 20 30 0 10 20 30 40 Terminal Episode Return Preprint Figure 27: [Atari] Sharpness Aware Minimization (SAM) Loss: Evolution of ranks as we increase the weight of auxiliary losses. We see that some amount of weight on the SAM loss helps mitigate the extent of rank collapse but we observe no clear relationship with the agent performance. Figure 28: [DeepMind lab-SeekAvoid Snapshot] Sharpness Aware Minimization (SAM) Loss We do not observe any rank collapse as we continue training with the LSTM networks (because of the tanh activations discussed in Section 6) used in DeepMind lab dataset over a spectrum of diﬀerent weights for the SAM loss. A.5 Eﬀective rank and the value error A curious relationship is between the eﬀective rank and the value error, because a potential for the rank collapse or Phase 3 with the TD-learning algorithms can be the errors propagating thorough the bootstrapped targets. Figure 29 shows the correlation between the value error and the eﬀective ranks. There is a strong anti-correlation between the eﬀective ranks and the performance of the agents except on the levels where the expert agent that generated the dataset performs poorly at the end of the training (e.g. IceHockey.) This 33 Preprint Figure 29: [Atari]: These plots shows the correlation between the value error and the eﬀective rank for oﬄine DQN agent trained for 20M steps on online policy selection games for Atari. There is an apparent anti-correlation between the eﬀective rank and the value error. Namely, as the value error of an agent when evaluated in the environment increases the eﬀective rank decreases. The correlation is signiﬁcant on most Atari levels except IceHockey where the expert agent that generated the dataset performs poorly. makes the hypothesis that the extrapolation error can cause rank collapse (or push the agent to Phase 3) more plausible. A.6 CURL on DeepMind Lab In Figure 30 shows the CURL results on DeepMind lab dataset. We couldn’t ﬁnd any consistent pattern across various CURL loss weights. Figure 30: [DeepMind lab-SeekAvoid:] auxiliary losses Evolution of ranks as we increase the weight of auxiliary loss. While some auxiliary loss helps the model perform well, there is no clear correlation between rank and performance. 34 Preprint A.7 Learning rate evolution In Figure 31, we also perform a hyperparameter selection by evaluating the model in the environment at various stages during the training. As the oﬄine DQN is trained longer, the optimal learning rate for the best agent performance when evaluated online goes down. As one increases the number of training steps of an agent, we need to change the learning rate accordingly since the number of training steps aﬀects the best learning rate. Figure 31: [Atari]: Evolution of the optimal learning rate found by online evaluations in the environment. As the model is trained longer the optimal learning rate found by online evaluations goes down. A.8 Learning curves long training regime and the diﬀerent phases of learning In this subsection, we investigate the eﬀect of changing learning rates on the eﬀective rank and the performance of the agent on RL Unplugged Atari online policy selection games. We train the oﬄine DQN for 20M learning steps which is ten times longer than typical oﬄine Atari agents (Gulcehre et al., 2020). We evaluated twelve learning rates in (cid:2)10−2, 10−5(cid:3) equally spaced in logarithmic space. We show the eﬀective rank learning and performance curves in Figure 32. It is easy to identify diﬀerent phases of learning in most of those curves. A.9 Eﬀective rank and the performance Figure 33 shows the correlation between the eﬀective rank and the performance of an agent trained with minibatch of size 32. On most Atari online policy selection games it is possible to see a very strong correlation but on some games the correlation is not there. It seems like even Figure 34 depicts the correlation between the eﬀective rank and the performance of a DQN agent with ReLU activation function. There is a signiﬁcant correlation on most Atari games. As we discussed earlier, long training setting with ReLU activation functions where the eﬀect of the rank is the strongest on the performance. A.10 Interventions to eﬀective rank and randomized controlled trials (RCT) As the rank is a continuous variable, we can ﬁlter out the experiments with certain ranks with a threshold τ . In the control case (λ(1)), we assume β > τ and for λ(0), we would have β ≤ τ . We can also write this as: E[λ|do(β > τ )] − E[λ|do(β ≤ τ )]. (5) A.11 Computation of feature ranks Here, we present the Python code-stub that we used across our experiments (similar to Kumar et al. (2020a)) to compute the feature ranks of the pre-output layer’s features: import numpy as np def compute_rank_from_features(feature_matrix, rank_delta=0.01): 35 0.00030 0.00025 0.000231 e t a R g n n r a e L i 0.00020 0.00015 0.00010 0.00005 0.000066 0.000019 1e+05 1.2e+06 2.4e+06 3.6e+06 4.8e+06 6e+06 7.2e+06 8.4e+06 9.6e+061.08e+071.2e+071.32e+071.44e+07 Learning Steps Preprint Figure 32: [Atari] Eﬀective rank curves: Eﬀective rank and performance oﬄine DQN agent across nine Atari online policy selection games. We train the oﬄine DQN agent for 20M learning steps and evaluated 12 learning rates in (cid:2)10−2, 10−5(cid:3) equally spaced in logarithmic space. Let us note that in all games rank goes down early in the training (Phase 1), then goes up (phase 2) and for some learning rate the eﬀective rank collapses (Phase 3). Correspondingly, the performance is low in the beginning of the training (Phase 1), goes up and stays high for a while (Phase 2), and sometimes it the performance collapses (Phase 3). 36 Preprint Figure 33: [Atari] The correlation between ranks and returns for DQN trained with minibatch size of 32. We ran each network with three learning rates and ﬁve diﬀerent seeds but we only show the mean across those ﬁve seeds here. We can see very strong correlations on some games, but that correlation is not consistent. Figure 34: [Atari] The correlation between ranks and returns for DQN trained for a network trained for 20M learning steps and 12 diﬀerent learning rates with minibatch size of 256. We can see that there is signiﬁcant positive correlation between the rank and the learning rates for most online policy selection games. 37 BeamRider, Spearman corr: 0.75 DemonAttack, Spearman corr: 0.87 200 DoubleDunk, Spearman corr: 0.44 IceHockey, Spearman corr: -0.06 1000 1500 2000 100 50 0 1.8 1.4 1 s k n a R e v i t c e f f E l i a n m r e T −16 −8 RoadRunner, Spearman corr: -0.48 −12 −4 12 9 6 3 0 0 MsPacman, Spearman corr: 0.59 10000 5000 0 Robotank, Spearman corr: 0.46 1000 2000 3000 150 100 50 0 75 50 25 0 75 50 25 0 1.15 1.10 1.05 1 100 90 80 70 60 50 90 60 30 0 −24 −22 Pooyan, Spearman corr: -0.33 −22.5 −23.5 −23 Learning Rates 3e-05 0.0001 0.0003 0 Zaxxon, Spearman corr: 0.80 100 300 200 0 250 500 750 0 20 40 Episode return 60 0 2000 4000 6000 8000 Preprint Hyper-parameters Training batch size Rank calculation batch size Num training steps Learning rate Optimizer Feedforward hidden layer size Num hidden layers Activation Memory Discount bsuite 32 512 1e6 3e-4 Adam 64 2 ReLU None 0.99 Atari 256 512 2e6 3e-5 Adam 512 1 ReLU None 0.99 DeepMind lab 4 (episodes) 512 2e4 1e-3 Adam 256 1 ReLU LSTM gates) LSTM 0.997 (tanh for Table 3: The default hyper-parameters used in our work across diﬀerent domains. """"""Computes rank of the features based on how many singular values are significant."""""" sing_values = np.linalg.svd(feature_matrix, compute_uv=False) cumsum = np.cumsum(sing_values) nuclear_norm = np.sum(sing_values) approximate_rank_threshold = 1.0 - rank_delta threshold_crossed = ( cumsum >= approximate_rank_threshold * nuclear_norm) effective_rank = sing_values.shape[0] - np.sum(threshold_crossed) + 1 return effective_rank A.12 Hyperparameters Here, we list the standard set of hyper-parameters that were used in diﬀerent domains: bsuite, Atari, and DeepMind Lab respectively. These are the default hyper-parameters, which may diﬀer when stated so in our speciﬁc ablation studies. For the DMLLAB task, we use the same network that was used in Gulcehre et al. (2021). For all the Atari tasks, we use the same convolution torso that was used in Gulcehre et al. (2020) which involves three layers of convolution with ReLU activations in between. • Layer 1 - Conv2d(channels=32, kernel_shape=[8, 8], stride=[4, 4]) • Layer 2 - Conv2d(channels=64, kernel_shape=[4, 4], stride=[2, 2]) • Layer 3 - Conv2d(channels=64, kernel_shape=[3, 3], stride=[1, 1]) A.13 bsuite phase transitions and bottleneck capacity We illustrate the phase transition of a simple MLPs with ReLU activations. In Figure 35, we have a network of size (64, bottleneck units, 64) where we vary the number of bottleneck units. In Figure 36, we have a network of size (64, bottleneck units) where we vary the number of bottleneck units. In both cases, having smaller number of bottleneck units reduces the performance of the mode and agents were able to solve the problem even when the penultimate layer’s eﬀective rank was small. With the larger learning rate the right handside ﬁgures (b), the eﬀective ranks tend to be lower. A.14 Activation sparsity on bsuite In Figure 37, we show that the activations of the ReLU network becomes very sparse during the course of training. The sparsity of the ReLU units seems to be signiﬁcantly higher than the ELU units at the end of training. 38 Preprint a) b) Figure 35: [bsuite] - Catch dataset: Phase transition plots of a network with hidden layers of size (64, bottleneck units, 64). On the y-axis, we show the log2(bottleneck units), and x-axis is the number of gradient steps. The ﬁgures on the left hand-side (a) is trained with the learning rate with 8e-5 and right hand-side (b) experiments are trained with learning rate of 3e-4. The ﬁrst row shows the episodic returns. The second row shows the eﬀective rank of the second layer (bottleneck units) and the third row is showing the penultimate layer’s eﬀective rank. The eﬀective rank collapses much faster with the learning rate of 4e-3 than 5e-8. The low ranks can still have good performance. The low bottleneck units causes the eﬀective rank of the last layer to collapse faster. The performance of the network with the small number of bottleneck units is poor. The eﬀective rank of the small number of bottleneck units is smaller. 39 Preprint Figure 36: [bsuite] - Catch dataset: Phase transition plots of a network with hidden layers of size (64, bottleneck units). On the y-axis, we show the log2(bottleneck units), and x-axis is the number of gradient steps. The ﬁgures on the left hand-side (a) is trained with the learning rate with 8e-5 and right hand-side (b) experiments are trained with learning rate of 3e-4. The ﬁrst row shows the episodic returns. The second row shows the eﬀective rank of the second layer (bottleneck units). The eﬀective rank collapses much faster with the learning rate of 4e-3 than 5e-8. The low ranks can still have good performance. The small number of bottleneck units causes the eﬀective rank of the layer to collapse faster. The performance of the network with the small number of bottleneck units is poor. The eﬀective rank of the small number of bottleneck units is smaller. Figure 37: [bsuite Catch] Gram matrices of activations: Gram matrices of activations of a two-layer MLP with ReLU and ELU activation functions. The activations of the ReLU units become sparser when compared to ELU units at the end of the training due to dead ReLU units. 40 Initialization (step 0) Convergence (step 900k) ReLU ELU","['l', 'x', 'preprint', 'empirical', 'study', 'implicit', 'regularization', 'deep', 'oﬄine', 'caglar', 'gulcehre∗', 'srivatsan', 'ostrovski', 'farajtabar', 'deepmind', 'abstract', 'deep', 'neural', 'network', 'commonly', 'use', 'function', 'approximator', 'oﬄine', 'inforcement', 'learn', 'prior', 'work', 'show', 'neural', 'net', 'train', 'tdlearning', 'gradient', 'descent', 'exhibit', 'implicit', 'regularization', 'characterize', 'parameterization', 'network', 'speciﬁcally', 'rank', 'penultimate', 'feature', 'layer', 'also', 'call', 'eﬀective', 'rank', 'observe', 'drastically', 'collapse', 'training', 'turn', 'collapse', 'argue', 'reduce', 'model', 'ability', 'far', 'adapt', 'later', 'stage', 'learn', 'lead', 'diminished', 'ﬁnal', 'performance', 'association', 'eﬀective', 'rank', 'performance', 'make', 'eﬀective', 'rank', 'compelling', 'oﬄine', 'rl', 'primarily', 'oﬄine', 'policy', 'evaluation', 'work', 'conduct', 'careful', 'empirical', 'study', 'relation', 'eﬀective', 'rank', 'performance', 'oﬄine', 'dataset', 'atari', 'deepmind', 'lab', 'observe', 'direct', 'association', 'exist', 'restrict', 'setting', 'disappear', 'extensive', 'hyperparameter', 'sweep', 'also', 'empirically', 'identify', 'phase', 'learn', 'explain', 'impact', 'implicit', 'regularization', 'learning', 'dynamic', 'find', 'bootstrappe', 'alone', 'insuﬃcient', 'explain', 'collapse', 'eﬀective', 'rank', 'far', 'show', 'several', 'factor', 'confound', 'relationship', 'eﬀective', 'rank', 'performance', 'conclude', 'study', 'association', 'simplistic', 'assumption', 'highly', 'misleading', 'content', 'introduction', 'background', 'eﬀective', 'rank', 'implicit', 'underregularization', 'auxiliary', 'loss', 'deep', 'oﬄine', 'experimental', 'setup', 'eﬀective', 'rank', 'performance', 'lifespan', 'learn', 'deep', 'qnetwork', 'eﬀect', 'dataset', 'size', '∗indicate', 'joint', 'author', 'preprint', 'interaction', 'rank', 'performance', 'eﬀect', 'activation', 'function', 'optimization', 'eﬀect', 'loss', 'function', 'qlearning', 'behavior', 'clone', 'curl', 'eﬀect', 'selfsupervision', 'tandem', 'oﬄine', 'policy', 'selection', 'robustness', 'input', 'perturbation', 'discussion', 'appendix', 'impact', 'depth', 'spectral', 'density', 'hessian', 'oﬄine', 'a3', 'deepmind', 'lab', 'performance', 'eﬀective', 'rank', 'smooth', 'loss', 'behavior', 'rank', 'collapse', 'eﬀective', 'rank', 'value', 'error', 'a6', 'curl', 'deepmind', 'lab', 'learning', 'rate', 'evolution', 'a8', 'learn', 'curve', 'long', 'training', 'regime', 'diﬀerent', 'phase', 'learn', 'eﬀective', 'rank', 'performance', 'a10', 'intervention', 'eﬀective', 'rank', 'randomize', 'control', 'trial', 'rct', 'a11', 'computation', 'feature', 'rank', 'a12', 'hyperparameter', 'a13', 'bsuite', 'phase', 'transition', 'bottleneck', 'capacity', 'a14', 'activation', 'sparsity', 'bsuite', 'introduction', 'use', 'deep', 'network', 'function', 'approximator', 'reinforcement', 'learn', 'refer', 'deep', 'reinforcement', 'learn', 'become', 'dominant', 'paradigm', 'solve', 'complex', 'task', 'recently', 'drl', 'literature', 'focus', 'paradigm', 'agent', 'interact', 'environment', 'explore', 'learn', 'lead', 'remarkable', 'result', 'go', 'silver', 'vinyal', 'dota', 'berner', 'robotic', 'andrychowicz', 'unfortunately', 'need', 'interact', 'environment', 'make', 'algorithm', 'unsuitable', 'unsafe', 'many', 'realworld', 'application', 'action', 'take', 'serious', 'ethical', 'harmful', 'consequence', 'preprint', 'costly', 'contrast', 'oﬄine', 'paradigm', 'fu', 'fujimoto', 'also', 'know', 'batch', 'ernst', 'lange', 'agent', 'learn', 'ﬁxed', 'dataset', 'previously', 'log', 'possibly', 'unknown', 'agent', 'ability', 'make', 'oﬄine', 'applicable', 'real', 'world', 'recently', 'show', 'oﬄine', 'method', 'couple', 'learning', 'loss', 'suﬀer', 'eﬀective', 'rank', 'collapse', 'penultimate', 'layer', 'activation', 'render', 'network', 'become', 'underparameterized', 'far', 'demonstrate', 'signiﬁcant', 'fraction', 'atari', 'game', 'collapse', 'eﬀective', 'rank', 'collapse', 'correspond', 'performance', 'degradation', 'subsequently', 'explain', 'rank', 'collapse', 'phenomenon', 'analyze', 'tdlearning', 'loss', 'bootstrapped', 'target', 'kernel', 'linear', 'regression', 'setup', 'simpliﬁed', 'scenario', 'bootstrappe', 'lead', 'selfdistillation', 'cause', 'severe', 'underparametrization', 'poor', 'performance', 'also', 'observe', 'analyze', 'nevertheless', 'study', 'rank', 'representation', 'supervised', 'learning', 'set', 'image', 'classiﬁcation', 'task', 'argue', 'low', 'rank', 'lead', 'well', 'performance', 'thus', 'lowrank', 'representation', 'act', 'implicit', 'regularizer', 'figure', 'atari', 'rank', 'performance', 'broad', 'vs', 'narrow', 'hyperparameter', 'sweep', 'correlation', 'eﬀective', 'rank', 'agent', 'performance', 'end', 'training', 'diﬀerent', 'atari', 'game', 'report', 'regression', 'line', 'narrow', 'sweep', 'cover', 'single', 'oﬄine', 'small', 'minibatch', 'size', 'learning', 'rate', 'sweep', 'similar', 'hyperparameter', 'sweep', 'deﬁne', 'unplugged', 'gulcehre', 'broad', 'setting', 'include', 'datum', 'diﬀerent', 'model', 'large', 'hyperparameter', 'sweep', 'narrow', 'setup', 'positive', 'relationship', 'eﬀective', 'rank', 'agent', 'performance', 'relationship', 'disappear', 'broad', 'data', 'setup', 'almost', 'reverse', 'typically', 'machine', 'learning', 'rely', 'empirical', 'evidence', 'extrapolate', 'rule', 'behavior', 'learned', 'system', 'experimental', 'datum', 'often', 'extrapolation', 'base', 'limited', 'number', 'experiment', 'due', 'constraint', 'computation', 'time', 'unfortunately', 'extremely', 'useful', 'extrapolation', 'always', 'generalize', 'well', 'setting', '2020a', 'concretely', 'propose', 'causal', 'link', 'rank', 'performance', 'system', 'tempt', 'extrapolate', 'result', 'agent', 'perform', 'poorly', 'rank', 'collapse', 'existence', 'causal', 'preprint', 'link', 'herein', 'refer', 'rank', 'collapse', 'hypothesis', 'work', 'careful', 'largescale', 'empirical', 'analysis', 'potential', 'causal', 'link', 'use', 'oﬄine', 'setting', 'also', 'use', '2020a', 'also', 'tandem', 'ostrovski', 'set', 'existence', 'causal', 'link', 'beneﬁcial', 'oﬄine', 'rl', 'control', 'rank', 'model', 'improve', 'performance', 'see', 'regularization', 'term', 'explore', '2020a', 'investigate', 'eﬀective', 'rank', 'use', 'model', 'selection', 'setting', 'oﬄine', 'evaluation', 'prove', 'elusive', 'key', 'observation', 'rank', 'performance', 'correlate', 'restrict', 'setting', 'correlation', 'disappear', 'increase', 'range', 'hyperparameter', 'model', 'figure', 'athis', 'factor', 'hyperparameter', 'architecture', 'confound', 'rank', 'penultimate', 'layer', 'factor', 'control', 'carefully', 'conclusion', 'draw', 'experiment', 'base', 'rank', 'mislead', 'instead', 'show', 'diﬀerent', 'factor', 'aﬀect', 'network', 'rank', 'aﬀecte', 'performance', 'ﬁnding', 'indicate', 'factor', 'variation', 'control', 'many', 'still', 'unaware', 'rank', 'alone', 'misleading', 'indicator', 'performance', 'deep', 'q', 'network', 'exhibit', 'phase', 'training', 'show', 'rank', 'use', 'identify', 'diﬀerent', 'stage', 'learn', 'qlearne', 'factor', 'control', 'carefully', 'believe', 'study', 'similar', 'eg', 'reemphasize', 'importance', 'critically', 'judge', 'understanding', 'behavior', 'neural', 'network', 'base', 'simpliﬁed', 'mathematical', 'model', 'empirical', 'evidence', 'limited', 'set', 'experiment', 'key', 'observation', 'deep', 'qlearning', 'approach', 'go', 'phase', 'learn', 'simple', 'behavior', 'complex', 'underparameterization', 'figure', 'phase', 'identiﬁe', 'eﬀective', 'rank', 'performance', 'give', 'task', 'athe', 'phase', 'learn', 'happen', 'training', 'model', 'test', 'time', 'third', 'phase', 'learning', 'potentially', 'lead', 'agent', 'lose', 'representation', 'capacity', 'ensue', 'poor', 'performance', 'figure', 'lifespan', 'learn', 'deep', 'qlearne', 'plot', 'side', 'illustrate', 'evolution', 'eﬀective', 'rank', 'plot', 'righthand', 'side', 'demonstrate', 'evolution', 'performance', 'training', 'ﬁrst', 'phase', 'model', 'learn', 'easytolearn', 'behavior', 'simplistic', 'nature', 'ignore', 'many', 'factor', 'environmental', 'variation', 'eﬀective', 'rank', 'collapse', 'minimal', 'value', 'ﬁrst', 'phase', 'model', 'need', 'large', 'capacity', 'learn', 'simple', 'behavior', 'phase', 'model', 'learn', 'complex', 'behavior', 'identify', 'obtain', 'large', 'return', 'policy', 'evaluate', 'environment', 'typically', 'supervised', 'learning', 'phase', 'follow', 'overﬁtte', 'however', 'oﬄine', 'speciﬁcally', 'tdlearning', 'approach', 'try', 'observe', 'often', 'follow', 'underﬁttingunderparameterization', 'phase', 'k', 'r', 'e', 'r', 'u', 'e', 'r', 'p', 'e', 'training', 'step', 'training', 'step', 'phase', 'learn', 'phase', 'simple', 'behavior', 'phase', 'complex', 'behavior', 'phase', 'underparametrization', 'preprint', 'organize', 'rest', 'paper', 'contribution', 'order', 'introduce', 'paper', 'follow', '•', 'section', 'present', 'related', 'work', 'section', 'explain', 'experimental', 'design', 'rely', 'section', 'study', 'extent', 'impact', 'diﬀerent', 'intervention', 'architecture', 'loss', 'function', 'optimization', 'causal', 'link', 'rank', 'agent', 'performance', 'intervention', 'model', 'introduce', 'auxiliary', 'loss', 'eg', 'laskin', 'activation', 'function', 'increase', 'eﬀective', 'rank', 'necessarily', 'improve', 'performance', 'ﬁnding', 'indicate', 'rank', 'penultimate', 'layer', 'enough', 'explain', 'agent', 'performance', 'also', 'identify', 'setting', 'rank', 'strongly', 'correlate', 'performance', 'dqn', 'relu', 'activation', 'many', 'learn', 'step', 'ﬁxed', 'dataset', 'section', 'show', 'deep', 'q', 'network', 'go', 'stage', 'learning', 'stage', 'identiﬁe', 'use', 'rank', 'hyperparameter', 'model', 'control', 'carefully', 'section', 'describe', 'main', 'outcome', 'investigation', 'particularly', 'analyze', 'impact', 'intervention', 'describe', 'early', 'provide', 'counterexample', 'help', 'contradict', 'rank', 'collapse', 'hypothesis', 'establish', 'link', 'rank', 'performance', 'aﬀecte', 'several', 'confound', 'factor', 'variation', 'section', 'ablate', 'compare', 'robustness', 'dqn', 'model', 'respect', 'random', 'perturbation', 'introduce', 'evaluate', 'agent', 'environment', 'find', 'oﬄine', 'dqn', 'agent', 'robust', 'behavior', 'clone', 'agent', 'high', 'eﬀective', 'rank', 'section', 'present', 'summary', 'ﬁnding', 'implication', 'oﬄine', 'rl', 'along', 'potential', 'future', 'research', 'direction', 'background', 'eﬀective', 'rank', 'implicit', 'underregularization', 'choice', 'architecture', 'optimizer', 'impose', 'speciﬁc', 'implicit', 'bias', 'prefer', 'certain', 'solution', 'study', 'impact', 'implicit', 'bias', 'generalization', 'neural', 'network', 'often', 'refer', 'implicit', 'regularization', 'plethora', 'literature', 'study', 'diﬀerent', 'source', 'implicit', 'regularization', 'initialization', 'parameter', 'glorot', 'bengio', 'architecture', 'sagun', 'optimization', 'barrett', 'dherin', 'rank', 'feature', 'matrix', 'neural', 'network', 'source', 'implicit', 'regularization', 'active', 'area', 'study', 'speciﬁcally', 'context', 'supervised', 'learning', 'arora', 'pennington', 'daneshmand', 'work', 'study', 'phenomenon', 'implicit', 'regularization', 'eﬀective', 'rank', 'last', 'hidden', 'layer', 'network', 'formally', 'deﬁne', 'deﬁnition', 'eﬀective', 'rank', 'recently', 'study', 'impact', 'eﬀective', 'rank', 'generalization', 'general', 'context', 'batch', 'size', 'n', 'unit', 'feature', 'layer', 'propose', 'eﬀective', 'rank', 'formulation', 'present', 'equation', 'feature', 'matrix', '∈', 'n', '≥', 'use', 'threshold', 'value', 'δ', 'singular', 'value', 'σiφ', 'descend', 'order', 'provide', 'speciﬁc', 'implementation', 'eﬀective', 'rank', 'use', 'paper', 'preprint', 'terminology', 'assumption', 'note', 'threshold', 'value', 'work', 'ﬁxe', 'similar', '2020a', 'paper', 'use', 'term', 'eﬀective', 'rank', 'describe', 'rank', 'last', 'hidden', 'layer', 'feature', 'state', 'otherwise', 'choice', 'consistent', 'prior', 'work', '2020a', 'last', 'layer', 'act', 'representation', 'bottleneck', 'output', 'layer', 'suggest', 'eﬀective', 'rank', 'deep', 'rl', 'model', 'train', 'tdlearne', 'objective', 'collapse', 'implicit', 'regularization', 'happen', 'repeat', 'iteration', 'dataset', 'selfdistillation', 'eﬀect', 'emerge', 'bootstrappe', 'loss', 'support', 'hypothesis', 'theoretical', 'analysis', 'empirical', 'result', 'theoretical', 'analysis', 'provide', 'paper', 'assume', 'simpliﬁed', 'set', 'inﬁnitesimally', 'small', 'learning', 'rate', 'batch', 'gradient', 'descent', 'linear', 'network', 'line', 'theory', 'paper', 'topic', 'auxiliary', 'loss', 'additionally', 'ablate', 'eﬀect', 'use', 'auxiliary', 'loss', 'agent', 'performance', 'rank', 'speciﬁcally', 'choose', 'contrastive', 'unsupervised', 'representation', 'reinforcement', 'learn', 'curl', 'laskin', 'loss', 'design', 'use', 'contrastive', 'selfsupervised', 'learning', 'setting', 'standard', 'rl', 'algorithm', 'r', 'lqs', 'r', 'lq', 'refer', 'standard', 'loss', 'curl', 'loss', 'contrastive', 'loss', 'feature', 'current', 'observation', 'randomly', 'augmented', 'observation', 'ˆs', 'describe', 'laskin', 'also', 'ablate', 'sharpnessaware', 'minimization', 'approach', 'seek', 'parameter', 'uniformly', 'low', 'loss', 'neighborhood', 'lead', 'ﬂatter', 'local', 'minima', 'choose', 'means', 'well', 'understand', 'geometry', 'loss', 'landscape', 'help', 'inform', 'correlation', 'eﬀective', 'rank', 'agent', 'performance', 'oﬄine', 'algorithm', 'experiment', 'focus', 'analyze', 'mostly', 'deep', 'qlearning', 'dqn', 'simplify', 'experiment', 'facilitate', 'deep', 'investigation', 'rank', 'collapse', 'hypothesis', 'deep', 'oﬄine', 'online', 'require', 'interaction', 'environment', 'learn', 'use', 'random', 'exploration', 'however', 'online', 'interaction', 'environment', 'unsafe', 'unethical', 'oﬄine', 'method', 'suﬀer', 'problem', 'leverage', 'oﬄine', 'datum', 'learn', 'policy', 'enable', 'application', 'rl', 'real', 'world', 'menick', 'konyushkova', 'focus', 'oﬄine', 'importance', 'realworld', 'application', 'previous', 'work', 'show', 'implicit', 'regularization', 'eﬀect', 'pronounced', 'oﬄine', '2020a', 'early', 'example', 'oﬄine', 'rl', 'algorithm', 'leastsquare', 'temporal', 'diﬀerence', 'method', 'bradtke', 'lagoudaki', 'parr', 'ﬁtte', 'qiteration', 'riedmiller', 'recently', 'valuebase', 'approach', 'oﬄine', 'rl', 'quite', 'popular', 'valuebase', 'approach', 'typically', 'low', 'value', 'estimate', 'unseen', 'stateaction', 'pair', '2020b', 'uncertainty', 'agarwal', 'et', 'also', 'include', 'rbve', 'gulcehre', 'category', 'regularize', 'q', 'function', 'rewarding', 'transition', 'prevent', 'learn', 'suboptimal', 'policy', 'similar', 'rbve', 'mathieu', 'also', 'show', 'method', 'use', 'singlestep', 'policy', 'improvement', 'work', 'well', 'task', 'large', 'action', 'space', 'low', 'stateaction', 'coverage', 'paper', 'simplicity', 'popularity', 'mainly', 'study', 'action', 'valuebase', 'method', 'oﬄine', 'dqn', 'agarwal', 'oﬄine', 'moreover', 'also', 'sparingly', 'use', 'batch', 'constrained', 'deep', 'qlearning', 'fujimoto', 'popular', 'oﬄine', 'use', 'behavior', 'policy', 'constrain', 'action', 'take', 'target', 'network', 'oﬄine', 'approach', 'explain', 'rely', 'pessimistic', 'value', 'estimate', 'jin', 'gulcehre', '2020b', 'mainly', 'oﬄine', 'dataset', 'lack', 'exhaustive', 'exploration', 'extrapolate', 'value', 'state', 'action', 'see', 'training', 'set', 'result', 'preprint', 'extrapolation', 'error', 'catastrophic', 'tdlearne', 'fujimoto', 'hand', 'online', 'common', 'practice', 'inductive', 'bias', 'keep', 'optimistic', 'value', 'function', 'encourage', 'exploration', 'machado', 'also', 'experiment', 'tandem', 'setting', 'propose', 'ostrovski', 'employ', 'independently', 'initialize', 'online', 'active', 'oﬄine', 'passive', 'network', 'training', 'loop', 'online', 'agent', 'explore', 'drive', 'data', 'generation', 'process', 'agent', 'perform', 'identical', 'learning', 'update', 'identical', 'sequence', 'training', 'batch', 'order', 'tandem', 'form', 'oﬄine', 'still', 'traditional', 'oﬄine', 'set', 'ﬁxed', 'dataset', 'tandem', 'behavior', 'policy', 'change', 'time', 'make', 'learn', 'nonstationary', 'interested', 'setting', 'agent', 'necessarily', 'reuse', 'datum', 'repeatedly', 'point', 'lyle', 'potential', 'cause', 'rank', 'collapse', 'experimental', 'setup', 'test', 'verify', 'diﬀerent', 'aspect', 'rank', 'collapse', 'hypothesis', 'potential', 'impact', 'agent', 'performance', 'run', 'large', 'number', 'experiment', 'bsuite', 'osband', 'atari', 'bellemare', 'deepmind', 'lab', 'beattie', 'environment', 'experiment', 'use', 'experimental', 'protocol', 'dataset', 'hyperparameter', 'state', 'otherwise', 'provide', 'detail', 'architecture', 'default', 'hyperparameter', 'a12', 'bsuite', 'run', 'ablation', 'experiment', 'bsuite', 'fully', 'oﬄine', 'set', 'oﬄine', 'dataset', 'one', 'use', 'use', 'dqn', 'agent', 'representative', 'td', 'learn', 'multilayer', 'feedforward', 'network', 'represent', 'value', 'function', 'bsuite', 'provide', 'small', 'playground', 'environment', 'let', 'test', 'certain', 'hypothesis', 'computationally', 'prohibitive', 'domain', 'eg', 'compute', 'hessian', 'respect', 'terminal', 'feature', 'agent', 'generalization', 'performance', 'atari', 'test', 'observation', 'also', 'true', 'higherdimensional', 'input', 'feature', 'image', 'run', 'experiment', 'atari', 'dataset', 'use', 'oﬄine', 'dqn', 'agent', 'representative', 'tdlearne', 'convolutional', 'network', 'function', 'approximator', 'atari', 'conduct', 'largescale', 'experiment', 'diﬀerent', 'conﬁguration', 'smallscale', 'experiment', 'oﬄine', 'dqn', 'atari', 'minibatch', 'size', 'train', 'gradient', 'step', 'diﬀerent', 'learning', 'rate', '−', '−', '−', '−', 'run', 'experiment', 'observe', 'observation', 'hold', 'default', 'training', 'scenario', 'identiﬁe', 'unplug', 'gulcehre', 'oﬄine', 'dqn', 'atari', 'minibatch', 'size', 'train', 'gradient', 'step', 'diﬀerent', 'learning', 'rate', '×', '×', '×', 'run', 'experiment', 'explore', 'eﬀect', 'reduce', 'minibatch', 'size', 'b', 'largescale', 'experiment', 'long', 'run', 'learning', 'rate', 'sweep', 'dqn25620', 'oﬄine', 'dqn', 'train', 'gradient', 'step', 'diﬀerent', 'learning', 'rate', 'evenly', 'space', 'logspace', 'train', 'minibatche', 'size', 'purpose', 'experiment', 'explore', 'eﬀect', 'wide', 'range', 'learn', 'rate', 'long', 'training', 'eﬀective', 'rank', 'long', 'run', 'intervention', 'dqnintervention', 'oﬄine', 'dqn', 'train', 'gradient', 'step', 'minibatche', 'size', 'diﬀerent', 'hyperparameter', 'intervention', 'activation', 'function', 'size', 'auxiliary', 'loss', 'purpose', 'experiment', 'understand', 'impact', 'intervention', 'eﬀective', 'rank', 'course', 'long', 'training', 'deepmind', 'lab', 'bsuite', 'atari', 'present', 'relatively', 'simple', 'fully', 'observable', 'task', 'require', 'memory', 'deepmind', 'lab', 'task', 'complex', 'partially', 'observable', 'task', 'diﬃcult', 'obtain', 'good', 'coverage', 'dataset', 'even', 'collect', 'billion', 'transition', 'speciﬁcally', 'conduct', 'observational', 'study', 'eﬀective', 'rank', 'deepmind', 'lab', 'dataset', 'preprint', 'datum', 'collect', 'welltrained', 'agent', 'seekavoid', 'level', 'dataset', 'collect', 'add', 'diﬀerent', 'level', 'action', 'exploration', 'noise', 'welltrained', 'agent', 'order', 'get', 'dataset', 'large', 'coverage', 'stateaction', 'space', 'figure', 'structural', 'causal', 'model', 'diﬀerent', 'factor', 'test', 'represent', 'model', 'selection', 'method', 'use', 'determine', 'h', 'denote', 'observe', 'confounder', 'choose', 'beginning', 'training', 'include', 'task', 'model', 'architecture', 'include', 'depth', 'number', 'unit', 'learn', 'rate', 'number', 'gradient', 'step', 'train', 'eﬀective', 'rank', 'penultimate', 'layer', 'agent', 'performance', 'measure', 'episodic', 'return', 'agent', 'attain', 'evaluate', 'environment', 'represent', 'unobserved', 'confounder', 'change', 'training', 'aﬀect', 'performance', 'number', 'dead', 'unit', 'parameter', 'norm', 'underlie', 'factor', 'inﬂuence', 'learn', 'dynamic', 'test', 'eﬀect', 'factor', 'intervention', 'illustrate', 'structural', 'causal', 'model', 'interaction', 'diﬀerent', 'factor', 'like', 'test', 'figure', 'explore', 'relationship', 'rank', 'performance', 'intervene', 'h', 'represent', 'potential', 'exogenous', 'source', 'implicit', 'regularization', 'architecture', 'dataset', 'size', 'loss', 'function', 'include', 'auxiliary', 'loss', 'intervention', 'h', 'result', 'randomized', 'control', 'trial', 'rct', 'represent', 'unobserved', 'factor', 'aﬀect', 'performance', 'denote', 'eﬀective', 'rank', 'β', 'activation', 'norm', 'number', 'dead', 'unit', 'easy', 'justify', 'relationship', 'argue', 'also', 'confound', 'show', 'confounding', 'eﬀect', 'β', 'intervention', 'beta', 'auxiliary', 'loss', 'architectural', 'change', 'increase', 'rank', 'aﬀect', 'performance', 'aim', 'understand', 'nature', 'relationship', 'term', 'ﬁgure', 'describe', 'notice', 'empirical', 'exploration', 'overload', 'term', 'performance', 'agent', 'refer', 'episodic', 'return', 'attain', 'agent', 'evaluate', 'online', 'environment', 'stochastic', 'environment', 'dataset', 'limited', 'coverage', 'oﬄine', 'evaluation', 'performance', 'generalization', 'ability', 'correlate', 'setting', 'oﬄine', 'agent', 'need', 'generalize', 'evaluate', 'environment', 'stochasticity', 'initial', 'condition', 'transition', 'environment', 'example', 'atari', 'case', 'stochasticity', 'arise', 'sticky', 'action', 'deepmind', 'lab', 'arise', 'randomization', 'initial', 'position', 'lemon', 'apple', 'limited', 'coverage', 'coverage', 'environment', 'dataset', 'often', 'limit', 'thus', 'agent', 'likely', 'encounter', 'state', 'action', 'never', 'see', 'training', 'eﬀective', 'rank', 'performance', 'base', 'result', 'tempt', 'extrapolate', 'positive', 'causal', 'link', 'eﬀective', 'rank', 'last', 'hidden', 'layer', 'agent', 'performance', 'measure', 'episodic', 'return', 'β', 'eﬀective', 'rank', 'h', 'observed', 'confounder', 'performance', 'hide', 'confounder', 'selection', 'method', 'β', 'preprint', 'attain', 'evaluate', 'environment', 'explore', 'potentially', 'interesting', 'relationship', 'large', 'scale', 'adopt', 'proof', 'contradiction', 'approach', 'evaluate', 'agent', 'hyperparameter', 'setup', 'deﬁne', 'atari', 'dataset', 'unplugged', 'gulcehre', 'hyperparameter', 'sweep', 'deﬁne', 'deepmind', 'lab', 'gulcehre', 'narrow', 'set', 'hyperparameter', 'correlation', 'exist', 'observe', 'figure', 'however', 'case', 'notice', 'broad', 'hyperparameter', 'sweep', 'make', 'correlation', 'performance', 'rank', 'disappear', 'see', 'figure', 'deepmind', 'lab', 'figure', 'a3', 'particular', 'ﬁnd', 'hyperparameter', 'setting', 'lead', 'low', 'collapse', 'rank', 'high', 'performance', 'par', 'good', 'performance', 'report', 'restricted', 'hyperparameter', 'range', 'setting', 'lead', 'high', 'rank', 'poor', 'performance', 'show', 'correlation', 'eﬀective', 'rank', 'performance', 'trust', 'oﬄine', 'policy', 'selection', 'follow', 'section', 'far', 'present', 'speciﬁc', 'ablation', 'help', 'understand', 'dependence', 'eﬀective', 'rank', 'performance', 'correlation', 'speciﬁc', 'hyperparameter', 'intervention', 'lifespan', 'learn', 'deep', 'qnetwork', 'empirically', 'find', 'eﬀective', 'rank', 'suﬃcient', 'identify', 'phase', 'train', 'oﬄine', 'dqn', 'agent', 'relu', 'activation', 'function', 'figure', 'eﬀective', 'rank', 'suﬃcient', 'identify', 'stage', 'still', 'imply', 'direct', 'causal', 'link', 'eﬀective', 'rank', 'performance', 'discuss', 'follow', 'section', 'several', 'factor', 'confound', 'eﬀective', 'rank', 'make', 'less', 'reliable', 'guide', 'metric', 'oﬄine', 'rl', 'confounder', 'carefully', 'control', 'phase', 'simple', 'behavior', 'eﬀective', 'rank', 'model', 'collapse', 'small', 'value', 'case', 'single', 'digit', 'value', 'gradually', 'start', 'increase', 'phase', 'model', 'learn', 'easy', 'learn', 'behavior', 'low', 'performance', 'evaluate', 'environment', 'hypothesize', 'implicit', 'bias', 'sgd', 'learn', 'function', 'increase', 'complexity', 'training', 'iteration', 'therefore', 'early', 'train', 'network', 'rely', 'simple', 'behaviour', 'myopic', 'variation', 'datum', 'hence', 'model', 'low', 'rank', 'rank', 'collapse', 'early', 'beginning', 'training', 'happen', 'abruptly', 'handful', 'gradient', 'update', 'rank', 'collapse', 'single', 'digit', 'number', 'however', 'early', 'rank', 'collapse', 'degrade', 'performance', 'agent', 'call', 'rank', 'collapse', 'early', 'training', 'selfpruning', 'eﬀect', 'phase', 'complex', 'behavior', 'phase', 'eﬀective', 'rank', 'model', 'increase', 'usually', 'ﬂatten', 'model', 'start', 'learn', 'complex', 'behavior', 'achieve', 'high', 'return', 'evaluate', 'environment', 'phase', 'underﬁttingunderparameterization', 'phase', 'eﬀective', 'rank', 'collapse', 'small', 'value', 'often', 'performance', 'agent', 'often', 'collapse', 'third', 'phase', 'call', 'underﬁtting', 'agent', 'performance', 'usually', 'drop', 'eﬀective', 'rank', 'also', 'collapse', 'cause', 'agent', 'lose', 'part', 'capacity', 'phase', 'always', 'observe', 'performance', 'collapse', 'eﬀective', 'run', 'end', 'training', 'setting', 'demonstrate', 'diﬀerent', 'ablation', 'typically', 'supervised', 'learning', 'phase', 'follow', 'overﬁtte', 'oﬄine', 'tdlearning', 'ﬁnd', 'evidence', 'overﬁtte', 'believe', 'phase', 'primarily', 'due', 'target', 'qnetwork', 'need', 'extrapolate', 'action', 'see', 'training', 'cause', 'extrapolation', 'error', 'describe', 'piece', 'evidence', 'support', 'hypothesis', 'present', 'figure', 'suggest', 'eﬀective', 'rank', 'value', 'error', 'agent', 'correlate', 'well', 'phase', 'low', 'eﬀective', 'rank', 'poor', 'performance', 'cause', 'large', 'number', 'dead', 'relu', 'unit', 'karniadaki', 'also', 'show', 'network', 'increase', 'number', 'dead', 'unit', 'become', 'underparameterized', 'negatively', 'inﬂuence', 'agent', 'performance', 'possible', 'identify', 'phase', 'many', 'learning', 'curve', 'provide', 'paper', 'ﬁrst', 'phase', 'agree', 'work', '’s', 'implicit', 'bias', 'learn', 'function', 'increase', 'complexity', 'give', 'ﬁxed', 'model', 'architecture', 'possible', 'observe', 'phase', 'training', 'fundamentally', 'depend', 'preprint', 'figure', 'phase', 'learn', 'icehockey', 'unplug', 'game', 'illustrate', 'diﬀerent', 'phase', 'learn', 'oﬄine', 'dqn', 'agent', 'use', 'learning', 'rate', 'blue', 'region', 'plot', 'identiﬁes', 'phase', 'green', 'region', 'phase', 'red', 'region', 'phase', 'learn', 'icehockey', 'game', 'expert', 'generate', 'dataset', 'perform', 'quite', 'poorly', 'thus', 'majority', 'datum', 'random', 'exploration', 'datum', 'easy', 'see', 'phase', 'oﬄine', 'dqn', 'perform', 'poorly', 'never', 'manage', 'get', 'phase', 'performance', 'agent', 'icehockey', 'poor', 'eﬀective', 'rank', 'network', 'low', 'training', 'mspacman', 'observe', 'phase', 'model', 'transition', 'phase', 'phase', 'quickly', 'follow', 'underﬁtting', 'regime', 'eﬀective', 'rank', 'collapse', 'agent', 'perform', 'poorly', 'hyperparameter', 'phase', 'oﬄine', 'go', 'training', 'depend', 'hyperparameter', 'learning', 'rate', 'early', 'stopping', 'training', 'budget', 'example', 'early', 'stop', 'model', 'stop', 'second', 'phase', 'learning', 'rate', 'small', 'parameter', 'move', 'much', 'slow', 'model', 'never', 'get', 'phase', 'model', 'large', 'enough', 'never', 'learn', 'transition', 'phase', 'phase', 'datum', 'distribution', 'data', 'distribution', 'big', 'inﬂuence', 'phase', 'agent', 'go', 'thorough', 'example', 'dataset', 'contain', 'random', 'exploratory', 'datum', 'orl', 'method', 'fail', 'learn', 'complex', 'behavior', 'datum', 'result', 'never', 'transition', 'phase', 'phase', 'learning', 'paradigm', 'learn', 'include', 'optimizer', 'loss', 'function', 'inﬂuence', 'phase', 'agent', 'go', 'training', 'example', 'observe', 'phase', 'happen', 'oﬄine', 'tdlearning', 'approach', 'possible', 'avoid', 'phase', 'underﬁtting', 'ﬁnde', 'correct', 'hyperparameter', 'believe', 'third', 'phase', 'observe', 'nonstationarity', 'loss', 'due', 'bootstrappe', 'error', 'propagate', 'bootstrappe', 'target', 'underﬁtting', 'regime', 'appear', 'network', 'train', 'long', 'enough', 'quality', 'complexity', 'datum', 'agent', 'learn', 'also', 'play', 'key', 'role', 'decide', 'learn', 'phase', 'observe', 'training', 'figure', 'demonstrate', 'diﬀerent', 'phase', 'learn', 'icehockey', 'mspacman', 'game', 'icehockey', 'expert', 'generate', 'dataset', 'poor', 'performance', 'game', 'oﬄine', 'dqn', 'stick', 'phase', 'manage', 'learn', 'complex', 'behavior', 'push', 'phase', 'mspacman', 'phase', 'present', 'provide', 'learn', 'curve', 'online', 'policy', 'selection', 'atari', 'game', 'twelve', 'learning', 'rate', 'r', 'e', 'e', '−25', '−5', '−125', 'r', 'u', 'e', 'r', 'p', 'e', 'icehockey', 'learner', 'step', 'mspacman', 'learner', 'step', 'x1000', 'phase', 'learn', 'phase', 'simple', 'behavior', 'phase', 'complex', 'behavior', 'phase', 'underparametrization', 'preprint', 'figure', 'show', 'relationship', 'eﬀective', 'rank', 'learn', 'rate', 'ﬁgure', 'eﬀect', 'learn', 'rate', 'diﬀerent', 'phase', 'learning', 'distinguishable', 'low', 'learning', 'rate', 'rank', 'low', 'agent', 'never', 'transition', 'phase', 'phase', 'large', 'learning', 'rate', 'eﬀective', 'rank', 'low', 'agent', 'phase', 'therefore', 'distribution', 'eﬀective', 'rank', 'learn', 'rate', 'gaussianlike', 'shape', 'depict', 'ﬁgure', 'distribution', 'rank', 'shift', 'low', 'learning', 'rate', 'train', 'agent', 'long', 'slow', 'model', 'low', 'learning', 'rate', 'start', 'enter', 'phase', 'model', 'train', 'large', 'learning', 'rate', 'enter', 'phase', 'figure', 'atari', 'bar', 'chart', 'eﬀective', 'rank', 'respect', 'learning', 'rate', 'learn', 'step', 'gradient', 'step', 'rank', 'distribute', 'almost', 'gaussian', 'learn', 'step', 'mode', 'distribution', 'rank', 'shift', 'left', 'mode', 'go', 'game', 'well', 'namely', 'train', 'network', 'long', 'rank', 'go', 'particular', 'large', 'learning', 'rate', 'low', 'learning', 'rate', 'rank', 'low', 'model', 'stick', 'phase', 'large', 'learning', 'rate', 'get', 'phase', 'quickly', 'thus', 'low', 'rank', 'eﬀect', 'dataset', 'size', 'use', 'size', 'dataset', 'possible', 'proxy', 'metric', 'coverage', 'agent', 'observe', 'oﬄine', 'datum', 'uniformly', 'sample', 'diﬀerent', 'proportion', 'transition', 'entire', 'dataset', 'transition', 'rlunplugge', 'atari', 'benchmark', 'dataset', 'understand', 'agent', 'behave', 'diﬀerent', 'amount', 'training', 'datum', 'factor', 'aﬀecte', 'rank', 'network', 'figure', 'show', 'evolution', 'rank', 'return', 'course', 'train', 'eﬀective', 'rank', 'performance', 'collapse', 'severely', 'low', 'data', 'proportion', 'learn', 'entire', 'preprint', 'dataset', 'subsample', 'network', 'never', 'transition', 'phase', 'phase', 'however', 'proportion', 'dataset', 'subsample', 'increase', 'agent', 'learn', 'complex', 'behavior', 'get', 'phase', 'eﬀective', 'rank', 'collapse', 'less', 'severely', 'large', 'proportion', 'dataset', 'agent', 'tend', 'perform', 'considerably', 'well', 'particular', 'see', 'phase', 'initial', 'decrease', 'rank', 'correlate', 'increase', 'performance', 'speculate', 'due', 'network', 'reduce', 'reliance', 'spurious', 'part', 'observation', 'lead', 'representation', 'generalize', 'well', 'state', 'worth', 'note', 'ordering', 'policy', 'obtain', 'use', 'agent', 'performance', 'correspond', 'ordering', 'policy', 'respect', 'eﬀective', 'rank', 'training', 'example', 'oﬄine', 'dqn', 'train', 'full', 'dataset', 'perform', 'well', 'dataset', 'agent', 'train', 'use', 'datum', 'sometimes', 'high', 'rank', 'similar', 'observation', 'make', 'end', 'training', 'network', 'train', 'full', 'dataset', 'underperform', 'compare', 'one', 'train', 'datum', 'even', 'rank', 'high', 'figure', 'atari', 'size', 'evolution', 'rank', 'return', 'vary', 'fraction', 'datum', 'available', 'agent', 'train', 'see', 'agent', 'see', 'little', 'datum', 'collapse', 'term', 'rank', 'performance', 'agent', 'see', 'datum', 'good', 'performance', 'even', 'allow', 'shrinkage', 'rank', 'training', 'interaction', 'rank', 'performance', 'understand', 'interaction', 'eﬀect', 'diﬀerent', 'factor', 'rank', 'performance', 'oﬄine', 'dqn', 'several', 'experiment', 'test', 'eﬀect', 'diﬀerent', 'hyperparameter', 'eﬀective', 'rank', 'agent', 'performance', 'figure', 'show', 'causal', 'graph', 'eﬀective', 'rank', 'confounder', 'agent', 'performance', 'ideally', 'like', 'intervene', 'node', 'graph', 'measure', 'eﬀect', 'rank', 'continuous', 'random', 'variable', 'possible', 'directly', 'intervene', 'eﬀective', 'rank', 'instead', 'emulate', 'intervention', 'eﬀective', 'rank', 'conditioning', 'threshold', 'β', 'control', 'case', 'assume', 'τ', 'λ0', 'write', 'average', 'treatment', 'eﬀect', 'eat', 'eﬀect', 'set', 'eﬀective', 'rank', 'large', 'value', 'performance', 'ateλ', 'eλλ0', 'let', 'note', 'ateλ', 'quantity', 'necessarily', 'measure', 'causal', 'eﬀect', 'know', 'confound', 'study', 'impact', 'factor', 'activation', 'function', 'learning', 'rate', 'datum', 'split', 'target', 'update', 'step', 'curl', 'loss', 'gravitar', 'level', 'choose', 'game', 'preprint', 'relu', 'figure', 'atari', 'correlation', 'plot', 'eﬀective', 'rank', 'performance', 'measure', 'term', 'episode', 'return', 'evaluate', 'agent', 'environment', 'oﬄine', 'dqn', 'game', 'diﬀerent', 'hyperparameter', 'conﬁguration', 'train', 'learn', 'step', 'strong', 'correlation', 'relu', 'function', 'correlation', 'disappear', 'network', 'activation', 'function', 'signiﬁcant', 'correlation', 'eﬀective', 'rank', 'performance', 'game', 'complete', 'datum', 'still', 'positive', 'correlation', 'exist', 'gravitar', 'game', 'result', 'aﬀecte', 'paradox', 'subgroup', 'datum', 'split', 'group', 'concern', 'activation', 'function', 'show', 'consistent', 'correlation', 'easytoexplore', 'atari', 'game', 'relatively', 'dense', 'reward', 'gravitar', 'hardtoexplore', 'sparse', 'reward', 'set', 'bellemare', 'table', 'present', 'result', 'intervene', 'thresholde', 'diﬀerent', 'quantile', 'eﬀective', 'rank', 'choose', 'network', 'high', 'eﬀective', 'rank', 'relu', 'network', 'statistically', 'signiﬁcant', 'positive', 'eﬀect', 'agent', 'performance', 'concern', 'diﬀerent', 'quantile', 'gravitar', 'agent', 'performance', 'measure', 'term', 'normalize', 'score', 'describe', 'however', 'result', 'activation', 'function', 'mixed', 'eﬀect', 'change', 'rank', 'statistically', 'signiﬁcant', 'impact', 'performance', 'case', 'figure', 'show', 'correlation', 'rank', 'performance', 'agent', 'network', 'relu', 'activation', 'strongly', 'correlate', 'rank', 'performance', 'experimental', 'datum', 'prone', 'paradox', 'imply', 'trend', 'correlation', 'case', 'appear', 'several', 'subgroup', 'datum', 'disappear', 'reverse', 'group', 'aggregate', 'lead', 'mislead', 'conclusion', 'divide', 'datum', 'equalsize', 'subgroup', 'hyperparameter', 'model', 'variant', 'ﬁnd', 'consistent', 'trend', 'disappear', 'combine', 'figure', 'thus', 'experiment', 'appear', 'aﬀecte', 'preprint', 'table', 'atari', 'average', 'treatment', 'eﬀect', 'network', 'high', 'rank', 'concern', 'diﬀerent', 'quantile', 'report', 'average', 'treatment', 'eﬀect', 'eat', 'uncertainty', 'use', 'conﬁdence', 'interval', 'use', 'standard', 'error', 'pvalue', 'game', 'high', 'eﬀective', 'rank', 'seem', 'small', 'eﬀect', 'gravitar', 'let', 'note', 'gravitar', 'sparse', 'reward', 'problem', 'densereward', 'overall', 'eﬀect', 'prominent', 'relu', 'tanh', 'network', 'statistically', 'signiﬁcant', 'boldface', 'ate', 'eﬀect', 'statistically', 'signiﬁcant', 'p', 'type', 'column', 'table', 'indicate', 'activation', 'function', 'use', 'experiment', 'intervention', 'combined', 'type', 'correspond', 'combination', 'tanh', 'relu', 'experiment', 'level', 'combine', 'relu', 'tanh', 'combine', 'relu', 'tanh', 'eat', '±', 'quantile095', 'pvalue', 'eat', '±', '±', '±', '±', '±', 'pvalue', 'eat', '±', '±', '±', 'pvalue', 'eat', '±', '±', 'pvalue', 'table', 'atari', 'average', 'treatment', 'eﬀect', 'diﬀerent', 'intervention', 'gravitar', 'quantity', 'interest', 'terminal', 'rank', 'feature', 'activation', 'function', 'learning', 'rate', 'considerable', 'eﬀect', 'terminal', 'feature', 'rank', 'setup', 'indicate', 'boldface', 'change', 'activation', 'function', 'relu', 'tanh', 'improve', 'eﬀective', 'rank', 'change', 'learning', 'rate', '×', '×', 'reduce', 'rank', 'u', 'activation', 'function', 'learn', 'rate', 'target', 'update', 'step', 'loss', 'weight', 'level', 'curl', 'loss', 'weight', 'datum', 'split', 'control', 'u', 'relu', '×', 'treatment', '×', 'gravitar', 'ytu', '−', '±', '±', '±', '±', '±', 'analysis', 'set', 'control', 'set', 'train', 'relu', 'activation', 'learning', 'rate', '−', 'auxiliary', 'loss', 'training', 'full', 'dataset', 'level', 'present', 'average', 'treatment', 'eﬀect', 'eat', 'diﬀerence', 'rank', 'intervention', 'present', 'absent', 'experiment', 'change', 'variable', 'table', 'change', 'activation', 'function', 'relu', 'tanh', 'drastically', 'aﬀect', 'eﬀective', 'rank', 'feature', 'sync', 'early', 'observation', 'level', 'figure', 'present', 'heatmap', 'ate', 'demonstrate', 'eﬀective', 'rank', 'change', 'variable', 'change', 'original', 'control', 'simultaneously', 'activation', 'function', 'learn', 'rate', 'signiﬁcantly', 'aﬀect', 'terminal', 'rank', 'also', 'observe', 'interesting', 'combination', 'lead', 'model', 'converge', 'low', 'rank', 'example', 'use', 'loss', 'dropout', 'observation', 'far', 'reinforce', 'belief', 'diﬀerent', 'factor', 'aﬀect', 'phenomenon', 'figure', 'atari', 'ablation', 'eﬀect', 'diﬀerent', 'pair', 'intervention', 'control', 'set', 'control', 'dropout', 'curl', 'loss', 'smoothing', 'full', 'dataset', 'target', 'update', 'period', 'step', 'overall', 'change', 'activation', 'function', 'learning', 'rate', 'large', 'eﬀect', 'rank', 'learn', 'rate3e5', 'curl', 'dropout', 'rate05', 'target', 'update', 'steps200', 'level', 'namegravitar', 'h', 'n', 'v', 'e', 'r', 'g', 'r', 'e', 'l', 'h', 'g', 'h', 'g', 'r', 'r', 'u', 'p', 'r', 'z', 'r', 'v', 'r', 'n', 'l', 'e', 'p', 'e', 'e', 'p', 'u', 'e', 'g', 'r', 'preprint', 'extent', 'rank', 'collapse', 'mitigate', 'rank', 'collapse', 'alone', 'never', 'fully', 'ﬁx', 'agent', 'learn', 'ability', 'diﬀerent', 'environment', 'eﬀect', 'activation', 'function', 'severe', 'rank', 'collapse', 'phase', 'apparent', 'atari', 'model', 'simple', 'convolutional', 'neural', 'network', 'relu', 'activation', 'function', 'study', 'phenomenon', 'dataset', 'rank', 'collapse', 'seem', 'happen', 'similarly', 'important', 'note', 'solve', 'task', 'eﬀectively', 'agent', 'need', 'memory', 'hence', 'network', 'recurrent', 'core', 'lstm', 'standard', 'lstms', 'use', 'tanh', 'activation', 'investigate', 'setting', 'help', 'understand', 'role', 'choice', 'architecture', 'behavior', 'model', 'rank', 'figure', 'show', 'output', 'feature', 'lstm', 'network', 'deepmind', 'lab', 'dataset', 'experience', 'detrimental', 'eﬀective', 'rank', 'collapse', 'diﬀerent', 'exploration', 'noise', 'use', 'tanh', 'activation', 'function', 'cell', 'however', 'replace', 'tanh', 'activation', 'function', 'lstm', 'cell', 'relu', 'replace', 'lstm', 'feedforward', 'use', 'relu', 'activation', 'see', 'figure', 'eﬀective', 'rank', 'case', 'collapse', 'small', 'value', 'end', 'training', 'behavior', 'show', 'choice', 'activation', 'function', 'considerable', 'eﬀect', 'model', 'rank', 'collapse', 'training', 'subsequently', 'ability', 'learn', 'expressive', 'value', 'function', 'policy', 'environment', 'susceptible', 'enter', 'phase', 'training', 'observation', 'agent', 'network', 'relu', 'unit', 'tend', 'dead', 'unit', 'cause', 'eﬀective', 'rank', 'collapse', 'phase', 'learn', 'activation', 'tanh', 'suﬀer', 'similar', 'collapse', 'activation', 'function', 'inﬂuence', 'network', 'learn', 'dynamic', 'performance', 'note', 'activation', 'function', 'inﬂuence', 'rank', 'layer', 'initialization', 'figure', 'present', 'ﬁnding', 'bsuite', 'level', 'general', 'eﬀective', 'rank', 'penultimate', 'layer', 'relu', 'activation', 'collapse', 'fast', 'elu', 'tanh', 'tend', 'maintain', 'relatively', 'high', 'rank', 'relu', 'eﬀective', 'rank', 'go', 'catch', 'environment', 'activation', 'become', 'sparser', 'unit', 'die', 'illustrate', 'sparsity', 'activation', 'gram', 'matrix', 'feature', 'last', 'hidden', 'layer', 'feedforward', 'network', 'train', 'bsuite', 'figure', 'a14', 'figure', 'bsuite', 'eﬀective', 'rank', 'diﬀerent', 'activation', 'function', 'magnitude', 'drop', 'eﬀective', 'rank', 'severe', 'relu', 'sigmoid', 'activation', 'function', 'optimization', 'inﬂuence', 'minibatch', 'size', 'learning', 'rate', 'learning', 'dynamic', 'wellstudied', 'phenomenon', 'argue', 'ratio', 'size', 'learning', 'rate', 'relate', 'implicit', 'regularization', 'also', 'aﬀect', 'learn', 'dynamic', 'thus', 'evident', 'factor', 'impact', 'eﬀective', 'rank', 'performance', 'focus', 'setup', 'see', 'correlation', 'eﬀective', 'rank', 'performance', 'investigate', 'emerge', 'preprint', 'figure', 'deepmind', 'lab', 'seekavoid', 'activation', 'function', 'evolution', 'rank', 'typical', 'lstm', 'network', 'activation', 'cell', 'lstm', 'deepmind', 'lab', 'see', 'typical', 'lstm', 'network', 'get', 'phase', 'however', 'change', 'activation', 'function', 'cell', 'gate', 'tanh', 'relu', 'eﬀective', 'rank', 'collapse', 'small', 'value', 'eﬀective', 'rank', 'collapse', 'lstm', 'replace', 'feedforward', 'network', 'case', 'relu', 'activation', 'analysis', 'ablation', 'table', 'figure', 'illustrate', 'learning', 'rate', 'prominent', 'factor', 'performance', 'agent', 'general', 'strong', 'consistent', 'correlation', 'eﬀective', 'rank', 'performance', 'diﬀerent', 'model', 'hyperparameter', 'setting', 'however', 'figure', 'show', 'correlation', 'exist', 'speciﬁc', 'set', 'particular', 'minibatch', 'size', 'learn', 'rate', 'far', 'figure', 'narrow', 'correlation', 'eﬀective', 'rank', 'performance', 'exist', 'oﬄine', 'dqn', 'relu', 'activation', 'function', 'thus', 'section', 'focus', 'regime', 'correlation', 'exist', 'oﬄine', 'dqn', 'relu', 'investigate', 'learning', 'rate', 'minibatche', 'aﬀect', 'rank', 'run', 'several', 'experiment', 'explore', 'relationship', 'minibatch', 'size', 'learning', 'rate', 'rank', 'section', 'report', 'result', 'atari', 'diﬀerent', 'setting', 'die', 'relu', 'problem', 'wellstudied', 'issue', 'supervised', 'learning', 'glorot', 'high', 'learning', 'rate', 'unstable', 'learning', 'dynamic', 'relu', 'unit', 'stick', 'regime', 'relu', 'activation', 'function', 'compute', 'number', 'dead', 'relu', 'unit', 'penultimate', 'layer', 'network', 'number', 'unit', 'activation', 'value', 'input', 'increase', 'learning', 'rate', 'increase', 'number', 'dead', 'unit', 'see', 'figure', 'even', 'observe', 'use', 'large', 'learning', 'rate', 'cause', 'dead', 'relu', 'unit', 'rank', 'collapse', 'poor', 'performance', 'see', 'figure', 'hence', 'behavior', 'unique', 'oﬄine', 'loss', 'nevertheless', 'model', 'tdlearning', 'loss', 'catastrophic', 'rank', 'collapse', 'many', 'dead', 'unit', 'low', 'learning', 'rate', 'let', 'note', 'eﬀective', 'rank', 'depend', 'number', 'unit', 'number', 'dead', 'unit', 'η', 'layer', 'easy', 'see', 'eﬀective', 'rank', 'upperbounde', '−', 'η', 'figure', 'observe', 'strong', 'correlation', 'number', 'dead', 'relu', 'unit', 'penultimate', 'layer', 'relu', 'network', 'eﬀective', 'rank', 'observation', 'pace', 'learn', 'inﬂuence', 'number', 'dead', 'unit', 'eﬀective', 'rank', 'large', 'learn', 'rate', 'small', 'minibatche', 'number', 'dead', 'unit', 'increase', 'eﬀective', 'rank', 'decrease', 'however', 'performance', 'poor', 'eﬀective', 'rank', 'severely', 'low', 'finally', 'look', 'eﬀective', 'rank', 'shape', 'end', 'training', 'test', 'diﬀerent', 'learning', 'rate', 'try', 'understand', 'interaction', 'learning', 'rate', 'eﬀective', 'rank', 'representation', 'summarize', 'main', 'result', 'figure', 'training', 'oﬄine', 'dqn', 'decrease', 'preprint', 'figure', 'atari', 'learn', 'curve', 'oﬄine', 'dqn', 'gradient', 'step', 'training', 'minibatch', 'size', 'increase', 'learning', 'rate', 'increase', 'number', 'dead', 'unit', 'relu', 'network', 'result', 'increase', 'learning', 'rate', 'also', 'cause', 'severe', 'collapse', 'well', 'align', 'well', 'number', 'dead', 'unit', 'observe', 'behavior', 'also', 'happen', 'network', 'use', 'saturate', 'activation', 'function', 'sigmoid', 'eﬀective', 'rank', 'eﬀective', 'rank', 'low', 'small', 'large', 'learning', 'rate', 'high', 'learning', 'rate', 'see', 'early', 'training', 'long', 'lead', 'many', 'dead', 'relu', 'unit', 'turn', 'cause', 'eﬀective', 'rank', 'diminish', 'see', 'figure', 'moreover', 'see', 'ﬁgure', 'phase', 'eﬀective', 'rank', 'number', 'dead', 'unit', 'low', 'thus', 'rank', 'collapse', 'phase', 'cause', 'number', 'dead', 'unit', 'phase', 'number', 'dead', 'unit', 'high', 'eﬀective', 'rank', 'drastically', 'low', 'drastically', 'low', 'eﬀective', 'rank', 'cause', 'network', 'large', 'number', 'dead', 'unit', 'phase', 'phase', 'believe', 'underparameterization', 'poor', 'performance', 'cause', 'number', 'dead', 'unit', 'relu', 'dqn', 'show', 'case', 'karniadaki', 'supervised', 'learning', 'overall', 'observe', 'high', 'correlation', 'eﬀective', 'rank', 'agent', 'performance', 'use', 'relu', 'activation', 'network', 'long', 'training', 'also', 'present', 'analysis', 'control', 'loss', 'landscape', 'aﬀect', 'rank', 'performance', 'eﬀect', 'loss', 'function', 'qlearning', 'behavior', 'clone', 'behavior', 'clone', 'method', 'learn', 'behavior', 'policy', 'oﬄine', 'dataset', 'use', 'supervised', 'learning', 'approach', 'pomerleau', 'policy', 'learn', 'learn', 'mimic', 'behavior', 'policy', 'thus', 'performance', 'learn', 'agent', 'highly', 'limit', 'quality', 'datum', 'agent', 'train', 'compare', 'qlearne', 'figure', 'conﬁrm', 'default', 'hyperparameter', 'eﬀective', 'rank', 'collapse', 'end', 'training', 'contrast', 'show', 'preprint', 'figure', 'atari', 'learn', 'curve', 'oﬄine', 'dqn', 'gradient', 'step', 'training', 'minibatch', 'size', 'increase', 'minibatch', 'size', 'improve', 'performance', 'network', 'large', 'learning', 'rate', 'eﬀective', 'rank', 'collapse', 'outperform', 'bc', 'even', 'rank', 'considerably', 'low', 'learn', 'rank', 'predictive', 'performance', 'figure', 'behavior', 'indicate', 'unreliability', 'eﬀective', 'rank', 'oﬄine', 'model', 'selection', 'curl', 'eﬀect', 'selfsupervision', 'study', 'add', 'auxiliary', 'loss', 'term', 'propose', 'equation', 'training', 'help', 'model', 'mitigate', 'rank', 'collapse', 'atari', 'experiment', 'use', 'curl', 'loss', 'describe', 'laskin', 'modiﬁcation', 'deepmind', 'lab', 'task', 'require', 'memory', 'apply', 'similar', 'curl', 'loss', 'feature', 'aggregate', 'mean', 'state', 'timestep', 'experiment', 'also', 'sweep', 'weight', 'curl', 'loss', 'figure', 'show', 'rank', 'return', 'atari', 'deepmind', 'lab', 'result', 'see', 'a6', 'atari', 'game', 'use', 'large', 'weight', 'auxiliary', 'loss', 'prevent', 'rank', 'collapse', 'simultaneously', 'deteriorate', 'agent', 'performance', 'speculate', 'borrow', 'intuition', 'supervised', 'learning', 'literature', 'role', 'rank', 'implicit', 'regularizer', 'scenario', 'large', 'rank', 'prevent', 'network', 'ignore', 'spurious', 'part', 'observation', 'aﬀect', 'ability', 'generalize', 'hand', 'moderate', 'weight', 'curl', 'auxiliary', 'loss', 'signiﬁcantly', 'change', 'rank', 'performance', 'agent', 'previously', 'agarwal', 'show', 'curl', 'loss', 'improve', 'performance', 'rl', 'agent', 'atari', 'statistically', 'meaningful', 'way', 'deepmind', 'lab', 'game', 'observe', 'rank', 'collapse', 'none', 'deepmind', 'lab', 'experiment', 'agent', 'enter', 'phase', 'phase', 'due', 'use', 'activation', 'function', 'lstm', 'base', 'investigation', 'role', 'activation', 'function', 'section', 'preprint', 'figure', 'atari', 'learn', 'curve', 'gradient', 'step', 'minibatch', 'size', 'large', 'enough', 'learning', 'rate', 'rank', 'also', 'collapse', 'hypothesize', 'rank', 'collapse', 'side', 'eﬀect', 'learn', 'general', 'due', 'tdlearne', 'base', 'loss', 'figure', 'atari', 'scatter', 'plot', 'correlation', 'number', 'dead', 'unit', 'eﬀective', 'rank', 'end', 'training', 'observe', 'eﬀective', 'rank', 'strongly', 'correlate', 'number', 'dead', 'unit', 'also', 'use', 'large', 'learning', 'rate', 'increase', 'number', 'dead', 'unit', 'network', 'tandem', 'propose', 'possible', 'hypothesis', 'observed', 'rank', 'collapse', 'reuse', 'transition', 'sample', 'multiple', 'time', 'particularly', 'prevalent', 'oﬄine', 'set', 'setting', 'hypothesis', 'test', 'directly', 'propose', 'ostrovski', 'secondary', 'passive', 'agent', 'train', 'data', 'stream', 'generate', 'architecturally', 'equivalent', 'independently', 'initialize', 'baseline', 'agent', 'train', 'regular', 'online', 'rl', 'fashion', 'passive', 'agent', 'tend', 'underperform', 'active', 'agent', 'identical', 'architecture', 'learn', 'datum', 'stream', 'preprint', 'figure', 'atari', 'oﬄine', 'dqn', 'behavior', 'clone', 'compare', 'dqn', 'agent', 'atari', 'dataset', 'use', 'architecture', 'dataset', 'training', 'protocol', 'baseline', 'use', 'hyperparameter', 'deﬁne', 'comparison', 'rank', 'dqn', 'agent', 'signiﬁcantly', 'low', 'achieve', 'high', 'return', 'figure', 'atari', 'auxiliary', 'loss', 'evolution', 'rank', 'increase', 'weight', 'auxiliary', 'loss', 'see', 'strong', 'weight', 'auxiliary', 'loss', 'help', 'mitigate', 'rank', 'collapse', 'prevent', 'model', 'learn', 'useful', 'representation', 'setup', 'present', 'clean', 'ablation', 'set', 'agent', 'use', 'datum', 'way', 'particular', 'diﬀere', 'reuse', 'data', 'sample', 'diﬀerence', 'performance', 'rank', 'representation', 'directly', 'attributable', 'reuse', 'datum', 'figure', 'summarize', 'result', 'tandemdqn', 'use', 'tieleman', 'optimizer', 'passive', 'agent', 'reuse', 'datum', 'similar', 'fashion', 'online', 'agent', 'observe', 'collapse', 'low', 'rank', 'passive', 'agent', 'performance', 'tend', 'signiﬁcantly', 'case', 'catastrophically', 'bad', 'online', 'agent', 'satisfactorily', 'explain', 'extent', 'diﬀerence', 'eﬀective', 'rank', 'alone', 'think', 'qlearning', 'preprint', 'figure', 'atari', 'tandem', 'investigate', 'eﬀect', 'choice', 'optimizer', 'rmsprop', 'rank', 'performance', 'model', 'active', 'solid', 'line', 'passive', 'agent', 'dash', 'line', 'observe', 'rank', 'passive', 'agent', 'low', 'active', 'agent', 'figure', 'atari', 'fork', 'tandem', 'evaluate', 'diﬀerent', 'loss', 'function', 'fork', 'tandem', 'setting', 'passive', 'agent', 'fork', 'online', 'agent', 'online', 'agent', 'parameter', 'freeze', 'still', 'parameter', 'online', 'agent', 'update', 'fork', 'passive', 'agent', 'see', 'frame', 'training', 'denote', 'dash', 'line', 'ﬁgure', 'observe', 'use', 'bve', 'return', 'loss', 'improve', 'agent', 'rank', 'agent', 'performance', 'still', 'poor', 'seem', 'eﬃcient', 'enough', 'exploit', 'datum', 'generate', 'active', 'agent', 'learn', 'complex', 'behavior', 'put', 'passive', 'agent', 'phase', 'also', 'notice', 'achieve', 'well', 'performance', 'rmsprop', 'rank', 'model', 'train', 'tend', 'low', 'model', 'train', 'rmsprop', 'figure', 'investigate', 'eﬀect', 'diﬀerent', 'learning', 'algorithm', 'rank', 'performance', 'agent', 'fork', 'tandem', 'setting', 'fork', 'tandem', 'set', 'agent', 'ﬁrstly', 'train', 'fraction', 'total', 'training', 'time', 'agent', 'fork', 'active', 'passive', 'agent', 'start', 'network', 'weight', 'active', 'agent', 'frozen', 'train', 'continue', 'generate', 'datum', 'policy', 'train', 'passive', 'agent', 'generate', 'datum', 'remaining', 'training', 'time', 'see', 'rank', 'ﬂatline', 'fork', 'qlearning', 'agent', 'performance', 'collapse', 'dramatically', 'contrast', 'rank', 'bve', 'agent', 'train', 'onpolicy', 'monte', 'carlo', 'return', 'go', 'breakout', 'pong', 'e', 'p', 'r', 'l', 'e', 'r', 'e', 'e', 'r', 'e', 'seaquest', 'spaceinvader', 'qlearne', 'frame', 'breakout', 'pong', 'seaqu', 'spaceinvader', 'e', 'p', 'n', 'r', 'e', 'r', 'p', 'e', 'frame', 'qlearne', 'preprint', 'performance', 'still', 'drop', 'nevertheless', 'decline', 'performance', 'bve', 'agent', 'train', 'carlo', 'return', 'bad', 'qlearning', 'agent', 'game', 'oﬄine', 'policy', 'selection', 'section', 'find', 'large', 'learning', 'rate', 'sweep', 'set', 'rank', 'number', 'dead', 'unit', 'high', 'correlation', 'performance', 'oﬄine', 'policy', 'selection', 'paine', 'et', 'aim', 'choose', 'good', 'policy', 'online', 'interaction', 'purely', 'oﬄine', 'datum', 'apparent', 'correlation', 'rank', 'performance', 'raise', 'natural', 'question', 'well', 'rank', 'perform', 'oﬄine', 'policy', 'selection', 'approach', 'analysis', 'dqn25620', 'experiment', 'previously', 'observe', 'strong', 'correlation', 'rank', 'performance', 'run', 'dqn', 'network', 'describe', 'dqn25620', 'experimental', 'setting', 'end', 'training', 'perform', 'oﬄine', 'policy', 'selection', 'use', 'eﬀective', 'rank', 'select', 'good', 'learning', 'rate', 'base', 'learning', 'rate', 'yield', 'maximum', 'eﬀective', 'rank', 'minimum', 'number', 'dead', 'unit', 'figure', 'atari', 'dqn25620', 'plot', 'depict', 'simple', 'regret', 'dqn', 'agent', 'relu', 'activation', 'use', 'eﬀective', 'rank', 'number', 'dead', 'unit', 'left', 'show', 'simple', 'regret', 'select', 'good', 'learning', 'rate', 'use', 'eﬀective', 'rank', 'right', 'hand', 'side', 'show', 'simple', 'regret', 'achieve', 'base', 'number', 'dead', 'unit', 'figure', 'illustrate', 'simple', 'regret', 'atari', 'oﬄine', 'policy', 'selection', 'game', 'simple', 'regret', 'recommend', 'policy', 'measure', 'close', 'agent', 'good', 'policy', 'compute', 'describe', 'simple', 'regret', 'mean', 'oﬄine', 'policy', 'selection', 'mechanism', 'successfully', 'choose', 'good', 'policy', 'mean', 'choose', 'bad', 'policy', 'train', 'step', 'simple', 'regret', 'number', 'dead', 'unit', 'policy', 'selection', 'method', 'poor', 'contrast', 'simple', 'regret', 'achieve', 'select', 'agent', 'high', 'eﬀective', 'rank', 'good', 'mean', 'simple', 'regret', 'achieve', 'use', 'number', 'dead', 'unit', 'oﬄine', 'policy', 'selection', 'op', 'method', 'uncertainty', '±011', 'compute', 'standard', 'error', 'ﬁve', 'seed', 'contrast', 'simple', 'regret', 'achieve', 'use', 'eﬀective', 'rank', 'op', 'method', '±', 'eﬀective', 'rank', 'learning', 'rate', 'collapse', 'train', 'long', 'model', 'enter', 'phase', 'learn', 'number', 'dead', 'unit', 'increase', 'learn', 'step', 'mean', 'simple', 'regret', 'compute', 'use', 'eﬀective', 'rank', 'op', 'method', 'become', '±', 'mean', 'simple', 'regret', '±', 'number', 'dead', 'unit', 'op', 'let', 'note', 'learn', 'step', 'typical', 'training', 'agent', 'rl', 'unplugged', 'atari', 'dataset', 'number', 'dead', 'unit', 'become', 'good', 'metric', 'op', 'network', 'train', 'long', 'step', 'experiment', 'rank', 'become', 'drastically', 'small', 'learning', 'rate', 'conﬁguration', 'however', 'eﬀective', 'rank', 'seem', 'good', 'metric', 'op', 'early', 'training', 'nevertheless', 'prior', 'information', 'task', 'agent', 'challenge', 'conclude', 'number', 'dead', 'unit', 'eﬀective', 'rank', 'appropriate', 'op', 'metric', 'factor', 'number', 'training', 'step', 'activation', 'function', 'hyperparameter', 'confound', 'eﬀective', 'rank', 'number', 'dead', 'unit', 'thus', 'believe', 'metric', 'test', 'unreliable', 'general', 'op', 'setting', 'control', 'extraneous', 'factor', 'e', 'r', 'g', 'e', 'r', 'e', 'p', 'l', 'effective', 'rank', 'number', 'dead', 'unit', 'step', 'step', 'k', 'n', 'e', 'h', 'e', 'c', 'r', 'e', 'u', 'r', 'r', 'n', 'z', 'r', 'r', 'e', 'b', 'c', 'p', 'p', 'k', 'b', 'r', 'e', 'b', 'u', 'n', 'e', 'h', 'e', 'c', 'r', 'e', 'u', 'r', 'r', 'r', 'r', 'e', 'b', 'c', 'p', 'n', 'p', 'k', 'b', 'r', 'e', 'b', 'u', 'l', 'preprint', 'figure', 'compare', 'eﬀective', 'rank', 'op', 'method', 'respect', 'percentage', 'improvement', 'online', 'policy', 'selection', 'maximize', 'dian', 'reward', 'achieve', 'agent', 'atari', 'game', 'eﬀective', 'rank', 'mspacman', 'pooyan', 'robotank', 'doubledunk', 'perform', 'competitively', 'online', 'policy', 'selection', 'eﬀective', 'rank', 'complementary', 'tool', 'net', 'work', 'relu', 'activation', 'function', 'believe', 'useful', 'metric', 'monitor', 'train', 'ing', 'addition', 'number', 'dead', 'unit', 'well', 'picture', 'performance', 'agent', 'robustness', 'input', 'perturbation', 'figure', 'atari', 'dqn25620', 'picte', 'percentage', 'improvement', 'oﬄine', 'policy', 'selection', 'use', 'rank', 'individually', 'online', 'policy', 'selection', 'use', 'reward', 'atari', 'game', 'select', 'good', 'learning', 'rate', 'use', 'eﬀective', 'rank', 'oﬄine', 'policy', 'selection', 'method', 'perform', 'relatively', 'well', 'compare', 'online', 'policy', 'selection', 'base', 'median', 'normalize', 'score', 'atari', 'game', 'several', 'work', 'suggest', 'lowrank', 'model', 'robust', 'input', 'perturbation', 'speciﬁcally', 'adversarial', 'one', 'diﬃcult', 'measure', 'eﬀect', 'lowrank', 'resentation', 'agent', 'performance', 'rank', 'robust', 'measure', 'depend', 'diﬀer', 'ent', 'factor', 'activation', 'function', 'learn', 'turn', 'eﬀect', 'generalization', 'independently', 'rank', 'however', 'easy', 'validate', 'antithesis', 'namely', 'robust', 'agent', 'need', 'high', 'eﬀective', 'rank', 'less', 'robust', 'agent', 'easily', 'test', 'hypothesis', 'compare', 'dqn', 'agent', 'robustness', 'metric', 'deﬁne', 'robustness', 'metric', 'ρp', 'base', 'variable', 'noise', 'level', 'noise', 'distribution', 'dp', 'score', 'obtain', 'agent', 'evaluate', 'environment', 'noise', 'level', 'p', 'scoredp', 'scored0', 'represent', 'average', 'score', 'achieve', 'agent', 'perturbation', 'apply', 'deﬁne', 'ρp', 'follow', '−', 'scored0', 'scored0', 'scored0', 'experiment', 'compare', 'dqn', 'agent', 'already', 'know', 'much', 'large', 'rank', 'atari', 'game', 'dqn', 'train', 'agent', 'dataset', 'data', 'augmentation', 'data', 'augmentation', 'apply', 'input', 'agent', 'evaluate', 'environment', 'evaluate', 'agent', 'roadrunner', 'game', 'unplug', 'online', 'policy', 'selection', 'game', 'exclude', 'doubledunk', 'zaxxon', 'pooyan', 'game', 'performance', 'level', 'poor', 'close', 'performance', 'random', 'agent', 'robustness', 'metric', 'game', 'meaningful', 'icehockey', 'result', 'negative', 'thus', 'shift', 'score', 'make', 'sure', 'nonnegative', 'robustness', 'experiment', 'use', 'hyperparameter', 'use', 'atari', 'dqn', 'figure', 'investigate', 'robustness', 'dqn', 'agent', 'robustness', 'random', 'shift', 'respect', 'random', 'translation', 'image', 'perturbation', 'apply', 'observation', 'describe', 'yarat', 'evaluate', 'agent', 'atari', 'environment', 'vary', 'degree', 'input', 'scaling', 'notice', 'performance', 'deteriorate', 'abruptly', 'performance', 'dqn', 'also', 'decrease', 'well', 'e', 'e', 'r', 'p', 'e', 'g', 'e', 'r', 'e', 'p', '−', 'n', 'e', 'h', 'e', 'c', 'r', 'e', 'u', 'r', 'r', 'n', 'z', 'r', 'r', 'e', 'b', 'c', 'p', 'n', 'n', 'b', 'r', 'e', 'b', 'u', 'l', 'preprint', 'figure', 'atari', 'robustness', 'random', 'shift', 'evaluation', 'measure', 'robustness', 'dqn', 'random', 'shift', 'atari', 'game', 'unplug', 'dqn', 'agent', 'train', 'oﬄine', 'perturbation', 'input', 'rl', 'unplugged', 'dataset', 'however', 'evaluation', 'time', 'perturb', 'input', 'image', 'random', 'shift', 'datum', 'augmentation', 'overall', 'dqn', 'robust', 'evaluationtime', 'randomscale', 'image', 'perturbation', 'see', 'train', 'diﬀerence', 'pronounced', 'icehockey', 'beamrider', 'game', 'dqn', 'achieve', 'mean', 'auc', 'diﬀerent', 'game', 'get', 'figure', 'atari', 'robustness', 'random', 'scaling', 'evaluation', 'measure', 'robustness', 'dqn', 'random', 'scaling', 'input', 'atari', 'game', 'unplug', 'dqn', 'agent', 'train', 'oﬄine', 'data', 'augmentation', 'rl', 'unplugged', 'dataset', 'however', 'perturb', 'observation', 'evaluation', 'random', 'scale', 'datum', 'augmentation', 'observe', 'dqn', 'robust', 'evaluationtime', 'randomscale', 'datum', 'augmentation', 'diﬀerence', 'pronounced', 'icehockey', 'beamrider', 'robotank', 'game', 'dqn', 'achieve', 'mean', 'auc', 'diﬀerent', 'game', 'achieve', 'e', 'r', 'e', 'u', 'r', 'robotank', 'beamrider', 'roadrunner', 'dqn', 'random', 'shift', 'rate', 'e', 'r', 'e', 'u', 'r', 'mspacman', 'robotank', 'beamrider', 'roadrunner', 'random', 'scale', 'value', 'dqn', 'preprint', 'robustness', 'random', 'scaling', 'figure', 'explore', 'robustness', 'dqn', 'agent', 'respect', 'random', 'scaling', 'image', 'perturbation', 'method', 'apply', 'observation', 'feed', 'deep', 'qnetwork', 'randomly', 'scale', 'image', 'input', 'describe', 'evaluate', 'agent', 'online', 'environment', 'test', 'vary', 'level', 'image', 'scale', 'result', 'similar', 'random', 'shift', 'experiment', 'dqn', 'agent', 'robust', 'random', 'perturbation', 'online', 'environment', 'accord', 'empirical', 'result', 'obtain', 'experiment', 'possible', 'see', 'dqn', 'robust', 'evaluationtime', 'input', 'perturbation', 'speciﬁcally', 'random', 'shift', 'scaling', 'agent', 'thus', 'robust', 'representation', 'necessarily', 'require', 'high', 'eﬀective', 'rank', 'discussion', 'work', 'empirically', 'investigate', 'previously', 'hypothesize', 'connection', 'low', 'eﬀective', 'rank', 'poor', 'performance', 'find', 'relationship', 'eﬀective', 'rank', 'performance', 'simple', 'previously', 'conjecture', 'discover', 'oﬄine', 'agent', 'train', 'qlearne', 'training', 'go', 'phase', 'eﬀective', 'rank', 'collapse', 'severely', 'low', 'value', 'ﬁrst', 'phase', 'call', 'selfpruning', 'phase', 'agent', 'start', 'learn', 'basic', 'behavior', 'dataset', 'second', 'phase', 'eﬀective', 'rank', 'start', 'go', 'third', 'phase', 'eﬀective', 'rank', 'collapse', 'several', 'factor', 'learn', 'rate', 'activation', 'function', 'number', 'training', 'step', 'inﬂuence', 'occurrence', 'persistence', 'extent', 'severity', 'phase', 'learn', 'general', 'low', 'rank', 'always', 'indicative', 'poor', 'performance', 'strong', 'empirical', 'evidence', 'propose', 'hypothesis', 'try', 'explain', 'underlie', 'phenomenon', 'feature', 'useful', 'task', 'neural', 'network', 'try', 'solve', 'low', 'rank', 'correlate', 'robust', 'internal', 'representation', 'lead', 'well', 'generalization', 'unfortunately', 'reason', 'mean', 'rank', 'low', 'hard', 'general', 'rank', 'agnostic', 'direction', 'variation', 'datum', 'ignore', 'higherorder', 'term', 'hint', 'compact', 'representation', 'datum', 'dimension', 'result', 'indicate', 'agent', 'eﬀective', 'rank', 'performance', 'correlate', 'restrict', 'setting', 'relu', 'activation', 'function', 'qlearne', 'ﬁxed', 'architecture', 'however', 'show', 'experiment', 'correlation', 'primarily', 'spurious', 'setting', 'disappear', 'simple', 'modiﬁcation', 'change', 'activation', 'function', 'learning', 'rate', 'find', 'several', 'way', 'improve', 'eﬀective', 'rank', 'agent', 'improve', 'performance', 'use', 'tanh', 'instead', 'relu', 'auxiliary', 'loss', 'eg', 'curl', 'optimizer', 'method', 'address', 'rank', 'collapse', 'underlying', 'learning', 'deﬁciency', 'cause', 'collapse', 'poor', 'performance', 'nevertheless', 'result', 'show', 'dynamic', 'rank', 'agent', 'performance', 'learning', 'still', 'poorly', 'understand', 'need', 'theoretical', 'investigation', 'understand', 'relationship', 'factor', 'also', 'observe', 'tandem', 'oﬄine', 'setting', 'rank', 'collapse', 'minimal', 'value', 'early', 'training', 'unexplained', 'variance', 'agent', 'later', 'stage', 'learn', 'overall', 'cause', 'role', 'early', 'rank', 'collapse', 'remain', 'unknown', 'believe', 'understand', 'potential', 'eﬀect', 'essential', 'understand', 'largescale', 'agent', 'practical', 'learning', 'dynamic', 'existence', 'lowrank', 'highperforme', 'policy', 'suggest', 'network', 'overparameterize', 'task', 'parsimonious', 'representation', 'emerge', 'naturally', 'tdlearningbase', 'bootstrapping', 'loss', 'relu', 'network', 'oﬄine', 'set', 'discard', 'dead', 'relu', 'unit', 'achieve', 'eﬃcient', 'inference', 'believe', 'ﬁnding', 'give', 'inspiration', 'new', 'family', 'prune', 'algorithm', 'acknowledgement', 'like', 'thank', 'clare', 'lyle', 'dabney', 'discussion', 'want', 'thank', 'rowland', 'agarwal', 'aviral', 'feedback', 'early', 'draft', 'version', 'paper', 'want', 'thank', 'winckler', 'help', 'infrastructure', 'codebase', 'inception', 'project', 'like', 'thank', 'developer', 'bradbury', 'optax', 'hessel', 'preprint', 'reference', 'rishabh', 'agarwal', 'dale', 'schuurman', 'mohammad', 'optimistic', 'perspective', 'oﬄine', 'reinforce', 'ment', 'learning', 'international', 'conference', 'machine', 'learning', 'page', 'pmlr', 'rishabh', 'agarwal', 'max', 'schwarzer', 'bellemare', 'deep', 'reinforcement', 'learn', 'edge', 'statistical', 'precipice', 'advance', 'neural', 'information', 'processing', 'system', 'openai', 'dexterous', 'inhand', 'manipulation', 'international', 'journal', '3913–20', 'sanjeev', 'arora', 'yupe', 'implicit', 'regularization', 'deep', 'matrix', 'factorization', 'advance', 'neural', 'information', 'processing', 'system', 'regularization', 'international', 'conference', 'learn', 'representation', 'leibo', 'heinrich', 'küttler', 'green', 'preprint', 'bellemare', 'sriram', 'srinivasan', 'ostrovski', 'remi', 'muno', 'unify', 'countbase', 'exploration', 'intrinsic', 'motivation', 'advance', 'neural', 'information', 'processing', 'system', 'marc', 'g', 'bellemare', 'yavar', 'naddaf', 'arcade', 'learn', 'environment', 'evaluation', 'platform', 'general', 'agent', 'journal', 'artiﬁcial', 'intelligence', 'research', '47253–279', 'large', 'scale', 'deep', 'reinforcement', 'learn', 'arxiv', 'preprint', 'skye', 'qiao', 'composable', 'transformation', 'pythonnumpy', 'program', 'bradtke', 'linear', 'leastsquare', 'algorithm', 'temporal', 'diﬀerence', 'learning', 'machine', 'learn', 'rethink', 'atrous', 'convolution', 'semantic', 'image', 'segmentation', 'arxiv', 'preprint', 'lucchi', 'batch', 'normalization', 'provably', 'avoid', 'rank', 'collapse', 'randomly', 'initialise', 'deep', 'network', 'advance', 'neural', 'information', 'processing', 'system', 'pascanu', 'caglar', 'kyunghyun', 'surya', 'bengio', 'identify', 'attack', 'saddle', 'point', 'problem', 'highdimensional', 'preprint', 'laurent', 'pascanu', 'samy', 'bengio', 'generalize', 'deep', 'net', 'international', 'conference', 'machine', 'learning', 'volume', 'page', 'pmlr', 'todd', 'hester', 'challenge', 'realworld', 'reinforcement', 'learn', 'preprint', 'ernst', 'pierre', 'geurt', 'louis', 'wehenkel', 'treebased', 'batch', 'mode', 'reinforcement', 'learn', 'journal', 'machine', 'learn', 'research', 'preprint', 'pierre', 'behnam', 'sharpnessaware', 'minimization', 'eﬃciently', 'improve', 'generalization', 'arxiv', 'preprint', 'd4rl', 'dataset', 'deep', 'datadriven', 'reinforcement', 'learn', 'arxiv', 'preprint', 'address', 'function', 'approximation', 'error', 'actorcritic', 'method', 'arxiv', 'preprint', 'fujimoto', 'benchmarke', 'batch', 'deep', 'reinforcement', 'learning', 'algorithm', 'preprint', 'behrooz', 'investigation', 'neural', 'net', 'optimization', 'hessian', 'density', 'arxiv', 'preprint', 'bengio', 'understand', 'diﬃculty', 'train', 'deep', 'feedforward', 'neural', 'network', 'international', 'conference', 'artiﬁcial', 'intelligence', 'statistic', 'page', 'jmlr', 'workshop', 'conference', 'proceeding', 'antoine', 'borde', 'bengio', 'deep', 'sparse', 'neural', 'network', 'international', 'conference', 'artiﬁcial', 'intelligence', 'statistic', 'page', 'jmlr', 'workshop', 'conference', 'proceeding', 'caglar', 'gulcehre', 'noisy', 'activation', 'function', 'international', 'conference', 'machine', 'learning', 'page', 'pmlr', 'caglar', 'rishabh', 'unplugged', 'benchmark', 'oﬄine', 'reinforcement', 'learn', 'preprint', 'caglar', 'gulcehre', 'freita', 'regularize', 'behavior', 'value', 'estimation', 'preprint', 'kaime', 'xiangyu', 'shaoqe', 'delve', 'deep', 'rectiﬁer', 'surpass', 'human', 'level', 'performance', 'imagenet', 'classiﬁcation', 'proceeding', 'ieee', 'international', 'conference', 'computer', 'vision', 'page', 'igor', 'babuschkin', 'haiku', 'sonnet', 'url', 'composable', 'gradient', 'transformation', 'optimisation', 'url', 'baumli', 'serkan', 'cabi', 'caglar', 'nando', 'freita', 'acme', 'research', 'framework', 'distribute', 'reinforcement', 'learn', 'preprint', 'deep', 'residual', 'network', 'generalize', 'well', 'deep', 'feedforward', 'network', 'neural', 'tangent', 'kernel', 'preprint', 'minyoung', 'isola', 'lowrank', 'simplicity', 'bias', 'deep', 'network', 'arxiv', 'preprint', 'maximilian', 'gregory', 'transient', 'nonstationarity', 'generalisation', 'deep', 'reinforcement', 'learn', 'preprint', 'preprint', 'ying', 'pessimism', 'provably', 'eﬃcient', 'oﬄine', 'rl', 'international', 'conference', 'machine', 'learning', 'page', 'pmlr', 'nakkiran', 'boaz', 'sgd', 'neural', 'network', 'learn', 'function', 'increase', 'complexity', 'advance', 'neural', 'information', 'processing', 'system', 'dheevatsa', 'mudigere', 'jorge', 'nocedal', 'mikhail', 'smelyanskiy', 'ping', 'largebatch', 'training', 'deep', 'learning', 'generalization', 'gap', 'sharp', 'preprint', 'diederik', 'p', 'kingma', 'method', 'stochastic', 'optimization', 'international', 'conference', 'learn', 'representation', 'caglar', 'gulcehre', 'cosmin', 'nando', 'freitas', 'active', 'oﬄine', 'policy', 'selection', 'advance', 'neural', 'information', 'processing', 'system', 'aviral', 'stabilize', 'oﬀpolicy', 'qlearne', 'conference', 'neural', 'information', 'processing', 'system', 'page', 'bootstrappe', 'error', 'reduction', 'aviral', 'implicit', 'underparameterization', 'inhibit', 'dataeﬃcient', 'deep', 'reinforcement', 'learn', 'arxiv', 'preprint', '2020a', 'aviral', 'conservative', 'qlearne', 'oﬄine', 'reinforce', 'ment', 'learn', 'preprint', 'aviral', 'value', 'base', 'deep', 'reinforcement', 'learning', 'require', 'explicit', 'regularization', 'real', 'life', 'workshop', 'overparameterization', 'pitfall', 'opportunity', 'workshop', 'icml', 'comfiled1fg43h5oagqpksjpwbfadyezafmvjm6view', 'aviral', 'valuebase', 'deep', 'reinforcement', 'learning', 'require', 'explicit', 'preprint', 'aviral', 'workﬂow', 'oﬄine', 'modelfree', 'robotic', 'reinforcement', 'learn', 'preprint', 'michail', 'g', 'lagoudaki', 'leastsquare', 'policy', 'iteration', 'journal', 'machine', 'learn', 'research', 'sascha', 'batch', 'reinforcement', 'learning', 'wiering', 'editor', 'reinforcement', 'learn', 'stateoftheart', 'page', 'pieter', 'abbeel', 'contrastive', 'unsupervised', 'representation', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learning', 'page', 'pmlr', 'fu', 'oﬄine', 'reinforcement', 'learn', 'tutorial', 'review', 'perspective', 'open', 'problem', 'preprint', 'visualize', 'loss', 'landscape', 'neural', 'net', 'preprint', 'yuanzhi', 'yingyu', 'overparameterize', 'neural', 'network', 'stochastic', 'gradient', 'descent', 'structure', 'datum', 'arxiv', 'preprint', 'preprint', 'clare', 'rowland', 'ostrovski', 'dabney', 'eﬀect', 'auxiliary', 'task', 'representation', 'dynamic', 'international', 'conference', 'artiﬁcial', 'intelligence', 'statistic', 'page', 'pmlr', 'sriram', 'srinivasan', 'domainindependent', 'optimistic', 'initialization', 'reinforcement', 'learning', 'workshop', 'twentyninth', 'aaai', 'conference', 'artiﬁcial', 'intelligence', 'selfregularization', 'deep', 'neural', 'network', 'evidence', 'random', 'matrix', 'theory', 'implication', 'learn', 'journal', 'machine', 'learn', 'research', 'ozair', 'srivatsan', 'caglar', 'unplug', 'large', 'scale', 'oﬄine', 'reinforcement', 'learning', 'deep', 'rl', 'workshop', 'neurip', 'aslanide', 'young', 'campbellgillingham', 'geoﬀrey', 'nat', 'mcaleese', 'teach', 'language', 'model', 'support', 'answer', 'preprint', 'rusu', 'veness', 'marc', 'g', 'bellemare', 'fidjeland', 'georg', 'ostrovski', 'demis', 'humanlevel', 'control', 'deep', 'reinforcement', 'learn', 'nature', 'farajtabar', 'preprint', 'andre', 'saraiva', 'behaviour', 'suite', 'reinforcement', 'learn', 'preprint', 'georg', 'ostrovski', 'pablo', 'dabney', 'diﬃculty', 'passive', 'learning', 'deep', 'reinforce', 'ment', 'learning', 'advance', 'neural', 'information', 'processing', 'system', 'nando', 'freitas', 'hyperparameter', 'selection', 'oﬄine', 'reinforcement', 'learn', 'arxiv', 'preprint', 'random', 'matrix', 'theory', 'deep', 'learning', 'advance', 'neural', 'information', 'processing', 'system', 'dean', 'pomerleau', 'autonomous', 'land', 'vehicle', 'neural', 'network', 'conference', 'neural', 'information', 'processing', 'system', 'page', 'ﬁtte', 'iteration', 'ﬁrst', 'experience', 'data', 'eﬃcient', 'neural', 'reinforcement', 'learning', 'method', 'pavel', 'b', 'brazdil', 'alípio', 'mário', 'luís', 'torgo', 'editor', 'european', 'conference', 'machine', 'learning', 'page', 'levent', 'sagun', 'léon', 'bottou', 'lecun', 'singularity', 'hessian', 'deep', 'learning', 'preprint', 'levent', 'sagun', 'utku', 'evci', 'empirical', 'analysis', 'hessian', 'overparametrized', 'neural', 'network', 'philip', 'torr', 'puneet', 'robustness', 'deep', 'lowrank', 'representation', 'arxiv', 'preprint', 'oﬄine', 'reinforcement', 'learning', 'autonomous', 'driving', 'safety', 'exploration', 'enhancement', 'arxiv', 'preprint', 'preprint', 'yeonjong', 'karniadaki', 'trainability', 'relu', 'network', 'datadependent', 'initialization', 'journal', 'machine', 'learn', 'modeling', 'compute', 'simonyan', 'master', 'game', 'go', 'human', 'knowledge', 'nature', 'interpretation', 'interaction', 'contingency', 'table', 'journal', 'royal', 'statistical', 'society', 'series', 'methodological', '132238–241', 'quoc', 'bayesian', 'perspective', 'generalization', 'stochastic', 'preprint', 'soham', 'origin', 'implicit', 'regularization', 'stochastic', 'preprint', 'tijman', 'tieleman', 'geoﬀrey', 'divide', 'gradient', 'run', 'average', 'recent', 'magnitude', 'coursera', 'neural', 'network', 'machine', 'learn', 'hado', 'deep', 'reinforcement', 'learn', 'double', 'qlearning', 'thirtieth', 'conference', 'artiﬁcial', 'intelligence', 'oriol', 'vinyal', 'igor', 'babuschkin', 'wojciech', 'ewald', 'petko', 'georgiev', 'grandmaster', 'level', 'use', 'multiagent', 'reinforcement', 'learn', 'nature', 'alekh', 'agarwal', 'bellmanconsistent', 'pessimism', 'oﬄine', 'reinforcement', 'learning', 'advance', 'neural', 'information', 'processing', 'system', 'yarat', 'ilya', 'fergus', 'image', 'augmentation', 'need', 'regularize', 'deep', 'reinforcement', 'learn', 'pixel', 'international', 'conference', 'learn', 'representation', 'appendix', 'impact', 'depth', 'pennington', 'explore', 'relationship', 'rank', 'depth', 'neural', 'network', 'initialization', 'find', 'rank', 'layer', 'feature', 'matrix', 'decrease', 'proportionally', 'index', 'layer', 'deep', 'network', 'test', 'performance', 'feedforward', 'network', 'bsuite', 'task', 'train', 'use', 'double', 'qlearne', 'layer', 'see', 'eﬀect', 'number', 'layer', 'eﬀective', 'rank', 'network', 'use', 'relu', 'activation', 'function', 'use', 'initialization', 'figure', 'illustrate', 'rank', 'collapse', 'progress', 'low', 'layer', 'high', 'layer', 'proportionally', 'end', 'training', 'well', 'network', 'eﬀective', 'rank', 'rank', 'penultimate', 'layer', 'drop', 'minimal', 'value', 'task', 'regardless', 'network', 'number', 'layer', 'last', 'layer', 'network', 'act', 'bottleneck', 'thus', 'collapse', 'eﬀective', 'rank', 'reduce', 'expressivity', 'nevertheless', 'deep', 'network', 'rank', 'shallow', 'one', 'learn', 'represent', 'large', 'class', 'function', 'less', 'underparametrized', 'agent', 'exhibit', 'poor', 'performance', 'eﬀective', 'rank', 'collapse', 'point', 'relu', 'unit', 'die', 'become', 'irrespective', 'input', 'thus', 'bsuite', 'deep', 'network', 'layered', 'network', 'perform', 'bad', 'layered', 'density', 'hessian', 'oﬄine', 'analyze', 'eigenvalue', 'spectrum', 'hessian', 'common', 'way', 'investigate', 'learning', 'dynamic', 'loss', 'surface', 'deep', 'learning', 'method', 'ghorbani', 'understand', 'hessian', 'loss', 'landscape', 'eigenvalue', 'spectrum', 'help', 'design', 'well', 'optimization', 'algorithm', 'preprint', 'l', 'r', 'figure', 'rank', 'depth', 'network', 'evolution', 'rank', 'diﬀerent', 'layer', 'deep', 'neural', 'network', 'ﬁgure', 'left', 'l', 'rank', 'catch', 'diﬀerent', 'layer', 'ﬁgure', 'center', 'rank', 'mountaincar', 'diﬀerent', 'layer', 'ﬁgure', 'right', 'r', 'cartpole', 'analyze', 'eigenvalue', 'spectrum', 'single', 'hide', 'layer', 'feedforward', 'network', 'train', 'bsuite', 'catch', 'dataset', 'unplug', 'gulcehre', 'understand', 'losslandscape', 'network', 'low', 'eﬀective', 'rank', 'compare', 'model', 'high', 'rank', 'end', 'training', 'establish', 'figure', 'elu', 'activation', 'function', 'network', 'signiﬁcantly', 'high', 'eﬀective', 'rank', 'relu', 'network', 'compare', 'network', 'also', 'look', 'diﬀerence', 'eigenvalue', 'spectrum', 'network', 'high', 'low', 'rank', 'network', 'input', 'relatively', 'lowdimensional', 'compute', 'full', 'hessian', 'dataset', 'rather', 'lowrank', 'approximation', 'eigenvalue', 'spectrum', 'hessian', 'relu', 'elu', 'activation', 'function', 'show', 'figure', 'rank', 'collapse', 'fast', 'relu', 'elu', 'gradient', 'update', 'relu', 'network', 'concentrate', 'eigenvalue', 'hessian', 'due', 'dead', 'unit', 'relu', 'network', 'glorot', 'hand', 'elu', 'network', 'large', 'eigenvalue', 'number', 'gradient', 'update', 'figure', 'summarize', 'distribution', 'eigenvalue', 'hessian', 'matrix', 'elu', 'relu', 'network', 'result', 'hessian', 'feature', 'matrix', 'penultimate', 'layer', 'network', 'lowrank', 'moreover', 'case', 'landscape', 'relu', 'network', 'ﬂatt', 'elu', 'notion', 'wide', 'basin', 'sagun', 'mean', 'relu', 'network', 'ﬁnd', 'simple', 'solution', 'thus', 'ﬂatter', 'landscape', 'conﬁrme', 'simple', 'function', 'learn', 'less', 'capacity', 'use', 'end', 'training', 'induce', 'low', 'rank', 'representation', 'figure', 'bsuite', 'catch', 'spectral', 'density', 'hessian', 'visualization', 'spectral', 'density', 'full', 'hessian', 'network', 'train', 'unit', 'use', 'relu', 'leave', 'elu', 'right', 'activation', 'function', 'eigenvalue', 'hessian', 'oﬄine', 'dqn', 'relu', 'activation', 'function', 'concentrate', 'less', 'eigenvalue', 'elu', 'network', 'also', 'concentrate', 'around', 'large', 'outlier', 'eigenvalue', 'r', 'layer', 'net', 'layer', 'net', 'layer', 'net', 'network', 'depth', 'r', 'e', 'e', 'layer', 'net', 'layer', 'net', 'layer', 'net', 'network', 'depth', 'r', 'e', 'e', 'layer', 'net', 'layer', 'net', 'layer', 'net', 'network', 'depth', 'learner', 'step', 'learner', 'step', 'eigenvalue', 'learner', 'step', 'learner', 'step', 'eigenvalue', 'preprint', 'figure', 'bsuite', 'catch', 'hessian', 'eigenvalue', 'evs', 'oﬄine', 'dqn', 'visualize', 'percentage', 'positive', 'negative', 'nearzero', 'eigenvalue', 'hessian', 'oﬄine', 'dqn', 'elu', 'relu', 'activation', 'function', 'catch', 'dataset', 'bsuite', 'absolute', 'value', 'eigenvalue', 'less', 'consider', 'nearzero', 'eigenvalue', 'see', 'elu', 'network', 'positive', 'negative', 'eigenvalue', 'almost', 'evenly', 'distribute', 'however', 'relu', 'network', 'majority', 'eigenvalue', 'nearzero', 'evs', 'exactly', 'negative', 'positive', 'eigenvalue', 'a3', 'deepmind', 'lab', 'performance', 'eﬀective', 'rank', 'figure', 'show', 'correlation', 'eﬀective', 'rank', 'performance', 'r2d2', 'train', 'curl', 'train', 'look', 'variant', 'separately', 'aggregate', 'see', 'strong', 'correlation', 'performance', 'rank', 'agent', 'figure', 'deepmind', 'lab', 'eﬀective', 'rank', 'performance', 'correlation', 'feature', 'rank', 'episode', 'return', 'diﬀerent', 'exploration', 'noise', 'deepmind', 'lab', 'dataset', 'include', 'datum', 'model', 'train', 'curl', 'train', 'observe', 'strong', 'correlation', 'eﬀective', 'rank', 'performance', 'diﬀerent', 'noise', 'exploration', 'level', 'data', 'optimizer', 'smooth', 'loss', 'behavior', 'rank', 'collapse', 'study', 'relationship', 'smooth', 'loss', 'landscape', 'rank', 'collapse', 'use', 'sharpness', 'aware', 'minimization', 'loss', 'potentially', 'create', 'ﬂatter', 'loss', 'surface', 'order', 'see', 'smooth', 'loss', 'landscape', 'rank', 'performance', 'dynamic', 'diﬀerently', 'figure', 'show', 'evolution', 'feature', 'rank', 'generalization', 'performance', 'atari', 'deepmind', 'lab', 'respectively', 'observe', 'clear', 'relation', 'extent', 'smooth', 'loss', 'feature', 'rank', 'generalization', 'performance', 'nearzero', 'positive', 'evs', 'negative', 'relu', 'dqn', 'elu', 'dqn', 'ep', 'kendall', 'τ025', 'pearson', 'ep', 'kendall', 'pearson', 'ep', 'kendall', 'pearson', 'ep', 'kendall', 'pearson', 'variant', 'curl', 'r', 'e', 'e', 'l', 'r', 'e', 'terminal', 'episode', 'return', 'preprint', 'figure', 'atari', 'sharpness', 'minimization', 'loss', 'evolution', 'rank', 'increase', 'weight', 'auxiliary', 'loss', 'see', 'amount', 'weight', 'loss', 'help', 'mitigate', 'extent', 'rank', 'collapse', 'observe', 'clear', 'relationship', 'agent', 'performance', 'figure', 'deepmind', 'labseekavoid', 'snapshot', 'sharpness', 'aware', 'minimization', 'loss', 'observe', 'rank', 'collapse', 'continue', 'train', 'lstm', 'network', 'tanh', 'activation', 'discuss', 'section', 'use', 'deepmind', 'lab', 'dataset', 'spectrum', 'diﬀerent', 'weight', 'eﬀective', 'rank', 'value', 'error', 'curious', 'relationship', 'eﬀective', 'rank', 'value', 'error', 'potential', 'rank', 'collapse', 'phase', 'tdlearning', 'algorithm', 'error', 'propagate', 'thorough', 'bootstrappe', 'target', 'figure', 'show', 'correlation', 'value', 'error', 'eﬀective', 'rank', 'strong', 'anticorrelation', 'eﬀective', 'rank', 'performance', 'agent', 'level', 'expert', 'agent', 'generate', 'dataset', 'perform', 'poorly', 'end', 'training', 'eg', 'icehockey', 'preprint', 'figure', 'atari', 'plot', 'show', 'correlation', 'value', 'error', 'eﬀective', 'rank', 'oﬄine', 'dqn', 'agent', 'train', 'step', 'online', 'policy', 'selection', 'game', 'atari', 'apparent', 'anticorrelation', 'eﬀective', 'rank', 'value', 'error', 'namely', 'value', 'error', 'agent', 'evaluate', 'environment', 'increase', 'eﬀective', 'rank', 'decrease', 'correlation', 'signiﬁcant', 'atari', 'level', 'icehockey', 'expert', 'agent', 'generate', 'dataset', 'perform', 'poorly', 'make', 'hypothesis', 'extrapolation', 'error', 'cause', 'rank', 'collapse', 'push', 'agent', 'phase', 'plausible', 'a6', 'curl', 'deepmind', 'lab', 'figure', 'show', 'curl', 'result', 'deepmind', 'lab', 'dataset', 'ﬁnd', 'consistent', 'pattern', 'various', 'curl', 'loss', 'weight', 'figure', 'deepmind', 'labseekavoid', 'auxiliary', 'loss', 'evolution', 'rank', 'increase', 'weight', 'auxiliary', 'loss', 'auxiliary', 'loss', 'help', 'model', 'perform', 'well', 'clear', 'correlation', 'rank', 'performance', 'preprint', 'a7', 'learn', 'rate', 'evolution', 'figure', 'also', 'perform', 'hyperparameter', 'selection', 'evaluate', 'model', 'environment', 'various', 'stage', 'training', 'oﬄine', 'dqn', 'train', 'long', 'optimal', 'learning', 'rate', 'good', 'agent', 'performance', 'evaluate', 'online', 'go', 'increase', 'number', 'training', 'step', 'agent', 'need', 'change', 'learning', 'rate', 'accordingly', 'number', 'training', 'step', 'aﬀect', 'good', 'learning', 'rate', 'figure', 'atari', 'evolution', 'optimal', 'learning', 'rate', 'find', 'online', 'evaluation', 'environment', 'model', 'train', 'long', 'optimal', 'learning', 'rate', 'find', 'online', 'evaluation', 'go', 'learn', 'curve', 'long', 'training', 'regime', 'diﬀerent', 'phase', 'learn', 'subsection', 'investigate', 'eﬀect', 'change', 'learn', 'rate', 'eﬀective', 'rank', 'performance', 'agent', 'unplugged', 'online', 'policy', 'selection', 'game', 'train', 'oﬄine', 'dqn', 'learn', 'step', 'time', 'long', 'typical', 'oﬄine', 'atari', 'agent', 'gulcehre', 'evaluate', 'learning', 'rate', 'equally', 'space', 'logarithmic', 'space', 'show', 'eﬀective', 'rank', 'learning', 'performance', 'curve', 'figure', 'easy', 'identify', 'diﬀerent', 'phase', 'learn', 'curve', 'a9', 'eﬀective', 'rank', 'performance', 'figure', 'show', 'correlation', 'eﬀective', 'rank', 'performance', 'agent', 'train', 'minibatch', 'size', 'atari', 'online', 'policy', 'selection', 'game', 'possible', 'see', 'strong', 'correlation', 'game', 'correlation', 'seem', 'even', 'figure', 'depict', 'correlation', 'eﬀective', 'rank', 'performance', 'dqn', 'agent', 'relu', 'activation', 'function', 'signiﬁcant', 'correlation', 'atari', 'game', 'discuss', 'early', 'long', 'training', 'set', 'relu', 'activation', 'function', 'eﬀect', 'rank', 'strong', 'performance', 'a10', 'intervention', 'eﬀective', 'rank', 'randomize', 'control', 'trial', 'rct', 'rank', 'continuous', 'variable', 'ﬁlter', 'experiment', 'certain', 'rank', 'threshold', 'control', 'case', 'assume', 'τ', 'λ0', 'also', 'write', 'eλdoβ', 'eλdoβ', 'a11', 'computation', 'feature', 'rank', 'present', 'codestub', 'use', 'experiment', 'similar', '2020a', 'compute', 'feature', 'rank', 'preoutput', 'layer', 'feature', 'import', 'numpy', 'def', 'computerankfromfeaturesfeaturematrix', 'rankdelta001', 'e', 'r', 'g', 'r', 'e', 'l', 'learn', 'step', 'preprint', 'figure', 'atari', 'eﬀective', 'rank', 'curve', 'eﬀective', 'rank', 'performance', 'oﬄine', 'dqn', 'agent', 'atari', 'online', 'policy', 'selection', 'game', 'train', 'oﬄine', 'dqn', 'agent', 'learn', 'step', 'evaluate', 'learning', 'rate', 'equally', 'space', 'logarithmic', 'space', 'let', 'note', 'game', 'rank', 'go', 'early', 'training', 'phase', 'go', 'phase', 'learning', 'rate', 'eﬀective', 'rank', 'collapse', 'phase', 'correspondingly', 'performance', 'low', 'beginning', 'training', 'phase', 'go', 'stay', 'high', 'phase', 'sometimes', 'performance', 'collapse', 'phase', 'preprint', 'figure', 'atari', 'correlation', 'rank', 'return', 'dqn', 'train', 'minibatch', 'size', 'run', 'network', 'learning', 'rate', 'ﬁve', 'diﬀerent', 'seed', 'show', 'mean', 'ﬁve', 'seed', 'see', 'strong', 'correlation', 'game', 'correlation', 'consistent', 'figure', 'atari', 'correlation', 'rank', 'return', 'dqn', 'train', 'network', 'train', 'learn', 'step', 'diﬀerent', 'learning', 'rate', 'minibatch', 'size', 'see', 'signiﬁcant', 'positive', 'correlation', 'rank', 'learn', 'rate', 'online', 'policy', 'selection', 'game', 'beamrider', 'spearman', 'spearman', 'doubledunk', 'spearman', 'corr', 'r', 'e', 'e', 'l', 'r', 'e', 'robotank', 'spearman', 'pooyan', 'spearman', 'learn', 'rate', 'spearman', 'episode', 'return', 'preprint', 'hyperparameter', 'training', 'batch', 'size', 'rank', 'calculation', 'batch', 'size', 'num', 'training', 'step', 'learn', 'rate', 'optimizer', 'feedforward', 'hidden', 'layer', 'size', 'hide', 'layer', 'activation', 'memory', 'discount', 'bsuite', '1e6', '3e4', 'relu', 'none', 'atari', 'relu', 'none', 'deepmind', 'lab', 'episode', 'relu', 'lstm', 'gate', 'lstm', 'tanh', 'table', 'default', 'hyperparameter', 'use', 'work', 'diﬀerent', 'domain', 'compute', 'rank', 'feature', 'base', 'many', 'singular', 'value', 'significant', 'singvalue', 'nplinalgsvdfeaturematrix', 'npcumsumsingvalue', 'nuclearnorm', 'npsumsingvalue', 'thresholdcrosse', 'approximaterankthreshold', 'nuclearnorm', 'effectiverank', 'singvaluesshape0', 'npsumthresholdcrosse', 'return', 'effectiverank', 'a12', 'hyperparameter', 'list', 'standard', 'set', 'hyperparameter', 'use', 'diﬀerent', 'domain', 'bsuite', 'atari', 'deepmind', 'lab', 'respectively', 'default', 'hyperparameter', 'diﬀer', 'state', 'speciﬁc', 'ablation', 'study', 'task', 'use', 'network', 'use', 'atari', 'task', 'use', 'convolution', 'torso', 'use', 'involve', 'layer', 'convolution', 'relu', 'activation', 'layer', 'kernelshape8', 'stride4', 'layer', 'kernelshape4', 'stride2', 'layer', 'kernelshape3', 'stride1', 'a13', 'bsuite', 'phase', 'transition', 'bottleneck', 'capacity', 'illustrate', 'phase', 'transition', 'simple', 'mlp', 'relu', 'activation', 'figure', 'network', 'size', 'bottleneck', 'unit', 'vary', 'number', 'bottleneck', 'unit', 'figure', 'network', 'size', 'bottleneck', 'unit', 'vary', 'number', 'bottleneck', 'unit', 'case', 'small', 'number', 'bottleneck', 'unit', 'reduce', 'performance', 'mode', 'agent', 'able', 'solve', 'problem', 'even', 'penultimate', 'layer', 'eﬀective', 'rank', 'small', 'large', 'learning', 'rate', 'right', 'handside', 'ﬁgure', 'eﬀective', 'rank', 'tend', 'low', 'a14', 'activation', 'sparsity', 'bsuite', 'figure', 'show', 'activation', 'relu', 'network', 'become', 'sparse', 'course', 'train', 'sparsity', 'relu', 'unit', 'seem', 'signiﬁcantly', 'high', 'elu', 'unit', 'end', 'training', 'preprint', 'b', 'figure', 'catch', 'dataset', 'phase', 'transition', 'plot', 'network', 'hidden', 'layer', 'size', 'bottleneck', 'unit', 'yaxis', 'show', 'unit', 'xaxis', 'number', 'gradient', 'step', 'ﬁgure', 'left', 'handside', 'train', 'learning', 'rate', '8e5', 'right', 'handside', 'b', 'experiment', 'train', 'learning', 'rate', '3e4', 'row', 'show', 'episodic', 'return', 'second', 'row', 'show', 'eﬀective', 'rank', 'second', 'layer', 'bottleneck', 'unit', 'third', 'row', 'show', 'penultimate', 'layer', 'eﬀective', 'rank', 'eﬀective', 'rank', 'collapse', 'much', 'fast', 'learning', 'rate', '5e8', 'low', 'rank', 'still', 'good', 'performance', 'low', 'bottleneck', 'unit', 'cause', 'eﬀective', 'rank', 'last', 'layer', 'collapse', 'fast', 'performance', 'network', 'small', 'number', 'bottleneck', 'unit', 'poor', 'eﬀective', 'rank', 'small', 'number', 'bottleneck', 'unit', 'small', 'preprint', 'figure', 'bsuite', 'catch', 'dataset', 'phase', 'transition', 'plot', 'network', 'hidden', 'layer', 'size', 'bottleneck', 'unit', 'yaxis', 'show', 'unit', 'xaxis', 'number', 'gradient', 'step', 'ﬁgure', 'left', 'handside', 'train', 'learning', 'rate', '8e5', 'right', 'handside', 'b', 'experiment', 'train', 'learning', 'rate', '3e4', 'row', 'show', 'episodic', 'return', 'second', 'row', 'show', 'eﬀective', 'rank', 'second', 'layer', 'bottleneck', 'unit', 'eﬀective', 'rank', 'collapse', 'much', 'fast', 'learning', 'rate', '5e8', 'low', 'rank', 'still', 'good', 'performance', 'small', 'number', 'bottleneck', 'unit', 'cause', 'eﬀective', 'rank', 'layer', 'collapse', 'fast', 'performance', 'network', 'small', 'number', 'bottleneck', 'unit', 'poor', 'eﬀective', 'rank', 'small', 'number', 'bottleneck', 'unit', 'small', 'figure', 'bsuite', 'catch', 'gram', 'matrix', 'activation', 'gram', 'matrix', 'activation', 'twolayer', 'relu', 'elu', 'activation', 'function', 'activation', 'relu', 'unit', 'become', 'sparser', 'compare', 'elu', 'unit', 'end', 'training', 'dead', 'relu', 'unit', 'initialization', 'step', 'convergence', 'step', 'relu', 'elu']"
