title,url,date,summary,text,cleaning,tokens
Now we know what OpenAI’s superalignment team has been up to,https://www.technologyreview.com/2023/12/14/1085344/openai-super-alignment-rogue-agi-gpt-4/,2023-12-14,"<p>The firm wants to prevent a superintelligence from going rogue. This is the first step.</p>
","OpenAI has announced the first results from its superalignment team, the firm’s in-house initiative dedicated to preventing a superintelligence—a hypothetical future computer that can outsmart humans—from going rogue. Unlike many of the company’s announcements, this heralds no big breakthrough. In a low-key research paper, the team describes a technique that lets a less powerful large language model supervise a more powerful one—and suggests that this might be a small step toward figuring out how humans might supervise superhuman machines.   Less than a month after OpenAI was rocked by a crisis when its CEO, Sam Altman, was fired by its oversight board (in an apparent coup led by chief scientist Ilya Sutskever) and then reinstated three days later, the message is clear: it’s back to business as usual. Yet OpenAI’s business is not usual. Many researchers still question whether machines will ever match human intelligence, let alone outmatch it. OpenAI’s team takes machines’ eventual superiority as given. “AI progress in the last few years has been just extraordinarily rapid,” says Leopold Aschenbrenner, a researcher on the superalignment team. “We’ve been crushing all the benchmarks, and that progress is continuing unabated.” For Aschenbrenner and others at the company, models with human-like abilities are just around the corner. “But it won’t stop there,” he says. “We’re going to have superhuman models, models that are much smarter than us. And that presents fundamental new technical challenges.” In July, Sutskever and fellow OpenAI scientist Jan Leike set up the superalignment team to address those challenges. “I’m doing it for my own self-interest,” Sutskever told MIT Technology Review in September. “It’s obviously important that any superintelligence anyone builds does not go rogue. Obviously.” Amid speculation that Altman was fired for playing fast and loose with his company’s approach to AI safety, Sutskever’s superalignment team loomed behind the headlines. Many have been waiting to see exactly what it has been up to.  The question the team wants to answer is how to rein in, or “align,” hypothetical future models that are far smarter than we are, known as superhuman models. Alignment means making sure a model does what you want it to do and does not do what you don’t want it to do. Superalignment applies this idea to superhuman models. One of the most widespread techniques used to align existing models is called reinforcement learning via human feedback. In a nutshell, human testers score a model’s responses, upvoting behavior that they want to see and downvoting behavior they don’t. This feedback is then used to train the model to produce only the kind of responses that human testers liked. This technique is a big part of what makes ChatGPT so engaging.    The problem is that it requires humans to be able to tell what is and isn’t desirable behavior in the first place. But a superhuman model—the idea goes—might do things that a human tester can’t understand and thus would not be able to score. (It might even try to hide its true behavior from humans, Sutskever told us.)   The researchers point out that the problem is hard to study because superhuman machines do not exist. So they used stand-ins. Instead of looking at how humans could supervise superhuman machines, they looked at how GPT-2, a model that OpenAI released five years ago, could supervise GPT-4, OpenAI’s latest and most powerful model. “If you can do that, it might be evidence that you can use similar techniques to have humans supervise superhuman models,” says Collin Burns, another researcher on the superalignment team.    The team took GPT-2 and trained it to perform a handful of different tasks, including a set of chess puzzles and 22 common natural-language-processing tests that assess inference, sentiment analysis, and so on. They used GPT-2’s responses to those tests and puzzles to train GPT-4 to perform the same tasks. It’s as if a 12th grader were taught how to do a task by a third grader. The trick was to do it without GPT-4 taking too big a hit in performance. The results were mixed. The team measured the gap in performance between GPT-4 trained on GPT-2’s best guesses and GPT-4 trained on correct answers. They found that GPT-4 trained by GPT-2 performed 20% to 70% better than GPT-2 on the language tasks but did less well on the chess puzzles. The fact that GPT-4 outdid its teacher at all is impressive, says team member Pavel Izmailov: “This is a really surprising and positive result.” But it fell far short of what it could do by itself, he says. They conclude that the approach is promising but needs more work. “It is an interesting idea,” says Thilo Hagendorff, an AI researcher at the University of Stuttgart in Germany who works on alignment. But he thinks that GPT-2 might be too dumb to be a good teacher. “GPT-2 tends to give nonsensical responses to any task that is slightly complex or requires reasoning,” he says. Hagendorff would like to know what would happen if GPT-3 were used instead. He also notes that this approach does not address Sutskever’s hypothetical scenario in which a superintelligence hides its true behavior and pretends to be aligned when it isn’t. “Future superhuman models will likely possess emergent abilities which are unknown to researchers,” says Hagendorff. “How can alignment work in these cases?” But it is easy to point out shortcomings, he says. He is pleased to see OpenAI moving from speculation to experiment: “I applaud OpenAI for their effort.” OpenAI now wants to recruit others to its cause. Alongside this research update, the company announced a new $10 million money pot that it plans to use to fund people working on superalignment. It will offer grants of up to $2 million to university labs, nonprofits, and individual researchers and one-year fellowships of $150,000 to graduate students. “We’re really excited about this,” says Aschenbrenner. “We really think there’s a lot that new researchers can contribute.” ","OpenAI has announced the first results from its superalignment team , the firm ’ s in-house initiative dedicated to preventing a superintelligence—a hypothetical future computer that can outsmart humans—from going rogue . Unlike many of the company ’ s announcements , this heralds no big breakthrough . In a low-key research paper , the team describes a technique that lets a less powerful large language model supervise a more powerful one—and suggests that this might be a small step toward figuring out how humans might supervise superhuman machines . Less than a month after OpenAI was rocked by a crisis when its CEO , Sam Altman , was fired by its oversight board ( in an apparent coup led by chief scientist Ilya Sutskever ) and then reinstated three days later , the message is clear : it ’ s back to business as usual . Yet OpenAI ’ s business is not usual . Many researchers still question whether machines will ever match human intelligence , let alone outmatch it . OpenAI ’ s team takes machines ’ eventual superiority as given . “ AI progress in the last few years has been just extraordinarily rapid , ” says Leopold Aschenbrenner , a researcher on the superalignment team . “ We ’ ve been crushing all the benchmarks , and that progress is continuing unabated. ” For Aschenbrenner and others at the company , models with human-like abilities are just around the corner . “ But it won ’ t stop there , ” he says . “ We ’ re going to have superhuman models , models that are much smarter than us . And that presents fundamental new technical challenges. ” In July , Sutskever and fellow OpenAI scientist Jan Leike set up the superalignment team to address those challenges . “ I ’ m doing it for my own self-interest , ” Sutskever told MIT Technology Review in September . “ It ’ s obviously important that any superintelligence anyone builds does not go rogue . Obviously. ” Amid speculation that Altman was fired for playing fast and loose with his company ’ s approach to AI safety , Sutskever ’ s superalignment team loomed behind the headlines . Many have been waiting to see exactly what it has been up to . The question the team wants to answer is how to rein in , or “ align , ” hypothetical future models that are far smarter than we are , known as superhuman models . Alignment means making sure a model does what you want it to do and does not do what you don ’ t want it to do . Superalignment applies this idea to superhuman models . One of the most widespread techniques used to align existing models is called reinforcement learning via human feedback . In a nutshell , human testers score a model ’ s responses , upvoting behavior that they want to see and downvoting behavior they don ’ t . This feedback is then used to train the model to produce only the kind of responses that human testers liked . This technique is a big part of what makes ChatGPT so engaging . The problem is that it requires humans to be able to tell what is and isn ’ t desirable behavior in the first place . But a superhuman model—the idea goes—might do things that a human tester can ’ t understand and thus would not be able to score . ( It might even try to hide its true behavior from humans , Sutskever told us . ) The researchers point out that the problem is hard to study because superhuman machines do not exist . So they used stand-ins . Instead of looking at how humans could supervise superhuman machines , they looked at how GPT-2 , a model that OpenAI released five years ago , could supervise GPT-4 , OpenAI ’ s latest and most powerful model . “ If you can do that , it might be evidence that you can use similar techniques to have humans supervise superhuman models , ” says Collin Burns , another researcher on the superalignment team . The team took GPT-2 and trained it to perform a handful of different tasks , including a set of chess puzzles and 22 common natural-language-processing tests that assess inference , sentiment analysis , and so on . They used GPT-2 ’ s responses to those tests and puzzles to train GPT-4 to perform the same tasks . It ’ s as if a 12th grader were taught how to do a task by a third grader . The trick was to do it without GPT-4 taking too big a hit in performance . The results were mixed . The team measured the gap in performance between GPT-4 trained on GPT-2 ’ s best guesses and GPT-4 trained on correct answers . They found that GPT-4 trained by GPT-2 performed 20 % to 70 % better than GPT-2 on the language tasks but did less well on the chess puzzles . The fact that GPT-4 outdid its teacher at all is impressive , says team member Pavel Izmailov : “ This is a really surprising and positive result. ” But it fell far short of what it could do by itself , he says . They conclude that the approach is promising but needs more work . “ It is an interesting idea , ” says Thilo Hagendorff , an AI researcher at the University of Stuttgart in Germany who works on alignment . But he thinks that GPT-2 might be too dumb to be a good teacher . “ GPT-2 tends to give nonsensical responses to any task that is slightly complex or requires reasoning , ” he says . Hagendorff would like to know what would happen if GPT-3 were used instead . He also notes that this approach does not address Sutskever ’ s hypothetical scenario in which a superintelligence hides its true behavior and pretends to be aligned when it isn ’ t . “ Future superhuman models will likely possess emergent abilities which are unknown to researchers , ” says Hagendorff . “ How can alignment work in these cases ? ” But it is easy to point out shortcomings , he says . He is pleased to see OpenAI moving from speculation to experiment : “ I applaud OpenAI for their effort. ” OpenAI now wants to recruit others to its cause . Alongside this research update , the company announced a new $ 10 million money pot that it plans to use to fund people working on superalignment . It will offer grants of up to $ 2 million to university labs , nonprofits , and individual researchers and one-year fellowships of $ 150,000 to graduate students . “ We ’ re really excited about this , ” says Aschenbrenner . “ We really think there ’ s a lot that new researchers can contribute . ”","['announce', 'first', 'result', 'superalignment', 'team', 'firm', 'inhouse', 'initiative', 'dedicate', 'prevent', 'superintelligence', 'hypothetical', 'future', 'computer', 'outsmart', 'human', 'go', 'rogue', 'many', 'company', 'announcement', 'herald', 'big', 'breakthrough', 'lowkey', 'research', 'paper', 'team', 'describe', 'technique', 'let', 'less', 'powerful', 'large', 'language', 'model', 'supervise', 'powerful', 'suggest', 'small', 'step', 'figure', 'human', 'supervise', 'superhuman', 'machine', 'less', 'month', 'rock', 'crisis', 'ceo', 'fire', 'oversight', 'board', 'apparent', 'coup', 'lead', 'chief', 'scientist', 'ilya', 'sutskever', 'reinstate', 'day', 'later', 'message', 'clear', 'back', 'business', 'usual', 'yet', 'business', 'usual', 'many', 'researcher', 'still', 'question', 'machine', 'ever', 'match', 'human', 'intelligence', 'let', 'alone', 'outmatch', 'team', 'take', 'machine', 'eventual', 'superiority', 'give', 'ai', 'progress', 'last', 'year', 'extraordinarily', 'rapid', 'say', 'researcher', 'superalignment', 'team', 'crush', 'benchmark', 'progress', 'continue', 'unabated', 'aschenbrenner', 'company', 'model', 'humanlike', 'ability', 'corner', 'win', 'stop', 'say', 'go', 'superhuman', 'model', 'model', 'much', 'smart', 'present', 'fundamental', 'new', 'technical', 'challenge', 'sutskever', 'scientist', 'set', 'superalignment', 'team', 'address', 'challenge', 'selfinter', 'sutskever', 'tell', 'mit', 'technology', 'review', 'obviously', 'important', 'superintelligence', 'build', 'go', 'rogue', 'obviously', 'speculation', 'fire', 'play', 'fast', 'loose', 'company', 'approach', 'ai', 'safety', 'sutskever', 'superalignment', 'team', 'loom', 'headline', 'many', 'wait', 'see', 'exactly', 'question', 'team', 'want', 'answer', 'rein', 'align', 'hypothetical', 'future', 'model', 'far', 'smart', 'know', 'superhuman', 'model', 'alignment', 'mean', 'make', 'sure', 'model', 'want', 'want', 'superalignment', 'apply', 'idea', 'superhuman', 'model', 'widespread', 'technique', 'use', 'align', 'exist', 'model', 'call', 'reinforcement', 'learning', 'human', 'feedback', 'nutshell', 'human', 'tester', 'score', 'model', 'response', 'upvote', 'behavior', 'want', 'see', 'downvoting', 'behavior', 'feedback', 'use', 'train', 'model', 'produce', 'kind', 'response', 'human', 'tester', 'like', 'technique', 'big', 'part', 'make', 'chatgpt', 'engage', 'problem', 'require', 'human', 'able', 'tell', 'desirable', 'behavior', 'first', 'place', 'superhuman', 'model', 'idea', 'go', 'thing', 'human', 'tester', 'understand', 'thus', 'able', 'score', 'even', 'try', 'hide', 'true', 'behavior', 'human', 'sutskever', 'tell', 'researcher', 'point', 'problem', 'hard', 'study', 'superhuman', 'machine', 'exist', 'use', 'standin', 'instead', 'look', 'human', 'supervise', 'superhuman', 'machine', 'look', 'gpt2', 'model', 'release', 'year', 'ago', 'supervise', 'late', 'powerful', 'model', 'evidence', 'use', 'similar', 'technique', 'human', 'supervise', 'superhuman', 'model', 'say', 'burn', 'researcher', 'superalignment', 'team', 'team', 'take', 'gpt2', 'train', 'perform', 'handful', 'different', 'task', 'include', 'set', 'chess', 'puzzle', 'common', 'naturallanguageprocessing', 'test', 'assess', 'inference', 'sentiment', 'analysis', 'use', 'gpt2', 'response', 'test', 'puzzle', 'train', 'gpt4', 'perform', 'task', '12th', 'grader', 'teach', 'task', 'third', 'grader', 'trick', 'gpt4', 'take', 'big', 'hit', 'performance', 'result', 'mix', 'team', 'measure', 'gap', 'performance', 'gpt4', 'train', 'good', 'guess', 'gpt4', 'train', 'correct', 'answer', 'find', 'train', 'gpt2', 'perform', 'well', 'gpt2', 'language', 'task', 'less', 'well', 'chess', 'puzzle', 'fact', 'outdo', 'teacher', 'impressive', 'say', 'team', 'member', 'pavel', 'izmailov', 'really', 'surprising', 'positive', 'result', 'fall', 'far', 'short', 'say', 'conclude', 'approach', 'promise', 'need', 'work', 'interesting', 'idea', 'say', 'researcher', 'university', 'stuttgart', 'work', 'alignment', 'think', 'gpt2', 'dumb', 'good', 'teacher', 'gpt2', 'tend', 'give', 'nonsensical', 'response', 'task', 'slightly', 'complex', 'require', 'reasoning', 'say', 'like', 'know', 'happen', 'use', 'instead', 'also', 'note', 'approach', 'address', 'sutskever', 'hypothetical', 'scenario', 'superintelligence', 'hide', 'true', 'behavior', 'pretend', 'align', 'future', 'superhuman', 'model', 'likely', 'possess', 'emergent', 'ability', 'unknown', 'researcher', 'say', 'alignment', 'work', 'case', 'easy', 'point', 'shortcoming', 'say', 'pleased', 'see', 'openai', 'move', 'speculation', 'experiment', 'applaud', 'effort', 'want', 'recruit', 'cause', 'research', 'update', 'company', 'announce', 'new', 'money', 'pot', 'plan', 'use', 'fund', 'people', 'work', 'superalignment', 'offer', 'grant', 'university', 'individual', 'researcher', 'oneyear', 'fellowship', 'graduate', 'student', 'really', 'excited', 'say', 'aschenbrenner', 'really', 'think', 'lot', 'new', 'researcher', 'contribute']"
Google DeepMind used a large language model to solve an unsolvable math problem,https://www.technologyreview.com/2023/12/14/1085318/google-deepmind-large-language-model-solve-unsolvable-math-problem-cap-set/,2023-12-14,"<p>They had to throw away most of what it produced but there was gold among the garbage.</p>
","Google DeepMind has used a large language model to crack a famous unsolved problem in pure mathematics. In a paper published in Nature today, the researchers say it is the first time a large language model has been used to discover a solution to a long-standing scientific puzzle—producing verifiable and valuable new information that did not previously exist. “It’s not in the training data—it wasn’t even known,” says coauthor Pushmeet Kohli, vice president of research at Google DeepMind. Large language models have a reputation for making things up, not for providing new facts. Google DeepMind’s new tool, called FunSearch, could change that. It shows that they can indeed make discoveries—if they are coaxed just so, and if you throw out the majority of what they come up with. FunSearch (so called because it searches for mathematical functions, not because it’s fun) continues a streak of discoveries in fundamental math and computer science that DeepMind has made using AI. First AlphaTensor found a way to speed up a calculation at the heart of many different kinds of code, beating a 50-year record. Then AlphaDev found ways to make key algorithms used trillions of times a day run faster. Yet those tools did not use large language models. Built on top of DeepMind’s game-playing AI AlphaZero, both solved math problems by treating them as if they were puzzles in Go or chess. The trouble is that they are stuck in their lanes, says Bernardino Romera-Paredes, a researcher at the company who worked on both AlphaTensor and FunSearch: “AlphaTensor is great at matrix multiplication, but basically nothing else.” FunSearch takes a different tack. It combines a large language model called Codey, a version of Google’s PaLM 2 that is fine-tuned on computer code, with other systems that reject incorrect or nonsensical answers and plug good ones back in. “To be very honest with you, we have hypotheses, but we don’t know exactly why this works,” says Alhussein Fawzi, a research scientist at Google DeepMind. “In the beginning of the project, we didn’t know whether this would work at all.” The researchers started by sketching out the problem they wanted to solve in Python, a popular programming language. But they left out the lines in the program that would specify how to solve it. That is where FunSearch comes in. It gets Codey to fill in the blanks—in effect, to suggest code that will solve the problem. A second algorithm then checks and scores what Codey comes up with. The best suggestions—even if not yet correct—are saved and given back to Codey, which tries to complete the program again. “Many will be nonsensical, some will be sensible, and a few will be truly inspired,” says Kohli. “You take those truly inspired ones and you say, ‘Okay, take these ones and repeat.’” After a couple of million suggestions and a few dozen repetitions of the overall process—which took a few days—FunSearch was able to come up with code that produced a correct and previously unknown solution to the cap set problem, which involves finding the largest size of a certain type of set. Imagine plotting dots on graph paper. The cap set problem is like trying to figure out how many dots you can put down without three of them ever forming a straight line. It’s super niche, but important. Mathematicians do not even agree on how to solve it, let alone what the solution is. (It is also connected to matrix multiplication, the computation that AlphaTensor found a way to speed up.) Terence Tao at the University of California, Los Angeles, who has won many of the top awards in mathematics, including the Fields Medal, called the cap set problem “perhaps my favorite open question” in a 2007 blog post. Tao is intrigued by what FunSearch can do. “This is a promising paradigm,” he says. “It is an interesting way to leverage the power of large language models.” A key advantage that FunSearch has over AlphaTensor is that it can, in theory, be used to find solutions to a wide range of problems. That’s because it produces code—a recipe for generating the solution, rather than the solution itself. Different code will solve different problems. FunSearch’s results are also easier to understand. A recipe is often clearer than the weird mathematical solution it produces, says Fawzi. To test its versatility, the researchers used FunSearch to approach another hard problem in math: the bin packing problem, which involves trying to pack items into as few bins as possible. This is important for a range of applications in computer science, from data center management to e-commerce. FunSearch came up with a way to solve it that’s faster than human-devised ones. Mathematicians are “still trying to figure out the best way to incorporate large language models into our research workflow in ways that harness their power while mitigating their drawbacks,” Tao says. “This certainly indicates one possible way forward.” ","Google DeepMind has used a large language model to crack a famous unsolved problem in pure mathematics . In a paper published in Nature today , the researchers say it is the first time a large language model has been used to discover a solution to a long-standing scientific puzzle—producing verifiable and valuable new information that did not previously exist . “ It ’ s not in the training data—it wasn ’ t even known , ” says coauthor Pushmeet Kohli , vice president of research at Google DeepMind . Large language models have a reputation for making things up , not for providing new facts . Google DeepMind ’ s new tool , called FunSearch , could change that . It shows that they can indeed make discoveries—if they are coaxed just so , and if you throw out the majority of what they come up with . FunSearch ( so called because it searches for mathematical functions , not because it ’ s fun ) continues a streak of discoveries in fundamental math and computer science that DeepMind has made using AI . First AlphaTensor found a way to speed up a calculation at the heart of many different kinds of code , beating a 50-year record . Then AlphaDev found ways to make key algorithms used trillions of times a day run faster . Yet those tools did not use large language models . Built on top of DeepMind ’ s game-playing AI AlphaZero , both solved math problems by treating them as if they were puzzles in Go or chess . The trouble is that they are stuck in their lanes , says Bernardino Romera-Paredes , a researcher at the company who worked on both AlphaTensor and FunSearch : “ AlphaTensor is great at matrix multiplication , but basically nothing else. ” FunSearch takes a different tack . It combines a large language model called Codey , a version of Google ’ s PaLM 2 that is fine-tuned on computer code , with other systems that reject incorrect or nonsensical answers and plug good ones back in . “ To be very honest with you , we have hypotheses , but we don ’ t know exactly why this works , ” says Alhussein Fawzi , a research scientist at Google DeepMind . “ In the beginning of the project , we didn ’ t know whether this would work at all. ” The researchers started by sketching out the problem they wanted to solve in Python , a popular programming language . But they left out the lines in the program that would specify how to solve it . That is where FunSearch comes in . It gets Codey to fill in the blanks—in effect , to suggest code that will solve the problem . A second algorithm then checks and scores what Codey comes up with . The best suggestions—even if not yet correct—are saved and given back to Codey , which tries to complete the program again . “ Many will be nonsensical , some will be sensible , and a few will be truly inspired , ” says Kohli . “ You take those truly inspired ones and you say , ‘ Okay , take these ones and repeat. ’ ” After a couple of million suggestions and a few dozen repetitions of the overall process—which took a few days—FunSearch was able to come up with code that produced a correct and previously unknown solution to the cap set problem , which involves finding the largest size of a certain type of set . Imagine plotting dots on graph paper . The cap set problem is like trying to figure out how many dots you can put down without three of them ever forming a straight line . It ’ s super niche , but important . Mathematicians do not even agree on how to solve it , let alone what the solution is . ( It is also connected to matrix multiplication , the computation that AlphaTensor found a way to speed up . ) Terence Tao at the University of California , Los Angeles , who has won many of the top awards in mathematics , including the Fields Medal , called the cap set problem “ perhaps my favorite open question ” in a 2007 blog post . Tao is intrigued by what FunSearch can do . “ This is a promising paradigm , ” he says . “ It is an interesting way to leverage the power of large language models. ” A key advantage that FunSearch has over AlphaTensor is that it can , in theory , be used to find solutions to a wide range of problems . That ’ s because it produces code—a recipe for generating the solution , rather than the solution itself . Different code will solve different problems . FunSearch ’ s results are also easier to understand . A recipe is often clearer than the weird mathematical solution it produces , says Fawzi . To test its versatility , the researchers used FunSearch to approach another hard problem in math : the bin packing problem , which involves trying to pack items into as few bins as possible . This is important for a range of applications in computer science , from data center management to e-commerce . FunSearch came up with a way to solve it that ’ s faster than human-devised ones . Mathematicians are “ still trying to figure out the best way to incorporate large language models into our research workflow in ways that harness their power while mitigating their drawbacks , ” Tao says . “ This certainly indicates one possible way forward . ”","['use', 'large', 'language', 'model', 'crack', 'famous', 'unsolved', 'problem', 'pure', 'mathematic', 'paper', 'publish', 'nature', 'today', 'researcher', 'say', 'first', 'time', 'large', 'language', 'model', 'use', 'discover', 'solution', 'longstanding', 'scientific', 'puzzle', 'produce', 'verifiable', 'valuable', 'new', 'information', 'previously', 'exist', 'training', 'datum', 'even', 'know', 'say', 'coauthor', 'vice', 'president', 'research', 'large', 'language', 'model', 'reputation', 'make', 'thing', 'provide', 'new', 'fact', 'new', 'tool', 'call', 'funsearch', 'change', 'show', 'indeed', 'make', 'discovery', 'coax', 'throw', 'majority', 'come', 'funsearch', 'call', 'search', 'mathematical', 'function', 'fun', 'continue', 'streak', 'discovery', 'fundamental', 'math', 'computer', 'science', 'deepmind', 'make', 'use', 'ai', 'first', 'alphatensor', 'find', 'way', 'speed', 'calculation', 'heart', 'many', 'different', 'kind', 'code', 'beat', '50year', 'record', 'alphadev', 'find', 'way', 'make', 'key', 'algorithm', 'use', 'trillion', 'time', 'day', 'run', 'fast', 'yet', 'tool', 'use', 'large', 'language', 'model', 'build', 'top', 'deepmind', 'gameplaye', 'ai', 'alphazero', 'solve', 'math', 'problem', 'treat', 'puzzle', 'go', 'chess', 'trouble', 'stick', 'lane', 'say', 'bernardino', 'romeraparede', 'researcher', 'company', 'work', 'alphatensor', 'funsearch', 'alphatensor', 'great', 'matrix', 'multiplication', 'basically', 'else', 'funsearch', 'take', 'different', 'tack', 'combine', 'large', 'language', 'model', 'call', 'codey', 'version', 'palm', 'finetune', 'computer', 'code', 'system', 'reject', 'incorrect', 'nonsensical', 'answer', 'plug', 'good', 'one', 'back', 'honest', 'hypothesis', 'know', 'exactly', 'work', 'say', 'fawzi', 'research', 'scientist', 'beginning', 'project', 'know', 'work', 'researcher', 'start', 'sketch', 'problem', 'want', 'solve', 'popular', 'programming', 'language', 'leave', 'line', 'program', 'specify', 'solve', 'funsearch', 'come', 'get', 'codey', 'fill', 'blank', 'effect', 'suggest', 'code', 'solve', 'problem', 'second', 'algorithm', 'check', 'score', 'codey', 'come', 'good', 'suggestion', 'even', 'yet', 'correct', 'save', 'give', 'back', 'try', 'complete', 'program', 'many', 'nonsensical', 'sensible', 'truly', 'inspire', 'say', 'take', 'truly', 'inspire', 'one', 'say', 'take', 'one', 'repeat', 'couple', 'suggestion', 'dozen', 'repetition', 'overall', 'process', 'take', 'day', 'funsearch', 'able', 'come', 'code', 'produce', 'correct', 'previously', 'unknown', 'solution', 'cap', 'set', 'problem', 'involve', 'find', 'large', 'size', 'certain', 'type', 'set', 'imagine', 'plot', 'dot', 'graph', 'paper', 'cap', 'set', 'problem', 'try', 'figure', 'many', 'dot', 'put', 'ever', 'form', 'straight', 'line', 'super', 'niche', 'important', 'mathematician', 'even', 'agree', 'solve', 'let', 'alone', 'solution', 'also', 'connect', 'matrix', 'multiplication', 'computation', 'alphatensor', 'find', 'way', 'speed', 'terence', 'win', 'many', 'top', 'award', 'mathematic', 'include', 'medal', 'call', 'cap', 'set', 'problem', 'perhaps', 'favorite', 'open', 'question', 'blog', 'post', 'intrigue', 'funsearch', 'promising', 'paradigm', 'say', 'interesting', 'way', 'leverage', 'power', 'large', 'language', 'model', 'key', 'advantage', 'funsearch', 'alphatensor', 'theory', 'use', 'find', 'solution', 'wide', 'range', 'problem', 'produce', 'code', 'recipe', 'generate', 'solution', 'rather', 'solution', 'different', 'code', 'solve', 'different', 'problem', 'funsearch', 'result', 'also', 'easy', 'understand', 'recipe', 'often', 'clear', 'weird', 'mathematical', 'solution', 'produce', 'say', 'fawzi', 'test', 'versatility', 'researcher', 'use', 'funsearch', 'approach', 'hard', 'problem', 'math', 'packing', 'problem', 'involve', 'try', 'pack', 'item', 'bin', 'possible', 'important', 'range', 'application', 'computer', 'science', 'datum', 'center', 'management', 'ecommerce', 'funsearch', 'come', 'way', 'solve', 'fast', 'humandevise', 'one', 'mathematician', 'still', 'try', 'figure', 'good', 'way', 'incorporate', 'large', 'language', 'model', 'research', 'workflow', 'way', 'harness', 'power', 'mitigate', 'drawback', 'say', 'certainly', 'indicate', 'possible', 'way', 'forward']"
This new system can teach a robot a simple household task within 20 minutes,https://www.technologyreview.com/2023/12/14/1085231/new-system-teach-robot-household-task/,2023-12-14,"<p><b>The Dobb-E domestic robotics system was trained in real people’s homes and could help solve the field’s data problem.</b></p>
","A new system that teaches robots a domestic task in around 20 minutes could help the field of robotics overcome one of its biggest challenges: a lack of training data.  The open-source system, called Dobb-E, was trained using data collected from real homes. It can help to teach a robot how to open an air fryer, close a door, or straighten a cushion, among other tasks.  While other types of AI, such as large language models, are trained on huge repositories of data scraped from the internet, the same can’t be done with robots, because the data needs to be physically collected. This makes it a lot harder to build and scale training databases.   Similarly, while it’s relatively easy to train robots to execute tasks inside a laboratory, these conditions don’t necessarily translate to the messy unpredictability of a real home.  To combat these problems, the team came up with a simple, easily replicable way to collect the data needed to train Dobb-E—using an iPhone attached to a reacher-grabber stick, the kind typically used to pick up trash. Then they set the iPhone to record videos of what was happening. Volunteers in 22 homes in New York completed certain tasks using the stick, including opening and closing doors and drawers, turning lights on and off, and placing tissues in the trash. The iPhones’ lidar systems, motion sensors, and gyroscopes were used to record data on movement, depth, and rotation—important information when it comes to training a robot to replicate the actions on its own. After they’d collected just 13 hours’ worth of recordings in total, the team used the data to train an AI model to instruct a robot in how to carry out the actions. The model used self-supervised learning techniques, which teach neural networks to spot patterns in data sets by themselves, without being guided by labeled examples. The next step involved testing how reliably a commercially available robot called Stretch, which consists of a wheeled unit, a tall pole, and a retractable arm, was able to use the AI system to execute the tasks. An iPhone held in a 3D-printed mount was attached to Stretch’s arm to replicate the setup on the stick. The researchers tested the robot in 10 homes in New York over 30 days, and it completed 109 household tasks with an overall success rate of 81%. Each task typically took Dobb-E around 20 minutes to learn: five minutes of demonstration from a human using the stick and attached iPhone, followed by 15 minutes of fine-tuning, when the system compared its previous training with the new demonstration.  Large language models combined with confidence scores help them recognize uncertainty. That could be key to making robots safe and trustworthy. Once the fine-tuning was complete, the robot was able to complete simple tasks like pouring from a cup, opening blinds and shower curtains, or pulling board-game boxes from a shelf. It could also perform multiple actions in quick succession, such as placing a can in a recycling bag and then lifting the bag.  However, not every task was successful. The system was confused by reflective surfaces like mirrors. Also, because the robot’s center of gravity is low, tasks that require pulling something heavy at height, like opening fridge doors, proved too risky to attempt.  The research represents tangible progress for the home robotics field, says Charlie C. Kemp, cofounder of the robotics firm Hello Robot and a former associate professor at Georgia Tech. Although the Dobb-E team used Hello Robot’s research robot, Kemp was not involved in the project. “The future of home robots is really coming. It’s not just some crazy dream anymore,” he says. “Scaling up data has always been a challenge in robotics, and this is a very creative, clever approach to that problem.” To date, Roomba and other robotic vacuum cleaners are the only real commercial home robot successes, says Jiajun Wu, an assistant professor of computer science at Stanford University who was not involved in the research. Their job is easier because Roombas don’t interact with objects—in fact, their aim is to avoid them. It’s much more challenging to develop home robots capable of doing a wider range of tasks, which is what this research could help advance.  The NYU research team has made all elements of the project open source, and they’re hoping others will download the code and help expand the range of tasks that robots running Dobb-E will be able to achieve. “Our hope is that when we get more and more data, at some point when Dobb-E sees a new home, you don’t have to show it more examples,” says Lerrel Pinto, a computer science researcher at New York University who worked on the project.  “We want to get to the point when we don’t have to teach the robot new tasks, because it already knows all the tasks in most houses,” he says. ","A new system that teaches robots a domestic task in around 20 minutes could help the field of robotics overcome one of its biggest challenges : a lack of training data . The open-source system , called Dobb-E , was trained using data collected from real homes . It can help to teach a robot how to open an air fryer , close a door , or straighten a cushion , among other tasks . While other types of AI , such as large language models , are trained on huge repositories of data scraped from the internet , the same can ’ t be done with robots , because the data needs to be physically collected . This makes it a lot harder to build and scale training databases . Similarly , while it ’ s relatively easy to train robots to execute tasks inside a laboratory , these conditions don ’ t necessarily translate to the messy unpredictability of a real home . To combat these problems , the team came up with a simple , easily replicable way to collect the data needed to train Dobb-E—using an iPhone attached to a reacher-grabber stick , the kind typically used to pick up trash . Then they set the iPhone to record videos of what was happening . Volunteers in 22 homes in New York completed certain tasks using the stick , including opening and closing doors and drawers , turning lights on and off , and placing tissues in the trash . The iPhones ’ lidar systems , motion sensors , and gyroscopes were used to record data on movement , depth , and rotation—important information when it comes to training a robot to replicate the actions on its own . After they ’ d collected just 13 hours ’ worth of recordings in total , the team used the data to train an AI model to instruct a robot in how to carry out the actions . The model used self-supervised learning techniques , which teach neural networks to spot patterns in data sets by themselves , without being guided by labeled examples . The next step involved testing how reliably a commercially available robot called Stretch , which consists of a wheeled unit , a tall pole , and a retractable arm , was able to use the AI system to execute the tasks . An iPhone held in a 3D-printed mount was attached to Stretch ’ s arm to replicate the setup on the stick . The researchers tested the robot in 10 homes in New York over 30 days , and it completed 109 household tasks with an overall success rate of 81 % . Each task typically took Dobb-E around 20 minutes to learn : five minutes of demonstration from a human using the stick and attached iPhone , followed by 15 minutes of fine-tuning , when the system compared its previous training with the new demonstration . Large language models combined with confidence scores help them recognize uncertainty . That could be key to making robots safe and trustworthy . Once the fine-tuning was complete , the robot was able to complete simple tasks like pouring from a cup , opening blinds and shower curtains , or pulling board-game boxes from a shelf . It could also perform multiple actions in quick succession , such as placing a can in a recycling bag and then lifting the bag . However , not every task was successful . The system was confused by reflective surfaces like mirrors . Also , because the robot ’ s center of gravity is low , tasks that require pulling something heavy at height , like opening fridge doors , proved too risky to attempt . The research represents tangible progress for the home robotics field , says Charlie C. Kemp , cofounder of the robotics firm Hello Robot and a former associate professor at Georgia Tech . Although the Dobb-E team used Hello Robot ’ s research robot , Kemp was not involved in the project . “ The future of home robots is really coming . It ’ s not just some crazy dream anymore , ” he says . “ Scaling up data has always been a challenge in robotics , and this is a very creative , clever approach to that problem. ” To date , Roomba and other robotic vacuum cleaners are the only real commercial home robot successes , says Jiajun Wu , an assistant professor of computer science at Stanford University who was not involved in the research . Their job is easier because Roombas don ’ t interact with objects—in fact , their aim is to avoid them . It ’ s much more challenging to develop home robots capable of doing a wider range of tasks , which is what this research could help advance . The NYU research team has made all elements of the project open source , and they ’ re hoping others will download the code and help expand the range of tasks that robots running Dobb-E will be able to achieve . “ Our hope is that when we get more and more data , at some point when Dobb-E sees a new home , you don ’ t have to show it more examples , ” says Lerrel Pinto , a computer science researcher at New York University who worked on the project . “ We want to get to the point when we don ’ t have to teach the robot new tasks , because it already knows all the tasks in most houses , ” he says .","['new', 'system', 'teach', 'robot', 'domestic', 'task', 'around', 'minute', 'help', 'field', 'robotic', 'overcome', 'big', 'challenge', 'lack', 'training', 'datum', 'opensource', 'system', 'call', 'dobbe', 'train', 'use', 'datum', 'collect', 'real', 'home', 'help', 'teach', 'robot', 'open', 'air', 'fryer', 'close', 'door', 'straighten', 'cushion', 'task', 'type', 'ai', 'large', 'language', 'model', 'train', 'huge', 'repository', 'datum', 'scrape', 'internet', 'robot', 'datum', 'need', 'physically', 'collect', 'make', 'lot', 'hard', 'build', 'scale', 'training', 'database', 'similarly', 'relatively', 'easy', 'train', 'robot', 'execute', 'task', 'laboratory', 'condition', 'necessarily', 'translate', 'messy', 'unpredictability', 'real', 'home', 'combat', 'problem', 'team', 'come', 'simple', 'easily', 'replicable', 'way', 'collect', 'datum', 'need', 'train', 'dobbe', 'use', 'iphone', 'attach', 'reachergrabber', 'stick', 'kind', 'typically', 'use', 'pick', 'trash', 'set', 'iphone', 'record', 'video', 'happen', 'volunteer', 'home', 'complete', 'certain', 'task', 'use', 'stick', 'include', 'opening', 'closing', 'door', 'drawer', 'turn', 'light', 'place', 'tissue', 'trash', 'iphone', 'lidar', 'system', 'motion', 'sensor', 'gyroscope', 'use', 'record', 'datum', 'movement', 'depth', 'rotation', 'important', 'information', 'come', 'train', 'robot', 'replicate', 'action', 'collect', 'hour', 'worth', 'recording', 'total', 'team', 'use', 'datum', 'train', 'ai', 'model', 'instruct', 'robot', 'carry', 'action', 'model', 'use', 'selfsupervise', 'learn', 'technique', 'teach', 'neural', 'network', 'spot', 'pattern', 'data', 'set', 'guide', 'label', 'example', 'next', 'step', 'involve', 'test', 'reliably', 'commercially', 'available', 'robot', 'call', 'stretch', 'consist', 'wheeled', 'unit', 'tall', 'pole', 'retractable', 'arm', 'able', 'use', 'system', 'execute', 'task', 'iphone', 'hold', 'mount', 'attach', 'stretch', 'arm', 'replicate', 'setup', 'stick', 'researcher', 'test', 'robot', 'home', 'day', 'complete', 'household', 'task', 'overall', 'success', 'rate', 'task', 'typically', 'take', 'dobbe', 'minute', 'learn', 'minute', 'demonstration', 'human', 'use', 'stick', 'attach', 'iphone', 'follow', 'minute', 'finetune', 'system', 'compare', 'previous', 'training', 'new', 'demonstration', 'large', 'language', 'model', 'combine', 'confidence', 'score', 'help', 'recognize', 'uncertainty', 'key', 'make', 'robot', 'safe', 'trustworthy', 'finetuning', 'complete', 'robot', 'able', 'complete', 'simple', 'task', 'pour', 'cup', 'opening', 'blind', 'shower', 'curtain', 'pull', 'boardgame', 'box', 'shelf', 'also', 'perform', 'multiple', 'action', 'quick', 'succession', 'place', 'recycling', 'bag', 'lift', 'bag', 'however', 'task', 'successful', 'system', 'confuse', 'reflective', 'surface', 'mirror', 'also', 'robot', 'center', 'gravity', 'low', 'task', 'require', 'pull', 'heavy', 'height', 'open', 'fridge', 'door', 'prove', 'risky', 'attempt', 'research', 'represent', 'tangible', 'progress', 'home', 'robotic', 'field', 'say', 'cofounder', 'robotic', 'firm', 'robot', 'former', 'associate', 'professor', 'dobbe', 'team', 'use', 'research', 'robot', 'involve', 'project', 'future', 'home', 'robot', 'really', 'come', 'crazy', 'dream', 'anymore', 'say', 'scale', 'datum', 'always', 'challenge', 'robotic', 'creative', 'clever', 'approach', 'problem', 'date', 'roomba', 'robotic', 'vacuum', 'cleaner', 'real', 'commercial', 'home', 'robot', 'success', 'say', 'assistant', 'professor', 'computer', 'science', 'involve', 'research', 'job', 'easy', 'interact', 'object', 'fact', 'aim', 'avoid', 'much', 'challenging', 'develop', 'home', 'robot', 'capable', 'wide', 'range', 'task', 'research', 'help', 'advance', 'research', 'team', 'make', 'element', 'project', 'open', 'source', 'hope', 'download', 'code', 'help', 'expand', 'range', 'task', 'robot', 'run', 'dobbe', 'able', 'achieve', 'hope', 'get', 'datum', 'point', 'dobbe', 'see', 'new', 'home', 'show', 'example', 'say', 'computer', 'science', 'researcher', 'work', 'project', 'want', 'get', 'point', 'teach', 'robot', 'new', 'task', 'already', 'know', 'task', 'house', 'say']"
Five things you need to know about the EU’s new AI Act,https://www.technologyreview.com/2023/12/11/1084942/five-things-you-need-to-know-about-the-eus-new-ai-act/,2023-12-11,"<p>The EU is poised to effectively become the world’s AI police, creating binding rules on transparency, ethics, and more.</p>
","This story is from The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here. It’s done. It’s over. Two and a half years after it was first introduced—after months of lobbying and political arm-wrestling, plus grueling final negotiations that took nearly 40 hours—EU lawmakers have reached a deal over the AI Act. It will be the world’s first sweeping AI law.  The AI Act was conceived as a landmark bill that would mitigate harm in areas where using AI poses the biggest risk to fundamental rights, such as health care, education, border surveillance, and public services, as well as banning uses that pose an “unacceptable risk.”  “High risk” AI systems will have to adhere to strict rules that require risk-mitigation systems, high-quality data sets, better documentation, and human oversight, for example. The vast majority of AI uses, such as recommender systems and spam filters, will get a free pass.  The AI Act is a major deal in that it will introduce important rules and enforcement mechanisms to a hugely influential sector that is currently a Wild West.  Tech companies love to talk about how committed they are to AI ethics. But when it comes to concrete measures, the conversation dries up. And anyway, actions speak louder than words. Responsible AI teams are often the first to see cuts during layoffs, and in truth, tech companies can decide to change their AI ethics policies at any time. OpenAI, for example, started off as an “open” AI research lab before closing up public access to its research to protect its competitive advantage, just like every other AI startup.  The AI Act will change that. The regulation imposes legally binding rules requiring tech companies to notify people when they are interacting with a chatbot or with biometric categorization or emotion recognition systems. It’ll also require them to label deepfakes and AI-generated content, and design systems in such a way that AI-generated media can be detected. This is a step beyond the voluntary commitments that leading AI companies made to the White House to simply develop AI provenance tools, such as watermarking.  The bill will also require all organizations that offer essential services, such as insurance and banking, to conduct an impact assessment on how using AI systems will affect people’s fundamental rights.  When the AI Act was first introduced, in 2021, people were still talking about the metaverse. (Can you imagine!)  Fast-forward to now, and in a post-ChatGPT world, lawmakers felt they had to take so-called foundation models—powerful AI models that can be used for many different purposes—into account in the regulation. This sparked intense debate over what sorts of models should be regulated, and whether regulation would kill innovation.  The AI Act will require foundation models and AI systems built on top of them to draw up better documentation, comply with EU copyright law, and share more information about what data the model was trained on. For the most powerful models, there are extra requirements. Tech companies will have to share how secure and energy efficient their AI models are, for example.  But here’s the catch: The compromise lawmakers found was to apply a stricter set of rules to only the most powerful AI models, as categorized by the computing power needed to train them. And it will be up to companies to assess whether they fall under stricter rules.  A European Commission official would not confirm whether the current cutoff would capture powerful models such as OpenAI’s GPT-4 or Google’s Gemini, because only the companies themselves know how much computing power was used to train their models. The official did say that as the technology develops, the EU could change the way it measures how powerful AI models are.  The AI Act will set up a new European AI Office to coordinate compliance, implementation, and enforcement. It will be the first body globally to enforce binding rules on AI, and the EU hopes this will help it become the world’s go-to tech regulator. The AI Act’s governance mechanism also includes a scientific panel of independent experts to offer guidance on the systemic risks AI poses, and how to classify and test models.  The fines for noncompliance are steep: from 1.5% to 7% of a firm’s global sales turnover, depending on the severity of the offense and size of the company.  Europe will also become the one of the first places in the world where citizens will be able to launch complaints about AI systems and receive explanations about how AI systems came to the conclusions that affect them.  By becoming the first to formalize rules around AI, the EU retains its first-mover advantage. Much like the GDPR, the AI Act could become a global standard. Companies elsewhere that want to do business in the world’s second-largest economy will have to comply with the law. The EU’s rules also go a step further than ones introduced by the US, such as the White House executive order, because they are binding.  Some AI uses are now completely banned in the EU: biometric categorization systems that use sensitive characteristics; untargeted scraping of facial images from the internet or CCTV footage to create facial recognition databases like Clearview AI; emotion recognition at work or in schools; social scoring; AI systems that manipulate human behavior; and AI that is used to exploit people’s vulnerabilities.  Predictive policing is also banned, unless it is used with “clear human assessment and objective facts, which basically do not simply leave the decision of going after a certain individual in a criminal investigation only because an algorithm says so,” according to an EU Commission official. However, the AI Act does not apply to AI systems that have been developed exclusively for military and defense uses.  One of the bloodiest fights over the AI Act has always been how to regulate police use of biometric systems in public places, which many fear could lead to mass surveillance. While the European Parliament pushed for a near-total ban on the technology, some EU countries, such as France, have resisted this fiercely. They want to use it to fight crime and terrorism.  European police forces will only be able to use biometric identification systems in public places if they get court approval first, and only for 16 different specific crimes, such as terrorism, human trafficking, sexual exploitation of children, and drug trafficking. Law enforcement authorities may also use high-risk AI systems that don’t pass European standards in “exceptional circumstances relating to public security.”  It might take weeks or even months before we see the final wording of the bill. The text still needs to go through technical tinkering, and has to be approved by European countries and the EU Parliament before it officially enters into law.  Once it is in force, tech companies have two years to implement the rules. The bans on AI uses will apply after six months, and companies developing foundation models will have to comply with the law within one year.  ","This story is from The Algorithm , our weekly newsletter on AI . To get stories like this in your inbox first , sign up here . It ’ s done . It ’ s over . Two and a half years after it was first introduced—after months of lobbying and political arm-wrestling , plus grueling final negotiations that took nearly 40 hours—EU lawmakers have reached a deal over the AI Act . It will be the world ’ s first sweeping AI law . The AI Act was conceived as a landmark bill that would mitigate harm in areas where using AI poses the biggest risk to fundamental rights , such as health care , education , border surveillance , and public services , as well as banning uses that pose an “ unacceptable risk. ” “ High risk ” AI systems will have to adhere to strict rules that require risk-mitigation systems , high-quality data sets , better documentation , and human oversight , for example . The vast majority of AI uses , such as recommender systems and spam filters , will get a free pass . The AI Act is a major deal in that it will introduce important rules and enforcement mechanisms to a hugely influential sector that is currently a Wild West . Tech companies love to talk about how committed they are to AI ethics . But when it comes to concrete measures , the conversation dries up . And anyway , actions speak louder than words . Responsible AI teams are often the first to see cuts during layoffs , and in truth , tech companies can decide to change their AI ethics policies at any time . OpenAI , for example , started off as an “ open ” AI research lab before closing up public access to its research to protect its competitive advantage , just like every other AI startup . The AI Act will change that . The regulation imposes legally binding rules requiring tech companies to notify people when they are interacting with a chatbot or with biometric categorization or emotion recognition systems . It ’ ll also require them to label deepfakes and AI-generated content , and design systems in such a way that AI-generated media can be detected . This is a step beyond the voluntary commitments that leading AI companies made to the White House to simply develop AI provenance tools , such as watermarking . The bill will also require all organizations that offer essential services , such as insurance and banking , to conduct an impact assessment on how using AI systems will affect people ’ s fundamental rights . When the AI Act was first introduced , in 2021 , people were still talking about the metaverse . ( Can you imagine ! ) Fast-forward to now , and in a post-ChatGPT world , lawmakers felt they had to take so-called foundation models—powerful AI models that can be used for many different purposes—into account in the regulation . This sparked intense debate over what sorts of models should be regulated , and whether regulation would kill innovation . The AI Act will require foundation models and AI systems built on top of them to draw up better documentation , comply with EU copyright law , and share more information about what data the model was trained on . For the most powerful models , there are extra requirements . Tech companies will have to share how secure and energy efficient their AI models are , for example . But here ’ s the catch : The compromise lawmakers found was to apply a stricter set of rules to only the most powerful AI models , as categorized by the computing power needed to train them . And it will be up to companies to assess whether they fall under stricter rules . A European Commission official would not confirm whether the current cutoff would capture powerful models such as OpenAI ’ s GPT-4 or Google ’ s Gemini , because only the companies themselves know how much computing power was used to train their models . The official did say that as the technology develops , the EU could change the way it measures how powerful AI models are . The AI Act will set up a new European AI Office to coordinate compliance , implementation , and enforcement . It will be the first body globally to enforce binding rules on AI , and the EU hopes this will help it become the world ’ s go-to tech regulator . The AI Act ’ s governance mechanism also includes a scientific panel of independent experts to offer guidance on the systemic risks AI poses , and how to classify and test models . The fines for noncompliance are steep : from 1.5 % to 7 % of a firm ’ s global sales turnover , depending on the severity of the offense and size of the company . Europe will also become the one of the first places in the world where citizens will be able to launch complaints about AI systems and receive explanations about how AI systems came to the conclusions that affect them . By becoming the first to formalize rules around AI , the EU retains its first-mover advantage . Much like the GDPR , the AI Act could become a global standard . Companies elsewhere that want to do business in the world ’ s second-largest economy will have to comply with the law . The EU ’ s rules also go a step further than ones introduced by the US , such as the White House executive order , because they are binding . Some AI uses are now completely banned in the EU : biometric categorization systems that use sensitive characteristics ; untargeted scraping of facial images from the internet or CCTV footage to create facial recognition databases like Clearview AI ; emotion recognition at work or in schools ; social scoring ; AI systems that manipulate human behavior ; and AI that is used to exploit people ’ s vulnerabilities . Predictive policing is also banned , unless it is used with “ clear human assessment and objective facts , which basically do not simply leave the decision of going after a certain individual in a criminal investigation only because an algorithm says so , ” according to an EU Commission official . However , the AI Act does not apply to AI systems that have been developed exclusively for military and defense uses . One of the bloodiest fights over the AI Act has always been how to regulate police use of biometric systems in public places , which many fear could lead to mass surveillance . While the European Parliament pushed for a near-total ban on the technology , some EU countries , such as France , have resisted this fiercely . They want to use it to fight crime and terrorism . European police forces will only be able to use biometric identification systems in public places if they get court approval first , and only for 16 different specific crimes , such as terrorism , human trafficking , sexual exploitation of children , and drug trafficking . Law enforcement authorities may also use high-risk AI systems that don ’ t pass European standards in “ exceptional circumstances relating to public security. ” It might take weeks or even months before we see the final wording of the bill . The text still needs to go through technical tinkering , and has to be approved by European countries and the EU Parliament before it officially enters into law . Once it is in force , tech companies have two years to implement the rules . The bans on AI uses will apply after six months , and companies developing foundation models will have to comply with the law within one year .","['story', 'weekly', 'newsletter', 'ai', 'get', 'story', 'inbox', 'first', 'sign', 'half', 'year', 'first', 'introduce', 'month', 'lobbying', 'political', 'armwrestling', 'grueling', 'final', 'negotiation', 'take', 'nearly', 'hour', 'lawmaker', 'reach', 'deal', 'act', 'world', 'first', 'sweeping', 'law', 'conceive', 'landmark', 'bill', 'mitigate', 'harm', 'area', 'use', 'ai', 'pose', 'big', 'risk', 'fundamental', 'right', 'health', 'care', 'education', 'border', 'surveillance', 'public', 'service', 'well', 'ban', 'use', 'pose', 'unacceptable', 'risk', 'high', 'risk', 'ai', 'system', 'adhere', 'strict', 'rule', 'require', 'riskmitigation', 'system', 'highquality', 'datum', 'set', 'well', 'documentation', 'human', 'oversight', 'example', 'vast', 'majority', 'use', 'recommender', 'system', 'spam', 'filter', 'get', 'free', 'pass', 'major', 'deal', 'introduce', 'important', 'rule', 'enforcement', 'mechanism', 'hugely', 'influential', 'sector', 'currently', 'wild', 'west', 'tech', 'company', 'love', 'talk', 'committed', 'ai', 'ethic', 'come', 'concrete', 'measure', 'conversation', 'dry', 'anyway', 'action', 'speak', 'louder', 'word', 'responsible', 'team', 'often', 'first', 'see', 'cut', 'layoff', 'truth', 'tech', 'company', 'decide', 'change', 'ethic', 'policy', 'time', 'openai', 'example', 'start', 'open', 'ai', 'research', 'lab', 'close', 'public', 'access', 'research', 'protect', 'competitive', 'advantage', 'startup', 'change', 'regulation', 'impose', 'legally', 'bind', 'rule', 'require', 'tech', 'company', 'notify', 'people', 'interact', 'chatbot', 'biometric', 'categorization', 'emotion', 'recognition', 'system', 'also', 'require', 'label', 'deepfake', 'aigenerate', 'content', 'design', 'system', 'way', 'aigenerate', 'medium', 'detect', 'step', 'voluntary', 'commitment', 'lead', 'ai', 'company', 'make', 'simply', 'develop', 'ai', 'provenance', 'tool', 'watermarke', 'bill', 'also', 'require', 'organization', 'offer', 'essential', 'service', 'insurance', 'banking', 'conduct', 'impact', 'assessment', 'use', 'system', 'affect', 'people', 'fundamental', 'right', 'first', 'introduce', 'people', 'still', 'talk', 'metaverse', 'imagine', 'fastforward', 'postchatgpt', 'world', 'lawmaker', 'feel', 'take', 'socalled', 'foundation', 'model', 'powerful', 'ai', 'model', 'use', 'many', 'different', 'purpose', 'account', 'regulation', 'spark', 'intense', 'debate', 'sort', 'model', 'regulate', 'regulation', 'kill', 'innovation', 'require', 'foundation', 'model', 'ai', 'system', 'build', 'top', 'draw', 'well', 'documentation', 'comply', 'copyright', 'law', 'share', 'information', 'datum', 'model', 'train', 'powerful', 'model', 'extra', 'requirement', 'tech', 'company', 'share', 'secure', 'energy', 'efficient', 'model', 'example', 'catch', 'compromise', 'lawmaker', 'find', 'apply', 'strict', 'set', 'rule', 'powerful', 'model', 'categorize', 'computing', 'power', 'need', 'train', 'company', 'assess', 'fall', 'strict', 'rule', 'official', 'confirm', 'current', 'cutoff', 'capture', 'powerful', 'model', 'gpt4', 'gemini', 'company', 'know', 'much', 'computing', 'power', 'use', 'train', 'model', 'official', 'say', 'technology', 'develop', 'change', 'way', 'measure', 'powerful', 'model', 'set', 'new', 'european', 'ai', 'office', 'coordinate', 'compliance', 'implementation', 'enforcement', 'first', 'body', 'globally', 'enforce', 'bind', 'rule', 'ai', 'hope', 'help', 'become', 'world', 'goto', 'tech', 'regulator', 'governance', 'mechanism', 'also', 'include', 'scientific', 'panel', 'independent', 'expert', 'offer', 'guidance', 'systemic', 'risk', 'ai', 'pose', 'classify', 'test', 'model', 'fine', 'noncompliance', 'steep', 'firm', 'global', 'sale', 'turnover', 'depend', 'severity', 'offense', 'size', 'company', 'also', 'become', 'first', 'place', 'world', 'citizen', 'able', 'launch', 'complaint', 'ai', 'system', 'receive', 'explanation', 'system', 'come', 'conclusion', 'affect', 'become', 'first', 'formalize', 'rule', 'around', 'ai', 'retain', 'firstmover', 'advantage', 'much', 'gdpr', 'become', 'global', 'standard', 'company', 'elsewhere', 'want', 'business', 'world', 'secondlarg', 'economy', 'comply', 'law', 'rule', 'also', 'go', 'step', 'far', 'one', 'introduce', 'order', 'bind', 'ai', 'use', 'completely', 'ban', 'biometric', 'categorization', 'system', 'use', 'sensitive', 'characteristic', 'untargete', 'scraping', 'facial', 'image', 'internet', 'cctv', 'footage', 'create', 'facial', 'recognition', 'database', 'clearview', 'ai', 'emotion', 'recognition', 'work', 'school', 'social', 'scoring', 'ai', 'system', 'manipulate', 'human', 'behavior', 'ai', 'use', 'exploit', 'people', 'vulnerability', 'predictive', 'policing', 'also', 'ban', 'use', 'clear', 'human', 'assessment', 'objective', 'fact', 'basically', 'simply', 'leave', 'decision', 'go', 'certain', 'individual', 'criminal', 'investigation', 'say', 'accord', 'official', 'however', 'apply', 'ai', 'system', 'develop', 'exclusively', 'military', 'defense', 'use', 'bloody', 'fight', 'always', 'regulate', 'police', 'use', 'biometric', 'system', 'public', 'place', 'many', 'fear', 'lead', 'mass', 'surveillance', 'european', 'push', 'neartotal', 'ban', 'technology', 'country', 'resist', 'fiercely', 'want', 'use', 'fight', 'crime', 'terrorism', 'european', 'police', 'force', 'able', 'use', 'biometric', 'identification', 'system', 'public', 'place', 'get', 'court', 'approval', 'first', 'different', 'specific', 'crime', 'terrorism', 'human', 'trafficking', 'sexual', 'exploitation', 'child', 'drug', 'trafficking', 'law', 'enforcement', 'authority', 'also', 'use', 'highrisk', 'system', 'pass', 'european', 'standard', 'exceptional', 'circumstance', 'relate', 'public', 'security', 'take', 'week', 'even', 'month', 'see', 'final', 'wording', 'bill', 'text', 'still', 'need', 'go', 'technical', 'tinkering', 'approve', 'european', 'country', 'parliament', 'officially', 'enter', 'law', 'force', 'tech', 'company', 'year', 'implement', 'rule', 'ban', 'use', 'apply', 'month', 'company', 'develop', 'foundation', 'model', 'comply', 'law', 'year']"
These robots know when to ask for help,https://www.technologyreview.com/2023/12/08/1084672/these-robots-know-when-to-ask-for-help/,2023-12-08,"<p>Large language models combined with confidence scores help them recognize uncertainty. That could be key to making robots safe and trustworthy.</p>
","There are two bowls on the kitchen table: one made of plastic, the other metal. You ask the robot to pick up the bowl and put it in the microwave. Which one will it choose? A human might ask for clarification, but given the vague command, the robot may place the metal bowl in the microwave, causing sparks to fly. A new training model, dubbed “KnowNo,” aims to address this problem by teaching robots to ask for our help when orders are unclear. At the same time, it ensures they seek clarification only when necessary, minimizing needless back-and-forth. The result is a smart assistant that tries to make sure it understands what you want without bothering you too much. Andy Zeng, a research scientist at Google DeepMind who helped develop the new technique, says that while robots can be powerful in many specific scenarios, they are often bad at generalized tasks that require common sense. For example, when asked to bring you a Coke, the robot needs to first understand that it needs to go into the kitchen, look for the refrigerator, and open the fridge door. Conventionally, these smaller substeps had to be manually programmed, because otherwise the robot would not know that people usually keep their drinks in the kitchen. That’s something large language models (LLMs) could help to fix, because they have a lot of common-sense knowledge baked in, says Zeng.  Now when the robot is asked to bring a Coke, an LLM, which has a generalized understanding of the world, can generate a step-by-step guide for the robot to follow. The problem with LLMs, though, is that there’s no way to guarantee that their instructions are possible for the robot to execute. Maybe the person doesn’t have a refrigerator in the kitchen, or the fridge door handle is broken. In these situations, robots need to ask humans for help. KnowNo makes that possible by combining large language models with statistical tools that quantify confidence levels.  When given an ambiguous instruction like “Put the bowl in the microwave,” KnowNo first generates multiple possible next actions using the language model. Then it creates a confidence score predicting the likelihood that each potential choice is the best one. These confidence estimates are sized up against a predetermined certainty threshold, which indicates exactly how confident or conservative the user wants a robot to be in its actions. For example, a robot with a success rate of 80% should make the correct decision at least 80% of the time. Large language models are the next big thing for robotics, making cars and other robots quicker to train and easier to control (if you trust them). This is useful in situations with varying degrees of risk, says Anirudha Majumdar, an assistant professor of mechanical and aerospace engineering at Princeton and the senior author of the study.  You may want your cleaning robot to be more independent, despite a few mistakes here and there, so that you don’t have to supervise it too closely. But for medical applications, robots must be extremely cautious, with the highest level of success possible. When there is more than one option for how to proceed, the robot pauses to ask for clarification instead of blindly continuing: “Which bowl should I pick up—the metal or the plastic one?” KnownNo was tested on three robots in more than 150 different scenarios. Results showed that KnowNo-trained robots had more consistent success rates while needing less human assistance than those trained without the same statistical calculations. The paper describing the research was presented at the Conference on Robot Learning in November. Because human language is often ambiguous, teaching robots to recognize and respond to uncertainty can improve their performance. Studies show that people prefer robots that ask questions, says Dylan Losey, an assistant professor at Virginia Tech who specializes in human-robot interaction and was not involved in this research. When robots reach out for help, it increases transparency about how they’re deciding what to do, which leads to better interactions, he says.  Allen Ren, a PhD student at Princeton and the study’s lead author, says there are several ways to improve KnowNo. Right now, it assumes robots’ vision is always reliable, which may not be the case with faulty sensors. Also, the model can be updated to factor in potential errors coming from human help. AI’s ability to express uncertainty will make us trust robots more, says Majumdar. “Quantifying uncertainty is a missing piece in a lot of our systems,” he says. “It allows us to be more confident about how safe and successful the robots will be.” ","There are two bowls on the kitchen table : one made of plastic , the other metal . You ask the robot to pick up the bowl and put it in the microwave . Which one will it choose ? A human might ask for clarification , but given the vague command , the robot may place the metal bowl in the microwave , causing sparks to fly . A new training model , dubbed “ KnowNo , ” aims to address this problem by teaching robots to ask for our help when orders are unclear . At the same time , it ensures they seek clarification only when necessary , minimizing needless back-and-forth . The result is a smart assistant that tries to make sure it understands what you want without bothering you too much . Andy Zeng , a research scientist at Google DeepMind who helped develop the new technique , says that while robots can be powerful in many specific scenarios , they are often bad at generalized tasks that require common sense . For example , when asked to bring you a Coke , the robot needs to first understand that it needs to go into the kitchen , look for the refrigerator , and open the fridge door . Conventionally , these smaller substeps had to be manually programmed , because otherwise the robot would not know that people usually keep their drinks in the kitchen . That ’ s something large language models ( LLMs ) could help to fix , because they have a lot of common-sense knowledge baked in , says Zeng . Now when the robot is asked to bring a Coke , an LLM , which has a generalized understanding of the world , can generate a step-by-step guide for the robot to follow . The problem with LLMs , though , is that there ’ s no way to guarantee that their instructions are possible for the robot to execute . Maybe the person doesn ’ t have a refrigerator in the kitchen , or the fridge door handle is broken . In these situations , robots need to ask humans for help . KnowNo makes that possible by combining large language models with statistical tools that quantify confidence levels . When given an ambiguous instruction like “ Put the bowl in the microwave , ” KnowNo first generates multiple possible next actions using the language model . Then it creates a confidence score predicting the likelihood that each potential choice is the best one . These confidence estimates are sized up against a predetermined certainty threshold , which indicates exactly how confident or conservative the user wants a robot to be in its actions . For example , a robot with a success rate of 80 % should make the correct decision at least 80 % of the time . Large language models are the next big thing for robotics , making cars and other robots quicker to train and easier to control ( if you trust them ) . This is useful in situations with varying degrees of risk , says Anirudha Majumdar , an assistant professor of mechanical and aerospace engineering at Princeton and the senior author of the study . You may want your cleaning robot to be more independent , despite a few mistakes here and there , so that you don ’ t have to supervise it too closely . But for medical applications , robots must be extremely cautious , with the highest level of success possible . When there is more than one option for how to proceed , the robot pauses to ask for clarification instead of blindly continuing : “ Which bowl should I pick up—the metal or the plastic one ? ” KnownNo was tested on three robots in more than 150 different scenarios . Results showed that KnowNo-trained robots had more consistent success rates while needing less human assistance than those trained without the same statistical calculations . The paper describing the research was presented at the Conference on Robot Learning in November . Because human language is often ambiguous , teaching robots to recognize and respond to uncertainty can improve their performance . Studies show that people prefer robots that ask questions , says Dylan Losey , an assistant professor at Virginia Tech who specializes in human-robot interaction and was not involved in this research . When robots reach out for help , it increases transparency about how they ’ re deciding what to do , which leads to better interactions , he says . Allen Ren , a PhD student at Princeton and the study ’ s lead author , says there are several ways to improve KnowNo . Right now , it assumes robots ’ vision is always reliable , which may not be the case with faulty sensors . Also , the model can be updated to factor in potential errors coming from human help . AI ’ s ability to express uncertainty will make us trust robots more , says Majumdar . “ Quantifying uncertainty is a missing piece in a lot of our systems , ” he says . “ It allows us to be more confident about how safe and successful the robots will be . ”","['bowl', 'kitchen', 'table', 'make', 'plastic', 'metal', 'ask', 'robot', 'pick', 'bowl', 'put', 'microwave', 'one', 'choose', 'human', 'ask', 'clarification', 'give', 'vague', 'command', 'robot', 'place', 'metal', 'bowl', 'microwave', 'cause', 'spark', 'fly', 'new', 'training', 'model', 'dub', 'aim', 'address', 'problem', 'teach', 'robot', 'ask', 'help', 'order', 'unclear', 'time', 'ensure', 'seek', 'clarification', 'necessary', 'minimize', 'needless', 'backandforth', 'result', 'smart', 'assistant', 'try', 'make', 'sure', 'understand', 'want', 'bother', 'much', 'research', 'scientist', 'help', 'develop', 'new', 'technique', 'say', 'robot', 'powerful', 'many', 'specific', 'scenario', 'often', 'bad', 'generalize', 'task', 'require', 'common', 'sense', 'example', 'ask', 'bring', 'coke', 'robot', 'need', 'first', 'understand', 'need', 'go', 'kitchen', 'look', 'refrigerator', 'open', 'fridge', 'door', 'conventionally', 'small', 'substep', 'manually', 'program', 'otherwise', 'robot', 'know', 'people', 'usually', 'keep', 'drink', 'kitchen', 'large', 'language', 'model', 'llm', 'help', 'fix', 'lot', 'commonsense', 'knowledge', 'bake', 'say', 'robot', 'ask', 'bring', 'coke', 'llm', 'generalized', 'understanding', 'world', 'generate', 'stepbystep', 'guide', 'robot', 'follow', 'problem', 'llm', 'way', 'guarantee', 'instruction', 'possible', 'robot', 'execute', 'maybe', 'person', 'refrigerator', 'kitchen', 'fridge', 'door', 'handle', 'break', 'situation', 'robot', 'need', 'ask', 'human', 'help', 'make', 'possible', 'combine', 'large', 'language', 'model', 'statistical', 'tool', 'quantify', 'confidence', 'level', 'give', 'ambiguous', 'instruction', 'put', 'bowl', 'microwave', 'first', 'generate', 'multiple', 'possible', 'next', 'action', 'use', 'language', 'model', 'create', 'confidence', 'score', 'predict', 'likelihood', 'potential', 'choice', 'good', 'confidence', 'estimate', 'sized', 'predetermine', 'certainty', 'threshold', 'indicate', 'exactly', 'confident', 'conservative', 'user', 'want', 'robot', 'action', 'example', 'robot', 'success', 'rate', 'make', 'correct', 'decision', 'least', 'time', 'large', 'language', 'model', 'next', 'big', 'thing', 'robotic', 'make', 'car', 'robot', 'quick', 'train', 'easy', 'control', 'trust', 'useful', 'situation', 'vary', 'degree', 'risk', 'say', 'anirudha', 'majumdar', 'assistant', 'professor', 'mechanical', 'aerospace', 'engineering', 'senior', 'author', 'study', 'want', 'cleaning', 'robot', 'independent', 'mistake', 'supervise', 'closely', 'medical', 'application', 'robot', 'extremely', 'cautious', 'high', 'level', 'success', 'possible', 'option', 'proceed', 'robot', 'pause', 'ask', 'clarification', 'instead', 'blindly', 'continue', 'bowl', 'pick', 'metal', 'plastic', 'test', 'robot', 'different', 'scenario', 'result', 'show', 'knownotrained', 'robot', 'consistent', 'success', 'rate', 'need', 'less', 'human', 'assistance', 'train', 'statistical', 'calculation', 'paper', 'describe', 'research', 'present', 'conference', 'robot', 'learning', 'human', 'language', 'often', 'ambiguous', 'teaching', 'robot', 'recognize', 'respond', 'uncertainty', 'improve', 'performance', 'study', 'show', 'people', 'prefer', 'robot', 'ask', 'question', 'say', 'assistant', 'professor', 'specialize', 'humanrobot', 'interaction', 'involve', 'research', 'robot', 'reach', 'help', 'increase', 'transparency', 'decide', 'lead', 'well', 'interaction', 'say', 'phd', 'student', 'study', 'lead', 'author', 'say', 'several', 'way', 'improve', 'knowno', 'right', 'assume', 'robot', 'vision', 'always', 'reliable', 'case', 'faulty', 'sensor', 'also', 'model', 'update', 'factor', 'potential', 'error', 'come', 'human', 'help', 'ability', 'express', 'uncertainty', 'make', 'trust', 'robot', 'say', 'majumdar', 'quantify', 'uncertainty', 'miss', 'piece', 'lot', 'system', 'say', 'allow', 'confident', 'safe', 'successful', 'robot']"
Google CEO Sundar Pichai on Gemini and the coming age of AI,https://www.technologyreview.com/2023/12/06/1084539/google-ceo-sundar-pichai-on-gemini-and-the-coming-age-of-ai/,2023-12-06,"<p>In an in-depth interview, Pichai predicts: “This will be one of the biggest things we all grapple with for the next decade.”</p>
","Google released the first phase of its next-generation AI model, Gemini, today. Gemini reflects years of efforts from inside Google, overseen and driven by its CEO, Sundar Pichai.  (You can read all about Gemini in our report from Melissa Heikkilä and Will Douglas Heaven here.)  Pichai, who previously oversaw Chrome and Android, is famously product obsessed. In his first founder’s letter as CEO in 2016, he predicted that “[w]e will move from mobile first to an AI first world.” In the years since, Pichai has infused AI deeply into all of Google’s products, from Android devices all the way up to the cloud.  Despite that, the last year has largely been defined by the AI releases from another company, OpenAI. The rollout of DALL-E and GPT-3.5 last year, followed by GPT-4 this year, dominated the sector and kicked off an arms race between startups and tech giants alike.  Gemini is now the latest effort in that race. This state-of-the-art system was led by Google DeepMind, the newly integrated organization led by Demis Hassabis that brings together the company’s AI teams under one umbrella. You can experience Gemini in Bard today, and it will become integrated across the company’s line of products throughout 2024.  We sat down with Sundar Pichai at Google’s offices in Mountain View, California, on the eve of Gemini’s launch to discuss what it will mean for Google, its products, AI, and society writ large.  The following transcript represents Pichai in his own words. The conversation has been edited for clarity and readability.  MIT Technology Review: Why is Gemini exciting? Can you tell me what’s the big picture that you see as it relates to AI, its power, its usefulness, the direction as it goes into all of your products?  Sundar Pichai: A specific part of what makes it exciting is it’s a natively multimodal model from the ground up. Just like humans, it’s not just learning on text alone. It’s text, audio, code. So the model is innately more capable because of that, and I think will help us tease out newer capabilities and contribute to the progress of the field. That’s exciting.  It’s also exciting because Gemini Ultra is state of the art in 30 of the 32 leading benchmarks, and particularly in the multimodal benchmarks. That MMMU benchmark—it shows the progress there. I personally find it exciting that in MMLU [massive multi-task language understanding], which has been one of the leading benchmarks, it crossed the 90% threshold, which is a big milestone. The state of the art two years ago was  30, or 40%. So just think about how much the field is progressing. Approximately 89% is a human expert across these 57 subjects. It’s the first model to cross that threshold.  I’m excited, also, because it’s finally coming in our products. It’s going to be available to developers. It’s a platform. AI is a profound platform shift, bigger than web or mobile. And so it represents a big step for us from that moment as well. Let’s start with those benchmarks. It seemed to be ahead of GPT-4 in almost all of them, or most all of them, but not by a lot. Whereas GPT-4 seemed like a very large leap forward. Are we starting to plateau with what we’re going to see some of these large-language-model technologies be able to do, or do you think we will continue to have these big growth curves? First of all, looking ahead, we do see a lot of headroom. Some of the benchmarks are already high. You have to realize, when you’re trying to go to something from 85%, you’re now at that edge of the curve. So it may not seem like much, but it’s making progress. We are going to need newer benchmarks, too. It’s part of the reason we also looked at the MMLU multimodal benchmark. [For] some of these new benchmarks, the state of the art is still much lower. There’s a lot of progress ahead. The scaling laws are still going to work. As we make the models bigger, there’s going to be more progress. When I take it in the totality of it, I genuinely feel like we are at the very beginning.  I’m interested in what you see as the key breakthroughs of Gemini, and how they will be applied.  It’s so difficult for people to imagine the leaps that will happen. We are providing APIs, and people will imagine it in pretty deep ways. I think multimodality will be big. As we teach these models to reason more, there will be bigger and bigger breakthroughs. Deeper breakthroughs are to come yet.  One way to think about this question is Gemini Pro. It does very well on benchmarks. But when we put it in Bard, I could feel it as a user. We’ve been testing it, and the favorability ratings go up across all categories pretty significantly. It’s why we’re calling it one of our biggest upgrades yet. And when we do side-by-side blind evaluations, it really shows the outperformance. So you make these better models improve on benchmarks. It makes progress. And we’ll continue training and pick it up from there.  But I can’t wait to put it in our products. These models are so capable. Actually designing the product experiences to take advantage of all what the models have—stuff will be exciting for the next few months. It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it? I imagine there was an enormous amount of pressure to get Gemini out the door. I’m curious what you learned by seeing what had happened with GPT-4’s release. What did you learn? What approaches changed in that time frame? One thing, at least to me: it feels very far from a zero-sum game, right? Think about how profound the shift to AI is, and how early we are. There’s a world of opportunity ahead.  But to your specific question, it’s a rich field in which we are all progressing. There is a scientific component to it, there’s an academic component to it; being published a lot, seeing how models like GPT-4 work in the real world. We have learned from that. Safety is an important area. So in part with Gemini, there are safety techniques we have learned and improved on based on how models are working out in the real world. It shows the importance of various things like fine-tuning. One of the things we showed with Med-PaLM 2 was to take a model like PaLM, to really fine-tune it to a specific domain, show it could outperform state-of-the-art models. And so that was a way by which we learned the power of fine-tuning.  A lot of that is applied as we are working our way through Gemini. Part of the reason we are taking some more time with Ultra [the more advanced version of Gemini that will be available next year] is to make sure we are testing it rigorously for safety. But we’re also fine-tuning it to really tease out the capabilities.  When you see some of these releases come out and people begin tinkering with them in the real world, they’ll have hallucinations, or they can reveal some of the private data that their models are trained on. And I wonder how much of that is inherent in the technology, given the data that it’s trained on, if that’s inevitable. If it is inevitable, what types of things do you try and do to limit that? You’re right. These are all active fields of research. In fact, we just published a paper which shows how these models can reveal training data by a series of prompts. Hallucination is not a solved problem. I think we are all making progress on it, and there’s more work to be done. There are some fundamental limitations we need to work through. One example is if you take Gemini Ultra, we are actively red-teaming these models with external third parties using it who are specialists in these things.  In areas like multimodality, we want to be bold and we want to be responsible. We will be more careful with multimodal rollouts, because the chances of wrong use cases are higher.  But you are right in the sense that it is still a technology which is work in progress, which is why they won’t make sense for everything. Which is why in search, we are being more careful about how we use it, and when and what, where we use it, and then when we trigger it. They have these amazing capabilities, and they have clear shortcomings. This is the hard work ahead for all of us. Do you think ultimately this is going to be a solved problem—hallucinations, or with revealing  other training data?  With the current technology of auto-regressive LLMs, hallucinations are not a solved problem. But future AI systems may not look like what we have today. This is one version of technology. It’s like when people thought there is no way you can fit a computer in your pocket. There were people who were really opinionated, 20 years ago. Similarly, looking at these systems and saying you can’t design better systems. I don’t subscribe to that view. There are already many research explorations underway to think about how else to come upon these problems. You’ve talked about how profound a shift this is. In some of these last shifts, like the shift to mobile, it didn’t necessarily increase productivity, which has been flat for a long time. I think there’s an argument that it may have even worsened income inequality. What type of work is Google doing to try to make sure that this shift is more widely beneficial to society? It’s a very important question. I think about it on a few levels. One thing at Google we’ve always been focused on is: How do we get technology access as broadly available as possible? So I would argue even in the case of mobile, the work we do with Android—hundreds of millions of people wouldn’t have otherwise had computing access. We work hard to push toward an affordable smartphone, to maybe sub-$50.  So making AI helpful for everyone is the framework I think about. You try to promote access to as many people as possible. I think that’s one part of it. We are thinking deeply about applying it to use cases which can benefit people. For example, the reason we did flood forecasting early on is because we realized, AI can detect patterns and do it well. We’re using it to translate 1,000 languages. We’re literally trying to bring content now in languages where otherwise you wouldn’t have had access.  This doesn’t solve all the problems you’re talking about. But being deliberate about when and where, what kind of problems you’re going to focus on—we’ve always been focused on that. Take areas like AlphaFold. We have provided an open database for viruses everywhere in the world. But ... who uses it first? Where does it get sold? AI is not going to magically make things better on some of the more difficult issues like inequality; it could exacerbate it.  But what is important is you make sure that technology is available for everyone. You’re developing it early and giving people access and engaging in conversation so that society can think about it and adapt to it.  If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems. We’ve definitely, in this technology, participated earlier on than other technologies. You know, the recent UK AI Safety Forum or work in the US with Congress and the administration. We are trying to do more public-private partnerships, pulling in nonprofit and academic institutions earlier.  Impacts on areas like jobs need to be studied deeply, but I do think there are surprises. There’ll be surprising positive externalities, there’ll be negative externalities too. Solving the negative externalities is larger than any one company. It’s the role of all the stakeholders in society. So I don’t have easy answers there.  I can give you plenty of examples of the benefits mobile brings. I think that will be true of this too. We already showed it with areas like diabetic retinopathy. There are just not enough doctors in many parts of the world to detect it.  Just like I felt giving people access to Google Search everywhere in the world made a positive difference, I think that’s the way to think about expanding access to AI. There are things that are clearly going to make people more productive. Programming is a great example of this. And yet, that democratization of this technology is the very thing that is threatening jobs.  And even if you don’t have all the answers for society—and it’s not incumbent on one company to solve society’s problems—one company can put out a product that can dramatically change the world and have this profound impact.  We never offered facial-recognition APIs. But people built APIs and the technology moves forward. So it is also not in any one company’s hands. Technology will move forward.  I think the answer is more complex than that. Societies can also get left behind. If you don’t adopt these technologies, it could impact your economic competitiveness. You could lose more jobs.  I think the right answer is to responsibly deploy technology and make progress and think about areas where it can cause disproportionate harm and do work to mitigate it. There will be newer types of jobs. If you look at the last 50, 60 years, there are studies from economists from MIT which show most of the new jobs that have been created are in new areas which have come since then.  There will be newer jobs that are created. There will be jobs which are made better, where some of the repetitive work is freed up in a way that you can express yourself more creatively. You could be a doctor, you could be a radiologist, you could be a programmer. The amount of time you’re spending on routine tasks versus higher-order thinking—all that could change, making the job more meaningful. Then there are jobs which could be displaced. So, as a society, how do you retrain, reskill people, and create opportunities?  The last year has really brought out this philosophical split in the way people think we should approach AI. You could talk about it as being safety first or business use cases first, or accelerationists versus doomers. You’re in a position where you have to bridge all of that philosophy and bring it together. I wonder what you personally think about trying to bridge those interests at Google, which is going to be a leader in this field, into this new world. I’m a technology optimist. I have always felt, based on my personal life, a belief in people and humanity. And so overall, I think humanity will harness technology to its benefit. So I’ve always been an optimist. You’re right: a powerful technology like AI—there is a duality to it.  Which means there will be times we will boldly move forward because I think we can push the state of the art. For example, if AI can help us solve problems like cancer or climate change, you want to do everything in your power to move forward fast. But you definitely need society to develop frameworks to adapt, be it to deepfakes or to job displacement, etc. This is going to be a frontier—no different from climate change. This will be one of the biggest things we all grapple with for the next decade ahead. Another big, unsettled thing is the legal landscape around AI. There are questions about fair use, questions about being able to protect the outputs. And it seems like it’s going to be a really big deal for intellectual property. What do you tell people who are using your products, to give them a sense of security, that what they’re doing isn’t going to get them sued? These are not all topics that will have easy answers. When we build products, like Search and YouTube and stuff in the pre-AI world, we’ve always been trying to get the value exchange right. It’s no different for AI. We are definitely focused on making sure we can train on data that is allowed to be trained on, consistent with the law, giving people a chance to opt out of the training. And then there’s a layer about that—about what is fair use. It’s important to create value for the creators of the original content. These are important areas. The internet was an example of it. Or when e-commerce started: How do you draw the line between e-commerce and regular commerce?  There’ll be new legal frameworks developed over time, I think is how I would think about it as this area evolves. But meanwhile, we will work hard to be on the right side of the law and make sure we also have deep relationships with many providers of content today. There are some areas where it’s contentious, but we are working our way through those things, and I am committed to working to figure it out. We have to create that win-win ecosystem for all of this to work over time.  Something that people are very worried about with the web now is the future of search. When you have a type of technology that just answers questions for you, based on information from around the web, there’s a fear people may no longer need to visit those sites. This also seems like it could have implications for Google. I also wonder if you’re thinking about it in terms of your own business.  One of the unique value propositions we’ve had in Search is we are helping users find and learn new things, find answers, but always with a view of sharing with them the richness and the diversity that exists on the web. That will be true, even as we go through our journey with Search Generative Experience. It’s an important principle by which we are developing our product. I don’t think people always come to Search saying, “Just answer it for me.” There may be a question or two for which you may want that, but even then you come back, you learn more, or even in that journey, go deeper. We constantly want to make sure we are getting it right. And I don’t think that’s going to change. It’s important that we get the balance right there.  Similarly, if you deliver value deeply, there is commercial value in what you’re delivering. We had questions like this from desktop to mobile. It’s not new to us. I feel comfortable based on everything we are seeing and how users respond to high-quality ads. YouTube is a good example where we have developed subscription models. That’s also worked well.  How do you think people’s experience is going to change next year, as these products begin to really hit the marketplace and they begin to interact? How is their experience gonna change? I think a year out from now, anybody starting on something in Google Docs will expect something different. And if you give it to them, and later put them back in the version of Google Docs we had, let’s say, in 2022, they will find it so out of date. It’s like, for my kids, if they don’t have spell-check, they fundamentally will think it’s broken. And you and I may remember what it was to use these products before spell-check. But more than any other company, we’ve incorporated so much AI in Search, people take it for granted. That’s one thing I’ve learned over time. They take it for granted.  In terms of what new stuff people can do, as we develop the multimodal capabilities, people will be able to do more complex tasks in a way that they weren’t able to do before. And there’ll be real use cases which are way more powerful.Correction: This story was updated to fix transcription errors. Notably, MMMU was incorrectly transcribed as MMLU, and search generative experience originally appeared as search related experience. ","Google released the first phase of its next-generation AI model , Gemini , today . Gemini reflects years of efforts from inside Google , overseen and driven by its CEO , Sundar Pichai . ( You can read all about Gemini in our report from Melissa Heikkilä and Will Douglas Heaven here . ) Pichai , who previously oversaw Chrome and Android , is famously product obsessed . In his first founder ’ s letter as CEO in 2016 , he predicted that “ [ w ] e will move from mobile first to an AI first world. ” In the years since , Pichai has infused AI deeply into all of Google ’ s products , from Android devices all the way up to the cloud . Despite that , the last year has largely been defined by the AI releases from another company , OpenAI . The rollout of DALL-E and GPT-3.5 last year , followed by GPT-4 this year , dominated the sector and kicked off an arms race between startups and tech giants alike . Gemini is now the latest effort in that race . This state-of-the-art system was led by Google DeepMind , the newly integrated organization led by Demis Hassabis that brings together the company ’ s AI teams under one umbrella . You can experience Gemini in Bard today , and it will become integrated across the company ’ s line of products throughout 2024 . We sat down with Sundar Pichai at Google ’ s offices in Mountain View , California , on the eve of Gemini ’ s launch to discuss what it will mean for Google , its products , AI , and society writ large . The following transcript represents Pichai in his own words . The conversation has been edited for clarity and readability . MIT Technology Review : Why is Gemini exciting ? Can you tell me what ’ s the big picture that you see as it relates to AI , its power , its usefulness , the direction as it goes into all of your products ? Sundar Pichai : A specific part of what makes it exciting is it ’ s a natively multimodal model from the ground up . Just like humans , it ’ s not just learning on text alone . It ’ s text , audio , code . So the model is innately more capable because of that , and I think will help us tease out newer capabilities and contribute to the progress of the field . That ’ s exciting . It ’ s also exciting because Gemini Ultra is state of the art in 30 of the 32 leading benchmarks , and particularly in the multimodal benchmarks . That MMMU benchmark—it shows the progress there . I personally find it exciting that in MMLU [ massive multi-task language understanding ] , which has been one of the leading benchmarks , it crossed the 90 % threshold , which is a big milestone . The state of the art two years ago was 30 , or 40 % . So just think about how much the field is progressing . Approximately 89 % is a human expert across these 57 subjects . It ’ s the first model to cross that threshold . I ’ m excited , also , because it ’ s finally coming in our products . It ’ s going to be available to developers . It ’ s a platform . AI is a profound platform shift , bigger than web or mobile . And so it represents a big step for us from that moment as well . Let ’ s start with those benchmarks . It seemed to be ahead of GPT-4 in almost all of them , or most all of them , but not by a lot . Whereas GPT-4 seemed like a very large leap forward . Are we starting to plateau with what we ’ re going to see some of these large-language-model technologies be able to do , or do you think we will continue to have these big growth curves ? First of all , looking ahead , we do see a lot of headroom . Some of the benchmarks are already high . You have to realize , when you ’ re trying to go to something from 85 % , you ’ re now at that edge of the curve . So it may not seem like much , but it ’ s making progress . We are going to need newer benchmarks , too . It ’ s part of the reason we also looked at the MMLU multimodal benchmark . [ For ] some of these new benchmarks , the state of the art is still much lower . There ’ s a lot of progress ahead . The scaling laws are still going to work . As we make the models bigger , there ’ s going to be more progress . When I take it in the totality of it , I genuinely feel like we are at the very beginning . I ’ m interested in what you see as the key breakthroughs of Gemini , and how they will be applied . It ’ s so difficult for people to imagine the leaps that will happen . We are providing APIs , and people will imagine it in pretty deep ways . I think multimodality will be big . As we teach these models to reason more , there will be bigger and bigger breakthroughs . Deeper breakthroughs are to come yet . One way to think about this question is Gemini Pro . It does very well on benchmarks . But when we put it in Bard , I could feel it as a user . We ’ ve been testing it , and the favorability ratings go up across all categories pretty significantly . It ’ s why we ’ re calling it one of our biggest upgrades yet . And when we do side-by-side blind evaluations , it really shows the outperformance . So you make these better models improve on benchmarks . It makes progress . And we ’ ll continue training and pick it up from there . But I can ’ t wait to put it in our products . These models are so capable . Actually designing the product experiences to take advantage of all what the models have—stuff will be exciting for the next few months . It outmatches GPT-4 in almost all ways—but only by a little . Was the buzz worth it ? I imagine there was an enormous amount of pressure to get Gemini out the door . I ’ m curious what you learned by seeing what had happened with GPT-4 ’ s release . What did you learn ? What approaches changed in that time frame ? One thing , at least to me : it feels very far from a zero-sum game , right ? Think about how profound the shift to AI is , and how early we are . There ’ s a world of opportunity ahead . But to your specific question , it ’ s a rich field in which we are all progressing . There is a scientific component to it , there ’ s an academic component to it ; being published a lot , seeing how models like GPT-4 work in the real world . We have learned from that . Safety is an important area . So in part with Gemini , there are safety techniques we have learned and improved on based on how models are working out in the real world . It shows the importance of various things like fine-tuning . One of the things we showed with Med-PaLM 2 was to take a model like PaLM , to really fine-tune it to a specific domain , show it could outperform state-of-the-art models . And so that was a way by which we learned the power of fine-tuning . A lot of that is applied as we are working our way through Gemini . Part of the reason we are taking some more time with Ultra [ the more advanced version of Gemini that will be available next year ] is to make sure we are testing it rigorously for safety . But we ’ re also fine-tuning it to really tease out the capabilities . When you see some of these releases come out and people begin tinkering with them in the real world , they ’ ll have hallucinations , or they can reveal some of the private data that their models are trained on . And I wonder how much of that is inherent in the technology , given the data that it ’ s trained on , if that ’ s inevitable . If it is inevitable , what types of things do you try and do to limit that ? You ’ re right . These are all active fields of research . In fact , we just published a paper which shows how these models can reveal training data by a series of prompts . Hallucination is not a solved problem . I think we are all making progress on it , and there ’ s more work to be done . There are some fundamental limitations we need to work through . One example is if you take Gemini Ultra , we are actively red-teaming these models with external third parties using it who are specialists in these things . In areas like multimodality , we want to be bold and we want to be responsible . We will be more careful with multimodal rollouts , because the chances of wrong use cases are higher . But you are right in the sense that it is still a technology which is work in progress , which is why they won ’ t make sense for everything . Which is why in search , we are being more careful about how we use it , and when and what , where we use it , and then when we trigger it . They have these amazing capabilities , and they have clear shortcomings . This is the hard work ahead for all of us . Do you think ultimately this is going to be a solved problem—hallucinations , or with revealing other training data ? With the current technology of auto-regressive LLMs , hallucinations are not a solved problem . But future AI systems may not look like what we have today . This is one version of technology . It ’ s like when people thought there is no way you can fit a computer in your pocket . There were people who were really opinionated , 20 years ago . Similarly , looking at these systems and saying you can ’ t design better systems . I don ’ t subscribe to that view . There are already many research explorations underway to think about how else to come upon these problems . You ’ ve talked about how profound a shift this is . In some of these last shifts , like the shift to mobile , it didn ’ t necessarily increase productivity , which has been flat for a long time . I think there ’ s an argument that it may have even worsened income inequality . What type of work is Google doing to try to make sure that this shift is more widely beneficial to society ? It ’ s a very important question . I think about it on a few levels . One thing at Google we ’ ve always been focused on is : How do we get technology access as broadly available as possible ? So I would argue even in the case of mobile , the work we do with Android—hundreds of millions of people wouldn ’ t have otherwise had computing access . We work hard to push toward an affordable smartphone , to maybe sub- $ 50 . So making AI helpful for everyone is the framework I think about . You try to promote access to as many people as possible . I think that ’ s one part of it . We are thinking deeply about applying it to use cases which can benefit people . For example , the reason we did flood forecasting early on is because we realized , AI can detect patterns and do it well . We ’ re using it to translate 1,000 languages . We ’ re literally trying to bring content now in languages where otherwise you wouldn ’ t have had access . This doesn ’ t solve all the problems you ’ re talking about . But being deliberate about when and where , what kind of problems you ’ re going to focus on—we ’ ve always been focused on that . Take areas like AlphaFold . We have provided an open database for viruses everywhere in the world . But ... who uses it first ? Where does it get sold ? AI is not going to magically make things better on some of the more difficult issues like inequality ; it could exacerbate it . But what is important is you make sure that technology is available for everyone . You ’ re developing it early and giving people access and engaging in conversation so that society can think about it and adapt to it . If OpenAI 's new model can solve grade-school math , it could pave the way for more powerful systems . We ’ ve definitely , in this technology , participated earlier on than other technologies . You know , the recent UK AI Safety Forum or work in the US with Congress and the administration . We are trying to do more public-private partnerships , pulling in nonprofit and academic institutions earlier . Impacts on areas like jobs need to be studied deeply , but I do think there are surprises . There ’ ll be surprising positive externalities , there ’ ll be negative externalities too . Solving the negative externalities is larger than any one company . It ’ s the role of all the stakeholders in society . So I don ’ t have easy answers there . I can give you plenty of examples of the benefits mobile brings . I think that will be true of this too . We already showed it with areas like diabetic retinopathy . There are just not enough doctors in many parts of the world to detect it . Just like I felt giving people access to Google Search everywhere in the world made a positive difference , I think that ’ s the way to think about expanding access to AI . There are things that are clearly going to make people more productive . Programming is a great example of this . And yet , that democratization of this technology is the very thing that is threatening jobs . And even if you don ’ t have all the answers for society—and it ’ s not incumbent on one company to solve society ’ s problems—one company can put out a product that can dramatically change the world and have this profound impact . We never offered facial-recognition APIs . But people built APIs and the technology moves forward . So it is also not in any one company ’ s hands . Technology will move forward . I think the answer is more complex than that . Societies can also get left behind . If you don ’ t adopt these technologies , it could impact your economic competitiveness . You could lose more jobs . I think the right answer is to responsibly deploy technology and make progress and think about areas where it can cause disproportionate harm and do work to mitigate it . There will be newer types of jobs . If you look at the last 50 , 60 years , there are studies from economists from MIT which show most of the new jobs that have been created are in new areas which have come since then . There will be newer jobs that are created . There will be jobs which are made better , where some of the repetitive work is freed up in a way that you can express yourself more creatively . You could be a doctor , you could be a radiologist , you could be a programmer . The amount of time you ’ re spending on routine tasks versus higher-order thinking—all that could change , making the job more meaningful . Then there are jobs which could be displaced . So , as a society , how do you retrain , reskill people , and create opportunities ? The last year has really brought out this philosophical split in the way people think we should approach AI . You could talk about it as being safety first or business use cases first , or accelerationists versus doomers . You ’ re in a position where you have to bridge all of that philosophy and bring it together . I wonder what you personally think about trying to bridge those interests at Google , which is going to be a leader in this field , into this new world . I ’ m a technology optimist . I have always felt , based on my personal life , a belief in people and humanity . And so overall , I think humanity will harness technology to its benefit . So I ’ ve always been an optimist . You ’ re right : a powerful technology like AI—there is a duality to it . Which means there will be times we will boldly move forward because I think we can push the state of the art . For example , if AI can help us solve problems like cancer or climate change , you want to do everything in your power to move forward fast . But you definitely need society to develop frameworks to adapt , be it to deepfakes or to job displacement , etc . This is going to be a frontier—no different from climate change . This will be one of the biggest things we all grapple with for the next decade ahead . Another big , unsettled thing is the legal landscape around AI . There are questions about fair use , questions about being able to protect the outputs . And it seems like it ’ s going to be a really big deal for intellectual property . What do you tell people who are using your products , to give them a sense of security , that what they ’ re doing isn ’ t going to get them sued ? These are not all topics that will have easy answers . When we build products , like Search and YouTube and stuff in the pre-AI world , we ’ ve always been trying to get the value exchange right . It ’ s no different for AI . We are definitely focused on making sure we can train on data that is allowed to be trained on , consistent with the law , giving people a chance to opt out of the training . And then there ’ s a layer about that—about what is fair use . It ’ s important to create value for the creators of the original content . These are important areas . The internet was an example of it . Or when e-commerce started : How do you draw the line between e-commerce and regular commerce ? There ’ ll be new legal frameworks developed over time , I think is how I would think about it as this area evolves . But meanwhile , we will work hard to be on the right side of the law and make sure we also have deep relationships with many providers of content today . There are some areas where it ’ s contentious , but we are working our way through those things , and I am committed to working to figure it out . We have to create that win-win ecosystem for all of this to work over time . Something that people are very worried about with the web now is the future of search . When you have a type of technology that just answers questions for you , based on information from around the web , there ’ s a fear people may no longer need to visit those sites . This also seems like it could have implications for Google . I also wonder if you ’ re thinking about it in terms of your own business . One of the unique value propositions we ’ ve had in Search is we are helping users find and learn new things , find answers , but always with a view of sharing with them the richness and the diversity that exists on the web . That will be true , even as we go through our journey with Search Generative Experience . It ’ s an important principle by which we are developing our product . I don ’ t think people always come to Search saying , “ Just answer it for me. ” There may be a question or two for which you may want that , but even then you come back , you learn more , or even in that journey , go deeper . We constantly want to make sure we are getting it right . And I don ’ t think that ’ s going to change . It ’ s important that we get the balance right there . Similarly , if you deliver value deeply , there is commercial value in what you ’ re delivering . We had questions like this from desktop to mobile . It ’ s not new to us . I feel comfortable based on everything we are seeing and how users respond to high-quality ads . YouTube is a good example where we have developed subscription models . That ’ s also worked well . How do you think people ’ s experience is going to change next year , as these products begin to really hit the marketplace and they begin to interact ? How is their experience gon na change ? I think a year out from now , anybody starting on something in Google Docs will expect something different . And if you give it to them , and later put them back in the version of Google Docs we had , let ’ s say , in 2022 , they will find it so out of date . It ’ s like , for my kids , if they don ’ t have spell-check , they fundamentally will think it ’ s broken . And you and I may remember what it was to use these products before spell-check . But more than any other company , we ’ ve incorporated so much AI in Search , people take it for granted . That ’ s one thing I ’ ve learned over time . They take it for granted . In terms of what new stuff people can do , as we develop the multimodal capabilities , people will be able to do more complex tasks in a way that they weren ’ t able to do before . And there ’ ll be real use cases which are way more powerful.Correction : This story was updated to fix transcription errors . Notably , MMMU was incorrectly transcribed as MMLU , and search generative experience originally appeared as search related experience .","['release', 'first', 'phase', 'nextgeneration', 'model', 'gemini', 'today', 'gemini', 'reflect', 'year', 'effort', 'oversee', 'drive', 'ceo', 'sundar', 'read', 'gemini', 'report', 'heikkilä', 'previously', 'oversee', 'chrome', 'android', 'famously', 'product', 'obsess', 'first', 'founder', 'letter', 'ceo', 'predict', 'e', 'move', 'mobile', 'first', 'first', 'world', 'year', 'infuse', 'ai', 'deeply', 'product', 'android', 'device', 'way', 'cloud', 'last', 'year', 'largely', 'define', 'release', 'company', 'openai', 'rollout', 'dalle', 'gpt35', 'last', 'year', 'follow', 'gpt4', 'year', 'dominate', 'sector', 'kick', 'arm', 'race', 'startup', 'tech', 'giant', 'gemini', 'late', 'effort', 'race', 'stateoftheart', 'system', 'lead', 'newly', 'integrate', 'organization', 'lead', 'bring', 'together', 'company', 'ai', 'team', 'umbrella', 'experience', 'gemini', 'bard', 'today', 'become', 'integrate', 'company', 'line', 'product', 'sit', 'sundar', 'office', 'mountain', 'view', 'eve', 'launch', 'discuss', 'mean', 'product', 'ai', 'society', 'writ', 'large', 'follow', 'transcript', 'represent', 'word', 'conversation', 'edit', 'clarity', 'readability', 'mit', 'technology', 'review', 'gemini', 'exciting', 'tell', 'big', 'picture', 'see', 'relate', 'ai', 'power', 'usefulness', 'direction', 'go', 'product', 'sundar', 'specific', 'part', 'make', 'exciting', 'natively', 'multimodal', 'model', 'ground', 'human', 'learn', 'text', 'alone', 'text', 'audio', 'code', 'model', 'innately', 'capable', 'think', 'help', 'tease', 'new', 'capability', 'contribute', 'progress', 'field', 'exciting', 'also', 'exciting', 'gemini', 'ultra', 'state', 'art', 'lead', 'benchmark', 'particularly', 'multimodal', 'benchmark', 'mmmu', 'benchmark', 'show', 'progress', 'personally', 'find', 'exciting', 'mmlu', 'massive', 'multitask', 'language', 'understanding', 'lead', 'benchmark', 'cross', 'threshold', 'big', 'milestone', 'state', 'art', 'year', 'ago', 'think', 'much', 'field', 'progress', 'approximately', 'human', 'expert', 'subject', 'first', 'model', 'cross', 'threshold', 'excite', 'also', 'finally', 'come', 'product', 'go', 'available', 'developer', 'platform', 'ai', 'profound', 'platform', 'shift', 'big', 'web', 'mobile', 'represent', 'big', 'step', 'moment', 'well', 'let', 'start', 'benchmark', 'seem', 'ahead', 'gpt4', 'almost', 'lot', 'gpt4', 'seem', 'large', 'leap', 'forward', 'start', 'plateau', 'go', 'see', 'largelanguagemodel', 'technology', 'able', 'think', 'continue', 'big', 'growth', 'curve', 'first', 'look', 'ahead', 'see', 'lot', 'headroom', 'benchmark', 'already', 'high', 'realize', 'try', 'go', 'edge', 'curve', 'seem', 'much', 'make', 'progress', 'go', 'need', 'new', 'benchmark', 'part', 'reason', 'also', 'look', 'mmlu', 'multimodal', 'benchmark', 'new', 'benchmark', 'state', 'art', 'still', 'much', 'low', 'lot', 'progress', 'ahead', 'scaling', 'law', 'still', 'go', 'work', 'make', 'model', 'big', 'go', 'progress', 'take', 'totality', 'genuinely', 'feel', 'beginning', 'interested', 'see', 'key', 'breakthrough', 'gemini', 'apply', 'difficult', 'people', 'imagine', 'leap', 'happen', 'provide', 'apis', 'people', 'imagine', 'pretty', 'deep', 'way', 'think', 'multimodality', 'big', 'teach', 'model', 'reason', 'big', 'big', 'breakthrough', 'deep', 'breakthrough', 'come', 'yet', 'way', 'think', 'question', 'gemini', 'well', 'benchmark', 'put', 'feel', 'user', 'test', 'favorability', 'rating', 'go', 'category', 'pretty', 'significantly', 'call', 'big', 'upgrade', 'yet', 'sidebyside', 'blind', 'evaluation', 'really', 'show', 'outperformance', 'make', 'well', 'model', 'improve', 'benchmark', 'make', 'progress', 'continue', 'training', 'pick', 'wait', 'put', 'product', 'model', 'capable', 'actually', 'design', 'product', 'experience', 'take', 'advantage', 'model', 'stuff', 'exciting', 'next', 'month', 'outmatch', 'gpt4', 'almost', 'way', 'little', 'buzz', 'worth', 'imagine', 'enormous', 'amount', 'pressure', 'get', 'gemini', 'door', 'curious', 'learn', 'see', 'happen', 'release', 'learn', 'approach', 'change', 'time', 'frame', 'thing', 'least', 'feel', 'far', 'zerosum', 'game', 'right', 'think', 'profound', 'shift', 'ai', 'early', 'world', 'opportunity', 'ahead', 'specific', 'question', 'rich', 'field', 'progress', 'scientific', 'component', 'academic', 'component', 'publish', 'lot', 'see', 'model', 'gpt4', 'work', 'real', 'world', 'learn', 'safety', 'important', 'area', 'part', 'gemini', 'safety', 'technique', 'learn', 'improve', 'base', 'model', 'work', 'real', 'world', 'show', 'importance', 'various', 'thing', 'finetune', 'thing', 'show', 'take', 'model', 'palm', 'really', 'finetune', 'specific', 'domain', 'show', 'outperform', 'stateoftheart', 'model', 'way', 'learn', 'power', 'finetune', 'lot', 'apply', 'work', 'way', 'gemini', 'part', 'reason', 'take', 'time', 'advanced', 'version', 'gemini', 'available', 'next', 'year', 'make', 'sure', 'test', 'rigorously', 'safety', 'also', 'finetune', 'really', 'tease', 'capability', 'see', 'release', 'come', 'people', 'begin', 'tinker', 'real', 'world', 'hallucination', 'reveal', 'private', 'datum', 'model', 'train', 'wonder', 'much', 'inherent', 'technology', 'give', 'datum', 'train', 'inevitable', 'inevitable', 'type', 'thing', 'try', 'limit', 'right', 'active', 'field', 'research', 'fact', 'publish', 'paper', 'show', 'model', 'reveal', 'training', 'datum', 'series', 'prompt', 'hallucination', 'solve', 'problem', 'think', 'make', 'progress', 'work', 'fundamental', 'limitation', 'need', 'work', 'example', 'take', 'gemini', 'actively', 'redteame', 'model', 'external', 'third', 'party', 'use', 'specialist', 'thing', 'area', 'multimodality', 'want', 'bold', 'want', 'responsible', 'careful', 'multimodal', 'rollout', 'chance', 'wrong', 'use', 'case', 'high', 'right', 'sense', 'still', 'technology', 'work', 'progress', 'win', 'make', 'sense', 'search', 'careful', 'use', 'use', 'trigger', 'amazing', 'capability', 'clear', 'shortcoming', 'hard', 'work', 'ahead', 'think', 'ultimately', 'go', 'solve', 'problem', 'hallucination', 'reveal', 'training', 'datum', 'current', 'technology', 'autoregressive', 'llm', 'hallucination', 'solve', 'problem', 'future', 'ai', 'system', 'look', 'today', 'version', 'technology', 'people', 'think', 'way', 'fit', 'computer', 'pocket', 'people', 'really', 'opinionate', 'year', 'ago', 'similarly', 'look', 'system', 'say', 'design', 'well', 'system', 'subscribe', 'view', 'already', 'many', 'research', 'exploration', 'underway', 'think', 'else', 'come', 'problem', 'talk', 'profound', 'shift', 'last', 'shift', 'shift', 'mobile', 'necessarily', 'increase', 'productivity', 'flat', 'long', 'time', 'think', 'argument', 'even', 'worsen', 'income', 'inequality', 'type', 'work', 'try', 'make', 'sure', 'shift', 'widely', 'beneficial', 'society', 'important', 'question', 'think', 'level', 'thing', 'always', 'focus', 'get', 'technology', 'access', 'broadly', 'available', 'possible', 'argue', 'even', 'case', 'mobile', 'work', 'android', 'hundred', 'million', 'people', 'otherwise', 'compute', 'access', 'work', 'hard', 'push', 'affordable', 'smartphone', 'maybe', 'sub', 'make', 'ai', 'helpful', 'framework', 'think', 'try', 'promote', 'access', 'many', 'people', 'possible', 'think', 'part', 'think', 'deeply', 'apply', 'use', 'case', 'benefit', 'people', 'example', 'reason', 'flood', 'forecasting', 'early', 'realize', 'ai', 'detect', 'pattern', 'well', 'use', 'translate', 'language', 'literally', 'try', 'bring', 'content', 'language', 'otherwise', 'access', 'solve', 'problem', 'talk', 'deliberate', 'kind', 'problem', 'go', 'focus', 'always', 'focus', 'take', 'area', 'alphafold', 'provide', 'open', 'database', 'virus', 'everywhere', 'world', 'use', 'first', 'sell', 'go', 'magically', 'make', 'thing', 'well', 'difficult', 'issue', 'inequality', 'exacerbate', 'important', 'make', 'sure', 'technology', 'available', 'develop', 'early', 'give', 'people', 'access', 'engage', 'conversation', 'society', 'think', 'adapt', 'new', 'model', 'solve', 'gradeschool', 'math', 'pave', 'way', 'powerful', 'system', 'definitely', 'technology', 'participate', 'early', 'technology', 'know', 'recent', 'safety', 'forum', 'work', 'administration', 'try', 'publicprivate', 'partnership', 'pull', 'nonprofit', 'academic', 'institution', 'early', 'impact', 'area', 'job', 'need', 'study', 'deeply', 'think', 'surprise', 'surprising', 'positive', 'externality', 'negative', 'externality', 'solve', 'negative', 'externality', 'large', 'company', 'role', 'stakeholder', 'society', 'easy', 'answer', 'give', 'plenty', 'example', 'benefit', 'mobile', 'bring', 'think', 'true', 'already', 'show', 'area', 'diabetic', 'enough', 'doctor', 'many', 'part', 'world', 'detect', 'feel', 'give', 'people', 'access', 'google', 'search', 'everywhere', 'world', 'make', 'positive', 'difference', 'think', 'way', 'think', 'expand', 'access', 'ai', 'thing', 'clearly', 'go', 'make', 'people', 'productive', 'programming', 'great', 'example', 'yet', 'democratization', 'technology', 'thing', 'threaten', 'job', 'even', 'answer', 'society', 'incumbent', 'company', 'solve', 'society', 'problem', 'company', 'put', 'product', 'dramatically', 'change', 'world', 'profound', 'impact', 'never', 'offer', 'facialrecognition', 'apis', 'people', 'build', 'apis', 'technology', 'move', 'forward', 'also', 'company', 'hand', 'technology', 'move', 'forward', 'think', 'answer', 'complex', 'society', 'also', 'leave', 'behind', 'adopt', 'technology', 'impact', 'economic', 'competitiveness', 'lose', 'job', 'think', 'right', 'answer', 'responsibly', 'deploy', 'technology', 'make', 'progress', 'think', 'area', 'cause', 'disproportionate', 'harm', 'work', 'mitigate', 'new', 'type', 'job', 'look', 'last', 'year', 'study', 'economist', 'mit', 'show', 'new', 'job', 'create', 'new', 'area', 'come', 'new', 'job', 'create', 'job', 'make', 'well', 'repetitive', 'work', 'free', 'way', 'express', 'creatively', 'doctor', 'radiologist', 'programmer', 'amount', 'time', 'spend', 'routine', 'task', 'higherorder', 'thinking', 'change', 'make', 'job', 'meaningful', 'job', 'displace', 'society', 'retrain', 'reskill', 'people', 'create', 'opportunity', 'last', 'year', 'really', 'bring', 'philosophical', 'split', 'way', 'people', 'think', 'approach', 'ai', 'talk', 'safety', 'first', 'business', 'use', 'case', 'first', 'accelerationist', 'doomer', 'position', 'bridge', 'philosophy', 'bring', 'together', 'wonder', 'personally', 'think', 'try', 'bridge', 'interest', 'go', 'leader', 'field', 'new', 'world', 'technology', 'optimist', 'always', 'feel', 'base', 'personal', 'life', 'belief', 'people', 'humanity', 'overall', 'think', 'humanity', 'harness', 'technology', 'benefit', 'always', 'optimist', 'right', 'powerful', 'technology', 'ai', 'duality', 'mean', 'time', 'boldly', 'move', 'forward', 'think', 'push', 'state', 'art', 'example', 'help', 'solve', 'problem', 'cancer', 'climate', 'change', 'want', 'power', 'move', 'forward', 'fast', 'definitely', 'need', 'society', 'develop', 'framework', 'adapt', 'deepfake', 'job', 'displacement', 'go', 'frontier', 'different', 'climate', 'change', 'big', 'thing', 'grapple', 'next', 'decade', 'ahead', 'big', 'unsettled', 'thing', 'legal', 'landscape', 'around', 'ai', 'question', 'fair', 'use', 'question', 'able', 'protect', 'output', 'seem', 'go', 'really', 'big', 'deal', 'intellectual', 'property', 'tell', 'people', 'use', 'product', 'give', 'sense', 'security', 'go', 'get', 'sue', 'topic', 'easy', 'answer', 'build', 'product', 'search', 'youtube', 'stuff', 'preai', 'world', 'always', 'try', 'get', 'value', 'exchange', 'right', 'different', 'ai', 'definitely', 'focused', 'make', 'sure', 'train', 'datum', 'allow', 'train', 'consistent', 'law', 'give', 'people', 'chance', 'opt', 'training', 'layer', 'fair', 'use', 'important', 'create', 'value', 'creator', 'original', 'content', 'important', 'area', 'internet', 'example', 'ecommerce', 'start', 'draw', 'line', 'ecommerce', 'regular', 'commerce', 'new', 'legal', 'framework', 'develop', 'time', 'think', 'think', 'area', 'evolve', 'meanwhile', 'work', 'hard', 'right', 'side', 'law', 'make', 'sure', 'also', 'deep', 'relationship', 'many', 'provider', 'content', 'today', 'area', 'contentious', 'work', 'way', 'thing', 'committed', 'work', 'figure', 'create', 'winwin', 'ecosystem', 'work', 'time', 'people', 'worried', 'web', 'future', 'search', 'type', 'technology', 'answer', 'question', 'base', 'information', 'web', 'fear', 'people', 'long', 'need', 'visit', 'site', 'also', 'seem', 'implication', 'also', 'wonder', 'think', 'term', 'business', 'unique', 'value', 'proposition', 'search', 'help', 'user', 'find', 'learn', 'new', 'thing', 'find', 'answer', 'always', 'view', 'share', 'richness', 'diversity', 'exist', 'web', 'true', 'even', 'go', 'journey', 'search', 'generative', 'experience', 'important', 'principle', 'develop', 'product', 'think', 'people', 'always', 'come', 'search', 'say', 'answer', 'question', 'want', 'even', 'come', 'back', 'learn', 'even', 'journey', 'go', 'deeply', 'constantly', 'want', 'make', 'sure', 'get', 'right', 'think', 'go', 'change', 'important', 'get', 'balance', 'right', 'similarly', 'deliver', 'value', 'deeply', 'commercial', 'value', 'deliver', 'question', 'desktop', 'mobile', 'new', 'feel', 'comfortable', 'base', 'see', 'user', 'respond', 'highquality', 'ad', 'youtube', 'good', 'example', 'develop', 'subscription', 'model', 'also', 'work', 'well', 'think', 'people', 'experience', 'go', 'change', 'next', 'year', 'product', 'begin', 'really', 'hit', 'marketplace', 'begin', 'interact', 'experience', 'go', 'change', 'think', 'year', 'start', 'expect', 'different', 'give', 'later', 'put', 'back', 'version', 'let', 'say', 'find', 'date', 'kid', 'fundamentally', 'think', 'break', 'remember', 'use', 'product', 'spellcheck', 'company', 'incorporate', 'much', 'ai', 'search', 'people', 'take', 'grant', 'thing', 'learn', 'time', 'take', 'grant', 'term', 'new', 'stuff', 'people', 'develop', 'multimodal', 'capability', 'people', 'able', 'complex', 'task', 'way', 'able', 'real', 'use', 'case', 'way', 'powerfulcorrection', 'story', 'update', 'fix', 'transcription', 'error', 'notably', 'mmmu', 'incorrectly', 'transcribe', 'mmlu', 'search', 'generative', 'experience', 'originally', 'appear', 'search', 'relate', 'experience']"
Google DeepMind’s new Gemini model looks amazing—but could signal peak AI hype,https://www.technologyreview.com/2023/12/06/1084471/google-deepminds-new-gemini-model-looks-amazing-but-could-signal-peak-ai-hype/,2023-12-06,"<p>It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it?</p>
","Hype about Gemini, Google DeepMind’s long-rumored response to OpenAI’s GPT-4, has been building for months. Today the company finally revealed what it has been working on in secret all this time. Was the hype justified? Yes—and no.  Gemini is Google’s biggest AI launch yet—its push to take on competitors OpenAI and Microsoft in the race for AI supremacy. There is no doubt that the model is pitched as best-in-class across a wide range of capabilities—an “everything machine,” as one observer puts it.  In an in-depth interview, Pichai predicts: “This will be one of the biggest things we all grapple with for the next decade.” “The model is innately more capable,” Sundar Pichai, the CEO of Google and its parent company Alphabet, told MIT Technology Review. “It’s a platform. AI is a profound platform shift, bigger than web or mobile. And so it represents a big step for us.”  It’s a big step for Google, but not necessarily a giant leap for the field as a whole. Google DeepMind claims that Gemini outmatches GPT-4 on 30 out of 32 standard measures of performance. And yet the margins between them are thin. What Google DeepMind has done is pull AI’s best current capabilities into one powerful package. To judge from demos, it does many things very well—but few things that we haven’t seen before. For all the buzz about the next big thing, Gemini could be a sign that we’ve reached peak AI hype. At least for now.  Chirag Shah, a professor at the University of Washington who specializes in online search, compares the launch to Apple’s introduction of a new iPhone every year. “Maybe we just have risen to a different threshold now, where this doesn’t impress us as much because we’ve just seen so much,” he says.  Like GPT-4, Gemini is multimodal, meaning it is trained to handle multiple kinds of input: text, images, audio. It can combine these different formats to answer questions about everything from household chores to college math to economics.  In a demo for journalists yesterday, Google showed Gemini’s ability to take an existing screenshot of a chart, analyze hundreds of pages of research with new data, and then update the chart with that new information. In another example, Gemini is shown pictures of an omelet cooking in a pan and asked (using speech, not text) if the omelet is cooked yet. “It’s not ready because the eggs are still runny,” it replies.  Most people will have to wait for the full experience, however. The version launched today is a back end to Bard, Google’s text-based search chatbot, which the company says will give it more advanced reasoning, planning, and understanding capabilities. Gemini’s full release will be staggered over the coming months. The new Gemini-boosted Bard will initially be available in English in more than 170 countries, not including the EU and the UK. This is to let the company “engage” with local regulators, says Sissie Hsiao, a Google vice president in charge of Bard.  Gemini also comes in three sizes: Ultra, Pro and Nano. Ultra is the full-powered version; Pro and Nano are tailored to applications that run with more limited computing resources. Nano is designed to run on devices, such as Google’s new Pixel phones. Developers and businesses will be able to access Gemini Pro starting December 13. Gemini Ultra, the most powerful model, will be available “early next year” following “extensive trust and safety checks,” Google executives told reporters on a press call.  “I think of it as the Gemini era of models,” Pichai told us. “This is how Google DeepMind is going to build and make progress on AI. So it will always represent the frontier of where we are making progress on AI technology.” OpenAI’s most powerful model, GPT-4, is seen as the industry’s gold standard. While Google boasted that Gemini outperforms OpenAI’s previous model, GPT 3.5, company executives dodged questions about how far the model exceeds GPT-4.  We got a first look at the much-anticipated big new language model from OpenAI. But this time how it works is even more deeply under wraps. But the firm highlights one benchmark in particular, called MMLU (massive multitask language understanding). This is a set of tests designed to measure the performance of models on tasks involving text and images, including reading comprehension, college math, and multiple-choice quizzes in physics, economics, and social sciences. On the text-only questions, Gemini scores 90% and human experts score approximately 89%, says Pichai. GPT-4 scores 86% on these types of questions. On the multimodal questions, Gemini scores 59%, while GPT-4 scores 57%. “It’s the first model to cross that threshold,” Pichai says.  Gemini’s performance against benchmark data sets is very impressive, says Melanie Mitchell, an artificial-intelligence researcher at the Santa Fe Institute in New Mexico. “It’s clear that Gemini is a very sophisticated AI system,” says Mitchell. But “it’s not obvious to me that Gemini is actually substantially more capable than GPT-4,” she adds. While the model has good benchmark scores, it is hard to know how to interpret these numbers given that we don’t know what’s in the training data, says Percy Liang, director of Stanford’s Center for Research on Foundation Models.  Mitchell also notes that Gemini performs much better on language and code benchmarks than on images and video. “Multimodal foundation models still have a ways to go to be generally and robustly useful for many tasks,” she says.  Using feedback from human testers, Google DeepMind has trained Gemini to be more factually accurate, to give attribution when asked to, and to hedge rather than spit out nonsense when faced with a question it cannot answer. The company claims that this mitigates the problem of hallucinations. But without a radical overhaul of the base technology, large language models will continue to make things up.  Experts say it’s unclear whether the benchmarks Google is using to measure Gemini’s performance offer that much insight, and without transparency, it’s hard to check Google’s claims.  “Google is advertising Gemini as an everything machine—a general-purpose model that can be used in many different ways,” says Emily Bender, a professor of computational linguistics at the University of Washington. But the company is using narrow benchmarks to evaluate models that it expects to be used for these diverse purposes. “This means it effectively can’t be thoroughly evaluated,” she says.   Ultimately, for the average user, the incremental improvement over competing models might not make much difference, says Shah. “It’s more about convenience, brand recognition, existing integration, than people really thinking ‘Oh, this is better,’” he says.  Gemini has been a long time coming. In April 2023, Google announced it was merging its AI research unit Google Brain with DeepMind, Alphabet’s London-based AI research lab. So Google has had all year to develop its answer to OpenAI’s most advanced large language model, GPT-4, which debuted in March and is the backbone of the paid version of ChatGPT. With hopes and fears about the technology running wild, it's time to agree on what it can and can't do. Google has been under intense pressure to show investors it can match and overtake competitors in AI. Although the company has been developing and using powerful AI models for years, it has been hesitant to launch tools that the public can play with for fears of reputational damage and safety concerns.  “Google has been very cautious about releasing this stuff to the public,” Geoffrey Hinton told MIT Technology Review in April when he left the company. “There are too many bad things that could happen, and Google didn’t want to ruin its reputation.” Faced with tech that seemed untrustworthy or unmarketable, Google played it safe—until the greater risk became missing out. Google has learned the hard way how launching flawed products can backfire. When it unveiled its ChatGPT competitor Bard in February, scientists soon noticed a factual error in the company’s own advertisement for the chatbot, an incident that subsequently wiped $100 billion off its share price.  In May, Google announced it was rolling out generative AI into most of its products, from email to productivity software. But the results failed to impress critics: the chatbot made references to emails that didn’t exist, for example.   This is a consistent problem with large language models. Although excellent at generating text that sounds like something a human could have written, generative AI systems regularly make things up. And that’s not the only problem with them. They are also easy to hack, and riddled with biases. Using them is also highly polluting.  Google has solved neither these problems nor the hallucination issue. Its solution to the latter problem is a tool that lets people use Google search to double-check the chatbot’s answers, but that relies on the accuracy of the online search results themselves.  Gemini may be the pinnacle of this wave of generative AI. But it’s not clear where AI built on large language models goes next. Some researchers believe this could be a plateau rather than the foot of the next peak.  Pichai is undeterred. “Looking ahead, we do see a lot of headroom,” he says. “I think multimodality will be big. As we teach these models to reason more, there will be bigger and bigger breakthroughs. Deeper breakthroughs are to come yet.  “When I take in the totality of it, I genuinely feel like we are at the very beginning.” Mat Honan contributed reporting.  ","Hype about Gemini , Google DeepMind ’ s long-rumored response to OpenAI ’ s GPT-4 , has been building for months . Today the company finally revealed what it has been working on in secret all this time . Was the hype justified ? Yes—and no . Gemini is Google ’ s biggest AI launch yet—its push to take on competitors OpenAI and Microsoft in the race for AI supremacy . There is no doubt that the model is pitched as best-in-class across a wide range of capabilities—an “ everything machine , ” as one observer puts it . In an in-depth interview , Pichai predicts : “ This will be one of the biggest things we all grapple with for the next decade. ” “ The model is innately more capable , ” Sundar Pichai , the CEO of Google and its parent company Alphabet , told MIT Technology Review . “ It ’ s a platform . AI is a profound platform shift , bigger than web or mobile . And so it represents a big step for us. ” It ’ s a big step for Google , but not necessarily a giant leap for the field as a whole . Google DeepMind claims that Gemini outmatches GPT-4 on 30 out of 32 standard measures of performance . And yet the margins between them are thin . What Google DeepMind has done is pull AI ’ s best current capabilities into one powerful package . To judge from demos , it does many things very well—but few things that we haven ’ t seen before . For all the buzz about the next big thing , Gemini could be a sign that we ’ ve reached peak AI hype . At least for now . Chirag Shah , a professor at the University of Washington who specializes in online search , compares the launch to Apple ’ s introduction of a new iPhone every year . “ Maybe we just have risen to a different threshold now , where this doesn ’ t impress us as much because we ’ ve just seen so much , ” he says . Like GPT-4 , Gemini is multimodal , meaning it is trained to handle multiple kinds of input : text , images , audio . It can combine these different formats to answer questions about everything from household chores to college math to economics . In a demo for journalists yesterday , Google showed Gemini ’ s ability to take an existing screenshot of a chart , analyze hundreds of pages of research with new data , and then update the chart with that new information . In another example , Gemini is shown pictures of an omelet cooking in a pan and asked ( using speech , not text ) if the omelet is cooked yet . “ It ’ s not ready because the eggs are still runny , ” it replies . Most people will have to wait for the full experience , however . The version launched today is a back end to Bard , Google ’ s text-based search chatbot , which the company says will give it more advanced reasoning , planning , and understanding capabilities . Gemini ’ s full release will be staggered over the coming months . The new Gemini-boosted Bard will initially be available in English in more than 170 countries , not including the EU and the UK . This is to let the company “ engage ” with local regulators , says Sissie Hsiao , a Google vice president in charge of Bard . Gemini also comes in three sizes : Ultra , Pro and Nano . Ultra is the full-powered version ; Pro and Nano are tailored to applications that run with more limited computing resources . Nano is designed to run on devices , such as Google ’ s new Pixel phones . Developers and businesses will be able to access Gemini Pro starting December 13 . Gemini Ultra , the most powerful model , will be available “ early next year ” following “ extensive trust and safety checks , ” Google executives told reporters on a press call . “ I think of it as the Gemini era of models , ” Pichai told us . “ This is how Google DeepMind is going to build and make progress on AI . So it will always represent the frontier of where we are making progress on AI technology. ” OpenAI ’ s most powerful model , GPT-4 , is seen as the industry ’ s gold standard . While Google boasted that Gemini outperforms OpenAI ’ s previous model , GPT 3.5 , company executives dodged questions about how far the model exceeds GPT-4 . We got a first look at the much-anticipated big new language model from OpenAI . But this time how it works is even more deeply under wraps . But the firm highlights one benchmark in particular , called MMLU ( massive multitask language understanding ) . This is a set of tests designed to measure the performance of models on tasks involving text and images , including reading comprehension , college math , and multiple-choice quizzes in physics , economics , and social sciences . On the text-only questions , Gemini scores 90 % and human experts score approximately 89 % , says Pichai . GPT-4 scores 86 % on these types of questions . On the multimodal questions , Gemini scores 59 % , while GPT-4 scores 57 % . “ It ’ s the first model to cross that threshold , ” Pichai says . Gemini ’ s performance against benchmark data sets is very impressive , says Melanie Mitchell , an artificial-intelligence researcher at the Santa Fe Institute in New Mexico . “ It ’ s clear that Gemini is a very sophisticated AI system , ” says Mitchell . But “ it ’ s not obvious to me that Gemini is actually substantially more capable than GPT-4 , ” she adds . While the model has good benchmark scores , it is hard to know how to interpret these numbers given that we don ’ t know what ’ s in the training data , says Percy Liang , director of Stanford ’ s Center for Research on Foundation Models . Mitchell also notes that Gemini performs much better on language and code benchmarks than on images and video . “ Multimodal foundation models still have a ways to go to be generally and robustly useful for many tasks , ” she says . Using feedback from human testers , Google DeepMind has trained Gemini to be more factually accurate , to give attribution when asked to , and to hedge rather than spit out nonsense when faced with a question it can not answer . The company claims that this mitigates the problem of hallucinations . But without a radical overhaul of the base technology , large language models will continue to make things up . Experts say it ’ s unclear whether the benchmarks Google is using to measure Gemini ’ s performance offer that much insight , and without transparency , it ’ s hard to check Google ’ s claims . “ Google is advertising Gemini as an everything machine—a general-purpose model that can be used in many different ways , ” says Emily Bender , a professor of computational linguistics at the University of Washington . But the company is using narrow benchmarks to evaluate models that it expects to be used for these diverse purposes . “ This means it effectively can ’ t be thoroughly evaluated , ” she says . Ultimately , for the average user , the incremental improvement over competing models might not make much difference , says Shah . “ It ’ s more about convenience , brand recognition , existing integration , than people really thinking ‘ Oh , this is better , ’ ” he says . Gemini has been a long time coming . In April 2023 , Google announced it was merging its AI research unit Google Brain with DeepMind , Alphabet ’ s London-based AI research lab . So Google has had all year to develop its answer to OpenAI ’ s most advanced large language model , GPT-4 , which debuted in March and is the backbone of the paid version of ChatGPT . With hopes and fears about the technology running wild , it 's time to agree on what it can and ca n't do . Google has been under intense pressure to show investors it can match and overtake competitors in AI . Although the company has been developing and using powerful AI models for years , it has been hesitant to launch tools that the public can play with for fears of reputational damage and safety concerns . “ Google has been very cautious about releasing this stuff to the public , ” Geoffrey Hinton told MIT Technology Review in April when he left the company . “ There are too many bad things that could happen , and Google didn ’ t want to ruin its reputation. ” Faced with tech that seemed untrustworthy or unmarketable , Google played it safe—until the greater risk became missing out . Google has learned the hard way how launching flawed products can backfire . When it unveiled its ChatGPT competitor Bard in February , scientists soon noticed a factual error in the company ’ s own advertisement for the chatbot , an incident that subsequently wiped $ 100 billion off its share price . In May , Google announced it was rolling out generative AI into most of its products , from email to productivity software . But the results failed to impress critics : the chatbot made references to emails that didn ’ t exist , for example . This is a consistent problem with large language models . Although excellent at generating text that sounds like something a human could have written , generative AI systems regularly make things up . And that ’ s not the only problem with them . They are also easy to hack , and riddled with biases . Using them is also highly polluting . Google has solved neither these problems nor the hallucination issue . Its solution to the latter problem is a tool that lets people use Google search to double-check the chatbot ’ s answers , but that relies on the accuracy of the online search results themselves . Gemini may be the pinnacle of this wave of generative AI . But it ’ s not clear where AI built on large language models goes next . Some researchers believe this could be a plateau rather than the foot of the next peak . Pichai is undeterred . “ Looking ahead , we do see a lot of headroom , ” he says . “ I think multimodality will be big . As we teach these models to reason more , there will be bigger and bigger breakthroughs . Deeper breakthroughs are to come yet . “ When I take in the totality of it , I genuinely feel like we are at the very beginning. ” Mat Honan contributed reporting .","['hype', 'longrumore', 'response', 'build', 'month', 'today', 'company', 'finally', 'reveal', 'work', 'secret', 'time', 'hype', 'justify', 'gemini', 'big', 'ai', 'launch', 'yet', 'push', 'take', 'competitor', 'race', 'supremacy', 'doubt', 'model', 'pitch', 'bestinclass', 'wide', 'range', 'capability', 'machine', 'observer', 'put', 'indepth', 'interview', 'pichai', 'predict', 'big', 'thing', 'grapple', 'next', 'decade', 'model', 'innately', 'capable', 'sundar', 'ceo', 'parent', 'company', 'alphabet', 'tell', 'mit', 'technology', 'review', 'platform', 'ai', 'profound', 'platform', 'shift', 'big', 'web', 'mobile', 'represent', 'big', 'step', 'big', 'step', 'necessarily', 'giant', 'leap', 'field', 'whole', 'deepmind', 'claim', 'gemini', 'outmatch', 'gpt4', 'standard', 'measure', 'performance', 'yet', 'margin', 'thin', 'deepmind', 'pull', 'ai', 'good', 'current', 'capability', 'powerful', 'package', 'judge', 'many', 'thing', 'well', 'thing', 'see', 'buzz', 'next', 'big', 'thing', 'gemini', 'sign', 'reach', 'peak', 'hype', 'least', 'shah', 'professor', 'specialize', 'online', 'search', 'compare', 'launch', 'introduction', 'new', 'iphone', 'year', 'maybe', 'rise', 'different', 'threshold', 'impress', 'much', 'see', 'much', 'say', 'gpt4', 'gemini', 'multimodal', 'mean', 'train', 'handle', 'multiple', 'kind', 'input', 'text', 'image', 'audio', 'combine', 'different', 'format', 'answer', 'question', 'household', 'chore', 'college', 'math', 'economic', 'demo', 'journalist', 'yesterday', 'show', 'ability', 'take', 'exist', 'screenshot', 'chart', 'analyze', 'hundred', 'page', 'research', 'new', 'datum', 'update', 'chart', 'new', 'information', 'example', 'gemini', 'show', 'picture', 'omelet', 'cooking', 'pan', 'ask', 'use', 'speech', 'text', 'omelet', 'cook', 'yet', 'ready', 'egg', 'still', 'runny', 'reply', 'people', 'wait', 'full', 'experience', 'however', 'version', 'launch', 'today', 'back', 'end', 'bard', 'textbase', 'search', 'chatbot', 'company', 'say', 'give', 'advanced', 'reasoning', 'planning', 'understand', 'capability', 'gemini', 'full', 'release', 'stagger', 'come', 'month', 'new', 'geminibooste', 'bard', 'initially', 'available', 'country', 'include', 'let', 'company', 'engage', 'local', 'regulator', 'say', 'vice', 'president', 'charge', 'gemini', 'also', 'come', 'size', 'ultra', 'pro', 'fullpowere', 'version', 'pro', 'tailor', 'application', 'run', 'limited', 'computing', 'resource', 'design', 'run', 'device', 'new', 'pixel', 'phone', 'developer', 'business', 'able', 'access', 'gemini', 'pro', 'start', 'gemini', 'ultra', 'powerful', 'model', 'available', 'early', 'next', 'year', 'follow', 'extensive', 'trust', 'safety', 'check', 'executive', 'tell', 'reporter', 'press', 'call', 'think', 'era', 'model', 'tell', 'deepmind', 'go', 'build', 'make', 'progress', 'ai', 'always', 'represent', 'frontier', 'make', 'progress', 'technology', 'powerful', 'model', 'see', 'industry', 'gold', 'standard', 'boast', 'outperform', 'previous', 'model', 'company', 'executive', 'dodge', 'question', 'far', 'model', 'exceed', 'get', 'first', 'look', 'muchanticipate', 'big', 'new', 'language', 'model', 'time', 'work', 'even', 'deeply', 'wrap', 'firm', 'highlight', 'benchmark', 'particular', 'call', 'mmlu', 'massive', 'multitask', 'language', 'understand', 'set', 'test', 'design', 'measure', 'performance', 'model', 'task', 'involve', 'text', 'image', 'include', 'read', 'comprehension', 'college', 'math', 'multiplechoice', 'quiz', 'physics', 'economic', 'social', 'science', 'textonly', 'question', 'gemini', 'score', 'human', 'expert', 'score', 'approximately', 'say', 'score', 'type', 'question', 'multimodal', 'question', 'gemini', 'score', 'gpt4', 'score', 'first', 'model', 'cross', 'threshold', 'say', 'performance', 'benchmark', 'data', 'set', 'impressive', 'say', 'artificialintelligence', 'researcher', 'clear', 'gemini', 'sophisticated', 'ai', 'system', 'say', 'mitchell', 'obvious', 'gemini', 'actually', 'substantially', 'capable', 'add', 'model', 'good', 'benchmark', 'score', 'hard', 'know', 'interpret', 'number', 'give', 'know', 'training', 'datum', 'say', 'director', 'center', 'research', 'foundation', 'model', 'also', 'note', 'gemini', 'perform', 'much', 'well', 'language', 'code', 'benchmark', 'image', 'video', 'multimodal', 'foundation', 'model', 'still', 'way', 'go', 'generally', 'robustly', 'useful', 'many', 'task', 'say', 'use', 'feedback', 'human', 'tester', 'train', 'gemini', 'factually', 'accurate', 'give', 'attribution', 'ask', 'hedge', 'rather', 'spit', 'nonsense', 'face', 'question', 'answer', 'company', 'claim', 'mitigate', 'problem', 'hallucination', 'radical', 'overhaul', 'base', 'technology', 'large', 'language', 'model', 'continue', 'make', 'thing', 'expert', 'say', 'unclear', 'benchmark', 'use', 'measure', 'performance', 'offer', 'much', 'insight', 'transparency', 'hard', 'check', 'claim', 'advertise', 'gemini', 'machine', 'generalpurpose', 'model', 'use', 'many', 'different', 'way', 'say', 'professor', 'computational', 'linguistic', 'company', 'use', 'narrow', 'benchmark', 'evaluate', 'model', 'expect', 'use', 'diverse', 'purpose', 'mean', 'effectively', 'thoroughly', 'evaluate', 'say', 'ultimately', 'average', 'user', 'incremental', 'improvement', 'compete', 'model', 'make', 'much', 'difference', 'say', 'shah', 'convenience', 'brand', 'recognition', 'exist', 'integration', 'people', 'really', 'think', 'well', 'say', 'long', 'time', 'come', 'announce', 'merge', 'research', 'unit', 'deepmind', 'alphabet', 'londonbase', 'ai', 'research', 'lab', 'year', 'develop', 'answer', 'advanced', 'large', 'language', 'model', 'debut', 'backbone', 'pay', 'version', 'chatgpt', 'hope', 'fear', 'technology', 'run', 'wild', 'time', 'agree', 'intense', 'pressure', 'show', 'investor', 'match', 'overtake', 'competitor', 'ai', 'company', 'develop', 'use', 'powerful', 'model', 'year', 'hesitant', 'launch', 'tool', 'public', 'play', 'fear', 'reputational', 'damage', 'safety', 'concern', 'cautious', 'release', 'stuff', 'public', 'tell', 'technology', 'review', 'leave', 'company', 'many', 'bad', 'thing', 'happen', 'want', 'ruin', 'reputation', 'face', 'tech', 'seem', 'untrustworthy', 'unmarketable', 'play', 'safe', 'great', 'risk', 'miss', 'learn', 'hard', 'way', 'launch', 'flawed', 'product', 'backfire', 'unveil', 'chatgpt', 'competitor', 'bard', 'scientist', 'soon', 'notice', 'factual', 'error', 'company', 'advertisement', 'chatbot', 'incident', 'subsequently', 'wipe', 'share', 'price', 'announce', 'roll', 'generative', 'ai', 'product', 'email', 'productivity', 'software', 'result', 'fail', 'impress', 'critic', 'chatbot', 'make', 'reference', 'email', 'exist', 'example', 'consistent', 'problem', 'large', 'language', 'model', 'excellent', 'generate', 'text', 'sound', 'human', 'write', 'generative', 'ai', 'system', 'regularly', 'make', 'thing', 'problem', 'also', 'easy', 'hack', 'riddle', 'bias', 'use', 'also', 'highly', 'pollute', 'solve', 'problem', 'hallucination', 'issue', 'solution', 'latter', 'problem', 'tool', 'let', 'people', 'use', 'google', 'search', 'doublecheck', 'chatbot', 'answer', 'rely', 'accuracy', 'online', 'search', 'result', 'gemini', 'pinnacle', 'wave', 'generative', 'ai', 'clear', 'build', 'large', 'language', 'model', 'go', 'next', 'researcher', 'believe', 'plateau', 'rather', 'foot', 'next', 'peak', 'undeterred', 'look', 'ahead', 'see', 'lot', 'headroom', 'say', 'think', 'multimodality', 'big', 'teach', 'model', 'reason', 'big', 'big', 'breakthrough', 'deep', 'breakthrough', 'come', 'yet', 'take', 'totality', 'genuinely', 'feel', 'beginning', 'contribute', 'report']"
How AI assistants are already changing the way code gets made,https://www.technologyreview.com/2023/12/06/1084457/ai-assistants-copilot-changing-code-software-development-github-openai/,2023-12-06,"<p>AI coding assistants are here to stay—but just how big a difference they make is still unclear.</p>
","Two weeks into the coding class he was teaching at Duke University in North Carolina this spring, Noah Gift told his students to throw out the course materials he’d given them. Instead of working with Python, one of the most popular entry-level programming languages, the students would now be using Rust, a language that was newer, more powerful, and much harder to learn. Gift, a software developer with 25 years of experience, had only just learned Rust himself. But he was confident his students would be fine with the last-minute switch-up. That’s because they’d also each get a special new sidekick: an AI tool called Copilot, a turbocharged autocomplete for computer code, built on top of OpenAI’s latest large language models, GPT-3.5 and GPT-4. Copilot is made by GitHub, a firm that runs an online software development platform used by more than 100 million programmers. The tool monitors every keystroke you make, predicts what you are trying to do on the fly, and offers up a nonstop stream of code snippets you could use to do it. Gift, who had been told about Copilot by someone he knew at GitHub’s parent company, Microsoft, saw its potential at once. “There’s no way I could have learned Rust as quickly as I did without Copilot,” he says. “I basically had a supersmart assistant next to me that could answer my questions while I tried to level up. It was pretty obvious to me that we should start using it in class.” Gift isn't alone. Ask a room of computer science students or programmers if they use Copilot, and many now raise a hand. All the people interviewed for this article said they used Copilot themselves—even those who pointed out problems with the tool. Like ChatGPT with education, Copilot is upending an entire profession by giving people new ways to perform old tasks. Packaged as a paid-for plug-in for Microsoft’s Visual Studio software (a kind of industry-standard multi-tool for writing, debugging, and deploying code), Copilot is the slickest version of this tech. But it’s not the only tool available to coders. In August, Meta released a free code-generation model called Code Llama, based on Llama 2, Meta’s answer to GPT-4. The same month, Stability AI—the firm behind the image-making model Stable Diffusion—put out StableCode. And, of course, there’s ChatGPT, which OpenAI has pitched from the start as a chatbot that can help write and debug code. Exclusive conversations that take us behind the scenes of a cultural phenomenon. “It’s the first time that machine-learning models have been really useful for a lot of people,” says Gabriel Synnaeve, who led the team behind Code Llama at Meta. “It’s not just nerding out—it’s actually useful.” With Microsoft and Google about to stir similar generative models into office software used by billions around the world (Microsoft has started using Copilot as a brand name across Office 365), it’s worth asking exactly what these tools do for programmers. How are they changing the basics of a decades-old job? Will they help programmers make more and better software? Or will they get bogged down in legal fights over IP and copyright?  On the surface, writing code involves typing statements and instructions in some programming language into a text file. This then gets translated into machine code that a computer can run—a level up from the 0s and 1s of binary. In practice, programmers also spend a lot of time googling, looking up workarounds for common problems or skimming online forums for faster ways to write an algorithm. Existing chunks of prewritten code then get repurposed, and new software often comes together like a collage.  But these look-ups take time and let programmers out of the flow of converting thoughts into code, says Thomas Dohmke, GitHub’s CEO: “You’ve got a lot of tabs open, you’re planning a vacation, maybe you’re reading the news. At last you copy the text you need and go back to your code, but it’s 20 minutes later and you lost the flow.” The key idea behind Copilot and other programs like it, sometimes called code assistants, is to put the information that programmers need right next to the code they are writing. The tool tracks the code and comments (descriptions or notes written in natural language) in the file that a programmer is working on, as well as other files that it links to or that have been edited in the same project, and sends all this text to the large language model behind Copilot as a prompt. (GitHub co-developed Copilot's model, called Codex, with OpenAI. It is a large language model fine-tuned on code.) Copilot then predicts what the programmer is trying to do and suggests code to do it. This round trip between code and Codex happens multiple times a second, the prompt updating as the programmer types. At any moment, the programmer can accept what Copilot suggests by hitting the tab key, or ignore it and carry on typing.  The tab button seems to get hit a lot. A study of almost a million Copilot users published by GitHub and the consulting firm Keystone Strategy in June—a year after the tool’s general release—found that programmers accepted on average around 30% of its suggestions, according to GitHub’s user data.  “In the last year Copilot has suggested—and had okayed by developers—more than a billion lines of code,” says Dohmke. “Out there, running inside computers, is code generated by a stochastic parrot.” Copilot has changed the basic skills of coding. As with ChatGPT or image makers like Stable Diffusion, the tool’s output is often not exactly what’s wanted—but it can be close. “Maybe it’s correct, maybe it’s not—but it’s a good start,” says Arghavan Moradi Dakhel, a researcher at Polytechnique Montréal in Canada who studies the use of machine-learning tools in software development. Programming becomes prompting: rather than coming up with code from scratch, the work involves tweaking half-formed code and nudging a large language model to produce something more on point.  But Copilot isn’t everywhere yet. Some firms, including Apple, have asked employees not to use it, wary of leaking IP and other private data to competitors. For Justin Gottschlich, CEO of Merly, a startup that uses AI to analyze code across large software projects, that will always be a deal-breaker: “If I’m Google or Intel and my IP is my source code, I’m never going to use it,” he says. “Why don’t I just send you all my trade secrets too? It’s just put-your-pants-on-before-you-leave-the-house kind of obvious.” Dohmke is aware this is a turn-off for key customers and says that the firm is working on a version of Copilot that businesses can run in-house, so that code isn’t sent to Microsoft’s servers.  Copilot is also at the center of a lawsuit filed by programmers unhappy that their code was used to train the models behind it without their consent. Microsoft has offered indemnity to users of its models who are wary of potential litigation. But the legal issues will take years to play out in the courts. Dohmke is bullish, confident that the pros outweigh the cons: “We will adjust to whatever US, UK, or European lawmakers tell us to do,” he says. “But there is a middle balance here between protecting rights—and protecting privacy—and us as humanity making a step forward.” That’s the kind of fighting talk you’d expect from a CEO. But this is new, uncharted territory. If nothing else, GitHub is leading a brazen experiment that could pave the way for a wider range of AI-powered professional assistants.  GitHub started working on Copilot in June 2020, soon after OpenAI released GPT-3. Programmers have always been on the lookout for shortcuts and speedups. “It’s part of the DNA of being a software developer,” says Dohmke. “We wanted to solve this problem of boilerplate code—can we generate code that is no fun to write but takes up time?” The narrative around cheating students doesn’t tell the whole story. Meet the teachers who think generative AI could actually make learning better. The first sign they were onto something came when they asked programmers at the company to submit coding tests that they might ask somebody at a job interview: “Here’s some code—finish it off.” GitHub gave these to an early version of the tool and let it try each test 150 times. Given that many attempts, they found that the tool could solve 92% of them. They tried again with 50,000 problems taken from GitHub’s online platform, and the tool solved just over half of them. “That gave us confidence that we could build what ultimately became Copilot,” says Dohmke. In 2023, a team of GitHub and Microsoft researchers tested the impact of Copilot on programmers in a small study. They asked 95 people to build a web server (a non-trivial task, but one involving the kind of common, boilerplate code that Dohmke refers to) and gave half access to Copilot. Those using Copilot completed the task on average 55% faster.  A powerful AI that replaces the need for googling is useful—but is it a game changer? Opinion is split. “The way that I would think about it is that you have an experienced developer sitting next to you whispering recommendations,” says Marco Iansiti, a Keystone Strategy cofounder and a professor at Harvard Business School, where he studies digital transformation. “You used to have to look things up on your own, and now—whammo—here comes the suggestion automatically.” Gottschlich, who has been working on automatic code generation for years, is less impressed. “To be frank, code assistants are fairly uninteresting in the larger scheme of things,” he says, referring to the new wave of tools based on large language models, like Copilot. “They are principally bound by what the human programmer is capable of doing. They’ll never likely at this stage be able to do something miraculous beyond what the human programmer is doing.” Gottschlich, who claims that Merly’s tech finds bugs in code and fixes them by itself (but who doesn’t shed light on how that works), is thinking bigger. He sees AI one day taking on the management of vast and complex libraries of code, directing human engineers in how to maintain it. But he doesn’t think large language models are the right tech for that job. Even so, small changes to a task that millions of people do all the time can add up fast. Iansiti, for example, makes a huge claim: he believes that the impact of Copilot—and tools like it—could add $1.5 trillion to the global economy by 2030. “It’s more of a back-of-the-envelope thing, not really an academic estimate, but it could be a lot larger than that as well,” he says. “There’s so much stuff that hinges on software. If you move the needle on how software development really works, it will have an infinite impact on the economy.” For Iansiti, it’s not just about getting existing developers to produce more code. He argues that these tools will increase the demand for programmers because companies could get more code for less money. At the same time, there will be more coders because these tools lower the barrier to entry. “We’re going to see an expansion in who can contribute to software development,” he says. Or as Idan Gazit, GitHub’s senior director of research, puts it: imagine if anyone who picked up a guitar could play a basic tune straight away. There would be a lot more guitar players and a lot more music. Many agree that Copilot makes it easier to pick up programming—as Gift found. “Rust has got a reputation for being a very difficult language,” he says. “But I was pleasantly shocked at how well the students did and at the projects they built—how complex and useful they were.” Gift says they were able to build complete web apps with chatbots in them. Not everyone was happy with Gift’s syllabus change, however. He says that one of his teaching assistants told new students not to use Copilot because it was a crutch that would stop them from learning properly. Gift accepts that Copilot is like training wheels that you might not want to take off. But he doesn’t think that’s a problem: “What are we trying to do? We’re trying to build complex systems.” And to do that, he argues, programmers should use whatever tools are available. It is true that the history of computing has seen programmers rely on more and more layers of software between themselves and the machine code that computers can run. They have gone from punch cards and assembly code to programming languages like Python that are relatively easy to read and write. That’s possible because such languages get translated into machine code by software called compilers. “When I started coding in the ’80s and ’90s, you still had to know how a CPU worked,” says Dohmke. “Now when you write a web application, you almost never think about the CPU or the web server.” Add in a long list of bug-catching and code-testing tools, and programmers are used to a large amount of automated support. In many ways, Copilot and others are just the latest wave. “I used Python for 25 years because it was written to be readable by humans,” says Gift. “In my opinion, that doesn’t matter anymore.”  But he points out that Copilot isn’t a free pass. “Copilot reflects your ability,” he says. “It lifts everyone up a little bit, but if you’re a poor programmer you’ll still have weaknesses.” Work to be done A big problem with assessing the true impact of such tools is that most of the data is still anecdotal. GitHub’s study showed that programmers were accepting 30% of suggestions (“30% is out of this world in any kind of industry scenario,” says Dohmke), but it is not clear why the programmers accepted those suggestions and rejected others. The same study also revealed that less experienced programmers accepted more suggestions and that programmers accepted more suggestions as they grew used to the tool—but, again, not why. “We need to go a lot deeper to understand what that means,” says Iansiti. “There’s work to be done to really get a sense of how the coding process itself is developing, and that work is all TBD.” Most independent studies of tools like Copilot have focused on the correctness of the code that they suggest. Like all large language models, these tools can produce nonsense. With code it can be hard to tell—especially for less experienced users, who also seem to rely on Copilot the most. Several teams of researchers in the last couple of years have found that Copilot can insert bugs or security flaws into code. GitHub has been busy improving the accuracy of Copilot’s suggestions. It claims that the latest version of the tool runs code through a second model trained to filter out common security bugs before making a suggestion to users. But there are other quality issues beyond bugs, says Dakhel. She and her colleagues have found that Copilot can suggest code that is overly complex or doesn’t adhere to what professionals consider best practices, which is a problem because complex or unclear code is harder for other people to read, check, and extend. The problem is that models are only as good as their training data. And Copilot’s models were trained on a vast library of code taken from GitHub’s online repository, which goes back 15 years. This code contains not only bugs but also security flaws that were not known about when the code was written. Add to this the fact that inexperienced programmers use the tool more than experienced ones, and it could make more work for software development teams in the long run, says Dakhel. Expert programmers may have to spend more time double-checking the code put through by non-experts. Dakhel now hopes to study the gap between expert and non-expert programmers more fully. Before Copilot was released, she and her colleagues were using machine learning to detect expert programmers by their code. But Copilot messed with her data because now it was harder to tell whether code had been written by an expert programmer or a less experienced one with AI help. Now, having played around with Copilot herself, she plans to use her approach to study what kind of boost it gives. “I’m curious to know if junior developers using such a tool will be predicted to be expert developers or if it’s still detectable that they are junior developers,” she says. “It could be a way of measuring how big a level up these tools give people.""  Ultimately, we might not have to wait long before the jury is in. Software development is one of the most well documented and thoroughly measured of business activities. If Copilot works, it will get used. If it doesn’t, it won’t. In the meantime, these tools are getting better all the time. Yet it is worth noting that programming—typing text onto a screen—is a small part of the overall job of software development. It involves managing multiple parts of a complex puzzle, including designing the code, testing it, and deploying it. Copilot, like many programs before it, can make parts of that job faster, but it won’t reinvent it completely. “There’s always going to be programmers,” says Synnaeve. “They will get a lot of help, but in the end what matters is understanding which problems need solving. To do that really well and translate that into a program—that’s the job of programmers.” ","Two weeks into the coding class he was teaching at Duke University in North Carolina this spring , Noah Gift told his students to throw out the course materials he ’ d given them . Instead of working with Python , one of the most popular entry-level programming languages , the students would now be using Rust , a language that was newer , more powerful , and much harder to learn . Gift , a software developer with 25 years of experience , had only just learned Rust himself . But he was confident his students would be fine with the last-minute switch-up . That ’ s because they ’ d also each get a special new sidekick : an AI tool called Copilot , a turbocharged autocomplete for computer code , built on top of OpenAI ’ s latest large language models , GPT-3.5 and GPT-4 . Copilot is made by GitHub , a firm that runs an online software development platform used by more than 100 million programmers . The tool monitors every keystroke you make , predicts what you are trying to do on the fly , and offers up a nonstop stream of code snippets you could use to do it . Gift , who had been told about Copilot by someone he knew at GitHub ’ s parent company , Microsoft , saw its potential at once . “ There ’ s no way I could have learned Rust as quickly as I did without Copilot , ” he says . “ I basically had a supersmart assistant next to me that could answer my questions while I tried to level up . It was pretty obvious to me that we should start using it in class. ” Gift is n't alone . Ask a room of computer science students or programmers if they use Copilot , and many now raise a hand . All the people interviewed for this article said they used Copilot themselves—even those who pointed out problems with the tool . Like ChatGPT with education , Copilot is upending an entire profession by giving people new ways to perform old tasks . Packaged as a paid-for plug-in for Microsoft ’ s Visual Studio software ( a kind of industry-standard multi-tool for writing , debugging , and deploying code ) , Copilot is the slickest version of this tech . But it ’ s not the only tool available to coders . In August , Meta released a free code-generation model called Code Llama , based on Llama 2 , Meta ’ s answer to GPT-4 . The same month , Stability AI—the firm behind the image-making model Stable Diffusion—put out StableCode . And , of course , there ’ s ChatGPT , which OpenAI has pitched from the start as a chatbot that can help write and debug code . Exclusive conversations that take us behind the scenes of a cultural phenomenon . “ It ’ s the first time that machine-learning models have been really useful for a lot of people , ” says Gabriel Synnaeve , who led the team behind Code Llama at Meta . “ It ’ s not just nerding out—it ’ s actually useful. ” With Microsoft and Google about to stir similar generative models into office software used by billions around the world ( Microsoft has started using Copilot as a brand name across Office 365 ) , it ’ s worth asking exactly what these tools do for programmers . How are they changing the basics of a decades-old job ? Will they help programmers make more and better software ? Or will they get bogged down in legal fights over IP and copyright ? On the surface , writing code involves typing statements and instructions in some programming language into a text file . This then gets translated into machine code that a computer can run—a level up from the 0s and 1s of binary . In practice , programmers also spend a lot of time googling , looking up workarounds for common problems or skimming online forums for faster ways to write an algorithm . Existing chunks of prewritten code then get repurposed , and new software often comes together like a collage . But these look-ups take time and let programmers out of the flow of converting thoughts into code , says Thomas Dohmke , GitHub ’ s CEO : “ You ’ ve got a lot of tabs open , you ’ re planning a vacation , maybe you ’ re reading the news . At last you copy the text you need and go back to your code , but it ’ s 20 minutes later and you lost the flow. ” The key idea behind Copilot and other programs like it , sometimes called code assistants , is to put the information that programmers need right next to the code they are writing . The tool tracks the code and comments ( descriptions or notes written in natural language ) in the file that a programmer is working on , as well as other files that it links to or that have been edited in the same project , and sends all this text to the large language model behind Copilot as a prompt . ( GitHub co-developed Copilot 's model , called Codex , with OpenAI . It is a large language model fine-tuned on code . ) Copilot then predicts what the programmer is trying to do and suggests code to do it . This round trip between code and Codex happens multiple times a second , the prompt updating as the programmer types . At any moment , the programmer can accept what Copilot suggests by hitting the tab key , or ignore it and carry on typing . The tab button seems to get hit a lot . A study of almost a million Copilot users published by GitHub and the consulting firm Keystone Strategy in June—a year after the tool ’ s general release—found that programmers accepted on average around 30 % of its suggestions , according to GitHub ’ s user data . “ In the last year Copilot has suggested—and had okayed by developers—more than a billion lines of code , ” says Dohmke . “ Out there , running inside computers , is code generated by a stochastic parrot. ” Copilot has changed the basic skills of coding . As with ChatGPT or image makers like Stable Diffusion , the tool ’ s output is often not exactly what ’ s wanted—but it can be close . “ Maybe it ’ s correct , maybe it ’ s not—but it ’ s a good start , ” says Arghavan Moradi Dakhel , a researcher at Polytechnique Montréal in Canada who studies the use of machine-learning tools in software development . Programming becomes prompting : rather than coming up with code from scratch , the work involves tweaking half-formed code and nudging a large language model to produce something more on point . But Copilot isn ’ t everywhere yet . Some firms , including Apple , have asked employees not to use it , wary of leaking IP and other private data to competitors . For Justin Gottschlich , CEO of Merly , a startup that uses AI to analyze code across large software projects , that will always be a deal-breaker : “ If I ’ m Google or Intel and my IP is my source code , I ’ m never going to use it , ” he says . “ Why don ’ t I just send you all my trade secrets too ? It ’ s just kind of obvious. ” Dohmke is aware this is a turn-off for key customers and says that the firm is working on a version of Copilot that businesses can run in-house , so that code isn ’ t sent to Microsoft ’ s servers . Copilot is also at the center of a lawsuit filed by programmers unhappy that their code was used to train the models behind it without their consent . Microsoft has offered indemnity to users of its models who are wary of potential litigation . But the legal issues will take years to play out in the courts . Dohmke is bullish , confident that the pros outweigh the cons : “ We will adjust to whatever US , UK , or European lawmakers tell us to do , ” he says . “ But there is a middle balance here between protecting rights—and protecting privacy—and us as humanity making a step forward. ” That ’ s the kind of fighting talk you ’ d expect from a CEO . But this is new , uncharted territory . If nothing else , GitHub is leading a brazen experiment that could pave the way for a wider range of AI-powered professional assistants . GitHub started working on Copilot in June 2020 , soon after OpenAI released GPT-3 . Programmers have always been on the lookout for shortcuts and speedups . “ It ’ s part of the DNA of being a software developer , ” says Dohmke . “ We wanted to solve this problem of boilerplate code—can we generate code that is no fun to write but takes up time ? ” The narrative around cheating students doesn ’ t tell the whole story . Meet the teachers who think generative AI could actually make learning better . The first sign they were onto something came when they asked programmers at the company to submit coding tests that they might ask somebody at a job interview : “ Here ’ s some code—finish it off. ” GitHub gave these to an early version of the tool and let it try each test 150 times . Given that many attempts , they found that the tool could solve 92 % of them . They tried again with 50,000 problems taken from GitHub ’ s online platform , and the tool solved just over half of them . “ That gave us confidence that we could build what ultimately became Copilot , ” says Dohmke . In 2023 , a team of GitHub and Microsoft researchers tested the impact of Copilot on programmers in a small study . They asked 95 people to build a web server ( a non-trivial task , but one involving the kind of common , boilerplate code that Dohmke refers to ) and gave half access to Copilot . Those using Copilot completed the task on average 55 % faster . A powerful AI that replaces the need for googling is useful—but is it a game changer ? Opinion is split . “ The way that I would think about it is that you have an experienced developer sitting next to you whispering recommendations , ” says Marco Iansiti , a Keystone Strategy cofounder and a professor at Harvard Business School , where he studies digital transformation . “ You used to have to look things up on your own , and now—whammo—here comes the suggestion automatically. ” Gottschlich , who has been working on automatic code generation for years , is less impressed . “ To be frank , code assistants are fairly uninteresting in the larger scheme of things , ” he says , referring to the new wave of tools based on large language models , like Copilot . “ They are principally bound by what the human programmer is capable of doing . They ’ ll never likely at this stage be able to do something miraculous beyond what the human programmer is doing. ” Gottschlich , who claims that Merly ’ s tech finds bugs in code and fixes them by itself ( but who doesn ’ t shed light on how that works ) , is thinking bigger . He sees AI one day taking on the management of vast and complex libraries of code , directing human engineers in how to maintain it . But he doesn ’ t think large language models are the right tech for that job . Even so , small changes to a task that millions of people do all the time can add up fast . Iansiti , for example , makes a huge claim : he believes that the impact of Copilot—and tools like it—could add $ 1.5 trillion to the global economy by 2030 . “ It ’ s more of a back-of-the-envelope thing , not really an academic estimate , but it could be a lot larger than that as well , ” he says . “ There ’ s so much stuff that hinges on software . If you move the needle on how software development really works , it will have an infinite impact on the economy. ” For Iansiti , it ’ s not just about getting existing developers to produce more code . He argues that these tools will increase the demand for programmers because companies could get more code for less money . At the same time , there will be more coders because these tools lower the barrier to entry . “ We ’ re going to see an expansion in who can contribute to software development , ” he says . Or as Idan Gazit , GitHub ’ s senior director of research , puts it : imagine if anyone who picked up a guitar could play a basic tune straight away . There would be a lot more guitar players and a lot more music . Many agree that Copilot makes it easier to pick up programming—as Gift found . “ Rust has got a reputation for being a very difficult language , ” he says . “ But I was pleasantly shocked at how well the students did and at the projects they built—how complex and useful they were. ” Gift says they were able to build complete web apps with chatbots in them . Not everyone was happy with Gift ’ s syllabus change , however . He says that one of his teaching assistants told new students not to use Copilot because it was a crutch that would stop them from learning properly . Gift accepts that Copilot is like training wheels that you might not want to take off . But he doesn ’ t think that ’ s a problem : “ What are we trying to do ? We ’ re trying to build complex systems. ” And to do that , he argues , programmers should use whatever tools are available . It is true that the history of computing has seen programmers rely on more and more layers of software between themselves and the machine code that computers can run . They have gone from punch cards and assembly code to programming languages like Python that are relatively easy to read and write . That ’ s possible because such languages get translated into machine code by software called compilers . “ When I started coding in the ’ 80s and ’ 90s , you still had to know how a CPU worked , ” says Dohmke . “ Now when you write a web application , you almost never think about the CPU or the web server. ” Add in a long list of bug-catching and code-testing tools , and programmers are used to a large amount of automated support . In many ways , Copilot and others are just the latest wave . “ I used Python for 25 years because it was written to be readable by humans , ” says Gift . “ In my opinion , that doesn ’ t matter anymore. ” But he points out that Copilot isn ’ t a free pass . “ Copilot reflects your ability , ” he says . “ It lifts everyone up a little bit , but if you ’ re a poor programmer you ’ ll still have weaknesses. ” Work to be done A big problem with assessing the true impact of such tools is that most of the data is still anecdotal . GitHub ’ s study showed that programmers were accepting 30 % of suggestions ( “ 30 % is out of this world in any kind of industry scenario , ” says Dohmke ) , but it is not clear why the programmers accepted those suggestions and rejected others . The same study also revealed that less experienced programmers accepted more suggestions and that programmers accepted more suggestions as they grew used to the tool—but , again , not why . “ We need to go a lot deeper to understand what that means , ” says Iansiti . “ There ’ s work to be done to really get a sense of how the coding process itself is developing , and that work is all TBD. ” Most independent studies of tools like Copilot have focused on the correctness of the code that they suggest . Like all large language models , these tools can produce nonsense . With code it can be hard to tell—especially for less experienced users , who also seem to rely on Copilot the most . Several teams of researchers in the last couple of years have found that Copilot can insert bugs or security flaws into code . GitHub has been busy improving the accuracy of Copilot ’ s suggestions . It claims that the latest version of the tool runs code through a second model trained to filter out common security bugs before making a suggestion to users . But there are other quality issues beyond bugs , says Dakhel . She and her colleagues have found that Copilot can suggest code that is overly complex or doesn ’ t adhere to what professionals consider best practices , which is a problem because complex or unclear code is harder for other people to read , check , and extend . The problem is that models are only as good as their training data . And Copilot ’ s models were trained on a vast library of code taken from GitHub ’ s online repository , which goes back 15 years . This code contains not only bugs but also security flaws that were not known about when the code was written . Add to this the fact that inexperienced programmers use the tool more than experienced ones , and it could make more work for software development teams in the long run , says Dakhel . Expert programmers may have to spend more time double-checking the code put through by non-experts . Dakhel now hopes to study the gap between expert and non-expert programmers more fully . Before Copilot was released , she and her colleagues were using machine learning to detect expert programmers by their code . But Copilot messed with her data because now it was harder to tell whether code had been written by an expert programmer or a less experienced one with AI help . Now , having played around with Copilot herself , she plans to use her approach to study what kind of boost it gives . “ I ’ m curious to know if junior developers using such a tool will be predicted to be expert developers or if it ’ s still detectable that they are junior developers , ” she says . “ It could be a way of measuring how big a level up these tools give people . '' Ultimately , we might not have to wait long before the jury is in . Software development is one of the most well documented and thoroughly measured of business activities . If Copilot works , it will get used . If it doesn ’ t , it won ’ t . In the meantime , these tools are getting better all the time . Yet it is worth noting that programming—typing text onto a screen—is a small part of the overall job of software development . It involves managing multiple parts of a complex puzzle , including designing the code , testing it , and deploying it . Copilot , like many programs before it , can make parts of that job faster , but it won ’ t reinvent it completely . “ There ’ s always going to be programmers , ” says Synnaeve . “ They will get a lot of help , but in the end what matters is understanding which problems need solving . To do that really well and translate that into a program—that ’ s the job of programmers . ”","['week', 'code', 'class', 'teach', 'spring', 'gift', 'tell', 'student', 'throw', 'course', 'material', 'give', 'instead', 'work', 'popular', 'entrylevel', 'programming', 'language', 'student', 'use', 'rust', 'language', 'new', 'powerful', 'much', 'hard', 'learn', 'gift', 'software', 'developer', 'year', 'experience', 'learn', 'rust', 'confident', 'student', 'fine', 'lastminute', 'switchup', 'also', 'get', 'special', 'new', 'sidekick', 'ai', 'tool', 'call', 'copilot', 'turbocharge', 'autocomplete', 'computer', 'code', 'build', 'top', 'late', 'large', 'language', 'model', 'gpt35', 'gpt4', 'copilot', 'make', 'firm', 'run', 'online', 'software', 'development', 'platform', 'use', 'programmer', 'tool', 'monitor', 'keystroke', 'make', 'predict', 'try', 'fly', 'offer', 'nonstop', 'stream', 'code', 'snippet', 'use', 'gift', 'tell', 'copilot', 'know', 'parent', 'company', 'see', 'potential', 'way', 'learn', 'rust', 'quickly', 'copilot', 'say', 'basically', 'supersmart', 'assistant', 'next', 'answer', 'question', 'try', 'level', 'pretty', 'obvious', 'start', 'use', 'class', 'gift', 'alone', 'ask', 'room', 'computer', 'science', 'student', 'programmer', 'use', 'copilot', 'many', 'raise', 'hand', 'people', 'interview', 'article', 'say', 'use', 'copilot', 'even', 'point', 'problem', 'tool', 'chatgpt', 'education', 'copilot', 'upend', 'entire', 'profession', 'give', 'people', 'new', 'way', 'perform', 'old', 'task', 'package', 'paidfor', 'plugin', 'visual', 'studio', 'software', 'kind', 'industrystandard', 'multitool', 'write', 'debugging', 'deploy', 'code', 'copilot', 'slick', 'version', 'tech', 'tool', 'available', 'coder', 'meta', 'release', 'free', 'codegeneration', 'model', 'call', 'base', 'meta', 'answer', 'month', 'stability', 'ai', 'firm', 'imagemake', 'model', 'stable', 'diffusion', 'put', 'stablecode', 'course', 'chatgpt', 'openai', 'pitch', 'start', 'chatbot', 'help', 'write', 'debug', 'code', 'exclusive', 'conversation', 'take', 'scene', 'cultural', 'phenomenon', 'first', 'time', 'machinelearning', 'model', 'really', 'useful', 'lot', 'people', 'say', 'lead', 'team', 'nerde', 'actually', 'useful', 'stir', 'similar', 'generative', 'model', 'office', 'software', 'use', 'billion', 'world', 'start', 'use', 'copilot', 'brand', 'name', 'office', 'worth', 'ask', 'exactly', 'tool', 'programmer', 'change', 'basic', 'decadesold', 'job', 'help', 'programmer', 'make', 'well', 'software', 'bogge', 'legal', 'fight', 'ip', 'copyright', 'surface', 'writing', 'code', 'involve', 'type', 'statement', 'instruction', 'programming', 'language', 'text', 'file', 'translate', 'machine', 'code', 'computer', 'run', 'level', 'binary', 'practice', 'programmer', 'also', 'spend', 'lot', 'time', 'google', 'look', 'workaround', 'common', 'problem', 'skim', 'online', 'forum', 'fast', 'way', 'write', 'exist', 'chunk', 'prewritten', 'code', 'repurpose', 'new', 'software', 'often', 'come', 'together', 'collage', 'lookup', 'take', 'time', 'let', 'programmer', 'flow', 'convert', 'thought', 'code', 'say', 'ceo', 'get', 'lot', 'tab', 'open', 'plan', 'vacation', 'maybe', 'read', 'news', 'last', 'copy', 'text', 'need', 'go', 'back', 'code', 'minute', 'later', 'lose', 'flow', 'key', 'idea', 'copilot', 'program', 'sometimes', 'call', 'code', 'assistant', 'put', 'information', 'programmer', 'need', 'right', 'next', 'code', 'write', 'tool', 'track', 'code', 'comment', 'description', 'note', 'write', 'natural', 'language', 'file', 'programmer', 'work', 'well', 'file', 'link', 'edit', 'project', 'send', 'text', 'large', 'language', 'model', 'copilot', 'prompt', 'codevelope', 'copilot', 'model', 'call', 'codex', 'large', 'language', 'model', 'finetune', 'code', 'copilot', 'predict', 'programmer', 'try', 'suggest', 'code', 'round', 'trip', 'code', 'codex', 'happen', 'multiple', 'time', 'second', 'prompt', 'update', 'programmer', 'type', 'moment', 'programmer', 'accept', 'copilot', 'suggest', 'hit', 'tab', 'key', 'ignore', 'carry', 'type', 'tab', 'button', 'seem', 'hit', 'lot', 'study', 'almost', 'copilot', 'user', 'publish', 'consult', 'firm', 'keystone', 'strategy', 'year', 'tool', 'general', 'release', 'find', 'programmer', 'accept', 'average', 'around', 'suggestion', 'accord', 'user', 'datum', 'last', 'year', 'copilot', 'suggest', 'okay', 'developer', 'line', 'code', 'say', 'run', 'computer', 'code', 'generate', 'stochastic', 'parrot', 'copilot', 'change', 'basic', 'skill', 'code', 'chatgpt', 'image', 'maker', 'stable', 'diffusion', 'tool', 'output', 'often', 'exactly', 'want', 'close', 'maybe', 'correct', 'maybe', 'good', 'start', 'say', 'researcher', 'polytechnique', 'study', 'use', 'machinelearning', 'tool', 'software', 'development', 'programming', 'become', 'prompt', 'rather', 'come', 'code', 'scratch', 'work', 'involve', 'tweak', 'halfformed', 'code', 'nudge', 'large', 'language', 'model', 'produce', 'point', 'copilot', 'everywhere', 'yet', 'firm', 'include', 'apple', 'ask', 'employee', 'use', 'wary', 'leak', 'ip', 'private', 'datum', 'competitor', 'ceo', 'merly', 'startup', 'use', 'ai', 'analyze', 'code', 'large', 'software', 'project', 'always', 'dealbreaker', 'ip', 'source', 'code', 'never', 'go', 'use', 'say', 'send', 'trade', 'secret', 'kind', 'obvious', 'dohmke', 'aware', 'turnoff', 'key', 'customer', 'say', 'firm', 'work', 'version', 'copilot', 'business', 'run', 'inhouse', 'code', 'send', 'server', 'copilot', 'also', 'center', 'lawsuit', 'file', 'programmer', 'unhappy', 'code', 'use', 'train', 'model', 'consent', 'offer', 'indemnity', 'user', 'model', 'wary', 'potential', 'litigation', 'legal', 'issue', 'take', 'year', 'play', 'court', 'bullish', 'confident', 'pro', 'outweigh', 'con', 'adjust', 'european', 'lawmaker', 'tell', 'say', 'middle', 'balance', 'protect', 'right', 'protect', 'privacy', 'humanity', 'make', 'step', 'forward', 'kind', 'fight', 'talk', 'expect', 'ceo', 'new', 'uncharted', 'territory', 'else', 'lead', 'brazen', 'experiment', 'pave', 'way', 'wide', 'range', 'aipowere', 'professional', 'assistant', 'start', 'work', 'copilot', 'soon', 'release', 'gpt3', 'programmer', 'always', 'lookout', 'shortcut', 'speedup', 'part', 'dna', 'software', 'developer', 'say', 'want', 'solve', 'problem', 'generate', 'code', 'fun', 'write', 'take', 'time', 'narrative', 'cheat', 'student', 'tell', 'whole', 'story', 'meet', 'teacher', 'think', 'generative', 'ai', 'actually', 'make', 'learn', 'well', 'first', 'sign', 'come', 'ask', 'programmer', 'company', 'submit', 'code', 'test', 'ask', 'job', 'interview', 'code', 'finish', 'give', 'early', 'version', 'tool', 'let', 'try', 'test', 'time', 'give', 'many', 'attempt', 'find', 'tool', 'solve', 'try', 'problem', 'take', 'online', 'platform', 'tool', 'solve', 'half', 'give', 'confidence', 'build', 'ultimately', 'become', 'copilot', 'say', 'team', 'researcher', 'test', 'impact', 'copilot', 'programmer', 'small', 'study', 'ask', 'people', 'build', 'web', 'server', 'nontrivial', 'task', 'involve', 'kind', 'common', 'code', 'dohmke', 'refer', 'give', 'half', 'access', 'copilot', 'use', 'copilot', 'complete', 'task', 'average', 'fast', 'powerful', 'ai', 'replace', 'need', 'googling', 'useful', 'game', 'changer', 'opinion', 'split', 'way', 'think', 'experienced', 'developer', 'sit', 'next', 'whispering', 'recommendation', 'say', 'keystone', 'strategy', 'cofounder', 'professor', 'school', 'study', 'digital', 'transformation', 'use', 'look', 'thing', 'whammo', 'come', 'suggestion', 'automatically', 'gottschlich', 'work', 'automatic', 'code', 'generation', 'year', 'less', 'impressed', 'frank', 'code', 'assistant', 'fairly', 'uninteresting', 'large', 'scheme', 'thing', 'say', 'refer', 'new', 'wave', 'tool', 'base', 'large', 'language', 'model', 'copilot', 'principally', 'bind', 'human', 'programmer', 'capable', 'never', 'likely', 'stage', 'able', 'miraculous', 'human', 'programmer', 'gottschlich', 'claim', 'merly', 'tech', 'find', 'bug', 'code', 'fix', 'shed', 'light', 'work', 'think', 'big', 'see', 'day', 'take', 'management', 'vast', 'complex', 'library', 'code', 'direct', 'human', 'engineer', 'maintain', 'think', 'large', 'language', 'model', 'right', 'tech', 'job', 'even', 'small', 'change', 'task', 'million', 'people', 'time', 'add', 'fast', 'iansiti', 'example', 'make', 'huge', 'claim', 'believe', 'impact', 'copilot', 'tool', 'add', 'global', 'economy', 'backoftheenvelope', 'thing', 'really', 'academic', 'estimate', 'lot', 'large', 'well', 'say', 'much', 'stuff', 'hinge', 'software', 'move', 'needle', 'software', 'development', 'really', 'work', 'infinite', 'impact', 'economy', 'iansiti', 'get', 'exist', 'developer', 'produce', 'code', 'argue', 'tool', 'increase', 'demand', 'programmer', 'company', 'get', 'code', 'less', 'money', 'time', 'coder', 'tool', 'lower', 'barrier', 'entry', 'go', 'see', 'expansion', 'contribute', 'software', 'development', 'say', 'senior', 'director', 'research', 'put', 'imagine', 'pick', 'guitar', 'play', 'basic', 'tune', 'straight', 'away', 'lot', 'guitar', 'player', 'lot', 'music', 'many', 'agree', 'copilot', 'make', 'easy', 'pick', 'programming', 'gift', 'find', 'rust', 'get', 'reputation', 'difficult', 'language', 'say', 'pleasantly', 'shock', 'well', 'student', 'project', 'build', 'complex', 'useful', 'gift', 'say', 'able', 'build', 'complete', 'web', 'app', 'chatbot', 'happy', 'gift', 'syllabus', 'change', 'however', 'say', 'teaching', 'assistant', 'tell', 'new', 'student', 'use', 'copilot', 'crutch', 'stop', 'learn', 'properly', 'gift', 'accept', 'copilot', 'training', 'wheel', 'want', 'take', 'think', 'problem', 'try', 'try', 'build', 'complex', 'system', 'argue', 'programmer', 'use', 'tool', 'available', 'true', 'history', 'computing', 'see', 'programmer', 'rely', 'layer', 'software', 'machine', 'code', 'computer', 'run', 'go', 'punch', 'card', 'assembly', 'code', 'programming', 'language', 'python', 'relatively', 'easy', 'read', 'write', 'possible', 'language', 'translate', 'machine', 'code', 'software', 'call', 'compiler', 'start', 'code', '’', '80', '90', 'still', 'know', 'cpu', 'work', 'say', 'write', 'web', 'application', 'almost', 'never', 'think', 'cpu', 'web', 'server', 'add', 'long', 'list', 'bugcatching', 'codetesting', 'tool', 'programmer', 'use', 'large', 'amount', 'automate', 'support', 'many', 'way', 'copilot', 'late', 'wave', 'use', 'python', 'year', 'write', 'readable', 'human', 'say', 'gift', 'opinion', 'matter', 'anymore', 'point', 'copilot', 'free', 'pass', 'copilot', 'reflect', 'ability', 'say', 'lift', 'little', 'bit', 'poor', 'programmer', 'still', 'weakness', 'work', 'big', 'problem', 'assess', 'true', 'impact', 'tool', 'datum', 'still', 'anecdotal', 'study', 'show', 'programmer', 'accept', 'suggestion', 'world', 'kind', 'industry', 'scenario', 'say', 'clear', 'programmer', 'accept', 'suggestion', 'reject', 'study', 'also', 'reveal', 'less', 'experienced', 'programmer', 'accept', 'suggestion', 'programmer', 'accept', 'suggestion', 'grow', 'use', 'tool', 'need', 'go', 'lot', 'deep', 'understand', 'mean', 'say', 'iansiti', 'work', 'really', 'get', 'sense', 'code', 'process', 'develop', 'work', 'tbd', 'independent', 'study', 'tool', 'copilot', 'focus', 'correctness', 'code', 'suggest', 'large', 'language', 'model', 'tool', 'produce', 'nonsense', 'code', 'hard', 'tell', 'especially', 'less', 'experienced', 'user', 'also', 'seem', 'rely', 'copilot', 'several', 'team', 'researcher', 'last', 'couple', 'year', 'find', 'copilot', 'insert', 'bug', 'security', 'flaw', 'busy', 'improve', 'accuracy', 'copilot', 'suggestion', 'claim', 'late', 'version', 'tool', 'run', 'code', 'second', 'model', 'train', 'filter', 'common', 'security', 'bug', 'make', 'suggestion', 'user', 'quality', 'issue', 'bug', 'say', 'colleague', 'find', 'copilot', 'suggest', 'code', 'overly', 'complex', 'adhere', 'professional', 'consider', 'good', 'practice', 'problem', 'complex', 'unclear', 'code', 'hard', 'people', 'read', 'check', 'extend', 'problem', 'model', 'good', 'training', 'datum', 'copilot', 'model', 'train', 'vast', 'library', 'code', 'take', 'online', 'repository', 'go', 'back', 'year', 'code', 'contain', 'bug', 'also', 'security', 'flaw', 'know', 'code', 'write', 'add', 'fact', 'inexperienced', 'programmer', 'use', 'tool', 'experienced', 'one', 'make', 'work', 'software', 'development', 'team', 'long', 'run', 'say', 'expert', 'programmer', 'spend', 'time', 'doublechecke', 'code', 'put', 'nonexpert', 'hope', 'study', 'gap', 'expert', 'nonexpert', 'programmer', 'fully', 'copilot', 'release', 'colleague', 'use', 'machine', 'learning', 'detect', 'expert', 'programmer', 'code', 'copilot', 'mess', 'datum', 'hard', 'tell', 'code', 'write', 'expert', 'programmer', 'less', 'experienced', 'one', 'help', 'play', 'around', 'copilot', 'plan', 'use', 'approach', 'study', 'kind', 'boost', 'give', 'curious', 'know', 'junior', 'developer', 'use', 'tool', 'predict', 'expert', 'developer', 'still', 'detectable', 'junior', 'developer', 'say', 'way', 'measure', 'big', 'level', 'tool', 'give', 'people', 'ultimately', 'wait', 'long', 'jury', 'software', 'development', 'well', 'document', 'thoroughly', 'measure', 'business', 'activity', 'copilot', 'work', 'use', 'win', 'meantime', 'tool', 'get', 'well', 'time', 'yet', 'worth', 'note', 'programming', 'type', 'text', 'screen', 'small', 'part', 'overall', 'job', 'software', 'development', 'involve', 'manage', 'multiple', 'part', 'complex', 'puzzle', 'include', 'design', 'code', 'test', 'deploy', 'copilot', 'many', 'program', 'make', 'part', 'job', 'fast', 'win', 'reinvent', 'completely', 'always', 'go', 'programmer', 'say', 'synnaeve', 'get', 'lot', 'help', 'end', 'matter', 'understanding', 'problem', 'need', 'solve', 'really', 'well', 'translate', 'program', 'job', 'programmer']"
AI’s carbon footprint is bigger than you think,https://www.technologyreview.com/2023/12/05/1084417/ais-carbon-footprint-is-bigger-than-you-think/,2023-12-05,"<p>Generating one image takes as much energy as fully charging your smartphone.</p>
","This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here. World leaders are currently in Dubai for the UN COP28 climate talks. As 2023 is set to become the hottest year on record, this year’s meeting is a moment of reckoning for oil and gas companies. There is also renewed focus and enthusiasm on boosting cleantech startups. The stakes could not be higher.  But there’s one thing people aren’t talking enough about, and that’s the carbon footprint of AI. One part of the reason is that big tech companies don’t share the carbon footprint of training and using their massive models, and we don’t have standardized ways of measuring the emissions AI is responsible for. And while we know training AI models is highly polluting, the emissions attributable to using AI have been a missing piece so far. That is, until now.  I just published a story on new research that calculated the real carbon footprint of using generative AI models. Generating one image takes as much energy as fully charging your smartphone, according to the study from researchers at the AI startup Hugging Face and Carnegie Mellon University. This has big implications for the planet, because tech companies are integrating these powerful models into everything from online search to email, and they get used billions of times a day. If you want to know more, you can read the full story here.  Cutting-edge technology doesn’t have to harm the planet, and research like this is very important in helping us get concrete numbers about emissions. It will also help people understand that the cloud we think that AI models live on is actually very tangible, says Sasha Luccioni, an AI researcher at Hugging Face who led the work.  Once we have those numbers, we can start thinking about when using powerful models is actually necessary and when smaller, more nimble models might be more appropriate, she says.  Vijay Gadepally, a research scientist at the MIT Lincoln lab who did not participate in the research, has similar thoughts. Knowing the carbon footprint of each use of AI might make people more thoughtful about the way they use these models, he says.  Luccioni’s research also highlights how the emissions related to using AI will depend on where it’s being used, says Jesse Dodge, a research scientist at the Allen Institute for AI, who was not part of the study. The carbon footprint of AI in places where the power grid is relatively clean, such as France, will be much lower than it is in places with a grid that is heavily reliant on fossil fuels, such as some parts of the US. While the electricity consumed by running AI models is fixed, we might be able to reduce the overall carbon footprint of these models by running them in areas where the power grid consists of more renewable sources, he says.  While climate change is extremely anxiety inducing, it’s vital we better understand the tech sector’s effect on our planet. Studies like this one might help us come up with creative solutions that allow us to reap the benefits of AI while minimizing the harm.  After all, it’s hard to fix something you can’t measure.  Google DeepMind’s new AI tool helped create more than 700 new materials From EV batteries to solar cells to microchips, new materials can supercharge technological breakthroughs. But discovering them usually takes months or even years of trial-and-error research. A new tool from Google DeepMind uses deep learning to dramatically speed up the process of discovering new materials.  What’s the big deal: Called graphical networks for material exploration (GNoME), the technology has already been used to predict structures for 2.2 million new materials, of which more than 700 have gone on to be created in the lab and are now being tested. GNoME can be described as AlphaFold for materials discovery. Thanks to GNoME, the number of known stable materials has grown almost tenfold, to 421,000. Read more from June Kim here.  A high school’s deepfake porn scandal is pushing US lawmakers into actionLegislators are responding quickly after teens used AI to create nonconsensual sexually explicit images. (MIT Technology Review)  He wanted privacy. His college gave him none.This great investigation shows just how much college students are being subjected to increasing amounts of surveillance tech, including homework trackers, test-taking software, and even license plate readers. (The Markup) ChatGPT is leaking its secrets Two new stories show how vulnerable AI chatbots are to leaking data, putting personal and proprietary information at risk. The first story, by Wired, shows how easily OpenAI’s custom ChatGPT bots spill the initial instructions they were given when they were created. Another one, by 404 Media, shows how researchers at Google DeepMind were able to get a chatbot to reveal its data by asking it to repeat specific words over and over.  What it’s like being a prompt engineer earning $200K A fun story on the people paid six figures to get AI chatbots to do what they say. (The Wall Street Journal) ","This story originally appeared in The Algorithm , our weekly newsletter on AI . To get stories like this in your inbox first , sign up here . World leaders are currently in Dubai for the UN COP28 climate talks . As 2023 is set to become the hottest year on record , this year ’ s meeting is a moment of reckoning for oil and gas companies . There is also renewed focus and enthusiasm on boosting cleantech startups . The stakes could not be higher . But there ’ s one thing people aren ’ t talking enough about , and that ’ s the carbon footprint of AI . One part of the reason is that big tech companies don ’ t share the carbon footprint of training and using their massive models , and we don ’ t have standardized ways of measuring the emissions AI is responsible for . And while we know training AI models is highly polluting , the emissions attributable to using AI have been a missing piece so far . That is , until now . I just published a story on new research that calculated the real carbon footprint of using generative AI models . Generating one image takes as much energy as fully charging your smartphone , according to the study from researchers at the AI startup Hugging Face and Carnegie Mellon University . This has big implications for the planet , because tech companies are integrating these powerful models into everything from online search to email , and they get used billions of times a day . If you want to know more , you can read the full story here . Cutting-edge technology doesn ’ t have to harm the planet , and research like this is very important in helping us get concrete numbers about emissions . It will also help people understand that the cloud we think that AI models live on is actually very tangible , says Sasha Luccioni , an AI researcher at Hugging Face who led the work . Once we have those numbers , we can start thinking about when using powerful models is actually necessary and when smaller , more nimble models might be more appropriate , she says . Vijay Gadepally , a research scientist at the MIT Lincoln lab who did not participate in the research , has similar thoughts . Knowing the carbon footprint of each use of AI might make people more thoughtful about the way they use these models , he says . Luccioni ’ s research also highlights how the emissions related to using AI will depend on where it ’ s being used , says Jesse Dodge , a research scientist at the Allen Institute for AI , who was not part of the study . The carbon footprint of AI in places where the power grid is relatively clean , such as France , will be much lower than it is in places with a grid that is heavily reliant on fossil fuels , such as some parts of the US . While the electricity consumed by running AI models is fixed , we might be able to reduce the overall carbon footprint of these models by running them in areas where the power grid consists of more renewable sources , he says . While climate change is extremely anxiety inducing , it ’ s vital we better understand the tech sector ’ s effect on our planet . Studies like this one might help us come up with creative solutions that allow us to reap the benefits of AI while minimizing the harm . After all , it ’ s hard to fix something you can ’ t measure . Google DeepMind ’ s new AI tool helped create more than 700 new materials From EV batteries to solar cells to microchips , new materials can supercharge technological breakthroughs . But discovering them usually takes months or even years of trial-and-error research . A new tool from Google DeepMind uses deep learning to dramatically speed up the process of discovering new materials . What ’ s the big deal : Called graphical networks for material exploration ( GNoME ) , the technology has already been used to predict structures for 2.2 million new materials , of which more than 700 have gone on to be created in the lab and are now being tested . GNoME can be described as AlphaFold for materials discovery . Thanks to GNoME , the number of known stable materials has grown almost tenfold , to 421,000 . Read more from June Kim here . A high school ’ s deepfake porn scandal is pushing US lawmakers into actionLegislators are responding quickly after teens used AI to create nonconsensual sexually explicit images . ( MIT Technology Review ) He wanted privacy . His college gave him none.This great investigation shows just how much college students are being subjected to increasing amounts of surveillance tech , including homework trackers , test-taking software , and even license plate readers . ( The Markup ) ChatGPT is leaking its secrets Two new stories show how vulnerable AI chatbots are to leaking data , putting personal and proprietary information at risk . The first story , by Wired , shows how easily OpenAI ’ s custom ChatGPT bots spill the initial instructions they were given when they were created . Another one , by 404 Media , shows how researchers at Google DeepMind were able to get a chatbot to reveal its data by asking it to repeat specific words over and over . What it ’ s like being a prompt engineer earning $ 200K A fun story on the people paid six figures to get AI chatbots to do what they say . ( The Wall Street Journal )","['story', 'originally', 'appear', 'weekly', 'newsletter', 'ai', 'get', 'story', 'inbox', 'first', 'sign', 'world', 'leader', 'currently', 'dubai', 'climate', 'talk', 'set', 'become', 'hot', 'year', 'record', 'year', 'meeting', 'moment', 'reckon', 'oil', 'gas', 'company', 'also', 'renew', 'focus', 'enthusiasm', 'boost', 'cleantech', 'startup', 'stake', 'high', 'thing', 'people', 'talk', 'enough', 'carbon', 'footprint', 'ai', 'part', 'reason', 'big', 'tech', 'company', 'share', 'carbon', 'footprint', 'training', 'use', 'massive', 'model', 'standardize', 'way', 'measure', 'emission', 'ai', 'responsible', 'know', 'training', 'model', 'highly', 'pollute', 'emission', 'attributable', 'use', 'miss', 'piece', 'far', 'publish', 'story', 'new', 'research', 'calculate', 'real', 'carbon', 'footprint', 'use', 'generative', 'model', 'generate', 'image', 'take', 'much', 'energy', 'fully', 'charge', 'smartphone', 'accord', 'study', 'researcher', 'ai', 'hug', 'face', 'carnegie', 'big', 'implication', 'planet', 'tech', 'company', 'integrate', 'powerful', 'model', 'online', 'search', 'email', 'use', 'billion', 'time', 'day', 'want', 'know', 'read', 'full', 'story', 'cuttingedge', 'technology', 'harm', 'planet', 'research', 'important', 'help', 'get', 'concrete', 'number', 'emission', 'also', 'help', 'people', 'understand', 'cloud', 'think', 'ai', 'model', 'live', 'actually', 'tangible', 'say', 'researcher', 'hug', 'face', 'lead', 'work', 'number', 'start', 'think', 'use', 'powerful', 'model', 'actually', 'necessary', 'small', 'nimble', 'model', 'appropriate', 'say', 'gadepally', 'research', 'scientist', 'participate', 'research', 'similar', 'thought', 'know', 'carbon', 'footprint', 'use', 'ai', 'make', 'people', 'thoughtful', 'way', 'use', 'model', 'say', 'research', 'also', 'highlight', 'emission', 'relate', 'use', 'ai', 'depend', 'use', 'say', 'research', 'scientist', 'part', 'study', 'carbon', 'footprint', 'ai', 'place', 'power', 'grid', 'relatively', 'clean', 'much', 'low', 'place', 'grid', 'heavily', 'reliant', 'fossil', 'fuel', 'part', 'electricity', 'consume', 'run', 'ai', 'model', 'fix', 'able', 'reduce', 'overall', 'carbon', 'footprint', 'model', 'run', 'area', 'power', 'grid', 'consist', 'renewable', 'source', 'say', 'climate', 'change', 'extremely', 'anxiety', 'induce', 'vital', 'well', 'understand', 'tech', 'sector', 'effect', 'planet', 'study', 'one', 'help', 'come', 'creative', 'solution', 'allow', 'reap', 'benefit', 'ai', 'minimize', 'harm', 'hard', 'fix', 'measure', 'new', 'tool', 'help', 'create', 'new', 'material', 'ev', 'battery', 'solar', 'cell', 'microchip', 'new', 'material', 'supercharge', 'technological', 'breakthrough', 'discover', 'usually', 'take', 'month', 'even', 'year', 'research', 'new', 'tool', 'use', 'deep', 'learning', 'dramatically', 'speed', 'process', 'discover', 'new', 'material', 'big', 'deal', 'call', 'graphical', 'network', 'material', 'exploration', 'gnome', 'technology', 'already', 'use', 'predict', 'structure', 'new', 'material', 'go', 'create', 'lab', 'test', 'gnome', 'describe', 'alphafold', 'material', 'discovery', 'thank', 'gnome', 'number', 'know', 'stable', 'material', 'grow', 'almost', 'tenfold', 'read', 'high', 'school', 'deepfake', 'porn', 'scandal', 'push', 'lawmaker', 'actionlegislator', 'respond', 'quickly', 'teen', 'use', 'ai', 'create', 'nonconsensual', 'sexually', 'explicit', 'image', 'mit', 'technology', 'review', 'want', 'privacy', 'college', 'give', 'great', 'investigation', 'show', 'much', 'college', 'student', 'subject', 'increase', 'amount', 'surveillance', 'tech', 'include', 'homework', 'tracker', 'testtake', 'software', 'even', 'license', 'plate', 'reader', 'markup', 'chatgpt', 'leak', 'secret', 'new', 'story', 'show', 'vulnerable', 'chatbot', 'leak', 'datum', 'put', 'personal', 'proprietary', 'information', 'risk', 'first', 'story', 'wire', 'show', 'easily', 'openai', 'bot', 'spill', 'initial', 'instruction', 'give', 'create', 'one', 'medium', 'show', 'researcher', 'able', 'get', 'chatbot', 'reveal', 'datum', 'ask', 'repeat', 'specific', 'word', 'prompt', 'engineer', 'earn', 'fun', 'story', 'people', 'pay', 'figure', 'get', 'ai', 'chatbot', 'say']"
Make no mistake—AI is owned by Big Tech,https://www.technologyreview.com/2023/12/05/1084393/make-no-mistake-ai-is-owned-by-big-tech/,2023-12-05,"<p>If we’re not careful, Microsoft, Amazon, and other large companies will leverage their position to set the policy agenda for AI, as they have in many other sectors.</p>
","Until late November, when the epic saga of OpenAI’s board breakdown unfolded, the casual observer could be forgiven for assuming that the industry around generative AI was a vibrant competitive ecosystem.  But this is not the case—nor has it ever been. And understanding why is fundamental to understanding what AI is, and what threats it poses. Put simply, in the context of the current paradigm of building larger- and larger-scale AI systems, there is no AI without Big Tech. With vanishingly few exceptions, every startup, new entrant, and even AI research lab is dependent on these firms. All rely on the computing infrastructure of Microsoft, Amazon, and Google to train their systems, and on those same firms’ vast consumer market reach to deploy and sell their AI products.  Indeed, many startups simply license and rebrand AI models created and sold by these tech giants or their partner startups. This is because large tech firms have accrued significant advantages over the past decade. Thanks to platform dominance and the self-reinforcing properties of the surveillance business model, they own and control the ingredients necessary to develop and deploy large-scale AI. They also shape the incentive structures for the field of research and development in AI, defining the technology’s present and future.  An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. The recent OpenAI saga, in which Microsoft exerted its quiet but firm dominance over the “capped profit” entity, provides a powerful demonstration of what we’ve been analyzing for the last half-decade. To wit: those with the money make the rules. And right now, they’re engaged in a race to the bottom, releasing systems before they’re ready in an attempt to retain their dominant position.  Concentrated power isn’t just a problem for markets. Relying on a few unaccountable corporate actors for core infrastructure is a problem for democracy, culture, and individual and collective agency. Without significant intervention, the AI market will only end up rewarding and entrenching the very same companies that reaped the profits of the invasive surveillance business model that has powered the commercial internet, often at the expense of the public.  The Cambridge Analytica scandal was just one among many that exposed this seedy reality. Such concentration also creates single points of failure, which raises real security threats. And Securities and Exchange Commission chair Gary Gensler has warned that having a small number of AI models and actors at the foundation of the AI ecosystem poses systemic risks to the financial order, in which the effects of a single failure could be distributed much more widely.  The assertion that AI is contingent on—and exacerbates—concentration of power in the tech industry has often been met with pushback. Investors who have moved quickly from Web3 to the metaverse to AI are keen to realize returns in an ecosystem where a frenzied press cycle drives valuations toward profitable IPOs and acquisitions, even if the promises of the  technology in question aren’t ever realized.  But the attempted ouster—and subsequent reintegration—of OpenAI cofounders Sam Altman and Greg Brockman doesn’t just bring the power and influence of Microsoft into sharp focus; it also proves our case that these commercial arrangements give Big Tech profound control over the trajectory of AI. The story is fairly simple: after apparently being blindsided by the board’s decision, Microsoft moved to protect its investment and its road map to profit. The company quickly exerted its weight, rallying behind Altman and promising to “acquihire” those who wanted to defect.  Microsoft now has a seat on OpenAI’s board, albeit a nonvoting one. But the true leverage that Big Tech holds in the AI landscape is the combination of its computing power, data, and vast market reach. In order to pursue its bigger-is-better approach to AI development, OpenAI made a deal. It exclusively licenses its GPT-4 system and all other OpenAI models to Microsoft in exchange for access to Microsoft’s computing infrastructure.  For companies hoping to build base models, there is little alternative to working with either Microsoft, Google, or Amazon. And those at the center of AI are well aware of this, as illustrated by Sam Altman’s furtive search for Saudi and Emirati sovereign investment in a hardware venture he hoped would rival Nvidia. That company holds a near monopoly on state-of-the-art chips for AI training and is another key choke point along the AI supply chain. US regulators have since unwound an initial investment by Saudi Arabia into an Altman-backed company, RainAI, reinforcing the difficulty OpenAI faces in navigating the even more concentrated chipmaking market. In an exclusive interview with MIT Technology Review, Nadella shares his view of the platform shift for developers. There are few meaningful alternatives, even for those willing to go the extra mile to build industry-independent AI. As we’ve outlined elsewhere, “‘open-source AI”—an ill-defined term that’s currently used to describe everything from Meta’s (comparatively closed) LLaMA-2 and Eleuther’s (maximally open) Pythia series—can’t on its own offer escape velocity from industry concentration. For one thing, many open-source AI projects operate through compute credits, revenue sharing, or other contractual arrangements with tech giants that grapple with the same structural dependencies. In addition, Big Tech has a long legacy of capturing, or otherwise attempting to seek profit from, open-source development. Open-source AI can offer transparency, reusability, and extensibility, and these can be positive. But it does not address the problem of concentrated power in the AI market.  The OpenAI-Microsoft saga also demonstrates a fact that’s frequently lost in the hype around AI: there isn’t yet a clear business model outside of increasing cloud profits for Big Tech by bundling AI services with cloud infrastructure. And a business model is important when you’re talking about systems that can cost hundreds of millions of dollars to train and develop.  Microsoft isn’t alone here: Amazon, for example, runs a marketplace for AI models, on which all of its products, and a handful of others, operate using Amazon Web Services. The company recently struck an investment deal of up to $4 billion with Anthropic, which has also pledged to use Amazon’s in-house chip, Trainium, optimized for building large-scale AI.  Big Tech is becoming increasingly assertive in its maneuverings to protect its hold over the market. Make no mistake: though OpenAI was in the crosshairs this time, now that we’ve all seen what it looks like for a small entity when a big firm it depends on decides to flex, others will be paying attention and falling in line.  Regulation could help, but government policy often winds up entrenching, rather than mitigating, the power of these companies as they leverage their access to money and their political clout. Take Microsoft’s recent moves in the UK as an example: last week it announced a £2.5 billion investment in building out cloud infrastructure in the UK, a move lauded by a prime minister who has clearly signaled his ambitions to build a homegrown AI sector in the UK as his primary legacy. This news can’t be read in isolation: it is a clear attempt to blunt an investigation into the cloud market by the UK’s competition regulator following a study that specifically called out concerns registered by a range of market participants regarding Microsoft’s anticompetitive behavior.  We break down what you missed and what’s next for the AI industry.  From OpenAI’s (ultimately empty) threat to leave the EU over the AI Act to Meta’s lobbying to exempt open-source AI from basic accountability obligations to Microsoft’s push for restrictive licensing to the Big Tech–funded campaign to embed fellows in Congress, we’re seeing increasingly aggressive stances from large firms that are trying to shore up their dominance by wielding their considerable economic and political power. Tech industry giants are already circling their wagons as new regulations emerge from the White House, the EU, and elsewhere. But it’s clear we need to go much further. Now’s the time for a meaningful and robust accountability regime that places the interests of the public above the promises of firms not known for keeping them.  We need aggressive transparency mandates that clear away the opacity around fundamental issues like the data AI companies are accessing to train their models. We also need liability regimes that place the burden on companies to demonstrate that they meet baseline privacy, security, and bias standards before their AI products are publicly released. And to begin to address concentration, we need bold regulation that forces business separation between different layers of the AI stack and doesn’t allow Big Tech to leverage its dominance in infrastructure to consolidate its position in the market for AI models and applications.  But if governments keep giving the same narrow group of industry interests primacy in guiding policy, we won’t get far. After last week’s events, it’s all too clear what these companies serve: their bottom line.  Amba Kak is executive director and Sarah Myers West is managing director of the AI Now Institute, a New York–based policy research organization focused on artificial intelligence. Meredith Whittaker, the president of Signal, is the institute’s chief advisor.  ","Until late November , when the epic saga of OpenAI ’ s board breakdown unfolded , the casual observer could be forgiven for assuming that the industry around generative AI was a vibrant competitive ecosystem . But this is not the case—nor has it ever been . And understanding why is fundamental to understanding what AI is , and what threats it poses . Put simply , in the context of the current paradigm of building larger- and larger-scale AI systems , there is no AI without Big Tech . With vanishingly few exceptions , every startup , new entrant , and even AI research lab is dependent on these firms . All rely on the computing infrastructure of Microsoft , Amazon , and Google to train their systems , and on those same firms ’ vast consumer market reach to deploy and sell their AI products . Indeed , many startups simply license and rebrand AI models created and sold by these tech giants or their partner startups . This is because large tech firms have accrued significant advantages over the past decade . Thanks to platform dominance and the self-reinforcing properties of the surveillance business model , they own and control the ingredients necessary to develop and deploy large-scale AI . They also shape the incentive structures for the field of research and development in AI , defining the technology ’ s present and future . An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they ’ ve made him change the focus of his life ’ s work . The recent OpenAI saga , in which Microsoft exerted its quiet but firm dominance over the “ capped profit ” entity , provides a powerful demonstration of what we ’ ve been analyzing for the last half-decade . To wit : those with the money make the rules . And right now , they ’ re engaged in a race to the bottom , releasing systems before they ’ re ready in an attempt to retain their dominant position . Concentrated power isn ’ t just a problem for markets . Relying on a few unaccountable corporate actors for core infrastructure is a problem for democracy , culture , and individual and collective agency . Without significant intervention , the AI market will only end up rewarding and entrenching the very same companies that reaped the profits of the invasive surveillance business model that has powered the commercial internet , often at the expense of the public . The Cambridge Analytica scandal was just one among many that exposed this seedy reality . Such concentration also creates single points of failure , which raises real security threats . And Securities and Exchange Commission chair Gary Gensler has warned that having a small number of AI models and actors at the foundation of the AI ecosystem poses systemic risks to the financial order , in which the effects of a single failure could be distributed much more widely . The assertion that AI is contingent on—and exacerbates—concentration of power in the tech industry has often been met with pushback . Investors who have moved quickly from Web3 to the metaverse to AI are keen to realize returns in an ecosystem where a frenzied press cycle drives valuations toward profitable IPOs and acquisitions , even if the promises of the technology in question aren ’ t ever realized . But the attempted ouster—and subsequent reintegration—of OpenAI cofounders Sam Altman and Greg Brockman doesn ’ t just bring the power and influence of Microsoft into sharp focus ; it also proves our case that these commercial arrangements give Big Tech profound control over the trajectory of AI . The story is fairly simple : after apparently being blindsided by the board ’ s decision , Microsoft moved to protect its investment and its road map to profit . The company quickly exerted its weight , rallying behind Altman and promising to “ acquihire ” those who wanted to defect . Microsoft now has a seat on OpenAI ’ s board , albeit a nonvoting one . But the true leverage that Big Tech holds in the AI landscape is the combination of its computing power , data , and vast market reach . In order to pursue its bigger-is-better approach to AI development , OpenAI made a deal . It exclusively licenses its GPT-4 system and all other OpenAI models to Microsoft in exchange for access to Microsoft ’ s computing infrastructure . For companies hoping to build base models , there is little alternative to working with either Microsoft , Google , or Amazon . And those at the center of AI are well aware of this , as illustrated by Sam Altman ’ s furtive search for Saudi and Emirati sovereign investment in a hardware venture he hoped would rival Nvidia . That company holds a near monopoly on state-of-the-art chips for AI training and is another key choke point along the AI supply chain . US regulators have since unwound an initial investment by Saudi Arabia into an Altman-backed company , RainAI , reinforcing the difficulty OpenAI faces in navigating the even more concentrated chipmaking market . In an exclusive interview with MIT Technology Review , Nadella shares his view of the platform shift for developers . There are few meaningful alternatives , even for those willing to go the extra mile to build industry-independent AI . As we ’ ve outlined elsewhere , “ ‘ open-source AI ” —an ill-defined term that ’ s currently used to describe everything from Meta ’ s ( comparatively closed ) LLaMA-2 and Eleuther ’ s ( maximally open ) Pythia series—can ’ t on its own offer escape velocity from industry concentration . For one thing , many open-source AI projects operate through compute credits , revenue sharing , or other contractual arrangements with tech giants that grapple with the same structural dependencies . In addition , Big Tech has a long legacy of capturing , or otherwise attempting to seek profit from , open-source development . Open-source AI can offer transparency , reusability , and extensibility , and these can be positive . But it does not address the problem of concentrated power in the AI market . The OpenAI-Microsoft saga also demonstrates a fact that ’ s frequently lost in the hype around AI : there isn ’ t yet a clear business model outside of increasing cloud profits for Big Tech by bundling AI services with cloud infrastructure . And a business model is important when you ’ re talking about systems that can cost hundreds of millions of dollars to train and develop . Microsoft isn ’ t alone here : Amazon , for example , runs a marketplace for AI models , on which all of its products , and a handful of others , operate using Amazon Web Services . The company recently struck an investment deal of up to $ 4 billion with Anthropic , which has also pledged to use Amazon ’ s in-house chip , Trainium , optimized for building large-scale AI . Big Tech is becoming increasingly assertive in its maneuverings to protect its hold over the market . Make no mistake : though OpenAI was in the crosshairs this time , now that we ’ ve all seen what it looks like for a small entity when a big firm it depends on decides to flex , others will be paying attention and falling in line . Regulation could help , but government policy often winds up entrenching , rather than mitigating , the power of these companies as they leverage their access to money and their political clout . Take Microsoft ’ s recent moves in the UK as an example : last week it announced a £2.5 billion investment in building out cloud infrastructure in the UK , a move lauded by a prime minister who has clearly signaled his ambitions to build a homegrown AI sector in the UK as his primary legacy . This news can ’ t be read in isolation : it is a clear attempt to blunt an investigation into the cloud market by the UK ’ s competition regulator following a study that specifically called out concerns registered by a range of market participants regarding Microsoft ’ s anticompetitive behavior . We break down what you missed and what ’ s next for the AI industry . From OpenAI ’ s ( ultimately empty ) threat to leave the EU over the AI Act to Meta ’ s lobbying to exempt open-source AI from basic accountability obligations to Microsoft ’ s push for restrictive licensing to the Big Tech–funded campaign to embed fellows in Congress , we ’ re seeing increasingly aggressive stances from large firms that are trying to shore up their dominance by wielding their considerable economic and political power . Tech industry giants are already circling their wagons as new regulations emerge from the White House , the EU , and elsewhere . But it ’ s clear we need to go much further . Now ’ s the time for a meaningful and robust accountability regime that places the interests of the public above the promises of firms not known for keeping them . We need aggressive transparency mandates that clear away the opacity around fundamental issues like the data AI companies are accessing to train their models . We also need liability regimes that place the burden on companies to demonstrate that they meet baseline privacy , security , and bias standards before their AI products are publicly released . And to begin to address concentration , we need bold regulation that forces business separation between different layers of the AI stack and doesn ’ t allow Big Tech to leverage its dominance in infrastructure to consolidate its position in the market for AI models and applications . But if governments keep giving the same narrow group of industry interests primacy in guiding policy , we won ’ t get far . After last week ’ s events , it ’ s all too clear what these companies serve : their bottom line . Amba Kak is executive director and Sarah Myers West is managing director of the AI Now Institute , a New York–based policy research organization focused on artificial intelligence . Meredith Whittaker , the president of Signal , is the institute ’ s chief advisor .","['late', 'epic', 'saga', 'board', 'breakdown', 'unfold', 'casual', 'observer', 'forgive', 'assume', 'industry', 'generative', 'ai', 'vibrant', 'competitive', 'ecosystem', 'case', 'ever', 'understand', 'fundamental', 'understand', 'ai', 'threat', 'put', 'simply', 'context', 'current', 'paradigm', 'build', 'large', 'largerscale', 'system', 'ai', 'big', 'tech', 'vanishingly', 'exception', 'startup', 'new', 'entrant', 'even', 'ai', 'research', 'lab', 'dependent', 'firm', 'rely', 'compute', 'infrastructure', 'train', 'system', 'firm', 'vast', 'consumer', 'market', 'reach', 'deploy', 'sell', 'product', 'indeed', 'many', 'startup', 'simply', 'license', 'rebrand', 'ai', 'model', 'create', 'sell', 'tech', 'giant', 'partner', 'startup', 'large', 'tech', 'firm', 'accrue', 'significant', 'advantage', 'past', 'decade', 'thank', 'platform', 'dominance', 'selfreinforce', 'property', 'surveillance', 'business', 'model', 'control', 'ingredient', 'necessary', 'develop', 'deploy', 'largescale', 'ai', 'also', 'shape', 'incentive', 'structure', 'field', 'research', 'development', 'define', 'technology', 'present', 'future', 'exclusive', 'conversation', 'sutskever', 'fear', 'future', 'ai', 'make', 'change', 'focus', 'life', 'work', 'recent', 'openai', 'saga', 'exert', 'quiet', 'firm', 'dominance', 'cap', 'profit', 'entity', 'provide', 'powerful', 'demonstration', '’', 'analyze', 'last', 'halfdecade', 'wit', 'money', 'make', 'rule', 'right', 'engage', 'race', 'bottom', 'release', 'system', 'ready', 'attempt', 'retain', 'dominant', 'position', 'concentrated', 'power', 'problem', 'market', 'rely', 'unaccountable', 'corporate', 'actor', 'core', 'infrastructure', 'problem', 'democracy', 'culture', 'individual', 'collective', 'agency', 'significant', 'intervention', 'ai', 'market', 'end', 'reward', 'entrench', 'company', 'reap', 'profit', 'invasive', 'surveillance', 'business', 'model', 'power', 'commercial', 'internet', 'often', 'expense', 'public', 'scandal', 'many', 'expose', 'seedy', 'reality', 'concentration', 'also', 'create', 'single', 'point', 'failure', 'raise', 'real', 'security', 'threat', 'warn', 'small', 'number', 'model', 'actor', 'foundation', 'ecosystem', 'pose', 'systemic', 'risk', 'financial', 'order', 'effect', 'single', 'failure', 'distribute', 'much', 'widely', 'assertion', 'ai', 'contingent', 'exacerbate', 'concentration', 'power', 'tech', 'industry', 'often', 'meet', 'pushback', 'investor', 'move', 'quickly', 'web3', 'metaverse', 'ai', 'keen', 'realize', 'return', 'ecosystem', 'frenzied', 'press', 'cycle', 'drive', 'valuation', 'profitable', 'ipos', 'acquisition', 'even', 'promise', 'technology', 'question', 'ever', 'realize', 'attempt', 'ouster', 'subsequent', 'reintegration', 'cofounder', 'bring', 'power', 'influence', 'sharp', 'focus', 'also', 'prove', 'case', 'commercial', 'arrangement', 'give', 'big', 'tech', 'profound', 'control', 'trajectory', 'ai', 'story', 'fairly', 'simple', 'apparently', 'blindside', 'board', 'decision', 'move', 'protect', 'investment', 'road', 'map', 'profit', 'company', 'quickly', 'exert', 'weight', 'rally', 'promise', 'acquihire', 'want', 'defect', 'seat', 'board', 'nonvoting', 'one', 'true', 'leverage', 'big', 'tech', 'hold', 'ai', 'landscape', 'combination', 'computing', 'power', 'datum', 'vast', 'market', 'reach', 'order', 'pursue', 'biggerisbetter', 'approach', 'ai', 'development', 'make', 'deal', 'exclusively', 'license', 'gpt4', 'system', 'openai', 'model', 'exchange', 'access', 'compute', 'infrastructure', 'company', 'hope', 'build', 'base', 'model', 'little', 'alternative', 'work', 'center', 'ai', 'well', 'aware', 'illustrate', 'furtive', 'search', 'saudi', 'emirati', 'sovereign', 'investment', 'hardware', 'venture', 'hope', 'rival', 'company', 'hold', 'near', 'monopoly', 'stateoftheart', 'chip', 'ai', 'training', 'key', 'choke', 'point', 'supply', 'chain', 'regulator', 'unwind', 'initial', 'investment', 'altmanbacked', 'company', 'rainai', 'reinforce', 'difficulty', 'openai', 'face', 'navigate', 'even', 'concentrated', 'chipmake', 'market', 'exclusive', 'interview', 'mit', 'technology', 'review', 'share', 'view', 'platform', 'shift', 'developer', 'meaningful', 'alternative', 'even', 'willing', 'go', 'extra', 'mile', 'build', 'ai', '’', 'outline', 'elsewhere', 'opensource', 'ai', 'illdefined', 'term', 'currently', 'use', 'describe', 'meta', 'comparatively', 'close', 'llama2', 'eleuther', 'maximally', 'open', 'offer', 'escape', 'velocity', 'industry', 'concentration', 'thing', 'many', 'opensource', 'ai', 'project', 'operate', 'compute', 'credit', 'revenue', 'sharing', 'contractual', 'arrangement', 'tech', 'giant', 'grapple', 'structural', 'dependency', 'addition', 'big', 'tech', 'long', 'legacy', 'capturing', 'otherwise', 'attempt', 'seek', 'profit', 'development', 'opensource', 'offer', 'transparency', 'reusability', 'extensibility', 'positive', 'address', 'problem', 'concentrated', 'power', 'market', 'saga', 'also', 'demonstrate', 'fact', 'frequently', 'lose', 'hype', 'around', 'ai', 'yet', 'clear', 'business', 'model', 'outside', 'increase', 'cloud', 'profit', 'big', 'tech', 'bundle', 'service', 'cloud', 'infrastructure', 'business', 'model', 'important', 'talk', 'system', 'cost', 'hundred', 'million', 'dollar', 'train', 'develop', 'alone', 'example', 'run', 'marketplace', 'model', 'product', 'handful', 'operate', 'use', 'amazon', 'web', 'service', 'company', 'recently', 'strike', 'investment', 'deal', 'anthropic', 'also', 'pledge', 'use', 'inhouse', 'chip', 'trainium', 'optimize', 'build', 'largescale', 'ai', 'big', 'tech', 'become', 'increasingly', 'assertive', 'maneuvering', 'protect', 'hold', 'market', 'make', 'mistake', 'openai', 'crosshair', 'time', 'see', 'look', 'small', 'entity', 'big', 'firm', 'depend', 'decide', 'flex', 'pay', 'attention', 'fall', 'line', 'regulation', 'help', 'government', 'policy', 'often', 'wind', 'entrench', 'rather', 'mitigate', 'power', 'company', 'leverage', 'access', 'money', 'political', 'clout', 'take', 'recent', 'move', 'example', 'last', 'week', 'announce', 'investment', 'build', 'cloud', 'infrastructure', 'move', 'laud', 'prime', 'minister', 'clearly', 'signal', 'ambition', 'build', 'homegrown', 'ai', 'sector', 'primary', 'legacy', 'news', 'read', 'isolation', 'clear', 'attempt', 'blunt', 'investigation', 'cloud', 'market', 'competition', 'regulator', 'follow', 'study', 'specifically', 'call', 'concern', 'register', 'range', 'market', 'participant', 'regard', 'anticompetitive', 'behavior', 'break', 'miss', 'next', 'industry', 'ultimately', 'empty', 'threat', 'leave', 'act', 'lobby', 'exempt', 'opensource', 'ai', 'basic', 'accountability', 'obligation', 'push', 'restrictive', 'licensing', 'big', 'tech', 'fund', 'campaign', 'embed', 'fellow', 'see', 'increasingly', 'aggressive', 'stance', 'large', 'firm', 'try', 'shore', 'dominance', 'wield', 'considerable', 'economic', 'political', 'power', 'tech', 'industry', 'giant', 'already', 'circle', 'wagon', 'new', 'regulation', 'emerge', 'elsewhere', 'clear', 'need', 'go', 'much', 'far', 'time', 'meaningful', 'robust', 'accountability', 'regime', 'place', 'interest', 'public', 'promise', 'firm', 'know', 'keep', 'need', 'aggressive', 'transparency', 'mandate', 'clear', 'away', 'opacity', 'fundamental', 'issue', 'datum', 'ai', 'company', 'access', 'train', 'model', 'also', 'need', 'liability', 'regime', 'place', 'burden', 'company', 'demonstrate', 'meet', 'baseline', 'privacy', 'security', 'bias', 'standard', 'product', 'publicly', 'release', 'begin', 'address', 'concentration', 'need', 'bold', 'regulation', 'force', 'business', 'separation', 'different', 'layer', 'stack', 'allow', 'big', 'tech', 'leverage', 'dominance', 'infrastructure', 'consolidate', 'position', 'market', 'model', 'application', 'government', 'keep', 'give', 'narrow', 'group', 'industry', 'interest', 'primacy', 'guide', 'policy', 'win', 'get', 'far', 'last', 'week', 'event', 'clear', 'company', 'serve', 'bottom', 'line', 'amba', 'executive', 'director', 'manage', 'director', 'institute', 'base', 'policy', 'research', 'organization', 'focus', 'artificial', 'intelligence', 'president', 'signal', 'chief', 'advisor']"
Human brain cells hooked up to a chip can do speech recognition,https://www.technologyreview.com/2023/12/11/1084926/human-brain-cells-chip-organoid-speech-recognition/,2023-12-11,"<p>Clusters of brain cells grown in the lab have shown potential as a new type of hybrid bio-computer.</p>
","Brain organoids, clumps of human brain cells grown in a dish, can be hooked up to an electronic chip and carry out simple computational tasks, a new study shows.  Feng Guo and his team at Indiana University Bloomington generated a brain organoid from stem cells, attached it to a computer chip, and connected their setup, known as Brainoware, to an AI tool. They found that this hybrid system could process, learn, and remember information. It was even able to carry out some rudimentary speech recognition. The work, published today in Nature Electronics, could one day lead to new kinds of bio-computers that are more efficient than conventional computers. Scientists have been trying to build computers based on advanced biological systems for decades. Guo says that such computers could overcome some challenges of silicon-based computers, such as bottlenecks in data processing.  Conventional computers are much better than brains in dealing with numbers, but human brains are better at processing complex information while using relatively little energy. “This is a first demonstration of using brain organoids [for computing],” says Guo. “It’s exciting to see the possibilities of organoids for biocomputing in the future.”      Researchers are using organoids to unlock one of the human body’s most mysterious—and miraculous—processes. With Brainoware, Guo aimed to use actual brain cells to send and receive information. When the researchers applied electrical stimulation to the hybrid system they’d built, Brainoware responded to those signals, and changes occurred in its neural networks. According to the researchers, this result suggests that the hybrid system did process information, and could perhaps even perform computing tasks without supervision. Guo and his colleagues then attempted to see if Brainoware could perform any useful tasks. In one test, they used Brainoware to try to solve mathematical equations. They also gave it a benchmark test for speech recognition, using 240 audio clips of eight people pronouncing Japanese vowels. The clips were converted into electrical signals and applied to the Brainoware system. This generated signals in the neural networks of the brain organoid, which were then fed into an AI tool for decoding. The researchers found that the brain organoid–AI system could decode the signals from the audio recordings, which is a form of speech recognition, says Guo. “But the accuracy was low,” he says. Although the system improved with training, reaching an accuracy of about 78%, it was still less accurate than artificial neural networks, according to the study.  Lena Smirnova, an assistant professor of public health at Johns Hopkins University,  points out that brain organoids do not have the ability to truly hear speech but simply exhibit “a reaction” to pulses of electrical stimulation from the audio clips. And the study did not demonstrate whether Brainoware can process and store information over the long term or learn multiple tasks. Generating brain cell cultures in a lab and maintaining them long enough to perform computations is also a huge undertaking. Still, she adds, “it’s a really good demonstration that shows the capabilities of brain organoids.” ","Brain organoids , clumps of human brain cells grown in a dish , can be hooked up to an electronic chip and carry out simple computational tasks , a new study shows . Feng Guo and his team at Indiana University Bloomington generated a brain organoid from stem cells , attached it to a computer chip , and connected their setup , known as Brainoware , to an AI tool . They found that this hybrid system could process , learn , and remember information . It was even able to carry out some rudimentary speech recognition . The work , published today in Nature Electronics , could one day lead to new kinds of bio-computers that are more efficient than conventional computers . Scientists have been trying to build computers based on advanced biological systems for decades . Guo says that such computers could overcome some challenges of silicon-based computers , such as bottlenecks in data processing . Conventional computers are much better than brains in dealing with numbers , but human brains are better at processing complex information while using relatively little energy . “ This is a first demonstration of using brain organoids [ for computing ] , ” says Guo . “ It ’ s exciting to see the possibilities of organoids for biocomputing in the future. ” Researchers are using organoids to unlock one of the human body ’ s most mysterious—and miraculous—processes . With Brainoware , Guo aimed to use actual brain cells to send and receive information . When the researchers applied electrical stimulation to the hybrid system they ’ d built , Brainoware responded to those signals , and changes occurred in its neural networks . According to the researchers , this result suggests that the hybrid system did process information , and could perhaps even perform computing tasks without supervision . Guo and his colleagues then attempted to see if Brainoware could perform any useful tasks . In one test , they used Brainoware to try to solve mathematical equations . They also gave it a benchmark test for speech recognition , using 240 audio clips of eight people pronouncing Japanese vowels . The clips were converted into electrical signals and applied to the Brainoware system . This generated signals in the neural networks of the brain organoid , which were then fed into an AI tool for decoding . The researchers found that the brain organoid–AI system could decode the signals from the audio recordings , which is a form of speech recognition , says Guo . “ But the accuracy was low , ” he says . Although the system improved with training , reaching an accuracy of about 78 % , it was still less accurate than artificial neural networks , according to the study . Lena Smirnova , an assistant professor of public health at Johns Hopkins University , points out that brain organoids do not have the ability to truly hear speech but simply exhibit “ a reaction ” to pulses of electrical stimulation from the audio clips . And the study did not demonstrate whether Brainoware can process and store information over the long term or learn multiple tasks . Generating brain cell cultures in a lab and maintaining them long enough to perform computations is also a huge undertaking . Still , she adds , “ it ’ s a really good demonstration that shows the capabilities of brain organoids . ”","['brain', 'organoid', 'clump', 'human', 'brain', 'cell', 'grow', 'dish', 'hook', 'electronic', 'chip', 'carry', 'simple', 'computational', 'task', 'new', 'study', 'show', 'team', 'generate', 'brain', 'organoid', 'stem', 'cell', 'attach', 'computer', 'chip', 'connect', 'setup', 'know', 'brainoware', 'tool', 'find', 'hybrid', 'system', 'process', 'learn', 'remember', 'information', 'even', 'able', 'carry', 'rudimentary', 'speech', 'recognition', 'work', 'publish', 'today', 'nature', 'electronic', 'day', 'lead', 'new', 'kind', 'biocomputer', 'efficient', 'conventional', 'computer', 'scientist', 'try', 'build', 'computer', 'base', 'advanced', 'biological', 'system', 'decade', 'say', 'computer', 'overcome', 'challenge', 'siliconbase', 'computer', 'bottleneck', 'datum', 'process', 'conventional', 'computer', 'much', 'well', 'brain', 'deal', 'number', 'human', 'brain', 'well', 'process', 'complex', 'information', 'use', 'relatively', 'little', 'energy', 'first', 'demonstration', 'use', 'brain', 'organoid', 'computing', 'say', 'exciting', 'see', 'possibility', 'organoid', 'biocompute', 'future', 'researcher', 'use', 'organoid', 'unlock', 'human', 'body', 'mysterious', 'miraculous', 'process', 'brainoware', 'guo', 'aim', 'use', 'actual', 'brain', 'cell', 'send', 'receive', 'information', 'researcher', 'apply', 'electrical', 'stimulation', 'hybrid', 'system', 'build', 'brainoware', 'respond', 'signal', 'change', 'occur', 'neural', 'network', 'accord', 'researcher', 'result', 'suggest', 'hybrid', 'system', 'process', 'information', 'perhaps', 'even', 'perform', 'compute', 'task', 'supervision', 'guo', 'colleague', 'attempt', 'see', 'brainoware', 'perform', 'useful', 'task', 'test', 'use', 'brainoware', 'try', 'solve', 'mathematical', 'equation', 'also', 'give', 'benchmark', 'test', 'speech', 'recognition', 'use', 'audio', 'clip', 'people', 'pronounce', 'japanese', 'vowel', 'clip', 'convert', 'electrical', 'signal', 'apply', 'brainoware', 'system', 'generate', 'signal', 'neural', 'network', 'brain', 'organoid', 'feed', 'tool', 'decode', 'researcher', 'find', 'brain', 'organoid', 'ai', 'system', 'decode', 'signal', 'audio', 'recording', 'form', 'speech', 'say', 'accuracy', 'low', 'say', 'system', 'improve', 'training', 'reach', 'accuracy', 'still', 'less', 'accurate', 'artificial', 'neural', 'network', 'accord', 'study', 'lena', 'assistant', 'professor', 'public', 'health', 'point', 'brain', 'organoid', 'ability', 'truly', 'hear', 'speech', 'simply', 'exhibit', 'reaction', 'pulse', 'electrical', 'stimulation', 'audio', 'clip', 'study', 'demonstrate', 'brainoware', 'process', 'store', 'information', 'long', 'term', 'learn', 'multiple', 'task', 'generate', 'brain', 'cell', 'culture', 'lab', 'maintain', 'long', 'enough', 'perform', 'computation', 'also', 'huge', 'undertaking', 'still', 'add', 'really', 'good', 'demonstration', 'show', 'capability', 'brain', 'organoid']"
