title,url,date,summary,text,cleaning,tokens
 Four trends that changed AI in 2023,https://www.technologyreview.com/2023/12/19/1085696/four-trends-that-changed-ai-in-2023/,2023-12-19,"<p>Endless product launches, boardroom coups, intense policy debates, and a race to find the next big thing — it’s been a busy year.</p>
","This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here. This has been one of the craziest years in AI in a long time: endless product launches, boardroom coups, intense policy debates about AI doom, and a race to find the next big thing. But we’ve also seen concrete tools and policies aimed at getting the AI sector to behave more responsibly and hold powerful players accountable. That gives me a lot of hope for the future of AI.  Here’s what 2023 taught me:  The year started with Big Tech going all in on generative AI. The runaway success of OpenAI’s ChatGPT prompted every major tech company to release its own version. This year might go down in history as the year we saw the most AI launches: Meta’s LLaMA 2, Google’s Bard chatbot and Gemini, Baidu’s Ernie Bot, OpenAI’s GPT-4, and a handful of other models, including one from a French open-source challenger, Mistral.  But despite the initial hype, we haven’t seen any AI applications become an overnight success. Microsoft and Google pitched powerful AI-powered search, but it turned out to be more of a dud than a killer app. The fundamental flaws in language models, such as the fact that they frequently make stuff up, led to some embarrassing (and, let’s be honest, hilarious) gaffes. Microsoft’s Bing would frequently reply to people’s questions with conspiracy theories, and suggested that a New York Times reporter leave his wife. Google’s Bard generated factually incorrect answers for its marketing campaign, which wiped $100 billion off the company’s share price. There is now a frenetic hunt for a popular AI product that everyone will want to adopt. Both OpenAI and Google are experimenting with allowing companies and developers to create customized AI chatbots and letting people build their own applications using AI—no coding skills needed. Perhaps generative AI will end up embedded in boring but useful tools to help us boost our productivity at work. It might take the form of AI assistants—maybe with voice capabilities—and coding support. Next year will be crucial in determining the real value of generative AI. Even though tech companies are rolling out large language models into products at a frenetic pace, there is still a lot we don’t know about how they work. They make stuff up and have severe gender and ethnic biases. This year we also found out that different language models generate texts with different political biases, and that they make great tools for hacking people’s private information. Text-to-image models can be prompted to spit out copyrighted images and pictures of real people, and they can easily be tricked into generating disturbing images. It’s been great to see so much research into the flaws of these models, because this could take us a step closer to understanding why they behave the way they do, and ultimately fix them. Generative models can be very unpredictable, and this year there were lots of attempts to try to make them behave as their creators want them to. OpenAI shared that it uses a technique called reinforcement learning from human feedback, which uses feedback from users to help guide ChatGPT to more desirable answers. A study from the AI lab Anthropic showed how simple natural-language instructions can steer large language models to make their results less toxic. But sadly, a lot of these attempts end up being quick fixes rather than permanent ones. Then there are misguided approaches like banning seemingly innocuous words such as “placenta” from image-generating AI systems to avoid producing gore. Tech companies come up with workarounds like these because they don’t know why models generate the content they do.  We also got a better sense of AI’s true carbon footprint. Generating an image using a powerful AI model takes as much energy as fully charging your smartphone, researchers at the AI startup Hugging Face and Carnegie Mellon University found. Until now, the exact amount of energy generative AI uses has been a missing piece of the puzzle. More research into this could help us shift the way we use AI to be more sustainable.  Chatter about the possibility that AI poses an existential risk to humans became familiar this year. Hundreds of scientists, business leaders, and policymakers have spoken up, from deep-learning pioneers Geoffrey Hinton and Yoshua Bengio to the CEOs of top AI firms, such as Sam Altman and Demis Hassabis, to the California congressman Ted Lieu and the former president of Estonia Kersti Kaljulaid. Existential risk has become one of the biggest memes in AI. The hypothesis is that one day we will build an AI that is far smarter than humans, and this could lead to grave consequences. It’s an ideology championed by many in Silicon Valley, including Ilya Sutskever, OpenAI’s chief scientist, who played a pivotal role in ousting OpenAI CEO Sam Altman (and then reinstating him a few days later).  But not everyone agrees with this idea. Meta’s AI leaders Yann LeCun and Joelle Pineau have said that these fears are “ridiculous” and the conversation about AI risks has become “unhinged.” Many other power players in AI, such as researcher Joy Buolamwini, say that focusing on hypothetical risks distracts from the very real harms AI is causing today.  Nevertheless, the increased attention on the technology’s potential to cause extreme harm has prompted many important conversations about AI policy and animated lawmakers all over the world to take action.  Thanks to ChatGPT, everyone from the US Senate to the G7 was talking about AI policy and regulation this year. In early December, European lawmakers wrapped up a busy policy year when they agreed on the AI Act, which will introduce binding rules and standards on how to develop the riskiest AI more responsibly. It will also ban certain “unacceptable” applications of AI, such as police use of facial recognition in public places. The White House, meanwhile, introduced an executive order on AI, plus voluntary commitments from leading AI companies. Its efforts aimed to bring more transparency and standards for AI and gave a lot of freedom to agencies to adapt AI rules to fit their sectors. One concrete policy proposal that got a lot of attention was watermarks—invisible signals in text and images that can be detected by computers, in order to flag AI-generated content. These could be used to track plagiarism or help fight disinformation, and this year we saw research that succeeded in applying them to AI-generated text and images.It wasn’t just lawmakers that were busy, but lawyers too. We saw a record number of  lawsuits, as artists and writers argued that AI companies had scraped their intellectual property without their consent and with no compensation. In an exciting counter-offensive, researchers at the University of Chicago developed Nightshade, a new data-poisoning tool that lets artists fight back against generative AI by messing up training data in ways that could cause serious damage to image-generating AI models. There is a resistance brewing, and I expect more grassroots efforts to shift tech’s power balance next year.  Now we know what OpenAI’s superalignment team has been up to OpenAI has announced the first results from its superalignment team, its in-house initiative dedicated to preventing a superintelligence—a hypothetical future AI that can outsmart humans—from going rogue. The team is led by chief scientist Ilya Sutskever, who was part of the group that just last month fired OpenAI’s CEO, Sam Altman, only to reinstate him a few days later. Business as usual: Unlike many of the company’s announcements, this heralds no big breakthrough. In a low-key research paper, the team describes a technique that lets a less powerful large language model supervise a more powerful one—and suggests that this might be a small step toward figuring out how humans might supervise superhuman machines. Read more from Will Douglas Heaven.  Google DeepMind used a large language model to solve an unsolvable math problemIn a paper published in Nature, the company says it is the first time a large language model has been used to discover a solution to a long-standing scientific puzzle—producing verifiable and valuable new information that did not previously exist. (MIT Technology Review) This new system can teach a robot a simple household task within 20 minutesA new open-source system, called Dobb-E, was trained using data collected from real homes. It can help to teach a robot how to open an air fryer, close a door, or straighten a cushion, among other tasks. It could also help the field of robotics overcome one of its biggest challenges: a lack of training data.  (MIT Technology Review) ChatGPT is turning the internet into plumbingGerman media giant Axel Springer, which owns Politico and Business Insider, announced a partnership with OpenAI, in which the tech company will be able to use its news articles as training data and the news organizations will be able to use ChatGPT to do summaries of news. This column has a clever point: tech companies are increasingly becoming gatekeepers for online content, and journalism is just “plumbing for a digital faucet.” (The Atlantic) Meet the former French official pushing for looser AI rules after joining startup MistralA profile of Mistral AI cofounder Cédric O, who used to be France’s digital minister. Before joining France’s AI unicorn, he was a vocal proponent of strict laws for tech, but he lobbied hard against rules in the AI Act that would have restricted Mistral’s models. He was successful: the company’s models don’t meet the computing threshold set by the law, and its open-source models are also exempt from transparency obligations. (Bloomberg)  ","This story originally appeared in The Algorithm , our weekly newsletter on AI . To get stories like this in your inbox first , sign up here . This has been one of the craziest years in AI in a long time : endless product launches , boardroom coups , intense policy debates about AI doom , and a race to find the next big thing . But we ’ ve also seen concrete tools and policies aimed at getting the AI sector to behave more responsibly and hold powerful players accountable . That gives me a lot of hope for the future of AI . Here ’ s what 2023 taught me : The year started with Big Tech going all in on generative AI . The runaway success of OpenAI ’ s ChatGPT prompted every major tech company to release its own version . This year might go down in history as the year we saw the most AI launches : Meta ’ s LLaMA 2 , Google ’ s Bard chatbot and Gemini , Baidu ’ s Ernie Bot , OpenAI ’ s GPT-4 , and a handful of other models , including one from a French open-source challenger , Mistral . But despite the initial hype , we haven ’ t seen any AI applications become an overnight success . Microsoft and Google pitched powerful AI-powered search , but it turned out to be more of a dud than a killer app . The fundamental flaws in language models , such as the fact that they frequently make stuff up , led to some embarrassing ( and , let ’ s be honest , hilarious ) gaffes . Microsoft ’ s Bing would frequently reply to people ’ s questions with conspiracy theories , and suggested that a New York Times reporter leave his wife . Google ’ s Bard generated factually incorrect answers for its marketing campaign , which wiped $ 100 billion off the company ’ s share price . There is now a frenetic hunt for a popular AI product that everyone will want to adopt . Both OpenAI and Google are experimenting with allowing companies and developers to create customized AI chatbots and letting people build their own applications using AI—no coding skills needed . Perhaps generative AI will end up embedded in boring but useful tools to help us boost our productivity at work . It might take the form of AI assistants—maybe with voice capabilities—and coding support . Next year will be crucial in determining the real value of generative AI . Even though tech companies are rolling out large language models into products at a frenetic pace , there is still a lot we don ’ t know about how they work . They make stuff up and have severe gender and ethnic biases . This year we also found out that different language models generate texts with different political biases , and that they make great tools for hacking people ’ s private information . Text-to-image models can be prompted to spit out copyrighted images and pictures of real people , and they can easily be tricked into generating disturbing images . It ’ s been great to see so much research into the flaws of these models , because this could take us a step closer to understanding why they behave the way they do , and ultimately fix them . Generative models can be very unpredictable , and this year there were lots of attempts to try to make them behave as their creators want them to . OpenAI shared that it uses a technique called reinforcement learning from human feedback , which uses feedback from users to help guide ChatGPT to more desirable answers . A study from the AI lab Anthropic showed how simple natural-language instructions can steer large language models to make their results less toxic . But sadly , a lot of these attempts end up being quick fixes rather than permanent ones . Then there are misguided approaches like banning seemingly innocuous words such as “ placenta ” from image-generating AI systems to avoid producing gore . Tech companies come up with workarounds like these because they don ’ t know why models generate the content they do . We also got a better sense of AI ’ s true carbon footprint . Generating an image using a powerful AI model takes as much energy as fully charging your smartphone , researchers at the AI startup Hugging Face and Carnegie Mellon University found . Until now , the exact amount of energy generative AI uses has been a missing piece of the puzzle . More research into this could help us shift the way we use AI to be more sustainable . Chatter about the possibility that AI poses an existential risk to humans became familiar this year . Hundreds of scientists , business leaders , and policymakers have spoken up , from deep-learning pioneers Geoffrey Hinton and Yoshua Bengio to the CEOs of top AI firms , such as Sam Altman and Demis Hassabis , to the California congressman Ted Lieu and the former president of Estonia Kersti Kaljulaid . Existential risk has become one of the biggest memes in AI . The hypothesis is that one day we will build an AI that is far smarter than humans , and this could lead to grave consequences . It ’ s an ideology championed by many in Silicon Valley , including Ilya Sutskever , OpenAI ’ s chief scientist , who played a pivotal role in ousting OpenAI CEO Sam Altman ( and then reinstating him a few days later ) . But not everyone agrees with this idea . Meta ’ s AI leaders Yann LeCun and Joelle Pineau have said that these fears are “ ridiculous ” and the conversation about AI risks has become “ unhinged. ” Many other power players in AI , such as researcher Joy Buolamwini , say that focusing on hypothetical risks distracts from the very real harms AI is causing today . Nevertheless , the increased attention on the technology ’ s potential to cause extreme harm has prompted many important conversations about AI policy and animated lawmakers all over the world to take action . Thanks to ChatGPT , everyone from the US Senate to the G7 was talking about AI policy and regulation this year . In early December , European lawmakers wrapped up a busy policy year when they agreed on the AI Act , which will introduce binding rules and standards on how to develop the riskiest AI more responsibly . It will also ban certain “ unacceptable ” applications of AI , such as police use of facial recognition in public places . The White House , meanwhile , introduced an executive order on AI , plus voluntary commitments from leading AI companies . Its efforts aimed to bring more transparency and standards for AI and gave a lot of freedom to agencies to adapt AI rules to fit their sectors . One concrete policy proposal that got a lot of attention was watermarks—invisible signals in text and images that can be detected by computers , in order to flag AI-generated content . These could be used to track plagiarism or help fight disinformation , and this year we saw research that succeeded in applying them to AI-generated text and images.It wasn ’ t just lawmakers that were busy , but lawyers too . We saw a record number of lawsuits , as artists and writers argued that AI companies had scraped their intellectual property without their consent and with no compensation . In an exciting counter-offensive , researchers at the University of Chicago developed Nightshade , a new data-poisoning tool that lets artists fight back against generative AI by messing up training data in ways that could cause serious damage to image-generating AI models . There is a resistance brewing , and I expect more grassroots efforts to shift tech ’ s power balance next year . Now we know what OpenAI ’ s superalignment team has been up to OpenAI has announced the first results from its superalignment team , its in-house initiative dedicated to preventing a superintelligence—a hypothetical future AI that can outsmart humans—from going rogue . The team is led by chief scientist Ilya Sutskever , who was part of the group that just last month fired OpenAI ’ s CEO , Sam Altman , only to reinstate him a few days later . Business as usual : Unlike many of the company ’ s announcements , this heralds no big breakthrough . In a low-key research paper , the team describes a technique that lets a less powerful large language model supervise a more powerful one—and suggests that this might be a small step toward figuring out how humans might supervise superhuman machines . Read more from Will Douglas Heaven . Google DeepMind used a large language model to solve an unsolvable math problemIn a paper published in Nature , the company says it is the first time a large language model has been used to discover a solution to a long-standing scientific puzzle—producing verifiable and valuable new information that did not previously exist . ( MIT Technology Review ) This new system can teach a robot a simple household task within 20 minutesA new open-source system , called Dobb-E , was trained using data collected from real homes . It can help to teach a robot how to open an air fryer , close a door , or straighten a cushion , among other tasks . It could also help the field of robotics overcome one of its biggest challenges : a lack of training data . ( MIT Technology Review ) ChatGPT is turning the internet into plumbingGerman media giant Axel Springer , which owns Politico and Business Insider , announced a partnership with OpenAI , in which the tech company will be able to use its news articles as training data and the news organizations will be able to use ChatGPT to do summaries of news . This column has a clever point : tech companies are increasingly becoming gatekeepers for online content , and journalism is just “ plumbing for a digital faucet. ” ( The Atlantic ) Meet the former French official pushing for looser AI rules after joining startup MistralA profile of Mistral AI cofounder Cédric O , who used to be France ’ s digital minister . Before joining France ’ s AI unicorn , he was a vocal proponent of strict laws for tech , but he lobbied hard against rules in the AI Act that would have restricted Mistral ’ s models . He was successful : the company ’ s models don ’ t meet the computing threshold set by the law , and its open-source models are also exempt from transparency obligations . ( Bloomberg )","['story', 'originally', 'appear', 'weekly', 'newsletter', 'ai', 'get', 'story', 'inbox', 'first', 'sign', 'crazy', 'year', 'ai', 'long', 'time', 'endless', 'product', 'launch', 'boardroom', 'coup', 'intense', 'policy', 'debate', 'doom', 'race', 'find', 'next', 'big', 'thing', 'also', 'see', 'concrete', 'tool', 'policy', 'aim', 'get', 'ai', 'sector', 'behave', 'responsibly', 'hold', 'powerful', 'player', 'accountable', 'give', 'lot', 'hope', 'future', 'ai', 'teach', 'year', 'start', 'big', 'tech', 'go', 'generative', 'ai', 'runaway', 'success', 'chatgpt', 'prompt', 'major', 'tech', 'company', 'release', 'version', 'year', 'go', 'history', 'year', 'see', 'launch', 'meta', 'chatbot', 'gemini', 'baidu', 'handful', 'model', 'include', 'french', 'opensource', 'challenger', 'mistral', 'initial', 'hype', 'see', 'application', 'become', 'overnight', 'success', 'pitch', 'powerful', 'aipowere', 'search', 'turn', 'dud', 'killer', 'app', 'fundamental', 'flaw', 'language', 'model', 'fact', 'frequently', 'make', 'stuff', 'lead', 'embarrassing', 'let', 'honest', 'hilarious', 'gaffe', 'bing', 'frequently', 'reply', 'people', 'question', 'conspiracy', 'theory', 'suggest', 'reporter', 'leave', 'wife', 'generate', 'factually', 'incorrect', 'answer', 'marketing', 'campaign', 'wipe', 'company', 'share', 'price', 'frenetic', 'hunt', 'popular', 'ai', 'product', 'want', 'adopt', 'openai', 'experiment', 'allow', 'company', 'developer', 'create', 'customize', 'chatbot', 'let', 'people', 'build', 'application', 'use', 'ai', 'code', 'skill', 'need', 'perhaps', 'generative', 'ai', 'end', 'embed', 'boring', 'useful', 'tool', 'help', 'boost', 'productivity', 'work', 'take', 'form', 'assistant', 'maybe', 'voice', 'capability', 'code', 'support', 'next', 'year', 'crucial', 'determine', 'real', 'value', 'generative', 'ai', 'even', 'tech', 'company', 'roll', 'large', 'language', 'model', 'product', 'frenetic', 'pace', 'still', 'lot', 'know', 'work', 'make', 'stuff', 'severe', 'gender', 'ethnic', 'bias', 'year', 'also', 'find', 'different', 'language', 'model', 'generate', 'text', 'different', 'political', 'bias', 'make', 'great', 'tool', 'hack', 'people', 'private', 'information', 'texttoimage', 'model', 'prompt', 'spit', 'copyright', 'image', 'picture', 'real', 'people', 'easily', 'trick', 'generate', 'disturbing', 'image', 'great', 'see', 'much', 'research', 'flaw', 'model', 'take', 'step', 'close', 'understanding', 'behave', 'way', 'ultimately', 'fix', 'generative', 'model', 'unpredictable', 'year', 'lot', 'attempt', 'try', 'make', 'behave', 'creator', 'want', 'share', 'use', 'technique', 'call', 'reinforcement', 'learning', 'human', 'feedback', 'use', 'feedback', 'user', 'help', 'guide', 'chatgpt', 'desirable', 'answer', 'study', 'anthropic', 'show', 'simple', 'naturallanguage', 'instruction', 'steer', 'large', 'language', 'model', 'make', 'result', 'less', 'toxic', 'sadly', 'lot', 'attempt', 'end', 'quick', 'fix', 'rather', 'permanent', 'one', 'misguided', 'approach', 'ban', 'seemingly', 'innocuous', 'word', 'placenta', 'imagegenerate', 'ai', 'system', 'avoid', 'produce', 'tech', 'company', 'come', 'workaround', 'know', 'model', 'generate', 'content', 'also', 'get', 'well', 'sense', 'true', 'carbon', 'footprint', 'generate', 'image', 'use', 'powerful', 'model', 'take', 'much', 'energy', 'fully', 'charge', 'smartphone', 'researcher', 'ai', 'hug', 'face', 'carnegie', 'find', 'exact', 'amount', 'energy', 'generative', 'use', 'miss', 'piece', 'puzzle', 'research', 'help', 'shift', 'way', 'use', 'ai', 'sustainable', 'chatter', 'possibility', 'pose', 'existential', 'risk', 'human', 'become', 'familiar', 'year', 'hundred', 'scientist', 'business', 'leader', 'policymaker', 'speak', 'deeplearne', 'pioneer', 'ceo', 'top', 'firm', 'demis', 'te', 'former', 'president', 'existential', 'risk', 'become', 'big', 'meme', 'ai', 'hypothesis', 'day', 'build', 'ai', 'far', 'smart', 'human', 'lead', 'grave', 'consequence', 'ideology', 'champion', 'many', 'silicon', 'valley', 'include', 'sutskever', 'chief', 'scientist', 'play', 'pivotal', 'role', 'oust', 'ceo', 'reinstate', 'day', 'later', 'agree', 'idea', 'meta', 'ai', 'leader', 'say', 'fear', 'ridiculous', 'conversation', 'ai', 'risk', 'unhinge', 'many', 'power', 'player', 'ai', 'researcher', 'joy', 'buolamwini', 'say', 'focus', 'hypothetical', 'risk', 'distract', 'real', 'harm', 'cause', 'today', 'nevertheless', 'increase', 'attention', 'technology', 'potential', 'cause', 'extreme', 'harm', 'prompt', 'many', 'important', 'conversation', 'ai', 'policy', 'animate', 'lawmaker', 'world', 'take', 'action', 'thank', 'chatgpt', 'talk', 'ai', 'policy', 'regulation', 'year', 'early', 'european', 'lawmaker', 'wrap', 'busy', 'policy', 'year', 'agree', 'introduce', 'bind', 'rule', 'standard', 'develop', 'risky', 'ai', 'responsibly', 'also', 'ban', 'certain', 'unacceptable', 'application', 'ai', 'police', 'use', 'facial', 'recognition', 'public', 'place', 'meanwhile', 'introduce', 'executive', 'order', 'ai', 'voluntary', 'commitment', 'lead', 'ai', 'company', 'effort', 'aim', 'bring', 'transparency', 'standard', 'ai', 'give', 'lot', 'freedom', 'agency', 'adapt', 'ai', 'rule', 'fit', 'sector', 'concrete', 'policy', 'proposal', 'get', 'lot', 'attention', 'watermark', 'invisible', 'signal', 'text', 'image', 'detect', 'computer', 'order', 'flag', 'aigenerate', 'content', 'use', 'track', 'plagiarism', 'help', 'fight', 'disinformation', 'year', 'see', 'research', 'succeed', 'apply', 'aigenerate', 'text', 'imagesit', 'lawmaker', 'busy', 'lawyer', 'see', 'record', 'number', 'lawsuit', 'artist', 'writer', 'argue', 'company', 'scrape', 'intellectual', 'property', 'consent', 'compensation', 'exciting', 'counteroffensive', 'researcher', 'develop', 'nightshade', 'new', 'datapoisone', 'tool', 'let', 'artist', 'fight', 'back', 'generative', 'ai', 'mess', 'training', 'datum', 'way', 'cause', 'serious', 'damage', 'imagegenerate', 'model', 'resistance', 'brewing', 'expect', 'grassroots', 'effort', 'shift', 'tech', 'power', 'balance', 'next', 'year', 'know', 'openai', 'superalignment', 'team', 'announce', 'first', 'result', 'superalignment', 'team', 'inhouse', 'initiative', 'dedicate', 'prevent', 'superintelligence', 'hypothetical', 'future', 'ai', 'outsmart', 'human', 'go', 'rogue', 'team', 'lead', 'chief', 'scientist', 'ilya', 'sutskever', 'part', 'group', 'last', 'month', 'fire', 'ceo', 'reinstate', 'day', 'later', 'business', 'usual', 'many', 'company', 'announcement', 'herald', 'big', 'breakthrough', 'lowkey', 'research', 'paper', 'team', 'describe', 'technique', 'let', 'less', 'powerful', 'large', 'language', 'model', 'supervise', 'powerful', 'suggest', 'small', 'step', 'figure', 'human', 'supervise', 'superhuman', 'machine', 'read', 'deepmind', 'use', 'large', 'language', 'model', 'solve', 'unsolvable', 'math', 'problemin', 'paper', 'publish', 'nature', 'company', 'say', 'first', 'time', 'large', 'language', 'model', 'use', 'discover', 'solution', 'longstanding', 'scientific', 'puzzle', 'produce', 'verifiable', 'valuable', 'new', 'information', 'previously', 'exist', 'mit', 'technology', 'review', 'new', 'system', 'teach', 'robot', 'simple', 'household', 'task', 'minutesa', 'new', 'opensource', 'system', 'call', 'dobbe', 'train', 'use', 'datum', 'collect', 'real', 'home', 'help', 'teach', 'robot', 'open', 'air', 'fryer', 'close', 'door', 'straighten', 'cushion', 'task', 'also', 'help', 'field', 'robotic', 'overcome', 'big', 'challenge', 'lack', 'training', 'datum', 'mit', 'technology', 'review', 'chatgpt', 'turn', 'internet', 'plumbinggerman', 'medium', 'giant', 'politico', 'business', 'insider', 'announce', 'partnership', 'openai', 'tech', 'company', 'able', 'use', 'news', 'article', 'training', 'datum', 'news', 'organization', 'able', 'use', 'chatgpt', 'summary', 'news', 'column', 'clever', 'point', 'tech', 'company', 'increasingly', 'become', 'gatekeeper', 'online', 'content', 'journalism', 'plumbing', 'digital', 'faucet', 'meet', 'former', 'french', 'official', 'push', 'loose', 'rule', 'join', 'startup', 'mistrala', 'profile', 'mistral', 'ai', 'use', 'digital', 'minister', 'join', 'unicorn', 'vocal', 'proponent', 'strict', 'law', 'tech', 'lobby', 'hard', 'rule', 'restrict', 'mistral', 'model', 'successful', 'company', 'model', 'meet', 'computing', 'threshold', 'set', 'law', 'opensource', 'model', 'also', 'exempt', 'transparency', 'obligation']"
These six questions will dictate the future of generative AI,https://www.technologyreview.com/2023/12/19/1084505/generative-ai-artificial-intelligence-bias-jobs-copyright-misinformation/,2023-12-19,"<p>Generative AI took the world by storm in 2023. Its future—and ours—will be shaped by what we do next.</p>
","It was a stranger who first brought home for me how big this year’s vibe shift was going to be. As we waited for a stuck elevator together in March, she told me she had just used ChatGPT to help her write a report for her marketing job. She hated writing reports because she didn’t think she was very good at it. But this time her manager had praised her. Did it feel like cheating? Hell no, she said. You do what you can to keep up. That stranger’s experience of generative AI is one among millions. People in the street (and in elevators) are now figuring out what this radical new technology is for and wondering what it can do for them. In many ways the buzz around generative AI right now recalls the early days of the internet: there’s a sense of excitement and expectancy—and a feeling that we’re making it up as we go.  That is to say, we’re in the dot-com boom, circa 2000. Many companies will go bust. It may take years before we see this era’s Facebook (now Meta), Twitter (now X), or TikTok emerge. “People are reluctant to imagine what could be the future in 10 years, because no one wants to look foolish,” says Alison Smith, head of generative AI at Booz Allen Hamilton, a technology consulting firm. “But I think it’s going to be something wildly beyond our expectations.” Here’s the catch: it is impossible to know all the ways a technology will be misused until it is used. The internet changed everything—how we work and play, how we spend time with friends and family, how we learn, how we consume, how we fall in love, and so much more. But it also brought us cyber-­bullying, revenge porn, and troll factories. It facilitated genocide, fueled mental-health crises, and made surveillance capitalism—with its addictive algorithms and predatory advertising—the dominant market force of our time. These downsides became clear only when people started using it in vast numbers and killer apps like social media arrived. Generative AI is likely to be the same. With the infrastructure in place—the base generative models from OpenAI, Google, Meta, and a handful of others—people other than the ones who built it will start using and misusing it in ways its makers never dreamed of. “We’re not going to fully understand the potential and the risks without having individual users really play around with it,” says Smith. Generative AI was trained on the internet and so has inherited many of its unsolved issues, including those related to bias, misinformation, copyright infringement, human rights abuses, and all-round economic upheaval. But we’re not going in blind.  Here are six unresolved questions to bear in mind as we watch the generative-AI revolution unfold. This time around, we have a chance to do better.  Bias has become a byword for AI-related harms, for good reason. Real-world data, especially text and images scraped from the internet, is riddled with it, from gender stereotypes to racial discrimination. Models trained on that data encode those biases and then reinforce them wherever they are used. Chatbots and image generators tend to portray engineers as white and male and nurses as white and female. Black people risk being misidentified by police departments’ facial recognition programs, leading to wrongful arrest. Hiring algorithms favor men over women, entrenching a bias they were sometimes brought in to address.  Without new data sets or a new way to train models (both of which could take years of work), the root cause of the bias problem is here to stay. But that hasn’t stopped it from being a hot topic of research. OpenAI has worked to make its large language models less biased using techniques such as reinforcement learning from human feedback (RLHF). This steers the output of a model toward the kind of text that human testers say they prefer. Other techniques involve using synthetic data sets. For example, Runway, a startup that makes generative models for video production, has trained a version of the popular image-making model Stable Diffusion on synthetic data such as AI-generated images of people who vary in ethnicity, gender, profession, and age. The company reports that models trained on this data set generate more images of people with darker skin and more images of women. Request an image of a businessperson, and outputs now include women in headscarves; images of doctors will depict people who are diverse in skin color and gender; and so on. Critics dismiss these solutions as Band-Aids on broken base models, hiding rather than fixing the problem. But Geoff Schaefer, a colleague of Smith’s at Booz Allen Hamilton who is head of responsible AI at the firm, argues that such algorithmic biases can expose societal biases in a way that’s useful in the long run.  As an example, he notes that even when explicit information about race is removed from a data set, racial bias can still skew data-driven decision-making because race can be inferred from people’s addresses—revealing patterns of segregation and housing discrimination. “We got a bunch of data together in one place, and that correlation became really clear,” he says. Schaefer thinks something similar could happen with this generation of AI: “These biases across society are going to pop out.” And that will lead to more targeted policymaking, he says. But many would balk at such optimism. Just because a problem is out in the open doesn’t guarantee it’s going to get fixed. Policymakers are still trying to address social biases that were exposed years ago—in housing, hiring, loans, policing, and more. In the meantime, individuals live with the consequences.  Prediction: Bias will continue to be an inherent feature of most generative AI models. But workarounds and rising awareness could help policymakers address the most obvious examples.  Outraged that tech companies should profit from their work without consent, artists and writers (and coders) have launched class action lawsuits against OpenAI, Microsoft, and others, claiming copyright infringement. Getty is suing Stability AI, the firm behind the image maker Stable Diffusion. These cases are a big deal. Celebrity claimants such as Sarah Silverman and George R.R. Martin have drawn media attention. And the cases are set to rewrite the rules around what does and does not count as fair use of another’s work, at least in the US. Philosophers, cognitive scientists, and engineers are grappling with what it would take for AI to become conscious. But don’t hold your breath. It will be years before the courts make their final decisions, says Katie Gardner, a partner specializing in intellectual-property licensing at the law firm Gunderson Dettmer, which represents more than 280 AI companies. By that point, she says, “the technology will be so entrenched in the economy that it’s not going to be undone.”   In the meantime, the tech industry is building on these alleged infringements at breakneck pace. “I don’t expect companies will wait and see,” says Gardner. “There may be some legal risks, but there are so many other risks with not keeping up.” Some companies have taken steps to limit the possibility of infringement. OpenAI and Meta claim to have introduced ways for creators to remove their work from future data sets. OpenAI now prevents users of DALL-E from requesting images in the style of living artists. But, Gardner says, “these are all actions to bolster their arguments in the litigation.”  Google, Microsoft, and OpenAI now offer to protect users of their models from potential legal action. Microsoft’s indemnification policy for its generative coding assistant GitHub Copilot, which is the subject of a class action lawsuit on behalf of software developers whose code it was trained on, would in principle protect those who use it while the courts shake things out. “We’ll take that burden on so the users of our products don’t have to worry about it,” Microsoft CEO Satya Nadella told MIT Technology Review.  At the same time, new kinds of licensing deals are popping up. Shutterstock has signed a six-year deal with OpenAI for the use of its images. And Adobe claims its own image-making model, called Firefly, was trained only on licensed images, images from its Adobe Stock data set, or images no longer under copyright. Some contributors to Adobe Stock, however, say they weren’t consulted and aren’t happy about it. Resentment is fierce. Now artists are fighting back with technology of their own. One tool, called Nightshade, lets users alter images in ways that are imperceptible to humans but devastating to machine-learning models, making them miscategorize images during training. Expect a big realignment of norms around sharing and repurposing media online. Prediction: High-profile lawsuits will continue to draw attention, but that’s unlikely to stop companies from building on generative models. New marketplaces will spring up around ethical data sets, and a cat-and-mouse game between companies and creators will develop. We’ve long heard that AI is coming for our jobs. One difference this time is that white-collar workers—data analysts, doctors, lawyers, and (gulp) journalists—look to be at risk too. Chatbots can ace high school tests, professional medical licensing examinations, and the bar exam. They can summarize meetings and even write basic news articles. What’s left for the rest of us? The truth is far from straightforward. Many researchers deny that the performance of large language models is evidence of true smarts. But even if it were, there is a lot more to most professional roles than the tasks those models can do. Last summer, Ethan Mollick, who studies innovation at the Wharton School of the University of Pennsylvania, helped run an experiment with the Boston Consulting Group to look at the impact of ChatGPT on consultants. The team gave hundreds of consultants 18 tasks related to a fictional shoe company, such as “Propose at least 10 ideas for a new shoe targeting an underserved market or sport” and “Segment the footwear industry market based on users.” Some of the group used ChatGPT to help them; some didn’t. The results were striking: “Consultants using ChatGPT-4 outperformed those who did not, by a lot. On every dimension. Every way we measured performance,” Mollick writes in a blog post about the study.   Many businesses are already using large language models to find and fetch information, says Nathan Benaich, founder of the VC firm Air Street Capital and leader of the team behind the State of AI Report, a comprehensive annual summary of research and industry trends. He finds that welcome: “Hopefully, analysts will just become an AI model,” he says. “This stuff’s mostly a big pain in the ass.” His point is that handing over grunt work to machines lets people focus on more fulfilling parts of their jobs. The tech also seems to level out skills across a workforce: early studies, like Mollick’s with consultants and others with coders, suggest that less experienced people get a bigger boost from using AI. (There are caveats, though. Mollick found that people who relied too much on GPT-4 got careless and were less likely to catch errors when the model made them.) Generative AI won’t just change desk jobs. Image- and video-making models could make it possible to produce endless streams of pictures and film without human illustrators, camera operators, or actors. The strikes by writers and actors in the US in 2023 made it clear that this will be a flashpoint for years to come. Even so, many researchers see this technology as empowering, not replacing, workers overall. Technology has been coming for jobs since the industrial revolution, after all. New jobs get created as old ones die out. “I feel really strongly that it is a net positive,” says Smith. But change is always painful, and net gains can hide individual losses. Technological upheaval also tends to concentrate wealth and power, fueling inequality. “In my mind, the question is no longer about whether AI is going to reshape work, but what we want that to mean,” writes Mollick. Prediction: Fears of mass job losses will prove exaggerated. But generative tools will continue to proliferate in the workplace. Roles may change; new skills may need to be learned. Three of the most viral images of 2023 were photos of the pope wearing a Balenciaga puffy, Donald Trump being wrestled to the ground by cops, and an explosion at the Pentagon. All fake; all seen and shared by millions of people. Using generative models to create fake text or images is easier than ever. Many warn of a misinformation overload. OpenAI has collaborated on research that highlights many potential misuses of its own tech for fake-news campaigns. In a 2023 report it warned that large language models could be used to produce more persuasive propaganda—harder to detect as such—at massive scales. Experts in the US and the EU are already saying that elections are at risk. It was no surprise that the Biden administration made labeling and detection of AI-generated content a focus of its executive order on artificial intelligence in October. But the order fell short of legally requiring tool makers to label text or images as the creations of an AI. And the best detection tools don’t yet work well enough to be trusted. The European Union's AI Act, agreed this month, goes further. Part of the sweeping legislation requires companies to watermark AI-generated text, images, or video, and to make it clear to people when they are interacting with a chatbot. And the AI Act has teeth: the rules will be binding and come with steep fines for noncompliance.  These are three of the most viral images of 2023. All fake; all seen and shared by millions of people. The US has also said it will audit any AI that might pose threats to national security, including election interference. It’s a great step, says Benaich. But even the developers of these models don’t know their full capabilities: “The idea that governments or other independent bodies could force companies to fully test their models before they’re released seems unrealistic.”   Here’s the catch: it’s impossible to know all the ways a technology will be misused until it is used. “In 2023 there was a lot of discussion about slowing down the development of AI,” says Schaefer. “But we take the opposite view.” Unless these tools get used by as many people in as many different ways as possible, we’re not going to make them better, he says: “We’re not going to understand the nuanced ways that these weird risks will manifest or what events will trigger them.” Prediction: New forms of misuse will continue to surface as use ramps up. There will be a few standout examples, possibly involving electoral manipulation.    The development costs of generative AI, both human and environmental, are also to be reckoned with. The invisible-worker problem is an open secret: we are spared the worst of what generative models can produce thanks in part to crowds of hidden (often poorly paid) laborers who tag training data and weed out toxic, sometimes traumatic, output during testing. These are the sweatshops of the data age. In 2023, OpenAI’s use of workers in Kenya came under scrutiny by popular media outlets such as Time and the Wall Street Journal. OpenAI wanted to improve its generative models by building a filter that would hide hateful, obscene, and otherwise offensive content from users. But to do that it needed people to find and label a large number of examples of such toxic content so that its automatic filter could learn to spot them. OpenAI had hired the outsourcing firm Sama, which in turn is alleged to have used low-paid workers in Kenya who were given little support.  With generative AI now a mainstream concern, the human costs will come into sharper focus, putting pressure on companies building these models to address the labor conditions of workers around the world who are contracted to help improve their tech. AI is making its way into decision-making in battle. Who’s to blame when something goes wrong? The other great cost, the amount of energy required to train large generative models, is set to climb before the situation gets better. In August, Nvidia announced Q2 2024 earnings of more than $13.5 billion, twice as much as the same period the year before. The bulk of that revenue ($10.3 billion) comes from data centers—in other words, other firms using Nvidia’s hardware to train AI models. “The demand is pretty extraordinary,” says Nvidia CEO Jensen Huang. “We’re at liftoff for generative AI.” He acknowledges the energy problem and predicts that the boom could even drive a change in the type of computing hardware deployed. “The vast majority of the world’s computing infrastructure will have to be energy efficient,” he says. Prediction: Greater public awareness of the labor and environmental costs of AI will put pressure on tech companies. But don’t expect significant improvement on either front soon. Doomerism—the fear that the creation of smart machines could have disastrous, even apocalyptic consequences—has long been an undercurrent in AI. But peak hype, plus a high-profile announcement from AI pioneer Geoffrey Hinton in May that he was now scared of the tech he helped build, brought it to the surface. Few issues in 2023 were as divisive. AI luminaries like Hinton and fellow Turing Award winner Yann LeCun, who founded Meta’s AI lab and who finds doomerism preposterous, engage in public spats, throwing shade at each other on social media. Hinton, OpenAI CEO Sam Altman, and others have suggested that (future) AI systems should have safeguards similar to those used for nuclear weapons. Such talk gets people’s attention. But in an article he co-wrote in Vox in July, Matt Korda, project manager for the Nuclear Information Project at the Federation of American Scientists, decried these “muddled analogies” and the “calorie-free media panic” they provoke. It’s hard to understand what’s real and what’s not because we don’t know the incentives of the people raising alarms, says Benaich: “It does seem bizarre that many people are getting extremely wealthy off the back of this stuff, and a lot of the people are the same ones who are mandating for greater control. It’s like, ‘Hey, I’ve invented something that’s really powerful! It has a lot of risks, but I have the antidote.’” Some worry about the impact of all this fearmongering. On X, deep-learning pioneer Andrew Ng wrote: “My greatest fear for the future of AI is if overhyped risks (such as human extinction) lets tech lobbyists get enacted stifling regulations that suppress open-source and crush innovation.” The debate also channels resources and researchers away from more immediate risks, such as bias, job upheavals, and misinformation (see above). “Some people push existential risk because they think it will benefit their own company,” says François Chollet, an influential AI researcher at Google. “Talking about existential risk both highlights how ethically aware and responsible you are and distracts from more realistic and pressing issues.” Benaich points out that some of the people ringing the alarm with one hand are raising $100 million for their companies with the other. “You could say that doomerism is a fundraising strategy,” he says. Prediction: The fearmongering will die down, but the influence on policymakers’ agendas may be felt for some time. Calls to refocus on more immediate harms will continue.   It’s strange to think that ChatGPT almost didn’t happen. Before its launch in November 2022, Ilya Sutskever, cofounder and chief scientist at OpenAI, wasn’t impressed by its accuracy. Others in the company worried it wasn’t much of an advance. Under the hood, ChatGPT was more remix than revolution. It was driven by GPT-3.5, a large language model that OpenAI had developed several months earlier. But the chatbot rolled a handful of engaging tweaks—in particular, responses that were more conversational and more on point—into one accessible package. “It was capable and convenient,” says Sutskever. “It was the first time AI progress became visible to people outside of AI.” The hype kicked off by ChatGPT hasn’t yet run its course. “AI is the only game in town,” says Sutskever. “It’s the biggest thing in tech, and tech is the biggest thing in the economy. And I think that we will continue to be surprised by what AI can do.” But now that we’ve seen what AI can do, maybe the immediate question is what it’s for. OpenAI built this technology without a real use in mind. Here’s a thing, the researchers seemed to say when they released ChatGPT. Do what you want with it. Everyone has been scrambling to figure out what that is since. “I find ChatGPT useful,” says Sutskever. “I use it quite regularly for all kinds of random things.” He says he uses it to look up certain words, or to help him express himself more clearly. Sometimes he uses it to look up facts (even though it’s not always factual). Other people at OpenAI use it for vacation planning (“What are the top three diving spots in the world?”) or coding tips or IT support.   Useful, but not game-changing. Most of those examples can be done with existing tools, like search. Meanwhile, staff inside Google are said to be having doubts about the usefulness of the company’s own chatbot, Bard (now powered by Google's GPT-4 rival, Gemini, launched last month). “The biggest challenge I’m still thinking of: what are LLMs truly useful for, in terms of helpfulness?” Cathy Pearl, a user experience lead for Bard, wrote on Discord in August, according to Bloomberg. “Like really making a difference. TBD!”  Without a killer app, the “wow” effect ebbs away. Stats from the investment firm Sequoia Capital show that despite viral launches, AI apps like ChatGPT, Character.ai, and Lensa, which lets users create stylized (and sexist) avatars of themselves, lose users faster than existing popular services like YouTube and Instagram and TikTok. “The laws of consumer tech still apply,” says Benaich. “There will be a lot of experimentation, a lot of things dead in the water after a couple of months of hype.” Of course, the early days of the internet were also littered with false starts. Before it changed the world, the dot-com boom ended in bust. There’s always the chance that today’s generative AI will fizzle out and be eclipsed by the next big thing to come along. Whatever happens, now that AI is fully in the mainstream, niche concerns have become everyone’s problem. As Schaefer says, “We’re going to be forced to grapple with these issues in ways that we haven’t before.”  ","It was a stranger who first brought home for me how big this year ’ s vibe shift was going to be . As we waited for a stuck elevator together in March , she told me she had just used ChatGPT to help her write a report for her marketing job . She hated writing reports because she didn ’ t think she was very good at it . But this time her manager had praised her . Did it feel like cheating ? Hell no , she said . You do what you can to keep up . That stranger ’ s experience of generative AI is one among millions . People in the street ( and in elevators ) are now figuring out what this radical new technology is for and wondering what it can do for them . In many ways the buzz around generative AI right now recalls the early days of the internet : there ’ s a sense of excitement and expectancy—and a feeling that we ’ re making it up as we go . That is to say , we ’ re in the dot-com boom , circa 2000 . Many companies will go bust . It may take years before we see this era ’ s Facebook ( now Meta ) , Twitter ( now X ) , or TikTok emerge . “ People are reluctant to imagine what could be the future in 10 years , because no one wants to look foolish , ” says Alison Smith , head of generative AI at Booz Allen Hamilton , a technology consulting firm . “ But I think it ’ s going to be something wildly beyond our expectations. ” Here ’ s the catch : it is impossible to know all the ways a technology will be misused until it is used . The internet changed everything—how we work and play , how we spend time with friends and family , how we learn , how we consume , how we fall in love , and so much more . But it also brought us cyber-­bullying , revenge porn , and troll factories . It facilitated genocide , fueled mental-health crises , and made surveillance capitalism—with its addictive algorithms and predatory advertising—the dominant market force of our time . These downsides became clear only when people started using it in vast numbers and killer apps like social media arrived . Generative AI is likely to be the same . With the infrastructure in place—the base generative models from OpenAI , Google , Meta , and a handful of others—people other than the ones who built it will start using and misusing it in ways its makers never dreamed of . “ We ’ re not going to fully understand the potential and the risks without having individual users really play around with it , ” says Smith . Generative AI was trained on the internet and so has inherited many of its unsolved issues , including those related to bias , misinformation , copyright infringement , human rights abuses , and all-round economic upheaval . But we ’ re not going in blind . Here are six unresolved questions to bear in mind as we watch the generative-AI revolution unfold . This time around , we have a chance to do better . Bias has become a byword for AI-related harms , for good reason . Real-world data , especially text and images scraped from the internet , is riddled with it , from gender stereotypes to racial discrimination . Models trained on that data encode those biases and then reinforce them wherever they are used . Chatbots and image generators tend to portray engineers as white and male and nurses as white and female . Black people risk being misidentified by police departments ’ facial recognition programs , leading to wrongful arrest . Hiring algorithms favor men over women , entrenching a bias they were sometimes brought in to address . Without new data sets or a new way to train models ( both of which could take years of work ) , the root cause of the bias problem is here to stay . But that hasn ’ t stopped it from being a hot topic of research . OpenAI has worked to make its large language models less biased using techniques such as reinforcement learning from human feedback ( RLHF ) . This steers the output of a model toward the kind of text that human testers say they prefer . Other techniques involve using synthetic data sets . For example , Runway , a startup that makes generative models for video production , has trained a version of the popular image-making model Stable Diffusion on synthetic data such as AI-generated images of people who vary in ethnicity , gender , profession , and age . The company reports that models trained on this data set generate more images of people with darker skin and more images of women . Request an image of a businessperson , and outputs now include women in headscarves ; images of doctors will depict people who are diverse in skin color and gender ; and so on . Critics dismiss these solutions as Band-Aids on broken base models , hiding rather than fixing the problem . But Geoff Schaefer , a colleague of Smith ’ s at Booz Allen Hamilton who is head of responsible AI at the firm , argues that such algorithmic biases can expose societal biases in a way that ’ s useful in the long run . As an example , he notes that even when explicit information about race is removed from a data set , racial bias can still skew data-driven decision-making because race can be inferred from people ’ s addresses—revealing patterns of segregation and housing discrimination . “ We got a bunch of data together in one place , and that correlation became really clear , ” he says . Schaefer thinks something similar could happen with this generation of AI : “ These biases across society are going to pop out. ” And that will lead to more targeted policymaking , he says . But many would balk at such optimism . Just because a problem is out in the open doesn ’ t guarantee it ’ s going to get fixed . Policymakers are still trying to address social biases that were exposed years ago—in housing , hiring , loans , policing , and more . In the meantime , individuals live with the consequences . Prediction : Bias will continue to be an inherent feature of most generative AI models . But workarounds and rising awareness could help policymakers address the most obvious examples . Outraged that tech companies should profit from their work without consent , artists and writers ( and coders ) have launched class action lawsuits against OpenAI , Microsoft , and others , claiming copyright infringement . Getty is suing Stability AI , the firm behind the image maker Stable Diffusion . These cases are a big deal . Celebrity claimants such as Sarah Silverman and George R.R . Martin have drawn media attention . And the cases are set to rewrite the rules around what does and does not count as fair use of another ’ s work , at least in the US . Philosophers , cognitive scientists , and engineers are grappling with what it would take for AI to become conscious . But don ’ t hold your breath . It will be years before the courts make their final decisions , says Katie Gardner , a partner specializing in intellectual-property licensing at the law firm Gunderson Dettmer , which represents more than 280 AI companies . By that point , she says , “ the technology will be so entrenched in the economy that it ’ s not going to be undone. ” In the meantime , the tech industry is building on these alleged infringements at breakneck pace . “ I don ’ t expect companies will wait and see , ” says Gardner . “ There may be some legal risks , but there are so many other risks with not keeping up. ” Some companies have taken steps to limit the possibility of infringement . OpenAI and Meta claim to have introduced ways for creators to remove their work from future data sets . OpenAI now prevents users of DALL-E from requesting images in the style of living artists . But , Gardner says , “ these are all actions to bolster their arguments in the litigation. ” Google , Microsoft , and OpenAI now offer to protect users of their models from potential legal action . Microsoft ’ s indemnification policy for its generative coding assistant GitHub Copilot , which is the subject of a class action lawsuit on behalf of software developers whose code it was trained on , would in principle protect those who use it while the courts shake things out . “ We ’ ll take that burden on so the users of our products don ’ t have to worry about it , ” Microsoft CEO Satya Nadella told MIT Technology Review . At the same time , new kinds of licensing deals are popping up . Shutterstock has signed a six-year deal with OpenAI for the use of its images . And Adobe claims its own image-making model , called Firefly , was trained only on licensed images , images from its Adobe Stock data set , or images no longer under copyright . Some contributors to Adobe Stock , however , say they weren ’ t consulted and aren ’ t happy about it . Resentment is fierce . Now artists are fighting back with technology of their own . One tool , called Nightshade , lets users alter images in ways that are imperceptible to humans but devastating to machine-learning models , making them miscategorize images during training . Expect a big realignment of norms around sharing and repurposing media online . Prediction : High-profile lawsuits will continue to draw attention , but that ’ s unlikely to stop companies from building on generative models . New marketplaces will spring up around ethical data sets , and a cat-and-mouse game between companies and creators will develop . We ’ ve long heard that AI is coming for our jobs . One difference this time is that white-collar workers—data analysts , doctors , lawyers , and ( gulp ) journalists—look to be at risk too . Chatbots can ace high school tests , professional medical licensing examinations , and the bar exam . They can summarize meetings and even write basic news articles . What ’ s left for the rest of us ? The truth is far from straightforward . Many researchers deny that the performance of large language models is evidence of true smarts . But even if it were , there is a lot more to most professional roles than the tasks those models can do . Last summer , Ethan Mollick , who studies innovation at the Wharton School of the University of Pennsylvania , helped run an experiment with the Boston Consulting Group to look at the impact of ChatGPT on consultants . The team gave hundreds of consultants 18 tasks related to a fictional shoe company , such as “ Propose at least 10 ideas for a new shoe targeting an underserved market or sport ” and “ Segment the footwear industry market based on users. ” Some of the group used ChatGPT to help them ; some didn ’ t . The results were striking : “ Consultants using ChatGPT-4 outperformed those who did not , by a lot . On every dimension . Every way we measured performance , ” Mollick writes in a blog post about the study . Many businesses are already using large language models to find and fetch information , says Nathan Benaich , founder of the VC firm Air Street Capital and leader of the team behind the State of AI Report , a comprehensive annual summary of research and industry trends . He finds that welcome : “ Hopefully , analysts will just become an AI model , ” he says . “ This stuff ’ s mostly a big pain in the ass. ” His point is that handing over grunt work to machines lets people focus on more fulfilling parts of their jobs . The tech also seems to level out skills across a workforce : early studies , like Mollick ’ s with consultants and others with coders , suggest that less experienced people get a bigger boost from using AI . ( There are caveats , though . Mollick found that people who relied too much on GPT-4 got careless and were less likely to catch errors when the model made them . ) Generative AI won ’ t just change desk jobs . Image- and video-making models could make it possible to produce endless streams of pictures and film without human illustrators , camera operators , or actors . The strikes by writers and actors in the US in 2023 made it clear that this will be a flashpoint for years to come . Even so , many researchers see this technology as empowering , not replacing , workers overall . Technology has been coming for jobs since the industrial revolution , after all . New jobs get created as old ones die out . “ I feel really strongly that it is a net positive , ” says Smith . But change is always painful , and net gains can hide individual losses . Technological upheaval also tends to concentrate wealth and power , fueling inequality . “ In my mind , the question is no longer about whether AI is going to reshape work , but what we want that to mean , ” writes Mollick . Prediction : Fears of mass job losses will prove exaggerated . But generative tools will continue to proliferate in the workplace . Roles may change ; new skills may need to be learned . Three of the most viral images of 2023 were photos of the pope wearing a Balenciaga puffy , Donald Trump being wrestled to the ground by cops , and an explosion at the Pentagon . All fake ; all seen and shared by millions of people . Using generative models to create fake text or images is easier than ever . Many warn of a misinformation overload . OpenAI has collaborated on research that highlights many potential misuses of its own tech for fake-news campaigns . In a 2023 report it warned that large language models could be used to produce more persuasive propaganda—harder to detect as such—at massive scales . Experts in the US and the EU are already saying that elections are at risk . It was no surprise that the Biden administration made labeling and detection of AI-generated content a focus of its executive order on artificial intelligence in October . But the order fell short of legally requiring tool makers to label text or images as the creations of an AI . And the best detection tools don ’ t yet work well enough to be trusted . The European Union 's AI Act , agreed this month , goes further . Part of the sweeping legislation requires companies to watermark AI-generated text , images , or video , and to make it clear to people when they are interacting with a chatbot . And the AI Act has teeth : the rules will be binding and come with steep fines for noncompliance . These are three of the most viral images of 2023 . All fake ; all seen and shared by millions of people . The US has also said it will audit any AI that might pose threats to national security , including election interference . It ’ s a great step , says Benaich . But even the developers of these models don ’ t know their full capabilities : “ The idea that governments or other independent bodies could force companies to fully test their models before they ’ re released seems unrealistic. ” Here ’ s the catch : it ’ s impossible to know all the ways a technology will be misused until it is used . “ In 2023 there was a lot of discussion about slowing down the development of AI , ” says Schaefer . “ But we take the opposite view. ” Unless these tools get used by as many people in as many different ways as possible , we ’ re not going to make them better , he says : “ We ’ re not going to understand the nuanced ways that these weird risks will manifest or what events will trigger them. ” Prediction : New forms of misuse will continue to surface as use ramps up . There will be a few standout examples , possibly involving electoral manipulation . The development costs of generative AI , both human and environmental , are also to be reckoned with . The invisible-worker problem is an open secret : we are spared the worst of what generative models can produce thanks in part to crowds of hidden ( often poorly paid ) laborers who tag training data and weed out toxic , sometimes traumatic , output during testing . These are the sweatshops of the data age . In 2023 , OpenAI ’ s use of workers in Kenya came under scrutiny by popular media outlets such as Time and the Wall Street Journal . OpenAI wanted to improve its generative models by building a filter that would hide hateful , obscene , and otherwise offensive content from users . But to do that it needed people to find and label a large number of examples of such toxic content so that its automatic filter could learn to spot them . OpenAI had hired the outsourcing firm Sama , which in turn is alleged to have used low-paid workers in Kenya who were given little support . With generative AI now a mainstream concern , the human costs will come into sharper focus , putting pressure on companies building these models to address the labor conditions of workers around the world who are contracted to help improve their tech . AI is making its way into decision-making in battle . Who ’ s to blame when something goes wrong ? The other great cost , the amount of energy required to train large generative models , is set to climb before the situation gets better . In August , Nvidia announced Q2 2024 earnings of more than $ 13.5 billion , twice as much as the same period the year before . The bulk of that revenue ( $ 10.3 billion ) comes from data centers—in other words , other firms using Nvidia ’ s hardware to train AI models . “ The demand is pretty extraordinary , ” says Nvidia CEO Jensen Huang . “ We ’ re at liftoff for generative AI. ” He acknowledges the energy problem and predicts that the boom could even drive a change in the type of computing hardware deployed . “ The vast majority of the world ’ s computing infrastructure will have to be energy efficient , ” he says . Prediction : Greater public awareness of the labor and environmental costs of AI will put pressure on tech companies . But don ’ t expect significant improvement on either front soon . Doomerism—the fear that the creation of smart machines could have disastrous , even apocalyptic consequences—has long been an undercurrent in AI . But peak hype , plus a high-profile announcement from AI pioneer Geoffrey Hinton in May that he was now scared of the tech he helped build , brought it to the surface . Few issues in 2023 were as divisive . AI luminaries like Hinton and fellow Turing Award winner Yann LeCun , who founded Meta ’ s AI lab and who finds doomerism preposterous , engage in public spats , throwing shade at each other on social media . Hinton , OpenAI CEO Sam Altman , and others have suggested that ( future ) AI systems should have safeguards similar to those used for nuclear weapons . Such talk gets people ’ s attention . But in an article he co-wrote in Vox in July , Matt Korda , project manager for the Nuclear Information Project at the Federation of American Scientists , decried these “ muddled analogies ” and the “ calorie-free media panic ” they provoke . It ’ s hard to understand what ’ s real and what ’ s not because we don ’ t know the incentives of the people raising alarms , says Benaich : “ It does seem bizarre that many people are getting extremely wealthy off the back of this stuff , and a lot of the people are the same ones who are mandating for greater control . It ’ s like , ‘ Hey , I ’ ve invented something that ’ s really powerful ! It has a lot of risks , but I have the antidote. ’ ” Some worry about the impact of all this fearmongering . On X , deep-learning pioneer Andrew Ng wrote : “ My greatest fear for the future of AI is if overhyped risks ( such as human extinction ) lets tech lobbyists get enacted stifling regulations that suppress open-source and crush innovation. ” The debate also channels resources and researchers away from more immediate risks , such as bias , job upheavals , and misinformation ( see above ) . “ Some people push existential risk because they think it will benefit their own company , ” says François Chollet , an influential AI researcher at Google . “ Talking about existential risk both highlights how ethically aware and responsible you are and distracts from more realistic and pressing issues. ” Benaich points out that some of the people ringing the alarm with one hand are raising $ 100 million for their companies with the other . “ You could say that doomerism is a fundraising strategy , ” he says . Prediction : The fearmongering will die down , but the influence on policymakers ’ agendas may be felt for some time . Calls to refocus on more immediate harms will continue . It ’ s strange to think that ChatGPT almost didn ’ t happen . Before its launch in November 2022 , Ilya Sutskever , cofounder and chief scientist at OpenAI , wasn ’ t impressed by its accuracy . Others in the company worried it wasn ’ t much of an advance . Under the hood , ChatGPT was more remix than revolution . It was driven by GPT-3.5 , a large language model that OpenAI had developed several months earlier . But the chatbot rolled a handful of engaging tweaks—in particular , responses that were more conversational and more on point—into one accessible package . “ It was capable and convenient , ” says Sutskever . “ It was the first time AI progress became visible to people outside of AI. ” The hype kicked off by ChatGPT hasn ’ t yet run its course . “ AI is the only game in town , ” says Sutskever . “ It ’ s the biggest thing in tech , and tech is the biggest thing in the economy . And I think that we will continue to be surprised by what AI can do. ” But now that we ’ ve seen what AI can do , maybe the immediate question is what it ’ s for . OpenAI built this technology without a real use in mind . Here ’ s a thing , the researchers seemed to say when they released ChatGPT . Do what you want with it . Everyone has been scrambling to figure out what that is since . “ I find ChatGPT useful , ” says Sutskever . “ I use it quite regularly for all kinds of random things. ” He says he uses it to look up certain words , or to help him express himself more clearly . Sometimes he uses it to look up facts ( even though it ’ s not always factual ) . Other people at OpenAI use it for vacation planning ( “ What are the top three diving spots in the world ? ” ) or coding tips or IT support . Useful , but not game-changing . Most of those examples can be done with existing tools , like search . Meanwhile , staff inside Google are said to be having doubts about the usefulness of the company ’ s own chatbot , Bard ( now powered by Google 's GPT-4 rival , Gemini , launched last month ) . “ The biggest challenge I ’ m still thinking of : what are LLMs truly useful for , in terms of helpfulness ? ” Cathy Pearl , a user experience lead for Bard , wrote on Discord in August , according to Bloomberg . “ Like really making a difference . TBD ! ” Without a killer app , the “ wow ” effect ebbs away . Stats from the investment firm Sequoia Capital show that despite viral launches , AI apps like ChatGPT , Character.ai , and Lensa , which lets users create stylized ( and sexist ) avatars of themselves , lose users faster than existing popular services like YouTube and Instagram and TikTok . “ The laws of consumer tech still apply , ” says Benaich . “ There will be a lot of experimentation , a lot of things dead in the water after a couple of months of hype. ” Of course , the early days of the internet were also littered with false starts . Before it changed the world , the dot-com boom ended in bust . There ’ s always the chance that today ’ s generative AI will fizzle out and be eclipsed by the next big thing to come along . Whatever happens , now that AI is fully in the mainstream , niche concerns have become everyone ’ s problem . As Schaefer says , “ We ’ re going to be forced to grapple with these issues in ways that we haven ’ t before . ”","['stranger', 'first', 'bring', 'home', 'big', 'year', 'vibe', 'shift', 'go', 'wait', 'stuck', 'elevator', 'together', 'tell', 'use', 'chatgpt', 'help', 'write', 'report', 'marketing', 'job', 'hate', 'write', 'report', 'think', 'good', 'time', 'manager', 'praise', 'feel', 'cheat', 'hell', 'say', 'keep', 'stranger', 'experience', 'generative', 'ai', 'million', 'people', 'street', 'elevator', 'figure', 'radical', 'new', 'technology', 'wonder', 'many', 'way', 'buzz', 'around', 'generative', 'ai', 'right', 'recall', 'early', 'day', 'internet', 'sense', 'excitement', 'expectancy', 'feeling', 'make', 'go', 'say', 'dotcom', 'boom', 'circa', 'many', 'company', 'go', 'bust', 'take', 'year', 'see', 'era', 'facebook', 'meta', 'twitter', 'tiktok', 'emerge', 'people', 'reluctant', 'imagine', 'future', 'year', 'one', 'want', 'look', 'foolish', 'say', 'head', 'generative', 'ai', 'technology', 'consulting', 'firm', 'think', 'go', 'wildly', 'expectation', 'catch', 'impossible', 'know', 'way', 'technology', 'misuse', 'use', 'internet', 'change', 'work', 'play', 'spend', 'time', 'friend', 'family', 'learn', 'consume', 'fall', 'love', 'much', 'also', 'bring', 'cyber\xadbullye', 'revenge', 'porn', 'troll', 'factory', 'facilitate', 'genocide', 'fuel', 'mentalhealth', 'crisis', 'make', 'surveillance', 'capitalism', 'addictive', 'algorithm', 'predatory', 'advertising', 'dominant', 'market', 'force', 'time', 'downside', 'become', 'clear', 'people', 'start', 'use', 'vast', 'number', 'killer', 'app', 'social', 'medium', 'arrive', 'generative', 'ai', 'likely', 'infrastructure', 'place', 'base', 'generative', 'model', 'meta', 'handful', 'people', 'one', 'build', 'start', 'use', 'misuse', 'way', 'maker', 'never', 'dream', 'go', 'fully', 'understand', 'potential', 'risk', 'individual', 'user', 'really', 'play', 'say', 'generative', 'ai', 'train', 'internet', 'inherit', 'many', 'unsolved', 'issue', 'include', 'relate', 'bias', 'misinformation', 'copyright', 'infringement', 'human', 'right', 'abuse', 'economic', 'upheaval', 'go', 'blind', 'unresolved', 'question', 'bear', 'mind', 'watch', 'unfold', 'time', 'chance', 'well', 'bias', 'become', 'byword', 'airelate', 'harm', 'good', 'reason', 'especially', 'text', 'image', 'scrape', 'internet', 'riddle', 'gender', 'stereotype', 'racial', 'discrimination', 'model', 'train', 'datum', 'encode', 'bias', 'reinforce', 'use', 'chatbot', 'image', 'generator', 'tend', 'portray', 'engineer', 'white', 'male', 'nurse', 'white', 'female', 'black', 'people', 'risk', 'misidentifie', 'police', 'department', 'facial', 'recognition', 'program', 'lead', 'wrongful', 'arrest', 'hire', 'algorithm', 'favor', 'man', 'woman', 'entrench', 'bias', 'sometimes', 'bring', 'address', 'new', 'data', 'set', 'new', 'way', 'train', 'model', 'take', 'year', 'work', 'root', 'cause', 'bias', 'problem', 'stay', 'stop', 'hot', 'topic', 'research', 'work', 'make', 'large', 'language', 'model', 'less', 'biased', 'use', 'technique', 'reinforcement', 'learning', 'human', 'feedback', 'steer', 'output', 'model', 'kind', 'text', 'human', 'tester', 'say', 'prefer', 'technique', 'involve', 'use', 'synthetic', 'datum', 'set', 'example', 'runway', 'startup', 'make', 'generative', 'model', 'video', 'production', 'train', 'version', 'popular', 'imagemake', 'model', 'stable', 'diffusion', 'synthetic', 'datum', 'aigenerate', 'image', 'people', 'vary', 'ethnicity', 'gender', 'profession', 'age', 'company', 'report', 'model', 'train', 'datum', 'set', 'generate', 'image', 'people', 'dark', 'skin', 'image', 'woman', 'request', 'image', 'businessperson', 'output', 'include', 'woman', 'headscarf', 'image', 'doctor', 'depict', 'people', 'diverse', 'skin', 'color', 'gender', 'critic', 'dismiss', 'solution', 'bandaid', 'broken', 'base', 'model', 'hide', 'rather', 'fix', 'problem', 'geoff', 'colleague', 'head', 'responsible', 'ai', 'firm', 'argue', 'algorithmic', 'bias', 'expose', 'societal', 'bias', 'way', 'useful', 'long', 'run', 'example', 'note', 'even', 'explicit', 'information', 'race', 'remove', 'data', 'set', 'racial', 'bias', 'still', 'skew', 'datadriven', 'decisionmake', 'race', 'infer', 'people', 'address', 'reveal', 'pattern', 'segregation', 'housing', 'discrimination', 'get', 'bunch', 'datum', 'together', 'place', 'correlation', 'become', 'really', 'clear', 'say', 'think', 'similar', 'happen', 'generation', 'ai', 'bias', 'society', 'go', 'pop', 'lead', 'targeted', 'policymaking', 'say', 'many', 'balk', 'optimism', 'problem', 'open', 'guarantee', 'go', 'get', 'fix', 'policymaker', 'still', 'try', 'address', 'social', 'bias', 'expose', 'year', 'ago', 'housing', 'hire', 'loan', 'policing', 'meantime', 'individual', 'live', 'consequence', 'prediction', 'bias', 'continue', 'inherent', 'feature', 'generative', 'model', 'workaround', 'rise', 'awareness', 'help', 'policymaker', 'address', 'obvious', 'example', 'outraged', 'tech', 'company', 'profit', 'work', 'consent', 'artist', 'writer', 'coder', 'launch', 'class', 'action', 'lawsuit', 'claim', 'copyright', 'infringement', 'getty', 'sue', 'stability', 'ai', 'firm', 'image', 'maker', 'stable', 'diffusion', 'case', 'big', 'deal', 'celebrity', 'claimant', 'silverman', 'rr', 'draw', 'media', 'attention', 'case', 'set', 'rewrite', 'rule', 'count', 'fair', 'use', 'work', 'least', 'philosopher', 'cognitive', 'scientist', 'engineer', 'grapple', 'take', 'ai', 'become', 'conscious', 'hold', 'breath', 'year', 'court', 'make', 'final', 'decision', 'say', 'gardner', 'partner', 'specialize', 'intellectualproperty', 'licensing', 'law', 'firm', 'gunderson', 'dettmer', 'represent', 'company', 'point', 'say', 'technology', 'entrenched', 'economy', 'go', 'undo', 'meantime', 'tech', 'industry', 'build', 'allege', 'infringement', 'breakneck', 'pace', 'expect', 'company', 'wait', 'see', 'say', 'gardner', 'legal', 'risk', 'many', 'risk', 'keep', 'company', 'take', 'step', 'limit', 'possibility', 'infringement', 'openai', 'meta', 'claim', 'introduce', 'way', 'creator', 'remove', 'work', 'future', 'data', 'set', 'prevent', 'user', 'dalle', 'request', 'image', 'style', 'live', 'artist', 'gardner', 'say', 'action', 'bolster', 'argument', 'litigation', 'offer', 'protect', 'user', 'model', 'potential', 'legal', 'action', 'indemnification', 'policy', 'generative', 'code', 'assistant', 'copilot', 'subject', 'class', 'action', 'lawsuit', 'behalf', 'software', 'developer', 'code', 'train', 'principle', 'protect', 'use', 'court', 'shake', 'thing', 'take', 'burden', 'user', 'product', 'worry', 'tell', 'technology', 'review', 'time', 'new', 'kind', 'licensing', 'deal', 'pop', 'shutterstock', 'sign', 'sixyear', 'deal', 'openai', 'use', 'image', 'adobe', 'claim', 'imagemaking', 'model', 'call', 'firefly', 'train', 'license', 'image', 'image', 'adobe', 'stock', 'datum', 'set', 'image', 'long', 'copyright', 'contributor', 'adobe', 'stock', 'however', 'say', 'consult', 'happy', 'resentment', 'fierce', 'artist', 'fight', 'back', 'technology', 'tool', 'call', 'nightshade', 'let', 'user', 'alter', 'image', 'way', 'imperceptible', 'human', 'devastate', 'machinelearning', 'model', 'make', 'miscategorize', 'image', 'training', 'expect', 'big', 'realignment', 'norm', 'share', 'repurpose', 'medium', 'online', 'prediction', 'highprofile', 'lawsuit', 'continue', 'draw', 'attention', 'unlikely', 'stop', 'company', 'build', 'generative', 'model', 'new', 'marketplace', 'spring', 'ethical', 'data', 'set', 'catandmouse', 'game', 'company', 'creator', 'develop', 'long', 'hear', 'come', 'job', 'difference', 'time', 'whitecollar', 'worker', 'datum', 'analyst', 'doctor', 'lawyer', 'gulp', 'journalist', 'look', 'risk', 'chatbot', 'ace', 'high', 'school', 'test', 'professional', 'medical', 'licensing', 'examination', 'bar', 'exam', 'summarize', 'meeting', 'even', 'write', 'basic', 'news', 'article', 'leave', 'rest', 'truth', 'far', 'straightforward', 'many', 'researcher', 'deny', 'performance', 'large', 'language', 'model', 'evidence', 'true', 'smart', 'even', 'lot', 'professional', 'role', 'task', 'model', 'last', 'summer', 'mollick', 'study', 'innovation', 'school', 'help', 'run', 'experiment', 'group', 'look', 'impact', 'chatgpt', 'consultant', 'team', 'give', 'hundred', 'consultant', 'task', 'relate', 'fictional', 'shoe', 'company', 'propose', 'least', 'idea', 'new', 'shoe', 'target', 'underserved', 'market', 'sport', 'segment', 'footwear', 'industry', 'market', 'base', 'user', 'group', 'use', 'chatgpt', 'help', 'result', 'strike', 'consultant', 'use', 'chatgpt4', 'outperform', 'lot', 'dimension', 'way', 'measure', 'performance', 'mollick', 'write', 'blog', 'post', 'study', 'many', 'business', 'already', 'use', 'large', 'language', 'model', 'find', 'fetch', 'information', 'say', 'founder', 'capital', 'leader', 'team', 'state', 'report', 'comprehensive', 'annual', 'summary', 'research', 'industry', 'trend', 'find', 'welcome', 'hopefully', 'analyst', 'become', 'model', 'say', 'stuff', 'mostly', 'big', 'pain', 'ass', 'point', 'hand', 'grunt', 'work', 'machine', 'let', 'people', 'focus', 'fulfil', 'part', 'job', 'tech', 'also', 'seem', 'level', 'skill', 'workforce', 'early', 'study', 'mollick', 'consultant', 'coder', 'suggest', 'less', 'experienced', 'people', 'get', 'big', 'boost', 'use', 'caveat', 'mollick', 'find', 'people', 'rely', 'much', 'gpt4', 'get', 'careless', 'less', 'likely', 'catch', 'error', 'model', 'make', 'generative', 'win', 'change', 'desk', 'job', 'image', 'videomaking', 'model', 'make', 'possible', 'produce', 'endless', 'stream', 'picture', 'film', 'human', 'illustrator', 'camera', 'operator', 'actor', 'strike', 'writer', 'actor', 'make', 'clear', 'flashpoint', 'year', 'come', 'even', 'many', 'researcher', 'see', 'technology', 'empower', 'replace', 'worker', 'overall', 'technology', 'come', 'job', 'industrial', 'revolution', 'new', 'job', 'create', 'old', 'one', 'die', 'feel', 'really', 'strongly', 'net', 'positive', 'say', 'change', 'always', 'painful', 'net', 'gain', 'hide', 'individual', 'loss', 'technological', 'upheaval', 'also', 'tend', 'concentrate', 'wealth', 'power', 'fuel', 'inequality', 'mind', 'question', 'long', 'go', 'reshape', 'work', 'want', 'mean', 'write', 'mollick', 'prediction', 'fear', 'mass', 'job', 'loss', 'prove', 'exaggerated', 'generative', 'tool', 'continue', 'proliferate', 'workplace', 'role', 'change', 'new', 'skill', 'need', 'learn', 'viral', 'image', 'photo', 'pope', 'wear', 'balenciaga', 'puffy', 'trump', 'wrestle', 'ground', 'cop', 'explosion', 'fake', 'see', 'share', 'million', 'people', 'use', 'generative', 'model', 'create', 'fake', 'text', 'image', 'easy', 'ever', 'many', 'warn', 'misinformation', 'overload', 'collaborate', 'research', 'highlight', 'many', 'potential', 'misuse', 'tech', 'fakenews', 'campaign', 'report', 'warn', 'large', 'language', 'model', 'use', 'produce', 'persuasive', 'propaganda', 'hard', 'detect', 'massive', 'scale', 'expert', 'already', 'say', 'election', 'risk', 'surprise', 'administration', 'make', 'labeling', 'detection', 'aigenerate', 'content', 'focus', 'executive', 'order', 'artificial', 'intelligence', 'order', 'fall', 'short', 'legally', 'require', 'tool', 'maker', 'label', 'text', 'image', 'creation', 'ai', 'good', 'detection', 'tool', 'yet', 'work', 'well', 'enough', 'trust', 'agree', 'month', 'go', 'part', 'sweeping', 'legislation', 'require', 'company', 'watermark', 'aigenerate', 'text', 'image', 'video', 'make', 'clear', 'people', 'interact', 'chatbot', 'tooth', 'rule', 'bind', 'come', 'steep', 'fine', 'noncompliance', 'viral', 'image', 'fake', 'see', 'share', 'million', 'people', 'also', 'say', 'audit', 'ai', 'pose', 'threat', 'national', 'security', 'include', 'election', 'interference', 'great', 'step', 'say', 'benaich', 'even', 'developer', 'model', 'know', 'full', 'capability', 'idea', 'government', 'independent', 'body', 'force', 'company', 'fully', 'test', 'model', 'release', 'seem', 'unrealistic', 'catch', 'impossible', 'know', 'way', 'technology', 'misuse', 'use', 'lot', 'discussion', 'slow', 'development', 'ai', 'say', 'schaefer', 'take', 'opposite', 'view', 'tool', 'use', 'many', 'people', 'many', 'different', 'way', 'possible', 'go', 'make', 'well', 'say', 'go', 'understand', 'nuance', 'way', 'weird', 'risk', 'manifest', 'event', 'trigger', 'prediction', 'new', 'form', 'misuse', 'continue', 'surface', 'use', 'ramp', 'standout', 'example', 'possibly', 'involve', 'electoral', 'manipulation', 'development', 'cost', 'generative', 'ai', 'human', 'environmental', 'also', 'reckon', 'invisibleworker', 'problem', 'open', 'secret', 'spare', 'bad', 'generative', 'model', 'produce', 'thank', 'part', 'crowd', 'hidden', 'often', 'poorly', 'pay', 'laborer', 'tag', 'training', 'datum', 'weed', 'toxic', 'sometimes', 'traumatic', 'output', 'test', 'sweatshop', 'datum', 'age', 'use', 'worker', 'come', 'scrutiny', 'popular', 'media', 'outlet', 'time', 'want', 'improve', 'generative', 'model', 'build', 'filter', 'hide', 'hateful', 'obscene', 'otherwise', 'offensive', 'content', 'user', 'need', 'people', 'find', 'label', 'large', 'number', 'example', 'toxic', 'content', 'automatic', 'filter', 'learn', 'spot', 'hire', 'outsourcing', 'firm', 'turn', 'allege', 'use', 'lowpaid', 'worker', 'give', 'little', 'support', 'generative', 'ai', 'mainstream', 'concern', 'human', 'cost', 'come', 'sharp', 'focus', 'put', 'pressure', 'company', 'build', 'model', 'address', 'labor', 'condition', 'worker', 'world', 'contract', 'help', 'improve', 'tech', 'ai', 'make', 'way', 'decisionmake', 'battle', 'blame', 'go', 'wrong', 'great', 'cost', 'amount', 'energy', 'require', 'train', 'large', 'generative', 'model', 'set', 'climb', 'situation', 'get', 'well', 'announce', 'q2', 'earning', 'twice', 'much', 'period', 'year', 'bulk', 'revenue', 'come', 'data', 'center', 'word', 'firm', 'use', 'hardware', 'train', 'model', 'demand', 'pretty', 'extraordinary', 'say', 'liftoff', 'generative', 'ai', 'acknowledge', 'energy', 'problem', 'predict', 'boom', 'even', 'drive', 'change', 'type', 'computing', 'hardware', 'deploy', 'vast', 'majority', 'world', 'compute', 'infrastructure', 'energy', 'efficient', 'say', 'prediction', 'great', 'public', 'awareness', 'labor', 'environmental', 'cost', 'ai', 'put', 'pressure', 'tech', 'company', 'expect', 'significant', 'improvement', 'front', 'soon', 'doomerism', 'fear', 'creation', 'smart', 'machine', 'disastrous', 'even', 'apocalyptic', 'consequence', 'long', 'undercurrent', 'ai', 'hype', 'highprofile', 'announcement', 'scared', 'tech', 'help', 'build', 'bring', 'surface', 'issue', 'divisive', 'ai', 'luminary', 'fellow', 'ture', 'award', 'winner', 'found', 'meta', 'ai', 'lab', 'find', 'doomerism', 'preposterous', 'engage', 'public', 'spat', 'throw', 'shade', 'social', 'medium', 'suggest', 'future', 'ai', 'system', 'safeguard', 'similar', 'use', 'nuclear', 'weapon', 'talk', 'get', 'people', 'attention', 'article', 'cowrote', 'project', 'manager', 'nuclear', 'information', 'project', 'scientist', 'decry', 'muddle', 'analogy', 'caloriefree', 'medium', 'panic', 'provoke', 'hard', 'understand', 'real', 'know', 'incentive', 'people', 'raise', 'alarm', 'say', 'benaich', 'seem', 'bizarre', 'many', 'people', 'get', 'extremely', 'wealthy', 'back', 'stuff', 'lot', 'people', 'one', 'mandate', 'great', 'control', 'invent', 'really', 'powerful', 'lot', 'risk', 'antidote', 'worry', 'impact', 'fearmongere', 'deeplearning', 'write', 'great', 'fear', 'future', 'ai', 'overhype', 'risk', 'human', 'extinction', 'let', 'tech', 'lobbyist', 'enact', 'stifle', 'regulation', 'suppress', 'opensource', 'crush', 'innovation', 'debate', 'also', 'channel', 'resource', 'researcher', 'away', 'immediate', 'risk', 'bias', 'job', 'upheaval', 'misinformation', 'see', 'people', 'push', 'existential', 'risk', 'think', 'benefit', 'company', 'say', 'françois', 'influential', 'researcher', 'talk', 'existential', 'risk', 'highlight', 'ethically', 'aware', 'responsible', 'distract', 'realistic', 'press', 'issue', 'benaich', 'point', 'people', 'ring', 'alarm', 'hand', 'raise', 'company', 'say', 'doomerism', 'fundraising', 'strategy', 'say', 'prediction', 'fearmongere', 'die', 'influence', 'policymaker', 'agenda', 'feel', 'time', 'call', 'refocus', 'immediate', 'harm', 'continue', 'strange', 'think', 'chatgpt', 'almost', 'happen', 'launch', 'ilya', 'sutskever', 'cofounder', 'chief', 'scientist', 'impress', 'accuracy', 'company', 'worry', 'much', 'advance', 'hood', 'chatgpt', 'remix', 'revolution', 'drive', 'gpt35', 'large', 'language', 'model', 'develop', 'several', 'month', 'early', 'chatbot', 'roll', 'handful', 'engage', 'tweak', 'particular', 'response', 'conversational', 'point', 'accessible', 'package', 'capable', 'convenient', 'say', 'sutskever', 'first', 'time', 'progress', 'become', 'visible', 'people', 'ai', 'hype', 'kick', 'chatgpt', 'yet', 'run', 'course', 'ai', 'game', 'town', 'say', 'sutskever', 'big', 'thing', 'tech', 'tech', 'big', 'thing', 'economy', 'think', 'continue', 'surprise', 'ai', 'see', 'ai', 'maybe', 'immediate', 'question', 'openai', 'build', 'technology', 'real', 'use', 'mind', 'thing', 'researcher', 'seem', 'say', 'release', 'chatgpt', 'want', 'scramble', 'figure', 'find', 'chatgpt', 'useful', 'say', 'sutskever', 'use', 'quite', 'regularly', 'kind', 'random', 'thing', 'say', 'use', 'look', 'certain', 'word', 'help', 'express', 'clearly', 'sometimes', 'use', 'look', 'fact', 'even', 'always', 'factual', 'people', 'use', 'vacation', 'planning', 'top', 'diving', 'spot', 'world', 'code', 'tip', 'support', 'useful', 'gamechange', 'example', 'exist', 'tool', 'search', 'meanwhile', 'staff', 'say', 'doubt', 'usefulness', 'company', 'chatbot', 'bard', 'power', 'rival', 'gemini', 'launch', 'last', 'month', 'big', 'challenge', 'still', 'think', 'llm', 'truly', 'useful', 'term', 'helpfulness', 'cathy', 'pearl', 'user', 'experience', 'lead', 'bard', 'write', 'discord', 'accord', 'really', 'make', 'difference', 'tbd', 'killer', 'app', 'wow', 'effect', 'ebb', 'away', 'stat', 'investment', 'firm', 'sequoia', 'capital', 'show', 'viral', 'launch', 'ai', 'app', 'chatgpt', 'characterai', 'let', 'user', 'create', 'stylize', 'sexist', 'avatar', 'lose', 'user', 'fast', 'exist', 'popular', 'service', 'youtube', 'instagram', 'tiktok', 'law', 'consumer', 'tech', 'still', 'apply', 'say', 'benaich', 'lot', 'experimentation', 'lot', 'thing', 'dead', 'water', 'couple', 'month', 'hype', 'course', 'early', 'day', 'internet', 'also', 'litter', 'false', 'start', 'change', 'world', 'dotcom', 'boom', 'end', 'bust', 'always', 'chance', 'today', 'generative', 'ai', 'fizzle', 'eclipse', 'next', 'big', 'thing', 'come', 'happen', 'ai', 'fully', 'mainstream', 'niche', 'concern', 'become', 'problem', 'say', 'go', 'force', 'grapple', 'issue', 'way']"
Navigating a shifting customer-engagement landscape with generative AI,https://www.technologyreview.com/2023/12/18/1085299/navigating-a-shifting-customer-engagement-landscape-with-generative-ai/,2023-12-18,"One can’t step into the same river twice. This simple representation of change as the only constant was taught by the Greek philosopher Heraclitus more than 2000 years ago. Today, it rings truer than ever with the advent of generative AI. The emergence of generative AI is having a profound effect on today’s enterprises—business leaders…","In partnership with One can’t step into the same river twice. This simple representation of change as the only constant was taught by the Greek philosopher Heraclitus more than 2000 years ago. Today, it rings truer than ever with the advent of generative AI. The emergence of generative AI is having a profound effect on today’s enterprises—business leaders face a rapidly changing technology that they need to grasp to meet evolving consumer expectations. “Across all industries, customers are at the core, and tapping into their latent needs is one of the most important elements to sustain and grow a business,” says Akhilesh Ayer, executive vice president and global head of AI, analytics, data, and research practice at WNS Triange, a unit of WNS Global Services, a leading business process management company. “Generative AI is a new way for companies to realize this need.” Generative AI’s ability to harness customer data in a highly sophisticated manner means enterprises are accelerating plans to invest in and leverage the technology’s capabilities. In a study titled “The Future of Enterprise Data & AI,” Corinium Intelligence and WNS Triange surveyed 100 global C-suite leaders and decision-makers specializing in AI, analytics, and data. Seventy-six percent of the respondents said that their organizations are already using or planning to use generative AI. According to McKinsey, while generative AI will affect most business functions, “four of them will likely account for 75% of the total annual value it can deliver.” Among these are marketing and sales and customer operations. Yet, despite the technology’s benefits, many leaders are unsure about the right approach to take and mindful of the risks associated with large investments. One of the first challenges organizations need to overcome is senior leadership alignment. “You need the necessary strategy; you need the ability to have the necessary buy-in of people,” says Ayer. “You need to make sure that you've got the right use case and business case for each one of them.” In other words, a clearly defined roadmap and precise business objectives are as crucial as understanding whether a process is amenable to the use of generative AI. The implementation of a generative AI strategy can take time. According to Ayer, business leaders should maintain a realistic perspective on the duration required for formulating a strategy, conduct necessary training across various teams and functions, and identify the areas of value addition. And for any generative AI deployment to work seamlessly, the right data ecosystems must be in place. Ayer cites WNS Triange’s collaboration with an insurer to create a claims process by leveraging generative AI. Thanks to the new technology, the insurer can immediately assess the severity of a vehicle’s damage from an accident and make a claims recommendation based on the unstructured data provided by the client. “Because this can be immediately assessed by a surveyor and they can reach a recommendation quickly, this instantly improves the insurer’s ability to satisfy their policyholders and reduce the claims processing time,” Ayer explains. All that, however, would not be possible without data on past claims history, repair costs, transaction data, and other necessary data sets to extract clear value from generative AI analysis. “Be very clear about data sufficiency. Don't jump into a program where eventually you realize you don't have the necessary data,” Ayer says. Enterprises are increasingly aware that they must embrace generative AI, but knowing where to begin is another thing. “You start off wanting to make sure you don't repeat mistakes other people have made,” says Ayer. An external provider can help organizations avoid those mistakes and leverage best practices and frameworks for testing and defining explainability and benchmarks for return on investment (ROI). Using pre-built solutions by external partners can expedite time to market and increase a generative AI program’s value. These solutions can harness pre-built industry-specific generative AI platforms to accelerate deployment. “Generative AI programs can be extremely complicated,” Ayer points out. “There are a lot of infrastructure requirements, touch points with customers, and internal regulations. Organizations will also have to consider using pre-built solutions to accelerate speed to value. Third-party service providers bring the expertise of having an integrated approach to all these elements.” Ayer offers the example of WNS Triange helping a travel intermediary use generative AI to deal with customer inquiries about airline rescheduling, cancellations, and other itinerary complications. “Our solution is immediately able to go into a thousand policy documents, pick out the policy parameters relevant to the query… and then come back quickly not only with the response but with a nice, summarized, human-like response,” he says. In another example, Ayer shares that his company helped a global retailer create generative AI–driven designs for personalized gift cards. “The customer experience goes up tremendously,” he says. As with any emerging technology, however, there are organizational, technical, and implementation barriers to overcome when adopting generative AI. Organizational:  One of the major hurdles businesses can face is people. “There is often immediate resistance to the adoption of generative AI because it affects the way people work on a daily basis,” says Ayer. As a result, securing internal buy-in from all teams and being mindful of a skills gap is a must. Additionally, the ability to create a business case for investment—and getting buy-in from the C-suite—will help expedite the adoption of generative AI tools. Technical: The second set of obstacles relates to large language models (LLMs) and mechanisms to safeguard against hallucinations and bias and ensure data quality. “Companies need to figure out if generative AI can solve the whole problem or if they still need human input to validate the outputs from LLM models,” Ayer explains. At the same time, organizations must ask whether the generative AI models being used have been appropriately trained within the customer context or with the enterprise’s own data and insights. If not, there is a high chance that the response will be incorrect. Another related challenge is bias: If the underlying data has certain biases, the modeling of the LLM could be unfair. “There have to be mechanisms to address that,” says Ayer. Other issues, such as data quality, output authenticity, and explainability, also must be addressed. Implementation: The final set of challenges relates to actual implementation. The cost of implementation can be significant, especially if companies cannot orchestrate a viable solution, says Ayer. In addition, the right infrastructure and people must be in place to avoid resource constraints. And users must be convinced that the output will be relevant and of high quality, so as to gain their acceptance for the program’s implementation. Lastly, privacy and ethical issues must be addressed. The Corinium Intelligence and WNS Triange survey showed that almost 72% of respondents were concerned about ethical AI decision-making. The focus of future investment The entire ecosystem of generative AI is moving quickly. Enterprises must be agile and adapt quickly to change to ensure customer expectations are met and maintain a competitive edge. While it is almost impossible to anticipate what’s next with such a new and fast-developing technology, Ayer says that organizations that want to harness the potential of generative AI are likely to increase investment in three areas: However, it shouldn’t be a case of throwing everything at the wall and seeing what sticks. Ayer advises that organizations examine ROI from the effectiveness of services or products provided to customers. Business leaders must clearly demonstrate and measure a marked improvement in customer satisfaction levels using generative AI–based interventions. “Along with a defined generative AI strategy, companies need to understand how to apply and build use cases, how to execute them at scale and speed to market, and how to measure their success,” says Ayer. Leveraging generative AI for customer engagement is typically a multi-pronged approach, and a successful partnership can help with every stage. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In partnership with One can ’ t step into the same river twice . This simple representation of change as the only constant was taught by the Greek philosopher Heraclitus more than 2000 years ago . Today , it rings truer than ever with the advent of generative AI . The emergence of generative AI is having a profound effect on today ’ s enterprises—business leaders face a rapidly changing technology that they need to grasp to meet evolving consumer expectations . “ Across all industries , customers are at the core , and tapping into their latent needs is one of the most important elements to sustain and grow a business , ” says Akhilesh Ayer , executive vice president and global head of AI , analytics , data , and research practice at WNS Triange , a unit of WNS Global Services , a leading business process management company . “ Generative AI is a new way for companies to realize this need. ” Generative AI ’ s ability to harness customer data in a highly sophisticated manner means enterprises are accelerating plans to invest in and leverage the technology ’ s capabilities . In a study titled “ The Future of Enterprise Data & AI , ” Corinium Intelligence and WNS Triange surveyed 100 global C-suite leaders and decision-makers specializing in AI , analytics , and data . Seventy-six percent of the respondents said that their organizations are already using or planning to use generative AI . According to McKinsey , while generative AI will affect most business functions , “ four of them will likely account for 75 % of the total annual value it can deliver. ” Among these are marketing and sales and customer operations . Yet , despite the technology ’ s benefits , many leaders are unsure about the right approach to take and mindful of the risks associated with large investments . One of the first challenges organizations need to overcome is senior leadership alignment . “ You need the necessary strategy ; you need the ability to have the necessary buy-in of people , ” says Ayer . “ You need to make sure that you 've got the right use case and business case for each one of them. ” In other words , a clearly defined roadmap and precise business objectives are as crucial as understanding whether a process is amenable to the use of generative AI . The implementation of a generative AI strategy can take time . According to Ayer , business leaders should maintain a realistic perspective on the duration required for formulating a strategy , conduct necessary training across various teams and functions , and identify the areas of value addition . And for any generative AI deployment to work seamlessly , the right data ecosystems must be in place . Ayer cites WNS Triange ’ s collaboration with an insurer to create a claims process by leveraging generative AI . Thanks to the new technology , the insurer can immediately assess the severity of a vehicle ’ s damage from an accident and make a claims recommendation based on the unstructured data provided by the client . “ Because this can be immediately assessed by a surveyor and they can reach a recommendation quickly , this instantly improves the insurer ’ s ability to satisfy their policyholders and reduce the claims processing time , ” Ayer explains . All that , however , would not be possible without data on past claims history , repair costs , transaction data , and other necessary data sets to extract clear value from generative AI analysis . “ Be very clear about data sufficiency . Do n't jump into a program where eventually you realize you do n't have the necessary data , ” Ayer says . Enterprises are increasingly aware that they must embrace generative AI , but knowing where to begin is another thing . “ You start off wanting to make sure you do n't repeat mistakes other people have made , ” says Ayer . An external provider can help organizations avoid those mistakes and leverage best practices and frameworks for testing and defining explainability and benchmarks for return on investment ( ROI ) . Using pre-built solutions by external partners can expedite time to market and increase a generative AI program ’ s value . These solutions can harness pre-built industry-specific generative AI platforms to accelerate deployment . “ Generative AI programs can be extremely complicated , ” Ayer points out . “ There are a lot of infrastructure requirements , touch points with customers , and internal regulations . Organizations will also have to consider using pre-built solutions to accelerate speed to value . Third-party service providers bring the expertise of having an integrated approach to all these elements. ” Ayer offers the example of WNS Triange helping a travel intermediary use generative AI to deal with customer inquiries about airline rescheduling , cancellations , and other itinerary complications . “ Our solution is immediately able to go into a thousand policy documents , pick out the policy parameters relevant to the query… and then come back quickly not only with the response but with a nice , summarized , human-like response , ” he says . In another example , Ayer shares that his company helped a global retailer create generative AI–driven designs for personalized gift cards . “ The customer experience goes up tremendously , ” he says . As with any emerging technology , however , there are organizational , technical , and implementation barriers to overcome when adopting generative AI . Organizational : One of the major hurdles businesses can face is people . “ There is often immediate resistance to the adoption of generative AI because it affects the way people work on a daily basis , ” says Ayer . As a result , securing internal buy-in from all teams and being mindful of a skills gap is a must . Additionally , the ability to create a business case for investment—and getting buy-in from the C-suite—will help expedite the adoption of generative AI tools . Technical : The second set of obstacles relates to large language models ( LLMs ) and mechanisms to safeguard against hallucinations and bias and ensure data quality . “ Companies need to figure out if generative AI can solve the whole problem or if they still need human input to validate the outputs from LLM models , ” Ayer explains . At the same time , organizations must ask whether the generative AI models being used have been appropriately trained within the customer context or with the enterprise ’ s own data and insights . If not , there is a high chance that the response will be incorrect . Another related challenge is bias : If the underlying data has certain biases , the modeling of the LLM could be unfair . “ There have to be mechanisms to address that , ” says Ayer . Other issues , such as data quality , output authenticity , and explainability , also must be addressed . Implementation : The final set of challenges relates to actual implementation . The cost of implementation can be significant , especially if companies can not orchestrate a viable solution , says Ayer . In addition , the right infrastructure and people must be in place to avoid resource constraints . And users must be convinced that the output will be relevant and of high quality , so as to gain their acceptance for the program ’ s implementation . Lastly , privacy and ethical issues must be addressed . The Corinium Intelligence and WNS Triange survey showed that almost 72 % of respondents were concerned about ethical AI decision-making . The focus of future investment The entire ecosystem of generative AI is moving quickly . Enterprises must be agile and adapt quickly to change to ensure customer expectations are met and maintain a competitive edge . While it is almost impossible to anticipate what ’ s next with such a new and fast-developing technology , Ayer says that organizations that want to harness the potential of generative AI are likely to increase investment in three areas : However , it shouldn ’ t be a case of throwing everything at the wall and seeing what sticks . Ayer advises that organizations examine ROI from the effectiveness of services or products provided to customers . Business leaders must clearly demonstrate and measure a marked improvement in customer satisfaction levels using generative AI–based interventions . “ Along with a defined generative AI strategy , companies need to understand how to apply and build use cases , how to execute them at scale and speed to market , and how to measure their success , ” says Ayer . Leveraging generative AI for customer engagement is typically a multi-pronged approach , and a successful partnership can help with every stage . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'step', 'river', 'simple', 'representation', 'change', 'constant', 'teach', 'greek', 'philosopher', 'heraclitus', 'year', 'ago', 'today', 'ring', 'truer', 'ever', 'advent', 'generative', 'ai', 'emergence', 'generative', 'ai', 'profound', 'effect', 'today', 'enterprise', 'business', 'leader', 'face', 'rapidly', 'change', 'technology', 'need', 'grasp', 'meet', 'evolve', 'consumer', 'expectation', 'industry', 'customer', 'core', 'tap', 'latent', 'need', 'important', 'element', 'sustain', 'grow', 'business', 'say', 'executive', 'vice', 'president', 'global', 'head', 'ai', 'analytic', 'datum', 'research', 'practice', 'unit', 'global', 'service', 'lead', 'business', 'process', 'management', 'company', 'generative', 'ai', 'new', 'way', 'company', 'realize', 'need', 'generative', 'ai', 'ability', 'harness', 'customer', 'datum', 'highly', 'sophisticated', 'manner', 'mean', 'enterprise', 'accelerate', 'plan', 'invest', 'leverage', 'technology', 'capability', 'study', 'title', 'future', 'enterprise', 'datum', 'ai', 'corinium', 'intelligence', 'survey', 'global', 'csuite', 'leader', 'decisionmaker', 'specialize', 'analytic', 'datum', 'seventysix', 'percent', 'respondent', 'say', 'organization', 'already', 'use', 'plan', 'use', 'generative', 'ai', 'accord', 'mckinsey', 'generative', 'ai', 'affect', 'business', 'function', 'likely', 'account', 'total', 'annual', 'value', 'deliver', 'marketing', 'sale', 'customer', 'operation', 'yet', 'technology', 'benefit', 'many', 'leader', 'unsure', 'right', 'approach', 'take', 'mindful', 'risk', 'associate', 'large', 'investment', 'first', 'challenge', 'organization', 'need', 'overcome', 'senior', 'leadership', 'alignment', 'need', 'necessary', 'strategy', 'need', 'ability', 'necessary', 'buyin', 'people', 'say', 'ayer', 'need', 'make', 'sure', 'get', 'right', 'use', 'case', 'business', 'case', 'word', 'clearly', 'define', 'roadmap', 'precise', 'business', 'objective', 'crucial', 'understand', 'process', 'amenable', 'use', 'generative', 'ai', 'implementation', 'generative', 'ai', 'strategy', 'take', 'time', 'accord', 'ayer', 'business', 'leader', 'maintain', 'realistic', 'perspective', 'duration', 'require', 'formulate', 'strategy', 'conduct', 'necessary', 'training', 'various', 'team', 'function', 'identify', 'area', 'value', 'addition', 'generative', 'ai', 'deployment', 'work', 'seamlessly', 'right', 'datum', 'ecosystem', 'place', 'ayer', 'cite', 'collaboration', 'insurer', 'create', 'claim', 'process', 'leverage', 'generative', 'ai', 'thank', 'new', 'technology', 'insurer', 'immediately', 'assess', 'severity', 'vehicle', 'damage', 'accident', 'make', 'claim', 'recommendation', 'base', 'unstructured', 'datum', 'provide', 'client', 'immediately', 'assess', 'surveyor', 'reach', 'recommendation', 'quickly', 'instantly', 'improve', 'insurer', 'ability', 'satisfy', 'policyholder', 'reduce', 'claim', 'processing', 'time', 'ayer', 'explain', 'however', 'possible', 'datum', 'past', 'claim', 'history', 'repair', 'cost', 'transaction', 'datum', 'necessary', 'data', 'set', 'extract', 'clear', 'value', 'generative', 'ai', 'analysis', 'clear', 'data', 'sufficiency', 'jump', 'program', 'eventually', 'realize', 'necessary', 'datum', 'ayer', 'say', 'enterprise', 'increasingly', 'aware', 'embrace', 'generative', 'ai', 'know', 'begin', 'thing', 'start', 'want', 'make', 'sure', 'repeat', 'mistake', 'people', 'make', 'say', 'ayer', 'external', 'provider', 'help', 'organization', 'avoid', 'mistake', 'leverage', 'good', 'practice', 'framework', 'testing', 'define', 'explainability', 'benchmark', 'return', 'investment', 'roi', 'use', 'prebuilt', 'solution', 'external', 'partner', 'expedite', 'time', 'market', 'increase', 'generative', 'ai', 'program', 'value', 'solution', 'harness', 'prebuilt', 'industryspecific', 'generative', 'ai', 'platform', 'accelerate', 'deployment', 'generative', 'ai', 'program', 'extremely', 'complicated', 'ayer', 'point', 'lot', 'infrastructure', 'requirement', 'touch', 'point', 'customer', 'internal', 'regulation', 'organization', 'also', 'consider', 'use', 'prebuilt', 'solution', 'accelerate', 'speed', 'value', 'thirdparty', 'service', 'provider', 'bring', 'expertise', 'integrate', 'approach', 'element', 'ayer', 'offer', 'example', 'triange', 'help', 'travel', 'intermediary', 'use', 'generative', 'ai', 'deal', 'customer', 'inquiry', 'airline', 'reschedule', 'cancellation', 'itinerary', 'complication', 'solution', 'immediately', 'able', 'go', 'policy', 'document', 'pick', 'policy', 'parameter', 'relevant', 'query', 'come', 'back', 'quickly', 'response', 'nice', 'summarize', 'humanlike', 'response', 'say', 'example', 'ayer', 'share', 'company', 'help', 'global', 'retailer', 'create', 'generative', 'ai', 'drive', 'design', 'personalized', 'gift', 'card', 'customer', 'experience', 'go', 'tremendously', 'say', 'emerge', 'technology', 'however', 'organizational', 'technical', 'implementation', 'barrier', 'overcome', 'adopt', 'generative', 'ai', 'organizational', 'major', 'hurdle', 'business', 'face', 'people', 'often', 'immediate', 'resistance', 'adoption', 'generative', 'ai', 'affect', 'way', 'people', 'work', 'daily', 'basis', 'say', 'ayer', 'result', 'secure', 'internal', 'buyin', 'team', 'mindful', 'skill', 'gap', 'additionally', 'ability', 'create', 'business', 'case', 'investment', 'get', 'csuite', 'help', 'expedite', 'adoption', 'generative', 'ai', 'tool', 'technical', 'second', 'set', 'obstacle', 'relate', 'large', 'language', 'model', 'llm', 'mechanism', 'safeguard', 'hallucination', 'bias', 'ensure', 'data', 'quality', 'company', 'need', 'figure', 'generative', 'ai', 'solve', 'whole', 'problem', 'still', 'need', 'human', 'input', 'validate', 'output', 'llm', 'model', 'ayer', 'explain', 'time', 'organization', 'ask', 'generative', 'ai', 'model', 'use', 'appropriately', 'train', 'customer', 'context', 'enterprise', 'datum', 'insight', 'high', 'chance', 'response', 'incorrect', 'relate', 'challenge', 'bias', 'underlie', 'datum', 'certain', 'bias', 'modeling', 'llm', 'unfair', 'mechanism', 'address', 'say', 'ayer', 'issue', 'data', 'quality', 'output', 'authenticity', 'explainability', 'also', 'address', 'implementation', 'final', 'set', 'challenge', 'relate', 'actual', 'implementation', 'cost', 'implementation', 'significant', 'especially', 'company', 'orchestrate', 'viable', 'solution', 'say', 'ayer', 'addition', 'right', 'infrastructure', 'people', 'place', 'avoid', 'resource', 'constraint', 'user', 'convince', 'output', 'relevant', 'high', 'quality', 'gain', 'acceptance', 'program', 'implementation', 'lastly', 'privacy', 'ethical', 'issue', 'address', 'corinium', 'intelligence', 'triange', 'survey', 'show', 'almost', 'respondent', 'concern', 'ethical', 'decisionmake', 'focus', 'future', 'investment', 'entire', 'ecosystem', 'generative', 'ai', 'move', 'quickly', 'enterprise', 'agile', 'adapt', 'quickly', 'change', 'ensure', 'customer', 'expectation', 'meet', 'maintain', 'competitive', 'edge', 'almost', 'impossible', 'anticipate', 'next', 'new', 'fastdevelope', 'technology', 'ayer', 'say', 'organization', 'want', 'harness', 'potential', 'generative', 'ai', 'likely', 'increase', 'investment', 'area', 'however', 'case', 'throw', 'wall', 'see', 'stick', 'ayer', 'advise', 'organization', 'examine', 'roi', 'effectiveness', 'service', 'product', 'provide', 'customer', 'business', 'leader', 'clearly', 'demonstrate', 'measure', 'marked', 'improvement', 'customer', 'satisfaction', 'level', 'use', 'generative', 'ai', 'base', 'intervention', 'define', 'generative', 'ai', 'strategy', 'company', 'need', 'understand', 'apply', 'build', 'use', 'case', 'execute', 'scale', 'speed', 'market', 'measure', 'success', 'say', 'ayer', 'leverage', 'generative', 'ai', 'customer', 'engagement', 'typically', 'multipronged', 'approach', 'successful', 'partnership', 'help', 'stage', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']"
