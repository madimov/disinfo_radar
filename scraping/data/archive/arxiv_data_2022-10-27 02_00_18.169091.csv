title,url,date,text,cleaning,tokens
Hindering Adversarial Attacks with Implicit Neural Representations,"[{'href': 'http://arxiv.org/abs/2210.13982v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.13982v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-10-22 13:10:24,"Bootstrap Advantage Estimation for
Policy Optimization in Reinforcement Learning

Md Masudur Rahman, Yexiang Xue
Department of Computer Science
Purdue University, West Lafayette, Indiana, USA
{rahman64, yexiang}@purdue.edu

2
2
0
2

t
c
O
3
1

]

G
L
.
s
c
[

1
v
2
1
3
7
0
.
0
1
2
2
:
v
i
X
r
a

Abstract—This paper proposes an advantage estimation ap-
proach based on data augmentation for policy optimization.
Unlike using data augmentation on the input to learn value and
policy function as existing methods use, our method uses data
augmentation to compute a bootstrap advantage estimation. This
Bootstrap Advantage Estimation (BAE) is then used for learning
and updating the gradient of policy and value function. To
demonstrate the effectiveness of our approach, we conducted ex-
periments on several environments. These environments are from
three benchmarks: Procgen, Deepmind Control, and Pybullet,
which include both image and vector-based observations; discrete
and continuous action spaces. We observe that our method
reduces the policy and the value loss better than the Generalized
advantage estimation (GAE) method and eventually improves
cumulative return. Furthermore, our method performs better
than two recently proposed data augmentation techniques (RAD
and DRAC). Overall, our method performs better empirically
than baselines in sample efﬁciency and generalization, where the
agent is tested in unseen environments.

Index Terms—Deep Reinforcement Learning, Advantage Esti-

mation, Generalization in Reinforcement Learning

I. INTRODUCTION

The policy gradient method directly involves learning policy
function, which enjoys performance improvement in function
approximation settings [1]. The policy gradient theorem gives
a rather simple formulation of the gradient estimation, which
gives an unbias estimation [2]. However, it requires the re-
turn estimation of the entire trajectory, leading to very high
variance. A commonly used technique to reduce variance is
to use a baseline, which can help reduce variance without
introducing bias. Several effective methods originated from
this concept [3], [4]. An effective way is to use the value
function as a baseline to indicate whether the action taken
by the current policy is better than the average action taken in
that state, which can be formulated as an advantage estimation.
However, based on a single trajectory, the estimate can be local
and have high variance. Thus we can add a truncated scenario
where the value function can potentially give a global estimate.
Combining these two is the Generalized advantage estimation
[5] which shows strong empirical results [6].

However, due to procedural style content generation, the
value estimation can be erroneous and give different advantage
estimates even when the observation context changes. At the
same time, the semantic meaning remains the same. As the
procedural scenario can exist in a real-world scenario and
might cause agent to perform sup-optimally [7], [8], thus this

advantage estimation can be problematic in those scenarios,
resulting in poor sample efﬁciency. This issue might exist
partly due to difﬁculty in reducing policy estimation (policy
loss) and value function estimation (value loss).

the reward semantic remains the same, but

This paper proposes Bootstrap Advantage Estimation
(BAE), which calculates the advantage estimation by comput-
ing advantage estimation on the original and its transformed
observations. We assume the transformation to be a semantic
invariant;
the
contextual information can be changed. For example, if the
then changing the
background of a game is not relevant,
background color from red to green can be a semantic invariant
transformation. The ultimate goal is to train an agent to be
robust against any such background change, which performs
well in the blue background in this example. The transformed
observation can be of any form; we experimented with data
augmentation-based observation transformation (e.g., random
crop, amplitude scaling). The intuition is that taking advantage
of estimate over augmented data forces the advantage estimate
to consider the error over many variations of the observations.
We demonstrate our BAE on the policy gradient method (i.e.,
PPO [3]) and show a comparison over GAE-based estimation.
We observed that our method BAE achieved better sample ef-
ﬁciency and zero-shot generalization in Procgen environments
(starpilot and miner) with image-based observation.

In recent times, data augmentation demonstrated an effective
choice in improving sample efﬁciency in high-dimensional ob-
servation space and improving generalization [9]–[11]. Though
this process sometimes generates empirical success [10], such
methods might lead to detrimental performance [11] as we
observed in our experiments. To mitigate this issue,
the
DRAC [11] method suggests regularizing the policy network
and value network by augmented observation and not using
augmented data for policy training. In contrast, we propose
a novel way to leverage data augmentation. Our method
augmented observations for advantage estimation, one of the
core components of many policy optimization algorithms (e.g.,
PPO). We conducted extensive experiments on six environ-
ments consisting of image and vector-based observation; and
discrete and continuous action spaces. Our method falls in the
general model-free on-policy category, and we experimented
with Proximal Policy Optimization (PPO) [3] in this paper.
In particular, our experiments on Procgen Starpilot and Miner
environments demonstrate that our method can be beneﬁcial in

 
 
 
 
 
 
the zero-shot generalization setup compared to baseline GAE
[5], and two data augmentation techniques: RAD [10], and
DRAC [11].

We further evaluated our method on several robotic lo-
comotion tasks with the high-dimensional observation from
Deepmind Control Suite [12]: Quadruped Run, and Cartpole
- Three Poles; and PyBullet [13]: Minitaur and HalfCheetah.
In experiments, we observe that our method BAE performs
better than baseline agents, including base PPO, RAD, and
DRAC. Our method achieves a much lower loss for policy
and value function estimation. Eventually, it performs better
in sample efﬁciency and zero-shot generalization than baseline
agents, including data augmentation. We observe that the base-
line data augmentation methods (RAD and DRAC) sometimes
worsen the base model performance. In contrast, our BAE
method improves the performance in most tested environments
and performs consistently with the base algorithm in other
cases. These results show that our method BAE is more
robust in performance compared to baseline data augmentation
methods.

The source code of our method is available at https://github.

com/masud99r/bae.

II. PRELIMINARIES AND PROBLEM SETTINGS

Reinforcement Learning We assume the task/environment
is a Markov Decision Process (MDP) which is denoted by
M = (S, A, P, R, γ), where S is state space, A is that action
space, P is the transition probability between states resulted by
an action. In this setup, at every timestep t, the agent take an
action at ∈ A in a state st ∈ S and the environment transition
to next state st+1 ∈ S determined by the transition probability
P (st+1|st, at). In a reinforcement learning framework, the
goal of the agent is to learn a policy π ∈ Π by maximizing the
expected return in an MDP, where Π is a set of all possible
policies and π is a single policy which is a mapping from
state to action. The policy which achieves the highest expected
return is optimal π∗ ∈ Π.
Policy Gradient Proximal Policy Optimization (PPO) [3], a
type of policy gradient method which achieved tremendous
success and is popularly used in many setups because of its
effective learning and simple implementation. However, the
choice of implementation details might impact the perfor-
mance of such algorithms in signiﬁcant ways [6], [14], [15].
These implementation details consist of training individual
components of the algorithms, such as learning value function,
policy function, and advantage estimation. The following is the
objective of the PPO [3].

Lπ = −Et[

πθ(at|st)
πθold(at|st)

At]

(1)

, where πθ(at|st) is the probability of choosing action at give
state st at timestep t using the current policy parameterized by
θ. On the other hand the πθold(at|st) refer to the probabilities
using an old policy parameterized by previous parameter
values θold. The advantage At is an estimation which is the
advantage of taking action at at st. A popular and effective

choice of estimating advantage using a value function is as
follows (equation 2):

At = −V (st) + rt + γrr+1 + ... + γT −t+1rT −1 + γT −tV (sT ).
(2)
Here V (s) is a value function that gives the average future
return under the underlying policy. The ﬁrst term V (st) is the
value prediction at timestep t, and the rest of the terms except
the last term in the equation is the discounted Monte Carlo
Estimation which can be computed for a given episode from t
to T −1 (T > t). The last term V (sT ) is the value prediction at
state sT . Thus overall, this At represents how much the current
action at is doing compared to the current value prediction.
The more the advantage of action, the more the policy should
weigh that action. This is done by multiplying πθ(at|st)
with At. The πθold(at|st) in Equation 2 introduced due to
importance sampling which allows estimating the advantage
from the old policy samples. For details discussion, we refer
the reader to [3], [4].
Value Function Estimation An effective value function esti-
mation [3] is to regress value prediction with an advantage-
based return estimation. Here the Verror = |Vprediction −
VReturn|, where Vprediction is the predicted value and the
VReturn is value computed from the rewards R = (cid:80)
t rt of
sampled trajectories and advantage A. Thus, VReturn = A+R.
Note that in this way, both the policy (in equation 1 and value
function is dependent on the advantage estimation. Thus, an
accurate advantage estimation should give us lower policy and
value losses and thus a better performing policy.
Generalization in RL Now we turn attention to the scenarios
where different episode varies by confounding features in
these confounders (also called
the observation. Note that
context)impact the reward in the environment; however, they
might misguide the agent to think otherwise. Due to nature,
the agent might overﬁt the confounding features and fails to
generalize to slightly modiﬁed test environments [7], [8]. For
a details overview of generalization in reinforcement learning,
we refer the reader to the servery papers [16].
Data augmentation in various forms has been leveraged [10],
idea is to transform the observation so
[11]. The general
that the observation’s semantic meaning remains the same
but the contextual information changes. However, the context
information is readily not available, and thus we need to
impose various assumptions that certain transformations on the
observation s(cid:48) = f (s) keep the reward semantic. For image-
based observation, various image manipulation can be used,
such as cropping, rotation, and color-jitter.
Advantage Estimation An essential component
in policy
training is to estimate the advantage. Generalized Advantage
Estimation (GAE) [5] is a useful way to compute advan-
tage, which combines the value function estimate and the
Monte Carlo method. However, the data augmentation-based
regularization approaches [10], [11] do not handle the case
of advantage estimation. The advantage is estimated using a
single trajectory; thus, the computed advantage has a high
variance due to the systematic noise in the advantage esti-

mation. Given two similar states (observation), the advantage
estimation should be the same. For example, an observation
with the same semantic but a different background (red and
blue) should have the same advantage if the background is not
essential and thus confounded.

III. BOOSTRAP ADVANTAGE ESTIMATION (BAE)

We proposed to bootstrap the advantage estimation using
observation transformation to mitigate the abovementioned
issue. Formally, we generate m additional estimation with m
transformation. For each such transformation i, we compute
an estimate as in equation 3.

A(k,i)
t

= −V (f (st+1, vi))+rt+γrr+1+...+γT −tV (f (st+k, vi)),

(3)
where v0 refer to no augmentation. Furthermore, ﬁnally, we
take the average of all estimates as in equation 4 to estimate
the ﬁnal advantage estimation for k-step return.

A(k,b)
t

=

1
m + 1

(A(k,0)
t

+ A(k,1)
t

+ A(k,2)
t

+ .. + A(k,m)

t

) (4)

Finally, we can achieve bootstrap advantage estimation of a
trajectory of length T by combining several k-step returns
using exponential-weighted average as in 5 following the GAE
method [5].
ABAE(γ,λ)

t +...+λT −1A(T,b)

= (1−λ)(A(1,b)

t +λA(2,b)

) (5)

t

t

This ABAE(γ,λ)
is used to compute the advantage at state st
t
of timestep t in an episode. Note that, our BAE differs from
the GAE [5] in computing the k-step return as in equation
4. Algorithm 1 shows the details step of using our BAE with
the PPO-based policy optimization method. In this paper, we

Algorithm 1 BAE for Policy Optimization

1: Get

transformation function f (s, v) with augmentation

type v

for each environment step do

2: Get PPO for policy optimization RL agent
3: for each iteration do
4:
5:
6:
7:
8:
9:

at ∼ πθ(at|st)
st+1 ∼ P (st+1|st, at)
rt ∼ R(st, at)
B ←− B ∪ {(st, at, rt, st+1)}

10:

11:

12:

end for
Transform all s ∈ B to get B(cid:48) using v augmentation
with function f (s, v).
Compute Bootstrap Advantage Estimate (BAE) from
data B and B(cid:48) using equation 5.
Perform PPO updates with BAE to optimize for Lπ as
in equation 1

13: end for

leverage PPO [3] as the base RL algorithm, which uses gen-
eralized advantage estimation (GAE) as the default estimator.
In contrast, our method BAE-PPO uses bootstrap advantage

estimation (BAE) instead of GAE. The data augmentation
baselines RAD and DRAC use base PPO with GAE advantage
estimation. In the experiments we use m = 1 in equation
4. This means we use one data augmentation approach and
combine it with original advantage estimation as in equation
5.

Note that we do not apply any observation transformation
in other parts of the agent objective, and thus equation 2
remains theoretically and practically sound. Furthermore, we
empirically show how our Bootstrap Advantage Estimation
leads to a smaller value, policy loss, and performance boost.
Finally, we also compared the baseline RAD [10] and DRAC
[11] and show that our method performs better in many setups.

IV. EXPERIMENTS

A. Setup

Environments We experimented with image-based observa-
tions with discrete action space and vector-based observations
with continuous action space.
Procgen We use Procgen [17]: Starpilot and Miner (Figure 1)
which use image-based observation and procedural generation
to produce challenging game logic that changes episode by
episode. This benchmark allows for evaluating both sample
efﬁciency and generalization capacity of RL agents. Each
environment has around 100K levels. A subset of levels can
be used to train the agent, and then the full distribution, that
is, 100K levels, can be used to test the agent’s generalization
capacity. For our experiment, we use the standard evaluation
protocol from [17]; 200 levels of each environment are used
for training in the difﬁculty level easy. All the environments
have discrete action space of dimension 16. Intuitively, during
training, the agent has access to a limited number of envi-
ronment variability (e.g., 200 levels). The trained agent is
tested on all the available variabilities, which consist of unseen
scenarios. Thus, to master the game, the agent must focus on
essential aspects of the state and ignore irrelevant information
such as background color.

Fig. 1. Procgen: Some snapshots of Starpilot and Miner. The environments
are generated procedurally, which results in different observations (e.g.,
background) in each episode.

two environments

Deepmind Control We use
from
dm control [12]: Quadruped run, and Cartpole with three
poles (Figure 2). The Quadruped Run has high-dimensional
vector observation, and the task is to run as far as possible.
On the other hand, the Cartpole variation consists of three
procedurally generated poles. The complexity of these environ-
ments is suitable for evaluating the data augmentation-based
approaches.

[Left] Deepmind Control: Some snapshots of Deepmind Control
Fig. 2.
tasks. [Right] Pybullet: Some snapshots of Pybullet Minitaur quadruped and
HalfCheetah environments. These environments contain vector-based state
space, and the action space is continuous.

Fig. 3. Starpilot Env. Training time policy and value loss [lower is better].
Our method achieves lowest value and policy losses than the base algorithm
(GAE-PPO) and data augmentation baselines (RAD and DRAC).

Pybullet We use Pybullet [13]: Minitaur quadruped and
HalfCheetah environments with vector-based observation.
Each observation consists of raw sensory inputs. The Minitaur
quadruped is a 4-legged robot, and the task is to travel as long
as possible on ﬂat ground. Furthermore, the HalfCheetah is a
two-legged robot that can control its movement in 2D, and
the task is to travel as much distance as possible. The action
spaces are continuous in these environments. Snippets of these
environments are in Figure 2.
Baselines All agents usage on-policy PPO [3] as the base
policy. We compare our method with Generalized Advantage
Estimation (GAE) [5] which is referred to as GAE-PPO in
our experiments. GAE is shown to perform better compared
to other advantage estimation techniques [6]. Moreover, we
compare with the data augmentation-based approach uses data
augmentation to transform the observation and then uses the
transformed observation to train the base policy. In particular,
we compare our method with existing baselines RAD [10]
and DRAC [11]. RAD, referred to as RAD-PPO, proposes
various data augmentation techniques to improve learning
from pixel-based observation. DRAC, referred to as DRAC-
PPO leverages the data augmentation to regularize the policy
and value learning, showing improved performance in policy
learning. In our method, we replaced the GAE estimation with
our proposed Bootstrap Advantage Estimation (BAE), which
is referred to as BAE-PPO.
the agents,

including our BAE-PPO and
baseline, using the implementation available in [15]. In a
PPO-based scenario, many factors have been identiﬁed as key
in implementing algorithms that impact the performance [6],
[14]. Thus, we use the same implementation logic for all the
baselines and our method for a fair comparison.
Data augmentation We evaluate Cutout Color data augmen-
tation for image-based observation, which performs best in
our setup compared to another popularly used Random Crop.
Thus, we report Cutout Color data augmentation results for
RAD, DRAC, and our BAE. We use the implementation avail-
able in RAD [10] for data augmentation. For the vector-based
observation robotic task, we use a random amplitude scale
proposed in RAD [10]. This method multiply the observation
with a number generated randomly between a range α to β.
We used best performing range α = 0.8 to β = 1.4 for our

We build all

BAE method, and a range α = 0.6 to β = 1.2 for RAD, and
DRAC (suggested in RAD [10]).
Implementation and Hyper-parameters For the Procgen
Starpilot and Miner, we report mean and standard deviation
across 3 seeds run following the setup of Procgen paper [17].
We used an Nvidia A100 GPU to run agents with the IMPALA
CNN model [18] on the image observation-based Procgen
environments. We use neural networks to represent policy and
value functions for vector-based observations. For Deepmind
control environments, we report results with 10 random seed
runs, and for Pybullet environments, we report results over 5
seeds. For all experiments, we keep the common hyperparam-
eters the same for a fair comparison. The implementation and
hyperparameters are based on [15], [19]. For all results, we
report the mean (showed in solid line) and standard deviation
(showed in shaded areas) across runs.

B. Results

The PPO-based agent’s objective consists of value loss and
policy loss. The objective is to reduce them and potentially
improve the expected return. We show that our method BAE
reduces the losses and thus learns a better value function and
policy than the baselines. We then show how our method
performs in the expected return.
Procgen Results Figure 3 shows value and policy loss during
policy training on Procgen Starpilot environments. As the
training progresses,
the loss of our method BAE reduces
drastically compared to the baselines. These results show the
sign of the effectiveness of our method in reducing the agent’s
losses. Note that the advantage estimation is used to train both
the value function and the policy; thus, better estimation of
advantages should generally give better value and policy. In
this sense, our method shows empirical evidence that it can
help better advantage estimation.

We observe that in the Starpilot environment, the success in
policy and value loss translates to the ﬁnal return. In Figure 4
we see that our method (BAE-PPO) shows improved sample
efﬁciency (Train Return) and generalization capacity compared
to the baseline GAE-PPO, RAD-PPO, and DRAC-PPO.

Note that the RAD-PPO performance worsens the perfor-
mance of the base GAE-PPO algorithms. This result is con-
sistent with the ﬁndings in [11], and it shows that naive data

Starpilot Env. Sample efﬁciency performance measured in train
Fig. 4.
time return. We see our method BAE-PPO achieves higher returns where
DRAC does not improve the base agent’s (GAE-PPO) performance. RAD
slightly worsens the performance of the base agent. [Right] Generalization
performance measured in test time return. We see a similar trend and observe
that our method performs the best.

[Left] Performance on Quadruped run environments. Our method
Fig. 7.
BAE-PPO shows higher mean returns compared to all other agents. Data
augmentation baseline RAD and DRAC worsen the performance of the base
agent (GAE-PPO). [Right] Our method BAE consistently achieves a higher
mean where DRAC fails to improve upon the base agent, and RAD worsens
the base performance.

Fig. 5. Miner Env. Training time policy and value loss [lower is better].
The value loss of BAE eventually become lower. Losses of RAD increases
compared to base PPO.

Fig. 6. Miner Env. [Left] Sample efﬁciency performance measured in train
time return and [Right] Generalization performance measured in test time
return. Overall, we see our method BAE-PPO shows consistence improvement
over baselines.

augmentation can be detrimental to performance. Furthermore,
we observe a similar trend for the value and policy loss results.
We observe a similar performance trend in the Procgen
Miner environment. In Figure 5, we see that in our method,
BAE shows a smaller value loss eventually despite being
higher at the beginning compared to the baselines. GAE and
DRAC show slightly lower values in the policy loss plot than
BAE. However, both show smaller policy losses in general.

In performance measure (Figure 6), we see that our method
shows better performance throughout the training than the
baseline in both sample efﬁciency and generalization. The
performance difference is consistent across timestep. Similar
to Starpilot, RAD also performs worse compared to base GAE-
PPO.

We further evaluate our method on vector-based state space

and continuous robotic tasks: Deepmind control and Pybullet.
Note that the setup of these benchmarks are different from
Procgen’s train-test setup, and here we evaluate how our agents
can perform in high-dimensional states and procedurally gen-
erated task. Thus we can only report returns during training.

Deepmind Control Results Figure 7 shows performance
compariosn on Quadruped run and Cartpole - Three Poles
environments. We observe that our method BAE achieves the
best performance in both environments. On the other hand, the
baseline RAD severely worsens the base agent’s (GAE-PPO)
performance in both environments. Another baseline, DRAC,
worsens the base agents’ performance in Quadruped Run and
fails to improve performance in the Cartpole Three Poles envi-
ronment. These results show that properly using the augmenta-
tion in policy learning can lead to strong performance. In this
case, all data augmentation agents (RAD, DRAC, and BAE)
use the same random amplitude modulation augmentation.
However, using this augmentation in advantage computation,
our method BAE shows a substantial performance boost. On
the other hand, other baselines, RAD and DRAC, worsen the
performance (Quadruped Run).

Pybullet Results In Figure 8, for the HalfCheetah environ-
ment, we see that our method BAE performs better than
base agent GAE-PPO and other data augmentation baselines
RAD and DRAC. On the other hand, in Minitaur, the data
augmentation baseline DRAC and RAD worsen the base
agent’s (GAE-PPO) performance where BAE can maintain the
base performance.

Overall, these results show the robustness of our method in
performance compared to baseline data augmentation methods.
Therefore, the proposed augmented observation is expected
not to worsen the base performance. However, in our exper-
iments, we observe that the RAD and DRAC barely match
the base agent’s (GAE-PPO) results and sometimes worsen
the performance. These variabilities in performance hinder
the widespread adaptation of these methods. In contrast, our
method BAE shows a consistent performance across various
tasks without reducing the base agent’s performance.

[2] R. J. Williams, “Simple statistical gradient-following algorithms for
connectionist reinforcement learning,” Machine learning, vol. 8, no. 3,
pp. 229–256, 1992.

[3] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,
2017.

[4] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust
region policy optimization,” in International conference on machine
learning. PMLR, 2015, pp. 1889–1897.

[5] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-
dimensional continuous control using generalized advantage estimation,”
in Proceedings of the International Conference on Learning Represen-
tations (ICLR), 2016.

[6] M. Andrychowicz, A. Raichuk, P. Sta´nczyk, M. Orsini, S. Girgin,
R. Marinier, L. Hussenot, M. Geist, O. Pietquin, M. Michalski, S. Gelly,
and O. Bachem, “What matters for on-policy deep actor-critic methods?
a large-scale study,” in International Conference on Learning Represen-
tations, 2021.

[7] X. Song, Y. Jiang, S. Tu, Y. Du, and B. Neyshabur, “Observational
overﬁtting in reinforcement learning,” in International Conference on
Learning Representations, 2020.

[8] C. Zhang, O. Vinyals, R. Munos, and S. Bengio, “A study on overﬁtting
in deep reinforcement learning,” arXiv preprint arXiv:1804.06893, 2018.
[9] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman, “Quantifying
generalization in reinforcement learning,” in International Conference
on Machine Learning. PMLR, 2019, pp. 1282–1289.

[10] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas,
“Reinforcement learning with augmented data,” in Advances in neural
information processing systems, 2020.

[11] R. Raileanu, M. Goldstein, D. Yarats, I. Kostrikov, and R. Fergus,
“Automatic data augmentation for generalization in deep reinforcement
learning,” arXiv preprint arXiv:2006.12862, 2020.

[12] S. Tunyasuvunakool, A. Muldal, Y. Doron, S. Liu, S. Bohez, J. Merel,
T. Erez, T. Lillicrap, N. Heess, and Y. Tassa, “dm control: Software and
tasks for continuous control,” Software Impacts, vol. 6, p. 100022, 2020.
[13] E. Coumans and Y. Bai, “Pybullet, a python module for physics
simulation for games, robotics and machine learning,” http://pybullet.org,
2016–2021.

[14] L. Engstrom, A. Ilyas, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph,
and A. Madry, “Implementation matters in deep rl: A case study on
ppo and trpo,” in International Conference on Learning Representations,
2020.

[15] S. Huang, R. F. J. Dossa, A. Rafﬁn, A. Kanervisto, and W. Wang, “The
37 implementation details of proximal policy optimization,” in ICLR
Blog Track, 2022. [Online]. Available: https://iclr-blog-track.github.io/
2022/03/25/ppo-implementation-details/

[16] R. Kirk, A. Zhang, E. Grefenstette, and T. Rockt¨aschel, “A sur-
vey of generalisation in deep reinforcement learning,” arXiv preprint
arXiv:2111.09794, 2021.

[17] K. Cobbe, C. Hesse, J. Hilton, and J. Schulman, “Leveraging procedu-
ral generation to benchmark reinforcement learning,” in International
conference on machine learning. PMLR, 2020, pp. 2048–2056.
[18] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward,
Y. Doron, V. Firoiu, T. Harley, I. Dunning et al., “Impala: Scalable dis-
tributed deep-rl with importance weighted actor-learner architectures,”
arXiv preprint arXiv:1802.01561, 2018.

[19] S. Huang, R. F. J. Dossa, C. Ye, and J. Braga, “Cleanrl: High-quality
single-ﬁle implementations of deep reinforcement learning algorithms,”
2021.

[20] J. Peters and S. Schaal, “Reinforcement learning of motor skills with
policy gradients,” Neural networks, vol. 21, no. 4, pp. 682–697, 2008.
[21] C. Wu, A. Rajeswaran, Y. Duan, V. Kumar, A. M. Bayen, S. Kakade,
I. Mordatch, and P. Abbeel, “Variance reduction for policy gradient with
action-dependent factorized baselines,” in International Conference on
Learning Representations, 2018.

[22] M. Igl, K. Ciosek, Y. Li, S. Tschiatschek, C. Zhang, S. Devlin, and
K. Hofmann, “Generalization in reinforcement learning with selective
noise injection and information bottleneck,” in Advances in neural
information processing systems, 2019, pp. 13 978–13 990.

[23] M. M. Rahman and Y. Xue, “Bootstrap state representation using style
transfer for better generalization in deep reinforcement learning,” in
European Conference on Machine Learning and Principles and Practice
of Knowledge Discovery in Databases (ECML-PKDD 2022), 2022.

Performance on Minitaur [Left] and HalfCheetah [Right] environ-
Fig. 8.
ments. Our method shows better or similar performance compared to the base
agent (GAE-PPO), where the data augmentation baselines sometimes worsen
the base performance.

V. RELATED WORK

Advantage Estimation. The baseline has been leveraged to
reduce variance in policy gradient update [2], [20], [21].
Furthermore, effective modiﬁcation of such methods is the use
of advantage estimation. This method is commonly used in
policy optimization and enjoys strong empirical success [6],
especially the Generalized Advantage Estimation (GAE) [5]
method. In contrast to GAE, our method BAE leverages data
augmentation and incorporates advantage computation across
various semantically similar states. Empirically we observe
that our method of computing advantage can be beneﬁcial over
GAE, especially in high-dimensional and procedural generated
environments.
Data augmentation. Data augmentation has been demon-
strated to be an effective and efﬁcient approaches to improve
performance [9]–[11]. Other methods proposed to improve
generalization which includes regularization [22], and style-
transfer [23]. Depending on how the augmented observation
is used, the method can be different; for example, RAD [10]
and DRAC [11]. In contrast to these methods, our method
incorporates data augmentation into advantage estimation,
which shows better empirical performance compared to these
methods (RAD and DRAC).

VI. CONCLUSION

In this paper, we propose a data augmentation-based advan-
tage estimation method for policy optimization. Our Bootstrap
advantage estimation (BAE) method replaces the GAE method
in policy gradient-based algorithms. We demonstrated the
effectiveness of our method on PPO algorithms. Furthermore,
we evaluated our methods on both image-based observation
space with discrete action space and vector-based observation
with continuous action space (Procgen, Deepmind Control,
and Pybullet). Our BAE method showed better performance
in various environment setups than GAE. Furthermore, our
method performs better than two existing data augmentation
techniques (RAD and DRAC).

REFERENCES

[1] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradi-
ent methods for reinforcement learning with function approximation,”
Advances in neural information processing systems, vol. 12, 1999.

","Bootstrap Advantage Estimation for Policy Optimization in Reinforcement Learning Md Masudur Rahman, Yexiang Xue Department of Computer Science Purdue University, West Lafayette, Indiana, USA {rahman64, yexiang}@purdue.edu 2 2 0 2 t c O 3 1 ] G L . s c [ 1 v 2 1 3 7 0 . 0 1 2 2 : v i X r a Abstract—This paper proposes an advantage estimation ap- proach based on data augmentation for policy optimization. Unlike using data augmentation on the input to learn value and policy function as existing methods use, our method uses data augmentation to compute a bootstrap advantage estimation. This Bootstrap Advantage Estimation (BAE) is then used for learning and updating the gradient of policy and value function. To demonstrate the effectiveness of our approach, we conducted ex- periments on several environments. These environments are from three benchmarks: Procgen, Deepmind Control, and Pybullet, which include both image and vector-based observations; discrete and continuous action spaces. We observe that our method reduces the policy and the value loss better than the Generalized advantage estimation (GAE) method and eventually improves cumulative return. Furthermore, our method performs better than two recently proposed data augmentation techniques (RAD and DRAC). Overall, our method performs better empirically than baselines in sample efﬁciency and generalization, where the agent is tested in unseen environments. Index Terms—Deep Reinforcement Learning, Advantage Esti- mation, Generalization in Reinforcement Learning I. INTRODUCTION The policy gradient method directly involves learning policy function, which enjoys performance improvement in function approximation settings [1]. The policy gradient theorem gives a rather simple formulation of the gradient estimation, which gives an unbias estimation [2]. However, it requires the re- turn estimation of the entire trajectory, leading to very high variance. A commonly used technique to reduce variance is to use a baseline, which can help reduce variance without introducing bias. Several effective methods originated from this concept [3], [4]. An effective way is to use the value function as a baseline to indicate whether the action taken by the current policy is better than the average action taken in that state, which can be formulated as an advantage estimation. However, based on a single trajectory, the estimate can be local and have high variance. Thus we can add a truncated scenario where the value function can potentially give a global estimate. Combining these two is the Generalized advantage estimation [5] which shows strong empirical results [6]. However, due to procedural style content generation, the value estimation can be erroneous and give different advantage estimates even when the observation context changes. At the same time, the semantic meaning remains the same. As the procedural scenario can exist in a real-world scenario and might cause agent to perform sup-optimally [7], [8], thus this advantage estimation can be problematic in those scenarios, resulting in poor sample efﬁciency. This issue might exist partly due to difﬁculty in reducing policy estimation (policy loss) and value function estimation (value loss). the reward semantic remains the same, but This paper proposes Bootstrap Advantage Estimation (BAE), which calculates the advantage estimation by comput- ing advantage estimation on the original and its transformed observations. We assume the transformation to be a semantic invariant; the contextual information can be changed. For example, if the then changing the background of a game is not relevant, background color from red to green can be a semantic invariant transformation. The ultimate goal is to train an agent to be robust against any such background change, which performs well in the blue background in this example. The transformed observation can be of any form; we experimented with data augmentation-based observation transformation (e.g., random crop, amplitude scaling). The intuition is that taking advantage of estimate over augmented data forces the advantage estimate to consider the error over many variations of the observations. We demonstrate our BAE on the policy gradient method (i.e., PPO [3]) and show a comparison over GAE-based estimation. We observed that our method BAE achieved better sample ef- ﬁciency and zero-shot generalization in Procgen environments (starpilot and miner) with image-based observation. In recent times, data augmentation demonstrated an effective choice in improving sample efﬁciency in high-dimensional ob- servation space and improving generalization [9]–[11]. Though this process sometimes generates empirical success [10], such methods might lead to detrimental performance [11] as we observed in our experiments. To mitigate this issue, the DRAC [11] method suggests regularizing the policy network and value network by augmented observation and not using augmented data for policy training. In contrast, we propose a novel way to leverage data augmentation. Our method augmented observations for advantage estimation, one of the core components of many policy optimization algorithms (e.g., PPO). We conducted extensive experiments on six environ- ments consisting of image and vector-based observation; and discrete and continuous action spaces. Our method falls in the general model-free on-policy category, and we experimented with Proximal Policy Optimization (PPO) [3] in this paper. In particular, our experiments on Procgen Starpilot and Miner environments demonstrate that our method can be beneﬁcial in the zero-shot generalization setup compared to baseline GAE [5], and two data augmentation techniques: RAD [10], and DRAC [11]. We further evaluated our method on several robotic lo- comotion tasks with the high-dimensional observation from Deepmind Control Suite [12]: Quadruped Run, and Cartpole - Three Poles; and PyBullet [13]: Minitaur and HalfCheetah. In experiments, we observe that our method BAE performs better than baseline agents, including base PPO, RAD, and DRAC. Our method achieves a much lower loss for policy and value function estimation. Eventually, it performs better in sample efﬁciency and zero-shot generalization than baseline agents, including data augmentation. We observe that the base- line data augmentation methods (RAD and DRAC) sometimes worsen the base model performance. In contrast, our BAE method improves the performance in most tested environments and performs consistently with the base algorithm in other cases. These results show that our method BAE is more robust in performance compared to baseline data augmentation methods. The source code of our method is available at https://github. com/masud99r/bae. II. PRELIMINARIES AND PROBLEM SETTINGS Reinforcement Learning We assume the task/environment is a Markov Decision Process (MDP) which is denoted by M = (S, A, P, R, γ), where S is state space, A is that action space, P is the transition probability between states resulted by an action. In this setup, at every timestep t, the agent take an action at ∈ A in a state st ∈ S and the environment transition to next state st+1 ∈ S determined by the transition probability P (st+1|st, at). In a reinforcement learning framework, the goal of the agent is to learn a policy π ∈ Π by maximizing the expected return in an MDP, where Π is a set of all possible policies and π is a single policy which is a mapping from state to action. The policy which achieves the highest expected return is optimal π∗ ∈ Π. Policy Gradient Proximal Policy Optimization (PPO) [3], a type of policy gradient method which achieved tremendous success and is popularly used in many setups because of its effective learning and simple implementation. However, the choice of implementation details might impact the perfor- mance of such algorithms in signiﬁcant ways [6], [14], [15]. These implementation details consist of training individual components of the algorithms, such as learning value function, policy function, and advantage estimation. The following is the objective of the PPO [3]. Lπ = −Et[ πθ(at|st) πθold(at|st) At] (1) , where πθ(at|st) is the probability of choosing action at give state st at timestep t using the current policy parameterized by θ. On the other hand the πθold(at|st) refer to the probabilities using an old policy parameterized by previous parameter values θold. The advantage At is an estimation which is the advantage of taking action at at st. A popular and effective choice of estimating advantage using a value function is as follows (equation 2): At = −V (st) + rt + γrr+1 + ... + γT −t+1rT −1 + γT −tV (sT ). (2) Here V (s) is a value function that gives the average future return under the underlying policy. The ﬁrst term V (st) is the value prediction at timestep t, and the rest of the terms except the last term in the equation is the discounted Monte Carlo Estimation which can be computed for a given episode from t to T −1 (T > t). The last term V (sT ) is the value prediction at state sT . Thus overall, this At represents how much the current action at is doing compared to the current value prediction. The more the advantage of action, the more the policy should weigh that action. This is done by multiplying πθ(at|st) with At. The πθold(at|st) in Equation 2 introduced due to importance sampling which allows estimating the advantage from the old policy samples. For details discussion, we refer the reader to [3], [4]. Value Function Estimation An effective value function esti- mation [3] is to regress value prediction with an advantage- based return estimation. Here the Verror = |Vprediction − VReturn|, where Vprediction is the predicted value and the VReturn is value computed from the rewards R = (cid:80) t rt of sampled trajectories and advantage A. Thus, VReturn = A+R. Note that in this way, both the policy (in equation 1 and value function is dependent on the advantage estimation. Thus, an accurate advantage estimation should give us lower policy and value losses and thus a better performing policy. Generalization in RL Now we turn attention to the scenarios where different episode varies by confounding features in these confounders (also called the observation. Note that context)impact the reward in the environment; however, they might misguide the agent to think otherwise. Due to nature, the agent might overﬁt the confounding features and fails to generalize to slightly modiﬁed test environments [7], [8]. For a details overview of generalization in reinforcement learning, we refer the reader to the servery papers [16]. Data augmentation in various forms has been leveraged [10], idea is to transform the observation so [11]. The general that the observation’s semantic meaning remains the same but the contextual information changes. However, the context information is readily not available, and thus we need to impose various assumptions that certain transformations on the observation s(cid:48) = f (s) keep the reward semantic. For image- based observation, various image manipulation can be used, such as cropping, rotation, and color-jitter. Advantage Estimation An essential component in policy training is to estimate the advantage. Generalized Advantage Estimation (GAE) [5] is a useful way to compute advan- tage, which combines the value function estimate and the Monte Carlo method. However, the data augmentation-based regularization approaches [10], [11] do not handle the case of advantage estimation. The advantage is estimated using a single trajectory; thus, the computed advantage has a high variance due to the systematic noise in the advantage esti- mation. Given two similar states (observation), the advantage estimation should be the same. For example, an observation with the same semantic but a different background (red and blue) should have the same advantage if the background is not essential and thus confounded. III. BOOSTRAP ADVANTAGE ESTIMATION (BAE) We proposed to bootstrap the advantage estimation using observation transformation to mitigate the abovementioned issue. Formally, we generate m additional estimation with m transformation. For each such transformation i, we compute an estimate as in equation 3. A(k,i) t = −V (f (st+1, vi))+rt+γrr+1+...+γT −tV (f (st+k, vi)), (3) where v0 refer to no augmentation. Furthermore, ﬁnally, we take the average of all estimates as in equation 4 to estimate the ﬁnal advantage estimation for k-step return. A(k,b) t = 1 m + 1 (A(k,0) t + A(k,1) t + A(k,2) t + .. + A(k,m) t ) (4) Finally, we can achieve bootstrap advantage estimation of a trajectory of length T by combining several k-step returns using exponential-weighted average as in 5 following the GAE method [5]. ABAE(γ,λ) t +...+λT −1A(T,b) = (1−λ)(A(1,b) t +λA(2,b) ) (5) t t This ABAE(γ,λ) is used to compute the advantage at state st t of timestep t in an episode. Note that, our BAE differs from the GAE [5] in computing the k-step return as in equation 4. Algorithm 1 shows the details step of using our BAE with the PPO-based policy optimization method. In this paper, we Algorithm 1 BAE for Policy Optimization 1: Get transformation function f (s, v) with augmentation type v for each environment step do 2: Get PPO for policy optimization RL agent 3: for each iteration do 4: 5: 6: 7: 8: 9: at ∼ πθ(at|st) st+1 ∼ P (st+1|st, at) rt ∼ R(st, at) B ←− B ∪ {(st, at, rt, st+1)} 10: 11: 12: end for Transform all s ∈ B to get B(cid:48) using v augmentation with function f (s, v). Compute Bootstrap Advantage Estimate (BAE) from data B and B(cid:48) using equation 5. Perform PPO updates with BAE to optimize for Lπ as in equation 1 13: end for leverage PPO [3] as the base RL algorithm, which uses gen- eralized advantage estimation (GAE) as the default estimator. In contrast, our method BAE-PPO uses bootstrap advantage estimation (BAE) instead of GAE. The data augmentation baselines RAD and DRAC use base PPO with GAE advantage estimation. In the experiments we use m = 1 in equation 4. This means we use one data augmentation approach and combine it with original advantage estimation as in equation 5. Note that we do not apply any observation transformation in other parts of the agent objective, and thus equation 2 remains theoretically and practically sound. Furthermore, we empirically show how our Bootstrap Advantage Estimation leads to a smaller value, policy loss, and performance boost. Finally, we also compared the baseline RAD [10] and DRAC [11] and show that our method performs better in many setups. IV. EXPERIMENTS A. Setup Environments We experimented with image-based observa- tions with discrete action space and vector-based observations with continuous action space. Procgen We use Procgen [17]: Starpilot and Miner (Figure 1) which use image-based observation and procedural generation to produce challenging game logic that changes episode by episode. This benchmark allows for evaluating both sample efﬁciency and generalization capacity of RL agents. Each environment has around 100K levels. A subset of levels can be used to train the agent, and then the full distribution, that is, 100K levels, can be used to test the agent’s generalization capacity. For our experiment, we use the standard evaluation protocol from [17]; 200 levels of each environment are used for training in the difﬁculty level easy. All the environments have discrete action space of dimension 16. Intuitively, during training, the agent has access to a limited number of envi- ronment variability (e.g., 200 levels). The trained agent is tested on all the available variabilities, which consist of unseen scenarios. Thus, to master the game, the agent must focus on essential aspects of the state and ignore irrelevant information such as background color. Fig. 1. Procgen: Some snapshots of Starpilot and Miner. The environments are generated procedurally, which results in different observations (e.g., background) in each episode. two environments Deepmind Control We use from dm control [12]: Quadruped run, and Cartpole with three poles (Figure 2). The Quadruped Run has high-dimensional vector observation, and the task is to run as far as possible. On the other hand, the Cartpole variation consists of three procedurally generated poles. The complexity of these environ- ments is suitable for evaluating the data augmentation-based approaches. [Left] Deepmind Control: Some snapshots of Deepmind Control Fig. 2. tasks. [Right] Pybullet: Some snapshots of Pybullet Minitaur quadruped and HalfCheetah environments. These environments contain vector-based state space, and the action space is continuous. Fig. 3. Starpilot Env. Training time policy and value loss [lower is better]. Our method achieves lowest value and policy losses than the base algorithm (GAE-PPO) and data augmentation baselines (RAD and DRAC). Pybullet We use Pybullet [13]: Minitaur quadruped and HalfCheetah environments with vector-based observation. Each observation consists of raw sensory inputs. The Minitaur quadruped is a 4-legged robot, and the task is to travel as long as possible on ﬂat ground. Furthermore, the HalfCheetah is a two-legged robot that can control its movement in 2D, and the task is to travel as much distance as possible. The action spaces are continuous in these environments. Snippets of these environments are in Figure 2. Baselines All agents usage on-policy PPO [3] as the base policy. We compare our method with Generalized Advantage Estimation (GAE) [5] which is referred to as GAE-PPO in our experiments. GAE is shown to perform better compared to other advantage estimation techniques [6]. Moreover, we compare with the data augmentation-based approach uses data augmentation to transform the observation and then uses the transformed observation to train the base policy. In particular, we compare our method with existing baselines RAD [10] and DRAC [11]. RAD, referred to as RAD-PPO, proposes various data augmentation techniques to improve learning from pixel-based observation. DRAC, referred to as DRAC- PPO leverages the data augmentation to regularize the policy and value learning, showing improved performance in policy learning. In our method, we replaced the GAE estimation with our proposed Bootstrap Advantage Estimation (BAE), which is referred to as BAE-PPO. the agents, including our BAE-PPO and baseline, using the implementation available in [15]. In a PPO-based scenario, many factors have been identiﬁed as key in implementing algorithms that impact the performance [6], [14]. Thus, we use the same implementation logic for all the baselines and our method for a fair comparison. Data augmentation We evaluate Cutout Color data augmen- tation for image-based observation, which performs best in our setup compared to another popularly used Random Crop. Thus, we report Cutout Color data augmentation results for RAD, DRAC, and our BAE. We use the implementation avail- able in RAD [10] for data augmentation. For the vector-based observation robotic task, we use a random amplitude scale proposed in RAD [10]. This method multiply the observation with a number generated randomly between a range α to β. We used best performing range α = 0.8 to β = 1.4 for our We build all BAE method, and a range α = 0.6 to β = 1.2 for RAD, and DRAC (suggested in RAD [10]). Implementation and Hyper-parameters For the Procgen Starpilot and Miner, we report mean and standard deviation across 3 seeds run following the setup of Procgen paper [17]. We used an Nvidia A100 GPU to run agents with the IMPALA CNN model [18] on the image observation-based Procgen environments. We use neural networks to represent policy and value functions for vector-based observations. For Deepmind control environments, we report results with 10 random seed runs, and for Pybullet environments, we report results over 5 seeds. For all experiments, we keep the common hyperparam- eters the same for a fair comparison. The implementation and hyperparameters are based on [15], [19]. For all results, we report the mean (showed in solid line) and standard deviation (showed in shaded areas) across runs. B. Results The PPO-based agent’s objective consists of value loss and policy loss. The objective is to reduce them and potentially improve the expected return. We show that our method BAE reduces the losses and thus learns a better value function and policy than the baselines. We then show how our method performs in the expected return. Procgen Results Figure 3 shows value and policy loss during policy training on Procgen Starpilot environments. As the training progresses, the loss of our method BAE reduces drastically compared to the baselines. These results show the sign of the effectiveness of our method in reducing the agent’s losses. Note that the advantage estimation is used to train both the value function and the policy; thus, better estimation of advantages should generally give better value and policy. In this sense, our method shows empirical evidence that it can help better advantage estimation. We observe that in the Starpilot environment, the success in policy and value loss translates to the ﬁnal return. In Figure 4 we see that our method (BAE-PPO) shows improved sample efﬁciency (Train Return) and generalization capacity compared to the baseline GAE-PPO, RAD-PPO, and DRAC-PPO. Note that the RAD-PPO performance worsens the perfor- mance of the base GAE-PPO algorithms. This result is con- sistent with the ﬁndings in [11], and it shows that naive data Starpilot Env. Sample efﬁciency performance measured in train Fig. 4. time return. We see our method BAE-PPO achieves higher returns where DRAC does not improve the base agent’s (GAE-PPO) performance. RAD slightly worsens the performance of the base agent. [Right] Generalization performance measured in test time return. We see a similar trend and observe that our method performs the best. [Left] Performance on Quadruped run environments. Our method Fig. 7. BAE-PPO shows higher mean returns compared to all other agents. Data augmentation baseline RAD and DRAC worsen the performance of the base agent (GAE-PPO). [Right] Our method BAE consistently achieves a higher mean where DRAC fails to improve upon the base agent, and RAD worsens the base performance. Fig. 5. Miner Env. Training time policy and value loss [lower is better]. The value loss of BAE eventually become lower. Losses of RAD increases compared to base PPO. Fig. 6. Miner Env. [Left] Sample efﬁciency performance measured in train time return and [Right] Generalization performance measured in test time return. Overall, we see our method BAE-PPO shows consistence improvement over baselines. augmentation can be detrimental to performance. Furthermore, we observe a similar trend for the value and policy loss results. We observe a similar performance trend in the Procgen Miner environment. In Figure 5, we see that in our method, BAE shows a smaller value loss eventually despite being higher at the beginning compared to the baselines. GAE and DRAC show slightly lower values in the policy loss plot than BAE. However, both show smaller policy losses in general. In performance measure (Figure 6), we see that our method shows better performance throughout the training than the baseline in both sample efﬁciency and generalization. The performance difference is consistent across timestep. Similar to Starpilot, RAD also performs worse compared to base GAE- PPO. We further evaluate our method on vector-based state space and continuous robotic tasks: Deepmind control and Pybullet. Note that the setup of these benchmarks are different from Procgen’s train-test setup, and here we evaluate how our agents can perform in high-dimensional states and procedurally gen- erated task. Thus we can only report returns during training. Deepmind Control Results Figure 7 shows performance compariosn on Quadruped run and Cartpole - Three Poles environments. We observe that our method BAE achieves the best performance in both environments. On the other hand, the baseline RAD severely worsens the base agent’s (GAE-PPO) performance in both environments. Another baseline, DRAC, worsens the base agents’ performance in Quadruped Run and fails to improve performance in the Cartpole Three Poles envi- ronment. These results show that properly using the augmenta- tion in policy learning can lead to strong performance. In this case, all data augmentation agents (RAD, DRAC, and BAE) use the same random amplitude modulation augmentation. However, using this augmentation in advantage computation, our method BAE shows a substantial performance boost. On the other hand, other baselines, RAD and DRAC, worsen the performance (Quadruped Run). Pybullet Results In Figure 8, for the HalfCheetah environ- ment, we see that our method BAE performs better than base agent GAE-PPO and other data augmentation baselines RAD and DRAC. On the other hand, in Minitaur, the data augmentation baseline DRAC and RAD worsen the base agent’s (GAE-PPO) performance where BAE can maintain the base performance. Overall, these results show the robustness of our method in performance compared to baseline data augmentation methods. Therefore, the proposed augmented observation is expected not to worsen the base performance. However, in our exper- iments, we observe that the RAD and DRAC barely match the base agent’s (GAE-PPO) results and sometimes worsen the performance. These variabilities in performance hinder the widespread adaptation of these methods. In contrast, our method BAE shows a consistent performance across various tasks without reducing the base agent’s performance. [2] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement learning,” Machine learning, vol. 8, no. 3, pp. 229–256, 1992. [3] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox- imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347, 2017. [4] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region policy optimization,” in International conference on machine learning. PMLR, 2015, pp. 1889–1897. [5] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High- dimensional continuous control using generalized advantage estimation,” in Proceedings of the International Conference on Learning Represen- tations (ICLR), 2016. [6] M. Andrychowicz, A. Raichuk, P. Sta´nczyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot, M. Geist, O. Pietquin, M. Michalski, S. Gelly, and O. Bachem, “What matters for on-policy deep actor-critic methods? a large-scale study,” in International Conference on Learning Represen- tations, 2021. [7] X. Song, Y. Jiang, S. Tu, Y. Du, and B. Neyshabur, “Observational overﬁtting in reinforcement learning,” in International Conference on Learning Representations, 2020. [8] C. Zhang, O. Vinyals, R. Munos, and S. Bengio, “A study on overﬁtting in deep reinforcement learning,” arXiv preprint arXiv:1804.06893, 2018. [9] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman, “Quantifying generalization in reinforcement learning,” in International Conference on Machine Learning. PMLR, 2019, pp. 1282–1289. [10] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas, “Reinforcement learning with augmented data,” in Advances in neural information processing systems, 2020. [11] R. Raileanu, M. Goldstein, D. Yarats, I. Kostrikov, and R. Fergus, “Automatic data augmentation for generalization in deep reinforcement learning,” arXiv preprint arXiv:2006.12862, 2020. [12] S. Tunyasuvunakool, A. Muldal, Y. Doron, S. Liu, S. Bohez, J. Merel, T. Erez, T. Lillicrap, N. Heess, and Y. Tassa, “dm control: Software and tasks for continuous control,” Software Impacts, vol. 6, p. 100022, 2020. [13] E. Coumans and Y. Bai, “Pybullet, a python module for physics simulation for games, robotics and machine learning,” http://pybullet.org, 2016–2021. [14] L. Engstrom, A. Ilyas, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph, and A. Madry, “Implementation matters in deep rl: A case study on ppo and trpo,” in International Conference on Learning Representations, 2020. [15] S. Huang, R. F. J. Dossa, A. Rafﬁn, A. Kanervisto, and W. Wang, “The 37 implementation details of proximal policy optimization,” in ICLR Blog Track, 2022. [Online]. Available: https://iclr-blog-track.github.io/ 2022/03/25/ppo-implementation-details/ [16] R. Kirk, A. Zhang, E. Grefenstette, and T. Rockt¨aschel, “A sur- vey of generalisation in deep reinforcement learning,” arXiv preprint arXiv:2111.09794, 2021. [17] K. Cobbe, C. Hesse, J. Hilton, and J. Schulman, “Leveraging procedu- ral generation to benchmark reinforcement learning,” in International conference on machine learning. PMLR, 2020, pp. 2048–2056. [18] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning et al., “Impala: Scalable dis- tributed deep-rl with importance weighted actor-learner architectures,” arXiv preprint arXiv:1802.01561, 2018. [19] S. Huang, R. F. J. Dossa, C. Ye, and J. Braga, “Cleanrl: High-quality single-ﬁle implementations of deep reinforcement learning algorithms,” 2021. [20] J. Peters and S. Schaal, “Reinforcement learning of motor skills with policy gradients,” Neural networks, vol. 21, no. 4, pp. 682–697, 2008. [21] C. Wu, A. Rajeswaran, Y. Duan, V. Kumar, A. M. Bayen, S. Kakade, I. Mordatch, and P. Abbeel, “Variance reduction for policy gradient with action-dependent factorized baselines,” in International Conference on Learning Representations, 2018. [22] M. Igl, K. Ciosek, Y. Li, S. Tschiatschek, C. Zhang, S. Devlin, and K. Hofmann, “Generalization in reinforcement learning with selective noise injection and information bottleneck,” in Advances in neural information processing systems, 2019, pp. 13 978–13 990. [23] M. M. Rahman and Y. Xue, “Bootstrap state representation using style transfer for better generalization in deep reinforcement learning,” in European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2022), 2022. Performance on Minitaur [Left] and HalfCheetah [Right] environ- Fig. 8. ments. Our method shows better or similar performance compared to the base agent (GAE-PPO), where the data augmentation baselines sometimes worsen the base performance. V. RELATED WORK Advantage Estimation. The baseline has been leveraged to reduce variance in policy gradient update [2], [20], [21]. Furthermore, effective modiﬁcation of such methods is the use of advantage estimation. This method is commonly used in policy optimization and enjoys strong empirical success [6], especially the Generalized Advantage Estimation (GAE) [5] method. In contrast to GAE, our method BAE leverages data augmentation and incorporates advantage computation across various semantically similar states. Empirically we observe that our method of computing advantage can be beneﬁcial over GAE, especially in high-dimensional and procedural generated environments. Data augmentation. Data augmentation has been demon- strated to be an effective and efﬁcient approaches to improve performance [9]–[11]. Other methods proposed to improve generalization which includes regularization [22], and style- transfer [23]. Depending on how the augmented observation is used, the method can be different; for example, RAD [10] and DRAC [11]. In contrast to these methods, our method incorporates data augmentation into advantage estimation, which shows better empirical performance compared to these methods (RAD and DRAC). VI. CONCLUSION In this paper, we propose a data augmentation-based advan- tage estimation method for policy optimization. Our Bootstrap advantage estimation (BAE) method replaces the GAE method in policy gradient-based algorithms. We demonstrated the effectiveness of our method on PPO algorithms. Furthermore, we evaluated our methods on both image-based observation space with discrete action space and vector-based observation with continuous action space (Procgen, Deepmind Control, and Pybullet). Our BAE method showed better performance in various environment setups than GAE. Furthermore, our method performs better than two existing data augmentation techniques (RAD and DRAC). REFERENCES [1] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradi- ent methods for reinforcement learning with function approximation,” Advances in neural information processing systems, vol. 12, 1999.","['bootstrap', 'advantage', 'estimation', 'policy', 'optimization', 'reinforcement', 'learning', 'computer', 'science', 'c', 'x', 'abstract', 'paper', 'propose', 'advantage', 'estimation', 'proach', 'base', 'datum', 'augmentation', 'policy', 'optimization', 'use', 'datum', 'augmentation', 'input', 'learn', 'value', 'policy', 'function', 'exist', 'method', 'use', 'method', 'use', 'data', 'augmentation', 'compute', 'bootstrap', 'advantage', 'estimation', 'bootstrap', 'advantage', 'estimation', 'bae', 'use', 'learn', 'update', 'gradient', 'policy', 'value', 'function', 'demonstrate', 'effectiveness', 'approach', 'conduct', 'ex', 'periment', 'several', 'environment', 'environment', 'benchmark', 'procgen', 'deepmind', 'control', 'pybullet', 'include', 'image', 'vectorbase', 'observation', 'discrete', 'continuous', 'action', 'space', 'observe', 'method', 'reduce', 'policy', 'value', 'loss', 'well', 'generalize', 'advantage', 'estimation', 'method', 'eventually', 'improve', 'cumulative', 'return', 'furthermore', 'method', 'perform', 'well', 'recently', 'propose', 'datum', 'augmentation', 'technique', 'drac', 'overall', 'method', 'perform', 'well', 'empirically', 'baseline', 'sample', 'efﬁciency', 'generalization', 'agent', 'test', 'unseen', 'environment', 'index', 'term', 'deep', 'reinforcement', 'learn', 'advantage', 'mation', 'generalization', 'reinforcement', 'learning', 'introduction', 'policy', 'gradient', 'method', 'directly', 'involve', 'learn', 'policy', 'function', 'enjoy', 'performance', 'improvement', 'function', 'approximation', 'setting', 'policy', 'gradient', 'theorem', 'give', 'rather', 'simple', 'formulation', 'gradient', 'estimation', 'give', 'estimation', 'however', 'require', 'turn', 'estimation', 'entire', 'trajectory', 'lead', 'high', 'variance', 'commonly', 'use', 'technique', 'reduce', 'variance', 'use', 'baseline', 'help', 'reduce', 'variance', 'introduce', 'bias', 'several', 'effective', 'method', 'originate', 'concept', 'effective', 'way', 'use', 'value', 'function', 'baseline', 'indicate', 'action', 'take', 'current', 'policy', 'well', 'average', 'action', 'take', 'state', 'formulate', 'advantage', 'estimation', 'however', 'base', 'single', 'trajectory', 'estimate', 'local', 'high', 'variance', 'thus', 'add', 'truncated', 'scenario', 'value', 'function', 'potentially', 'give', 'global', 'estimate', 'combine', 'generalize', 'advantage', 'estimation', 'show', 'strong', 'empirical', 'result', 'however', 'due', 'procedural', 'style', 'content', 'generation', 'value', 'estimation', 'erroneous', 'give', 'different', 'advantage', 'estimate', 'even', 'observation', 'context', 'change', 'time', 'semantic', 'meaning', 'remain', 'procedural', 'scenario', 'exist', 'realworld', 'scenario', 'cause', 'agent', 'perform', 'supoptimally', 'thus', 'advantage', 'estimation', 'problematic', 'scenario', 'result', 'poor', 'sample', 'efﬁciency', 'issue', 'exist', 'partly', 'due', 'difﬁculty', 'reduce', 'policy', 'estimation', 'policy', 'loss', 'value', 'function', 'estimation', 'value', 'loss', 'reward', 'semantic', 'remain', 'paper', 'propose', 'bootstrap', 'advantage', 'estimation', 'bae', 'calculate', 'advantage', 'estimation', 'comput', 'e', 'advantage', 'estimation', 'original', 'transformed', 'observation', 'assume', 'transformation', 'semantic', 'invariant', 'contextual', 'information', 'change', 'example', 'change', 'background', 'game', 'relevant', 'background', 'color', 'red', 'green', 'semantic', 'invariant', 'transformation', 'ultimate', 'goal', 'train', 'agent', 'robust', 'background', 'change', 'perform', 'well', 'blue', 'background', 'example', 'transform', 'observation', 'form', 'experiment', 'datum', 'augmentationbase', 'observation', 'transformation', 'eg', 'random', 'crop', 'amplitude', 'scale', 'intuition', 'take', 'advantage', 'estimate', 'augment', 'datum', 'force', 'advantage', 'estimate', 'consider', 'error', 'many', 'variation', 'observation', 'demonstrate', 'bae', 'policy', 'gradient', 'method', 'ppo', 'show', 'comparison', 'gaebased', 'estimation', 'observe', 'method', 'bae', 'achieve', 'well', 'sample', 'ﬁciency', 'zeroshot', 'generalization', 'procgen', 'environment', 'starpilot', 'miner', 'imagebase', 'observation', 'recent', 'datum', 'augmentation', 'demonstrate', 'effective', 'choice', 'improve', 'sample', 'efﬁciency', 'servation', 'space', 'improve', 'generalization', 'process', 'sometimes', 'generate', 'empirical', 'success', 'method', 'lead', 'detrimental', 'performance', 'observe', 'experiment', 'mitigate', 'issue', 'drac', 'method', 'suggest', 'regularize', 'policy', 'network', 'value', 'network', 'augmented', 'observation', 'use', 'augmented', 'datum', 'policy', 'training', 'contrast', 'propose', 'novel', 'way', 'leverage', 'datum', 'augmentation', 'method', 'augment', 'observation', 'advantage', 'estimation', 'core', 'component', 'many', 'policy', 'optimization', 'algorithm', 'ppo', 'conduct', 'extensive', 'experiment', 'environ', 'ment', 'consist', 'image', 'vectorbase', 'observation', 'discrete', 'continuous', 'action', 'space', 'method', 'fall', 'general', 'modelfree', 'onpolicy', 'category', 'experiment', 'proximal', 'policy', 'optimization', 'ppo', 'paper', 'particular', 'experiment', 'procgen', 'starpilot', 'miner', 'environment', 'demonstrate', 'method', 'beneﬁcial', 'generalization', 'setup', 'compare', 'baseline', 'datum', 'augmentation', 'technique', 'drac', 'far', 'evaluate', 'method', 'several', 'robotic', 'lo', 'comotion', 'task', 'highdimensional', 'observation', 'quadrupe', 'run', 'cartpole', 'pole', 'pybullet', 'minitaur', 'experiment', 'observe', 'method', 'bae', 'perform', 'well', 'baseline', 'agent', 'include', 'base', 'drac', 'method', 'achieve', 'much', 'low', 'loss', 'policy', 'value', 'function', 'estimation', 'eventually', 'perform', 'well', 'sample', 'efﬁciency', 'zeroshot', 'generalization', 'baseline', 'agent', 'include', 'datum', 'augmentation', 'observe', 'base', 'line', 'datum', 'augmentation', 'method', 'drac', 'sometimes', 'worsen', 'base', 'model', 'performance', 'contrast', 'bae', 'method', 'improve', 'performance', 'test', 'environment', 'perform', 'consistently', 'base', 'case', 'result', 'show', 'method', 'bae', 'robust', 'performance', 'compare', 'baseline', 'datum', 'augmentation', 'method', 'source', 'code', 'method', 'available', 'commasud99rbae', 'preliminary', 'problem', 'setting', 'reinforcement', 'learn', 'assume', 'taskenvironment', 'markov', 'decision', 'process', 'mdp', 'denote', 'p', 'r', 'γ', 'state', 'space', 'action', 'space', 'p', 'transition', 'probability', 'state', 'result', 'action', 'setup', 'timestep', 'agent', 'take', 'action', 'state', 'environment', 'transition', 'next', 'state', 'determine', 'transition', 'probability', 'p', 'st1st', 'reinforcement', 'learning', 'framework', 'goal', 'agent', 'learn', 'policy', '∈', 'π', 'maximize', 'expect', 'return', 'mdp', 'set', 'possible', 'policy', 'π', 'single', 'policy', 'mapping', 'state', 'action', 'policy', 'achieve', 'high', 'expect', 'return', 'optimal', 'policy', 'gradient', 'proximal', 'policy', 'optimization', 'ppo', 'type', 'policy', 'gradient', 'method', 'achieve', 'tremendous', 'success', 'popularly', 'use', 'many', 'setup', 'effective', 'learning', 'simple', 'implementation', 'however', 'choice', 'implementation', 'detail', 'impact', 'perfor', 'mance', 'algorithm', 'signiﬁcant', 'way', 'implementation', 'detail', 'consist', 'train', 'individual', 'component', 'algorithm', 'learn', 'value', 'function', 'policy', 'function', 'advantage', 'estimation', 'follow', 'objective', 'probability', 'choose', 'action', 'give', 'state', 'timestep', 'use', 'current', 'policy', 'parameterize', 'θ', 'hand', 'πθoldatst', 'refer', 'probability', 'use', 'old', 'policy', 'parameterize', 'previous', 'parameter', 'value', 'θold', 'advantage', 'estimation', 'advantage', 'take', 'action', 'popular', 'effective', 'choice', 'estimate', 'advantage', 'use', 'value', 'function', 'follow', 'equation', '−v', 'γrr1', 'value', 'function', 'give', 'average', 'future', 'return', 'underlying', 'policy', 'ﬁrst', 'term', 'value', 'prediction', 'timestep', 'rest', 'term', 'last', 'term', 'equation', 'discount', 'monte', 'estimation', 'compute', 'give', 'episode', 'last', 'term', 'value', 'prediction', 'thus', 'overall', 'represent', 'much', 'current', 'action', 'compare', 'current', 'value', 'prediction', 'advantage', 'action', 'policy', 'weigh', 'action', 'multiply', 'πθatst', 'πθoldatst', 'equation', 'introduce', 'importance', 'sampling', 'allow', 'estimate', 'advantage', 'old', 'policy', 'sample', 'detail', 'discussion', 'refer', 'reader', 'value', 'function', 'estimation', 'effective', 'value', 'function', 'mation', 'regress', 'value', 'prediction', 'advantage', 'base', 'return', 'estimation', 'verror', 'vprediction', '−', 'vreturn', 'vprediction', 'predict', 'value', 'vreturn', 'value', 'compute', 'reward', 'r', 'rt', 'sample', 'trajectory', 'advantage', 'thus', 'vreturn', 'note', 'way', 'policy', 'equation', 'value', 'function', 'dependent', 'advantage', 'estimation', 'thus', 'accurate', 'advantage', 'estimation', 'give', 'low', 'policy', 'value', 'loss', 'thus', 'well', 'perform', 'policy', 'generalization', 'turn', 'attention', 'scenario', 'different', 'episode', 'vary', 'confound', 'feature', 'confounder', 'also', 'call', 'observation', 'note', 'contextimpact', 'reward', 'environment', 'however', 'misguide', 'agent', 'think', 'otherwise', 'nature', 'agent', 'overﬁt', 'confound', 'feature', 'fail', 'generalize', 'slightly', 'modiﬁe', 'test', 'environment', 'detail', 'overview', 'generalization', 'reinforcement', 'learning', 'refer', 'reader', 'servery', 'paper', 'datum', 'augmentation', 'various', 'form', 'leverage', 'idea', 'transform', 'observation', 'general', 'observation', 'semantic', 'meaning', 'remain', 'contextual', 'information', 'change', 'however', 'context', 'information', 'readily', 'available', 'thus', 'need', 'impose', 'various', 'assumption', 'certain', 'transformation', 'observation', 'scid48', 'keep', 'reward', 'semantic', 'image', 'base', 'observation', 'various', 'image', 'manipulation', 'use', 'cropping', 'rotation', 'colorjitter', 'advantage', 'estimation', 'essential', 'component', 'policy', 'training', 'estimate', 'advantage', 'generalize', 'advantage', 'estimation', 'useful', 'way', 'compute', 'tage', 'combine', 'value', 'function', 'estimate', 'monte', 'carlo', 'method', 'however', 'datum', 'augmentationbase', 'regularization', 'approach', 'handle', 'case', 'advantage', 'estimation', 'advantage', 'estimate', 'use', 'single', 'trajectory', 'thus', 'compute', 'advantage', 'high', 'variance', 'systematic', 'noise', 'advantage', 'mation', 'give', 'similar', 'state', 'observation', 'advantage', 'estimation', 'example', 'observation', 'semantic', 'different', 'background', 'red', 'blue', 'advantage', 'background', 'essential', 'thus', 'confound', 'boostrap', 'advantage', 'estimation', 'bae', 'propose', 'bootstrap', 'advantage', 'estimation', 'use', 'observation', 'transformation', 'mitigate', 'abovementione', 'issue', 'formally', 'generate', 'additional', 'estimation', 'transformation', 'transformation', 'compute', 'estimate', 'equation', 'refer', 'augmentation', 'furthermore', 'ﬁnally', 'take', 'average', 'estimate', 'equation', 'estimate', 'ﬁnal', 'advantage', 'estimation', 'kstep', 'return', 'finally', 'achieve', 'bootstrap', 'advantage', 'estimation', 'trajectory', 'length', 'combine', 'several', 'kstep', 'return', 'use', 'exponentialweighte', 'average', 'follow', 'gae', 'method', 'λa2b', 'abaeγλ', 'use', 'compute', 'advantage', 'timestep', 'episode', 'note', 'bae', 'differ', 'compute', 'kstep', 'return', 'equation', 'show', 'detail', 'step', 'use', 'bae', 'ppobase', 'policy', 'optimization', 'method', 'paper', 'algorithm', 'bae', 'policy', 'optimization', 'get', 'transformation', 'function', 'augmentation', 'type', 'v', 'environment', 'step', 'get', 'ppo', 'policy', 'optimization', 'agent', 'iteration', '∼', 'πθatst', 'st1', '∼', 'p', 'st1st', 'b', 'end', 'transform', 'b', 'get', 'bcid48', 'use', 'augmentation', 'function', 'compute', 'advantage', 'estimate', 'bae', 'datum', 'b', 'bcid48', 'use', 'equation', 'perform', 'ppo', 'update', 'bae', 'optimize', 'equation', 'end', 'leverage', 'ppo', 'base', 'use', 'eralize', 'advantage', 'estimation', 'default', 'estimator', 'contrast', 'method', 'baeppo', 'use', 'bootstrap', 'advantage', 'estimation', 'bae', 'instead', 'datum', 'augmentation', 'baseline', 'rad', 'drac', 'use', 'base', 'ppo', 'advantage', 'estimation', 'experiment', 'use', 'equation', 'mean', 'use', 'datum', 'augmentation', 'approach', 'combine', 'original', 'advantage', 'estimation', 'equation', 'note', 'apply', 'observation', 'transformation', 'part', 'agent', 'objective', 'thus', 'equation', 'remain', 'theoretically', 'practically', 'sound', 'furthermore', 'empirically', 'show', 'bootstrap', 'advantage', 'estimation', 'lead', 'small', 'value', 'policy', 'loss', 'performance', 'boost', 'finally', 'also', 'compare', 'baseline', 'drac', 'show', 'method', 'perform', 'well', 'many', 'setup', 'experiment', 'setup', 'environment', 'experiment', 'imagebase', 'observa', 'tion', 'discrete', 'action', 'space', 'vectorbase', 'observation', 'continuous', 'action', 'space', 'procgen', 'use', 'procgen', 'starpilot', 'miner', 'figure', 'use', 'imagebase', 'observation', 'procedural', 'generation', 'produce', 'challenging', 'game', 'logic', 'change', 'episode', 'episode', 'benchmark', 'allow', 'evaluate', 'sample', 'efﬁciency', 'generalization', 'capacity', 'agent', 'environment', 'level', 'subset', 'level', 'use', 'train', 'agent', 'full', 'distribution', 'level', 'use', 'test', 'agent', 'generalization', 'capacity', 'experiment', 'use', 'standard', 'evaluation', 'protocol', 'level', 'environment', 'use', 'training', 'difﬁculty', 'level', 'environment', 'discrete', 'action', 'space', 'dimension', 'intuitively', 'train', 'agent', 'access', 'limited', 'number', 'envi', 'ronment', 'variability', 'eg', 'level', 'train', 'agent', 'test', 'available', 'variability', 'consist', 'unseen', 'scenario', 'thus', 'master', 'game', 'agent', 'focus', 'essential', 'aspect', 'state', 'ignore', 'irrelevant', 'information', 'background', 'color', 'fig', 'procgen', 'snapshot', 'starpilot', 'miner', 'environment', 'generate', 'procedurally', 'result', 'different', 'observation', 'eg', 'background', 'episode', 'environment', 'deepmind', 'control', 'use', 'dm', 'control', 'quadrupe', 'run', 'cartpole', 'pole', 'figure', 'quadrupe', 'run', 'highdimensional', 'vector', 'observation', 'task', 'run', 'far', 'possible', 'hand', 'cartpole', 'variation', 'consist', 'procedurally', 'generate', 'pole', 'complexity', 'environ', 'ment', 'suitable', 'evaluate', 'datum', 'augmentationbase', 'approach', 'leave', 'deepmind', 'control', 'snapshot', 'fig', 'task', 'right', 'pybullet', 'snapshot', 'pybullet', 'minitaur', 'quadrupe', 'environment', 'environment', 'contain', 'vectorbase', 'state', 'space', 'action', 'space', 'continuous', 'fig', 'starpilot', 'env', 'training', 'time', 'policy', 'value', 'loss', 'lower', 'well', 'method', 'achieve', 'low', 'value', 'policy', 'loss', 'base', 'gaeppo', 'data', 'augmentation', 'baseline', 'rad', 'drac', 'pybullet', 'use', 'pybullet', 'minitaur', 'quadrupe', 'environment', 'vectorbase', 'observation', 'observation', 'consist', 'raw', 'sensory', 'input', 'minitaur', 'quadrupe', 'robot', 'task', 'travel', 'long', 'possible', 'ﬂat', 'ground', 'furthermore', 'twolegged', 'robot', 'control', 'movement', 'task', 'travel', 'much', 'distance', 'possible', 'action', 'space', 'continuous', 'environment', 'snippet', 'environment', 'figure', 'baseline', 'agent', 'usage', 'onpolicy', 'ppo', 'base', 'policy', 'compare', 'method', 'generalized', 'advantage', 'estimation', 'refer', 'gaeppo', 'experiment', 'show', 'perform', 'well', 'compare', 'advantage', 'estimation', 'technique', 'moreover', 'compare', 'datum', 'augmentationbase', 'approach', 'use', 'data', 'augmentation', 'transform', 'observation', 'use', 'transform', 'observation', 'train', 'base', 'policy', 'particular', 'compare', 'method', 'exist', 'baseline', 'rad', 'rad', 'refer', 'radppo', 'propose', 'various', 'data', 'augmentation', 'technique', 'improve', 'learn', 'pixelbase', 'observation', 'drac', 'refer', 'drac', 'ppo', 'leverage', 'datum', 'augmentation', 'regularize', 'policy', 'value', 'learn', 'show', 'improved', 'performance', 'policy', 'learning', 'method', 'replace', 'estimation', 'propose', 'bootstrap', 'advantage', 'estimation', 'bae', 'refer', 'baeppo', 'agent', 'include', 'baeppo', 'baseline', 'use', 'implementation', 'available', 'ppobase', 'scenario', 'many', 'factor', 'identiﬁe', 'key', 'implement', 'algorithm', 'impact', 'performance', 'thus', 'use', 'implementation', 'logic', 'baseline', 'method', 'fair', 'comparison', 'datum', 'augmentation', 'evaluate', 'cutout', 'color', 'datum', 'augman', 'tation', 'imagebase', 'observation', 'perform', 'good', 'setup', 'compare', 'popularly', 'use', 'random', 'crop', 'thus', 'report', 'cutout', 'color', 'datum', 'augmentation', 'result', 'bae', 'use', 'implementation', 'avail', 'able', 'data', 'augmentation', 'vectorbase', 'observation', 'robotic', 'task', 'use', 'random', 'amplitude', 'scale', 'propose', 'method', 'multiply', 'observation', 'number', 'generate', 'randomly', 'range', 'α', 'use', 'good', 'perform', 'range', 'build', 'bae', 'method', 'range', 'suggest', 'implementation', 'hyperparameter', 'procgen', 'starpilot', 'miner', 'report', 'mean', 'standard', 'deviation', 'seed', 'run', 'follow', 'setup', 'procgen', 'paper', 'use', 'gpu', 'run', 'agent', 'model', 'image', 'observationbase', 'procgen', 'environment', 'use', 'neural', 'network', 'represent', 'policy', 'value', 'function', 'vectorbase', 'observation', 'deepmind', 'control', 'environment', 'report', 'result', 'random', 'seed', 'run', 'pybullet', 'environment', 'report', 'result', 'seed', 'experiment', 'keep', 'common', 'hyperparam', 'eter', 'fair', 'comparison', 'implementation', 'hyperparameter', 'base', 'result', 'report', 'mean', 'show', 'solid', 'line', 'standard', 'deviation', 'show', 'shaded', 'area', 'run', 'b', 'result', 'ppobase', 'agent', 'objective', 'consist', 'value', 'loss', 'policy', 'loss', 'objective', 'reduce', 'potentially', 'improve', 'expect', 'return', 'show', 'method', 'bae', 'reduce', 'loss', 'thus', 'learn', 'well', 'value', 'function', 'policy', 'baseline', 'show', 'method', 'perform', 'expect', 'return', 'procgen', 'result', 'figure', 'show', 'value', 'policy', 'loss', 'policy', 'training', 'procgen', 'starpilot', 'environment', 'training', 'progress', 'loss', 'method', 'bae', 'reduce', 'drastically', 'compare', 'baseline', 'result', 'show', 'sign', 'effectiveness', 'method', 'reduce', 'agent', 'loss', 'note', 'advantage', 'estimation', 'use', 'train', 'value', 'function', 'policy', 'thus', 'well', 'estimation', 'advantage', 'generally', 'give', 'well', 'value', 'policy', 'sense', 'method', 'show', 'empirical', 'evidence', 'help', 'well', 'advantage', 'estimation', 'observe', 'starpilot', 'environment', 'success', 'policy', 'value', 'loss', 'translate', 'ﬁnal', 'return', 'figure', 'see', 'method', 'baeppo', 'show', 'improved', 'sample', 'efﬁciency', 'train', 'return', 'generalization', 'capacity', 'compare', 'baseline', 'gaeppo', 'note', 'radppo', 'performance', 'worsen', 'perfor', 'mance', 'base', 'gaeppo', 'algorithm', 'result', 'con', 'sistent', 'ﬁnding', 'show', 'naive', 'datum', 'starpilot', 'env', 'sample', 'efﬁciency', 'performance', 'measure', 'train', 'fig', 'time', 'return', 'see', 'method', 'baeppo', 'achieve', 'high', 'return', 'drac', 'improve', 'base', 'agent', 'gaeppo', 'performance', 'slightly', 'worsen', 'performance', 'base', 'agent', 'right', 'generalization', 'performance', 'measure', 'test', 'time', 'return', 'see', 'similar', 'trend', 'observe', 'method', 'perform', 'good', 'left', 'performance', 'quadrupe', 'run', 'environment', 'method', 'fig', 'baeppo', 'show', 'high', 'mean', 'return', 'compare', 'agent', 'datum', 'augmentation', 'baseline', 'rad', 'drac', 'worsen', 'performance', 'base', 'agent', 'gaeppo', 'method', 'bae', 'consistently', 'achieve', 'high', 'mean', 'drac', 'fail', 'improve', 'base', 'agent', 'worsen', 'base', 'performance', 'fig', 'miner', 'env', 'training', 'time', 'policy', 'value', 'loss', 'lower', 'well', 'value', 'loss', 'bae', 'eventually', 'become', 'low', 'loss', 'rad', 'increase', 'compare', 'base', 'ppo', 'fig', 'miner', 'env', 'leave', 'sample', 'efﬁciency', 'performance', 'measure', 'train', 'time', 'return', 'right', 'generalization', 'performance', 'measure', 'test', 'time', 'return', 'overall', 'see', 'method', 'baeppo', 'show', 'consistence', 'improvement', 'baseline', 'augmentation', 'detrimental', 'performance', 'furthermore', 'observe', 'similar', 'trend', 'value', 'policy', 'loss', 'result', 'observe', 'similar', 'performance', 'trend', 'procgen', 'miner', 'environment', 'figure', 'see', 'method', 'bae', 'show', 'small', 'value', 'loss', 'eventually', 'high', 'beginning', 'compare', 'baseline', 'drac', 'show', 'slightly', 'low', 'value', 'policy', 'loss', 'plot', 'bae', 'however', 'show', 'small', 'policy', 'loss', 'general', 'performance', 'measure', 'figure', 'see', 'method', 'show', 'well', 'performance', 'training', 'baseline', 'sample', 'efﬁciency', 'generalization', 'performance', 'difference', 'consistent', 'timestep', 'similar', 'starpilot', 'rad', 'also', 'perform', 'bad', 'compare', 'base', 'gae', 'ppo', 'far', 'evaluate', 'method', 'vectorbase', 'state', 'space', 'continuous', 'robotic', 'task', 'deepmind', 'control', 'pybullet', 'note', 'setup', 'benchmark', 'different', 'traint', 'setup', 'evaluate', 'agent', 'perform', 'highdimensional', 'state', 'procedurally', 'erate', 'task', 'thus', 'report', 'return', 'train', 'deepmind', 'control', 'result', 'figure', 'show', 'performance', 'compariosn', 'quadrupe', 'run', 'cartpole', 'pole', 'environment', 'observe', 'method', 'bae', 'achieve', 'good', 'performance', 'environment', 'hand', 'baseline', 'rad', 'severely', 'worsen', 'base', 'agent', 'gaeppo', 'performance', 'environment', 'baseline', 'drac', 'worsen', 'base', 'agent', 'performance', 'quadrupe', 'run', 'fail', 'improve', 'performance', 'cartpole', 'pole', 'envi', 'ronment', 'result', 'show', 'properly', 'use', 'augmenta', 'tion', 'policy', 'learning', 'lead', 'strong', 'performance', 'case', 'datum', 'augmentation', 'agent', 'bae', 'use', 'random', 'amplitude', 'modulation', 'augmentation', 'however', 'use', 'augmentation', 'advantage', 'computation', 'method', 'bae', 'show', 'substantial', 'performance', 'boost', 'hand', 'baseline', 'worsen', 'performance', 'quadrupe', 'run', 'pybullet', 'result', 'figure', 'ment', 'see', 'method', 'bae', 'perform', 'well', 'base', 'agent', 'gaeppo', 'datum', 'augmentation', 'baseline', 'drac', 'hand', 'minitaur', 'datum', 'augmentation', 'baseline', 'worsen', 'base', 'agent', 'gaeppo', 'performance', 'bae', 'maintain', 'base', 'performance', 'overall', 'result', 'show', 'robustness', 'method', 'performance', 'compare', 'baseline', 'datum', 'augmentation', 'method', 'therefore', 'propose', 'augmented', 'observation', 'expect', 'worsen', 'base', 'performance', 'however', 'exper', 'iment', 'observe', 'barely', 'match', 'base', 'agent', 'gaeppo', 'result', 'sometimes', 'worsen', 'performance', 'variability', 'performance', 'hinder', 'widespread', 'adaptation', 'method', 'contrast', 'method', 'bae', 'show', 'consistent', 'performance', 'various', 'task', 'reduce', 'base', 'agent', 'performance', 'r', 'simple', 'statistical', 'gradientfollowing', 'algorithm', 'connectionist', 'reinforcement', 'learn', 'machine', 'learning', 'vol', 'p', 'dhariwal', 'radford', 'klimov', 'imal', 'policy', 'optimization', 'preprint', 'abbeel', 'trust', 'region', 'policy', 'optimization', 'international', 'conference', 'machine', 'learning', 'pmlr', 'p', 'abbeel', 'high', 'dimensional', 'continuous', 'control', 'use', 'generalize', 'advantage', 'estimation', 'proceeding', 'international', 'conference', 'learn', 'represen', 'tation', 'iclr', 'andrychowicz', 'raichuk', 'p', 'orsini', 'girgin', 'r', 'marini', 'l', 'geist', 'pietquin', 'michalski', 'gelly', 'bachem', 'matter', 'onpolicy', 'deep', 'actorcritic', 'method', 'largescale', 'study', 'international', 'conference', 'learn', 'represen', 'tation', 'observational', 'overﬁtting', 'reinforcement', 'learning', 'international', 'conference', 'learn', 'representation', 'c', 'vinyal', 'r', 'muno', 'bengio', 'study', 'overﬁtte', 'deep', 'reinforcement', 'learn', 'arxiv', 'preprint', 'schulman', 'quantify', 'generalization', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learning', 'pmlr', 'stooke', 'p', 'abbeel', 'reinforcement', 'learn', 'augmented', 'datum', 'advance', 'neural', 'information', 'processing', 'system', 'r', 'raileanu', 'yarat', 'r', 'fergus', 'automatic', 'datum', 'augmentation', 'generalization', 'deep', 'reinforcement', 'learn', 'arxiv', 'preprint', 'tunyasuvunakool', 'muldal', 'lillicrap', 'heess', 'tassa', 'dm', 'control', 'software', 'task', 'continuous', 'control', 'software', 'impact', 'vol', 'p', 'e', 'couman', 'python', 'module', 'physics', 'simulation', 'game', 'robotic', 'machine', 'learning', 'l', 'ilyas', 'santurkar', 'l', 'rudolph', 'madry', 'implementation', 'matter', 'deep', 'case', 'study', 'international', 'conference', 'learn', 'representation', 'dossa', 'rafﬁn', 'kanervisto', 'implementation', 'detail', 'proximal', 'policy', 'optimization', 'blog', 'track', 'online', 'available', 'r', 'sur', 'vey', 'generalisation', 'deep', 'reinforcement', 'learn', 'preprint', 'schulman', 'leverage', 'procedu', 'generation', 'benchmark', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learning', 'pmlr', 'l', 'espeholt', 'h', 'soyer', 'r', 'muno', 'ward', 'harley', 'dun', 'scalable', 'tribute', 'deeprl', 'importance', 'weight', 'actorlearner', 'architecture', 'preprint', 'c', 'ye', 'braga', 'cleanrl', 'highquality', 'singleﬁle', 'implementation', 'deep', 'reinforcement', 'learning', 'algorithm', 'peter', 'schaal', 'reinforcement', 'learning', 'motor', 'skill', 'policy', 'gradient', 'neural', 'network', 'vol', 'pp', 'rajeswaran', 'kakade', 'mordatch', 'p', 'abbeel', 'variance', 'reduction', 'policy', 'gradient', 'actiondependent', 'factorize', 'baseline', 'international', 'conference', 'learn', 'representation', 'generalization', 'reinforcement', 'learning', 'selective', 'noise', 'injection', 'information', 'bottleneck', 'advance', 'neural', 'information', 'processing', 'system', '978–13', 'state', 'representation', 'use', 'style', 'transfer', 'well', 'generalization', 'deep', 'reinforcement', 'learning', 'european', 'conference', 'machine', 'learning', 'principle', 'practice', 'knowledge', 'discovery', 'database', 'ecmlpkdd', 'performance', 'minitaur', 'leave', 'right', 'environ', 'fig', 'ment', 'method', 'show', 'well', 'similar', 'performance', 'compare', 'base', 'agent', 'gaeppo', 'datum', 'augmentation', 'baseline', 'sometimes', 'worsen', 'base', 'performance', 'relate', 'work', 'advantage', 'estimation', 'baseline', 'leverage', 'reduce', 'variance', 'policy', 'gradient', 'update', 'furthermore', 'effective', 'modiﬁcation', 'method', 'use', 'advantage', 'estimation', 'method', 'commonly', 'use', 'policy', 'optimization', 'enjoy', 'strong', 'empirical', 'success', 'especially', 'generalize', 'advantage', 'estimation', 'method', 'contrast', 'method', 'bae', 'leverage', 'datum', 'augmentation', 'incorporate', 'advantage', 'computation', 'various', 'semantically', 'similar', 'state', 'empirically', 'observe', 'method', 'compute', 'advantage', 'beneﬁcial', 'especially', 'highdimensional', 'procedural', 'generate', 'environment', 'data', 'augmentation', 'datum', 'augmentation', 'demon', 'strate', 'effective', 'efﬁcient', 'approach', 'improve', 'performance', 'method', 'propose', 'improve', 'generalization', 'include', 'style', 'transfer', 'depend', 'augmented', 'observation', 'use', 'method', 'different', 'example', 'drac', 'contrast', 'method', 'method', 'incorporate', 'data', 'augmentation', 'advantage', 'estimation', 'show', 'well', 'empirical', 'performance', 'compare', 'method', 'rad', 'drac', 'vi', 'conclusion', 'paper', 'propose', 'datum', 'augmentationbase', 'tage', 'estimation', 'method', 'policy', 'optimization', 'bootstrap', 'advantage', 'estimation', 'bae', 'method', 'replace', 'gae', 'method', 'policy', 'gradientbase', 'algorithm', 'demonstrate', 'effectiveness', 'method', 'ppo', 'algorithm', 'furthermore', 'evaluate', 'method', 'imagebase', 'observation', 'space', 'discrete', 'action', 'space', 'vectorbase', 'observation', 'continuous', 'action', 'space', 'procgen', 'deepmind', 'control', 'pybullet', 'bae', 'method', 'show', 'well', 'performance', 'various', 'environment', 'setup', 'furthermore', 'method', 'perform', 'well', 'exist', 'datum', 'augmentation', 'technique', 'drac', 'reference', 'r', 'mcallester', 'mansour', 'policy', 'gradi', 'ent', 'method', 'reinforcement', 'learning', 'function', 'approximation', 'advance', 'neural', 'information', 'processing', 'system', 'vol']"
Planning for Sample Efficient Imitation Learning,"[{'href': 'http://arxiv.org/abs/2210.09598v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.09598v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-10-18 05:19:26,"2
2
0
2

t
c
O
3
2

]
h
p
-
t
n
a
u
q
[

1
v
3
5
7
2
1
.
0
1
2
2
:
v
i
X
r
a

Google’s 2019 “Quantum Supremacy” Claims:
Data, Documentation, and Discussion

Gil Kalai, Yosef Rinott & Tomer Shoham

October 25, 2022

Abstract

In October 2019, Nature published a paper [3] describing an exper-
imental work that took place at Google. The paper claims to demon-
strate quantum (computational) supremacy on a 53-qubit quantum
computer. Since September 2019 the authors have been involved in
a long-term project to study various statistical aspects of the Google
experiment.
In particular, we have been trying to gather the rele-
vant data and information, to reconstruct and verify those parts of
the Google 2019 supremacy experiments that are based on classical
computations (unless they require too heavy computation), and to
put the data under statistical analysis. We have now (August 2022)
concluded the part relating to the gathering of data and information
needed for our study of the 2019 Google experiment, and this docu-
ment describes the available data and information for the Google 2019
experiment and some of our results and plans.

1

Introduction

The 2019 paper “Quantum supremacy using a programmable superconduct-
ing processor” [3] claimed that Google’s Sycamore processor performed a
certain computation in about 200 seconds, while a state-of-the-art classical
supercomputer would take, according to Google’s team estimates, approxi-
mately 10,000 years to perform the same computation. Google’s Sycamore
quantum computer performed a sampling task ; that is, it generates ran-
dom bitstrings, of length 53, with considerable noise, from a certain discrete

1

 
 
 
 
 
 
probability distribution supported on all such 253 bitstrings. The speciﬁc
Google’s sampling task is referred to as random circuit sampling (RCS, for
short). Google’s announcement of quantum supremacy was compared by
[1]) to landmark technological achievements such
various writers (See, e.g.
as the Wright brothers’ invention of a motor-operated airplane, the Sputnik,
and the landing on the moon, as well as to landmark scientiﬁc achievement
such as Fermi’s demonstration of a nuclear chain reaction, the discovery of
the Higgs boson, and the LIGO detection of gravitational waves.

In 2020 a team from the University of Science and Technology of China
(USTC) claimed [30] that the sampling task computed by their photonic
Jiuzhang quantum computer, would take 2.5 billion years to perform on a
classical supercomputer. USTC’s quantum computers also required about
200 seconds for their tasks. This task is referred to as Gaussian boson sam-
In 2021, another team from USTC repeated the
pling (GBS, for short).
Google 2019 RCS experiment with a superconducting processor Zuchongzhi
of 60 qubits and depth 24 [29, 32], and claimed to achieve an even stronger
form of quantum advantage compared to the Google 2019 experiment.

The Google experiment represented a very large leap in various aspects
of the human ability to control noisy quantum systems. For example the
Google AI team previously reported an experiment with nine qubits [17].
This leap is especially impressive in terms of the dimensions of the Hilbert
space representing a state of the computer. (From dimension 100–500 in [17]
to dimension 1016 in [3].)

As for Google’s “quantum supremacy” claims, progress by several groups
[19, 18, 31, 11, 12, 20, 8] (and more) exhibits classical algorithms which are
ten orders of magnitude faster than those used in the Google paper. This
was achieved, for example, by Pan, Chen, and Zhang [18] in 2021. Our study
concentrates on other aspects of the Google experiment.

As we already mentioned, the Google announcement was regarded as
a major scientiﬁc and technological event. On its own it gave some evi-
dence that the “strong Church–Turing thesis” had been violated and it was
described as an ironclad refutation of claims by some scientists (including
Kalai) that quantum computation is not possible. Of no less importance
is that quantum supremacy was considered as a major intermediate step
towards exhibiting experimentally quantum error-correction codes needed
for building larger quantum computers. The announcement of quantum
supremacy stirred a great deal of enthusiasm among scientists and in the
general public, and garnered signiﬁcant media attention. It had substantial

2

Figure 1: The price of bitcoin in USD in a period of 4 weeks around
9/23/2019. Source: CoinDesk

impact; for example, following the media attention surrounding the leaking
of the supremacy claims around September 22, 2019, the value of bitcoin
(and other digital currencies) sharply dropped by more than 10% and it is a
reasonable possibility that given the potential eﬀects of quantum computers
on the safety of digital currencies, the quantum supremacy claims caused
this drop.

Putting the Google supremacy claims under scrutiny

The Google paper appeared in October 2019 and a month earlier it was
brieﬂy posted on a NASA server and became publicly available. Since the
announcement of the quantum supremacy claim, the ﬁrst-named author (and
other researchers) raised various concerns about some aspects of the claims.
A few months later the authors initiated what has become a long-term project
to study various statistical aspects of the Google experiment. In particular,
we have been trying to gather the relevant data and information and to
reconstruct and verify those parts of the Google 2019 supremacy experiments
that are based on classical computations. (Unless they require too heavy

3

computation; we carried some heavy computation on the cloud for which
we put a cap of 2000 dollars on our spending.) We also performed several
“sanity tests” of the experiment.

The structure of this paper

In Section 2 we provide a brief background on the Google 2019 experiment,
describe the various types of circuits used in the experiment and the chronol-
ogy of the various Sycamore experiments as reported by the Google team. In
Section 3 we describe the nature of the calibration process. In Section 4 we
describe the requested data from the Google team, the data that was provided
between October, 2019 and June 2022, and some other details related to the
Google experiment and other NISQ supremacy experiments. In Section 5 we
raise two proposals for future experiments. In Section 6 we discuss what we
regard as the main questions in the evaluation of the Google 2019 experiment
and list some conﬁrmations, refutations, concerns, and weaknesses, and in
Section 7 we brieﬂy discuss where we are now in our study.

2 Google’s 2019 quantum supremacy claim

2.1 A brief background

In this draft we will assume knowledge of the Google 2019 experiment,
Google’s noise model, Google’s FXEB linear cross entropy ﬁdelity estima-
tor, and Google’s formula (77) in [4] for predicting the ﬁdelity of a circuit
from the ﬁdelity of its components. We will give here a brief summary of
these topics.

The Google 2019 experiment is based on the building of a quantum com-
puter (circuit) with n superconducting qubits, that perform m rounds of
computation. The computation is carried out by a 1-qubit and 2-qubit gates.
At the end of the computation the qubits are measured, leading to a string
of zeroes and ones of length n. The ultimate experiment was for n = 53
and m = 20. It involved 1113 1-qubit gates and 430 2-qubit gates. For that
experiment the Google team produced a sample of three million 0-1 vectors
of length 53.

Every circuit C with n qubits describes a probability distribution PC(x)
(In fact, it describes a 2n-dimensional vector

for 0-1 vectors of length n.

4

of complex amplitudes; to every 0-1 vector x, there is an associated ampli-
tude z(x) and PC(x) = |z(x)|2.) The quantum computer enables to sample
according to the probability distribution PC(x) with a considerable amount
of noise. When n and m are not too large classical simulations enable to
compute the amplitudes themselves (and hence the probabilities PC(x)).
Google’s supremacy claims are based on the fact that these classical simula-
tions quickly become infeasible as n and m grow.

The Google basic noise model for the noisy samples their Sycamore device

actually produces is

NC(x) = φPC + (1 − φ)2−n,

(1)

where φ is the ﬁdelity, a parameter that roughly describes the quality of the
sample.
(The ﬁdelity has a precise meaning in terms of the actual noisy
quantum process carried out by the Google Sycamore device.)

Based on their noise model (and the fact that the distribution PC is
an instance of a Porter–Thomas distribution) the Google paper describes a
statistic called the linear cross entropy estimator (and denoted by FXEB.)
Once the quantum computer produces a sequence ˜x of N samples ˜x =
(˜x(1), ˜x(2), . . . , ˜x(N )), the following “linear cross entropy” estimator FXEB for
the ﬁdelity is computed

FXEB(˜x) =

1
N

N
(cid:88)

i=1

2nPC(˜x(i)) − 1.

(2)

Computing FXEB requires knowledge of PC(x) for sampled bitstrings.

The Google supremacy claim is based also on the following a priori pre-
diction for the ﬁdelity of a circuit based on the probabilities of error for the
individual components.

ˆφ =

(cid:89)

g∈G1

(1 − eg)

(1 − eg)

(cid:89)

g∈G2

(cid:89)

q∈Q

(1 − eq).

(3)

Here G1 is the set of 1-gates (gates operating on a single qubit), G2 is the
set of 2-gates (gates operating on two qubits), and Q is the set of qubits. For
a gate g, the term eg in the formula refers to the probability of an error (1
minus the ﬁdelity) of the individual gate g. For a qubit q, eq is the probability
of a read-out error when we measure the qubit q.

The Google supremacy paper [3] made two crucial claims regarding the

ultimate 53-qubit samples.

5

A) The ﬁdelity φ of their sample is above 1/1000.

B) Producing a sample with similar ﬁdelity would require 10,000 years on

a supercomputer.

For claim A) regarding the value of φ, the argument relies on an extrapo-
lation argument that has two ingredients. One ingredient is a few hundred
experiments in the classically tractable regime: the regime where the prob-
ability distribution PC can be computed by a classical computer and the
performance of the quantum computer can be tested directly. The other in-
gredient is the theoretical formula (3) for predicting the ﬁdelity. According
to the paper, the ﬁdelity of entire circuits closely agrees with the prediction
of formula (3) (Formula (77) in [4]) with a deviation below 10–20 percent.
There are around 200 reported experiments in the classically tractable regime
including ones carried out on simpliﬁed circuits (which are easier to simu-
late on classical computers). These experiments support the claim that the
prediction given by Formula (77) for the ﬁdelity is indeed very robust and
applies to the 53-qubit circuit in the supremacy regime.

For claim B) regarding the classical diﬃculty, the Google team mainly re-
lies on extrapolation from the running time of a speciﬁc algorithm they used.
They also rely on the computational complexity support for the assertion
that the task at hand is asymptotically diﬃcult.

Remark: The Google team proposed a pretty good approximation for

Formula (3) based on averaged ﬁdelities.

ˆˆφ = (1 − 0.0016)|G1|(1 − 0.0062)|G2|(1 − 0.038)n.

(4)

2.2 The types of circuits of the Google experiment

The circuits used in the 2019 Google supremacy experiment had the fol-
lowing structure. The qubits were arranged on a planar grid, so a single
qubit was identiﬁed via two coordinates, like qubit (3, 3). The circuits had
two types of layers: one type of layer consists of 2-gates acting on pairs of
(neighboring) qubits. After each such layer of two gates there was another
layer of randomly chosen 1-gates acting on every qubit. The layers of 1-gates
consist of the ”programmable” ingredient in the experiment and they change
from circuit to circuit. The layers of 2-gates are ﬁxed throughout the ex-
periment according to a certain pattern. The pattern EFGH was used in

6

all experiments conducted between February to May 2019 and a new pat-
tern ABCDCDAB was used in June 2019 to produce the samples of the
“supremacy” circuit, namely circuits which would require a huge classical
computation. Each letter like E corresponds to a ﬁxed set of 2-gates act-
ing in parallel on the qubits (with the convention that for circuits with a
smaller number of qubits we regard only 2-gates that involve the qubits in
the circuit). So Pattern EFGH means that we ﬁrst apply a layer of random
1-gates on all qubits, then apply 2-gates according to E, next apply another
layer of random 1-gates on all qubits, then apply 2-gates according to F and
continue (in a periodic manner). The new pattern ABCDCDAB is based
on new types of layers for the 2-gates and on a period of length eight.

The depth m of a circuit refers to the number of layers of 2-qubits. So, for
example, the 2-gates layers of a circuit with pattern EFGH of depth m = 14
are E, F, G, H, E, F, G, H, E, F, G, H, E, F.

The six types of circuits

The Google 2019 experiment relies on six types of circuits. The ﬁrst three
types are:

a) The full circuits of pattern EFGH,

b) The elided circuits of pattern EFGH,

c) The patch circuits of pattern EFGH.

Types b) and c) are simpliﬁed forms of type a) for which there are quick algo-
rithms to compute the amplitudes for all values of n up to 53. In particular,
patch circuits consist of two separate circuits on two disjoint sets of qubits.
The Google paper [3] is based on running the quantum computer on a few
hundred circuits of type a) b) and c) and computing the ﬁdelity estimator
based on the amplitudes.

There was also a preliminary stage of calibration that, based on exper-
iments on 1-qubit and 2-qubit circuits, determined the precise adjustments
for the experimental 2-gates. Those adjustments are the same for all circuits
of type a), b) and c). See Section 3.

As far as we understand, the initial plan of the Google team to demon-
strate quantum supremacy was to compute the empirical ﬁdelities for circuits
of type b) and c) for a number of qubits between 12 and 53 and for circuits

7

of type a) for a number of qubits up to n = 43 and to use this information
to estimate the ﬁdelity for circuits of type a) with 53 qubits and depths (m)
between 12 and 20.

However, in May 2019, the Google team discovered an eﬃcient classical
algorithm for the circuits of type a). (Because of this discovery, the full cir-
cuits with pattern EFGH are also referred to as “veriﬁable full circuits.”)
The new algorithm discovered by the Google team is related to tensor net-
works methods which later led to important discoveries in this direction.1
Using this algorithm, the Google team computed the amplitudes for the ex-
perimental bitstrings for type a) circuits with n = 53 m = 14.

Subsequently, for the purpose of demonstrating quantum supremacy, the
Google team moved in May 2019 to a diﬀerent architecture based on a new
pattern ABCDCDAB which is harder to simulate by classical computers.
This led to the following three new types of circuits.

d) The full circuits with pattern ABCDCDAB. (These circuits are also

referred to as “supremacy full circuits.”)

e) The elided circuits of pattern ABCDCDAB,

f) The patch circuits of pattern ABCDCDAB.

For this architecture the Google team produced samples only with n = 53
and depths 12, 14, 16, 18 and 20. The simulators available to the Google team
were not powerful enough for computing the amplitudes and for checking the
ﬁdelity for circuits of type (d).

The circuits with pattern ABCDCDAB required to run the calibration
process again on 1-qubit and 2-qubit circuits based on the new architecture
and this has led to new adjustments for the 2-gates. Circuits of the new
pattern ABCDCDAB (and the calibration process) were not tested for less
than 53 qubits.

2.3 A chronology of the Google Sycamore experiments

Prior to the development of the Sycamore quantum computer, in 2018, the
Google team had attempted to develop a 72-qubit chip called “Bristlecone,”

1The Google team pointed out that they also implemented a related tensor network

method in 2017 [6].

8

but due to diﬃculties the team later proceeded to the development of the
Sycamore 54-qubit computer (with 53 eﬀective qubits).

Here is a list of experiments for the Sycamore quantum computer in-
cluding early experiments based on earlier calibration methods. For every
experiment fresh new random circuits were generated. (Namely, the improve-
ments in the calibration procedures in a certain experiment were not based
on improving the ﬁdelity for circuits that were used in an earlier experiment.)
Each experiment took 1-2 days to perform. Each of the ﬁrst six ex-
periments is based on an improved method for calibration. There were six
experiments for circuits of pattern EFGH and one (the last) experiment is
based on the new architecture with pattern ABCDCDAB.

2.3.1 Earlier experiments

1. Date: 02/08/2019

Full circuits with pattern EFGH; numbers of qubits: 2, 3, 4, . . . , 49

2. Date: 02/13/2019

Full circuits with pattern EFGH; numbers of qubits: 2, 3, 4, . . . , 49

3. Date: 03/14/2019

Full circuits with pattern EFGH; numbers of qubits: 2, 3, 4, . . . , 51

4. Date: 03/22/2019

Full circuits with pattern EFGH; numbers of qubits: 33, 34, . . . , 51

5. Date: 03/27/2019

Full circuits with pattern EFGH; numbers of qubits: 10, 20, 33, 34, 35, 36, 37, 38, 51

2.3.2 The experiments on which the Google paper is based

6. Date: 04/22/2019

Patch, elided, and full circuits with pattern EFGH; Numbers of qubits:
12, 14, 16, . . . , 36, 38, 39, 40, . . . , 50, 51, depth m = 14

A few days later the Sycamore quantum computer produced the 53-
qubit samples for patch, elided, and full circuits with the EFGH pat-
tern also with depth m = 14.

9

7. Date: 06/13/2019

Numbers of qubits: 53, depth m = 12, 14, 16, 18, 20

Type of circuits: patch, elided, full with pattern ABCDCDAB.

3 The nature of the calibration process

The Sycamore quantum computer has systematic deviations from the ideal
circuit it describes, and the calibration process accounts for small systematic
errors in the experimental circuits compared to the random circuits they
represent. The calibration process is crucial since the combined eﬀect of
such systematic errors can slash the ﬁdelity to zero and it is important to
account for them.

By the calibration process we refer in this paper to a method which,
based on multiple runs of 1-qubit and 2-qubit quantum circuits, adjusted the
deﬁnition of the 2-gates of the experimental circuits so that certain systematic
forms of noise will be greatly reduced. This calibration adjustment was
carried out simultaneously for all the experiments for patch, elided, and
veriﬁable full circuits.

It is easy to identify from the Python program how the random circuits
(We could also use the QSIM ﬁles for
were modiﬁed by the calibration.
that purpose.) For example, consider a single 2-gate acting on two qubits
A = (3, 3) and B = (3, 4).

In the ideal description of the circuit, this 2-gate is described in the

Python program for the circuit as follows:

cirq.Moment(

cirq.FSimGate(theta=1.57079632679, phi=0.52359877559).
on( cirq.GridQubit(3, 3), cirq.GridQubit(3, 4)),

In this 2-gate (that is sometimes referred to as the “standard 2-gate”) θ =
π/2, and φ = π/6. We will now show the part of the program that describes
2-gates modiﬁcations for this speciﬁc gate. The calibration consists of adding
to the description of the circuits, two ﬁxed rotations on each of the 2 qubits
involved in this particular 2-gate.

The two added single qubit rotations are

cirq.Rz (np.pi * 2.5333591271878086). on (cirq.GridQubit (3, 3)),
cirq.Rz (np.pi * -2.4748096263683066). on (cirq.GridQubit (3, 4)),

10

and these two added single qubit rotations are the same for every appearance
of the 2-gate involving A and B. The values that appear here in the single
qubit rotations, as well as the adjustments for the deﬁnition of 2-gates, were
computed based on many experiments for 1-qubit and 2-qubit circuits for
these two qubits (3,3) and (3,4). (We neither have the precise algorithm for
this computation nor the data with the outcomes of the 1-qubit and 2-qubit
experiments.)

In addition, the 2-gate acting on qubits A and B is modiﬁed to

cirq.Moment(

cirq.FSimGate(theta=1.2947043217999283, phi=0.4859467238431821).
on( cirq.GridQubit(3, 3), cirq.GridQubit(3, 4)),

The calibrated values θ = 1.2947043217999283 and φ = 0.4859467238431821

are the same for all 2-gates acting on these two qubits in all circuits. (Some-
times the calibrated 2-gate is referred to as “native” 2-gate.) These values
for θ and φ replace the values for the “standard” gate in the original random
circuit θ = π/2, and φ = π/6.

Remarks:
1. We note that the adjustments for 2-gate involving qubits A and B
depend (only) on multiple runs of 1-qubit and 2-qubit quantum circuits in-
volving these two qubits.

2. We also note that there might have been additional actions to account
for the fact that certain components of the Sycamore chip degrade over time.
3. In the December 2020 Internet discussion [24] Sergio Boixo from the
Google team referred to Figure S30 in [4] and asserted that in the speciﬁc case
of the 2019 experiment using “Sycamore” standard gates instead of native
gates halves the ﬁdelity. However, this claim referred only to the two-gate
adjustments and not to the one-gate adjustments, and moving to the original
circuit with standard gates slashes the ﬁdelity to zero. (The eﬀect of the two-
gate adjustments represents the worst-case situation for n = 53, m = 20, and
it is smaller for smaller circuits.)

Sergio Boixo later clariﬁed that since 2020 (for later experiments but not
for the 2019 supremacy experiment) the Google team developed a method
to carry out the 1-qubit calibrations using physical control of the device
rather than using a modiﬁcation of the deﬁnition of the circuit. (This later
development is not relevant to the 2019 experiment and we did not study it.)

11

4 Data, documentations, and discussions

4.1 The supplementary data

The Google paper was leaked around September 23, 2019 and the paper was
published by “Nature” a month later on October 23, 2019. The data for the
Google 2019 experiment can be found in [5]. The Google team uploaded data
to the server in ﬁve dates as follows:

October 22/23, 2019, publication date

1. Files with 500,000 bitstrings for many of the experimental circuits with
n < 53 were uploaded and for n = 53 samples consists of a few million
bitstrings were uploaded. The bitstrings for several numbers of qubits n
were missing on this date (e.g. missing for n=16, 32, 34, 38, 39, 42, . . . , 51).
In addition, the bitstrings for the patch circuits were missing.
2. QSIM ﬁles with descriptions for the corresponding experimental circuits,
as well as Python ﬁles for computing their amplitudes.

January 23, 2020

Missing bitstrings for full and elided circuits were uploaded. Amplitude ﬁles
for all circuits (not including patch circuits) except (roughly) 200 circuits
were uploaded. Some additional data including information on noise for
individual components of the Sycamore quantum computer was uploaded.

January 22, 2021 & May 18, 2021

Amplitude ﬁles for all circuits (not including patch circuits) that were com-
puted by the Google team were uploaded except eight circuits for which
the amplitudes were computed in J¨ulich (J¨ulich Research Center, Germany).
Readout error data was uploaded.

June 13, 2022

The J¨ulich amplitudes were uploaded. Bitstring ﬁles (100,000 bitstrings per
circuit) and QSIM and Python ﬁles for the patch circuits with n < 53. (These
ﬁles are still missing for n = 53.)

12

4.2 Requests for data

4.2.1 Early requests from October 2019

Here are Kalai’s requests from the Google team from early October 2019.

(a) Larger samples from the quantum circuits, comparison of the empirical

distribution with the model.

(b) The bitstrings produced by the quantum computer and a description

of the circuits

(c) The amplitudes that were computed for the samples for each circuit

(d) The full list of amplitudes. (Here we refer to the 2n amplitudes for all

bitstrings and not only those in the sample.)

(e) Timetable for the experiments and calibrations

(f) (November 11, 2019.) The values of the individual ﬁdelities eq and eg

for every qubit q and gate g used in Formula 3.

4.2.2 Subsequent requests

(g) Readout errors information. We discussed this matter in January 2021
and we received useful data regarding the readout errors shortly after-
wards. In this case, we received the required data quickly, in a very
satisfactory way.

(h) Amplitudes for the veriﬁable experiments.

(September 2021) As we
already mentioned, the Google team developed useful algorithms and
programs to compute the amplitudes for the experiments of full cir-
cuits of pattern EFGH. (This was the reason for moving to a diﬀerent
pattern for the “supremacy experiment.”) They used these programs
for 10 53-qubit depth-14 circuits (this required several hours for each
circuit on the Google supercomputer). For a useful reliability-control
of the experiment we proposed that the amplitudes be computed for
the remaining (over 100) full circuits, and at least for 2 speciﬁc ones.

(i) The optimization programs to move from data on 1-qubit and 2-qubit
circuits to 1-gate and 2-gate corrections in the deﬁnition of the cir-
cuits and the raw data gathered from 1-qubit and 2-qubit circuits. We
requested it in January 2022.

13

4.3 Data provided by the Google team

1. Google’s researchers were under press embargo and did not respond to

most issues before the paper appeared.

2. As we already mentioned, on October 23, 2019 the paper was pub-
lished in Nature, the supplementary raw data contains (b) (bitstrings,
description of the circuits in a form of a Python program to compute
the amplitudes) for many of the circuits used in the experiments and
on January 2020 bitstrings for other circuits were added. We requested
the data for the “patch circuits” in June 2021 and again in December
2021. The data for the patch circuits was uploaded on the server in
June 2022 except for the 53-qubit depth-14 EFGH patch circuits. (As
we mentioned, the data has only 100K samples per every patch circuit.)

3. The Google team promised to supply the amplitudes (that they com-
puted as part of the experiment) for their samples (c) . They uploaded
amplitudes for many circuits in January 2020, and for additional two
hundred circuits in May 2021. Uploading the amplitudes that were
computed for 8 circuits in an external facility (J¨ulich) required approval
of the J¨ulich team and they were uploaded in June 2022.

4. Regarding the request (d) for the full list of amplitudes, the Google
team informed us that the full lists of amplitudes were discarded. (In-
deed, for large values of n this is a huge amount of data.) Moreover, for
circuits with pattern EFGH the Google team developed (around May
2019) algorithms for computing the amplitudes for the sample that did
not require computing all the 2n amplitudes and these algorithms were
used in some of these cases.

5. Regarding larger samples (a). Until August 2022, the Google team did
not supply any further samples produced by the quantum computer.
As for comparison between the empirical distribution and the noise
model, the Google team referred (already in September 2019) for such
a comparison to an earlier experiment (on 9 qubits) [17].

6. Regarding timetable and some details for the experiments and calibra-
tions (request (e)), in May 2022 we received a brief timetable and some
useful details regarding the ﬁnal experiments and ﬁve earlier ones. See
Section 2.3.

14

7. Regarding item (h), the Google team told us that they would not per-
form these further computations as they require a lot of human and
computational eﬀorts.

However, using new algorithms, amplitudes for all the veriﬁable exper-
iments were computed by Kalachev, Panteleev, and Yung [11] and the
results give a strong support to Google’s 2019 claims and predictions
regarding the linear cross entropy of their samples.

8. Regarding item (i), the Google team informed us that they could not
share the full proprietary calibration system (that required many years
of development) needed to move from data on 1-qubit and 2-qubit to
1-gate and 2-gate corrections in the deﬁnition of the circuits. They
pointed out that the main innovation of the calibration program was
made public.

9. In July 2022, Adam Zalcman and Sergio Boixo gave us a useful Python

program for splitting the patch circuits into the two patches.

10. Regarding item (f) the individual gate- and qubit- ﬁdelities. We asked
for it several times in 2019 and early 2020 and again in September 2022.
(It turned out recently that this data is crucial for the study of patch
circuits.)

4.4 The discussion with the Google team

Since October 2019 through August 2022, we had good discussions (initiated
by Scott Aaronson), mainly by email, with the Google team and especially
with John Martinis and Sergio Boixo, on various aspects of their experiment
and on some of our ﬁndings. Overall, the Google team welcomed us (and
others) in putting their experiment under careful scrutiny, even though they
had been aware since 2019 of the ﬁrst author’s concerns [10] about the reli-
ability of the Google 2019 Sycamore paper. The discussions were easy going
and in good atmosphere. A video discussion between Boixo, Kalai, Rinott
and Shoham in October 2021 was especially fruitful. The basic methodol-
ogy of trying to pass the data from the Google experiment through various
“sanity tests” was overall agreed upon by the Google team although at times
we had diﬀerent interpretations of speciﬁc ﬁndings. There was a single issue
regarding our concerns about the calibration process in connection with the

15

2019 Google video [9], discussed in late 2021, that led to a somewhat more
tense exchange, and in particular where John Martinis criticized attempts to
pass judgment on the Google 2019 experiment based on a short video meant
for general audience, rather than on the paper itself. Still, also in this case,
we had a useful discussion which ultimately shed some light on aspects of
the Google 2019 experiment.

Overall, we were not shy to ask for data and information, and in a few
cases the Google team was not shy to decline our requests. Most of our
requests for data and information were met, although arguably, most of our
requests should have been part of the supplementary material of the Google
paper to start with.

Since December 2020 until November 2021, we conducted in parallel a
useful discussion with Chao-Yang Lu and members of the Gaussian boson
sampling team from USTC along with members of the Google team and
other researchers regarding the issue of spooﬁng and k-point correlations of
the Gaussian boson sampling experiment. (This discussion was also initiated
by Scott Aaronson.) We also had a brief email correspondence with the
USTC team that replicated the Google 2019 experiment.

4.5 The discrepancy with the outcomes of J¨ulich Re-

search Center team

One little mystery that was settled following our discussions with the Google
team involved the data from the J¨ulich team. The amplitudes for 8 large cir-
cuits with 39, 42 and 43 qubits, were computed by researchers from J¨ulich’s
research center using their own powerful simulators and (later, after the pub-
lication of the paper) when the Google team checked the computation with
their own simulators, in some cases the amplitudes were diﬀerent, and in one
case also the estimated ﬁdelity was lower than expected. Initially, the thought
was that the (small) diﬀerences between the outcomes were caused by nu-
merical diﬀerences between the simulators. However, this was not the reason.
It turned out that the problem was not a numerical diﬀerence between the
simulators but rather using a (slightly) diﬀerent calibration method. (See
Section 3.) In the simulations coming from J¨ulich the researchers used cir-
cuits and measurements corresponding to the correct experiment, but the
Google team slightly improved the parameters of the circuits a few days af-
ter running the experiment, and the J¨ulich team used an older version. This

16

explains the discrepancy in the amplitudes and why the ﬁdelity estimated
with the J¨ulich simulation was lower for one of the circuits. In other words,
the amplitudes were computed for precisely the same experiment, but the
calibration, namely the formalization of the experiment as a quantum circuit
(the description of the “native” gates) was slightly diﬀerent.

Remark (October, 2021): It turned out that for seven out of the eight
J¨ulich circuits the new calibration was applied and only for one circuit the
J¨ulich team had used the earlier calibration.

4.6 Data from other sources

It could have been valuable to perform some of our statistical analysis on data
from other quantum computers and we try to obtain similar data (descrip-
tion of calibrated circuits, bitstrings and amplitude ﬁles) from other quantum
computers especially those from IBM. The experiment closest to the Google
one (that we know about) that was carried out on an IBM quantum com-
puters is by Kim et al. [16]. We were told, however, that in a boarder sense,
many of IBM’s benchmarks are based on random circuits of some sort.

Also by now, Google, NASA, and various other groups have powerful sim-
ulators that also allow to introduce noise. This provides further opportunity
for our statistical analysis, which, so far, we have not used.

4.7 The USTC replication

In June 2021 scientists from USTC [29] described a close replication of
the Google 2019 experiment with 56-qubit depth 20 quantum circuits on
their Zuchongzhi quantum computer. Later in September 2021, they de-
scribed an improved experiment with 60-qubit depth-24 circuits [32]. The
team from USTC shared the description of the circuits (in Matlab) for each
n = 15, 18, 21, 24, ..., 54, 56, and it seems that they sampled (or shared) less
bitstrings than Google did; for example, for n = 15 they sampled 200K bit-
strings, whereas the Google team sampled 500K. Carefully studying the data
from these experiments (and perhaps asking for additional data) would be
an interesting direction for further research.

17

4.8 Discussions over scientiﬁc blogs

There were useful discussions over a few scientiﬁc blogs regarding the Google
2019 experiment, and especially over Aaronson’s blog “Shtetl Optimized”
(SO) and also over Kalai’s blog “Combinatorics and More.” Reference [34]
contains links to useful blog discussion mainly at the time of the Google
supremacy announcements. For example, in a discussion over SO (December
2020) a commentator “Till” asserted [27] that the calibration process actually
adjusted the deﬁnition of the circuit to the device. Namely, that the random
circuits were generated with “standard gates” but the deﬁnition of 2-gates
was modiﬁed in the calibration process (in the same way for all circuits). (See
Section 3.) This was new for us and for several others who thought that the
calibration process is a physical process performed on the Sycamore device.
Aaronson’s own conclusion of the discussion was: “So, my summary would
be that yes, there’s a calibration phase, and the 2-qubit gates used depend
on the outcome of that phase, but there’s still a clear separation enforced
between the calibration phase and the actual running of the QC.”

5 Two proposals for future experiments

5.1 A proposal for blind experiments

In [22] we proposed the following protocol for an evaluation of Google’s ex-
periment (here we describe a small variant). First, Google will share the
parameters of their calibration. Next, independent scientists will prepare
several programs (circuits) for Sycamore to be run with about n qubits, for
which computing the sampling probabilities should be a task that takes sev-
eral months (on a classical computer). These programs will be sent to Google
for implementation. Google will send back the implemented programs and
large samples that they produce in a short time, which is assumed to pre-
clude computation of the relevant amplitudes of the calibrated circuits. Using
classical computers the scientists will take their time and compute the set of
amplitudes for each calibrated circuit. They will then evaluate the relation
between those amplitudes and the samples they received. Such a protocol is
likely to be relevant to other quantum supremacy demonstrations which are
being pursued very actively these days. Overall the Google team welcomed
the idea of conducting blind experiments in the future and agreed to our
speciﬁc proposed protocol.

18

Remark: Over an Internet discussion (on Aaronson’s blog, Feb. 2020)
Craig Gidney (a member of the Google team) [25] asserted that various ex-
periments for the early pattern that were initially considered computationally
hard, turned out to be easier than expected and they served and could fur-
ther serve as sort of blind tests for the 2019 experiment. The Google team
made a similar comment in our email correspondence in January 2021: “We
did publish a lot of data that no-one has been able to analyze yet. I hope
that eventually some of this data will be analyzed, which will be an inter-
esting conﬁrmation. Analyzing 39 and 40 qubit has actually become easy in
the meantime.” These remarks motivated our request (h) in Section 4.2. As
we mentioned, the vast progress in simulation techniques has made it pos-
sible to examine the ﬁdelity estimators and they were veriﬁed by Kalachev,
Panteleev, and Yung, in [11].

5.2 Testing calibration strategies by other groups on

Google’s Sycamore data

As a follow up to our request for the calibration programs (item (i) in Section
4.2) that could not be met, we raised the possibility of sharing raw data
gathered from 1-qubit and 2-qubit circuits which formed the input for the
calibration programs needed to describe the “native gates”. This will give an
opportunity to other groups to test their own calibration methods (without
revealing Google’s full proprietary calibration system). The Google team
asserted that this might be possible in principle, but it will require some
considerable eﬀorts, and it is not clear if the 2019 data was kept. It might be
better and easier to implement this proposal on more recent or even future
Sycamore experiments. Given the central place of the calibration stage in
NISQ experiments this direction could be valuable in its own right.

6 On the evaluation of the Google 2019 ex-

periment

In this section we discuss the overall evaluation of the Google 2019 exper-
iment. We start with ﬁve central questions regarding the experiment and
continue with a list of conﬁrmations, refutations, concerns, and weaknesses
of the experiment. Some issues discussed in this section are not directly re-

19

lated to the data and information gathering that we discussed in the previous
sections, and we also refer to important works by other groups.

6.1 Five central questions

As we already mentioned, several ingredients of the Google Sycamore experi-
ment represent major progress in human ability to control quantum systems.
When we bring the Sycamore 2019 to test, there are some central problems
that come up:

a) Were the sampling tasks achieved as claimed?

In our opinion, the ﬁndings of our paper [22] show that the answer is negative.
A “sampling task” in the traditional sense, namely reaching (approximately)
a sample from some known-in-advanced distribution (or even known-after-
the-fact distribution) was not achieved in the 2019 experiment. The empirical
distribution is quite diﬀerent from the Google basic noise model (1) and mov-
ing to our more detailed noise model gives only a small improvement. The
Google team disagrees with our opinion and notes that they write explicitely
in the supplement [4] (around Equation (24)) that they do not necessarily
assume the basic noise model (1).

b) Are the statistical tools for estimating the ﬁdelity satisfactory?

Our study in [22] shows that given the Google noise model, the FXEB-
estimator for the ﬁdelity is quite good. (We oﬀer some improvements mainly
when the number of qubits is small.)
It is an interesting question if this
estimator measures ﬁdelity when we are far away from the Google noise
model, see [8].

We note that it might be legitimate to base quantum supremacy claims
on the hardness to sample with a high value of FXEB, without referring to
the question of whether a speciﬁc sampling task was achieved or if FXEB
genuinely estimates the ﬁdelity, see [2, 8].

c) Are the claims regarding the estimated ﬁdelity of the samples valid?

Here, for example, the Google team claimed to achieve for random circuit
sampling (full circuits) with m = 14, for n = 12 with estimated (FXEB) ﬁ-
delity 0.3694, for n = 22 with estimated ﬁdelity 0.165, n = 32 with estimated

20

ﬁdelity 0.071. Even leaving aside larger depth and larger numbers of qubits,
the question is to what extent we can regard this achievement as solid. We
note that the speciﬁc computations, regarding the (FXEB) value of the sam-
ples given the description of the (calibrated) circuits were veriﬁed, and the
remaining question is about the reliability of the overall claim.

d) Are the claims regarding the predictive power of the a-priori ﬁdelity

estimations valid? (Here we refer to Formula (3).)

Here, for example, the a-priori prediction based on Formula (3) (Formula
(77) in [4]) for m = 14 and n = 12, 22, 32 are 0.386, 0.1554, 0.062 respec-
tively. Question d) is whether we have solid evidence for the claim that the
simple formula (77) (and the statistical independence assumptions allowing
it) provides accurate prediction of the FXEB estimator. Here, the speciﬁc
computations were not checked (the individual ﬁdelities were not shared, see
Section 4 item (f)), but they are supported by our approximate computa-
tions based on averaged values of the ﬁdelities. As in the previous item,
the remaining question is about the reliability of Google’s overall claim and
here the excellent prediction power of the a priori ﬁdelity estimation raised
concerns about it (item 10 in Section 6.2, below).

e) Are the claims regarding the classical diﬃculty of the sampling task

correct?

Regarding Google’s “quantum supremacy” claims, a very short summary
is that by now classical algorithms are ten orders of magnitude faster than
those used in the Google paper and hence the speed-up is ten orders of
magnitude lower than Google’s fantastic claims. See, e.g., [19, 18, 31, 11,
12, 20]. The Google paper claims that their ultimate task that required 200
seconds for the quantum computer would require 10,000 years on a powerful
supercomputer. With the new algorithms the task can be done in a matter
of seconds.

A few days after the publication of the Google supremacy paper re-
searchers from IBM [20] exhibited a theoretical improvement of six orders
of magnitudes (albeit on a more powerful supercomputer). In response, the
Google team pointed out that their paper [3] did anticipate some progress in
classical algorithms: “We expect that lower simulation costs than reported
here will eventually be achieved, but we also expect that they will be consis-
tently outpaced by hardware improvements on larger quantum processors.”

21

As a matter of fact, the Google team welcomed better algorithms and wrote
in [3] that the bitstring samples from all circuits have been archived [5] “to en-
courage development and testing of more advanced veriﬁcation algorithms.”
While weakening Google’s supremacy claims, the progress in classical
algorithms has also led to major support for Google’s claims on items c)
and d). In the recent work by Gleb Kalachev, Pavel Panteleev, and Man-
Hong Yung, [11] the authors were able to compute the ﬁdelity for samples for
which 2019 Google algorithms were too slow to handle. The ﬁdelity values
that Gleb, Pavel, and Man-Hong computed agree perfectly with Google’s
(77) prediction. This gives a very strong support to an aﬃrmative answer
to items c) and d). Another major support for both claims c) and d) are
the 2021 replications by a group in USTC of the Google 2019 experiments
[29, 32]. Yet more support for claims c) and d) came from our own study. In
[22] we oﬀered (as a “sanity test”) an alternative way to estimate the ﬁdelity
based on a “secondary” component of the theoretical distributions that arises
from readout errors. In a subsequent work [23] we checked this alternative
estimators for the ﬁdelity and found a good match with the FXEB estimators
for n ≤ 30. (See Figure 2.) This work nicely implements Fourier tools.

We remark that on many items mentioned in this section we carried out
statistical analysis that is not presented here, and on most items there is room
for more detailed discussion. This is especially the case for the comparison
of the empirical distribution with the noise model (6.1a) and the prediction
power of equation (77) (6.1d).

Three very concrete questions for circuits with 22 qubits

To be completely concrete let us ask three questions about (veriﬁable full)
random circuit sampling of a circuit C of the kind discussed in the Google
paper with n = 22 qubits and depth m = 14.

1. Can humanity produce at present samples which are good approxima-
tion of the Google noise model or any other speciﬁc noise model?

2. Did humanity reach the ability to produce samples for quantum circuit

C with FXEB ﬁdelity estimated above 0.15?

3. Did humanity reach the ability to predict, for a quantum circuit C,
with good accuracy, the FXEB ﬁdelity estimator based on the ﬁdelity
of the individual components of this circuit?

22

The ﬁndings of our paper [22] indicate that the answer to the ﬁrst ques-
tion is negative. The Google supremacy paper and subsequent conﬁrmations
present a strong case for a positive answer to the other two questions. But
there are remaining doubts and concerns that need to be carefully checked,
and not enough replications. (We are aware only of the Google experiment
itself and the two USTC replications.)2

6.2 Conﬁrmations, refutations, concerns, and weak-

nesses

Here is a list, without much commentary of weaknesses, concerns, conﬁrma-
tions, and refutations of the Google 2019 quantum supremacy experiment.

A) Conﬁrmations

1. The computations of the ﬁdelity estimators and related computations
reported in the Google paper were conﬁrmed by us up to our own
computational limits (and several other groups conﬁrmed it well beyond
what we did).

2. Major conﬁrmation by Kalachev, Panteleev, and Yung [11] for the
Google 2019 paper’s experimental claims in the “supremacy region.”
This gives a strong support to the reliability of the 2019 Google exper-
iment.

3. Our readout errors/Fourier study largely supports (for n ≤ 30) the
experimental claims and the experiment’s reliability [23], see Figure 2.

4. A successful replication was announced and published by a group from

USTC [29, 32].

5. The Google ﬁdelity estimators and statistical methodology are overall

good [22].

2For comparison, after the ﬁrst Wright brothers ﬂight on December 17, 1903 (120 feet)
there were subsequent ﬂights: 852 feet on January 1, 1904; 1,297 feet on February 23 1904;
1,810 feet on May 14, 1904; 4,781 feet on September 23, 1904; 5,360 feet on December 17,
1904; 12,540 feet on January 1, 1905; 24 miles on May 22, 1905; 34 miles on July 4, 1905;
39 miles on October 5, 1905; 56 miles on November 23, 1905, and 63 miles on December
31, 1905. (Source: GPT-3)

23

B) Refutations

6. Google’s fantastic supremacy claims were largely (but not fully) refuted

[19, 18, 31, 11, 12, 20]. (See Section 6.1 above.)

7. The data does not ﬁt the Google noise model [22] or any other speciﬁc
(See Section 6.1 above.); the Google team disagrees on this

model.
point.

AB) Mixed conﬁrmation/refutation

8. The boson sampling quantum supremacy experiments give a mixed
signal. They were regarded as independent conﬁrmation of quantum
supremacy using a very diﬀerent quantum device, however the com-
putational hardness claims are in tension with old (Kalai and Kindler
[15]) and new [21, 28] works.

C) Concerns

9. The Google experiment represents amazing leaps in human ability to
control quantum devices. The non-gradual advances are surprising and
there isn’t a clear technological reason behind them.3

We note that here we refer to the ability expressed by random circuit sam-
pling experiments. The remarkable progress in building the hardware itself
(by several groups in academia and industry including the Google team) was
gradual.

10. The prediction power for the a priori ﬁdelity estimation (Formula (77)
in [4]) raised concerns about reliability. This matter is raised and dis-
cussed in [14] and is also mentioned in [22].

The concern regarding the prediction of linear cross-entropy ﬁdelity es-
timators based on the ﬁdelities of individual components, appears to be the
strongest challenge to date to the reliability of the Google experiment and the
USTC replication. There are two important remarks regarding this concern:

3The Google supremacy experiment was often compared to IBM’s “Deep Blue” victory
against the chess world champion Kasparov; in that case, however, progress was gradual
and earlier chess programs could already win against chess grand-masters.

24

First, the Google team and other researchers regard this prediction power as
one of the main achievements of the Google experiment. Second, [11] con-
ﬁrmed these predictions for many circuits for which the Google team could
not even compute the amplitudes. (This is a sort of “blind test” veriﬁcation.)

11. When you move away from Google’s noise model, the ﬁdelity estimators
may not capture the actual ﬁdelity of the quantum system. (See e.g.
[8].)

12. Some aspects of the calibration process may seem overly surprising
(this represents some preliminary analysis that we presented in our
email discussions in November 2021).

13. There are no suﬃcient replications of the supremacy experiment and
veriﬁcations of the experimental ﬁndings even in the 12-40 qubit range.
(Neither by other groups nor by the Google team itself.)

D) Weaknesses

14. While the experiment was characterized as performing a sampling com-
putational task, the empirical distribution was not compared to the
noise model, and the experimental samples are too small to allow such
comparison (directly) when n > 14.

15. The supremacy claims were based on a single algorithm. The Google
team itself developed better algorithms to related computational tasks.

When the Google team found an improved type of algorithms (around
May 2019) they switched to a diﬀerent class of circuits for which the
new algorithm did not apply without suﬃcient evidence that similar
improvements could not be made for them as well. (Later, as we men-
tioned, improvements were also achieved to the new class of circuits.)

16. The results were not presented to the academic community before pub-
lication (e.g., as an arXiv publication), and careful review of the ex-
periment prior to publication may have been insuﬃcient.

17. The main ﬁdelity estimator is not unbiased and can be improved [22].

25

18. Moving to a new pattern ABCDCDAB weakened the power of the
extrapolation argument. For the new pattern no experiments for cir-
cuits with less than 53 qubits were conducted. (This was a weakness in
planning the experiment although some of the ﬁdelity predictions are
now conﬁrmed [11].)

19. Improvements of the calibration process were interlaced (namely oc-
curred at the same time) with the experiment. The description of
the calibration process in Google’s popular video [9] may be in tension
with the claim that there was a clear separation between the calibration
stage and the experiment itself.

20. The calibration process itself somewhat weakens the claim for a “pro-
grammable quantum computer”. (This concern was raised by a com-
mentator “Till” in an Internet discussion [27].)

21. The eﬀect of the calibration process is very large (it is not a “ﬁne

tuning” of some sort).

22. The precise programs for the calibration process are commercial secrets.
(However, the main innovation in calibration was the use of XEB, and
that code is open sourced.)

23. The levels of transparency and of documentation of the Google 2019

experiment seem insuﬃcient.

Of course, some of the items above are controversial, some of them are
rather minor, and also many of these items are related. Items (15), (16) are
a-priori weaknesses that turned out to be crucial (refutation (6) above). The
recent conﬁrmations (2) indicate that weakness (18) may have no baring on
the assertions for large circuits, but we still regard it as a weakness in the
planning of the experiment. (The Google team disagrees with us.)

We also note that expectations expressed in the Google 2019 paper [3]
seem overly optimistic: “Quantum processors have thus reached the regime
of quantum supremacy. We expect that their computational power will con-
tinue to grow at a double-exponential rate: the classical cost of simulating
a quantum circuit increases exponentially with computational volume, and
hardware improvements will probably follow a quantum-processor equiva-
lent of Moore’s law, doubling this computational volume every few years.”
(Compare with later analysis in [33, 8].)

26

Figure 2: The decay of Fourier–Walsh contributions as predicted by the theory
(bold black curve)([7, 23]) and as demonstrated by samples of a 16-qubit
Sycamore computer (ten random circuits).

27

7 Where we are (September 2022)

Now that the Google 2019 experiment data-gathering stage needed for our
study has largely come to an end, here is a brief summary of where we are.
1. We compared the empirical distribution with Google’s noise model
and with other noise models. Some of this analysis is discussed in [22], and
a few other ﬁndings were discussed with the Google team.

2. In [22] we oﬀered an alternative way (that could serve as a “sanity
test”) to estimate the ﬁdelity based on a linear cross entropy test for a “sec-
ondary” component of the theoretical distributions that arises from readout
errors. In a subsequent work in progress [23] we used Fourier methods to
check this alternative estimator for the ﬁdelity, and found a good match
with the primary linear cross entropy ﬁdelity estimators for n ≤ 30.

3. We carried out some statistical analysis of the eﬀect of the calibration
process. Continuing in this direction, we plan, among other things, to study
a proposal of John Martinis (made on Aaronson’s blog in December 2020
[26], as well as in our discussions) for improving the calibration.

4. We plan to extend some of our statistical studies to the new data from
June 2022, and especially to the data for the patch circuits. There is also
some remaining analysis to be carried out for the earlier data. We also plan
to extend our statistical study to data coming from other NISQ computers
and from simulators that incorporate the eﬀect of noise.

5. There is one important data item that is still not publicly available,
namely, the ﬁdelities of individual components that are used in Formula (3)
(Formula (77) in [4]). This information consists only of a few hundred real
numbers and we hope that the Google team will share it in the near future.
The Google team provided useful approximate versions for formula (77) (such
as Formula 4) but we found out that for the study of patch circuits the full
data is required.

6.

(October 2022.) On the theoretical side, we recently realized that
the remarkable work of Gao, Kalinowski, Chou, Lukin, Barak, and Choi [8]
is relevant to some of the concerns discussed above. Analysis based on [8]
strengthens concern (10) and sheds some doubts on our conﬁrmation (3) (the
second item above).

28

Acknowledgements

Research supported by ERC grant 834735. We thank many colleagues includ-
ing members of the Google team for helpful discussion and Carsten Voelk-
mann for many corrections and helpful suggestions.

References

[1] S. Aaronson, Why Google’s quantum supremacy milestone matters,

Opinion, New York Times Oct. 30, 2019.

[2] S. Aaronson and S. Gunn, On the classical hardness of spooﬁng linear

cross-entropy benchmarking, 2019, arXiv:1910.12085.

[3] F. Arute et al., Quantum supremacy using a programmable supercon-

ducting processor, Nature, 574 (2019), 505–510.

[4] F. Arute et al. (2019), Supplementary information for “Quantum
supremacy using a programmable superconducting processor,” 2019,
arXiv:1910.11333.

[5] F. Arute et al., Supplementary data for “Quantum supremacy
2019,

using
https://datadryad.org/stash/dataset/doi:10.5061/dryad.k6t1rj8.

superconducting

programmable

processor,”

a

[6] S. Boixo, S. V. Isakov, V. N. Smelyanskiy, and H. Neven, Simulation
of low-depth quantum circuits as complex undirected graphical models,
arXiv:1712.05384, 2017.

[7] S. Boixo, V. N. Smelyanskiy, and H. Neven, Fourier analysis of sampling

from noisy chaotic quantum circuits, 2017, arXiv:1708.01875.

[8] X. Gao, M. Kalinowski, C.-N. Chou, M. D. Lukin, B. Barak, and S.
Choi, Limitations of Linear Cross-Entropy as a Measure for Quantum
Advantage, 2021, arXiv:2112.01657.

[9] Google, Demonstrating Quantum Supremacy, 4:42 minutes video for a
large audience, Oct. 23, 2019. URL: https://youtu.be/-ZNEzzDcllU .

29

[10] S.

Irani

(Moderator), Supremacy Panel, Hebrew University of
Jerusalem, Dec. 2019. Participants: D. Aharonov, B. Barak, A. Bouland,
G. Kalai, S. Aaronson, S. Boixo, and U. Vazirani. (Video.) URL:
https://youtu.be/ Yb7uIGBynU .

[11] G. Kalachev, P. Panteleev, and M.-H. Yung, Multi-tensor contraction
for XEB veriﬁcation of quantum circuits, 2021, arXiv:2108.05665.

[12] G. Kalachev, P. Panteleev, P. F. Zhou, and M.-H. Yung, Classical
sampling of random quantum circuits with bounded ﬁdelity, 2021,
arXiv:2112.15083.

[13] G. Kalai, The Google quantum supremacy demo, videotaped lecture,

December 2019. URL: https://youtu.be/p18P1y8GD9U.

[14] G. Kalai, The argument against Quantum Computers, the quantum
laws of nature, and Google’s supremacy claims, in: The Intercontinen-
tal Academia Laws: ’Rigidity and Dynamics,’ (M. J. Hannon and E. Z.
Rabinovici (ed.)) Proceedings of the ICA Workshops 2018&2019, Singa-
pore and Birmingham, World Scientiﬁc, to appear. arXiv:2008.05188.

[15] G. Kalai and G. Kindler, Gaussian noise sensitivity and BosonSampling,

2014, arXiv:1409.3093.

[16] J.-S. Kim, L. S. Bishop, A. D. Corcoles, S. Merkel, J. A. Smolin, S.
Sheldon, Hardware-eﬃcient random circuits to classify noise in a multi-
qubit system, Phys. Rev. A 104 (2021), 022609.

[17] C. Neill et al., A blueprint for demonstrating quantum supremacy with

superconducting qubits. Science 360 (2018), 195-199

[18] F. Pan, K. Chen, and P. Zhang, Solving the sampling problem of the
Sycamore quantum supremacy circuits, Phys. Rev. Lett. 129 (2022),
090502

[19] F. Pan and P. Zhang, Simulating the Sycamore quantum supremacy

circuits, 2021, arXiv:2103.03074

[20] E. Pednault, J. A. Gunnels, G. Nannicini, L. Horesh, and R. Wisni-
eﬀ, Leveraging secondary storage to simulate deep 54-qubit Sycamore
circuits, 2019, arXiv:1910.09534.

30

[21] A. S. Popova and A. N. Rubtsov, Cracking the quantum advantage

threshold for Gaussian boson sampling, 2021, arXiv:2106.01445.

[22] Y. Rinott, T. Shoham, and G. Kalai, Statistical aspects of the quantum
supremacy demonstration, Statistical Science 37 (2022), 322–347.

[23] Y. Rinott, T. Shoham, and G. Kalai, Quantum Advantage Demonstra-
tions via Random Circuit Sampling: Fourier Expansion and Statistics,
manuscript in progress.

[24] Shtetl Optimized, A comment by S. Boixo , Dec. 2020. URL:

https://scottaaronson.blog/?p=5159#comment-1870651 .

[25] Shtetl Optimized, A comment by C. Gidney, Feb. 2020. URL:

https://scottaaronson.blog/?p=4608#comment-1830545 .

[26] Shtetl Optimized, A comment by J. Martinis, Dec. 2020. URL:

https://scottaaronson.blog/?p=5159#comment-1870734 .

[27] Shtetl Optimized, A comment by

“Till”, Dec.

2020. URL:

https://scottaaronson.blog/?p=5159#comment-1869118 .

[28] B. Villalonga, M. Yuezhen Niu, L. Li, H. Neven, J. C. Platt, V. N.
Smelyanskiy, and S. Boixo, Eﬃcient approximation of experimental
Gaussian boson sampling, 2021, arXiv:2109.11525.

[29] Y. Wu et al., Strong quantum computational advantage using a super-
conducting quantum processor. Phys. Rev. Lett. 127 (2021), 180501.

[30] H.-S. Zhong et al., Quantum computational advantage using photons,

Science 370 (2020), 1460–1463.

[31] Y. Zhou, E. M. Stoudenmire, and X. Waintal, What limits the simula-

tion of quantum computers?, 2020, arXiv:2002.07730.

[32] Q. Zhu et al., Quantum Computational Advantage via 60-Qubit 24-
Cycle Random Circuit Sampling, Science Bulletin, Volume 67 (2022),
240-245. arXiv:2109.03494.

[33] A. Zlokapa, S. Boixo, and D. Lidar, Boundaries of quantum supremacy

via random circuit sampling, 2020, arXiv:2005.02464.

31

[34] Various links to useful discussions over scientiﬁc blogs of the Google’s
2019 paper and related developments. (SO stands for Shtetl Optimized,
Scott Aaronson’s blog; CaM stands for Combinatorics and More, Gil
Kalai’s blog; WoT stands for Windows on Theory, Boaz Barak’s blog.

1. Scott’s Supreme Quantum Supremacy FAQ! SO, Sept 23, 2019.

https://scottaaronson.blog/?p=4317.

2. Quantum

computers:

amazing

progress

IBM),

and
claims

extraordinary
&
supremacy
(Google). CaM,
https://gilkalai.wordpress.com/2019/09/23/quantum-computers-
amazing-progress-google-ibm-and-extraordinary-but-probably-
false-supremacy-claims-google/

probably
23,

Sept

but

(Google
false
2019.

3. The story of Poincar´e and his friend the baker, CaM, Oct.
13, 2019. https://gilkalai.wordpress.com/2019/10/13/the-story-of-
poincare-and-his-friend-the-baker/

4. Quantum supremacy:

the gloves are oﬀ, SO, Oct. 23, 2019/

https://scottaaronson.blog/?p=4372.

5. Boaz’s inferior classical

inferiority FAQ, WoT, Oct. 24, 2021.
https://windowsontheory.org/2019/10/24/boazs-inferior-classical-
inferiority-faq/

6. Gil’s Collegial Quantum Supremacy Skepticism FAQ, CaM Nov.
13, 2019, https://gilkalai.wordpress.com/2019/11/13/gils-collegial-
quantum-supremacy-skepticism-faq/

7. My

“Quantum Supremacy:

Tour,
2020 World
https://scottaaronson.blog/?p=4608

Speaking

Skeptics Were Wrong”
2020
SO,

Feb.

17,

8. Quantum supremacy, now with BosonSampling, SO, Dec. 3, 2020,

https://scottaaronson.blog/?p=5122.

9. Chinese BosonSampling experiment: the gloves are oﬀ, SO, Dec.

16, 2020. https://scottaaronson.blog/?p=5159

Gil Kalai, Hebrew University of Jerusalem and Reichman University, gil.kalai@gmail.com
Yosef Rinott, Hebrew University of Jerusalem, yosef.rinott@mail.huji.ac.il
Tomer Shoham, Hebrew University of Jerusalem, tomer.shohamm@gmail.com

32

","2 2 0 2 t c O 3 2 ] h p - t n a u q [ 1 v 3 5 7 2 1 . 0 1 2 2 : v i X r a Google’s 2019 “Quantum Supremacy” Claims: Data, Documentation, and Discussion Gil Kalai, Yosef Rinott & Tomer Shoham October 25, 2022 Abstract In October 2019, Nature published a paper [3] describing an exper- imental work that took place at Google. The paper claims to demon- strate quantum (computational) supremacy on a 53-qubit quantum computer. Since September 2019 the authors have been involved in a long-term project to study various statistical aspects of the Google experiment. In particular, we have been trying to gather the rele- vant data and information, to reconstruct and verify those parts of the Google 2019 supremacy experiments that are based on classical computations (unless they require too heavy computation), and to put the data under statistical analysis. We have now (August 2022) concluded the part relating to the gathering of data and information needed for our study of the 2019 Google experiment, and this docu- ment describes the available data and information for the Google 2019 experiment and some of our results and plans. 1 Introduction The 2019 paper “Quantum supremacy using a programmable superconduct- ing processor” [3] claimed that Google’s Sycamore processor performed a certain computation in about 200 seconds, while a state-of-the-art classical supercomputer would take, according to Google’s team estimates, approxi- mately 10,000 years to perform the same computation. Google’s Sycamore quantum computer performed a sampling task ; that is, it generates ran- dom bitstrings, of length 53, with considerable noise, from a certain discrete 1 probability distribution supported on all such 253 bitstrings. The speciﬁc Google’s sampling task is referred to as random circuit sampling (RCS, for short). Google’s announcement of quantum supremacy was compared by [1]) to landmark technological achievements such various writers (See, e.g. as the Wright brothers’ invention of a motor-operated airplane, the Sputnik, and the landing on the moon, as well as to landmark scientiﬁc achievement such as Fermi’s demonstration of a nuclear chain reaction, the discovery of the Higgs boson, and the LIGO detection of gravitational waves. In 2020 a team from the University of Science and Technology of China (USTC) claimed [30] that the sampling task computed by their photonic Jiuzhang quantum computer, would take 2.5 billion years to perform on a classical supercomputer. USTC’s quantum computers also required about 200 seconds for their tasks. This task is referred to as Gaussian boson sam- In 2021, another team from USTC repeated the pling (GBS, for short). Google 2019 RCS experiment with a superconducting processor Zuchongzhi of 60 qubits and depth 24 [29, 32], and claimed to achieve an even stronger form of quantum advantage compared to the Google 2019 experiment. The Google experiment represented a very large leap in various aspects of the human ability to control noisy quantum systems. For example the Google AI team previously reported an experiment with nine qubits [17]. This leap is especially impressive in terms of the dimensions of the Hilbert space representing a state of the computer. (From dimension 100–500 in [17] to dimension 1016 in [3].) As for Google’s “quantum supremacy” claims, progress by several groups [19, 18, 31, 11, 12, 20, 8] (and more) exhibits classical algorithms which are ten orders of magnitude faster than those used in the Google paper. This was achieved, for example, by Pan, Chen, and Zhang [18] in 2021. Our study concentrates on other aspects of the Google experiment. As we already mentioned, the Google announcement was regarded as a major scientiﬁc and technological event. On its own it gave some evi- dence that the “strong Church–Turing thesis” had been violated and it was described as an ironclad refutation of claims by some scientists (including Kalai) that quantum computation is not possible. Of no less importance is that quantum supremacy was considered as a major intermediate step towards exhibiting experimentally quantum error-correction codes needed for building larger quantum computers. The announcement of quantum supremacy stirred a great deal of enthusiasm among scientists and in the general public, and garnered signiﬁcant media attention. It had substantial 2 Figure 1: The price of bitcoin in USD in a period of 4 weeks around 9/23/2019. Source: CoinDesk impact; for example, following the media attention surrounding the leaking of the supremacy claims around September 22, 2019, the value of bitcoin (and other digital currencies) sharply dropped by more than 10% and it is a reasonable possibility that given the potential eﬀects of quantum computers on the safety of digital currencies, the quantum supremacy claims caused this drop. Putting the Google supremacy claims under scrutiny The Google paper appeared in October 2019 and a month earlier it was brieﬂy posted on a NASA server and became publicly available. Since the announcement of the quantum supremacy claim, the ﬁrst-named author (and other researchers) raised various concerns about some aspects of the claims. A few months later the authors initiated what has become a long-term project to study various statistical aspects of the Google experiment. In particular, we have been trying to gather the relevant data and information and to reconstruct and verify those parts of the Google 2019 supremacy experiments that are based on classical computations. (Unless they require too heavy 3 computation; we carried some heavy computation on the cloud for which we put a cap of 2000 dollars on our spending.) We also performed several “sanity tests” of the experiment. The structure of this paper In Section 2 we provide a brief background on the Google 2019 experiment, describe the various types of circuits used in the experiment and the chronol- ogy of the various Sycamore experiments as reported by the Google team. In Section 3 we describe the nature of the calibration process. In Section 4 we describe the requested data from the Google team, the data that was provided between October, 2019 and June 2022, and some other details related to the Google experiment and other NISQ supremacy experiments. In Section 5 we raise two proposals for future experiments. In Section 6 we discuss what we regard as the main questions in the evaluation of the Google 2019 experiment and list some conﬁrmations, refutations, concerns, and weaknesses, and in Section 7 we brieﬂy discuss where we are now in our study. 2 Google’s 2019 quantum supremacy claim 2.1 A brief background In this draft we will assume knowledge of the Google 2019 experiment, Google’s noise model, Google’s FXEB linear cross entropy ﬁdelity estima- tor, and Google’s formula (77) in [4] for predicting the ﬁdelity of a circuit from the ﬁdelity of its components. We will give here a brief summary of these topics. The Google 2019 experiment is based on the building of a quantum com- puter (circuit) with n superconducting qubits, that perform m rounds of computation. The computation is carried out by a 1-qubit and 2-qubit gates. At the end of the computation the qubits are measured, leading to a string of zeroes and ones of length n. The ultimate experiment was for n = 53 and m = 20. It involved 1113 1-qubit gates and 430 2-qubit gates. For that experiment the Google team produced a sample of three million 0-1 vectors of length 53. Every circuit C with n qubits describes a probability distribution PC(x) (In fact, it describes a 2n-dimensional vector for 0-1 vectors of length n. 4 of complex amplitudes; to every 0-1 vector x, there is an associated ampli- tude z(x) and PC(x) = |z(x)|2.) The quantum computer enables to sample according to the probability distribution PC(x) with a considerable amount of noise. When n and m are not too large classical simulations enable to compute the amplitudes themselves (and hence the probabilities PC(x)). Google’s supremacy claims are based on the fact that these classical simula- tions quickly become infeasible as n and m grow. The Google basic noise model for the noisy samples their Sycamore device actually produces is NC(x) = φPC + (1 − φ)2−n, (1) where φ is the ﬁdelity, a parameter that roughly describes the quality of the sample. (The ﬁdelity has a precise meaning in terms of the actual noisy quantum process carried out by the Google Sycamore device.) Based on their noise model (and the fact that the distribution PC is an instance of a Porter–Thomas distribution) the Google paper describes a statistic called the linear cross entropy estimator (and denoted by FXEB.) Once the quantum computer produces a sequence ˜x of N samples ˜x = (˜x(1), ˜x(2), . . . , ˜x(N )), the following “linear cross entropy” estimator FXEB for the ﬁdelity is computed FXEB(˜x) = 1 N N (cid:88) i=1 2nPC(˜x(i)) − 1. (2) Computing FXEB requires knowledge of PC(x) for sampled bitstrings. The Google supremacy claim is based also on the following a priori pre- diction for the ﬁdelity of a circuit based on the probabilities of error for the individual components. ˆφ = (cid:89) g∈G1 (1 − eg) (1 − eg) (cid:89) g∈G2 (cid:89) q∈Q (1 − eq). (3) Here G1 is the set of 1-gates (gates operating on a single qubit), G2 is the set of 2-gates (gates operating on two qubits), and Q is the set of qubits. For a gate g, the term eg in the formula refers to the probability of an error (1 minus the ﬁdelity) of the individual gate g. For a qubit q, eq is the probability of a read-out error when we measure the qubit q. The Google supremacy paper [3] made two crucial claims regarding the ultimate 53-qubit samples. 5 A) The ﬁdelity φ of their sample is above 1/1000. B) Producing a sample with similar ﬁdelity would require 10,000 years on a supercomputer. For claim A) regarding the value of φ, the argument relies on an extrapo- lation argument that has two ingredients. One ingredient is a few hundred experiments in the classically tractable regime: the regime where the prob- ability distribution PC can be computed by a classical computer and the performance of the quantum computer can be tested directly. The other in- gredient is the theoretical formula (3) for predicting the ﬁdelity. According to the paper, the ﬁdelity of entire circuits closely agrees with the prediction of formula (3) (Formula (77) in [4]) with a deviation below 10–20 percent. There are around 200 reported experiments in the classically tractable regime including ones carried out on simpliﬁed circuits (which are easier to simu- late on classical computers). These experiments support the claim that the prediction given by Formula (77) for the ﬁdelity is indeed very robust and applies to the 53-qubit circuit in the supremacy regime. For claim B) regarding the classical diﬃculty, the Google team mainly re- lies on extrapolation from the running time of a speciﬁc algorithm they used. They also rely on the computational complexity support for the assertion that the task at hand is asymptotically diﬃcult. Remark: The Google team proposed a pretty good approximation for Formula (3) based on averaged ﬁdelities. ˆˆφ = (1 − 0.0016)|G1|(1 − 0.0062)|G2|(1 − 0.038)n. (4) 2.2 The types of circuits of the Google experiment The circuits used in the 2019 Google supremacy experiment had the fol- lowing structure. The qubits were arranged on a planar grid, so a single qubit was identiﬁed via two coordinates, like qubit (3, 3). The circuits had two types of layers: one type of layer consists of 2-gates acting on pairs of (neighboring) qubits. After each such layer of two gates there was another layer of randomly chosen 1-gates acting on every qubit. The layers of 1-gates consist of the ”programmable” ingredient in the experiment and they change from circuit to circuit. The layers of 2-gates are ﬁxed throughout the ex- periment according to a certain pattern. The pattern EFGH was used in 6 all experiments conducted between February to May 2019 and a new pat- tern ABCDCDAB was used in June 2019 to produce the samples of the “supremacy” circuit, namely circuits which would require a huge classical computation. Each letter like E corresponds to a ﬁxed set of 2-gates act- ing in parallel on the qubits (with the convention that for circuits with a smaller number of qubits we regard only 2-gates that involve the qubits in the circuit). So Pattern EFGH means that we ﬁrst apply a layer of random 1-gates on all qubits, then apply 2-gates according to E, next apply another layer of random 1-gates on all qubits, then apply 2-gates according to F and continue (in a periodic manner). The new pattern ABCDCDAB is based on new types of layers for the 2-gates and on a period of length eight. The depth m of a circuit refers to the number of layers of 2-qubits. So, for example, the 2-gates layers of a circuit with pattern EFGH of depth m = 14 are E, F, G, H, E, F, G, H, E, F, G, H, E, F. The six types of circuits The Google 2019 experiment relies on six types of circuits. The ﬁrst three types are: a) The full circuits of pattern EFGH, b) The elided circuits of pattern EFGH, c) The patch circuits of pattern EFGH. Types b) and c) are simpliﬁed forms of type a) for which there are quick algo- rithms to compute the amplitudes for all values of n up to 53. In particular, patch circuits consist of two separate circuits on two disjoint sets of qubits. The Google paper [3] is based on running the quantum computer on a few hundred circuits of type a) b) and c) and computing the ﬁdelity estimator based on the amplitudes. There was also a preliminary stage of calibration that, based on exper- iments on 1-qubit and 2-qubit circuits, determined the precise adjustments for the experimental 2-gates. Those adjustments are the same for all circuits of type a), b) and c). See Section 3. As far as we understand, the initial plan of the Google team to demon- strate quantum supremacy was to compute the empirical ﬁdelities for circuits of type b) and c) for a number of qubits between 12 and 53 and for circuits 7 of type a) for a number of qubits up to n = 43 and to use this information to estimate the ﬁdelity for circuits of type a) with 53 qubits and depths (m) between 12 and 20. However, in May 2019, the Google team discovered an eﬃcient classical algorithm for the circuits of type a). (Because of this discovery, the full cir- cuits with pattern EFGH are also referred to as “veriﬁable full circuits.”) The new algorithm discovered by the Google team is related to tensor net- works methods which later led to important discoveries in this direction.1 Using this algorithm, the Google team computed the amplitudes for the ex- perimental bitstrings for type a) circuits with n = 53 m = 14. Subsequently, for the purpose of demonstrating quantum supremacy, the Google team moved in May 2019 to a diﬀerent architecture based on a new pattern ABCDCDAB which is harder to simulate by classical computers. This led to the following three new types of circuits. d) The full circuits with pattern ABCDCDAB. (These circuits are also referred to as “supremacy full circuits.”) e) The elided circuits of pattern ABCDCDAB, f) The patch circuits of pattern ABCDCDAB. For this architecture the Google team produced samples only with n = 53 and depths 12, 14, 16, 18 and 20. The simulators available to the Google team were not powerful enough for computing the amplitudes and for checking the ﬁdelity for circuits of type (d). The circuits with pattern ABCDCDAB required to run the calibration process again on 1-qubit and 2-qubit circuits based on the new architecture and this has led to new adjustments for the 2-gates. Circuits of the new pattern ABCDCDAB (and the calibration process) were not tested for less than 53 qubits. 2.3 A chronology of the Google Sycamore experiments Prior to the development of the Sycamore quantum computer, in 2018, the Google team had attempted to develop a 72-qubit chip called “Bristlecone,” 1The Google team pointed out that they also implemented a related tensor network method in 2017 [6]. 8 but due to diﬃculties the team later proceeded to the development of the Sycamore 54-qubit computer (with 53 eﬀective qubits). Here is a list of experiments for the Sycamore quantum computer in- cluding early experiments based on earlier calibration methods. For every experiment fresh new random circuits were generated. (Namely, the improve- ments in the calibration procedures in a certain experiment were not based on improving the ﬁdelity for circuits that were used in an earlier experiment.) Each experiment took 1-2 days to perform. Each of the ﬁrst six ex- periments is based on an improved method for calibration. There were six experiments for circuits of pattern EFGH and one (the last) experiment is based on the new architecture with pattern ABCDCDAB. 2.3.1 Earlier experiments 1. Date: 02/08/2019 Full circuits with pattern EFGH; numbers of qubits: 2, 3, 4, . . . , 49 2. Date: 02/13/2019 Full circuits with pattern EFGH; numbers of qubits: 2, 3, 4, . . . , 49 3. Date: 03/14/2019 Full circuits with pattern EFGH; numbers of qubits: 2, 3, 4, . . . , 51 4. Date: 03/22/2019 Full circuits with pattern EFGH; numbers of qubits: 33, 34, . . . , 51 5. Date: 03/27/2019 Full circuits with pattern EFGH; numbers of qubits: 10, 20, 33, 34, 35, 36, 37, 38, 51 2.3.2 The experiments on which the Google paper is based 6. Date: 04/22/2019 Patch, elided, and full circuits with pattern EFGH; Numbers of qubits: 12, 14, 16, . . . , 36, 38, 39, 40, . . . , 50, 51, depth m = 14 A few days later the Sycamore quantum computer produced the 53- qubit samples for patch, elided, and full circuits with the EFGH pat- tern also with depth m = 14. 9 7. Date: 06/13/2019 Numbers of qubits: 53, depth m = 12, 14, 16, 18, 20 Type of circuits: patch, elided, full with pattern ABCDCDAB. 3 The nature of the calibration process The Sycamore quantum computer has systematic deviations from the ideal circuit it describes, and the calibration process accounts for small systematic errors in the experimental circuits compared to the random circuits they represent. The calibration process is crucial since the combined eﬀect of such systematic errors can slash the ﬁdelity to zero and it is important to account for them. By the calibration process we refer in this paper to a method which, based on multiple runs of 1-qubit and 2-qubit quantum circuits, adjusted the deﬁnition of the 2-gates of the experimental circuits so that certain systematic forms of noise will be greatly reduced. This calibration adjustment was carried out simultaneously for all the experiments for patch, elided, and veriﬁable full circuits. It is easy to identify from the Python program how the random circuits (We could also use the QSIM ﬁles for were modiﬁed by the calibration. that purpose.) For example, consider a single 2-gate acting on two qubits A = (3, 3) and B = (3, 4). In the ideal description of the circuit, this 2-gate is described in the Python program for the circuit as follows: cirq.Moment( cirq.FSimGate(theta=1.57079632679, phi=0.52359877559). on( cirq.GridQubit(3, 3), cirq.GridQubit(3, 4)), In this 2-gate (that is sometimes referred to as the “standard 2-gate”) θ = π/2, and φ = π/6. We will now show the part of the program that describes 2-gates modiﬁcations for this speciﬁc gate. The calibration consists of adding to the description of the circuits, two ﬁxed rotations on each of the 2 qubits involved in this particular 2-gate. The two added single qubit rotations are cirq.Rz (np.pi * 2.5333591271878086). on (cirq.GridQubit (3, 3)), cirq.Rz (np.pi * -2.4748096263683066). on (cirq.GridQubit (3, 4)), 10 and these two added single qubit rotations are the same for every appearance of the 2-gate involving A and B. The values that appear here in the single qubit rotations, as well as the adjustments for the deﬁnition of 2-gates, were computed based on many experiments for 1-qubit and 2-qubit circuits for these two qubits (3,3) and (3,4). (We neither have the precise algorithm for this computation nor the data with the outcomes of the 1-qubit and 2-qubit experiments.) In addition, the 2-gate acting on qubits A and B is modiﬁed to cirq.Moment( cirq.FSimGate(theta=1.2947043217999283, phi=0.4859467238431821). on( cirq.GridQubit(3, 3), cirq.GridQubit(3, 4)), The calibrated values θ = 1.2947043217999283 and φ = 0.4859467238431821 are the same for all 2-gates acting on these two qubits in all circuits. (Some- times the calibrated 2-gate is referred to as “native” 2-gate.) These values for θ and φ replace the values for the “standard” gate in the original random circuit θ = π/2, and φ = π/6. Remarks: 1. We note that the adjustments for 2-gate involving qubits A and B depend (only) on multiple runs of 1-qubit and 2-qubit quantum circuits in- volving these two qubits. 2. We also note that there might have been additional actions to account for the fact that certain components of the Sycamore chip degrade over time. 3. In the December 2020 Internet discussion [24] Sergio Boixo from the Google team referred to Figure S30 in [4] and asserted that in the speciﬁc case of the 2019 experiment using “Sycamore” standard gates instead of native gates halves the ﬁdelity. However, this claim referred only to the two-gate adjustments and not to the one-gate adjustments, and moving to the original circuit with standard gates slashes the ﬁdelity to zero. (The eﬀect of the two- gate adjustments represents the worst-case situation for n = 53, m = 20, and it is smaller for smaller circuits.) Sergio Boixo later clariﬁed that since 2020 (for later experiments but not for the 2019 supremacy experiment) the Google team developed a method to carry out the 1-qubit calibrations using physical control of the device rather than using a modiﬁcation of the deﬁnition of the circuit. (This later development is not relevant to the 2019 experiment and we did not study it.) 11 4 Data, documentations, and discussions 4.1 The supplementary data The Google paper was leaked around September 23, 2019 and the paper was published by “Nature” a month later on October 23, 2019. The data for the Google 2019 experiment can be found in [5]. The Google team uploaded data to the server in ﬁve dates as follows: October 22/23, 2019, publication date 1. Files with 500,000 bitstrings for many of the experimental circuits with n < 53 were uploaded and for n = 53 samples consists of a few million bitstrings were uploaded. The bitstrings for several numbers of qubits n were missing on this date (e.g. missing for n=16, 32, 34, 38, 39, 42, . . . , 51). In addition, the bitstrings for the patch circuits were missing. 2. QSIM ﬁles with descriptions for the corresponding experimental circuits, as well as Python ﬁles for computing their amplitudes. January 23, 2020 Missing bitstrings for full and elided circuits were uploaded. Amplitude ﬁles for all circuits (not including patch circuits) except (roughly) 200 circuits were uploaded. Some additional data including information on noise for individual components of the Sycamore quantum computer was uploaded. January 22, 2021 & May 18, 2021 Amplitude ﬁles for all circuits (not including patch circuits) that were com- puted by the Google team were uploaded except eight circuits for which the amplitudes were computed in J¨ulich (J¨ulich Research Center, Germany). Readout error data was uploaded. June 13, 2022 The J¨ulich amplitudes were uploaded. Bitstring ﬁles (100,000 bitstrings per circuit) and QSIM and Python ﬁles for the patch circuits with n < 53. (These ﬁles are still missing for n = 53.) 12 4.2 Requests for data 4.2.1 Early requests from October 2019 Here are Kalai’s requests from the Google team from early October 2019. (a) Larger samples from the quantum circuits, comparison of the empirical distribution with the model. (b) The bitstrings produced by the quantum computer and a description of the circuits (c) The amplitudes that were computed for the samples for each circuit (d) The full list of amplitudes. (Here we refer to the 2n amplitudes for all bitstrings and not only those in the sample.) (e) Timetable for the experiments and calibrations (f) (November 11, 2019.) The values of the individual ﬁdelities eq and eg for every qubit q and gate g used in Formula 3. 4.2.2 Subsequent requests (g) Readout errors information. We discussed this matter in January 2021 and we received useful data regarding the readout errors shortly after- wards. In this case, we received the required data quickly, in a very satisfactory way. (h) Amplitudes for the veriﬁable experiments. (September 2021) As we already mentioned, the Google team developed useful algorithms and programs to compute the amplitudes for the experiments of full cir- cuits of pattern EFGH. (This was the reason for moving to a diﬀerent pattern for the “supremacy experiment.”) They used these programs for 10 53-qubit depth-14 circuits (this required several hours for each circuit on the Google supercomputer). For a useful reliability-control of the experiment we proposed that the amplitudes be computed for the remaining (over 100) full circuits, and at least for 2 speciﬁc ones. (i) The optimization programs to move from data on 1-qubit and 2-qubit circuits to 1-gate and 2-gate corrections in the deﬁnition of the cir- cuits and the raw data gathered from 1-qubit and 2-qubit circuits. We requested it in January 2022. 13 4.3 Data provided by the Google team 1. Google’s researchers were under press embargo and did not respond to most issues before the paper appeared. 2. As we already mentioned, on October 23, 2019 the paper was pub- lished in Nature, the supplementary raw data contains (b) (bitstrings, description of the circuits in a form of a Python program to compute the amplitudes) for many of the circuits used in the experiments and on January 2020 bitstrings for other circuits were added. We requested the data for the “patch circuits” in June 2021 and again in December 2021. The data for the patch circuits was uploaded on the server in June 2022 except for the 53-qubit depth-14 EFGH patch circuits. (As we mentioned, the data has only 100K samples per every patch circuit.) 3. The Google team promised to supply the amplitudes (that they com- puted as part of the experiment) for their samples (c) . They uploaded amplitudes for many circuits in January 2020, and for additional two hundred circuits in May 2021. Uploading the amplitudes that were computed for 8 circuits in an external facility (J¨ulich) required approval of the J¨ulich team and they were uploaded in June 2022. 4. Regarding the request (d) for the full list of amplitudes, the Google team informed us that the full lists of amplitudes were discarded. (In- deed, for large values of n this is a huge amount of data.) Moreover, for circuits with pattern EFGH the Google team developed (around May 2019) algorithms for computing the amplitudes for the sample that did not require computing all the 2n amplitudes and these algorithms were used in some of these cases. 5. Regarding larger samples (a). Until August 2022, the Google team did not supply any further samples produced by the quantum computer. As for comparison between the empirical distribution and the noise model, the Google team referred (already in September 2019) for such a comparison to an earlier experiment (on 9 qubits) [17]. 6. Regarding timetable and some details for the experiments and calibra- tions (request (e)), in May 2022 we received a brief timetable and some useful details regarding the ﬁnal experiments and ﬁve earlier ones. See Section 2.3. 14 7. Regarding item (h), the Google team told us that they would not per- form these further computations as they require a lot of human and computational eﬀorts. However, using new algorithms, amplitudes for all the veriﬁable exper- iments were computed by Kalachev, Panteleev, and Yung [11] and the results give a strong support to Google’s 2019 claims and predictions regarding the linear cross entropy of their samples. 8. Regarding item (i), the Google team informed us that they could not share the full proprietary calibration system (that required many years of development) needed to move from data on 1-qubit and 2-qubit to 1-gate and 2-gate corrections in the deﬁnition of the circuits. They pointed out that the main innovation of the calibration program was made public. 9. In July 2022, Adam Zalcman and Sergio Boixo gave us a useful Python program for splitting the patch circuits into the two patches. 10. Regarding item (f) the individual gate- and qubit- ﬁdelities. We asked for it several times in 2019 and early 2020 and again in September 2022. (It turned out recently that this data is crucial for the study of patch circuits.) 4.4 The discussion with the Google team Since October 2019 through August 2022, we had good discussions (initiated by Scott Aaronson), mainly by email, with the Google team and especially with John Martinis and Sergio Boixo, on various aspects of their experiment and on some of our ﬁndings. Overall, the Google team welcomed us (and others) in putting their experiment under careful scrutiny, even though they had been aware since 2019 of the ﬁrst author’s concerns [10] about the reli- ability of the Google 2019 Sycamore paper. The discussions were easy going and in good atmosphere. A video discussion between Boixo, Kalai, Rinott and Shoham in October 2021 was especially fruitful. The basic methodol- ogy of trying to pass the data from the Google experiment through various “sanity tests” was overall agreed upon by the Google team although at times we had diﬀerent interpretations of speciﬁc ﬁndings. There was a single issue regarding our concerns about the calibration process in connection with the 15 2019 Google video [9], discussed in late 2021, that led to a somewhat more tense exchange, and in particular where John Martinis criticized attempts to pass judgment on the Google 2019 experiment based on a short video meant for general audience, rather than on the paper itself. Still, also in this case, we had a useful discussion which ultimately shed some light on aspects of the Google 2019 experiment. Overall, we were not shy to ask for data and information, and in a few cases the Google team was not shy to decline our requests. Most of our requests for data and information were met, although arguably, most of our requests should have been part of the supplementary material of the Google paper to start with. Since December 2020 until November 2021, we conducted in parallel a useful discussion with Chao-Yang Lu and members of the Gaussian boson sampling team from USTC along with members of the Google team and other researchers regarding the issue of spooﬁng and k-point correlations of the Gaussian boson sampling experiment. (This discussion was also initiated by Scott Aaronson.) We also had a brief email correspondence with the USTC team that replicated the Google 2019 experiment. 4.5 The discrepancy with the outcomes of J¨ulich Re- search Center team One little mystery that was settled following our discussions with the Google team involved the data from the J¨ulich team. The amplitudes for 8 large cir- cuits with 39, 42 and 43 qubits, were computed by researchers from J¨ulich’s research center using their own powerful simulators and (later, after the pub- lication of the paper) when the Google team checked the computation with their own simulators, in some cases the amplitudes were diﬀerent, and in one case also the estimated ﬁdelity was lower than expected. Initially, the thought was that the (small) diﬀerences between the outcomes were caused by nu- merical diﬀerences between the simulators. However, this was not the reason. It turned out that the problem was not a numerical diﬀerence between the simulators but rather using a (slightly) diﬀerent calibration method. (See Section 3.) In the simulations coming from J¨ulich the researchers used cir- cuits and measurements corresponding to the correct experiment, but the Google team slightly improved the parameters of the circuits a few days af- ter running the experiment, and the J¨ulich team used an older version. This 16 explains the discrepancy in the amplitudes and why the ﬁdelity estimated with the J¨ulich simulation was lower for one of the circuits. In other words, the amplitudes were computed for precisely the same experiment, but the calibration, namely the formalization of the experiment as a quantum circuit (the description of the “native” gates) was slightly diﬀerent. Remark (October, 2021): It turned out that for seven out of the eight J¨ulich circuits the new calibration was applied and only for one circuit the J¨ulich team had used the earlier calibration. 4.6 Data from other sources It could have been valuable to perform some of our statistical analysis on data from other quantum computers and we try to obtain similar data (descrip- tion of calibrated circuits, bitstrings and amplitude ﬁles) from other quantum computers especially those from IBM. The experiment closest to the Google one (that we know about) that was carried out on an IBM quantum com- puters is by Kim et al. [16]. We were told, however, that in a boarder sense, many of IBM’s benchmarks are based on random circuits of some sort. Also by now, Google, NASA, and various other groups have powerful sim- ulators that also allow to introduce noise. This provides further opportunity for our statistical analysis, which, so far, we have not used. 4.7 The USTC replication In June 2021 scientists from USTC [29] described a close replication of the Google 2019 experiment with 56-qubit depth 20 quantum circuits on their Zuchongzhi quantum computer. Later in September 2021, they de- scribed an improved experiment with 60-qubit depth-24 circuits [32]. The team from USTC shared the description of the circuits (in Matlab) for each n = 15, 18, 21, 24, ..., 54, 56, and it seems that they sampled (or shared) less bitstrings than Google did; for example, for n = 15 they sampled 200K bit- strings, whereas the Google team sampled 500K. Carefully studying the data from these experiments (and perhaps asking for additional data) would be an interesting direction for further research. 17 4.8 Discussions over scientiﬁc blogs There were useful discussions over a few scientiﬁc blogs regarding the Google 2019 experiment, and especially over Aaronson’s blog “Shtetl Optimized” (SO) and also over Kalai’s blog “Combinatorics and More.” Reference [34] contains links to useful blog discussion mainly at the time of the Google supremacy announcements. For example, in a discussion over SO (December 2020) a commentator “Till” asserted [27] that the calibration process actually adjusted the deﬁnition of the circuit to the device. Namely, that the random circuits were generated with “standard gates” but the deﬁnition of 2-gates was modiﬁed in the calibration process (in the same way for all circuits). (See Section 3.) This was new for us and for several others who thought that the calibration process is a physical process performed on the Sycamore device. Aaronson’s own conclusion of the discussion was: “So, my summary would be that yes, there’s a calibration phase, and the 2-qubit gates used depend on the outcome of that phase, but there’s still a clear separation enforced between the calibration phase and the actual running of the QC.” 5 Two proposals for future experiments 5.1 A proposal for blind experiments In [22] we proposed the following protocol for an evaluation of Google’s ex- periment (here we describe a small variant). First, Google will share the parameters of their calibration. Next, independent scientists will prepare several programs (circuits) for Sycamore to be run with about n qubits, for which computing the sampling probabilities should be a task that takes sev- eral months (on a classical computer). These programs will be sent to Google for implementation. Google will send back the implemented programs and large samples that they produce in a short time, which is assumed to pre- clude computation of the relevant amplitudes of the calibrated circuits. Using classical computers the scientists will take their time and compute the set of amplitudes for each calibrated circuit. They will then evaluate the relation between those amplitudes and the samples they received. Such a protocol is likely to be relevant to other quantum supremacy demonstrations which are being pursued very actively these days. Overall the Google team welcomed the idea of conducting blind experiments in the future and agreed to our speciﬁc proposed protocol. 18 Remark: Over an Internet discussion (on Aaronson’s blog, Feb. 2020) Craig Gidney (a member of the Google team) [25] asserted that various ex- periments for the early pattern that were initially considered computationally hard, turned out to be easier than expected and they served and could fur- ther serve as sort of blind tests for the 2019 experiment. The Google team made a similar comment in our email correspondence in January 2021: “We did publish a lot of data that no-one has been able to analyze yet. I hope that eventually some of this data will be analyzed, which will be an inter- esting conﬁrmation. Analyzing 39 and 40 qubit has actually become easy in the meantime.” These remarks motivated our request (h) in Section 4.2. As we mentioned, the vast progress in simulation techniques has made it pos- sible to examine the ﬁdelity estimators and they were veriﬁed by Kalachev, Panteleev, and Yung, in [11]. 5.2 Testing calibration strategies by other groups on Google’s Sycamore data As a follow up to our request for the calibration programs (item (i) in Section 4.2) that could not be met, we raised the possibility of sharing raw data gathered from 1-qubit and 2-qubit circuits which formed the input for the calibration programs needed to describe the “native gates”. This will give an opportunity to other groups to test their own calibration methods (without revealing Google’s full proprietary calibration system). The Google team asserted that this might be possible in principle, but it will require some considerable eﬀorts, and it is not clear if the 2019 data was kept. It might be better and easier to implement this proposal on more recent or even future Sycamore experiments. Given the central place of the calibration stage in NISQ experiments this direction could be valuable in its own right. 6 On the evaluation of the Google 2019 ex- periment In this section we discuss the overall evaluation of the Google 2019 exper- iment. We start with ﬁve central questions regarding the experiment and continue with a list of conﬁrmations, refutations, concerns, and weaknesses of the experiment. Some issues discussed in this section are not directly re- 19 lated to the data and information gathering that we discussed in the previous sections, and we also refer to important works by other groups. 6.1 Five central questions As we already mentioned, several ingredients of the Google Sycamore experi- ment represent major progress in human ability to control quantum systems. When we bring the Sycamore 2019 to test, there are some central problems that come up: a) Were the sampling tasks achieved as claimed? In our opinion, the ﬁndings of our paper [22] show that the answer is negative. A “sampling task” in the traditional sense, namely reaching (approximately) a sample from some known-in-advanced distribution (or even known-after- the-fact distribution) was not achieved in the 2019 experiment. The empirical distribution is quite diﬀerent from the Google basic noise model (1) and mov- ing to our more detailed noise model gives only a small improvement. The Google team disagrees with our opinion and notes that they write explicitely in the supplement [4] (around Equation (24)) that they do not necessarily assume the basic noise model (1). b) Are the statistical tools for estimating the ﬁdelity satisfactory? Our study in [22] shows that given the Google noise model, the FXEB- estimator for the ﬁdelity is quite good. (We oﬀer some improvements mainly when the number of qubits is small.) It is an interesting question if this estimator measures ﬁdelity when we are far away from the Google noise model, see [8]. We note that it might be legitimate to base quantum supremacy claims on the hardness to sample with a high value of FXEB, without referring to the question of whether a speciﬁc sampling task was achieved or if FXEB genuinely estimates the ﬁdelity, see [2, 8]. c) Are the claims regarding the estimated ﬁdelity of the samples valid? Here, for example, the Google team claimed to achieve for random circuit sampling (full circuits) with m = 14, for n = 12 with estimated (FXEB) ﬁ- delity 0.3694, for n = 22 with estimated ﬁdelity 0.165, n = 32 with estimated 20 ﬁdelity 0.071. Even leaving aside larger depth and larger numbers of qubits, the question is to what extent we can regard this achievement as solid. We note that the speciﬁc computations, regarding the (FXEB) value of the sam- ples given the description of the (calibrated) circuits were veriﬁed, and the remaining question is about the reliability of the overall claim. d) Are the claims regarding the predictive power of the a-priori ﬁdelity estimations valid? (Here we refer to Formula (3).) Here, for example, the a-priori prediction based on Formula (3) (Formula (77) in [4]) for m = 14 and n = 12, 22, 32 are 0.386, 0.1554, 0.062 respec- tively. Question d) is whether we have solid evidence for the claim that the simple formula (77) (and the statistical independence assumptions allowing it) provides accurate prediction of the FXEB estimator. Here, the speciﬁc computations were not checked (the individual ﬁdelities were not shared, see Section 4 item (f)), but they are supported by our approximate computa- tions based on averaged values of the ﬁdelities. As in the previous item, the remaining question is about the reliability of Google’s overall claim and here the excellent prediction power of the a priori ﬁdelity estimation raised concerns about it (item 10 in Section 6.2, below). e) Are the claims regarding the classical diﬃculty of the sampling task correct? Regarding Google’s “quantum supremacy” claims, a very short summary is that by now classical algorithms are ten orders of magnitude faster than those used in the Google paper and hence the speed-up is ten orders of magnitude lower than Google’s fantastic claims. See, e.g., [19, 18, 31, 11, 12, 20]. The Google paper claims that their ultimate task that required 200 seconds for the quantum computer would require 10,000 years on a powerful supercomputer. With the new algorithms the task can be done in a matter of seconds. A few days after the publication of the Google supremacy paper re- searchers from IBM [20] exhibited a theoretical improvement of six orders of magnitudes (albeit on a more powerful supercomputer). In response, the Google team pointed out that their paper [3] did anticipate some progress in classical algorithms: “We expect that lower simulation costs than reported here will eventually be achieved, but we also expect that they will be consis- tently outpaced by hardware improvements on larger quantum processors.” 21 As a matter of fact, the Google team welcomed better algorithms and wrote in [3] that the bitstring samples from all circuits have been archived [5] “to en- courage development and testing of more advanced veriﬁcation algorithms.” While weakening Google’s supremacy claims, the progress in classical algorithms has also led to major support for Google’s claims on items c) and d). In the recent work by Gleb Kalachev, Pavel Panteleev, and Man- Hong Yung, [11] the authors were able to compute the ﬁdelity for samples for which 2019 Google algorithms were too slow to handle. The ﬁdelity values that Gleb, Pavel, and Man-Hong computed agree perfectly with Google’s (77) prediction. This gives a very strong support to an aﬃrmative answer to items c) and d). Another major support for both claims c) and d) are the 2021 replications by a group in USTC of the Google 2019 experiments [29, 32]. Yet more support for claims c) and d) came from our own study. In [22] we oﬀered (as a “sanity test”) an alternative way to estimate the ﬁdelity based on a “secondary” component of the theoretical distributions that arises from readout errors. In a subsequent work [23] we checked this alternative estimators for the ﬁdelity and found a good match with the FXEB estimators for n ≤ 30. (See Figure 2.) This work nicely implements Fourier tools. We remark that on many items mentioned in this section we carried out statistical analysis that is not presented here, and on most items there is room for more detailed discussion. This is especially the case for the comparison of the empirical distribution with the noise model (6.1a) and the prediction power of equation (77) (6.1d). Three very concrete questions for circuits with 22 qubits To be completely concrete let us ask three questions about (veriﬁable full) random circuit sampling of a circuit C of the kind discussed in the Google paper with n = 22 qubits and depth m = 14. 1. Can humanity produce at present samples which are good approxima- tion of the Google noise model or any other speciﬁc noise model? 2. Did humanity reach the ability to produce samples for quantum circuit C with FXEB ﬁdelity estimated above 0.15? 3. Did humanity reach the ability to predict, for a quantum circuit C, with good accuracy, the FXEB ﬁdelity estimator based on the ﬁdelity of the individual components of this circuit? 22 The ﬁndings of our paper [22] indicate that the answer to the ﬁrst ques- tion is negative. The Google supremacy paper and subsequent conﬁrmations present a strong case for a positive answer to the other two questions. But there are remaining doubts and concerns that need to be carefully checked, and not enough replications. (We are aware only of the Google experiment itself and the two USTC replications.)2 6.2 Conﬁrmations, refutations, concerns, and weak- nesses Here is a list, without much commentary of weaknesses, concerns, conﬁrma- tions, and refutations of the Google 2019 quantum supremacy experiment. A) Conﬁrmations 1. The computations of the ﬁdelity estimators and related computations reported in the Google paper were conﬁrmed by us up to our own computational limits (and several other groups conﬁrmed it well beyond what we did). 2. Major conﬁrmation by Kalachev, Panteleev, and Yung [11] for the Google 2019 paper’s experimental claims in the “supremacy region.” This gives a strong support to the reliability of the 2019 Google exper- iment. 3. Our readout errors/Fourier study largely supports (for n ≤ 30) the experimental claims and the experiment’s reliability [23], see Figure 2. 4. A successful replication was announced and published by a group from USTC [29, 32]. 5. The Google ﬁdelity estimators and statistical methodology are overall good [22]. 2For comparison, after the ﬁrst Wright brothers ﬂight on December 17, 1903 (120 feet) there were subsequent ﬂights: 852 feet on January 1, 1904; 1,297 feet on February 23 1904; 1,810 feet on May 14, 1904; 4,781 feet on September 23, 1904; 5,360 feet on December 17, 1904; 12,540 feet on January 1, 1905; 24 miles on May 22, 1905; 34 miles on July 4, 1905; 39 miles on October 5, 1905; 56 miles on November 23, 1905, and 63 miles on December 31, 1905. (Source: GPT-3) 23 B) Refutations 6. Google’s fantastic supremacy claims were largely (but not fully) refuted [19, 18, 31, 11, 12, 20]. (See Section 6.1 above.) 7. The data does not ﬁt the Google noise model [22] or any other speciﬁc (See Section 6.1 above.); the Google team disagrees on this model. point. AB) Mixed conﬁrmation/refutation 8. The boson sampling quantum supremacy experiments give a mixed signal. They were regarded as independent conﬁrmation of quantum supremacy using a very diﬀerent quantum device, however the com- putational hardness claims are in tension with old (Kalai and Kindler [15]) and new [21, 28] works. C) Concerns 9. The Google experiment represents amazing leaps in human ability to control quantum devices. The non-gradual advances are surprising and there isn’t a clear technological reason behind them.3 We note that here we refer to the ability expressed by random circuit sam- pling experiments. The remarkable progress in building the hardware itself (by several groups in academia and industry including the Google team) was gradual. 10. The prediction power for the a priori ﬁdelity estimation (Formula (77) in [4]) raised concerns about reliability. This matter is raised and dis- cussed in [14] and is also mentioned in [22]. The concern regarding the prediction of linear cross-entropy ﬁdelity es- timators based on the ﬁdelities of individual components, appears to be the strongest challenge to date to the reliability of the Google experiment and the USTC replication. There are two important remarks regarding this concern: 3The Google supremacy experiment was often compared to IBM’s “Deep Blue” victory against the chess world champion Kasparov; in that case, however, progress was gradual and earlier chess programs could already win against chess grand-masters. 24 First, the Google team and other researchers regard this prediction power as one of the main achievements of the Google experiment. Second, [11] con- ﬁrmed these predictions for many circuits for which the Google team could not even compute the amplitudes. (This is a sort of “blind test” veriﬁcation.) 11. When you move away from Google’s noise model, the ﬁdelity estimators may not capture the actual ﬁdelity of the quantum system. (See e.g. [8].) 12. Some aspects of the calibration process may seem overly surprising (this represents some preliminary analysis that we presented in our email discussions in November 2021). 13. There are no suﬃcient replications of the supremacy experiment and veriﬁcations of the experimental ﬁndings even in the 12-40 qubit range. (Neither by other groups nor by the Google team itself.) D) Weaknesses 14. While the experiment was characterized as performing a sampling com- putational task, the empirical distribution was not compared to the noise model, and the experimental samples are too small to allow such comparison (directly) when n > 14. 15. The supremacy claims were based on a single algorithm. The Google team itself developed better algorithms to related computational tasks. When the Google team found an improved type of algorithms (around May 2019) they switched to a diﬀerent class of circuits for which the new algorithm did not apply without suﬃcient evidence that similar improvements could not be made for them as well. (Later, as we men- tioned, improvements were also achieved to the new class of circuits.) 16. The results were not presented to the academic community before pub- lication (e.g., as an arXiv publication), and careful review of the ex- periment prior to publication may have been insuﬃcient. 17. The main ﬁdelity estimator is not unbiased and can be improved [22]. 25 18. Moving to a new pattern ABCDCDAB weakened the power of the extrapolation argument. For the new pattern no experiments for cir- cuits with less than 53 qubits were conducted. (This was a weakness in planning the experiment although some of the ﬁdelity predictions are now conﬁrmed [11].) 19. Improvements of the calibration process were interlaced (namely oc- curred at the same time) with the experiment. The description of the calibration process in Google’s popular video [9] may be in tension with the claim that there was a clear separation between the calibration stage and the experiment itself. 20. The calibration process itself somewhat weakens the claim for a “pro- grammable quantum computer”. (This concern was raised by a com- mentator “Till” in an Internet discussion [27].) 21. The eﬀect of the calibration process is very large (it is not a “ﬁne tuning” of some sort). 22. The precise programs for the calibration process are commercial secrets. (However, the main innovation in calibration was the use of XEB, and that code is open sourced.) 23. The levels of transparency and of documentation of the Google 2019 experiment seem insuﬃcient. Of course, some of the items above are controversial, some of them are rather minor, and also many of these items are related. Items (15), (16) are a-priori weaknesses that turned out to be crucial (refutation (6) above). The recent conﬁrmations (2) indicate that weakness (18) may have no baring on the assertions for large circuits, but we still regard it as a weakness in the planning of the experiment. (The Google team disagrees with us.) We also note that expectations expressed in the Google 2019 paper [3] seem overly optimistic: “Quantum processors have thus reached the regime of quantum supremacy. We expect that their computational power will con- tinue to grow at a double-exponential rate: the classical cost of simulating a quantum circuit increases exponentially with computational volume, and hardware improvements will probably follow a quantum-processor equiva- lent of Moore’s law, doubling this computational volume every few years.” (Compare with later analysis in [33, 8].) 26 Figure 2: The decay of Fourier–Walsh contributions as predicted by the theory (bold black curve)([7, 23]) and as demonstrated by samples of a 16-qubit Sycamore computer (ten random circuits). 27 7 Where we are (September 2022) Now that the Google 2019 experiment data-gathering stage needed for our study has largely come to an end, here is a brief summary of where we are. 1. We compared the empirical distribution with Google’s noise model and with other noise models. Some of this analysis is discussed in [22], and a few other ﬁndings were discussed with the Google team. 2. In [22] we oﬀered an alternative way (that could serve as a “sanity test”) to estimate the ﬁdelity based on a linear cross entropy test for a “sec- ondary” component of the theoretical distributions that arises from readout errors. In a subsequent work in progress [23] we used Fourier methods to check this alternative estimator for the ﬁdelity, and found a good match with the primary linear cross entropy ﬁdelity estimators for n ≤ 30. 3. We carried out some statistical analysis of the eﬀect of the calibration process. Continuing in this direction, we plan, among other things, to study a proposal of John Martinis (made on Aaronson’s blog in December 2020 [26], as well as in our discussions) for improving the calibration. 4. We plan to extend some of our statistical studies to the new data from June 2022, and especially to the data for the patch circuits. There is also some remaining analysis to be carried out for the earlier data. We also plan to extend our statistical study to data coming from other NISQ computers and from simulators that incorporate the eﬀect of noise. 5. There is one important data item that is still not publicly available, namely, the ﬁdelities of individual components that are used in Formula (3) (Formula (77) in [4]). This information consists only of a few hundred real numbers and we hope that the Google team will share it in the near future. The Google team provided useful approximate versions for formula (77) (such as Formula 4) but we found out that for the study of patch circuits the full data is required. 6. (October 2022.) On the theoretical side, we recently realized that the remarkable work of Gao, Kalinowski, Chou, Lukin, Barak, and Choi [8] is relevant to some of the concerns discussed above. Analysis based on [8] strengthens concern (10) and sheds some doubts on our conﬁrmation (3) (the second item above). 28 Acknowledgements Research supported by ERC grant 834735. We thank many colleagues includ- ing members of the Google team for helpful discussion and Carsten Voelk- mann for many corrections and helpful suggestions. References [1] S. Aaronson, Why Google’s quantum supremacy milestone matters, Opinion, New York Times Oct. 30, 2019. [2] S. Aaronson and S. Gunn, On the classical hardness of spooﬁng linear cross-entropy benchmarking, 2019, arXiv:1910.12085. [3] F. Arute et al., Quantum supremacy using a programmable supercon- ducting processor, Nature, 574 (2019), 505–510. [4] F. Arute et al. (2019), Supplementary information for “Quantum supremacy using a programmable superconducting processor,” 2019, arXiv:1910.11333. [5] F. Arute et al., Supplementary data for “Quantum supremacy 2019, using https://datadryad.org/stash/dataset/doi:10.5061/dryad.k6t1rj8. superconducting programmable processor,” a [6] S. Boixo, S. V. Isakov, V. N. Smelyanskiy, and H. Neven, Simulation of low-depth quantum circuits as complex undirected graphical models, arXiv:1712.05384, 2017. [7] S. Boixo, V. N. Smelyanskiy, and H. Neven, Fourier analysis of sampling from noisy chaotic quantum circuits, 2017, arXiv:1708.01875. [8] X. Gao, M. Kalinowski, C.-N. Chou, M. D. Lukin, B. Barak, and S. Choi, Limitations of Linear Cross-Entropy as a Measure for Quantum Advantage, 2021, arXiv:2112.01657. [9] Google, Demonstrating Quantum Supremacy, 4:42 minutes video for a large audience, Oct. 23, 2019. URL: https://youtu.be/-ZNEzzDcllU . 29 [10] S. Irani (Moderator), Supremacy Panel, Hebrew University of Jerusalem, Dec. 2019. Participants: D. Aharonov, B. Barak, A. Bouland, G. Kalai, S. Aaronson, S. Boixo, and U. Vazirani. (Video.) URL: https://youtu.be/ Yb7uIGBynU . [11] G. Kalachev, P. Panteleev, and M.-H. Yung, Multi-tensor contraction for XEB veriﬁcation of quantum circuits, 2021, arXiv:2108.05665. [12] G. Kalachev, P. Panteleev, P. F. Zhou, and M.-H. Yung, Classical sampling of random quantum circuits with bounded ﬁdelity, 2021, arXiv:2112.15083. [13] G. Kalai, The Google quantum supremacy demo, videotaped lecture, December 2019. URL: https://youtu.be/p18P1y8GD9U. [14] G. Kalai, The argument against Quantum Computers, the quantum laws of nature, and Google’s supremacy claims, in: The Intercontinen- tal Academia Laws: ’Rigidity and Dynamics,’ (M. J. Hannon and E. Z. Rabinovici (ed.)) Proceedings of the ICA Workshops 2018&2019, Singa- pore and Birmingham, World Scientiﬁc, to appear. arXiv:2008.05188. [15] G. Kalai and G. Kindler, Gaussian noise sensitivity and BosonSampling, 2014, arXiv:1409.3093. [16] J.-S. Kim, L. S. Bishop, A. D. Corcoles, S. Merkel, J. A. Smolin, S. Sheldon, Hardware-eﬃcient random circuits to classify noise in a multi- qubit system, Phys. Rev. A 104 (2021), 022609. [17] C. Neill et al., A blueprint for demonstrating quantum supremacy with superconducting qubits. Science 360 (2018), 195-199 [18] F. Pan, K. Chen, and P. Zhang, Solving the sampling problem of the Sycamore quantum supremacy circuits, Phys. Rev. Lett. 129 (2022), 090502 [19] F. Pan and P. Zhang, Simulating the Sycamore quantum supremacy circuits, 2021, arXiv:2103.03074 [20] E. Pednault, J. A. Gunnels, G. Nannicini, L. Horesh, and R. Wisni- eﬀ, Leveraging secondary storage to simulate deep 54-qubit Sycamore circuits, 2019, arXiv:1910.09534. 30 [21] A. S. Popova and A. N. Rubtsov, Cracking the quantum advantage threshold for Gaussian boson sampling, 2021, arXiv:2106.01445. [22] Y. Rinott, T. Shoham, and G. Kalai, Statistical aspects of the quantum supremacy demonstration, Statistical Science 37 (2022), 322–347. [23] Y. Rinott, T. Shoham, and G. Kalai, Quantum Advantage Demonstra- tions via Random Circuit Sampling: Fourier Expansion and Statistics, manuscript in progress. [24] Shtetl Optimized, A comment by S. Boixo , Dec. 2020. URL: https://scottaaronson.blog/?p=5159#comment-1870651 . [25] Shtetl Optimized, A comment by C. Gidney, Feb. 2020. URL: https://scottaaronson.blog/?p=4608#comment-1830545 . [26] Shtetl Optimized, A comment by J. Martinis, Dec. 2020. URL: https://scottaaronson.blog/?p=5159#comment-1870734 . [27] Shtetl Optimized, A comment by “Till”, Dec. 2020. URL: https://scottaaronson.blog/?p=5159#comment-1869118 . [28] B. Villalonga, M. Yuezhen Niu, L. Li, H. Neven, J. C. Platt, V. N. Smelyanskiy, and S. Boixo, Eﬃcient approximation of experimental Gaussian boson sampling, 2021, arXiv:2109.11525. [29] Y. Wu et al., Strong quantum computational advantage using a super- conducting quantum processor. Phys. Rev. Lett. 127 (2021), 180501. [30] H.-S. Zhong et al., Quantum computational advantage using photons, Science 370 (2020), 1460–1463. [31] Y. Zhou, E. M. Stoudenmire, and X. Waintal, What limits the simula- tion of quantum computers?, 2020, arXiv:2002.07730. [32] Q. Zhu et al., Quantum Computational Advantage via 60-Qubit 24- Cycle Random Circuit Sampling, Science Bulletin, Volume 67 (2022), 240-245. arXiv:2109.03494. [33] A. Zlokapa, S. Boixo, and D. Lidar, Boundaries of quantum supremacy via random circuit sampling, 2020, arXiv:2005.02464. 31 [34] Various links to useful discussions over scientiﬁc blogs of the Google’s 2019 paper and related developments. (SO stands for Shtetl Optimized, Scott Aaronson’s blog; CaM stands for Combinatorics and More, Gil Kalai’s blog; WoT stands for Windows on Theory, Boaz Barak’s blog. 1. Scott’s Supreme Quantum Supremacy FAQ! SO, Sept 23, 2019. https://scottaaronson.blog/?p=4317. 2. Quantum computers: amazing progress IBM), and claims extraordinary & supremacy (Google). CaM, https://gilkalai.wordpress.com/2019/09/23/quantum-computers- amazing-progress-google-ibm-and-extraordinary-but-probably- false-supremacy-claims-google/ probably 23, Sept but (Google false 2019. 3. The story of Poincar´e and his friend the baker, CaM, Oct. 13, 2019. https://gilkalai.wordpress.com/2019/10/13/the-story-of- poincare-and-his-friend-the-baker/ 4. Quantum supremacy: the gloves are oﬀ, SO, Oct. 23, 2019/ https://scottaaronson.blog/?p=4372. 5. Boaz’s inferior classical inferiority FAQ, WoT, Oct. 24, 2021. https://windowsontheory.org/2019/10/24/boazs-inferior-classical- inferiority-faq/ 6. Gil’s Collegial Quantum Supremacy Skepticism FAQ, CaM Nov. 13, 2019, https://gilkalai.wordpress.com/2019/11/13/gils-collegial- quantum-supremacy-skepticism-faq/ 7. My “Quantum Supremacy: Tour, 2020 World https://scottaaronson.blog/?p=4608 Speaking Skeptics Were Wrong” 2020 SO, Feb. 17, 8. Quantum supremacy, now with BosonSampling, SO, Dec. 3, 2020, https://scottaaronson.blog/?p=5122. 9. Chinese BosonSampling experiment: the gloves are oﬀ, SO, Dec. 16, 2020. https://scottaaronson.blog/?p=5159 Gil Kalai, Hebrew University of Jerusalem and Reichman University, gil.kalai@gmail.com Yosef Rinott, Hebrew University of Jerusalem, yosef.rinott@mail.huji.ac.il Tomer Shoham, Hebrew University of Jerusalem, tomer.shohamm@gmail.com 32","['c', 'p', 'u', 'supremacy', 'claim', 'datum', 'documentation', 'discussion', 'rinott', 'tomer', 'abstract', 'nature', 'publish', 'paper', 'describe', 'exper', 'imental', 'work', 'take', 'place', 'paper', 'claim', 'supremacy', '53qubit', 'quantum', 'computer', 'author', 'involve', 'longterm', 'project', 'study', 'various', 'statistical', 'aspect', 'experiment', 'particular', 'try', 'gather', 'vant', 'datum', 'information', 'reconstruct', 'verify', 'part', 'supremacy', 'experiment', 'base', 'classical', 'computation', 'require', 'heavy', 'computation', 'put', 'datum', 'statistical', 'analysis', 'conclude', 'part', 'relate', 'gathering', 'datum', 'information', 'need', 'study', 'experiment', 'docu', 'ment', 'describe', 'available', 'datum', 'information', 'experiment', 'result', 'plan', 'introduction', 'paper', 'quantum', 'supremacy', 'use', 'programmable', 'superconduct', 'e', 'processor', 'claim', 'processor', 'perform', 'certain', 'computation', 'second', 'stateoftheart', 'classical', 'supercomputer', 'take', 'accord', 'team', 'estimate', 'approxi', 'mately', 'year', 'perform', 'computation', 'computer', 'perform', 'sample', 'task', 'generate', 'run', 'dom', 'bitstring', 'length', 'considerable', 'noise', 'certain', 'discrete', 'probability', 'distribution', 'support', 'bitstring', 'sample', 'task', 'refer', 'random', 'circuit', 'sample', 'rcs', 'short', 'announcement', 'quantum', 'supremacy', 'compare', 'landmark', 'technological', 'achievement', 'various', 'writer', 'see', 'eg', 'invention', 'motoroperate', 'airplane', 'sputnik', 'landing', 'moon', 'well', 'landmark', 'scientiﬁc', 'achievement', 'fermi', 'demonstration', 'nuclear', 'chain', 'reaction', 'discovery', 'ligo', 'detection', 'gravitational', 'wave', 'team', 'science', 'technology', 'claim', 'sample', 'task', 'compute', 'photonic', 'take', 'year', 'perform', 'classical', 'supercomputer', 'computer', 'also', 'require', 'second', 'task', 'task', 'refer', 'team', 'repeat', 'pling', 'gbs', 'short', 'rcs', 'experiment', 'superconducte', 'processor', 'zuchongzhi', 'qubit', 'depth', 'claim', 'achieve', 'even', 'strong', 'form', 'quantum', 'advantage', 'compare', 'experiment', 'experiment', 'represent', 'large', 'leap', 'various', 'aspect', 'human', 'ability', 'control', 'noisy', 'quantum', 'system', 'example', 'team', 'previously', 'report', 'experiment', 'qubit', 'leap', 'especially', 'impressive', 'term', 'dimension', 'hilbert', 'space', 'represent', 'state', 'computer', 'dimension', 'dimension', 'claim', 'progress', 'several', 'group', 'exhibit', 'classical', 'algorithm', 'order', 'magnitude', 'fast', 'use', 'paper', 'achieve', 'example', 'study', 'concentrate', 'aspect', 'experiment', 'already', 'mention', 'announcement', 'regard', 'major', 'scientiﬁc', 'technological', 'event', 'give', 'evi', 'dence', 'strong', 'church', 'ture', 'thesis', 'violate', 'describe', 'ironclad', 'refutation', 'claim', 'scientist', 'include', 'quantum', 'computation', 'possible', 'less', 'importance', 'quantum', 'supremacy', 'consider', 'major', 'intermediate', 'step', 'exhibit', 'experimentally', 'quantum', 'errorcorrection', 'code', 'need', 'build', 'large', 'quantum', 'computer', 'announcement', 'quantum', 'supremacy', 'stir', 'great', 'deal', 'enthusiasm', 'scientist', 'general', 'public', 'garner', 'signiﬁcant', 'media', 'attention', 'substantial', 'figure', 'price', 'bitcoin', 'usd', 'period', 'week', 'source', 'coindesk', 'impact', 'example', 'follow', 'medium', 'attention', 'surround', 'leaking', 'supremacy', 'claim', 'value', 'bitcoin', 'digital', 'currency', 'sharply', 'drop', 'reasonable', 'possibility', 'give', 'potential', 'eﬀect', 'quantum', 'computer', 'safety', 'digital', 'currency', 'supremacy', 'claim', 'cause', 'drop', 'put', 'supremacy', 'claim', 'scrutiny', 'paper', 'appear', 'month', 'early', 'brieﬂy', 'post', 'server', 'become', 'publicly', 'available', 'announcement', 'supremacy', 'claim', 'ﬁrstnamed', 'author', 'researcher', 'raise', 'various', 'concern', 'aspect', 'claim', 'month', 'later', 'author', 'initiate', 'become', 'longterm', 'project', 'study', 'various', 'statistical', 'aspect', 'experiment', 'particular', 'try', 'gather', 'relevant', 'datum', 'information', 'reconstruct', 'verify', 'part', 'supremacy', 'experiment', 'base', 'classical', 'computation', 'require', 'heavy', 'computation', 'carry', 'heavy', 'computation', 'cloud', 'put', 'cap', 'dollar', 'spending', 'also', 'perform', 'several', 'sanity', 'test', 'experiment', 'structure', 'paper', 'section', 'provide', 'brief', 'background', 'experiment', 'describe', 'various', 'type', 'circuit', 'use', 'experiment', 'chronol', 'ogy', 'various', 'sycamore', 'experiment', 'report', 'team', 'section', 'describe', 'nature', 'calibration', 'process', 'section', 'describe', 'request', 'datum', 'team', 'datum', 'provide', 'detail', 'relate', 'experiment', 'nisq', 'supremacy', 'experiment', 'section', 'raise', 'proposal', 'future', 'experiment', 'section', 'discuss', 'regard', 'main', 'question', 'evaluation', 'experiment', 'list', 'conﬁrmation', 'refutation', 'concern', 'weakness', 'section', 'brieﬂy', 'discuss', 'study', 'supremacy', 'claim', 'brief', 'background', 'draft', 'assume', 'knowledge', 'cross', 'ﬁdelity', 'estima', 'tor', 'formula', 'predict', 'ﬁdelity', 'circuit', 'ﬁdelity', 'component', 'give', 'brief', 'summary', 'topic', 'experiment', 'base', 'building', 'quantum', 'com', 'puter', 'circuit', 'superconducte', 'qubit', 'perform', 'round', 'computation', 'computation', 'carry', 'gate', 'end', 'computation', 'qubit', 'measure', 'lead', 'string', 'zero', 'one', 'length', 'ultimate', 'experiment', 'involve', 'gate', 'gate', 'experiment', 'team', 'produce', 'sample', 'vector', 'length', 'circuit', 'qubit', 'describe', 'probability', 'distribution', 'pcx', 'fact', 'describe', 'vector', 'vector', 'length', 'complex', 'amplitude', 'vector', 'associated', 'ampli', 'tude', 'zx', 'pcx', 'quantum', 'computer', 'enable', 'sample', 'accord', 'probability', 'distribution', 'pcx', 'considerable', 'amount', 'noise', 'large', 'classical', 'simulation', 'enable', 'compute', 'amplitude', 'hence', 'probability', 'pcx', 'supremacy', 'claim', 'base', 'fact', 'classical', 'simula', 'tion', 'quickly', 'become', 'infeasible', 'n', 'grow', 'basic', 'noise', 'model', 'noisy', 'sample', 'sycamore', 'device', 'actually', 'produce', 'φ', 'ﬁdelity', 'parameter', 'roughly', 'describe', 'quality', 'sample', 'ﬁdelity', 'precise', 'meaning', 'term', 'actual', 'noisy', 'quantum', 'process', 'carry', 'sycamore', 'device', 'base', 'noise', 'model', 'fact', 'distribution', 'instance', 'porter', 'distribution', 'paper', 'describe', 'statistic', 'call', 'linear', 'cross', 'estimator', 'denote', 'fxeb', 'quantum', 'computer', 'produce', 'sequence', '˜x', 'n', 'sample', '˜x', 'following', 'linear', 'cross', 'estimator', 'fxeb', 'ﬁdelity', 'compute', 'i1', 'compute', 'fxeb', 'require', 'knowledge', 'pcx', 'sample', 'bitstring', 'supremacy', 'claim', 'base', 'also', 'follow', 'priori', 'pre', 'diction', 'ﬁdelity', 'circuit', 'base', 'probability', 'error', 'individual', 'component', 'cid89', 'eg', '−', 'eg', 'cid89', 'eq', 'g1', 'set', 'gate', 'operate', 'single', 'qubit', 'set', 'gate', 'operate', 'qubit', 'q', 'set', 'qubit', 'gate', 'term', 'eg', 'formula', 'refer', 'probability', 'error', 'ﬁdelity', 'individual', 'gate', 'qubit', 'q', 'eq', 'probability', 'readout', 'error', 'measure', 'qubit', 'q', 'supremacy', 'paper', 'make', 'crucial', 'claim', 'regard', 'ultimate', '53qubit', 'sample', 'ﬁdelity', 'sample', 'produce', 'sample', 'similar', 'ﬁdelity', 'require', 'year', 'supercomputer', 'claim', 'regard', 'value', 'argument', 'rely', 'extrapo', 'lation', 'argument', 'ingredient', 'ingredient', 'experiment', 'classically', 'tractable', 'regime', 'regime', 'prob', 'ability', 'distribution', 'compute', 'classical', 'computer', 'performance', 'quantum', 'computer', 'test', 'directly', 'gredient', 'theoretical', 'formula', 'predict', 'ﬁdelity', 'accord', 'paper', 'ﬁdelity', 'entire', 'circuit', 'closely', 'agree', 'prediction', 'formula', 'formula', 'deviation', 'percent', 'around', 'report', 'experiment', 'classically', 'tractable', 'regime', 'include', 'one', 'carry', 'simpliﬁed', 'circuit', 'easy', 'simu', 'late', 'classical', 'computer', 'experiment', 'support', 'claim', 'prediction', 'give', 'formula', 'ﬁdelity', 'indeed', 'robust', 'apply', '53qubit', 'circuit', 'supremacy', 'regime', 'claim', 'regard', 'classical', 'diﬃculty', 'team', 'mainly', 'lie', 'extrapolation', 'run', 'time', 'speciﬁc', 'use', 'also', 'rely', 'computational', 'complexity', 'support', 'assertion', 'task', 'hand', 'asymptotically', 'diﬃcult', 'remark', 'team', 'propose', 'pretty', 'good', 'approximation', 'formula', 'base', 'average', 'ﬁdelitie', '−', '00016g11', '−', '00062g21', '−', 'type', 'circuit', 'experiment', 'circuit', 'use', 'supremacy', 'experiment', 'fol', 'lowing', 'structure', 'qubit', 'arrange', 'planar', 'grid', 'single', 'qubit', 'identiﬁe', 'coordinate', 'qubit', 'circuit', 'type', 'layer', 'type', 'layer', 'consist', 'act', 'pair', 'neighboring', 'qubit', 'layer', 'gate', 'layer', 'randomly', 'choose', '1gate', 'act', 'qubit', 'layer', 'consist', 'programmable', 'ingredient', 'experiment', 'change', 'circuit', 'circuit', 'layer', 'ﬁxe', 'periment', 'accord', 'certain', 'pattern', 'pattern', 'efgh', 'use', 'experiment', 'conduct', 'new', 'pat', 'use', 'produce', 'sample', 'supremacy', 'namely', 'circuit', 'require', 'huge', 'classical', 'computation', 'letter', 'e', 'correspond', 'ﬁxed', 'set', 'act', 'e', 'parallel', 'qubit', 'convention', 'circuit', 'small', 'number', 'qubit', 'regard', 'involve', 'qubit', 'circuit', 'pattern', 'mean', 'ﬁrst', 'apply', 'layer', 'random', '1gate', 'qubit', 'apply', 'accord', 'e', 'next', 'apply', 'layer', 'random', '1gate', 'qubit', 'apply', 'accord', 'continue', 'periodic', 'manner', 'new', 'pattern', 'abcdcdab', 'base', 'new', 'type', 'layer', 'period', 'length', 'depth', 'circuit', 'refer', 'number', 'layer', 'example', 'layer', 'circuit', 'pattern', 'efgh', 'depth', 'e', 'g', 'h', 'e', 'g', 'h', 'e', 'g', 'h', 'e', 'type', 'circuit', 'experiment', 'rely', 'type', 'circuit', 'type', 'full', 'circuit', 'pattern', 'efgh', 'b', 'elide', 'circuit', 'pattern', 'efgh', 'patch', 'circuit', 'pattern', 'efgh', 'type', 'simpliﬁed', 'form', 'type', 'quick', 'algo', 'rithm', 'compute', 'amplitude', 'value', 'particular', 'patch', 'circuit', 'consist', 'separate', 'circuit', 'disjoint', 'set', 'qubit', 'paper', 'base', 'run', 'quantum', 'computer', 'circuit', 'type', 'b', 'compute', 'ﬁdelity', 'estimator', 'base', 'amplitude', 'also', 'preliminary', 'stage', 'calibration', 'base', 'exper', 'iment', 'circuit', 'determine', 'precise', 'adjustment', 'experimental', '2gate', 'adjustment', 'circuit', 'type', 'b', 'see', 'section', 'far', 'understand', 'initial', 'plan', 'team', 'compute', 'empirical', 'ﬁdelitie', 'circuit', 'type', 'b', 'number', 'qubit', 'circuit', 'type', 'number', 'qubit', 'use', 'information', 'estimate', 'ﬁdelity', 'circuit', 'type', 'qubit', 'depth', 'however', 'team', 'discover', 'eﬃcient', 'classical', 'algorithm', 'circuit', 'type', 'discovery', 'full', 'cir', 'cuit', 'pattern', 'efgh', 'also', 'refer', 'veriﬁable', 'full', 'circuit', 'new', 'discover', 'team', 'relate', 'tensor', 'net', 'work', 'method', 'later', 'lead', 'important', 'discovery', 'direction1', 'use', 'team', 'compute', 'amplitude', 'perimental', 'bitstring', 'type', 'circuit', 'subsequently', 'purpose', 'demonstrate', 'supremacy', 'team', 'move', 'diﬀerent', 'architecture', 'base', 'new', 'pattern', 'abcdcdab', 'hard', 'simulate', 'classical', 'computer', 'lead', 'follow', 'new', 'type', 'circuit', 'full', 'circuit', 'pattern', 'circuit', 'also', 'refer', 'supremacy', 'full', 'circuit', 'e', 'elide', 'circuit', 'pattern', 'patch', 'circuit', 'pattern', 'abcdcdab', 'architecture', 'team', 'produce', 'sample', 'depth', 'simulator', 'available', 'team', 'powerful', 'enough', 'compute', 'amplitude', 'check', 'ﬁdelity', 'circuit', 'type', 'circuit', 'pattern', 'require', 'run', 'calibration', 'process', 'circuit', 'base', 'new', 'architecture', 'lead', 'new', 'adjustment', 'circuit', 'new', 'pattern', 'abcdcdab', 'calibration', 'process', 'test', 'less', 'qubit', 'chronology', 'sycamore', 'experiment', 'prior', 'development', 'sycamore', 'quantum', 'computer', 'team', 'attempt', 'develop', '72qubit', 'chip', 'call', 'bristlecone', 'team', 'point', 'also', 'implement', 'relate', 'tensor', 'network', 'method', 'due', 'diﬃcultie', 'team', 'later', 'proceed', 'development', 'sycamore', 'computer', 'eﬀective', 'qubit', 'list', 'experiment', 'sycamore', 'quantum', 'computer', 'clude', 'early', 'experiment', 'base', 'early', 'calibration', 'method', 'experiment', 'fresh', 'new', 'random', 'circuit', 'generate', 'namely', 'improve', 'ment', 'calibration', 'procedure', 'certain', 'experiment', 'base', 'improve', 'ﬁdelity', 'circuit', 'use', 'early', 'experiment', 'experiment', 'take', 'day', 'perform', 'ﬁrst', 'ex', 'periment', 'base', 'improved', 'method', 'calibration', 'experiment', 'circuit', 'pattern', 'efgh', 'last', 'experiment', 'base', 'new', 'architecture', 'pattern', 'early', 'experiment', 'date', 'full', 'circuit', 'pattern', 'efgh', 'number', 'qubit', 'date', 'full', 'circuit', 'pattern', 'efgh', 'number', 'qubit', 'date', 'full', 'circuit', 'pattern', 'efgh', 'number', 'qubit', 'date', 'full', 'circuit', 'pattern', 'efgh', 'number', 'qubit', 'date', 'full', 'circuit', 'pattern', 'efgh', 'number', 'qubit', 'experiment', 'paper', 'base', 'date', 'patch', 'elide', 'full', 'circuit', 'pattern', 'efgh', 'number', 'qubit', 'depth', 'day', 'later', 'sycamore', 'quantum', 'computer', 'produce', 'qubit', 'sample', 'patch', 'elide', 'full', 'circuit', 'also', 'depth', 'date', 'number', 'qubit', 'depth', 'type', 'circuit', 'patch', 'elide', 'full', 'pattern', 'nature', 'calibration', 'process', 'sycamore', 'quantum', 'computer', 'systematic', 'deviation', 'ideal', 'circuit', 'describe', 'calibration', 'process', 'account', 'small', 'systematic', 'error', 'experimental', 'circuit', 'compare', 'random', 'circuit', 'represent', 'calibration', 'process', 'crucial', 'combine', 'eﬀect', 'systematic', 'error', 'slash', 'ﬁdelity', 'important', 'account', 'calibration', 'process', 'refer', 'paper', 'method', 'base', 'multiple', 'run', 'quantum', 'circuit', 'adjust', 'deﬁnition', 'experimental', 'circuit', 'certain', 'systematic', 'form', 'noise', 'greatly', 'reduce', 'calibration', 'adjustment', 'carry', 'simultaneously', 'experiment', 'patch', 'elide', 'veriﬁable', 'full', 'circuit', 'easy', 'identify', 'program', 'random', 'circuit', 'also', 'use', 'qsim', 'ﬁle', 'modiﬁe', 'calibration', 'purpose', 'example', 'consider', 'single', 'act', 'qubit', 'ideal', 'description', 'circuit', 'describe', 'program', 'circuit', 'follow', 'cirqmoment', 'phi052359877559', 'cirqgridqubit3', 'sometimes', 'refer', 'standard', 'θ', 'π2', 'π6', 'show', 'part', 'program', 'describe', 'modiﬁcation', 'speciﬁc', 'gate', 'calibration', 'consist', 'add', 'description', 'circuit', 'ﬁxed', 'rotation', 'qubit', 'involve', 'particular', 'add', 'single', 'qubit', 'rotation', 'cirqgridqubit', 'cirqrz', 'cirqgridqubit', 'add', 'single', 'qubit', 'rotation', 'appearance', 'involve', 'value', 'appear', 'single', 'qubit', 'rotation', 'well', 'adjustment', 'deﬁnition', 'compute', 'base', 'many', 'experiment', 'circuit', 'qubit', 'precise', 'algorithm', 'computation', 'datum', 'outcome', 'experiment', 'addition', 'act', 'qubit', 'b', 'modiﬁed', 'cirqmoment', 'phi04859467238431821', 'cirqgridqubit3', 'calibrate', 'value', '2gate', 'act', 'qubit', 'circuit', 'time', 'calibrate', 'refer', 'native', 'value', 'θ', 'φ', 'replace', 'value', 'standard', 'gate', 'original', 'circuit', 'π2', 'remark', 'note', 'adjustment', 'involve', 'qubit', 'b', 'depend', 'multiple', 'run', 'quantum', 'circuit', 'volve', 'qubit', 'also', 'note', 'additional', 'action', 'account', 'fact', 'certain', 'component', 'sycamore', 'chip', 'degrade', 'time', 'internet', 'discussion', 'sergio', 'boixo', 'team', 'refer', 'figure', 's30', 'assert', 'speciﬁc', 'case', 'experiment', 'use', 'sycamore', 'standard', 'gate', 'instead', 'native', 'gate', 'halve', 'ﬁdelity', 'however', 'claim', 'refer', 'twogate', 'adjustment', 'onegate', 'adjustment', 'move', 'original', 'circuit', 'standard', 'gate', 'slash', 'ﬁdelity', 'eﬀect', 'gate', 'adjustment', 'represent', 'worstcase', 'situation', 'small', 'small', 'circuit', 'sergio', 'boixo', 'later', 'clariﬁe', 'later', 'experiment', 'supremacy', 'experiment', 'team', 'develop', 'method', 'carry', 'calibration', 'use', 'physical', 'control', 'device', 'rather', 'use', 'modiﬁcation', 'deﬁnition', 'circuit', 'later', 'development', 'relevant', 'experiment', 'study', 'datum', 'documentation', 'discussion', 'supplementary', 'datum', 'paper', 'leak', 'paper', 'publish', 'nature', 'month', 'later', 'datum', 'experiment', 'find', 'team', 'upload', 'datum', 'server', 'ﬁve', 'date', 'follow', 'publication', 'date', 'file', 'bitstring', 'many', 'experimental', 'circuit', 'upload', 'sample', 'consist', 'bitstring', 'upload', 'bitstring', 'several', 'number', 'qubit', 'miss', 'date', 'miss', 'n16', 'addition', 'bitstring', 'patch', 'circuit', 'miss', 'qsim', 'ﬁle', 'description', 'corresponding', 'experimental', 'circuit', 'well', 'python', 'ﬁle', 'compute', 'amplitude', 'miss', 'bitstring', 'full', 'elide', 'circuit', 'upload', 'amplitude', 'ﬁle', 'circuit', 'include', 'patch', 'circuit', 'roughly', 'circuit', 'upload', 'additional', 'datum', 'include', 'information', 'noise', 'individual', 'component', 'sycamore', 'quantum', 'computer', 'upload', 'amplitude', 'ﬁle', 'circuit', 'include', 'patch', 'circuit', 'put', 'team', 'upload', 'circuit', 'amplitude', 'compute', 'j¨ulich', 'j¨ulich', 'research', 'readout', 'error', 'datum', 'upload', 'j¨ulich', 'amplitude', 'upload', 'bitstre', 'ﬁle', 'bitstring', 'circuit', 'qsim', 'python', 'ﬁle', 'patch', 'circuit', 'ﬁle', 'still', 'miss', 'request', 'datum', 'early', 'request', 'request', 'team', 'early', 'large', 'sample', 'quantum', 'circuit', 'comparison', 'empirical', 'distribution', 'model', 'bitstring', 'produce', 'quantum', 'computer', 'description', 'circuit', 'c', 'amplitude', 'compute', 'sample', 'circuit', 'full', 'list', 'amplitude', 'refer', 'amplitude', 'bitstring', 'sample', 'e', 'timetable', 'experiment', 'calibration', 'value', 'individual', 'ﬁdelitie', 'eq', 'eg', 'qubit', 'q', 'gate', 'g', 'use', 'formula', 'subsequent', 'request', 'readout', 'error', 'information', 'discuss', 'matter', 'receive', 'useful', 'datum', 'regard', 'readout', 'error', 'shortly', 'ward', 'case', 'receive', 'require', 'datum', 'quickly', 'satisfactory', 'way', 'h', 'amplitude', 'veriﬁable', 'experiment', 'already', 'mention', 'team', 'develop', 'useful', 'algorithm', 'program', 'compute', 'amplitude', 'experiment', 'full', 'cir', 'cuit', 'pattern', 'efgh', 'reason', 'move', 'diﬀerent', 'pattern', 'supremacy', 'experiment', 'use', 'program', 'depth14', 'circuit', 'require', 'several', 'hour', 'circuit', 'supercomputer', 'useful', 'reliabilitycontrol', 'experiment', 'propose', 'amplitude', 'compute', 'remain', 'full', 'circuit', 'least', 'speciﬁc', 'one', 'optimization', 'program', 'move', 'datum', 'circuit', 'correction', 'deﬁnition', 'cir', 'cuit', 'raw', 'datum', 'gather', 'circuit', 'request', 'datum', 'provide', 'team', 'researcher', 'press', 'embargo', 'respond', 'issue', 'paper', 'appear', 'already', 'mention', 'paper', 'pub', 'lishe', 'nature', 'supplementary', 'raw', 'datum', 'contain', 'bitstring', 'description', 'circuit', 'form', 'program', 'compute', 'amplitude', 'many', 'circuit', 'use', 'experiment', 'bitstring', 'circuit', 'add', 'request', 'datum', 'circuit', 'datum', 'patch', 'circuit', 'upload', 'server', '53qubit', 'depth14', 'circuit', 'mention', 'datum', 'sample', 'team', 'promise', 'supply', 'amplitude', 'put', 'part', 'experiment', 'sample', 'upload', 'amplitude', 'many', 'circuit', 'additional', 'circuit', 'upload', 'amplitude', 'compute', 'circuit', 'external', 'facility', 'j¨ulich', 'require', 'approval', 'j¨ulich', 'team', 'upload', 'regard', 'request', 'full', 'list', 'amplitude', 'team', 'inform', 'full', 'list', 'amplitude', 'discard', 'deed', 'large', 'value', 'huge', 'amount', 'datum', 'moreover', 'circuit', 'pattern', 'efgh', 'team', 'develop', 'around', 'algorithm', 'compute', 'amplitude', 'sample', 'require', 'compute', 'amplitude', 'algorithm', 'use', 'case', 'regard', 'large', 'sample', 'team', 'supply', 'sample', 'produce', 'quantum', 'computer', 'comparison', 'empirical', 'distribution', 'noise', 'model', 'team', 'refer', 'already', 'comparison', 'early', 'experiment', 'qubit', 'regard', 'timetable', 'detail', 'experiment', 'calibra', 'tion', 'request', 'e', 'receive', 'brief', 'timetable', 'useful', 'detail', 'regard', 'ﬁnal', 'experiment', 'ﬁve', 'early', 'one', 'see', 'section', 'regard', 'item', 'h', 'team', 'tell', 'form', 'computation', 'require', 'lot', 'human', 'computational', 'eﬀort', 'however', 'use', 'new', 'algorithm', 'amplitude', 'veriﬁable', 'exper', 'iment', 'compute', 'result', 'give', 'strong', 'support', 'claim', 'prediction', 'regard', 'linear', 'cross', 'entropy', 'sample', 'regard', 'item', 'team', 'inform', 'share', 'full', 'proprietary', 'calibration', 'system', 'require', 'many', 'year', 'development', 'need', 'move', 'datum', 'correction', 'deﬁnition', 'circuit', 'point', 'main', 'innovation', 'calibration', 'program', 'make', 'public', 'boixo', 'give', 'useful', 'python', 'program', 'split', 'patch', 'circuit', 'patch', 'regard', 'item', 'individual', 'gate', 'qubit', 'ﬁdelitie', 'ask', 'several', 'time', 'early', 'turn', 'recently', 'data', 'crucial', 'study', 'circuit', 'discussion', 'team', 'good', 'discussion', 'initiate', 'scott', 'aaronson', 'mainly', 'email', 'team', 'especially', 'sergio', 'boixo', 'various', 'aspect', 'experiment', 'ﬁnding', 'overall', 'team', 'welcome', 'put', 'experiment', 'careful', 'scrutiny', 'even', 'aware', 'author', 'concern', 'reli', 'ability', 'sycamore', 'paper', 'discussion', 'easy', 'go', 'good', 'atmosphere', 'video', 'discussion', 'kalai', 'shoham', 'especially', 'fruitful', 'basic', 'methodol', 'ogy', 'try', 'pass', 'datum', 'experiment', 'various', 'sanity', 'test', 'overall', 'agree', 'team', 'time', 'diﬀerent', 'interpretation', 'speciﬁc', 'ﬁnding', 'single', 'issue', 'regard', 'concern', 'calibration', 'process', 'connection', 'video', 'discuss', 'late', 'lead', 'somewhat', 'tense', 'exchange', 'particular', 'criticize', 'attempt', 'pass', 'judgment', 'experiment', 'base', 'short', 'video', 'mean', 'general', 'audience', 'rather', 'paper', 'still', 'also', 'case', 'useful', 'discussion', 'ultimately', 'shed', 'light', 'aspect', 'experiment', 'overall', 'shy', 'ask', 'datum', 'information', 'case', 'team', 'shy', 'decline', 'request', 'request', 'datum', 'information', 'meet', 'arguably', 'request', 'part', 'supplementary', 'material', 'paper', 'start', 'conduct', 'parallel', 'useful', 'discussion', 'member', 'sample', 'team', 'member', 'team', 'researcher', 'regard', 'issue', 'spooﬁng', 'kpoint', 'correlation', 'sample', 'experiment', 'discussion', 'also', 'initiate', 'aaronson', 'also', 'brief', 'email', 'correspondence', 'team', 'replicate', 'experiment', 'discrepancy', 'outcome', 'j¨ulich', 'search', 'center', 'team', 'little', 'mystery', 'settle', 'follow', 'discussion', 'team', 'involve', 'datum', 'j¨ulich', 'team', 'amplitude', 'large', 'cir', 'cuit', 'qubit', 'compute', 'researcher', 'j¨ulich', 'research', 'center', 'use', 'powerful', 'simulator', 'later', 'pub', 'lication', 'paper', 'team', 'check', 'computation', 'simulator', 'case', 'amplitude', 'diﬀerent', 'case', 'also', 'estimate', 'ﬁdelity', 'low', 'expect', 'initially', 'thought', 'small', 'diﬀerence', 'outcome', 'cause', 'merical', 'diﬀerence', 'simulator', 'however', 'reason', 'turn', 'problem', 'numerical', 'diﬀerence', 'simulator', 'rather', 'use', 'slightly', 'diﬀerent', 'calibration', 'method', 'see', 'section', 'simulation', 'come', 'j¨ulich', 'researcher', 'use', 'cir', 'cuit', 'measurement', 'correspond', 'correct', 'experiment', 'team', 'slightly', 'improve', 'parameter', 'circuit', 'day', 'ter', 'run', 'experiment', 'j¨ulich', 'team', 'use', 'old', 'version', 'explain', 'discrepancy', 'amplitude', 'ﬁdelity', 'estimate', 'j¨ulich', 'simulation', 'low', 'circuit', 'word', 'amplitude', 'compute', 'precisely', 'experiment', 'calibration', 'namely', 'formalization', 'experiment', 'quantum', 'circuit', 'description', 'native', 'gate', 'slightly', 'diﬀerent', 'remark', 'turn', 'j¨ulich', 'circuit', 'new', 'calibration', 'apply', 'circuit', 'j¨ulich', 'team', 'use', 'early', 'calibration', 'datum', 'source', 'valuable', 'perform', 'statistical', 'analysis', 'datum', 'quantum', 'computer', 'try', 'obtain', 'similar', 'datum', 'descrip', 'tion', 'calibrate', 'circuit', 'bitstring', 'amplitude', 'ﬁle', 'quantum', 'computer', 'especially', 'experiment', 'close', 'one', 'know', 'carry', 'quantum', 'com', 'puter', 'tell', 'however', 'board', 'sense', 'many', 'benchmark', 'base', 'random', 'circuit', 'sort', 'also', 'various', 'group', 'powerful', 'sim', 'ulator', 'also', 'allow', 'introduce', 'noise', 'provide', 'opportunity', 'statistical', 'analysis', 'far', 'use', 'replication', 'scientist', 'describe', 'close', 'replication', 'experiment', 'depth', 'quantum', 'circuit', 'quantum', 'computer', 'later', 'scribe', 'improve', 'experiment', '60qubit', 'depth24', 'circuit', 'team', 'share', 'description', 'circuit', 'seem', 'sample', 'share', 'less', 'bitstring', 'example', 'sample', 'bit', 'string', 'team', 'sample', 'carefully', 'study', 'datum', 'experiment', 'perhaps', 'ask', 'additional', 'datum', 'interesting', 'direction', 'research', 'discussion', 'scientiﬁc', 'blog', 'useful', 'discussion', 'scientiﬁc', 'blog', 'regard', 'experiment', 'especially', 'blog', 'shtetl', 'optimize', 'also', 'blog', 'combinatoric', 'reference', 'contain', 'link', 'useful', 'blog', 'discussion', 'mainly', 'time', 'supremacy', 'announcement', 'example', 'discussion', 'commentator', 'assert', 'calibration', 'process', 'actually', 'adjust', 'deﬁnition', 'circuit', 'device', 'namely', 'random', 'circuit', 'generate', 'standard', 'gate', 'deﬁnition', 'modiﬁe', 'calibration', 'process', 'way', 'circuit', 'see', 'section', 'new', 'several', 'think', 'calibration', 'process', 'physical', 'process', 'perform', 'sycamore', 'device', 'aaronson', 'conclusion', 'discussion', 'summary', '’', 'calibration', 'phase', 'gate', 'use', 'depend', 'outcome', 'phase', '’', 'still', 'clear', 'separation', 'enforce', 'calibration', 'phase', 'actual', 'running', 'proposal', 'future', 'experiment', 'proposal', 'blind', 'experiment', 'propose', 'following', 'protocol', 'evaluation', 'periment', 'describe', 'small', 'variant', 'first', 'google', 'share', 'parameter', 'calibration', 'independent', 'scientist', 'prepare', 'several', 'program', 'circuit', 'sycamore', 'run', 'qubit', 'compute', 'sample', 'probability', 'task', 'take', 'sev', 'eral', 'month', 'classical', 'computer', 'program', 'send', 'google', 'implementation', 'google', 'send', 'back', 'implement', 'program', 'large', 'sample', 'produce', 'short', 'time', 'assume', 'pre', 'clude', 'computation', 'relevant', 'amplitude', 'calibrate', 'circuit', 'use', 'classical', 'computer', 'scientist', 'take', 'time', 'compute', 'set', 'amplitude', 'calibrate', 'circuit', 'evaluate', 'relation', 'amplitude', 'sample', 'receive', 'protocol', 'likely', 'relevant', 'quantum', 'supremacy', 'demonstration', 'pursue', 'actively', 'day', 'overall', 'team', 'welcome', 'idea', 'conduct', 'blind', 'experiment', 'future', 'agree', 'speciﬁc', 'propose', 'protocol', 'remark', 'internet', 'discussion', 'member', 'team', 'assert', 'various', 'periment', 'early', 'pattern', 'initially', 'consider', 'computationally', 'hard', 'turn', 'easy', 'expect', 'serve', 'fur', 'ther', 'serve', 'sort', 'blind', 'test', 'experiment', 'team', 'make', 'similar', 'comment', 'email', 'correspondence', 'publish', 'lot', 'datum', 'noone', 'able', 'analyze', 'yet', 'hope', 'eventually', 'datum', 'analyze', 'inter', 'esting', 'conﬁrmation', 'analyze', 'qubit', 'actually', 'become', 'easy', 'meantime', 'remark', 'motivate', 'request', 'h', 'section', 'mention', 'vast', 'progress', 'simulation', 'technique', 'make', 'pos', 'sible', 'examine', 'ﬁdelity', 'estimator', 'veriﬁe', 'testing', 'calibration', 'strategy', 'group', 'sycamore', 'datum', 'follow', 'request', 'calibration', 'program', 'item', 'section', 'meet', 'raise', 'possibility', 'share', 'raw', 'datum', 'gather', 'circuit', 'form', 'input', 'calibration', 'program', 'need', 'describe', 'native', 'gate', 'give', 'opportunity', 'group', 'test', 'calibration', 'method', 'reveal', 'full', 'proprietary', 'calibration', 'system', 'team', 'assert', 'possible', 'principle', 'require', 'considerable', 'eﬀort', 'clear', 'datum', 'keep', 'well', 'easy', 'implement', 'proposal', 'recent', 'even', 'future', 'sycamore', 'experiment', 'give', 'central', 'place', 'calibration', 'stage', 'nisq', 'experiment', 'direction', 'valuable', 'right', 'evaluation', 'ex', 'periment', 'section', 'discuss', 'overall', 'evaluation', 'exper', 'iment', 'start', 'ﬁve', 'central', 'question', 'regard', 'experiment', 'continue', 'list', 'conﬁrmation', 'refutation', 'concern', 'weakness', 'experiment', 'issue', 'discuss', 'section', 'directly', 'late', 'datum', 'information', 'gathering', 'discuss', 'previous', 'section', 'also', 'refer', 'important', 'work', 'group', 'central', 'question', 'already', 'mention', 'several', 'ingredient', 'ment', 'represent', 'major', 'progress', 'human', 'ability', 'control', 'quantum', 'system', 'bring', 'sycamore', 'test', 'central', 'problem', 'come', 'sample', 'task', 'achieve', 'claim', 'opinion', 'ﬁnding', 'paper', 'show', 'answer', 'negative', 'sample', 'task', 'traditional', 'sense', 'namely', 'reach', 'approximately', 'sample', 'knowninadvanced', 'distribution', 'even', 'knownafter', 'thefact', 'distribution', 'achieve', 'experiment', 'empirical', 'distribution', 'quite', 'diﬀerent', 'basic', 'noise', 'model', 'mov', 'ing', 'detailed', 'noise', 'model', 'give', 'small', 'improvement', 'team', 'disagree', 'opinion', 'note', 'write', 'explicitely', 'supplement', 'equation', 'necessarily', 'assume', 'basic', 'noise', 'model', 'b', 'statistical', 'tool', 'estimate', 'ﬁdelity', 'satisfactory', 'study', 'show', 'give', 'noise', 'model', 'fxeb', 'estimator', 'ﬁdelity', 'quite', 'good', 'oﬀer', 'improvement', 'mainly', 'number', 'qubit', 'small', 'interesting', 'question', 'estimator', 'measure', 'ﬁdelity', 'far', 'away', 'noise', 'model', 'see', 'note', 'legitimate', 'base', 'quantum', 'supremacy', 'claim', 'hardness', 'sample', 'high', 'value', 'fxeb', 'refer', 'question', 'speciﬁc', 'sample', 'task', 'achieve', 'fxeb', 'genuinely', 'estimate', 'ﬁdelity', 'see', 'c', 'claim', 'regard', 'estimate', 'ﬁdelity', 'sample', 'valid', 'example', 'team', 'claim', 'achieve', 'random', 'circuit', 'sample', 'full', 'circuit', 'estimate', 'fxeb', 'ﬁ', 'delity', 'estimate', 'ﬁdelity', 'estimate', 'ﬁdelity', 'even', 'leave', 'aside', 'large', 'depth', 'large', 'number', 'qubit', 'question', 'extent', 'regard', 'achievement', 'solid', 'note', 'speciﬁc', 'computation', 'regard', 'fxeb', 'value', 'ple', 'give', 'description', 'calibrate', 'circuit', 'veriﬁe', 'remain', 'question', 'reliability', 'overall', 'claim', 'claim', 'regard', 'predictive', 'power', 'apriori', 'ﬁdelity', 'estimation', 'valid', 'refer', 'formula', 'example', 'apriori', 'prediction', 'base', 'formula', 'formula', 'respec', 'tively', 'question', 'solid', 'evidence', 'claim', 'simple', 'formula', 'statistical', 'independence', 'assumption', 'allow', 'provide', 'accurate', 'prediction', 'fxeb', 'estimator', 'speciﬁc', 'computation', 'check', 'individual', 'ﬁdelitie', 'share', 'see', 'section', 'item', 'support', 'approximate', 'computa', 'tion', 'base', 'average', 'value', 'ﬁdelitie', 'previous', 'item', 'remain', 'question', 'reliability', 'overall', 'claim', 'excellent', 'prediction', 'power', 'priori', 'ﬁdelity', 'estimation', 'raise', 'concern', 'item', 'section', 'e', 'claim', 'regard', 'classical', 'diﬃculty', 'sample', 'task', 'correct', 'regard', 'claim', 'short', 'summary', 'classical', 'algorithm', 'order', 'magnitude', 'fast', 'use', 'paper', 'hence', 'speedup', 'order', 'magnitude', 'low', 'fantastic', 'claim', 'see', 'paper', 'claim', 'ultimate', 'task', 'require', 'second', 'quantum', 'computer', 'require', 'year', 'powerful', 'supercomputer', 'new', 'algorithm', 'task', 'matter', 'second', 'day', 'publication', 'supremacy', 'paper', 'searcher', 'exhibit', 'theoretical', 'improvement', 'order', 'magnitude', 'powerful', 'supercomputer', 'response', 'team', 'point', 'paper', 'anticipate', 'progress', 'classical', 'algorithm', 'expect', 'low', 'simulation', 'cost', 'report', 'eventually', 'achieve', 'also', 'expect', 'consis', 'tently', 'outpace', 'hardware', 'improvement', 'large', 'quantum', 'processor', 'matter', 'fact', 'team', 'welcome', 'well', 'algorithm', 'write', 'bitstring', 'sample', 'circuit', 'archive', 'courage', 'development', 'testing', 'advanced', 'veriﬁcation', 'algorithm', 'weaken', 'supremacy', 'claim', 'progress', 'classical', 'algorithm', 'also', 'lead', 'major', 'support', 'claim', 'item', 'recent', 'work', 'man', 'author', 'able', 'compute', 'ﬁdelity', 'sample', 'google', 'algorithm', 'slow', 'handle', 'ﬁdelity', 'value', 'gleb', 'pavel', 'compute', 'agree', 'perfectly', 'prediction', 'give', 'strong', 'support', 'aﬃrmative', 'answer', 'item', 'major', 'support', 'claim', 'replication', 'group', 'experiment', 'support', 'claim', 'c', 'come', 'study', 'oﬀere', 'sanity', 'test', 'alternative', 'way', 'estimate', 'ﬁdelity', 'base', 'secondary', 'component', 'theoretical', 'distribution', 'arise', 'readout', 'error', 'subsequent', 'work', 'check', 'alternative', 'estimator', 'ﬁdelity', 'find', 'good', 'match', 'fxeb', 'estimator', 'see', 'figure', 'work', 'nicely', 'implement', 'fouri', 'tool', 'remark', 'many', 'item', 'mention', 'section', 'carry', 'statistical', 'analysis', 'present', 'item', 'room', 'detailed', 'discussion', 'especially', 'case', 'comparison', 'empirical', 'distribution', 'noise', 'model', '61a', 'prediction', 'power', 'equation', 'concrete', 'question', 'circuit', 'qubit', 'completely', 'concrete', 'let', 'ask', 'question', 'veriﬁable', 'full', 'random', 'circuit', 'sampling', 'circuit', 'c', 'kind', 'discuss', 'paper', 'qubit', 'depth', 'humanity', 'produce', 'present', 'sample', 'good', 'approxima', 'tion', 'noise', 'model', 'speciﬁc', 'noise', 'model', 'humanity', 'reach', 'ability', 'produce', 'sample', 'fxeb', 'ﬁdelity', 'estimate', 'humanity', 'reach', 'ability', 'predict', 'good', 'accuracy', 'fxeb', 'ﬁdelity', 'estimator', 'base', 'ﬁdelity', 'individual', 'component', 'circuit', 'ﬁnding', 'paper', 'indicate', 'answer', 'ﬁrst', 'tion', 'negative', 'supremacy', 'paper', 'subsequent', 'conﬁrmation', 'present', 'strong', 'case', 'positive', 'answer', 'question', 'remain', 'doubt', 'concern', 'need', 'carefully', 'check', 'enough', 'replication', 'aware', 'experiment', 'replications2', 'conﬁrmation', 'refutation', 'concern', 'weak', 'ness', 'list', 'much', 'commentary', 'weakness', 'concern', 'tion', 'refutation', 'supremacy', 'experiment', 'conﬁrmation', 'computation', 'ﬁdelity', 'estimator', 'related', 'computation', 'report', 'paper', 'conﬁrme', 'computational', 'limit', 'several', 'group', 'conﬁrme', 'well', 'major', 'conﬁrmation', 'paper', 'experimental', 'claim', 'supremacy', 'region', 'give', 'strong', 'support', 'reliability', 'iment', 'readout', 'errorsfouri', 'study', 'largely', 'support', 'experimental', 'claim', 'experiment', 'reliability', 'see', 'figure', 'successful', 'replication', 'announce', 'publish', 'group', 'ﬁdelity', 'estimator', 'statistical', 'methodology', 'overall', 'good', 'comparison', 'brother', 'ﬂight', 'foot', 'subsequent', 'ﬂight', 'foot', 'foot', 'foot', 'foot', 'foot', 'foot', 'mile', 'mile', 'mile', 'mile', 'mile', 'source', 'gpt3', 'b', 'refutation', 'supremacy', 'claim', 'largely', 'fully', 'refute', 'see', 'section', 'datum', 'ﬁt', 'noise', 'model', 'speciﬁc', 'see', 'section', 'team', 'disagree', 'model', 'point', 'mixed', 'conﬁrmationrefutation', 'sample', 'quantum', 'supremacy', 'experiment', 'give', 'mixed', 'signal', 'regard', 'independent', 'conﬁrmation', 'supremacy', 'use', 'diﬀerent', 'quantum', 'device', 'however', 'com', 'putational', 'hardness', 'claim', 'tension', 'old', 'kalai', 'kindl', 'new', 'work', 'c', 'concern', 'experiment', 'represent', 'amazing', 'leap', 'human', 'ability', 'control', 'quantum', 'device', 'nongradual', 'advance', 'surprising', 'clear', 'technological', 'reason', 'them3', 'note', 'refer', 'ability', 'express', 'pling', 'experiment', 'remarkable', 'progress', 'build', 'hardware', 'several', 'group', 'academia', 'industry', 'include', 'team', 'gradual', 'prediction', 'power', 'priori', 'ﬁdelity', 'estimation', 'formula', 'raise', 'concern', 'reliability', 'matter', 'raise', 'cuss', 'also', 'mention', 'concern', 'regard', 'prediction', 'linear', 'crossentropy', 'ﬁdelity', 'timator', 'base', 'ﬁdelitie', 'individual', 'component', 'appear', 'strong', 'challenge', 'date', 'reliability', 'experiment', 'replication', 'important', 'remark', 'regard', 'concern', 'supremacy', 'experiment', 'often', 'compare', 'deep', 'blue', 'victory', 'chess', 'world', 'case', 'however', 'progress', 'gradual', 'early', 'chess', 'program', 'already', 'win', 'chess', 'grandmaster', 'first', 'team', 'researcher', 'regard', 'prediction', 'power', 'main', 'achievement', 'experiment', 'second', 'con', 'ﬁrme', 'prediction', 'many', 'circuit', 'team', 'even', 'compute', 'amplitude', 'sort', 'blind', 'test', 'veriﬁcation', 'move', 'away', 'noise', 'model', 'ﬁdelity', 'estimator', 'capture', 'actual', 'ﬁdelity', 'quantum', 'system', 'see', 'aspect', 'calibration', 'process', 'seem', 'overly', 'surprising', 'represent', 'preliminary', 'analysis', 'present', 'email', 'discussion', 'suﬃcient', 'replication', 'supremacy', 'experiment', 'veriﬁcation', 'experimental', 'ﬁnding', 'even', 'qubit', 'range', 'group', 'team', 'weakness', 'experiment', 'characterize', 'perform', 'sampling', 'com', 'putational', 'task', 'empirical', 'distribution', 'compare', 'noise', 'model', 'experimental', 'sample', 'small', 'allow', 'comparison', 'directly', 'supremacy', 'claim', 'base', 'single', 'team', 'develop', 'well', 'algorithm', 'relate', 'computational', 'task', 'team', 'find', 'improved', 'type', 'algorithm', 'around', 'switch', 'diﬀerent', 'class', 'circuit', 'new', 'apply', 'suﬃcient', 'evidence', 'similar', 'improvement', 'make', 'well', 'later', 'man', 'tione', 'improvement', 'also', 'achieve', 'new', 'class', 'circuit', 'result', 'present', 'academic', 'community', 'pub', 'lication', 'eg', 'arxiv', 'publication', 'careful', 'review', 'periment', 'prior', 'publication', 'insuﬃcient', 'main', 'ﬁdelity', 'estimator', 'unbiased', 'improve', 'move', 'new', 'pattern', 'weaken', 'power', 'extrapolation', 'argument', 'new', 'pattern', 'experiment', 'cir', 'cuit', 'less', 'qubit', 'conduct', 'weakness', 'plan', 'experiment', 'ﬁdelity', 'prediction', 'conﬁrme', 'improvement', 'calibration', 'process', 'interlace', 'namely', 'oc', 'curre', 'time', 'experiment', 'description', 'calibration', 'process', 'popular', 'video', 'tension', 'claim', 'clear', 'separation', 'calibration', 'stage', 'experiment', 'calibration', 'process', 'somewhat', 'weaken', 'claim', 'pro', 'grammable', 'quantum', 'computer', 'concern', 'raise', 'com', 'mentator', 'internet', 'discussion', 'eﬀect', 'calibration', 'process', 'large', 'ﬁne', 'tuning', 'sort', 'precise', 'program', 'calibration', 'process', 'commercial', 'secret', 'however', 'main', 'innovation', 'calibration', 'use', 'code', 'open', 'sourced', 'level', 'transparency', 'documentation', 'experiment', 'seem', 'insuﬃcient', 'course', 'item', 'controversial', 'rather', 'minor', 'also', 'many', 'item', 'relate', 'item', 'apriori', 'weakness', 'turn', 'crucial', 'refutation', 'recent', 'conﬁrmation', 'indicate', 'weakness', 'baring', 'assertion', 'large', 'circuit', 'still', 'regard', 'weakness', 'planning', 'experiment', 'team', 'disagree', 'also', 'note', 'expectation', 'express', 'paper', 'seem', 'overly', 'optimistic', 'quantum', 'processor', 'thus', 'reach', 'regime', 'supremacy', 'expect', 'computational', 'power', 'con', 'tinue', 'grow', 'doubleexponential', 'rate', 'classical', 'cost', 'simulate', 'quantum', 'circuit', 'increase', 'exponentially', 'computational', 'volume', 'hardware', 'improvement', 'probably', 'follow', 'quantumprocessor', 'equiva', 'law', 'double', 'computational', 'volume', 'year', 'compare', 'later', 'analysis', 'figure', 'decay', 'fouri', 'walsh', 'contribution', 'predict', 'theory', 'bold', 'black', 'curve7', 'demonstrate', 'sample', '16qubit', 'sycamore', 'computer', 'random', 'circuit', 'experiment', 'datagathering', 'stage', 'need', 'study', 'largely', 'come', 'end', 'brief', 'summary', 'compare', 'empirical', 'distribution', 'noise', 'model', 'noise', 'model', 'analysis', 'discuss', 'ﬁnding', 'discuss', 'team', 'oﬀere', 'alternative', 'way', 'serve', 'sanity', 'test', 'estimate', 'ﬁdelity', 'base', 'linear', 'cross', 'test', 'ondary', 'component', 'theoretical', 'distribution', 'arise', 'readout', 'error', 'subsequent', 'work', 'progress', 'use', 'fouri', 'method', 'check', 'alternative', 'estimator', 'ﬁdelity', 'find', 'good', 'match', 'primary', 'linear', 'cross', 'ﬁdelity', 'estimator', 'carry', 'statistical', 'analysis', 'eﬀect', 'calibration', 'process', 'continue', 'direction', 'plan', 'thing', 'study', 'proposal', 'make', 'blog', 'well', 'discussion', 'improve', 'calibration', 'plan', 'extend', 'statistical', 'study', 'new', 'datum', 'especially', 'datum', 'patch', 'circuit', 'also', 'remain', 'analysis', 'carry', 'early', 'datum', 'also', 'plan', 'extend', 'statistical', 'study', 'datum', 'come', 'nisq', 'computer', 'simulator', 'incorporate', 'eﬀect', 'noise', 'important', 'data', 'item', 'still', 'publicly', 'available', 'namely', 'ﬁdelitie', 'individual', 'component', 'use', 'formula', 'formula', 'information', 'consist', 'real', 'number', 'hope', 'team', 'share', 'near', 'future', 'team', 'provide', 'useful', 'approximate', 'version', 'formula', 'formula', 'find', 'study', 'circuit', 'full', 'datum', 'require', 'theoretical', 'side', 'recently', 'realize', 'remarkable', 'work', 'relevant', 'concern', 'discuss', 'analysis', 'base', 'strengthen', 'concern', 'shed', 'doubt', 'conﬁrmation', 'second', 'item', 'acknowledgement', 'research', 'support', 'grant', 'thank', 'many', 'colleague', 'includ', 'e', 'member', 'team', 'helpful', 'discussion', 'mann', 'many', 'correction', 'helpful', 'suggestion', 'reference', 'aaronson', 'milestone', 'matter', 'opinion', 'aaronson', 'classical', 'hardness', 'spooﬁng', 'linear', 'crossentropy', 'benchmarke', 'arute', 'use', 'programmable', 'supercon', 'ducting', 'processor', 'nature', 'arute', 'supplementary', 'information', 'quantum', 'supremacy', 'use', 'programmable', 'superconducte', 'processor', 'arute', 'supplementary', 'datum', 'supremacy', 'use', 'superconducte', 'programmable', 'processor', 'boixo', 'smelyanskiy', 'h', 'neven', 'simulation', 'lowdepth', 'quantum', 'circuit', 'complex', 'undirected', 'graphical', 'model', 'boixo', 'n', 'smelyanskiy', 'h', 'neven', 'fouri', 'analysis', 'sample', 'noisy', 'chaotic', 'circuit', 'limitation', 'linear', 'crossentropy', 'measure', 'quantum', 'advantage', 'google', 'demonstrate', 'supremacy', 'minute', 'video', 'large', 'audience', 'oct', 'url', 'irani', 'moderator', 'supremacy', 'panel', 'hebrew', 'participant', 'aharonov', 'b', 'bouland', 'aaronson', 'boixo', 'contraction', 'veriﬁcation', 'quantum', 'circuit', 'g', 'panteleev', 'p', 'classical', 'sampling', 'random', 'quantum', 'circuit', 'bound', 'ﬁdelity', 'g', 'supremacy', 'demo', 'videotape', 'lecture', 'url', 'g', 'kalai', 'argument', 'quantum', 'computer', 'quantum', 'law', 'nature', 'claim', 'law', 'rigidity', 'dynamic', 'e', 'z', 'rabinovici', 'ed', 'proceeding', 'workshop', 'singa', 'pore', 'scientiﬁc', 'appear', 'g', 'kalai', 'kindl', 'gaussian', 'sensitivity', 'bosonsampling', 'corcole', 'smolin', 'hardwareeﬃcient', 'random', 'circuit', 'classify', 'noise', 'multi', 'qubit', 'system', 'phy', 'rev', 'c', 'neill', 'blueprint', 'demonstrate', 'quantum', 'supremacy', 'superconducte', 'qubit', 'science', 'solve', 'sample', 'problem', 'sycamore', 'supremacy', 'circuit', 'phy', 'rev', 'pan', 'simulate', 'sycamore', 'circuit', 'e', 'pednault', 'gunnel', 'horesh', 'r', 'wisni', 'leverage', 'secondary', 'storage', 'simulate', 'deep', '54qubit', 'sycamore', 'circuit', 'popova', 'rubtsov', 'crack', 'quantum', 'advantage', 'threshold', 'sample', 'shoham', 'kalai', 'statistical', 'aspect', 'supremacy', 'demonstration', 'statistical', 'science', 'shoham', 'tion', 'random', 'circuit', 'sample', 'fouri', 'expansion', 'statistic', 'manuscript', 'progress', 'shtetl', 'optimize', 'comment', 'url', 'shtetl', 'optimize', 'comment', 'url', 'shtetl', 'optimize', 'comment', 'url', 'shtetl', 'optimize', 'comment', 'url', 'b', 'villalonga', 'yuezhen', 'platt', 'smelyanskiy', 'boixo', 'eﬃcient', 'approximation', 'experimental', 'sample', 'computational', 'advantage', 'use', 'super', 'conduct', 'quantum', 'processor', 'phy', 'rev', 'advantage', 'use', 'photon', 'science', 'stoudenmire', 'waintal', 'limit', 'simula', 'tion', 'quantum', 'computer', 'q', 'advantage', 'cycle', 'circuit', 'sample', 'science', 'bulletin', 'volume', 'boixo', 'lidar', 'boundary', 'supremacy', 'circuit', 'sample', 'various', 'link', 'useful', 'discussion', 'scientiﬁc', 'blog', 'paper', 'relate', 'development', 'stand', 'shtetl', 'optimize', 'cam', 'stand', 'combinatoric', 'wot', 'stand', 'window', 'theory', 'boaz', 'faq', 'quantum', 'computer', 'amazing', 'progress', 'claim', 'extraordinary', 'supremacy', 'cam', 'amazingprogressgoogleibmandextraordinarybutprobably', 'falsesupremacyclaimsgoogle', 'probably', 'sept', 'false', 'story', 'poincar´e', 'friend', 'baker', 'cam', 'poincareandhisfriendthebaker', 'quantum', 'supremacy', 'glove', 'boaz', 'inferior', 'classical', 'inferiority', 'faq', 'wot', 'inferiorityfaq', 'collegial', 'supremacy', 'skepticism', 'faq', 'cam', 'nov', 'quantum', 'supremacy', 'tour', 'world', 'speak', 'skeptic', 'wrong', 'quantum', 'supremacy', 'bosonsample', 'chinese', 'bosonsampling', 'experiment', 'glove']"
"Bootstrap Advantage Estimation for Policy Optimization in Reinforcement
  Learning","[{'href': 'http://arxiv.org/abs/2210.07312v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.07312v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-10-13 19:30:43,"2
2
0
2

t
c
O
3
2

]

G
L
.
s
c
[

1
v
2
6
4
3
1
.
0
1
2
2
:
v
i
X
r
a

Artiﬁcial Intelligence-Based Methods for Fusion of
Electronic Health Records and Imaging Data*

Farida Mohsen1, Hazrat Ali1, Nady El Hajj1,2, and Zubair Shah1,*

1College of Science and Engineering, Hamad Bin Khalifa University, Qatar Foundation, 34110 Doha, Qatar
2College of Health and Life Sciences, Hamad Bin Khalifa University, Qatar Foundation, 34110 Doha, Qatar
*Correspondence to: Zubair Shah

ABSTRACT

Healthcare data are inherently multimodal, including electronic health records (EHR), medical images, and multi-omics data.
Combining these multimodal data sources contributes to a better understanding of human health and provides optimal
personalized healthcare. The most important question when using multimodal data is how to fuse them - a ﬁeld of growing
interest among researchers. Advances in artiﬁcial intelligence (AI) technologies, particularly machine learning (ML), enable
the fusion of these different data modalities to provide multimodal insights. To this end, in this scoping review, we focus
on synthesizing and analyzing the literature that uses AI techniques to fuse multimodal medical data for different clinical
applications. More speciﬁcally, we focus on studies that only fused EHR with medical imaging data to develop various AI
methods for clinical applications. We present a comprehensive analysis of the various fusion strategies, the diseases and
clinical outcomes for which multimodal fusion was used, the ML algorithms used to perform multimodal fusion for each clinical
application, and the available multimodal medical datasets. We followed the PRISMA-ScR (Preferred Reporting Items for
Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) guidelines. We searched Embase, PubMed, Scopus,
and Google Scholar to retrieve relevant studies. After pre-processing and screening, we extracted data from 34 studies that
fulﬁlled the inclusion criteria. We found that studies fusing imaging data with EHR are increasing and doubling from 2020
to 2021. In our analysis, a typical workﬂow was observed: feeding raw data, fusing different data modalities by applying
conventional machine learning (ML) or deep learning (DL) algorithms, and ﬁnally, evaluating the multimodal fusion through
clinical outcome predictions. Speciﬁcally, early fusion was the most used technique in most applications for multimodal learning
(22 out of 34 studies). We found that multimodality fusion models outperformed traditional single-modality models for the same
task. Disease diagnosis and prediction were the most common clinical outcomes (reported in 20 and 10 studies, respectively)
from a clinical outcome perspective. Neurological disorders were the dominant category (16 studies). From an AI perspective,
conventional ML models were the most used (19 studies), followed by DL models (16 studies). Multimodal data used in the
included studies were mostly from private repositories (21 studies). Through this scoping review, we offer new insights for
researchers interested in knowing the current state of knowledge within this research ﬁeld.

Introduction

Over the past decade, digitization of health data have grown tremendously with increasing data repositories spanning the
healthcare sectors1. Healthcare data are inherently multimodal, including electronic health records (EHR), medical imaging,
multi-omics, and environmental data. In many applications of medicine, the integration (fusion) of different data sources has
become necessary for effective prediction, diagnosis, treatment, and planning decisions by combining the complementary
power of different modalities, thereby bringing us closer to the goal of precision medicine2, 3.

Data fusion is the process of combining several data modalities, each providing different viewpoints on a common
phenomenon to solve an inference problem. The purpose of fusion techniques is to effectively take advantage of cooperative
and complementary features of different modalities4, 5. For example, in interpreting medical images, clinical data is often
necessary for making effective diagnostic decisions. Many studies found that missing pertinent clinical and laboratory data
during image interpretation decreases the radiologists’ ability to accurately make diagnostic decisions6. The signiﬁcance of
clinical data to support the accurate interpretation of imaging data is well established in radiology as well as in a wide variety
of imaging-based medical specialties such as dermatology, ophthalmology, and pathology that depend on clinical context to
interpret imaging data correctly7–9.

Thanks to the advances of AI and ML models, one can achieve a useful fusion of multimodal data with high-dimensionality10,
various statistical properties, and different missing value patterns11. Multimodal ML is the domain that can integrate different

*This is pre-print of paper accepted for publication in Scientiﬁc Reports. Cite the ﬁnal version from Nature Scientiﬁc Reports.

Email: haali2@hbku.edu.qa, zshah@hbku.edu.qa

 
 
 
 
 
 
data modalities. In recent years, multimodal data fusion has gained much attention for automating clinical outcome prediction
and diagnosis. This can be seen in Alzheimer’s disease diagnosis and prediction12–15 when imaging data were combined
with speciﬁc lab test results and demographic data as inputs to ML models, and better performance was achieved than the
single-source models. Similarly, fusing pathological images with patient demographic data observed an increase in performance
in comparison with single modality models for breast cancer diagnosis16. Several studies found similar advantages in various
medical imaging applications, including diabetic retinopathy prediction, COVID-19 detection, and glaucoma diagnosis17–19.
This scoping review focuses on studies that use AI models to fuse medical images with EHR data for different clinical
applications. Modality fusion strategies play a signiﬁcant role in these studies. In the literature, some other reviews have been
published on the use of AI for multimodal medical data fusion20–26; however, they differ from our review in terms of their
scope and coverage. Some previous studies focused on the fusion of different medical imaging modalities20, 21; they did not
consider the EHR in conjunction with imaging modalities. Other reviews focused on the fusion of omics data with other data
modalities using DL models22, 23. Another study24 focused on the fusion of various internet of medical things (IoMTs) data for
smart healthcare applications. Liu et al.27 focused exclusively on integrating multimodal EHR data, where multimodality refers
to structured data and unstructured free texts in EHR, using conventional ML and DL techniques. Huang et al.26 discussed
fusion strategies of structured EHR data and medical imaging using DL models emphasizing fusion techniques and feature
extraction methods. Furthermore, their review covered the research till 2019 and retrieved only 17 studies. In contrast, our
review focuses on studies using conventional ML or DL techniques with EHR and medical imaging data, covering 34 recent
studies. Table 1 provides a detailed comparison of our review with existing reviews.

The primary purpose of our scoping review is to explore and analyze published scientiﬁc literature that fuses EHR and

medical imaging using AI models. Therefore, our study aims to answer the following questions:

1. Fusion Strategies: what fusion strategies have been used by researchers to combine medical imaging data with EHR?

What is the most used method?

2. Diseases: For what type of diseases are fusion methods implemented?

3. Clinical outcomes and ML methods: What types of clinical outcomes are addressed using the different fusion strategies?

What kind of ML algorithms are used for each clinical outcome?

4. Resource: What are the publicly accessible medical multimodal datasets?

We believe that this review will provide a comprehensive overview to the readers on the advancements made in multimodal
ML for EHRs and medical imaging data. Furthermore, the reader will develop an understanding of how ML models could be
designed to align data from different modalities for various clinical tasks. Besides, we believe that our review will help identify
the lack of multimodal data resources for medical imaging and EHR, thus motivating the research community to develop more
multimodal medical data.

Preliminaries

We ﬁrst identify the EHR and medical imaging modalities that are the focus of this review. Then, we present the data fusion
strategies that we use to investigate the studies from the perspective of multimodal fusion.

Data modalities
In this review, we focus on studies that use two primary data modalities:

• Medical imaging modality: This includes N-dimensional imaging information acquired in clinical practice, such as X-ray,
Magnetic Resonance Imaging (MRI), functional MRI (fMRI), structural MRI (sMRI), Positron Emission Tomography
(PET), Computed Tomography (CT), and Ultrasound.

• EHR data: This includes both structured and unstructured free-text data. Structured data include coded data such as
diagnosis codes, procedure codes, numerical data such as laboratory test results, and categorical data such as demographic
information, family history, vital signs, and medications. Unstructured data include medical reports and clinical notes.

In our review, we consider studies combining the two modalities of EHR and imaging. However, there exist cases where the
data could contain only multiple EHR modalities (structured and unstructured) or multiple imaging modalities (e.g., PET and
MRI). We consider such data as a single modality, i.e., the EHR modality or imaging modality.

2/20

Previous Reviews
A review on multimodal
fusion:
medical
image
Compendious
analysis
of medical modalities,
databases,
multimodal
fusion
and
quality metrics20
Advances in multimodality
data fusion in neuroimag-
ing21

techniques

Year Scope and Coverage
2022 Their review focused on the fusion of differ-

ent medical imaging modalities.

2021 Their review focused on the fusion of dif-
ferent imaging modalities, considering neu-
roimaging applications for brain diseases
and neurological disorders.

An overview of deep learn-
ing methods for multimodal
medical data mining22

2022 Their review focused on the fusion of dif-
ferent types of multi-omics data with EHR
and different imaging modalities, only con-
sidering DL models for speciﬁc diseases
(COVID-19, cancer, and Alzheimer’s).

2022 Their review focused on the fusion of dif-
ferent types of multi-omics data with EHR
and imaging modalities, considering only
DL models. Moreover, they did not provide
a summary of the freely accessible multi-
modal datasets and a summary of evaluation
measures used to evaluate the multimodal
models.

2021 Their survey did not focus on fusing med-
ical imaging with EHR but rather covered
the fusion of IoMTs data for smart health-
care applications and covered studies pub-
lished untill 2020. Moreover, in their review,
multimodality referred to fusing either dif-
ferent 1D medical signals (such as electro-
cardiogram (ECG) and biosignals), different
medical imaging modalities, or 1D medical
signals with imaging.

2021 Their review focused on the fusion of struc-
tured and unstructured EHR data and did not
consider medical imaging modalities. More-
over, they did not provide a summary of the
freely accessible multimodal datasets and
a summary of evaluation measures used to
evaluate the multimodal models.

Multimodal deep learning
for biomedical data fusion:
a review23

A comprehensive survey on
multimodal medical signals
fusion for smart healthcare
systems24

learning

for
Machine
multimodal
electronic
health records-based re-
search: Challenges and
perspectives27

Fusion of medical
imag-
ing and electronic health
records using deep learning:
a systematic review and im-
plementation guidelines26

Comparative contribution of our review
Our review focused on the fusion of medi-
cal imaging with multimodal EHR data and
considered different imaging modalities as
a single modality. The two reviews did not
share any common studies.

Our review focused on the fusion of medical
imaging with EHR data, considering vari-
ous diseases, such as neurological disorders,
cancer, cardiovascular diseases, psychiatric
disorders, eye diseases, and Covid-19. The
two reviews did not share any common stud-
ies.
Our review focused on the fusion of medi-
cal imaging with EHR data, considering all
AI models for various diseases, such as neu-
rological disorders, cancer, cardiovascular
diseases, psychiatric disorders, eye diseases,
and Covid-19. The two reviews did not share
any common studies.
Our review focused on the fusion of med-
ical imaging with EHR data, considering
all AI models. Moreover, o ur study pro-
vided a summary of the accessible multi-
modal datasets and a summary of evaluation
measures used to evaluate the multimodal
models. The two reviews only shared two
common studies.
Our review focused on the fusion of medical
imaging with EHR (structured and unstruc-
tured) for different clinical applications. It
included 34 studies, most of them published
in 2021 and 2022, with no study common
between the two reviews.

Our review focused on the fusion of medi-
cal imaging with EHR and considered struc-
tured and unstructured data in EHR as a sin-
gle modality. The two reviews did not share
any common studies.

2020 Their review focused on the fusion of struc-
tured EHR data and medical imaging, con-
sidering only DL models, and included only
17 studies published until 2019.

Our review focused on the fusion of medi-
cal imaging with EHR data, considering all
AI models, and included 34 studies, almost
more than half published in 2020 and 2021.

Table 1. Comparison with previous reviews.

3/20

Fusion strategies
As outlined in26, fusion approaches can be categorized into early, late, and joint fusion. These strategies are classiﬁed depending
on the stage in which the features are fused in the ML model. Our scoping review follows the deﬁnitions in26 and attempts to
match each study to its taxonomy. In this section, we brieﬂy describe each fusion strategy:

• Early fusion: It joins features of multiple input modalities at the input level before being fed into a single ML algorithm
for training26. The modality features are extracted either manually or by using different methods such as neural networks
(NN), software, statistical methods, and word embedding models. When NN are used to extract features, early fusion
requires training multiple models: the feature extraction models and the single fusion model. There are two types of joint
fusion: type I and type II. Type I fuses the original features without extracting features, while type II fuses extracted
features from modalities.

• Late fusion: It trains separate ML models on data of each modality, and the ﬁnal decision leverages the predictions of
each model26. Aggregation methods such as weighted average voting, majority voting, or a meta-classiﬁer are used to
make the ﬁnal decision. This type of fusion is often known as decision-level fusion.

• Joint fusion: It combines the learned features from intermediate layers of NN with features from other modalities as
inputs to a ﬁnal model during training26. In contrast to early fusion, the loss from the ﬁnal model is propagated back to
the feature extraction model during training so that the learned feature representations are improved through iterative
updating of the feature weights. NNs are used for joint fusion since they can propagate loss from the ﬁnal model to the
feature extractor(s). There are two types of joint fusion: type I and type II. The former is when NNs are used to extract
features from all modalities. The latter is when not all the input modalities’ features are extracted using NNs26.

Methods

In this scoping review, we followed the guidelines recommended by the PRISMA-ScR28.

Search strategy
In a structured search, we searched four databases, including Scopus, PubMed, Embase, and Google Scholar, to retrieve the
relevant studies. We note here that MEDLINE is covered in PubMed . For Google Scholar search results, we selected the ﬁrst
110 relevant studies, as, beyond 110 entries, the search results rapidly lost relevancy and were unmatched to our review’s topic.
Furthermore, we limited our search to English-language articles published in the last seven years between January 1, 2015, and
January 6, 2022. The search was based on abstracts and titles and was conducted between January 3 and January 6, 2022.

In this scoping review, we focused on applying AI models to multimodal medical data-based applications. The term
multimodal refers to combining medical imaging and EHR, as described in Preliminaries section. Therefore, our search string
incorporated three major terms connected by AND:( (“Artiﬁcial Intelligence” OR “machine learning” OR “deep learning”)
AND “multimodality fusion” AND (“medical imaging” OR “electronic health records”)). We used different forms of each term.
We provide the complete search string for all databases in Appendix 1 of the supplementary material.

Inclusion and exclusion criteria
We included all studies that fused EHR with medical imaging modalities using an AI model for any clinical application. As
AI models, we considered classical ML models, DL models, transfer learning, ensemble learning, etc as mentioned in the
search terms in Appendix 1 of the supplementary material. We did not consider studies that use classical statistical models
such as regression in our review. Our deﬁnition of imaging modalities is any type of medical imaging used in clinical practice,
such as MRI, PET, CT scans, and Ultrasound. We considered both structured and unstructured free-text patients’ data for
EHR modalities as described in Preliminaries section . Only peer-reviewed studies and conference proceedings were included.
Moreover, all included studies were limited to English language only. We did not enforce restrictions on types of disorders,
diseases or clinical tasks.

We excluded studies that used a single data modality. Also, we excluded studies that used different types of data from the
same modality, such as studies that only combined two or more imaging types (e.g. PET and MRI), as we considered this
single modality. Moreover, studies that integrated original imaging modalities with extracted imaging features were excluded as
this was still considered a single modality. Also, studies that combined multi-omics data modality were excluded. In addition,
studies that were unrelated to the medical ﬁeld or did not use AI-based models were excluded. We excluded reviews, conference
abstracts, proposals, editorials, commentaries, letters to editors, preprints, and short letters articles. Non-English publications
were also excluded.

4/20

Study selection
We used Rayyan web-based review management tool29 for the ﬁrst screening and study selection. After removing duplicates,
we screened the studies based on title and abstract. Subsequently, full-text of the selected studies from the title and abstract
screening were assessed for eligibility using our inclusion and exclusion criteria. Two authors (F.M. and H.A.) conducted the
study selection and resolved any conﬂict through discussion. A third author (Z.S.) was consulted when an agreement could not
be reached.

Data extraction
From the ﬁnal included studies, a data extraction form was designed and piloted on four studies to develop a systematic and
accurate data extraction process. The extracted data from the studies are ﬁrst author’s name, year, the country of the ﬁrst
author’s institution, disease’s name, clinical outcome, imaging modality, EHR modality, fusion strategy, feature extraction
methods, data source, AI models, evaluation metrics, and comparison with single modality. In Appendix 2 of the supplementary
material, we provide the extracted information description in detail. One author (F.M.) performed the data extraction, and two
other authors (Z.S. and H.A.) reviewed and veriﬁed the extracted data. Any disagreement was resolved through discussion and
consensus between the three authors.

Data synthesis
Following the data extraction, we used a narrative approach to synthesize the data. We analyzed the studies from ﬁve
perspectives: fusion strategies, diseases, clinical outcomes with ML algorithms, data sources/type, and evaluation mechanism.
For fusion strategies, we focused on how the multimodal data was fused. In addition, we recorded implementation details of
the model, such as feature extraction and single modality evaluation. We also extracted information on the diseases for which
fusion methods were implemented. Furthermore, we analyzed where the data fusion models were applied for clinical outcomes
and what ML models were used for each task. Moreover, we focused on the type of imaging and EHR data used by the studies,
the source of data, and its availability. Finally, for evaluation, we focused on the evaluation metrics used by each study.

Study quality assessment
In accordance with the guidelines for scoping reviews30, 31, we did not perform quality assessments of the included studies.

Results

Search results
A total of 1158 studies were retrieved from the initial search. After duplicates elimination, 971 studies were retained. Based
on our study selection criteria (see Methods), 44 studies remained for full-text review after excluding articles based on their
abstract and title. Moreover, 10 studies were removed after the full-text screening. Finally, 34 studies met our inclusion criteria
and were selected for data extraction and synthesis. Figure 1 shows a ﬂowchart of the study screening and selection process.

Demographics of the studies
As presented in Table 2, approximately two-thirds of the studies were journal articles (n= 23, ∼ 68%)12–15, 17, 19, 25, 32–46,
whereas 11 studies were conference proceedings (∼ 32%)16, 47–56. Most of the studies were published between 2020 and 2022
(n = 22, ∼ 65%). Figure 2 shows a visualization of the publication type-wise and year-wise distribution of the studies. The
included studies were published in 13 countries; however, most of these studies were from the USA (n = 10, ∼ 30%) and China
(n = 8, ∼ 24%).

Data fusion strategies
We mapped the included studies to the taxonomy of fusion strategies outlined in the Preliminaries Section. A primary interest
of our review is to identify the fusion strategies that the included studies used to improve the performance of ML models for
different clinical outcomes.

Early fusion
The majority of the included studies (n = 22, ∼ 65%) used early fusion to combine medical imaging and non-imaging data.
When the input modalities have different dimensions, such as when combining one-dimensional (1D) EHR data with 2D or 3D
imaging data, it is essential to extract high-level imaging features in 1D before fusing with 1D EHR data. To accomplish this,
various methods were used in the studies, including neural network-based features extraction, data generation through software,
or manual extraction of features. Out of the 22 early fusion studies, 19 studies12, 13, 15, 25, 33–36, 39, 41–45, 50–53 used manual or
software-based imaging features, and 3 studies used neural network-based architectures to extract imaging features before
combining with other clinical data modality16, 18, 54. Six out of the 19 studies that used manual or software-based features

5/20

Figure 1. PRISMA ﬂow chart for study identiﬁcation, screening, and selection.

6/20

Characteristics
Year

Number of studies

2022
2021
2020
2019
2018
2016
2015
Country

United States of America (USA)
China
United Kingdom (UK)
Germany
India
Australia
Denmark
Iran
Korea
Pakistan
Kingdom of Saudi Arabia
Singapore

Publication type

Journal
Conference

1
14
7
2
5
4
1

10
8
4
2
2
1
1
1
1
1
1
1

23
11

Table 2. Demographics of the included studies.

reduced the feature dimension before concatenating the two modalities’ features using different methods25, 36, 45, 50–52. Such
methods include recursive feature elimination52, a ﬁlter-based method using Pearson correlation coefﬁcient51, Random Forest
feature selection based on Gini importance50, Relief-based feature selection method25, a wrapper-based method using backward
feature elimination36, and a rank-based method using Gini coefﬁcients45. Moreover, 3 studies13, 15, 44 utilized the principal
component analysis (PCA) dimensionality reduction technique to reduce the feature dimension.

In the studies that used neural network-based architectures to extract imaging features, CNN architectures were used in
three studies16, 18, 54. These studies concatenated the multimodal features (CNN-extracted and EHR features) for their fusion
strategy.

Fourteen early fusion studies evaluated their fusion models’ performance against that of single modality mod-
els12, 13, 15, 16, 18, 25, 32–34, 36, 41–44, 51 . As a result, 13 of these studies exhibited a better performance for fusion when compared
with their imaging-only and clinical-only counterparts12, 13, 15, 16, 18, 25, 32–34, 41–44, 51.

Joint fusion
Joint fusion was the second most common fusion strategy used in 10 out of the 34 studies. In these studies, different neural
network-based methods were used for processing the imaging and EHR data modalities. Chen et al.39 used the Visual Geometry
Group (VGG-16) architecture to extract features from MRI images, while they used a bidirectional long-short term memory
(LSTM ) network with an attention layer to learn feature representation from MRI reports. Then, they concatenated the learned
features of the two modalities before feeding them into a stacked K-nearest neighbor (KNN) attention pooling layer. Grant
et al.55 used a Residual Network (ResNet50) architecture to extract relevant features from the imaging modality and fully
connected NN to process the non-imaging data. They directly concatenated the learned feature representation of the imaging
and non-imaging data and fed them into two fully connected networks. Yidong et al.19 used a Bayesian CNN encoder-decoder
to extract imaging features and a Bayesian Multilayer perception (MLP) encoder-decoder to process the medical indicators data.
The study directly concatenated the two feature vectors and fed the resulted vector into another Bayesian MLP. Samak et al.47
utilized CNN with a self-attention mechanism to extract the imaging features and fully connected NNs to process the metadata
information. Lili et al.39 used VGG-19 architecture to extract the multimodal MRI features and fully connected networks for
clinical data. The study concatenated the two feature vectors and fed them into fully connected NN. Another study46 applied

7/20

Figure 2. The distribution of studies by the type of publication and the year.

CNN layers for imaging features extraction and word embeddings (Word2vec) with self-attention for textual medical data. In
another research38, Fang et al. applied a ResNet architecture and MLP for imaging and clinical data feature extraction. Then,
the authors fused the feature vectors by concatenation and fed them into an LSTM network followed by a fully connected
network. Hsu et al.17 concatenated the imaging features extracted using Inception-V3 model with the clinical data features
before feeding them to fully connected NN. In56, Sharma et al. used CNN to extract image features and then concatenated them
directly with the clinical data to feed into a SoftMax classiﬁer. Xu et al.53 used AlexNet architecture to convert the imaging data
into a feature vector fusible with other non-image modalities. Then, they jointly learned the non-linear correlations among all
modalities using fully connected NN. Out of 10 joint fusion studies, seven studies evaluated their fusion models’ performance
against that of a single modality and reported a performance improvement when fusion was used17, 39, 46, 47, 49, 53, 55.

Late fusion
Late fusion was the least common fusion approach used in the included studies, as only two studies used it. Qiu et al.37 trained
three independent imaging models that took a single MRI slice as input, then aggregated the prediction of these models using
maximum, mean, and majority voting. After combining the results of these aggregations by majority vote, the study performed
late fusion with the clinical data models. In another study40, Huang et al. trained four different late fusion models. Three
models took the average of the predicted probabilities from the imaging and EHR modality models as the ﬁnal prediction. The
fourth model used an NN classiﬁer as an aggregator, which took as input the single modality models’ prediction. The study also
created early, joint fusion models and two single modality models to compare with late fusion performance. As a result, the late
fusion outperformed both the early and joint fusion models and the single modality models.

Diseases
We categorized the diseases and disorders in the included studies into seven types: neurological disorders, cancer, cardiovascular
diseases, Covid-19, psychiatric disorders, eye diseases, and other diseases. The majority of the included studies focused on
neurological disorders (n = 16). Table 3 shows the distribution of the included studies in terms of the diseases and disorders
they covered.

Clinical outcomes and machine learning models
Multimodal ML enables a wide range of clinical applications such as diagnosis, early prediction, patient stratiﬁcation,
phenotyping, biomarkers identiﬁcation, etc. In this review, we labeled each study according to its clinical outcome. We

8/20

Disease Category
Neurological disorders

Alzheimer’s disease (AD)
Mild cognitive impairment (MCI)
Ischemic Stroke
Demyelinating diseases
Neurodevelopmental Deﬁcits
Epilepsy

Cancer

Breast Cancer
Glioblastoma
Lung Cancer
Upper Gastrointestinal (UGI) Cancer

Cardiovascular diseases

Aortic stenosis
Cardiomegaly
Myocardial Infarction

Psychiatric disorder
Bipolar disorder
Schizophrenia

Eye diseases

Diabetic Retinopathy (DR)
Glaucoma
COVID-19
Other diseases

Cervical dysplasia
Pulmonary Embolism (PE)
Hepatitis B

Number of studies
18
7
4
2
1
1
1
5
2
1
1
1
3
1
1
1
2
1
1
2
1
1
3
3
1
1
1

Study reference

12–15, 44, 48, 49

37, 42, 50, 51

35, 47

32

39

34

16, 41

43

55

46

54

55

56

33

36

17

19

18, 25, 38

53

40

52

Table 3. Disease distribution covered by the 34 studies.

categorized the retrieved clinical tasks into two main categories: diagnosis and prediction. Though some of the studies
mentioned detection, classiﬁcation, diagnosis, and prediction, we categorized them under the diagnosis category. Under the
early prediction group, we considered only the studies that predict diseases before onset, identify signiﬁcant risk factors, predict
mortality and overall survival, and predict a treatment outcome. These clinical outcomes were implemented using multimodal
ML models. This section summarizes the different clinical tasks of the retrieved studies, the fusion strategy used, and the ML
models that were developed for each task. Figure 3 shows the distribution of fusion strategies associated with different diseases’
and clinical outcomes.

Diagnosis
The most common applied clinical outcome in the included studies was the diagnosis, reported in 20 (∼ 59%) studies. In these
studies, EHRs were combined with medical imaging to diagnose a spectrum of diseases including neurological disorders (n
= 9)4, 13–15, 32, 37, 42, 49, 50, psychiatric disorders (n = 2)33, 36, CVD (n =3)54–56, Cancer (n = 2)16, 55, and four studies for other
different diseases18, 19, 40, 53. Speciﬁcally, most of the studies that focused on detecting neurological diseases were for AD
(n = 4)13–15, 49, and MCI (n = 4)37, 42, 50, 51.

Early fusion was the most utilized technique for diagnosis purposes used in 13 studies. These studies employed different
ML models on the fused imaging and EHR data for diagnosing different diseases. Most of these studies were for diagnosing
neurological and and psychiatric disorders such as AD13–15, MCI42, 50, 51, demyelinating diseases32, bipolar disorder33, and
schizophrenia36 . Parvathy et al.13 reported diagnosing AD by fusing sMRI and PET imaging features with mini-mental state
examination (MMSE) score, clinical dementia rating (CDR), and age of the subjects. They fed the fused features vector to
different ML models, including support vector machine (SVM), random forest (RF), and gaussian process (GP) for classiﬁcation.
Niyas et al.14 classiﬁed AD by fusing MRI, PET, demographic data, and lab tests, including cognitive tests and Cerebro-Spinal
Fluid (CSF) test. They applied dynamic ensemble of classiﬁers selection algorithms using a different pool of classiﬁers on
the fused features for classiﬁcation. Hamid et al.15 combined MRI and PET imaging features with personal information and

9/20

neurological data such as MMSE and CRF features for AD early diagnosis. In their study, they fed the fused features into SVM
for classiﬁcation. For MCI diagnosis, Matteo et al.42 proposed combining MRI imaging with cognitive assessments for MCI
diagnosis. They concatenated the features of both modalities and fed them into a linear and quadratic discriminant analysis
algorithm for diagnosis. Parisa et al.50, 51 integrated features extracted from MRI and PET images with neuropsychological
tests and demographic data (gender, age, and education) to diagnose MCI early. They trained SVM and deep NNs using the
fused features for classiﬁcation in50 and51, respectively. In another study32, Xin et al. combined MRI imaging with structured
data extracted from EHRs to diagnose demyelinating diseases using SVM. For bipolar disorder, Rashmin et al.33 combined
multimodal imaging features with neuropsychological tests and personal information features. They fed them into SVM to
differentiate bipolar patients from healthy patients. Ebdrup et al.36 proposed integrating MRI and diffusion tensor imaging
tractography (DTI) imaging with neurocognitive tests and clinical data for schizophrenia classiﬁcation. Then, they fused the
features of the two modalities and fed them to different types of ML classiﬁers, including SVM, RF, linear regression (LR),
decision tree (DT), and Naïve Bayes (NB) for classiﬁcation.

Moreover, two studies implemented multimodality early fusion to diagnose different cancer diseases16, 55. Yan et al.16
fused pathological images and structured data extracted from EHRs to classify malignant and benign breast cancer. Then, they
fused the features of the two modalities and fed them to two fully connected NN followed by a SoftMax layer for classiﬁcation.
Seung et al.55 combined PET imaging with clinical and demographic data for differentiating lung adenocarcinoma (ADC) from
squamous cell carcinoma. They fed the integrated features into different algorithms such as SVM, RF, LR, NB, and artiﬁcial
neural network (ANN) for classiﬁcation. For COVID-19 diagnosis, Ming et al.18 combined CT images with clinical features
and fed them into different ML models, including SVM, RF, and KNN for diagnosis. Finally, Tanveer et al.54 combined features
from echocardiogram reports and images, with diagnosis information for the detection of patients with aortic stenosis CVD.
Their study fed the combined features to an RF learning framework to detect patients likely to have the disease.

Joint fusion was used for diagnostic purposes in 5 studies19, 49, 53, 55, 56. These studies employed different types of DL
architectures to learn and fuse the imaging and EHR data for diagnosis purposes. In19, they proposed a Bayesian deep
multisource learning (BDMSL) model that integrated retinal images with medical indicators data to diagnose glaucoma. For
this model, they used Bayesian CNN encoder-decoder to extract imaging features and a Bayesian MLP encoder-decoder
to process the medical indicators data. The two feature vectors were directly concatenated and fed into Bayesian MLP for
classiﬁcation. Chen et al.49 used DL for multimodal feature extraction and classiﬁcation to detect AD; the authors used the
VGG-16 model to extract features from MRI images and a bidirectional LSTM network with an attention layer to learn features
from MRI reports. Then, they fed the fused features into a stacked KNN pooling layer to classify the patient’s diagnosis
data. In53, Xu et al. proposed an end-to-end deep multimodal framework that can learn better complementary features from
the image and non-image modalities for cervical dysplasia diagnosis. They used CNN, speciﬁcally AlexNet architecture, to
convert the cervigram image data into a feature vector fusible with other non-image modalities. After that, they jointly learned
the non-linear correlations among all modalities using fully connected NN for cervical dysplasia classiﬁcation. Another two
studies55, 56 also employed DL models to jointly learn multimodal feature representation for diagnosing CVDs. The former55
proposed a multimodal network for cardiomegaly classiﬁcation, which simultaneously integrates the non-imaging intensive care
unit (ICU) data (laboratory values, vital sign values, and static patient metadata, including demographics) and the imaging data
(chest X-ray). They used a ResNet50 architecture to extract features from the X-ray images and fully connected NN to process
the ICU data. To join the learned imaging and non-imaging features, they concatenated the learned feature representation and
fed them into two fully connected layers to generate a label for cardiomegaly diagnosis. The latter study56 proposed a stacked
multimodal architecture called SM2N2, which integrated clinical information and MRI images. In their research, they used
CNN to extract imaging features, and then they concatenated these features with clinical data to feed into a SoftMax classiﬁer
for myocardial infarction detection.

Late fusion was implemented in 2 studies37, 40 for disease diagnosis purposes. Fang et al.37 proposed the fusion of MRI
scans, logical memory (LM) tests, and MMSE for MCI classiﬁcation. Their study utilized VGG-11 architecture for MRI feature
extraction and developed two MLP models for MMSE and LM test results. Then, they combined both MRI and MLP models
using majority voting. As a result, the fusion model outperformed the individual models. Huang et al.40 utilized a non-open
dataset comprising CT scans and EHR data to train two unimodal and four late fusion models for PE diagnosis. They used their
previously implemented architecture (PENet)57 to encode the CT images and a feedforward network to encode the tabular data.
The late fusion approach performed best among the fusion models and outperformed the models trained on the image-only and
the tabular-only data.

Early Prediction
Prediction tasks were reported in 14 (∼ 41.2%) studies. In these studies, EHRs were fused with medical imaging to predict
different outcomes, including disease prediction, mortality prediction, survival prediction, and treatment outcome prediction.
Ten studies of the prediction tasks were disease prediction12, 17, 34, 38, 39, 41, 44, 46, 48, 52, which involved determining whether an
individual might develop a given disease in the future. The second most common prediction task was treatment outcome

10/20

prediction reported in 2 studies35, 47, followed by one study for mortality prediction and overall survival prediction25, 43,
respectively.

The early fusion technique was used in 6 studies12, 34, 41, 44, 48, 52 for disease prediction. Minhas et al.12 proposed an early
fusion model to predict which subjects will progress from MCI to AD in the future. The study concatenated MRI extracted
features with demographic and neuropsychological biomarkers before feeding them to an SVM model for prediction. Ali et
al.34 proposed a model to predict Epileptogenic-Zone in the Temporal Lobe by feeding MRI extracted features integrated
with set-of-semiology features into various ML models such as LR, SVM, and Gradient Boosting. Ma et al.41 fused MRI and
clinicopathological features for predicting metachronous distant metastasis (DM) in breast cancer. They fed the concatenated
features to an LR model. Another study44 combined MRI-derived features and high-throughput brain phenotyping to diagnose
and predict the onset of AD. They fed the fused features into different ML classiﬁers, including RF, SVM, and LR. Ulyana et
al.48 trained a deep, fully connected network as a regressor in a 5-year longitudinal study on AD to predict cognitive test scores
at multiple future time points. Their model produced MMSE scores for ten unique future time points at six-month intervals
by combing biomarkers from cognitive test scores, PET, and MRI. They early fused imaging features with the cognitive test
scores through concatenation before feeding them into the fully connected network. Finally, Bai et al.52 compared different
multimodal biomarkers (clinical data, biochemical and hemologic parameters, and ultrasound elastography parameters) for
predicting the assessment of ﬁbrosis in chronic hepatitis B using SVM.

For disease prediction, joint fusion was used in 4 studies17, 38, 39, 46. Hsu et al.17 proposed a deep multimodal fusion
model that trained heterogeneous data from fundus images and non-image data for DR screening. They concatenated the
imaging extracted features from Inception-V3 with the clinical data features before feeding them to fully connected NN
followed by SoftMax layer for classiﬁcation. Fang et al.38 developed a prediction system by jointly fusing CT scans and
clinical data to predict the progression of COVID-19 malignancy. In their study, the feature extraction part applied a ResNet
architecture and MLP for CT and clinical data, respectively. Then, they concatenated the different features and fed them into
an LSTM network followed by a fully connected NN for prediction. In39, the authors proposed a deep multimodal model for
predicting neurodevelopmental deﬁcits at 2 years of age. Their model consisted of a feature extractor and fusion classiﬁer.
In the feature extractor, they used VGG-19 architecture to extract MRI features and fully connected NN for clinical data.
Then, the study combined the extracted features of the two modalities and fed their combination to another fully connected
network in the fusion classiﬁer for prediction. To evaluate the performance of the modality fusion, they tested their model
using a single modality of MRI and clinical features. The results showed that multimodal fusion outperformed the single
modality performance. Another study46 also used multimodal joint fusion for UGI cancer screening. Their model integrated
features extracted from UGI endoscopic images with corresponding textual medical data. They applied CNN for image feature
extraction and word embeddings (Word2vec) with self-attention for textual medical data feature extraction. After that, they
concatenated the extracted features of the two modalities and fed them into fully connected NN for prediction. Their results
showed that multimodal fusion outperformed the single modality performance.

For treatment outcome prediction35, 47, the former35 implemented early fusion while the latter47 used joint fusion. For acute
ischemic stroke, Gianluca et al.35 evaluated the predictive power of imaging, clinical, and angiographic features to predict the
outcome of acute ischemic stroke using ML. The study early fused all features into gradient boosting classiﬁers for prediction.
In47, the authors proposed a DL model to directly exploit multimodal data (clinical metadata and non-contrast CT (NCCT)
imaging data) to predict the success of endovascular treatment for ischemic stroke. They utilized CNN with a self-attention
mechanism to extract the features of images, and then they concatenated them with the metadata information. Then, the
classiﬁcation stage of the proposed model processed the fused features through a fully connected NN, followed by the Softmax
function applied to the outputs. Their results showed that multimodal fusion outperformed the single modality performance.
Both the mortality and overall survival prediction studies25, 43 implemented early fusion. In25, they developed a model to
predict COVID-19 ventilatory support and mortality early on to prioritize patients and manage the hospital resources’ allocation.
They fused patients’ CT images and EHR data features by concatenation before feeding them to different ML models, including
SVM, RF, LR, and eXtreme gradient boosting. They evaluated the performance against single modality models and observed
that the results for multimodal fusion were better. The other study43 aimed to develop ML models to predict glioblastoma
patients’ overall survival (OS) and progression-free survival (PFS) based on combining treatment features, pathological, clinical,
PET/CT-derived information, and semantic MRI-based features. They concatenated the features of all modalities and fed them
to an RF model. The study showed that the model based on multimodal fusion data outperformed the single modality models.

Datasets
Patient Data Types
The included studies reported medical imaging and EHRs (structured and non-structured) patient’s data types. In terms of
imaging modality, CT, MRI, fMRI, structural MRI (sMRI), PET, Diffusion MRI, DTI, ultrasound, X-ray, fundus images, and
PET were used in the studies. MRI and PET images were the most utilized modalities. Out of the included 34 studies, 13 used

11/20

Figure 3. Fusion strategies associated with clinical outcomes for different diseases.

12/20

MRI images, and 8 used PET images mostly for AD diagnosis and prediction. In terms of EHRs, structured data was the most
commonly used modality (n = 32). Table 4 summarizes the types of imaging and EHR data used in the studies.

Data Type
Imaging Data
MRI imaging

MRI
DTI
fMRI
sMRI and Diffusion MRI

PET
CT
X-ray
fundus images
Ultrasound

Echocardiography
Pathological images
Cervigram images
Endoscopy images

EHR Data
Structured
Unstructured

Number of studies

Study reference

13
3
2
1
8
7
2
2
1
1
1
1
1

32
2

12, 14, 15, 32, 33, 37, 41–43, 48–51

33, 36, 39

33, 39

44

13–15, 43, 45, 48, 50, 51

18, 35, 38, 40, 43, 45, 47

25, 55

17, 19

52

54

16

53

46

12–19, 25, 32–45, 47, 48, 50–56

46, 49

Table 4. Patient data types used in the included studies.

Patient data Resources
Almost two-thirds of the studies included in this scoping review used private data sources (clinical data that are not publicly
available) (n = 21, ∼ 59%). In contrast, publicly accessible datasets were used in only 13 studies. We observed that the most
used public dataset was the “Alzheimer’s Disease Neuroimaging Initiative” dataset (ADNI)58 , where 7 out of 13 studies
used the dataset. Other publicly available datasets that were used among the included studies were the “National Alzheimer’s
Coordinating Center” (NACC) dataset59, the “Medical Information Mart for Intensive Care” (MIMIC-IV) dataset60, the
""National Cancer Institute"" (NCI) dataset, ADNI TADPOLE dataset61 , and MR CLEAN Trial dataset62. In Table 5, we
summarize the public multimodal medical datasets and their clinical applications. Considering these datasets for each clinical
task, the most popular is ADNI for AD and MCI disease diagnosis and prediction.

Evaluation metrics
Evaluation metrics are mainly dependent on the clinical task. Typically, accuracy, the area under the curve (AUC), sensitivity,
speciﬁcity, F1- measure, and precision are mostly used for the evaluation of diagnosis and prediction tasks. Table 6 shows the
distribution of the evaluation measures used in the included studies

Discussion

This section summarizes our ﬁndings and provides future directions for research on the multimodal fusion of medical imaging
and EHR.

Principal ﬁndings
We found that multimodal models that combined EHR and medical imaging data generally outperformed single modality
models for the same task in disease diagnosis or prediction. Since our review shows that the fusion of medical imaging and
clinical context data can improve the performance of AI models, we recommend attempting fusion approaches when multimodal
data is obtainable. Moreover, through this review, we observed certain trends in the ﬁeld of multimodality fusion in the medical
area, which can be categorized as:

• Resources: We observed that multimodal data resources of medical imaging and EHR are limited owing to privacy
considerations. The most prominent dataset was the ADNI, containing MRI and PET images collected from about 1700
individuals in addition to clinical and genetic information. Considering ADNI’s contributions in advancing the research,
similar multimodal datasets should be developed for other medical data sources too.

13/20

Description

URL

Clinical outcomes

Study refer-
ence
13, 15

Disease diagnosis (AD)

Disease diagnosis (MCI)

Disease Prediction (AD)

50, 51

12, 44, 48

Disease diagnosis (AD)

14

https://adni.
loni.usc.edu/
data-samples/
data-types/

https://tadpole.
grand-challenge.
org/Data/

Public
Dataset
ADNI

ADNI TAD-
POLE

NACC

MIMIC-CXR,
MIMIC-IV

NCI

MR CLEAN
Trial

ADNI represents a series of
studies, including ADNI 1,
2, and 3, designed to study
MCI and its progression into
AD. It has MRI and PET im-
ages along with clinical and
genetic information58.
ADNI has a simpliﬁed coun-
terpart, TADPOLE, which
has a subset of ADNI-3 sam-
ples and features.ATDPOLE
does not include raw images,
but it has processed struc-
tural information about the
images such as ROI averages,
thicknesses of the cortex and
volumes of brain sub-regions,
etc61.
The NACC dataset was es-
tablished to facilitate collab-
orative AD research. The
dataset comprises MRI data,
demographic data, neuropsy-
chological
testing scores,
and clinical diagnosis of pa-
tients59.

MIMIC-CXR is a dataset of
patient chest radiographs. It
contains X-ray studies for
64,588 patients63.
MIMIC-IV is a database for
patients admitted to critical
care units comprising patient
stay information, patient’s
ICU data, and lookup tables
to allow linking to MIMIC-
CXR60.
Data collections produced
by major NCI initiatives are
listed in the NCI Data Cata-
log, including Clinical data,
Genomics, imaging, and Pro-
teomics.

A longitudinal study of 500
patients
treated with en-
dovascular therapy in The
Netherlands for acute is-
chemic stroke comprising
NCCT images, CT Angiogra-
phy (CTA) images, and clini-
cal metadata information on
its patients62.

https://
naccdata.org/
requesting-data/
nacc-data

Disease Diagnosis (MCI)

37

MIMIC-CXR:https:
//www.nature.
com/articles/
s41597-019-0322-0

MIMIC-IV:https:
//physionet.org/
content/mimiciv/0.
4/

https://
datascience.cancer.
gov/resources/
nci-data-catalog

Disease
diomegaly)

diagnosis

(Car-

55

Disease diagnosis (Cervical
dysplasia)

53

https://www.
mrclean-trial.org/
home.html

Treatment outcome predic-
tion (ischemic stroke)

47

Table 5. Multimodal medical datasets and clinical outcome applications.

14/20

12–15, 17, 19, 32–34, 37–41, 45, 46, 50, 51, 53, 54

Study Reference
12, 15–19, 25, 32–42, 44–47, 49–56

Number of studies
31
20
17
15
7
3

Evaluation metrics
Accuracy
Sensitivity (recall)
AUC
Speciﬁcity
Precision
Positive predictive value (PPV) and Nega-
tive predictive value (NPV)
Matthews correlation coefﬁcient (MCC)
C-index
Root-Mean Squared Error (RMSE)
The numbers in the second column do not sum up to 34 as many studies used more than a single metric.

12, 14, 15, 17, 32–34, 38–41, 46, 50, 51, 53

13, 19, 34, 37, 45, 54

15, 34, 40

2
1
1

34, 41

43

48

12, 15–17, 19, 25, 32, 35, 37–39, 41, 45, 47, 52, 53, 55

Table 6. The distribution of evaluation metrics in the included studies.

• Fusion implementation: Early fusion was the most commonly used technique in most applications for multimodal
learning. Before fusing 1D EHRs data with image data in 2D or 3D, images data was converted to a 1D vector by
extracting high-level representations using manual or software-generated features12, 13, 15, 25, 33–36, 39, 41–45, 50–53, or CNN-
extracted features8, 16, 54. The learned imaging features from CNN often resulted in better task-speciﬁc performance than
manually or software-derived features64. Based on this reviewed studies, early fusion models performed better than
conventional single-modality models on the same task. Researchers can use the early fusion method as a ﬁrst attempt to
learn multimodal representations since it can learn to exploit the interactions and correlations between features of each
modality. Furthermore, it only requires one model to be trained, making the pipeline for training easier than that of joint
and late fusion. However, if imaging features are extracted with CNN, early fusion requires multiple models to be trained.

Joint fusion was the second most commonly used fusion approach. From a modality perspective, CNNs appeared to
be the best option for image feature extraction. Tabular data were mainly processed using dense layers when fed into a
model, while text data were mostly processed using LSTM layers followed by the attention layer. Most of the current
research directly concatenated the feature vectors of the different modalities to combine multimodal data. Using NNs to
implement joint fusion can be a limitation when dealing with small datasets, which means that joint fusion is preferred
with large datasets. For small datasets, it is preferable to use early or late fusion methods as they can be implemented
using classical ML techniques. Nevertheless, we expect and agree with26 that joint fusion models can provide better
results than other fusion strategies because they update their feature representations iteratively by propagating the loss to
all the feature extraction models, aiming to learn correlations across modalities.

Based on the performance reported in the included studies, it is preferred to try the early and joint fusion when the
relation between the two data modalities is complementary. In this review, AD diagnosis is an example in which imaging
and EHRs data are dependent as relevant and accurate knowledge of the patient’s current symptomatology, personal
information and imaging reports can help doctors interpret imaging results in a suitable clinical context, resulting in a
more precise diagnosis. Therefore, all AD diagnosis studies in this review implemented either early fusion13–15 or joint
fusion49 for multimodal learning.

On the other hand, it is preferred to try late fusion when input modalities do not complement each other. For example,
the brain MRI pixel data and the quantitative result of an MMSE (e.g., Qiu et al.37) for diagnosing MCI are independent,
making them appropriate candidates for inclusion in the late fusion strategy. Also, late fusion does not impose the
requirement of a huge amount of training data, so it could be used when the modalities data sizes are small. Moreover,
late fusion strategy could be attempted when the concatenation of feature vectors from multiple modalities results in
high-dimensional vectors that are difﬁcult for ML algorithms to learn without overﬁtting unless many input samples are
available. In late fusion, multiple models are employed, each specialized in a single modality, thereby limiting the size of
the input feature vector for each model. Furthermore, late fusion could be used when data is incomplete or missing, i.e.,
some patients have only imaging data but no clinical data or vice versa. This is because late fusion uses independent
models for different modalities, and aggregation methods like averaging and majority voting can be used even when
predictions from a modality are not present. Moreover, predictions could be disproportionately inﬂuenced by the most
feature-rich input modality when the number of features is very different between the input data modalities65; in this
scenario, late fusion is preferable because it allows training each model using each modality separately.

• Applications: In this review, we found that AD diagnosis and prediction12–15, 44, 48, 49 were the most common applications

15/20

addressed in a multimodal setting among studies. Using ML fusion techniques consistently demonstrated improved
AD diagnosis, while clinicians experience difﬁculty with accurate and reliable diagnosis even when multimodal data is
available26. This emphasizes the utility and signiﬁcance of multimodal fusion approaches in clinical applications.

• Prospects: In this review, we noted that multimodal medical data fusion is growing due to its potential in achieving
state-of-the-art performance for healthcare applications. Nonetheless, this growth is hampered by the absence of adequate
data for benchmarking methods. This is not surprising, given the privacy concerns surrounding revealing healthcare
data. Moreover, we observed a lack of complexity in the used non-imaging data, particularly in the context of heavily
feature-rich data included in the EHR. For example, the majority of studies focused mostly on basic demographic data
like gender and age12, 15, 44, 51, a limited number of studies also included medical histories such as smoking status and
hypertension18, 55 or speciﬁc clinical characteristics that are known to be associated with a certain disease, such as an
MMSE for diagnosing AD. In addition to selecting the disease-associated features, future research may beneﬁt from
using vast amounts of feature-rich data, as demonstrated in domains outside of medicine, such as autonomous driving66.

Future directions
Although we focus on EHR and medical imaging as multimodal data, other modalities such as multi-omics and environmental
data could also be integrated using the aforementioned fusion approaches. As the causes of many diseases are complex, many
factors, including inherited genetics, lifestyle, and living environments, contribute to the development of diseases. Therefore,
combining multisource data, e.g. EHR, imaging, and multi-omics data, may lead to a holistic view that can improve patient
outcomes through personalized medicine.

Although we focus on EHR and medical imaging as multimodal data, other modalities such as multi-omics and environmen-
tal data could also be integrated using the aforementioned fusion approaches. As the causes of many diseases are complex, many
factors, including inherited genetics, lifestyle, and living environments, contribute to the development of diseases. Therefore,
combining multisource data, e.g. EHR, imaging, and multi-omics data, may lead to a holistic view that can improve patient
outcomes through personalized medicine.

Moreover, the unavailability of multimodal public data is a limitation that hinders the development of corresponding
research. Many factors (e.g., gender, ethnicity, environmental factors) could inﬂuence the research directions or even clinical
decision, relying on a few publicly available datasets might not be enough for making conclusive clinical claims to the global
population27. Consequently, it is imperative to encourage the sharing of ﬂexible data among institutions and hospitals in order
to facilitate the exploration of a wider range of population data for clinical research. In ML, federated learning (FL)67, 68
provides the ability to collect data safely and securely from multiple centers. It may be used to collect multimodal data from
various centers to train a large-scale model without collecting data directly.

Limitations
Our search was limited to studies published within the previous seven years (2015-2022). We only considered studies published
in English, which may have led to leaving out some studies published in other languages. We solely included studies fusing
EHR with medical imaging. We did not include studies that used other data modalities such as multi-omics data, as they are out
of the scope of this work. Because positive results are typically reported disproportionately, publication bias might be another
limitation of this review. This bias may result in an overestimation of the beneﬁts associated with multimodal data analysis.
The studies included in this review employed various input modalities, investigated various clinical tasks for different diseases,
and reported different performance metrics; hence a direct comparison of the results presented in the studies is not always
applicable. Furthermore, not all articles provided conﬁdence bounds, making it difﬁcult to compare their results statistically.

Conclusion

Multimodal ML is an area of research that is gaining attention within the medical ﬁeld. This review surveyed multimodal
medical ML literature that combines EHR with medical imaging data. It discussed fusion strategies, the clinical tasks and ML
models that implemented data fusion, the type of diseases, and the publicly accessible multimodal data for medical imaging
and EHRs. Furthermore, it highlighted some directions to pave the way for future research. Our ﬁnding suggests that there is
a growing interest in multimodal medical data. Still, most studies combine the modalities with relatively simple strategies,
which despite being shown to be effective, might not fully exploit the rich information embedded in these modalities. As this is
a fast-growing ﬁeld and new AI models with multimodal data are constantly being developed, there might exist studies that
fall outside our deﬁnition of fusion strategies or use a combination of these strategies. We believe that the development of
this ﬁeld will give rise to more comprehensive multimodal medical data analysis and will be of great support to the clinical
decision-making process.

16/20

Data availability

The data generated during this scoping review is provided as supplementary materials.

References

1. Murdoch, T. B. & Detsky, A. S. The inevitable application of big data to health care. Jama 309, 1351–1352 (2013).

2. Obermeyer, Z. & Emanuel, E. J. Predicting the future—big data, machine learning, and clinical medicine. The New Engl.

journal medicine 375, 1216 (2016).

3. Roski, J., Bo-Linn, G. W. & Andrews, T. A. Creating value in health care through big data: opportunities and policy

implications. Heal. affairs 33, 1115–1122 (2014).

4. Lozano-Perez, T. Autonomous robot vehicles (Springer Science & Business Media, 2012).

5. Castanedo, F. A review of data fusion techniques. The scientiﬁc world journal 2013 (2013).

6. Cohen, M. D. Accuracy of information on imaging requisitions: does it matter? J. Am. Coll. Radiol. 4, 617–621 (2007).

7. Comfere, N. I. et al. Provider-to-provider communication in dermatology and implications of missing clinical information

in skin biopsy requisition forms: a systematic review. Int. journal dermatology 53, 549–557 (2014).

8. Jonas, J. B. et al. Glaucoma. The Lancet 390, 2183–2193, DOI: https://doi.org/10.1016/S0140-6736(17)31469-1 (2017).

9. Comfere, N. I. et al. Dermatopathologists’ concerns and challenges with clinical information in the skin biopsy requisition

form: a mixed-methods study. J. cutaneous pathology 42, 333–345 (2015).

10. Li, Y., Wu, F.-X. & Ngom, A. A review on machine learning principles for multi-view biological data integration. Brieﬁngs

bioinformatics 19, 325–340 (2018).

11. Ramachandram, D. & Taylor, G. W. Deep multimodal learning: A survey on recent advances and trends. IEEE signal

processing magazine 34, 96–108 (2017).

12. Minhas, S. et al. Early mci-to-ad conversion prediction using future value forecasting of multimodal features. Comput.

intelligence neuroscience 2021 (2021).

13. Pillai, P. S., Leong, T.-Y., Initiative, A. D. N. et al. Fusing heterogeneous data for alzheimer’s disease classiﬁcation. In

MEDINFO 2015: eHealth-enabled Health, 731–735 (IOS Press, 2015).

14. KP, M. N. & Thiyagarajan, P. Alzheimer’s classiﬁcation using dynamic ensemble of classiﬁers selection algorithms: A

performance analysis. Biomed. Signal Process. Control. 68, 102729 (2021).

15. Akramifard, H., Balafar, M. A., Razavi, S. N. & Ramli, A. R. Early detection of alzheimer’s disease based on clinical trials,
three-dimensional imaging data, and personal information using autoencoders. J. medical signals sensors 11, 120 (2021).

16. Yan, R. et al. Richer fusion network for breast cancer classiﬁcation based on multimodal data. BMC Med. Informatics

Decis. Mak. 21, 1–15 (2021).

17. Hsu, M.-Y. et al. Deep learning for automated diabetic retinopathy screening fused with heterogeneous data from ehrs can

lead to earlier referral decisions. Transl. Vis. Sci. & Technol. 10, 18–18 (2021).

18. Xu, M. et al. Accurately differentiating between patients with covid-19, patients with other viral infections, and healthy

individuals: multimodal late fusion learning approach. J. Med. Internet Res. 23, e25535 (2021).

19. Chai, Y., Bian, Y., Liu, H., Li, J. & Xu, J. Glaucoma diagnosis in the chinese context: An uncertainty information-centric

bayesian deep learning model. Inf. Process. & Manag. 58, 102454 (2021).

20. Azam, M. A. et al. A review on multimodal medical image fusion: Compendious analysis of medical modalities,
multimodal databases, fusion techniques and quality metrics. Comput. Biol. Medicine 144, 105253, DOI: https://doi.org/
10.1016/j.compbiomed.2022.105253 (2022).

21. Zhang, Y.-D. et al. Advances in multimodal data fusion in neuroimaging: Overview, challenges, and novel orientation. Inf.

Fusion 64, 149–187, DOI: https://doi.org/10.1016/j.inffus.2020.07.006 (2020).

22. Behrad, F. & Saniee Abadeh, M. An overview of deep learning methods for multimodal medical data mining. Expert. Syst.

with Appl. 200, 117006, DOI: https://doi.org/10.1016/j.eswa.2022.117006 (2022).

23. Stahlschmidt, S. R., Ulfenborg, B. & Synnergren, J. Multimodal deep learning for biomedical data fusion: a review.

Brieﬁngs Bioinforma. 23 (2022).

17/20

24. Muhammad, G. et al. A comprehensive survey on multimodal medical signals fusion for smart healthcare systems. Inf.

Fusion 76, 355–375, DOI: https://doi.org/10.1016/j.inffus.2021.06.007 (2021).

25. Aljouie, A. F. et al. Early prediction of covid-19 ventilation requirement and mortality from routinely collected baseline

chest radiographs, laboratory, and clinical data with machine learning. J. multidisciplinary healthcare 14, 2017 (2021).

26. Huang, S.-C., Pareek, A., Seyyedi, S., Banerjee, I. & Lungren, M. P. Fusion of medical imaging and electronic health
records using deep learning: a systematic review and implementation guidelines. NPJ digital medicine 3, 1–9 (2020).

27. Liu, Z. et al. Machine learning for multimodal electronic health records-based research: Challenges and perspectives.

arXiv preprint arXiv:2111.04898 (2021).

28. Tricco, A. C. et al. Prisma extension for scoping reviews (prisma-scr): checklist and explanation. Annals internal medicine

169, 467–473 (2018).

29. Ouzzani, M., Hammady, H., Fedorowicz, Z. & Elmagarmid, A. Rayyan—a web and mobile app for systematic reviews.

Syst. reviews 5, 1–10 (2016).

30. Arksey, H. & O’Malley, L. Scoping studies: towards a methodological framework. Int. journal social research methodology

8, 19–32 (2005).

31. Grant, M. J. & Booth, A. A typology of reviews: an analysis of 14 review types and associated methodologies. Heal.

information & libraries journal 26, 91–108 (2009).

32. Xin, B., Huang, J., Zhou, Y., Lu, J. & Wang, X. Interpretation on deep multimodal fusion for diagnostic classiﬁcation. In

2021 International Joint Conference on Neural Networks (IJCNN), 1–8 (IEEE, 2021).

33. Achalia, R. et al. A proof of concept machine learning analysis using multimodal neuroimaging and neurocognitive

measures as predictive biomarker in bipolar disorder. Asian J. Psychiatry 50, 101984 (2020).

34. Alim-Marvasti, A. et al. Machine learning for localizing epileptogenic-zone in the temporal lobe: Quantifying the value of

multimodal clinical-semiology and imaging concordance. Front. digital health 3, 8 (2021).

35. Brugnara, G. et al. Multimodal predictive modeling of endovascular treatment outcome for acute ischemic stroke using

machine-learning. Stroke 51, 3541–3551 (2020).

36. Ebdrup, B. H. et al. Accuracy of diagnostic classiﬁcation algorithms using cognitive-, electrophysiological-, and neu-

roanatomical data in antipsychotic-naïve schizophrenia patients. Psychol. medicine 49, 2754–2763 (2019).

37. Qiu, S. et al. Fusion of deep learning models of mri scans, mini–mental state examination, and logical memory test
enhances diagnosis of mild cognitive impairment. Alzheimer’s & Dementia: Diagn. Assess. & Dis. Monit. 10, 737–749
(2018).

38. Fang, C. et al. Deep learning for predicting covid-19 malignant progression. Med. image analysis 72, 102096 (2021).

39. He, L. et al. Deep multimodal learning from mri and clinical data for early prediction of neurodevelopmental deﬁcits in

very preterm infants. Front. neuroscience 15 (2021).

40. Huang, S.-C., Pareek, A., Zamanian, R., Banerjee, I. & Lungren, M. P. Multimodal fusion with deep neural networks for
leveraging ct imaging and electronic health record: a case-study in pulmonary embolism detection. Sci. reports 10, 1–9
(2020).

41. Ma, W. et al. Distant metastasis prediction via a multi-feature fusion model in breast cancer. Aging (Albany NY) 12, 18151

(2020).

42. De Marco, M., Beltrachini, L., Biancardi, A., Frangi, A. F. & Venneri, A. Machine-learning support to individual diagnosis
of mild cognitive impairment using multimodal mri and cognitive assessments. Alzheimer Dis. & Assoc. Disord. 31,
278–286 (2017).

43. Peeken, J. C. et al. Combining multimodal imaging and treatment features improves machine learning-based prognostic

assessment in patients with glioblastoma multiforme. Cancer medicine 8, 128–136 (2019).

44. Wang, Y. et al. Diagnosis and prognosis of alzheimer’s disease using brain morphometry and white matter connectomes.

NeuroImage: Clin. 23, 101859 (2019).

45. Hyun, S. H., Ahn, M. S., Koh, Y. W. & Lee, S. J. A machine-learning approach using pet-based radiomics to predict the

histological subtypes of lung cancer. Clin. nuclear medicine 44, 956–960 (2019).

46. Ding, S., Huang, H., Li, Z., Liu, X. & Yang, S. Scnet: a novel ugi cancer screening framework based on semantic-level

multimodal data fusion. IEEE J. Biomed. Heal. Informatics 25, 143–151 (2020).

18/20

47. Samak, Z. A., Clatworthy, P. & Mirmehdi, M. Prediction of thrombectomy functional outcomes using multimodal data. In

Annual Conference on Medical Image Understanding and Analysis, 267–279 (Springer, 2020).

48. Morar, U. et al. A deep-learning approach for the prediction of mini-mental state examination scores in a multimodal
longitudinal study. In 2020 International Conference on Computational Science and Computational Intelligence (CSCI),
761–766 (IEEE, 2020).

49. Chen, D., Zhang, L. & Ma, C. A multimodal diagnosis predictive model of alzheimer’s disease with few-shot learning. In
2020 International Conference on Public Health and Data Science (ICPHDS), 273–277, DOI: 10.1109/ICPHDS51617.
2020.00060 (2020).

50. Forouzannezhad, P., Abbaspour, A., Cabrerizo, M. & Adjouadi, M. Early diagnosis of mild cognitive impairment
using random forest feature selection. In 2018 IEEE Biomedical Circuits and Systems Conference (BioCAS), 1–4, DOI:
10.1109/BIOCAS.2018.8584773 (2018).

51. Forouzannezhad, P., Abbaspour, A., Li, C., Cabrerizo, M. & Adjouadi, M. A deep neural network approach for early
diagnosis of mild cognitive impairment using multiple features. In 2018 17th IEEE International Conference on Machine
Learning and Applications (ICMLA), 1341–1346, DOI: 10.1109/ICMLA.2018.00218 (2018).

52. Bai, Y., Chen, X., Dong, C., Liu, Y. & 0001, Z. Z. A comparison of multimodal biomarkers for chronic hepatitis b assessment
using recursive feature elimination. In 38th Annual International Conference of the IEEE Engineering in Medicine and
Biology Society, EMBC 2016, Orlando, FL, USA, August 16-20, 2016, 2448–2451, DOI: 10.1109/EMBC.2016.7591225
(IEEE, 2016).

53. Xu, T., Zhang, H., Huang, X., Zhang, S. & Metaxas, D. N. Multimodal deep learning for cervical dysplasia diagnosis. In
Ourselin, S., Joskowicz, L., Sabuncu, M. R., Unal, G. & Wells, W. (eds.) Medical Image Computing and Computer-Assisted
Intervention – MICCAI 2016, 115–123 (Springer International Publishing, Cham, 2016).

54. Syeda-Mahmood, T. et al. Identifying patients at risk for aortic stenosis through learning from multimodal data. In
Ourselin, S., Joskowicz, L., Sabuncu, M. R., Unal, G. & Wells, W. (eds.) Medical Image Computing and Computer-Assisted
Intervention - MICCAI 2016, 238–245 (Springer International Publishing, Cham, 2016).

55. Grant, D., Papie˙z, B. W., Parsons, G., Tarassenko, L. & Mahdi, A. Deep learning classiﬁcation of cardiomegaly using
combined imaging and non-imaging icu data. In Papie˙z, B. W., Yaqub, M., Jiao, J., Namburete, A. I. L. & Noble, J. A.
(eds.) Medical Image Understanding and Analysis, 547–558 (Springer International Publishing, Cham, 2021).

56. Sharma, R., Eick, C. F. & Tsekos, N. V. Sm2n2: A stacked architecture for multimodal data and its application to
myocardial infarction detection. In Puyol Anton, E. et al. (eds.) Statistical Atlases and Computational Models of the Heart.
M&Ms and EMIDEC Challenges, 342–350 (Springer International Publishing, Cham, 2021).

57. Huang, S.-C. et al. Penet—a scalable deep-learning model for automated diagnosis of pulmonary embolism using

volumetric ct imaging. NPJ digital medicine 3, 1–9 (2020).

58. Mueller, S. et al. The alzheimer’s disease neuroimaging initiative. Neuroimaging Clin. North Am. 15, 869–877, DOI:

10.1016/j.nic.2005.09.008 (2005).

59. Beekly, D. et al. The national alzheimer’s coordinating center (nacc) database: an alzheimer disease database. Alzheimer

disease associated disorders 18, 270–277 (2004).

60. Alistair, J. et al. Mimic-iv (version 0.4). PhysioNet DOI: https://doi.org/10.13026/a3wn-hq05 (2020).

61. Marinescu, R. V. et al. Tadpole challenge: prediction of longitudinal evolution in alzheimer’s disease. arXiv preprint

arXiv:1805.03909 (2018).

62. Fransen, P. S. et al. Mr clean, a multicenter randomized clinical trial of endovascular treatment for acute ischemic stroke in

the netherlands: study protocol for a randomized controlled trial. Trials 15, 1–11 (2014).

63. Johnson, A. E. et al. Mimic-cxr, a de-identiﬁed publicly available database of chest radiographs with free-text reports. Sci.

data 6, 1–8 (2019).

64. Goodfellow, I., Bengio, Y. & Courville, A. Deep learning (MIT press, 2016).

65. Reda, I. et al. Deep learning role in early diagnosis of prostate cancer. Technol. cancer research & treatment 17,

1533034618775530 (2018).

66. Hecker, S., Dai, D. & Van Gool, L. End-to-end learning of driving models with surround-view cameras and route planners.

In Proceedings of the european conference on computer vision (eccv), 435–453 (2018).

19/20

67. Li, T., Sahu, A. K., Talwalkar, A. & Smith, V. Federated learning: Challenges, methods, and future directions. IEEE Signal

Process. Mag. 37, 50–60 (2020).

68. Al, H., Alam, T., Househ, M. & Shah, Z. Federated learning and internet of medical things – opportunities and challenges.

In 20th International Conference on Informatics, Management, and Technology in Healthcare (ICIMTH) (2022).

Author contributions statement

F.M., H.A., Z.S. contributed to conceptualization. F.M. and H.A. administered the project. F.M. curated the data, performed
data synthesis, and contributed to writing—original draft. H.A and N.E performed writing—review and editing. Z.S. and H.A.
supervised the study. All authors read and approved the ﬁnal manuscript.

Additional information

Supplementary Methods
Competing interests
The authors declare that they have no competing interests.

20/20

","2 2 0 2 t c O 3 2 ] G L . s c [ 1 v 2 6 4 3 1 . 0 1 2 2 : v i X r a Artiﬁcial Intelligence-Based Methods for Fusion of Electronic Health Records and Imaging Data* Farida Mohsen1, Hazrat Ali1, Nady El Hajj1,2, and Zubair Shah1,* 1College of Science and Engineering, Hamad Bin Khalifa University, Qatar Foundation, 34110 Doha, Qatar 2College of Health and Life Sciences, Hamad Bin Khalifa University, Qatar Foundation, 34110 Doha, Qatar *Correspondence to: Zubair Shah ABSTRACT Healthcare data are inherently multimodal, including electronic health records (EHR), medical images, and multi-omics data. Combining these multimodal data sources contributes to a better understanding of human health and provides optimal personalized healthcare. The most important question when using multimodal data is how to fuse them - a ﬁeld of growing interest among researchers. Advances in artiﬁcial intelligence (AI) technologies, particularly machine learning (ML), enable the fusion of these different data modalities to provide multimodal insights. To this end, in this scoping review, we focus on synthesizing and analyzing the literature that uses AI techniques to fuse multimodal medical data for different clinical applications. More speciﬁcally, we focus on studies that only fused EHR with medical imaging data to develop various AI methods for clinical applications. We present a comprehensive analysis of the various fusion strategies, the diseases and clinical outcomes for which multimodal fusion was used, the ML algorithms used to perform multimodal fusion for each clinical application, and the available multimodal medical datasets. We followed the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) guidelines. We searched Embase, PubMed, Scopus, and Google Scholar to retrieve relevant studies. After pre-processing and screening, we extracted data from 34 studies that fulﬁlled the inclusion criteria. We found that studies fusing imaging data with EHR are increasing and doubling from 2020 to 2021. In our analysis, a typical workﬂow was observed: feeding raw data, fusing different data modalities by applying conventional machine learning (ML) or deep learning (DL) algorithms, and ﬁnally, evaluating the multimodal fusion through clinical outcome predictions. Speciﬁcally, early fusion was the most used technique in most applications for multimodal learning (22 out of 34 studies). We found that multimodality fusion models outperformed traditional single-modality models for the same task. Disease diagnosis and prediction were the most common clinical outcomes (reported in 20 and 10 studies, respectively) from a clinical outcome perspective. Neurological disorders were the dominant category (16 studies). From an AI perspective, conventional ML models were the most used (19 studies), followed by DL models (16 studies). Multimodal data used in the included studies were mostly from private repositories (21 studies). Through this scoping review, we offer new insights for researchers interested in knowing the current state of knowledge within this research ﬁeld. Introduction Over the past decade, digitization of health data have grown tremendously with increasing data repositories spanning the healthcare sectors1. Healthcare data are inherently multimodal, including electronic health records (EHR), medical imaging, multi-omics, and environmental data. In many applications of medicine, the integration (fusion) of different data sources has become necessary for effective prediction, diagnosis, treatment, and planning decisions by combining the complementary power of different modalities, thereby bringing us closer to the goal of precision medicine2, 3. Data fusion is the process of combining several data modalities, each providing different viewpoints on a common phenomenon to solve an inference problem. The purpose of fusion techniques is to effectively take advantage of cooperative and complementary features of different modalities4, 5. For example, in interpreting medical images, clinical data is often necessary for making effective diagnostic decisions. Many studies found that missing pertinent clinical and laboratory data during image interpretation decreases the radiologists’ ability to accurately make diagnostic decisions6. The signiﬁcance of clinical data to support the accurate interpretation of imaging data is well established in radiology as well as in a wide variety of imaging-based medical specialties such as dermatology, ophthalmology, and pathology that depend on clinical context to interpret imaging data correctly7–9. Thanks to the advances of AI and ML models, one can achieve a useful fusion of multimodal data with high-dimensionality10, various statistical properties, and different missing value patterns11. Multimodal ML is the domain that can integrate different *This is pre-print of paper accepted for publication in Scientiﬁc Reports. Cite the ﬁnal version from Nature Scientiﬁc Reports. Email: haali2@hbku.edu.qa, zshah@hbku.edu.qa data modalities. In recent years, multimodal data fusion has gained much attention for automating clinical outcome prediction and diagnosis. This can be seen in Alzheimer’s disease diagnosis and prediction12–15 when imaging data were combined with speciﬁc lab test results and demographic data as inputs to ML models, and better performance was achieved than the single-source models. Similarly, fusing pathological images with patient demographic data observed an increase in performance in comparison with single modality models for breast cancer diagnosis16. Several studies found similar advantages in various medical imaging applications, including diabetic retinopathy prediction, COVID-19 detection, and glaucoma diagnosis17–19. This scoping review focuses on studies that use AI models to fuse medical images with EHR data for different clinical applications. Modality fusion strategies play a signiﬁcant role in these studies. In the literature, some other reviews have been published on the use of AI for multimodal medical data fusion20–26; however, they differ from our review in terms of their scope and coverage. Some previous studies focused on the fusion of different medical imaging modalities20, 21; they did not consider the EHR in conjunction with imaging modalities. Other reviews focused on the fusion of omics data with other data modalities using DL models22, 23. Another study24 focused on the fusion of various internet of medical things (IoMTs) data for smart healthcare applications. Liu et al.27 focused exclusively on integrating multimodal EHR data, where multimodality refers to structured data and unstructured free texts in EHR, using conventional ML and DL techniques. Huang et al.26 discussed fusion strategies of structured EHR data and medical imaging using DL models emphasizing fusion techniques and feature extraction methods. Furthermore, their review covered the research till 2019 and retrieved only 17 studies. In contrast, our review focuses on studies using conventional ML or DL techniques with EHR and medical imaging data, covering 34 recent studies. Table 1 provides a detailed comparison of our review with existing reviews. The primary purpose of our scoping review is to explore and analyze published scientiﬁc literature that fuses EHR and medical imaging using AI models. Therefore, our study aims to answer the following questions: 1. Fusion Strategies: what fusion strategies have been used by researchers to combine medical imaging data with EHR? What is the most used method? 2. Diseases: For what type of diseases are fusion methods implemented? 3. Clinical outcomes and ML methods: What types of clinical outcomes are addressed using the different fusion strategies? What kind of ML algorithms are used for each clinical outcome? 4. Resource: What are the publicly accessible medical multimodal datasets? We believe that this review will provide a comprehensive overview to the readers on the advancements made in multimodal ML for EHRs and medical imaging data. Furthermore, the reader will develop an understanding of how ML models could be designed to align data from different modalities for various clinical tasks. Besides, we believe that our review will help identify the lack of multimodal data resources for medical imaging and EHR, thus motivating the research community to develop more multimodal medical data. Preliminaries We ﬁrst identify the EHR and medical imaging modalities that are the focus of this review. Then, we present the data fusion strategies that we use to investigate the studies from the perspective of multimodal fusion. Data modalities In this review, we focus on studies that use two primary data modalities: • Medical imaging modality: This includes N-dimensional imaging information acquired in clinical practice, such as X-ray, Magnetic Resonance Imaging (MRI), functional MRI (fMRI), structural MRI (sMRI), Positron Emission Tomography (PET), Computed Tomography (CT), and Ultrasound. • EHR data: This includes both structured and unstructured free-text data. Structured data include coded data such as diagnosis codes, procedure codes, numerical data such as laboratory test results, and categorical data such as demographic information, family history, vital signs, and medications. Unstructured data include medical reports and clinical notes. In our review, we consider studies combining the two modalities of EHR and imaging. However, there exist cases where the data could contain only multiple EHR modalities (structured and unstructured) or multiple imaging modalities (e.g., PET and MRI). We consider such data as a single modality, i.e., the EHR modality or imaging modality. 2/20 Previous Reviews A review on multimodal fusion: medical image Compendious analysis of medical modalities, databases, multimodal fusion and quality metrics20 Advances in multimodality data fusion in neuroimag- ing21 techniques Year Scope and Coverage 2022 Their review focused on the fusion of differ- ent medical imaging modalities. 2021 Their review focused on the fusion of dif- ferent imaging modalities, considering neu- roimaging applications for brain diseases and neurological disorders. An overview of deep learn- ing methods for multimodal medical data mining22 2022 Their review focused on the fusion of dif- ferent types of multi-omics data with EHR and different imaging modalities, only con- sidering DL models for speciﬁc diseases (COVID-19, cancer, and Alzheimer’s). 2022 Their review focused on the fusion of dif- ferent types of multi-omics data with EHR and imaging modalities, considering only DL models. Moreover, they did not provide a summary of the freely accessible multi- modal datasets and a summary of evaluation measures used to evaluate the multimodal models. 2021 Their survey did not focus on fusing med- ical imaging with EHR but rather covered the fusion of IoMTs data for smart health- care applications and covered studies pub- lished untill 2020. Moreover, in their review, multimodality referred to fusing either dif- ferent 1D medical signals (such as electro- cardiogram (ECG) and biosignals), different medical imaging modalities, or 1D medical signals with imaging. 2021 Their review focused on the fusion of struc- tured and unstructured EHR data and did not consider medical imaging modalities. More- over, they did not provide a summary of the freely accessible multimodal datasets and a summary of evaluation measures used to evaluate the multimodal models. Multimodal deep learning for biomedical data fusion: a review23 A comprehensive survey on multimodal medical signals fusion for smart healthcare systems24 learning for Machine multimodal electronic health records-based re- search: Challenges and perspectives27 Fusion of medical imag- ing and electronic health records using deep learning: a systematic review and im- plementation guidelines26 Comparative contribution of our review Our review focused on the fusion of medi- cal imaging with multimodal EHR data and considered different imaging modalities as a single modality. The two reviews did not share any common studies. Our review focused on the fusion of medical imaging with EHR data, considering vari- ous diseases, such as neurological disorders, cancer, cardiovascular diseases, psychiatric disorders, eye diseases, and Covid-19. The two reviews did not share any common stud- ies. Our review focused on the fusion of medi- cal imaging with EHR data, considering all AI models for various diseases, such as neu- rological disorders, cancer, cardiovascular diseases, psychiatric disorders, eye diseases, and Covid-19. The two reviews did not share any common studies. Our review focused on the fusion of med- ical imaging with EHR data, considering all AI models. Moreover, o ur study pro- vided a summary of the accessible multi- modal datasets and a summary of evaluation measures used to evaluate the multimodal models. The two reviews only shared two common studies. Our review focused on the fusion of medical imaging with EHR (structured and unstruc- tured) for different clinical applications. It included 34 studies, most of them published in 2021 and 2022, with no study common between the two reviews. Our review focused on the fusion of medi- cal imaging with EHR and considered struc- tured and unstructured data in EHR as a sin- gle modality. The two reviews did not share any common studies. 2020 Their review focused on the fusion of struc- tured EHR data and medical imaging, con- sidering only DL models, and included only 17 studies published until 2019. Our review focused on the fusion of medi- cal imaging with EHR data, considering all AI models, and included 34 studies, almost more than half published in 2020 and 2021. Table 1. Comparison with previous reviews. 3/20 Fusion strategies As outlined in26, fusion approaches can be categorized into early, late, and joint fusion. These strategies are classiﬁed depending on the stage in which the features are fused in the ML model. Our scoping review follows the deﬁnitions in26 and attempts to match each study to its taxonomy. In this section, we brieﬂy describe each fusion strategy: • Early fusion: It joins features of multiple input modalities at the input level before being fed into a single ML algorithm for training26. The modality features are extracted either manually or by using different methods such as neural networks (NN), software, statistical methods, and word embedding models. When NN are used to extract features, early fusion requires training multiple models: the feature extraction models and the single fusion model. There are two types of joint fusion: type I and type II. Type I fuses the original features without extracting features, while type II fuses extracted features from modalities. • Late fusion: It trains separate ML models on data of each modality, and the ﬁnal decision leverages the predictions of each model26. Aggregation methods such as weighted average voting, majority voting, or a meta-classiﬁer are used to make the ﬁnal decision. This type of fusion is often known as decision-level fusion. • Joint fusion: It combines the learned features from intermediate layers of NN with features from other modalities as inputs to a ﬁnal model during training26. In contrast to early fusion, the loss from the ﬁnal model is propagated back to the feature extraction model during training so that the learned feature representations are improved through iterative updating of the feature weights. NNs are used for joint fusion since they can propagate loss from the ﬁnal model to the feature extractor(s). There are two types of joint fusion: type I and type II. The former is when NNs are used to extract features from all modalities. The latter is when not all the input modalities’ features are extracted using NNs26. Methods In this scoping review, we followed the guidelines recommended by the PRISMA-ScR28. Search strategy In a structured search, we searched four databases, including Scopus, PubMed, Embase, and Google Scholar, to retrieve the relevant studies. We note here that MEDLINE is covered in PubMed . For Google Scholar search results, we selected the ﬁrst 110 relevant studies, as, beyond 110 entries, the search results rapidly lost relevancy and were unmatched to our review’s topic. Furthermore, we limited our search to English-language articles published in the last seven years between January 1, 2015, and January 6, 2022. The search was based on abstracts and titles and was conducted between January 3 and January 6, 2022. In this scoping review, we focused on applying AI models to multimodal medical data-based applications. The term multimodal refers to combining medical imaging and EHR, as described in Preliminaries section. Therefore, our search string incorporated three major terms connected by AND:( (“Artiﬁcial Intelligence” OR “machine learning” OR “deep learning”) AND “multimodality fusion” AND (“medical imaging” OR “electronic health records”)). We used different forms of each term. We provide the complete search string for all databases in Appendix 1 of the supplementary material. Inclusion and exclusion criteria We included all studies that fused EHR with medical imaging modalities using an AI model for any clinical application. As AI models, we considered classical ML models, DL models, transfer learning, ensemble learning, etc as mentioned in the search terms in Appendix 1 of the supplementary material. We did not consider studies that use classical statistical models such as regression in our review. Our deﬁnition of imaging modalities is any type of medical imaging used in clinical practice, such as MRI, PET, CT scans, and Ultrasound. We considered both structured and unstructured free-text patients’ data for EHR modalities as described in Preliminaries section . Only peer-reviewed studies and conference proceedings were included. Moreover, all included studies were limited to English language only. We did not enforce restrictions on types of disorders, diseases or clinical tasks. We excluded studies that used a single data modality. Also, we excluded studies that used different types of data from the same modality, such as studies that only combined two or more imaging types (e.g. PET and MRI), as we considered this single modality. Moreover, studies that integrated original imaging modalities with extracted imaging features were excluded as this was still considered a single modality. Also, studies that combined multi-omics data modality were excluded. In addition, studies that were unrelated to the medical ﬁeld or did not use AI-based models were excluded. We excluded reviews, conference abstracts, proposals, editorials, commentaries, letters to editors, preprints, and short letters articles. Non-English publications were also excluded. 4/20 Study selection We used Rayyan web-based review management tool29 for the ﬁrst screening and study selection. After removing duplicates, we screened the studies based on title and abstract. Subsequently, full-text of the selected studies from the title and abstract screening were assessed for eligibility using our inclusion and exclusion criteria. Two authors (F.M. and H.A.) conducted the study selection and resolved any conﬂict through discussion. A third author (Z.S.) was consulted when an agreement could not be reached. Data extraction From the ﬁnal included studies, a data extraction form was designed and piloted on four studies to develop a systematic and accurate data extraction process. The extracted data from the studies are ﬁrst author’s name, year, the country of the ﬁrst author’s institution, disease’s name, clinical outcome, imaging modality, EHR modality, fusion strategy, feature extraction methods, data source, AI models, evaluation metrics, and comparison with single modality. In Appendix 2 of the supplementary material, we provide the extracted information description in detail. One author (F.M.) performed the data extraction, and two other authors (Z.S. and H.A.) reviewed and veriﬁed the extracted data. Any disagreement was resolved through discussion and consensus between the three authors. Data synthesis Following the data extraction, we used a narrative approach to synthesize the data. We analyzed the studies from ﬁve perspectives: fusion strategies, diseases, clinical outcomes with ML algorithms, data sources/type, and evaluation mechanism. For fusion strategies, we focused on how the multimodal data was fused. In addition, we recorded implementation details of the model, such as feature extraction and single modality evaluation. We also extracted information on the diseases for which fusion methods were implemented. Furthermore, we analyzed where the data fusion models were applied for clinical outcomes and what ML models were used for each task. Moreover, we focused on the type of imaging and EHR data used by the studies, the source of data, and its availability. Finally, for evaluation, we focused on the evaluation metrics used by each study. Study quality assessment In accordance with the guidelines for scoping reviews30, 31, we did not perform quality assessments of the included studies. Results Search results A total of 1158 studies were retrieved from the initial search. After duplicates elimination, 971 studies were retained. Based on our study selection criteria (see Methods), 44 studies remained for full-text review after excluding articles based on their abstract and title. Moreover, 10 studies were removed after the full-text screening. Finally, 34 studies met our inclusion criteria and were selected for data extraction and synthesis. Figure 1 shows a ﬂowchart of the study screening and selection process. Demographics of the studies As presented in Table 2, approximately two-thirds of the studies were journal articles (n= 23, ∼ 68%)12–15, 17, 19, 25, 32–46, whereas 11 studies were conference proceedings (∼ 32%)16, 47–56. Most of the studies were published between 2020 and 2022 (n = 22, ∼ 65%). Figure 2 shows a visualization of the publication type-wise and year-wise distribution of the studies. The included studies were published in 13 countries; however, most of these studies were from the USA (n = 10, ∼ 30%) and China (n = 8, ∼ 24%). Data fusion strategies We mapped the included studies to the taxonomy of fusion strategies outlined in the Preliminaries Section. A primary interest of our review is to identify the fusion strategies that the included studies used to improve the performance of ML models for different clinical outcomes. Early fusion The majority of the included studies (n = 22, ∼ 65%) used early fusion to combine medical imaging and non-imaging data. When the input modalities have different dimensions, such as when combining one-dimensional (1D) EHR data with 2D or 3D imaging data, it is essential to extract high-level imaging features in 1D before fusing with 1D EHR data. To accomplish this, various methods were used in the studies, including neural network-based features extraction, data generation through software, or manual extraction of features. Out of the 22 early fusion studies, 19 studies12, 13, 15, 25, 33–36, 39, 41–45, 50–53 used manual or software-based imaging features, and 3 studies used neural network-based architectures to extract imaging features before combining with other clinical data modality16, 18, 54. Six out of the 19 studies that used manual or software-based features 5/20 Figure 1. PRISMA ﬂow chart for study identiﬁcation, screening, and selection. 6/20 Characteristics Year Number of studies 2022 2021 2020 2019 2018 2016 2015 Country United States of America (USA) China United Kingdom (UK) Germany India Australia Denmark Iran Korea Pakistan Kingdom of Saudi Arabia Singapore Publication type Journal Conference 1 14 7 2 5 4 1 10 8 4 2 2 1 1 1 1 1 1 1 23 11 Table 2. Demographics of the included studies. reduced the feature dimension before concatenating the two modalities’ features using different methods25, 36, 45, 50–52. Such methods include recursive feature elimination52, a ﬁlter-based method using Pearson correlation coefﬁcient51, Random Forest feature selection based on Gini importance50, Relief-based feature selection method25, a wrapper-based method using backward feature elimination36, and a rank-based method using Gini coefﬁcients45. Moreover, 3 studies13, 15, 44 utilized the principal component analysis (PCA) dimensionality reduction technique to reduce the feature dimension. In the studies that used neural network-based architectures to extract imaging features, CNN architectures were used in three studies16, 18, 54. These studies concatenated the multimodal features (CNN-extracted and EHR features) for their fusion strategy. Fourteen early fusion studies evaluated their fusion models’ performance against that of single modality mod- els12, 13, 15, 16, 18, 25, 32–34, 36, 41–44, 51 . As a result, 13 of these studies exhibited a better performance for fusion when compared with their imaging-only and clinical-only counterparts12, 13, 15, 16, 18, 25, 32–34, 41–44, 51. Joint fusion Joint fusion was the second most common fusion strategy used in 10 out of the 34 studies. In these studies, different neural network-based methods were used for processing the imaging and EHR data modalities. Chen et al.39 used the Visual Geometry Group (VGG-16) architecture to extract features from MRI images, while they used a bidirectional long-short term memory (LSTM ) network with an attention layer to learn feature representation from MRI reports. Then, they concatenated the learned features of the two modalities before feeding them into a stacked K-nearest neighbor (KNN) attention pooling layer. Grant et al.55 used a Residual Network (ResNet50) architecture to extract relevant features from the imaging modality and fully connected NN to process the non-imaging data. They directly concatenated the learned feature representation of the imaging and non-imaging data and fed them into two fully connected networks. Yidong et al.19 used a Bayesian CNN encoder-decoder to extract imaging features and a Bayesian Multilayer perception (MLP) encoder-decoder to process the medical indicators data. The study directly concatenated the two feature vectors and fed the resulted vector into another Bayesian MLP. Samak et al.47 utilized CNN with a self-attention mechanism to extract the imaging features and fully connected NNs to process the metadata information. Lili et al.39 used VGG-19 architecture to extract the multimodal MRI features and fully connected networks for clinical data. The study concatenated the two feature vectors and fed them into fully connected NN. Another study46 applied 7/20 Figure 2. The distribution of studies by the type of publication and the year. CNN layers for imaging features extraction and word embeddings (Word2vec) with self-attention for textual medical data. In another research38, Fang et al. applied a ResNet architecture and MLP for imaging and clinical data feature extraction. Then, the authors fused the feature vectors by concatenation and fed them into an LSTM network followed by a fully connected network. Hsu et al.17 concatenated the imaging features extracted using Inception-V3 model with the clinical data features before feeding them to fully connected NN. In56, Sharma et al. used CNN to extract image features and then concatenated them directly with the clinical data to feed into a SoftMax classiﬁer. Xu et al.53 used AlexNet architecture to convert the imaging data into a feature vector fusible with other non-image modalities. Then, they jointly learned the non-linear correlations among all modalities using fully connected NN. Out of 10 joint fusion studies, seven studies evaluated their fusion models’ performance against that of a single modality and reported a performance improvement when fusion was used17, 39, 46, 47, 49, 53, 55. Late fusion Late fusion was the least common fusion approach used in the included studies, as only two studies used it. Qiu et al.37 trained three independent imaging models that took a single MRI slice as input, then aggregated the prediction of these models using maximum, mean, and majority voting. After combining the results of these aggregations by majority vote, the study performed late fusion with the clinical data models. In another study40, Huang et al. trained four different late fusion models. Three models took the average of the predicted probabilities from the imaging and EHR modality models as the ﬁnal prediction. The fourth model used an NN classiﬁer as an aggregator, which took as input the single modality models’ prediction. The study also created early, joint fusion models and two single modality models to compare with late fusion performance. As a result, the late fusion outperformed both the early and joint fusion models and the single modality models. Diseases We categorized the diseases and disorders in the included studies into seven types: neurological disorders, cancer, cardiovascular diseases, Covid-19, psychiatric disorders, eye diseases, and other diseases. The majority of the included studies focused on neurological disorders (n = 16). Table 3 shows the distribution of the included studies in terms of the diseases and disorders they covered. Clinical outcomes and machine learning models Multimodal ML enables a wide range of clinical applications such as diagnosis, early prediction, patient stratiﬁcation, phenotyping, biomarkers identiﬁcation, etc. In this review, we labeled each study according to its clinical outcome. We 8/20 Disease Category Neurological disorders Alzheimer’s disease (AD) Mild cognitive impairment (MCI) Ischemic Stroke Demyelinating diseases Neurodevelopmental Deﬁcits Epilepsy Cancer Breast Cancer Glioblastoma Lung Cancer Upper Gastrointestinal (UGI) Cancer Cardiovascular diseases Aortic stenosis Cardiomegaly Myocardial Infarction Psychiatric disorder Bipolar disorder Schizophrenia Eye diseases Diabetic Retinopathy (DR) Glaucoma COVID-19 Other diseases Cervical dysplasia Pulmonary Embolism (PE) Hepatitis B Number of studies 18 7 4 2 1 1 1 5 2 1 1 1 3 1 1 1 2 1 1 2 1 1 3 3 1 1 1 Study reference 12–15, 44, 48, 49 37, 42, 50, 51 35, 47 32 39 34 16, 41 43 55 46 54 55 56 33 36 17 19 18, 25, 38 53 40 52 Table 3. Disease distribution covered by the 34 studies. categorized the retrieved clinical tasks into two main categories: diagnosis and prediction. Though some of the studies mentioned detection, classiﬁcation, diagnosis, and prediction, we categorized them under the diagnosis category. Under the early prediction group, we considered only the studies that predict diseases before onset, identify signiﬁcant risk factors, predict mortality and overall survival, and predict a treatment outcome. These clinical outcomes were implemented using multimodal ML models. This section summarizes the different clinical tasks of the retrieved studies, the fusion strategy used, and the ML models that were developed for each task. Figure 3 shows the distribution of fusion strategies associated with different diseases’ and clinical outcomes. Diagnosis The most common applied clinical outcome in the included studies was the diagnosis, reported in 20 (∼ 59%) studies. In these studies, EHRs were combined with medical imaging to diagnose a spectrum of diseases including neurological disorders (n = 9)4, 13–15, 32, 37, 42, 49, 50, psychiatric disorders (n = 2)33, 36, CVD (n =3)54–56, Cancer (n = 2)16, 55, and four studies for other different diseases18, 19, 40, 53. Speciﬁcally, most of the studies that focused on detecting neurological diseases were for AD (n = 4)13–15, 49, and MCI (n = 4)37, 42, 50, 51. Early fusion was the most utilized technique for diagnosis purposes used in 13 studies. These studies employed different ML models on the fused imaging and EHR data for diagnosing different diseases. Most of these studies were for diagnosing neurological and and psychiatric disorders such as AD13–15, MCI42, 50, 51, demyelinating diseases32, bipolar disorder33, and schizophrenia36 . Parvathy et al.13 reported diagnosing AD by fusing sMRI and PET imaging features with mini-mental state examination (MMSE) score, clinical dementia rating (CDR), and age of the subjects. They fed the fused features vector to different ML models, including support vector machine (SVM), random forest (RF), and gaussian process (GP) for classiﬁcation. Niyas et al.14 classiﬁed AD by fusing MRI, PET, demographic data, and lab tests, including cognitive tests and Cerebro-Spinal Fluid (CSF) test. They applied dynamic ensemble of classiﬁers selection algorithms using a different pool of classiﬁers on the fused features for classiﬁcation. Hamid et al.15 combined MRI and PET imaging features with personal information and 9/20 neurological data such as MMSE and CRF features for AD early diagnosis. In their study, they fed the fused features into SVM for classiﬁcation. For MCI diagnosis, Matteo et al.42 proposed combining MRI imaging with cognitive assessments for MCI diagnosis. They concatenated the features of both modalities and fed them into a linear and quadratic discriminant analysis algorithm for diagnosis. Parisa et al.50, 51 integrated features extracted from MRI and PET images with neuropsychological tests and demographic data (gender, age, and education) to diagnose MCI early. They trained SVM and deep NNs using the fused features for classiﬁcation in50 and51, respectively. In another study32, Xin et al. combined MRI imaging with structured data extracted from EHRs to diagnose demyelinating diseases using SVM. For bipolar disorder, Rashmin et al.33 combined multimodal imaging features with neuropsychological tests and personal information features. They fed them into SVM to differentiate bipolar patients from healthy patients. Ebdrup et al.36 proposed integrating MRI and diffusion tensor imaging tractography (DTI) imaging with neurocognitive tests and clinical data for schizophrenia classiﬁcation. Then, they fused the features of the two modalities and fed them to different types of ML classiﬁers, including SVM, RF, linear regression (LR), decision tree (DT), and Naïve Bayes (NB) for classiﬁcation. Moreover, two studies implemented multimodality early fusion to diagnose different cancer diseases16, 55. Yan et al.16 fused pathological images and structured data extracted from EHRs to classify malignant and benign breast cancer. Then, they fused the features of the two modalities and fed them to two fully connected NN followed by a SoftMax layer for classiﬁcation. Seung et al.55 combined PET imaging with clinical and demographic data for differentiating lung adenocarcinoma (ADC) from squamous cell carcinoma. They fed the integrated features into different algorithms such as SVM, RF, LR, NB, and artiﬁcial neural network (ANN) for classiﬁcation. For COVID-19 diagnosis, Ming et al.18 combined CT images with clinical features and fed them into different ML models, including SVM, RF, and KNN for diagnosis. Finally, Tanveer et al.54 combined features from echocardiogram reports and images, with diagnosis information for the detection of patients with aortic stenosis CVD. Their study fed the combined features to an RF learning framework to detect patients likely to have the disease. Joint fusion was used for diagnostic purposes in 5 studies19, 49, 53, 55, 56. These studies employed different types of DL architectures to learn and fuse the imaging and EHR data for diagnosis purposes. In19, they proposed a Bayesian deep multisource learning (BDMSL) model that integrated retinal images with medical indicators data to diagnose glaucoma. For this model, they used Bayesian CNN encoder-decoder to extract imaging features and a Bayesian MLP encoder-decoder to process the medical indicators data. The two feature vectors were directly concatenated and fed into Bayesian MLP for classiﬁcation. Chen et al.49 used DL for multimodal feature extraction and classiﬁcation to detect AD; the authors used the VGG-16 model to extract features from MRI images and a bidirectional LSTM network with an attention layer to learn features from MRI reports. Then, they fed the fused features into a stacked KNN pooling layer to classify the patient’s diagnosis data. In53, Xu et al. proposed an end-to-end deep multimodal framework that can learn better complementary features from the image and non-image modalities for cervical dysplasia diagnosis. They used CNN, speciﬁcally AlexNet architecture, to convert the cervigram image data into a feature vector fusible with other non-image modalities. After that, they jointly learned the non-linear correlations among all modalities using fully connected NN for cervical dysplasia classiﬁcation. Another two studies55, 56 also employed DL models to jointly learn multimodal feature representation for diagnosing CVDs. The former55 proposed a multimodal network for cardiomegaly classiﬁcation, which simultaneously integrates the non-imaging intensive care unit (ICU) data (laboratory values, vital sign values, and static patient metadata, including demographics) and the imaging data (chest X-ray). They used a ResNet50 architecture to extract features from the X-ray images and fully connected NN to process the ICU data. To join the learned imaging and non-imaging features, they concatenated the learned feature representation and fed them into two fully connected layers to generate a label for cardiomegaly diagnosis. The latter study56 proposed a stacked multimodal architecture called SM2N2, which integrated clinical information and MRI images. In their research, they used CNN to extract imaging features, and then they concatenated these features with clinical data to feed into a SoftMax classiﬁer for myocardial infarction detection. Late fusion was implemented in 2 studies37, 40 for disease diagnosis purposes. Fang et al.37 proposed the fusion of MRI scans, logical memory (LM) tests, and MMSE for MCI classiﬁcation. Their study utilized VGG-11 architecture for MRI feature extraction and developed two MLP models for MMSE and LM test results. Then, they combined both MRI and MLP models using majority voting. As a result, the fusion model outperformed the individual models. Huang et al.40 utilized a non-open dataset comprising CT scans and EHR data to train two unimodal and four late fusion models for PE diagnosis. They used their previously implemented architecture (PENet)57 to encode the CT images and a feedforward network to encode the tabular data. The late fusion approach performed best among the fusion models and outperformed the models trained on the image-only and the tabular-only data. Early Prediction Prediction tasks were reported in 14 (∼ 41.2%) studies. In these studies, EHRs were fused with medical imaging to predict different outcomes, including disease prediction, mortality prediction, survival prediction, and treatment outcome prediction. Ten studies of the prediction tasks were disease prediction12, 17, 34, 38, 39, 41, 44, 46, 48, 52, which involved determining whether an individual might develop a given disease in the future. The second most common prediction task was treatment outcome 10/20 prediction reported in 2 studies35, 47, followed by one study for mortality prediction and overall survival prediction25, 43, respectively. The early fusion technique was used in 6 studies12, 34, 41, 44, 48, 52 for disease prediction. Minhas et al.12 proposed an early fusion model to predict which subjects will progress from MCI to AD in the future. The study concatenated MRI extracted features with demographic and neuropsychological biomarkers before feeding them to an SVM model for prediction. Ali et al.34 proposed a model to predict Epileptogenic-Zone in the Temporal Lobe by feeding MRI extracted features integrated with set-of-semiology features into various ML models such as LR, SVM, and Gradient Boosting. Ma et al.41 fused MRI and clinicopathological features for predicting metachronous distant metastasis (DM) in breast cancer. They fed the concatenated features to an LR model. Another study44 combined MRI-derived features and high-throughput brain phenotyping to diagnose and predict the onset of AD. They fed the fused features into different ML classiﬁers, including RF, SVM, and LR. Ulyana et al.48 trained a deep, fully connected network as a regressor in a 5-year longitudinal study on AD to predict cognitive test scores at multiple future time points. Their model produced MMSE scores for ten unique future time points at six-month intervals by combing biomarkers from cognitive test scores, PET, and MRI. They early fused imaging features with the cognitive test scores through concatenation before feeding them into the fully connected network. Finally, Bai et al.52 compared different multimodal biomarkers (clinical data, biochemical and hemologic parameters, and ultrasound elastography parameters) for predicting the assessment of ﬁbrosis in chronic hepatitis B using SVM. For disease prediction, joint fusion was used in 4 studies17, 38, 39, 46. Hsu et al.17 proposed a deep multimodal fusion model that trained heterogeneous data from fundus images and non-image data for DR screening. They concatenated the imaging extracted features from Inception-V3 with the clinical data features before feeding them to fully connected NN followed by SoftMax layer for classiﬁcation. Fang et al.38 developed a prediction system by jointly fusing CT scans and clinical data to predict the progression of COVID-19 malignancy. In their study, the feature extraction part applied a ResNet architecture and MLP for CT and clinical data, respectively. Then, they concatenated the different features and fed them into an LSTM network followed by a fully connected NN for prediction. In39, the authors proposed a deep multimodal model for predicting neurodevelopmental deﬁcits at 2 years of age. Their model consisted of a feature extractor and fusion classiﬁer. In the feature extractor, they used VGG-19 architecture to extract MRI features and fully connected NN for clinical data. Then, the study combined the extracted features of the two modalities and fed their combination to another fully connected network in the fusion classiﬁer for prediction. To evaluate the performance of the modality fusion, they tested their model using a single modality of MRI and clinical features. The results showed that multimodal fusion outperformed the single modality performance. Another study46 also used multimodal joint fusion for UGI cancer screening. Their model integrated features extracted from UGI endoscopic images with corresponding textual medical data. They applied CNN for image feature extraction and word embeddings (Word2vec) with self-attention for textual medical data feature extraction. After that, they concatenated the extracted features of the two modalities and fed them into fully connected NN for prediction. Their results showed that multimodal fusion outperformed the single modality performance. For treatment outcome prediction35, 47, the former35 implemented early fusion while the latter47 used joint fusion. For acute ischemic stroke, Gianluca et al.35 evaluated the predictive power of imaging, clinical, and angiographic features to predict the outcome of acute ischemic stroke using ML. The study early fused all features into gradient boosting classiﬁers for prediction. In47, the authors proposed a DL model to directly exploit multimodal data (clinical metadata and non-contrast CT (NCCT) imaging data) to predict the success of endovascular treatment for ischemic stroke. They utilized CNN with a self-attention mechanism to extract the features of images, and then they concatenated them with the metadata information. Then, the classiﬁcation stage of the proposed model processed the fused features through a fully connected NN, followed by the Softmax function applied to the outputs. Their results showed that multimodal fusion outperformed the single modality performance. Both the mortality and overall survival prediction studies25, 43 implemented early fusion. In25, they developed a model to predict COVID-19 ventilatory support and mortality early on to prioritize patients and manage the hospital resources’ allocation. They fused patients’ CT images and EHR data features by concatenation before feeding them to different ML models, including SVM, RF, LR, and eXtreme gradient boosting. They evaluated the performance against single modality models and observed that the results for multimodal fusion were better. The other study43 aimed to develop ML models to predict glioblastoma patients’ overall survival (OS) and progression-free survival (PFS) based on combining treatment features, pathological, clinical, PET/CT-derived information, and semantic MRI-based features. They concatenated the features of all modalities and fed them to an RF model. The study showed that the model based on multimodal fusion data outperformed the single modality models. Datasets Patient Data Types The included studies reported medical imaging and EHRs (structured and non-structured) patient’s data types. In terms of imaging modality, CT, MRI, fMRI, structural MRI (sMRI), PET, Diffusion MRI, DTI, ultrasound, X-ray, fundus images, and PET were used in the studies. MRI and PET images were the most utilized modalities. Out of the included 34 studies, 13 used 11/20 Figure 3. Fusion strategies associated with clinical outcomes for different diseases. 12/20 MRI images, and 8 used PET images mostly for AD diagnosis and prediction. In terms of EHRs, structured data was the most commonly used modality (n = 32). Table 4 summarizes the types of imaging and EHR data used in the studies. Data Type Imaging Data MRI imaging MRI DTI fMRI sMRI and Diffusion MRI PET CT X-ray fundus images Ultrasound Echocardiography Pathological images Cervigram images Endoscopy images EHR Data Structured Unstructured Number of studies Study reference 13 3 2 1 8 7 2 2 1 1 1 1 1 32 2 12, 14, 15, 32, 33, 37, 41–43, 48–51 33, 36, 39 33, 39 44 13–15, 43, 45, 48, 50, 51 18, 35, 38, 40, 43, 45, 47 25, 55 17, 19 52 54 16 53 46 12–19, 25, 32–45, 47, 48, 50–56 46, 49 Table 4. Patient data types used in the included studies. Patient data Resources Almost two-thirds of the studies included in this scoping review used private data sources (clinical data that are not publicly available) (n = 21, ∼ 59%). In contrast, publicly accessible datasets were used in only 13 studies. We observed that the most used public dataset was the “Alzheimer’s Disease Neuroimaging Initiative” dataset (ADNI)58 , where 7 out of 13 studies used the dataset. Other publicly available datasets that were used among the included studies were the “National Alzheimer’s Coordinating Center” (NACC) dataset59, the “Medical Information Mart for Intensive Care” (MIMIC-IV) dataset60, the ""National Cancer Institute"" (NCI) dataset, ADNI TADPOLE dataset61 , and MR CLEAN Trial dataset62. In Table 5, we summarize the public multimodal medical datasets and their clinical applications. Considering these datasets for each clinical task, the most popular is ADNI for AD and MCI disease diagnosis and prediction. Evaluation metrics Evaluation metrics are mainly dependent on the clinical task. Typically, accuracy, the area under the curve (AUC), sensitivity, speciﬁcity, F1- measure, and precision are mostly used for the evaluation of diagnosis and prediction tasks. Table 6 shows the distribution of the evaluation measures used in the included studies Discussion This section summarizes our ﬁndings and provides future directions for research on the multimodal fusion of medical imaging and EHR. Principal ﬁndings We found that multimodal models that combined EHR and medical imaging data generally outperformed single modality models for the same task in disease diagnosis or prediction. Since our review shows that the fusion of medical imaging and clinical context data can improve the performance of AI models, we recommend attempting fusion approaches when multimodal data is obtainable. Moreover, through this review, we observed certain trends in the ﬁeld of multimodality fusion in the medical area, which can be categorized as: • Resources: We observed that multimodal data resources of medical imaging and EHR are limited owing to privacy considerations. The most prominent dataset was the ADNI, containing MRI and PET images collected from about 1700 individuals in addition to clinical and genetic information. Considering ADNI’s contributions in advancing the research, similar multimodal datasets should be developed for other medical data sources too. 13/20 Description URL Clinical outcomes Study refer- ence 13, 15 Disease diagnosis (AD) Disease diagnosis (MCI) Disease Prediction (AD) 50, 51 12, 44, 48 Disease diagnosis (AD) 14 https://adni. loni.usc.edu/ data-samples/ data-types/ https://tadpole. grand-challenge. org/Data/ Public Dataset ADNI ADNI TAD- POLE NACC MIMIC-CXR, MIMIC-IV NCI MR CLEAN Trial ADNI represents a series of studies, including ADNI 1, 2, and 3, designed to study MCI and its progression into AD. It has MRI and PET im- ages along with clinical and genetic information58. ADNI has a simpliﬁed coun- terpart, TADPOLE, which has a subset of ADNI-3 sam- ples and features.ATDPOLE does not include raw images, but it has processed struc- tural information about the images such as ROI averages, thicknesses of the cortex and volumes of brain sub-regions, etc61. The NACC dataset was es- tablished to facilitate collab- orative AD research. The dataset comprises MRI data, demographic data, neuropsy- chological testing scores, and clinical diagnosis of pa- tients59. MIMIC-CXR is a dataset of patient chest radiographs. It contains X-ray studies for 64,588 patients63. MIMIC-IV is a database for patients admitted to critical care units comprising patient stay information, patient’s ICU data, and lookup tables to allow linking to MIMIC- CXR60. Data collections produced by major NCI initiatives are listed in the NCI Data Cata- log, including Clinical data, Genomics, imaging, and Pro- teomics. A longitudinal study of 500 patients treated with en- dovascular therapy in The Netherlands for acute is- chemic stroke comprising NCCT images, CT Angiogra- phy (CTA) images, and clini- cal metadata information on its patients62. https:// naccdata.org/ requesting-data/ nacc-data Disease Diagnosis (MCI) 37 MIMIC-CXR:https: //www.nature. com/articles/ s41597-019-0322-0 MIMIC-IV:https: //physionet.org/ content/mimiciv/0. 4/ https:// datascience.cancer. gov/resources/ nci-data-catalog Disease diomegaly) diagnosis (Car- 55 Disease diagnosis (Cervical dysplasia) 53 https://www. mrclean-trial.org/ home.html Treatment outcome predic- tion (ischemic stroke) 47 Table 5. Multimodal medical datasets and clinical outcome applications. 14/20 12–15, 17, 19, 32–34, 37–41, 45, 46, 50, 51, 53, 54 Study Reference 12, 15–19, 25, 32–42, 44–47, 49–56 Number of studies 31 20 17 15 7 3 Evaluation metrics Accuracy Sensitivity (recall) AUC Speciﬁcity Precision Positive predictive value (PPV) and Nega- tive predictive value (NPV) Matthews correlation coefﬁcient (MCC) C-index Root-Mean Squared Error (RMSE) The numbers in the second column do not sum up to 34 as many studies used more than a single metric. 12, 14, 15, 17, 32–34, 38–41, 46, 50, 51, 53 13, 19, 34, 37, 45, 54 15, 34, 40 2 1 1 34, 41 43 48 12, 15–17, 19, 25, 32, 35, 37–39, 41, 45, 47, 52, 53, 55 Table 6. The distribution of evaluation metrics in the included studies. • Fusion implementation: Early fusion was the most commonly used technique in most applications for multimodal learning. Before fusing 1D EHRs data with image data in 2D or 3D, images data was converted to a 1D vector by extracting high-level representations using manual or software-generated features12, 13, 15, 25, 33–36, 39, 41–45, 50–53, or CNN- extracted features8, 16, 54. The learned imaging features from CNN often resulted in better task-speciﬁc performance than manually or software-derived features64. Based on this reviewed studies, early fusion models performed better than conventional single-modality models on the same task. Researchers can use the early fusion method as a ﬁrst attempt to learn multimodal representations since it can learn to exploit the interactions and correlations between features of each modality. Furthermore, it only requires one model to be trained, making the pipeline for training easier than that of joint and late fusion. However, if imaging features are extracted with CNN, early fusion requires multiple models to be trained. Joint fusion was the second most commonly used fusion approach. From a modality perspective, CNNs appeared to be the best option for image feature extraction. Tabular data were mainly processed using dense layers when fed into a model, while text data were mostly processed using LSTM layers followed by the attention layer. Most of the current research directly concatenated the feature vectors of the different modalities to combine multimodal data. Using NNs to implement joint fusion can be a limitation when dealing with small datasets, which means that joint fusion is preferred with large datasets. For small datasets, it is preferable to use early or late fusion methods as they can be implemented using classical ML techniques. Nevertheless, we expect and agree with26 that joint fusion models can provide better results than other fusion strategies because they update their feature representations iteratively by propagating the loss to all the feature extraction models, aiming to learn correlations across modalities. Based on the performance reported in the included studies, it is preferred to try the early and joint fusion when the relation between the two data modalities is complementary. In this review, AD diagnosis is an example in which imaging and EHRs data are dependent as relevant and accurate knowledge of the patient’s current symptomatology, personal information and imaging reports can help doctors interpret imaging results in a suitable clinical context, resulting in a more precise diagnosis. Therefore, all AD diagnosis studies in this review implemented either early fusion13–15 or joint fusion49 for multimodal learning. On the other hand, it is preferred to try late fusion when input modalities do not complement each other. For example, the brain MRI pixel data and the quantitative result of an MMSE (e.g., Qiu et al.37) for diagnosing MCI are independent, making them appropriate candidates for inclusion in the late fusion strategy. Also, late fusion does not impose the requirement of a huge amount of training data, so it could be used when the modalities data sizes are small. Moreover, late fusion strategy could be attempted when the concatenation of feature vectors from multiple modalities results in high-dimensional vectors that are difﬁcult for ML algorithms to learn without overﬁtting unless many input samples are available. In late fusion, multiple models are employed, each specialized in a single modality, thereby limiting the size of the input feature vector for each model. Furthermore, late fusion could be used when data is incomplete or missing, i.e., some patients have only imaging data but no clinical data or vice versa. This is because late fusion uses independent models for different modalities, and aggregation methods like averaging and majority voting can be used even when predictions from a modality are not present. Moreover, predictions could be disproportionately inﬂuenced by the most feature-rich input modality when the number of features is very different between the input data modalities65; in this scenario, late fusion is preferable because it allows training each model using each modality separately. • Applications: In this review, we found that AD diagnosis and prediction12–15, 44, 48, 49 were the most common applications 15/20 addressed in a multimodal setting among studies. Using ML fusion techniques consistently demonstrated improved AD diagnosis, while clinicians experience difﬁculty with accurate and reliable diagnosis even when multimodal data is available26. This emphasizes the utility and signiﬁcance of multimodal fusion approaches in clinical applications. • Prospects: In this review, we noted that multimodal medical data fusion is growing due to its potential in achieving state-of-the-art performance for healthcare applications. Nonetheless, this growth is hampered by the absence of adequate data for benchmarking methods. This is not surprising, given the privacy concerns surrounding revealing healthcare data. Moreover, we observed a lack of complexity in the used non-imaging data, particularly in the context of heavily feature-rich data included in the EHR. For example, the majority of studies focused mostly on basic demographic data like gender and age12, 15, 44, 51, a limited number of studies also included medical histories such as smoking status and hypertension18, 55 or speciﬁc clinical characteristics that are known to be associated with a certain disease, such as an MMSE for diagnosing AD. In addition to selecting the disease-associated features, future research may beneﬁt from using vast amounts of feature-rich data, as demonstrated in domains outside of medicine, such as autonomous driving66. Future directions Although we focus on EHR and medical imaging as multimodal data, other modalities such as multi-omics and environmental data could also be integrated using the aforementioned fusion approaches. As the causes of many diseases are complex, many factors, including inherited genetics, lifestyle, and living environments, contribute to the development of diseases. Therefore, combining multisource data, e.g. EHR, imaging, and multi-omics data, may lead to a holistic view that can improve patient outcomes through personalized medicine. Although we focus on EHR and medical imaging as multimodal data, other modalities such as multi-omics and environmen- tal data could also be integrated using the aforementioned fusion approaches. As the causes of many diseases are complex, many factors, including inherited genetics, lifestyle, and living environments, contribute to the development of diseases. Therefore, combining multisource data, e.g. EHR, imaging, and multi-omics data, may lead to a holistic view that can improve patient outcomes through personalized medicine. Moreover, the unavailability of multimodal public data is a limitation that hinders the development of corresponding research. Many factors (e.g., gender, ethnicity, environmental factors) could inﬂuence the research directions or even clinical decision, relying on a few publicly available datasets might not be enough for making conclusive clinical claims to the global population27. Consequently, it is imperative to encourage the sharing of ﬂexible data among institutions and hospitals in order to facilitate the exploration of a wider range of population data for clinical research. In ML, federated learning (FL)67, 68 provides the ability to collect data safely and securely from multiple centers. It may be used to collect multimodal data from various centers to train a large-scale model without collecting data directly. Limitations Our search was limited to studies published within the previous seven years (2015-2022). We only considered studies published in English, which may have led to leaving out some studies published in other languages. We solely included studies fusing EHR with medical imaging. We did not include studies that used other data modalities such as multi-omics data, as they are out of the scope of this work. Because positive results are typically reported disproportionately, publication bias might be another limitation of this review. This bias may result in an overestimation of the beneﬁts associated with multimodal data analysis. The studies included in this review employed various input modalities, investigated various clinical tasks for different diseases, and reported different performance metrics; hence a direct comparison of the results presented in the studies is not always applicable. Furthermore, not all articles provided conﬁdence bounds, making it difﬁcult to compare their results statistically. Conclusion Multimodal ML is an area of research that is gaining attention within the medical ﬁeld. This review surveyed multimodal medical ML literature that combines EHR with medical imaging data. It discussed fusion strategies, the clinical tasks and ML models that implemented data fusion, the type of diseases, and the publicly accessible multimodal data for medical imaging and EHRs. Furthermore, it highlighted some directions to pave the way for future research. Our ﬁnding suggests that there is a growing interest in multimodal medical data. Still, most studies combine the modalities with relatively simple strategies, which despite being shown to be effective, might not fully exploit the rich information embedded in these modalities. As this is a fast-growing ﬁeld and new AI models with multimodal data are constantly being developed, there might exist studies that fall outside our deﬁnition of fusion strategies or use a combination of these strategies. We believe that the development of this ﬁeld will give rise to more comprehensive multimodal medical data analysis and will be of great support to the clinical decision-making process. 16/20 Data availability The data generated during this scoping review is provided as supplementary materials. References 1. Murdoch, T. B. & Detsky, A. S. The inevitable application of big data to health care. Jama 309, 1351–1352 (2013). 2. Obermeyer, Z. & Emanuel, E. J. Predicting the future—big data, machine learning, and clinical medicine. The New Engl. journal medicine 375, 1216 (2016). 3. Roski, J., Bo-Linn, G. W. & Andrews, T. A. Creating value in health care through big data: opportunities and policy implications. Heal. affairs 33, 1115–1122 (2014). 4. Lozano-Perez, T. Autonomous robot vehicles (Springer Science & Business Media, 2012). 5. Castanedo, F. A review of data fusion techniques. The scientiﬁc world journal 2013 (2013). 6. Cohen, M. D. Accuracy of information on imaging requisitions: does it matter? J. Am. Coll. Radiol. 4, 617–621 (2007). 7. Comfere, N. I. et al. Provider-to-provider communication in dermatology and implications of missing clinical information in skin biopsy requisition forms: a systematic review. Int. journal dermatology 53, 549–557 (2014). 8. Jonas, J. B. et al. Glaucoma. The Lancet 390, 2183–2193, DOI: https://doi.org/10.1016/S0140-6736(17)31469-1 (2017). 9. Comfere, N. I. et al. Dermatopathologists’ concerns and challenges with clinical information in the skin biopsy requisition form: a mixed-methods study. J. cutaneous pathology 42, 333–345 (2015). 10. Li, Y., Wu, F.-X. & Ngom, A. A review on machine learning principles for multi-view biological data integration. Brieﬁngs bioinformatics 19, 325–340 (2018). 11. Ramachandram, D. & Taylor, G. W. Deep multimodal learning: A survey on recent advances and trends. IEEE signal processing magazine 34, 96–108 (2017). 12. Minhas, S. et al. Early mci-to-ad conversion prediction using future value forecasting of multimodal features. Comput. intelligence neuroscience 2021 (2021). 13. Pillai, P. S., Leong, T.-Y., Initiative, A. D. N. et al. Fusing heterogeneous data for alzheimer’s disease classiﬁcation. In MEDINFO 2015: eHealth-enabled Health, 731–735 (IOS Press, 2015). 14. KP, M. N. & Thiyagarajan, P. Alzheimer’s classiﬁcation using dynamic ensemble of classiﬁers selection algorithms: A performance analysis. Biomed. Signal Process. Control. 68, 102729 (2021). 15. Akramifard, H., Balafar, M. A., Razavi, S. N. & Ramli, A. R. Early detection of alzheimer’s disease based on clinical trials, three-dimensional imaging data, and personal information using autoencoders. J. medical signals sensors 11, 120 (2021). 16. Yan, R. et al. Richer fusion network for breast cancer classiﬁcation based on multimodal data. BMC Med. Informatics Decis. Mak. 21, 1–15 (2021). 17. Hsu, M.-Y. et al. Deep learning for automated diabetic retinopathy screening fused with heterogeneous data from ehrs can lead to earlier referral decisions. Transl. Vis. Sci. & Technol. 10, 18–18 (2021). 18. Xu, M. et al. Accurately differentiating between patients with covid-19, patients with other viral infections, and healthy individuals: multimodal late fusion learning approach. J. Med. Internet Res. 23, e25535 (2021). 19. Chai, Y., Bian, Y., Liu, H., Li, J. & Xu, J. Glaucoma diagnosis in the chinese context: An uncertainty information-centric bayesian deep learning model. Inf. Process. & Manag. 58, 102454 (2021). 20. Azam, M. A. et al. A review on multimodal medical image fusion: Compendious analysis of medical modalities, multimodal databases, fusion techniques and quality metrics. Comput. Biol. Medicine 144, 105253, DOI: https://doi.org/ 10.1016/j.compbiomed.2022.105253 (2022). 21. Zhang, Y.-D. et al. Advances in multimodal data fusion in neuroimaging: Overview, challenges, and novel orientation. Inf. Fusion 64, 149–187, DOI: https://doi.org/10.1016/j.inffus.2020.07.006 (2020). 22. Behrad, F. & Saniee Abadeh, M. An overview of deep learning methods for multimodal medical data mining. Expert. Syst. with Appl. 200, 117006, DOI: https://doi.org/10.1016/j.eswa.2022.117006 (2022). 23. Stahlschmidt, S. R., Ulfenborg, B. & Synnergren, J. Multimodal deep learning for biomedical data fusion: a review. Brieﬁngs Bioinforma. 23 (2022). 17/20 24. Muhammad, G. et al. A comprehensive survey on multimodal medical signals fusion for smart healthcare systems. Inf. Fusion 76, 355–375, DOI: https://doi.org/10.1016/j.inffus.2021.06.007 (2021). 25. Aljouie, A. F. et al. Early prediction of covid-19 ventilation requirement and mortality from routinely collected baseline chest radiographs, laboratory, and clinical data with machine learning. J. multidisciplinary healthcare 14, 2017 (2021). 26. Huang, S.-C., Pareek, A., Seyyedi, S., Banerjee, I. & Lungren, M. P. Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines. NPJ digital medicine 3, 1–9 (2020). 27. Liu, Z. et al. Machine learning for multimodal electronic health records-based research: Challenges and perspectives. arXiv preprint arXiv:2111.04898 (2021). 28. Tricco, A. C. et al. Prisma extension for scoping reviews (prisma-scr): checklist and explanation. Annals internal medicine 169, 467–473 (2018). 29. Ouzzani, M., Hammady, H., Fedorowicz, Z. & Elmagarmid, A. Rayyan—a web and mobile app for systematic reviews. Syst. reviews 5, 1–10 (2016). 30. Arksey, H. & O’Malley, L. Scoping studies: towards a methodological framework. Int. journal social research methodology 8, 19–32 (2005). 31. Grant, M. J. & Booth, A. A typology of reviews: an analysis of 14 review types and associated methodologies. Heal. information & libraries journal 26, 91–108 (2009). 32. Xin, B., Huang, J., Zhou, Y., Lu, J. & Wang, X. Interpretation on deep multimodal fusion for diagnostic classiﬁcation. In 2021 International Joint Conference on Neural Networks (IJCNN), 1–8 (IEEE, 2021). 33. Achalia, R. et al. A proof of concept machine learning analysis using multimodal neuroimaging and neurocognitive measures as predictive biomarker in bipolar disorder. Asian J. Psychiatry 50, 101984 (2020). 34. Alim-Marvasti, A. et al. Machine learning for localizing epileptogenic-zone in the temporal lobe: Quantifying the value of multimodal clinical-semiology and imaging concordance. Front. digital health 3, 8 (2021). 35. Brugnara, G. et al. Multimodal predictive modeling of endovascular treatment outcome for acute ischemic stroke using machine-learning. Stroke 51, 3541–3551 (2020). 36. Ebdrup, B. H. et al. Accuracy of diagnostic classiﬁcation algorithms using cognitive-, electrophysiological-, and neu- roanatomical data in antipsychotic-naïve schizophrenia patients. Psychol. medicine 49, 2754–2763 (2019). 37. Qiu, S. et al. Fusion of deep learning models of mri scans, mini–mental state examination, and logical memory test enhances diagnosis of mild cognitive impairment. Alzheimer’s & Dementia: Diagn. Assess. & Dis. Monit. 10, 737–749 (2018). 38. Fang, C. et al. Deep learning for predicting covid-19 malignant progression. Med. image analysis 72, 102096 (2021). 39. He, L. et al. Deep multimodal learning from mri and clinical data for early prediction of neurodevelopmental deﬁcits in very preterm infants. Front. neuroscience 15 (2021). 40. Huang, S.-C., Pareek, A., Zamanian, R., Banerjee, I. & Lungren, M. P. Multimodal fusion with deep neural networks for leveraging ct imaging and electronic health record: a case-study in pulmonary embolism detection. Sci. reports 10, 1–9 (2020). 41. Ma, W. et al. Distant metastasis prediction via a multi-feature fusion model in breast cancer. Aging (Albany NY) 12, 18151 (2020). 42. De Marco, M., Beltrachini, L., Biancardi, A., Frangi, A. F. & Venneri, A. Machine-learning support to individual diagnosis of mild cognitive impairment using multimodal mri and cognitive assessments. Alzheimer Dis. & Assoc. Disord. 31, 278–286 (2017). 43. Peeken, J. C. et al. Combining multimodal imaging and treatment features improves machine learning-based prognostic assessment in patients with glioblastoma multiforme. Cancer medicine 8, 128–136 (2019). 44. Wang, Y. et al. Diagnosis and prognosis of alzheimer’s disease using brain morphometry and white matter connectomes. NeuroImage: Clin. 23, 101859 (2019). 45. Hyun, S. H., Ahn, M. S., Koh, Y. W. & Lee, S. J. A machine-learning approach using pet-based radiomics to predict the histological subtypes of lung cancer. Clin. nuclear medicine 44, 956–960 (2019). 46. Ding, S., Huang, H., Li, Z., Liu, X. & Yang, S. Scnet: a novel ugi cancer screening framework based on semantic-level multimodal data fusion. IEEE J. Biomed. Heal. Informatics 25, 143–151 (2020). 18/20 47. Samak, Z. A., Clatworthy, P. & Mirmehdi, M. Prediction of thrombectomy functional outcomes using multimodal data. In Annual Conference on Medical Image Understanding and Analysis, 267–279 (Springer, 2020). 48. Morar, U. et al. A deep-learning approach for the prediction of mini-mental state examination scores in a multimodal longitudinal study. In 2020 International Conference on Computational Science and Computational Intelligence (CSCI), 761–766 (IEEE, 2020). 49. Chen, D., Zhang, L. & Ma, C. A multimodal diagnosis predictive model of alzheimer’s disease with few-shot learning. In 2020 International Conference on Public Health and Data Science (ICPHDS), 273–277, DOI: 10.1109/ICPHDS51617. 2020.00060 (2020). 50. Forouzannezhad, P., Abbaspour, A., Cabrerizo, M. & Adjouadi, M. Early diagnosis of mild cognitive impairment using random forest feature selection. In 2018 IEEE Biomedical Circuits and Systems Conference (BioCAS), 1–4, DOI: 10.1109/BIOCAS.2018.8584773 (2018). 51. Forouzannezhad, P., Abbaspour, A., Li, C., Cabrerizo, M. & Adjouadi, M. A deep neural network approach for early diagnosis of mild cognitive impairment using multiple features. In 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), 1341–1346, DOI: 10.1109/ICMLA.2018.00218 (2018). 52. Bai, Y., Chen, X., Dong, C., Liu, Y. & 0001, Z. Z. A comparison of multimodal biomarkers for chronic hepatitis b assessment using recursive feature elimination. In 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2016, Orlando, FL, USA, August 16-20, 2016, 2448–2451, DOI: 10.1109/EMBC.2016.7591225 (IEEE, 2016). 53. Xu, T., Zhang, H., Huang, X., Zhang, S. & Metaxas, D. N. Multimodal deep learning for cervical dysplasia diagnosis. In Ourselin, S., Joskowicz, L., Sabuncu, M. R., Unal, G. & Wells, W. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016, 115–123 (Springer International Publishing, Cham, 2016). 54. Syeda-Mahmood, T. et al. Identifying patients at risk for aortic stenosis through learning from multimodal data. In Ourselin, S., Joskowicz, L., Sabuncu, M. R., Unal, G. & Wells, W. (eds.) Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016, 238–245 (Springer International Publishing, Cham, 2016). 55. Grant, D., Papie˙z, B. W., Parsons, G., Tarassenko, L. & Mahdi, A. Deep learning classiﬁcation of cardiomegaly using combined imaging and non-imaging icu data. In Papie˙z, B. W., Yaqub, M., Jiao, J., Namburete, A. I. L. & Noble, J. A. (eds.) Medical Image Understanding and Analysis, 547–558 (Springer International Publishing, Cham, 2021). 56. Sharma, R., Eick, C. F. & Tsekos, N. V. Sm2n2: A stacked architecture for multimodal data and its application to myocardial infarction detection. In Puyol Anton, E. et al. (eds.) Statistical Atlases and Computational Models of the Heart. M&Ms and EMIDEC Challenges, 342–350 (Springer International Publishing, Cham, 2021). 57. Huang, S.-C. et al. Penet—a scalable deep-learning model for automated diagnosis of pulmonary embolism using volumetric ct imaging. NPJ digital medicine 3, 1–9 (2020). 58. Mueller, S. et al. The alzheimer’s disease neuroimaging initiative. Neuroimaging Clin. North Am. 15, 869–877, DOI: 10.1016/j.nic.2005.09.008 (2005). 59. Beekly, D. et al. The national alzheimer’s coordinating center (nacc) database: an alzheimer disease database. Alzheimer disease associated disorders 18, 270–277 (2004). 60. Alistair, J. et al. Mimic-iv (version 0.4). PhysioNet DOI: https://doi.org/10.13026/a3wn-hq05 (2020). 61. Marinescu, R. V. et al. Tadpole challenge: prediction of longitudinal evolution in alzheimer’s disease. arXiv preprint arXiv:1805.03909 (2018). 62. Fransen, P. S. et al. Mr clean, a multicenter randomized clinical trial of endovascular treatment for acute ischemic stroke in the netherlands: study protocol for a randomized controlled trial. Trials 15, 1–11 (2014). 63. Johnson, A. E. et al. Mimic-cxr, a de-identiﬁed publicly available database of chest radiographs with free-text reports. Sci. data 6, 1–8 (2019). 64. Goodfellow, I., Bengio, Y. & Courville, A. Deep learning (MIT press, 2016). 65. Reda, I. et al. Deep learning role in early diagnosis of prostate cancer. Technol. cancer research & treatment 17, 1533034618775530 (2018). 66. Hecker, S., Dai, D. & Van Gool, L. End-to-end learning of driving models with surround-view cameras and route planners. In Proceedings of the european conference on computer vision (eccv), 435–453 (2018). 19/20 67. Li, T., Sahu, A. K., Talwalkar, A. & Smith, V. Federated learning: Challenges, methods, and future directions. IEEE Signal Process. Mag. 37, 50–60 (2020). 68. Al, H., Alam, T., Househ, M. & Shah, Z. Federated learning and internet of medical things – opportunities and challenges. In 20th International Conference on Informatics, Management, and Technology in Healthcare (ICIMTH) (2022). Author contributions statement F.M., H.A., Z.S. contributed to conceptualization. F.M. and H.A. administered the project. F.M. curated the data, performed data synthesis, and contributed to writing—original draft. H.A and N.E performed writing—review and editing. Z.S. and H.A. supervised the study. All authors read and approved the ﬁnal manuscript. Additional information Supplementary Methods Competing interests The authors declare that they have no competing interests. 20/20","['c', 'artiﬁcial', 'intelligencebase', 'method', 'fusion', 'electronic', 'health', 'record', 'image', 'datum', 'farida', 'mohsen1', 'ali1', 'zubair', 'shah1', 'science', 'qatar', 'health', 'life', 'qatar', 'foundation', 'qatar', 'correspondence', 'zubair', 'shah', 'abstract', 'healthcare', 'datum', 'inherently', 'multimodal', 'include', 'electronic', 'health', 'record', 'ehr', 'medical', 'image', 'multiomics', 'datum', 'combine', 'multimodal', 'datum', 'source', 'contribute', 'well', 'understanding', 'human', 'health', 'provide', 'optimal', 'personalize', 'healthcare', 'important', 'question', 'use', 'multimodal', 'datum', 'fuse', 'ﬁeld', 'grow', 'interest', 'researcher', 'advance', 'artiﬁcial', 'intelligence', 'ai', 'technology', 'particularly', 'machine', 'learning', 'ml', 'enable', 'fusion', 'different', 'datum', 'modality', 'provide', 'multimodal', 'insight', 'end', 'scoping', 'review', 'focus', 'synthesize', 'analyze', 'literature', 'use', 'ai', 'technique', 'fuse', 'multimodal', 'medical', 'datum', 'different', 'clinical', 'application', 'speciﬁcally', 'focus', 'study', 'fuse', 'ehr', 'medical', 'imaging', 'datum', 'develop', 'various', 'ai', 'method', 'clinical', 'application', 'present', 'comprehensive', 'analysis', 'various', 'fusion', 'strategy', 'disease', 'clinical', 'outcome', 'multimodal', 'fusion', 'use', 'algorithm', 'use', 'perform', 'multimodal', 'fusion', 'clinical', 'application', 'available', 'multimodal', 'medical', 'dataset', 'follow', 'prismascr', 'prefer', 'reporting', 'item', 'systematic', 'review', 'metaanalyse', 'extension', 'scope', 'review', 'guideline', 'search', 'embase', 'pubme', 'scholar', 'retrieve', 'relevant', 'study', 'preprocesse', 'screening', 'extract', 'datum', 'study', 'fulﬁlle', 'inclusion', 'criterion', 'find', 'study', 'fuse', 'imaging', 'datum', 'ehr', 'increase', 'double', 'analysis', 'typical', 'workﬂow', 'observe', 'feed', 'raw', 'datum', 'fuse', 'different', 'datum', 'modality', 'apply', 'conventional', 'machine', 'learn', 'ml', 'deep', 'learn', 'dl', 'algorithm', 'ﬁnally', 'evaluate', 'multimodal', 'fusion', 'clinical', 'outcome', 'prediction', 'speciﬁcally', 'early', 'fusion', 'use', 'technique', 'application', 'multimodal', 'learning', 'study', 'find', 'multimodality', 'fusion', 'model', 'outperform', 'traditional', 'singlemodality', 'model', 'task', 'disease', 'diagnosis', 'prediction', 'common', 'clinical', 'outcome', 'report', 'study', 'respectively', 'clinical', 'outcome', 'perspective', 'neurological', 'disorder', 'dominant', 'category', 'study', 'ai', 'perspective', 'conventional', 'ml', 'model', 'use', 'study', 'follow', 'model', 'study', 'multimodal', 'datum', 'use', 'include', 'study', 'mostly', 'private', 'repository', 'study', 'scoping', 'review', 'offer', 'new', 'insight', 'researcher', 'interested', 'know', 'current', 'state', 'knowledge', 'research', 'ﬁeld', 'introduction', 'past', 'decade', 'digitization', 'health', 'datum', 'grow', 'tremendously', 'increase', 'data', 'repository', 'span', 'sectors1', 'datum', 'inherently', 'multimodal', 'include', 'electronic', 'health', 'record', 'ehr', 'medical', 'imaging', 'multiomic', 'environmental', 'datum', 'many', 'application', 'medicine', 'integration', 'fusion', 'different', 'datum', 'source', 'become', 'necessary', 'effective', 'prediction', 'diagnosis', 'treatment', 'planning', 'decision', 'combine', 'complementary', 'power', 'different', 'modality', 'thereby', 'bring', 'close', 'goal', 'precision', 'medicine2', 'datum', 'fusion', 'process', 'combine', 'several', 'datum', 'modality', 'provide', 'different', 'viewpoint', 'common', 'phenomenon', 'solve', 'inference', 'problem', 'purpose', 'fusion', 'technique', 'effectively', 'take', 'advantage', 'cooperative', 'complementary', 'feature', 'different', 'modalities4', 'example', 'interpret', 'medical', 'image', 'clinical', 'datum', 'often', 'necessary', 'make', 'effective', 'diagnostic', 'decision', 'many', 'study', 'find', 'miss', 'pertinent', 'clinical', 'laboratory', 'datum', 'image', 'interpretation', 'decrease', 'radiologist', 'ability', 'accurately', 'make', 'diagnostic', 'decisions6', 'signiﬁcance', 'clinical', 'datum', 'support', 'accurate', 'interpretation', 'imaging', 'datum', 'well', 'establish', 'radiology', 'well', 'wide', 'variety', 'imagingbase', 'medical', 'specialty', 'dermatology', 'ophthalmology', 'pathology', 'depend', 'clinical', 'context', 'interpret', 'imaging', 'datum', 'thank', 'advance', 'model', 'achieve', 'useful', 'fusion', 'multimodal', 'datum', 'highdimensionality10', 'various', 'statistical', 'property', 'different', 'missing', 'value', 'patterns11', 'multimodal', 'domain', 'integrate', 'different', 'preprint', 'paper', 'accept', 'publication', 'scientiﬁc', 'report', 'cite', 'ﬁnal', 'version', 'nature', 'scientiﬁc', 'report', 'email', 'datum', 'modality', 'recent', 'year', 'multimodal', 'datum', 'fusion', 'gain', 'much', 'attention', 'automate', 'clinical', 'outcome', 'prediction', 'diagnosis', 'see', 'disease', 'diagnosis', 'prediction12–15', 'imaging', 'datum', 'combine', 'speciﬁc', 'lab', 'test', 'result', 'demographic', 'datum', 'input', 'ml', 'model', 'well', 'performance', 'achieve', 'singlesource', 'model', 'similarly', 'fuse', 'pathological', 'image', 'patient', 'demographic', 'datum', 'observe', 'increase', 'performance', 'comparison', 'single', 'modality', 'model', 'breast', 'cancer', 'diagnosis16', 'several', 'study', 'find', 'similar', 'advantage', 'various', 'medical', 'imaging', 'application', 'include', 'diabetic', 'retinopathy', 'prediction', 'covid19', 'detection', 'glaucoma', 'diagnosis17–19', 'scoping', 'review', 'focus', 'study', 'use', 'ai', 'model', 'fuse', 'medical', 'image', 'ehr', 'datum', 'different', 'clinical', 'application', 'modality', 'fusion', 'strategy', 'play', 'signiﬁcant', 'role', 'study', 'literature', 'review', 'publish', 'use', 'ai', 'multimodal', 'medical', 'datum', 'fusion20–26', 'however', 'differ', 'review', 'term', 'scope', 'coverage', 'previous', 'study', 'focus', 'fusion', 'different', 'medical', 'imaging', 'modalities20', 'consider', 'ehr', 'conjunction', 'imaging', 'modality', 'review', 'focus', 'fusion', 'omic', 'datum', 'data', 'modality', 'use', 'models22', 'study24', 'focus', 'fusion', 'various', 'internet', 'medical', 'thing', 'iomts', 'datum', 'smart', 'healthcare', 'application', 'focus', 'exclusively', 'integrate', 'multimodal', 'ehr', 'datum', 'multimodality', 'refer', 'structured', 'datum', 'unstructured', 'free', 'text', 'ehr', 'use', 'conventional', 'ml', 'technique', 'discuss', 'fusion', 'strategy', 'structured', 'ehr', 'datum', 'medical', 'imaging', 'use', 'dl', 'model', 'emphasize', 'fusion', 'technique', 'feature', 'extraction', 'method', 'furthermore', 'review', 'cover', 'research', 'retrieve', 'study', 'contrast', 'review', 'focus', 'study', 'use', 'conventional', 'ml', 'dl', 'technique', 'ehr', 'medical', 'imaging', 'datum', 'cover', 'recent', 'study', 'table', 'provide', 'detailed', 'comparison', 'review', 'exist', 'review', 'primary', 'purpose', 'scoping', 'review', 'explore', 'analyze', 'publish', 'scientiﬁc', 'literature', 'fuse', 'ehr', 'medical', 'imaging', 'use', 'model', 'therefore', 'study', 'aim', 'answer', 'follow', 'question', 'fusion', 'strategy', 'fusion', 'strategy', 'use', 'researcher', 'combine', 'medical', 'imaging', 'datum', 'ehr', 'used', 'method', 'disease', 'type', 'disease', 'fusion', 'method', 'implement', 'clinical', 'outcome', 'ml', 'method', 'type', 'clinical', 'outcome', 'address', 'use', 'different', 'fusion', 'strategy', 'kind', 'ml', 'algorithm', 'use', 'clinical', 'outcome', 'resource', 'publicly', 'accessible', 'medical', 'multimodal', 'dataset', 'believe', 'review', 'provide', 'comprehensive', 'overview', 'reader', 'advancement', 'make', 'multimodal', 'ml', 'ehrs', 'medical', 'imaging', 'datum', 'furthermore', 'reader', 'develop', 'understanding', 'ml', 'model', 'design', 'align', 'datum', 'different', 'modality', 'various', 'clinical', 'task', 'believe', 'review', 'help', 'identify', 'lack', 'multimodal', 'datum', 'resource', 'medical', 'imaging', 'ehr', 'thus', 'motivate', 'research', 'community', 'develop', 'multimodal', 'medical', 'datum', 'preliminary', 'ﬁrst', 'identify', 'ehr', 'medical', 'imaging', 'modality', 'focus', 'review', 'present', 'data', 'fusion', 'strategy', 'use', 'investigate', 'study', 'perspective', 'multimodal', 'fusion', 'datum', 'modality', 'review', 'focus', 'study', 'use', 'primary', 'data', 'modality', 'medical', 'imaging', 'modality', 'include', 'ndimensional', 'imaging', 'information', 'acquire', 'clinical', 'practice', 'magnetic', 'imaging', 'functional', 'mri', 'fmri', 'structural', 'mri', 'smri', 'emission', 'tomography', 'pet', 'compute', 'tomography', 'ct', 'ultrasound', 'ehr', 'datum', 'include', 'structure', 'unstructured', 'freetext', 'datum', 'structure', 'datum', 'include', 'code', 'datum', 'diagnosis', 'code', 'procedure', 'code', 'numerical', 'datum', 'laboratory', 'test', 'result', 'categorical', 'datum', 'demographic', 'information', 'family', 'history', 'vital', 'sign', 'medication', 'unstructured', 'datum', 'include', 'medical', 'report', 'clinical', 'note', 'review', 'consider', 'study', 'combine', 'modality', 'ehr', 'imaging', 'however', 'exist', 'case', 'datum', 'contain', 'multiple', 'ehr', 'modality', 'structure', 'unstructured', 'multiple', 'imaging', 'modality', 'mri', 'consider', 'datum', 'single', 'modality', 'ehr', 'modality', 'imaging', 'modality', 'previous', 'review', 'review', 'multimodal', 'fusion', 'medical', 'image', 'compendious', 'analysis', 'medical', 'modality', 'database', 'multimodal', 'fusion', 'quality', 'metrics20', 'advance', 'multimodality', 'datum', 'fusion', 'neuroimag', 'technique', 'year', 'scope', 'coverage', 'review', 'focus', 'fusion', 'differ', 'ent', 'medical', 'imaging', 'modality', 'review', 'focus', 'fusion', 'dif', 'ferent', 'imaging', 'modality', 'consider', 'neu', 'roimaging', 'application', 'brain', 'disease', 'neurological', 'disorder', 'overview', 'deep', 'learn', 'e', 'method', 'multimodal', 'medical', 'datum', 'mining22', 'review', 'focus', 'fusion', 'dif', 'ferent', 'type', 'multiomic', 'datum', 'ehr', 'different', 'imaging', 'modality', 'sidere', 'dl', 'model', 'speciﬁc', 'disease', 'covid19', 'cancer', 'review', 'focus', 'fusion', 'dif', 'ferent', 'type', 'multiomic', 'datum', 'ehr', 'imaging', 'modality', 'consider', 'dl', 'model', 'moreover', 'provide', 'summary', 'freely', 'accessible', 'multi', 'modal', 'dataset', 'summary', 'evaluation', 'measure', 'use', 'evaluate', 'multimodal', 'model', 'survey', 'focus', 'fuse', 'med', 'ical', 'imaging', 'ehr', 'rather', 'cover', 'fusion', 'iomts', 'datum', 'smart', 'health', 'care', 'application', 'cover', 'study', 'pub', 'lishe', 'untill', 'moreover', 'review', 'multimodality', 'refer', 'fuse', 'dif', 'ferent', 'medical', 'signal', 'biosignal', 'different', 'medical', 'imaging', 'modality', 'medical', 'signal', 'image', 'review', 'focus', 'fusion', 'struc', 'ture', 'unstructured', 'ehr', 'datum', 'consider', 'medical', 'imaging', 'modality', 'provide', 'summary', 'freely', 'accessible', 'multimodal', 'dataset', 'summary', 'evaluation', 'measure', 'use', 'evaluate', 'multimodal', 'model', 'multimodal', 'deep', 'learning', 'biomedical', 'datum', 'fusion', 'review23', 'comprehensive', 'survey', 'multimodal', 'medical', 'signal', 'fusion', 'smart', 'healthcare', 'systems24', 'learn', 'machine', 'multimodal', 'electronic', 'health', 'recordsbase', 'search', 'challenge', 'perspectives27', 'fusion', 'medical', 'imag', 'ing', 'electronic', 'health', 'record', 'use', 'deep', 'learn', 'systematic', 'review', 'plementation', 'guidelines26', 'comparative', 'contribution', 'review', 'review', 'focus', 'fusion', 'medi', 'cal', 'image', 'multimodal', 'ehr', 'datum', 'consider', 'different', 'imaging', 'modality', 'single', 'modality', 'review', 'share', 'common', 'study', 'review', 'focus', 'fusion', 'medical', 'imaging', 'ehr', 'datum', 'consider', 'ous', 'disease', 'neurological', 'disorder', 'cancer', 'cardiovascular', 'disease', 'psychiatric', 'disorder', 'eye', 'disease', 'covid19', 'review', 'share', 'common', 'stud', 'ie', 'review', 'focus', 'fusion', 'medi', 'cal', 'image', 'ehr', 'datum', 'consider', 'ai', 'model', 'various', 'disease', 'neu', 'rological', 'disorder', 'cancer', 'cardiovascular', 'disease', 'psychiatric', 'disorder', 'eye', 'disease', 'covid19', 'review', 'share', 'common', 'study', 'review', 'focus', 'fusion', 'med', 'ical', 'imaging', 'ehr', 'datum', 'consider', 'ai', 'model', 'moreover', 'study', 'vide', 'summary', 'accessible', 'multi', 'modal', 'dataset', 'summary', 'evaluation', 'measure', 'use', 'evaluate', 'multimodal', 'model', 'review', 'share', 'common', 'study', 'review', 'focus', 'fusion', 'medical', 'imaging', 'ehr', 'structure', 'unstruc', 'ture', 'different', 'clinical', 'application', 'include', 'study', 'publish', 'study', 'common', 'review', 'review', 'focus', 'fusion', 'medi', 'cal', 'image', 'ehr', 'consider', 'ture', 'unstructured', 'datum', 'ehr', 'sin', 'gle', 'modality', 'review', 'share', 'common', 'study', 'review', 'focus', 'fusion', 'ture', 'ehr', 'datum', 'medical', 'imaging', 'con', 'sidere', 'dl', 'model', 'include', 'study', 'publish', 'review', 'focus', 'fusion', 'medi', 'cal', 'image', 'ehr', 'datum', 'consider', 'ai', 'model', 'include', 'study', 'almost', 'half', 'publish', 'table', 'comparison', 'previous', 'review', 'fusion', 'strategy', 'outline', 'fusion', 'approach', 'categorize', 'early', 'late', 'joint', 'fusion', 'strategy', 'classiﬁe', 'depend', 'stage', 'feature', 'fuse', 'model', 'scoping', 'review', 'follow', 'deﬁnition', 'attempt', 'match', 'study', 'taxonomy', 'section', 'brieﬂy', 'describe', 'fusion', 'strategy', 'early', 'fusion', 'join', 'feature', 'multiple', 'input', 'modality', 'input', 'level', 'feed', 'single', 'ml', 'training26', 'modality', 'feature', 'extract', 'either', 'manually', 'use', 'different', 'method', 'neural', 'network', 'software', 'statistical', 'method', 'word', 'embed', 'model', 'use', 'extract', 'feature', 'early', 'fusion', 'require', 'train', 'multiple', 'model', 'feature', 'extraction', 'model', 'single', 'fusion', 'model', 'type', 'joint', 'fusion', 'type', 'type', 'fuse', 'original', 'feature', 'extract', 'feature', 'type', 'fuse', 'extract', 'feature', 'modality', 'late', 'fusion', 'train', 'separate', 'ml', 'model', 'datum', 'modality', 'ﬁnal', 'decision', 'leverage', 'prediction', 'model26', 'aggregation', 'method', 'weight', 'average', 'voting', 'majority', 'voting', 'metaclassiﬁer', 'use', 'make', 'ﬁnal', 'decision', 'type', 'fusion', 'often', 'know', 'decisionlevel', 'fusion', 'joint', 'fusion', 'combine', 'learn', 'feature', 'intermediate', 'layer', 'feature', 'modality', 'input', 'ﬁnal', 'model', 'training26', 'contrast', 'early', 'fusion', 'loss', 'ﬁnal', 'model', 'propagate', 'back', 'feature', 'extraction', 'model', 'training', 'learned', 'feature', 'representation', 'improve', 'iterative', 'updating', 'feature', 'weight', 'nn', 'use', 'joint', 'fusion', 'propagate', 'loss', 'ﬁnal', 'model', 'feature', 'extractor', 'type', 'joint', 'fusion', 'type', 'former', 'nn', 'use', 'extract', 'feature', 'modality', 'latter', 'input', 'modality', 'feature', 'extract', 'use', 'method', 'scoping', 'review', 'follow', 'guideline', 'recommend', 'prismascr28', 'search', 'strategy', 'structure', 'search', 'search', 'database', 'include', 'scopus', 'pubme', 'embase', 'scholar', 'retrieve', 'relevant', 'study', 'note', 'medline', 'cover', 'pubmed', 'scholar', 'search', 'result', 'select', 'ﬁrst', 'relevant', 'study', 'entry', 'search', 'result', 'rapidly', 'lose', 'relevancy', 'unmatched', 'review', '’s', 'topic', 'furthermore', 'limit', 'search', 'englishlanguage', 'article', 'publish', 'last', 'year', 'search', 'base', 'abstract', 'title', 'conduct', 'scoping', 'review', 'focus', 'apply', 'ai', 'model', 'multimodal', 'medical', 'database', 'application', 'term', 'multimodal', 'refer', 'combine', 'medical', 'imaging', 'ehr', 'describe', 'preliminary', 'section', 'therefore', 'search', 'string', 'incorporate', 'major', 'term', 'connect', 'artiﬁcial', 'intelligence', 'machine', 'learning', 'deep', 'learning', 'multimodality', 'fusion', 'medical', 'imaging', 'electronic', 'health', 'record', 'use', 'different', 'form', 'term', 'provide', 'complete', 'search', 'string', 'database', 'supplementary', 'material', 'inclusion', 'exclusion', 'criterion', 'include', 'study', 'fuse', 'ehr', 'medical', 'imaging', 'modality', 'use', 'ai', 'model', 'clinical', 'application', 'model', 'consider', 'classical', 'ml', 'model', 'model', 'transfer', 'learn', 'ensemble', 'learning', 'mention', 'search', 'term', 'supplementary', 'material', 'consider', 'study', 'use', 'classical', 'statistical', 'model', 'regression', 'review', 'deﬁnition', 'imaging', 'modality', 'type', 'medical', 'imaging', 'use', 'clinical', 'practice', 'pet', 'ct', 'scan', 'ultrasound', 'consider', 'structured', 'unstructured', 'freetext', 'patient', 'datum', 'ehr', 'modality', 'describe', 'preliminary', 'section', 'peerreviewe', 'study', 'conference', 'proceeding', 'include', 'moreover', 'include', 'study', 'limit', 'english', 'language', 'enforce', 'restriction', 'type', 'disorder', 'disease', 'clinical', 'task', 'exclude', 'study', 'use', 'single', 'data', 'modality', 'also', 'exclude', 'study', 'use', 'different', 'type', 'datum', 'modality', 'study', 'combine', 'imaging', 'type', 'eg', 'pet', 'mri', 'consider', 'single', 'modality', 'moreover', 'study', 'integrate', 'original', 'imaging', 'modality', 'extracted', 'imaging', 'feature', 'exclude', 'still', 'consider', 'single', 'modality', 'also', 'study', 'combined', 'multiomic', 'datum', 'modality', 'exclude', 'addition', 'study', 'unrelated', 'medical', 'ﬁeld', 'use', 'aibased', 'model', 'exclude', 'exclude', 'review', 'conference', 'abstract', 'proposal', 'editorial', 'commentarie', 'letter', 'editor', 'preprint', 'short', 'letter', 'article', 'nonenglish', 'publication', 'also', 'exclude', 'study', 'selection', 'use', 'rayyan', 'webbase', 'review', 'management', 'tool29', 'ﬁrst', 'screening', 'study', 'selection', 'remove', 'duplicate', 'screen', 'study', 'base', 'title', 'abstract', 'subsequently', 'fulltext', 'select', 'study', 'title', 'abstract', 'screening', 'assess', 'eligibility', 'use', 'inclusion', 'exclusion', 'criterion', 'author', 'conduct', 'study', 'selection', 'resolve', 'conﬂict', 'discussion', 'third', 'author', 'consult', 'agreement', 'reach', 'data', 'extraction', 'ﬁnal', 'include', 'study', 'datum', 'extraction', 'form', 'design', 'pilot', 'study', 'develop', 'systematic', 'accurate', 'datum', 'extraction', 'process', 'extract', 'datum', 'study', 'name', 'year', 'country', 'author', 'name', 'clinical', 'outcome', 'imaging', 'modality', 'ehr', 'modality', 'fusion', 'strategy', 'feature', 'extraction', 'method', 'data', 'source', 'ai', 'model', 'evaluation', 'metric', 'comparison', 'single', 'modality', 'supplementary', 'material', 'provide', 'extract', 'information', 'description', 'detail', 'author', 'perform', 'datum', 'extraction', 'author', 'zs', 'review', 'veriﬁe', 'extract', 'datum', 'disagreement', 'resolve', 'discussion', 'consensus', 'author', 'datum', 'synthesis', 'follow', 'data', 'extraction', 'use', 'narrative', 'approach', 'synthesize', 'datum', 'analyze', 'study', 'ﬁve', 'perspective', 'fusion', 'strategy', 'disease', 'clinical', 'outcome', 'datum', 'sourcestype', 'evaluation', 'mechanism', 'fusion', 'strategy', 'focus', 'multimodal', 'datum', 'fuse', 'addition', 'record', 'implementation', 'detail', 'model', 'feature', 'extraction', 'single', 'modality', 'evaluation', 'also', 'extract', 'information', 'disease', 'fusion', 'method', 'implement', 'furthermore', 'analyze', 'data', 'fusion', 'model', 'apply', 'clinical', 'outcome', 'ml', 'model', 'use', 'task', 'moreover', 'focus', 'type', 'imaging', 'ehr', 'datum', 'use', 'study', 'source', 'datum', 'availability', 'finally', 'evaluation', 'focus', 'evaluation', 'metric', 'use', 'study', 'study', 'quality', 'assessment', 'accordance', 'guideline', 'scope', 'reviews30', 'perform', 'quality', 'assessment', 'include', 'study', 'result', 'search', 'result', 'total', 'study', 'retrieve', 'initial', 'search', 'duplicate', 'elimination', 'study', 'retain', 'base', 'study', 'selection', 'criterion', 'see', 'method', 'study', 'remain', 'fulltext', 'review', 'exclude', 'article', 'base', 'abstract', 'title', 'moreover', 'study', 'remove', 'fulltext', 'screen', 'finally', 'study', 'meet', 'inclusion', 'criterion', 'select', 'data', 'extraction', 'synthesis', 'figure', 'show', 'ﬂowchart', 'study', 'screening', 'selection', 'process', 'demographic', 'study', 'present', 'table', 'approximately', 'twothird', 'study', 'journal', 'article', '∼', '6812–15', 'study', 'conference', 'proceeding', '∼', 'study', 'publish', '∼', 'figure', 'show', 'visualization', 'publication', 'typewise', 'yearwise', 'distribution', 'study', 'include', 'study', 'publish', 'country', 'study', '∼', '∼', 'datum', 'fusion', 'strategy', 'map', 'include', 'study', 'taxonomy', 'fusion', 'strategy', 'outline', 'preliminary', 'section', 'primary', 'interest', 'review', 'identify', 'fusion', 'strategy', 'include', 'study', 'use', 'improve', 'performance', 'model', 'different', 'clinical', 'outcome', 'early', 'fusion', 'majority', 'include', 'study', '∼', 'use', 'early', 'fusion', 'combine', 'medical', 'imaging', 'nonimage', 'datum', 'input', 'modality', 'different', 'dimension', 'combine', 'onedimensional', 'ehr', 'datum', 'imaging', 'datum', 'essential', 'extract', 'highlevel', 'imaging', 'feature', 'fuse', 'ehr', 'datum', 'accomplish', 'various', 'method', 'use', 'study', 'include', 'neural', 'networkbase', 'feature', 'extraction', 'datum', 'generation', 'software', 'manual', 'extraction', 'feature', 'early', 'fusion', 'study', 'studies12', 'use', 'manual', 'softwarebase', 'imaging', 'feature', 'study', 'use', 'neural', 'networkbase', 'architecture', 'extract', 'image', 'feature', 'combine', 'clinical', 'datum', 'modality16', 'study', 'use', 'manual', 'softwarebase', 'feature', 'figure', 'prisma', 'ﬂow', 'chart', 'study', 'identiﬁcation', 'screening', 'selection', 'characteristic', 'year', 'number', 'study', 'country', 'type', 'table', 'demographic', 'include', 'study', 'reduce', 'feature', 'dimension', 'concatenate', 'modality', 'feature', 'use', 'different', 'methods25', 'method', 'include', 'recursive', 'feature', 'elimination52', 'ﬁlterbased', 'method', 'use', 'pearson', 'correlation', 'random', 'forest', 'feature', 'selection', 'base', 'gini', 'importance50', 'reliefbase', 'feature', 'selection', 'method25', 'wrapperbased', 'method', 'use', 'backward', 'feature', 'elimination36', 'rankbase', 'method', 'use', 'gini', 'coefﬁcients45', 'moreover', 'studies13', 'utilize', 'principal', 'component', 'analysis', 'pca', 'dimensionality', 'reduction', 'technique', 'reduce', 'feature', 'dimension', 'study', 'use', 'neural', 'networkbase', 'architecture', 'extract', 'image', 'architecture', 'use', 'studies16', 'study', 'concatenate', 'multimodal', 'feature', 'cnnextracte', 'ehr', 'feature', 'fusion', 'strategy', 'early', 'fusion', 'study', 'evaluate', 'fusion', 'model', 'performance', 'single', 'modality', 'mod', 'result', 'study', 'exhibit', 'well', 'performance', 'fusion', 'compare', 'imagingonly', 'clinicalonly', 'joint', 'fusion', 'joint', 'fusion', 'second', 'common', 'fusion', 'strategy', 'use', 'study', 'study', 'different', 'neural', 'networkbase', 'method', 'use', 'process', 'imaging', 'ehr', 'data', 'modality', 'use', 'visual', 'geometry', 'group', 'architecture', 'extract', 'feature', 'mri', 'image', 'use', 'bidirectional', 'longshort', 'term', 'memory', 'lstm', 'network', 'attention', 'layer', 'learn', 'feature', 'representation', 'report', 'concatenate', 'learn', 'feature', 'modality', 'feed', 'stack', 'knearest', 'neighbor', 'knn', 'attention', 'pool', 'layer', 'grant', 'use', 'residual', 'network', 'resnet50', 'architecture', 'extract', 'relevant', 'feature', 'imaging', 'modality', 'fully', 'connect', 'process', 'nonimage', 'datum', 'directly', 'concatenate', 'learn', 'feature', 'representation', 'imaging', 'nonimage', 'datum', 'feed', 'fully', 'connect', 'network', 'yidong', 'use', 'bayesian', 'encoderdecoder', 'extract', 'image', 'feature', 'bayesian', 'multilayer', 'perception', 'encoderdecoder', 'process', 'medical', 'indicator', 'data', 'study', 'directly', 'concatenate', 'feature', 'vector', 'feed', 'result', 'vector', 'bayesian', 'utilize', 'selfattention', 'mechanism', 'extract', 'imaging', 'feature', 'fully', 'connect', 'nn', 'process', 'metadata', 'information', 'lili', 'use', 'architecture', 'extract', 'multimodal', 'mri', 'feature', 'fully', 'connect', 'network', 'clinical', 'datum', 'study', 'concatenate', 'feature', 'vector', 'feed', 'fully', 'connect', 'study46', 'apply', 'figure', 'distribution', 'study', 'type', 'publication', 'year', 'layer', 'image', 'feature', 'extraction', 'word', 'embedding', 'selfattention', 'textual', 'medical', 'datum', 'research38', 'apply', 'resnet', 'architecture', 'imaging', 'clinical', 'datum', 'feature', 'extraction', 'author', 'fuse', 'feature', 'vector', 'concatenation', 'feed', 'lstm', 'network', 'follow', 'fully', 'connect', 'network', 'concatenate', 'imaging', 'feature', 'extract', 'use', 'inceptionv3', 'model', 'clinical', 'datum', 'feature', 'feed', 'fully', 'connect', 'use', 'extract', 'image', 'feature', 'concatenate', 'directly', 'clinical', 'datum', 'feed', 'softmax', 'classiﬁer', 'use', 'alexnet', 'architecture', 'convert', 'imaging', 'datum', 'feature', 'vector', 'fusible', 'nonimage', 'modality', 'jointly', 'learn', 'nonlinear', 'correlation', 'modality', 'use', 'fully', 'connected', 'joint', 'fusion', 'study', 'study', 'evaluate', 'fusion', 'model', 'performance', 'single', 'modality', 'report', 'performance', 'improvement', 'fusion', 'used17', 'late', 'fusion', 'late', 'fusion', 'least', 'common', 'fusion', 'approach', 'use', 'include', 'study', 'study', 'use', 'train', 'independent', 'imaging', 'model', 'take', 'single', 'mri', 'slice', 'input', 'aggregate', 'prediction', 'model', 'use', 'maximum', 'mean', 'majority', 'voting', 'combine', 'result', 'aggregation', 'majority', 'vote', 'study', 'perform', 'late', 'fusion', 'clinical', 'datum', 'model', 'study40', 'train', 'different', 'late', 'fusion', 'model', 'model', 'take', 'average', 'predict', 'probability', 'imaging', 'ehr', 'modality', 'model', 'ﬁnal', 'prediction', 'fourth', 'model', 'use', 'nn', 'classiﬁer', 'aggregator', 'take', 'input', 'single', 'modality', 'model', 'prediction', 'study', 'also', 'create', 'early', 'joint', 'fusion', 'model', 'single', 'modality', 'model', 'compare', 'late', 'fusion', 'performance', 'result', 'late', 'fusion', 'outperform', 'early', 'joint', 'fusion', 'model', 'single', 'modality', 'model', 'disease', 'categorize', 'disease', 'disorder', 'include', 'study', 'type', 'neurological', 'disorder', 'cancer', 'cardiovascular', 'disease', 'covid19', 'psychiatric', 'disorder', 'eye', 'disease', 'disease', 'majority', 'include', 'study', 'focus', 'neurological', 'disorder', 'table', 'show', 'distribution', 'include', 'study', 'term', 'disease', 'disorder', 'cover', 'clinical', 'outcome', 'machine', 'learning', 'model', 'multimodal', 'ml', 'enable', 'wide', 'range', 'clinical', 'application', 'diagnosis', 'early', 'prediction', 'patient', 'stratiﬁcation', 'phenotype', 'biomarker', 'identiﬁcation', 'review', 'label', 'study', 'accord', 'clinical', 'outcome', 'disease', 'category', 'neurological', 'disorder', 'mild', 'cognitive', 'impairment', 'ischemic', 'stroke', 'demyelinate', 'disease', 'neurodevelopmental', 'deﬁcit', 'epilepsy', 'cancer', 'breast', 'cancer', 'cancer', 'upper', 'gastrointestinal', 'cancer', 'cardiovascular', 'disease', 'myocardial', 'infarction', 'psychiatric', 'disorder', 'bipolar', 'disorder', 'schizophrenia', 'eye', 'disease', 'diabetic', 'disease', 'cervical', 'dysplasia', 'pulmonary', 'embolism', 'hepatitis', 'b', 'number', 'study', 'study', 'reference', 'table', 'disease', 'distribution', 'cover', 'study', 'categorize', 'retrieve', 'clinical', 'task', 'main', 'category', 'diagnosis', 'prediction', 'study', 'mention', 'detection', 'classiﬁcation', 'diagnosis', 'prediction', 'categorize', 'diagnosis', 'category', 'early', 'prediction', 'group', 'consider', 'study', 'predict', 'disease', 'onset', 'identify', 'signiﬁcant', 'risk', 'factor', 'predict', 'mortality', 'overall', 'survival', 'predict', 'treatment', 'outcome', 'clinical', 'outcome', 'implement', 'use', 'multimodal', 'ml', 'model', 'section', 'summarize', 'different', 'clinical', 'task', 'retrieve', 'study', 'fusion', 'strategy', 'use', 'model', 'develop', 'task', 'figure', 'show', 'distribution', 'fusion', 'strategy', 'associate', 'different', 'disease', 'clinical', 'outcome', 'diagnosis', 'common', 'apply', 'clinical', 'outcome', 'include', 'study', 'diagnosis', 'report', '∼', 'study', 'study', 'ehr', 'combine', 'medical', 'imaging', 'diagnose', 'spectrum', 'disease', 'include', 'neurological', 'disorder', 'psychiatric', 'disorder', 'cvd', 'cancer', 'study', 'different', 'diseases18', 'speciﬁcally', 'study', 'focus', 'detect', 'neurological', 'disease', 'ad', 'early', 'fusion', 'utilize', 'technique', 'diagnosis', 'purpose', 'use', 'study', 'study', 'employ', 'different', 'ml', 'model', 'fuse', 'imaging', 'ehr', 'datum', 'diagnose', 'different', 'disease', 'study', 'diagnose', 'neurological', 'psychiatric', 'disorder', 'ad13–15', 'demyelinate', 'diseases32', 'bipolar', 'disorder33', 'schizophrenia36', 'parvathy', 'report', 'diagnose', 'ad', 'fuse', 'smri', 'pet', 'imaging', 'feature', 'minimental', 'state', 'examination', 'mmse', 'score', 'clinical', 'dementia', 'rating', 'cdr', 'age', 'subject', 'feed', 'fuse', 'feature', 'vector', 'different', 'ml', 'model', 'include', 'support', 'vector', 'machine', 'svm', 'random', 'forest', 'rf', 'gaussian', 'process', 'gp', 'classiﬁcation', 'niyas', 'et', 'classiﬁed', 'ad', 'fuse', 'pet', 'demographic', 'datum', 'lab', 'test', 'include', 'cognitive', 'test', 'cerebrospinal', 'fluid', 'csf', 'test', 'apply', 'dynamic', 'ensemble', 'classiﬁer', 'selection', 'algorithm', 'use', 'different', 'pool', 'classiﬁer', 'fuse', 'feature', 'classiﬁcation', 'hamid', 'combine', 'mri', 'pet', 'imaging', 'feature', 'personal', 'information', 'neurological', 'datum', 'mmse', 'crf', 'feature', 'ad', 'early', 'diagnosis', 'study', 'feed', 'fuse', 'feature', 'svm', 'classiﬁcation', 'diagnosis', 'matteo', 'propose', 'combine', 'mri', 'imaging', 'cognitive', 'assessment', 'diagnosis', 'concatenate', 'feature', 'modality', 'feed', 'linear', 'quadratic', 'discriminant', 'analysis', 'diagnosis', 'parisa', 'integrate', 'feature', 'extract', 'mri', 'pet', 'image', 'neuropsychological', 'test', 'demographic', 'datum', 'gender', 'age', 'education', 'diagnose', 'early', 'train', 'svm', 'deep', 'nn', 'use', 'fuse', 'feature', 'classiﬁcation', 'respectively', 'study32', 'xin', 'combine', 'mri', 'imaging', 'structure', 'datum', 'extract', 'ehr', 'diagnose', 'demyelinating', 'disease', 'use', 'svm', 'bipolar', 'disorder', 'rashmin', 'et', 'combine', 'multimodal', 'imaging', 'feature', 'neuropsychological', 'test', 'personal', 'information', 'feature', 'feed', 'svm', 'differentiate', 'bipolar', 'patient', 'healthy', 'patient', 'ebdrup', 'propose', 'integrate', 'mri', 'diffusion', 'tensor', 'imaging', 'tractography', 'image', 'neurocognitive', 'test', 'clinical', 'datum', 'schizophrenia', 'classiﬁcation', 'fuse', 'feature', 'modality', 'feed', 'different', 'type', 'ml', 'classiﬁer', 'include', 'linear', 'regression', 'lr', 'decision', 'tree', 'naïve', 'baye', 'nb', 'classiﬁcation', 'moreover', 'study', 'implement', 'multimodality', 'early', 'fusion', 'diagnose', 'different', 'cancer', 'diseases16', 'yan', 'fuse', 'pathological', 'image', 'structured', 'datum', 'extract', 'ehr', 'classify', 'malignant', 'benign', 'breast', 'cancer', 'fuse', 'feature', 'modality', 'feed', 'fully', 'connect', 'follow', 'softmax', 'layer', 'classiﬁcation', 'seung', 'combine', 'pet', 'imaging', 'clinical', 'demographic', 'datum', 'differentiate', 'lung', 'adenocarcinoma', 'adc', 'squamous', 'cell', 'carcinoma', 'feed', 'integrate', 'feature', 'different', 'algorithm', 'lr', 'nb', 'artiﬁcial', 'neural', 'network', 'ann', 'classiﬁcation', 'covid19', 'diagnosis', 'combine', 'ct', 'image', 'clinical', 'feature', 'feed', 'different', 'ml', 'model', 'include', 'svm', 'knn', 'diagnosis', 'finally', 'tanveer', 'combine', 'feature', 'echocardiogram', 'report', 'image', 'diagnosis', 'information', 'detection', 'patient', 'aortic', 'stenosis', 'study', 'feed', 'combine', 'feature', 'rf', 'learn', 'framework', 'detect', 'patient', 'likely', 'disease', 'joint', 'fusion', 'use', 'diagnostic', 'purpose', 'studies19', 'study', 'employ', 'different', 'type', 'dl', 'architecture', 'learn', 'fuse', 'imaging', 'ehr', 'datum', 'diagnosis', 'purpose', 'in19', 'propose', 'bayesian', 'deep', 'multisource', 'learn', 'model', 'integrate', 'retinal', 'image', 'medical', 'indicator', 'datum', 'diagnose', 'glaucoma', 'model', 'use', 'encoderdecoder', 'extract', 'image', 'feature', 'bayesian', 'encoderdecoder', 'process', 'medical', 'indicator', 'data', 'feature', 'vector', 'directly', 'concatenate', 'feed', 'bayesian', 'classiﬁcation', 'use', 'dl', 'multimodal', 'feature', 'extraction', 'classiﬁcation', 'detect', 'ad', 'author', 'use', 'model', 'extract', 'feature', 'mri', 'image', 'bidirectional', 'lstm', 'network', 'attention', 'layer', 'learn', 'feature', 'report', 'feed', 'fuse', 'feature', 'stacked', 'knn', 'pool', 'layer', 'classify', 'patient', 'diagnosis', 'datum', 'propose', 'endtoend', 'deep', 'multimodal', 'framework', 'learn', 'well', 'complementary', 'feature', 'image', 'nonimage', 'modality', 'cervical', 'dysplasia', 'diagnosis', 'use', 'speciﬁcally', 'alexnet', 'architecture', 'convert', 'cervigram', 'image', 'datum', 'feature', 'vector', 'fusible', 'nonimage', 'modality', 'jointly', 'learn', 'nonlinear', 'correlation', 'modality', 'use', 'fully', 'connect', 'cervical', 'dysplasia', 'classiﬁcation', 'studies55', 'also', 'employ', 'dl', 'model', 'jointly', 'learn', 'multimodal', 'feature', 'representation', 'diagnose', 'former55', 'propose', 'multimodal', 'network', 'classiﬁcation', 'simultaneously', 'integrate', 'nonimage', 'intensive', 'care', 'unit', 'icu', 'datum', 'laboratory', 'value', 'vital', 'sign', 'value', 'static', 'patient', 'metadata', 'include', 'demographic', 'imaging', 'use', 'resnet50', 'architecture', 'extract', 'feature', 'image', 'fully', 'connect', 'process', 'icu', 'datum', 'join', 'learn', 'imaging', 'nonimage', 'feature', 'concatenate', 'learn', 'feature', 'representation', 'feed', 'fully', 'connect', 'layer', 'generate', 'label', 'diagnosis', 'latter', 'study56', 'propose', 'stacked', 'multimodal', 'architecture', 'call', 'sm2n2', 'integrate', 'clinical', 'information', 'mri', 'image', 'research', 'use', 'extract', 'image', 'feature', 'concatenate', 'feature', 'clinical', 'datum', 'feed', 'softmax', 'classiﬁer', 'myocardial', 'infarction', 'detection', 'late', 'fusion', 'implement', 'studies37', 'disease', 'diagnosis', 'purpose', 'propose', 'fusion', 'mri', 'scan', 'logical', 'memory', 'test', 'mmse', 'classiﬁcation', 'study', 'utilize', 'architecture', 'mri', 'feature', 'extraction', 'develop', 'mlp', 'model', 'mmse', 'test', 'result', 'combine', 'mri', 'model', 'use', 'majority', 'voting', 'result', 'fusion', 'model', 'outperform', 'individual', 'model', 'utilize', 'nonopen', 'dataset', 'comprise', 'ct', 'scan', 'ehr', 'datum', 'train', 'unimodal', 'late', 'fusion', 'model', 'diagnosis', 'use', 'previously', 'implement', 'architecture', 'penet57', 'encode', 'ct', 'image', 'feedforward', 'network', 'encode', 'tabular', 'datum', 'late', 'fusion', 'approach', 'perform', 'well', 'fusion', 'model', 'outperform', 'model', 'train', 'imageonly', 'tabularonly', 'datum', 'early', 'prediction', 'prediction', 'task', 'report', '∼', 'study', 'study', 'ehrs', 'fuse', 'medical', 'imaging', 'predict', 'different', 'outcome', 'include', 'disease', 'prediction', 'mortality', 'prediction', 'survival', 'prediction', 'treatment', 'outcome', 'prediction', 'study', 'prediction', 'task', 'disease', 'prediction12', 'involve', 'determine', 'individual', 'develop', 'give', 'disease', 'future', 'second', 'common', 'prediction', 'task', 'treatment', 'outcome', 'prediction', 'report', 'studies35', 'follow', 'study', 'mortality', 'prediction', 'overall', 'survival', 'prediction25', 'respectively', 'early', 'fusion', 'technique', 'use', 'studies12', 'disease', 'prediction', 'minha', 'propose', 'early', 'fusion', 'model', 'predict', 'subject', 'progress', 'ad', 'future', 'study', 'concatenate', 'mri', 'extract', 'feature', 'demographic', 'neuropsychological', 'biomarker', 'feed', 'svm', 'model', 'prediction', 'ali', 'propose', 'model', 'predict', 'epileptogeniczone', 'temporal', 'lobe', 'feed', 'extract', 'feature', 'integrate', 'setofsemiology', 'feature', 'various', 'ml', 'model', 'svm', 'gradient', 'boost', 'fuse', 'mri', 'clinicopathological', 'feature', 'predict', 'metachronous', 'distant', 'metastasis', 'dm', 'breast', 'cancer', 'feed', 'concatenate', 'feature', 'lr', 'model', 'study44', 'combine', 'mriderive', 'feature', 'highthroughput', 'brain', 'phenotyping', 'diagnose', 'predict', 'onset', 'ad', 'feed', 'fuse', 'feature', 'different', 'ml', 'classiﬁer', 'include', 'svm', 'train', 'deep', 'fully', 'connect', 'network', 'regressor', 'longitudinal', 'study', 'ad', 'predict', 'cognitive', 'test', 'score', 'multiple', 'future', 'time', 'point', 'model', 'produce', 'mmse', 'score', 'unique', 'future', 'time', 'point', 'sixmonth', 'interval', 'comb', 'biomarker', 'cognitive', 'test', 'score', 'pet', 'mri', 'early', 'fuse', 'imaging', 'feature', 'cognitive', 'test', 'score', 'concatenation', 'feed', 'fully', 'connect', 'network', 'finally', 'bai', 'compare', 'different', 'multimodal', 'biomarker', 'clinical', 'datum', 'biochemical', 'hemologic', 'parameter', 'ultrasound', 'elastography', 'parameter', 'predict', 'assessment', 'ﬁbrosis', 'chronic', 'hepatitis', 'b', 'use', 'svm', 'disease', 'prediction', 'joint', 'fusion', 'use', 'studies17', 'hsu', 'propose', 'deep', 'multimodal', 'fusion', 'model', 'train', 'heterogeneous', 'datum', 'fundus', 'image', 'nonimage', 'datum', 'screening', 'concatenate', 'imaging', 'extract', 'feature', 'inceptionv3', 'clinical', 'datum', 'feature', 'feed', 'fully', 'connect', 'follow', 'softmax', 'layer', 'classiﬁcation', 'develop', 'prediction', 'system', 'jointly', 'fuse', 'ct', 'scan', 'clinical', 'datum', 'predict', 'progression', 'covid19', 'malignancy', 'study', 'feature', 'extraction', 'part', 'apply', 'resnet', 'architecture', 'ct', 'clinical', 'datum', 'respectively', 'concatenate', 'different', 'feature', 'feed', 'lstm', 'network', 'follow', 'fully', 'connected', 'prediction', 'in39', 'author', 'propose', 'deep', 'multimodal', 'model', 'predict', 'neurodevelopmental', 'deﬁcit', 'year', 'age', 'model', 'consist', 'feature', 'extractor', 'fusion', 'classiﬁer', 'feature', 'extractor', 'use', 'architecture', 'extract', 'mri', 'feature', 'fully', 'connect', 'clinical', 'datum', 'study', 'combine', 'extract', 'feature', 'modality', 'feed', 'combination', 'fully', 'connect', 'network', 'fusion', 'classiﬁer', 'prediction', 'evaluate', 'performance', 'modality', 'fusion', 'test', 'model', 'use', 'single', 'modality', 'mri', 'clinical', 'feature', 'result', 'show', 'multimodal', 'fusion', 'outperform', 'single', 'modality', 'performance', 'study46', 'also', 'use', 'multimodal', 'joint', 'fusion', 'ugi', 'cancer', 'screen', 'model', 'integrate', 'feature', 'extract', 'ugi', 'endoscopic', 'image', 'correspond', 'textual', 'medical', 'datum', 'apply', 'image', 'feature', 'extraction', 'word', 'embedding', 'selfattention', 'textual', 'medical', 'datum', 'feature', 'extraction', 'concatenate', 'extract', 'feature', 'modality', 'feed', 'fully', 'connected', 'prediction', 'result', 'show', 'multimodal', 'fusion', 'outperform', 'single', 'modality', 'performance', 'treatment', 'outcome', 'prediction35', 'former35', 'implement', 'early', 'fusion', 'latter47', 'use', 'joint', 'fusion', 'acute', 'ischemic', 'stroke', 'gianluca', 'evaluate', 'predictive', 'power', 'image', 'clinical', 'angiographic', 'feature', 'predict', 'outcome', 'acute', 'ischemic', 'stroke', 'use', 'ml', 'study', 'early', 'fuse', 'feature', 'gradient', 'boost', 'classiﬁer', 'prediction', 'author', 'propose', 'dl', 'model', 'directly', 'exploit', 'multimodal', 'datum', 'clinical', 'metadata', 'noncontrast', 'ncct', 'imaging', 'datum', 'predict', 'success', 'endovascular', 'treatment', 'ischemic', 'stroke', 'utilize', 'selfattention', 'mechanism', 'extract', 'feature', 'image', 'concatenate', 'metadata', 'information', 'classiﬁcation', 'stage', 'propose', 'model', 'process', 'fuse', 'feature', 'fully', 'connect', 'follow', 'softmax', 'function', 'apply', 'output', 'result', 'show', 'multimodal', 'fusion', 'outperform', 'single', 'modality', 'performance', 'mortality', 'overall', 'survival', 'prediction', 'studies25', 'implement', 'early', 'fusion', 'develop', 'model', 'predict', 'covid19', 'ventilatory', 'support', 'mortality', 'early', 'prioritize', 'patient', 'manage', 'hospital', 'resource', 'allocation', 'fuse', 'patient', 'ct', 'image', 'ehr', 'datum', 'feature', 'concatenation', 'feed', 'different', 'ml', 'model', 'include', 'extreme', 'gradient', 'boosting', 'evaluate', 'performance', 'single', 'modality', 'model', 'observe', 'result', 'multimodal', 'fusion', 'well', 'study43', 'aim', 'develop', 'ml', 'model', 'predict', 'glioblastoma', 'patient', 'overall', 'survival', 'os', 'survival', 'pfs', 'base', 'combine', 'treatment', 'feature', 'pathological', 'clinical', 'petctderive', 'information', 'semantic', 'mribased', 'feature', 'concatenate', 'feature', 'modality', 'feed', 'rf', 'model', 'study', 'show', 'model', 'base', 'multimodal', 'fusion', 'datum', 'outperform', 'single', 'modality', 'model', 'dataset', 'patient', 'datum', 'type', 'include', 'study', 'report', 'medical', 'imaging', 'ehrs', 'structured', 'nonstructured', 'patient', 'datum', 'type', 'term', 'image', 'modality', 'ct', 'structural', 'mri', 'smri', 'pet', 'diffusion', 'image', 'pet', 'use', 'study', 'mri', 'pet', 'image', 'utilize', 'modality', 'include', 'study', 'use', 'figure', 'fusion', 'strategy', 'associate', 'clinical', 'outcome', 'different', 'disease', 'mri', 'image', 'use', 'pet', 'image', 'mostly', 'ad', 'diagnosis', 'prediction', 'term', 'ehr', 'structured', 'datum', 'commonly', 'use', 'modality', 'table', 'summarize', 'type', 'imaging', 'ehr', 'datum', 'use', 'study', 'data', 'type', 'imaging', 'datum', 'mri', 'smri', 'diffusion', 'image', 'ultrasound', 'echocardiography', 'pathological', 'image', 'cervigram', 'image', 'endoscopy', 'image', 'ehr', 'datum', 'structure', 'unstructured', 'number', 'study', 'study', 'reference', 'table', 'patient', 'datum', 'type', 'use', 'include', 'study', 'patient', 'datum', 'resource', 'almost', 'twothird', 'study', 'include', 'scoping', 'review', 'use', 'private', 'data', 'source', 'clinical', 'datum', 'publicly', 'available', '∼', 'contrast', 'publicly', 'accessible', 'dataset', 'use', 'study', 'observe', 'use', 'public', 'dataset', '’s', 'disease', 'neuroimage', 'initiative', 'dataset', 'adni58', 'study', 'use', 'dataset', 'publicly', 'available', 'dataset', 'use', 'include', 'study', 'national', '’s', 'coordinate', 'center', 'dataset59', 'medical', 'information', 'mart', 'intensive', 'care', 'mimiciv', 'dataset60', 'dataset61', 'dataset62', 'table', 'summarize', 'public', 'multimodal', 'medical', 'dataset', 'clinical', 'application', 'consider', 'dataset', 'clinical', 'task', 'popular', 'adni', 'ad', 'disease', 'diagnosis', 'prediction', 'evaluation', 'metric', 'evaluation', 'metric', 'mainly', 'dependent', 'clinical', 'task', 'typically', 'accuracy', 'area', 'curve', 'speciﬁcity', 'f1', 'measure', 'precision', 'mostly', 'use', 'evaluation', 'diagnosis', 'prediction', 'task', 'table', 'show', 'distribution', 'evaluation', 'measure', 'use', 'include', 'study', 'discussion', 'section', 'summarize', 'ﬁnding', 'provide', 'future', 'direction', 'research', 'multimodal', 'fusion', 'medical', 'imaging', 'ehr', 'principal', 'ﬁnding', 'find', 'multimodal', 'model', 'combine', 'ehr', 'medical', 'imaging', 'datum', 'generally', 'outperform', 'single', 'modality', 'model', 'task', 'disease', 'diagnosis', 'prediction', 'review', 'show', 'fusion', 'medical', 'imaging', 'clinical', 'context', 'datum', 'improve', 'performance', 'model', 'recommend', 'attempt', 'fusion', 'approach', 'multimodal', 'datum', 'obtainable', 'moreover', 'review', 'observe', 'certain', 'trend', 'ﬁeld', 'multimodality', 'fusion', 'medical', 'area', 'categorize', 'resource', 'observe', 'multimodal', 'datum', 'resource', 'medical', 'imaging', 'ehr', 'limit', 'owe', 'privacy', 'consideration', 'prominent', 'dataset', 'adni', 'contain', 'mri', 'pet', 'image', 'collect', 'individual', 'addition', 'clinical', 'genetic', 'information', 'consider', 'adni', 'contribution', 'advance', 'research', 'similar', 'multimodal', 'dataset', 'develop', 'medical', 'datum', 'source', 'description', 'url', 'clinical', 'outcome', 'study', 'refer', 'ence', 'disease', 'diagnosis', 'ad', 'disease', 'diagnosis', 'disease', 'prediction', 'ad', 'disease', 'diagnosis', 'ad', 'datasample', 'datatype', 'grandchallenge', 'orgdata', 'public', 'dataset', 'adni', 'adni', 'adni', 'represent', 'series', 'study', 'include', 'adni', 'design', 'study', 'progression', 'ad', 'mri', 'pet', 'age', 'clinical', 'genetic', 'information58', 'adni', 'simpliﬁed', 'subset', 'ple', 'featuresatdpole', 'include', 'raw', 'image', 'process', 'tural', 'information', 'image', 'roi', 'average', 'thickness', 'cortex', 'volume', 'brain', 'subregion', 'tablishe', 'facilitate', 'orative', 'ad', 'research', 'dataset', 'comprise', 'mri', 'datum', 'demographic', 'datum', 'neuropsy', 'chological', 'testing', 'score', 'clinical', 'diagnosis', 'tients59', 'mimiccxr', 'dataset', 'patient', 'chest', 'radiograph', 'contain', 'study', 'patients63', 'mimiciv', 'database', 'patient', 'admit', 'critical', 'care', 'unit', 'comprise', 'patient', 'stay', 'information', 'patient', 'icu', 'datum', 'lookup', 'table', 'allow', 'link', 'mimic', 'cxr60', 'datum', 'collection', 'produce', 'major', 'nci', 'initiative', 'list', 'datum', 'cata', 'log', 'include', 'clinical', 'datum', 'genomic', 'image', 'pro', 'teomic', 'longitudinal', 'study', 'patient', 'treat', 'dovascular', 'therapy', 'netherland', 'acute', 'chemic', 'stroke', 'comprise', 'ncct', 'image', 'angiogra', 'phy', 'image', 'clini', 'cal', 'information', 'patients62', 'requestingdata', 'naccdata', 'disease', 'diagnosis', 'mci', 'mimiccxrhttps', 'wwwnature', 'comarticle', 'mimicivhttps', 'physionetorg', 'contentmimiciv0', 'datasciencecancer', 'govresource', 'diagnosis', 'car', 'disease', 'diagnosis', 'cervical', 'dysplasia', 'outcome', 'predic', 'tion', 'ischemic', 'stroke', 'table', 'multimodal', 'medical', 'dataset', 'clinical', 'outcome', 'application', 'study', 'reference', 'number', 'study', 'evaluation', 'metric', 'accuracy', 'sensitivity', 'precision', 'positive', 'predictive', 'value', 'ppv', 'nega', 'tive', 'predictive', 'value', 'correlation', 'coefﬁcient', 'mcc', 'cindex', 'rootmean', 'square', 'error', 'rmse', 'number', 'second', 'column', 'sum', 'many', 'study', 'use', 'single', 'metric', 'table', 'distribution', 'evaluation', 'metric', 'include', 'study', 'fusion', 'implementation', 'early', 'fusion', 'commonly', 'use', 'technique', 'application', 'multimodal', 'learning', 'fuse', 'ehr', 'datum', 'image', 'datum', 'image', 'datum', 'convert', 'vector', 'extract', 'highlevel', 'representation', 'use', 'manual', 'softwaregenerate', 'extract', 'features8', 'learn', 'image', 'feature', 'often', 'result', 'well', 'taskspeciﬁc', 'performance', 'manually', 'softwarederive', 'features64', 'base', 'review', 'study', 'early', 'fusion', 'model', 'perform', 'well', 'conventional', 'singlemodality', 'model', 'task', 'researcher', 'use', 'early', 'fusion', 'method', 'ﬁrst', 'attempt', 'learn', 'multimodal', 'representation', 'learn', 'exploit', 'interaction', 'correlation', 'feature', 'modality', 'furthermore', 'require', 'model', 'train', 'make', 'pipeline', 'training', 'easy', 'joint', 'late', 'fusion', 'however', 'imaging', 'feature', 'extract', 'early', 'fusion', 'require', 'multiple', 'model', 'train', 'joint', 'fusion', 'second', 'commonly', 'use', 'fusion', 'approach', 'modality', 'perspective', 'cnn', 'appear', 'good', 'option', 'image', 'feature', 'extraction', 'tabular', 'datum', 'mainly', 'process', 'use', 'dense', 'layer', 'feed', 'model', 'text', 'datum', 'mostly', 'process', 'use', 'lstm', 'layer', 'follow', 'attention', 'layer', 'current', 'research', 'directly', 'concatenate', 'feature', 'vector', 'different', 'modality', 'combine', 'multimodal', 'datum', 'use', 'nn', 'implement', 'joint', 'fusion', 'limitation', 'deal', 'small', 'dataset', 'mean', 'joint', 'fusion', 'prefer', 'large', 'dataset', 'small', 'dataset', 'preferable', 'use', 'early', 'late', 'fusion', 'method', 'implement', 'use', 'classical', 'ml', 'technique', 'nevertheless', 'expect', 'agree', 'with26', 'joint', 'fusion', 'model', 'provide', 'well', 'result', 'fusion', 'strategy', 'update', 'feature', 'representation', 'iteratively', 'propagate', 'loss', 'feature', 'extraction', 'model', 'aim', 'learn', 'correlation', 'modality', 'base', 'performance', 'report', 'include', 'study', 'prefer', 'try', 'early', 'joint', 'fusion', 'relation', 'datum', 'modality', 'complementary', 'review', 'ad', 'diagnosis', 'example', 'image', 'ehrs', 'datum', 'dependent', 'relevant', 'accurate', 'knowledge', 'patient', 'current', 'symptomatology', 'personal', 'information', 'imaging', 'report', 'help', 'doctor', 'interpret', 'imaging', 'result', 'suitable', 'clinical', 'context', 'result', 'precise', 'diagnosis', 'therefore', 'ad', 'diagnosis', 'study', 'review', 'implement', 'early', 'fusion13–15', 'joint', 'fusion49', 'multimodal', 'learning', 'hand', 'prefer', 'try', 'late', 'fusion', 'input', 'modality', 'complement', 'example', 'brain', 'quantitative', 'result', 'mmse', 'diagnose', 'independent', 'make', 'appropriate', 'candidate', 'inclusion', 'late', 'fusion', 'strategy', 'also', 'late', 'fusion', 'impose', 'requirement', 'huge', 'amount', 'training', 'datum', 'use', 'modality', 'data', 'size', 'small', 'moreover', 'late', 'fusion', 'strategy', 'attempt', 'concatenation', 'feature', 'vector', 'multiple', 'modality', 'result', 'highdimensional', 'vector', 'difﬁcult', 'ml', 'algorithm', 'learn', 'overﬁtte', 'many', 'input', 'sample', 'available', 'late', 'fusion', 'multiple', 'model', 'employ', 'specialize', 'single', 'modality', 'thereby', 'limit', 'size', 'input', 'feature', 'vector', 'model', 'furthermore', 'late', 'fusion', 'use', 'datum', 'incomplete', 'miss', 'patient', 'image', 'datum', 'clinical', 'datum', 'vice', 'versa', 'late', 'fusion', 'use', 'independent', 'model', 'different', 'modality', 'aggregation', 'method', 'averaging', 'majority', 'voting', 'use', 'even', 'prediction', 'modality', 'present', 'moreover', 'prediction', 'disproportionately', 'inﬂuence', 'featurerich', 'input', 'modality', 'number', 'feature', 'different', 'input', 'datum', 'modalities65', 'scenario', 'late', 'fusion', 'preferable', 'allow', 'train', 'model', 'use', 'modality', 'separately', '•', 'application', 'review', 'find', 'ad', 'diagnosis', 'prediction12–15', 'common', 'application', 'address', 'multimodal', 'setting', 'study', 'use', 'fusion', 'technique', 'consistently', 'demonstrate', 'improved', 'ad', 'diagnosis', 'clinician', 'experience', 'difﬁculty', 'accurate', 'reliable', 'diagnosis', 'even', 'multimodal', 'datum', 'available26', 'emphasize', 'utility', 'signiﬁcance', 'multimodal', 'fusion', 'approach', 'clinical', 'application', 'prospect', 'review', 'note', 'multimodal', 'medical', 'datum', 'fusion', 'grow', 'potential', 'achieve', 'stateoftheart', 'performance', 'healthcare', 'application', 'nonetheless', 'growth', 'hamper', 'absence', 'adequate', 'datum', 'benchmarking', 'method', 'surprising', 'give', 'privacy', 'concern', 'surround', 'reveal', 'healthcare', 'datum', 'moreover', 'observe', 'lack', 'complexity', 'used', 'nonimage', 'datum', 'particularly', 'context', 'heavily', 'featurerich', 'datum', 'include', 'ehr', 'example', 'majority', 'study', 'focus', 'mostly', 'basic', 'demographic', 'datum', 'gender', 'age12', 'limited', 'number', 'study', 'also', 'include', 'medical', 'history', 'smoking', 'status', 'hypertension18', 'speciﬁc', 'clinical', 'characteristic', 'know', 'associate', 'certain', 'disease', 'mmse', 'diagnose', 'ad', 'addition', 'select', 'diseaseassociate', 'feature', 'future', 'research', 'beneﬁt', 'use', 'vast', 'amount', 'featurerich', 'datum', 'demonstrate', 'domain', 'outside', 'medicine', 'autonomous', 'driving66', 'future', 'direction', 'focus', 'ehr', 'medical', 'imaging', 'multimodal', 'datum', 'modality', 'multiomic', 'environmental', 'datum', 'also', 'integrate', 'use', 'aforementioned', 'fusion', 'approach', 'cause', 'many', 'disease', 'complex', 'many', 'factor', 'include', 'inherit', 'genetic', 'lifestyle', 'living', 'environment', 'contribute', 'development', 'disease', 'therefore', 'combine', 'multisource', 'datum', 'eg', 'ehr', 'imaging', 'multiomic', 'datum', 'lead', 'holistic', 'view', 'improve', 'patient', 'outcome', 'personalized', 'medicine', 'focus', 'ehr', 'medical', 'imaging', 'multimodal', 'datum', 'modality', 'multiomic', 'environman', 'tal', 'datum', 'also', 'integrate', 'use', 'aforementioned', 'fusion', 'approach', 'cause', 'many', 'disease', 'complex', 'many', 'factor', 'include', 'inherit', 'genetic', 'lifestyle', 'living', 'environment', 'contribute', 'development', 'disease', 'therefore', 'combine', 'multisource', 'datum', 'eg', 'ehr', 'imaging', 'multiomic', 'datum', 'lead', 'holistic', 'view', 'improve', 'patient', 'outcome', 'personalized', 'medicine', 'moreover', 'unavailability', 'multimodal', 'public', 'datum', 'limitation', 'hinder', 'development', 'correspond', 'research', 'many', 'factor', 'eg', 'gender', 'ethnicity', 'environmental', 'factor', 'inﬂuence', 'research', 'direction', 'even', 'clinical', 'decision', 'rely', 'publicly', 'available', 'dataset', 'enough', 'make', 'conclusive', 'clinical', 'claim', 'global', 'population27', 'consequently', 'imperative', 'encourage', 'sharing', 'ﬂexible', 'datum', 'institution', 'hospital', 'order', 'facilitate', 'exploration', 'wide', 'range', 'population', 'datum', 'clinical', 'research', 'federate', 'learn', 'provide', 'ability', 'collect', 'datum', 'safely', 'securely', 'multiple', 'center', 'use', 'collect', 'multimodal', 'datum', 'various', 'center', 'train', 'largescale', 'model', 'collect', 'datum', 'directly', 'limitation', 'search', 'limit', 'study', 'publish', 'previous', 'year', 'consider', 'study', 'publish', 'lead', 'leave', 'study', 'publish', 'language', 'solely', 'include', 'study', 'fuse', 'ehr', 'medical', 'imaging', 'include', 'study', 'use', 'data', 'modality', 'multiomics', 'datum', 'scope', 'work', 'positive', 'result', 'typically', 'report', 'disproportionately', 'publication', 'bias', 'limitation', 'review', 'bias', 'result', 'overestimation', 'beneﬁts', 'associate', 'multimodal', 'datum', 'analysis', 'study', 'include', 'review', 'employ', 'various', 'input', 'modality', 'investigate', 'various', 'clinical', 'task', 'different', 'disease', 'report', 'different', 'performance', 'metric', 'hence', 'direct', 'comparison', 'result', 'present', 'study', 'always', 'applicable', 'furthermore', 'article', 'provide', 'conﬁdence', 'bound', 'make', 'difﬁcult', 'compare', 'result', 'statistically', 'conclusion', 'multimodal', 'area', 'research', 'gain', 'attention', 'medical', 'ﬁeld', 'review', 'survey', 'multimodal', 'medical', 'ml', 'literature', 'combine', 'ehr', 'medical', 'imaging', 'datum', 'discuss', 'fusion', 'strategy', 'clinical', 'task', 'ml', 'model', 'implement', 'data', 'fusion', 'type', 'disease', 'publicly', 'accessible', 'multimodal', 'datum', 'medical', 'imaging', 'ehrs', 'furthermore', 'highlight', 'direction', 'pave', 'way', 'future', 'research', 'ﬁnding', 'suggest', 'grow', 'interest', 'multimodal', 'medical', 'datum', 'still', 'study', 'combine', 'modality', 'relatively', 'simple', 'strategy', 'show', 'effective', 'fully', 'exploit', 'rich', 'information', 'embed', 'modality', 'fastgrowing', 'ﬁeld', 'new', 'ai', 'model', 'multimodal', 'datum', 'constantly', 'develop', 'exist', 'study', 'fall', 'deﬁnition', 'fusion', 'strategy', 'use', 'combination', 'strategy', 'believe', 'development', 'ﬁeld', 'give', 'rise', 'comprehensive', 'multimodal', 'medical', 'datum', 'analysis', 'great', 'support', 'clinical', 'decisionmaking', 'process', 'datum', 'availability', 'datum', 'generate', 'scoping', 'review', 'provide', 'supplementary', 'material', 'reference', 'detsky', 'inevitable', 'application', 'big', 'datum', 'health', 'care', 'obermeyer', 'z', 'emanuel', 'j', 'predict', 'future', 'big', 'datum', 'machine', 'learning', 'clinical', 'medicine', 'new', 'roski', 'create', 'value', 'health', 'care', 'big', 'datum', 'opportunity', 'policy', 'implication', 'heal', 'affair', 'autonomous', 'robot', 'vehicle', 'springer', 'science', 'business', 'medium', 'castanedo', 'review', 'datum', 'fusion', 'technique', 'scientiﬁc', 'world', 'cohen', 'accuracy', 'information', 'imaging', 'requisition', 'matter', 'comfere', 'et', 'providertoprovider', 'communication', 'dermatology', 'implication', 'miss', 'clinical', 'information', 'skin', 'requisition', 'form', 'systematic', 'review', 'int', 'dermatology', 'jona', 'comfere', 'et', 'concern', 'challenge', 'clinical', 'information', 'skin', 'biopsy', 'requisition', 'form', 'mixedmethod', 'study', 'cutaneous', 'pathology', 'review', 'machine', 'learning', 'principle', 'multiview', 'biological', 'datum', 'integration', 'brieﬁng', 'ramachandram', 'deep', 'multimodal', 'learn', 'survey', 'recent', 'advance', 'trend', 'ieee', 'signal', 'processing', 'magazine', 'conversion', 'prediction', 'use', 'future', 'value', 'forecasting', 'multimodal', 'feature', 'comput', 'intelligence', 'neuroscience', 'pillai', 'fuse', 'heterogeneous', 'datum', 'disease', 'classiﬁcation', 'medinfo', 'ehealthenable', 'health', 'io', 'press', 'kp', '’s', 'classiﬁcation', 'use', 'dynamic', 'ensemble', 'classiﬁer', 'selection', 'algorithm', 'performance', 'analysis', 'biomed', 'signal', 'process', 'control', 'akramifard', 'h', 'balafar', 'ramli', 'r', 'early', 'detection', 'disease', 'base', 'clinical', 'trial', 'threedimensional', 'imaging', 'datum', 'personal', 'information', 'use', 'autoencoder', 'medical', 'signal', 'sensor', 'yan', 'r', 'rich', 'fusion', 'network', 'breast', 'cancer', 'classiﬁcation', 'base', 'multimodal', 'datum', 'bmc', 'med', 'informatic', 'et', 'deep', 'learning', 'automate', 'diabetic', 'retinopathy', 'screening', 'fuse', 'heterogeneous', 'datum', 'ehr', 'lead', 'early', 'referral', 'decision', 'transl', 'technol', 'xu', 'accurately', 'differentiate', 'patient', 'covid19', 'patient', 'viral', 'infection', 'healthy', 'individual', 'multimodal', 'late', 'fusion', 'learn', 'approach', 'med', 'internet', 'diagnosis', 'chinese', 'context', 'uncertainty', 'informationcentric', 'bayesian', 'deep', 'learning', 'model', 'inf', 'process', 'manag', 'azam', 'review', 'multimodal', 'medical', 'image', 'fusion', 'compendious', 'analysis', 'medical', 'modality', 'multimodal', 'database', 'fusion', 'technique', 'quality', 'metric', 'comput', 'biol', 'medicine', 'advance', 'multimodal', 'data', 'fusion', 'neuroimaging', 'overview', 'challenge', 'novel', 'orientation', 'inf', 'fusion', 'behrad', 'overview', 'deep', 'learning', 'method', 'multimodal', 'medical', 'datum', 'mining', 'expert', 'syst', 'synnergren', 'multimodal', 'deep', 'learning', 'biomedical', 'datum', 'fusion', 'review', 'brieﬁng', 'muhammad', 'g', 'comprehensive', 'survey', 'multimodal', 'medical', 'signal', 'fusion', 'smart', 'healthcare', 'system', 'inf', 'fusion', 'aljouie', 'early', 'prediction', 'covid19', 'ventilation', 'requirement', 'mortality', 'routinely', 'collect', 'baseline', 'chest', 'radiograph', 'laboratory', 'clinical', 'datum', 'machine', 'learn', 'multidisciplinary', 'healthcare', 'banerjee', 'lungren', 'p', 'fusion', 'medical', 'imaging', 'electronic', 'health', 'record', 'use', 'deep', 'learn', 'systematic', 'review', 'implementation', 'guideline', 'digital', 'medicine', 'machine', 'learn', 'multimodal', 'electronic', 'health', 'recordsbase', 'research', 'challenge', 'perspective', 'preprint', 'c', 'prisma', 'extension', 'scope', 'review', 'prismascr', 'explanation', 'annal', 'internal', 'medicine', 'ouzzani', 'elmagarmid', 'rayyan', 'web', 'mobile', 'app', 'systematic', 'review', 'syst', 'review', 'arksey', 'l', 'scope', 'study', 'methodological', 'framework', 'int', 'research', 'methodology', 'grant', 'booth', 'typology', 'review', 'analysis', 'review', 'type', 'associated', 'methodology', 'heal', 'information', 'journal', 'interpretation', 'deep', 'multimodal', 'fusion', 'diagnostic', 'classiﬁcation', 'international', 'joint', 'conference', 'neural', 'network', 'ieee', 'achalia', 'r', 'proof', 'concept', 'machine', 'learn', 'analysis', 'use', 'multimodal', 'neuroimaging', 'neurocognitive', 'measure', 'predictive', 'biomarker', 'bipolar', 'disorder', 'asian', 'j', 'psychiatry', 'alimmarvasti', 'machine', 'learn', 'localize', 'epileptogeniczone', 'temporal', 'lobe', 'quantify', 'value', 'multimodal', 'clinicalsemiology', 'imaging', 'concordance', 'front', 'digital', 'health', 'brugnara', 'predictive', 'modeling', 'endovascular', 'treatment', 'outcome', 'acute', 'ischemic', 'stroke', 'use', 'machinelearne', 'stroke', 'ebdrup', 'b', 'h', 'accuracy', 'diagnostic', 'classiﬁcation', 'algorithm', 'use', 'cognitive', 'electrophysiological', 'neu', 'roanatomical', 'datum', 'schizophrenia', 'patient', 'psychol', 'medicine', 'fusion', 'deep', 'learning', 'model', 'mri', 'scan', 'mini', 'mental', 'state', 'examination', 'logical', 'memory', 'test', 'enhance', 'diagnosis', 'mild', 'cognitive', 'impairment', 'dementia', 'diagn', 'assess', 'deep', 'learning', 'predict', 'covid19', 'malignant', 'progression', 'image', 'analysis', 'l', 'deep', 'multimodal', 'learn', 'mri', 'clinical', 'datum', 'early', 'prediction', 'neurodevelopmental', 'deﬁcit', 'preterm', 'infant', 'front', 'neuroscience', 'zamanian', 'r', 'banerjee', 'lungren', 'p', 'multimodal', 'fusion', 'deep', 'neural', 'network', 'leverage', 'ct', 'imaging', 'electronic', 'health', 'record', 'casestudy', 'pulmonary', 'embolism', 'detection', 'report', 'metastasis', 'prediction', 'multifeature', 'fusion', 'model', 'breast', 'cancer', 'aging', 'albany', 'ny', 'biancardi', 'machinelearne', 'support', 'individual', 'diagnosis', 'mild', 'cognitive', 'impairment', 'use', 'multimodal', 'mri', 'cognitive', 'assessment', 'alzheimer', 'peeken', 'c', 'combine', 'multimodal', 'imaging', 'treatment', 'feature', 'improve', 'machine', 'learningbase', 'prognostic', 'assessment', 'patient', 'glioblastoma', 'cancer', 'medicine', 'diagnosis', 'prognosis', 'disease', 'use', 'brain', 'morphometry', 'white', 'matter', 'connectome', 'machinelearne', 'approach', 'use', 'petbased', 'radiomic', 'predict', 'histological', 'subtype', 'lung', 'cancer', 'scnet', 'novel', 'ugi', 'cancer', 'screening', 'framework', 'base', 'semanticlevel', 'multimodal', 'datum', 'fusion', 'ieee', 'j', 'biomed', 'heal', 'informatic', 'samak', 'clatworthy', 'mirmehdi', 'prediction', 'thrombectomy', 'functional', 'outcome', 'use', 'multimodal', 'datum', 'annual', 'conference', 'medical', 'image', 'understanding', 'analysis', 'springer', 'morar', 'deeplearning', 'approach', 'prediction', 'minimental', 'state', 'examination', 'score', 'multimodal', 'longitudinal', 'study', 'international', 'conference', 'computational', 'science', 'computational', 'intelligence', 'ieee', 'multimodal', 'diagnosis', 'predictive', 'model', 'disease', 'fewshot', 'learning', 'international', 'conference', 'public', 'health', 'datum', 'science', 'icphd', 'forouzannezhad', 'p', 'abbaspour', 'cabrerizo', 'early', 'diagnosis', 'mild', 'cognitive', 'impairment', 'use', 'random', 'forest', 'feature', 'selection', 'ieee', 'biomedical', 'circuit', 'system', 'conference', 'forouzannezhad', 'p', 'abbaspour', 'deep', 'neural', 'network', 'approach', 'early', 'diagnosis', 'mild', 'cognitive', 'impairment', 'use', 'multiple', 'feature', '17th', 'ieee', 'international', 'conference', 'machine', 'learning', 'application', 'icmla', 'bai', 'z', 'comparison', 'multimodal', 'biomarker', 'chronic', 'b', 'assessment', 'use', 'recursive', 'feature', 'elimination', '38th', 'annual', 'international', 'conference', 'ieee', 'engineering', 'medicine', 'biology', 'society', 'embc', 'orlando', 'ieee', 'multimodal', 'deep', 'learning', 'cervical', 'dysplasia', 'diagnosis', 'r', 'unal', 'well', 'medical', 'image', 'computing', 'computerassiste', 'intervention', 'springer', 'international', 'publishing', 'syedamahmood', 'identify', 'patient', 'risk', 'aortic', 'stenosis', 'learn', 'multimodal', 'datum', 'r', 'unal', 'well', 'medical', 'image', 'computing', 'computerassiste', 'intervention', 'miccai', 'springer', 'international', 'publishing', 'grant', 'b', 'parson', 'mahdi', 'deep', 'learning', 'classiﬁcation', 'use', 'combined', 'imaging', 'nonimage', 'icu', 'datum', 'yaqub', 'noble', 'ed', 'medical', 'image', 'understanding', 'analysis', 'springer', 'international', 'publishing', 'tsekos', 'v', 'sm2n2', 'stacked', 'architecture', 'multimodal', 'datum', 'application', 'myocardial', 'infarction', 'detection', 'ed', 'statistical', 'atlas', 'computational', 'model', 'heart', 'mms', 'challenge', 'springer', 'international', 'publishing', 'scalable', 'deeplearning', 'model', 'automate', 'diagnosis', 'pulmonary', 'embolism', 'use', 'medicine', 'mueller', 'disease', 'neuroimage', 'initiative', 'neuroimage', '101016jnic200509008', 'beekly', '’s', 'coordinate', 'center', 'alzheimer', 'disease', 'database', 'alzheimer', 'disease', 'associate', 'disorder', 'alistair', 'marinescu', 'r', 'prediction', 'longitudinal', 'evolution', 'preprint', 'fransen', 'p', 'multicenter', 'randomize', 'clinical', 'trial', 'endovascular', 'treatment', 'acute', 'ischemic', 'stroke', 'study', 'protocol', 'randomize', 'control', 'trial', 'trial', 'johnson', 'e', 'publicly', 'available', 'database', 'chest', 'radiograph', 'report', 'goodfellow', 'bengio', 'deep', 'learning', 'mit', 'press', 'reda', 'deep', 'learning', 'role', 'early', 'diagnosis', 'prostate', 'cancer', 'technol', 'cancer', 'research', 'treatment', 'hecker', 'endtoend', 'learn', 'drive', 'model', 'surroundview', 'camera', 'route', 'planner', 'proceeding', 'conference', 'computer', 'vision', 'eccv', 'sahu', 'smith', 'federated', 'learning', 'challenge', 'method', 'future', 'direction', 'ieee', 'signal', 'process', 'shah', 'federated', 'learning', 'internet', 'medical', 'thing', 'opportunity', 'challenge', '20th', 'international', 'conference', 'informatic', 'management', 'technology', 'healthcare', 'icimth', 'author', 'contribution', 'statement', 'contribute', 'conceptualization', 'fm', 'administer', 'project', 'curate', 'datum', 'perform', 'datum', 'synthesis', 'contribute', 'write', 'original', 'draft', 'perform', 'writing', 'review', 'edit', 'zs', 'supervise', 'author', 'read', 'approve', 'ﬁnal', 'manuscript', 'additional', 'information', 'supplementary', 'method', 'compete', 'interest', 'author', 'declare', 'compete', 'interest']"
Driver Locations Harvesting Attack on pRide,"[{'href': 'http://arxiv.org/abs/2210.13263v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.13263v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-10-24 14:03:36,"Hindering Adversarial Attacks with Implicit Neural Representations

Andrei A. Rusu 1 Dan A. Calian 1 Sven Gowal 1 Raia Hadsell 1

2
2
0
2

t
c
O
2
2

]

G
L
.
s
c
[

1
v
2
8
9
3
1
.
0
1
2
2
:
v
i
X
r
a

Abstract
We introduce the Lossy Implicit Network Activa-
tion Coding (LINAC) defence, an input transfor-
mation which successfully hinders several com-
mon adversarial attacks on CIFAR-10 classiﬁers
for perturbations up to (cid:15) = 8/255 in L∞ norm
and (cid:15) = 0.5 in L2 norm.
Implicit neural rep-
resentations are used to approximately encode
pixel colour intensities in 2D images such that
classiﬁers trained on transformed data appear to
have robustness to small perturbations without
adversarial training or large drops in performance.
The seed of the random number generator used
to initialise and train the implicit neural represen-
tation turns out to be necessary information for
stronger generic attacks, suggesting its role as a
private key. We devise a Parametric Bypass Ap-
proximation (PBA) attack strategy for key-based
defences, which successfully invalidates an ex-
isting method in this category. Interestingly, our
LINAC defence also hinders some transfer and
adaptive attacks, including our novel PBA strat-
egy. Our results emphasise the importance of a
broad range of customised attacks despite appar-
ent robustness according to standard evaluations.
LINAC source code and parameters of defended
classiﬁer evaluated throughout this submission
are available: https://github.com/deepmind/linac.

1. Introduction

Training Deep Neural Network (DNN) classiﬁers which are
accurate yet generally robust to small adversarial perturba-
tions is an open problem in computer vision and beyond,
inspiring much empirical and foundational research into
modern DNNs. Szegedy et al. (2014) showed that DNNs are
not inherently robust to imperceptible input perturbations,
which reliably cross learned decision boundaries, even those
of different models trained on similar data. With hindsight,

1DeepMind, London, UK. Correspondence to: Andrei A. Rusu

<andrei@deepmind.com>.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

it becomes evident that two related yet distinct design prin-
ciples have been at the core of proposed defences ever since.
Intuitively, accurate DNN classiﬁers could be considered ro-
bust in practice if: (I) their decision boundaries were largely
insensitive to all adversarial perturbations, and/or (II) com-
puting any successful adversarial perturbations was shown
to be expensive, ideally intractable. Early defences built
on principle (I) include the adversarial training approach of
Madry et al. (2018) and the veriﬁable defences of Hein &
Andriushchenko (2017); Raghunathan et al. (2018), with
many recent works continually reﬁning such algorithms,
e.g. Cohen et al. (2019); Gowal et al. (2020); Rebufﬁ et al.
(2021). A wide range of defences were built, or shown
to operate, largely on principle (II), including adversarial
detection methods (Carlini & Wagner, 2017a), input trans-
formations (Guo et al., 2018) and denoising strategies (Liao
et al., 2018; Niu et al., 2020). Many such approaches have
since been circumvented by more effective attacks, such as
those proposed by Carlini & Wagner (2017b), or by using
“adaptive attacks” (Athalye et al., 2018; Tramer et al., 2020).

Despite the effectiveness of recent attacks against these
defences, Garg et al. (2020) convincingly argue on a theo-
retical basis that principle (II) is sound; similarly to cryp-
tography, robust learning could rely on computational hard-
ness, even in cases where small adversarial perturbations
do exist and would be found by a hypothetical, computa-
tionally unbounded adversary. However, constructing such
robust classiﬁers for problems of interest, e.g. image clas-
siﬁcation, remains an open problem. Recent works have
proposed defences based on cryptographic principles, such
as the pseudo-random block pixel shufﬂing approach of
AprilPyone & Kiya (2021a). As we will show, employing
cryptographic principles in algorithm design is not in it-
self enough to prevent efﬁcient attacks. Nevertheless, we
build on the concept of key-based input transformation and
propose a novel defence based on Implicit Neural Repre-
sentations (INRs). We demonstrate that our Lossy Implicit
Neural Activation Coding (LINAC) defence hinders most
standard and even adaptive attacks, more so than the related
approaches we have tested, without making any claims of
robustness about our defended classiﬁer.

Contributions: (1) We demonstrate empirically that lossy
INRs can be used in a standard CIFAR-10 image classiﬁca-
tion pipeline if they are computed using the same implicit

 
 
 
 
 
 
Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

network initialisation, a novel observation which makes our
LINAC defence possible. (2) The seed of the random num-
ber generator used for initialising and computing INRs is
shown to be an effective and compact private key, since with-
holding this information hinders a suite of standard adver-
sarial attacks widely used for robustness evaluations. (3) We
report our systematic efforts to circumvent the LINAC de-
fence with transfer and a series of adaptive attacks, designed
to expose and exploit potential weaknesses of LINAC. (4) To
the same end we propose the novel Parametric Bypass Ap-
proximation (PBA) attack strategy, valid under our threat
model, and applicable to other defences using secret keys.
We demonstrate its effectiveness by invalidating an existing
key-based defence which was previously assumed robust.

2. Related Work

Adversarial Robustness. Much progress has been made
towards robust image classiﬁers along the adversarial train-
ing (Madry et al., 2018) route, which has been extensively
explored and is well reviewed, e.g. in (Schott et al., 2019;
Pang et al., 2020; Gowal et al., 2020; Rebufﬁ et al., 2021).
While such approaches can be effective against current at-
tacks, a complementary line of work investigates certiﬁed
defences, which offer guarantees of robustness around ex-
amples for some well deﬁned sets (Wong & Kolter, 2018;
Raghunathan et al., 2018; Cohen et al., 2019). Indeed, many
such works acknowledge the need for complementary ap-
proaches, irrespective of the success of adversarial training
and the well understood difﬁculties in combining methods
(He et al., 2017). The proliﬁc work on defences against
adversarial perturbations has spurred the development of
stronger attacks (Carlini & Wagner, 2017b; Brendel et al.,
2018; Andriushchenko et al., 2020) and standardisation of
evaluation strategies for threat models of interest (Athalye
et al., 2018; Croce & Hein, 2020), including adaptive attacks
(Tramer et al., 2020). Alongside the empirical progress to-
wards building robust predictors, this line of research has
yielded an improved understanding of current deep learn-
ing models (Ilyas et al., 2019; Engstrom et al., 2019), the
limitations of effective adversarial robustness techniques
(Jacobsen et al., 2018), and the data required to train them
(Schmidt et al., 2018).

Athalye et al. (2018) show that a number of defences pri-
marily hinder gradient-based adversarial attacks by obfus-
cating gradients. Various forms are identiﬁed, such as gra-
dient shattering (Goodfellow et al., 2014), gradient masking
(Papernot et al., 2017), exploding and vanishing gradients
(Song et al., 2018b), stochastic gradients (Dhillon et al.,
2018) and a number of input transformations aimed at coun-
tering adversarial examples, including noise ﬁltering ap-
proaches using PCA or image quilting (Guo et al., 2018),
the Saak transform (Song et al., 2018a), low-pass ﬁltering

(Shaham et al., 2018), matrix estimation (Yang et al., 2019)
and JPEG compression (Dziugaite et al., 2016; Das et al.,
2017; 2018). Indeed, many such defences have been pro-
posed, as reviewed by Niu et al. (2020), they have ranked
highly in competitions (Kurakin et al., 2018), and many
have since been shown to be less robust than previously
thought, e.g. by Athalye et al. (2018) and Tramer et al.
(2020), who use adaptive attacks to demonstrate that several
input transformations offer little to no robustness.

To build on such insights, it is worth identifying the “ingre-
dients” essential to the success of adversarial attacks. Most
effective attacks, including adaptive ones, assume the ability
to approximate the outputs of the targeted model for arbi-
trary inputs. This is reasonable when applying the correct
transformation is tractable for the attacker. Hence, deny-
ing access to such computations seems to be a promising
direction for hindering adversarial attacks. AprilPyone &
Kiya (2020; 2021b); MaungMaung & Kiya (2021) borrow
standard practice from cryptography and assume that an
attacker has full knowledge of the defence’s algorithm and
parameters, short of a small number of bits which make
up a private key. Another critical “weakness” of such in-
put denoising defences is that they can be approximated
by the identity mapping for the purpose of computing gra-
dients (Athalye et al., 2018). Even complex parametric
approaches, which learn stochastic generative models of
the input distribution, are susceptible to reparameterisation
and Expectation-over-Transformation (EoT) attacks in the
white-box setting. Thus, it is worth investigating whether
non-parametric, lossy and fully deterministic input transfor-
mations exist such that downstream models can still perform
tasks of interest to high accuracy, while known and novel
attack strategies are either ruled out, or at least substantially
hindered, including adaptive attacks.

Implicit Neural Representations. Neural networks have
been used to parameterise many kinds of signals, see the
work by Sitzmann (2020) for an extensive list, with remark-
able recent advances in scene representations (Mildenhall
et al., 2020) and image processing (Sitzmann et al., 2020).
INRs have been used in isolation per image or scene, not
for generalisation across images. Some exceptions exist
in unsupervised learning, e.g. Skorokhodov et al. (2021)
parameterise GAN decoders such that they directly output
INRs of images, rather than colour intensities for all pixels.
In this paper we show that INRs can be used to discover
functional decompositions of RGB images which enable
comparable generalisation to learning on the original signal
encoding (i.e. RGB).

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

Figure 1. Visual depiction of LINAC, our proposed input transformation. An RGB image x is converted into an Activation Image t(x) with
identical spatial dimensions, but H channels instead of 3. A neural network model which maps pixel coordinates to RGB colour intensities
is ﬁt such that it approximates x. The resulting model parameters (after ﬁtting) are called the Implicit Neural Representation (INR) of
image x. In order to output correct RGB colour intensities for all pixels, the implicit neural network needs to compute a hierarchical
functional decomposition of x. We empirically choose an intermediate representation to deﬁne our transformation. Activations in the
middle hidden layer are associated with their corresponding pixel coordinates to form the output Activation Image t(x), with as many
channels as there are units in the middle layer (H).

3. Hindering Adversarial Attacks with
Implicit Neural Representations

In this section we introduce LINAC, our proposed input
transformation which hinders adversarial attacks by leverag-
ing implicit neural representations, also illustrated in Fig. 1.

Setup. We consider a supervised learning task with a dataset
D ⊂ X × Y of pairs of images x and their correspond-
ing labels y. We use a deterministic input transformation
t : X → H which transforms input images, x (cid:55)→ t(x), while
preserving their spatial dimensions. Further, we consider
a classiﬁer fθ, parameterised by θ, whose parameters are
estimated by Empirical Risk Minimisation (ERM) to map
transformed inputs to labels fθ : H → Y. The model is not
adversarially trained, yet ﬁnding adversarial examples for it
is hindered by LINAC, as we demonstrate through extensive
evaluations in Section 5.

Implicit Neural Representations. For an image x, its im-
plicit neural representation is given by a multi-layer per-
ceptron (MLP) Φ = hL ◦ hL−1 ◦ · · · ◦ h0, Φ : R2 → R3,
with L hidden layers, which maps spatial coordinates to
their corresponding colours. Φφ is a solution to the implicit
equation:

Φ(p) − x(p) = 0,

(1)

where p are spatial coordinates (i.e. pixel locations) and
x(p) are the corresponding image colours. Our input trans-
formation leverages this implicit neural representation to
encode images in an approximate manner.

Reconstruction Loss. The implicit equation (1) can be
translated (Sitzmann et al., 2020) into a standard recon-
struction loss between image colours and the output of a
multi-layer perceptron Φφ at each (2D) pixel location pi,j,

L(φ, x) =

(cid:88)

i,j

||Φφ(pi,j) − x(pi,j)||2
2.

(2)

Algorithm 1 The LINAC Transform

Inputs: RGB image x (with size I × J × 3); private key;
number of epochs N ; mini-batch size M ; number of MLP
layers L; representation layer K; learning rate µ.
Output: Activation Image t(x) (with size I × J × H).
rng = INIT PRNG(private key)
(cid:46) Seed rng.
φ(0) = INIT MLP(rng, L)
S = (cid:98)I · J/M (cid:99)
φ = φ(0)
for epoch = 0 . . . N − 1 do

(cid:46) Num. mini-batches per epoch.

P = SHUFFLE AND SPLIT PIXELS(x, rng, S)
for m = 0 . . . S − 1 do

(cid:96) = 1

M ·I·J

(cid:80)

(i,j)∈P[m]

||Φφ(pi,j) − x(pi,j)||2
2

φ = φ − µ∇φ(cid:96)

end for

end for
ˆφx = φ
Return t(x) applying Eq. 3 using ˆφx and layer K.

We provide pseudocode for the LINAC transform in Al-
gorithm 1 and a discussion of computational and memory
requirements in Appendix A.1.4. For each individual im-
age x, we estimate ˆφx, an approximate local minimiser of
L(φ, x), using a stochastic iterative minimisation procedure
with mini-batches of M pixels grouped into epochs, which
cover the entire image in random order, for a total of N
passes through all pixels.

Private Key. A random number generator is used for: (1)
generating the initial MLP parameters φ(0) and (2) for decid-
ing which random subsets of pixels make up mini-batches
in each epoch. This random number generator is seeded by
a 64-bit integer which we keep secret and denote as the pri-
vate key. Hence, for all inputs x we start each independent
optimisation from the same set of initial parameters φ(0),
and we use the same shufﬂing of pixels across epochs.

i

Pixel(i, j) 
Intensities
x(pi,j) = (r, g, b)

j

x: RGB Image In

Implicit Neural
Representation

pi,j = (i, j)

x(pi,j) = (r, g, b)

… …

……

…

 t(x): Activation Image Out
i

Representation 
of Pixel(i, j)
Intensities
…

j

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

Lossy Implicit Network Activation Coding (LINAC).
We consider the lossy encoding of each pixel (i, j) in im-
age x as the H-dimensional intermediate activations vector
of layer K of the MLP evaluated at that pixel position:
cx(i, j) = (hK−1
)(pi,j) with K < L. We build
the lossy implicit network activation coding transformation
of an image x by stacking together the encodings of all its
pixels in its 2D image grid, concatenating on the feature
dimension axis. The LINAC transformation t(x) of the
I × J × 3 image x is given by:

◦ · · · ◦ h0
ˆφx

ˆφx

t(x) =






cx(0, 0)
...
cx(I − 1, 0)

. . .
. . .
. . .

cx(0, J − 1)
...
cx(I − 1, J − 1)




 ,

(3)

and has dimensionality I × J × H, where H is the number
of outputs of the K-th layer of the MLP. By construction,
our input transformation preserves the spatial dimensions
of each image while increasing the feature dimensionality
(from 3, the image’s original number of colour channels,
to H); this means that standard network architectures used
for image classiﬁcation (e.g. convolutional models) can be
readily trained as the classiﬁer fθ.

All omitted implementation details are provided in Ap-
pendix A, and sensitivity analyses of LINAC to its hyper-
parameters are reported in Appendix C.

Threat Model. We are interested in hindering adversarial
attacks on a nominally-trained classiﬁer fθ(t(x)), which
operates on transformed inputs (i.e. on t(x) rather than on
x), using a private key of our choosing. Next, we describe
the threat model of interest by stating the conditions under
which the LINAC defence is meant to hinder adversarial
attacks on fθ, following AprilPyone & Kiya (2021a).

We assume attackers do not have access to the private key,
the integer seed of the random number generator used for
computing the LINAC transformation, but otherwise have
full algorithmic knowledge about our approach. Speciﬁcally,
we assume an attacker has complete information about the
classiﬁcation pipeline, including the architecture, training
dataset and weights of the defended classiﬁer. This includes
full knowledge of the LINAC algorithm, the implicit net-
work architecture, parameter initialisation scheme and all
the ﬁtting details, except for the private key.

4. Attacking the LINAC Defence

Setup. We are interested in evaluating the apparent ro-
bustness of a LINAC-defended classiﬁer, fˆθ, which has
been trained by ERM to classify transformed inputs from
the dataset D. Speciﬁcally, its parameters ˆθ minimise
Ex,y∼D [LCE(fθ(t(x)), y)] , where LCE is the cross-entropy
loss and t(x) is the LINAC transformation applied to image
x using the private key.

Input Perturbations. Classiﬁers defended by LINAC are
not adversarially trained (Madry et al., 2018) to increase
their robustness to speciﬁc Lp norm-bounded input pertur-
bations. Furthermore, the LINAC defence is inherently
agnostic about particular notions of maximum input pertur-
bations. Nevertheless, to provide results comparable with a
broad set of defences from the literature, we perform eval-
uations on standard Lp norm-bounded input perturbations
with: (1) a maximum perturbation radius of (cid:15) = 8/255 in
the L∞ norm, and (2) one of (cid:15) = 0.5 in the L2 norm.

Adapting Existing Attacks. Without access to the pri-
vate key an attacker cannot compute the LINAC transfor-
mation exactly. However, an attacker could acquire ac-
cess to model inferences by attempting to brute-force guess
the private key. Another option would be to train surro-
gate models with LINAC, but using keys chosen by the at-
tacker, in the hope that decision boundaries of these models
would be similar enough to mount effective transfer attacks.
More advanced attackers could modify LINAC itself to en-
able strong Backward Pass Differentiable Approximation
(BPDA) (Athalye et al., 2018) attacks. We evaluate the
success of these and other standard attacks in Section 5.

Designing Adaptive Attacks. Athalye et al. (2018) provide
an excellent set of guidelines for designing and perform-
ing successful adaptive attacks, while also standardising
results reporting and aggregation. Of particular interest for
defences based on input transformations are the BPDA and
Expectation-over-Transformation (EoT) attack strategies.
Subsequent work convincingly argues that adaptive attacks
are not meant to be general, and must be customised, or
“adapted”, to each defence in turn (Tramer et al., 2020).
While BPDA and EoT generate strong attacks on input
transformations, they both rely on being able to compute
the forward transformation or approximate it with samples.
Indeed, the authors mention that substitution of both the
forward and backward passes with approximations leads to
either completely ineffective, or much less effective attacks.

Parametric Bypass Approximation (PBA). Inspired by
the reparameterisation strategies of Athalye et al. (2018), we
propose a bespoke attack by making use of several pieces of
information available under our threat model: the parametric
form of the defended classiﬁer fθ(t(x)), its training dataset
D and loss function LCE, and its trained weights ˆθ.

A Parametric Bypass Approximation of an unknown nui-
sance transformation u : X → H is a surrogate parametric
function hψ : X → H, parameterised by a solution to the
following optimisation problem:

ψ∗ = arg min

ψ

E
x,y∼D

(cid:2) LCE(fˆθ(hψ(x)), y)

(cid:3) .

(4)

This formulation seeks a set of parameters ψ∗ which min-
imise the original classiﬁcation loss while keeping the de-

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

fended classiﬁer’s parameters frozen at ˆθ. Similar with
classiﬁer training, this optimisation problem can be solved
efﬁciently using Stochastic Gradient Descent (SGD).

A PBA adversarial attack can then proceed by approximat-
ing the defended classiﬁer fˆθ(u(·)) with those of the bypass
classiﬁer fˆθ(hψ∗ (·)) in both forward and backward passes
when computing adversarial examples, e.g. using Projected
Gradient Descent (PGD).

The main advantages of the PBA strategy are that no for-
ward passes through the nuisance transformation u(·) are
required, and that it admits efﬁcient computation of many
attacks to fˆθ, including gradient-based ones. In Section 5
we demonstrate the effectiveness of PBA beyond the LINAC
defence. We show that, even though the surrogate transfor-
mation is ﬁt on training data only, the defended classiﬁer
operating on samples passed through hψ∗ (bypassing u)
demonstrates nearly identical generalisation to the test set.
Furthermore, we also show that PBA has greater success at
ﬁnding adversarial examples for the LINAC defence com-
pared to other methods. Lastly, we use PBA to invalidate an
existing key-based defence proposed in the literature.

5. Results

5.1. Evaluation Methodology

Since LINAC makes no assumptions about adversarial per-
turbations, we are able to evaluate a single defended classi-
ﬁer model against all attack strategies considered, in contrast
to much adversarial training research (Madry et al., 2018).

To obtain a more comprehensive picture of apparent robust-
ness we start from the rigorous evaluation methodology used
by Gowal et al. (2019); Rebufﬁ et al. (2021). We perform
untargeted PGD attacks with 100 steps and 10 randomised
restarts, as well as multi-targeted (MT) PGD attacks using
200 steps and 20 restarts. Anticipating the danger of ob-
fuscated gradients skewing results, we also evaluate with
the Square approach of Andriushchenko et al. (2020), a
powerful gradient-free attack, with 10000 evaluations and
10 restarts. For precise comparisons with the broader liter-
ature we also report evaluations using the parameter-free
AutoAttack (AA) strategy of Croce & Hein (2020).

Following Athalye et al. (2018) we aggregate results across
attacks by only counting as accurate robust predictions those
test images for which the defended classiﬁer predicts the
correct class with and without adversarial perturbations,
computed using all methods above. We report this as Best
Known robust accuracy.

In instances where several surrogate models are used to
compute adversarial perturbations, also known as transfer
attacks, we report Best Adversary results aggregated for
each individual attack, which is deﬁned as robust accuracy

Figure 2. Results of direct attack on private key. A histogram of
accuracies of the same defended classiﬁer with inputs transformed
using either the correct key or 100000 randomly chosen keys. An
appropriate surrogate transformation is not found, invalidating
attack vectors which rely on access to the outputs of the defended
model on attacker chosen inputs.

against all source models considered.

We aggregate evaluations across these two dimensions (at-
tacks & surrogate models) by providing a single robust ac-
curacy number against all attacks computed using all source
models for each standard convention of maximum perturba-
tion and norm, enabling easy comparisons with results in
the literature.

5.2. Attacks with Surrogate Transformations & Models

A majority of adversarial attack strategies critically depend
on approximating the outputs of the defended classiﬁer for
inputs chosen by the attacker. The private key is kept secret
in our threat model, which means that an attacker can neither
compute the precise input transformation used to train the
defended classiﬁer, nor its outputs on novel data. Hence,
an attacker must ﬁnd appropriate surrogate transformations,
or surrogate classiﬁer models, in order to perform effective
adversarial attacks. We investigate both strategies below.

Firstly, we empirically check that the outputs of the de-
fended classiﬁer cannot be usefully approximated without
knowledge of the private key. It is reasonable to hypothesise
that transformations with different keys may lead to simi-
lar functional representations of the input signal. We start
investigating this hypothesis by simply computing the accu-
racy of the defended model on clean input data transformed
with LINAC, but using keys chosen by the attacker, also
known as a brute-force key attack, which is valid under our
threat model. As reported in Figure 2, the accuracy of our
LINAC defended classiﬁer on test inputs transformed with
the correct private key is over 93%. In an attempt to ﬁnd a
surrogate transformation, 100000 keys are picked uniformly
at random. For each key, we independently evaluated the
accuracy of the classiﬁer using a batch of 100 test examples,
and we report the resulting accuracy estimates for all keys
with a histogram plot. The mean accuracy with random key
guesses is around 30%, with a top accuracy of just 57% (see
Table 4 in Appendix B.1 for a breakdown). Hence, using
LINAC with incorrect keys leads to poor approximations
of classiﬁer outputs on correctly transformed data. This

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

Norm

Attack

Nominal
Source

AA
MT
PGD
Square
Best Known

AA
MT
PGD
Square
Best Known

92.77
84.57
85.99
85.12
81.91
90.84
87.55
88.61
88.58
86.06

L∞

L2

Transfer Attack Source Models

Adversarial
Training
(L∞)
80.42
72.96
60.97
65.69
54.97
86.75
85.34
82.39
84.50
79.42

Adversarial
Training
(L2)
70.29
56.08
44.06
52.66
39.20
80.83
84.81
74.19
79.31
71.92

Defended
Surrogates
(Attacker Keys)
84.00
85.70
87.32
75.91
75.64
88.27
87.31
88.36
84.08
83.48

Reconstruction-
Based Surrogates
(BPDA)
59.40
55.37
56.00
69.14
51.17
74.59
74.98
75.00
83.26
71.89

Best Adversary

All Source
Models

41.18
47.91
41.22
49.76
37.04
71.32
73.83
70.90
77.68
68.41

Table 1. CIFAR-10 test set robust accuracy (%) of a single LINAC defended classiﬁer according to a suite of L∞ and L2 transfer attacks,
valid under our threat model, using various source classiﬁers to generate adversarial perturbations.

suggest that the learned decision boundaries of the defended
classiﬁer are not invariant to the private key used by LINAC.

Figure 3. Plots of CIFAR-10 test-set robust accuracy estimates
(Best Known) vs. number of attacker-trained surrogate models. We
also plot the clean accuracy of 93.08% for reference.

While we could not ﬁnd a useful surrogate transformation
by random guessing, it is still possible that transformations
with different keys preserve largely the same input informa-
tion. So, the second option of an attacker is to check whether
decision boundaries of models defended with LINAC and
different keys are in fact very similar, which would enable
powerful transfer attacks from such surrogate models. To
this end, 10 independent models defended with LINAC were
trained from scratch, each using a different key chosen by
the attacker. We used the most promising 10 keys from
the brute-force key attack for this purpose. In Figure 3 we
report Best Known robust accuracy plotted against the num-
ber of surrogate models used in these joint attacks, and we
aggregate results over all 10 attacking models in the fourth
column of Table 1. However, this attack vector has limited
success. Under transfer attacks with such surrogate models,
the robust accuracy of our defended classiﬁer appears to
be high. While PGD and MT may fail due to vanishing
or exploding gradients (Athalye et al., 2018), Square is a
gradient-free attack, and does not suffer from such issues.

Robust accuracy estimates according to Square are higher
than 83% against any individual surrogate model, irrespec-
tive of perturbation norm. A complete breakdown of results
is given in Table 5 of Appendix B.1. Attacking with all 10
surrogate models together, robust accuracy to Square is still
higher that 75%, and the estimate is not improved by further
aggregating over attacks. This evidence further support the
hypothesis that decision boundaries of classiﬁers defended
with LINAC depend on their respective keys, and may differ
enough across keys to hinder transfer attacks with surro-
gates. Investing an order of magnitude more computation
into such attacks leads to modest reductions in apparent
robustness.

Lastly, an attacker may strive to employ BPDA, one of
the most effective and general strategies against defences
using nuisance transformations. BPDA attacks require:
(1) the ability to compute the exact forward transforma-
tion and (2) ﬁnding a usefully differentiable approximation
to the said transformation for use in the backwards pass of
gradient-based attacks. In many cases this would be enough
to allow the attacker to compute adversarial examples, per-
haps at a somewhat higher computational cost (Athalye
et al., 2018; Tramer et al., 2020).

Our LINAC defence presents further challenges by design.
Exact forward computations (model inferences) require the
private key. An attacker cannot exactly compute the input
transformation even for training set images, e.g. in order for
some differentiable parametric approximation to be learned
in a supervised fashion. Furthermore, surrogate models
defended using LINAC and attacker chosen keys do not
appear to be usefully differentiable, as suggested by results
in Table 1. Nevertheless, an attacker could still hope that our
defence “ﬁlters out” information in a largely key-agnostic
manner, and that the choice of implicit network representa-
tion layer is not essential. Hence, they have the option of
modifying LINAC to output activations of the last, rather
than the middle layer of the implicit network. This amounts

)

%

(

y
c
a
r
u
c
c
a

t
s
u
b
o
r

t
e
s
-
t
s
e
t

90

80

70

60

50

40

30

20

None

1

2

Surrogates - L2
Surrogates - L

BPDA - L2
BPDA - L

4

3

5
number of surrogate models

6

7

8

9

10

 
 
 
Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

Norm

L∞

L2

Attack
AA
MT
PGD
Square
Best Known

AA
MT
PGD
Square
Best Known

All Source Models
Transfer
41.18
47.91
41.22
49.76
37.04
71.32
73.83
70.90
77.68
68.41

Adaptive Attacks
PBA
BPDA
68.34
59.40
46.75
55.37
44.05
56.00
48.59
69.14
35.32
51.17
73.10
74.59
67.85
74.98
66.93
75.00
74.70
83.26
61.23
71.89

Table 2. CIFAR-10 test set robust accuracy (%) of a single LINAC
defended classiﬁer w.r.t. a suite of L∞ and L2 attacks, valid under
our threat model, using different strategies such as transfer and
adaptive attacks. Our novel PBA adaptive attacks are overall more
effective that both transfer and BPDA attack strategies.

to reducing LINAC to an approximate reconstruction of
the original signal. While such surrogate models with at-
tacker chosen keys would still have to be trained for the
purpose, they would be vulnerable to strong BPDA attacks,
which may transfer well to our defended classiﬁer. Appar-
ent robustness estimates according to such transfer BPDA
attacks are plotted in Figure 3 as a function of the number
of surrogate models used jointly in the attack. In the ﬁfth
column of Table 1 we provide aggregate apparent robust
accuracies using 10 such surrogates, showing that transfer
BPDA attacks are more successful than previous attempts;
any such reconstruction-based surrogate model can be used
to reveal that the robust accuracy of our defended classiﬁer
cannot be higher than 65%, particularly with standard L∞
multi-targeted (MT) attacks (see Table 6 in Appendix B.1
for a detailed breakdown of results). Interestingly, when 10
surrogate models are used together, L∞ robust accuracy esti-
mates drop to 51%. The reduction is less severe in standard
L2 attacks, where accuracy against all surrogates appears
to be still over 71%. These results conﬁrm that the BPDA
strategy is a valuable tool for investigating the robustness
of a wide range of defences, even when its assumptions are
not fully met.

5.3. Transfer Attacks with Nominal and Adversarially

Trained Source Models

Since our defended classiﬁer is not adversarially trained,
one could assume that its decision boundaries may be simi-
lar to those of a nominal, undefended classiﬁer. We show
in the ﬁrst column of Table 1 that transfer attacks with a
nominally trained source model have limited success, es-
pecially considering that such undefended classiﬁers have
below chance robust accuracies according to the very same
evaluations.

Another possibility is that that our defended model may
be susceptible to the promising attack directions to which
adversarially trained robust classiﬁers are vulnerable. We

report in the second and third columns of Table 1 that this is
indeed the case to some extent. Of all adversaries considered
thus far, a robust model adversarially trained to tolerate
perturbations of up to size (cid:15) = 0.5 in L2 norm leads to the
most effective transfer attacks. This holds to a lesser degree
for an adversarially trained model with perturbations of size
(cid:15) = 8/255 in L∞ norm. Despite the success of evaluations
using the former source model, no one attack method comes
close to the effectiveness of the joint strategy, reported as
Best Known robust accuracy.

Furthermore, it is important to note that ensemble transfer
attacks are much stronger than those computed with any
given source model. Aggregated over four attack types
and 23 different source models, the robust accuracy of our
LINAC defended classiﬁer is revealed to be at most half of
what initial results suggested according to aggregate L∞
evaluations; this does not appear to be the case for L2 at-
tacks, however, which continue to be substantially hindered
by LINAC. Robust accuracy could still be above 68% ac-
cording to the latter attack type, even in aggregate. In order
to better characterise the implications of LINAC we make
use of novel adaptive attacks in the following subsection.

5.4. PBA Attacks Against LINAC

Thus far we have shown that strong transfer attacks can be
performed by using an ensemble of diverse source models
to compute adversarial perturbations over many repeated
trials. While ultimately more reliable, this is a cumbersome
evaluation protocol, requiring two order of magnitude more
computation than standard evaluations.

In Section 4 we have introduced PBA, an attack strategy pur-
posefully designed to be effective against input transforma-
tions (or network modules) which deny both inference and
gradient computations, despite classiﬁer parameters, train-
ing loss and dataset being available to the attacker. Follow-
ing this novel strategy we successfully trained a parametric
bypass approximation (PBA) of the LINAC transform and
its associated defended classiﬁer. Intriguingly, the decision
boundaries of the resulting bypass classiﬁer generalise very
well. Accuracy on clean test data is 95.35%. Furthermore,
the bypass classiﬁer can be readily shown to have 0% robust
accuracy using PGD attacks. This indicates that any appar-
ent robustness in evaluations can be largely attributed to
the LINAC transform successfully hindering attacks, since
the decision boundaries of our defended classiﬁer are sus-
ceptible to adversarial perturbations, and hence cannot be
considered to add any inherent robustness by themselves.

In Table 2 we show that standard attacks using the trained
PBA mapping against our LINAC defended classiﬁer are
even more effective than BPDA attacks using 10 source
models. Interestingly, PBA almost uniformly leads to more
effective attacks, regardless of strategy. PGD attacks us-

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

(A)

(B)

(C)

(D)

(E)

Figure 4. Decision boundaries of ﬁve different classiﬁers (rows) around the same ﬁve randomly chosen test examples (columns), plotted
along their respective adversarial directions according to the AT (L2) model (horizontal), and the same random direction (vertical): (A) An
undefended, nominally trained CIFAR-10 classiﬁer; (B) LINAC defended classiﬁers using a random key; (C) LINAC defended classiﬁers
using the private key; this is the model we evaluated throughout the submission; (D) The by-pass classiﬁer resulting from our novel PBA
attack on model (C); (E) An adversarially trained classiﬁer, AT (L2) in the main text, used to generate transfer attacks. We observe that
boundaries of nominal model (A) are different from those of LINAC (B) & (C); LINAC decision boundaries seem less smooth compared
to other models; as suspected boundaries appear different across keys (B) vs. (C), which corroborates our observations of robustness to
transfer attacks with surrogates. The adversarially trained model (E) is robust to the vertical dimension (random noise); LINAC models
(B) & (C) also appear less sensitive to random noise compared the nominal model (A). PBA by-pass classiﬁer (D) boundaries are much
smoother and different from the true boundaries of the attacked model (C), which may explain why LINAC withstands the novel attack in
many cases. Notice that PBA approximated boundaries (D) can be both closer and farther away from test examples compared to the true
model’s (C), which makes it less clear how useful such approximations are for future attacks on LINAC.

ing PBA give the most accurate picture of robustness of all
strategies, suggesting that the matter of obfuscated gradi-
ents is largely mitigated by our novel strategy. Aggregated
over different attack types, PBA is the most effective and
efﬁcient evaluation strategy which does not make use of
the private key, and hence is valid under the adopted threat
model. Based on these evaluations alone, one may conclude
that robust accuracy was over 35% under attacks of size at
most (cid:15) = 8/255 in L∞ norm, and over 61% for attacks of
size (cid:15) = 0.5 in L2 norm. The apparent robustness differ-
ence between L∞ and L2 attacks persists, suggesting that
LINAC primarily hinders the latter type of attacks.

5.5. Towards Explaining the Apparent Robustness

Decision boundary inspection. We plotted decision bound-
aries of several classiﬁers around ﬁve randomly chosen test
examples in Figure 4. All boundary plots are centred on
test examples (columns), use appropriate adversarial direc-
tions as the horizontal dimension, and a random direction as
the vertical. As expected, we observe differences between
LINAC defended classiﬁers which use different keys. Fur-
thermore, we found that LINAC boundaries can be more
“complicated” relative to those of other models, which may
explain why PBA attacks are not completely effective.

RGB Reconstruction vs. Lossy Encodings. Setting the
representation layer index K = L renders our LINAC trans-

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

Full
PCA

Block
PCA

JPEG
(23)

JPEG
(10)

Block
Pixel
Shufﬂe

LINAC
(Ours)

96.10

96.39

88.15

81.17

96.98

93.08

Standard Standard BPDA BPDA
32.58
–
27.48
6.34

11.90
–
17.49
5.36

0.00
0.00
0.00
0.00

0.00
0.00
0.00
0.00

0.00

0.00

0.61

2.26

0.00
0.03
0.41
12.85

0.06
0.00
0.17
11.66

62.95
–
60.37
28.33

62.98
–
60.38
21.92

0.02

0.00

14.94

14.56

PBA
0.18
0.00
0.00
0.00

0.00

0.02
0.00
0.02
6.13

0.00

PBA
68.34
46.75
44.05
48.59

35.32

73.10
67.85
66.93
74.70

61.23

Clean
Accuracy:
Norm Attack

L∞

L2

AA
MT
PGD
Square
Best
Known

AA
MT
PGD
Square
Best
Known

Table 3. CIFAR-10 test set robust accuracy (%) of several classi-
ﬁers defended using related input transformations according to
evaluations using adversarial perturbations bounded in L∞ and
L2 norms. Reporting results of strongest known attack strategy for
each method, valid according to its own threat model.

form into an approximate RGB input reconstruction, since
L is the index of the implicit network output layer. We
conﬁrmed that setting K = L = 5 and N = 100 epochs
offers no robustness, since the resulting reconstructions are
precise and BPDA attacks are successful. Clean accuracy
was 96.91%, virtually matching that of a nominally trained
classiﬁer. Hence, any apparent robustness must be due to the
number of INR ﬁtting epochs N , and/or the choice of repre-
sentation layer index K. Intuitively, both hyper-parameters
control how “lossy” our transformation is.

Naturally, we were interested in reducing the computational
overhead of LINAC. Aiming to match the clean accuracy
of state-of-the-art adversarially trained robust classiﬁers,
speciﬁcally 93% (Rebufﬁ et al., 2021), we empirically chose
N = 10 epochs as a trade-off between speed and clean accu-
racy. The activation coding layer index K = 2 out of L = 5
hidden layers was chosen according to the same principle,
as the lowest level representation which did not reduce clean
accuracy below the target threshold. We further characterise
and illustrate our LINAC transform in Appendix D.

Performance Considerations. LINAC is as expensive as
inference with a WideResNet-70-16 (Zagoruyko & Ko-
modakis, 2016) on CIFAR-10 images. This cost is dom-
inated by the ﬁtting of INRs. It could be reduced with an
adaptive form of “early stopping” based on loss values, or
by leveraging advances in INR research (e.g. Sitzmann et al.
(2020)). We leave these investigations, and scaling LINAC
to larger images, for future work.

Sensitivity Analyses. The apparent robustness of LINAC
defended classiﬁers is largely insensitive to the number of
hidden layers L ≥ 3 of the implicit MLP, as well as the
number of features F ≥ 3 in its positional input encoding,
hence we relegated the sensitivity analyses to Appendix C.

5.6. PBA Beyond LINAC and Methodology Validation

We show in the one-but-last column of Table 3 that PBA
successfully and completely invalidates the Block Pixel
Shufﬂe approach of AprilPyone & Kiya (2021a), despite
its good reported robustness against all attacks. We further
investigate using adversarially trained source models, see
full results in Table 7 of Appendix B.1. In summary, our
analysis conﬁrms that the apparent robust accuracy of Block
Pixel Shufﬂe according to valid attacks bounded in L2 norm
remains high at 69%. Hence, PBA is indeed the only known
valid attack on this defence which is completely successful.

Finally, we validate our evaluation methodology by test-
ing its effectiveness against similar defences. We perform
the same evaluations on the Principal Component Analysis
(PCA) based defence of Shaham et al. (2018), and the JPEG-
based defences of Das et al. (2017; 2018); Guo et al. (2018).
In Table 3 we report the Best Known robust accuracies of
these defences according to our evaluation methodology,
which are directly comparable with our reported LINAC
results. We observe that LINAC successfully hinders much
stronger attacks than these alternative strategies.

6. Conclusions

In this work we introduce LINAC, a novel key-based de-
fence using implicit neural representations, and demonstrate
its effectiveness for hindering standard adversarial attacks
on CIFAR-10 classiﬁers. We systematically attempt to cir-
cumvent our defence by adapting a host of widely used
attacks from the literature, including transfer and adaptive
attacks, but LINAC maintains strong apparent robustness.
Consequently, we challenge LINAC by introducing a novel
adaptive attack strategy (PBA) which is indeed more suc-
cessful at discovering adversarial examples. We also show
that PBA can be used to completely invalidate an existing
key-based defence. These are some of the latest attempts to
leverage computational hardness for adversarial robustness,
and successful PBA attacks on existing methods enable
further progress.

References

Andriushchenko, M., Croce, F., Flammarion, N., and Hein,
M. Square attack: a query-efﬁcient black-box adversarial
attack via random search. In European Conference on
Computer Vision, pp. 484–501. Springer, 2020.

AprilPyone, M. and Kiya, H. An extension of encryption-
inspired adversarial defense with secret keys against ad-
versarial examples. In 2020 Asia-Paciﬁc Signal and In-
formation Processing Association Annual Summit and
Conference (APSIPA ASC), pp. 1369–1374. IEEE, 2020.

AprilPyone, M. and Kiya, H. Block-wise image transfor-

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

mation with secret key for adversarially robust defense.
IEEE Transactions on Information Forensics and Secu-
rity, 16:2709–2723, 2021a.

AprilPyone, M. and Kiya, H. Transfer learning-based
arXiv preprint

model protection with secret key.
arXiv:2103.03525, 2021b.

Athalye, A., Carlini, N., and Wagner, D. A. Obfuscated
gradients give a false sense of security: Circumventing
defenses to adversarial examples. In ICML, 2018.

Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,
C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J.,
Wanderman-Milne, S., and Zhang, Q. JAX: composable
transformations of Python+NumPy programs. 2018. URL
http://github.com/google/jax.

Brendel, W., Rauber, J., and Bethge, M. Decision-based
adversarial attacks: Reliable attacks against black-box
machine learning models. In International Conference
on Learning Representations, 2018.

Carlini, N. and Wagner, D. Adversarial examples are not
easily detected: Bypassing ten detection methods.
In
Proceedings of the 10th ACM workshop on artiﬁcial in-
telligence and security, pp. 3–14, 2017a.

Carlini, N. and Wagner, D. Towards evaluating the robust-
In 2017 ieee symposium on

ness of neural networks.
security and privacy (sp), pp. 39–57. IEEE, 2017b.

Cohen, J., Rosenfeld, E., and Kolter, Z. Certiﬁed adver-
sarial robustness via randomized smoothing. In Interna-
tional Conference on Machine Learning, pp. 1310–1320.
PMLR, 2019.

Croce, F. and Hein, M. Reliable evaluation of adversarial
robustness with an ensemble of diverse parameter-free
attacks. In International conference on machine learning,
pp. 2206–2216. PMLR, 2020.

Das, N., Shanbhogue, M., Chen, S.-T., Hohman, F., Chen,
L., Kounavis, M. E., and Chau, D. H. Keeping the
bad guys out: Protecting and vaccinating deep learning
with jpeg compression. arXiv preprint arXiv:1705.02900,
2017.

Das, N., Shanbhogue, M., Chen, S.-T., Hohman, F., Li,
S., Chen, L., Kounavis, M. E., and Chau, D. H. Shield:
Fast, practical defense and vaccination for deep learn-
ing using jpeg compression. In Proceedings of the 24th
ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 196–204, 2018.

Dhillon, G. S., Azizzadenesheli, K., Lipton, Z. C., Bern-
stein, J. D., Kossaiﬁ, J., Khanna, A., and Anandkumar,
A. Stochastic activation pruning for robust adversarial

defense. In International Conference on Learning Repre-
sentations, 2018.

Dziugaite, G. K., Ghahramani, Z., and Roy, D. M. A study
of the effect of jpg compression on adversarial images.
arXiv preprint arXiv:1608.00853, 2016.

Engstrom, L.,

Ilyas, A., Santurkar, S., Tsipras, D.,
Tran, B., and Madry, A. Adversarial robustness as
arXiv preprint
a prior for learned representations.
arXiv:1906.00945, 2019.

Frostig, R., Johnson, M. J., and Leary, C. Compiling ma-
chine learning programs via high-level tracing. Systems
for Machine Learning, 2018.

Garg, S., Jha, S., Mahloujifar, S., and Mohammad, M. Ad-
versarially robust learning could leverage computational
hardness. In Algorithmic Learning Theory, pp. 364–385.
PMLR, 2020.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain-
ing and harnessing adversarial examples. arXiv preprint
arXiv:1412.6572, 2014.

Gowal, S., Uesato, J., Qin, C., Huang, P., Mann, T. A., and
Kohli, P. An alternative surrogate loss for pgd-based
adversarial testing. CoRR, abs/1910.09338, 2019. URL
http://arxiv.org/abs/1910.09338.

Gowal, S., Qin, C., Uesato, J., Mann, T., and Kohli, P.
Uncovering the limits of adversarial training against
norm-bounded adversarial examples. arXiv preprint
arXiv:2010.03593, 2020.

Guo, C., Rana, M., Cisse, M., and van der Maaten, L. Coun-
tering adversarial images using input transformations. In
International Conference on Learning Representations,
2018. URL https://openreview.net/forum?
id=SyJ7ClWCb.

Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers,
R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,
Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van
Kerkwijk, M. H., Brett, M., Haldane, A., del R´ıo, J. F.,
Wiebe, M., Peterson, P., G´erard-Marchant, P., Sheppard,
K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C.,
and Oliphant, T. E. Array programming with NumPy.
Nature, 585(7825):357–362, September 2020. doi: 10.
1038/s41586-020-2649-2. URL https://doi.org/
10.1038/s41586-020-2649-2.

He, W., Wei, J., Chen, X., Carlini, N., and Song, D. Adver-
sarial example defenses: ensembles of weak defenses are
not strong. In Proceedings of the 11th USENIX Confer-
ence on Offensive Technologies, pp. 15–15, 2017.

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

Hein, M. and Andriushchenko, M. Formal guarantees on the
robustness of a classiﬁer against adversarial manipulation.
In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pp. 2263–2273,
2017.

Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran,
B., and Madry, A. Adversarial examples are not
bugs, they are features.
In Wallach, H., Larochelle,
H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and
Garnett, R. (eds.), Advances in Neural Information Pro-
cessing Systems, volume 32. Curran Associates,
URL https://proceedings.
Inc.,
neurips.cc/paper/2019/file/
e2c420d928d4bf8ce0ff2ec19b371514-Paper.
pdf.

2019.

Jacobsen, J.-H., Behrmann, J., Zemel, R., and Bethge, M.
Excessive invariance causes adversarial vulnerability. In
International Conference on Learning Representations,
2018.

Kingma, D. P. and Ba, J. Adam: A method for stochastic

optimization. In ICLR (Poster), 2015.

Kurakin, A., Goodfellow, I., Bengio, S., Dong, Y., Liao,
F., Liang, M., Pang, T., Zhu, J., Hu, X., Xie, C., et al.
Adversarial attacks and defences competition. In The
NIPS’17 Competition: Building Intelligent Systems, pp.
195–231. Springer, 2018.

Liao, F., Liang, M., Dong, Y., Pang, T., Hu, X., and Zhu,
J. Defense against adversarial attacks using high-level
representation guided denoiser, 2018.

Loshchilov, I. and Hutter, F. SGDR: stochastic gradient
descent with restarts. CoRR, abs/1608.03983, 2016. URL
http://arxiv.org/abs/1608.03983.

Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
Vladu, A. Towards deep learning models resistant
In International Conference
to adversarial attacks.
on Learning Representations, 2018. URL https://
openreview.net/forum?id=rJzIBfZAb.

MaungMaung, A. and Kiya, H. A protection method of
trained cnn model with secret key from unauthorized
access. arXiv preprint arXiv:2105.14756, 2021.

Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T.,
Ramamoorthi, R., and Ng, R. Nerf: Representing scenes
as neural radiance ﬁelds for view synthesis. In European
conference on computer vision (ECCV), pp. 405–421.
Springer, 2020.

Niu, Z., Chen, Z., Li, L., Yang, Y., Li, B., and Yi, J. On the
limitations of denoising strategies as adversarial defenses.
CoRR, abs/2012.09384, 2020. URL https://arxiv.
org/abs/2012.09384.

Pang, T., Yang, X., Dong, Y., Su, H., and Zhu, J. Bag of
tricks for adversarial training. In International Confer-
ence on Learning Representations, 2020.

Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik,
Z. B., and Swami, A. Practical black-box attacks against
machine learning. In Proceedings of the 2017 ACM on
Asia conference on computer and communications secu-
rity, pp. 506–519, 2017.

Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed
defenses against adversarial examples. In International
Conference on Learning Representations, 2018.

Ramachandran, P., Zoph, B., and Le, Q. V. Searching for
activation functions. CoRR, abs/1710.05941, 2017. URL
http://arxiv.org/abs/1710.05941.

Rebufﬁ, S.-A., Gowal, S., Calian, D. A., Stimberg, F., Wiles,
O., and Mann, T. Fixing data augmentation to improve
adversarial robustness. arXiv preprint arXiv:2103.01946,
2021.

Schmidt, L., Santurkar, S., Tsipras, D., Talwar, K., and
Madry, A. Adversarially robust generalization requires
more data. In NeurIPS, 2018.

Schott, L., Rauber, J., Bethge, M., and Brendel, W. Towards
the ﬁrst adversarially robust neural network model on
mnist. In Seventh International Conference on Learning
Representations (ICLR 2019), pp. 1–16, 2019.

Shaham, U., Garritano, J., Yamada, Y., Weinberger, E.,
Cloninger, A., Cheng, X., Stanton, K., and Kluger, Y. De-
fending against adversarial images using basis functions
transformations. arXiv preprint arXiv:1803.10840, 2018.

Sitzmann, V. Awesome Implicit Representations - A cu-
rated list of resources on implicit neural representations.
2020. URL https://github.com/vsitzmann/
awesome-implicit-representations.

Sitzmann, V., Martel, J., Bergman, A., Lindell, D., and Wet-
zstein, G. Implicit neural representations with periodic
activation functions. Advances in Neural Information
Processing Systems, 33, 2020.

Skorokhodov, I., Ignatyev, S., and Elhoseiny, M. Adversarial
generation of continuous images. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 10753–10764, 2021.

Song, S., Chen, Y., Cheung, N.-M., and Kuo, C.-C. J. De-
fense against adversarial attacks with saak transform.
arXiv preprint arXiv:1808.01785, 2018a.

Song, Y., Kim, T., Nowozin, S., Ermon, S., and Kushman, N.
Pixeldefend: Leveraging generative models to understand

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

and defend against adversarial examples. In International
Conference on Learning Representations, 2018b.

Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Er-
han, D., Goodfellow, I., and Fergus, R.
Intriguing
properties of neural networks. In International Confer-
ence on Learning Representations, 2014. URL http:
//arxiv.org/abs/1312.6199.

Tieleman, T. and Hinton, G. Lecture 6.5-rmsprop, coursera:
Neural networks for machine learning. University of
Toronto, Technical Report, 2012.

Tramer, F., Carlini, N., Brendel, W., and Madry, A. On adap-
tive attacks to adversarial example defenses. Advances in
Neural Information Processing Systems, 33, 2020.

Wong, E. and Kolter, J. Z. Provable defenses against adver-
sarial examples via the convex outer adversarial polytope.
In ICML, 2018.

Yang, Y., Zhang, G., Katabi, D., and Xu, Z. Me-net: To-
wards effective adversarial robustness with matrix estima-
tion. In International Conference on Machine Learning,
pp. 7025–7034. PMLR, 2019.

Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo, Y.
Cutmix: Regularization strategy to train strong classiﬁers
with localizable features. CoRR, abs/1905.04899, 2019.
URL http://arxiv.org/abs/1905.04899.

Zagoruyko, S. and Komodakis, N. Wide residual networks.
In British Machine Vision Conference 2016. British Ma-
chine Vision Association, 2016.

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

A. LINAC Implementation Details

A.1. Implicit Neural Representations

A.1.1. RANDOM NUMBER GENERATION FOR INRS

Our LINAC defence is fully deterministic by design. We used a random 64-bit signed integer as the private key, which
seeded the state of the pseudo-random number generator in JAX (Frostig et al., 2018; Bradbury et al., 2018). The precise
value of the private key used to train the defended model evaluated throughout this work was: −2314326399425823309. It
was itself selected randomly, by initialising the random number generator of the NumPy library (Harris et al., 2020) with
seed 42 and using the ﬁrst int64 integer.

A.1.2. INPUT AND OUTPUT ENCODINGS

Following Mildenhall et al. (2020) we use a positional encoding of pixel coordinates to a higher dimensional space to better
capture higher-frequency information. Each pixel coordinate d is normalised to [−1, 1] and transformed as follows:

γ(d) = [sin(20πd), cos(20πd), sin(21πd), cos(21πd), . . . , sin(2F −1πd), cos(2F −1πd)]

(5)

We used F = 5 frequencies in all our experiments and a L = 5 hidden layer MLP with H = 256 units per layer and ReLU
non-linearities. Activations in the middle hidden layer were used for computing the LINAC transform, hence K = 2.

As per standard practice for CIFAR-10 classiﬁcation, pixel colour intensities were scaled to have 0 mean across the training
dataset and each colour channel separately. Intensities were then standardised to 1 standard deviation across the training
dataset, independently across channels.

A.1.3. FITTING

Fitting the parameters of the implicit neural network was done using Adam (Kingma & Ba, 2015), with default parameters
and a learning rate µ = 0.001. We used mini-batches with M = 32 random pixels and trained for N = 10 epochs. An
epoch constitutes a pass through the entire set of pixels in the input image with dimensions I × J × C = 32 × 32 × 3 in
random order. The total number of optimisation steps performed was 320. A cosine learning rate decay schedule was used
for better convergence, with the minimum value of the multiplier α = 0.0001 (Loshchilov & Hutter, 2016).

A.1.4. COMPUTATIONAL AND MEMORY REQUIREMENTS

The LINAC transform’s computational complexity scales with the number of pixels (I · J) of the input image and the number
of epochs through the pixels (N ). It takes I · J · N backward passes through the implicit network Φ to ﬁt its parameters φ.
LINAC’s memory complexity is dominated by the number of parameters of the INR (|φ|). Empirically, the LINAC transform
is itself as expensive as inference with a WideResNet-70-16 model (Zagoruyko & Komodakis, 2016) on CIFAR-10 images.

A.2. Defended Classiﬁers

Since the proposed input transformation preserves spatial structure, we perform image classiﬁcation using transformed
inputs in an identical manner as with RGB colour images, except for the higher number of channels of transformed inputs.
Hence, we employ a standard classiﬁcation pipeline following (Zagoruyko & Komodakis, 2016), using a WideResNet-
70-16 classiﬁer. We reiterate that our proposed transformation changes the number of input channels, but not the spatial
dimensions. Hence, small differences between our models and other WideResNet-70-16 results reported in the literature
could conceivably appear only due to different numbers of input channels. However, practically this leads to less than a
0.2% increase in the total number of model parameters, limited to the ﬁrst convolutional layer, which uses ﬁlters with 256
channels instead of 3.

We used the Swish activation function proposed by Ramachandran et al. (2017) for all the classiﬁers. Training was performed
with Nesterov Momentum SGD (Tieleman & Hinton, 2012) m = 0.9, using mini-batches of size 1024, for a total of 1000
epochs, or 48880 parameter updates. The initial learning rate was µ = 0.4, reduced by a factor of 10 four times, at epochs:
650, 800, 900 and 950. We performed a hyper-parameter sweep over the weight-decay scale with the following grid:
{0., 0.0001, 0.0005, 0.0010}. We maintain an exponential moving average of classiﬁer parameters (with a decay rate of
r = 0.995); we report accuracies using the ﬁnal average of classiﬁer parameters.

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

A.2.1. PERFORMANCE CONSIDERATIONS

We use the CutMix data augmentation strategy of Yun et al. (2019) directly on RGB images from the training set of CIFAR-
10, prior to transforming them with LINAC. This has an impact on computational considerations, since pre-computing the
transformed dataset ofﬂine in order to save training time becomes more challenging. For ease of prototyping we chose to
implement LINAC as a preprocessing layer, which could have an impact on training time if used naively, but not if the
transformation is applied asynchronously on the buffer of data feeding the device used for model training. We also found
empirically that the proposed transformation renders itself to very effective parallelisation using modern SIMD devices,
despite the fact that there is no parameter sharing between implicit models of different inputs; this is likely due to the ability
of modern libraries such as JAX (Frostig et al., 2018) to vectorise operations across tensors holding parameters for many
distinct neural networks.

It is important to note that inference and training costs of defended classiﬁers are roughly double those of the nominal
classiﬁer. Hence, the LINAC transform has comparable cost to inference with a WideResNet-70-16 model.

B. Evaluation Details

B.1. Attacks with Surrogate Models

We provide a breakdown of evaluations using surrogate models initially reported in Section 5. We report the best 10 keys
from the brute-force attack on the private key in Table 4. These keys were also used to train surrogate models defended with
LINAC for use in transfer attacks, see Table 5 for complete results. Reconstruction-based surrogate models defended with
modiﬁed LINAC, and using the same 10 best-guess attacker keys, were used to perform BPDA transfer attacks, reported in
Table 6.

Position Clean Test Accuracy (%)

1
2
3
4
5
6
7
8
9
10

57.00
55.00
55.00
55.00
55.00
55.00
54.00
54.00
54.00
54.00

Attacker Key
1383227977468296715
-3328443931658504707
-127094507362684985
-7808219206569127925
-8772667224621836765
-70640792831170485
8263151932495004089
-4594861196100637268
-6520968232434877967
-8722766234183220599

Table 4. Top 10 keys in brute-force key attack, also used to train surrogate models.

LINAC Defence
Norm Attack Name

L∞

L2

AA
MT
PGD
Square
Best Known

AA
MT
PGD
Square
Best Known

Defended Surrogate Source Models (Attacker Keys)

key1
93.05
89.70
88.05
83.37
82.22
91.20
90.29
88.99
87.84
86.51

key2
92.99
89.48
88.19
83.34
82.10
91.15
90.47
89.00
87.83
86.41

key3
93.06
89.63
88.20
83.60
82.43
91.24
90.30
88.92
87.77
86.59

key4
93.00
89.54
88.23
83.43
82.26
91.17
90.50
88.93
88.15
86.42

key5
93.01
89.59
88.24
83.06
81.93
91.18
90.54
88.93
87.88
86.33

key6
93.05
89.55
88.17
83.38
82.33
91.18
90.37
89.08
88.11
86.62

key7
93.00
89.36
88.06
83.41
82.24
91.20
90.20
88.97
88.19
86.50

key8
93.00
89.58
88.20
83.30
82.10
91.20
90.32
88.98
87.94
86.58

key9
93.15
89.69
88.29
83.36
82.32
91.21
90.56
88.95
87.85
86.58

key10
93.21
89.47
88.10
83.21
82.17
91.16
90.25
89.03
88.12
86.53

Best Adversary
(against all models)
84.00
85.70
87.32
75.91
75.64
88.27
87.31
88.36
84.08
83.48

Table 5. CIFAR-10 test set robust accuracy (%) of a single LINAC defended classiﬁer w.r.t. a suite of L∞ and L2 transfer attacks, valid
under our threat model, using surrogate classiﬁers defended with LINAC, but trained with attacker-chosen keys. The clean accuracy of
our defended classiﬁer is 93.08%.

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

LINAC Defence
Norm Attack Name

L∞

L2

AA
MT
PGD
Square
Best Known

AA
MT
PGD
Square
Best Known

Reconstruction-Based Surrogate Source Models Using Attacker Keys

key1
91.47
68.35
69.35
82.91
62.87
85.94
82.12
82.23
87.30
78.68

key2
91.24
69.13
71.57
82.98
64.59
86.29
82.27
82.77
87.34
79.15

key3
91.21
67.71
69.15
82.47
62.63
85.95
81.91
82.20
87.22
78.36

key4
91.31
68.63
70.55
82.92
63.65
86.51
82.13
82.60
87.40
78.93

key5
91.32
67.53
69.30
82.66
62.40
85.52
81.78
81.89
87.35
78.46

key6
91.35
69.07
70.46
82.80
64.02
86.16
82.42
82.33
87.44
79.08

key7
91.31
68.81
69.67
82.84
63.54
86.28
82.24
82.53
87.52
78.92

key8
91.63
67.98
69.48
82.91
62.93
86.34
82.07
82.38
87.21
78.83

key9
91.33
68.40
69.97
82.91
63.40
86.46
82.21
82.69
87.26
78.80

key10
91.46
69.24
70.85
82.93
64.15
86.27
81.96
82.68
87.46
78.46

Best Adversary
(against all models)
59.40
55.37
56.00
69.14
51.17
74.59
74.98
75.00
83.26
71.89

Table 6. CIFAR-10 test set robust accuracy (%) of a single LINAC defended classiﬁer according to a suite of L∞ and L2 BPDA attacks,
valid under our threat model, using reconstruction-based surrogate classiﬁers.

B.2. Transfer Attacks with Adversarially Trained Models

For mounting transfer attacks we have taken adversarially trained models from previous work (Rebufﬁ et al., 2021), with
checkpoints available online1. These models have been adversarially trained on CIFAR-10 using additional synthetic
generated data and CutMix data augmentation. To mount transfer attacks we use the WideResNet-106-16 model (trained to
defend against L∞ norm-bounded perturbations of size (cid:15) = 8/255) and the WideResNet-70-16 model (trained to defend
against L2 norm-bounded perturbations of size (cid:15) = 0.5).

B.3. PBA Implementation Details

B.3.1. PBA FOR LINAC

We used a single convolutional layer (k = 3 × 3) with biases to implement hψ(x), the PBA of the nuisance transformation,
mapping from the 3 RGB channels of input images to the H = 256 channels output by LINAC.

The parameters ψ of the bypass approximation were trained by minimising the cross-entropy loss on the CIFAR-10 training
set using Momentum SGD with a learning rate µ = 0.1. 100 epochs sufﬁced to optimise PBA parameters, with four learning
rate reductions by a factor of 0.1 at epochs: 65, 80, 90, 95.

B.3.2. PBA FOR BLOCK PIXEL SHUFFLE

We implemented the Block Pixel Shufﬂe defence of AprilPyone & Kiya (2021a) using blocks of size 4 × 4, as recommended
in the original work. We used the same private key value as that of our defended LINAC classiﬁer. The private key serves as
the seed of a pseudo-random number generator, which is used to sample a permutation of all pixel positions in a block. The
same permutation is applied to all blocks. We illustrate the transform in Figure 5.

Figure 5. Example of Pixel Block Shufﬂe transformation (AprilPyone & Kiya, 2021a). An original CIFAR-10 image (left) is split into a
grid of 4 × 4 blocks of adjacent pixels, and the same random permutation is used to shufﬂe pixel positions within every block (middle).
The transformed image is constructed by spatially concatenating the blocks according to their original positions in the grid (right).

A classiﬁer defended with Block Pixel Shufﬂe was trained with the same procedure as our defended LINAC classiﬁer. We
can report a clean CIFAR-10 test set accuracy of 97.03%, which is higher to that reported by AprilPyone & Kiya (2021a),

1https://github.com/deepmind/deepmind-research/tree/master/adversarial_robustness

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

but consistent with the superior CutMix (Yun et al., 2019) data augmentation procedure we used for all defended classiﬁers.

According its own “white-box” threat model (AprilPyone & Kiya, 2021a), all the implementation details of the defence
are known to an attacker except the private key. We exploit the block structure and use a single linear layer without biases,
and initialised with the identity mapping, to compute a parametric bypass approximation (PBA) for the this defence. We
found that using a smaller initial learning rate µ = 0.001 results in stable convergence. We used 300 epochs to optimise
PBA parameters, with four learning rate reductions by a factor of 0.1 at epochs: 275, 285, 290, 295.

An extensive evaluation of the resulting defended classiﬁer is given in Table 7. We ﬁnd that transfer attacks which are
agnostic to the defence can be more successful when adversarial examples are computed using robust source models, but
one may infer some level of robustness. Using PBA attacks valid under the threat model (“white-box”) we successfully
circumvent the defence, with a Best Known CIFAR-10 robust test-set accuracy of 0% under adversarial perturbations of size
up to (cid:15) = 8/255 in L∞ norm, and up to (cid:15) = 0.5 in L2 norm.

Block Pixel Shufﬂe Defence

Transfer Attack Source Models

Adaptive
Attacks

Best
Adversary

Norm Attack Name

Nominal
Source

L∞

L2

AA
MT
PGD
Square
Best Known

AA
MT
PGD
Square
Best Known

85.78
78.87
69.17
69.16
60.65
94.14
93.93
92.92
91.69
90.41

Adversarial
Training
(L∞)
69.09
56.49
39.05
46.25
30.61
90.02
92.25
87.54
88.80
85.29

Adversarial
Training
(L2)
73.86
27.72
31.19
42.63
21.17
83.35
77.02
77.80
84.88
69.00

PBA

0.18
0.00
0.00
0.00
0.00
0.02
0.00
0.02
6.13
0.00

All Source
Models

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.02
6.13
0.00

Table 7. CIFAR-10 test set robust accuracy (%) of Block Pixel Shufﬂe approach (AprilPyone & Kiya, 2021a) against standard L∞ and L2
bounded attacks using both transfer and our novel PBA strategy.

C. Sensitivity of LINAC to Hyper-Parameters

We performed sensitivity analyses of LINAC to its hyper-parameters. For efﬁciency reasons we report robust accuracies
according to untargeted PGD attacks with 100 steps and 10 restarts, using an adversarially trained robust model (L2) (Rebufﬁ
et al., 2021) to generate adversarial perturbations.

F = 3 F = 5 F = 7 F = 10

93.61

Clean
Accuracy:
L∞ PGD 43.51
L2
PGD 74.99

93.08

93.78

93.65

44.06
74.19

43.46
75.96

43.90
74.91

Figure 6. CIFAR-10 test-set clean and robust accuracies under transfer attacks of LINAC defended classiﬁers with different numbers of
positional encoding frequencies F , keeping all other hyper-parameters constant.

In Figure 6 we provide a sensitivity analysis across the number of frequencies F used for positional encoding (Mildenhall
et al., 2020), keeping all other hyper-parameters the same. Note that we used F = 5 for our defended classiﬁer evaluated in

)

%

(

y
c
a
r
u
c
c
a

t
e
s
-
t
s
e
t

100

90

80

70

60

50

40

30

20

10

0

3

4

clean accuracy
L  robust accuracy
L2 robust accuracy

5
8
F (number of frequencies)

6

7

9

10

 
 
Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

the main paper.

L = 3 L = 4 L = 5 L = 6 L = 7 L = 8

95.21

Clean
Accuracy:
L∞ PGD 44.57
L2
PGD 80.40

94.30

93.08

91.70

91.20

90.41

44.55
77.16

44.06
74.19

43.70
71.95

43.81
71.22

43.81
70.08

Figure 7. CIFAR-10 test-set clean and robust accuracies (under transfer attacks) of LINAC defended classiﬁers with different numbers of
implicit network layers L, keeping all other hyper-parameters ﬁxed.

In Figure 7 we vary the number of implicit network layers L, keeping all other hyper-parameters the same, including the
representation layer index K = 2 and number of epochs N = 10. Note that we used L = 5 for our defended classiﬁer
evaluated in the main paper.

K = 0 K = 1 K = 2 K = 3 K = 4

50.32

Clean
Accuracy:
L∞ PGD 36.63
L2
PGD 42.17

86.92

93.08

95.07

95.49

44.55
67.26

44.06
74.19

43.99
78.53

42.83
81.03

Figure 8. CIFAR-10 test-set clean and robust accuracies (under transfer attacks) of LINAC defended classiﬁers with different implicit
network layers used to output representations (K), keeping all other hyper-parameters the same.

In Figure 8 we change the index of the LINAC representation layer K, keeping all other hyper-parameters unchanged. Note
that we used K = 2 for our defended classiﬁer evaluated in the main paper.

In Figure 9 we analyse the sensitivity of LINAC to the number of epochs N , keeping all other hyper-parameters constant.
Note that we used N = 10 for our defended classiﬁer evaluated in the main paper.

D. Characterising the LINAC Transform

In Figure 10 we plot learning curves characterising implicit network ﬁtting, as used for our defended classiﬁer. Mean and
standard deviation of errors across independent learning processes for the entire CIFAR-10 test-set are plotted as functions
of optimisation steps, using a log-scale for errors. The ﬁnal mean value of such errors is 0.04325, which conﬁrms that our
LINAC approach leads to lossy representations.

A histogram of ﬁnal sum squared errors for the entire test-set of CIFAR-10 is provided in Figure 11.

For a qualitative evaluation of such statistics, we provide examples of original images, their reconstructions and difference
images, using LINAC and the private key in Figure 12 and, for comparison, a different key in Figure 13. We observe that
encoding errors using LINAC are key dependent. Furthermore, signiﬁcant amounts of information seem to be left out by
LINAC. Some difference images could be recognised as the correct class, most likely due to high-frequency information
which is not well represented.

Finally, we provide a number of plots for qualitative comparisons of LINAC transforms. Figure 14 shows three different

)

%

(

y
c
a
r
u
c
c
a

t
e
s
-
t
s
e
t

100

90

80

70

60

50

40

30

20

10

0

clean accuracy
L  robust accuracy
L2 robust accuracy

3

4

5

6

7

8

L (number of hidden layers)

 
 
)

%

(

y
c
a
r
u
c
c
a

t
e
s
-
t
s
e
t

100

90

80

70

60

50

40

30

20

10

0

0

clean accuracy
L  robust accuracy
L2 robust accuracy

1

2
K (index of representation layer)

3

4

 
 
Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

N = 5 N = 10 N = 15 N = 20 N = 25 N = 50

85.74

Clean
Accuracy:
L∞ PGD 49.01
L2
PGD 70.65

93.08

94.59

95.41

95.66

95.87

44.06
74.19

40.86
75.69

38.56
76.28

37.63
76.42

35.90
78.06

Figure 9. CIFAR-10 test-set clean and robust accuracies (under transfer attacks) of LINAC defended classiﬁers with implicit networks
trained for N epochs, keeping all other hyper-parameters constant.

Figure 10. Independent ﬁtting of implicit neural networks to CIFAR-10 test-set images in order to compute their LINAC transforms. Sum
squared encoding errors, averaged over pixels, are plotted against ﬁtting steps.

images encoded and their respective LINAC representations encoded as RGB channels. Figures 15, 16 and 17 plot LINAC
transforms of the same respective images, but using with different keys, one on each RGB colour channel.

)

%

(

y
c
a
r
u
c
c
a

t
e
s
-
t
s
e
t

100

90

80

70

60

50

40

30

20

10

0

5

10

15

clean accuracy
L  robust accuracy
L2 robust accuracy

25

20
35
N (number of epochs)

30

40

45

50

 
 
l

)
e
a
c
s
-
g
o
l
(

r
o
r
r
e

101

100

10 1

10 2

0

32

64

96

128

160
steps

192

224

256

288

320

 
Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

Figure 11. Histogram of LINAC transform encoding errors plotted for the entire CIFAR-10 test-set. The overall mean value of such errors
is 0.04325, which conﬁrms that our LINAC approach leads to lossy representations.

mean error

1400

1200

1000

800

600

400

200

0

10 3

10 2

10 1

errors

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

Figure 12. Image approximations computed for LINAC with the private key, as used for our defended classiﬁer. Original images and
labels are plotted in the ﬁrst column. Note that labels are not used for LINAC. Implicit network outputs are plotted in the second column.
Difference images and sum squared errors, averaged over pixels, are plotted in the third column. Note that LINAC uses lossy image
approximations.

horse

airplane

frog

truck

dog

automobile

horse

airplane

cat

bird

1

1

1

1

1

1

1

2

1

0

2

1

0

1

0

2

1

0

1

0

2

1

0

2

1

0

2

1

0

1

1.0

0.5

0.0

0.5

1.0

1.5

2

1

0

1

4.69e-02

3.95e-02

6.85e-02

4.23e-02

5.38e-02

6.74e-02

2.46e-02

2.44e-02

1.67e-02

3.44e-02

2

1

0

2

1

0

1

0

2

1

0

2

1

0

2

1

0

2

1

0

2

1

0

1

0

2

1

0

1

1

2

1

1

2

1

2

1

1

1

1

0.50

0.25

0.00

0.25

0.50

0.5

0.0

0.5

1.0

0.5

0.0

0.5

0.25

0.00

0.25

0.50

0.75

0.4

0.2

0.0

0.2

0.4

0.6

0.5

0.0

0.5

0.4

0.2

0.0

0.2

0.4

0.6

0.6

0.4

0.2

0.0

0.2

0.4

0.4

0.2

0.0

0.2

0.6

0.4

0.2

0.0

0.2

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

Figure 13. Image approximations computed for LINAC with a different, attacker chosen key. Original images and labels are plotted in
the ﬁrst column. Note that labels are not used for LINAC. Implicit network outputs are plotted in the second column. Difference images
and sum squared errors, averaged over pixels, are plotted in the third column. Note that LINAC leads to lossy image approximations
which are key dependent.

horse

airplane

frog

truck

dog

automobile

horse

airplane

cat

bird

1

1

1

1

1

1

1

2

1

0

2

1

0

1

0

2

1

0

1

0

2

1

0

2

1

0

2

1

0

1

1.0

0.5

0.0

0.5

1.0

1.5

2

1

0

1

5.87e-02

3.79e-02

6.16e-02

5.00e-02

7.63e-02

7.23e-02

2.22e-02

3.57e-02

1.40e-02

5.16e-02

1

1

2

1

1

2

1

1

2

1

0

2

1

0

1

0

2

1

0

2

1

0

2

1

0

2

1

0

2

1

0

1

1.0

0.5

0.0

0.5

1.0

1.5

2

1

0

1

0.75

0.50

0.25

0.00

0.25

0.50

0.25

0.00

0.25

0.50

0.75

0.50

0.25

0.00

0.25

0.50

0.50

0.25

0.00

0.25

0.50

0.75

0.5

0.0

0.5

0.5

0.0

0.5

1.0

0.2

0.0

0.2

0.4

0.4

0.2

0.0

0.2

0.4

0.2

0.0

0.2

0.4

0.2

0.0

0.2

0.4

Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

Figure 14. Comparing transforms of the 3 top images using LINAC with the private key, as done for our defended classiﬁer. The respective
activation images with H = 256 channels were plotted in a 16 × 16 grid of slices of the same size with original images. Respective slices
over the channel dimension of activation images were combined as RGB channels in this plot (bottom), in order to compare channel
representations for the three input images (top). Each square in the grid represents the activations of a LINAC representation channel for
all pixels in the original image. Different values of RGB signify differences in LINAC representations across images.

0

1

2

1

0

1

2

2

1

0

1

1

0

1

1

r
e
y
a

l

n
e
d
d
h

i

1

2

3

4

5

6

7

8
hidden unit position in layer

9

10

11

12

13

14

15

16

 
Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

Figure 15. Comparing LINAC transforms of the same image using the private key and two other random keys. The respective activation
images with H = 256 channels were plotted in a 16 × 16 grid of slices of the same size with original images. Respective slices
over the channel dimension of resulting activation images were combined as RGB channels in this plot (bottom), in order to compare
channel representations with three different keys for the same input image. Each square in the grid represents the activations of a LINAC
representation channel for all pixels in the original image. Different values of RGB signify differences in LINAC representations across
keys.

0

1

2

1

0

1

2

2

1

0

1

2

1

0

1

1

r
e
y
a

l

n
e
d
d
h

i

1

2

3

4

5

6

7

8
hidden unit position in layer

9

10

11

12

13

14

15

16

 
Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

Figure 16. Comparing LINAC transforms of the same image using the private key and two other random keys. The respective activation
images with H = 256 channels were plotted in a 16 × 16 grid of slices of the same size with original images. Respective slices
over the channel dimension of resulting activation images were combined as RGB channels in this plot (bottom), in order to compare
channel representations with three different keys for the same input image. Each square in the grid represents the activations of a LINAC
representation channel for all pixels in the original image. Different values of RGB signify differences in LINAC representations across
keys.

0

1

2

1

0

1

2

2

1

0

1

2

1

0

1

1

r
e
y
a

l

n
e
d
d
h

i

1

2

3

4

5

6

7

8
hidden unit position in layer

9

10

11

12

13

14

15

16

 
Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC)

Figure 17. Comparing LINAC transforms of the same image using the private key and two other random keys. The respective activation
images with H = 256 channels were plotted in a 16 × 16 grid of slices of the same size with original images. Respective slices
over the channel dimension of resulting activation images were combined as RGB channels in this plot (bottom), in order to compare
channel representations with three different keys for the same input image. Each square in the grid represents the activations of a LINAC
representation channel for all pixels in the original image. Different values of RGB signify differences in LINAC representations across
keys.

0

1

2

1

0

1

1

0

1

1

0

1

1

r
e
y
a

l

n
e
d
d
h

i

1

2

3

4

5

6

7

8
hidden unit position in layer

9

10

11

12

13

14

15

16

 
","Hindering Adversarial Attacks with Implicit Neural Representations Andrei A. Rusu 1 Dan A. Calian 1 Sven Gowal 1 Raia Hadsell 1 2 2 0 2 t c O 2 2 ] G L . s c [ 1 v 2 8 9 3 1 . 0 1 2 2 : v i X r a Abstract We introduce the Lossy Implicit Network Activa- tion Coding (LINAC) defence, an input transfor- mation which successfully hinders several com- mon adversarial attacks on CIFAR-10 classiﬁers for perturbations up to (cid:15) = 8/255 in L∞ norm and (cid:15) = 0.5 in L2 norm. Implicit neural rep- resentations are used to approximately encode pixel colour intensities in 2D images such that classiﬁers trained on transformed data appear to have robustness to small perturbations without adversarial training or large drops in performance. The seed of the random number generator used to initialise and train the implicit neural represen- tation turns out to be necessary information for stronger generic attacks, suggesting its role as a private key. We devise a Parametric Bypass Ap- proximation (PBA) attack strategy for key-based defences, which successfully invalidates an ex- isting method in this category. Interestingly, our LINAC defence also hinders some transfer and adaptive attacks, including our novel PBA strat- egy. Our results emphasise the importance of a broad range of customised attacks despite appar- ent robustness according to standard evaluations. LINAC source code and parameters of defended classiﬁer evaluated throughout this submission are available: https://github.com/deepmind/linac. 1. Introduction Training Deep Neural Network (DNN) classiﬁers which are accurate yet generally robust to small adversarial perturba- tions is an open problem in computer vision and beyond, inspiring much empirical and foundational research into modern DNNs. Szegedy et al. (2014) showed that DNNs are not inherently robust to imperceptible input perturbations, which reliably cross learned decision boundaries, even those of different models trained on similar data. With hindsight, 1DeepMind, London, UK. Correspondence to: Andrei A. Rusu <andrei@deepmind.com>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). it becomes evident that two related yet distinct design prin- ciples have been at the core of proposed defences ever since. Intuitively, accurate DNN classiﬁers could be considered ro- bust in practice if: (I) their decision boundaries were largely insensitive to all adversarial perturbations, and/or (II) com- puting any successful adversarial perturbations was shown to be expensive, ideally intractable. Early defences built on principle (I) include the adversarial training approach of Madry et al. (2018) and the veriﬁable defences of Hein & Andriushchenko (2017); Raghunathan et al. (2018), with many recent works continually reﬁning such algorithms, e.g. Cohen et al. (2019); Gowal et al. (2020); Rebufﬁ et al. (2021). A wide range of defences were built, or shown to operate, largely on principle (II), including adversarial detection methods (Carlini & Wagner, 2017a), input trans- formations (Guo et al., 2018) and denoising strategies (Liao et al., 2018; Niu et al., 2020). Many such approaches have since been circumvented by more effective attacks, such as those proposed by Carlini & Wagner (2017b), or by using “adaptive attacks” (Athalye et al., 2018; Tramer et al., 2020). Despite the effectiveness of recent attacks against these defences, Garg et al. (2020) convincingly argue on a theo- retical basis that principle (II) is sound; similarly to cryp- tography, robust learning could rely on computational hard- ness, even in cases where small adversarial perturbations do exist and would be found by a hypothetical, computa- tionally unbounded adversary. However, constructing such robust classiﬁers for problems of interest, e.g. image clas- siﬁcation, remains an open problem. Recent works have proposed defences based on cryptographic principles, such as the pseudo-random block pixel shufﬂing approach of AprilPyone & Kiya (2021a). As we will show, employing cryptographic principles in algorithm design is not in it- self enough to prevent efﬁcient attacks. Nevertheless, we build on the concept of key-based input transformation and propose a novel defence based on Implicit Neural Repre- sentations (INRs). We demonstrate that our Lossy Implicit Neural Activation Coding (LINAC) defence hinders most standard and even adaptive attacks, more so than the related approaches we have tested, without making any claims of robustness about our defended classiﬁer. Contributions: (1) We demonstrate empirically that lossy INRs can be used in a standard CIFAR-10 image classiﬁca- tion pipeline if they are computed using the same implicit Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) network initialisation, a novel observation which makes our LINAC defence possible. (2) The seed of the random num- ber generator used for initialising and computing INRs is shown to be an effective and compact private key, since with- holding this information hinders a suite of standard adver- sarial attacks widely used for robustness evaluations. (3) We report our systematic efforts to circumvent the LINAC de- fence with transfer and a series of adaptive attacks, designed to expose and exploit potential weaknesses of LINAC. (4) To the same end we propose the novel Parametric Bypass Ap- proximation (PBA) attack strategy, valid under our threat model, and applicable to other defences using secret keys. We demonstrate its effectiveness by invalidating an existing key-based defence which was previously assumed robust. 2. Related Work Adversarial Robustness. Much progress has been made towards robust image classiﬁers along the adversarial train- ing (Madry et al., 2018) route, which has been extensively explored and is well reviewed, e.g. in (Schott et al., 2019; Pang et al., 2020; Gowal et al., 2020; Rebufﬁ et al., 2021). While such approaches can be effective against current at- tacks, a complementary line of work investigates certiﬁed defences, which offer guarantees of robustness around ex- amples for some well deﬁned sets (Wong & Kolter, 2018; Raghunathan et al., 2018; Cohen et al., 2019). Indeed, many such works acknowledge the need for complementary ap- proaches, irrespective of the success of adversarial training and the well understood difﬁculties in combining methods (He et al., 2017). The proliﬁc work on defences against adversarial perturbations has spurred the development of stronger attacks (Carlini & Wagner, 2017b; Brendel et al., 2018; Andriushchenko et al., 2020) and standardisation of evaluation strategies for threat models of interest (Athalye et al., 2018; Croce & Hein, 2020), including adaptive attacks (Tramer et al., 2020). Alongside the empirical progress to- wards building robust predictors, this line of research has yielded an improved understanding of current deep learn- ing models (Ilyas et al., 2019; Engstrom et al., 2019), the limitations of effective adversarial robustness techniques (Jacobsen et al., 2018), and the data required to train them (Schmidt et al., 2018). Athalye et al. (2018) show that a number of defences pri- marily hinder gradient-based adversarial attacks by obfus- cating gradients. Various forms are identiﬁed, such as gra- dient shattering (Goodfellow et al., 2014), gradient masking (Papernot et al., 2017), exploding and vanishing gradients (Song et al., 2018b), stochastic gradients (Dhillon et al., 2018) and a number of input transformations aimed at coun- tering adversarial examples, including noise ﬁltering ap- proaches using PCA or image quilting (Guo et al., 2018), the Saak transform (Song et al., 2018a), low-pass ﬁltering (Shaham et al., 2018), matrix estimation (Yang et al., 2019) and JPEG compression (Dziugaite et al., 2016; Das et al., 2017; 2018). Indeed, many such defences have been pro- posed, as reviewed by Niu et al. (2020), they have ranked highly in competitions (Kurakin et al., 2018), and many have since been shown to be less robust than previously thought, e.g. by Athalye et al. (2018) and Tramer et al. (2020), who use adaptive attacks to demonstrate that several input transformations offer little to no robustness. To build on such insights, it is worth identifying the “ingre- dients” essential to the success of adversarial attacks. Most effective attacks, including adaptive ones, assume the ability to approximate the outputs of the targeted model for arbi- trary inputs. This is reasonable when applying the correct transformation is tractable for the attacker. Hence, deny- ing access to such computations seems to be a promising direction for hindering adversarial attacks. AprilPyone & Kiya (2020; 2021b); MaungMaung & Kiya (2021) borrow standard practice from cryptography and assume that an attacker has full knowledge of the defence’s algorithm and parameters, short of a small number of bits which make up a private key. Another critical “weakness” of such in- put denoising defences is that they can be approximated by the identity mapping for the purpose of computing gra- dients (Athalye et al., 2018). Even complex parametric approaches, which learn stochastic generative models of the input distribution, are susceptible to reparameterisation and Expectation-over-Transformation (EoT) attacks in the white-box setting. Thus, it is worth investigating whether non-parametric, lossy and fully deterministic input transfor- mations exist such that downstream models can still perform tasks of interest to high accuracy, while known and novel attack strategies are either ruled out, or at least substantially hindered, including adaptive attacks. Implicit Neural Representations. Neural networks have been used to parameterise many kinds of signals, see the work by Sitzmann (2020) for an extensive list, with remark- able recent advances in scene representations (Mildenhall et al., 2020) and image processing (Sitzmann et al., 2020). INRs have been used in isolation per image or scene, not for generalisation across images. Some exceptions exist in unsupervised learning, e.g. Skorokhodov et al. (2021) parameterise GAN decoders such that they directly output INRs of images, rather than colour intensities for all pixels. In this paper we show that INRs can be used to discover functional decompositions of RGB images which enable comparable generalisation to learning on the original signal encoding (i.e. RGB). Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 1. Visual depiction of LINAC, our proposed input transformation. An RGB image x is converted into an Activation Image t(x) with identical spatial dimensions, but H channels instead of 3. A neural network model which maps pixel coordinates to RGB colour intensities is ﬁt such that it approximates x. The resulting model parameters (after ﬁtting) are called the Implicit Neural Representation (INR) of image x. In order to output correct RGB colour intensities for all pixels, the implicit neural network needs to compute a hierarchical functional decomposition of x. We empirically choose an intermediate representation to deﬁne our transformation. Activations in the middle hidden layer are associated with their corresponding pixel coordinates to form the output Activation Image t(x), with as many channels as there are units in the middle layer (H). 3. Hindering Adversarial Attacks with Implicit Neural Representations In this section we introduce LINAC, our proposed input transformation which hinders adversarial attacks by leverag- ing implicit neural representations, also illustrated in Fig. 1. Setup. We consider a supervised learning task with a dataset D ⊂ X × Y of pairs of images x and their correspond- ing labels y. We use a deterministic input transformation t : X → H which transforms input images, x (cid:55)→ t(x), while preserving their spatial dimensions. Further, we consider a classiﬁer fθ, parameterised by θ, whose parameters are estimated by Empirical Risk Minimisation (ERM) to map transformed inputs to labels fθ : H → Y. The model is not adversarially trained, yet ﬁnding adversarial examples for it is hindered by LINAC, as we demonstrate through extensive evaluations in Section 5. Implicit Neural Representations. For an image x, its im- plicit neural representation is given by a multi-layer per- ceptron (MLP) Φ = hL ◦ hL−1 ◦ · · · ◦ h0, Φ : R2 → R3, with L hidden layers, which maps spatial coordinates to their corresponding colours. Φφ is a solution to the implicit equation: Φ(p) − x(p) = 0, (1) where p are spatial coordinates (i.e. pixel locations) and x(p) are the corresponding image colours. Our input trans- formation leverages this implicit neural representation to encode images in an approximate manner. Reconstruction Loss. The implicit equation (1) can be translated (Sitzmann et al., 2020) into a standard recon- struction loss between image colours and the output of a multi-layer perceptron Φφ at each (2D) pixel location pi,j, L(φ, x) = (cid:88) i,j ||Φφ(pi,j) − x(pi,j)||2 2. (2) Algorithm 1 The LINAC Transform Inputs: RGB image x (with size I × J × 3); private key; number of epochs N ; mini-batch size M ; number of MLP layers L; representation layer K; learning rate µ. Output: Activation Image t(x) (with size I × J × H). rng = INIT PRNG(private key) (cid:46) Seed rng. φ(0) = INIT MLP(rng, L) S = (cid:98)I · J/M (cid:99) φ = φ(0) for epoch = 0 . . . N − 1 do (cid:46) Num. mini-batches per epoch. P = SHUFFLE AND SPLIT PIXELS(x, rng, S) for m = 0 . . . S − 1 do (cid:96) = 1 M ·I·J (cid:80) (i,j)∈P[m] ||Φφ(pi,j) − x(pi,j)||2 2 φ = φ − µ∇φ(cid:96) end for end for ˆφx = φ Return t(x) applying Eq. 3 using ˆφx and layer K. We provide pseudocode for the LINAC transform in Al- gorithm 1 and a discussion of computational and memory requirements in Appendix A.1.4. For each individual im- age x, we estimate ˆφx, an approximate local minimiser of L(φ, x), using a stochastic iterative minimisation procedure with mini-batches of M pixels grouped into epochs, which cover the entire image in random order, for a total of N passes through all pixels. Private Key. A random number generator is used for: (1) generating the initial MLP parameters φ(0) and (2) for decid- ing which random subsets of pixels make up mini-batches in each epoch. This random number generator is seeded by a 64-bit integer which we keep secret and denote as the pri- vate key. Hence, for all inputs x we start each independent optimisation from the same set of initial parameters φ(0), and we use the same shufﬂing of pixels across epochs. i Pixel(i, j) Intensities x(pi,j) = (r, g, b) j x: RGB Image In Implicit Neural Representation pi,j = (i, j) x(pi,j) = (r, g, b) … … …… … t(x): Activation Image Out i Representation of Pixel(i, j) Intensities … j Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Lossy Implicit Network Activation Coding (LINAC). We consider the lossy encoding of each pixel (i, j) in im- age x as the H-dimensional intermediate activations vector of layer K of the MLP evaluated at that pixel position: cx(i, j) = (hK−1 )(pi,j) with K < L. We build the lossy implicit network activation coding transformation of an image x by stacking together the encodings of all its pixels in its 2D image grid, concatenating on the feature dimension axis. The LINAC transformation t(x) of the I × J × 3 image x is given by: ◦ · · · ◦ h0 ˆφx ˆφx t(x) =    cx(0, 0) ... cx(I − 1, 0) . . . . . . . . . cx(0, J − 1) ... cx(I − 1, J − 1)    , (3) and has dimensionality I × J × H, where H is the number of outputs of the K-th layer of the MLP. By construction, our input transformation preserves the spatial dimensions of each image while increasing the feature dimensionality (from 3, the image’s original number of colour channels, to H); this means that standard network architectures used for image classiﬁcation (e.g. convolutional models) can be readily trained as the classiﬁer fθ. All omitted implementation details are provided in Ap- pendix A, and sensitivity analyses of LINAC to its hyper- parameters are reported in Appendix C. Threat Model. We are interested in hindering adversarial attacks on a nominally-trained classiﬁer fθ(t(x)), which operates on transformed inputs (i.e. on t(x) rather than on x), using a private key of our choosing. Next, we describe the threat model of interest by stating the conditions under which the LINAC defence is meant to hinder adversarial attacks on fθ, following AprilPyone & Kiya (2021a). We assume attackers do not have access to the private key, the integer seed of the random number generator used for computing the LINAC transformation, but otherwise have full algorithmic knowledge about our approach. Speciﬁcally, we assume an attacker has complete information about the classiﬁcation pipeline, including the architecture, training dataset and weights of the defended classiﬁer. This includes full knowledge of the LINAC algorithm, the implicit net- work architecture, parameter initialisation scheme and all the ﬁtting details, except for the private key. 4. Attacking the LINAC Defence Setup. We are interested in evaluating the apparent ro- bustness of a LINAC-defended classiﬁer, fˆθ, which has been trained by ERM to classify transformed inputs from the dataset D. Speciﬁcally, its parameters ˆθ minimise Ex,y∼D [LCE(fθ(t(x)), y)] , where LCE is the cross-entropy loss and t(x) is the LINAC transformation applied to image x using the private key. Input Perturbations. Classiﬁers defended by LINAC are not adversarially trained (Madry et al., 2018) to increase their robustness to speciﬁc Lp norm-bounded input pertur- bations. Furthermore, the LINAC defence is inherently agnostic about particular notions of maximum input pertur- bations. Nevertheless, to provide results comparable with a broad set of defences from the literature, we perform eval- uations on standard Lp norm-bounded input perturbations with: (1) a maximum perturbation radius of (cid:15) = 8/255 in the L∞ norm, and (2) one of (cid:15) = 0.5 in the L2 norm. Adapting Existing Attacks. Without access to the pri- vate key an attacker cannot compute the LINAC transfor- mation exactly. However, an attacker could acquire ac- cess to model inferences by attempting to brute-force guess the private key. Another option would be to train surro- gate models with LINAC, but using keys chosen by the at- tacker, in the hope that decision boundaries of these models would be similar enough to mount effective transfer attacks. More advanced attackers could modify LINAC itself to en- able strong Backward Pass Differentiable Approximation (BPDA) (Athalye et al., 2018) attacks. We evaluate the success of these and other standard attacks in Section 5. Designing Adaptive Attacks. Athalye et al. (2018) provide an excellent set of guidelines for designing and perform- ing successful adaptive attacks, while also standardising results reporting and aggregation. Of particular interest for defences based on input transformations are the BPDA and Expectation-over-Transformation (EoT) attack strategies. Subsequent work convincingly argues that adaptive attacks are not meant to be general, and must be customised, or “adapted”, to each defence in turn (Tramer et al., 2020). While BPDA and EoT generate strong attacks on input transformations, they both rely on being able to compute the forward transformation or approximate it with samples. Indeed, the authors mention that substitution of both the forward and backward passes with approximations leads to either completely ineffective, or much less effective attacks. Parametric Bypass Approximation (PBA). Inspired by the reparameterisation strategies of Athalye et al. (2018), we propose a bespoke attack by making use of several pieces of information available under our threat model: the parametric form of the defended classiﬁer fθ(t(x)), its training dataset D and loss function LCE, and its trained weights ˆθ. A Parametric Bypass Approximation of an unknown nui- sance transformation u : X → H is a surrogate parametric function hψ : X → H, parameterised by a solution to the following optimisation problem: ψ∗ = arg min ψ E x,y∼D (cid:2) LCE(fˆθ(hψ(x)), y) (cid:3) . (4) This formulation seeks a set of parameters ψ∗ which min- imise the original classiﬁcation loss while keeping the de- Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) fended classiﬁer’s parameters frozen at ˆθ. Similar with classiﬁer training, this optimisation problem can be solved efﬁciently using Stochastic Gradient Descent (SGD). A PBA adversarial attack can then proceed by approximat- ing the defended classiﬁer fˆθ(u(·)) with those of the bypass classiﬁer fˆθ(hψ∗ (·)) in both forward and backward passes when computing adversarial examples, e.g. using Projected Gradient Descent (PGD). The main advantages of the PBA strategy are that no for- ward passes through the nuisance transformation u(·) are required, and that it admits efﬁcient computation of many attacks to fˆθ, including gradient-based ones. In Section 5 we demonstrate the effectiveness of PBA beyond the LINAC defence. We show that, even though the surrogate transfor- mation is ﬁt on training data only, the defended classiﬁer operating on samples passed through hψ∗ (bypassing u) demonstrates nearly identical generalisation to the test set. Furthermore, we also show that PBA has greater success at ﬁnding adversarial examples for the LINAC defence com- pared to other methods. Lastly, we use PBA to invalidate an existing key-based defence proposed in the literature. 5. Results 5.1. Evaluation Methodology Since LINAC makes no assumptions about adversarial per- turbations, we are able to evaluate a single defended classi- ﬁer model against all attack strategies considered, in contrast to much adversarial training research (Madry et al., 2018). To obtain a more comprehensive picture of apparent robust- ness we start from the rigorous evaluation methodology used by Gowal et al. (2019); Rebufﬁ et al. (2021). We perform untargeted PGD attacks with 100 steps and 10 randomised restarts, as well as multi-targeted (MT) PGD attacks using 200 steps and 20 restarts. Anticipating the danger of ob- fuscated gradients skewing results, we also evaluate with the Square approach of Andriushchenko et al. (2020), a powerful gradient-free attack, with 10000 evaluations and 10 restarts. For precise comparisons with the broader liter- ature we also report evaluations using the parameter-free AutoAttack (AA) strategy of Croce & Hein (2020). Following Athalye et al. (2018) we aggregate results across attacks by only counting as accurate robust predictions those test images for which the defended classiﬁer predicts the correct class with and without adversarial perturbations, computed using all methods above. We report this as Best Known robust accuracy. In instances where several surrogate models are used to compute adversarial perturbations, also known as transfer attacks, we report Best Adversary results aggregated for each individual attack, which is deﬁned as robust accuracy Figure 2. Results of direct attack on private key. A histogram of accuracies of the same defended classiﬁer with inputs transformed using either the correct key or 100000 randomly chosen keys. An appropriate surrogate transformation is not found, invalidating attack vectors which rely on access to the outputs of the defended model on attacker chosen inputs. against all source models considered. We aggregate evaluations across these two dimensions (at- tacks & surrogate models) by providing a single robust ac- curacy number against all attacks computed using all source models for each standard convention of maximum perturba- tion and norm, enabling easy comparisons with results in the literature. 5.2. Attacks with Surrogate Transformations & Models A majority of adversarial attack strategies critically depend on approximating the outputs of the defended classiﬁer for inputs chosen by the attacker. The private key is kept secret in our threat model, which means that an attacker can neither compute the precise input transformation used to train the defended classiﬁer, nor its outputs on novel data. Hence, an attacker must ﬁnd appropriate surrogate transformations, or surrogate classiﬁer models, in order to perform effective adversarial attacks. We investigate both strategies below. Firstly, we empirically check that the outputs of the de- fended classiﬁer cannot be usefully approximated without knowledge of the private key. It is reasonable to hypothesise that transformations with different keys may lead to simi- lar functional representations of the input signal. We start investigating this hypothesis by simply computing the accu- racy of the defended model on clean input data transformed with LINAC, but using keys chosen by the attacker, also known as a brute-force key attack, which is valid under our threat model. As reported in Figure 2, the accuracy of our LINAC defended classiﬁer on test inputs transformed with the correct private key is over 93%. In an attempt to ﬁnd a surrogate transformation, 100000 keys are picked uniformly at random. For each key, we independently evaluated the accuracy of the classiﬁer using a batch of 100 test examples, and we report the resulting accuracy estimates for all keys with a histogram plot. The mean accuracy with random key guesses is around 30%, with a top accuracy of just 57% (see Table 4 in Appendix B.1 for a breakdown). Hence, using LINAC with incorrect keys leads to poor approximations of classiﬁer outputs on correctly transformed data. This Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Norm Attack Nominal Source AA MT PGD Square Best Known AA MT PGD Square Best Known 92.77 84.57 85.99 85.12 81.91 90.84 87.55 88.61 88.58 86.06 L∞ L2 Transfer Attack Source Models Adversarial Training (L∞) 80.42 72.96 60.97 65.69 54.97 86.75 85.34 82.39 84.50 79.42 Adversarial Training (L2) 70.29 56.08 44.06 52.66 39.20 80.83 84.81 74.19 79.31 71.92 Defended Surrogates (Attacker Keys) 84.00 85.70 87.32 75.91 75.64 88.27 87.31 88.36 84.08 83.48 Reconstruction- Based Surrogates (BPDA) 59.40 55.37 56.00 69.14 51.17 74.59 74.98 75.00 83.26 71.89 Best Adversary All Source Models 41.18 47.91 41.22 49.76 37.04 71.32 73.83 70.90 77.68 68.41 Table 1. CIFAR-10 test set robust accuracy (%) of a single LINAC defended classiﬁer according to a suite of L∞ and L2 transfer attacks, valid under our threat model, using various source classiﬁers to generate adversarial perturbations. suggest that the learned decision boundaries of the defended classiﬁer are not invariant to the private key used by LINAC. Figure 3. Plots of CIFAR-10 test-set robust accuracy estimates (Best Known) vs. number of attacker-trained surrogate models. We also plot the clean accuracy of 93.08% for reference. While we could not ﬁnd a useful surrogate transformation by random guessing, it is still possible that transformations with different keys preserve largely the same input informa- tion. So, the second option of an attacker is to check whether decision boundaries of models defended with LINAC and different keys are in fact very similar, which would enable powerful transfer attacks from such surrogate models. To this end, 10 independent models defended with LINAC were trained from scratch, each using a different key chosen by the attacker. We used the most promising 10 keys from the brute-force key attack for this purpose. In Figure 3 we report Best Known robust accuracy plotted against the num- ber of surrogate models used in these joint attacks, and we aggregate results over all 10 attacking models in the fourth column of Table 1. However, this attack vector has limited success. Under transfer attacks with such surrogate models, the robust accuracy of our defended classiﬁer appears to be high. While PGD and MT may fail due to vanishing or exploding gradients (Athalye et al., 2018), Square is a gradient-free attack, and does not suffer from such issues. Robust accuracy estimates according to Square are higher than 83% against any individual surrogate model, irrespec- tive of perturbation norm. A complete breakdown of results is given in Table 5 of Appendix B.1. Attacking with all 10 surrogate models together, robust accuracy to Square is still higher that 75%, and the estimate is not improved by further aggregating over attacks. This evidence further support the hypothesis that decision boundaries of classiﬁers defended with LINAC depend on their respective keys, and may differ enough across keys to hinder transfer attacks with surro- gates. Investing an order of magnitude more computation into such attacks leads to modest reductions in apparent robustness. Lastly, an attacker may strive to employ BPDA, one of the most effective and general strategies against defences using nuisance transformations. BPDA attacks require: (1) the ability to compute the exact forward transforma- tion and (2) ﬁnding a usefully differentiable approximation to the said transformation for use in the backwards pass of gradient-based attacks. In many cases this would be enough to allow the attacker to compute adversarial examples, per- haps at a somewhat higher computational cost (Athalye et al., 2018; Tramer et al., 2020). Our LINAC defence presents further challenges by design. Exact forward computations (model inferences) require the private key. An attacker cannot exactly compute the input transformation even for training set images, e.g. in order for some differentiable parametric approximation to be learned in a supervised fashion. Furthermore, surrogate models defended using LINAC and attacker chosen keys do not appear to be usefully differentiable, as suggested by results in Table 1. Nevertheless, an attacker could still hope that our defence “ﬁlters out” information in a largely key-agnostic manner, and that the choice of implicit network representa- tion layer is not essential. Hence, they have the option of modifying LINAC to output activations of the last, rather than the middle layer of the implicit network. This amounts ) % ( y c a r u c c a t s u b o r t e s - t s e t 90 80 70 60 50 40 30 20 None 1 2 Surrogates - L2 Surrogates - L BPDA - L2 BPDA - L 4 3 5 number of surrogate models 6 7 8 9 10 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Norm L∞ L2 Attack AA MT PGD Square Best Known AA MT PGD Square Best Known All Source Models Transfer 41.18 47.91 41.22 49.76 37.04 71.32 73.83 70.90 77.68 68.41 Adaptive Attacks PBA BPDA 68.34 59.40 46.75 55.37 44.05 56.00 48.59 69.14 35.32 51.17 73.10 74.59 67.85 74.98 66.93 75.00 74.70 83.26 61.23 71.89 Table 2. CIFAR-10 test set robust accuracy (%) of a single LINAC defended classiﬁer w.r.t. a suite of L∞ and L2 attacks, valid under our threat model, using different strategies such as transfer and adaptive attacks. Our novel PBA adaptive attacks are overall more effective that both transfer and BPDA attack strategies. to reducing LINAC to an approximate reconstruction of the original signal. While such surrogate models with at- tacker chosen keys would still have to be trained for the purpose, they would be vulnerable to strong BPDA attacks, which may transfer well to our defended classiﬁer. Appar- ent robustness estimates according to such transfer BPDA attacks are plotted in Figure 3 as a function of the number of surrogate models used jointly in the attack. In the ﬁfth column of Table 1 we provide aggregate apparent robust accuracies using 10 such surrogates, showing that transfer BPDA attacks are more successful than previous attempts; any such reconstruction-based surrogate model can be used to reveal that the robust accuracy of our defended classiﬁer cannot be higher than 65%, particularly with standard L∞ multi-targeted (MT) attacks (see Table 6 in Appendix B.1 for a detailed breakdown of results). Interestingly, when 10 surrogate models are used together, L∞ robust accuracy esti- mates drop to 51%. The reduction is less severe in standard L2 attacks, where accuracy against all surrogates appears to be still over 71%. These results conﬁrm that the BPDA strategy is a valuable tool for investigating the robustness of a wide range of defences, even when its assumptions are not fully met. 5.3. Transfer Attacks with Nominal and Adversarially Trained Source Models Since our defended classiﬁer is not adversarially trained, one could assume that its decision boundaries may be simi- lar to those of a nominal, undefended classiﬁer. We show in the ﬁrst column of Table 1 that transfer attacks with a nominally trained source model have limited success, es- pecially considering that such undefended classiﬁers have below chance robust accuracies according to the very same evaluations. Another possibility is that that our defended model may be susceptible to the promising attack directions to which adversarially trained robust classiﬁers are vulnerable. We report in the second and third columns of Table 1 that this is indeed the case to some extent. Of all adversaries considered thus far, a robust model adversarially trained to tolerate perturbations of up to size (cid:15) = 0.5 in L2 norm leads to the most effective transfer attacks. This holds to a lesser degree for an adversarially trained model with perturbations of size (cid:15) = 8/255 in L∞ norm. Despite the success of evaluations using the former source model, no one attack method comes close to the effectiveness of the joint strategy, reported as Best Known robust accuracy. Furthermore, it is important to note that ensemble transfer attacks are much stronger than those computed with any given source model. Aggregated over four attack types and 23 different source models, the robust accuracy of our LINAC defended classiﬁer is revealed to be at most half of what initial results suggested according to aggregate L∞ evaluations; this does not appear to be the case for L2 at- tacks, however, which continue to be substantially hindered by LINAC. Robust accuracy could still be above 68% ac- cording to the latter attack type, even in aggregate. In order to better characterise the implications of LINAC we make use of novel adaptive attacks in the following subsection. 5.4. PBA Attacks Against LINAC Thus far we have shown that strong transfer attacks can be performed by using an ensemble of diverse source models to compute adversarial perturbations over many repeated trials. While ultimately more reliable, this is a cumbersome evaluation protocol, requiring two order of magnitude more computation than standard evaluations. In Section 4 we have introduced PBA, an attack strategy pur- posefully designed to be effective against input transforma- tions (or network modules) which deny both inference and gradient computations, despite classiﬁer parameters, train- ing loss and dataset being available to the attacker. Follow- ing this novel strategy we successfully trained a parametric bypass approximation (PBA) of the LINAC transform and its associated defended classiﬁer. Intriguingly, the decision boundaries of the resulting bypass classiﬁer generalise very well. Accuracy on clean test data is 95.35%. Furthermore, the bypass classiﬁer can be readily shown to have 0% robust accuracy using PGD attacks. This indicates that any appar- ent robustness in evaluations can be largely attributed to the LINAC transform successfully hindering attacks, since the decision boundaries of our defended classiﬁer are sus- ceptible to adversarial perturbations, and hence cannot be considered to add any inherent robustness by themselves. In Table 2 we show that standard attacks using the trained PBA mapping against our LINAC defended classiﬁer are even more effective than BPDA attacks using 10 source models. Interestingly, PBA almost uniformly leads to more effective attacks, regardless of strategy. PGD attacks us- Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) (A) (B) (C) (D) (E) Figure 4. Decision boundaries of ﬁve different classiﬁers (rows) around the same ﬁve randomly chosen test examples (columns), plotted along their respective adversarial directions according to the AT (L2) model (horizontal), and the same random direction (vertical): (A) An undefended, nominally trained CIFAR-10 classiﬁer; (B) LINAC defended classiﬁers using a random key; (C) LINAC defended classiﬁers using the private key; this is the model we evaluated throughout the submission; (D) The by-pass classiﬁer resulting from our novel PBA attack on model (C); (E) An adversarially trained classiﬁer, AT (L2) in the main text, used to generate transfer attacks. We observe that boundaries of nominal model (A) are different from those of LINAC (B) & (C); LINAC decision boundaries seem less smooth compared to other models; as suspected boundaries appear different across keys (B) vs. (C), which corroborates our observations of robustness to transfer attacks with surrogates. The adversarially trained model (E) is robust to the vertical dimension (random noise); LINAC models (B) & (C) also appear less sensitive to random noise compared the nominal model (A). PBA by-pass classiﬁer (D) boundaries are much smoother and different from the true boundaries of the attacked model (C), which may explain why LINAC withstands the novel attack in many cases. Notice that PBA approximated boundaries (D) can be both closer and farther away from test examples compared to the true model’s (C), which makes it less clear how useful such approximations are for future attacks on LINAC. ing PBA give the most accurate picture of robustness of all strategies, suggesting that the matter of obfuscated gradi- ents is largely mitigated by our novel strategy. Aggregated over different attack types, PBA is the most effective and efﬁcient evaluation strategy which does not make use of the private key, and hence is valid under the adopted threat model. Based on these evaluations alone, one may conclude that robust accuracy was over 35% under attacks of size at most (cid:15) = 8/255 in L∞ norm, and over 61% for attacks of size (cid:15) = 0.5 in L2 norm. The apparent robustness differ- ence between L∞ and L2 attacks persists, suggesting that LINAC primarily hinders the latter type of attacks. 5.5. Towards Explaining the Apparent Robustness Decision boundary inspection. We plotted decision bound- aries of several classiﬁers around ﬁve randomly chosen test examples in Figure 4. All boundary plots are centred on test examples (columns), use appropriate adversarial direc- tions as the horizontal dimension, and a random direction as the vertical. As expected, we observe differences between LINAC defended classiﬁers which use different keys. Fur- thermore, we found that LINAC boundaries can be more “complicated” relative to those of other models, which may explain why PBA attacks are not completely effective. RGB Reconstruction vs. Lossy Encodings. Setting the representation layer index K = L renders our LINAC trans- Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Full PCA Block PCA JPEG (23) JPEG (10) Block Pixel Shufﬂe LINAC (Ours) 96.10 96.39 88.15 81.17 96.98 93.08 Standard Standard BPDA BPDA 32.58 – 27.48 6.34 11.90 – 17.49 5.36 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.61 2.26 0.00 0.03 0.41 12.85 0.06 0.00 0.17 11.66 62.95 – 60.37 28.33 62.98 – 60.38 21.92 0.02 0.00 14.94 14.56 PBA 0.18 0.00 0.00 0.00 0.00 0.02 0.00 0.02 6.13 0.00 PBA 68.34 46.75 44.05 48.59 35.32 73.10 67.85 66.93 74.70 61.23 Clean Accuracy: Norm Attack L∞ L2 AA MT PGD Square Best Known AA MT PGD Square Best Known Table 3. CIFAR-10 test set robust accuracy (%) of several classi- ﬁers defended using related input transformations according to evaluations using adversarial perturbations bounded in L∞ and L2 norms. Reporting results of strongest known attack strategy for each method, valid according to its own threat model. form into an approximate RGB input reconstruction, since L is the index of the implicit network output layer. We conﬁrmed that setting K = L = 5 and N = 100 epochs offers no robustness, since the resulting reconstructions are precise and BPDA attacks are successful. Clean accuracy was 96.91%, virtually matching that of a nominally trained classiﬁer. Hence, any apparent robustness must be due to the number of INR ﬁtting epochs N , and/or the choice of repre- sentation layer index K. Intuitively, both hyper-parameters control how “lossy” our transformation is. Naturally, we were interested in reducing the computational overhead of LINAC. Aiming to match the clean accuracy of state-of-the-art adversarially trained robust classiﬁers, speciﬁcally 93% (Rebufﬁ et al., 2021), we empirically chose N = 10 epochs as a trade-off between speed and clean accu- racy. The activation coding layer index K = 2 out of L = 5 hidden layers was chosen according to the same principle, as the lowest level representation which did not reduce clean accuracy below the target threshold. We further characterise and illustrate our LINAC transform in Appendix D. Performance Considerations. LINAC is as expensive as inference with a WideResNet-70-16 (Zagoruyko & Ko- modakis, 2016) on CIFAR-10 images. This cost is dom- inated by the ﬁtting of INRs. It could be reduced with an adaptive form of “early stopping” based on loss values, or by leveraging advances in INR research (e.g. Sitzmann et al. (2020)). We leave these investigations, and scaling LINAC to larger images, for future work. Sensitivity Analyses. The apparent robustness of LINAC defended classiﬁers is largely insensitive to the number of hidden layers L ≥ 3 of the implicit MLP, as well as the number of features F ≥ 3 in its positional input encoding, hence we relegated the sensitivity analyses to Appendix C. 5.6. PBA Beyond LINAC and Methodology Validation We show in the one-but-last column of Table 3 that PBA successfully and completely invalidates the Block Pixel Shufﬂe approach of AprilPyone & Kiya (2021a), despite its good reported robustness against all attacks. We further investigate using adversarially trained source models, see full results in Table 7 of Appendix B.1. In summary, our analysis conﬁrms that the apparent robust accuracy of Block Pixel Shufﬂe according to valid attacks bounded in L2 norm remains high at 69%. Hence, PBA is indeed the only known valid attack on this defence which is completely successful. Finally, we validate our evaluation methodology by test- ing its effectiveness against similar defences. We perform the same evaluations on the Principal Component Analysis (PCA) based defence of Shaham et al. (2018), and the JPEG- based defences of Das et al. (2017; 2018); Guo et al. (2018). In Table 3 we report the Best Known robust accuracies of these defences according to our evaluation methodology, which are directly comparable with our reported LINAC results. We observe that LINAC successfully hinders much stronger attacks than these alternative strategies. 6. Conclusions In this work we introduce LINAC, a novel key-based de- fence using implicit neural representations, and demonstrate its effectiveness for hindering standard adversarial attacks on CIFAR-10 classiﬁers. We systematically attempt to cir- cumvent our defence by adapting a host of widely used attacks from the literature, including transfer and adaptive attacks, but LINAC maintains strong apparent robustness. Consequently, we challenge LINAC by introducing a novel adaptive attack strategy (PBA) which is indeed more suc- cessful at discovering adversarial examples. We also show that PBA can be used to completely invalidate an existing key-based defence. These are some of the latest attempts to leverage computational hardness for adversarial robustness, and successful PBA attacks on existing methods enable further progress. References Andriushchenko, M., Croce, F., Flammarion, N., and Hein, M. Square attack: a query-efﬁcient black-box adversarial attack via random search. In European Conference on Computer Vision, pp. 484–501. Springer, 2020. AprilPyone, M. and Kiya, H. An extension of encryption- inspired adversarial defense with secret keys against ad- versarial examples. In 2020 Asia-Paciﬁc Signal and In- formation Processing Association Annual Summit and Conference (APSIPA ASC), pp. 1369–1374. IEEE, 2020. AprilPyone, M. and Kiya, H. Block-wise image transfor- Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) mation with secret key for adversarially robust defense. IEEE Transactions on Information Forensics and Secu- rity, 16:2709–2723, 2021a. AprilPyone, M. and Kiya, H. Transfer learning-based arXiv preprint model protection with secret key. arXiv:2103.03525, 2021b. Athalye, A., Carlini, N., and Wagner, D. A. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In ICML, 2018. Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs. 2018. URL http://github.com/google/jax. Brendel, W., Rauber, J., and Bethge, M. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. In International Conference on Learning Representations, 2018. Carlini, N. and Wagner, D. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM workshop on artiﬁcial in- telligence and security, pp. 3–14, 2017a. Carlini, N. and Wagner, D. Towards evaluating the robust- In 2017 ieee symposium on ness of neural networks. security and privacy (sp), pp. 39–57. IEEE, 2017b. Cohen, J., Rosenfeld, E., and Kolter, Z. Certiﬁed adver- sarial robustness via randomized smoothing. In Interna- tional Conference on Machine Learning, pp. 1310–1320. PMLR, 2019. Croce, F. and Hein, M. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206–2216. PMLR, 2020. Das, N., Shanbhogue, M., Chen, S.-T., Hohman, F., Chen, L., Kounavis, M. E., and Chau, D. H. Keeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression. arXiv preprint arXiv:1705.02900, 2017. Das, N., Shanbhogue, M., Chen, S.-T., Hohman, F., Li, S., Chen, L., Kounavis, M. E., and Chau, D. H. Shield: Fast, practical defense and vaccination for deep learn- ing using jpeg compression. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 196–204, 2018. Dhillon, G. S., Azizzadenesheli, K., Lipton, Z. C., Bern- stein, J. D., Kossaiﬁ, J., Khanna, A., and Anandkumar, A. Stochastic activation pruning for robust adversarial defense. In International Conference on Learning Repre- sentations, 2018. Dziugaite, G. K., Ghahramani, Z., and Roy, D. M. A study of the effect of jpg compression on adversarial images. arXiv preprint arXiv:1608.00853, 2016. Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Tran, B., and Madry, A. Adversarial robustness as arXiv preprint a prior for learned representations. arXiv:1906.00945, 2019. Frostig, R., Johnson, M. J., and Leary, C. Compiling ma- chine learning programs via high-level tracing. Systems for Machine Learning, 2018. Garg, S., Jha, S., Mahloujifar, S., and Mohammad, M. Ad- versarially robust learning could leverage computational hardness. In Algorithmic Learning Theory, pp. 364–385. PMLR, 2020. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Gowal, S., Uesato, J., Qin, C., Huang, P., Mann, T. A., and Kohli, P. An alternative surrogate loss for pgd-based adversarial testing. CoRR, abs/1910.09338, 2019. URL http://arxiv.org/abs/1910.09338. Gowal, S., Qin, C., Uesato, J., Mann, T., and Kohli, P. Uncovering the limits of adversarial training against norm-bounded adversarial examples. arXiv preprint arXiv:2010.03593, 2020. Guo, C., Rana, M., Cisse, M., and van der Maaten, L. Coun- tering adversarial images using input transformations. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=SyJ7ClWCb. Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del R´ıo, J. F., Wiebe, M., Peterson, P., G´erard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. Nature, 585(7825):357–362, September 2020. doi: 10. 1038/s41586-020-2649-2. URL https://doi.org/ 10.1038/s41586-020-2649-2. He, W., Wei, J., Chen, X., Carlini, N., and Song, D. Adver- sarial example defenses: ensembles of weak defenses are not strong. In Proceedings of the 11th USENIX Confer- ence on Offensive Technologies, pp. 15–15, 2017. Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Hein, M. and Andriushchenko, M. Formal guarantees on the robustness of a classiﬁer against adversarial manipulation. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 2263–2273, 2017. Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., and Madry, A. Adversarial examples are not bugs, they are features. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Pro- cessing Systems, volume 32. Curran Associates, URL https://proceedings. Inc., neurips.cc/paper/2019/file/ e2c420d928d4bf8ce0ff2ec19b371514-Paper. pdf. 2019. Jacobsen, J.-H., Behrmann, J., Zemel, R., and Bethge, M. Excessive invariance causes adversarial vulnerability. In International Conference on Learning Representations, 2018. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In ICLR (Poster), 2015. Kurakin, A., Goodfellow, I., Bengio, S., Dong, Y., Liao, F., Liang, M., Pang, T., Zhu, J., Hu, X., Xie, C., et al. Adversarial attacks and defences competition. In The NIPS’17 Competition: Building Intelligent Systems, pp. 195–231. Springer, 2018. Liao, F., Liang, M., Dong, Y., Pang, T., Hu, X., and Zhu, J. Defense against adversarial attacks using high-level representation guided denoiser, 2018. Loshchilov, I. and Hutter, F. SGDR: stochastic gradient descent with restarts. CoRR, abs/1608.03983, 2016. URL http://arxiv.org/abs/1608.03983. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant In International Conference to adversarial attacks. on Learning Representations, 2018. URL https:// openreview.net/forum?id=rJzIBfZAb. MaungMaung, A. and Kiya, H. A protection method of trained cnn model with secret key from unauthorized access. arXiv preprint arXiv:2105.14756, 2021. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. Nerf: Representing scenes as neural radiance ﬁelds for view synthesis. In European conference on computer vision (ECCV), pp. 405–421. Springer, 2020. Niu, Z., Chen, Z., Li, L., Yang, Y., Li, B., and Yi, J. On the limitations of denoising strategies as adversarial defenses. CoRR, abs/2012.09384, 2020. URL https://arxiv. org/abs/2012.09384. Pang, T., Yang, X., Dong, Y., Su, H., and Zhu, J. Bag of tricks for adversarial training. In International Confer- ence on Learning Representations, 2020. Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., and Swami, A. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia conference on computer and communications secu- rity, pp. 506–519, 2017. Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed defenses against adversarial examples. In International Conference on Learning Representations, 2018. Ramachandran, P., Zoph, B., and Le, Q. V. Searching for activation functions. CoRR, abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941. Rebufﬁ, S.-A., Gowal, S., Calian, D. A., Stimberg, F., Wiles, O., and Mann, T. Fixing data augmentation to improve adversarial robustness. arXiv preprint arXiv:2103.01946, 2021. Schmidt, L., Santurkar, S., Tsipras, D., Talwar, K., and Madry, A. Adversarially robust generalization requires more data. In NeurIPS, 2018. Schott, L., Rauber, J., Bethge, M., and Brendel, W. Towards the ﬁrst adversarially robust neural network model on mnist. In Seventh International Conference on Learning Representations (ICLR 2019), pp. 1–16, 2019. Shaham, U., Garritano, J., Yamada, Y., Weinberger, E., Cloninger, A., Cheng, X., Stanton, K., and Kluger, Y. De- fending against adversarial images using basis functions transformations. arXiv preprint arXiv:1803.10840, 2018. Sitzmann, V. Awesome Implicit Representations - A cu- rated list of resources on implicit neural representations. 2020. URL https://github.com/vsitzmann/ awesome-implicit-representations. Sitzmann, V., Martel, J., Bergman, A., Lindell, D., and Wet- zstein, G. Implicit neural representations with periodic activation functions. Advances in Neural Information Processing Systems, 33, 2020. Skorokhodov, I., Ignatyev, S., and Elhoseiny, M. Adversarial generation of continuous images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10753–10764, 2021. Song, S., Chen, Y., Cheung, N.-M., and Kuo, C.-C. J. De- fense against adversarial attacks with saak transform. arXiv preprint arXiv:1808.01785, 2018a. Song, Y., Kim, T., Nowozin, S., Ermon, S., and Kushman, N. Pixeldefend: Leveraging generative models to understand Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) and defend against adversarial examples. In International Conference on Learning Representations, 2018b. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Er- han, D., Goodfellow, I., and Fergus, R. Intriguing properties of neural networks. In International Confer- ence on Learning Representations, 2014. URL http: //arxiv.org/abs/1312.6199. Tieleman, T. and Hinton, G. Lecture 6.5-rmsprop, coursera: Neural networks for machine learning. University of Toronto, Technical Report, 2012. Tramer, F., Carlini, N., Brendel, W., and Madry, A. On adap- tive attacks to adversarial example defenses. Advances in Neural Information Processing Systems, 33, 2020. Wong, E. and Kolter, J. Z. Provable defenses against adver- sarial examples via the convex outer adversarial polytope. In ICML, 2018. Yang, Y., Zhang, G., Katabi, D., and Xu, Z. Me-net: To- wards effective adversarial robustness with matrix estima- tion. In International Conference on Machine Learning, pp. 7025–7034. PMLR, 2019. Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo, Y. Cutmix: Regularization strategy to train strong classiﬁers with localizable features. CoRR, abs/1905.04899, 2019. URL http://arxiv.org/abs/1905.04899. Zagoruyko, S. and Komodakis, N. Wide residual networks. In British Machine Vision Conference 2016. British Ma- chine Vision Association, 2016. Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) A. LINAC Implementation Details A.1. Implicit Neural Representations A.1.1. RANDOM NUMBER GENERATION FOR INRS Our LINAC defence is fully deterministic by design. We used a random 64-bit signed integer as the private key, which seeded the state of the pseudo-random number generator in JAX (Frostig et al., 2018; Bradbury et al., 2018). The precise value of the private key used to train the defended model evaluated throughout this work was: −2314326399425823309. It was itself selected randomly, by initialising the random number generator of the NumPy library (Harris et al., 2020) with seed 42 and using the ﬁrst int64 integer. A.1.2. INPUT AND OUTPUT ENCODINGS Following Mildenhall et al. (2020) we use a positional encoding of pixel coordinates to a higher dimensional space to better capture higher-frequency information. Each pixel coordinate d is normalised to [−1, 1] and transformed as follows: γ(d) = [sin(20πd), cos(20πd), sin(21πd), cos(21πd), . . . , sin(2F −1πd), cos(2F −1πd)] (5) We used F = 5 frequencies in all our experiments and a L = 5 hidden layer MLP with H = 256 units per layer and ReLU non-linearities. Activations in the middle hidden layer were used for computing the LINAC transform, hence K = 2. As per standard practice for CIFAR-10 classiﬁcation, pixel colour intensities were scaled to have 0 mean across the training dataset and each colour channel separately. Intensities were then standardised to 1 standard deviation across the training dataset, independently across channels. A.1.3. FITTING Fitting the parameters of the implicit neural network was done using Adam (Kingma & Ba, 2015), with default parameters and a learning rate µ = 0.001. We used mini-batches with M = 32 random pixels and trained for N = 10 epochs. An epoch constitutes a pass through the entire set of pixels in the input image with dimensions I × J × C = 32 × 32 × 3 in random order. The total number of optimisation steps performed was 320. A cosine learning rate decay schedule was used for better convergence, with the minimum value of the multiplier α = 0.0001 (Loshchilov & Hutter, 2016). A.1.4. COMPUTATIONAL AND MEMORY REQUIREMENTS The LINAC transform’s computational complexity scales with the number of pixels (I · J) of the input image and the number of epochs through the pixels (N ). It takes I · J · N backward passes through the implicit network Φ to ﬁt its parameters φ. LINAC’s memory complexity is dominated by the number of parameters of the INR (|φ|). Empirically, the LINAC transform is itself as expensive as inference with a WideResNet-70-16 model (Zagoruyko & Komodakis, 2016) on CIFAR-10 images. A.2. Defended Classiﬁers Since the proposed input transformation preserves spatial structure, we perform image classiﬁcation using transformed inputs in an identical manner as with RGB colour images, except for the higher number of channels of transformed inputs. Hence, we employ a standard classiﬁcation pipeline following (Zagoruyko & Komodakis, 2016), using a WideResNet- 70-16 classiﬁer. We reiterate that our proposed transformation changes the number of input channels, but not the spatial dimensions. Hence, small differences between our models and other WideResNet-70-16 results reported in the literature could conceivably appear only due to different numbers of input channels. However, practically this leads to less than a 0.2% increase in the total number of model parameters, limited to the ﬁrst convolutional layer, which uses ﬁlters with 256 channels instead of 3. We used the Swish activation function proposed by Ramachandran et al. (2017) for all the classiﬁers. Training was performed with Nesterov Momentum SGD (Tieleman & Hinton, 2012) m = 0.9, using mini-batches of size 1024, for a total of 1000 epochs, or 48880 parameter updates. The initial learning rate was µ = 0.4, reduced by a factor of 10 four times, at epochs: 650, 800, 900 and 950. We performed a hyper-parameter sweep over the weight-decay scale with the following grid: {0., 0.0001, 0.0005, 0.0010}. We maintain an exponential moving average of classiﬁer parameters (with a decay rate of r = 0.995); we report accuracies using the ﬁnal average of classiﬁer parameters. Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) A.2.1. PERFORMANCE CONSIDERATIONS We use the CutMix data augmentation strategy of Yun et al. (2019) directly on RGB images from the training set of CIFAR- 10, prior to transforming them with LINAC. This has an impact on computational considerations, since pre-computing the transformed dataset ofﬂine in order to save training time becomes more challenging. For ease of prototyping we chose to implement LINAC as a preprocessing layer, which could have an impact on training time if used naively, but not if the transformation is applied asynchronously on the buffer of data feeding the device used for model training. We also found empirically that the proposed transformation renders itself to very effective parallelisation using modern SIMD devices, despite the fact that there is no parameter sharing between implicit models of different inputs; this is likely due to the ability of modern libraries such as JAX (Frostig et al., 2018) to vectorise operations across tensors holding parameters for many distinct neural networks. It is important to note that inference and training costs of defended classiﬁers are roughly double those of the nominal classiﬁer. Hence, the LINAC transform has comparable cost to inference with a WideResNet-70-16 model. B. Evaluation Details B.1. Attacks with Surrogate Models We provide a breakdown of evaluations using surrogate models initially reported in Section 5. We report the best 10 keys from the brute-force attack on the private key in Table 4. These keys were also used to train surrogate models defended with LINAC for use in transfer attacks, see Table 5 for complete results. Reconstruction-based surrogate models defended with modiﬁed LINAC, and using the same 10 best-guess attacker keys, were used to perform BPDA transfer attacks, reported in Table 6. Position Clean Test Accuracy (%) 1 2 3 4 5 6 7 8 9 10 57.00 55.00 55.00 55.00 55.00 55.00 54.00 54.00 54.00 54.00 Attacker Key 1383227977468296715 -3328443931658504707 -127094507362684985 -7808219206569127925 -8772667224621836765 -70640792831170485 8263151932495004089 -4594861196100637268 -6520968232434877967 -8722766234183220599 Table 4. Top 10 keys in brute-force key attack, also used to train surrogate models. LINAC Defence Norm Attack Name L∞ L2 AA MT PGD Square Best Known AA MT PGD Square Best Known Defended Surrogate Source Models (Attacker Keys) key1 93.05 89.70 88.05 83.37 82.22 91.20 90.29 88.99 87.84 86.51 key2 92.99 89.48 88.19 83.34 82.10 91.15 90.47 89.00 87.83 86.41 key3 93.06 89.63 88.20 83.60 82.43 91.24 90.30 88.92 87.77 86.59 key4 93.00 89.54 88.23 83.43 82.26 91.17 90.50 88.93 88.15 86.42 key5 93.01 89.59 88.24 83.06 81.93 91.18 90.54 88.93 87.88 86.33 key6 93.05 89.55 88.17 83.38 82.33 91.18 90.37 89.08 88.11 86.62 key7 93.00 89.36 88.06 83.41 82.24 91.20 90.20 88.97 88.19 86.50 key8 93.00 89.58 88.20 83.30 82.10 91.20 90.32 88.98 87.94 86.58 key9 93.15 89.69 88.29 83.36 82.32 91.21 90.56 88.95 87.85 86.58 key10 93.21 89.47 88.10 83.21 82.17 91.16 90.25 89.03 88.12 86.53 Best Adversary (against all models) 84.00 85.70 87.32 75.91 75.64 88.27 87.31 88.36 84.08 83.48 Table 5. CIFAR-10 test set robust accuracy (%) of a single LINAC defended classiﬁer w.r.t. a suite of L∞ and L2 transfer attacks, valid under our threat model, using surrogate classiﬁers defended with LINAC, but trained with attacker-chosen keys. The clean accuracy of our defended classiﬁer is 93.08%. Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) LINAC Defence Norm Attack Name L∞ L2 AA MT PGD Square Best Known AA MT PGD Square Best Known Reconstruction-Based Surrogate Source Models Using Attacker Keys key1 91.47 68.35 69.35 82.91 62.87 85.94 82.12 82.23 87.30 78.68 key2 91.24 69.13 71.57 82.98 64.59 86.29 82.27 82.77 87.34 79.15 key3 91.21 67.71 69.15 82.47 62.63 85.95 81.91 82.20 87.22 78.36 key4 91.31 68.63 70.55 82.92 63.65 86.51 82.13 82.60 87.40 78.93 key5 91.32 67.53 69.30 82.66 62.40 85.52 81.78 81.89 87.35 78.46 key6 91.35 69.07 70.46 82.80 64.02 86.16 82.42 82.33 87.44 79.08 key7 91.31 68.81 69.67 82.84 63.54 86.28 82.24 82.53 87.52 78.92 key8 91.63 67.98 69.48 82.91 62.93 86.34 82.07 82.38 87.21 78.83 key9 91.33 68.40 69.97 82.91 63.40 86.46 82.21 82.69 87.26 78.80 key10 91.46 69.24 70.85 82.93 64.15 86.27 81.96 82.68 87.46 78.46 Best Adversary (against all models) 59.40 55.37 56.00 69.14 51.17 74.59 74.98 75.00 83.26 71.89 Table 6. CIFAR-10 test set robust accuracy (%) of a single LINAC defended classiﬁer according to a suite of L∞ and L2 BPDA attacks, valid under our threat model, using reconstruction-based surrogate classiﬁers. B.2. Transfer Attacks with Adversarially Trained Models For mounting transfer attacks we have taken adversarially trained models from previous work (Rebufﬁ et al., 2021), with checkpoints available online1. These models have been adversarially trained on CIFAR-10 using additional synthetic generated data and CutMix data augmentation. To mount transfer attacks we use the WideResNet-106-16 model (trained to defend against L∞ norm-bounded perturbations of size (cid:15) = 8/255) and the WideResNet-70-16 model (trained to defend against L2 norm-bounded perturbations of size (cid:15) = 0.5). B.3. PBA Implementation Details B.3.1. PBA FOR LINAC We used a single convolutional layer (k = 3 × 3) with biases to implement hψ(x), the PBA of the nuisance transformation, mapping from the 3 RGB channels of input images to the H = 256 channels output by LINAC. The parameters ψ of the bypass approximation were trained by minimising the cross-entropy loss on the CIFAR-10 training set using Momentum SGD with a learning rate µ = 0.1. 100 epochs sufﬁced to optimise PBA parameters, with four learning rate reductions by a factor of 0.1 at epochs: 65, 80, 90, 95. B.3.2. PBA FOR BLOCK PIXEL SHUFFLE We implemented the Block Pixel Shufﬂe defence of AprilPyone & Kiya (2021a) using blocks of size 4 × 4, as recommended in the original work. We used the same private key value as that of our defended LINAC classiﬁer. The private key serves as the seed of a pseudo-random number generator, which is used to sample a permutation of all pixel positions in a block. The same permutation is applied to all blocks. We illustrate the transform in Figure 5. Figure 5. Example of Pixel Block Shufﬂe transformation (AprilPyone & Kiya, 2021a). An original CIFAR-10 image (left) is split into a grid of 4 × 4 blocks of adjacent pixels, and the same random permutation is used to shufﬂe pixel positions within every block (middle). The transformed image is constructed by spatially concatenating the blocks according to their original positions in the grid (right). A classiﬁer defended with Block Pixel Shufﬂe was trained with the same procedure as our defended LINAC classiﬁer. We can report a clean CIFAR-10 test set accuracy of 97.03%, which is higher to that reported by AprilPyone & Kiya (2021a), 1https://github.com/deepmind/deepmind-research/tree/master/adversarial_robustness Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) but consistent with the superior CutMix (Yun et al., 2019) data augmentation procedure we used for all defended classiﬁers. According its own “white-box” threat model (AprilPyone & Kiya, 2021a), all the implementation details of the defence are known to an attacker except the private key. We exploit the block structure and use a single linear layer without biases, and initialised with the identity mapping, to compute a parametric bypass approximation (PBA) for the this defence. We found that using a smaller initial learning rate µ = 0.001 results in stable convergence. We used 300 epochs to optimise PBA parameters, with four learning rate reductions by a factor of 0.1 at epochs: 275, 285, 290, 295. An extensive evaluation of the resulting defended classiﬁer is given in Table 7. We ﬁnd that transfer attacks which are agnostic to the defence can be more successful when adversarial examples are computed using robust source models, but one may infer some level of robustness. Using PBA attacks valid under the threat model (“white-box”) we successfully circumvent the defence, with a Best Known CIFAR-10 robust test-set accuracy of 0% under adversarial perturbations of size up to (cid:15) = 8/255 in L∞ norm, and up to (cid:15) = 0.5 in L2 norm. Block Pixel Shufﬂe Defence Transfer Attack Source Models Adaptive Attacks Best Adversary Norm Attack Name Nominal Source L∞ L2 AA MT PGD Square Best Known AA MT PGD Square Best Known 85.78 78.87 69.17 69.16 60.65 94.14 93.93 92.92 91.69 90.41 Adversarial Training (L∞) 69.09 56.49 39.05 46.25 30.61 90.02 92.25 87.54 88.80 85.29 Adversarial Training (L2) 73.86 27.72 31.19 42.63 21.17 83.35 77.02 77.80 84.88 69.00 PBA 0.18 0.00 0.00 0.00 0.00 0.02 0.00 0.02 6.13 0.00 All Source Models 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 6.13 0.00 Table 7. CIFAR-10 test set robust accuracy (%) of Block Pixel Shufﬂe approach (AprilPyone & Kiya, 2021a) against standard L∞ and L2 bounded attacks using both transfer and our novel PBA strategy. C. Sensitivity of LINAC to Hyper-Parameters We performed sensitivity analyses of LINAC to its hyper-parameters. For efﬁciency reasons we report robust accuracies according to untargeted PGD attacks with 100 steps and 10 restarts, using an adversarially trained robust model (L2) (Rebufﬁ et al., 2021) to generate adversarial perturbations. F = 3 F = 5 F = 7 F = 10 93.61 Clean Accuracy: L∞ PGD 43.51 L2 PGD 74.99 93.08 93.78 93.65 44.06 74.19 43.46 75.96 43.90 74.91 Figure 6. CIFAR-10 test-set clean and robust accuracies under transfer attacks of LINAC defended classiﬁers with different numbers of positional encoding frequencies F , keeping all other hyper-parameters constant. In Figure 6 we provide a sensitivity analysis across the number of frequencies F used for positional encoding (Mildenhall et al., 2020), keeping all other hyper-parameters the same. Note that we used F = 5 for our defended classiﬁer evaluated in ) % ( y c a r u c c a t e s - t s e t 100 90 80 70 60 50 40 30 20 10 0 3 4 clean accuracy L robust accuracy L2 robust accuracy 5 8 F (number of frequencies) 6 7 9 10 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) the main paper. L = 3 L = 4 L = 5 L = 6 L = 7 L = 8 95.21 Clean Accuracy: L∞ PGD 44.57 L2 PGD 80.40 94.30 93.08 91.70 91.20 90.41 44.55 77.16 44.06 74.19 43.70 71.95 43.81 71.22 43.81 70.08 Figure 7. CIFAR-10 test-set clean and robust accuracies (under transfer attacks) of LINAC defended classiﬁers with different numbers of implicit network layers L, keeping all other hyper-parameters ﬁxed. In Figure 7 we vary the number of implicit network layers L, keeping all other hyper-parameters the same, including the representation layer index K = 2 and number of epochs N = 10. Note that we used L = 5 for our defended classiﬁer evaluated in the main paper. K = 0 K = 1 K = 2 K = 3 K = 4 50.32 Clean Accuracy: L∞ PGD 36.63 L2 PGD 42.17 86.92 93.08 95.07 95.49 44.55 67.26 44.06 74.19 43.99 78.53 42.83 81.03 Figure 8. CIFAR-10 test-set clean and robust accuracies (under transfer attacks) of LINAC defended classiﬁers with different implicit network layers used to output representations (K), keeping all other hyper-parameters the same. In Figure 8 we change the index of the LINAC representation layer K, keeping all other hyper-parameters unchanged. Note that we used K = 2 for our defended classiﬁer evaluated in the main paper. In Figure 9 we analyse the sensitivity of LINAC to the number of epochs N , keeping all other hyper-parameters constant. Note that we used N = 10 for our defended classiﬁer evaluated in the main paper. D. Characterising the LINAC Transform In Figure 10 we plot learning curves characterising implicit network ﬁtting, as used for our defended classiﬁer. Mean and standard deviation of errors across independent learning processes for the entire CIFAR-10 test-set are plotted as functions of optimisation steps, using a log-scale for errors. The ﬁnal mean value of such errors is 0.04325, which conﬁrms that our LINAC approach leads to lossy representations. A histogram of ﬁnal sum squared errors for the entire test-set of CIFAR-10 is provided in Figure 11. For a qualitative evaluation of such statistics, we provide examples of original images, their reconstructions and difference images, using LINAC and the private key in Figure 12 and, for comparison, a different key in Figure 13. We observe that encoding errors using LINAC are key dependent. Furthermore, signiﬁcant amounts of information seem to be left out by LINAC. Some difference images could be recognised as the correct class, most likely due to high-frequency information which is not well represented. Finally, we provide a number of plots for qualitative comparisons of LINAC transforms. Figure 14 shows three different ) % ( y c a r u c c a t e s - t s e t 100 90 80 70 60 50 40 30 20 10 0 clean accuracy L robust accuracy L2 robust accuracy 3 4 5 6 7 8 L (number of hidden layers) ) % ( y c a r u c c a t e s - t s e t 100 90 80 70 60 50 40 30 20 10 0 0 clean accuracy L robust accuracy L2 robust accuracy 1 2 K (index of representation layer) 3 4 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) N = 5 N = 10 N = 15 N = 20 N = 25 N = 50 85.74 Clean Accuracy: L∞ PGD 49.01 L2 PGD 70.65 93.08 94.59 95.41 95.66 95.87 44.06 74.19 40.86 75.69 38.56 76.28 37.63 76.42 35.90 78.06 Figure 9. CIFAR-10 test-set clean and robust accuracies (under transfer attacks) of LINAC defended classiﬁers with implicit networks trained for N epochs, keeping all other hyper-parameters constant. Figure 10. Independent ﬁtting of implicit neural networks to CIFAR-10 test-set images in order to compute their LINAC transforms. Sum squared encoding errors, averaged over pixels, are plotted against ﬁtting steps. images encoded and their respective LINAC representations encoded as RGB channels. Figures 15, 16 and 17 plot LINAC transforms of the same respective images, but using with different keys, one on each RGB colour channel. ) % ( y c a r u c c a t e s - t s e t 100 90 80 70 60 50 40 30 20 10 0 5 10 15 clean accuracy L robust accuracy L2 robust accuracy 25 20 35 N (number of epochs) 30 40 45 50 l ) e a c s - g o l ( r o r r e 101 100 10 1 10 2 0 32 64 96 128 160 steps 192 224 256 288 320 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 11. Histogram of LINAC transform encoding errors plotted for the entire CIFAR-10 test-set. The overall mean value of such errors is 0.04325, which conﬁrms that our LINAC approach leads to lossy representations. mean error 1400 1200 1000 800 600 400 200 0 10 3 10 2 10 1 errors Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 12. Image approximations computed for LINAC with the private key, as used for our defended classiﬁer. Original images and labels are plotted in the ﬁrst column. Note that labels are not used for LINAC. Implicit network outputs are plotted in the second column. Difference images and sum squared errors, averaged over pixels, are plotted in the third column. Note that LINAC uses lossy image approximations. horse airplane frog truck dog automobile horse airplane cat bird 1 1 1 1 1 1 1 2 1 0 2 1 0 1 0 2 1 0 1 0 2 1 0 2 1 0 2 1 0 1 1.0 0.5 0.0 0.5 1.0 1.5 2 1 0 1 4.69e-02 3.95e-02 6.85e-02 4.23e-02 5.38e-02 6.74e-02 2.46e-02 2.44e-02 1.67e-02 3.44e-02 2 1 0 2 1 0 1 0 2 1 0 2 1 0 2 1 0 2 1 0 2 1 0 1 0 2 1 0 1 1 2 1 1 2 1 2 1 1 1 1 0.50 0.25 0.00 0.25 0.50 0.5 0.0 0.5 1.0 0.5 0.0 0.5 0.25 0.00 0.25 0.50 0.75 0.4 0.2 0.0 0.2 0.4 0.6 0.5 0.0 0.5 0.4 0.2 0.0 0.2 0.4 0.6 0.6 0.4 0.2 0.0 0.2 0.4 0.4 0.2 0.0 0.2 0.6 0.4 0.2 0.0 0.2 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 13. Image approximations computed for LINAC with a different, attacker chosen key. Original images and labels are plotted in the ﬁrst column. Note that labels are not used for LINAC. Implicit network outputs are plotted in the second column. Difference images and sum squared errors, averaged over pixels, are plotted in the third column. Note that LINAC leads to lossy image approximations which are key dependent. horse airplane frog truck dog automobile horse airplane cat bird 1 1 1 1 1 1 1 2 1 0 2 1 0 1 0 2 1 0 1 0 2 1 0 2 1 0 2 1 0 1 1.0 0.5 0.0 0.5 1.0 1.5 2 1 0 1 5.87e-02 3.79e-02 6.16e-02 5.00e-02 7.63e-02 7.23e-02 2.22e-02 3.57e-02 1.40e-02 5.16e-02 1 1 2 1 1 2 1 1 2 1 0 2 1 0 1 0 2 1 0 2 1 0 2 1 0 2 1 0 2 1 0 1 1.0 0.5 0.0 0.5 1.0 1.5 2 1 0 1 0.75 0.50 0.25 0.00 0.25 0.50 0.25 0.00 0.25 0.50 0.75 0.50 0.25 0.00 0.25 0.50 0.50 0.25 0.00 0.25 0.50 0.75 0.5 0.0 0.5 0.5 0.0 0.5 1.0 0.2 0.0 0.2 0.4 0.4 0.2 0.0 0.2 0.4 0.2 0.0 0.2 0.4 0.2 0.0 0.2 0.4 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 14. Comparing transforms of the 3 top images using LINAC with the private key, as done for our defended classiﬁer. The respective activation images with H = 256 channels were plotted in a 16 × 16 grid of slices of the same size with original images. Respective slices over the channel dimension of activation images were combined as RGB channels in this plot (bottom), in order to compare channel representations for the three input images (top). Each square in the grid represents the activations of a LINAC representation channel for all pixels in the original image. Different values of RGB signify differences in LINAC representations across images. 0 1 2 1 0 1 2 2 1 0 1 1 0 1 1 r e y a l n e d d h i 1 2 3 4 5 6 7 8 hidden unit position in layer 9 10 11 12 13 14 15 16 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 15. Comparing LINAC transforms of the same image using the private key and two other random keys. The respective activation images with H = 256 channels were plotted in a 16 × 16 grid of slices of the same size with original images. Respective slices over the channel dimension of resulting activation images were combined as RGB channels in this plot (bottom), in order to compare channel representations with three different keys for the same input image. Each square in the grid represents the activations of a LINAC representation channel for all pixels in the original image. Different values of RGB signify differences in LINAC representations across keys. 0 1 2 1 0 1 2 2 1 0 1 2 1 0 1 1 r e y a l n e d d h i 1 2 3 4 5 6 7 8 hidden unit position in layer 9 10 11 12 13 14 15 16 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 16. Comparing LINAC transforms of the same image using the private key and two other random keys. The respective activation images with H = 256 channels were plotted in a 16 × 16 grid of slices of the same size with original images. Respective slices over the channel dimension of resulting activation images were combined as RGB channels in this plot (bottom), in order to compare channel representations with three different keys for the same input image. Each square in the grid represents the activations of a LINAC representation channel for all pixels in the original image. Different values of RGB signify differences in LINAC representations across keys. 0 1 2 1 0 1 2 2 1 0 1 2 1 0 1 1 r e y a l n e d d h i 1 2 3 4 5 6 7 8 hidden unit position in layer 9 10 11 12 13 14 15 16 Hindering Adversarial Attacks with Implicit Network Activation Coding (LINAC) Figure 17. Comparing LINAC transforms of the same image using the private key and two other random keys. The respective activation images with H = 256 channels were plotted in a 16 × 16 grid of slices of the same size with original images. Respective slices over the channel dimension of resulting activation images were combined as RGB channels in this plot (bottom), in order to compare channel representations with three different keys for the same input image. Each square in the grid represents the activations of a LINAC representation channel for all pixels in the original image. Different values of RGB signify differences in LINAC representations across keys. 0 1 2 1 0 1 1 0 1 1 0 1 1 r e y a l n e d d h i 1 2 3 4 5 6 7 8 hidden unit position in layer 9 10 11 12 13 14 15 16","['hinder', 'adversarial', 'attack', 'implicit', 'representation', 'andrei', 'calian', 'gowal', 'hadsell', 'c', 'x', 'abstract', 'introduce', 'lossy', 'implicit', 'network', 'activa', 'tion', 'code', 'linac', 'defence', 'input', 'transfor', 'mation', 'successfully', 'hinder', 'several', 'com', 'adversarial', 'attack', 'cifar10', 'classiﬁer', 'perturbation', 'l∞', 'norm', 'norm', 'implicit', 'resentation', 'use', 'approximately', 'encode', 'pixel', 'colour', 'intensity', 'image', 'classiﬁer', 'train', 'transform', 'datum', 'appear', 'robustness', 'small', 'perturbation', 'adversarial', 'training', 'large', 'drop', 'performance', 'seed', 'random', 'number', 'generator', 'use', 'initialise', 'train', 'implicit', 'neural', 'tation', 'turn', 'necessary', 'information', 'strong', 'generic', 'attack', 'suggest', 'role', 'private', 'key', 'devise', 'parametric', 'bypass', 'proximation', 'pba', 'attack', 'strategy', 'keybased', 'defence', 'successfully', 'invalidate', 'ex', 'isting', 'method', 'category', 'interestingly', 'linac', 'defence', 'also', 'hinder', 'transfer', 'adaptive', 'attack', 'include', 'novel', 'pba', 'result', 'emphasise', 'importance', 'broad', 'range', 'customise', 'attack', 'appar', 'ent', 'robustness', 'accord', 'standard', 'evaluation', 'linac', 'source', 'code', 'parameter', 'defended', 'classiﬁer', 'evaluate', 'submission', 'available', 'introduction', 'train', 'deep', 'neural', 'network', 'dnn', 'classiﬁer', 'accurate', 'generally', 'robust', 'small', 'adversarial', 'perturba', 'tion', 'open', 'problem', 'computer', 'vision', 'inspire', 'much', 'empirical', 'foundational', 'research', 'modern', 'dnn', 'szegedy', 'show', 'dnn', 'inherently', 'robust', 'imperceptible', 'input', 'perturbation', 'reliably', 'cross', 'learn', 'decision', 'boundary', 'even', 'different', 'model', 'train', 'similar', 'datum', 'hindsight', 'correspondence', 'andrei', 'rusu', 'andreideepmindcom', 'proceeding', 'international', 'conference', 'machine', 'learn', 'copy', 'right', 'author', 'become', 'evident', 'related', 'distinct', 'design', 'prin', 'ciple', 'core', 'propose', 'defence', 'ever', 'intuitively', 'accurate', 'dnn', 'classiﬁer', 'consider', 'bust', 'practice', 'decision', 'boundary', 'largely', 'insensitive', 'adversarial', 'perturbation', 'com', 'put', 'successful', 'adversarial', 'perturbation', 'show', 'expensive', 'ideally', 'intractable', 'early', 'defence', 'build', 'principle', 'include', 'adversarial', 'training', 'approach', 'madry', 'veriﬁable', 'defence', 'hein', 'andriushchenko', 'raghunathan', 'many', 'recent', 'work', 'continually', 'reﬁne', 'algorithm', 'cohen', 'et', 'wide', 'range', 'defence', 'build', 'show', 'operate', 'largely', 'principle', 'include', 'adversarial', 'detection', 'method', 'carlini', 'wagner', 'formation', 'denoise', 'strategy', 'liao', 'et', 'many', 'approach', 'circumvent', 'effective', 'attack', 'propose', 'wagner', 'use', 'adaptive', 'attack', 'athalye', 'tramer', 'effectiveness', 'recent', 'attack', 'defence', 'garg', 'convincingly', 'argue', 'theo', 'retical', 'basis', 'principle', 'sound', 'similarly', 'cryp', 'tography', 'robust', 'learning', 'rely', 'computational', 'hard', 'ness', 'even', 'case', 'small', 'adversarial', 'perturbation', 'exist', 'find', 'hypothetical', 'computa', 'tionally', 'unbounded', 'adversary', 'however', 'construct', 'robust', 'classiﬁer', 'problem', 'interest', 'eg', 'image', 'clas', 'siﬁcation', 'remain', 'open', 'problem', 'recent', 'work', 'propose', 'defence', 'base', 'cryptographic', 'principle', 'pseudorandom', 'block', 'pixel', 'shufﬂe', 'approach', 'aprilpyone', 'kiya', '2021a', 'show', 'employ', 'cryptographic', 'principle', 'design', 'self', 'enough', 'prevent', 'efﬁcient', 'attack', 'nevertheless', 'build', 'concept', 'keybase', 'input', 'transformation', 'propose', 'novel', 'defence', 'base', 'implicit', 'sentation', 'inrs', 'demonstrate', 'lossy', 'implicit', 'neural', 'activation', 'code', 'linac', 'defence', 'hinder', 'standard', 'even', 'adaptive', 'attack', 'related', 'approach', 'test', 'make', 'claim', 'robustness', 'defended', 'classiﬁer', 'contribution', 'demonstrate', 'empirically', 'lossy', 'inrs', 'use', 'standard', 'cifar10', 'image', 'classiﬁca', 'tion', 'pipeline', 'compute', 'use', 'implicit', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'network', 'initialisation', 'novel', 'observation', 'make', 'linac', 'defence', 'possible', 'seed', 'random', 'ber', 'generator', 'use', 'initialise', 'computing', 'inrs', 'show', 'effective', 'compact', 'private', 'key', 'hold', 'information', 'hinder', 'suite', 'standard', 'adver', 'sarial', 'attack', 'widely', 'use', 'robustness', 'evaluation', 'report', 'systematic', 'effort', 'circumvent', 'linac', 'transfer', 'series', 'adaptive', 'attack', 'design', 'expose', 'exploit', 'potential', 'weakness', 'linac', 'end', 'propose', 'novel', 'parametric', 'bypass', 'proximation', 'attack', 'strategy', 'valid', 'threat', 'model', 'applicable', 'defence', 'use', 'secret', 'key', 'demonstrate', 'effectiveness', 'invalidate', 'exist', 'keybase', 'defence', 'previously', 'assume', 'robust', 'relate', 'work', 'adversarial', 'robustness', 'much', 'progress', 'make', 'robust', 'image', 'classiﬁer', 'adversarial', 'train', 'e', 'madry', 'route', 'extensively', 'explore', 'well', 'review', 'eg', 'schott', 'pang', 'et', 'et', 'et', 'approach', 'effective', 'current', 'tack', 'complementary', 'line', 'work', 'investigate', 'certiﬁed', 'defence', 'offer', 'guarantee', 'robustness', 'ex', 'ample', 'well', 'deﬁne', 'set', 'kolter', 'cohen', 'indeed', 'many', 'work', 'acknowledge', 'need', 'complementary', 'proache', 'irrespective', 'success', 'adversarial', 'training', 'well', 'understand', 'difﬁcultie', 'combine', 'method', 'proliﬁc', 'work', 'defence', 'adversarial', 'perturbation', 'spur', 'development', 'strong', 'attack', 'wagner', 'brendel', 'andriushchenko', 'et', 'standardisation', 'evaluation', 'strategy', 'threat', 'model', 'interest', 'athalye', 'croce', 'hein', 'include', 'adaptive', 'attack', 'tramer', 'empirical', 'progress', 'ward', 'build', 'robust', 'predictor', 'line', 'research', 'yield', 'improved', 'understanding', 'current', 'deep', 'learn', 'ing', 'model', 'ilyas', 'et', 'limitation', 'effective', 'adversarial', 'robustness', 'technique', 'jacobsen', 'datum', 'require', 'train', 'schmidt', 'athalye', 'show', 'number', 'defence', 'pri', 'marily', 'hinder', 'gradientbase', 'adversarial', 'attack', 'obfu', 'cat', 'gradient', 'various', 'form', 'identiﬁe', 'dient', 'shatter', 'goodfellow', 'mask', 'explode', 'vanish', 'gradient', 'song', 'stochastic', 'gradient', 'dhillon', 'number', 'input', 'transformation', 'aim', 'tere', 'adversarial', 'example', 'include', 'noise', 'ﬁltere', 'proache', 'use', 'pca', 'image', 'quilting', 'guo', 'saak', 'transform', 'song', 'ﬁltering', 'shaham', 'matrix', 'estimation', 'compression', 'indeed', 'many', 'defence', 'pro', 'pose', 'review', 'niu', 'rank', 'highly', 'competition', 'kurakin', 'many', 'show', 'less', 'robust', 'previously', 'think', 'eg', 'tramer', 'use', 'adaptive', 'attack', 'demonstrate', 'several', 'input', 'transformation', 'offer', 'little', 'robustness', 'build', 'insight', 'worth', 'identify', 'ingre', 'dient', 'essential', 'success', 'adversarial', 'attack', 'effective', 'attack', 'include', 'adaptive', 'one', 'assume', 'ability', 'approximate', 'output', 'target', 'model', 'arbi', 'trary', 'input', 'reasonable', 'apply', 'correct', 'transformation', 'tractable', 'attacker', 'hence', 'deny', 'e', 'access', 'computation', 'seem', 'promising', 'direction', 'hinder', 'adversarial', 'attack', 'aprilpyone', 'kiya', 'borrow', 'standard', 'practice', 'cryptography', 'assume', 'attacker', 'full', 'knowledge', 'defence', 'parameter', 'short', 'small', 'number', 'bit', 'make', 'private', 'key', 'critical', 'weakness', 'put', 'denoise', 'defence', 'approximate', 'identity', 'mapping', 'purpose', 'compute', 'gra', 'dient', 'athalye', 'even', 'complex', 'parametric', 'approach', 'learn', 'stochastic', 'generative', 'model', 'input', 'distribution', 'susceptible', 'reparameterisation', 'expectationovertransformation', 'eot', 'attack', 'whitebox', 'setting', 'thus', 'worth', 'investigate', 'nonparametric', 'lossy', 'fully', 'deterministic', 'input', 'transfor', 'mation', 'exist', 'downstream', 'model', 'still', 'perform', 'task', 'interest', 'high', 'accuracy', 'know', 'novel', 'attack', 'strategy', 'either', 'rule', 'least', 'substantially', 'hinder', 'include', 'adaptive', 'attack', 'implicit', 'neural', 'representation', 'neural', 'network', 'use', 'parameterise', 'many', 'kind', 'signal', 'see', 'work', 'extensive', 'list', 'remark', 'able', 'recent', 'advance', 'scene', 'representation', 'mildenhall', 'image', 'processing', 'sitzmann', 'inrs', 'use', 'isolation', 'image', 'scene', 'generalisation', 'image', 'exception', 'exist', 'unsupervised', 'learn', 'parameterise', 'gan', 'decoder', 'directly', 'output', 'inrs', 'image', 'rather', 'colour', 'intensity', 'pixel', 'paper', 'show', 'inrs', 'use', 'discover', 'functional', 'decomposition', 'rgb', 'image', 'enable', 'comparable', 'generalisation', 'learn', 'original', 'signal', 'encoding', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'figure', 'visual', 'depiction', 'propose', 'input', 'transformation', 'rgb', 'image', 'convert', 'activation', 'image', 'identical', 'spatial', 'dimension', 'h', 'channel', 'instead', 'neural', 'network', 'model', 'map', 'pixel', 'coordinate', 'colour', 'intensity', 'approximate', 'result', 'model', 'parameter', 'ﬁtte', 'call', 'implicit', 'neural', 'representation', 'image', 'order', 'output', 'correct', 'colour', 'intensity', 'pixel', 'implicit', 'neural', 'network', 'need', 'compute', 'hierarchical', 'functional', 'decomposition', 'empirically', 'choose', 'intermediate', 'representation', 'deﬁne', 'transformation', 'activation', 'hide', 'layer', 'associate', 'correspond', 'pixel', 'coordinate', 'form', 'output', 'activation', 'image', 'tx', 'many', 'channel', 'unit', 'middle', 'layer', 'h', 'hinder', 'adversarial', 'attack', 'implicit', 'neural', 'representation', 'section', 'introduce', 'linac', 'propose', 'input', 'transformation', 'hinder', 'adversarial', 'attack', 'leverag', 'e', 'implicit', 'neural', 'representation', 'also', 'illustrate', 'fig', 'setup', 'consider', 'supervised', 'learning', 'task', 'dataset', '×', 'pair', 'image', 'correspond', 'ing', 'label', 'use', 'deterministic', 'input', 'transformation', 'h', 'transform', 'input', 'image', 'preserve', 'spatial', 'dimension', 'far', 'consider', 'classiﬁer', 'parameterise', 'parameter', 'estimate', 'empirical', 'risk', 'minimisation', 'erm', 'map', 'transform', 'input', 'label', 'model', 'adversarially', 'train', 'yet', 'ﬁnde', 'adversarial', 'example', 'hinder', 'linac', 'demonstrate', 'extensive', 'evaluation', 'section', 'implicit', 'neural', 'representation', 'image', 'plicit', 'neural', 'representation', 'give', 'multilayer', 'ceptron', '◦', '◦', 'h0', 'hide', 'layer', 'map', 'spatial', 'coordinate', 'correspond', 'colour', 'φφ', 'solution', 'implicit', 'equation', 'p', 'spatial', 'coordinate', 'pixel', 'location', 'xp', 'correspond', 'image', 'colour', 'input', 'tran', 'formation', 'leverage', 'implicit', 'neural', 'representation', 'encode', 'image', 'approximate', 'manner', 'reconstruction', 'loss', 'implicit', 'equation', 'translate', 'standard', 'recon', 'struction', 'loss', 'image', 'colour', 'output', 'multilayer', 'perceptron', 'φφ', 'pixel', 'location', 'pij', 'φφpij', '−', 'xpij2', 'algorithm', 'linac', 'transform', 'input', 'rgb', 'image', 'size', '×', '×', 'private', 'key', 'number', 'epoch', 'size', 'number', 'l', 'representation', 'layer', 'rate', 'output', 'activation', 'image', 'tx', 'size', '×', '×', 'init', 'prngprivate', 'key', 'seed', 'init', 'mlprng', 'l', 'φ0', 'num', 'minibatche', 'shuffle', 'split', '−', '−', 'xpij2', '−', 'end', 'end', 'φ', 'return', 'apply', 'eq', 'use', 'ˆφx', 'layer', 'provide', 'pseudocode', 'linac', 'transform', 'discussion', 'computational', 'memory', 'requirement', 'appendix', 'a14', 'individual', 'age', 'estimate', 'ˆφx', 'approximate', 'local', 'minimiser', 'use', 'stochastic', 'iterative', 'minimisation', 'procedure', 'minibatche', 'pixel', 'group', 'epoch', 'cover', 'entire', 'image', 'random', 'order', 'total', 'pass', 'pixel', 'private', 'key', 'random', 'number', 'generator', 'use', 'generate', 'initial', 'parameter', 'φ0', 'decid', 'ing', 'random', 'subset', 'pixel', 'make', 'minibatche', 'epoch', 'random', 'number', 'generator', 'seed', '64bit', 'integer', 'keep', 'secret', 'denote', 'key', 'hence', 'input', 'start', 'independent', 'optimisation', 'set', 'initial', 'parameter', 'φ0', 'use', 'shufﬂing', 'pixel', 'epoch', 'pixeli', 'intensity', 'r', 'image', 'implicit', 'neural', 'representation', 'pij', 'r', 'b', 'activation', 'image', 'representation', 'intensity', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'lossy', 'implicit', 'network', 'activation', 'code', 'linac', 'consider', 'lossy', 'encoding', 'pixel', 'age', 'hdimensional', 'intermediate', 'activation', 'vector', 'mlp', 'evaluate', 'pixel', 'position', 'pij', 'l', 'build', 'lossy', 'implicit', 'network', 'activation', 'code', 'transformation', 'image', 'x', 'stack', 'together', 'encoding', 'pixel', 'image', 'grid', 'concatenate', 'feature', 'dimension', 'axis', 'linac', 'transformation', '×', '×', 'image', 'x', 'give', '◦', '◦', 'h0', '\uf8ee', '\uf8ef', '\uf8f0', 'cxi', '−', 'cxi', '−', '−', '\uf8fb', 'dimensionality', '×', '×', 'h', 'h', 'number', 'output', 'kth', 'layer', 'mlp', 'construction', 'input', 'transformation', 'preserve', 'spatial', 'dimension', 'image', 'increase', 'feature', 'dimensionality', 'image', 'original', 'number', 'colour', 'channel', 'h', 'mean', 'standard', 'network', 'architecture', 'use', 'image', 'classiﬁcation', 'eg', 'convolutional', 'model', 'readily', 'train', 'classiﬁer', 'omit', 'implementation', 'detail', 'provide', 'sensitivity', 'analysis', 'linac', 'hyper', 'parameter', 'report', 'threat', 'model', 'interested', 'hinder', 'adversarial', 'attack', 'nominallytraine', 'classiﬁer', 'fθtx', 'operate', 'transformed', 'input', 'rather', 'x', 'use', 'private', 'key', 'choosing', 'next', 'describe', 'threat', 'model', 'interest', 'state', 'condition', 'linac', 'defence', 'mean', 'hinder', 'adversarial', 'attack', 'follow', 'aprilpyone', 'kiya', 'assume', 'attacker', 'access', 'private', 'key', 'integer', 'seed', 'random', 'number', 'generator', 'use', 'compute', 'linac', 'transformation', 'otherwise', 'full', 'algorithmic', 'knowledge', 'approach', 'speciﬁcally', 'assume', 'attacker', 'complete', 'information', 'classiﬁcation', 'pipeline', 'include', 'architecture', 'training', 'dataset', 'weight', 'defended', 'classiﬁer', 'include', 'full', 'knowledge', 'linac', 'implicit', 'net', 'work', 'architecture', 'parameter', 'initialisation', 'scheme', 'ﬁtte', 'detail', 'private', 'key', 'attack', 'linac', 'defence', 'setup', 'interested', 'evaluate', 'apparent', 'bustness', 'linacdefended', 'fˆθ', 'train', 'erm', 'classify', 'transform', 'input', 'dataset', 'speciﬁcally', 'parameter', 'ˆθ', 'lce', 'crossentropy', 'loss', 'linac', 'transformation', 'apply', 'image', 'use', 'private', 'key', 'input', 'perturbation', 'classiﬁer', 'defend', 'linac', 'adversarially', 'train', 'madry', 'increase', 'robustness', 'speciﬁc', 'normbounded', 'input', 'pertur', 'bation', 'furthermore', 'linac', 'defence', 'inherently', 'agnostic', 'particular', 'notion', 'maximum', 'input', 'pertur', 'bation', 'nevertheless', 'provide', 'result', 'comparable', 'broad', 'set', 'defence', 'literature', 'perform', 'eval', 'uation', 'standard', 'normbounde', 'input', 'perturbation', 'maximum', 'perturbation', 'radius', 'l∞', 'norm', 'l2', 'norm', 'adapt', 'exist', 'attack', 'access', 'attacker', 'compute', 'linac', 'transfor', 'mation', 'exactly', 'however', 'attacker', 'acquire', 'cess', 'model', 'inference', 'attempt', 'bruteforce', 'guess', 'private', 'key', 'option', 'train', 'gate', 'model', 'linac', 'use', 'key', 'choose', 'tacker', 'hope', 'decision', 'boundary', 'model', 'similar', 'enough', 'mount', 'effective', 'transfer', 'attack', 'advanced', 'attacker', 'modify', 'linac', 'able', 'strong', 'backward', 'pass', 'differentiable', 'approximation', 'bpda', 'athalye', 'attack', 'evaluate', 'success', 'standard', 'attack', 'section', 'design', 'adaptive', 'attack', 'athalye', 'provide', 'excellent', 'set', 'guideline', 'design', 'perform', 'e', 'successful', 'adaptive', 'attack', 'also', 'standardise', 'result', 'report', 'aggregation', 'particular', 'interest', 'defence', 'base', 'input', 'transformation', 'bpda', 'expectationovertransformation', 'attack', 'strategy', 'subsequent', 'work', 'convincingly', 'argue', 'adaptive', 'attack', 'mean', 'general', 'customise', 'adapt', 'defence', 'turn', 'tramer', 'eot', 'generate', 'strong', 'attack', 'input', 'transformation', 'rely', 'able', 'compute', 'forward', 'transformation', 'approximate', 'sample', 'indeed', 'author', 'mention', 'substitution', 'forward', 'backward', 'pass', 'approximation', 'lead', 'completely', 'ineffective', 'much', 'less', 'effective', 'attack', 'parametric', 'bypass', 'approximation', 'pba', 'inspire', 'reparameterisation', 'strategy', 'athalye', 'propose', 'bespoke', 'attack', 'make', 'use', 'several', 'piece', 'information', 'available', 'threat', 'model', 'parametric', 'form', 'defended', 'classiﬁer', 'fθtx', 'training', 'dataset', 'loss', 'function', 'lce', 'train', 'weight', 'parametric', 'bypass', 'approximation', 'unknown', 'nui', 'sance', 'transformation', 'u', 'h', 'surrogate', 'parametric', 'function', 'h', 'parameterise', 'solution', 'follow', 'optimisation', 'problem', 'cid2', 'lcefˆθhψx', 'formulation', 'seek', 'set', 'parameter', 'ψ∗', 'imise', 'original', 'classiﬁcation', 'loss', 'keep', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'fended', 'classiﬁer', 'parameter', 'freeze', 'ˆθ', 'similar', 'classiﬁer', 'training', 'optimisation', 'problem', 'solve', 'efﬁciently', 'use', 'stochastic', 'gradient', 'descent', 'sgd', 'pba', 'adversarial', 'attack', 'proceed', 'approximat', 'e', 'defended', 'bypass', 'classiﬁer', 'fˆθhψ∗', 'forward', 'backward', 'pass', 'compute', 'adversarial', 'example', 'eg', 'use', 'project', 'gradient', 'descent', 'pgd', 'main', 'advantage', 'pba', 'strategy', 'ward', 'pass', 'nuisance', 'transformation', 'u', 'require', 'admit', 'efﬁcient', 'computation', 'many', 'attack', 'fˆθ', 'include', 'gradientbased', 'one', 'section', 'demonstrate', 'effectiveness', 'pba', 'linac', 'defence', 'show', 'even', 'surrogate', 'transfor', 'mation', 'ﬁt', 'training', 'datum', 'defended', 'classiﬁer', 'operate', 'sample', 'pass', 'hψ∗', 'bypass', 'u', 'demonstrate', 'nearly', 'identical', 'generalisation', 'test', 'set', 'furthermore', 'also', 'show', 'pba', 'great', 'success', 'ﬁnde', 'adversarial', 'example', 'linac', 'defence', 'com', 'pare', 'method', 'lastly', 'use', 'pba', 'invalidate', 'exist', 'keybase', 'defence', 'propose', 'literature', 'result', 'evaluation', 'methodology', 'make', 'assumption', 'adversarial', 'turbation', 'able', 'evaluate', 'single', 'defended', 'classi', 'model', 'attack', 'strategy', 'consider', 'contrast', 'much', 'adversarial', 'training', 'research', 'madry', 'obtain', 'comprehensive', 'picture', 'apparent', 'robust', 'ness', 'start', 'rigorous', 'evaluation', 'methodology', 'use', 'gowal', 'perform', 'untargeted', 'pgd', 'attack', 'step', 'randomise', 'restart', 'well', 'multitargete', 'pgd', 'attack', 'use', 'step', 'restart', 'anticipate', 'danger', 'ob', 'fuscate', 'gradient', 'skew', 'result', 'also', 'evaluate', 'square', 'approach', 'andriushchenko', 'et', 'powerful', 'gradientfree', 'attack', 'evaluation', 'restart', 'precise', 'comparison', 'broad', 'liter', 'ature', 'also', 'report', 'evaluation', 'use', 'autoattack', 'aa', 'strategy', 'croce', 'hein', 'follow', 'athalye', 'aggregate', 'result', 'attack', 'count', 'accurate', 'robust', 'prediction', 'test', 'image', 'defended', 'classiﬁer', 'predict', 'correct', 'class', 'adversarial', 'perturbation', 'compute', 'use', 'method', 'report', 'well', 'know', 'robust', 'accuracy', 'instance', 'several', 'surrogate', 'model', 'use', 'compute', 'adversarial', 'perturbation', 'also', 'know', 'transfer', 'attack', 'report', 'good', 'adversary', 'result', 'aggregate', 'individual', 'attack', 'deﬁne', 'robust', 'accuracy', 'figure', 'result', 'direct', 'attack', 'private', 'key', 'histogram', 'accuracy', 'defended', 'classiﬁer', 'input', 'transform', 'use', 'correct', 'key', 'randomly', 'choose', 'key', 'appropriate', 'surrogate', 'transformation', 'find', 'invalidate', 'attack', 'vector', 'rely', 'access', 'output', 'defended', 'model', 'attacker', 'choose', 'input', 'source', 'model', 'consider', 'aggregate', 'evaluation', 'dimension', 'tack', 'surrogate', 'model', 'provide', 'single', 'robust', 'curacy', 'number', 'attack', 'compute', 'use', 'source', 'model', 'standard', 'convention', 'maximum', 'perturba', 'tion', 'norm', 'enable', 'easy', 'comparison', 'result', 'literature', 'attack', 'surrogate', 'transformation', 'model', 'majority', 'adversarial', 'attack', 'strategy', 'critically', 'depend', 'approximate', 'output', 'defended', 'classiﬁer', 'input', 'choose', 'attacker', 'private', 'key', 'keep', 'secret', 'threat', 'model', 'mean', 'attacker', 'compute', 'precise', 'input', 'transformation', 'use', 'train', 'defended', 'classiﬁer', 'output', 'novel', 'datum', 'hence', 'attacker', 'ﬁnd', 'appropriate', 'surrogate', 'transformation', 'surrogate', 'classiﬁer', 'model', 'order', 'perform', 'effective', 'adversarial', 'attack', 'investigate', 'strategy', 'firstly', 'empirically', 'check', 'output', 'fend', 'classiﬁer', 'usefully', 'approximate', 'knowledge', 'private', 'key', 'reasonable', 'hypothesise', 'transformation', 'different', 'key', 'lead', 'lar', 'functional', 'representation', 'input', 'signal', 'start', 'investigate', 'hypothesis', 'simply', 'compute', 'accu', 'racy', 'defended', 'model', 'clean', 'input', 'datum', 'transform', 'linac', 'use', 'key', 'choose', 'attacker', 'also', 'know', 'bruteforce', 'key', 'attack', 'valid', 'threat', 'model', 'report', 'figure', 'accuracy', 'linac', 'defend', 'classiﬁer', 'test', 'input', 'transform', 'correct', 'private', 'key', 'attempt', 'ﬁnd', 'surrogate', 'transformation', 'key', 'pick', 'uniformly', 'random', 'key', 'independently', 'evaluate', 'accuracy', 'classiﬁer', 'use', 'batch', 'test', 'example', 'report', 'result', 'accuracy', 'estimate', 'key', 'histogram', 'plot', 'mean', 'accuracy', 'random', 'key', 'guess', 'around', 'top', 'accuracy', 'see', 'table', 'b1', 'breakdown', 'hence', 'use', 'linac', 'incorrect', 'key', 'lead', 'poor', 'approximation', 'classiﬁer', 'output', 'correctly', 'transform', 'datum', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'norm', 'attack', 'nominal', 'source', 'aa', 'square', 'well', 'know', 'aa', 'square', 'well', 'know', 'l2', 'transfer', 'attack', 'source', 'model', 'adversarial', 'training', 'l∞', 'adversarial', 'training', 'l2', 'defend', 'surrogate', 'reconstruction', 'base', 'surrogate', 'good', 'source', 'model', 'table', 'cifar10', 'test', 'set', 'robust', 'accuracy', 'single', 'linac', 'defend', 'classiﬁer', 'accord', 'suite', 'l∞', 'l2', 'transfer', 'attack', 'valid', 'threat', 'model', 'use', 'various', 'source', 'classiﬁer', 'generate', 'adversarial', 'perturbation', 'suggest', 'learn', 'decision', 'boundary', 'defended', 'classiﬁer', 'invariant', 'private', 'key', 'use', 'linac', 'figure', 'plot', 'cifar10', 'testset', 'robust', 'accuracy', 'estimate', 'well', 'know', 'number', 'attackertraine', 'surrogate', 'model', 'also', 'plot', 'clean', 'accuracy', 'reference', 'ﬁnd', 'useful', 'surrogate', 'transformation', 'random', 'guess', 'still', 'possible', 'transformation', 'different', 'key', 'preserve', 'largely', 'input', 'tion', 'second', 'option', 'attacker', 'check', 'decision', 'boundary', 'model', 'defend', 'linac', 'different', 'key', 'fact', 'similar', 'enable', 'powerful', 'transfer', 'attack', 'surrogate', 'model', 'end', 'independent', 'model', 'defend', 'linac', 'train', 'scratch', 'use', 'different', 'key', 'choose', 'attacker', 'use', 'promising', 'key', 'bruteforce', 'key', 'attack', 'purpose', 'figure', 'report', 'well', 'know', 'robust', 'accuracy', 'plot', 'num', 'ber', 'surrogate', 'model', 'use', 'joint', 'attack', 'aggregate', 'result', 'attacking', 'model', 'fourth', 'column', 'table', 'however', 'attack', 'vector', 'limit', 'success', 'transfer', 'attack', 'surrogate', 'model', 'robust', 'accuracy', 'defended', 'classiﬁer', 'appear', 'high', 'pgd', 'fail', 'vanish', 'explode', 'gradient', 'athalye', 'gradientfree', 'attack', 'suffer', 'issue', 'robust', 'accuracy', 'estimate', 'accord', 'high', 'individual', 'surrogate', 'model', 'irrespec', 'tive', 'perturbation', 'norm', 'complete', 'breakdown', 'result', 'give', 'table', 'b1', 'attack', 'surrogate', 'model', 'together', 'robust', 'accuracy', 'square', 'still', 'high', 'estimate', 'improve', 'far', 'aggregate', 'attack', 'evidence', 'far', 'support', 'hypothesis', 'decision', 'boundary', 'classiﬁer', 'defend', 'depend', 'respective', 'key', 'differ', 'enough', 'key', 'hinder', 'transfer', 'attack', 'gate', 'invest', 'order', 'magnitude', 'computation', 'attack', 'lead', 'modest', 'reduction', 'apparent', 'robustness', 'lastly', 'attacker', 'strive', 'employ', 'effective', 'general', 'strategy', 'defence', 'use', 'nuisance', 'transformation', 'attack', 'require', 'ability', 'compute', 'exact', 'tion', 'ﬁnde', 'usefully', 'differentiable', 'approximation', 'say', 'transformation', 'use', 'backwards', 'pass', 'gradientbase', 'attack', 'many', 'case', 'enough', 'allow', 'attacker', 'compute', 'adversarial', 'example', 'hap', 'somewhat', 'high', 'computational', 'cost', 'athalye', 'tramer', 'linac', 'defence', 'present', 'challenge', 'design', 'exact', 'forward', 'computation', 'model', 'inference', 'require', 'private', 'key', 'attacker', 'exactly', 'compute', 'input', 'transformation', 'even', 'train', 'set', 'image', 'eg', 'order', 'differentiable', 'parametric', 'approximation', 'learn', 'supervised', 'fashion', 'surrogate', 'model', 'defend', 'use', 'linac', 'attacker', 'choose', 'key', 'appear', 'usefully', 'differentiable', 'suggest', 'result', 'table', 'nevertheless', 'attacker', 'still', 'hope', 'defence', 'ﬁlter', 'information', 'largely', 'keyagnostic', 'manner', 'choice', 'implicit', 'network', 'tion', 'layer', 'essential', 'hence', 'option', 'modify', 'linac', 'output', 'activation', 'last', 'rather', 'middle', 'layer', 'implicit', 'network', 'amount', 'r', 'u', 'c', 'u', 'r', 'e', 'none', 'surrogate', 'l2', 'surrogate', 'l', 'number', 'surrogate', 'model', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'norm', 'attack', 'aa', 'square', 'well', 'know', 'aa', 'square', 'well', 'know', 'source', 'model', 'transfer', 'adaptive', 'attack', 'table', 'cifar10', 'test', 'set', 'robust', 'accuracy', 'single', 'linac', 'defend', 'wrt', 'suite', 'l∞', 'l2', 'attack', 'valid', 'threat', 'model', 'use', 'different', 'strategy', 'transfer', 'adaptive', 'attack', 'novel', 'pba', 'adaptive', 'attack', 'overall', 'effective', 'transfer', 'bpda', 'attack', 'strategy', 'reduce', 'linac', 'approximate', 'reconstruction', 'original', 'signal', 'surrogate', 'model', 'tacker', 'choose', 'key', 'still', 'train', 'purpose', 'vulnerable', 'strong', 'bpda', 'attack', 'transfer', 'well', 'defended', 'ent', 'robustness', 'estimate', 'accord', 'transfer', 'bpda', 'attack', 'plot', 'figure', 'function', 'number', 'surrogate', 'model', 'use', 'jointly', 'attack', 'ﬁfth', 'column', 'table', 'provide', 'aggregate', 'apparent', 'robust', 'accuracy', 'use', 'surrogate', 'show', 'transfer', 'bpda', 'attack', 'successful', 'previous', 'attempt', 'reconstructionbased', 'surrogate', 'model', 'use', 'reveal', 'robust', 'accuracy', 'defended', 'classiﬁer', 'high', 'particularly', 'standard', 'l∞', 'multitargete', 'attack', 'see', 'table', 'b1', 'detailed', 'breakdown', 'result', 'interestingly', 'surrogate', 'model', 'use', 'together', 'robust', 'accuracy', 'mate', 'drop', 'reduction', 'less', 'severe', 'standard', 'l2', 'attack', 'accuracy', 'surrogate', 'appear', 'still', 'result', 'conﬁrm', 'bpda', 'strategy', 'valuable', 'tool', 'investigate', 'robustness', 'wide', 'range', 'defence', 'even', 'assumption', 'fully', 'meet', 'transfer', 'attack', 'nominal', 'adversarially', 'train', 'source', 'model', 'defended', 'classiﬁer', 'adversarially', 'train', 'one', 'assume', 'decision', 'boundary', 'simi', 'lar', 'nominal', 'undefended', 'classiﬁer', 'show', 'ﬁrst', 'column', 'table', 'transfer', 'attack', 'nominally', 'train', 'source', 'model', 'limit', 'success', 'pecially', 'consider', 'undefended', 'classiﬁer', 'chance', 'robust', 'accuracy', 'accord', 'evaluation', 'possibility', 'defended', 'model', 'susceptible', 'promising', 'attack', 'direction', 'adversarially', 'train', 'robust', 'classiﬁer', 'vulnerable', 'report', 'second', 'third', 'column', 'table', 'indeed', 'case', 'extent', 'adversary', 'consider', 'thus', 'far', 'robust', 'model', 'adversarially', 'train', 'tolerate', 'perturbation', 'size', 'norm', 'lead', 'effective', 'transfer', 'attack', 'hold', 'less', 'degree', 'adversarially', 'train', 'model', 'perturbation', 'size', 'l∞', 'norm', 'success', 'evaluation', 'use', 'former', 'source', 'model', 'attack', 'method', 'come', 'close', 'effectiveness', 'joint', 'strategy', 'report', 'well', 'know', 'robust', 'accuracy', 'furthermore', 'important', 'note', 'ensemble', 'transfer', 'attack', 'much', 'strong', 'compute', 'give', 'source', 'model', 'aggregate', 'attack', 'type', 'different', 'source', 'model', 'robust', 'accuracy', 'linac', 'defend', 'classiﬁer', 'reveal', 'half', 'initial', 'result', 'suggest', 'accord', 'aggregate', 'l∞', 'evaluation', 'appear', 'case', 'l2', 'tack', 'however', 'continue', 'substantially', 'hinder', 'linac', 'robust', 'accuracy', 'still', 'ac', 'cord', 'latter', 'attack', 'type', 'even', 'aggregate', 'order', 'well', 'characterise', 'implication', 'linac', 'make', 'use', 'novel', 'adaptive', 'attack', 'follow', 'subsection', 'pba', 'attack', 'linac', 'thus', 'far', 'show', 'strong', 'transfer', 'attack', 'perform', 'use', 'ensemble', 'diverse', 'source', 'model', 'compute', 'adversarial', 'perturbation', 'many', 'repeat', 'trial', 'ultimately', 'reliable', 'cumbersome', 'evaluation', 'protocol', 'require', 'order', 'magnitude', 'computation', 'standard', 'evaluation', 'section', 'introduce', 'pba', 'attack', 'strategy', 'pur', 'posefully', 'design', 'effective', 'input', 'transforma', 'tion', 'network', 'module', 'deny', 'inference', 'gradient', 'computation', 'classiﬁer', 'parameter', 'train', 'e', 'loss', 'dataset', 'available', 'attacker', 'follow', 'e', 'novel', 'strategy', 'successfully', 'train', 'parametric', 'bypass', 'approximation', 'pba', 'linac', 'transform', 'associate', 'defend', 'classiﬁer', 'intriguingly', 'decision', 'boundary', 'result', 'bypass', 'classiﬁer', 'generalise', 'well', 'accuracy', 'clean', 'test', 'datum', 'furthermore', 'bypass', 'classiﬁer', 'readily', 'show', 'robust', 'accuracy', 'use', 'pgd', 'attack', 'indicate', 'appar', 'ent', 'robustness', 'evaluation', 'largely', 'attribute', 'linac', 'transform', 'successfully', 'hinder', 'attack', 'decision', 'boundary', 'defended', 'classiﬁer', 'ceptible', 'adversarial', 'perturbation', 'hence', 'consider', 'add', 'inherent', 'robustness', 'table', 'show', 'standard', 'attack', 'use', 'train', 'pba', 'mapping', 'linac', 'defend', 'classiﬁer', 'even', 'effective', 'bpda', 'attack', 'use', 'source', 'model', 'interestingly', 'pba', 'almost', 'uniformly', 'lead', 'effective', 'attack', 'regardless', 'strategy', 'pgd', 'attack', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'b', 'c', 'e', 'figure', 'decision', 'boundary', 'ﬁve', 'different', 'classiﬁer', 'row', 'ﬁve', 'randomly', 'choose', 'test', 'example', 'column', 'plot', 'respective', 'adversarial', 'direction', 'accord', 'model', 'horizontal', 'random', 'direction', 'vertical', 'undefended', 'nominally', 'train', 'cifar10', 'classiﬁer', 'b', 'linac', 'defended', 'classiﬁer', 'use', 'random', 'key', 'linac', 'defended', 'classiﬁer', 'use', 'private', 'key', 'model', 'evaluate', 'submission', 'bypass', 'classiﬁer', 'result', 'novel', 'pba', 'attack', 'e', 'adversarially', 'train', 'classiﬁer', 'l2', 'main', 'text', 'use', 'generate', 'transfer', 'attack', 'observe', 'boundary', 'nominal', 'model', 'different', 'linac', 'c', 'linac', 'decision', 'boundary', 'seem', 'less', 'smooth', 'compare', 'model', 'suspect', 'boundary', 'appear', 'different', 'key', 'c', 'corroborate', 'observation', 'robustness', 'transfer', 'attack', 'surrogate', 'adversarially', 'train', 'model', 'e', 'robust', 'vertical', 'dimension', 'random', 'noise', 'linac', 'model', 'c', 'also', 'appear', 'less', 'sensitive', 'random', 'noise', 'compare', 'nominal', 'model', 'pba', 'bypass', 'classiﬁer', 'boundary', 'much', 'smooth', 'different', 'true', 'boundary', 'attack', 'model', 'c', 'explain', 'linac', 'withstand', 'novel', 'attack', 'many', 'case', 'notice', 'pba', 'approximate', 'boundary', 'close', 'far', 'away', 'test', 'example', 'compare', 'true', 'model', 'c', 'make', 'less', 'clear', 'useful', 'approximation', 'future', 'attack', 'linac', 'e', 'pba', 'give', 'accurate', 'picture', 'robustness', 'strategy', 'suggest', 'matter', 'obfuscated', 'gradi', 'ent', 'largely', 'mitigate', 'novel', 'strategy', 'aggregate', 'different', 'attack', 'type', 'pba', 'effective', 'efﬁcient', 'evaluation', 'strategy', 'make', 'use', 'private', 'key', 'hence', 'valid', 'adopt', 'threat', 'model', 'base', 'evaluation', 'alone', 'conclude', 'robust', 'accuracy', 'attack', 'size', 'l∞', 'norm', 'attack', 'size', 'norm', 'apparent', 'robustness', 'differ', 'ence', 'l∞', 'l2', 'attack', 'persist', 'suggest', 'linac', 'primarily', 'hinder', 'latter', 'type', 'attack', 'explain', 'apparent', 'robustness', 'decision', 'boundary', 'inspection', 'plot', 'decision', 'bind', 'arie', 'several', 'classiﬁer', 'ﬁve', 'randomly', 'choose', 'test', 'example', 'figure', 'boundary', 'plot', 'centre', 'test', 'example', 'column', 'use', 'appropriate', 'adversarial', 'direc', 'tion', 'horizontal', 'dimension', 'random', 'direction', 'vertical', 'expect', 'observe', 'difference', 'linac', 'defended', 'classiﬁer', 'use', 'different', 'key', 'fur', 'thermore', 'find', 'linac', 'boundary', 'complicated', 'relative', 'model', 'explain', 'pba', 'attack', 'completely', 'effective', 'rgb', 'reconstruction', 'lossy', 'encoding', 'set', 'representation', 'layer', 'index', 'render', 'linac', 'tran', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'full', 'pca', 'block', 'block', 'linac', 'standard', 'pba', 'pba', 'clean', 'accuracy', 'norm', 'attack', 'l2', 'aa', 'square', 'well', 'know', 'aa', 'square', 'well', 'know', 'table', 'cifar10', 'test', 'set', 'robust', 'accuracy', 'several', 'ﬁer', 'defend', 'use', 'related', 'input', 'transformation', 'accord', 'evaluation', 'use', 'adversarial', 'perturbation', 'bound', 'l∞', 'l2', 'norm', 'report', 'result', 'strong', 'know', 'attack', 'strategy', 'method', 'valid', 'accord', 'threat', 'model', 'form', 'approximate', 'rgb', 'input', 'reconstruction', 'index', 'implicit', 'network', 'output', 'layer', 'conﬁrme', 'set', 'epoch', 'offer', 'robustness', 'result', 'reconstruction', 'precise', 'bpda', 'attack', 'successful', 'clean', 'accuracy', 'virtually', 'match', 'nominally', 'train', 'classiﬁer', 'hence', 'apparent', 'robustness', 'number', 'ﬁtte', 'epoch', 'andor', 'choice', 'repre', 'sentation', 'layer', 'index', 'intuitively', 'hyperparameter', 'control', 'lossy', 'transformation', 'naturally', 'interested', 'reduce', 'computational', 'overhead', 'linac', 'aim', 'match', 'clean', 'accuracy', 'stateoftheart', 'adversarially', 'train', 'robust', 'classiﬁer', 'speciﬁcally', 'rebufﬁ', 'et', 'empirically', 'choose', 'epoch', 'tradeoff', 'speed', 'clean', 'racy', 'activation', 'code', 'layer', 'index', 'hide', 'layer', 'choose', 'accord', 'principle', 'low', 'level', 'representation', 'reduce', 'clean', 'accuracy', 'target', 'threshold', 'far', 'characterise', 'illustrate', 'linac', 'transform', 'appendix', 'performance', 'consideration', 'linac', 'expensive', 'inference', 'zagoruyko', 'cifar10', 'image', 'cost', 'dom', 'inate', 'ﬁtting', 'inrs', 'reduce', 'adaptive', 'form', 'early', 'stopping', 'base', 'loss', 'value', 'leverage', 'advance', 'research', 'leave', 'investigation', 'scale', 'linac', 'large', 'image', 'future', 'work', 'sensitivity', 'analyse', 'apparent', 'robustness', 'linac', 'defended', 'classiﬁer', 'largely', 'insensitive', 'number', 'hide', 'layer', 'implicit', 'mlp', 'well', 'number', 'feature', 'positional', 'input', 'encoding', 'hence', 'relegate', 'sensitivity', 'analysis', 'appendix', 'c', 'pba', 'linac', 'methodology', 'validation', 'show', 'onebutlast', 'column', 'table', 'pba', 'successfully', 'completely', 'invalidate', 'block', 'shufﬂe', 'approach', 'aprilpyone', 'kiya', '2021a', 'good', 'reported', 'robustness', 'attack', 'far', 'investigate', 'use', 'adversarially', 'train', 'source', 'model', 'see', 'full', 'result', 'table', 'b1', 'summary', 'analysis', 'conﬁrm', 'apparent', 'robust', 'accuracy', 'block', 'shufﬂe', 'accord', 'valid', 'attack', 'bound', 'l2', 'norm', 'remain', 'high', 'hence', 'pba', 'indeed', 'know', 'valid', 'attack', 'defence', 'completely', 'successful', 'finally', 'validate', 'evaluation', 'methodology', 'test', 'e', 'effectiveness', 'similar', 'defence', 'perform', 'evaluation', 'principal', 'component', 'analysis', 'pca', 'base', 'defence', 'base', 'defence', 'guo', 'table', 'report', 'well', 'know', 'robust', 'accuracy', 'defence', 'accord', 'evaluation', 'methodology', 'directly', 'comparable', 'report', 'linac', 'result', 'observe', 'successfully', 'hinder', 'much', 'strong', 'attack', 'alternative', 'strategy', 'conclusion', 'work', 'introduce', 'linac', 'novel', 'keybase', 'fence', 'use', 'implicit', 'neural', 'representation', 'demonstrate', 'effectiveness', 'hinder', 'standard', 'adversarial', 'attack', 'cifar10', 'classiﬁer', 'systematically', 'attempt', 'cir', 'cumvent', 'defence', 'adapt', 'host', 'widely', 'use', 'attack', 'literature', 'include', 'transfer', 'adaptive', 'attack', 'linac', 'maintain', 'strong', 'apparent', 'robustness', 'consequently', 'challenge', 'linac', 'introduce', 'novel', 'adaptive', 'attack', 'strategy', 'pba', 'indeed', 'cessful', 'discover', 'adversarial', 'example', 'also', 'show', 'pba', 'use', 'completely', 'invalidate', 'exist', 'keybase', 'defence', 'late', 'attempt', 'leverage', 'computational', 'hardness', 'adversarial', 'robustness', 'successful', 'pba', 'attack', 'exist', 'method', 'enable', 'progress', 'reference', 'andriushchenko', 'croce', 'flammarion', 'n', 'hein', 'square', 'attack', 'queryefﬁcient', 'blackbox', 'adversarial', 'attack', 'random', 'search', 'european', 'conference', 'computer', 'vision', 'springer', 'aprilpyone', 'extension', 'encryption', 'inspire', 'adversarial', 'defense', 'secret', 'key', 'ad', 'versarial', 'example', 'asiapaciﬁc', 'formation', 'processing', 'association', 'annual', 'summit', 'conference', 'apsipa', '1369–1374', 'ieee', 'aprilpyone', 'image', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'mation', 'secret', 'key', 'adversarially', 'robust', 'defense', 'ieee', 'transaction', 'information', 'forensic', 'secu', 'rity', 'aprilpyone', 'kiya', 'h', 'transfer', 'learningbase', 'preprint', 'model', 'protection', 'secret', 'key', 'athalye', 'carlini', 'n', 'wagner', 'obfuscated', 'gradient', 'give', 'false', 'sense', 'security', 'circumvent', 'defense', 'adversarial', 'example', 'icml', 'bradbury', 'r', 'j', 'maclaurin', 'necula', 'paszke', 'vanderpla', 'q', 'composable', 'transformation', 'pythonnumpy', 'program', 'decisionbase', 'adversarial', 'attack', 'reliable', 'attack', 'blackbox', 'machine', 'learning', 'model', 'international', 'conference', 'learn', 'representation', 'carlini', 'n', 'wagner', 'adversarial', 'example', 'easily', 'detect', 'bypass', 'detection', 'method', 'proceeding', '10th', 'acm', 'workshop', 'artiﬁcial', 'telligence', 'security', 'wagner', 'evaluate', 'robust', 'ieee', 'symposium', 'ness', 'neural', 'network', 'security', 'privacy', 'sp', 'ieee', 'cohen', 'rosenfeld', 'e', 'kolter', 'certiﬁed', 'adver', 'sarial', 'robustness', 'randomized', 'smoothing', 'interna', 'tional', 'conference', 'machine', 'learning', 'pmlr', 'croce', 'hein', 'reliable', 'evaluation', 'adversarial', 'robustness', 'ensemble', 'diverse', 'parameterfree', 'attack', 'international', 'conference', 'machine', 'learning', 'pmlr', 'da', 'shanbhogue', 'e', 'chau', 'h', 'keep', 'bad', 'guy', 'protect', 'vaccinate', 'deep', 'learning', 'preprint', 'da', 'shanbhogue', 'e', 'chau', 'shield', 'fast', 'practical', 'defense', 'vaccination', 'deep', 'learn', 'e', 'use', 'compression', 'proceeding', '24th', 'acm', 'international', 'conference', 'knowledge', 'discovery', 'data', 'mining', 'dhillon', 'bern', 'anandkumar', 'stochastic', 'activation', 'prune', 'robust', 'adversarial', 'defense', 'international', 'conference', 'learn', 'repre', 'sentation', 'dziugaite', 'roy', 'study', 'effect', 'jpg', 'compression', 'adversarial', 'image', 'arxiv', 'preprint', 'ilyas', 'santurkar', 'b', 'madry', 'adversarial', 'robustness', 'preprint', 'prior', 'learned', 'representation', 'frostig', 'r', 'leary', 'compile', 'chine', 'learn', 'program', 'highlevel', 'tracing', 'system', 'machine', 'learn', 'jha', 'mohammad', 'ad', 'versarially', 'robust', 'learning', 'leverage', 'computational', 'hardness', 'algorithmic', 'learning', 'theory', 'pmlr', 'goodfellow', 'shlen', 'explain', 'ing', 'harness', 'adversarial', 'example', 'preprint', 'p', 'alternative', 'surrogate', 'loss', 'pgdbased', 'adversarial', 'testing', 'corr', 'abs191009338', 'uesato', 'mann', 'p', 'uncover', 'limit', 'adversarial', 'training', 'normbounde', 'adversarial', 'example', 'preprint', 'maaten', 'l', 'tere', 'adversarial', 'image', 'use', 'input', 'transformation', 'international', 'conference', 'learn', 'representation', 'gommer', 'r', 'virtanen', 'p', 'cournapeau', 'r', 'picus', 'haldane', 'del', 'wiebe', 'g´erardmarchant', 'p', 'sheppard', 'gohlke', 'c', 'oliphant', 'e', 'array', 'program', 'numpy', 'nature', 'url', 'n', 'song', 'adver', 'sarial', 'example', 'defense', 'ensemble', 'weak', 'defense', 'strong', 'proceeding', '11th', 'usenix', 'confer', 'ence', 'offensive', 'technology', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'hein', 'andriushchenko', 'formal', 'guarantee', 'robustness', 'classiﬁer', 'adversarial', 'manipulation', 'proceeding', '31st', 'international', 'conference', 'neural', 'information', 'processing', 'system', 'ilyas', 'santurkar', 'tsipra', 'b', 'madry', 'adversarial', 'example', 'bug', 'feature', 'beygelzimer', 'dalch´ebuc', 'fox', 'e', 'garnett', 'r', 'ed', 'advance', 'neural', 'information', 'pro', 'cesse', 'system', 'volume', 'curran', 'e2c420d928d4bf8ce0ff2ec19b371514paper', 'pdf', 'r', 'excessive', 'invariance', 'cause', 'adversarial', 'vulnerability', 'international', 'conference', 'learn', 'representation', 'kingma', 'p', 'method', 'stochastic', 'optimization', 'poster', 'kurakin', 'goodfellow', 'bengio', 'et', 'adversarial', 'attack', 'defence', 'competition', 'competition', 'build', 'intelligent', 'system', 'springer', 'defense', 'adversarial', 'attack', 'use', 'highlevel', 'representation', 'guide', 'denoiser', 'loshchilov', 'hutter', 'stochastic', 'gradient', 'descent', 'restart', 'abs160803983', 'url', 'madry', 'makelov', 'schmidt', 'l', 'vladu', 'deep', 'learning', 'model', 'resistant', 'international', 'conference', 'adversarial', 'attack', 'learn', 'representation', 'h', 'protection', 'method', 'train', 'model', 'secret', 'key', 'unauthorized', 'access', 'arxiv', 'preprint', 'mildenhall', 'b', 'srinivasan', 'p', 'p', 'r', 'nerf', 'represent', 'scene', 'neural', 'radiance', 'ﬁeld', 'view', 'synthesis', 'european', 'conference', 'computer', 'vision', 'eccv', 'springer', 'niu', 'limitation', 'denoise', 'strategy', 'adversarial', 'defense', 'corr', 'abs201209384', 'pang', 'bag', 'trick', 'adversarial', 'training', 'international', 'confer', 'ence', 'learn', 'representation', 'papernot', 'p', 'goodfellow', 'jha', 'b', 'swami', 'practical', 'blackbox', 'attack', 'machine', 'learning', 'proceeding', 'acm', 'conference', 'computer', 'communication', 'secu', 'rity', 'raghunathan', 'defense', 'adversarial', 'example', 'international', 'conference', 'learn', 'representation', 'ramachandran', 'p', 'zoph', 'b', 'q', 'search', 'activation', 'function', 'stimberg', 'wile', 'mann', 'fix', 'datum', 'augmentation', 'improve', 'adversarial', 'robustness', 'arxiv', 'preprint', 'l', 'santurkar', 'tsipra', 'madry', 'adversarially', 'robust', 'generalization', 'require', 'datum', 'neurip', 'schott', 'l', 'brendel', 'ﬁrst', 'adversarially', 'robust', 'neural', 'network', 'model', 'mnist', 'seventh', 'international', 'conference', 'learn', 'representation', 'iclr', 'weinberg', 'e', 'cloninger', 'fend', 'adversarial', 'image', 'use', 'basis', 'function', 'transformation', 'preprint', 'implicit', 'representation', 'cu', 'rate', 'list', 'resource', 'implicit', 'neural', 'representation', 'awesomeimplicitrepresentation', 'sitzmann', 'martel', 'lindell', 'wet', 'zstein', 'g', 'implicit', 'neural', 'representation', 'periodic', 'activation', 'function', 'advance', 'neural', 'information', 'processing', 'system', 'skorokhodov', 'ignatyev', 'elhoseiny', 'adversarial', 'generation', 'continuous', 'image', 'proceeding', 'ieeecvf', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'fense', 'adversarial', 'attack', 'saak', 'transform', 'arxiv', 'preprint', 'song', 'pixeldefend', 'leverage', 'generative', 'model', 'understand', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'defend', 'adversarial', 'example', 'international', 'conference', 'learn', 'representation', 'zaremba', 'sutskever', 'bruna', 'goodfellow', 'fergus', 'r', 'intriguing', 'property', 'neural', 'network', 'international', 'confer', 'ence', 'learn', 'representation', 'url', 'tieleman', 'coursera', 'neural', 'network', 'machine', 'learn', 'tramer', 'brendel', 'madry', 'tive', 'attack', 'adversarial', 'example', 'defense', 'advance', 'neural', 'information', 'processing', 'system', 'e', 'z', 'provable', 'defense', 'adver', 'sarial', 'example', 'outer', 'adversarial', 'polytope', 'menet', 'ward', 'effective', 'adversarial', 'robustness', 'matrix', 'estima', 'tion', 'international', 'conference', 'machine', 'learning', 'pmlr', 'regularization', 'strategy', 'train', 'strong', 'classiﬁer', 'localizable', 'feature', 'komodaki', 'wide', 'residual', 'network', 'british', 'machine', 'vision', 'conference', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'linac', 'implementation', 'detail', 'implicit', 'neural', 'representation', 'a11', 'random', 'number', 'generation', 'inrs', 'linac', 'defence', 'fully', 'deterministic', 'design', 'use', 'random', '64bit', 'sign', 'integer', 'private', 'key', 'seed', 'state', 'pseudorandom', 'number', 'generator', 'bradbury', 'precise', 'value', 'private', 'key', 'use', 'train', 'defended', 'model', 'evaluate', 'work', 'select', 'randomly', 'initialise', 'random', 'number', 'generator', 'seed', 'use', 'ﬁrst', 'integer', 'a12', 'input', 'output', 'encoding', 'follow', 'mildenhall', 'use', 'positional', 'encoding', 'pixel', 'coordinate', 'high', 'dimensional', 'space', 'well', 'capture', 'higherfrequency', 'information', 'pixel', 'normalise', 'transform', 'follow', 'use', 'frequency', 'experiment', 'hide', 'layer', 'unit', 'layer', 'relu', 'nonlinearitie', 'activation', 'middle', 'hide', 'layer', 'use', 'compute', 'linac', 'transform', 'hence', 'standard', 'practice', 'cifar10', 'classiﬁcation', 'pixel', 'colour', 'intensity', 'scale', 'mean', 'training', 'dataset', 'colour', 'channel', 'separately', 'intensity', 'standardise', 'standard', 'deviation', 'training', 'dataset', 'independently', 'channel', 'parameter', 'implicit', 'network', 'use', 'default', 'parameter', 'learn', 'rate', '0001', 'use', 'minibatche', 'random', 'pixel', 'train', 'epoch', 'epoch', 'constitute', 'pass', 'entire', 'set', 'pixel', 'input', 'image', 'dimension', '×', '×', 'c', '×', '×', 'random', 'order', 'total', 'number', 'optimisation', 'step', 'perform', 'cosine', 'learning', 'rate', 'decay', 'schedule', 'use', 'well', 'convergence', 'minimum', 'value', 'multipli', 'hutter', 'a14', 'computational', 'memory', 'requirement', 'linac', 'transform', 'computational', 'complexity', 'scale', 'number', 'j', 'input', 'image', 'number', 'epoch', 'pixel', 'take', 'backward', 'pass', 'implicit', 'network', 'ﬁt', 'parameter', 'memory', 'complexity', 'dominate', 'number', 'parameter', 'empirically', 'linac', 'transform', 'expensive', 'inference', 'model', 'komodakis', 'cifar10', 'image', 'defend', 'classiﬁer', 'propose', 'input', 'transformation', 'preserve', 'spatial', 'structure', 'perform', 'image', 'classiﬁcation', 'use', 'transform', 'input', 'identical', 'manner', 'colour', 'image', 'high', 'number', 'channel', 'transform', 'input', 'hence', 'employ', 'standard', 'classiﬁcation', 'pipeline', 'follow', 'zagoruyko', 'komodakis', 'use', 'wideresnet', 'classiﬁer', 'reiterate', 'propose', 'transformation', 'change', 'number', 'input', 'channel', 'spatial', 'dimension', 'hence', 'small', 'difference', 'model', 'result', 'report', 'literature', 'conceivably', 'appear', 'different', 'number', 'input', 'channel', 'however', 'practically', 'lead', 'less', 'increase', 'total', 'number', 'model', 'parameter', 'limit', 'convolutional', 'layer', 'use', 'ﬁlter', 'channel', 'instead', 'use', 'swish', 'activation', 'function', 'propose', 'classiﬁer', 'training', 'perform', 'use', 'minibatche', 'size', 'total', 'epoch', 'parameter', 'update', 'initial', 'learning', 'rate', 'reduce', 'factor', 'time', 'epoch', 'perform', 'hyperparameter', 'sweep', 'weightdecay', 'scale', 'follow', 'grid', 'maintain', 'exponential', 'move', 'average', 'classiﬁer', 'parameter', 'decay', 'rate', 'report', 'accuracy', 'use', 'ﬁnal', 'average', 'classiﬁer', 'parameter', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'a21', 'performance', 'consideration', 'use', 'cutmix', 'data', 'augmentation', 'strategy', 'directly', 'image', 'training', 'set', 'cifar', 'prior', 'transform', 'linac', 'impact', 'computational', 'consideration', 'precompute', 'transform', 'dataset', 'ofﬂine', 'order', 'save', 'training', 'time', 'become', 'challenging', 'ease', 'prototype', 'choose', 'implement', 'linac', 'preprocesse', 'layer', 'impact', 'training', 'time', 'use', 'naively', 'transformation', 'apply', 'asynchronously', 'buffer', 'datum', 'feed', 'device', 'use', 'model', 'training', 'also', 'find', 'empirically', 'propose', 'transformation', 'render', 'effective', 'parallelisation', 'use', 'modern', 'simd', 'device', 'fact', 'parameter', 'share', 'implicit', 'model', 'different', 'input', 'likely', 'ability', 'modern', 'library', 'frostig', 'vectorise', 'operation', 'tensor', 'hold', 'parameter', 'many', 'distinct', 'neural', 'network', 'important', 'note', 'inference', 'training', 'cost', 'defended', 'classiﬁer', 'roughly', 'double', 'nominal', 'classiﬁer', 'hence', 'linac', 'transform', 'comparable', 'cost', 'inference', 'model', 'b', 'evaluation', 'detail', 'b1', 'attack', 'surrogate', 'model', 'provide', 'breakdown', 'evaluation', 'use', 'surrogate', 'model', 'initially', 'report', 'section', 'report', 'good', 'key', 'bruteforce', 'attack', 'private', 'key', 'table', 'key', 'also', 'use', 'train', 'surrogate', 'model', 'defend', 'linac', 'use', 'transfer', 'attack', 'see', 'table', 'complete', 'result', 'reconstructionbase', 'surrogate', 'model', 'defend', 'linac', 'use', 'attacker', 'key', 'use', 'perform', 'bpda', 'transfer', 'attack', 'report', 'table', 'position', 'clean', 'test', 'accuracy', 'attacker', 'key', 'table', 'top', 'key', 'bruteforce', 'key', 'attack', 'also', 'use', 'train', 'surrogate', 'model', 'linac', 'defence', 'norm', 'attack', 'name', 'l2', 'aa', 'square', 'well', 'know', 'aa', 'square', 'well', 'know', 'defended', 'surrogate', 'source', 'model', 'attacker', 'key', 'key1', 'key5', 'good', 'adversary', 'model', 'table', 'cifar10', 'test', 'set', 'robust', 'accuracy', 'single', 'linac', 'defend', 'wrt', 'suite', 'l∞', 'l2', 'transfer', 'attack', 'valid', 'threat', 'model', 'use', 'surrogate', 'classiﬁer', 'defend', 'linac', 'train', 'key', 'clean', 'accuracy', 'defended', 'classiﬁer', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'defence', 'norm', 'attack', 'name', 'l2', 'aa', 'square', 'well', 'know', 'aa', 'square', 'well', 'know', 'reconstructionbase', 'surrogate', 'source', 'model', 'use', 'attacker', 'key', 'key1', 'key2', 'key3', 'key4', 'key5', 'key7', 'key9', 'good', 'adversary', 'model', 'table', 'cifar10', 'test', 'set', 'robust', 'accuracy', 'single', 'linac', 'defend', 'classiﬁer', 'accord', 'suite', 'l∞', 'l2', 'attack', 'valid', 'threat', 'model', 'use', 'reconstructionbase', 'surrogate', 'classiﬁer', 'transfer', 'attack', 'adversarially', 'train', 'model', 'mount', 'transfer', 'attack', 'take', 'adversarially', 'train', 'model', 'previous', 'work', 'rebufﬁ', 'checkpoint', 'available', 'online1', 'model', 'adversarially', 'train', 'cifar10', 'use', 'additional', 'synthetic', 'generate', 'datum', 'cutmix', 'data', 'augmentation', 'mount', 'transfer', 'attack', 'use', 'model', 'train', 'defend', 'l∞', 'normbounde', 'perturbation', 'size', 'model', 'train', 'defend', 'l2', 'normbounde', 'perturbation', 'size', 'pba', 'implementation', 'detail', 'pba', 'linac', 'use', 'single', 'convolutional', 'layer', '×', 'bias', 'implement', 'pba', 'nuisance', 'transformation', 'map', 'rgb', 'channel', 'input', 'image', 'channel', 'output', 'linac', 'parameter', 'ψ', 'bypass', 'approximation', 'train', 'minimise', 'crossentropy', 'loss', 'cifar10', 'training', 'set', 'use', 'momentum', 'sgd', 'learning', 'rate', 'epoch', 'sufﬁce', 'optimise', 'pba', 'parameter', 'learning', 'rate', 'reduction', 'factor', 'epoch', 'b32', 'pba', 'block', 'pixel', 'shuffle', 'implement', 'block', 'shufﬂe', 'aprilpyone', 'kiya', '2021a', 'use', 'block', 'size', '×', 'recommend', 'original', 'work', 'use', 'private', 'key', 'value', 'defended', 'linac', 'classiﬁer', 'private', 'key', 'serve', 'seed', 'pseudorandom', 'number', 'generator', 'use', 'sample', 'permutation', 'pixel', 'position', 'block', 'permutation', 'apply', 'block', 'illustrate', 'transform', 'figure', 'figure', 'example', 'pixel', 'block', 'shufﬂe', 'transformation', 'aprilpyone', 'original', 'cifar10', 'image', 'leave', 'split', 'grid', '×', 'block', 'adjacent', 'pixel', 'random', 'permutation', 'use', 'shufﬂe', 'pixel', 'position', 'block', 'middle', 'transform', 'image', 'construct', 'spatially', 'concatenate', 'block', 'accord', 'original', 'position', 'grid', 'classiﬁer', 'defend', 'block', 'shufﬂe', 'train', 'procedure', 'defended', 'linac', 'classiﬁer', 'report', 'clean', 'cifar10', 'test', 'set', 'accuracy', 'high', 'report', 'aprilpyone', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'consistent', 'datum', 'augmentation', 'procedure', 'use', 'defended', 'classiﬁer', 'accord', 'whitebox', 'threat', 'model', 'aprilpyone', 'kiya', 'implementation', 'detail', 'defence', 'know', 'attacker', 'private', 'key', 'exploit', 'block', 'structure', 'use', 'single', 'linear', 'layer', 'bias', 'initialise', 'identity', 'mapping', 'compute', 'parametric', 'bypass', 'approximation', 'pba', 'defence', 'find', 'use', 'small', 'initial', 'learning', 'rate', 'result', 'stable', 'convergence', 'use', 'epoch', 'optimise', 'pba', 'parameter', 'learning', 'rate', 'reduction', 'factor', 'epoch', 'extensive', 'evaluation', 'result', 'defended', 'classiﬁer', 'give', 'table', 'transfer', 'attack', 'agnostic', 'defence', 'successful', 'adversarial', 'example', 'compute', 'use', 'robust', 'source', 'model', 'infer', 'level', 'robustness', 'use', 'pba', 'attack', 'valid', 'threat', 'model', 'successfully', 'circumvent', 'defence', 'well', 'know', 'cifar10', 'robust', 'testset', 'accuracy', 'adversarial', 'perturbation', 'size', 'l∞', 'norm', 'norm', 'shufﬂe', 'defence', 'transfer', 'attack', 'source', 'model', 'adaptive', 'attack', 'good', 'adversary', 'norm', 'attack', 'name', 'nominal', 'source', 'l∞', 'l2', 'aa', 'square', 'well', 'know', 'aa', 'square', 'well', 'know', 'adversarial', 'training', 'l∞', 'adversarial', 'training', 'l2', 'pba', 'source', 'model', 'table', 'cifar10', 'test', 'set', 'robust', 'accuracy', 'shufﬂe', 'approach', 'aprilpyone', 'kiya', 'standard', 'l∞', 'l2', 'bound', 'attack', 'use', 'transfer', 'novel', 'pba', 'strategy', 'sensitivity', 'linac', 'hyperparameter', 'perform', 'sensitivity', 'analysis', 'linac', 'hyperparameter', 'efﬁciency', 'reason', 'report', 'robust', 'accuracy', 'accord', 'untargeted', 'pgd', 'attack', 'step', 'restart', 'use', 'adversarially', 'train', 'robust', 'model', 'l2', 'rebufﬁ', 'generate', 'adversarial', 'perturbation', 'clean', 'accuracy', 'pgd', 'l2', 'pgd', 'figure', 'cifar10', 'testset', 'clean', 'robust', 'accuracy', 'transfer', 'attack', 'linac', 'defended', 'classiﬁer', 'different', 'number', 'positional', 'encoding', 'frequency', 'keep', 'hyperparameter', 'constant', 'figure', 'provide', 'sensitivity', 'analysis', 'number', 'frequency', 'use', 'positional', 'encoding', 'mildenhall', 'keep', 'hyperparameter', 'note', 'use', 'defended', 'classiﬁer', 'evaluate', 'r', 'u', 'c', 'e', 'clean', 'accuracy', 'l', 'robust', 'accuracy', 'l2', 'robust', 'accuracy', 'number', 'frequency', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'main', 'paper', 'l', 'l', 'l', 'clean', 'accuracy', 'pgd', 'l2', 'pgd', 'figure', 'cifar10', 'testset', 'clean', 'robust', 'accuracy', 'transfer', 'attack', 'linac', 'defended', 'classiﬁer', 'different', 'number', 'implicit', 'network', 'layer', 'l', 'keep', 'hyperparameter', 'ﬁxe', 'figure', 'vary', 'number', 'implicit', 'network', 'layer', 'l', 'keep', 'hyperparameter', 'include', 'representation', 'layer', 'index', 'number', 'epoch', 'note', 'use', 'defended', 'classiﬁer', 'evaluate', 'main', 'paper', 'clean', 'accuracy', 'pgd', 'l2', 'pgd', 'figure', 'cifar10', 'testset', 'clean', 'robust', 'accuracy', 'transfer', 'attack', 'linac', 'defended', 'classiﬁer', 'different', 'implicit', 'network', 'layer', 'use', 'output', 'representation', 'keep', 'hyperparameter', 'figure', 'change', 'index', 'linac', 'representation', 'layer', 'keep', 'hyperparameter', 'unchanged', 'note', 'use', 'defended', 'classiﬁer', 'evaluate', 'main', 'paper', 'figure', 'analyse', 'sensitivity', 'linac', 'number', 'epoch', 'keep', 'hyperparameter', 'constant', 'note', 'use', 'defended', 'classiﬁer', 'evaluate', 'main', 'paper', 'characterise', 'linac', 'transform', 'figure', 'plot', 'learn', 'curve', 'characterise', 'implicit', 'network', 'ﬁtte', 'use', 'defended', 'classiﬁer', 'mean', 'standard', 'deviation', 'error', 'independent', 'learning', 'process', 'entire', 'cifar10', 'testset', 'plot', 'function', 'optimisation', 'step', 'use', 'logscale', 'error', 'ﬁnal', 'mean', 'value', 'error', 'conﬁrm', 'linac', 'approach', 'lead', 'lossy', 'representation', 'histogram', 'ﬁnal', 'sum', 'square', 'error', 'entire', 'testset', 'cifar10', 'provide', 'figure', 'qualitative', 'evaluation', 'statistic', 'provide', 'example', 'original', 'image', 'reconstruction', 'difference', 'image', 'use', 'linac', 'private', 'key', 'figure', 'comparison', 'different', 'key', 'figure', 'observe', 'encode', 'error', 'use', 'linac', 'key', 'dependent', 'furthermore', 'signiﬁcant', 'amount', 'information', 'seem', 'leave', 'linac', 'difference', 'image', 'recognise', 'correct', 'class', 'likely', 'highfrequency', 'information', 'well', 'represent', 'finally', 'provide', 'number', 'plot', 'qualitative', 'comparison', 'linac', 'transform', 'figure', 'show', 'different', 'r', 'u', 'c', 'e', 'clean', 'accuracy', 'l', 'robust', 'accuracy', 'l2', 'robust', 'accuracy', 'l', 'number', 'hide', 'layer', 'r', 'u', 'c', 'e', 'clean', 'accuracy', 'l', 'robust', 'accuracy', 'l2', 'robust', 'accuracy', 'k', 'index', 'representation', 'layer', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'clean', 'accuracy', 'pgd', 'l2', 'pgd', 'figure', 'cifar10', 'testset', 'clean', 'robust', 'accuracy', 'transfer', 'attack', 'linac', 'defended', 'classiﬁer', 'implicit', 'network', 'train', 'epoch', 'keep', 'hyperparameter', 'constant', 'figure', 'independent', 'ﬁtting', 'implicit', 'neural', 'network', 'cifar10', 'testset', 'image', 'order', 'compute', 'linac', 'transform', 'sum', 'squared', 'encoding', 'error', 'average', 'pixel', 'plot', 'ﬁtte', 'step', 'image', 'encode', 'respective', 'linac', 'representation', 'encode', 'channel', 'figure', 'plot', 'linac', 'transform', 'respective', 'image', 'use', 'different', 'key', 'colour', 'channel', 'r', 'u', 'c', 'e', 'clean', 'accuracy', 'l', 'robust', 'accuracy', 'l2', 'robust', 'accuracy', 'n', 'number', 'epoch', 'l', 'c', 'l', 'r', 'r', 'r', 'e', 'step', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'figure', 'histogram', 'linac', 'transform', 'encoding', 'error', 'plot', 'entire', 'cifar10', 'testset', 'overall', 'mean', 'value', 'error', 'conﬁrm', 'linac', 'approach', 'lead', 'lossy', 'representation', 'mean', 'error', 'error', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'figure', 'image', 'approximation', 'compute', 'linac', 'private', 'key', 'use', 'defended', 'classiﬁer', 'original', 'image', 'label', 'plot', 'ﬁrst', 'column', 'note', 'label', 'use', 'linac', 'implicit', 'network', 'output', 'plot', 'second', 'column', 'difference', 'image', 'sum', 'squared', 'error', 'average', 'pixel', 'plot', 'third', 'column', 'note', 'linac', 'use', 'lossy', 'image', 'approximation', 'horse', 'airplane', 'frog', 'truck', 'dog', 'automobile', 'horse', 'airplane', 'cat', 'bird', '538e02', '244e02', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'figure', 'image', 'approximation', 'compute', 'linac', 'different', 'attacker', 'choose', 'key', 'original', 'image', 'label', 'plot', 'ﬁrst', 'column', 'note', 'label', 'use', 'linac', 'implicit', 'network', 'output', 'plot', 'second', 'column', 'difference', 'image', 'sum', 'squared', 'error', 'average', 'pixel', 'plot', 'third', 'column', 'note', 'linac', 'lead', 'lossy', 'image', 'approximation', 'key', 'dependent', 'horse', 'airplane', 'frog', 'truck', 'dog', 'automobile', 'horse', 'airplane', 'cat', 'bird', '500e02', '723e02', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'figure', 'compare', 'transform', 'top', 'image', 'use', 'linac', 'private', 'key', 'defended', 'classiﬁer', 'respective', 'activation', 'image', 'channel', 'plot', '×', 'grid', 'slice', 'size', 'original', 'image', 'respective', 'slice', 'channel', 'dimension', 'activation', 'image', 'combine', 'channel', 'plot', 'bottom', 'order', 'compare', 'channel', 'representation', 'input', 'image', 'top', 'square', 'grid', 'represent', 'activation', 'linac', 'representation', 'channel', 'pixel', 'original', 'image', 'different', 'value', 'rgb', 'signify', 'difference', 'linac', 'representation', 'image', 'r', 'e', 'l', 'e', 'h', 'hide', 'unit', 'position', 'layer', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'figure', 'compare', 'linac', 'transform', 'image', 'use', 'private', 'key', 'random', 'key', 'respective', 'activation', 'image', 'channel', 'plot', '×', 'grid', 'slice', 'size', 'original', 'image', 'respective', 'slice', 'channel', 'dimension', 'result', 'activation', 'image', 'combine', 'channel', 'plot', 'bottom', 'order', 'compare', 'channel', 'representation', 'different', 'key', 'input', 'image', 'square', 'grid', 'represent', 'activation', 'linac', 'representation', 'channel', 'pixel', 'original', 'image', 'different', 'value', 'rgb', 'signify', 'difference', 'linac', 'representation', 'key', 'r', 'e', 'l', 'e', 'h', 'hide', 'unit', 'position', 'layer', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'figure', 'compare', 'linac', 'transform', 'image', 'use', 'private', 'key', 'random', 'key', 'respective', 'activation', 'image', 'channel', 'plot', '×', 'grid', 'slice', 'size', 'original', 'image', 'respective', 'slice', 'channel', 'dimension', 'result', 'activation', 'image', 'combine', 'channel', 'plot', 'bottom', 'order', 'compare', 'channel', 'representation', 'different', 'key', 'input', 'image', 'square', 'grid', 'represent', 'activation', 'linac', 'representation', 'channel', 'pixel', 'original', 'image', 'different', 'value', 'rgb', 'signify', 'difference', 'linac', 'representation', 'key', 'r', 'e', 'l', 'e', 'h', 'hide', 'unit', 'position', 'layer', 'hinder', 'adversarial', 'attack', 'implicit', 'network', 'activation', 'code', 'linac', 'figure', 'compare', 'linac', 'transform', 'image', 'use', 'private', 'key', 'random', 'key', 'respective', 'activation', 'image', 'channel', 'plot', '×', 'grid', 'slice', 'size', 'original', 'image', 'respective', 'slice', 'channel', 'dimension', 'result', 'activation', 'image', 'combine', 'channel', 'plot', 'bottom', 'order', 'compare', 'channel', 'representation', 'different', 'key', 'input', 'image', 'square', 'grid', 'represent', 'activation', 'linac', 'representation', 'channel', 'pixel', 'original', 'image', 'different', 'value', 'rgb', 'signify', 'difference', 'linac', 'representation', 'key', 'r', 'e', 'l', 'e', 'h', 'hide', 'unit', 'position', 'layer']"
"Google's 2019 ""Quantum Supremacy'' Claims: Data, Documentation, and
  Discussion","[{'href': 'http://arxiv.org/abs/2210.12753v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.12753v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-10-23 15:36:25,"2
2
0
2

t
c
O
8
1

]

G
L
.
s
c
[

1
v
8
9
5
9
0
.
0
1
2
2
:
v
i
X
r
a

Planning for Sample Efﬁcient Imitation Learning

Zhao-Heng Yin∗ Weirui Ye†† Qifeng Chen∗ Yang Gao†‡§

∗HKUST †Tsinghua University

‡Shanghai Qi Zhi Institute

Abstract

Imitation learning is a class of promising policy learning algorithms that is free
from many practical issues with reinforcement learning, such as the reward design
issue and the exploration hardness. However, the current imitation algorithm strug-
gles to achieve both high performance and high in-environment sample efﬁciency
simultaneously. Behavioral Cloning (BC) does not need in-environment interac-
tions, but it suffers from the covariate shift problem which harms its performance.
Adversarial Imitation Learning (AIL) turns imitation learning into a distribution
matching problem. It can achieve better performance on some tasks but it requires
a large number of in-environment interactions. Inspired by the recent success of
EfﬁcientZero in RL, we propose EfﬁcientImitate (EI), a planning-based imitation
learning method that can achieve high in-environment sample efﬁciency and per-
formance simultaneously. Our algorithmic contribution in this paper is two-fold.
First, we extend AIL into the MCTS-based RL. Second, we show the seemingly
incompatible two classes of imitation algorithms (BC and AIL) can be naturally
uniﬁed under our framework, enjoying the beneﬁts of both. We benchmark our
method not only on the state-based DeepMind Control Suite, but also on the image
version which many previous works ﬁnd highly challenging. Experimental results
show that EI achieves state-of-the-art results in performance and sample efﬁciency.
EI shows over 4x gain in performance in the limited sample setting on state-based
and image-based tasks and can solve challenging problems like Humanoid, where
previous methods fail with small amount of interactions. Our code is available at
https://github.com/zhaohengyin/EfficientImitate.

1

Introduction

The real-world sequential decision process in robotics is highly challenging. Robots have to handle
high dimensional input such as images, need to solve long horizon problems, some critical timesteps
need highly accurate maneuver, and the learning process on the real robot has to be sample efﬁcient.
Imitation learning is a promising approach to solving those problems, given a small dataset of expert
demonstrations. However, current imitation algorithms struggle to achieve these goals simultaneously.
There are two kinds of popular imitation learning algorithms, Behavior Cloning (BC) and Adversarial
Imitation Learning (AIL). BC formulates imitation learning as a supervised learning problem. It
needs no in-environment samples, but it suffers from the covariate shift issue [40], often leading
to test time performance degradation. Adversarial Imitation Learning (AIL) [16, 6] casts imitation
learning as a distribution matching problem. Though AIL suffers less from the covariate shift
problem and can perform better than BC on some domains, it requires impractical number of online
interactions [20, 23] and can perform badly on image inputs [37]. These drawbacks heavily limit its
application in ﬁelds like robotics, where physical robot time matters. In summary, current imitation

∗zhaoheng.yin@connect.ust.hk, cqf@ust.hk
†ywr20@mails.tsinghua.edu.cn, gaoyangiiis@tsinghua.edu.cn
§Corresponding author.

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

 
 
 
 
 
 
Figure 1: Left: The system-level overview of EfﬁcientImitate. The agent (yellow area) takes actions
in the environment and stores the data in the replay buffer. The data in the replay buffer and expert
buffer are then used to train a model and AIL reward. The planning module then searches for
improved policy and value in each state in the replay buffer, based on which the policy and value
networks are optimized. Right: The planning procedure. We use a continuous version EfﬁcientZero
as our planner. We ﬁnd that MCTS uniquely beneﬁts from BC and can unify BC and AIL. For the
expansion of each node, we sample actions from both the current policy (black arrow) and a BC
policy (blue arrow). We use AIL reward (yellow cube) to encourage long-term distribution matching.
MCTS searches (pink area) for best actions to maximize the cumulative AIL reward and update
the estimated value and policy of the root node. The output of the planning procedure is the value
estimate and the policy estimate of s, and the value and policy networks are optimized to ﬁt them.

learning algorithms fail to achieve high online testing performance and high in-environment sample
efﬁciency at the same time. Further, the two types of imitation algorithms seem to be incompatible
since they are based on two completely different training objectives. Previous work ﬁnds that it is
hard to unify them naively[34].

Inspired by the recent success in sample efﬁcient RL, such as EfﬁcientZero [52], we propose a
planning-based imitation algorithm named EfﬁcientImitate (EI) that achieves high test performance
and high in-environment sample efﬁciency at the same time. Our method extends AIL to a model-
based setting with multi-step losses under the MCTS-based RL framework. Our algorithm also
uniﬁes the two types of the previous imitation algorithms (BC and AIL) naturally, thanks to the
planning component of our algorithm. Intuitively, BC gives a coarse solution that is correct most of
the time but fails to match the expert’s behavior in the long term. On the other hand, AIL knows the
goal of the learning, i.e., matching state action distribution, but doesn’t give the solution directly. Our
method’s planning component uniﬁes those two methods and shows a signiﬁcant performance boost,
especially in the harder tasks such as Humanoid. We illustrate the detailed procedure in Figure 1.

We validate our idea not only on state-based tasks but also on image-based tasks, which relatively few
previous algorithms can handle them [37]. EI achieves state-of-the-art results in sample efﬁciency
and performance. It shows over 4x gain in performance in the limited sample setting on state-based
and image-based tasks. On harder tasks such as Humanoid, the gain is even larger. A by-product of
this paper is that we extend the recent EfﬁcientZero algorithm to the continuous action space. We
open-source the code at https://github.com/zhaohengyin/EfficientImitate to facilitate
future research.

Our contributions in this paper are summarized as follows.

• We present EfﬁcientImitate, a sample efﬁcient imitation learning algorithm based on MCTS.

• We identify that MCTS can beneﬁt from BC by using BC actions during the search, which is
crucial for challenging tasks. EfﬁcientImitate suggests a natural way to unify BC and AIL.

• We conduct experiments in both the state and the image domain to evaluate EI. Experimental

results show that EI can achieve state-of-the-art sample efﬁciency and performance.

2

2 Related Work

2.1

Imitation Learning

Imitation learning (IL) aims to solve the sequential decision-making problems with expert demon-
stration data. It has wide applications in games and robotics [4]. Compared with RL, one major
beneﬁt of IL is that it can avoid the notorious reward design problem. IL can be divided into two
branches: BC [1] and IRL [33]. To solve the covariate shift problem of BC, researchers propose
methods like dataset aggregation [40] and noise injection [25]. But these methods either require extra
expert queries or exert constraints over the learning process. A recent variant branch of IRL is the
Adversarial Imitation Learning (AIL) [16, 6]. AIL models IL as a state-action distribution matching
problem. Many works extend AIL by using better distribution metrics such as f -divergence [54] and
Wasserstein distance [5]. Though these methods can learn better reward functions and somewhat
speed up the training, they do not directly focus on AIL’s sample efﬁciency problem. Some works
have drawn attention to sample efﬁciency, for example [23, 41, 55] propose to use off-policy training
in AIL training to reduce sample complexity. ValueDICE [24] reformulates AIL objective in an ofﬂine
min-max optimization process, but recent work points out that it is an improved form of BC [29].
VMAIL [37] uses a model-based approach to improve sample efﬁciency. It collects latent rollout
with a variational model and reduces online interactions. MoBILE [22] also shows a provably sample
efﬁcient model-based imitation approach. Compared with these methods, our method introduces
MCTS planning to the off-policy imitation learning and uses it to unify BC and AIL to take advantage
of both.

The idea of combining BC and AIL helps to improve the sample efﬁciency can be traced back to
GAIL[16]. GAIL suggests using BC to initialize the policy network, but [34] ﬁnds that this does
not work well because the initialized BC knowledge will be corrupted in AIL training. Then, [20]
proposes to use an annealed (decaying) BC loss to regularize policy training to solve this problem. But
in this work, we ﬁnd that such a BC regularizer can be harmful to exploration when BC is incorrect.
One concurrent work mitigates this issue by using adpative BC loss and replacing AIL reward with
optimal-transport-based imitation reward [13]. We also notice that AlphaGo [46] involves BC in their
method, but they focus on RL rather than IL, and BC is only used to initialize the policy. Different
from these methods, EI uses BC actions as candidates in MCTS.

2.2 Sample Efﬁciency in RL

The sample efﬁciency problem of imitation learning is closely related to that in RL. One line of
work ﬁnds that the reward signal is not a good data source for representation learning in RL and
is one reason for sample inefﬁciency. Then they utilize self-supervised representation learning to
accelerate representation learning and improve the sample efﬁciency. Researchers propose to use
contrastive learning [27], consistency-based learning [44, 53, 52], or pretrained representation [45]
for this purpose. Some works also explore the possibility of applying self-supervised representation
learning to imitation learning [4].

Another line of work focuses on RL with a learned model, which is promising for sample efﬁcient
learning [8, 10–12, 19, 28, 31, 52, 14]. These approaches usually imagine additional rollouts with
the learned model or use it as a more compact environment representation for RL. Besides, some
works also ﬁnd that data augmentation can effectively improve sample efﬁciency [26, 50, 49]. EI also
beneﬁts from these approaches. It includes a model and applies representation learning to boost sample
efﬁciency. In application, people also consider to augment RL with the demonstration [15, 38, 32, 21]
to improve the sample efﬁciency. This can be viewed as a combination of RL and imitation learning
and favor RL on real robots. We believe that our method can also be extended to this setting.

3 Background

3.1 Setting

We formalize the sequential decising making problem as Markov Decision Process M =
(S, A, R, T ). Here, S is the state space, A is the action space, R is the reward function, and
T is the transition dynamics. The agent’s state at timestep t is st ∈ S. The agent takes action at and

3

t=0 γtrt, where γ is a discount factor.

receives reward rt = R(st, at). Its state at timestep t + 1 is then st+1 ∼ T (st, at). The objective of
the agent is to maximize the return (cid:80)T
In the imitation learning problem studied here, the agent has no access to the reward function R and
transition dynamics T . It is provided with a ﬁxed expert demonstration dataset D = {τi}. Here,
each τi = (sE
T ) is an expert trajectory that can achieve high performance in
M. The agent can not solicit extra expert demonstrations but can interact with the MDP, observing
new states and actions, but not rewards. In this work, we deﬁne (in-environment) sample efﬁciency as
the number of online interactions during training. We expect the agent to achieve high performance
within a ﬁxed online sample budget.

1 , ...sE

T , aE

0 , aE

1 , aE

0 , sE

3.2 BC and AIL

BC considers imitation learning as a supervised learning problem. It trains a policy network π to
minimize the following loss function:

L = −E

(sE

i ,aE

i )∼D log π(aE

i |sE

i ).

(1)

AIL treats imitation learning as a distribution matching problem. One typical AIL algorithm is GAIL.
It trains a discriminator D to distinguish the agent generated state-action tuple (st, at) from those
(sE

i ) in the demonstration dataset by minimizing

i , aE

L = −E

(st,at)∼ρ,(sE

i ,aE

i )∼D

(cid:2)log(D(st, at)) + log(1 − D(sE

i , aE

i ))(cid:3) ,

(2)

where ρ is the state-action distribution induced by the agent. Meanwhile, it trains the agent to
maximize the return with respect to the adversarial reward rt = − log(1 − D(st, at)) using any
on-policy RL algorithm.

3.3 MuZero and its Extensions

Our planning method is based on MuZero [42] and its extensions. MuZero learns an environment
model for MCTS. The model consists of an encoder network f , a dynamics network g, and a reward
network R. It operates on abstract states [52]. Concretely, it gets the abstract state ht of the current
state st by ht = f (st). It can then predicts the future abstract states recursively by ht+1 = g(ht, at),
and the rewards by R(ht, at). Besides the model, MuZero also contains a policy network and a value
network. The policy network provides a prior over the actions at each node, and the value network
calculates the expected return of the node. MuZero uses the model, the policy network, and the value
network to search for improved policy and value for each state with MCTS. We refer the readers to
the original MuZero paper for details.

Sampled MuZero Sampled MuZero [17] extends MuZero from the discrete action domain to the
continuous action domain, which is of our interest in this paper. At each node s to expand, Sampled
MuZero samples K actions {ai}K
i=1 from current policy π(a|s). During the search, it selects action
a∗ from the sampled actions that maximize the probabilistic upper conﬁdence bound

a∗ = arg max
a∈{ai}

Q(s, a) + c(s)ˆπ(a|s)

(cid:112)(cid:80)

b N (s, b)
1 + N (s, a)

,

(3)

(cid:80)

where ˆπ(a|s) = 1
i δ(a, ai). Q(s, a) is the current Q-estimation of the pair (s, a). N (s, a)
K
denotes the times that this pair is visited in MCTS. c(s) is a weighting coeffcient. During policy
optimization, MuZero minimizes the Kullback-Leibler divergence between the current policy π and
the MCTS statistics πMCTS at the root node DKL(πMCTS||π).

EfﬁcientZero We also apply EfﬁcientZero [52] in this paper. EfﬁcientZero improves the sample
efﬁciency of MuZero by using a self-supervised representation learning method to regularize the
hidden representation. It uses a SimSiam-style structure [3] to enforce the similarity between the
predicted future representation and the real future representation.

4 EfﬁcientImitate

In this section, we present our EfﬁcientImitate algorithm. We ﬁrst present an MCTS-based approach
to solving the AIL problem in Section 4.1. Then we show a simple yet effective method to unify BC

4

Figure 2: Computation ﬂow of loss functions. Left: Multi-step Discriminator Loss. We do not
distinguish between the calculation for expert and agent here, and use a superscript (E) to indicate
that the computation applies to both. Right: Multi-step BC Loss. It applies to the expert sequences.

and AIL with MCTS in Section 4.2. We brieﬂy discuss the implementation in Section 4.3, and the
full details can be found in the Appendix.

4.1 Extending AIL to MCTS-based RL

Traditionally, the adversarial imitation learning (AIL) algorithm trains a discriminator D between the
policy samples and the expert samples and uses some form of D, such as − log(1 − D), as the reward
function. Then some model-free RL algorithms are used to maximize the cumulative reward. In
MCTS-based RL algorithms, such as MuZero [42] and EfﬁcientZero [52], the reward function is used
in the value target computation and the MCTS search. The use in value target computation is similar
to prior model-free RL algorithms, where the value target is computed with n-step value bootstrapping
on the actual observations. However, during the MCTS search, the rewards are computed on the
abstract state obtained by running the forward dynamics function ht+1 = g(ht, at) multiple times. If
we were training the discriminator only on actual observations of the expert and the policy rollouts,
the discriminator might not generalize well to abstract states outputted by the forward dynamics
functions. Therefore, we train the discriminator with the model-based rollout. Speciﬁcally, we sample
sequence (st, at+1, ..., at+n) in replay buffer B and expert sequence (sE
t(cid:48)+n) in
demonstration dataset D and minimizes following multi-step discriminator loss function:

t(cid:48)+1, ..., aE

t(cid:48) , aE

t(cid:48) , aE

LD = −E

(st,at:t+n)∼B,(sE

t(cid:48) ,aE

t(cid:48) :t(cid:48)+n

(cid:34) n
(cid:88)

i=0

)∼D

log(D(ht+i, at+i)) + log(1 − D(hE

t(cid:48)+i, aE

(cid:35)
t(cid:48)+i))

.

(4)

Here, ht+i (and ht(cid:48)+i) terms are produced by the forward dynamics in EfﬁcientZero (Figure 2).
We use the GAIL transition reward R(h, a) = − log(1 − D(h, a)), and then the MCTS planner
searches for action that can maximize cumulative GAIL reward to guarantee long-term distribution
matching. Note that V-MAIL also propose a similar discriminator training technique, but under the
Dreamer [11] model.

Besides, since the discriminator’s input is based on the representation rather than raw input, the
discriminator should be trained with the encoder jointly. However, this can lead to a highly non-
stationary reward during the bootstrap value calculation. To mitigate this issue, we also propose to
use a target discriminator for bootstrap value calculation. This can make the training more stable.

Though we use the GAIL reward here, one can also use other kinds of AIL and IRL reward functions
proposed in recent research. Using the GAIL reward can already achieve satisfactory performance in
our experiments. When the real reward presents, one may also combine this into planning [21]. This
may favor application scenarios where handcrafting a reward function is not hard. We do not study
this case here and leave it to future work.

4.2 Unifying BC and AIL in MCTS

As discussed in related work, researchers realize that using BC can improve AIL’s sample efﬁciency
by providing a good initial BC policy or constraining the policy to BC. However, these existing
solutions are not good enough in practice. The main pitfall in these methods is that BC knowledge in
the policy network is destined to be forgotten if the policy network is trained with the AIL objective,
and then BC will no longer be helpful [34].

5

We observe that MCTS can naturally unify the BC and AIL methods, enjoying the beneﬁt of both
and being free from this pitfall. We propose to plug BC actions into MCTS as candidates at each
node and use a planning process to search for an improved solution. This time, the BC actions are
consistently considered throughout the entire training procedure without being forgotten. Concretely,
we train a BC policy πBC and use a mixture policy ˜π for the sampling at each node in MCTS:

˜π = απBC + (1 − α)π.
(5)
α is a mixture factor, which is ﬁxed during training and π is the current policy. We use α = 0.25
in this paper. This ensures that a small fraction of action samples are from the BC policy. During
planning, the BC actions are evaluated and will be frequently visited and selected as output if they can
lead to long-term distribution matching. This can then reduce the effort of ﬁnding good expert-like
actions from scratch as desired. Moreover, another unique advantage of this procedure is that it does
not fully trust BC like [20], which forces the output of the policy network to be close to BC. When
BC is wrong due to covariate shifts or insufﬁcient demonstrations, it can neglect these BC actions and
allow the policy to search for better actions. This ensures that BC does not hurt training. However,
due to the conceptual simplicity, one arising question is whether this approach can be applied to other
model-based methods. Here, we take Dreamer [11] as an example. Though Dreamer builds a model
of the environment, it only uses the model to roll-out the policy for policy optimization. In other
words, the model is not used to evaluate whether a speciﬁc BC action is good or not in the long term,
so our idea can not be applied directly to Dreamer. From this example, we see that the core of our
idea is to leverage the power of planning, only with which the long-term outcomes of certain (BC)
actions can be calculated.

For the training of πBC, we minimize the following multi-step BC objective (Figure 2):

LBC = E

(sE

t(cid:48) ,aE

t(cid:48):t(cid:48)+n

)∼D

− log(πBC(aE

t(cid:48)+i|hE

t(cid:48)+i))

.

(6)

i=0
This is to avoid distributional shifts during multi-step prediction in MCTS. For the training of the
policy, we still minimize DKL(πMCTS||π).

(cid:34) n
(cid:88)

(cid:35)

Note that the BC design proposed here is not coupled with AIL. It can go beyond imitation learning
and be applied in other robot learning settings, such as RL with demonstration [38].

4.3

Implementation

We ﬁrst implement a continuous version EfﬁcientZero for planning, and the details can be found in
the Appendix. The BC policy network is simply a duplicate of the policy network. The discriminator
and BC policy networks share the same encoder network with the policy network and value network.
The overall loss function for optimization is

L = LEZ + λdLD + λbcLBC.
(7)
LEZ is EfﬁcientZero’s loss function (excluding reward loss). All the networks are trained jointly to
minimize this loss function 7. We use the Reanalyze algorithm [43, 52] for ofﬂine training, and we
require that all the samples should be reanalyzed.

Figure 3: Part of the tasks used in our experiments. From left to right: Reacher, Finger Spin, Cheetah
Run, Walker Walk, Hopper Hop, Humanoid Walk.

5 Experiments

In this section, we evaluate the sample efﬁciency of the proposed method. We measure the sample
efﬁciency by evaluating the performance of an algorithm at a small number of online samples. We
also analyze the effect of the BC actions and planning.

6

Table 1: Evaluation result on the state-based DeepMind Control Suite. We use the average score on
three random seeds. Our method can achieve the state of the art result compared with the baselines.

Task

Cartpole

Budget

BC
DAC

ValueDICE

SQIL

Ours

10k

0.59
0.13
±0.12
0.21
±0.01
0.23
±0.01
0.98
±0.01

Ball

10k

0.44
0.18
±0.01
0.23
±0.01
0.27
±0.05
0.99
±0.01

Reacher

Finger Cheetah Walker Hopper Humanoid

50k

0.83
0.22
±0.02
0.15
±0.01
0.21
±0.02
0.90
±0.04

50k

0.76
0.53
±0.05
0.04
±0.01
0.02
±0.00
0.99
±0.00

50k

0.58
0.33
±0.04
0.50
±0.08
0.05
±0.01
0.96
±0.02

50k

0.16
0.26
±0.04
0.54
±0.09
0.11
±0.03
1.03
±0.01

50k

0.03
0.00
±0.00
0.03
±0.00
0.24
±0.10
0.92
±0.02

500k

0.11
0.01
±0.00
0.00
±0.00
0.06
±0.01
0.74
±0.04

Table 2: Evaluation result on the image-based DeepMind Control Suite. We use the average score on
three random seeds. Our method can achieve state-of-the-art results compared with the baselines.

Task

Cartpole

Budget

BC
DAC

ValueDICE

SQIL

VMAIL

Ours

50k

0.30
0.08
±0.01
0.18
±0.02
0.26
±0.03
0.57
±0.03
0.94
±0.02

Ball

50k

0.32
0.26
±0.02
0.27
±0.02
0.77
±0.05
0.61
±0.11
0.93
±0.01

Finger

Cheetah

Reacher

Walker

Hopper

50k

0.14
0.00
±0.00
0.01
±0.00
0.00
±0.01
0.06
±0.03
1.00
±0.01

50k

0.37
0.04
±0.01
0.06
±0.01
0.06
±0.00
0.13
±0.04
0.92
±0.01

100k

0.26
0.25
±0.05
0.15
±0.02
0.36
±0.04
0.34
±0.02
0.86
±0.06

100k

0.15
0.10
±0.02
0.08
±0.00
0.32
±0.05
0.24
±0.07
0.98
±0.01

200k

0.02
0.01
±0.00
0.00
±0.00
0.04
±0.02
0.07
±0.04
0.70
±0.01

5.1 Setup

We use the DeepMind Control Suite [47] for evaluation. We use the following tasks: Cartpole
Swingup, Reacher Easy, Ball-in-cup Catch, Finger Spin, Cheetah Run, Walker Walk, Hopper Hop,
and Humanoid Walk. We conduct both state-based and image-based experiments. Note that many
previous imitation learning works use the OpenAI Gym [2] version of these tasks for evaluation. We
ﬁnd that the DMControl version used here brings extra challenges by using more challenging initial
states. Take the Walker task as an example; the initial state in OpenAI Gym is standing. However, in
DMControl, the agent’s initial state is lying on the ground, and the agent should also learn to stand
up ﬁrst from very limited data. For the state-based experiments, we allow 10k-50k online steps in
the environment based on the difﬁculty of each task. Since learning a robust and meaningful visual
representation requires more data for image-based experiments, we allow 50k-100k online steps.
Detailed setup will be shown in the result. We train SAC [9] policies to collect expert demonstrations
for imitation learning. The expert demonstrations are not subsampled. We use 5 demonstrations in
the state-based experiment, except for Reacher and Humanoid, where we use 20 demonstrations. We
use 20 demonstrations in the image-based experiments.

5.2 Baselines

(1) DAC DAC [23] is an
We present several baselines of sample efﬁcient imitation learning.
adversarial off-policy imitation learning method. It matches the distribution of the replay buffer
and that of the expert demonstration dataset using the TD3 [7] algorithm. (2) SQIL SQIL [39] is a
non-adversarial off-policy imitation learning method. It labels all the expert transitions with reward
1 and non-expert transitions with reward 0. Then it trains a SAC policy over these relabeled data.
SQIL is a regularized form of BC. (3) ValueDICE ValueDICE [24] considers imitation learning as
a distribution matching problem and solves it with a min-max optimization process. (4) VMAIL
VMAIL [37] is a model-based visual imitation learning method. It learns a variational model for
simulating on-policy rollouts. We only evaluate VMAIL on the image-based domain, as they did in

7

Figure 4: The performance curve on the state-based tasks. The results are averaged over three seeds.
The shaded area displays the range of one standard deviation.

Figure 5: The performance curve on the image-based tasks. The results are averaged over three seeds.
The shaded area displays the range of one standard deviation.

the original paper. Besides the online imitation learning baselines, we also include BC as an ofﬂine
baseline.

5.3 Results

State-based experiments Table 1 shows the state-based experiments’ evaluation results within
the given budget. We also plot the performance curve of four challenging tasks in Figure 4. The
performance is normalized to 0.0 and 1.0 with respect to the performance of the random agent and
the expert. We ﬁnd that our proposed method can outperform all the baselines by a large margin.
Except for BC, these baselines methods could hardly learn meaningful behaviors using a limited
online budget. We ﬁnd that DAC is a strong baseline. Its performance can grow to the expert in
200k samples in most tasks except Hopper, where it will eventually get stuck. Our method is much
more sample efﬁcient than the best of these baseline methods. For some tasks like Walker Walk and
Cheetah Run, our method only requires about 20k steps to reach near-expert performance, equivalent
to 80 online trajectories (around 0.5 hours in real). This result is notable for the robotics community.
It shows that online imitation learning is possible with only a handful of trials, and applying it directly
on a real locomotion robot is possible.

Image-based experiments So far, image-based tasks are still challenging for adversarial imitation
learning algorithms, and the evaluation of most of the prior AIL works is carried out in the state-based
tasks. Table 2 shows the evaluation result within the given budget in the image-based experiments (see
Figure 5 for curves). Our method can also learn expert behavior given a slightly larger budget. Still,
most of the baselines fail to match experts’ behavior using the given budget. We notice an inherent
difﬁculty in learning a robust visual representation for adversarial training in the limited data set in
image-based tasks. Discriminator can judge whether a behavior is expert-like using various possible
features in this case. Solving this open problem is out of the scope of this paper. In the presence of
such a difﬁculty, EI can still achieve good sample efﬁciency in most of the tasks.

8

Finger

Cheetah

Walker

Hopper

e
c
n
a
m
r
o
f
r
e
P

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

BC
DAC
SQIL
ValueDICE
Ours

0

50K

100K
Steps

150K

200K

0

50K

100K
Steps

150K

200K

0

50K

100K
Steps

150K

200K

0

50K

100K
Steps

150K

200K

Finger

Cheetah

Walker

Hopper

e
c
n
a
m
r
o
f
r
e
P

1.0

0.8

0.6

0.4

0.2

0.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

BC
DAC
SQIL
ValueDICE
VMAIL
Ours

0

50K

100K
Steps

150K

200K

0

50K

100K
Steps

150K

200K

0

50K

100K
Steps

150K

200K

0

50K

100K
Steps

150K

200K

Figure 6: The performance curves of our method with and without BC actions at each expansion.
The plots are sorted according to the difﬁculty of the corresponding task. The leftmost one is the
most difﬁcult task, Humanoid. The results are averaged over three seeds. BC actions have a great
impact on the sample efﬁciency and performance.

5.4 Analysis

Effect of BC We carry out ablation analysis on the BC to see whether it helps in our method. We
set α = 0 to remove all the BC actions and see how our method’s performance and sample efﬁciency
will change. The result is shown in Figure 6. We ﬁnd that the performance of our method degrades
after we remove all the BC actions in MCTS. The effect is task-speciﬁc, and we ﬁnd BC is more
helpful in those challenging tasks. For example, in tasks that are high-dimensional or have a hard
exploration process like Humanoid and Hopper, removing BC actions will make the learning process
stuck in local minima. At that local minima, the agent struggles to ﬁnd the correct action that matches
the expert’s behavior. Although removing BC actions does not trap the agent in local minima in some
relatively simpler tasks like Cheetah and Walker, it slows down the training. It doubles the number of
online interactions to reach expert performance. This result conﬁrms that using BC actions can indeed
provide a good solution to the distribution matching problem, which can help to speed up learning and
improve performance. We also notice that even when we remove the BC actions, the method is still
able to outperform the previous baselines; this suggests that planning with AIL alone is also powerful.

Other Ways to use BC We then study another two variants
of using BC: (1). BC-Ann. This variant does not use BC
actions in MCTS but exerts an annealing BC loss to the pol-
icy network like [20]. (2). BC-Rep. This variant does not
use BC actions in MCTS but still uses BC loss to regularize
the representation. We test these variants on the Humanoid
Walk (Figure 7). We ﬁnd that these variants do not lead to an
essential improvement. For BC-Ann, it harms the performance
in the early stage (before 100k) since the BC regularization will
constrain the agent’s policy near the BC policy, which contains
an error and hinders learning. The agent only starts to acquire
meaningful behavior after the regularization decays, but at that time, BC does not help much and can
not lead to improvement. Compared with BC-Ann, BC-Reg is more helpful here. This is possibly
because BC-Reg makes the encoder focus on more useful features. However, BC-Reg still gets
stuck in a local minimum. This result suggests that using BC actions directly for exploration can be
essential for improving AIL. Using BC simply as a regularizer may not be the ideal approach though
it can be useful sometimes.

Figure 7: Results of different ways
of using BC.

Ablation of Planning
In this part, we study to what extent planning can help to learn and whether
insufﬁcient search in MCTS leads to inferior performance. We study K, the number of sampled
actions at each node, and N , the number of simulations. The default value of K and N in the previous
experiments are 16 and 50. We sweep K ∈ {4, 8, 16, 24} and N ∈ {5, 10, 25, 50} to evaluate their
effects. We collect the result on the state-based Cheetah, Walker, and Hopper task and report the
averaged relative performance change at the given budget used in previous experiments (see Table 3).
The general trend is that larger K and N lead to better imitation results. We ﬁnd that varying K only
affects the performance a little, and K = 4 can also work well. Compared with K, N has a larger

9

Humanoid

Hopper

Cheetah

Walker

Finger

e
c
n
a
m
r
o
f
r
e
P

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

Ours (w/o BC)
Ours (w/ BC)

0

200K

400K

0

20K

40K

0

20K

40K

0

20K

40K

0

20K

40K

Steps

Steps

Steps

Steps

Steps

0.8

e
c
n
a
m
r
o
f
r
e
P

0.6

0.4

0.2

0.0

Humanoid

Ours (w/o BC)
Ours (BC Ann.)
Ours (BC Rep.)
Ours (w/ BC)

0

100K

200K

300K

400K

500K

Steps

impact. When the number of simulations becomes small, the performance drops signiﬁcantly. This
result also explains why we can achieve a large improvement over DAC even without BC.

Table 3: Ablation of planning. We report the relative change of performance at the given budget.

Param K = 4 K = 8 K = 16 K = 24

N = 5

N = 10 N = 25 N = 50

Result −8.4% −3.2%

0.0%

1.1%

−27.3% −15.5% −4.6%

0.0%

6 Discussion

In this paper, we presented EfﬁcientImitate, an MCTS-based imitation learning algorithm. We
extended AIL to a model-based setting and solved it with MCTS in a sample-efﬁcient way. Moreover,
we proposed a method to unify BC and AIL in MCTS, enjoying the beneﬁts of both. Experimental
results in state-based and image-based tasks showed that EfﬁcientImitate can achieve state-of-the-art
sample efﬁciency and performance.

Limitations One limitation of this work is that the computation process of MCTS is more expensive
compared with that of the model-free methods, though this is a common issue of model-based methods.
One possible approach to mitigate this issue can be using better MCTS acceleration methods [51].
Besides, in this paper we did not study the long horizon problem with multiple objects, which is a
common case in the robotic manipulation. However, this requires the model to predict the interaction
with multiple objects, which is still a challenging open problem in the learning community [36] and
orthogonal to our contribution. We believe that our framework can be combined with the works in
this ﬁeld to handle this challenge.

Future Work There are many problems to study along our direction. First, since we only use
the vanilla AIL algorithm here, it is interesting to see if using more advanced algorithms such as
optimal-transport-based learning [5] will make our algorithm more powerful. Second, due to the
modularity of our method, one can try to extend EfﬁcientImitate to more general settings like RL
with demonstration, which will also favor the application scenarios. Third, in this work we consider
an online learning setting, one possible future direction is to study the use of EfﬁcientImitate on the
existing ofﬂine interaction dataset to further reduce the dependence on in-environment samples.

In conclusion, we believe that this work shows a promising direction and opens up new possibilities
for model-based methods in robot learning.

Acknowledgments and Disclosure of Funding

This work is supported by the Ministry of Science and Technology of the People’s Republic of China,
the 2030 Innovation Megaprojects “Program on New Generation Artiﬁcial Intelligence” (Grant No.
2021AAA0150000). This work is also supported by a grant from the Guoqiang Institute, Tsinghua
University.

References

[1] M. Bain and C. Sammut. A framework for behavioural cloning. In Machine Intelligence 15,

pages 103–129, 1995.

[2] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba.

Openai gym. arXiv preprint arXiv:1606.01540, 2016.

[3] X. Chen and K. He. Exploring simple siamese representation learning. In IEEE Conference on

Computer Vision and Pattern Recognition, 2021.

[4] X. Chen, S. Toyer, C. Wild, S. Emmons, I. Fischer, K.-H. Lee, N. Alex, S. H. Wang, P. Luo,
S. Russell, et al. An empirical investigation of representation learning for imitation.
In
Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021.

10

[5] R. Dadashi, L. Hussenot, M. Geist, and O. Pietquin. Primal wasserstein imitation learning. In

International Conference on Learning Representations, 2021.

[6] J. Fu, K. Luo, and S. Levine. Learning robust rewards with adversarial inverse reinforcement

learning. In International Conference on Learning Representations, 2018.

[7] S. Fujimoto, H. Hoof, and D. Meger. Addressing function approximation error in actor-critic

methods. In International Conference on Machine Learning, 2018.

[8] C. Gelada, S. Kumar, J. Buckman, O. Nachum, and M. G. Bellemare. Deepmdp: Learning
continuous latent space models for representation learning. In International Conference on
Machine Learning, 2019.

[9] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy
deep reinforcement learning with a stochastic actor. In International Conference on Machine
Learning, 2018.

[10] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent
dynamics for planning from pixels. In International Conference on Machine Learning, 2019.

[11] D. Hafner, T. P. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by

latent imagination. In International Conference on Learning Representations, 2020.

[12] D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models.

In International Conference on Learning Representations, 2021.

[13] S. Haldar, V. Mathur, D. Yarats, and L. Pinto. Watch and match: Supercharging imitation with

regularized optimal transport. In Conference on Robot Learning, 2022.

[14] N. Hansen, X. Wang, and H. Su. Temporal difference learning for model predictive control. In

International Conference on Machine Learning, 2022.

[15] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan, J. Quan,
A. Sendonaris, I. Osband, et al. Deep q-learning from demonstrations. In AAAI Conference on
Artiﬁcial Intelligence, 2018.

[16] J. Ho and S. Ermon. Generative adversarial imitation learning. In Neural Information Processing

Systems, 2016.

[17] T. Hubert, J. Schrittwieser, I. Antonoglou, M. Barekatain, S. Schmitt, and D. Silver. Learning
and planning in complex action spaces. In International Conference on Machine Learning,
2021.

[18] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. In International Conference on Machine Learning, 2015.

[19] M. Janner, J. Fu, M. Zhang, and S. Levine. When to trust your model: Model-based policy

optimization. In Neural Information Processing Systems, 2019.

[20] R. Jena, C. Liu, and K. P. Sycara. Augmenting GAIL with BC for sample efﬁcient imitation

learning. In Conference on Robot Learning, 2020.

[21] B. Kang, Z. Jie, and J. Feng. Policy optimization with demonstrations.

In International

Conference on Machine Learning, 2018.

[22] R. Kidambi, J. Chang, and W. Sun. Mobile: Model-based imitation learning from observation

alone. In Neural Information Processing Systems, 2021.

[23] I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson. Discriminator-actor-
critic: Addressing sample inefﬁciency and reward bias in adversarial imitation learning. In
International Conference on Learning Representations, 2019.

[24] I. Kostrikov, O. Nachum, and J. Tompson. Imitation learning via off-policy distribution matching.

In International Conference on Learning Representations, 2020.

11

[25] M. Laskey, J. Lee, R. Fox, A. Dragan, and K. Goldberg. Dart: Noise injection for robust

imitation learning. In Conference on Robot Learning, 2017.

[26] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Reinforcement learning

with augmented data. In Neural Information Processing Systems, 2020.

[27] M. Laskin, A. Srinivas, and P. Abbeel. CURL: Contrastive unsupervised representations for

reinforcement learning. In International Conference on Machine Learning, 2020.

[28] A. X. Lee, A. Nagabandi, P. Abbeel, and S. Levine. Stochastic latent actor-critic: Deep
reinforcement learning with a latent variable model. Neural Information Processing Systems,
2020.

[29] Z. Li, T. Xu, Y. Yu, and Z.-Q. Luo. Rethinking valuedice: Does it really improve performance?

In International Conference on Learning Representations, 2022.

[30] A. L. Maas, A. Y. Hannun, A. Y. Ng, et al. Rectiﬁer nonlinearities improve neural network

acoustic models. In International Conference on Machine Learning, 2013.

[31] Y. Mu, Y. Zhuang, B. Wang, G. Zhu, W. Liu, J. Chen, P. Luo, S. Li, C. Zhang, and J. Hao. Model-
based reinforcement learning via imagination with derived memory. In Neural Information
Processing Systems, 2021.

[32] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming exploration
in reinforcement learning with demonstrations. In IEEE International Conference on Robotics
and Automation, 2018.

[33] A. Y. Ng, S. J. Russell, et al. Algorithms for inverse reinforcement learning. In International

Conference on Machine Learning, 2000.

[34] M. Orsini, A. Raichuk, L. Hussenot, D. Vincent, R. Dadashi, S. Girgin, M. Geist, O. Bachem,
O. Pietquin, and M. Andrychowicz. What matters for adversarial imitation learning? In Neural
Information Processing Systems, 2021.

[35] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning
library. In Neural Information Processing Systems, 2019.

[36] H. Qi, X. Wang, D. Pathak, Y. Ma, and J. Malik. Learning long-term visual dynamics with
region proposal interaction networks. In International Conference on Learning Representations,
2021.

[37] R. Rafailov, T. Yu, A. Rajeswaran, and C. Finn. Visual adversarial imitation learning using

variational models. In Neural Information Processing Systems, 2021.

[38] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine.
Learning complex dexterous manipulation with deep reinforcement learning and demonstrations.
In Robotics: Science and Systems, 2018.

[39] S. Reddy, A. D. Dragan, and S. Levine. SQIL: imitation learning via reinforcement learning

with sparse rewards. In International Conference on Learning Representations, 2020.

[40] S. Ross and D. Bagnell. Efﬁcient reductions for imitation learning. In International Conference

on Artiﬁcial Intelligence and Statistics, 2010.

[41] F. Sasaki, T. Yohira, and A. Kawaguchi. Sample efﬁcient imitation learning for continuous

control. In International Conference on Learning Representations, 2018.

[42] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lock-
hart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a
learned model. Nature, 588(7839):604–609, 2020.

[43] J. Schrittwieser, T. Hubert, A. Mandhane, M. Barekatain, I. Antonoglou, and D. Silver. Online
and ofﬂine reinforcement learning by planning with a learned model. In Neural Information
Processing Systems, 2021.

12

[44] M. Schwarzer, A. Anand, R. Goel, R. D. Hjelm, A. C. Courville, and P. Bachman. Data-efﬁcient
reinforcement learning with self-predictive representations. In International Conference on
Learning Representations, 2021.

[45] M. Schwarzer, N. Rajkumar, M. Noukhovitch, A. Anand, L. Charlin, R. D. Hjelm, P. Bachman,
and A. C. Courville. Pretraining representations for data-efﬁcient reinforcement learning. In
Neural Information Processing Systems, 2021.

[46] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural
networks and tree search. Nature, 529(7587):484–489, 2016.

[47] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki,
J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.

[48] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning

Research, 9(11), 2008.

[49] D. Yarats, R. Fergus, A. Lazaric, and L. Pinto. Mastering visual continuous control: Im-
In International Conference on Learning

proved data-augmented reinforcement learning.
Representations, 2022.

[50] D. Yarats, I. Kostrikov, and R. Fergus. Image augmentation is all you need: Regularizing deep
reinforcement learning from pixels. In International Conference on Learning Representations,
2020.

[51] W. Ye, P. Abbeel, and Y. Gao. Spending thinking time wisely: Accelerating mcts with virtual

expansions. In Neural Information Processing Systems, 2022.

[52] W. Ye, S. Liu, T. Kurutach, P. Abbeel, and Y. Gao. Mastering atari games with limited data. In

Neural Information Processing Systems, 2021.

[53] T. Yu, C. Lan, W. Zeng, M. Feng, Z. Zhang, and Z. Chen. Playvirtual: Augmenting cycle-
consistent virtual trajectories for reinforcement learning. In Neural Information Processing
Systems, 2021.

[54] X. Zhang, Y. Li, Z. Zhang, and Z.-L. Zhang. f-gail: Learning f-divergence for generative

adversarial imitation learning. In Neural Information Processing Systems, 2020.

[55] Z. Zhu, K. Lin, B. Dai, and J. Zhou. Off-policy imitation learning from observations. In Neural

Information Processing Systems, 2020.

13

A Implementation

In this part, we introduce our detailed implementation of EfﬁcientImitate. Our code is available at
https://github.com/zhaohengyin/EfficientImitate.

A.1 Model Details

Our model is based on the EfﬁcientZero [52] model. It is composed of several neural networks: rep-
resentation network f , dynamics network g, value network V , policy network π, BC policy network
πBC, discriminator network D, projector network, and predictor network. We use PyTorch [35] to
implement these networks. Their detailed structures are as follows.

A.1.1 State-based experiments

Representation Network The representation network is an MLP with one hidden layer of size 256.
Its output dimension is 128. It uses LeakyReLU [30] as the hidden layer’s activation function. The
output activation function is identity.

Dynamics Network The dynamics network is an MLP with one hidden layer of size 256. It con-
catenates the representation and the action as the input. Its output dimension is 128 (representation’s
dimension). It uses LeakyReLU as the hidden layer’s activation function. The output activation
function is identity.

Value Network The value network is an MLP with one hidden layer of size 128 (256 for Humanoid).
It uses LeakyReLU as the hidden layer’s activation function. The output activation function is identity.
We use the categorical value representation introduced in MuZero [42]. The value prediction is
discretized into 401 bins to represent the value between [−100, 100]. Therefore, the output dimension
of value network is 401.

Policy & BC Network The policy and the BC policy networks are both MLPs with one hidden
layer of size 128 (256 for Humanoid). These MLPs use LeakyReLU as the hidden layer’s activation
function. Their output activation functions are identity. They produce a Squarshed-Normal (Tanh-
Normal) action distribution [9], which is determined by a predicted mean and a predicted logstd.
Therefore, the output dimension is twice of the dimension of the action space.

Discriminator Network The discriminator network is an MLP with one hidden layer of size 128.
It uses LeakyReLU as hidden layer’s activation function. The output activation function is sigmoid.

Projector Network The projector network is an MLP with one hidden layer of size 512. It uses
ReLU [30] as the hidden layer’s activation function. The output activation function is identity. The
output dimension (projection dimension) is 128.

Predictor Network The predictor network is an MLP with one hidden layer of size 512. It uses
ReLU as the hidden layer’s activation function. The output activation function is identity. The output
dimension is 128.

A.1.2

Image-based experiments

Representation Network The representation network consists of a four layer convolutional neural
network and an MLP. Its structure is as follows.

• Convolution layer. Input dim: 12. Output dim: 32. Kernel Size: 5. Stride: 2. Padding: 2.

• ReLU activation.

• Convolution layer. Input dim: 32. Output dim: 32. Kernel Size: 3. Stride: 2. Padding: 1.

• ReLU activation.

• Convolution layer. Input dim: 32. Output dim: 32. Kernel Size: 3. Stride: 2. Padding: 1.

• ReLU activation.

14

• Convolution layer. Input dim: 32. Output dim: 32. Kernel Size: 3. Stride: 2. Padding: 1.

• Flatten.

• Linear layer. Output dim: 128.

• ReLU activation.

Dynamics Network The dynamics network is an MLP with one hidden layer of size 256. It con-
catenates the representation and the action as the input. Its output dimension is 128 (representation’s
dimension). It uses ReLU as the hidden layer’s activation function. The output activation function is
also ReLU.

Value Network The value network is an MLP with two hidden layers of size 100. It uses ReLU
as the hidden layer’s activation function. The output activation function is identity. We use the
categorical value representation introduced in MuZero. The value prediction is discretized into 401
bins to represent the value between [−40, 40]. Therefore, the output dimension of value network is
401.

Policy & BC Network The policy and the BC policy networks are both MLPs with two hidden
layers of size 100. These MLPs use ReLU as the hidden layer’s activation function. Their output
activation functions are identity. They produce a Squarshed-Normal action distribution, which is
determined by a predicted mean and a predicted logstd. Therefore, the output dimension of them is
twice of the dimension of the action space.

Discriminator Network The discriminator network is an MLP with one hidden layer of size 100.
It uses ReLU as hidden layer’s activation function. The output activation function is sigmoid.

Projector Network The projector network is an MLP. Its structure is as follows.

• Linear layer. Output dim: 1024. BatchNorm [18] with momentum 0.1. ReLU activation.

• Linear layer. Output dim: 1024. BatchNorm with momentum 0.1. ReLU activation.

• Linear layer. Output dim: 1024. BatchNorm with momentum 0.1.

Predictor Network The predictor network is an MLP. Its structure is as follows.

• Linear layer. Output dim: 512. BatchNorm with momentum 0.1. ReLU activation.

• Linear layer. Output dim: 1024.

A.2 MCTS Details

Our MCTS implementation is mainly based on the Sampled MuZero [17]. We also apply modiﬁca-
tions proposed by the EfﬁcientZero. The detailed procedure is as follows.

Expansion For the task having a continuous action space, we can not enumerate all the possible
actions at a node as the original MCTS algorithm. To solve this problem, we use the sampling method
proposed by the Sampled MuZero. For the expansion of a node s (in the representation space), we
sample K actions {ai}K
i=1 from current policy π(a|s). In this work we propose to integrate BC
actions into MCTS, then we actually sample from

˜π(a|s) := (1 − α)π(a|s) + απBC(a|s).

(8)

Here α = 0.25 is a mixture factor.

Selection For the action selection, we selects action a∗ from the sampled actions that maximize the
probabilistic upper conﬁdence bound

a∗ = arg max
a∈{ai}

Q(s, a) + c(s)ˆπ(a|s)

(cid:112)(cid:80)

b N (s, b)
1 + N (s, a)

,

(9)

15

where ˆπ(a|s) = 1
K
denotes the times that this pair is visited in MCTS. c(s) is a weighting coeffcient deﬁned by

i δ(a, ai). Q(s, a) is the current Q-estimation of the pair (s, a). N (s, a)

(cid:80)

c(s) = c1 + log

1 + c2 + (cid:80)
c2

b N (s, b)

,

(10)

where c1 = 1.25, c2 = 19625.

To encourage exploration, we also inject Dirichlet noise to ˆπ(a|s) at the root node. So ˆπ(a|s) becomes

ˆπ(a|s) := (1 − ρ)ˆπ(a|s) + ρND(ξ).

(11)

Here, ρ = 0.25 is a mixture factor. ND(ξ) is the Dirichlet distribution, and ξ is set to 0.3.

At the root node of MCTS, we use the discriminator network D and value network V to calculate
Q-value by its original deﬁnition: Q(s, a) = R(s, a) + γV (g(s, a)). At the other nodes, we use the
mean Q-value calculation used by the EfﬁcientZero.

Simulation & Backup The simulation and the backup process is the same as EfﬁcientZero’s
implementation, and we refer the readers to EfﬁcientZero for details.

A.3 Training Details and Hyperparameters

Finally, we introduce some important training details and the hyperparameters.

Initialization We initialize the weights and biases of the last layer of policy, BC policy, value, and
discriminator network to be zero. The other parameters are initialized by the default initializers in
PyTorch.

Discriminator
tor [34]. We also apply gradient penalty to the discriminator network.

In AIL training, it is very useful to apply the gradient penalty to the discrimina-

Target Update We propose to use a target model for the calculation of policy, value, and AIL
reward during reanalyze. The target model is updated periodically during training subject to an update
frequency.

The training hyperparameters used in the state-based experiments are in Table 4. The training
hyperparameters used in the image-based experiments are in Table 5.

B Environment Details

B.1 State-based experiments

The setup of each task in the state-based experiments is in Table 6.

B.2 Image-based experiments

The setup of of each task in the image-based experiments is in Table 7. We use a 48 × 48 resolution
in the image-based experiments.

C Other Ablations

We also perform ablations on the target discriminator and the multi-step discriminator loss. To
evaluate the effect of the target discriminator, we use the latest model to calculate AIL reward in
the value target. To evaluate the effect of the multi-step discriminator loss, we replace it with the
single-step discriminator loss. We conduct experiments on the state-based and image-based Walker
and Cheetah. The results are shown in Figure 9. We ﬁnd that removing these components will
not only lead to instability in training but also harm the performance. Compared with the target
discriminator, the multi-step discriminator loss has a larger impact on the image-based tasks.

16

D Computation Resources

All of our experiments are conducted on a server with 4 NVIDIA RTX 3090 GPUs, 64 CPU cores,
and 256GB RAM. For the most of state-based and image-based experiments except Humanoid Walk
and image-based Hopper Hop, our experiments require 12-18 hours of training. The main bottleneck
is at the Reanalyse [43, 52], where the minibatch cannot be produced and sent to the training loop at
a high frequency. We are improving the computation efﬁciency by using better parallel computation
implementation and applying MCTS speed up techniques.

E Visualization

One approach to interpret the learned model is by the t-SNE [48] plot. We use the image-based
Walker experiment as an example. We use the trained model at 100k env steps to generate the state
embeddings of one expert trajectory, and in environment trajectory at 0k, 25k, 50k, 75k, and 100k
steps. Then we use t-SNE to visualize the embeddings on the 2D plane. As is shown in the Figure 8,
the agent’s trajectory gradually matches expert’s trajectory (blue) during training. Moreover, the
expert’s trajectory has a circle structure, which represents the periodic pattern of the Walker’s walking
behavior. Therefore, our model can represent the environment in a meaningful way.

Figure 8: The t-SNE plot of the learned state embeddings.

Figure 9: The ablation of target discriminator and multi-step discriminator loss. The results are
averaged over three seeds. The shaded area displays the range of one standard deviation.

17

Cheetah (State)

Walker (State)

Cheetah (Image)

Walker (Image)

1.0

0.8

e
c
n
a
m
r
o
f
r
e
P

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

Ours(No Target)
Ours(No Multi)
Ours

0

10K

20K

30K

40K

50K

0

10K

20K

30K

40K

50K

0

10K

20K

30K

40K

50K

0

20K

40K

60K

80K

100K

Steps

Steps

Steps

Steps

Table 4: Hyperparameters for the state-based experiments

Discount Factor
Minibatch Size
Optimizer
Optimizer: Learning Rate
Optimizer: Momentum
Optimizer: Weight Decay
Maximum Gradient Norm
Unroll Steps
TD Steps
BC Loss Coeff.
Value Loss Coeff.
Policy Loss Coeff.
Discriminator Loss Coeff.
Gradient Penalty
Target Update Interval
Consistency Loss Coeff.
Reanalyze Ratio
Number of Simulations in MCTS
Number of Sampled Actions
BC Ratio α

0.99
256
SGD
0.01
0.9
1e-4
10
5
1
0.01
1.0
1.0
0.1 (1.0 for Humanoid)
1.0
200
2.0
1.0
50
16
0.25

Table 5: Hyperparameters for the image-based experiments

Discount Factor
Minibatch Size
Stacked Frames
Optimizer
Optimizer: Learning Rate
Optimizer: Momentum
Optimizer: Weight Decay
Maximum Gradient Norm
Unroll Steps
TD Steps
BC Loss Coeff.
Value Loss Coeff.
Policy Loss Coeff.
Discriminator Loss Coeff.
Gradient Penalty
Target Update Interval
Consistency Loss Coeff.
Reanalyze Ratio
Number of Simulations in MCTS
Number of Sampled Actions
BC Ratio α

18

0.99
128
4
SGD
0.02
0.9
1e-4
10
5
1
0.01
1.0
1.0
0.1
1.0
200
20.0
1.0
50
16
0.25

Table 6: Task setup in the state-based experiments

Task

Action Repeat

Expert Performance

Cartpole Swingup
Ball-in-cup Catch
Reacher Easy
Finger Spin
Walker Walk
Cheetah Run
Hopper Hop
Humanoid Walk

8
4
4
4
4
4
4
2

881.3
920.1
911.4
574.2
865.6
607.3
300.0
782.8

Table 7: Task setup in the state-based experiments

Task

Action Repeat

Expert Performance

Cartpole Swingup
Ball-in-cup Catch
Reacher Easy
Finger Spin
Walker Walk
Cheetah Run
Hopper Hop

881.3
920.1
911.4
574.2
873.5
607.3
300.0

8
4
4
4
2
4
4

19

","2 2 0 2 t c O 8 1 ] G L . s c [ 1 v 8 9 5 9 0 . 0 1 2 2 : v i X r a Planning for Sample Efﬁcient Imitation Learning Zhao-Heng Yin∗ Weirui Ye†† Qifeng Chen∗ Yang Gao†‡§ ∗HKUST †Tsinghua University ‡Shanghai Qi Zhi Institute Abstract Imitation learning is a class of promising policy learning algorithms that is free from many practical issues with reinforcement learning, such as the reward design issue and the exploration hardness. However, the current imitation algorithm strug- gles to achieve both high performance and high in-environment sample efﬁciency simultaneously. Behavioral Cloning (BC) does not need in-environment interac- tions, but it suffers from the covariate shift problem which harms its performance. Adversarial Imitation Learning (AIL) turns imitation learning into a distribution matching problem. It can achieve better performance on some tasks but it requires a large number of in-environment interactions. Inspired by the recent success of EfﬁcientZero in RL, we propose EfﬁcientImitate (EI), a planning-based imitation learning method that can achieve high in-environment sample efﬁciency and per- formance simultaneously. Our algorithmic contribution in this paper is two-fold. First, we extend AIL into the MCTS-based RL. Second, we show the seemingly incompatible two classes of imitation algorithms (BC and AIL) can be naturally uniﬁed under our framework, enjoying the beneﬁts of both. We benchmark our method not only on the state-based DeepMind Control Suite, but also on the image version which many previous works ﬁnd highly challenging. Experimental results show that EI achieves state-of-the-art results in performance and sample efﬁciency. EI shows over 4x gain in performance in the limited sample setting on state-based and image-based tasks and can solve challenging problems like Humanoid, where previous methods fail with small amount of interactions. Our code is available at https://github.com/zhaohengyin/EfficientImitate. 1 Introduction The real-world sequential decision process in robotics is highly challenging. Robots have to handle high dimensional input such as images, need to solve long horizon problems, some critical timesteps need highly accurate maneuver, and the learning process on the real robot has to be sample efﬁcient. Imitation learning is a promising approach to solving those problems, given a small dataset of expert demonstrations. However, current imitation algorithms struggle to achieve these goals simultaneously. There are two kinds of popular imitation learning algorithms, Behavior Cloning (BC) and Adversarial Imitation Learning (AIL). BC formulates imitation learning as a supervised learning problem. It needs no in-environment samples, but it suffers from the covariate shift issue [40], often leading to test time performance degradation. Adversarial Imitation Learning (AIL) [16, 6] casts imitation learning as a distribution matching problem. Though AIL suffers less from the covariate shift problem and can perform better than BC on some domains, it requires impractical number of online interactions [20, 23] and can perform badly on image inputs [37]. These drawbacks heavily limit its application in ﬁelds like robotics, where physical robot time matters. In summary, current imitation ∗zhaoheng.yin@connect.ust.hk, cqf@ust.hk †ywr20@mails.tsinghua.edu.cn, gaoyangiiis@tsinghua.edu.cn §Corresponding author. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). Figure 1: Left: The system-level overview of EfﬁcientImitate. The agent (yellow area) takes actions in the environment and stores the data in the replay buffer. The data in the replay buffer and expert buffer are then used to train a model and AIL reward. The planning module then searches for improved policy and value in each state in the replay buffer, based on which the policy and value networks are optimized. Right: The planning procedure. We use a continuous version EfﬁcientZero as our planner. We ﬁnd that MCTS uniquely beneﬁts from BC and can unify BC and AIL. For the expansion of each node, we sample actions from both the current policy (black arrow) and a BC policy (blue arrow). We use AIL reward (yellow cube) to encourage long-term distribution matching. MCTS searches (pink area) for best actions to maximize the cumulative AIL reward and update the estimated value and policy of the root node. The output of the planning procedure is the value estimate and the policy estimate of s, and the value and policy networks are optimized to ﬁt them. learning algorithms fail to achieve high online testing performance and high in-environment sample efﬁciency at the same time. Further, the two types of imitation algorithms seem to be incompatible since they are based on two completely different training objectives. Previous work ﬁnds that it is hard to unify them naively[34]. Inspired by the recent success in sample efﬁcient RL, such as EfﬁcientZero [52], we propose a planning-based imitation algorithm named EfﬁcientImitate (EI) that achieves high test performance and high in-environment sample efﬁciency at the same time. Our method extends AIL to a model- based setting with multi-step losses under the MCTS-based RL framework. Our algorithm also uniﬁes the two types of the previous imitation algorithms (BC and AIL) naturally, thanks to the planning component of our algorithm. Intuitively, BC gives a coarse solution that is correct most of the time but fails to match the expert’s behavior in the long term. On the other hand, AIL knows the goal of the learning, i.e., matching state action distribution, but doesn’t give the solution directly. Our method’s planning component uniﬁes those two methods and shows a signiﬁcant performance boost, especially in the harder tasks such as Humanoid. We illustrate the detailed procedure in Figure 1. We validate our idea not only on state-based tasks but also on image-based tasks, which relatively few previous algorithms can handle them [37]. EI achieves state-of-the-art results in sample efﬁciency and performance. It shows over 4x gain in performance in the limited sample setting on state-based and image-based tasks. On harder tasks such as Humanoid, the gain is even larger. A by-product of this paper is that we extend the recent EfﬁcientZero algorithm to the continuous action space. We open-source the code at https://github.com/zhaohengyin/EfficientImitate to facilitate future research. Our contributions in this paper are summarized as follows. • We present EfﬁcientImitate, a sample efﬁcient imitation learning algorithm based on MCTS. • We identify that MCTS can beneﬁt from BC by using BC actions during the search, which is crucial for challenging tasks. EfﬁcientImitate suggests a natural way to unify BC and AIL. • We conduct experiments in both the state and the image domain to evaluate EI. Experimental results show that EI can achieve state-of-the-art sample efﬁciency and performance. 2 2 Related Work 2.1 Imitation Learning Imitation learning (IL) aims to solve the sequential decision-making problems with expert demon- stration data. It has wide applications in games and robotics [4]. Compared with RL, one major beneﬁt of IL is that it can avoid the notorious reward design problem. IL can be divided into two branches: BC [1] and IRL [33]. To solve the covariate shift problem of BC, researchers propose methods like dataset aggregation [40] and noise injection [25]. But these methods either require extra expert queries or exert constraints over the learning process. A recent variant branch of IRL is the Adversarial Imitation Learning (AIL) [16, 6]. AIL models IL as a state-action distribution matching problem. Many works extend AIL by using better distribution metrics such as f -divergence [54] and Wasserstein distance [5]. Though these methods can learn better reward functions and somewhat speed up the training, they do not directly focus on AIL’s sample efﬁciency problem. Some works have drawn attention to sample efﬁciency, for example [23, 41, 55] propose to use off-policy training in AIL training to reduce sample complexity. ValueDICE [24] reformulates AIL objective in an ofﬂine min-max optimization process, but recent work points out that it is an improved form of BC [29]. VMAIL [37] uses a model-based approach to improve sample efﬁciency. It collects latent rollout with a variational model and reduces online interactions. MoBILE [22] also shows a provably sample efﬁcient model-based imitation approach. Compared with these methods, our method introduces MCTS planning to the off-policy imitation learning and uses it to unify BC and AIL to take advantage of both. The idea of combining BC and AIL helps to improve the sample efﬁciency can be traced back to GAIL[16]. GAIL suggests using BC to initialize the policy network, but [34] ﬁnds that this does not work well because the initialized BC knowledge will be corrupted in AIL training. Then, [20] proposes to use an annealed (decaying) BC loss to regularize policy training to solve this problem. But in this work, we ﬁnd that such a BC regularizer can be harmful to exploration when BC is incorrect. One concurrent work mitigates this issue by using adpative BC loss and replacing AIL reward with optimal-transport-based imitation reward [13]. We also notice that AlphaGo [46] involves BC in their method, but they focus on RL rather than IL, and BC is only used to initialize the policy. Different from these methods, EI uses BC actions as candidates in MCTS. 2.2 Sample Efﬁciency in RL The sample efﬁciency problem of imitation learning is closely related to that in RL. One line of work ﬁnds that the reward signal is not a good data source for representation learning in RL and is one reason for sample inefﬁciency. Then they utilize self-supervised representation learning to accelerate representation learning and improve the sample efﬁciency. Researchers propose to use contrastive learning [27], consistency-based learning [44, 53, 52], or pretrained representation [45] for this purpose. Some works also explore the possibility of applying self-supervised representation learning to imitation learning [4]. Another line of work focuses on RL with a learned model, which is promising for sample efﬁcient learning [8, 10–12, 19, 28, 31, 52, 14]. These approaches usually imagine additional rollouts with the learned model or use it as a more compact environment representation for RL. Besides, some works also ﬁnd that data augmentation can effectively improve sample efﬁciency [26, 50, 49]. EI also beneﬁts from these approaches. It includes a model and applies representation learning to boost sample efﬁciency. In application, people also consider to augment RL with the demonstration [15, 38, 32, 21] to improve the sample efﬁciency. This can be viewed as a combination of RL and imitation learning and favor RL on real robots. We believe that our method can also be extended to this setting. 3 Background 3.1 Setting We formalize the sequential decising making problem as Markov Decision Process M = (S, A, R, T ). Here, S is the state space, A is the action space, R is the reward function, and T is the transition dynamics. The agent’s state at timestep t is st ∈ S. The agent takes action at and 3 t=0 γtrt, where γ is a discount factor. receives reward rt = R(st, at). Its state at timestep t + 1 is then st+1 ∼ T (st, at). The objective of the agent is to maximize the return (cid:80)T In the imitation learning problem studied here, the agent has no access to the reward function R and transition dynamics T . It is provided with a ﬁxed expert demonstration dataset D = {τi}. Here, each τi = (sE T ) is an expert trajectory that can achieve high performance in M. The agent can not solicit extra expert demonstrations but can interact with the MDP, observing new states and actions, but not rewards. In this work, we deﬁne (in-environment) sample efﬁciency as the number of online interactions during training. We expect the agent to achieve high performance within a ﬁxed online sample budget. 1 , ...sE T , aE 0 , aE 1 , aE 0 , sE 3.2 BC and AIL BC considers imitation learning as a supervised learning problem. It trains a policy network π to minimize the following loss function: L = −E (sE i ,aE i )∼D log π(aE i |sE i ). (1) AIL treats imitation learning as a distribution matching problem. One typical AIL algorithm is GAIL. It trains a discriminator D to distinguish the agent generated state-action tuple (st, at) from those (sE i ) in the demonstration dataset by minimizing i , aE L = −E (st,at)∼ρ,(sE i ,aE i )∼D (cid:2)log(D(st, at)) + log(1 − D(sE i , aE i ))(cid:3) , (2) where ρ is the state-action distribution induced by the agent. Meanwhile, it trains the agent to maximize the return with respect to the adversarial reward rt = − log(1 − D(st, at)) using any on-policy RL algorithm. 3.3 MuZero and its Extensions Our planning method is based on MuZero [42] and its extensions. MuZero learns an environment model for MCTS. The model consists of an encoder network f , a dynamics network g, and a reward network R. It operates on abstract states [52]. Concretely, it gets the abstract state ht of the current state st by ht = f (st). It can then predicts the future abstract states recursively by ht+1 = g(ht, at), and the rewards by R(ht, at). Besides the model, MuZero also contains a policy network and a value network. The policy network provides a prior over the actions at each node, and the value network calculates the expected return of the node. MuZero uses the model, the policy network, and the value network to search for improved policy and value for each state with MCTS. We refer the readers to the original MuZero paper for details. Sampled MuZero Sampled MuZero [17] extends MuZero from the discrete action domain to the continuous action domain, which is of our interest in this paper. At each node s to expand, Sampled MuZero samples K actions {ai}K i=1 from current policy π(a|s). During the search, it selects action a∗ from the sampled actions that maximize the probabilistic upper conﬁdence bound a∗ = arg max a∈{ai} Q(s, a) + c(s)ˆπ(a|s) (cid:112)(cid:80) b N (s, b) 1 + N (s, a) , (3) (cid:80) where ˆπ(a|s) = 1 i δ(a, ai). Q(s, a) is the current Q-estimation of the pair (s, a). N (s, a) K denotes the times that this pair is visited in MCTS. c(s) is a weighting coeffcient. During policy optimization, MuZero minimizes the Kullback-Leibler divergence between the current policy π and the MCTS statistics πMCTS at the root node DKL(πMCTS||π). EfﬁcientZero We also apply EfﬁcientZero [52] in this paper. EfﬁcientZero improves the sample efﬁciency of MuZero by using a self-supervised representation learning method to regularize the hidden representation. It uses a SimSiam-style structure [3] to enforce the similarity between the predicted future representation and the real future representation. 4 EfﬁcientImitate In this section, we present our EfﬁcientImitate algorithm. We ﬁrst present an MCTS-based approach to solving the AIL problem in Section 4.1. Then we show a simple yet effective method to unify BC 4 Figure 2: Computation ﬂow of loss functions. Left: Multi-step Discriminator Loss. We do not distinguish between the calculation for expert and agent here, and use a superscript (E) to indicate that the computation applies to both. Right: Multi-step BC Loss. It applies to the expert sequences. and AIL with MCTS in Section 4.2. We brieﬂy discuss the implementation in Section 4.3, and the full details can be found in the Appendix. 4.1 Extending AIL to MCTS-based RL Traditionally, the adversarial imitation learning (AIL) algorithm trains a discriminator D between the policy samples and the expert samples and uses some form of D, such as − log(1 − D), as the reward function. Then some model-free RL algorithms are used to maximize the cumulative reward. In MCTS-based RL algorithms, such as MuZero [42] and EfﬁcientZero [52], the reward function is used in the value target computation and the MCTS search. The use in value target computation is similar to prior model-free RL algorithms, where the value target is computed with n-step value bootstrapping on the actual observations. However, during the MCTS search, the rewards are computed on the abstract state obtained by running the forward dynamics function ht+1 = g(ht, at) multiple times. If we were training the discriminator only on actual observations of the expert and the policy rollouts, the discriminator might not generalize well to abstract states outputted by the forward dynamics functions. Therefore, we train the discriminator with the model-based rollout. Speciﬁcally, we sample sequence (st, at+1, ..., at+n) in replay buffer B and expert sequence (sE t(cid:48)+n) in demonstration dataset D and minimizes following multi-step discriminator loss function: t(cid:48)+1, ..., aE t(cid:48) , aE t(cid:48) , aE LD = −E (st,at:t+n)∼B,(sE t(cid:48) ,aE t(cid:48) :t(cid:48)+n (cid:34) n (cid:88) i=0 )∼D log(D(ht+i, at+i)) + log(1 − D(hE t(cid:48)+i, aE (cid:35) t(cid:48)+i)) . (4) Here, ht+i (and ht(cid:48)+i) terms are produced by the forward dynamics in EfﬁcientZero (Figure 2). We use the GAIL transition reward R(h, a) = − log(1 − D(h, a)), and then the MCTS planner searches for action that can maximize cumulative GAIL reward to guarantee long-term distribution matching. Note that V-MAIL also propose a similar discriminator training technique, but under the Dreamer [11] model. Besides, since the discriminator’s input is based on the representation rather than raw input, the discriminator should be trained with the encoder jointly. However, this can lead to a highly non- stationary reward during the bootstrap value calculation. To mitigate this issue, we also propose to use a target discriminator for bootstrap value calculation. This can make the training more stable. Though we use the GAIL reward here, one can also use other kinds of AIL and IRL reward functions proposed in recent research. Using the GAIL reward can already achieve satisfactory performance in our experiments. When the real reward presents, one may also combine this into planning [21]. This may favor application scenarios where handcrafting a reward function is not hard. We do not study this case here and leave it to future work. 4.2 Unifying BC and AIL in MCTS As discussed in related work, researchers realize that using BC can improve AIL’s sample efﬁciency by providing a good initial BC policy or constraining the policy to BC. However, these existing solutions are not good enough in practice. The main pitfall in these methods is that BC knowledge in the policy network is destined to be forgotten if the policy network is trained with the AIL objective, and then BC will no longer be helpful [34]. 5 We observe that MCTS can naturally unify the BC and AIL methods, enjoying the beneﬁt of both and being free from this pitfall. We propose to plug BC actions into MCTS as candidates at each node and use a planning process to search for an improved solution. This time, the BC actions are consistently considered throughout the entire training procedure without being forgotten. Concretely, we train a BC policy πBC and use a mixture policy ˜π for the sampling at each node in MCTS: ˜π = απBC + (1 − α)π. (5) α is a mixture factor, which is ﬁxed during training and π is the current policy. We use α = 0.25 in this paper. This ensures that a small fraction of action samples are from the BC policy. During planning, the BC actions are evaluated and will be frequently visited and selected as output if they can lead to long-term distribution matching. This can then reduce the effort of ﬁnding good expert-like actions from scratch as desired. Moreover, another unique advantage of this procedure is that it does not fully trust BC like [20], which forces the output of the policy network to be close to BC. When BC is wrong due to covariate shifts or insufﬁcient demonstrations, it can neglect these BC actions and allow the policy to search for better actions. This ensures that BC does not hurt training. However, due to the conceptual simplicity, one arising question is whether this approach can be applied to other model-based methods. Here, we take Dreamer [11] as an example. Though Dreamer builds a model of the environment, it only uses the model to roll-out the policy for policy optimization. In other words, the model is not used to evaluate whether a speciﬁc BC action is good or not in the long term, so our idea can not be applied directly to Dreamer. From this example, we see that the core of our idea is to leverage the power of planning, only with which the long-term outcomes of certain (BC) actions can be calculated. For the training of πBC, we minimize the following multi-step BC objective (Figure 2): LBC = E (sE t(cid:48) ,aE t(cid:48):t(cid:48)+n )∼D − log(πBC(aE t(cid:48)+i|hE t(cid:48)+i)) . (6) i=0 This is to avoid distributional shifts during multi-step prediction in MCTS. For the training of the policy, we still minimize DKL(πMCTS||π). (cid:34) n (cid:88) (cid:35) Note that the BC design proposed here is not coupled with AIL. It can go beyond imitation learning and be applied in other robot learning settings, such as RL with demonstration [38]. 4.3 Implementation We ﬁrst implement a continuous version EfﬁcientZero for planning, and the details can be found in the Appendix. The BC policy network is simply a duplicate of the policy network. The discriminator and BC policy networks share the same encoder network with the policy network and value network. The overall loss function for optimization is L = LEZ + λdLD + λbcLBC. (7) LEZ is EfﬁcientZero’s loss function (excluding reward loss). All the networks are trained jointly to minimize this loss function 7. We use the Reanalyze algorithm [43, 52] for ofﬂine training, and we require that all the samples should be reanalyzed. Figure 3: Part of the tasks used in our experiments. From left to right: Reacher, Finger Spin, Cheetah Run, Walker Walk, Hopper Hop, Humanoid Walk. 5 Experiments In this section, we evaluate the sample efﬁciency of the proposed method. We measure the sample efﬁciency by evaluating the performance of an algorithm at a small number of online samples. We also analyze the effect of the BC actions and planning. 6 Table 1: Evaluation result on the state-based DeepMind Control Suite. We use the average score on three random seeds. Our method can achieve the state of the art result compared with the baselines. Task Cartpole Budget BC DAC ValueDICE SQIL Ours 10k 0.59 0.13 ±0.12 0.21 ±0.01 0.23 ±0.01 0.98 ±0.01 Ball 10k 0.44 0.18 ±0.01 0.23 ±0.01 0.27 ±0.05 0.99 ±0.01 Reacher Finger Cheetah Walker Hopper Humanoid 50k 0.83 0.22 ±0.02 0.15 ±0.01 0.21 ±0.02 0.90 ±0.04 50k 0.76 0.53 ±0.05 0.04 ±0.01 0.02 ±0.00 0.99 ±0.00 50k 0.58 0.33 ±0.04 0.50 ±0.08 0.05 ±0.01 0.96 ±0.02 50k 0.16 0.26 ±0.04 0.54 ±0.09 0.11 ±0.03 1.03 ±0.01 50k 0.03 0.00 ±0.00 0.03 ±0.00 0.24 ±0.10 0.92 ±0.02 500k 0.11 0.01 ±0.00 0.00 ±0.00 0.06 ±0.01 0.74 ±0.04 Table 2: Evaluation result on the image-based DeepMind Control Suite. We use the average score on three random seeds. Our method can achieve state-of-the-art results compared with the baselines. Task Cartpole Budget BC DAC ValueDICE SQIL VMAIL Ours 50k 0.30 0.08 ±0.01 0.18 ±0.02 0.26 ±0.03 0.57 ±0.03 0.94 ±0.02 Ball 50k 0.32 0.26 ±0.02 0.27 ±0.02 0.77 ±0.05 0.61 ±0.11 0.93 ±0.01 Finger Cheetah Reacher Walker Hopper 50k 0.14 0.00 ±0.00 0.01 ±0.00 0.00 ±0.01 0.06 ±0.03 1.00 ±0.01 50k 0.37 0.04 ±0.01 0.06 ±0.01 0.06 ±0.00 0.13 ±0.04 0.92 ±0.01 100k 0.26 0.25 ±0.05 0.15 ±0.02 0.36 ±0.04 0.34 ±0.02 0.86 ±0.06 100k 0.15 0.10 ±0.02 0.08 ±0.00 0.32 ±0.05 0.24 ±0.07 0.98 ±0.01 200k 0.02 0.01 ±0.00 0.00 ±0.00 0.04 ±0.02 0.07 ±0.04 0.70 ±0.01 5.1 Setup We use the DeepMind Control Suite [47] for evaluation. We use the following tasks: Cartpole Swingup, Reacher Easy, Ball-in-cup Catch, Finger Spin, Cheetah Run, Walker Walk, Hopper Hop, and Humanoid Walk. We conduct both state-based and image-based experiments. Note that many previous imitation learning works use the OpenAI Gym [2] version of these tasks for evaluation. We ﬁnd that the DMControl version used here brings extra challenges by using more challenging initial states. Take the Walker task as an example; the initial state in OpenAI Gym is standing. However, in DMControl, the agent’s initial state is lying on the ground, and the agent should also learn to stand up ﬁrst from very limited data. For the state-based experiments, we allow 10k-50k online steps in the environment based on the difﬁculty of each task. Since learning a robust and meaningful visual representation requires more data for image-based experiments, we allow 50k-100k online steps. Detailed setup will be shown in the result. We train SAC [9] policies to collect expert demonstrations for imitation learning. The expert demonstrations are not subsampled. We use 5 demonstrations in the state-based experiment, except for Reacher and Humanoid, where we use 20 demonstrations. We use 20 demonstrations in the image-based experiments. 5.2 Baselines (1) DAC DAC [23] is an We present several baselines of sample efﬁcient imitation learning. adversarial off-policy imitation learning method. It matches the distribution of the replay buffer and that of the expert demonstration dataset using the TD3 [7] algorithm. (2) SQIL SQIL [39] is a non-adversarial off-policy imitation learning method. It labels all the expert transitions with reward 1 and non-expert transitions with reward 0. Then it trains a SAC policy over these relabeled data. SQIL is a regularized form of BC. (3) ValueDICE ValueDICE [24] considers imitation learning as a distribution matching problem and solves it with a min-max optimization process. (4) VMAIL VMAIL [37] is a model-based visual imitation learning method. It learns a variational model for simulating on-policy rollouts. We only evaluate VMAIL on the image-based domain, as they did in 7 Figure 4: The performance curve on the state-based tasks. The results are averaged over three seeds. The shaded area displays the range of one standard deviation. Figure 5: The performance curve on the image-based tasks. The results are averaged over three seeds. The shaded area displays the range of one standard deviation. the original paper. Besides the online imitation learning baselines, we also include BC as an ofﬂine baseline. 5.3 Results State-based experiments Table 1 shows the state-based experiments’ evaluation results within the given budget. We also plot the performance curve of four challenging tasks in Figure 4. The performance is normalized to 0.0 and 1.0 with respect to the performance of the random agent and the expert. We ﬁnd that our proposed method can outperform all the baselines by a large margin. Except for BC, these baselines methods could hardly learn meaningful behaviors using a limited online budget. We ﬁnd that DAC is a strong baseline. Its performance can grow to the expert in 200k samples in most tasks except Hopper, where it will eventually get stuck. Our method is much more sample efﬁcient than the best of these baseline methods. For some tasks like Walker Walk and Cheetah Run, our method only requires about 20k steps to reach near-expert performance, equivalent to 80 online trajectories (around 0.5 hours in real). This result is notable for the robotics community. It shows that online imitation learning is possible with only a handful of trials, and applying it directly on a real locomotion robot is possible. Image-based experiments So far, image-based tasks are still challenging for adversarial imitation learning algorithms, and the evaluation of most of the prior AIL works is carried out in the state-based tasks. Table 2 shows the evaluation result within the given budget in the image-based experiments (see Figure 5 for curves). Our method can also learn expert behavior given a slightly larger budget. Still, most of the baselines fail to match experts’ behavior using the given budget. We notice an inherent difﬁculty in learning a robust visual representation for adversarial training in the limited data set in image-based tasks. Discriminator can judge whether a behavior is expert-like using various possible features in this case. Solving this open problem is out of the scope of this paper. In the presence of such a difﬁculty, EI can still achieve good sample efﬁciency in most of the tasks. 8 Finger Cheetah Walker Hopper e c n a m r o f r e P 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 BC DAC SQIL ValueDICE Ours 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K Finger Cheetah Walker Hopper e c n a m r o f r e P 1.0 0.8 0.6 0.4 0.2 0.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 BC DAC SQIL ValueDICE VMAIL Ours 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K Figure 6: The performance curves of our method with and without BC actions at each expansion. The plots are sorted according to the difﬁculty of the corresponding task. The leftmost one is the most difﬁcult task, Humanoid. The results are averaged over three seeds. BC actions have a great impact on the sample efﬁciency and performance. 5.4 Analysis Effect of BC We carry out ablation analysis on the BC to see whether it helps in our method. We set α = 0 to remove all the BC actions and see how our method’s performance and sample efﬁciency will change. The result is shown in Figure 6. We ﬁnd that the performance of our method degrades after we remove all the BC actions in MCTS. The effect is task-speciﬁc, and we ﬁnd BC is more helpful in those challenging tasks. For example, in tasks that are high-dimensional or have a hard exploration process like Humanoid and Hopper, removing BC actions will make the learning process stuck in local minima. At that local minima, the agent struggles to ﬁnd the correct action that matches the expert’s behavior. Although removing BC actions does not trap the agent in local minima in some relatively simpler tasks like Cheetah and Walker, it slows down the training. It doubles the number of online interactions to reach expert performance. This result conﬁrms that using BC actions can indeed provide a good solution to the distribution matching problem, which can help to speed up learning and improve performance. We also notice that even when we remove the BC actions, the method is still able to outperform the previous baselines; this suggests that planning with AIL alone is also powerful. Other Ways to use BC We then study another two variants of using BC: (1). BC-Ann. This variant does not use BC actions in MCTS but exerts an annealing BC loss to the pol- icy network like [20]. (2). BC-Rep. This variant does not use BC actions in MCTS but still uses BC loss to regularize the representation. We test these variants on the Humanoid Walk (Figure 7). We ﬁnd that these variants do not lead to an essential improvement. For BC-Ann, it harms the performance in the early stage (before 100k) since the BC regularization will constrain the agent’s policy near the BC policy, which contains an error and hinders learning. The agent only starts to acquire meaningful behavior after the regularization decays, but at that time, BC does not help much and can not lead to improvement. Compared with BC-Ann, BC-Reg is more helpful here. This is possibly because BC-Reg makes the encoder focus on more useful features. However, BC-Reg still gets stuck in a local minimum. This result suggests that using BC actions directly for exploration can be essential for improving AIL. Using BC simply as a regularizer may not be the ideal approach though it can be useful sometimes. Figure 7: Results of different ways of using BC. Ablation of Planning In this part, we study to what extent planning can help to learn and whether insufﬁcient search in MCTS leads to inferior performance. We study K, the number of sampled actions at each node, and N , the number of simulations. The default value of K and N in the previous experiments are 16 and 50. We sweep K ∈ {4, 8, 16, 24} and N ∈ {5, 10, 25, 50} to evaluate their effects. We collect the result on the state-based Cheetah, Walker, and Hopper task and report the averaged relative performance change at the given budget used in previous experiments (see Table 3). The general trend is that larger K and N lead to better imitation results. We ﬁnd that varying K only affects the performance a little, and K = 4 can also work well. Compared with K, N has a larger 9 Humanoid Hopper Cheetah Walker Finger e c n a m r o f r e P 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 Ours (w/o BC) Ours (w/ BC) 0 200K 400K 0 20K 40K 0 20K 40K 0 20K 40K 0 20K 40K Steps Steps Steps Steps Steps 0.8 e c n a m r o f r e P 0.6 0.4 0.2 0.0 Humanoid Ours (w/o BC) Ours (BC Ann.) Ours (BC Rep.) Ours (w/ BC) 0 100K 200K 300K 400K 500K Steps impact. When the number of simulations becomes small, the performance drops signiﬁcantly. This result also explains why we can achieve a large improvement over DAC even without BC. Table 3: Ablation of planning. We report the relative change of performance at the given budget. Param K = 4 K = 8 K = 16 K = 24 N = 5 N = 10 N = 25 N = 50 Result −8.4% −3.2% 0.0% 1.1% −27.3% −15.5% −4.6% 0.0% 6 Discussion In this paper, we presented EfﬁcientImitate, an MCTS-based imitation learning algorithm. We extended AIL to a model-based setting and solved it with MCTS in a sample-efﬁcient way. Moreover, we proposed a method to unify BC and AIL in MCTS, enjoying the beneﬁts of both. Experimental results in state-based and image-based tasks showed that EfﬁcientImitate can achieve state-of-the-art sample efﬁciency and performance. Limitations One limitation of this work is that the computation process of MCTS is more expensive compared with that of the model-free methods, though this is a common issue of model-based methods. One possible approach to mitigate this issue can be using better MCTS acceleration methods [51]. Besides, in this paper we did not study the long horizon problem with multiple objects, which is a common case in the robotic manipulation. However, this requires the model to predict the interaction with multiple objects, which is still a challenging open problem in the learning community [36] and orthogonal to our contribution. We believe that our framework can be combined with the works in this ﬁeld to handle this challenge. Future Work There are many problems to study along our direction. First, since we only use the vanilla AIL algorithm here, it is interesting to see if using more advanced algorithms such as optimal-transport-based learning [5] will make our algorithm more powerful. Second, due to the modularity of our method, one can try to extend EfﬁcientImitate to more general settings like RL with demonstration, which will also favor the application scenarios. Third, in this work we consider an online learning setting, one possible future direction is to study the use of EfﬁcientImitate on the existing ofﬂine interaction dataset to further reduce the dependence on in-environment samples. In conclusion, we believe that this work shows a promising direction and opens up new possibilities for model-based methods in robot learning. Acknowledgments and Disclosure of Funding This work is supported by the Ministry of Science and Technology of the People’s Republic of China, the 2030 Innovation Megaprojects “Program on New Generation Artiﬁcial Intelligence” (Grant No. 2021AAA0150000). This work is also supported by a grant from the Guoqiang Institute, Tsinghua University. References [1] M. Bain and C. Sammut. A framework for behavioural cloning. In Machine Intelligence 15, pages 103–129, 1995. [2] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [3] X. Chen and K. He. Exploring simple siamese representation learning. In IEEE Conference on Computer Vision and Pattern Recognition, 2021. [4] X. Chen, S. Toyer, C. Wild, S. Emmons, I. Fischer, K.-H. Lee, N. Alex, S. H. Wang, P. Luo, S. Russell, et al. An empirical investigation of representation learning for imitation. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. 10 [5] R. Dadashi, L. Hussenot, M. Geist, and O. Pietquin. Primal wasserstein imitation learning. In International Conference on Learning Representations, 2021. [6] J. Fu, K. Luo, and S. Levine. Learning robust rewards with adversarial inverse reinforcement learning. In International Conference on Learning Representations, 2018. [7] S. Fujimoto, H. Hoof, and D. Meger. Addressing function approximation error in actor-critic methods. In International Conference on Machine Learning, 2018. [8] C. Gelada, S. Kumar, J. Buckman, O. Nachum, and M. G. Bellemare. Deepmdp: Learning continuous latent space models for representation learning. In International Conference on Machine Learning, 2019. [9] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, 2018. [10] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, 2019. [11] D. Hafner, T. P. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020. [12] D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021. [13] S. Haldar, V. Mathur, D. Yarats, and L. Pinto. Watch and match: Supercharging imitation with regularized optimal transport. In Conference on Robot Learning, 2022. [14] N. Hansen, X. Wang, and H. Su. Temporal difference learning for model predictive control. In International Conference on Machine Learning, 2022. [15] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan, J. Quan, A. Sendonaris, I. Osband, et al. Deep q-learning from demonstrations. In AAAI Conference on Artiﬁcial Intelligence, 2018. [16] J. Ho and S. Ermon. Generative adversarial imitation learning. In Neural Information Processing Systems, 2016. [17] T. Hubert, J. Schrittwieser, I. Antonoglou, M. Barekatain, S. Schmitt, and D. Silver. Learning and planning in complex action spaces. In International Conference on Machine Learning, 2021. [18] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, 2015. [19] M. Janner, J. Fu, M. Zhang, and S. Levine. When to trust your model: Model-based policy optimization. In Neural Information Processing Systems, 2019. [20] R. Jena, C. Liu, and K. P. Sycara. Augmenting GAIL with BC for sample efﬁcient imitation learning. In Conference on Robot Learning, 2020. [21] B. Kang, Z. Jie, and J. Feng. Policy optimization with demonstrations. In International Conference on Machine Learning, 2018. [22] R. Kidambi, J. Chang, and W. Sun. Mobile: Model-based imitation learning from observation alone. In Neural Information Processing Systems, 2021. [23] I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson. Discriminator-actor- critic: Addressing sample inefﬁciency and reward bias in adversarial imitation learning. In International Conference on Learning Representations, 2019. [24] I. Kostrikov, O. Nachum, and J. Tompson. Imitation learning via off-policy distribution matching. In International Conference on Learning Representations, 2020. 11 [25] M. Laskey, J. Lee, R. Fox, A. Dragan, and K. Goldberg. Dart: Noise injection for robust imitation learning. In Conference on Robot Learning, 2017. [26] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Reinforcement learning with augmented data. In Neural Information Processing Systems, 2020. [27] M. Laskin, A. Srinivas, and P. Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, 2020. [28] A. X. Lee, A. Nagabandi, P. Abbeel, and S. Levine. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. Neural Information Processing Systems, 2020. [29] Z. Li, T. Xu, Y. Yu, and Z.-Q. Luo. Rethinking valuedice: Does it really improve performance? In International Conference on Learning Representations, 2022. [30] A. L. Maas, A. Y. Hannun, A. Y. Ng, et al. Rectiﬁer nonlinearities improve neural network acoustic models. In International Conference on Machine Learning, 2013. [31] Y. Mu, Y. Zhuang, B. Wang, G. Zhu, W. Liu, J. Chen, P. Luo, S. Li, C. Zhang, and J. Hao. Model- based reinforcement learning via imagination with derived memory. In Neural Information Processing Systems, 2021. [32] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In IEEE International Conference on Robotics and Automation, 2018. [33] A. Y. Ng, S. J. Russell, et al. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning, 2000. [34] M. Orsini, A. Raichuk, L. Hussenot, D. Vincent, R. Dadashi, S. Girgin, M. Geist, O. Bachem, O. Pietquin, and M. Andrychowicz. What matters for adversarial imitation learning? In Neural Information Processing Systems, 2021. [35] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Neural Information Processing Systems, 2019. [36] H. Qi, X. Wang, D. Pathak, Y. Ma, and J. Malik. Learning long-term visual dynamics with region proposal interaction networks. In International Conference on Learning Representations, 2021. [37] R. Rafailov, T. Yu, A. Rajeswaran, and C. Finn. Visual adversarial imitation learning using variational models. In Neural Information Processing Systems, 2021. [38] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. In Robotics: Science and Systems, 2018. [39] S. Reddy, A. D. Dragan, and S. Levine. SQIL: imitation learning via reinforcement learning with sparse rewards. In International Conference on Learning Representations, 2020. [40] S. Ross and D. Bagnell. Efﬁcient reductions for imitation learning. In International Conference on Artiﬁcial Intelligence and Statistics, 2010. [41] F. Sasaki, T. Yohira, and A. Kawaguchi. Sample efﬁcient imitation learning for continuous control. In International Conference on Learning Representations, 2018. [42] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lock- hart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020. [43] J. Schrittwieser, T. Hubert, A. Mandhane, M. Barekatain, I. Antonoglou, and D. Silver. Online and ofﬂine reinforcement learning by planning with a learned model. In Neural Information Processing Systems, 2021. 12 [44] M. Schwarzer, A. Anand, R. Goel, R. D. Hjelm, A. C. Courville, and P. Bachman. Data-efﬁcient reinforcement learning with self-predictive representations. In International Conference on Learning Representations, 2021. [45] M. Schwarzer, N. Rajkumar, M. Noukhovitch, A. Anand, L. Charlin, R. D. Hjelm, P. Bachman, and A. C. Courville. Pretraining representations for data-efﬁcient reinforcement learning. In Neural Information Processing Systems, 2021. [46] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016. [47] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. [48] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(11), 2008. [49] D. Yarats, R. Fergus, A. Lazaric, and L. Pinto. Mastering visual continuous control: Im- In International Conference on Learning proved data-augmented reinforcement learning. Representations, 2022. [50] D. Yarats, I. Kostrikov, and R. Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations, 2020. [51] W. Ye, P. Abbeel, and Y. Gao. Spending thinking time wisely: Accelerating mcts with virtual expansions. In Neural Information Processing Systems, 2022. [52] W. Ye, S. Liu, T. Kurutach, P. Abbeel, and Y. Gao. Mastering atari games with limited data. In Neural Information Processing Systems, 2021. [53] T. Yu, C. Lan, W. Zeng, M. Feng, Z. Zhang, and Z. Chen. Playvirtual: Augmenting cycle- consistent virtual trajectories for reinforcement learning. In Neural Information Processing Systems, 2021. [54] X. Zhang, Y. Li, Z. Zhang, and Z.-L. Zhang. f-gail: Learning f-divergence for generative adversarial imitation learning. In Neural Information Processing Systems, 2020. [55] Z. Zhu, K. Lin, B. Dai, and J. Zhou. Off-policy imitation learning from observations. In Neural Information Processing Systems, 2020. 13 A Implementation In this part, we introduce our detailed implementation of EfﬁcientImitate. Our code is available at https://github.com/zhaohengyin/EfficientImitate. A.1 Model Details Our model is based on the EfﬁcientZero [52] model. It is composed of several neural networks: rep- resentation network f , dynamics network g, value network V , policy network π, BC policy network πBC, discriminator network D, projector network, and predictor network. We use PyTorch [35] to implement these networks. Their detailed structures are as follows. A.1.1 State-based experiments Representation Network The representation network is an MLP with one hidden layer of size 256. Its output dimension is 128. It uses LeakyReLU [30] as the hidden layer’s activation function. The output activation function is identity. Dynamics Network The dynamics network is an MLP with one hidden layer of size 256. It con- catenates the representation and the action as the input. Its output dimension is 128 (representation’s dimension). It uses LeakyReLU as the hidden layer’s activation function. The output activation function is identity. Value Network The value network is an MLP with one hidden layer of size 128 (256 for Humanoid). It uses LeakyReLU as the hidden layer’s activation function. The output activation function is identity. We use the categorical value representation introduced in MuZero [42]. The value prediction is discretized into 401 bins to represent the value between [−100, 100]. Therefore, the output dimension of value network is 401. Policy & BC Network The policy and the BC policy networks are both MLPs with one hidden layer of size 128 (256 for Humanoid). These MLPs use LeakyReLU as the hidden layer’s activation function. Their output activation functions are identity. They produce a Squarshed-Normal (Tanh- Normal) action distribution [9], which is determined by a predicted mean and a predicted logstd. Therefore, the output dimension is twice of the dimension of the action space. Discriminator Network The discriminator network is an MLP with one hidden layer of size 128. It uses LeakyReLU as hidden layer’s activation function. The output activation function is sigmoid. Projector Network The projector network is an MLP with one hidden layer of size 512. It uses ReLU [30] as the hidden layer’s activation function. The output activation function is identity. The output dimension (projection dimension) is 128. Predictor Network The predictor network is an MLP with one hidden layer of size 512. It uses ReLU as the hidden layer’s activation function. The output activation function is identity. The output dimension is 128. A.1.2 Image-based experiments Representation Network The representation network consists of a four layer convolutional neural network and an MLP. Its structure is as follows. • Convolution layer. Input dim: 12. Output dim: 32. Kernel Size: 5. Stride: 2. Padding: 2. • ReLU activation. • Convolution layer. Input dim: 32. Output dim: 32. Kernel Size: 3. Stride: 2. Padding: 1. • ReLU activation. • Convolution layer. Input dim: 32. Output dim: 32. Kernel Size: 3. Stride: 2. Padding: 1. • ReLU activation. 14 • Convolution layer. Input dim: 32. Output dim: 32. Kernel Size: 3. Stride: 2. Padding: 1. • Flatten. • Linear layer. Output dim: 128. • ReLU activation. Dynamics Network The dynamics network is an MLP with one hidden layer of size 256. It con- catenates the representation and the action as the input. Its output dimension is 128 (representation’s dimension). It uses ReLU as the hidden layer’s activation function. The output activation function is also ReLU. Value Network The value network is an MLP with two hidden layers of size 100. It uses ReLU as the hidden layer’s activation function. The output activation function is identity. We use the categorical value representation introduced in MuZero. The value prediction is discretized into 401 bins to represent the value between [−40, 40]. Therefore, the output dimension of value network is 401. Policy & BC Network The policy and the BC policy networks are both MLPs with two hidden layers of size 100. These MLPs use ReLU as the hidden layer’s activation function. Their output activation functions are identity. They produce a Squarshed-Normal action distribution, which is determined by a predicted mean and a predicted logstd. Therefore, the output dimension of them is twice of the dimension of the action space. Discriminator Network The discriminator network is an MLP with one hidden layer of size 100. It uses ReLU as hidden layer’s activation function. The output activation function is sigmoid. Projector Network The projector network is an MLP. Its structure is as follows. • Linear layer. Output dim: 1024. BatchNorm [18] with momentum 0.1. ReLU activation. • Linear layer. Output dim: 1024. BatchNorm with momentum 0.1. ReLU activation. • Linear layer. Output dim: 1024. BatchNorm with momentum 0.1. Predictor Network The predictor network is an MLP. Its structure is as follows. • Linear layer. Output dim: 512. BatchNorm with momentum 0.1. ReLU activation. • Linear layer. Output dim: 1024. A.2 MCTS Details Our MCTS implementation is mainly based on the Sampled MuZero [17]. We also apply modiﬁca- tions proposed by the EfﬁcientZero. The detailed procedure is as follows. Expansion For the task having a continuous action space, we can not enumerate all the possible actions at a node as the original MCTS algorithm. To solve this problem, we use the sampling method proposed by the Sampled MuZero. For the expansion of a node s (in the representation space), we sample K actions {ai}K i=1 from current policy π(a|s). In this work we propose to integrate BC actions into MCTS, then we actually sample from ˜π(a|s) := (1 − α)π(a|s) + απBC(a|s). (8) Here α = 0.25 is a mixture factor. Selection For the action selection, we selects action a∗ from the sampled actions that maximize the probabilistic upper conﬁdence bound a∗ = arg max a∈{ai} Q(s, a) + c(s)ˆπ(a|s) (cid:112)(cid:80) b N (s, b) 1 + N (s, a) , (9) 15 where ˆπ(a|s) = 1 K denotes the times that this pair is visited in MCTS. c(s) is a weighting coeffcient deﬁned by i δ(a, ai). Q(s, a) is the current Q-estimation of the pair (s, a). N (s, a) (cid:80) c(s) = c1 + log 1 + c2 + (cid:80) c2 b N (s, b) , (10) where c1 = 1.25, c2 = 19625. To encourage exploration, we also inject Dirichlet noise to ˆπ(a|s) at the root node. So ˆπ(a|s) becomes ˆπ(a|s) := (1 − ρ)ˆπ(a|s) + ρND(ξ). (11) Here, ρ = 0.25 is a mixture factor. ND(ξ) is the Dirichlet distribution, and ξ is set to 0.3. At the root node of MCTS, we use the discriminator network D and value network V to calculate Q-value by its original deﬁnition: Q(s, a) = R(s, a) + γV (g(s, a)). At the other nodes, we use the mean Q-value calculation used by the EfﬁcientZero. Simulation & Backup The simulation and the backup process is the same as EfﬁcientZero’s implementation, and we refer the readers to EfﬁcientZero for details. A.3 Training Details and Hyperparameters Finally, we introduce some important training details and the hyperparameters. Initialization We initialize the weights and biases of the last layer of policy, BC policy, value, and discriminator network to be zero. The other parameters are initialized by the default initializers in PyTorch. Discriminator tor [34]. We also apply gradient penalty to the discriminator network. In AIL training, it is very useful to apply the gradient penalty to the discrimina- Target Update We propose to use a target model for the calculation of policy, value, and AIL reward during reanalyze. The target model is updated periodically during training subject to an update frequency. The training hyperparameters used in the state-based experiments are in Table 4. The training hyperparameters used in the image-based experiments are in Table 5. B Environment Details B.1 State-based experiments The setup of each task in the state-based experiments is in Table 6. B.2 Image-based experiments The setup of of each task in the image-based experiments is in Table 7. We use a 48 × 48 resolution in the image-based experiments. C Other Ablations We also perform ablations on the target discriminator and the multi-step discriminator loss. To evaluate the effect of the target discriminator, we use the latest model to calculate AIL reward in the value target. To evaluate the effect of the multi-step discriminator loss, we replace it with the single-step discriminator loss. We conduct experiments on the state-based and image-based Walker and Cheetah. The results are shown in Figure 9. We ﬁnd that removing these components will not only lead to instability in training but also harm the performance. Compared with the target discriminator, the multi-step discriminator loss has a larger impact on the image-based tasks. 16 D Computation Resources All of our experiments are conducted on a server with 4 NVIDIA RTX 3090 GPUs, 64 CPU cores, and 256GB RAM. For the most of state-based and image-based experiments except Humanoid Walk and image-based Hopper Hop, our experiments require 12-18 hours of training. The main bottleneck is at the Reanalyse [43, 52], where the minibatch cannot be produced and sent to the training loop at a high frequency. We are improving the computation efﬁciency by using better parallel computation implementation and applying MCTS speed up techniques. E Visualization One approach to interpret the learned model is by the t-SNE [48] plot. We use the image-based Walker experiment as an example. We use the trained model at 100k env steps to generate the state embeddings of one expert trajectory, and in environment trajectory at 0k, 25k, 50k, 75k, and 100k steps. Then we use t-SNE to visualize the embeddings on the 2D plane. As is shown in the Figure 8, the agent’s trajectory gradually matches expert’s trajectory (blue) during training. Moreover, the expert’s trajectory has a circle structure, which represents the periodic pattern of the Walker’s walking behavior. Therefore, our model can represent the environment in a meaningful way. Figure 8: The t-SNE plot of the learned state embeddings. Figure 9: The ablation of target discriminator and multi-step discriminator loss. The results are averaged over three seeds. The shaded area displays the range of one standard deviation. 17 Cheetah (State) Walker (State) Cheetah (Image) Walker (Image) 1.0 0.8 e c n a m r o f r e P 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 Ours(No Target) Ours(No Multi) Ours 0 10K 20K 30K 40K 50K 0 10K 20K 30K 40K 50K 0 10K 20K 30K 40K 50K 0 20K 40K 60K 80K 100K Steps Steps Steps Steps Table 4: Hyperparameters for the state-based experiments Discount Factor Minibatch Size Optimizer Optimizer: Learning Rate Optimizer: Momentum Optimizer: Weight Decay Maximum Gradient Norm Unroll Steps TD Steps BC Loss Coeff. Value Loss Coeff. Policy Loss Coeff. Discriminator Loss Coeff. Gradient Penalty Target Update Interval Consistency Loss Coeff. Reanalyze Ratio Number of Simulations in MCTS Number of Sampled Actions BC Ratio α 0.99 256 SGD 0.01 0.9 1e-4 10 5 1 0.01 1.0 1.0 0.1 (1.0 for Humanoid) 1.0 200 2.0 1.0 50 16 0.25 Table 5: Hyperparameters for the image-based experiments Discount Factor Minibatch Size Stacked Frames Optimizer Optimizer: Learning Rate Optimizer: Momentum Optimizer: Weight Decay Maximum Gradient Norm Unroll Steps TD Steps BC Loss Coeff. Value Loss Coeff. Policy Loss Coeff. Discriminator Loss Coeff. Gradient Penalty Target Update Interval Consistency Loss Coeff. Reanalyze Ratio Number of Simulations in MCTS Number of Sampled Actions BC Ratio α 18 0.99 128 4 SGD 0.02 0.9 1e-4 10 5 1 0.01 1.0 1.0 0.1 1.0 200 20.0 1.0 50 16 0.25 Table 6: Task setup in the state-based experiments Task Action Repeat Expert Performance Cartpole Swingup Ball-in-cup Catch Reacher Easy Finger Spin Walker Walk Cheetah Run Hopper Hop Humanoid Walk 8 4 4 4 4 4 4 2 881.3 920.1 911.4 574.2 865.6 607.3 300.0 782.8 Table 7: Task setup in the state-based experiments Task Action Repeat Expert Performance Cartpole Swingup Ball-in-cup Catch Reacher Easy Finger Spin Walker Walk Cheetah Run Hopper Hop 881.3 920.1 911.4 574.2 873.5 607.3 300.0 8 4 4 4 2 4 4 19","['c', 'planning', 'sample', 'efﬁcient', 'imitation', 'learn', 'abstract', 'imitation', 'learning', 'class', 'promise', 'policy', 'learning', 'algorithm', 'free', 'many', 'practical', 'issue', 'reinforcement', 'learning', 'reward', 'design', 'issue', 'exploration', 'hardness', 'however', 'current', 'imitation', 'strug', 'gle', 'achieve', 'high', 'performance', 'high', 'inenvironment', 'sample', 'efﬁciency', 'simultaneously', 'behavioral', 'cloning', 'need', 'inenvironment', 'interac', 'tion', 'suffer', 'covariate', 'shift', 'problem', 'harm', 'performance', 'adversarial', 'imitation', 'learn', 'ail', 'turn', 'imitation', 'learn', 'distribution', 'matching', 'problem', 'achieve', 'well', 'performance', 'task', 'require', 'large', 'number', 'inenvironment', 'interaction', 'inspire', 'recent', 'success', 'efﬁcientzero', 'propose', 'efﬁcientimitate', 'planningbased', 'imitation', 'learning', 'method', 'achieve', 'high', 'inenvironment', 'sample', 'efﬁciency', 'formance', 'simultaneously', 'algorithmic', 'contribution', 'paper', 'twofold', 'first', 'extend', 'ail', 'mctsbased', 'rl', 'second', 'show', 'seemingly', 'incompatible', 'class', 'imitation', 'algorithm', 'ail', 'naturally', 'uniﬁed', 'framework', 'enjoy', 'beneﬁts', 'benchmark', 'method', 'statebased', 'deepmind', 'control', 'suite', 'also', 'image', 'version', 'many', 'previous', 'work', 'highly', 'challenge', 'experimental', 'result', 'show', 'ei', 'achieve', 'stateoftheart', 'result', 'performance', 'sample', 'efﬁciency', 'ei', 'show', 'gain', 'performance', 'limited', 'sample', 'set', 'statebase', 'imagebase', 'task', 'solve', 'challenge', 'problem', 'humanoid', 'previous', 'method', 'fail', 'small', 'amount', 'interaction', 'code', 'available', 'introduction', 'sequential', 'decision', 'process', 'robotic', 'highly', 'challenging', 'robot', 'handle', 'high', 'dimensional', 'input', 'image', 'need', 'solve', 'long', 'horizon', 'problem', 'critical', 'timestep', 'need', 'highly', 'accurate', 'maneuver', 'learning', 'process', 'real', 'robot', 'sample', 'efﬁcient', 'imitation', 'learning', 'promising', 'approach', 'solve', 'problem', 'give', 'small', 'dataset', 'expert', 'demonstration', 'however', 'current', 'imitation', 'algorithm', 'struggle', 'achieve', 'goal', 'simultaneously', 'kind', 'popular', 'imitation', 'learn', 'algorithm', 'behavior', 'clone', 'bc', 'adversarial', 'imitation', 'learn', 'ail', 'formulate', 'imitation', 'learn', 'supervised', 'learning', 'problem', 'need', 'inenvironment', 'sample', 'suffer', 'covariate', 'shift', 'issue', 'often', 'lead', 'test', 'time', 'performance', 'degradation', 'adversarial', 'imitation', 'learn', 'ail', 'cast', 'imitation', 'learning', 'distribution', 'matching', 'problem', 'ail', 'suffer', 'less', 'covariate', 'shift', 'problem', 'perform', 'well', 'domain', 'require', 'impractical', 'number', 'online', 'interaction', 'perform', 'badly', 'image', 'input', 'drawback', 'heavily', 'limit', 'application', 'ﬁeld', 'robotic', 'physical', 'robot', 'time', 'matter', 'summary', 'current', 'imitation', '∗zhaohengyinconnectusthk', 'cqfusthk', 'correspond', 'author', '36th', 'conference', 'neural', 'information', 'processing', 'system', 'neurip', 'figure', 'leave', 'systemlevel', 'overview', 'efﬁcientimitate', 'agent', 'yellow', 'area', 'take', 'action', 'environment', 'store', 'datum', 'replay', 'buffer', 'datum', 'replay', 'buffer', 'expert', 'buffer', 'use', 'train', 'model', 'reward', 'planning', 'module', 'search', 'improved', 'policy', 'value', 'state', 'replay', 'buffer', 'base', 'policy', 'value', 'network', 'optimize', 'planning', 'procedure', 'use', 'continuous', 'version', 'efﬁcientzero', 'planner', 'mct', 'uniquely', 'beneﬁts', 'unify', 'ail', 'expansion', 'node', 'sample', 'action', 'current', 'policy', 'black', 'arrow', 'policy', 'blue', 'arrow', 'use', 'ail', 'reward', 'yellow', 'cube', 'encourage', 'longterm', 'distribution', 'matching', 'mct', 'search', 'pink', 'area', 'good', 'action', 'maximize', 'cumulative', 'ail', 'reward', 'update', 'estimate', 'value', 'policy', 'root', 'node', 'output', 'planning', 'procedure', 'value', 'estimate', 'policy', 'estimate', 'value', 'policy', 'network', 'optimize', 'ﬁt', 'learn', 'algorithm', 'fail', 'achieve', 'high', 'online', 'testing', 'performance', 'high', 'inenvironment', 'sample', 'efﬁciency', 'time', 'far', 'type', 'imitation', 'algorithm', 'seem', 'incompatible', 'base', 'completely', 'different', 'training', 'objective', 'previous', 'work', 'ﬁnd', 'hard', 'unify', 'naively34', 'inspire', 'recent', 'success', 'sample', 'efﬁcient', 'efﬁcientzero', 'propose', 'planningbased', 'imitation', 'name', 'efﬁcientimitate', 'ei', 'achieve', 'high', 'test', 'performance', 'high', 'inenvironment', 'sample', 'efﬁciency', 'time', 'method', 'extend', 'ail', 'model', 'base', 'setting', 'multistep', 'loss', 'mctsbased', 'rl', 'framework', 'also', 'uniﬁes', 'type', 'previous', 'imitation', 'algorithm', 'ail', 'naturally', 'thank', 'planning', 'component', 'intuitively', 'give', 'coarse', 'solution', 'correct', 'time', 'fail', 'match', 'expert', 'behavior', 'long', 'term', 'hand', 'ail', 'know', 'goal', 'learning', 'match', 'state', 'action', 'distribution', 'give', 'solution', 'directly', 'method', 'planning', 'component', 'uniﬁes', 'method', 'show', 'signiﬁcant', 'performance', 'boost', 'especially', 'hard', 'task', 'humanoid', 'illustrate', 'detailed', 'procedure', 'figure', 'validate', 'idea', 'statebase', 'task', 'also', 'imagebase', 'task', 'relatively', 'previous', 'algorithm', 'handle', 'ei', 'achieve', 'stateoftheart', 'result', 'sample', 'efﬁciency', 'performance', 'show', 'gain', 'performance', 'limited', 'sample', 'set', 'statebase', 'imagebase', 'task', 'hard', 'task', 'humanoid', 'gain', 'even', 'large', 'byproduct', 'paper', 'extend', 'recent', 'efﬁcientzero', 'algorithm', 'continuous', 'action', 'space', 'opensource', 'code', 'facilitate', 'future', 'research', 'contribution', 'paper', 'summarize', 'follow', 'present', 'efﬁcientimitate', 'sample', 'efﬁcient', 'imitation', 'learn', 'base', 'mct', 'identify', 'mct', 'beneﬁt', 'use', 'action', 'search', 'crucial', 'challenge', 'task', 'suggest', 'natural', 'way', 'unify', 'ail', 'conduct', 'experiment', 'state', 'image', 'domain', 'evaluate', 'experimental', 'result', 'show', 'ei', 'achieve', 'stateoftheart', 'sample', 'efﬁciency', 'performance', 'relate', 'work', 'imitation', 'learn', 'imitation', 'learn', 'aim', 'solve', 'sequential', 'decisionmake', 'problem', 'datum', 'wide', 'application', 'game', 'robotic', 'compare', 'major', 'beneﬁt', 'avoid', 'notorious', 'reward', 'design', 'problem', 'divide', 'branch', 'irl', 'solve', 'covariate', 'shift', 'problem', 'researcher', 'propose', 'method', 'dataset', 'aggregation', 'noise', 'injection', 'method', 'require', 'extra', 'expert', 'query', 'exert', 'constraint', 'learning', 'process', 'recent', 'variant', 'branch', 'irl', 'adversarial', 'imitation', 'learn', 'ail', 'ail', 'model', 'stateaction', 'distribution', 'matching', 'problem', 'many', 'work', 'extend', 'ail', 'use', 'well', 'distribution', 'metric', 'divergence', 'wasserstein', 'distance', 'method', 'learn', 'well', 'reward', 'function', 'somewhat', 'speed', 'training', 'directly', 'focus', 'ail', 'sample', 'efﬁciency', 'problem', 'work', 'draw', 'attention', 'sample', 'efﬁciency', 'example', 'propose', 'use', 'offpolicy', 'training', 'ail', 'training', 'reduce', 'sample', 'complexity', 'valuedice', 'reformulate', 'ail', 'objective', 'ofﬂine', 'minmax', 'optimization', 'process', 'recent', 'work', 'point', 'improved', 'form', 'vmail', 'use', 'modelbase', 'approach', 'improve', 'sample', 'efﬁciency', 'collect', 'latent', 'rollout', 'variational', 'model', 'reduce', 'online', 'interaction', 'mobile', 'also', 'show', 'provably', 'sample', 'efﬁcient', 'modelbase', 'imitation', 'approach', 'compare', 'method', 'method', 'introduce', 'mct', 'plan', 'offpolicy', 'imitation', 'learning', 'use', 'unify', 'ail', 'take', 'advantage', 'idea', 'combine', 'ail', 'help', 'improve', 'sample', 'efﬁciency', 'trace', 'back', 'gail16', 'gail', 'suggest', 'use', 'initialize', 'policy', 'network', 'ﬁnd', 'work', 'well', 'initialize', 'knowledge', 'corrupt', 'ail', 'training', 'propose', 'use', 'annealed', 'decaying', 'loss', 'regularize', 'policy', 'training', 'solve', 'problem', 'work', 'ﬁnd', 'regularizer', 'harmful', 'exploration', 'incorrect', 'concurrent', 'work', 'mitigate', 'issue', 'use', 'adpative', 'loss', 'replace', 'ail', 'reward', 'optimaltransportbased', 'imitation', 'reward', 'also', 'notice', 'alphago', 'involve', 'method', 'focus', 'rather', 'use', 'initialize', 'policy', 'different', 'method', 'ei', 'use', 'action', 'candidate', 'mct', 'sample', 'efﬁciency', 'sample', 'efﬁciency', 'problem', 'imitation', 'learning', 'closely', 'relate', 'line', 'work', 'ﬁnd', 'reward', 'good', 'data', 'source', 'representation', 'learn', 'rl', 'reason', 'sample', 'inefﬁciency', 'utilize', 'selfsupervise', 'representation', 'learn', 'accelerate', 'representation', 'learning', 'improve', 'sample', 'efﬁciency', 'researcher', 'propose', 'use', 'contrastive', 'learn', 'consistencybase', 'learn', 'pretraine', 'representation', 'purpose', 'work', 'also', 'explore', 'possibility', 'apply', 'selfsupervise', 'representation', 'learn', 'imitation', 'learn', 'line', 'work', 'focus', 'learned', 'model', 'promise', 'sample', 'efﬁcient', 'learn', 'approach', 'usually', 'imagine', 'additional', 'rollout', 'learn', 'model', 'use', 'compact', 'environment', 'representation', 'work', 'also', 'datum', 'augmentation', 'effectively', 'improve', 'sample', 'efﬁciency', 'ei', 'also', 'beneﬁts', 'approach', 'include', 'model', 'apply', 'representation', 'learn', 'boost', 'sample', 'efﬁciency', 'application', 'people', 'also', 'consider', 'augment', 'demonstration', 'improve', 'sample', 'efﬁciency', 'view', 'combination', 'rl', 'imitation', 'learning', 'favor', 'rl', 'real', 'robot', 'believe', 'method', 'also', 'extend', 'set', 'background', 'set', 'formalize', 'sequential', 'decising', 'making', 'problem', 'markov', 'decision', 'process', 'r', 'state', 'space', 'action', 'space', 'r', 'reward', 'function', 'transition', 'dynamic', 'agent', 'state', 'timestep', 'agent', 'take', 'action', 'γtrt', 'discount', 'factor', 'receive', 'rst', 'state', 'timestep', 'st1', '∼', 'objective', 'agent', 'maximize', 'return', 'imitation', 'learning', 'problem', 'study', 'agent', 'access', 'reward', 'function', 'r', 'transition', 'dynamic', 'provide', 'ﬁxed', 'expert', 'demonstration', 'dataset', 'τi', 'expert', 'trajectory', 'achieve', 'high', 'performance', 'agent', 'solicit', 'extra', 'expert', 'demonstration', 'interact', 'mdp', 'observe', 'new', 'state', 'action', 'reward', 'work', 'deﬁne', 'inenvironment', 'sample', 'efﬁciency', 'number', 'online', 'interaction', 'training', 'expect', 'agent', 'achieve', 'high', 'performance', 'ﬁxed', 'online', 'sample', 'budget', 'bc', 'consider', 'imitation', 'learn', 'supervised', 'learning', 'problem', 'train', 'policy', 'network', 'π', 'minimize', 'following', 'loss', 'function', 'log', 'πae', 'ail', 'treat', 'imitation', 'learning', 'distribution', 'matching', 'problem', 'typical', 'ail', 'gail', 'train', 'discriminator', 'distinguish', 'agent', 'generate', 'stateaction', 'tuple', 'demonstration', 'dataset', 'minimize', '−e', 'log1', '−', 'dse', 'cid3', 'stateaction', 'distribution', 'induce', 'agent', 'meanwhile', 'train', 'agent', 'maximize', 'return', 'respect', 'adversarial', 'reward', 'log1', '−', 'dst', 'use', 'onpolicy', 'muzero', 'extension', 'planning', 'method', 'base', 'extension', 'muzero', 'learn', 'environment', 'model', 'mct', 'model', 'consist', 'encoder', 'network', 'dynamic', 'network', 'reward', 'network', 'r', 'operate', 'abstract', 'state', 'concretely', 'get', 'abstract', 'state', 'ht', 'current', 'state', 'predict', 'future', 'abstract', 'state', 'recursively', 'ght', 'reward', 'rht', 'model', 'muzero', 'also', 'contain', 'policy', 'network', 'value', 'network', 'policy', 'network', 'provide', 'prior', 'action', 'node', 'value', 'network', 'calculate', 'expect', 'return', 'use', 'model', 'policy', 'network', 'value', 'network', 'search', 'improved', 'policy', 'value', 'state', 'mct', 'refer', 'reader', 'original', 'muzero', 'paper', 'detail', 'sample', 'sample', 'extend', 'muzero', 'discrete', 'action', 'domain', 'continuous', 'action', 'domain', 'interest', 'paper', 'node', 'expand', 'sample', 'sample', 'current', 'policy', 'πas', 'search', 'select', 'action', 'a∗', 'sample', 'action', 'maximize', 'probabilistic', 'upper', 'conﬁdence', 'bind', 'csˆπas', 'ai', 'current', 'qestimation', 'pair', 'denote', 'time', 'pair', 'visit', 'mct', 'cs', 'weight', 'coeffcient', 'policy', 'optimization', 'muzero', 'minimize', 'kullbackleibler', 'divergence', 'current', 'policy', 'π', 'mct', 'statistic', 'πmct', 'efﬁcientzero', 'also', 'apply', 'efﬁcientzero', 'paper', 'efﬁcientzero', 'improve', 'sample', 'efﬁciency', 'use', 'selfsupervise', 'representation', 'learning', 'method', 'regularize', 'hide', 'representation', 'use', 'simsiamstyle', 'structure', 'enforce', 'similarity', 'predict', 'future', 'representation', 'real', 'future', 'representation', 'efﬁcientimitate', 'section', 'present', 'efﬁcientimitate', 'ﬁrst', 'present', 'mctsbased', 'approach', 'solve', 'ail', 'problem', 'section', 'show', 'simple', 'yet', 'effective', 'method', 'unify', 'figure', 'computation', 'ﬂow', 'loss', 'function', 'leave', 'multistep', 'discriminator', 'loss', 'distinguish', 'calculation', 'expert', 'agent', 'use', 'superscript', 'e', 'indicate', 'computation', 'apply', 'right', 'multistep', 'loss', 'apply', 'expert', 'sequence', 'ail', 'mct', 'section', 'brieﬂy', 'discuss', 'implementation', 'section', 'full', 'detail', 'find', 'appendix', 'extend', 'ail', 'mctsbase', 'traditionally', 'adversarial', 'imitation', 'learn', 'train', 'discriminator', 'policy', 'sample', 'expert', 'sample', 'use', 'form', '−', 'log1', '−', 'reward', 'function', 'modelfree', 'rl', 'algorithm', 'use', 'maximize', 'cumulative', 'reward', 'mctsbased', 'algorithm', 'efﬁcientzero', 'reward', 'function', 'use', 'value', 'target', 'computation', 'mct', 'search', 'use', 'value', 'target', 'computation', 'similar', 'prior', 'modelfree', 'algorithm', 'value', 'target', 'compute', 'nstep', 'value', 'bootstrappe', 'actual', 'observation', 'however', 'mct', 'search', 'reward', 'compute', 'abstract', 'state', 'obtain', 'run', 'forward', 'dynamic', 'function', 'multiple', 'time', 'train', 'discriminator', 'actual', 'observation', 'expert', 'policy', 'rollout', 'discriminator', 'generalize', 'well', 'abstract', 'state', 'output', 'forward', 'dynamic', 'function', 'therefore', 'train', 'discriminator', 'modelbase', 'rollout', 'speciﬁcally', 'sample', 'sequence', 'atn', 'replay', 'buffer', 'b', 'expert', 'sequence', 'se', 'tcid48n', 'demonstration', 'dataset', 'minimize', 'follow', 'multistep', 'discriminator', 'loss', 'function', 'tcid48', 'tcid48', 'tcid48', 'tcid48', 'tcid48n', 'i0', 'logdhti', 'ati', 'dhe', 'tcid48i', 'tcid48i', 'hti', 'htcid48i', 'term', 'produce', 'forward', 'dynamic', 'efﬁcientzero', 'figure', 'use', 'gail', 'transition', 'reward', 'rh', '−', 'log1', '−', 'dh', 'mct', 'planner', 'search', 'action', 'maximize', 'cumulative', 'gail', 'reward', 'guarantee', 'longterm', 'distribution', 'matching', 'note', 'vmail', 'also', 'propose', 'similar', 'discriminator', 'training', 'technique', 'dreamer', 'model', 'besides', 'discriminator', 'input', 'base', 'representation', 'rather', 'raw', 'input', 'discriminator', 'train', 'encoder', 'jointly', 'lead', 'highly', 'non', 'stationary', 'reward', 'bootstrap', 'value', 'calculation', 'mitigate', 'issue', 'also', 'propose', 'use', 'target', 'discriminator', 'bootstrap', 'value', 'calculation', 'make', 'training', 'stable', 'use', 'gail', 'reward', 'also', 'use', 'kind', 'ail', 'reward', 'function', 'propose', 'recent', 'research', 'use', 'gail', 'reward', 'already', 'achieve', 'satisfactory', 'performance', 'experiment', 'real', 'reward', 'present', 'also', 'combine', 'plan', 'favor', 'application', 'scenario', 'handcraft', 'reward', 'function', 'hard', 'study', 'case', 'leave', 'future', 'work', 'unify', 'ail', 'mct', 'discuss', 'related', 'work', 'researcher', 'realize', 'use', 'improve', 'ail', '’s', 'sample', 'efﬁciency', 'provide', 'good', 'initial', 'policy', 'constrain', 'policy', 'however', 'exist', 'solution', 'good', 'enough', 'practice', 'main', 'pitfall', 'method', 'knowledge', 'policy', 'network', 'destine', 'forget', 'policy', 'network', 'train', 'ail', 'objective', 'long', 'helpful', 'observe', 'mct', 'naturally', 'unify', 'ail', 'method', 'enjoy', 'beneﬁt', 'free', 'pitfall', 'propose', 'plug', 'action', 'mct', 'candidate', 'node', 'use', 'planning', 'process', 'search', 'improved', 'solution', 'time', 'action', 'consistently', 'consider', 'entire', 'training', 'procedure', 'forget', 'concretely', 'train', 'policy', 'πbc', 'use', 'mixture', 'policy', '˜π', 'sampling', 'node', 'mct', 'απbc', '−', 'απ', 'α', 'mixture', 'factor', 'ﬁxe', 'training', 'current', 'policy', 'use', 'paper', 'ensure', 'small', 'fraction', 'action', 'sample', 'policy', 'plan', 'action', 'evaluate', 'frequently', 'visit', 'select', 'output', 'lead', 'longterm', 'distribution', 'matching', 'reduce', 'effort', 'ﬁnde', 'good', 'expertlike', 'action', 'scratch', 'desire', 'moreover', 'unique', 'advantage', 'procedure', 'fully', 'trust', 'force', 'output', 'policy', 'network', 'close', 'wrong', 'covariate', 'shift', 'insufﬁcient', 'demonstration', 'neglect', 'action', 'allow', 'policy', 'search', 'well', 'action', 'ensure', 'hurt', 'training', 'however', 'conceptual', 'simplicity', 'arise', 'question', 'approach', 'apply', 'modelbased', 'method', 'take', 'dreamer', 'example', 'dreamer', 'build', 'model', 'environment', 'use', 'model', 'rollout', 'policy', 'policy', 'optimization', 'word', 'model', 'use', 'evaluate', 'speciﬁc', 'action', 'good', 'long', 'term', 'idea', 'apply', 'directly', 'dreamer', 'example', 'see', 'core', 'idea', 'leverage', 'power', 'plan', 'longterm', 'outcome', 'certain', 'action', 'calculate', 'training', 'πbc', 'minimize', 'follow', 'multistep', 'objective', 'figure', 'lbc', 'tcid48', 'tcid48tcid48n', '−', 'logπbcae', 'tcid48ihe', 'i0', 'avoid', 'distributional', 'shift', 'multistep', 'prediction', 'mct', 'training', 'policy', 'still', 'minimize', 'note', 'design', 'propose', 'couple', 'ail', 'go', 'imitation', 'learning', 'apply', 'robot', 'learning', 'setting', 'rl', 'demonstration', 'implementation', 'ﬁrst', 'implement', 'continuous', 'version', 'efﬁcientzero', 'planning', 'detail', 'find', 'appendix', 'policy', 'network', 'simply', 'duplicate', 'policy', 'network', 'discriminator', 'policy', 'network', 'share', 'encoder', 'network', 'policy', 'network', 'value', 'network', 'overall', 'loss', 'function', 'optimization', 'loss', 'function', 'exclude', 'reward', 'loss', 'network', 'train', 'jointly', 'minimize', 'loss', 'function', 'use', 'reanalyze', 'ofﬂine', 'training', 'require', 'sample', 'reanalyze', 'figure', 'part', 'task', 'use', 'experiment', 'left', 'right', 'reacher', 'finger', 'spin', 'walker', 'walk', 'humanoid', 'walk', 'experiment', 'section', 'evaluate', 'sample', 'efﬁciency', 'propose', 'method', 'measure', 'sample', 'efﬁciency', 'evaluate', 'performance', 'algorithm', 'small', 'number', 'online', 'sample', 'also', 'analyze', 'effect', 'action', 'plan', 'table', 'evaluation', 'result', 'statebased', 'deepmind', 'control', 'suite', 'use', 'average', 'score', 'random', 'seed', 'method', 'achieve', 'state', 'art', 'result', 'compare', 'baseline', 'task', 'sqil', '±012', '±001', '±001', '±001', 'ball', '±001', '±001', '±005', '±001', 'reacher', 'finger', '±002', '±001', '±002', '±005', '±001', '±008', '±001', '±002', '±001', '±010', '±001', '±004', 'table', 'evaluation', 'result', 'imagebase', 'deepmind', 'control', 'suite', 'use', 'average', 'score', 'random', 'seed', 'method', 'achieve', 'stateoftheart', 'result', 'compare', 'baseline', 'task', 'vmail', '±001', '±002', '±002', '±005', '±011', '±001', 'finger', '±001', '±001', '±001', '±001', '±001', '±005', '±002', '±006', '±002', '±005', '±007', '±002', '±001', 'setup', 'use', 'deepmind', 'control', 'evaluation', 'use', 'follow', 'task', 'catch', 'finger', 'spin', 'walker', 'walk', 'hopper', 'humanoid', 'walk', 'conduct', 'statebase', 'imagebase', 'experiment', 'note', 'many', 'previous', 'imitation', 'learn', 'work', 'use', 'openai', 'gym', 'version', 'task', 'evaluation', 'dmcontrol', 'version', 'use', 'bring', 'extra', 'challenge', 'use', 'challenging', 'initial', 'state', 'take', 'walker', 'task', 'example', 'initial', 'state', 'openai', 'gym', 'stand', 'however', 'dmcontrol', 'agent', 'initial', 'state', 'lie', 'ground', 'agent', 'also', 'learn', 'stand', 'ﬁrst', 'limited', 'datum', 'statebased', 'experiment', 'allow', 'online', 'step', 'environment', 'base', 'difﬁculty', 'task', 'learn', 'robust', 'meaningful', 'visual', 'representation', 'require', 'datum', 'imagebase', 'experiment', 'allow', 'online', 'step', 'detailed', 'setup', 'show', 'result', 'train', 'sac', 'policy', 'collect', 'expert', 'demonstration', 'imitation', 'learn', 'expert', 'demonstration', 'subsample', 'use', 'demonstration', 'statebased', 'experiment', 'reacher', 'humanoid', 'use', 'demonstration', 'use', 'demonstration', 'imagebase', 'experiment', 'baseline', 'present', 'several', 'baseline', 'sample', 'efﬁcient', 'imitation', 'learn', 'adversarial', 'offpolicy', 'imitation', 'learning', 'method', 'match', 'distribution', 'replay', 'buffer', 'expert', 'demonstration', 'dataset', 'use', 'sqil', 'sqil', 'nonadversarial', 'offpolicy', 'imitation', 'learning', 'method', 'label', 'expert', 'transition', 'reward', 'nonexpert', 'transition', 'reward', 'train', 'sac', 'policy', 'relabele', 'datum', 'sqil', 'regularize', 'form', 'valuedice', 'valuedice', 'consider', 'imitation', 'learning', 'distribution', 'matching', 'problem', 'solve', 'minmax', 'optimization', 'process', 'vmail', 'vmail', 'modelbase', 'visual', 'imitation', 'learning', 'method', 'learn', 'variational', 'model', 'simulate', 'onpolicy', 'rollout', 'evaluate', 'vmail', 'imagebase', 'domain', 'figure', 'performance', 'curve', 'statebased', 'task', 'result', 'average', 'seed', 'shaded', 'area', 'display', 'range', 'standard', 'deviation', 'figure', 'performance', 'curve', 'imagebase', 'task', 'result', 'average', 'seed', 'shaded', 'area', 'display', 'range', 'standard', 'deviation', 'original', 'paper', 'online', 'imitation', 'learn', 'baseline', 'also', 'include', 'ofﬂine', 'baseline', 'result', 'statebase', 'experiment', 'table', 'show', 'statebased', 'experiment', 'evaluation', 'result', 'give', 'budget', 'also', 'plot', 'performance', 'curve', 'challenging', 'task', 'figure', 'performance', 'normalize', 'respect', 'performance', 'random', 'agent', 'expert', 'ﬁnd', 'propose', 'method', 'outperform', 'baseline', 'large', 'margin', 'baseline', 'method', 'hardly', 'learn', 'meaningful', 'behavior', 'use', 'limited', 'online', 'budget', 'ﬁnd', 'strong', 'baseline', 'performance', 'grow', 'expert', 'sample', 'task', 'hopper', 'eventually', 'get', 'stick', 'method', 'much', 'sample', 'efﬁcient', 'good', 'baseline', 'method', 'task', 'walk', 'run', 'method', 'require', '20k', 'step', 'reach', 'performance', 'equivalent', 'online', 'trajectory', 'hour', 'real', 'result', 'notable', 'robotic', 'community', 'show', 'online', 'imitation', 'learning', 'possible', 'handful', 'trial', 'apply', 'directly', 'real', 'locomotion', 'robot', 'possible', 'imagebase', 'experiment', 'far', 'imagebase', 'task', 'still', 'challenge', 'adversarial', 'imitation', 'learning', 'algorithm', 'evaluation', 'prior', 'ail', 'work', 'carry', 'statebased', 'task', 'table', 'show', 'evaluation', 'result', 'give', 'budget', 'imagebase', 'experiment', 'see', 'figure', 'curve', 'method', 'also', 'learn', 'expert', 'behavior', 'give', 'slightly', 'large', 'budget', 'still', 'baseline', 'fail', 'match', 'expert', 'behavior', 'use', 'give', 'budget', 'notice', 'inherent', 'difﬁculty', 'learn', 'robust', 'visual', 'representation', 'adversarial', 'training', 'limited', 'datum', 'set', 'imagebase', 'task', 'discriminator', 'judge', 'behavior', 'expertlike', 'use', 'various', 'possible', 'feature', 'case', 'solve', 'open', 'problem', 'scope', 'paper', 'presence', 'difﬁculty', 'ei', 'still', 'achieve', 'good', 'sample', 'efﬁciency', 'task', 'finger', 'n', 'r', 'r', 'e', 'p', 'valuedice', '50k', 'step', 'finger', 'n', 'r', 'r', 'e', 'p', 'valuedice', 'vmail', '50k', 'step', 'figure', 'performance', 'curve', 'method', 'action', 'expansion', 'plot', 'sort', 'accord', 'difﬁculty', 'correspond', 'task', 'leftmost', 'difﬁcult', 'task', 'humanoid', 'result', 'average', 'seed', 'action', 'great', 'impact', 'sample', 'efﬁciency', 'performance', 'analysis', 'effect', 'carry', 'ablation', 'analysis', 'see', 'help', 'method', 'set', 'remove', 'action', 'see', 'method', 'performance', 'sample', 'efﬁciency', 'change', 'result', 'show', 'figure', 'ﬁnd', 'performance', 'method', 'degrade', 'remove', 'action', 'mct', 'effect', 'taskspeciﬁc', 'helpful', 'challenge', 'task', 'example', 'task', 'highdimensional', 'hard', 'exploration', 'process', 'humanoid', 'hopper', 'remove', 'action', 'make', 'learning', 'process', 'stick', 'local', 'minima', 'local', 'minima', 'agent', 'struggle', 'correct', 'action', 'match', 'expert', 'behavior', 'remove', 'action', 'trap', 'agent', 'local', 'minima', 'relatively', 'simple', 'task', 'walker', 'slow', 'training', 'double', 'number', 'online', 'interaction', 'reach', 'expert', 'performance', 'result', 'conﬁrm', 'use', 'action', 'indeed', 'provide', 'good', 'solution', 'distribution', 'matching', 'problem', 'help', 'speed', 'learn', 'improve', 'performance', 'also', 'notice', 'even', 'remove', 'action', 'method', 'still', 'able', 'outperform', 'previous', 'baseline', 'suggest', 'plan', 'ail', 'alone', 'also', 'powerful', 'way', 'use', 'study', 'variant', 'use', 'bcann', 'variant', 'use', 'action', 'mct', 'exert', 'annealing', 'loss', 'pol', 'network', 'bcrep', 'variant', 'use', 'action', 'mct', 'still', 'use', 'loss', 'regularize', 'representation', 'test', 'variant', 'humanoid', 'walk', 'figure', 'ﬁnd', 'variant', 'lead', 'essential', 'improvement', 'bcann', 'harm', 'performance', 'early', 'stage', 'constrain', 'agent', 'policy', 'policy', 'contain', 'error', 'hinder', 'learn', 'agent', 'start', 'acquire', 'meaningful', 'behavior', 'regularization', 'decay', 'time', 'help', 'much', 'lead', 'improvement', 'compare', 'bcann', 'bcreg', 'helpful', 'possibly', 'make', 'encod', 'focus', 'useful', 'feature', 'however', 'bcreg', 'still', 'get', 'stick', 'local', 'minimum', 'result', 'suggest', 'use', 'action', 'directly', 'exploration', 'essential', 'improve', 'ail', 'use', 'simply', 'regularizer', 'ideal', 'approach', 'useful', 'sometimes', 'figure', 'result', 'different', 'way', 'use', 'ablation', 'plan', 'part', 'study', 'extent', 'planning', 'help', 'learn', 'insufﬁcient', 'search', 'mct', 'lead', 'inferior', 'performance', 'study', 'number', 'sample', 'action', 'node', 'number', 'simulation', 'default', 'value', 'previous', 'experiment', 'sweep', 'evaluate', 'effect', 'collect', 'result', 'statebased', 'walker', 'hopper', 'task', 'report', 'average', 'relative', 'performance', 'change', 'give', 'budget', 'use', 'previous', 'experiment', 'see', 'table', 'general', 'trend', 'large', 'k', 'n', 'lead', 'well', 'imitation', 'result', 'ﬁnd', 'vary', 'affect', 'performance', 'little', 'also', 'work', 'well', 'compare', 'large', 'humanoid', 'n', 'r', 'r', 'e', 'p', 'bc', '20k', '20k', '20k', '20k', '40k', 'step', 'step', 'step', 'step', 'step', 'e', 'r', 'r', 'e', 'p', 'humanoid', 'bc', 'step', 'impact', 'number', 'simulation', 'become', 'small', 'performance', 'drop', 'signiﬁcantly', 'result', 'also', 'explain', 'achieve', 'large', 'improvement', 'even', 'table', 'ablation', 'planning', 'report', 'relative', 'change', 'performance', 'give', 'budget', 'param', 'result', '−273', '−155', 'discussion', 'paper', 'present', 'efﬁcientimitate', 'mctsbased', 'imitation', 'learn', 'extend', 'ail', 'modelbased', 'setting', 'solve', 'mct', 'sampleefﬁcient', 'way', 'moreover', 'propose', 'method', 'unify', 'ail', 'mct', 'enjoy', 'beneﬁts', 'experimental', 'result', 'statebase', 'imagebase', 'task', 'show', 'efﬁcientimitate', 'achieve', 'stateoftheart', 'sample', 'efﬁciency', 'performance', 'limitation', 'limitation', 'work', 'computation', 'process', 'mct', 'expensive', 'compare', 'modelfree', 'method', 'common', 'issue', 'modelbase', 'method', 'possible', 'approach', 'mitigate', 'issue', 'use', 'well', 'mct', 'acceleration', 'method', 'paper', 'study', 'long', 'horizon', 'problem', 'multiple', 'object', 'common', 'case', 'robotic', 'manipulation', 'however', 'require', 'model', 'predict', 'interaction', 'multiple', 'object', 'still', 'challenge', 'open', 'problem', 'learn', 'community', 'orthogonal', 'contribution', 'believe', 'framework', 'combine', 'work', 'ﬁeld', 'handle', 'challenge', 'future', 'work', 'many', 'problem', 'study', 'direction', 'first', 'use', 'vanilla', 'interesting', 'see', 'use', 'advanced', 'algorithm', 'optimaltransportbased', 'learning', 'make', 'powerful', 'second', 'modularity', 'method', 'try', 'extend', 'efﬁcientimitate', 'general', 'setting', 'rl', 'demonstration', 'also', 'favor', 'application', 'scenario', 'third', 'work', 'consider', 'online', 'learning', 'set', 'possible', 'future', 'direction', 'study', 'use', 'efﬁcientimitate', 'exist', 'ofﬂine', 'interaction', 'dataset', 'far', 'reduce', 'dependence', 'inenvironment', 'sample', 'conclusion', 'believe', 'work', 'show', 'promising', 'direction', 'open', 'new', 'possibility', 'modelbase', 'method', 'robot', 'learn', 'acknowledgment', 'disclosure', 'funding', 'work', 'support', 'science', 'technology', 'people', 'innovation', 'megaproject', 'program', 'new', 'generation', 'artiﬁcial', 'intelligence', 'grant', 'work', 'also', 'support', 'grant', 'reference', 'bain', 'sammut', 'framework', 'behavioural', 'cloning', 'machine', 'intelligence', 'page', 'g', 'brockman', 'pettersson', 'schneider', 'zaremba', 'gym', 'preprint', 'explore', 'simple', 'siamese', 'representation', 'learn', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'x', 'toyer', 'emmon', 'fischer', 'empirical', 'investigation', 'representation', 'learn', 'imitation', 'conference', 'neural', 'information', 'processing', 'system', 'dataset', 'benchmark', 'track', 'r', 'geist', 'pietquin', 'primal', 'wasserstein', 'imitation', 'learning', 'international', 'conference', 'learn', 'representation', 'levine', 'learn', 'robust', 'reward', 'adversarial', 'inverse', 'reinforcement', 'learning', 'international', 'conference', 'learn', 'representation', 'fujimoto', 'h', 'hoof', 'meger', 'address', 'function', 'approximation', 'error', 'actorcritic', 'method', 'international', 'conference', 'machine', 'learning', 'nachum', 'g', 'bellemare', 'deepmdp', 'learn', 'continuous', 'latent', 'space', 'model', 'representation', 'learn', 'international', 'conference', 'machine', 'learn', 'haarnoja', 'zhou', 'p', 'abbeel', 'soft', 'actorcritic', 'offpolicy', 'maximum', 'entropy', 'deep', 'reinforcement', 'learn', 'stochastic', 'actor', 'international', 'conference', 'machine', 'learn', 'hafner', 'lillicrap', 'fischer', 'r', 'davidson', 'learn', 'latent', 'dynamic', 'plan', 'pixel', 'international', 'conference', 'machine', 'learn', 'hafner', 'lillicrap', 'norouzi', 'dream', 'control', 'learn', 'behavior', 'latent', 'imagination', 'international', 'conference', 'learn', 'representation', 'hafner', 'lillicrap', 'master', 'atari', 'discrete', 'world', 'model', 'international', 'conference', 'learn', 'representation', 'haldar', 'l', 'pinto', 'watch', 'match', 'supercharging', 'imitation', 'regularize', 'optimal', 'transport', 'conference', 'robot', 'learn', 'temporal', 'difference', 'learn', 'model', 'predictive', 'control', 'international', 'conference', 'machine', 'learn', 'hester', 'vecerik', 'pietquin', 'lanctot', 'schaul', 'piot', 'horgan', 'osband', 'qlearne', 'demonstration', 'conference', 'artiﬁcial', 'intelligence', 'generative', 'adversarial', 'imitation', 'learning', 'neural', 'information', 'processing', 'system', 'hubert', 'schrittwieser', 'schmitt', 'silver', 'learning', 'planning', 'complex', 'action', 'space', 'international', 'conference', 'machine', 'learning', 'ioffe', 'szegedy', 'batch', 'normalization', 'accelerate', 'deep', 'network', 'training', 'reduce', 'internal', 'covariate', 'shift', 'international', 'conference', 'machine', 'learn', 'levine', 'trust', 'model', 'modelbase', 'policy', 'optimization', 'neural', 'information', 'processing', 'system', 'r', 'jena', 'sycara', 'augment', 'gail', 'sample', 'efﬁcient', 'imitation', 'learn', 'conference', 'robot', 'learning', 'b', 'policy', 'optimization', 'demonstration', 'international', 'conference', 'machine', 'learn', 'r', 'mobile', 'modelbase', 'imitation', 'learn', 'observation', 'alone', 'neural', 'information', 'processing', 'system', 'tompson', 'discriminatoractor', 'critic', 'address', 'sample', 'inefﬁciency', 'reward', 'bias', 'adversarial', 'imitation', 'learning', 'international', 'conference', 'learn', 'representation', 'tompson', 'imitation', 'learn', 'offpolicy', 'distribution', 'matching', 'international', 'conference', 'learn', 'representation', 'laskey', 'dragan', 'noise', 'injection', 'robust', 'imitation', 'learning', 'conference', 'robot', 'learn', 'stooke', 'p', 'abbeel', 'reinforcement', 'learn', 'augmented', 'datum', 'neural', 'information', 'processing', 'system', 'srinivas', 'p', 'abbeel', 'contrastive', 'unsupervised', 'representation', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learning', 'nagabandi', 'p', 'abbeel', 'actorcritic', 'deep', 'reinforcement', 'learn', 'latent', 'variable', 'model', 'neural', 'information', 'processing', 'system', 'luo', 'rethinking', 'valuedice', 'really', 'improve', 'performance', 'international', 'conference', 'learn', 'representation', 'l', 'maas', 'nonlinearitie', 'improve', 'neural', 'network', 'acoustic', 'model', 'international', 'conference', 'machine', 'learn', 'base', 'reinforcement', 'learning', 'imagination', 'derived', 'memory', 'neural', 'information', 'processing', 'system', 'nair', 'b', 'mcgrew', 'p', 'abbeel', 'overcome', 'exploration', 'reinforcement', 'learn', 'demonstration', 'ieee', 'international', 'conference', 'robotic', 'automation', 'russell', 'et', 'algorithm', 'inverse', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learn', 'raichuk', 'l', 'hussenot', 'r', 'girgin', 'geist', 'bachem', 'pietquin', 'andrychowicz', 'matter', 'adversarial', 'imitation', 'learning', 'neural', 'information', 'processing', 'system', 'paszke', 'gross', 'massa', 'lerer', 'bradbury', 'l', 'antiga', 'imperative', 'style', 'highperformance', 'deep', 'learn', 'library', 'neural', 'information', 'processing', 'system', 'h', 'learn', 'longterm', 'visual', 'dynamic', 'region', 'proposal', 'interaction', 'network', 'international', 'conference', 'learn', 'representation', 'r', 'rajeswaran', 'finn', 'visual', 'adversarial', 'imitation', 'learning', 'use', 'variational', 'model', 'neural', 'information', 'processing', 'system', 'rajeswaran', 'learn', 'complex', 'dexterous', 'manipulation', 'deep', 'reinforcement', 'learning', 'demonstration', 'robotic', 'science', 'system', 'reddy', 'dragan', 'imitation', 'learn', 'reinforcement', 'learning', 'sparse', 'reward', 'international', 'conference', 'learn', 'representation', 'ross', 'bagnell', 'efﬁcient', 'reduction', 'imitation', 'learning', 'international', 'conference', 'artiﬁcial', 'intelligence', 'statistic', 'kawaguchi', 'sample', 'efﬁcient', 'imitation', 'learn', 'continuous', 'control', 'international', 'conference', 'learn', 'representation', 'j', 'schrittwieser', 'antonoglou', 'sifre', 'schmitt', 'guez', 'e', 'lock', 'graepel', 'atari', 'go', 'chess', 'shogi', 'plan', 'learn', 'model', 'nature', 'hubert', 'mandhane', 'silver', 'online', 'ofﬂine', 'reinforcement', 'learning', 'plan', 'learned', 'model', 'neural', 'information', 'processing', 'system', 'schwarzer', 'r', 'goel', 'r', 'hjelm', 'c', 'courville', 'bachman', 'dataefﬁcient', 'reinforcement', 'learning', 'selfpredictive', 'representation', 'international', 'conference', 'learn', 'representation', 'schwarzer', 'r', 'hjelm', 'p', 'bachman', 'courville', 'pretraine', 'representation', 'dataefﬁcient', 'reinforcement', 'learning', 'neural', 'information', 'processing', 'system', 'silver', 'maddison', 'guez', 'l', 'driessche', 'schrittwieser', 'antonoglou', 'lanctot', 'master', 'game', 'go', 'deep', 'neural', 'network', 'tree', 'search', 'nature', '5297587484–489', 'doron', 'muldal', 'budden', 'merel', 'lefrancq', 'preprint', 'l', 'der', 'maaten', 'visualize', 'datum', 'use', 'tsne', 'journal', 'machine', 'learn', 'research', 'yarat', 'r', 'fergus', 'lazaric', 'l', 'master', 'visual', 'continuous', 'control', 'international', 'conference', 'learn', 'prove', 'dataaugmented', 'reinforcement', 'learning', 'representation', 'yarat', 'r', 'fergus', 'image', 'augmentation', 'need', 'regularize', 'deep', 'reinforcement', 'learn', 'pixel', 'international', 'conference', 'learn', 'representation', 'ye', 'p', 'abbeel', 'spending', 'thinking', 'time', 'wisely', 'accelerate', 'mct', 'virtual', 'expansion', 'neural', 'information', 'processing', 'system', 'abbeel', 'master', 'atari', 'game', 'limited', 'datum', 'neural', 'information', 'processing', 'system', 'playvirtual', 'augment', 'cycle', 'consistent', 'virtual', 'trajectory', 'reinforcement', 'learning', 'neural', 'information', 'processing', 'system', 'fgail', 'learn', 'fdivergence', 'generative', 'adversarial', 'imitation', 'learning', 'neural', 'information', 'processing', 'system', 'offpolicy', 'imitation', 'learn', 'observation', 'neural', 'information', 'processing', 'system', 'implementation', 'part', 'introduce', 'detailed', 'implementation', 'efﬁcientimitate', 'code', 'available', 'model', 'detail', 'model', 'base', 'efﬁcientzero', 'model', 'compose', 'several', 'neural', 'network', 'rep', 'resentation', 'network', 'dynamic', 'network', 'network', 'policy', 'network', 'discriminator', 'network', 'projector', 'network', 'predictor', 'network', 'use', 'pytorch', 'implement', 'network', 'detailed', 'structure', 'follow', 'a11', 'statebase', 'experiment', 'representation', 'network', 'representation', 'network', 'mlp', 'hide', 'layer', 'size', 'output', 'dimension', 'use', 'leakyrelu', 'hidden', 'layer', 'activation', 'function', 'output', 'activation', 'function', 'identity', 'dynamic', 'network', 'dynamic', 'network', 'mlp', 'hide', 'layer', 'size', 'catenate', 'representation', 'action', 'input', 'output', 'dimension', 'representation', '’s', 'dimension', 'use', 'leakyrelu', 'hidden', 'layer', 'activation', 'function', 'output', 'activation', 'function', 'identity', 'value', 'network', 'value', 'network', 'mlp', 'hide', 'layer', 'size', 'humanoid', 'use', 'leakyrelu', 'hidden', 'layer', 'activation', 'function', 'output', 'activation', 'function', 'identity', 'use', 'categorical', 'value', 'representation', 'introduce', 'value', 'prediction', 'discretize', 'bin', 'represent', 'value', '−100', 'therefore', 'output', 'dimension', 'value', 'network', 'policy', 'network', 'policy', 'policy', 'network', 'mlp', 'hide', 'layer', 'size', 'humanoid', 'mlp', 'use', 'leakyrelu', 'hidden', 'layer', 'activation', 'function', 'output', 'activation', 'function', 'identity', 'produce', 'squarshednormal', 'normal', 'action', 'distribution', 'determine', 'predict', 'mean', 'predict', 'logstd', 'therefore', 'output', 'dimension', 'twice', 'dimension', 'action', 'space', 'network', 'discriminator', 'network', 'mlp', 'hide', 'layer', 'size', 'use', 'leakyrelu', 'hide', 'layer', 'activation', 'function', 'output', 'activation', 'function', 'sigmoid', 'projector', 'network', 'projector', 'network', 'mlp', 'hide', 'layer', 'size', 'use', 'relu', 'hidden', 'layer', 'activation', 'function', 'output', 'activation', 'function', 'identity', 'output', 'dimension', 'projection', 'dimension', 'predictor', 'network', 'predictor', 'network', 'mlp', 'hide', 'layer', 'size', 'use', 'relu', 'hidden', 'layer', 'activation', 'function', 'output', 'activation', 'function', 'identity', 'output', 'dimension', 'a12', 'imagebase', 'experiment', 'representation', 'network', 'representation', 'network', 'consist', 'layer', 'network', 'structure', 'follow', 'convolution', 'layer', 'input', 'dim', 'output', 'dim', 'kernel', 'size', 'stride', 'padding', '•', 'relu', 'activation', 'convolution', 'layer', 'input', 'dim', 'output', 'dim', 'kernel', 'size', 'stride', 'padding', 'relu', 'activation', 'convolution', 'layer', 'input', 'dim', 'output', 'dim', 'kernel', 'size', 'stride', 'padding', 'relu', 'activation', 'convolution', 'layer', 'input', 'dim', 'output', 'dim', 'kernel', 'size', 'stride', 'padding', '•', 'flatten', 'linear', 'layer', 'output', 'dim', 'relu', 'activation', 'dynamic', 'network', 'dynamic', 'network', 'mlp', 'hide', 'layer', 'size', 'catenate', 'representation', 'action', 'input', 'output', 'dimension', 'representation', '’s', 'dimension', 'use', 'relu', 'hidden', 'layer', 'activation', 'function', 'output', 'activation', 'function', 'also', 'relu', 'value', 'network', 'value', 'network', 'mlp', 'hide', 'layer', 'size', 'use', 'relu', 'hidden', 'layer', 'activation', 'function', 'output', 'activation', 'function', 'identity', 'use', 'categorical', 'value', 'representation', 'introduce', 'value', 'prediction', 'discretize', 'bin', 'represent', 'value', '−40', 'therefore', 'output', 'dimension', 'value', 'network', 'policy', 'network', 'policy', 'policy', 'network', 'mlp', 'hide', 'layer', 'size', 'mlp', 'use', 'relu', 'hidden', 'layer', 'activation', 'function', 'output', 'activation', 'function', 'identity', 'produce', 'squarshednormal', 'action', 'distribution', 'determine', 'predict', 'mean', 'predict', 'logstd', 'therefore', 'output', 'dimension', 'twice', 'dimension', 'action', 'space', 'network', 'discriminator', 'network', 'mlp', 'hide', 'layer', 'size', 'use', 'relu', 'hide', 'layer', 'activation', 'function', 'output', 'activation', 'function', 'sigmoid', 'projector', 'network', 'projector', 'network', 'structure', 'follow', 'linear', 'layer', 'output', 'batchnorm', 'momentum', 'relu', 'activation', 'linear', 'layer', 'output', 'dim', 'batchnorm', 'momentum', 'relu', 'activation', 'linear', 'layer', 'output', 'dim', 'batchnorm', 'momentum', 'predictor', 'network', 'predictor', 'network', 'structure', 'follow', 'linear', 'layer', 'output', 'dim', 'batchnorm', 'momentum', 'relu', 'activation', 'linear', 'layer', 'output', 'mct', 'detail', 'mct', 'implementation', 'mainly', 'base', 'sampled', 'muzero', 'also', 'apply', 'modiﬁca', 'tion', 'propose', 'efﬁcientzero', 'detailed', 'procedure', 'follow', 'expansion', 'task', 'continuous', 'action', 'space', 'enumerate', 'possible', 'action', 'node', 'original', 'mct', 'algorithm', 'solve', 'problem', 'use', 'sample', 'method', 'propose', 'sample', 'muzero', 'expansion', 'node', 'representation', 'space', 'sample', 'action', 'current', 'policy', 'πas', 'work', 'propose', 'integrate', 'action', 'mct', 'actually', 'sample', 'απas', 'απbcas', 'mixture', 'factor', 'selection', 'action', 'selection', 'select', 'action', 'a∗', 'sample', 'action', 'maximize', 'probabilistic', 'upper', 'conﬁdence', 'bind', 'csˆπas', 'denote', 'time', 'pair', 'visit', 'mct', 'cs', 'weight', 'coeffcient', 'deﬁne', 'ai', 'current', 'qestimation', 'pair', 'log', 'encourage', 'exploration', 'also', 'inject', 'dirichlet', 'noise', 'ˆπas', 'root', 'become', 'ρˆπas', 'ρndξ', 'mixture', 'factor', 'ndξ', 'dirichlet', 'distribution', 'set', 'root', 'node', 'mct', 'use', 'discriminator', 'network', 'value', 'network', 'calculate', 'qvalue', 'original', 'deﬁnition', 'node', 'use', 'mean', 'qvalue', 'calculation', 'use', 'efﬁcientzero', 'backup', 'simulation', 'backup', 'process', 'efﬁcientzero', 'implementation', 'refer', 'reader', 'efﬁcientzero', 'detail', 'a3', 'training', 'detail', 'hyperparameter', 'finally', 'introduce', 'important', 'training', 'detail', 'hyperparameter', 'initialization', 'initialize', 'weight', 'bias', 'last', 'layer', 'policy', 'policy', 'value', 'discriminator', 'network', 'parameter', 'initialize', 'default', 'initializer', 'pytorch', 'discriminator', 'tor', 'also', 'apply', 'gradient', 'penalty', 'discriminator', 'network', 'ail', 'training', 'useful', 'apply', 'gradient', 'penalty', 'target', 'update', 'propose', 'use', 'target', 'model', 'calculation', 'policy', 'value', 'reward', 'reanalyze', 'target', 'model', 'update', 'periodically', 'train', 'subject', 'update', 'frequency', 'training', 'hyperparameter', 'use', 'statebased', 'experiment', 'table', 'training', 'hyperparameter', 'use', 'imagebase', 'experiment', 'table', 'b', 'environment', 'detail', 'b1', 'statebase', 'experiment', 'setup', 'task', 'statebased', 'experiment', 'table', 'imagebase', 'experiment', 'setup', 'task', 'imagebase', 'experiment', 'table', 'use', '×', 'resolution', 'imagebase', 'experiment', 'c', 'ablation', 'also', 'perform', 'ablation', 'target', 'discriminator', 'multistep', 'discriminator', 'loss', 'evaluate', 'effect', 'target', 'discriminator', 'use', 'late', 'model', 'calculate', 'reward', 'value', 'target', 'evaluate', 'effect', 'multistep', 'discriminator', 'loss', 'replace', 'singlestep', 'discriminator', 'loss', 'conduct', 'experiment', 'statebased', 'imagebase', 'walker', 'result', 'show', 'figure', 'remove', 'component', 'lead', 'instability', 'training', 'also', 'harm', 'performance', 'compare', 'target', 'discriminator', 'multistep', 'discriminator', 'loss', 'large', 'impact', 'imagebase', 'task', 'computation', 'resource', 'experiment', 'conduct', 'server', 'nvidia', 'rtx', 'gpus', 'cpu', 'core', 'ram', 'statebased', 'imagebase', 'experiment', 'humanoid', 'walk', 'imagebase', 'hopper', 'hop', 'experiment', 'require', 'hour', 'train', 'main', 'bottleneck', 'reanalyse', 'minibatch', 'produce', 'send', 'training', 'loop', 'high', 'frequency', 'improve', 'computation', 'efﬁciency', 'use', 'well', 'parallel', 'computation', 'implementation', 'apply', 'mct', 'speed', 'technique', 'e', 'visualization', 'approach', 'interpret', 'learn', 'model', 'tsne', 'plot', 'use', 'imagebase', 'walker', 'experiment', 'example', 'use', 'train', 'model', 'env', 'step', 'generate', 'state', 'embedding', 'expert', 'trajectory', 'environment', 'trajectory', '25k', '50k', '75k', 'step', 'use', 'tsne', 'visualize', 'embedding', 'plane', 'show', 'figure', 'agent', 'trajectory', 'gradually', 'match', 'expert', 'trajectory', 'blue', 'training', 'moreover', 'expert', 'trajectory', 'circle', 'structure', 'represent', 'periodic', 'pattern', 'walk', 'behavior', 'therefore', 'model', 'represent', 'environment', 'meaningful', 'way', 'figure', 'tsne', 'plot', 'learn', 'state', 'embedding', 'figure', 'ablation', 'target', 'discriminator', 'multistep', 'discriminator', 'loss', 'result', 'average', 'seed', 'shaded', 'area', 'display', 'range', 'standard', 'deviation', 'state', 'image', 'walker', 'image', 'e', 'n', 'r', 'r', 'e', 'p', 'oursno', 'target', '10k', '20k', '40k', '10k', '20k', '40k', '10k', '20k', '40k', '20k', 'step', 'step', 'step', 'step', 'table', 'hyperparameter', 'statebased', 'experiment', 'discount', 'factor', 'minibatch', 'size', 'optimizer', 'optimizer', 'learning', 'rate', 'optimizer', 'momentum', 'optimizer', 'weight', 'decay', 'maximum', 'gradient', 'norm', 'unroll', 'step', 'step', 'loss', 'coeff', 'value', 'loss', 'coeff', 'policy', 'loss', 'penalty', 'target', 'update', 'interval', 'consistency', 'loss', 'coeff', 'reanalyze', 'ratio', 'number', 'simulation', 'mct', 'number', 'sample', 'action', 'ratio', 'sgd', 'humanoid', 'table', 'hyperparameter', 'imagebase', 'experiment', 'discount', 'factor', 'minibatch', 'size', 'stack', 'frame', 'optimizer', 'optimizer', 'learning', 'rate', 'optimizer', 'momentum', 'optimizer', 'weight', 'decay', 'maximum', 'gradient', 'norm', 'unroll', 'step', 'step', 'loss', 'coeff', 'value', 'loss', 'coeff', 'policy', 'loss', 'penalty', 'target', 'update', 'interval', 'consistency', 'loss', 'coeff', 'reanalyze', 'ratio', 'number', 'simulation', 'mct', 'number', 'sample', 'action', 'ratio', 'sgd', 'table', 'task', 'setup', 'statebased', 'experiment', 'task', 'action', 'repeat', 'expert', 'performance', 'reacher', 'easy', 'finger', 'spin', 'walk', 'run', 'walk', 'table', 'task', 'setup', 'statebased', 'experiment', 'task', 'action', 'repeat', 'expert', 'performance', 'reacher', 'easy', 'finger', 'spin', 'walk', 'run', 'hopper']"
"Artificial Intelligence-Based Methods for Fusion of Electronic Health
  Records and Imaging Data","[{'href': 'http://arxiv.org/abs/2210.13462v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.13462v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-10-23 07:13:37,"2
2
0
2

t
c
O
4
2

]

R
C
.
s
c
[

1
v
3
6
2
3
1
.
0
1
2
2
:
v
i
X
r
a

Driver Locations Harvesting Attack on pRide

Shyam Murthy[0000−0002−0222−322X]
Srinivas Vivek[0000−0002−8426−0859]

IIIT Bangalore, India.
shyam.sm@iiitb.ac.in
srinivas.vivek@iiitb.ac.in

Abstract. Privacy preservation in Ride-Hailing Services (RHS) is in-
tended to protect privacy of drivers and riders. pRide, published in
IEEE Trans. Vehicular Technology 2021, is a prediction based privacy-
preserving RHS protocol to match riders with an optimum driver. In
the protocol, the Service Provider (SP) homomorphically computes Eu-
clidean distances between encrypted locations of drivers and rider. Rider
selects an optimum driver using decrypted distances augmented by a
new-ride-emergence prediction. To improve the eﬀectiveness of driver se-
lection, the paper proposes an enhanced version where each driver gives
encrypted distances to each corner of her grid. To thwart a rider from
using these distances to launch an inference attack, the SP blinds these
distances before sharing them with the rider.
In this work, we propose a passive attack where an honest-but-curious
adversary rider who makes a single ride request and receives the blinded
distances from SP can recover the constants used to blind the distances.
Using the unblinded distances, rider to driver distance and Google Near-
est Road API, the adversary can obtain the precise locations of respond-
ing drivers. We conduct experiments with random on-road driver loca-
tions for four diﬀerent cities. Our experiments show that we can deter-
mine the precise locations of at least 80% of the drivers participating in
the enhanced pRide protocol.

Keywords: Ride-Hailing Services, Privacy and Censorship, Attacks

1

Introduction

According to a recent research by MordorIntelligence [8], the global Ride-Hailing
Services (RHS) market, valued at USD 113 billion in 2020, is expected to reach
USD 230 billion by 2026. With such a huge reach, individual privacy and secu-
rity issues are always of primary concern. Ride-Hailing Service Providers (SP)
like Uber, Lyft, Ola provide services in many parts of the world. Among other
features, the SP facilitates ride booking and fare payment options for their cus-
tomers, namely riders who subscribe with the SP for RHS. Drivers of vehicles

S. Murthy and S. Vivek are with the International Institute of Information Technol-
ogy Bangalore, India. Corresponding Author: Shyam Murthy, shyam.sm@iiitb.ac.in

 
 
 
 
 
 
2

S. Murthy and S. Vivek

such as cars and motorcycles sign-up with the SP in order to oﬀer rides. At the
time of subscription, the SP collects private information of riders and drivers in
order to provide services eﬀectively as well as required by local governance laws.
In addition, the SP collects statistics of riders and drivers for every ride that
is oﬀered in its network. This naturally brings up the topic of individual data
privacy concerns from both riders as well as drivers over their data held by the
SP. Also, curious or malicious drivers or riders might be interested in learning
more about the other parties. There are a number of works and their analysis
in the literature that look at privacy-preserving RHS, we list some of them in
Section 5.

Huang et al. proposed pRide [4], a privacy-preserving online RHS protocol
that aims to provide the optimum driver in a global perspective thereby minimiz-
ing the unnecessary travel distance to pick the rider. The protocol makes use of
a deep learning model to predict emergence of new ride requests in a ride-hailing
region to enable the SP to make use of such prediction while matching optimum
drivers to ride requests. They show that by using such a prediction model in a
global perspective, the overall distance travelled by a matching driver is mini-
mized compared with matching a nearest driver in the local region. The protocol
proposes to use a Somewhat Homomorphic Encryption (SHE) scheme to encrypt
rider and driver locations. The advantage of using a homomorphic encryption
scheme is that it allows computations on ciphertexts so that the result of compu-
tation is available only after decryption. Fully Homomorphic Encryption (FHE)
schemes that support potentially any number of homomorphic operations have
high cost in terms of large ciphertexts and high computation latency. Hence,
many practical applications that know, a priori, the bound on the number of
homomorphic operations, prefer to use SHE schemes. In the pRide paper, the
authors use the FV cryptosystem [1] in the implementation of their scheme.
Even though applications make use of semantically secure cryptosystems, care-
ful analysis is required to make sure no unintended security holes are introduced
while adapting the cryptosystem to their applications.

The pRide protocol, described in more detail in Section 2, has two parts,
the basic protocol and an enhanced version. We discuss the basic protocol in
this paragraph. In the initialization phase, SP divides the area of its operation
into grids, the details of which are made available to all parties. SP keeps a
record of ride requests emanating from each grid over speciﬁc time epochs and
trains a prediction model using this information. It then uses this information to
predict the grid-based distribution of requests for the next period, denoted by
P R(g), namely the prediction result for grid id g. Drivers, registered with the
SP, submit their current grid id to the SP so that the SP can maintain the driver
distribution map. A rider who wishes to hail a ride, picks a (public key, secret
key) pair, encrypts her coordinates and sends the ciphertext and public key to
SP along with the ride request. When SP receives the ride request, it performs a
search for a suitable driver in a preset order of grids around the rider’s grid and
obtains a list of candidate drivers using the driver distribution map. SP then
forwards the ride request to all candidate drivers. To oﬀer their ride, drivers

Driver Locations Harvesting Attack on pRide

3

respond to SP by encrypting their location using the rider’s public key. SP then
homomorphically computes the square of the Euclidean distance between rider
and drivers’ encrypted locations and forwards the same to the rider along with
P R(g) where g is the driver’s grid id. Rider decrypts the distances and picks
the shortest distance D0. It then performs two checks over the list of sorted
distances. First, is Di − D0 < D0 − Ddiag?, where Di is the distance for ith driver
and Ddiag is the length of the diagonal of the grid, and second, does the model
predict no new ride request emerging in the driver’s grid within a short time
period? When both these conditions are satisﬁed, the rider informs SP about
the selected index i after which the SP facilitates secure ride establishment with
the rider and selected driver.

In order to optimize their ride matching, the paper proposes enhanced pRide
built on top of the basic pRide protocol, but having a diﬀerent method to pick
the optimum driver. They show that they get better results when a driver also
provides her encrypted distance to the farthest corner of her grid. This way the
rider can use that distance, instead of Ddiag in the aforementioned check to
select the optimum driver. However, the authors notice that if such a distance is
decrypted by an adversarial rider, she can launch an inference attack to obtain
driver’s locations. In order to thwart such an attack, the paper proposes a novel
method where the driver provides SP with her encrypted distances to the four
corners of her grid. SP then picks random integers to homomorphically blind
the distances before sharing the same with the rider. Rider then decrypts the
blinded distances and applies a private comparison algorithm which determines
the result of the inequality Di − D0 < D0 − Dmaxdist, where Dmaxdist is the
distance between the driver and the farthest corner of her grid g. Finally, using
this inequality and P R(g), it outputs the optimum selected driver.

As described earlier, in the enhanced pRide protocol, the SP homomorphi-
cally blinds the encrypted distances with random integers before sharing them
with the rider. In this paper, we show that such a blinding scheme is insecure,
whence an adversary rider can recover the underlying distances and then deduce
the locations of at least 80% of the drivers responding to a single ride request of
the rider when using the enhanced pRide protocol.

1.1 Comparison with ORide [12] Protocol

The pRide paper shows that their enhanced scheme is more eﬀective with the
same level of security as that of the basic version with only a small compromise
in its eﬃciency. In addition, by way of experiments, they show their computation
cost is signiﬁcantly better compared to a state-of-the-art protocol named ORide
[12]. We note here that the method in the basic pRide protocol where the SP
employs the homomorphic property of SHE to compute the Euclidean distance
between driver and rider to share the encrypted distances with rider is identical
to what is described in the ORide paper. The part that is diﬀerent is that in the

Henceforth in the paper, we use the term distance to mean squared Euclidean dis-
tance.

4

S. Murthy and S. Vivek

ORide paper to pick the nearest driver, only drivers inside the rider’s grid are
chosen as candidate drivers, whereas in the pRide protocol, only drivers outside
the rider’s grid are candidate drivers so as to optimize in a global perspective.

In [5], Kumaraswamy et al. demonstrated a driver locations harvesting attack
by honest-but-curious riders on the ORide protocol, where they determine the
exact locations of 40% of drivers participating in the ORide protocol. In the same
paper, the authors also provide a mitigation solution wherein a driver gives her
perturbed location instead of her actual location. The aforementioned attack on
the ORide protocol and the mitigating solution are both applicable to the basic
pRide protocol.

In [10], Murthy et al. demonstrated a driver locations harvesting attack,
again by honest-but-curious adversary riders, using triangulation on the ORide
protocol, where they show that they can determine the exact locations of all
participating drivers in the ORide protocol. Further, they extend their method
onto the mitigation solution suggested by [5] and show that they can determine
locations of between 25% to 50% of the participating drivers.

As mentioned earlier, in the pRide protocol, the method where the rider
obtains encrypted driver distances is identical to that in the ORide protocol.
Due to this, any location harvesting attack on ORide, like in the cases of [5] and
[10], are also directly applicable to the basic pRide protocol.

1.2 Our Contribution

We present a passive driver location harvesting attack on the enhanced pRide
protocol. The honest-but-curious adversary rider issues a single ride request with
a search radius (SR) = 1, such that grids adjacent to the rider’s grid are searched
(as explained in the pRide paper, Section V-B-4, pp. 6). In our attack, the
adversary rider receives, per driver, a set of encrypted blinded distances between
the driver’s location and each corner of the driver’s grid. One would expect that
such a blinding process would make it hard for the rider to deduce anything
about the underlying distances.

Rider decrypts the ciphertexts received from SP to obtain blinded distances.
Next, by computing the Greatest Common Divisor (GCD) of the blinded dis-
tances and eliminating common factors, the rider recovers the blinding values
after which the distances are easily obtained. Rider now has the four distances
from driver to each corner of the driver’s grid. Using these distances, the rider
computes four equiprobable driver locations in each of the four grids adjacent to
the rider’s grid. This is due to the fact that the distances are in random order
and, so, there is no correlation between each corner of the grid and its distance to
the driver. Rider knows the distance between herself and each responding driver.
Now, using the distance between herself and a particular responding driver (say,
δ), the rider draws a rider-circle with center as her location and radius = δ.
Probable driver locations that lie on the rider-circle are ﬁltered in and in case
multiple such locations are obtained, Google Nearest Roads API [2] is used to
output one location that is closest to a motorable road. We conduct our experi-
ments using rectangular grids on four diﬀerent cities around the world and the

Driver Locations Harvesting Attack on pRide

5

results are summarized in Table 1. We show that we can obtain exact driver
locations of up to 80% of drivers who respond to a rider’s request.

Our attack invalidates Theorem 4, pp. 9, of the pRide paper [4], which states
that pRide is adaptively Laccess semantically secure against semi-honest adver-
saries, where Laccess gives the access pattern of the SP and rider, which is simply
the list of drivers that respond to a speciﬁc ride request. Hence, when our attack
is combined with that in [10], the driver location security of the pRide paper is
fully compromised, and so is the mitigation solution of [5] if applied to the basic
pRide protocol. We stress that the attack from [10] is not directly applicable to
the pRide protocol, but works only in combination with our attack.

The rest of the paper is organized as follows. Section 2 describes the pRide
protocol. Section 3 describes our attack. Section 4 gives details about our experi-
ments and results. Section 5 gives some of the recent works in privacy-preserving
RHS, followed by conclusions.

2 Overview of pRide Protocol

In this section, we provide an overview of the pRide protocol followed by a de-
scription of the threat model adopted therein. For more details, the interested
reader is referred to the original paper [4].
Remark : Unless qualiﬁed as enhanced or basic, we will use the term pRide pro-
tocol to refer to the complete pRide protocol, consisting of both the basic and
enhanced parts.

2.1 pRide Protocol

pRide is a privacy-preserving online ride-hailing protocol augmented with a grid-
based rider emergence prediction. The key objective of the protocol is to achieve
optimum driver selection in a global perspective instead of picking the nearest
driver as done in other works [12,13]. Selecting such a driver might be a better
choice in order to minimize the overall empty travel distance traversed by drivers
to pick up riders in the whole system. The prediction of requests based on deep
learning plays an important role in driver selection.

The protocol has two parts, the basic protocol and an enhancement, built
on top of the basic protocol, are summarized in the following steps. Steps 1 to
10 constitute the basic pRide protocol, followed by steps of the enhanced pRide
protocol.

1. The three parties involved in the pRide protocol are: driver, rider and service
provider (SP). The SP does not collude with either rider or drivers. The SP as
well as the users, namely, drivers and riders, are honest-but-curious entities
who execute the protocol correctly, but are keen on knowing about each
other’s sensitive information. The protocol aims to protect all users’ privacy
from other riders and drivers, such that the precise location of one party is
not learnt by the other party during the ride matching process. However, only

6

S. Murthy and S. Vivek

after a driver is matched with a rider, they start to communicate through a
secure channel.

2. During system initialization, the SP divides its area of its operation into
rectangular grids of suitable sizes (size is based on suﬃcient ride density so
as to maintain rider anonymity) and publishes the same. For example, a city
like New York City together with its surrounding boroughs, where the SP is
allowed to provide rides as permitted by local authorities, can be termed as
the SP’s area of operation.

3. Drivers, available to oﬀer rides, submit their real-time grid id to the SP to

enable it to maintain a driver distribution map.

4. Rider, wishing to hail a ride, generates a key pair (public key pk, private key
sk) from the FV SHE scheme [1], encrypts her location using pk, and submits
a ride-request along with her location ciphertext, her current grid id and pk
to the SP. The FV SHE scheme works on integers, hence, the coordinates of
users are encoded as integers using UTM format.

5. SP keeps a record of ride requests in each grid and maintains a real-time ride
request distribution map in every time period. It makes use of Convolutional
long short-term memory (Convolutional LSTM [15]) to train a prediction
model with the ride request distribution information. Based on a temporal
sequence of grid information, SP obtains prediction result P R(g), a non-
negative integer which predicts the number of requests in the next time
period for grid id g.

6. As soon as SP receives the ride request, it performs a driver search with a
search radius (SR) in a preset order of grids starting with the grid nearest to
rider. The rider’s grid is not searched so as to avoid the nearest driver who
would always be found in the rider’s grid. When SR = 1, only grids adjacent
to the rider are searched. Using the driver distribution map, SP creates a
list of candidate drivers and forwards the ride-request to all such drivers.
7. When the ith driver di receives the ride-request, she encrypts her location

using pk and forwards it to SP.

8. SP homomorphically computes the square of the Euclidean distance between
the rider and drivers’ locations. It then forwards these distances to rider along
with driver id i and P R(gi), gi is i’s grid id.

9. Rider uses sk to decrypt the distances and sorts them to obtain the smallest
distance D0. For each distance in the sorted list, she runs the following two
checks to pick the optimum driver:
(a) 2D0 − Di > Ddiag, where Di is the distance for ith driver and Ddiag is

the length of the diagonal of the grid.

(b) P R(gi), where gi is the driver’s grid id, which checks if no new ride

request is emerging in a short time in grid gi.

10. As soon as both the aforementioned conditions are satisﬁed, rider determines
the optimum driver and informs the same to SP to continue with secure ride
establishment between rider and selected driver.

Universal Transverse Mercator: a map-projection system for geographical locations
[19].

Driver Locations Harvesting Attack on pRide

7

11. In order to improve the eﬀectiveness of driver selection, the authors notice
that they can minimize the empty distance travelled by the driver by using
Dmaxdist instead of Ddiag in the ride selection check (Step 9), where Dmaxdist
is the distance between the driver and the farthest corner in her grid. How-
ever, the authors realize that an adversary rider, after decryption, can use
Dmaxdist to launch an inference attack to obtain driver’s precise location.
They, therefore, propose enhanced pRide to thwart such an attack.

12. In the enhanced pRide protocol, each driver, in addition to sending encryp-
tions of her coordinates, also sends the encryptions of distances to each corner
of her grid to the SP.

13. To pick the optimum driver, rider now needs to perform the check 2D0 −
Di > Dmaxdist, for each driver i, using a private comparison algorithm, as
explained below (Steps 15, 16 and 17).

14. As in the earlier basic pRide protocol, rider receives a list of distances to
each of the candidate drivers, decrypts them and selects the smallest D0.
15. In order to ﬁnd the optimum driver, for each Di, i > 0, rider sets D(cid:48) =

2D0 − Di, encrypts D(cid:48) as (cid:102)D(cid:48) and sends (cid:102)D(cid:48) and i to SP.

16. SP receives encrypted distances to each of the four corners of the ith driver’s
grid as ( (cid:102)Dll, (cid:103)Dlu, (cid:103)Drl, (cid:103)Dru). SP generates random positive blinding integers
e and r, and homomorphically blinds each of the ciphertexts as

(cid:102)V (cid:48) = e · (cid:102)D(cid:48) + (cid:101)r
(cid:102)Vll = e · (cid:102)Dll + (cid:101)r
(cid:102)Vlu = e · (cid:103)Dlu + (cid:101)r
(cid:102)Vrl = e · (cid:103)Drl + (cid:101)r
(cid:103)Vru = e · (cid:103)Dru + (cid:101)r.

(1)

It then sends each of these blinded values to rider.
Remark : Homomorphic addition of two ciphertexts, and homomorphic mul-
tiplication of ciphertext with plaintext can be done very eﬃciently in SHE.
17. Rider decrypts each of these blinded values and compares V (cid:48) with each of
(Vll, Vlu, Vrl, Vru). If V (cid:48) is greater than all the four values, then it implies
that D(cid:48) > Dmaxdist.

18. Rider then uses this comparison result and P R(g) value as in the basic pRide
protocol to select the optimum driver and informs the same to SP. If these
checks fail, then the Steps 15 through 18 are repeated until an optimum
driver is obtained by walking through each entry in the candidate driver list.

The authors evaluate the performance of their enhanced pRide protocol over
real-world datasets. Their results show that their protocol is eﬀective in saving
empty distance as well as in maintaining drivers’ privacy during the ride match-
ing process. Finally, they compare the basic and enhanced versions of pRide
and prove that the latter is more eﬀective in choosing the optimum driver with
the same level of privacy. The security of their protocol is based on the appar-
ent hardness of retrieving the blinding parameters when given only the blinded
values.

8

S. Murthy and S. Vivek

In our attack described in Section 3, we show that we can determine the
underlying distance values when given only their blinded values, where blinding
is done as described in Step 16. We then go on to use the distances to get the
precise coordinates of responding drivers.

2.2 Threat Model

We consider the same threat model considered in the pRide protocol, where all
parties, namely the SP, drivers and riders, are honest in executing the protocol.
Riders submit valid requests by encrypting their correct coordinates to the SP,
and the drivers also submit the encryptions of their current coordinates to the
SP. SP does not collude with either drivers or riders. Drivers do not collude with
riders.

All parties are honest-but-curious in the protocol. Thus, each party is curious
to know more about the sensitive information of the other party. In particular,
riders are curious to know about drivers’ locations and vice-versa. pRide also
considers the case of an adversary rider who follows the protocol correctly but
launches an inference attack by performing private computations on received
driver coordinates to infer drivers’ precise locations, and so the authors pro-
pose enhanced pRide to thwart such an attack. Their paper aims to preserve
driver and rider location information from SP, and to preserve driver location
information from rider.

In this paper, we consider the same threat model to model the adversaries.
The ride request issued by an honest-but-curious adversary rider is indistinguish-
able from a ride request issued by any other legitimate rider in the protocol. In
a real-life scenario, a competitor SP with the intention of harvesting driver in-
formation of another SP, can mount such an attack without being detected by
the target SP.

3 Our Attack

In this section, we present our driver location harvesting attack on the enhanced
pRide protocol by a honest-but-curious adversary rider (R). R issues a single ride
request as per the pRide protocol. SP will not be able to distinguish between a
ride request issued by an adversary rider versus another by a legitimate rider.
In this section, for ease of exposition, we explain the recovery of location of one
particular driver Dp, who has responded to ride request by R, shown in Figure 1.
Dp is located at distance δ from R. Our attack extends easily to all responding
drivers, since each response is handled independently by the SP.

3.1 Retrieving Distances

R issues a ride request as per the pRide protocol with search radius SR = 1. By
this, only the grids adjacent to the rider’s grid are searched by SP for candidate
drivers.

Driver Locations Harvesting Attack on pRide

9

We recall here the steps of pRide and enhanced pRide protocols from Section
2.1. In Step 14, the rider R obtains the distances between herself and all the re-
sponding drivers in the clear (distance between R and Dp is δ). In addition, from
Step 16, R receives the ciphertexts ((cid:102)V (cid:48), (cid:102)Vll, (cid:102)Vlu, (cid:102)Vrl, (cid:103)Vru), which after decryption
gives (V (cid:48), Vll, Vlu, Vrl, Vru). We know that (cid:102)D(cid:48) is the encryption of 2D0 − δ, and

V (cid:48) = e · (cid:102)D(cid:48) + (cid:101)r
Vll = e · (cid:102)Dll + (cid:101)r
Vlu = e · (cid:103)Dlu + (cid:101)r
Vrl = e · (cid:103)Drl + (cid:101)r
Vru = e · (cid:103)Dru + (cid:101)r,

(2)

where e and r are the blinding integers chosen by SP.

R then computes the diﬀerence of every pair from (Vll, Vlu, Vrl, Vru), decrypts
them using her secret key and stores them as (P, Q, R, S, T, U ), in no particular
order.
The diﬀerences, thus obtained, are

P = Vll − Vlu = e · (Dll − Dlu)
Q = Vll − Vrl = e · (Dll − Drl)
R = Vll − Vru = e · (Dll − Dru)
S = Vlu − Vrl = e · (Dlu − Drl)
T = Vlu − Vru = e · (Dlu − Dru)
U = Vrl − Vru = e · (Drl − Dru).

(3)

It can be easily seen that the GCD of any two of (P, Q, R, S, T ), say P and
Q, will give either e or its multiple. The latter case will occur when (Dll − Dlu)
and (Dll − Drl) are not relatively prime, and by eliminating any common factors
between them, we can hope to retrieve the exact value of e with a high proba-
bility.
1
ζ(n) ,
Remark : The probability of n randomly chosen integers being coprime is
where ζ is the Riemann Zeta function [20], and for two such integers the prob-
6
π2 . This means in about 60% of cases we can ﬁnd the value of e
ability is
straightaway, and in rest of the cases we can try to eliminate common factors.
Notice that each of the Dxy values are squares of the Euclidean distance between
the driver’s location and each corner of her grid. Let the driver’s coordinates (to
be determined) be (x, y) and the known corners of her grid be (x1, y1), (x2, y2),
(x3, y3) and (x4, y4). W.l.o.g,

Dll = (x1 − x)2 + (y1 − y)2
Dlu = (x2 − x)2 + (y2 − y)2.

(4)

(5)

10

S. Murthy and S. Vivek

Hence, P = e ·
pliﬁes to

(cid:16)(cid:0)(x1 − x)2 + (y1 − y)2(cid:1)−(cid:0)(x2 − x)2 + (y2 − y)2(cid:1)(cid:17)

, which sim-

P = e · (cid:0)(x1 − x2)(x1 + x2 − 2x) + (y1 − y2)(y1 + y2 − 2y)(cid:1).

By eliminating common factors, if any, we obtain

P (cid:48) = e · P/(cid:0)GCD(x1 − x2, y1 − y2) ∗ GCD(2, x1 + x2, y1 + y2)(cid:1).

(6)

(7)

And similarly, we get Q(cid:48), R(cid:48), S(cid:48), T (cid:48), U (cid:48). Finally, GCD(P (cid:48), Q(cid:48), R(cid:48), S(cid:48), T (cid:48), U (cid:48)) gives
the value of e.
Remark : The coordinates of each of the grids are known at system initialization
time. Hence, any common factors between the coordinates can be computed
oﬄine.

In Step 15, rider has the value of (cid:102)D(cid:48), using which the value of (cid:101)r is obtained
from V (cid:48) = e · (cid:102)D(cid:48) + (cid:101)r. And, ﬁnally, using e and (cid:101)r, ( (cid:102)Dll, (cid:103)Dlu, (cid:103)Drl, (cid:103)Dru), and, hence,
(Dll, Dlu, Drl, Dru) are obtained.
Remark : In case we obtain a negative value for (cid:101)r, it implies that our recovery of
e is in error.

3.2 Retrieving Driver Locations

R does not know the correlation between the Dxy distances and the corners
of the grid as they are distances given in random order. In addition, since the
search radius SR = 1, any of the four grids adjacent to the rider’s grid can be a
potential grid of driver Dp.

Using the four distance values (Dll, Dlu, Drl, Dru) as radii and each of the
respective grid corners as center of circles, rider obtains four points in each
grid where all the four circles intersect. These points, in their respective grids,
represent the equiprobable locations of driver Dp. Figure 1 gives a pictorial view
of our attack. Adversary rider R is located in grid g. Driver Dp is located in grid
g4, at a distance δ from R. Each of the four probable driver locations in each
adjacent grids g1 through g4 are shown as small blue dots in each grid.

Using the distance between R and Dp, namely δ, R draws a rider-circle
of radius = δ around herself. As long as the driver has reported her correct
coordinates, it is guaranteed that at least one of the 16 equiprobable driver
locations will lie on the circumference of the rider-circle. If more than one such
location is obtained, then the rider makes use of Google Nearest Road API [2] to
ﬁnd the nearest road to each of such locations. Since we assume that the driver
is located on a motorable road, the adversary algorithm will output the location
closest to the nearest road.

Remark : The mitigation solution of [5] can be applied to the pRide protocol.
While the attack of [10] is still applicable on the basic pRide protocol, we look
at our attack on its enhanced version, when the mitigation solution is applied to
the pRide protocol. In that case, in response to a ride request, the driver would
pick a uniform random location inside a circle of radius τ around her original

Driver Locations Harvesting Attack on pRide

11

Fig. 1. Recovered driver locations shown as small dots each in grid g1 through g4.

location. She then sends the encryption of that random location to the SP, as
well as the encrypted distances from the random location to each of the corners
of her grid. We note that τ should not be too large, as that would have an
adverse eﬀect on driver selection by rider. Our attack, described in Section 3,
would be applicable without any change. However, the retrieved location, in this
case, would be the random location picked by the driver instead of her actual
location. The adversary could then apply the attack of [10] to uncover the actual
driver locations, with a high probability.

4 Experiments and Results

We use Sagemath 8.6 [16] to implement our attack described in Section 3.1
where we retrieve driver distances. The attack, described in Section 3.2, where
we retrieve the driver locations, was implemented in Python and used the Google
Nearest Road APIs for Python [3]. Both parts of the attack were executed on
a commodity laptop with 512 GB SSD and a AMD Ryzen 5 processor. Our
Sagemath and Python programs are available at:
https://github.com/shyamsmurthy/nss2022.

4.1 Experiment Details

Our experiments were run on grids of size about 4km2 superimposed on maps
of 4 large cities around the world, namely, Los Angeles, London, New York City
and Paris. The size of the grid is comparable to what is reported in the pRide
paper. We have done experiments with the number of drivers as 5, 15 and 25
per grid, in each case distributed randomly throughout each grid but located
on motorable areas. We note here that the number of drivers does not have a

12

S. Murthy and S. Vivek

bearing on our attack since the SP encrypts and blinds each driver’s distances
independent of one other.

In each of the maps, we picked random driver locations situated on motorable
roads. Next, a rider location was picked from a random grid in the map. As
explained in Section 3.1, grids adjacent to the rider’s grid was examined and
distances between drivers in those grids and the rider were made available to
the rider. Except for the predicted result (P R) values, this is same as what is
available to the rider in the pRide protocol. The P R values do not have any
bearing on our attack since they do not have any eﬀect on either blinding or
encryption of distances.

Next, from each of the adjacent grids and for each driver in such grid, the
distances from each such driver to her respective grid corners were computed, and
blinded using random integers picked from the range [1, 224], as the maximum
UTM (northing) value of 107 can be represented using 24 bits. In addition, a
distance value known to the adversary is also blinded using the same random
integers. These blinded distances were made available to the adversary rider.
Again, this exactly mimics the behaviour of the enhanced pRide protocol.

Finally, we run the attack described in Section 3 to retrieve the distances

followed by retrieving the driver locations.
Remark 1 : It is claimed that the security of the pRide protocol relies on the hard-
ness of obtaining the blinding parameters when given only the blinded values.
We show in our attack that the adversary can recover the blinding parameters
with a high probability.
Remark 2 : In our experiments, we have used a search radius SR = 1. Our at-
tack methodology can be easily extended to higher values of search radius. Since
the order of grid traversal is known a priori, the new attack has to compute
equiprobable locations in each of the possible grids and continue with our driver
retrieval attack, as described in Section 3.2.

4.2 Results

The results of our experiments are tabulated in Table 1. The pRide paper uses
a 64 × 64 grid over the city of Chengdu, China, and mentions a maximum of
16000 drivers in their experiments, which translates to about 4 drivers per grid
on average. As it can be much larger in high density areas in the city, we run
our experiments with 5, 15 and 25 drivers per grid. It takes less than 1 second
to recover the locations of 25 drivers.

In order to retrieve the distances, we ﬁrst recover the blinding integers e and
r as described in Section 3.1. As shown in Table 1, we can retrieve at least 80%
of the distances successfully, averaged from 10 runs of the experiments for each
driver count over each city. In the unsuccessful cases, we ﬁnd that the value of
the blinding value e retrieved by our algorithm is a multiple of the actual value
of e, and we report this as a failure.

Next, we use the successfully retrieved distances to obtain the precise driver
locations. Here, we use our attack described in Section 3.2. We see that this part
correctly retrieves close to 99% of the driver locations. Hence, our overall driver

Driver Locations Harvesting Attack on pRide

13

location harvesting algorithm retrieves at least 80% of the drivers participating
in the enhanced pRide protocol.

Table 1. Percentage of driver locations recovered for multiple cities.

City

Los Angeles

London

New York City

Paris

Number of participating
drivers (per grid)
5
15
25
5
15
25
5
15
25
5
15
25

%age of driver
coordinates correctly recovered
80
95
89
85
81
86
90
95
93
85
93
88

5 Related Works

There is a large body of work on privacy-preserving RHS which consider preserv-
ing privacy of drivers and riders. ORide [12] and PrivateRide [13], both proposed
by Pham et al., were some of the early works that aimed to preserve rider pri-
vacy against SP and drivers. While PrivateRide makes use of a cloaking region
to maintain privacy, ORide scheme is based on SHE to encrypt driver and rider
locations so as to make use of homomorphic properties of SHE to select nearest
driver. Kumaraswamy et al. [5] proposed an attack that aims to determine loca-
tions of drivers participating in the ORide protocol. In their attack, an adversary
rider can reveal locations of up to 40% of drivers who respond to a single ride
request. They provide a countermeasure to thwart the attack while preserving
suﬃcient anonymity. Murthy et al. [10] proposed an attack that uses triangula-
tion by four colluding adversaries to obtain locations of all drivers participating
in the ORide protocol.

Luo et al. [7] proposed a privacy-preserving ride-matching service also named
pRide. Their protocol involves using two non-colluding servers: SP and CP (a
third-party crypto server), and uses Road Network Embedding (RNE) [14] such
that the road network is transformed to a higher dimension space to enable
eﬃcient distance computation between the network entities. However, the dis-
advantage of their scheme is the use of two non-colluding servers which incurs
inter-server communication costs. Yu et al. [22] proposed lpRide protocol which

14

S. Murthy and S. Vivek

also uses RNE but uses a modiﬁed version of Paillier encryption scheme [11]
to preserve privacy of participating entities. Vivek [17] demonstrated an attack
on the lpRide protocol where they show that any rider or driver can learn the
coordinates of other participating riders. TRACE [18] is a privacy-preserving dy-
namic spatial query RHS scheme proposed by Wang et al., that uses a quadtree
structure and provides high-eﬃciency in terms of complexity and communica-
tion overhead. Kumaraswamy et al. [6] demonstrated an attack on the TRACE
protocol where the SP can identify the exact locations of riders and drivers. Xie
et al. [21] proposed a protocol that also uses RNE to eﬃciently compute shortest
distances. Their scheme makes use of property-preserving hash functions where
the SP can not only compute the rider to driver distances, but also pick the
nearest driver. This way they eliminate the need for an auxiliary crypto server.
All the works listed earlier picks the nearest driver to fulﬁl a ride request. pRide
[4], proposed by Huang et al., does not match the nearest driver but considers
a global matching strategy with the aim of reducing the empty distance trav-
elled by driver to pick the rider. Murthy et al. [10] gave an attack on the ORide
protocol, using triangulation, where they recover locations of all participating
drivers. In addition, by using more number of colluding adversaries, they show
they can recover locations of up to 50% of drivers participating in the variant of
ORide protocol that uses the mitigation solution of [5].

6 Conclusions

In this paper, we presented an attack on enhanced pRide [4] protocol, a privacy-
preserving RHS. We show that an honest-but-curious adversary rider can de-
termine the coordinates of about 80% of drivers responding to the rider’s ride
request as per the pRide protocol.

From Section 1.1, we see that locations of all drivers participating in the
basic pRide protocol can be recovered by one or more adversary riders. As per the
protocol, the rider chooses the optimum driver when given the plaintext distances
to all drivers, and this fact is exploited by the adversary. Alternatively, the SP
can select the optimum driver homomorphically. Since sorting and searching are
high-depth circuits, it is not eﬃcient to perform these operations using SHE
schemes. However, FHE schemes can be explored to evaluate their suitability for
practical RHS solutions.

The enhanced pRide protocol needs to perform comparisons and in order to
preserve privacy, the values are blinded. However, since the order needs to be
preserved, the blinding values are the same for all the comparands, which leads
to the attack. Other secure order-preserving techniques need to be explored.
However, as shown in [9], careful analysis is needed which would otherwise lead
to further attacks.

In summary, we show that although protocols may seem secure in theory, a
thorough analysis should be done which otherwise would expose severe vulner-
abilities and security holes, as demonstrated by our attack in this paper.

Driver Locations Harvesting Attack on pRide

15

Acknowledgements. We thank the anonymous reviewers for their invaluable
comments and suggestions, which helped us improve the manuscript. This work
was partly funded by the INSPIRE Faculty Award (DST, Govt. of India) and the
Infosys Foundation Career Development Chair Professorship grant for Srinivas
Vivek.

References

1. Fan, J., Vercauteren, F.: Somewhat practical fully homomorphic encryption. Cryp-

tology ePrint Archive (2012), http://eprint.iacr.org/2012/144

2. Google: Google Maps Platform.

https://developers.google.com/maps/

documentation/roads/intro/ (2019), retrieved: August 01, 2022

3. Google: Google Maps Platform, client libraries for google maps web services.
https://developers.google.com/maps/web-services/client-library (2019),
retrieved: August 01, 2022

4. Huang, J., Luo, Y., Fu, S., Xu, M., Hu, B.: pride: Privacy-preserving online ride
hailing matching system with prediction. IEEE Transactions on Vehicular Tech-
nology 70(8), 7413–7425 (2021). https://doi.org/10.1109/TVT.2021.3090042
5. Kumaraswamy, D., Murthy, S., Vivek, S.: Revisiting driver privacy in oride. In: Se-
lected Areas in Cryptography - SAC 2021 - 28th International Conference, Univer-
sity of Victoria, British Columbia, Canada (Virtual Event), September 29- October
01, 2020. Lecture Notes in Computer Science, Springer (2021)

6. Kumaraswamy, D., Vivek, S.: Cryptanalysis of the privacy-preserving ride-hailing
service TRACE. In: Adhikari, A., Küsters, R., Preneel, B. (eds.) Progress in Cryp-
tology - INDOCRYPT 2021 - 22nd International Conference on Cryptology in In-
dia, Jaipur, India, December 12-15, 2021, Proceedings. Lecture Notes in Computer
Science, vol. 13143, pp. 462–484. Springer (2021). https://doi.org/10.1007/
978-3-030-92518-5_21, https://doi.org/10.1007/978-3-030-92518-5_21
7. Luo, Y., Jia, X., Fu, S., Xu, M.: pRide: Privacy-Preserving Ride Matching Over
Road Networks for Online Ride-Hailing Service. IEEE Trans. Information Forensics
and Security 14(7), 1791–1802 (2019)

8. Mordor Intelligence: Ride-Hailing Market - Growth, Trends, Covid-19 Im-
pact, And Forecasts (2022 - 2027). https://www.mordorintelligence.com/
industry-reports/ride-hailing-market (2020), retrieved: July 23, 2022

9. Murthy, S., Vivek, S.: Cryptanalysis of a protocol for eﬃcient sorting on SHE
encrypted data. In: Albrecht, M. (ed.) Cryptography and Coding - 17th IMA In-
ternational Conference, IMACC 2019, Oxford, UK, Proceedings. Lecture Notes in
Computer Science, vol. 11929, pp. 278–294. Springer (2019)

10. Murthy, S., Vivek, S.: Passive triangulation attack on oride (2022). https://doi.

org/10.48550/ARXIV.2208.12216, https://arxiv.org/abs/2208.12216

11. Nabeel, M., Appel, S., Bertino, E., Buchmann, A.P.: Privacy preserving context
aware publish subscribe systems. In: López, J., Huang, X., Sandhu, R. (eds.) Net-
work and System Security - 7th International Conference, NSS 2013, Madrid,
Spain, June 3-4, 2013. Proceedings. Lecture Notes in Computer Science, vol. 7873,
pp. 465–478. Springer (2013)

12. Pham, A., Dacosta, I., Endignoux, G., Troncoso-Pastoriza, J.R., Huguenin, K.,
Hubaux, J.: ORide: A Privacy-Preserving yet Accountable Ride-Hailing Service. In:
Kirda, E., Ristenpart, T. (eds.) 26th USENIX Security Symposium, USENIX Se-
curity 2017, Vancouver, BC, Canada, August 16-18, 2017. pp. 1235–1252. USENIX
Association (2017)

16

S. Murthy and S. Vivek

13. Pham, A., Dacosta, I., Jacot-Guillarmod, B., Huguenin, K., Hajar, T., Tramèr, F.,
Gligor, V.D., Hubaux, J.: PrivateRide: A Privacy-Enhanced Ride-Hailing Service.
PoPETs 2017(2), 38–56 (2017), https://doi.org/10.1515/popets-2017-0015
14. Shahabi, C., Kolahdouzan, M.R., Sharifzadeh, M.: A road network embedding
technique for k-nearest neighbor search in moving object databases. In: Voisard,
A., Chen, S. (eds.) ACM-GIS 2002, Proceedings of the Tenth ACM International
Symposium on Advances in Geographic Information Systems, McLean, VA (near
Washington, DC), USA, USA, November 8-9, 2002. pp. 94–10. ACM (2002)
15. Shi, X., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.k., Woo, W.c.: Convolutional
lstm network: A machine learning approach for precipitation nowcasting. In: Pro-
ceedings of the 28th International Conference on Neural Information Processing
Systems - Volume 1. p. 802–810. NIPS’15, MIT Press, Cambridge, MA, USA (2015)
16. Stein, W., et al.: Sage Mathematics Software (Version 8.6). The Sage Development

Team (2019), http://www.sagemath.org

17. Vivek, S.: Attacks on a privacy-preserving publish-subscribe system and a ride-
hailing service. In: Paterson, M.B. (ed.) Cryptography and Coding - 18th IMA
International Conference, IMACC 2021, Virtual Event, December 14-15, 2021,
Proceedings. Lecture Notes in Computer Science, vol. 13129, pp. 59–71. Springer
(2021). https://doi.org/10.1007/978-3-030-92641-0_4, https://doi.org/10.
1007/978-3-030-92641-0_4

18. Wang, F., Zhu, H., Liu, X., Lu, R., Li, F., Li, H., Zhang, S.: Eﬃcient and privacy-
preserving dynamic spatial query scheme for ride-hailing services. IEEE Transac-
tions on Vehicular Technology 67(11), 11084–11097 (2018)

19. Wikipedia
tem.
coordinate_system (2020), retrieved: August 01, 2022

sys-
https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_

contributors: Universal Transverse Mercator

coordinate

20. Wikipedia contributors: Coprime Integers. https://en.wikipedia.org/wiki/

Coprime_integers (2022), retrieved: August 09, 2022

21. Xie, H., Guo, Y., Jia, X.: A privacy-preserving online ride-hailing system without
involving a third trusted server. IEEE Transactions on Information Forensics and
Security 16, 3068–3081 (2021)

22. Yu, H., Shu, J., Jia, X., Zhang, H., Yu, X.: lpride: Lightweight and privacy-
preserving ride matching over road networks in online ride hailing systems. IEEE
Trans. Vehicular Technology 68(11), 10418–10428 (2019)

","2 2 0 2 t c O 4 2 ] R C . s c [ 1 v 3 6 2 3 1 . 0 1 2 2 : v i X r a Driver Locations Harvesting Attack on pRide Shyam Murthy[0000−0002−0222−322X] Srinivas Vivek[0000−0002−8426−0859] IIIT Bangalore, India. shyam.sm@iiitb.ac.in srinivas.vivek@iiitb.ac.in Abstract. Privacy preservation in Ride-Hailing Services (RHS) is in- tended to protect privacy of drivers and riders. pRide, published in IEEE Trans. Vehicular Technology 2021, is a prediction based privacy- preserving RHS protocol to match riders with an optimum driver. In the protocol, the Service Provider (SP) homomorphically computes Eu- clidean distances between encrypted locations of drivers and rider. Rider selects an optimum driver using decrypted distances augmented by a new-ride-emergence prediction. To improve the eﬀectiveness of driver se- lection, the paper proposes an enhanced version where each driver gives encrypted distances to each corner of her grid. To thwart a rider from using these distances to launch an inference attack, the SP blinds these distances before sharing them with the rider. In this work, we propose a passive attack where an honest-but-curious adversary rider who makes a single ride request and receives the blinded distances from SP can recover the constants used to blind the distances. Using the unblinded distances, rider to driver distance and Google Near- est Road API, the adversary can obtain the precise locations of respond- ing drivers. We conduct experiments with random on-road driver loca- tions for four diﬀerent cities. Our experiments show that we can deter- mine the precise locations of at least 80% of the drivers participating in the enhanced pRide protocol. Keywords: Ride-Hailing Services, Privacy and Censorship, Attacks 1 Introduction According to a recent research by MordorIntelligence [8], the global Ride-Hailing Services (RHS) market, valued at USD 113 billion in 2020, is expected to reach USD 230 billion by 2026. With such a huge reach, individual privacy and secu- rity issues are always of primary concern. Ride-Hailing Service Providers (SP) like Uber, Lyft, Ola provide services in many parts of the world. Among other features, the SP facilitates ride booking and fare payment options for their cus- tomers, namely riders who subscribe with the SP for RHS. Drivers of vehicles S. Murthy and S. Vivek are with the International Institute of Information Technol- ogy Bangalore, India. Corresponding Author: Shyam Murthy, shyam.sm@iiitb.ac.in 2 S. Murthy and S. Vivek such as cars and motorcycles sign-up with the SP in order to oﬀer rides. At the time of subscription, the SP collects private information of riders and drivers in order to provide services eﬀectively as well as required by local governance laws. In addition, the SP collects statistics of riders and drivers for every ride that is oﬀered in its network. This naturally brings up the topic of individual data privacy concerns from both riders as well as drivers over their data held by the SP. Also, curious or malicious drivers or riders might be interested in learning more about the other parties. There are a number of works and their analysis in the literature that look at privacy-preserving RHS, we list some of them in Section 5. Huang et al. proposed pRide [4], a privacy-preserving online RHS protocol that aims to provide the optimum driver in a global perspective thereby minimiz- ing the unnecessary travel distance to pick the rider. The protocol makes use of a deep learning model to predict emergence of new ride requests in a ride-hailing region to enable the SP to make use of such prediction while matching optimum drivers to ride requests. They show that by using such a prediction model in a global perspective, the overall distance travelled by a matching driver is mini- mized compared with matching a nearest driver in the local region. The protocol proposes to use a Somewhat Homomorphic Encryption (SHE) scheme to encrypt rider and driver locations. The advantage of using a homomorphic encryption scheme is that it allows computations on ciphertexts so that the result of compu- tation is available only after decryption. Fully Homomorphic Encryption (FHE) schemes that support potentially any number of homomorphic operations have high cost in terms of large ciphertexts and high computation latency. Hence, many practical applications that know, a priori, the bound on the number of homomorphic operations, prefer to use SHE schemes. In the pRide paper, the authors use the FV cryptosystem [1] in the implementation of their scheme. Even though applications make use of semantically secure cryptosystems, care- ful analysis is required to make sure no unintended security holes are introduced while adapting the cryptosystem to their applications. The pRide protocol, described in more detail in Section 2, has two parts, the basic protocol and an enhanced version. We discuss the basic protocol in this paragraph. In the initialization phase, SP divides the area of its operation into grids, the details of which are made available to all parties. SP keeps a record of ride requests emanating from each grid over speciﬁc time epochs and trains a prediction model using this information. It then uses this information to predict the grid-based distribution of requests for the next period, denoted by P R(g), namely the prediction result for grid id g. Drivers, registered with the SP, submit their current grid id to the SP so that the SP can maintain the driver distribution map. A rider who wishes to hail a ride, picks a (public key, secret key) pair, encrypts her coordinates and sends the ciphertext and public key to SP along with the ride request. When SP receives the ride request, it performs a search for a suitable driver in a preset order of grids around the rider’s grid and obtains a list of candidate drivers using the driver distribution map. SP then forwards the ride request to all candidate drivers. To oﬀer their ride, drivers Driver Locations Harvesting Attack on pRide 3 respond to SP by encrypting their location using the rider’s public key. SP then homomorphically computes the square of the Euclidean distance between rider and drivers’ encrypted locations and forwards the same to the rider along with P R(g) where g is the driver’s grid id. Rider decrypts the distances and picks the shortest distance D0. It then performs two checks over the list of sorted distances. First, is Di − D0 < D0 − Ddiag?, where Di is the distance for ith driver and Ddiag is the length of the diagonal of the grid, and second, does the model predict no new ride request emerging in the driver’s grid within a short time period? When both these conditions are satisﬁed, the rider informs SP about the selected index i after which the SP facilitates secure ride establishment with the rider and selected driver. In order to optimize their ride matching, the paper proposes enhanced pRide built on top of the basic pRide protocol, but having a diﬀerent method to pick the optimum driver. They show that they get better results when a driver also provides her encrypted distance to the farthest corner of her grid. This way the rider can use that distance, instead of Ddiag in the aforementioned check to select the optimum driver. However, the authors notice that if such a distance is decrypted by an adversarial rider, she can launch an inference attack to obtain driver’s locations. In order to thwart such an attack, the paper proposes a novel method where the driver provides SP with her encrypted distances to the four corners of her grid. SP then picks random integers to homomorphically blind the distances before sharing the same with the rider. Rider then decrypts the blinded distances and applies a private comparison algorithm which determines the result of the inequality Di − D0 < D0 − Dmaxdist, where Dmaxdist is the distance between the driver and the farthest corner of her grid g. Finally, using this inequality and P R(g), it outputs the optimum selected driver. As described earlier, in the enhanced pRide protocol, the SP homomorphi- cally blinds the encrypted distances with random integers before sharing them with the rider. In this paper, we show that such a blinding scheme is insecure, whence an adversary rider can recover the underlying distances and then deduce the locations of at least 80% of the drivers responding to a single ride request of the rider when using the enhanced pRide protocol. 1.1 Comparison with ORide [12] Protocol The pRide paper shows that their enhanced scheme is more eﬀective with the same level of security as that of the basic version with only a small compromise in its eﬃciency. In addition, by way of experiments, they show their computation cost is signiﬁcantly better compared to a state-of-the-art protocol named ORide [12]. We note here that the method in the basic pRide protocol where the SP employs the homomorphic property of SHE to compute the Euclidean distance between driver and rider to share the encrypted distances with rider is identical to what is described in the ORide paper. The part that is diﬀerent is that in the Henceforth in the paper, we use the term distance to mean squared Euclidean dis- tance. 4 S. Murthy and S. Vivek ORide paper to pick the nearest driver, only drivers inside the rider’s grid are chosen as candidate drivers, whereas in the pRide protocol, only drivers outside the rider’s grid are candidate drivers so as to optimize in a global perspective. In [5], Kumaraswamy et al. demonstrated a driver locations harvesting attack by honest-but-curious riders on the ORide protocol, where they determine the exact locations of 40% of drivers participating in the ORide protocol. In the same paper, the authors also provide a mitigation solution wherein a driver gives her perturbed location instead of her actual location. The aforementioned attack on the ORide protocol and the mitigating solution are both applicable to the basic pRide protocol. In [10], Murthy et al. demonstrated a driver locations harvesting attack, again by honest-but-curious adversary riders, using triangulation on the ORide protocol, where they show that they can determine the exact locations of all participating drivers in the ORide protocol. Further, they extend their method onto the mitigation solution suggested by [5] and show that they can determine locations of between 25% to 50% of the participating drivers. As mentioned earlier, in the pRide protocol, the method where the rider obtains encrypted driver distances is identical to that in the ORide protocol. Due to this, any location harvesting attack on ORide, like in the cases of [5] and [10], are also directly applicable to the basic pRide protocol. 1.2 Our Contribution We present a passive driver location harvesting attack on the enhanced pRide protocol. The honest-but-curious adversary rider issues a single ride request with a search radius (SR) = 1, such that grids adjacent to the rider’s grid are searched (as explained in the pRide paper, Section V-B-4, pp. 6). In our attack, the adversary rider receives, per driver, a set of encrypted blinded distances between the driver’s location and each corner of the driver’s grid. One would expect that such a blinding process would make it hard for the rider to deduce anything about the underlying distances. Rider decrypts the ciphertexts received from SP to obtain blinded distances. Next, by computing the Greatest Common Divisor (GCD) of the blinded dis- tances and eliminating common factors, the rider recovers the blinding values after which the distances are easily obtained. Rider now has the four distances from driver to each corner of the driver’s grid. Using these distances, the rider computes four equiprobable driver locations in each of the four grids adjacent to the rider’s grid. This is due to the fact that the distances are in random order and, so, there is no correlation between each corner of the grid and its distance to the driver. Rider knows the distance between herself and each responding driver. Now, using the distance between herself and a particular responding driver (say, δ), the rider draws a rider-circle with center as her location and radius = δ. Probable driver locations that lie on the rider-circle are ﬁltered in and in case multiple such locations are obtained, Google Nearest Roads API [2] is used to output one location that is closest to a motorable road. We conduct our experi- ments using rectangular grids on four diﬀerent cities around the world and the Driver Locations Harvesting Attack on pRide 5 results are summarized in Table 1. We show that we can obtain exact driver locations of up to 80% of drivers who respond to a rider’s request. Our attack invalidates Theorem 4, pp. 9, of the pRide paper [4], which states that pRide is adaptively Laccess semantically secure against semi-honest adver- saries, where Laccess gives the access pattern of the SP and rider, which is simply the list of drivers that respond to a speciﬁc ride request. Hence, when our attack is combined with that in [10], the driver location security of the pRide paper is fully compromised, and so is the mitigation solution of [5] if applied to the basic pRide protocol. We stress that the attack from [10] is not directly applicable to the pRide protocol, but works only in combination with our attack. The rest of the paper is organized as follows. Section 2 describes the pRide protocol. Section 3 describes our attack. Section 4 gives details about our experi- ments and results. Section 5 gives some of the recent works in privacy-preserving RHS, followed by conclusions. 2 Overview of pRide Protocol In this section, we provide an overview of the pRide protocol followed by a de- scription of the threat model adopted therein. For more details, the interested reader is referred to the original paper [4]. Remark : Unless qualiﬁed as enhanced or basic, we will use the term pRide pro- tocol to refer to the complete pRide protocol, consisting of both the basic and enhanced parts. 2.1 pRide Protocol pRide is a privacy-preserving online ride-hailing protocol augmented with a grid- based rider emergence prediction. The key objective of the protocol is to achieve optimum driver selection in a global perspective instead of picking the nearest driver as done in other works [12,13]. Selecting such a driver might be a better choice in order to minimize the overall empty travel distance traversed by drivers to pick up riders in the whole system. The prediction of requests based on deep learning plays an important role in driver selection. The protocol has two parts, the basic protocol and an enhancement, built on top of the basic protocol, are summarized in the following steps. Steps 1 to 10 constitute the basic pRide protocol, followed by steps of the enhanced pRide protocol. 1. The three parties involved in the pRide protocol are: driver, rider and service provider (SP). The SP does not collude with either rider or drivers. The SP as well as the users, namely, drivers and riders, are honest-but-curious entities who execute the protocol correctly, but are keen on knowing about each other’s sensitive information. The protocol aims to protect all users’ privacy from other riders and drivers, such that the precise location of one party is not learnt by the other party during the ride matching process. However, only 6 S. Murthy and S. Vivek after a driver is matched with a rider, they start to communicate through a secure channel. 2. During system initialization, the SP divides its area of its operation into rectangular grids of suitable sizes (size is based on suﬃcient ride density so as to maintain rider anonymity) and publishes the same. For example, a city like New York City together with its surrounding boroughs, where the SP is allowed to provide rides as permitted by local authorities, can be termed as the SP’s area of operation. 3. Drivers, available to oﬀer rides, submit their real-time grid id to the SP to enable it to maintain a driver distribution map. 4. Rider, wishing to hail a ride, generates a key pair (public key pk, private key sk) from the FV SHE scheme [1], encrypts her location using pk, and submits a ride-request along with her location ciphertext, her current grid id and pk to the SP. The FV SHE scheme works on integers, hence, the coordinates of users are encoded as integers using UTM format. 5. SP keeps a record of ride requests in each grid and maintains a real-time ride request distribution map in every time period. It makes use of Convolutional long short-term memory (Convolutional LSTM [15]) to train a prediction model with the ride request distribution information. Based on a temporal sequence of grid information, SP obtains prediction result P R(g), a non- negative integer which predicts the number of requests in the next time period for grid id g. 6. As soon as SP receives the ride request, it performs a driver search with a search radius (SR) in a preset order of grids starting with the grid nearest to rider. The rider’s grid is not searched so as to avoid the nearest driver who would always be found in the rider’s grid. When SR = 1, only grids adjacent to the rider are searched. Using the driver distribution map, SP creates a list of candidate drivers and forwards the ride-request to all such drivers. 7. When the ith driver di receives the ride-request, she encrypts her location using pk and forwards it to SP. 8. SP homomorphically computes the square of the Euclidean distance between the rider and drivers’ locations. It then forwards these distances to rider along with driver id i and P R(gi), gi is i’s grid id. 9. Rider uses sk to decrypt the distances and sorts them to obtain the smallest distance D0. For each distance in the sorted list, she runs the following two checks to pick the optimum driver: (a) 2D0 − Di > Ddiag, where Di is the distance for ith driver and Ddiag is the length of the diagonal of the grid. (b) P R(gi), where gi is the driver’s grid id, which checks if no new ride request is emerging in a short time in grid gi. 10. As soon as both the aforementioned conditions are satisﬁed, rider determines the optimum driver and informs the same to SP to continue with secure ride establishment between rider and selected driver. Universal Transverse Mercator: a map-projection system for geographical locations [19]. Driver Locations Harvesting Attack on pRide 7 11. In order to improve the eﬀectiveness of driver selection, the authors notice that they can minimize the empty distance travelled by the driver by using Dmaxdist instead of Ddiag in the ride selection check (Step 9), where Dmaxdist is the distance between the driver and the farthest corner in her grid. How- ever, the authors realize that an adversary rider, after decryption, can use Dmaxdist to launch an inference attack to obtain driver’s precise location. They, therefore, propose enhanced pRide to thwart such an attack. 12. In the enhanced pRide protocol, each driver, in addition to sending encryp- tions of her coordinates, also sends the encryptions of distances to each corner of her grid to the SP. 13. To pick the optimum driver, rider now needs to perform the check 2D0 − Di > Dmaxdist, for each driver i, using a private comparison algorithm, as explained below (Steps 15, 16 and 17). 14. As in the earlier basic pRide protocol, rider receives a list of distances to each of the candidate drivers, decrypts them and selects the smallest D0. 15. In order to ﬁnd the optimum driver, for each Di, i > 0, rider sets D(cid:48) = 2D0 − Di, encrypts D(cid:48) as (cid:102)D(cid:48) and sends (cid:102)D(cid:48) and i to SP. 16. SP receives encrypted distances to each of the four corners of the ith driver’s grid as ( (cid:102)Dll, (cid:103)Dlu, (cid:103)Drl, (cid:103)Dru). SP generates random positive blinding integers e and r, and homomorphically blinds each of the ciphertexts as (cid:102)V (cid:48) = e · (cid:102)D(cid:48) + (cid:101)r (cid:102)Vll = e · (cid:102)Dll + (cid:101)r (cid:102)Vlu = e · (cid:103)Dlu + (cid:101)r (cid:102)Vrl = e · (cid:103)Drl + (cid:101)r (cid:103)Vru = e · (cid:103)Dru + (cid:101)r. (1) It then sends each of these blinded values to rider. Remark : Homomorphic addition of two ciphertexts, and homomorphic mul- tiplication of ciphertext with plaintext can be done very eﬃciently in SHE. 17. Rider decrypts each of these blinded values and compares V (cid:48) with each of (Vll, Vlu, Vrl, Vru). If V (cid:48) is greater than all the four values, then it implies that D(cid:48) > Dmaxdist. 18. Rider then uses this comparison result and P R(g) value as in the basic pRide protocol to select the optimum driver and informs the same to SP. If these checks fail, then the Steps 15 through 18 are repeated until an optimum driver is obtained by walking through each entry in the candidate driver list. The authors evaluate the performance of their enhanced pRide protocol over real-world datasets. Their results show that their protocol is eﬀective in saving empty distance as well as in maintaining drivers’ privacy during the ride match- ing process. Finally, they compare the basic and enhanced versions of pRide and prove that the latter is more eﬀective in choosing the optimum driver with the same level of privacy. The security of their protocol is based on the appar- ent hardness of retrieving the blinding parameters when given only the blinded values. 8 S. Murthy and S. Vivek In our attack described in Section 3, we show that we can determine the underlying distance values when given only their blinded values, where blinding is done as described in Step 16. We then go on to use the distances to get the precise coordinates of responding drivers. 2.2 Threat Model We consider the same threat model considered in the pRide protocol, where all parties, namely the SP, drivers and riders, are honest in executing the protocol. Riders submit valid requests by encrypting their correct coordinates to the SP, and the drivers also submit the encryptions of their current coordinates to the SP. SP does not collude with either drivers or riders. Drivers do not collude with riders. All parties are honest-but-curious in the protocol. Thus, each party is curious to know more about the sensitive information of the other party. In particular, riders are curious to know about drivers’ locations and vice-versa. pRide also considers the case of an adversary rider who follows the protocol correctly but launches an inference attack by performing private computations on received driver coordinates to infer drivers’ precise locations, and so the authors pro- pose enhanced pRide to thwart such an attack. Their paper aims to preserve driver and rider location information from SP, and to preserve driver location information from rider. In this paper, we consider the same threat model to model the adversaries. The ride request issued by an honest-but-curious adversary rider is indistinguish- able from a ride request issued by any other legitimate rider in the protocol. In a real-life scenario, a competitor SP with the intention of harvesting driver in- formation of another SP, can mount such an attack without being detected by the target SP. 3 Our Attack In this section, we present our driver location harvesting attack on the enhanced pRide protocol by a honest-but-curious adversary rider (R). R issues a single ride request as per the pRide protocol. SP will not be able to distinguish between a ride request issued by an adversary rider versus another by a legitimate rider. In this section, for ease of exposition, we explain the recovery of location of one particular driver Dp, who has responded to ride request by R, shown in Figure 1. Dp is located at distance δ from R. Our attack extends easily to all responding drivers, since each response is handled independently by the SP. 3.1 Retrieving Distances R issues a ride request as per the pRide protocol with search radius SR = 1. By this, only the grids adjacent to the rider’s grid are searched by SP for candidate drivers. Driver Locations Harvesting Attack on pRide 9 We recall here the steps of pRide and enhanced pRide protocols from Section 2.1. In Step 14, the rider R obtains the distances between herself and all the re- sponding drivers in the clear (distance between R and Dp is δ). In addition, from Step 16, R receives the ciphertexts ((cid:102)V (cid:48), (cid:102)Vll, (cid:102)Vlu, (cid:102)Vrl, (cid:103)Vru), which after decryption gives (V (cid:48), Vll, Vlu, Vrl, Vru). We know that (cid:102)D(cid:48) is the encryption of 2D0 − δ, and V (cid:48) = e · (cid:102)D(cid:48) + (cid:101)r Vll = e · (cid:102)Dll + (cid:101)r Vlu = e · (cid:103)Dlu + (cid:101)r Vrl = e · (cid:103)Drl + (cid:101)r Vru = e · (cid:103)Dru + (cid:101)r, (2) where e and r are the blinding integers chosen by SP. R then computes the diﬀerence of every pair from (Vll, Vlu, Vrl, Vru), decrypts them using her secret key and stores them as (P, Q, R, S, T, U ), in no particular order. The diﬀerences, thus obtained, are P = Vll − Vlu = e · (Dll − Dlu) Q = Vll − Vrl = e · (Dll − Drl) R = Vll − Vru = e · (Dll − Dru) S = Vlu − Vrl = e · (Dlu − Drl) T = Vlu − Vru = e · (Dlu − Dru) U = Vrl − Vru = e · (Drl − Dru). (3) It can be easily seen that the GCD of any two of (P, Q, R, S, T ), say P and Q, will give either e or its multiple. The latter case will occur when (Dll − Dlu) and (Dll − Drl) are not relatively prime, and by eliminating any common factors between them, we can hope to retrieve the exact value of e with a high proba- bility. 1 ζ(n) , Remark : The probability of n randomly chosen integers being coprime is where ζ is the Riemann Zeta function [20], and for two such integers the prob- 6 π2 . This means in about 60% of cases we can ﬁnd the value of e ability is straightaway, and in rest of the cases we can try to eliminate common factors. Notice that each of the Dxy values are squares of the Euclidean distance between the driver’s location and each corner of her grid. Let the driver’s coordinates (to be determined) be (x, y) and the known corners of her grid be (x1, y1), (x2, y2), (x3, y3) and (x4, y4). W.l.o.g, Dll = (x1 − x)2 + (y1 − y)2 Dlu = (x2 − x)2 + (y2 − y)2. (4) (5) 10 S. Murthy and S. Vivek Hence, P = e · pliﬁes to (cid:16)(cid:0)(x1 − x)2 + (y1 − y)2(cid:1)−(cid:0)(x2 − x)2 + (y2 − y)2(cid:1)(cid:17) , which sim- P = e · (cid:0)(x1 − x2)(x1 + x2 − 2x) + (y1 − y2)(y1 + y2 − 2y)(cid:1). By eliminating common factors, if any, we obtain P (cid:48) = e · P/(cid:0)GCD(x1 − x2, y1 − y2) ∗ GCD(2, x1 + x2, y1 + y2)(cid:1). (6) (7) And similarly, we get Q(cid:48), R(cid:48), S(cid:48), T (cid:48), U (cid:48). Finally, GCD(P (cid:48), Q(cid:48), R(cid:48), S(cid:48), T (cid:48), U (cid:48)) gives the value of e. Remark : The coordinates of each of the grids are known at system initialization time. Hence, any common factors between the coordinates can be computed oﬄine. In Step 15, rider has the value of (cid:102)D(cid:48), using which the value of (cid:101)r is obtained from V (cid:48) = e · (cid:102)D(cid:48) + (cid:101)r. And, ﬁnally, using e and (cid:101)r, ( (cid:102)Dll, (cid:103)Dlu, (cid:103)Drl, (cid:103)Dru), and, hence, (Dll, Dlu, Drl, Dru) are obtained. Remark : In case we obtain a negative value for (cid:101)r, it implies that our recovery of e is in error. 3.2 Retrieving Driver Locations R does not know the correlation between the Dxy distances and the corners of the grid as they are distances given in random order. In addition, since the search radius SR = 1, any of the four grids adjacent to the rider’s grid can be a potential grid of driver Dp. Using the four distance values (Dll, Dlu, Drl, Dru) as radii and each of the respective grid corners as center of circles, rider obtains four points in each grid where all the four circles intersect. These points, in their respective grids, represent the equiprobable locations of driver Dp. Figure 1 gives a pictorial view of our attack. Adversary rider R is located in grid g. Driver Dp is located in grid g4, at a distance δ from R. Each of the four probable driver locations in each adjacent grids g1 through g4 are shown as small blue dots in each grid. Using the distance between R and Dp, namely δ, R draws a rider-circle of radius = δ around herself. As long as the driver has reported her correct coordinates, it is guaranteed that at least one of the 16 equiprobable driver locations will lie on the circumference of the rider-circle. If more than one such location is obtained, then the rider makes use of Google Nearest Road API [2] to ﬁnd the nearest road to each of such locations. Since we assume that the driver is located on a motorable road, the adversary algorithm will output the location closest to the nearest road. Remark : The mitigation solution of [5] can be applied to the pRide protocol. While the attack of [10] is still applicable on the basic pRide protocol, we look at our attack on its enhanced version, when the mitigation solution is applied to the pRide protocol. In that case, in response to a ride request, the driver would pick a uniform random location inside a circle of radius τ around her original Driver Locations Harvesting Attack on pRide 11 Fig. 1. Recovered driver locations shown as small dots each in grid g1 through g4. location. She then sends the encryption of that random location to the SP, as well as the encrypted distances from the random location to each of the corners of her grid. We note that τ should not be too large, as that would have an adverse eﬀect on driver selection by rider. Our attack, described in Section 3, would be applicable without any change. However, the retrieved location, in this case, would be the random location picked by the driver instead of her actual location. The adversary could then apply the attack of [10] to uncover the actual driver locations, with a high probability. 4 Experiments and Results We use Sagemath 8.6 [16] to implement our attack described in Section 3.1 where we retrieve driver distances. The attack, described in Section 3.2, where we retrieve the driver locations, was implemented in Python and used the Google Nearest Road APIs for Python [3]. Both parts of the attack were executed on a commodity laptop with 512 GB SSD and a AMD Ryzen 5 processor. Our Sagemath and Python programs are available at: https://github.com/shyamsmurthy/nss2022. 4.1 Experiment Details Our experiments were run on grids of size about 4km2 superimposed on maps of 4 large cities around the world, namely, Los Angeles, London, New York City and Paris. The size of the grid is comparable to what is reported in the pRide paper. We have done experiments with the number of drivers as 5, 15 and 25 per grid, in each case distributed randomly throughout each grid but located on motorable areas. We note here that the number of drivers does not have a 12 S. Murthy and S. Vivek bearing on our attack since the SP encrypts and blinds each driver’s distances independent of one other. In each of the maps, we picked random driver locations situated on motorable roads. Next, a rider location was picked from a random grid in the map. As explained in Section 3.1, grids adjacent to the rider’s grid was examined and distances between drivers in those grids and the rider were made available to the rider. Except for the predicted result (P R) values, this is same as what is available to the rider in the pRide protocol. The P R values do not have any bearing on our attack since they do not have any eﬀect on either blinding or encryption of distances. Next, from each of the adjacent grids and for each driver in such grid, the distances from each such driver to her respective grid corners were computed, and blinded using random integers picked from the range [1, 224], as the maximum UTM (northing) value of 107 can be represented using 24 bits. In addition, a distance value known to the adversary is also blinded using the same random integers. These blinded distances were made available to the adversary rider. Again, this exactly mimics the behaviour of the enhanced pRide protocol. Finally, we run the attack described in Section 3 to retrieve the distances followed by retrieving the driver locations. Remark 1 : It is claimed that the security of the pRide protocol relies on the hard- ness of obtaining the blinding parameters when given only the blinded values. We show in our attack that the adversary can recover the blinding parameters with a high probability. Remark 2 : In our experiments, we have used a search radius SR = 1. Our at- tack methodology can be easily extended to higher values of search radius. Since the order of grid traversal is known a priori, the new attack has to compute equiprobable locations in each of the possible grids and continue with our driver retrieval attack, as described in Section 3.2. 4.2 Results The results of our experiments are tabulated in Table 1. The pRide paper uses a 64 × 64 grid over the city of Chengdu, China, and mentions a maximum of 16000 drivers in their experiments, which translates to about 4 drivers per grid on average. As it can be much larger in high density areas in the city, we run our experiments with 5, 15 and 25 drivers per grid. It takes less than 1 second to recover the locations of 25 drivers. In order to retrieve the distances, we ﬁrst recover the blinding integers e and r as described in Section 3.1. As shown in Table 1, we can retrieve at least 80% of the distances successfully, averaged from 10 runs of the experiments for each driver count over each city. In the unsuccessful cases, we ﬁnd that the value of the blinding value e retrieved by our algorithm is a multiple of the actual value of e, and we report this as a failure. Next, we use the successfully retrieved distances to obtain the precise driver locations. Here, we use our attack described in Section 3.2. We see that this part correctly retrieves close to 99% of the driver locations. Hence, our overall driver Driver Locations Harvesting Attack on pRide 13 location harvesting algorithm retrieves at least 80% of the drivers participating in the enhanced pRide protocol. Table 1. Percentage of driver locations recovered for multiple cities. City Los Angeles London New York City Paris Number of participating drivers (per grid) 5 15 25 5 15 25 5 15 25 5 15 25 %age of driver coordinates correctly recovered 80 95 89 85 81 86 90 95 93 85 93 88 5 Related Works There is a large body of work on privacy-preserving RHS which consider preserv- ing privacy of drivers and riders. ORide [12] and PrivateRide [13], both proposed by Pham et al., were some of the early works that aimed to preserve rider pri- vacy against SP and drivers. While PrivateRide makes use of a cloaking region to maintain privacy, ORide scheme is based on SHE to encrypt driver and rider locations so as to make use of homomorphic properties of SHE to select nearest driver. Kumaraswamy et al. [5] proposed an attack that aims to determine loca- tions of drivers participating in the ORide protocol. In their attack, an adversary rider can reveal locations of up to 40% of drivers who respond to a single ride request. They provide a countermeasure to thwart the attack while preserving suﬃcient anonymity. Murthy et al. [10] proposed an attack that uses triangula- tion by four colluding adversaries to obtain locations of all drivers participating in the ORide protocol. Luo et al. [7] proposed a privacy-preserving ride-matching service also named pRide. Their protocol involves using two non-colluding servers: SP and CP (a third-party crypto server), and uses Road Network Embedding (RNE) [14] such that the road network is transformed to a higher dimension space to enable eﬃcient distance computation between the network entities. However, the dis- advantage of their scheme is the use of two non-colluding servers which incurs inter-server communication costs. Yu et al. [22] proposed lpRide protocol which 14 S. Murthy and S. Vivek also uses RNE but uses a modiﬁed version of Paillier encryption scheme [11] to preserve privacy of participating entities. Vivek [17] demonstrated an attack on the lpRide protocol where they show that any rider or driver can learn the coordinates of other participating riders. TRACE [18] is a privacy-preserving dy- namic spatial query RHS scheme proposed by Wang et al., that uses a quadtree structure and provides high-eﬃciency in terms of complexity and communica- tion overhead. Kumaraswamy et al. [6] demonstrated an attack on the TRACE protocol where the SP can identify the exact locations of riders and drivers. Xie et al. [21] proposed a protocol that also uses RNE to eﬃciently compute shortest distances. Their scheme makes use of property-preserving hash functions where the SP can not only compute the rider to driver distances, but also pick the nearest driver. This way they eliminate the need for an auxiliary crypto server. All the works listed earlier picks the nearest driver to fulﬁl a ride request. pRide [4], proposed by Huang et al., does not match the nearest driver but considers a global matching strategy with the aim of reducing the empty distance trav- elled by driver to pick the rider. Murthy et al. [10] gave an attack on the ORide protocol, using triangulation, where they recover locations of all participating drivers. In addition, by using more number of colluding adversaries, they show they can recover locations of up to 50% of drivers participating in the variant of ORide protocol that uses the mitigation solution of [5]. 6 Conclusions In this paper, we presented an attack on enhanced pRide [4] protocol, a privacy- preserving RHS. We show that an honest-but-curious adversary rider can de- termine the coordinates of about 80% of drivers responding to the rider’s ride request as per the pRide protocol. From Section 1.1, we see that locations of all drivers participating in the basic pRide protocol can be recovered by one or more adversary riders. As per the protocol, the rider chooses the optimum driver when given the plaintext distances to all drivers, and this fact is exploited by the adversary. Alternatively, the SP can select the optimum driver homomorphically. Since sorting and searching are high-depth circuits, it is not eﬃcient to perform these operations using SHE schemes. However, FHE schemes can be explored to evaluate their suitability for practical RHS solutions. The enhanced pRide protocol needs to perform comparisons and in order to preserve privacy, the values are blinded. However, since the order needs to be preserved, the blinding values are the same for all the comparands, which leads to the attack. Other secure order-preserving techniques need to be explored. However, as shown in [9], careful analysis is needed which would otherwise lead to further attacks. In summary, we show that although protocols may seem secure in theory, a thorough analysis should be done which otherwise would expose severe vulner- abilities and security holes, as demonstrated by our attack in this paper. Driver Locations Harvesting Attack on pRide 15 Acknowledgements. We thank the anonymous reviewers for their invaluable comments and suggestions, which helped us improve the manuscript. This work was partly funded by the INSPIRE Faculty Award (DST, Govt. of India) and the Infosys Foundation Career Development Chair Professorship grant for Srinivas Vivek. References 1. Fan, J., Vercauteren, F.: Somewhat practical fully homomorphic encryption. Cryp- tology ePrint Archive (2012), http://eprint.iacr.org/2012/144 2. Google: Google Maps Platform. https://developers.google.com/maps/ documentation/roads/intro/ (2019), retrieved: August 01, 2022 3. Google: Google Maps Platform, client libraries for google maps web services. https://developers.google.com/maps/web-services/client-library (2019), retrieved: August 01, 2022 4. Huang, J., Luo, Y., Fu, S., Xu, M., Hu, B.: pride: Privacy-preserving online ride hailing matching system with prediction. IEEE Transactions on Vehicular Tech- nology 70(8), 7413–7425 (2021). https://doi.org/10.1109/TVT.2021.3090042 5. Kumaraswamy, D., Murthy, S., Vivek, S.: Revisiting driver privacy in oride. In: Se- lected Areas in Cryptography - SAC 2021 - 28th International Conference, Univer- sity of Victoria, British Columbia, Canada (Virtual Event), September 29- October 01, 2020. Lecture Notes in Computer Science, Springer (2021) 6. Kumaraswamy, D., Vivek, S.: Cryptanalysis of the privacy-preserving ride-hailing service TRACE. In: Adhikari, A., Küsters, R., Preneel, B. (eds.) Progress in Cryp- tology - INDOCRYPT 2021 - 22nd International Conference on Cryptology in In- dia, Jaipur, India, December 12-15, 2021, Proceedings. Lecture Notes in Computer Science, vol. 13143, pp. 462–484. Springer (2021). https://doi.org/10.1007/ 978-3-030-92518-5_21, https://doi.org/10.1007/978-3-030-92518-5_21 7. Luo, Y., Jia, X., Fu, S., Xu, M.: pRide: Privacy-Preserving Ride Matching Over Road Networks for Online Ride-Hailing Service. IEEE Trans. Information Forensics and Security 14(7), 1791–1802 (2019) 8. Mordor Intelligence: Ride-Hailing Market - Growth, Trends, Covid-19 Im- pact, And Forecasts (2022 - 2027). https://www.mordorintelligence.com/ industry-reports/ride-hailing-market (2020), retrieved: July 23, 2022 9. Murthy, S., Vivek, S.: Cryptanalysis of a protocol for eﬃcient sorting on SHE encrypted data. In: Albrecht, M. (ed.) Cryptography and Coding - 17th IMA In- ternational Conference, IMACC 2019, Oxford, UK, Proceedings. Lecture Notes in Computer Science, vol. 11929, pp. 278–294. Springer (2019) 10. Murthy, S., Vivek, S.: Passive triangulation attack on oride (2022). https://doi. org/10.48550/ARXIV.2208.12216, https://arxiv.org/abs/2208.12216 11. Nabeel, M., Appel, S., Bertino, E., Buchmann, A.P.: Privacy preserving context aware publish subscribe systems. In: López, J., Huang, X., Sandhu, R. (eds.) Net- work and System Security - 7th International Conference, NSS 2013, Madrid, Spain, June 3-4, 2013. Proceedings. Lecture Notes in Computer Science, vol. 7873, pp. 465–478. Springer (2013) 12. Pham, A., Dacosta, I., Endignoux, G., Troncoso-Pastoriza, J.R., Huguenin, K., Hubaux, J.: ORide: A Privacy-Preserving yet Accountable Ride-Hailing Service. In: Kirda, E., Ristenpart, T. (eds.) 26th USENIX Security Symposium, USENIX Se- curity 2017, Vancouver, BC, Canada, August 16-18, 2017. pp. 1235–1252. USENIX Association (2017) 16 S. Murthy and S. Vivek 13. Pham, A., Dacosta, I., Jacot-Guillarmod, B., Huguenin, K., Hajar, T., Tramèr, F., Gligor, V.D., Hubaux, J.: PrivateRide: A Privacy-Enhanced Ride-Hailing Service. PoPETs 2017(2), 38–56 (2017), https://doi.org/10.1515/popets-2017-0015 14. Shahabi, C., Kolahdouzan, M.R., Sharifzadeh, M.: A road network embedding technique for k-nearest neighbor search in moving object databases. In: Voisard, A., Chen, S. (eds.) ACM-GIS 2002, Proceedings of the Tenth ACM International Symposium on Advances in Geographic Information Systems, McLean, VA (near Washington, DC), USA, USA, November 8-9, 2002. pp. 94–10. ACM (2002) 15. Shi, X., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.k., Woo, W.c.: Convolutional lstm network: A machine learning approach for precipitation nowcasting. In: Pro- ceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1. p. 802–810. NIPS’15, MIT Press, Cambridge, MA, USA (2015) 16. Stein, W., et al.: Sage Mathematics Software (Version 8.6). The Sage Development Team (2019), http://www.sagemath.org 17. Vivek, S.: Attacks on a privacy-preserving publish-subscribe system and a ride- hailing service. In: Paterson, M.B. (ed.) Cryptography and Coding - 18th IMA International Conference, IMACC 2021, Virtual Event, December 14-15, 2021, Proceedings. Lecture Notes in Computer Science, vol. 13129, pp. 59–71. Springer (2021). https://doi.org/10.1007/978-3-030-92641-0_4, https://doi.org/10. 1007/978-3-030-92641-0_4 18. Wang, F., Zhu, H., Liu, X., Lu, R., Li, F., Li, H., Zhang, S.: Eﬃcient and privacy- preserving dynamic spatial query scheme for ride-hailing services. IEEE Transac- tions on Vehicular Technology 67(11), 11084–11097 (2018) 19. Wikipedia tem. coordinate_system (2020), retrieved: August 01, 2022 sys- https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_ contributors: Universal Transverse Mercator coordinate 20. Wikipedia contributors: Coprime Integers. https://en.wikipedia.org/wiki/ Coprime_integers (2022), retrieved: August 09, 2022 21. Xie, H., Guo, Y., Jia, X.: A privacy-preserving online ride-hailing system without involving a third trusted server. IEEE Transactions on Information Forensics and Security 16, 3068–3081 (2021) 22. Yu, H., Shu, J., Jia, X., Zhang, H., Yu, X.: lpride: Lightweight and privacy- preserving ride matching over road networks in online ride hailing systems. IEEE Trans. Vehicular Technology 68(11), 10418–10428 (2019)","['c', 'driver', 'location', 'harvesting', 'attack', 'pride', 'abstract', 'privacy', 'preservation', 'ridehaile', 'service', 'rhs', 'tend', 'protect', 'privacy', 'driver', 'rider', 'pride', 'publish', 'ieee', 'vehicular', 'technology', 'prediction', 'base', 'privacy', 'preserve', 'rh', 'protocol', 'match', 'rider', 'optimum', 'driver', 'protocol', 'service', 'provider', 'sp', 'homomorphically', 'compute', 'clidean', 'distance', 'encrypted', 'location', 'driver', 'rider', 'rider', 'select', 'optimum', 'driver', 'use', 'decrypt', 'distance', 'augment', 'newrideemergence', 'prediction', 'improve', 'eﬀectiveness', 'driver', 'lection', 'paper', 'propose', 'enhanced', 'version', 'driver', 'give', 'encrypted', 'distance', 'corner', 'grid', 'thwart', 'rider', 'use', 'distance', 'launch', 'inference', 'attack', 'sp', 'blind', 'distance', 'share', 'rider', 'work', 'propose', 'passive', 'attack', 'honestbutcurious', 'adversary', 'rider', 'make', 'single', 'ride', 'request', 'receive', 'blind', 'distance', 'sp', 'recover', 'constant', 'use', 'blind', 'distance', 'use', 'unblinded', 'distance', 'rider', 'driver', 'distance', 'api', 'adversary', 'obtain', 'precise', 'location', 'respond', 'ing', 'driver', 'conduct', 'experiment', 'random', 'onroad', 'driver', 'loca', 'tion', 'diﬀerent', 'city', 'experiment', 'show', 'deter', 'precise', 'location', 'least', 'driver', 'participate', 'enhance', 'pride', 'protocol', 'keyword', 'ridehaile', 'service', 'privacy', 'censorship', 'attack', 'introduction', 'accord', 'recent', 'research', 'mordorintelligence', 'global', 'ridehaile', 'service', 'rhs', 'market', 'value', 'usd', 'expect', 'reach', 'usd', 'huge', 'reach', 'individual', 'privacy', 'secu', 'rity', 'issue', 'always', 'primary', 'concern', 'ridehaile', 'service', 'provider', 'sp', 'uber', 'lyft', 'ola', 'provide', 'service', 'many', 'part', 'world', 'feature', 'sp', 'facilitate', 'ride', 'booking', 'fare', 'payment', 'option', 'cus', 'tomer', 'namely', 'rider', 'subscribe', 'sp', 'rhs', 'driver', 'vehicle', 'murthy', 'information', 'technol', 'ogy', 'bangalore', 'correspond', 'author', 'murthy', 'vivek', 'car', 'motorcycle', 'signup', 'sp', 'order', 'oﬀer', 'ride', 'time', 'subscription', 'sp', 'collect', 'private', 'information', 'rider', 'driver', 'order', 'provide', 'service', 'eﬀectively', 'well', 'require', 'local', 'governance', 'law', 'addition', 'sp', 'collect', 'statistic', 'rider', 'driver', 'ride', 'oﬀere', 'network', 'naturally', 'bring', 'topic', 'individual', 'datum', 'privacy', 'concern', 'rider', 'well', 'driver', 'datum', 'hold', 'sp', 'also', 'curious', 'malicious', 'driver', 'rider', 'interested', 'learn', 'party', 'number', 'work', 'analysis', 'literature', 'look', 'privacypreserve', 'rhs', 'list', 'section', 'propose', 'pride', 'privacypreserve', 'online', 'rhs', 'protocol', 'aim', 'provide', 'optimum', 'driver', 'global', 'perspective', 'thereby', 'minimiz', 'e', 'unnecessary', 'travel', 'distance', 'pick', 'rider', 'protocol', 'make', 'use', 'deep', 'learning', 'model', 'predict', 'emergence', 'new', 'ride', 'request', 'ridehaile', 'region', 'enable', 'sp', 'make', 'use', 'prediction', 'match', 'optimum', 'driver', 'ride', 'request', 'show', 'use', 'prediction', 'model', 'global', 'perspective', 'overall', 'distance', 'travel', 'matching', 'driver', 'mini', 'mize', 'compare', 'match', 'near', 'driver', 'local', 'region', 'protocol', 'propose', 'use', 'somewhat', 'homomorphic', 'encryption', 'scheme', 'encrypt', 'rider', 'driver', 'location', 'advantage', 'use', 'homomorphic', 'encryption', 'scheme', 'allow', 'computation', 'ciphertexts', 'result', 'compu', 'tation', 'available', 'decryption', 'fully', 'homomorphic', 'encryption', 'fhe', 'scheme', 'support', 'potentially', 'number', 'homomorphic', 'operation', 'high', 'cost', 'term', 'large', 'ciphertexts', 'high', 'computation', 'latency', 'hence', 'many', 'practical', 'application', 'know', 'bound', 'number', 'homomorphic', 'operation', 'prefer', 'use', 'scheme', 'pride', 'paper', 'author', 'use', 'cryptosystem', 'implementation', 'scheme', 'even', 'application', 'make', 'use', 'semantically', 'secure', 'cryptosystem', 'care', 'ful', 'analysis', 'require', 'make', 'sure', 'unintended', 'security', 'hole', 'introduce', 'adapt', 'cryptosystem', 'application', 'pride', 'protocol', 'describe', 'detail', 'section', 'part', 'basic', 'protocol', 'enhance', 'version', 'discuss', 'basic', 'protocol', 'paragraph', 'initialization', 'phase', 'sp', 'divide', 'area', 'operation', 'grid', 'detail', 'make', 'available', 'party', 'sp', 'keep', 'record', 'ride', 'request', 'emanate', 'grid', 'speciﬁc', 'time', 'epoch', 'train', 'prediction', 'model', 'use', 'information', 'use', 'information', 'predict', 'gridbased', 'distribution', 'request', 'next', 'period', 'denote', 'p', 'rg', 'namely', 'prediction', 'result', 'grid', 'driver', 'register', 'sp', 'submit', 'current', 'grid', 'sp', 'sp', 'maintain', 'driver', 'distribution', 'map', 'rider', 'wish', 'hail', 'ride', 'pick', 'public', 'key', 'secret', 'key', 'pair', 'encrypt', 'coordinate', 'send', 'ciphertext', 'public', 'key', 'sp', 'ride', 'request', 'sp', 'receive', 'ride', 'request', 'perform', 'search', 'suitable', 'driver', 'preset', 'order', 'grid', 'rider', 'grid', 'obtain', 'list', 'candidate', 'driver', 'use', 'driver', 'distribution', 'map', 'sp', 'forward', 'ride', 'request', 'candidate', 'driver', 'oﬀer', 'ride', 'driver', 'driver', 'location', 'harvesting', 'attack', 'pride', 'respond', 'sp', 'encrypt', 'location', 'use', 'rider', 'public', 'key', 'sp', 'homomorphically', 'compute', 'square', 'euclidean', 'distance', 'rider', 'driver', 'encrypted', 'location', 'forward', 'rider', 'p', 'rg', 'g', 'driver', 'grid', 'rider', 'decrypt', 'distance', 'pick', 'short', 'distance', 'd0', 'perform', 'check', 'list', 'sorted', 'distance', 'first', 'd0', 'di', 'distance', 'length', 'diagonal', 'grid', 'second', 'model', 'predict', 'new', 'ride', 'request', 'emerge', 'driver', 'grid', 'short', 'time', 'period', 'condition', 'satisﬁed', 'rider', 'inform', 'sp', 'select', 'index', 'sp', 'facilitate', 'secure', 'ride', 'establishment', 'rider', 'select', 'driver', 'order', 'optimize', 'ride', 'match', 'paper', 'propose', 'enhanced', 'pride', 'build', 'top', 'basic', 'pride', 'protocol', 'diﬀerent', 'method', 'pick', 'optimum', 'driver', 'show', 'get', 'well', 'result', 'driver', 'also', 'provide', 'encrypted', 'distance', 'farth', 'corner', 'grid', 'way', 'rider', 'use', 'distance', 'instead', 'ddiag', 'aforementioned', 'check', 'select', 'optimum', 'driver', 'however', 'author', 'notice', 'distance', 'decrypt', 'adversarial', 'rider', 'launch', 'inference', 'attack', 'obtain', 'driver', 'location', 'order', 'thwart', 'attack', 'paper', 'propose', 'novel', 'method', 'driver', 'provide', 'sp', 'encrypt', 'distance', 'corner', 'grid', 'sp', 'pick', 'random', 'integer', 'homomorphically', 'blind', 'distance', 'share', 'rider', 'rider', 'decrypt', 'blind', 'distance', 'apply', 'private', 'comparison', 'algorithm', 'determine', 'result', 'inequality', 'd0', '−', 'dmaxdist', 'dmaxdist', 'distance', 'driver', 'farth', 'corner', 'grid', 'g', 'finally', 'use', 'inequality', 'p', 'output', 'optimum', 'select', 'driver', 'describe', 'early', 'enhance', 'pride', 'protocol', 'sp', 'homomorphi', 'cally', 'blind', 'encrypt', 'distance', 'random', 'integer', 'share', 'rider', 'paper', 'show', 'blind', 'scheme', 'insecure', 'adversary', 'rider', 'recover', 'underlying', 'distance', 'deduce', 'location', 'least', 'driver', 'respond', 'single', 'ride', 'request', 'rider', 'use', 'enhance', 'pride', 'protocol', 'comparison', 'protocol', 'pride', 'paper', 'show', 'enhanced', 'scheme', 'eﬀective', 'level', 'security', 'basic', 'version', 'small', 'compromise', 'eﬃciency', 'addition', 'way', 'experiment', 'show', 'computation', 'cost', 'signiﬁcantly', 'well', 'compare', 'stateoftheart', 'protocol', 'name', 'oride', 'note', 'method', 'basic', 'pride', 'protocol', 'sp', 'employ', 'homomorphic', 'property', 'compute', 'euclidean', 'distance', 'driver', 'rider', 'share', 'encrypt', 'distance', 'rider', 'identical', 'describe', 'oride', 'paper', 'part', 'diﬀerent', 'henceforth', 'paper', 'use', 'term', 'distance', 'mean', 'squared', 'euclidean', 'tance', 'murthy', 'vivek', 'oride', 'paper', 'pick', 'near', 'driver', 'driver', 'rider', 'grid', 'choose', 'candidate', 'driver', 'pride', 'protocol', 'driver', 'rider', 'grid', 'candidate', 'driver', 'optimize', 'global', 'perspective', 'kumaraswamy', 'demonstrate', 'driver', 'location', 'harvesting', 'attack', 'honestbutcurious', 'rider', 'oride', 'protocol', 'determine', 'exact', 'location', 'driver', 'participate', 'oride', 'protocol', 'paper', 'author', 'also', 'provide', 'mitigation', 'solution', 'driver', 'give', 'perturb', 'location', 'instead', 'actual', 'location', 'aforementioned', 'attack', 'protocol', 'mitigating', 'solution', 'applicable', 'basic', 'pride', 'protocol', 'murthy', 'demonstrate', 'driver', 'location', 'harvesting', 'attack', 'honestbutcurious', 'adversary', 'rider', 'use', 'triangulation', 'oride', 'protocol', 'show', 'determine', 'exact', 'location', 'participate', 'driver', 'oride', 'protocol', 'far', 'extend', 'method', 'mitigation', 'solution', 'suggest', 'show', 'determine', 'location', 'participate', 'driver', 'mention', 'early', 'pride', 'protocol', 'method', 'rider', 'obtain', 'encrypt', 'driver', 'distance', 'identical', 'location', 'harvesting', 'attack', 'oride', 'case', 'also', 'directly', 'applicable', 'basic', 'pride', 'protocol', 'contribution', 'present', 'passive', 'driver', 'location', 'harvesting', 'attack', 'enhance', 'pride', 'protocol', 'honestbutcurious', 'adversary', 'rider', 'issue', 'single', 'ride', 'request', 'search', 'radius', 'grid', 'adjacent', 'rider', 'grid', 'search', 'explain', 'pride', 'paper', 'section', 'attack', 'adversary', 'rider', 'receive', 'driver', 'set', 'encrypted', 'blind', 'distance', 'driver', 'location', 'corner', 'driver', 'grid', 'expect', 'blind', 'process', 'make', 'hard', 'rider', 'deduce', 'underlie', 'distance', 'rider', 'decrypt', 'ciphertexts', 'receive', 'sp', 'obtain', 'blind', 'distance', 'next', 'compute', 'great', 'common', 'divisor', 'gcd', 'blind', 'dis', 'tance', 'eliminate', 'common', 'factor', 'rider', 'recover', 'blind', 'value', 'distance', 'easily', 'obtain', 'rider', 'distance', 'driver', 'corner', 'driver', 'grid', 'use', 'distance', 'rider', 'compute', 'equiprobable', 'driver', 'location', 'grid', 'adjacent', 'rider', 'grid', 'due', 'fact', 'distance', 'random', 'order', 'correlation', 'corner', 'grid', 'distance', 'driver', 'rider', 'know', 'distance', 'respond', 'driver', 'use', 'distance', 'particular', 'respond', 'driver', 'say', 'rider', 'draw', 'ridercircle', 'center', 'location', 'δ', 'probable', 'driver', 'location', 'lie', 'ridercircle', 'ﬁltere', 'case', 'multiple', 'location', 'obtain', 'near', 'road', 'use', 'output', 'location', 'close', 'motorable', 'road', 'conduct', 'experi', 'ment', 'use', 'rectangular', 'grid', 'diﬀerent', 'city', 'world', 'driver', 'location', 'harvesting', 'attack', 'pride', 'result', 'summarize', 'table', 'show', 'obtain', 'exact', 'driver', 'location', 'driver', 'respond', 'rider', 'request', 'attack', 'invalidate', 'theorem', 'pp', 'pride', 'paper', 'state', 'pride', 'adaptively', 'laccess', 'semantically', 'secure', 'semihon', 'adver', 'sarie', 'laccess', 'give', 'access', 'pattern', 'sp', 'rider', 'simply', 'list', 'driver', 'respond', 'speciﬁc', 'ride', 'request', 'hence', 'attack', 'combine', 'driver', 'location', 'security', 'pride', 'paper', 'fully', 'compromise', 'mitigation', 'solution', 'apply', 'basic', 'pride', 'protocol', 'stress', 'attack', 'directly', 'applicable', 'pride', 'protocol', 'work', 'combination', 'attack', 'rest', 'paper', 'organize', 'follow', 'section', 'describe', 'section', 'describe', 'attack', 'section', 'give', 'detail', 'experi', 'ment', 'result', 'section', 'give', 'recent', 'work', 'privacypreserve', 'rh', 'follow', 'conclusion', 'overview', 'pride', 'protocol', 'section', 'provide', 'overview', 'pride', 'protocol', 'follow', 'de', 'scription', 'threat', 'model', 'adopt', 'therein', 'detail', 'interested', 'reader', 'refer', 'original', 'paper', 'remark', 'enhanced', 'basic', 'use', 'term', 'pride', 'tocol', 'refer', 'complete', 'pride', 'protocol', 'consist', 'basic', 'enhance', 'part', 'pride', 'protocol', 'pride', 'privacypreserving', 'online', 'ridehaile', 'protocol', 'augment', 'grid', 'base', 'rider', 'emergence', 'prediction', 'key', 'objective', 'protocol', 'achieve', 'optimum', 'driver', 'selection', 'global', 'perspective', 'instead', 'pick', 'near', 'driver', 'work', 'select', 'driver', 'well', 'choice', 'order', 'minimize', 'overall', 'empty', 'travel', 'distance', 'traverse', 'driver', 'pick', 'rider', 'whole', 'system', 'prediction', 'request', 'base', 'deep', 'learning', 'play', 'important', 'role', 'driver', 'selection', 'protocol', 'part', 'basic', 'protocol', 'enhancement', 'build', 'top', 'basic', 'protocol', 'summarize', 'follow', 'step', 'step', 'constitute', 'basic', 'pride', 'protocol', 'follow', 'step', 'enhance', 'pride', 'protocol', 'party', 'involve', 'pride', 'protocol', 'driver', 'rider', 'service', 'provider', 'sp', 'sp', 'collude', 'rider', 'driver', 'sp', 'well', 'user', 'namely', 'driver', 'rider', 'honestbutcurious', 'entity', 'execute', 'protocol', 'correctly', 'keen', 'know', 'sensitive', 'information', 'protocol', 'aim', 'protect', 'user', 'privacy', 'rider', 'driver', 'precise', 'location', 'party', 'learn', 'party', 'ride', 'matching', 'process', 'however', 'murthy', 'vivek', 'driver', 'match', 'rider', 'start', 'communicate', 'secure', 'channel', 'system', 'initialization', 'sp', 'divide', 'area', 'operation', 'rectangular', 'grid', 'suitable', 'size', 'size', 'base', 'suﬃcient', 'ride', 'density', 'maintain', 'rider', 'anonymity', 'publish', 'example', 'city', 'together', 'surround', 'borough', 'sp', 'allow', 'provide', 'ride', 'permit', 'local', 'authority', 'term', 'area', 'operation', 'driver', 'available', 'oﬀer', 'ride', 'submit', 'realtime', 'grid', 'sp', 'enable', 'maintain', 'driver', 'distribution', 'map', 'rider', 'wish', 'hail', 'ride', 'generate', 'key', 'pair', 'public', 'key', 'pk', 'private', 'key', 'sk', 'scheme', 'encrypt', 'location', 'use', 'pk', 'submit', 'riderequest', 'location', 'ciphertext', 'current', 'grid', 'pk', 'sp', 'scheme', 'work', 'integer', 'hence', 'coordinate', 'user', 'encode', 'integer', 'use', 'format', 'sp', 'keep', 'record', 'ride', 'request', 'grid', 'maintain', 'realtime', 'ride', 'request', 'distribution', 'map', 'time', 'period', 'make', 'use', 'convolutional', 'long', 'shortterm', 'memory', 'convolutional', 'lstm', 'train', 'prediction', 'model', 'ride', 'request', 'distribution', 'information', 'base', 'temporal', 'sequence', 'grid', 'information', 'sp', 'obtain', 'prediction', 'result', 'p', 'non', 'negative', 'integer', 'predict', 'number', 'request', 'next', 'time', 'period', 'grid', 'soon', 'sp', 'receive', 'ride', 'request', 'perform', 'driver', 'search', 'search', 'radius', 'preset', 'order', 'grid', 'start', 'grid', 'near', 'rider', 'rider', 'grid', 'search', 'avoid', 'near', 'driver', 'always', 'find', 'rider', 'grid', 'grid', 'adjacent', 'rider', 'search', 'use', 'driver', 'distribution', 'map', 'sp', 'create', 'list', 'candidate', 'driver', 'forward', 'riderequ', 'driver', 'receive', 'riderequ', 'encrypt', 'location', 'use', 'pk', 'forward', 'sp', 'sp', 'homomorphically', 'compute', 'square', 'euclidean', 'distance', 'rider', 'driver', 'location', 'forward', 'distance', 'rider', 'along', 'driver', 'p', 'rgi', '’s', 'grid', 'rider', 'use', 'sk', 'decrypt', 'distance', 'sort', 'obtain', 'small', 'distance', 'd0', 'distance', 'sorted', 'list', 'run', 'follow', 'check', 'pick', 'optimum', 'driver', '2d0', 'ddiag', 'distance', 'length', 'diagonal', 'grid', 'p', 'rgi', 'driver', 'grid', 'check', 'new', 'ride', 'request', 'emerge', 'short', 'time', 'grid', 'gi', 'soon', 'aforementioned', 'condition', 'satisﬁed', 'rider', 'determine', 'optimum', 'driver', 'inform', 'sp', 'continue', 'secure', 'ride', 'establishment', 'rider', 'select', 'driver', 'universal', 'transverse', 'mercator', 'mapprojection', 'system', 'geographical', 'location', 'driver', 'location', 'harvesting', 'attack', 'pride', 'order', 'improve', 'eﬀectiveness', 'driver', 'selection', 'author', 'notice', 'minimize', 'empty', 'distance', 'travel', 'driver', 'use', 'dmaxdist', 'instead', 'ddiag', 'ride', 'selection', 'check', 'step', 'dmaxdist', 'distance', 'driver', 'farth', 'corner', 'grid', 'ever', 'author', 'realize', 'adversary', 'rider', 'decryption', 'use', 'dmaxdist', 'launch', 'inference', 'attack', 'obtain', 'driver', 'precise', 'location', 'therefore', 'propose', 'enhanced', 'pride', 'thwart', 'attack', 'enhance', 'pride', 'protocol', 'driver', 'addition', 'send', 'encryp', 'tion', 'coordinate', 'also', 'send', 'encryption', 'distance', 'corner', 'grid', 'sp', 'pick', 'optimum', 'driver', 'rider', 'need', 'perform', 'check', 'di', 'dmaxdist', 'driver', 'use', 'private', 'comparison', 'explain', 'step', 'early', 'basic', 'pride', 'protocol', 'rider', 'receive', 'list', 'distance', 'candidate', 'driver', 'decrypt', 'select', 'small', 'd0', 'order', 'ﬁnd', 'optimum', 'driver', 'di', 'rider', 'set', 'dcid48', 'di', 'encrypt', 'dcid48', 'cid102dcid48', 'send', 'cid102dcid48', 'sp', 'sp', 'receive', 'encrypted', 'distance', 'corner', 'grid', 'cid103dru', 'generate', 'random', 'positive', 'blinding', 'integer', 'e', 'r', 'homomorphically', 'blind', 'ciphertexts', 'cid102v', 'e', 'cid102dcid48', 'cid102vrl', 'e', 'e', 'send', 'blind', 'value', 'rider', 'remark', 'homomorphic', 'addition', 'ciphertext', 'homomorphic', 'mul', 'tiplication', 'ciphertext', 'plaintext', 'eﬃciently', 'rider', 'decrypt', 'blind', 'value', 'compare', 'great', 'value', 'imply', 'dcid48', 'dmaxdist', 'rider', 'use', 'comparison', 'result', 'p', 'rg', 'value', 'basic', 'pride', 'protocol', 'select', 'optimum', 'driver', 'inform', 'sp', 'check', 'fail', 'step', 'repeat', 'optimum', 'driver', 'obtain', 'walk', 'entry', 'candidate', 'driver', 'list', 'author', 'evaluate', 'performance', 'enhance', 'pride', 'protocol', 'dataset', 'result', 'show', 'protocol', 'eﬀective', 'save', 'empty', 'distance', 'well', 'maintain', 'driver', 'privacy', 'ride', 'match', 'ing', 'process', 'finally', 'compare', 'basic', 'enhanced', 'version', 'pride', 'prove', 'latter', 'eﬀective', 'choose', 'optimum', 'driver', 'level', 'privacy', 'security', 'protocol', 'base', 'appar', 'ent', 'hardness', 'retrieve', 'blind', 'parameter', 'give', 'blind', 'value', 'murthy', 'vivek', 'attack', 'describe', 'section', 'show', 'determine', 'underlie', 'distance', 'value', 'give', 'blind', 'value', 'blind', 'describe', 'step', 'go', 'use', 'distance', 'get', 'precise', 'coordinate', 'respond', 'driver', 'threat', 'model', 'consider', 'threat', 'model', 'consider', 'pride', 'protocol', 'party', 'namely', 'sp', 'driver', 'rider', 'honest', 'execute', 'protocol', 'rider', 'submit', 'valid', 'request', 'encrypt', 'correct', 'coordinate', 'sp', 'driver', 'also', 'submit', 'encryption', 'current', 'coordinate', 'sp', 'sp', 'collude', 'driver', 'rider', 'driver', 'collude', 'rider', 'party', 'honestbutcurious', 'protocol', 'thus', 'party', 'curious', 'know', 'sensitive', 'information', 'party', 'particular', 'rider', 'curious', 'know', 'driver', 'location', 'viceversa', 'pride', 'also', 'consider', 'case', 'adversary', 'rider', 'follow', 'protocol', 'correctly', 'launch', 'inference', 'attack', 'perform', 'private', 'computation', 'receive', 'driver', 'coordinate', 'infer', 'driver', 'precise', 'location', 'author', 'pro', 'pose', 'enhanced', 'pride', 'thwart', 'attack', 'paper', 'aim', 'preserve', 'driver', 'rider', 'location', 'information', 'sp', 'preserve', 'driver', 'location', 'information', 'rider', 'paper', 'consider', 'threat', 'model', 'model', 'adversary', 'ride', 'request', 'issue', 'honestbutcurious', 'adversary', 'rider', 'indistinguish', 'able', 'ride', 'request', 'issue', 'legitimate', 'rider', 'protocol', 'reallife', 'scenario', 'competitor', 'sp', 'intention', 'harvesting', 'driver', 'formation', 'sp', 'mount', 'attack', 'detect', 'target', 'sp', 'attack', 'section', 'present', 'driver', 'location', 'harvesting', 'attack', 'enhance', 'pride', 'protocol', 'honestbutcurious', 'adversary', 'rider', 'r', 'r', 'issue', 'single', 'ride', 'request', 'pride', 'protocol', 'sp', 'able', 'distinguish', 'ride', 'request', 'issue', 'adversary', 'rider', 'legitimate', 'rider', 'section', 'ease', 'exposition', 'explain', 'recovery', 'location', 'particular', 'driver', 'respond', 'ride', 'request', 'show', 'figure', 'dp', 'locate', 'distance', 'r', 'attack', 'extend', 'easily', 'respond', 'driver', 'response', 'handle', 'independently', 'sp', 'retrieve', 'distance', 'r', 'issue', 'ride', 'request', 'pride', 'protocol', 'search', 'grid', 'adjacent', 'rider', 'grid', 'search', 'sp', 'candidate', 'driver', 'driver', 'location', 'harvesting', 'attack', 'pride', 'recall', 'step', 'pride', 'enhance', 'pride', 'protocol', 'section', 'step', 'rider', 'r', 'obtain', 'distance', 'sponde', 'driver', 'clear', 'distance', 'r', 'dp', 'δ', 'addition', 'step', 'r', 'receive', 'ciphertexts', 'cid102v', 'cid102vll', 'cid102vlu', 'cid102vrl', 'cid103vru', 'decryption', 'give', 'know', 'cid102dcid48', 'encryption', 'e', 'cid102dcid48', 'e', 'e', 'e', 'r', 'blind', 'integer', 'choose', 'sp', 'r', 'compute', 'diﬀerence', 'pair', 'decrypt', 'use', 'secret', 'key', 'store', 'p', 'r', 'particular', 'order', 'diﬀerence', 'thus', 'obtain', 'dlu', 'q', 'drl', 'r', 'dru', 'vrl', 'dlu', '−', 'drl', 'vru', 'dlu', 'dru', 'dru', 'easily', 'see', 'gcd', 'p', 'q', 'r', 'say', 'p', 'q', 'give', 'e', 'multiple', 'latter', 'case', 'occur', '−', 'dlu', 'dll', '−', 'drl', 'relatively', 'prime', 'eliminate', 'common', 'factor', 'hope', 'retrieve', 'exact', 'value', 'e', 'high', 'proba', 'bility', 'remark', 'probability', 'randomly', 'choose', 'integer', 'coprime', 'function', 'integer', 'prob', 'mean', 'case', 'ﬁnd', 'value', 'e', 'ability', 'straightaway', 'rest', 'case', 'try', 'eliminate', 'common', 'factor', 'notice', 'dxy', 'value', 'square', 'euclidean', 'distance', 'driver', 'location', 'corner', 'grid', 'let', 'driver', 'coordinate', 'determined', 'know', 'corner', 'grid', 'y1', 'dlu', 'murthy', 'vivek', 'hence', 'e', 'pliﬁes', 'cid16cid0x1', 'y1', '−', 'sim', 'y1', '−', 'eliminate', 'common', 'factor', 'obtain', 'p', 'e', 'x2', 'y2cid1', 'similarly', 'get', 'qcid48', 'rcid48', 'scid48', 'u', 'finally', 'gcdp', 'qcid48', 'rcid48', 'scid48', 'u', 'give', 'value', 'coordinate', 'grid', 'know', 'system', 'initialization', 'time', 'hence', 'common', 'factor', 'coordinate', 'compute', 'oﬄine', 'step', 'rider', 'value', 'cid102dcid48', 'use', 'value', 'obtain', 'e', 'cid102dcid48', 'ﬁnally', 'use', 'e', 'hence', 'dll', 'dlu', 'dru', 'obtain', 'remark', 'case', 'obtain', 'negative', 'value', 'imply', 'recovery', 'e', 'error', 'retrieve', 'driver', 'location', 'know', 'correlation', 'distance', 'corner', 'grid', 'distance', 'give', 'random', 'order', 'addition', 'search', 'grid', 'adjacent', 'rider', 'grid', 'potential', 'grid', 'driver', 'use', 'distance', 'value', 'dll', 'dlu', 'dru', 'radius', 'respective', 'grid', 'corner', 'center', 'circle', 'rider', 'obtain', 'point', 'grid', 'circle', 'intersect', 'point', 'respective', 'grid', 'represent', 'equiprobable', 'location', 'driver', 'dp', 'figure', 'give', 'pictorial', 'view', 'attack', 'adversary', 'rider', 'r', 'locate', 'grid', 'g', 'driver', 'locate', 'grid', 'g4', 'distance', 'δ', 'r', 'probable', 'driver', 'location', 'adjacent', 'grid', 'g1', 'show', 'small', 'blue', 'dot', 'grid', 'use', 'distance', 'r', 'namely', 'r', 'draw', 'ridercircle', 'δ', 'long', 'driver', 'report', 'correct', 'coordinate', 'guarantee', 'least', 'equiprobable', 'driver', 'location', 'lie', 'circumference', 'ridercircle', 'location', 'obtain', 'rider', 'make', 'use', 'near', 'road', 'location', 'assume', 'driver', 'locate', 'motorable', 'road', 'adversary', 'output', 'location', 'close', 'near', 'road', 'remark', 'mitigation', 'solution', 'apply', 'pride', 'protocol', 'attack', 'still', 'applicable', 'basic', 'pride', 'protocol', 'look', 'attack', 'enhanced', 'version', 'mitigation', 'solution', 'apply', 'pride', 'protocol', 'case', 'response', 'ride', 'request', 'driver', 'pick', 'uniform', 'random', 'location', 'circle', 'original', 'driver', 'location', 'harvesting', 'attack', 'pride', 'fig', 'recover', 'driver', 'location', 'show', 'small', 'dot', 'grid', 'g1', 'location', 'send', 'encryption', 'random', 'location', 'sp', 'well', 'encrypt', 'distance', 'random', 'location', 'corner', 'grid', 'note', 'large', 'adverse', 'eﬀect', 'driver', 'selection', 'rider', 'attack', 'describe', 'section', 'applicable', 'change', 'however', 'retrieve', 'location', 'case', 'random', 'location', 'pick', 'driver', 'instead', 'actual', 'location', 'adversary', 'apply', 'attack', 'uncover', 'actual', 'driver', 'location', 'high', 'probability', 'experiment', 'result', 'use', 'implement', 'attack', 'describe', 'section', 'retrieve', 'driver', 'distance', 'attack', 'describe', 'section', 'retrieve', 'driver', 'location', 'implement', 'use', 'near', 'road', 'apis', 'part', 'attack', 'execute', 'commodity', 'laptop', 'ssd', 'amd', 'ryzen', 'processor', 'sagemath', 'python', 'program', 'available', 'experiment', 'detail', 'experiment', 'run', 'grid', 'size', 'superimpose', 'map', 'large', 'city', 'world', 'namely', 'paris', 'size', 'grid', 'comparable', 'report', 'pride', 'paper', 'experiment', 'number', 'driver', 'grid', 'case', 'distribute', 'randomly', 'grid', 'locate', 'motorable', 'area', 'note', 'number', 'driver', 'murthy', 'vivek', 'bearing', 'attack', 'sp', 'encrypt', 'blind', 'driver', 'distance', 'independent', 'map', 'pick', 'random', 'driver', 'location', 'situate', 'motorable', 'road', 'rider', 'location', 'pick', 'random', 'grid', 'map', 'explain', 'section', 'grid', 'adjacent', 'rider', 'grid', 'examine', 'distance', 'driver', 'grid', 'rider', 'make', 'available', 'rider', 'predict', 'result', 'p', 'r', 'value', 'available', 'rider', 'pride', 'protocol', 'p', 'r', 'value', 'bearing', 'attack', 'eﬀect', 'blind', 'encryption', 'distance', 'next', 'adjacent', 'grid', 'driver', 'grid', 'distance', 'driver', 'respective', 'grid', 'corner', 'compute', 'blind', 'use', 'random', 'integer', 'pick', 'range', 'maximum', 'utm', 'northing', 'value', 'represent', 'use', 'bit', 'addition', 'distance', 'value', 'know', 'adversary', 'also', 'blind', 'use', 'random', 'integer', 'blind', 'distance', 'make', 'available', 'adversary', 'rider', 'exactly', 'mimic', 'behaviour', 'enhance', 'pride', 'protocol', 'finally', 'run', 'attack', 'describe', 'section', 'retrieve', 'distance', 'follow', 'retrieve', 'driver', 'location', 'remark', 'claim', 'security', 'pride', 'protocol', 'rely', 'hard', 'ness', 'obtain', 'blind', 'parameter', 'give', 'blind', 'value', 'show', 'attack', 'adversary', 'recover', 'blind', 'parameter', 'high', 'probability', 'remark', 'experiment', 'use', 'search', 'radius', 'tack', 'methodology', 'easily', 'extend', 'high', 'value', 'search', 'radius', 'order', 'grid', 'traversal', 'know', 'new', 'attack', 'compute', 'equiprobable', 'location', 'possible', 'grid', 'continue', 'driver', 'retrieval', 'attack', 'describe', 'section', 'result', 'result', 'experiment', 'tabulate', 'table', 'pride', 'paper', 'use', '×', 'grid', 'city', 'mention', 'maximum', 'driver', 'experiment', 'translate', 'driver', 'grid', 'average', 'much', 'large', 'high', 'density', 'area', 'city', 'run', 'experiment', 'driver', 'grid', 'take', 'less', 'second', 'recover', 'location', 'driver', 'order', 'retrieve', 'distance', 'ﬁrst', 'recover', 'blind', 'integer', 'e', 'r', 'describe', 'section', 'show', 'table', 'retrieve', 'least', 'distance', 'successfully', 'average', 'run', 'experiment', 'driver', 'count', 'city', 'unsuccessful', 'case', 'ﬁnd', 'value', 'blind', 'value', 'e', 'retrieve', 'multiple', 'actual', 'value', 'e', 'report', 'failure', 'next', 'use', 'successfully', 'retrieve', 'distance', 'obtain', 'precise', 'driver', 'location', 'use', 'attack', 'describe', 'section', 'see', 'part', 'correctly', 'retrieve', 'close', 'driver', 'location', 'hence', 'overall', 'driver', 'driver', 'location', 'harvesting', 'attack', 'pride', 'location', 'harvesting', 'retrieve', 'least', 'driver', 'participate', 'enhance', 'pride', 'protocol', 'table', 'percentage', 'driver', 'location', 'recover', 'multiple', 'city', 'number', 'participate', 'driver', 'grid', 'age', 'driver', 'coordinate', 'correctly', 'recover', 'relate', 'work', 'large', 'body', 'work', 'privacypreserve', 'rh', 'consider', 'preserv', 'e', 'privacy', 'driver', 'rider', 'oride', 'privateride', 'propose', 'pham', 'early', 'work', 'aim', 'preserve', 'rider', 'vacy', 'sp', 'driver', 'privateride', 'make', 'use', 'cloaking', 'region', 'maintain', 'privacy', 'oride', 'scheme', 'base', 'encrypt', 'driver', 'rider', 'location', 'make', 'use', 'homomorphic', 'property', 'select', 'near', 'driver', 'propose', 'attack', 'aim', 'determine', 'loca', 'tion', 'driver', 'participate', 'oride', 'protocol', 'attack', 'adversary', 'rider', 'reveal', 'location', 'driver', 'respond', 'single', 'ride', 'request', 'provide', 'countermeasure', 'thwart', 'attack', 'preserve', 'suﬃcient', 'anonymity', 'propose', 'attack', 'use', 'triangula', 'tion', 'collude', 'adversary', 'obtain', 'location', 'driver', 'participate', 'propose', 'privacypreserving', 'ridematche', 'service', 'also', 'name', 'pride', 'protocol', 'involve', 'use', 'noncollude', 'server', 'sp', 'cp', 'thirdparty', 'crypto', 'server', 'use', 'road', 'network', 'embed', 'rne', 'road', 'network', 'transform', 'high', 'dimension', 'space', 'enable', 'eﬃcient', 'distance', 'computation', 'network', 'entity', 'advantage', 'scheme', 'use', 'noncollude', 'server', 'incur', 'interserver', 'communication', 'cost', 'propose', 'protocol', 'murthy', 'also', 'use', 'rne', 'use', 'modiﬁed', 'version', 'paillier', 'encryption', 'scheme', 'preserve', 'privacy', 'participate', 'entity', 'vivek', 'demonstrate', 'attack', 'protocol', 'show', 'rider', 'driver', 'learn', 'coordinate', 'participate', 'rider', 'trace', 'privacypreserve', 'dy', 'namic', 'spatial', 'query', 'rhs', 'scheme', 'propose', 'use', 'quadtree', 'structure', 'provide', 'higheﬃciency', 'term', 'complexity', 'overhead', 'demonstrate', 'attack', 'trace', 'protocol', 'sp', 'identify', 'exact', 'location', 'rider', 'driver', 'propose', 'protocol', 'also', 'use', 'rne', 'eﬃciently', 'compute', 'shortest', 'distance', 'scheme', 'make', 'use', 'propertypreserve', 'hash', 'function', 'sp', 'compute', 'rider', 'driver', 'distance', 'also', 'pick', 'near', 'driver', 'way', 'eliminate', 'need', 'auxiliary', 'crypto', 'server', 'work', 'list', 'early', 'pick', 'near', 'driver', 'fulﬁl', 'ride', 'request', 'pride', 'propose', 'match', 'near', 'driver', 'consider', 'global', 'matching', 'strategy', 'aim', 'reduce', 'empty', 'distance', 'trav', 'elle', 'driver', 'pick', 'rider', 'give', 'attack', 'use', 'triangulation', 'recover', 'location', 'participate', 'driver', 'addition', 'use', 'number', 'collude', 'adversary', 'show', 'recover', 'location', 'driver', 'participate', 'variant', 'oride', 'protocol', 'use', 'mitigation', 'solution', 'conclusion', 'paper', 'present', 'attack', 'enhance', 'pride', 'protocol', 'privacy', 'preserve', 'rh', 'show', 'honestbutcurious', 'adversary', 'rider', 'termine', 'coordinate', 'driver', 'respond', 'rider', 'ride', 'request', 'pride', 'protocol', 'section', 'see', 'location', 'driver', 'participate', 'basic', 'pride', 'protocol', 'recover', 'adversary', 'rider', 'protocol', 'rider', 'choose', 'optimum', 'driver', 'give', 'plaintext', 'distance', 'driver', 'fact', 'exploit', 'adversary', 'alternatively', 'sp', 'select', 'optimum', 'driver', 'homomorphically', 'sort', 'search', 'highdepth', 'circuit', 'eﬃcient', 'perform', 'operation', 'use', 'scheme', 'however', 'fhe', 'scheme', 'explore', 'evaluate', 'suitability', 'practical', 'rhs', 'solution', 'enhance', 'pride', 'protocol', 'need', 'perform', 'comparison', 'order', 'preserve', 'privacy', 'value', 'blind', 'however', 'order', 'need', 'preserve', 'blind', 'value', 'comparand', 'lead', 'attack', 'secure', 'orderpreserving', 'technique', 'need', 'explore', 'however', 'show', 'careful', 'analysis', 'need', 'otherwise', 'lead', 'attack', 'summary', 'show', 'protocol', 'seem', 'secure', 'theory', 'thorough', 'analysis', 'otherwise', 'expose', 'severe', 'vulner', 'ability', 'security', 'hole', 'demonstrate', 'attack', 'paper', 'driver', 'location', 'harvesting', 'attack', 'pride', 'acknowledgement', 'thank', 'anonymous', 'reviewer', 'invaluable', 'comment', 'suggestion', 'help', 'improve', 'manuscript', 'work', 'partly', 'fund', 'inspire', 'faculty', 'award', 'govt', 'infosy', 'foundation', 'career', 'development', 'chair', 'professorship', 'grant', 'reference', 'fan', 'j', 'vercauteren', 'somewhat', 'practical', 'fully', 'homomorphic', 'encryption', 'cryp', 'tology', 'eprint', 'archive', 'map', 'platform', 'documentationroadsintro', 'retrieve', 'map', 'platform', 'client', 'library', 'map', 'web', 'service', 'retrieve', 'pride', 'privacypreserve', 'online', 'ride', 'hail', 'match', 'system', 'prediction', 'ieee', 'transaction', 'vehicular', 'tech', 'nology', 'revisit', 'driver', 'privacy', 'oride', 'lected', 'area', 'sac', '28th', 'international', 'conference', 'univer', 'sity', 'virtual', 'event', 'lecture', 'note', 'computer', 'science', 'springer', 'cryptanalysis', 'privacypreserving', 'ridehaile', 'service', 'trace', 'küster', 'r', 'preneel', 'ed', 'progress', 'cryp', 'tology', 'indocrypt', '22nd', 'international', 'conference', 'cryptology', 'proceeding', 'lecture', 'note', 'computer', 'science', 'vol', 'springer', 'pride', 'privacypreserving', 'ride', 'match', 'road', 'network', 'online', 'ridehaile', 'service', 'ieee', 'tran', 'information', 'forensic', 'security', 'mordor', 'intelligence', 'ridehaile', 'market', 'growth', 'trend', 'covid19', 'pact', 'forecast', 'industryreportsridehailingmarket', 'retrieve', 'cryptanalysis', 'protocol', 'eﬃcient', 'sort', 'encrypt', 'datum', 'albrecht', 'code', '17th', 'ternational', 'conference', 'imacc', 'lecture', 'note', 'computer', 'science', 'vol', 'springer', 'passive', 'triangulation', 'attack', 'oride', 'nabeel', 'privacy', 'preserve', 'context', 'aware', 'publish', 'subscribe', 'system', 'r', 'ed', 'net', 'work', 'system', 'security', '7th', 'international', 'conference', 'nss', 'proceeding', 'lecture', 'note', 'computer', 'science', 'vol', 'springer', 'pham', 'dacosta', 'endignoux', 'privacypreserving', 'yet', 'accountable', 'ridehaile', 'service', 'ristenpart', 'ed', '26th', 'usenix', 'security', 'symposium', 'usenix', 'curity', 'murthy', 'pham', 'dacosta', 'hajar', 'tramèr', 'privacyenhance', 'ridehaile', 'service', 'popet', 'shahabi', 'kolahdouzan', 'road', 'network', 'embed', 'technique', 'knear', 'neighbor', 'search', 'move', 'object', 'database', 'voisard', 'ed', 'acmgi', 'proceeding', 'tenth', 'acm', 'international', 'symposium', 'advance', 'geographic', 'information', 'acm', 'network', 'machine', 'learning', 'approach', 'precipitation', 'nowcaste', 'pro', 'ceeding', '28th', 'international', 'conference', 'neural', 'information', 'processing', 'system', 'volume', 'p', 'nips’15', 'mit', 'press', 'stein', 'sage', 'software', 'version', 'sage', 'development', 'team', 'attack', 'privacypreserving', 'publishsubscribe', 'system', 'ride', 'hail', 'service', 'paterson', 'code', '18th', 'international', 'conference', 'imacc', 'virtual', 'event', 'proceeding', 'lecture', 'note', 'computer', 'science', 'vol', 'pp', 'springer', 'eﬃcient', 'privacy', 'preserve', 'dynamic', 'spatial', 'query', 'scheme', 'ridehaile', 'service', 'ieee', 'transac', 'tion', 'vehicular', 'technology', 'tem', 'coordinatesystem', 'retrieve', 'sy', 'contributor', 'universal', 'transverse', 'mercator', 'coordinate', 'wikipedia', 'contributor', 'integer', 'coprimeinteger', 'retrieve', 'xie', 'privacypreserving', 'online', 'ridehaile', 'system', 'involve', 'third', 'trust', 'server', 'ieee', 'transaction', 'information', 'forensic', 'security', 'h', 'lpride', 'lightweight', 'privacy', 'preserve', 'ride', 'match', 'road', 'network', 'online', 'ride', 'hail', 'system', 'ieee', 'vehicular', 'technology']"
