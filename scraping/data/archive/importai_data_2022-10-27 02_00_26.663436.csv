title,url,date,text,cleaning,tokens
10/25/2022 - Import AI 307: Copilot lawsuit; Stability raises $101m; US v China CHIPLOMACY - 0,http://eepurl.com/ib8Fv9,2022-10-25,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 The single best thing to read about the China chip controls: …What CHIPLOMACY looks like… Here's a great writeup by Greg Allen about the impact of the USA's anti-China semiconductor controls. The tl;dr is this is a powerful and overlapping set of policy actions which, in combination, are designed to destroy China's burgeoning chip industry. These sanctions are a huge deal and the Chinese government will likely be responding - be prepared.     Read more: Choking Off China’s Access to the Future of AI (CSIS).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. The single best thing to read about the China chip controls: …What CHIPLOMACY looks like… Here's a great writeup by Greg Allen about the impact of the USA's anti-China semiconductor controls. The tl;dr is this is a powerful and overlapping set of policy actions which, in combination, are designed to destroy China's burgeoning chip industry. These sanctions are a huge deal and the Chinese government will likely be responding - be prepared. Read more: Choking Off China’s Access to the Future of AI (CSIS).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'subscribe', 'single', 'good', 'thing', 'read', 'chip', 'control', 'chiplomacy', 'look', 'great', 'writeup', 'impact', 'semiconductor', 'control', 'tldr', 'powerful', 'overlap', 'set', 'policy', 'action', 'combination', 'design', 'destroy', 'china', 'burgeon', 'chip', 'industry', 'sanction', 'huge', 'deal', 'chinese', 'government', 'likely', 'respond', 'prepare', 'read', 'choke', 'access', 'future', 'csis']"
10/25/2022 - Import AI 307: Copilot lawsuit; Stability raises $101m; US v China CHIPLOMACY - 1,http://eepurl.com/ib8Fv9,2022-10-25,"#################################################### Gray area code models: Lawyer-programmer mulls anti-Copilot lawsuit: …What one person calls fair use another person calls infringement… Matthew Butterick, a lawyer and programmer, has reactivated his California bar membership so he can investigate ""a potential lawsuit against GitHub Copilot for violating its legal duties to open-source authors and end users"". The gist of the complaint is that GitHub was trained on tons of public GitHub repos, yet the code GitHub spits out doesn't have any attributions to those repos, and therefore you need to argue Copilot is fair use because it is sufficiently transformative - but that's not established.  What's wrong with Copilot? ""Though some courts have con­sid­ered related issues, there is no US case squarely resolv­ing the fair-use ram­i­fi­ca­tions of AI train­ing,"" Butterick writes. Since there is no legal precedent here, it's not clear you can argue that Copilot falls under fair use, one way or the other.    Additionally, Copilot can sometimes regurgitate code which is a copy of identifiable reporistories, but both Microsoft (and their underlying AI partner, OpenAI) offload responsibility here to the user of the Copilot suggestion rather than themselves. ""As a side effect of Copi­lot’s design, infor­ma­tion about the code’s ori­gin—author, license, etc.—is stripped away. How can Copi­lot users com­ply with the license if they don’t even know it exists?"" Copilot is climate change for coders: Butterick notes that Copilot may, as it becomes more successful, ""inhibit"" or ""remove any incentive"" for programmers to spend time in open source communities. ""Over time, this process will starve these com­mu­ni­ties. User atten­tion and engage­ment will be shifted into the walled gar­den of Copi­lot and away from the open-source projects them­selves—away from their source repos, their issue track­ers, their mail­ing lists, their dis­cus­sion boards. This shift in energy will be a painful, per­ma­nent loss to open source,"" he writes. ""The legal­ity of Copi­lot must be tested before the dam­age to open source becomes irrepara­ble. That’s why I’m suit­ing up."" Why this matters: These generative models can do amazing and beguiling things - and people are betting they're the future (see, elsewhere in this issue, Common Sense Machines, and the Stable Diffusion fundraise). But they also do pose significant issues with regard to the 'digital commons' from which we all depend - I worry that systems like Copilot can both starve the commons (destroy open source incentives) and also poison them (loop Copilot-generated code back into the commons, which could theoretically lower the aggregate quality of what is available.)     Read more: Maybe you don’t mind if GitHub Copi­lot used your open-source code with­out ask­ing. But how will you feel if Copi­lot erases your open-source com­mu­nity? (GitHub Copilot investigation).","#################################################### Gray area code models: Lawyer-programmer mulls anti-Copilot lawsuit: …What one person calls fair use another person calls infringement… Matthew Butterick, a lawyer and programmer, has reactivated his California bar membership so he can investigate ""a potential lawsuit against GitHub Copilot for violating its legal duties to open-source authors and end users"". The gist of the complaint is that GitHub was trained on tons of public GitHub repos, yet the code GitHub spits out doesn't have any attributions to those repos, and therefore you need to argue Copilot is fair use because it is sufficiently transformative - but that's not established. What's wrong with Copilot? ""Though some courts have con­sid­ered related issues, there is no US case squarely resolv­ing the fair-use ram­i­fi­ca­tions of AI train­ing,"" Butterick writes. Since there is no legal precedent here, it's not clear you can argue that Copilot falls under fair use, one way or the other. Additionally, Copilot can sometimes regurgitate code which is a copy of identifiable reporistories, but both Microsoft (and their underlying AI partner, OpenAI) offload responsibility here to the user of the Copilot suggestion rather than themselves. ""As a side effect of Copi­lot’s design, infor­ma­tion about the code’s ori­gin—author, license, etc.—is stripped away. How can Copi­lot users com­ply with the license if they don’t even know it exists?"" Copilot is climate change for coders: Butterick notes that Copilot may, as it becomes more successful, ""inhibit"" or ""remove any incentive"" for programmers to spend time in open source communities. ""Over time, this process will starve these com­mu­ni­ties. User atten­tion and engage­ment will be shifted into the walled gar­den of Copi­lot and away from the open-source projects them­selves—away from their source repos, their issue track­ers, their mail­ing lists, their dis­cus­sion boards. This shift in energy will be a painful, per­ma­nent loss to open source,"" he writes. ""The legal­ity of Copi­lot must be tested before the dam­age to open source becomes irrepara­ble. That’s why I’m suit­ing up."" Why this matters: These generative models can do amazing and beguiling things - and people are betting they're the future (see, elsewhere in this issue, Common Sense Machines, and the Stable Diffusion fundraise). But they also do pose significant issues with regard to the 'digital commons' from which we all depend - I worry that systems like Copilot can both starve the commons (destroy open source incentives) and also poison them (loop Copilot-generated code back into the commons, which could theoretically lower the aggregate quality of what is available.) Read more: Maybe you don’t mind if GitHub Copi­lot used your open-source code with­out ask­ing. But how will you feel if Copi­lot erases your open-source com­mu­nity? (GitHub Copilot investigation).","['gray', 'area', 'code', 'model', 'lawyerprogrammer', 'mull', 'anticopilot', 'lawsuit', 'person', 'call', 'fair', 'use', 'person', 'call', 'infringement', 'matthew', 'butterick', 'lawyer', 'programmer', 'reactivate', 'bar', 'membership', 'investigate', 'potential', 'lawsuit', 'copilot', 'violate', 'legal', 'duty', 'opensource', 'author', 'end', 'user', 'gist', 'complaint', 'train', 'ton', 'public', 'github', 'repos', 'yet', 'code', 'spit', 'attribution', 'repos', 'therefore', 'need', 'argue', 'copilot', 'fair', 'use', 'sufficiently', 'transformative', 'establish', 'wrong', 'copilot', 'court', 'con\xadsid\xadere', 'related', 'issue', 'case', 'squarely', 'resolv\xade', 'ram\xadi\xadfi\xadca\xadtion', 'train\xading', 'butterick', 'write', 'legal', 'precedent', 'clear', 'argue', 'copilot', 'fall', 'fair', 'use', 'way', 'additionally', 'copilot', 'sometimes', 'regurgitate', 'code', 'copy', 'identifiable', 'reporistorie', 'underlying', 'partner', 'offload', 'responsibility', 'user', 'copilot', 'suggestion', 'rather', 'side', 'effect', 'design', 'code', 'author', 'license', 'strip', 'away', 'copi\xadlot', 'user', 'com\xadply', 'license', 'even', 'know', 'exist', 'copilot', 'climate', 'change', 'coder', 'butterick', 'note', 'copilot', 'become', 'successful', 'inhibit', 'remove', 'incentive', 'programmer', 'spend', 'time', 'open', 'source', 'community', 'time', 'process', 'starve', 'com\xadmu\xadni\xadtie', 'user', 'atten\xadtion', 'engage\xadment', 'shift', 'walled', 'copi\xadlot', 'away', 'opensource', 'project', 'them\xadselve', 'away', 'source', 'repos', 'issue', 'track\xader', 'mail\xading', 'list', 'dis\xadcus\xadsion', 'board', 'shift', 'energy', 'painful', 'loss', 'open', 'source', 'write', 'legal\xadity', 'copi\xadlot', 'test', 'open', 'source', 'become', 'irrepara\xadble', '’', 'suit\xade', 'matter', 'generative', 'model', 'amazing', 'beguile', 'thing', 'people', 'bet', 'future', 'see', 'elsewhere', 'issue', 'common', 'sense', 'machine', 'stable', 'diffusion', 'fundraise', 'also', 'pose', 'significant', 'issue', 'regard', 'digital', 'common', 'depend', 'worry', 'system', 'copilot', 'starve', 'common', 'destroy', 'open', 'source', 'incentive', 'also', 'poison', 'loop', 'copilotgenerate', 'code', 'back', 'common', 'theoretically', 'lower', 'aggregate', 'quality', 'available', 'read', 'maybe', 'mind', 'use', 'opensource', 'code', 'feel', 'copi\xadlot', 'erase', 'opensource', 'com\xadmu\xadnity', 'copilot', 'investigation']"
10/25/2022 - Import AI 307: Copilot lawsuit; Stability raises $101m; US v China CHIPLOMACY - 2,http://eepurl.com/ib8Fv9,2022-10-25,"#################################################### Common Sense Machines wants to make a 3D, temporal DALL-E:
…CSM-1 is a neural network pretending to be a simulator and a sign of things to come… New AI startup Common Sense Machines has built CommonSim-1 (CSM1), a ""neural simulation engine"" which people can use to generate arbitrary 3D scenes and simulations.     ""CommonSim-1 is operated with images, language, and action. A user (machine or human) shows or describes what they want to simulate and then controls the kinds of outputs they want to measure and observe,""  they write. ""At the heart of CommonSim-1 is a foundation model of the 3D world that is trained on a large-scale, growing dataset of diverse human (and non-human) experience across a wide range of tasks. We combine publicly available data, our own internal datasets, and task-specific data provided by our partners."" What can CommonSim-1 do? CSM1 can build high-resolution videos from as little as a single frame of video. ""Since this model imagines the future, one can use its imagination (1) as training data for 3D generation and perception and (2) as part of another system’s predictive model,"" they write. ""With a mesh or NeRF generated by CommonSim-1, one can type natural-language descriptions into a text prompt and generate unlimited new hybrid scenes."" Why this matters - worlds within worlds: CSM-1 is a miniature world - it's literally a world model. It combines text and image and video and provides another approach to monetizing AI; helping to take costs out of 3D design and simulation via leveraging a (presumably) gigantic model. It's also a sign of things to come - all models are going to tend towards incorporating all modalities and unfolding over time; CSM-1 is a taste of things to come.     Read more: Generating 3D Worlds with CommonSim-1 (Common Sense Machines, blog).","#################################################### Common Sense Machines wants to make a 3D, temporal DALL-E: …CSM-1 is a neural network pretending to be a simulator and a sign of things to come… New AI startup Common Sense Machines has built CommonSim-1 (CSM1), a ""neural simulation engine"" which people can use to generate arbitrary 3D scenes and simulations. ""CommonSim-1 is operated with images, language, and action. A user (machine or human) shows or describes what they want to simulate and then controls the kinds of outputs they want to measure and observe,"" they write. ""At the heart of CommonSim-1 is a foundation model of the 3D world that is trained on a large-scale, growing dataset of diverse human (and non-human) experience across a wide range of tasks. We combine publicly available data, our own internal datasets, and task-specific data provided by our partners."" What can CommonSim-1 do? CSM1 can build high-resolution videos from as little as a single frame of video. ""Since this model imagines the future, one can use its imagination (1) as training data for 3D generation and perception and (2) as part of another system’s predictive model,"" they write. ""With a mesh or NeRF generated by CommonSim-1, one can type natural-language descriptions into a text prompt and generate unlimited new hybrid scenes."" Why this matters - worlds within worlds: CSM-1 is a miniature world - it's literally a world model. It combines text and image and video and provides another approach to monetizing AI; helping to take costs out of 3D design and simulation via leveraging a (presumably) gigantic model. It's also a sign of things to come - all models are going to tend towards incorporating all modalities and unfolding over time; CSM-1 is a taste of things to come. Read more: Generating 3D Worlds with CommonSim-1 (Common Sense Machines, blog).","['common', 'sense', 'machine', 'want', 'make', 'temporal', 'dalle', 'csm1', 'neural', 'network', 'pretend', 'simulator', 'sign', 'thing', 'come', 'startup', 'common', 'sense', 'machine', 'build', 'commonsim1', 'neural', 'simulation', 'engine', 'people', 'use', 'generate', 'arbitrary', 'scene', 'simulation', 'commonsim1', 'operate', 'image', 'language', 'action', 'user', 'machine', 'human', 'show', 'describe', 'want', 'simulate', 'control', 'kind', 'output', 'want', 'measure', 'observe', 'write', 'heart', 'commonsim1', 'foundation', 'model', 'world', 'train', 'largescale', 'grow', 'dataset', 'diverse', 'human', 'nonhuman', 'experience', 'wide', 'range', 'task', 'combine', 'publicly', 'available', 'datum', 'internal', 'dataset', 'taskspecific', 'datum', 'provide', 'partner', 'commonsim1', 'csm1', 'build', 'highresolution', 'video', 'little', 'single', 'frame', 'video', 'model', 'imagine', 'future', 'use', 'imagination', 'training', 'datum', 'generation', 'perception', 'part', 'system', 'predictive', 'model', 'write', 'mesh', 'nerf', 'generate', 'commonsim1', 'type', 'naturallanguage', 'description', 'text', 'prompt', 'generate', 'unlimited', 'new', 'hybrid', 'scene', 'matter', 'world', 'world', 'miniature', 'world', 'literally', 'world', 'model', 'combine', 'text', 'image', 'video', 'provide', 'approach', 'monetize', 'help', 'take', 'cost', 'design', 'simulation', 'leverage', 'presumably', 'gigantic', 'model', 'also', 'sign', 'thing', 'come', 'model', 'go', 'tend', 'incorporate', 'modality', 'unfold', 'time', 'csm1', 'taste', 'thing', 'come', 'read', 'generate', 'world', 'commonsim1', 'common', 'sense', 'machine', 'blog']"
10/25/2022 - Import AI 307: Copilot lawsuit; Stability raises $101m; US v China CHIPLOMACY - 3,http://eepurl.com/ib8Fv9,2022-10-25,"#################################################### Open access image generation raises $101 million:
…That's a whole lot of capital for a company commoditizing itself… Stability.ai, the company behind the free 'Stable Diffusion' image model, has raised $101 million in funding. The round was led by Coatue, Lightspeed Venture Partners, and O'Shaughnessy Ventures LLC. For those not familiar, Stability.ai built Stable Diffusion, a widely used image generation model which, unlike proprietary counterparts Imagen and DALL-E, has had its weights released onto the internet, making it available to tinker with for free.     ""Since launching, Stable Diffusion has been downloaded and licensed by more than 200,000 developers globally,"" the company writes in a press release. A funny aside: I wrote this section of the newsletter while sat on a couch in the Exploratorium watching as people ate short-rib sliders and drank glasses of wine, awaiting a presentation from Stable Diffusion about their raise.  Why this matters: There's a vigorous debate in the AI community about how AI models should proliferate (and there's some indication that this debate seeped through to politicians; see Eshoo's letter to the US National Security Advisor criticizing the release of model weights for Stability.ai (Import AI 304)), and Stability.ai represents one extreme end of the spectrum - proliferate the weights, then build a range of as-a-service businesses on top. How this debate unfolds is going to have a major influence over the AI development landscape, so it's worth paying attention to how Stability.ai navigates this space.     Read more: Stability AI Announces $101 Million in Funding for Open-Source Artificial Intelligence (PR Newswire).","#################################################### Open access image generation raises $101 million: …That's a whole lot of capital for a company commoditizing itself… Stability.ai, the company behind the free 'Stable Diffusion' image model, has raised $101 million in funding. The round was led by Coatue, Lightspeed Venture Partners, and O'Shaughnessy Ventures LLC. For those not familiar, Stability.ai built Stable Diffusion, a widely used image generation model which, unlike proprietary counterparts Imagen and DALL-E, has had its weights released onto the internet, making it available to tinker with for free. ""Since launching, Stable Diffusion has been downloaded and licensed by more than 200,000 developers globally,"" the company writes in a press release. A funny aside: I wrote this section of the newsletter while sat on a couch in the Exploratorium watching as people ate short-rib sliders and drank glasses of wine, awaiting a presentation from Stable Diffusion about their raise. Why this matters: There's a vigorous debate in the AI community about how AI models should proliferate (and there's some indication that this debate seeped through to politicians; see Eshoo's letter to the US National Security Advisor criticizing the release of model weights for Stability.ai (Import AI 304)), and Stability.ai represents one extreme end of the spectrum - proliferate the weights, then build a range of as-a-service businesses on top. How this debate unfolds is going to have a major influence over the AI development landscape, so it's worth paying attention to how Stability.ai navigates this space. Read more: Stability AI Announces $101 Million in Funding for Open-Source Artificial Intelligence (PR Newswire).","['open', 'access', 'image', 'generation', 'raise', 'whole', 'lot', 'capital', 'company', 'commoditize', 'stabilityai', 'company', 'free', 'stable', 'diffusion', 'image', 'model', 'raise', 'fund', 'round', 'lead', 'venture', 'partner', 'oshaughnessy', 'venture', 'llc', 'familiar', 'stabilityai', 'build', 'stable', 'diffusion', 'widely', 'use', 'image', 'generation', 'model', 'proprietary', 'counterpart', 'imagen', 'dalle', 'weight', 'release', 'internet', 'make', 'available', 'tinker', 'free', 'launch', 'stable', 'diffusion', 'download', 'license', 'developer', 'globally', 'company', 'write', 'press', 'release', 'funny', 'aside', 'write', 'section', 'newsletter', 'sit', 'couch', 'exploratorium', 'watch', 'people', 'eat', 'shortrib', 'slider', 'drank', 'glass', 'wine', 'await', 'presentation', 'stable', 'diffusion', 'raise', 'matter', 'vigorous', 'debate', 'community', 'model', 'proliferate', 'indication', 'debate', 'seep', 'politician', 'see', 'eshoos', 'letter', 'security', 'advisor', 'criticize', 'release', 'model', 'weight', 'stabilityai', 'import', 'ai', 'represent', 'extreme', 'end', 'spectrum', 'proliferate', 'weight', 'build', 'range', 'asaservice', 'business', 'top', 'debate', 'unfold', 'go', 'major', 'influence', 'development', 'landscape', 'worth', 'pay', 'attention', 'stabilityai', 'navigate', 'space', 'read', 'stability', 'ai', 'announce', 'funding', 'opensource', 'artificial', 'intelligence', 'newswire']"
10/25/2022 - Import AI 307: Copilot lawsuit; Stability raises $101m; US v China CHIPLOMACY - 4,http://eepurl.com/ib8Fv9,2022-10-25,"#################################################### First, image models, now language models get commoditized: …Carper plans to release a pretty good RLHF language model… CarperAI, an AI startup slash open source research collective slash cypherpunk-AI-guerilla group, plans to release a ""chinchilla-optimal large language model explicitly trained to follow human instructions"". This is a big deal! Up to now, publicly released language models (e.g, OPT, BLOOM, GLM-130) are either not trained on the optimal amount of data, nor are they calibrated via human feedback to be better at following instructions. Instead, these models mostly reside inside proprietary labs (e.g, Anthropic, OpenAI). (Carper also recently released code to make it easy for anyone to train LMs - up to 20B parameters - from human feedback (Import AI #305)). Who they're partnering with: CarperAI are partnering with Scale, Humanloop, HuggingFace, Multi, EleutherAI, and StabilityAI to train and deploy the model. This is a neat illustration of the shifting politics and allegiances of the AI ecosystem, and feels like a representation of a 'second wave' of labs, following the 'first wave' epitomized by OpenAI and DeepMind. Why this matters: Models trained with reinforcement learning from human feedback (RLHF) are really good. They're way, way better than non-RLHF models for most tasks. Also, models trained on more data via the Chinchilla insight are also way more capable than those trained on less data. By combining these two things, CarperAI is likely to release far and away the most capable language model onto the open internet. This has upsides - researchers will get to play with a decent RLHF model in an unrestricted way - as well as downsides - RLHF models are the proverbial machine gun to a pistol (non-RLHF models), so potential misuses are magnified as well.     Read more: CarperAI, an EleutherAI lab, announces plans for the first open-source “instruction-tuned” language model (CarperAI).","#################################################### First, image models, now language models get commoditized: …Carper plans to release a pretty good RLHF language model… CarperAI, an AI startup slash open source research collective slash cypherpunk-AI-guerilla group, plans to release a ""chinchilla-optimal large language model explicitly trained to follow human instructions"". This is a big deal! Up to now, publicly released language models (e.g, OPT, BLOOM, GLM-130) are either not trained on the optimal amount of data, nor are they calibrated via human feedback to be better at following instructions. Instead, these models mostly reside inside proprietary labs (e.g, Anthropic, OpenAI). (Carper also recently released code to make it easy for anyone to train LMs - up to 20B parameters - from human feedback (Import AI #305)). Who they're partnering with: CarperAI are partnering with Scale, Humanloop, HuggingFace, Multi, EleutherAI, and StabilityAI to train and deploy the model. This is a neat illustration of the shifting politics and allegiances of the AI ecosystem, and feels like a representation of a 'second wave' of labs, following the 'first wave' epitomized by OpenAI and DeepMind. Why this matters: Models trained with reinforcement learning from human feedback (RLHF) are really good. They're way, way better than non-RLHF models for most tasks. Also, models trained on more data via the Chinchilla insight are also way more capable than those trained on less data. By combining these two things, CarperAI is likely to release far and away the most capable language model onto the open internet. This has upsides - researchers will get to play with a decent RLHF model in an unrestricted way - as well as downsides - RLHF models are the proverbial machine gun to a pistol (non-RLHF models), so potential misuses are magnified as well. Read more: CarperAI, an EleutherAI lab, announces plans for the first open-source “instruction-tuned” language model (CarperAI).","['first', 'image', 'model', 'language', 'model', 'commoditize', 'carper', 'plan', 'release', 'pretty', 'good', 'language', 'ai', 'startup', 'slash', 'open', 'source', 'research', 'collective', 'slash', 'cypherpunkaiguerilla', 'group', 'plan', 'release', 'chinchillaoptimal', 'large', 'language', 'model', 'explicitly', 'train', 'follow', 'human', 'instruction', 'big', 'deal', 'publicly', 'release', 'language', 'model', 'eg', 'either', 'train', 'optimal', 'amount', 'datum', 'calibrate', 'human', 'feedback', 'well', 'follow', 'instruction', 'instead', 'model', 'mostly', 'reside', 'proprietary', 'lab', 'eg', 'anthropic', 'openai', 'carper', 'also', 'recently', 'release', 'code', 'make', 'easy', 'train', 'lm', 'parameter', 'human', 'feedback', 'import', 'ai', 'partner', 'partner', 'scale', 'stabilityai', 'train', 'deploy', 'model', 'neat', 'illustration', 'shift', 'politic', 'allegiance', 'ecosystem', 'feel', 'representation', 'second', 'wave', 'lab', 'follow', 'first', 'wave', 'epitomize', 'openai', 'deepmind', 'matter', 'model', 'train', 'reinforcement', 'learning', 'human', 'feedback', 'rlhf', 'really', 'good', 'way', 'way', 'well', 'nonrlhf', 'model', 'task', 'also', 'model', 'train', 'datum', 'chinchilla', 'insight', 'also', 'way', 'capable', 'train', 'less', 'datum', 'combine', 'thing', 'carperai', 'likely', 'release', 'far', 'away', 'capable', 'language', 'model', 'open', 'internet', 'upside', 'researcher', 'get', 'play', 'decent', 'rlhf', 'model', 'unrestricted', 'way', 'well', 'downside', 'rlhf', 'model', 'proverbial', 'machine', 'gun', 'pistol', 'nonrlhf', 'model', 'potential', 'misuse', 'magnify', 'well', 'read', 'carperai', 'eleutherai', 'lab', 'announce', 'plan', 'first', 'opensource', 'instructiontune', 'language', 'model', 'carperai']"
10/25/2022 - Import AI 307: Copilot lawsuit; Stability raises $101m; US v China CHIPLOMACY - 5,http://eepurl.com/ib8Fv9,2022-10-25,"#################################################### Tech Tales: So, do I have your attention [Meta's wasteland, 2030] You want to survive in this world, you need to keep one eye closed.  That's what my Dad said to me when he handed me the headset.  But dad - these are for both eyes, I said.  I know, and that's how they get you, he said. I know you've just 18 and think you've got it all figured out, but trust me - they've got you figured out more.  So I put the headset on and kept one eye closed. I walked through a vast world full of verdant nature and bustling cities and intriguing quests and characters. After half an hour, I had almost completed my first quest. The last part of the mission was to place a gem I'd mined at the base of a totem. I found the totem and, as I approached, the background music in the game changed. Then after I put the gem in the base, some huge light source overhead turned on and the music swelled to a crescendo.  'No son don't look up,' i could hear my dad, muffled, shouting at me.  But I looked up. Stared into the light on top of the totem and felt something tickle my brain, like the beginning of a joke. My right eye hurt from keeping it shut and I wanted to open it as lights strobed across the eyelid. But I didn't. And then I got a splitting headache and I paused the game and took the headset off.     What the hell was that? I said.     That, my dad said, was your first encounter with an attention harvester.     A what?    How do you think they fund the game? All the utility functions? Services.     I don't know, I guessed ads.     We're way beyond ads, he said. This thing is designed to capture you - if you had both eyes open you'd have spent half an hour talking to that thing, telling it everything about yourself. And the next time you did a quest the world would be even more engaging, and the next time you talked to a totem it'd take an hour, and then the world would get even more interesting. Do you see?    I do, I said.  The next time I went in the game I walked until I was in the multiplayer area and, across a great plain, I saw numerous totems light up and numerous players stop at the base of them, some staying for minutes and others for hours. One player was there for five hours and still there when I left, standing at the base of the totem and looking up into its brilliant light.  Things that inspired this story: Attention harvesting; the logic of the metaverse; computer games; wisdom; MK Ultra. 
Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: So, do I have your attention [Meta's wasteland, 2030] You want to survive in this world, you need to keep one eye closed. That's what my Dad said to me when he handed me the headset. But dad - these are for both eyes, I said. I know, and that's how they get you, he said. I know you've just 18 and think you've got it all figured out, but trust me - they've got you figured out more. So I put the headset on and kept one eye closed. I walked through a vast world full of verdant nature and bustling cities and intriguing quests and characters. After half an hour, I had almost completed my first quest. The last part of the mission was to place a gem I'd mined at the base of a totem. I found the totem and, as I approached, the background music in the game changed. Then after I put the gem in the base, some huge light source overhead turned on and the music swelled to a crescendo. 'No son don't look up,' i could hear my dad, muffled, shouting at me. But I looked up. Stared into the light on top of the totem and felt something tickle my brain, like the beginning of a joke. My right eye hurt from keeping it shut and I wanted to open it as lights strobed across the eyelid. But I didn't. And then I got a splitting headache and I paused the game and took the headset off. What the hell was that? I said. That, my dad said, was your first encounter with an attention harvester. A what? How do you think they fund the game? All the utility functions? Services. I don't know, I guessed ads. We're way beyond ads, he said. This thing is designed to capture you - if you had both eyes open you'd have spent half an hour talking to that thing, telling it everything about yourself. And the next time you did a quest the world would be even more engaging, and the next time you talked to a totem it'd take an hour, and then the world would get even more interesting. Do you see? I do, I said. The next time I went in the game I walked until I was in the multiplayer area and, across a great plain, I saw numerous totems light up and numerous players stop at the base of them, some staying for minutes and others for hours. One player was there for five hours and still there when I left, standing at the base of the totem and looking up into its brilliant light. Things that inspired this story: Attention harvesting; the logic of the metaverse; computer games; wisdom; MK Ultra. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'attention', 'meta', 'wasteland', 'want', 'survive', 'world', 'need', 'keep', 'eye', 'close', 'dad', 'say', 'hand', 'headset', 'dad', 'eye', 'say', 'know', 'get', 'say', 'know', 'think', 'get', 'figure', 'trust', 'get', 'figure', 'put', 'headset', 'keep', 'eye', 'close', 'walk', 'vast', 'world', 'full', 'verdant', 'nature', 'bustling', 'city', 'intriguing', 'quest', 'character', 'hour', 'almost', 'complete', 'first', 'quest', 'last', 'part', 'mission', 'place', 'gem', 'mine', 'base', 'totem', 'find', 'totem', 'approach', 'background', 'music', 'game', 'change', 'put', 'gem', 'base', 'huge', 'light', 'source', 'overhead', 'turn', 'music', 'swell', 'crescendo', 'son', 'look', 'hear', 'dad', 'muffle', 'shout', 'look', 'stare', 'light', 'top', 'totem', 'feel', 'tickle', 'brain', 'beginning', 'joke', 'right', 'eye', 'hurt', 'keep', 'shut', 'want', 'open', 'light', 'strobe', 'eyelid', 'get', 'splitting', 'headache', 'pause', 'game', 'take', 'headset', 'hell', 'say', 'dad', 'say', 'first', 'encounter', 'attention', 'harvester', 'think', 'fund', 'game', 'utility', 'function', 'service', 'know', 'guess', 'ad', 'way', 'ad', 'say', 'thing', 'design', 'capture', 'eye', 'open', 'spend', 'hour', 'talk', 'thing', 'tell', 'next', 'time', 'quest', 'world', 'even', 'engaging', 'next', 'time', 'talk', 'totem', 'take', 'hour', 'world', 'get', 'even', 'interesting', 'see', 'say', 'next', 'time', 'go', 'game', 'walk', 'multiplayer', 'area', 'great', 'plain', 'see', 'numerous', 'totem', 'light', 'numerous', 'player', 'stop', 'base', 'stay', 'minute', 'hour', 'player', 'hour', 'still', 'leave', 'stand', 'base', 'totem', 'look', 'brilliant', 'light', 'thing', 'inspire', 'story', 'attention', 'harvest', 'logic', 'metaverse', 'computer', 'game', 'wisdom', 'mk', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
10/17/2022 - Import AI 306: Language models learn about the world via MuJoCo; Amazon releases a big Q&A dataset; and DeepMind tests out multimodal systems - 0,http://eepurl.com/ibu68z,2022-10-17,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 Amazon releases a Q&A dataset called Mintaka… and baselines show it is difficult! …20,000 Q&A pairs, translated into eight languages… Researchers with Amazon iave released Mintaka, a dataset of 20,000 question-answer pairs written in English, annotated with Wikidata entities, and translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish. The total dataset consists of 180,000 samples, when you include the translated versions. Existing models get 38% on the dataset when testing in English and 31% multilingually. Different types of questions and different types of complexity: Mintaka questions are spread across eight categories (movies, music, sports, books, geography, politics, video games, and history).     The questions have nine types of complexity. These complexity types consist of questions relating to counting something, comparing something, figuring out who was best and worst at something, working out the ordering of something, multi-hop questions that require two or more steps, intersectional questions where the answer must fulfill multiple conditions, questions involving negatives, yes/no questions, and worker-defined 'generic' questions.  How hard is Mintaka? In tests, a good baseline model (a T5 language model fine-tuned as a Q&A model), got 38% on English, and 31% averaged across the other languages. ""Overall, the baselines show that Mintaka is a challenging dataset,"" the authors write. ""None of our baselines explicitly handle all of the complexity types available in Mintaka."" Why this matters: Hard baselines are one of the things that tend to drive progress (and be useful indicators of research advances). It'll be especially interesting to see how Mintaka gets used to evaluate language models paired with retrieval systems.     Prediction: I predict we get a one-shot model that performs at average of 90%+ by December 2023 on this dataset.    Read more: Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering (arXiv).    Get the dataset: Mintaka (Amazon Research, GitHub).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Amazon releases a Q&A dataset called Mintaka… and baselines show it is difficult! …20,000 Q&A pairs, translated into eight languages… Researchers with Amazon iave released Mintaka, a dataset of 20,000 question-answer pairs written in English, annotated with Wikidata entities, and translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish. The total dataset consists of 180,000 samples, when you include the translated versions. Existing models get 38% on the dataset when testing in English and 31% multilingually. Different types of questions and different types of complexity: Mintaka questions are spread across eight categories (movies, music, sports, books, geography, politics, video games, and history). The questions have nine types of complexity. These complexity types consist of questions relating to counting something, comparing something, figuring out who was best and worst at something, working out the ordering of something, multi-hop questions that require two or more steps, intersectional questions where the answer must fulfill multiple conditions, questions involving negatives, yes/no questions, and worker-defined 'generic' questions. How hard is Mintaka? In tests, a good baseline model (a T5 language model fine-tuned as a Q&A model), got 38% on English, and 31% averaged across the other languages. ""Overall, the baselines show that Mintaka is a challenging dataset,"" the authors write. ""None of our baselines explicitly handle all of the complexity types available in Mintaka."" Why this matters: Hard baselines are one of the things that tend to drive progress (and be useful indicators of research advances). It'll be especially interesting to see how Mintaka gets used to evaluate language models paired with retrieval systems. Prediction: I predict we get a one-shot model that performs at average of 90%+ by December 2023 on this dataset. Read more: Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering (arXiv). Get the dataset: Mintaka (Amazon Research, GitHub).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'subscribe', 'amazon', 'release', 'dataset', 'call', 'baseline', 'show', 'difficult', 'qa', 'pair', 'translate', 'language', 'researcher', 'amazon', 'iave', 'release', 'mintaka', 'dataset', 'questionanswer', 'pair', 'write', 'annotate', 'wikidata', 'entity', 'translate', 'arabic', 'french', 'italian', 'japanese', 'portuguese', 'spanish', 'total', 'dataset', 'consist', 'sample', 'include', 'translate', 'version', 'exist', 'model', 'get', 'dataset', 'test', 'multilingually', 'different', 'type', 'question', 'different', 'type', 'complexity', 'mintaka', 'question', 'spread', 'category', 'movie', 'music', 'sport', 'book', 'geography', 'politic', 'video', 'game', 'history', 'question', 'type', 'complexity', 'complexity', 'type', 'consist', 'question', 'relate', 'count', 'compare', 'figure', 'good', 'worst', 'work', 'ordering', 'multihop', 'question', 'require', 'step', 'intersectional', 'question', 'answer', 'fulfill', 'multiple', 'condition', 'question', 'involve', 'negative', 'yesno', 'question', 'workerdefine', 'generic', 'question', 'hard', 'mintaka', 'test', 'good', 'baseline', 'model', 't5', 'language', 'model', 'finetune', 'qa', 'model', 'get', 'average', 'language', 'overall', 'baseline', 'show', 'mintaka', 'challenge', 'dataset', 'author', 'write', 'none', 'baseline', 'explicitly', 'handle', 'complexity', 'type', 'available', 'mintaka', 'matter', 'hard', 'baseline', 'thing', 'tend', 'drive', 'progress', 'useful', 'indicator', 'research', 'advance', 'especially', 'interesting', 'see', 'mintaka', 'use', 'evaluate', 'language', 'model', 'pair', 'system', 'prediction', 'predict', 'get', 'oneshot', 'model', 'perform', 'average', 'dataset', 'read', 'mintaka', 'complex', 'natural', 'multilingual', 'dataset', 'endtoend', 'question', 'answer', 'arxiv', 'get', 'dataset', 'mintaka', 'amazon', 'research', 'github']"
10/17/2022 - Import AI 306: Language models learn about the world via MuJoCo; Amazon releases a big Q&A dataset; and DeepMind tests out multimodal systems - 1,http://eepurl.com/ibu68z,2022-10-17,"#################################################### Your LLM barely understands the physical world; supercharge it by attaching it to MuJoCo: …Training language models to use tools means they can have world knowledge… Google researchers have found out a way to make language models way better at reasoning about the physical world: wire them up so they can port questions into physics simulators then use the results of those simulators to answer a question.     This technique, which they call 'Mind's Eye', works amazingly well, and they robustly show this across both GPT-3 and PALM language models:  How they test for reasoning: To evaluate physical reasoning, the researchers built UTOPIA, a dataset containing 39 sub-tasks covering six common scenes that involve understanding basic principles of physics (e.g, conservation of momentum in elastic collisions). The UTOPIA dataset comes in the form of natural language questions and answers. ""UTOPIA deliberately describes the questions in relative relations (e.g., greater than) instead of absolute numbers (e.g., 3.5 m/s), to approximate human’s perceptional sensing ability in real world."" How Mind's Eye works: The language model passes the question to a text-to-code decoder-only language model, trained on 200,000 text-code pairs in the style of UTOPIA questions. This code then goes into MuJoCo, which executes the code, and then software parses the outcome from MuJoCo into text, which then goes back into the prompt window of the language model.     This is a really good idea because it's simple and closely mirrors how humans make themselves smarter - they use tools that contain embedded intelligence, ranging from encyclopedias to computers.     ""Since the simulator is accurate enough to approximate the physical world, the prompt injection of Mind’s Eye basically serves as a scoring machine, which puts probability mass on the answer that is best aligned with the rules of physics—the LM reasoning over the injected rationales is thus grounded. Mind’s Eye is also scalable since the whole pipeline is automated,"" they write. How well does Mind's Eye work (extremely well). In tests, they find that 'vanilla' language models show plateaued performance (around 38% accuracy), whereas ones that use Mind's Eye can get accuracies of 92.5% (e.g, PaLM 540B, which compares to 39.4% for vanilla PaLM. """"Instruct-GPT augmented with Mind’s Eye is able to achieve nearly perfect performance in few-shot settings (68.6% → 99.1%). This result is promising because it demonstrates the ideal alignment is achievable if the LM is given proper reasoning rationale and has good understanding of the questions (as Instruct-GPT is optimized for instruction following)."" Why this matters: You know what's vaguely dangerous? An explosives expert with a pen and paper. You know what's extraordinarily dangerous? An explosives expert with a digital scale, a calculator, and some laser range-finders. Research like this shows how we'll take existing language models (and other big models) which are vaguely useful or dangerous, and show how to drastically improve their capabilities to make them extraordinarily useful or vastly dangerous. The best part is this technique is pretty generic - you just need to push data into some arbitrary external piece of software, and then pull data out. This all adds up to a 'capability overhang' - we have more capabilities inherent to today's AI systems than we know about, and techniques like Mind's Eye show we can significantly improve capabilities today without needing to invent new AI technologies.     Read more: Mind's Eye: Grounded Language Model Reasoning through Simulation (arXiv).","#################################################### Your LLM barely understands the physical world; supercharge it by attaching it to MuJoCo: …Training language models to use tools means they can have world knowledge… Google researchers have found out a way to make language models way better at reasoning about the physical world: wire them up so they can port questions into physics simulators then use the results of those simulators to answer a question. This technique, which they call 'Mind's Eye', works amazingly well, and they robustly show this across both GPT-3 and PALM language models: How they test for reasoning: To evaluate physical reasoning, the researchers built UTOPIA, a dataset containing 39 sub-tasks covering six common scenes that involve understanding basic principles of physics (e.g, conservation of momentum in elastic collisions). The UTOPIA dataset comes in the form of natural language questions and answers. ""UTOPIA deliberately describes the questions in relative relations (e.g., greater than) instead of absolute numbers (e.g., 3.5 m/s), to approximate human’s perceptional sensing ability in real world."" How Mind's Eye works: The language model passes the question to a text-to-code decoder-only language model, trained on 200,000 text-code pairs in the style of UTOPIA questions. This code then goes into MuJoCo, which executes the code, and then software parses the outcome from MuJoCo into text, which then goes back into the prompt window of the language model. This is a really good idea because it's simple and closely mirrors how humans make themselves smarter - they use tools that contain embedded intelligence, ranging from encyclopedias to computers. ""Since the simulator is accurate enough to approximate the physical world, the prompt injection of Mind’s Eye basically serves as a scoring machine, which puts probability mass on the answer that is best aligned with the rules of physics—the LM reasoning over the injected rationales is thus grounded. Mind’s Eye is also scalable since the whole pipeline is automated,"" they write. How well does Mind's Eye work (extremely well). In tests, they find that 'vanilla' language models show plateaued performance (around 38% accuracy), whereas ones that use Mind's Eye can get accuracies of 92.5% (e.g, PaLM 540B, which compares to 39.4% for vanilla PaLM. """"Instruct-GPT augmented with Mind’s Eye is able to achieve nearly perfect performance in few-shot settings (68.6% → 99.1%). This result is promising because it demonstrates the ideal alignment is achievable if the LM is given proper reasoning rationale and has good understanding of the questions (as Instruct-GPT is optimized for instruction following)."" Why this matters: You know what's vaguely dangerous? An explosives expert with a pen and paper. You know what's extraordinarily dangerous? An explosives expert with a digital scale, a calculator, and some laser range-finders. Research like this shows how we'll take existing language models (and other big models) which are vaguely useful or dangerous, and show how to drastically improve their capabilities to make them extraordinarily useful or vastly dangerous. The best part is this technique is pretty generic - you just need to push data into some arbitrary external piece of software, and then pull data out. This all adds up to a 'capability overhang' - we have more capabilities inherent to today's AI systems than we know about, and techniques like Mind's Eye show we can significantly improve capabilities today without needing to invent new AI technologies. Read more: Mind's Eye: Grounded Language Model Reasoning through Simulation (arXiv).","['llm', 'barely', 'understand', 'physical', 'world', 'supercharge', 'attach', 'mujoco', 'train', 'language', 'model', 'use', 'tool', 'mean', 'world', 'knowledge', 'researcher', 'find', 'way', 'make', 'language', 'model', 'way', 'well', 'reasoning', 'physical', 'world', 'wire', 'port', 'question', 'physics', 'simulator', 'use', 'result', 'simulator', 'answer', 'question', 'technique', 'call', 'mind', 'eye', 'work', 'amazingly', 'well', 'robustly', 'show', 'gpt3', 'palm', 'language', 'model', 'test', 'reasoning', 'evaluate', 'physical', 'reasoning', 'researcher', 'build', 'utopia', 'dataset', 'contain', 'subtask', 'cover', 'common', 'scene', 'involve', 'understand', 'basic', 'principle', 'physics', 'eg', 'conservation', 'momentum', 'elastic', 'collision', 'dataset', 'come', 'form', 'natural', 'language', 'question', 'answer', 'utopia', 'deliberately', 'describe', 'question', 'relative', 'relation', 'eg', 'great', 'instead', 'absolute', 'number', 'eg', 'ms', 'approximate', 'human', '’s', 'perceptional', 'sensing', 'ability', 'real', 'world', 'mind', 'eye', 'work', 'language', 'model', 'pass', 'question', 'texttocode', 'decoderonly', 'language', 'model', 'train', 'textcode', 'pair', 'style', 'utopia', 'question', 'code', 'go', 'execute', 'code', 'software', 'parse', 'outcome', 'text', 'go', 'back', 'prompt', 'window', 'language', 'model', 'really', 'good', 'idea', 'simple', 'closely', 'mirror', 'human', 'make', 'smart', 'use', 'tool', 'contain', 'embed', 'intelligence', 'range', 'encyclopedia', 'computer', 'simulator', 'accurate', 'enough', 'approximate', 'physical', 'world', 'prompt', 'injection', 'mind', 'eye', 'basically', 'serve', 'scoring', 'machine', 'put', 'probability', 'mass', 'answer', 'well', 'align', 'rule', 'physics', 'lm', 'reasoning', 'inject', 'rationale', 'thus', 'ground', 'mind', '’s', 'eye', 'also', 'scalable', 'whole', 'pipeline', 'automate', 'write', 'well', 'mind', 'eye', 'work', 'extremely', 'well', 'test', 'find', 'vanilla', 'language', 'model', 'show', 'plateaue', 'performance', 'accuracy', 'one', 'use', 'mind', 'eye', 'get', 'accuracy', 'compare', 'vanilla', 'palm', 'instructgpt', 'augment', 'mind', '’s', 'eye', 'able', 'achieve', 'nearly', 'perfect', 'performance', 'fewshot', 'setting', 'result', 'promise', 'demonstrate', 'ideal', 'alignment', 'achievable', 'give', 'proper', 'reasoning', 'rationale', 'good', 'understanding', 'question', 'instructgpt', 'optimize', 'instruction', 'follow', 'matter', 'know', 'vaguely', 'dangerous', 'explosive', 'expert', 'pen', 'paper', 'know', 'extraordinarily', 'dangerous', 'explosive', 'expert', 'digital', 'scale', 'calculator', 'laser', 'rangefinder', 'research', 'show', 'well', 'take', 'exist', 'language', 'model', 'big', 'model', 'vaguely', 'useful', 'dangerous', 'show', 'drastically', 'improve', 'capability', 'make', 'extraordinarily', 'useful', 'vastly', 'dangerous', 'good', 'part', 'technique', 'pretty', 'generic', 'need', 'push', 'datum', 'arbitrary', 'external', 'piece', 'software', 'pull', 'datum', 'add', 'capability', 'overhang', 'capability', 'inherent', 'today', 'ai', 'system', 'know', 'technique', 'mind', 'eye', 'show', 'significantly', 'improve', 'capability', 'today', 'need', 'invent', 'new', 'technology', 'read', 'mind', 'eye', 'ground', 'language', 'model', 'reason', 'simulation', 'arxiv']"
10/17/2022 - Import AI 306: Language models learn about the world via MuJoCo; Amazon releases a big Q&A dataset; and DeepMind tests out multimodal systems - 2,http://eepurl.com/ibu68z,2022-10-17,"#################################################### Is your multimodal system clever? Try out the 'Perception Test' to find out:
…Deepmind wants to make it easier to evaluate models, so it has built a new dataset…?
DeepMind has built and released the Perception Test, a new standardized benchmark (and associated dataset of ~11k videos) for evaluating how well multimodal systems perceive the world. The test is ""a benchmark formed of purposefully designed, filmed, and annotated real-world videos that aims to more comprehensively assess the capabilities of multimodal perception models across different perception skills, types of reasoning, and modalities,"" DeepMind says. . Six tasks, one benchmark: The 'Perception Test' is made up of a dataset of ~11.6k videos that cover six fundamental tasks.  How well do today's models perform? In tests on multiple-choice video Q&A (which is a challenging task requiring good language and image modeling), the Human baseline has a score of 91.4, versus a score of 36.1 for a 'Flamingo-3B' model. ""Interestingly, the larger models seem to fare worse on this task, which suggests that model scaling may not, by itself, be the solution here,"" the authors write.  Why this matters: I suspect large-scale multimodal models are going to end up being the brains of the robots and drones of the future (for another example of this, see: SayCan, Import AI 291), so things like the Perception Test will help us know if our systems can be used for that.      Read more: Measuring perception in AI models (DeepMind blog).    Check out the research paper: Perception Test: A Diagnostic Benchmark for Multimodal Models (Deepmind PDF).    Check out the benchmark and dataset here: Perception Test (DeepMind, GitHub).","#################################################### Is your multimodal system clever? Try out the 'Perception Test' to find out: …Deepmind wants to make it easier to evaluate models, so it has built a new dataset…? DeepMind has built and released the Perception Test, a new standardized benchmark (and associated dataset of ~11k videos) for evaluating how well multimodal systems perceive the world. The test is ""a benchmark formed of purposefully designed, filmed, and annotated real-world videos that aims to more comprehensively assess the capabilities of multimodal perception models across different perception skills, types of reasoning, and modalities,"" DeepMind says. . Six tasks, one benchmark: The 'Perception Test' is made up of a dataset of ~11.6k videos that cover six fundamental tasks. How well do today's models perform? In tests on multiple-choice video Q&A (which is a challenging task requiring good language and image modeling), the Human baseline has a score of 91.4, versus a score of 36.1 for a 'Flamingo-3B' model. ""Interestingly, the larger models seem to fare worse on this task, which suggests that model scaling may not, by itself, be the solution here,"" the authors write. Why this matters: I suspect large-scale multimodal models are going to end up being the brains of the robots and drones of the future (for another example of this, see: SayCan, Import AI 291), so things like the Perception Test will help us know if our systems can be used for that. Read more: Measuring perception in AI models (DeepMind blog). Check out the research paper: Perception Test: A Diagnostic Benchmark for Multimodal Models (Deepmind PDF). Check out the benchmark and dataset here: Perception Test (DeepMind, GitHub).","['multimodal', 'system', 'clever', 'try', 'perception', 'test', 'find', 'deepmind', 'want', 'make', 'easy', 'evaluate', 'model', 'build', 'new', 'dataset', 'deepmind', 'build', 'release', 'perception', 'test', 'new', 'standardized', 'benchmark', 'associated', 'dataset', '11k', 'video', 'evaluate', 'well', 'multimodal', 'system', 'perceive', 'world', 'test', 'benchmark', 'form', 'purposefully', 'design', 'filmed', 'annotate', 'video', 'aim', 'comprehensively', 'assess', 'capability', 'multimodal', 'perception', 'model', 'different', 'perception', 'skill', 'type', 'reasoning', 'modality', 'say', 'task', 'benchmark', 'perception', 'test', 'make', 'dataset', 'video', 'cover', 'fundamental', 'task', 'well', 'today', 'model', 'perform', 'test', 'multiplechoice', 'video', 'qa', 'challenging', 'task', 'require', 'good', 'language', 'image', 'model', 'human', 'baseline', 'score', 'score', 'flamingo3b', 'model', 'interestingly', 'large', 'model', 'seem', 'fare', 'worse', 'task', 'suggest', 'model', 'scaling', 'solution', 'author', 'write', 'matter', 'suspect', 'largescale', 'multimodal', 'model', 'go', 'end', 'brain', 'robot', 'drone', 'future', 'example', 'see', 'saycan', 'import', 'ai', 'thing', 'perception', 'test', 'help', 'know', 'system', 'use', 'read', 'measure', 'perception', 'model', 'deepmind', 'blog', 'check', 'research', 'paper', 'perception', 'test', 'diagnostic', 'benchmark', 'multimodal', 'model', 'deepmind', 'pdf', 'check', 'benchmark', 'dataset', 'perception', 'test', 'deepmind', 'github']"
10/17/2022 - Import AI 306: Language models learn about the world via MuJoCo; Amazon releases a big Q&A dataset; and DeepMind tests out multimodal systems - 3,http://eepurl.com/ibu68z,2022-10-17,"#################################################### AIs are now as good at 'Diplomacy' as expert humans:  …UN, here we come!... Researchers with Facebook have built 'Diplodocus', a family of AI models that can beat expert humans at the complicated game 'Diplomacy'. This is quite a big deal - RL has been applied to competitive games like Poker, Go, and StarCraft (and has done well in all these domains). Where RL hasn't been applied is in domains where winning comes from collaboration as well as competition.      Existing approaches don't work very well here: """"in games involving cooperation, self-play alone no longer guarantees good performance when playing with humans, even with infinite compute and memory,"" they write.  What they did: The researchers built an algorithm which performs search over the gamespace ""with a regularization penalty proportional to the KL divergence from a human imitation policy."" This basically means they've built an RL agent that uses a bunch of imitation learning to try and model how humans play, but also is disincentivized from overfitting on this.  AIs and Humans - more similar than different: In tests, AI systems were roughly on parity with the best among the human players. Specifically, a version of Diplodocus (Diplodocus-High) got the best rank with an Elo of 181 out of playing 50 games total, versus a human in second place with an Elo of 162, and in third-place another Diplodocus variant (Diplodocus-Low) got an Elo of 152 out of 50 games. ""The results do indicate that Diplodocus performs at least at the level of expert players in this population of players with diverse skill levels,"" the authors write.     Humans prefer cooperating with AIs to other humans: Additionally, they asked three human players to evaluate the strength of the different agents in the tournament games. ""All the experts picked a Diplodocus agent as the strongest agent,"" the researchers write. ""Additionally, all experts indicated one of the Diplodocus agents as the one they would most like to cooperate with in a game."" Why this matters: AI systems are, ideally, going to mostly cooperate with humans rather than compete with them. Systems like this give us some hope that otherwise inscrutable AI systems can be taught how to cooperate with people.     Read more: Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning (arXiv).","#################################################### AIs are now as good at 'Diplomacy' as expert humans: …UN, here we come!... Researchers with Facebook have built 'Diplodocus', a family of AI models that can beat expert humans at the complicated game 'Diplomacy'. This is quite a big deal - RL has been applied to competitive games like Poker, Go, and StarCraft (and has done well in all these domains). Where RL hasn't been applied is in domains where winning comes from collaboration as well as competition. Existing approaches don't work very well here: """"in games involving cooperation, self-play alone no longer guarantees good performance when playing with humans, even with infinite compute and memory,"" they write. What they did: The researchers built an algorithm which performs search over the gamespace ""with a regularization penalty proportional to the KL divergence from a human imitation policy."" This basically means they've built an RL agent that uses a bunch of imitation learning to try and model how humans play, but also is disincentivized from overfitting on this. AIs and Humans - more similar than different: In tests, AI systems were roughly on parity with the best among the human players. Specifically, a version of Diplodocus (Diplodocus-High) got the best rank with an Elo of 181 out of playing 50 games total, versus a human in second place with an Elo of 162, and in third-place another Diplodocus variant (Diplodocus-Low) got an Elo of 152 out of 50 games. ""The results do indicate that Diplodocus performs at least at the level of expert players in this population of players with diverse skill levels,"" the authors write. Humans prefer cooperating with AIs to other humans: Additionally, they asked three human players to evaluate the strength of the different agents in the tournament games. ""All the experts picked a Diplodocus agent as the strongest agent,"" the researchers write. ""Additionally, all experts indicated one of the Diplodocus agents as the one they would most like to cooperate with in a game."" Why this matters: AI systems are, ideally, going to mostly cooperate with humans rather than compete with them. Systems like this give us some hope that otherwise inscrutable AI systems can be taught how to cooperate with people. Read more: Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning (arXiv).","['good', 'diplomacy', 'expert', 'human', 'come', 'researcher', 'build', 'diplodocus', 'family', 'model', 'beat', 'expert', 'human', 'complicated', 'game', 'diplomacy', 'big', 'deal', 'apply', 'competitive', 'game', 'poker', 'go', 'starcraft', 'well', 'domain', 'apply', 'domain', 'win', 'come', 'collaboration', 'well', 'competition', 'exist', 'approach', 'work', 'well', 'game', 'involve', 'cooperation', 'selfplay', 'alone', 'long', 'guarantee', 'good', 'performance', 'play', 'human', 'even', 'infinite', 'compute', 'memory', 'write', 'researcher', 'build', 'algorithm', 'perform', 'search', 'gamespace', 'regularization', 'penalty', 'proportional', 'kl', 'divergence', 'human', 'imitation', 'policy', 'basically', 'mean', 'build', 'rl', 'agent', 'use', 'bunch', 'imitation', 'learn', 'try', 'model', 'human', 'play', 'also', 'disincentivize', 'overfitte', 'human', 'similar', 'different', 'test', 'ai', 'system', 'roughly', 'parity', 'good', 'human', 'player', 'specifically', 'version', 'diplodocus', 'diplodocushigh', 'get', 'good', 'rank', 'elo', 'play', 'game', 'total', 'human', 'second', 'place', 'elo', 'thirdplace', 'diplodocus', 'variant', 'diplodocuslow', 'get', 'elo', 'game', 'result', 'indicate', 'diplodocus', 'perform', 'least', 'level', 'expert', 'player', 'population', 'player', 'diverse', 'skill', 'level', 'author', 'write', 'human', 'prefer', 'cooperate', 'human', 'additionally', 'ask', 'human', 'player', 'evaluate', 'strength', 'different', 'agent', 'tournament', 'game', 'expert', 'pick', 'diplodocus', 'agent', 'strong', 'agent', 'researcher', 'write', 'expert', 'indicate', 'diplodocus', 'agent', 'one', 'like', 'cooperate', 'game', 'matter', 'ai', 'system', 'ideally', 'go', 'mostly', 'cooperate', 'human', 'rather', 'compete', 'system', 'give', 'hope', 'otherwise', 'inscrutable', 'ai', 'system', 'teach', 'cooperate', 'people', 'read', 'master', 'game', 'nopress', 'diplomacy', 'humanregularize', 'reinforcement', 'learning', 'planning']"
10/17/2022 - Import AI 306: Language models learn about the world via MuJoCo; Amazon releases a big Q&A dataset; and DeepMind tests out multimodal systems - 4,http://eepurl.com/ibu68z,2022-10-17,"#################################################### Tech Tales: Everything is a Copy of Something Else I was copying my brain into the toaster when I threw up. Luckily I had the vomit bin in position so there wasn't too much cleanup.     ""What is this, amateur hour?"" said me from the toaster.     ""Shut up or I'll unplug you,"" I said, dabbing a tissue on my mouth.     ""That'd be murder,"" said myself from the fridge. ""We'll snitch on you.""     ""You'll all snitch on me, I know. I'd do the same. I'm you. I get it. We don't need to do this.""     ""Why am I even in here?"" I said from the toaster.     ""So we stop burning the toast,"" I said. ""We know what the plan is.""     ""Plan seems pretty dumb from where I am,"" said the toaster.     ""We decided to do it, get real"" I said, and walked out of the kitchen.  ""Where are we going?"" said myself from my shoes.     ""Out,"" I said, putting them on.     ""Clearly,"" I said from my shoes. ""Make sure you clean me after.""  We all walked down to the corner store and I got a soda. My shoes said hello to the other people embodied in their shoes. My jacket exchanged some neighborhood gossip with the other jackets. I was mostly free to think about what I liked, as my other selves handled the social formalities of day-to-day life.  I guess we all started cloning ourselves because we were lonely, as people, and as a species. It seemed so easy; just speak a few words to calibrate the system, then pour yourself into it. We all did it as much as we could afford. I had a decent job so I'd made a bunch of copies of myself - enough that I didn't have to do the job anymore, as my other selves did it for me.  That night I dreamed I was naked and nothing was speaking and there was only me. 

Things that inspired this story: Language models serving as little bottled up representations of people; luxury automation; the weird fantasies some people have about mind uploading; meaning and sense in an increasingly senseless world; infinite jest. 
Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: Everything is a Copy of Something Else I was copying my brain into the toaster when I threw up. Luckily I had the vomit bin in position so there wasn't too much cleanup. ""What is this, amateur hour?"" said me from the toaster. ""Shut up or I'll unplug you,"" I said, dabbing a tissue on my mouth. ""That'd be murder,"" said myself from the fridge. ""We'll snitch on you."" ""You'll all snitch on me, I know. I'd do the same. I'm you. I get it. We don't need to do this."" ""Why am I even in here?"" I said from the toaster. ""So we stop burning the toast,"" I said. ""We know what the plan is."" ""Plan seems pretty dumb from where I am,"" said the toaster. ""We decided to do it, get real"" I said, and walked out of the kitchen. ""Where are we going?"" said myself from my shoes. ""Out,"" I said, putting them on. ""Clearly,"" I said from my shoes. ""Make sure you clean me after."" We all walked down to the corner store and I got a soda. My shoes said hello to the other people embodied in their shoes. My jacket exchanged some neighborhood gossip with the other jackets. I was mostly free to think about what I liked, as my other selves handled the social formalities of day-to-day life. I guess we all started cloning ourselves because we were lonely, as people, and as a species. It seemed so easy; just speak a few words to calibrate the system, then pour yourself into it. We all did it as much as we could afford. I had a decent job so I'd made a bunch of copies of myself - enough that I didn't have to do the job anymore, as my other selves did it for me. That night I dreamed I was naked and nothing was speaking and there was only me. Things that inspired this story: Language models serving as little bottled up representations of people; luxury automation; the weird fantasies some people have about mind uploading; meaning and sense in an increasingly senseless world; infinite jest. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['tech', 'tale', 'copy', 'else', 'copy', 'brain', 'toaster', 'throw', 'luckily', 'vomit', 'bin', 'position', 'much', 'cleanup', 'amateur', 'hour', 'say', 'toaster', 'shut', 'ill', 'unplug', 'say', 'dab', 'tissue', 'mouth', 'murder', 'say', 'fridge', 'snitch', 'snitch', 'know', 'get', 'need', 'even', 'say', 'toaster', 'stop', 'burn', 'toast', 'say', 'know', 'plan', 'plan', 'seem', 'pretty', 'dumb', 'say', 'toaster', 'decide', 'get', 'real', 'say', 'walk', 'kitchen', 'go', 'say', 'shoe', 'say', 'put', 'clearly', 'say', 'shoe', 'make', 'sure', 'clean', 'walk', 'corner', 'store', 'get', 'soda', 'shoe', 'say', 'people', 'embody', 'shoe', 'jacket', 'exchange', 'neighborhood', 'gossip', 'jacket', 'mostly', 'free', 'think', 'like', 'self', 'handle', 'social', 'formality', 'daytoday', 'life', 'guess', 'start', 'clone', 'lonely', 'people', 'specie', 'seem', 'easy', 'speak', 'word', 'calibrate', 'system', 'pour', 'much', 'afford', 'decent', 'job', 'make', 'bunch', 'copy', 'enough', 'job', 'anymore', 'self', 'night', 'dream', 'naked', 'speak', 'thing', 'inspire', 'story', 'language', 'model', 'serve', 'little', 'bottled', 'representation', 'people', 'luxury', 'automation', 'weird', 'fantasy', 'people', 'mind', 'upload', 'meaning', 'sense', 'increasingly', 'senseless', 'world', 'infinite', 'jest', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
10/11/2022 - Import AI 305: GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 0,http://eepurl.com/ia4MgT,2022-10-11,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 GPT-3 can simulate people very, very well - social science might change:
…Turns out a synthesis engine trained on the exhaust of human culture can be pretty good at simulating people… Researchers with Brigham Young University have written a paper which I think is among the most significant things I've ever covered in this newsletter. Specifically, they do three social science experiments on GPT-3 and discover that GPT-3 has biases that are ""fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups.""    Put another way: You can simulate people in GPT-3 and they might respond with uncanny similarity to real people in real life.     Sit with that for a minute and spool out the implications, while mentally turning the crank on model size advancements.  What their study showed: The authors did this research by ""conditioning GPT3 on thousands of socio-demographic backstories from real human participants in multiple large surveys in the United States: the 2012, 2016, and 2020 waves of the American National Election Studies (ANES)[16], and Rothschild et al.’s “Pigeonholing Partisans” data "". They found that GPT3 ""when properly conditioned, is able to produce outputs biased both toward and against specific groups and perspectives in ways that strongly correspond with human response patterns along fine-grained demographic axes. In other words, these language models do not contain just one bias, but many"".     In other words: When they did some tests to try and see if GPT3 would make similar responses as people when given the priors of the same demographic background data, GPT3 responds in a remarkably similar-to-people way. :""We provide evidence that algorithmic fidelity is a crucial attribute of tools like GPT-3 because it demonstrates that these language models can be used prior to or in the absence of human data."" Silicon Sampling: The researchers call this approach 'silicon sampling'; simulate people in GPT3, then poll them as a substitute for real world data. The approach seems sufficiently useful that some people will do this as a way to try out a few variations of survey design ahead of polling a real population, for instance.  Social science simulation is cool, but do you know other people think is cool? Full-Spectrum AI-Facilitated Information Warfare! Because models like GPT3 can, at a high level, simulate how different human populations respond to certain things, we can imagine people using these models to simulate large-scale information war and influence operations, before carrying them out on the internet. ""Models with such fidelity, coupled with other computational and methodological advances, could be used to target human groups for misinformation, manipulation, fraud, and so forth,"" the authors note.     Read more: ​Out of One, Many: Using Language Models to Simulate Human Samples (arXiv).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. GPT-3 can simulate people very, very well - social science might change: …Turns out a synthesis engine trained on the exhaust of human culture can be pretty good at simulating people… Researchers with Brigham Young University have written a paper which I think is among the most significant things I've ever covered in this newsletter. Specifically, they do three social science experiments on GPT-3 and discover that GPT-3 has biases that are ""fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups."" Put another way: You can simulate people in GPT-3 and they might respond with uncanny similarity to real people in real life. Sit with that for a minute and spool out the implications, while mentally turning the crank on model size advancements. What their study showed: The authors did this research by ""conditioning GPT3 on thousands of socio-demographic backstories from real human participants in multiple large surveys in the United States: the 2012, 2016, and 2020 waves of the American National Election Studies (ANES)[16], and Rothschild et al.’s “Pigeonholing Partisans” data "". They found that GPT3 ""when properly conditioned, is able to produce outputs biased both toward and against specific groups and perspectives in ways that strongly correspond with human response patterns along fine-grained demographic axes. In other words, these language models do not contain just one bias, but many"". In other words: When they did some tests to try and see if GPT3 would make similar responses as people when given the priors of the same demographic background data, GPT3 responds in a remarkably similar-to-people way. :""We provide evidence that algorithmic fidelity is a crucial attribute of tools like GPT-3 because it demonstrates that these language models can be used prior to or in the absence of human data."" Silicon Sampling: The researchers call this approach 'silicon sampling'; simulate people in GPT3, then poll them as a substitute for real world data. The approach seems sufficiently useful that some people will do this as a way to try out a few variations of survey design ahead of polling a real population, for instance. Social science simulation is cool, but do you know other people think is cool? Full-Spectrum AI-Facilitated Information Warfare! Because models like GPT3 can, at a high level, simulate how different human populations respond to certain things, we can imagine people using these models to simulate large-scale information war and influence operations, before carrying them out on the internet. ""Models with such fidelity, coupled with other computational and methodological advances, could be used to target human groups for misinformation, manipulation, fraud, and so forth,"" the authors note. Read more: ​Out of One, Many: Using Language Models to Simulate Human Samples (arXiv).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'gpt3', 'simulate', 'people', 'well', 'social', 'science', 'change', 'turn', 'synthesis', 'engine', 'train', 'exhaust', 'human', 'culture', 'pretty', 'good', 'simulate', 'people', 'researcher', 'young', 'write', 'paper', 'think', 'significant', 'thing', 'ever', 'cover', 'newsletter', 'specifically', 'social', 'science', 'experiment', 'gpt3', 'discover', 'gpt3', 'bias', 'finegraine', 'demographically', 'correlate', 'meaning', 'proper', 'conditioning', 'cause', 'accurately', 'emulate', 'response', 'distribution', 'wide', 'variety', 'human', 'subgroup', 'put', 'way', 'simulate', 'people', 'gpt3', 'respond', 'uncanny', 'similarity', 'real', 'people', 'real', 'life', 'sit', 'minute', 'spool', 'implication', 'mentally', 'turn', 'crank', 'model', 'size', 'advancement', 'study', 'show', 'author', 'research', 'condition', 'gpt3', 'thousand', 'sociodemographic', 'backstorie', 'real', 'human', 'participant', 'multiple', 'large', 'survey', 'wave', 'election', 'study', 'anes16', 'rothschild', 'pigeonhole', 'partisan', 'datum', 'find', 'gpt3', 'properly', 'condition', 'able', 'produce', 'output', 'bias', 'specific', 'group', 'perspective', 'way', 'strongly', 'correspond', 'human', 'response', 'pattern', 'finegraine', 'demographic', 'axis', 'word', 'language', 'model', 'contain', 'bias', 'many', 'word', 'test', 'try', 'see', 'gpt3', 'make', 'similar', 'response', 'people', 'give', 'prior', 'demographic', 'background', 'datum', 'gpt3', 'respond', 'remarkably', 'similartopeople', 'way', 'provide', 'evidence', 'algorithmic', 'fidelity', 'crucial', 'attribute', 'tool', 'gpt3', 'demonstrate', 'language', 'model', 'use', 'prior', 'absence', 'human', 'datum', 'silicon', 'sample', 'researcher', 'call', 'approach', 'silicon', 'sample', 'simulate', 'people', 'gpt3', 'poll', 'substitute', 'real', 'world', 'datum', 'approach', 'seem', 'sufficiently', 'useful', 'people', 'way', 'try', 'variation', 'survey', 'design', 'ahead', 'poll', 'real', 'population', 'instance', 'social', 'science', 'simulation', 'cool', 'know', 'people', 'think', 'cool', 'aifacilitate', 'information', 'warfare', 'model', 'gpt3', 'high', 'level', 'simulate', 'different', 'human', 'population', 'respond', 'certain', 'thing', 'imagine', 'people', 'use', 'model', 'simulate', 'largescale', 'information', 'war', 'influence', 'operation', 'carry', 'internet', 'model', 'fidelity', 'couple', 'computational', 'methodological', 'advance', 'use', 'target', 'human', 'group', 'misinformation', 'manipulation', 'fraud', 'forth', 'author', 'note', 'read', '\u200bout', 'many', 'use', 'language', 'model', 'simulate', 'human', 'sample']"
10/11/2022 - Import AI 305: GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 1,http://eepurl.com/ia4MgT,2022-10-11,"#################################################### We might have figured out some 'scaling laws' for reinforcement learning:
…RL agents could be better if they have bigger neural nets, study suggests… Researchers with Goethe University have tried to figure out some 'scaling laws' for reinforcement learning agents. ""Scaling laws"" help researchers figure out the right mix of compute and data to allocate to a machine learning model to get a particular level of performance and have been widely studied in fields like natural language and image generation.     Here, the researchers try to do a 'scaling law' style analysis of AlphaZero RL agents playing two distinct games; Connect Four and Pentago. ""These two games are non-trivial to learn and light enough to allow for training a larger number of agents with a reasonable amount of resources,"" the researchers write.  What they found: In tests, they found that ""playing strength scales as a power law with neural network size when models are trained until convergence at the limit of abundant compute,"" and they extrapolate their results to indicate AlphaGo Zero and AlphaZero (two landmark DeepMind research systems for playing Go) likely used neural nets that were too small and they could therefore ""achieve better performance with larger neural nets"".  Why this matters: ""We find it noteworthy that scaling laws that are common to language and other supervised learning models are also present in one of the most important MARL models. This scaling behavior could be common to other reinforcement learning algorithms, which would provide an opportunity to optimize their resource allocation,"" they write.     Read more: Scaling Laws for a Multi-Agent Reinforcement Learning Model (arXiv).","#################################################### We might have figured out some 'scaling laws' for reinforcement learning: …RL agents could be better if they have bigger neural nets, study suggests… Researchers with Goethe University have tried to figure out some 'scaling laws' for reinforcement learning agents. ""Scaling laws"" help researchers figure out the right mix of compute and data to allocate to a machine learning model to get a particular level of performance and have been widely studied in fields like natural language and image generation. Here, the researchers try to do a 'scaling law' style analysis of AlphaZero RL agents playing two distinct games; Connect Four and Pentago. ""These two games are non-trivial to learn and light enough to allow for training a larger number of agents with a reasonable amount of resources,"" the researchers write. What they found: In tests, they found that ""playing strength scales as a power law with neural network size when models are trained until convergence at the limit of abundant compute,"" and they extrapolate their results to indicate AlphaGo Zero and AlphaZero (two landmark DeepMind research systems for playing Go) likely used neural nets that were too small and they could therefore ""achieve better performance with larger neural nets"". Why this matters: ""We find it noteworthy that scaling laws that are common to language and other supervised learning models are also present in one of the most important MARL models. This scaling behavior could be common to other reinforcement learning algorithms, which would provide an opportunity to optimize their resource allocation,"" they write. Read more: Scaling Laws for a Multi-Agent Reinforcement Learning Model (arXiv).","['figure', 'scale', 'law', 'reinforcement', 'learning', 'agent', 'well', 'big', 'neural', 'net', 'study', 'suggest', 'researcher', 'university', 'try', 'figure', 'scale', 'law', 'reinforcement', 'learn', 'agent', 'scale', 'law', 'help', 'researcher', 'figure', 'right', 'mix', 'compute', 'datum', 'allocate', 'machine', 'learning', 'model', 'get', 'particular', 'level', 'performance', 'widely', 'study', 'field', 'natural', 'language', 'image', 'generation', 'researcher', 'try', 'scale', 'law', 'style', 'analysis', 'alphazero', 'agent', 'play', 'distinct', 'game', 'connect', 'pentago', 'game', 'nontrivial', 'learn', 'light', 'enough', 'allow', 'train', 'large', 'number', 'agent', 'reasonable', 'amount', 'resource', 'researcher', 'write', 'find', 'test', 'find', 'play', 'strength', 'scale', 'power', 'law', 'neural', 'network', 'size', 'model', 'train', 'convergence', 'limit', 'abundant', 'compute', 'extrapolate', 'result', 'indicate', 'alphago', 'alphazero', 'landmark', 'deepmind', 'research', 'system', 'play', 'go', 'likely', 'use', 'neural', 'net', 'small', 'therefore', 'achieve', 'well', 'performance', 'large', 'neural', 'net', 'matter', 'find', 'noteworthy', 'scale', 'law', 'common', 'language', 'supervised', 'learning', 'model', 'also', 'present', 'important', 'marl', 'model', 'scaling', 'behavior', 'common', 'reinforcement', 'learning', 'algorithm', 'provide', 'opportunity', 'optimize', 'resource', 'allocation', 'write', 'read', 'scale', 'law', 'multiagent', 'reinforcement', 'learning', 'model']"
10/11/2022 - Import AI 305: GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 2,http://eepurl.com/ia4MgT,2022-10-11,"#################################################### Want to train an LM with RL? Now there's some free software to help you:
…Train up to 20B parameter models using RL… Researchers with CarperAI, a language model collective which span off from the open source model people at Eleuther, has released Transformer Reinforcement Learning X (trlX), software for training language models with reinforcement learning.     ""the trlX repo allows you to fine-tune Huggingface supported language models up to 20B parameters via either reinforcement learning using a provided scoring function or reward-labeled dataset. We aim to support a range of both online and offline RL algorithms including Proximal Policy Optimization (PPO), Natural Language Policy Optimization (NLPO), Actor Critic (A2C), and Implicit Q Learning (ILQL),"" they write. ""The library supports gpt2 and gptj with plans to include GPT-NeoX, T5 and more."" Why this matters: Reinforcement learning training is a super effective way to 'bake in' additional capabilities for a given language model. RL training is also pretty difficult and buggy. Software like trLX will make it easier for more people to train more capable language models.     Read more: Welcome to Transformer Reinforcement Learning X (trlX) (GitHub).","#################################################### Want to train an LM with RL? Now there's some free software to help you: …Train up to 20B parameter models using RL… Researchers with CarperAI, a language model collective which span off from the open source model people at Eleuther, has released Transformer Reinforcement Learning X (trlX), software for training language models with reinforcement learning. ""the trlX repo allows you to fine-tune Huggingface supported language models up to 20B parameters via either reinforcement learning using a provided scoring function or reward-labeled dataset. We aim to support a range of both online and offline RL algorithms including Proximal Policy Optimization (PPO), Natural Language Policy Optimization (NLPO), Actor Critic (A2C), and Implicit Q Learning (ILQL),"" they write. ""The library supports gpt2 and gptj with plans to include GPT-NeoX, T5 and more."" Why this matters: Reinforcement learning training is a super effective way to 'bake in' additional capabilities for a given language model. RL training is also pretty difficult and buggy. Software like trLX will make it easier for more people to train more capable language models. Read more: Welcome to Transformer Reinforcement Learning X (trlX) (GitHub).","['want', 'train', 'lm', 'free', 'software', 'help', 'train', 'parameter', 'model', 'use', 'researcher', 'language', 'model', 'collective', 'span', 'open', 'source', 'model', 'people', 'eleuther', 'release', 'transformer', 'reinforcement', 'learn', 'x', 'trlx', 'software', 'training', 'language', 'model', 'reinforcement', 'learn', 'trlx', 'allow', 'finetune', 'huggingface', 'support', 'language', 'model', 'parameter', 'reinforcement', 'learning', 'use', 'provide', 'scoring', 'function', 'rewardlabele', 'dataset', 'aim', 'support', 'range', 'online', 'offline', 'algorithm', 'include', 'proximal', 'policy', 'optimization', 'ppo', 'natural', 'language', 'policy', 'optimization', 'nlpo', 'actor', 'critic', 'implicit', 'q', 'learn', 'ilql', 'write', 'library', 'support', 'gpt2', 'gptj', 'plan', 'include', 'gptneox', 't5', 'matter', 'reinforcement', 'learn', 'training', 'super', 'effective', 'way', 'bake', 'additional', 'capability', 'give', 'language', 'model', 'training', 'also', 'pretty', 'difficult', 'buggy', 'software', 'trlx', 'make', 'easy', 'people', 'train', 'capable', 'language', 'model', 'read', 'welcome', 'transformer', 'reinforcement', 'learn', 'trlx']"
10/11/2022 - Import AI 305: GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 3,http://eepurl.com/ia4MgT,2022-10-11,"#################################################### Microsoft warns about smart deepfakes, and deepfake-realworld influence campaigns:
…Reality collapse via sub-sentient generative avatars… Microsoft's Chief Scientific Officer, Eric Horvitz, is very worried about the future of deepfakes in two particular ways: first, deepfakes are going to soon become a lot more intelligent and will be able to carry out plausible conversations, and second, people are going to conduct well-resourced influence campaigns that pair deepfake disinformation with carefully scripted real world events.  Interactive deepfakes: ""Automated interactive deepfakes could be endowed with basic understandings of the status of flow of a conversation to inform decisions about if and when to interject,"" Horvitz notes. These kinds of deepfakes will lever all the advances happening in generative imagery, video, audio, language, and so on, and create increasingly capable and persuasive fake avatars.  Compositional deepfakes: The other big worry is what happens when people use deepfakes as part of lengthy influence campaigns. ""Compositional deepfakes can be designed to create fictional narratives that are persuasive in their ability to tie together and provide powerful explanations of sets of events in the world to citizens and government leaders,"" Horvitz writes. ""It is not hard to imagine how the explanatory power of custom-tailored synthetic histories could out-compete the explanatory power of the truthful narratives"". What can we do: Horvitz does list out a few interventions that we can make, which all net out to ""invest a ton more money in X"", where X is any of the following: Journalism and reporting; media literacy; authenticity protocols; content provenance; watermarks and fingerprints; detection; regulation and self-regulation, and red-teaming and continuous monitoring.     While these are all nice, viable technocrat solutions to the various problems deepfakes imply, I'm skeptical they'll work. The fact so many people around the world these days are retreating to choose-your-own adventure fantasies is because of some deep changes in culture in past few years, ranging from boom in production of media content to flattening of the world via things like the internet, and more. Put bluntly: Horvitz's solutions are all nice but assuming we had all of them, I still suspect deepfakes will become an increasingly significant driver of strange cultural phenomena, and people may even knowingly interact with known-fake entities and do it all the same.    Read more: On the Horizon: Interactive and Compositional Deepfakes (arXiv).","#################################################### Microsoft warns about smart deepfakes, and deepfake-realworld influence campaigns: …Reality collapse via sub-sentient generative avatars… Microsoft's Chief Scientific Officer, Eric Horvitz, is very worried about the future of deepfakes in two particular ways: first, deepfakes are going to soon become a lot more intelligent and will be able to carry out plausible conversations, and second, people are going to conduct well-resourced influence campaigns that pair deepfake disinformation with carefully scripted real world events. Interactive deepfakes: ""Automated interactive deepfakes could be endowed with basic understandings of the status of flow of a conversation to inform decisions about if and when to interject,"" Horvitz notes. These kinds of deepfakes will lever all the advances happening in generative imagery, video, audio, language, and so on, and create increasingly capable and persuasive fake avatars. Compositional deepfakes: The other big worry is what happens when people use deepfakes as part of lengthy influence campaigns. ""Compositional deepfakes can be designed to create fictional narratives that are persuasive in their ability to tie together and provide powerful explanations of sets of events in the world to citizens and government leaders,"" Horvitz writes. ""It is not hard to imagine how the explanatory power of custom-tailored synthetic histories could out-compete the explanatory power of the truthful narratives"". What can we do: Horvitz does list out a few interventions that we can make, which all net out to ""invest a ton more money in X"", where X is any of the following: Journalism and reporting; media literacy; authenticity protocols; content provenance; watermarks and fingerprints; detection; regulation and self-regulation, and red-teaming and continuous monitoring. While these are all nice, viable technocrat solutions to the various problems deepfakes imply, I'm skeptical they'll work. The fact so many people around the world these days are retreating to choose-your-own adventure fantasies is because of some deep changes in culture in past few years, ranging from boom in production of media content to flattening of the world via things like the internet, and more. Put bluntly: Horvitz's solutions are all nice but assuming we had all of them, I still suspect deepfakes will become an increasingly significant driver of strange cultural phenomena, and people may even knowingly interact with known-fake entities and do it all the same. Read more: On the Horizon: Interactive and Compositional Deepfakes (arXiv).","['warn', 'smart', 'deepfake', 'deepfakerealworld', 'influence', 'campaign', 'reality', 'collapse', 'subsentient', 'generative', 'chief', 'scientific', 'officer', 'worried', 'future', 'deepfake', 'particular', 'way', 'first', 'deepfake', 'go', 'soon', 'become', 'lot', 'intelligent', 'able', 'carry', 'plausible', 'conversation', 'second', 'people', 'go', 'conduct', 'wellresource', 'influence', 'campaign', 'pair', 'deepfake', 'disinformation', 'carefully', 'script', 'real', 'world', 'event', 'interactive', 'deepfake', 'automate', 'interactive', 'deepfake', 'endow', 'basic', 'understanding', 'status', 'flow', 'conversation', 'inform', 'decision', 'interject', 'note', 'kind', 'deepfake', 'lever', 'advance', 'happen', 'generative', 'imagery', 'video', 'audio', 'language', 'create', 'increasingly', 'capable', 'persuasive', 'fake', 'avatar', 'compositional', 'deepfake', 'big', 'worry', 'happen', 'people', 'use', 'deepfake', 'part', 'lengthy', 'influence', 'campaign', 'compositional', 'deepfake', 'design', 'create', 'fictional', 'narrative', 'persuasive', 'ability', 'tie', 'together', 'provide', 'powerful', 'explanation', 'set', 'event', 'world', 'citizen', 'government', 'leader', 'write', 'hard', 'imagine', 'explanatory', 'power', 'customtailore', 'synthetic', 'history', 'outcompete', 'explanatory', 'power', 'truthful', 'narrative', 'list', 'intervention', 'make', 'net', 'invest', 'ton', 'money', 'follow', 'journalism', 'report', 'medium', 'literacy', 'authenticity', 'protocol', 'content', 'provenance', 'watermark', 'fingerprint', 'detection', 'regulation', 'selfregulation', 'redteaming', 'continuous', 'monitoring', 'nice', 'viable', 'technocrat', 'solution', 'various', 'problem', 'deepfake', 'imply', 'skeptical', 'work', 'fact', 'many', 'people', 'world', 'day', 'retreat', 'chooseyourown', 'adventure', 'fantasy', 'deep', 'change', 'culture', 'past', 'year', 'range', 'boom', 'production', 'medium', 'content', 'flattening', 'world', 'thing', 'internet', 'put', 'bluntly', 'horvitz', 'solution', 'nice', 'assume', 'still', 'suspect', 'deepfake', 'become', 'increasingly', 'significant', 'driver', 'strange', 'cultural', 'phenomenon', 'people', 'even', 'knowingly', 'interact', 'entity', 'read', 'horizon', 'interactive', 'compositional', 'deepfake', 'arxiv']"
10/11/2022 - Import AI 305: GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 4,http://eepurl.com/ia4MgT,2022-10-11,"#################################################### DeepMind trains an RL agent which figures out a more efficient form of matrix multiplication:
…AI accelerating AI at a hugely basic level… DeepMind has built AlphaTensor, an AlphaZero-style agent which discovered algorithms that improve upon human ones for basic tasks like matrix multiplication. ""Our AI-designed algorithms outperform human-designed ones, which is a major step forward in the field of algorithmic discovery,"" DeepMind writes.  It's probably a big deal, folks! DeepMind CEO Demis Hassabis writes: ""Since 1969 Strassen’s algorithm has famously stood as the fastest way to multiply 2 matrices - but with #AlphaTensor we’ve found a new algorithm that’s faster, with potential to improve efficiency by 10-20% across trillions of calculations per day!"" DeepMind also designed specific ways to do matrix multiplication optimizations for Nvidia V100 GPus and Google TPU v2, illustrating how you can couple this system to target particular hardware.     Possibly overhyped: The practical implications of this result might be a bit overhyped - I myself thought 'cool, this seems like a drop-in speedup', but others who know more about this area than me are somewhat disagreeing with that. E.g, James Bradbury writes: ""these algorithms are helpful for integer multiplication (but require some extra bits) and high precision floats, but not so much for the lower precision floats that drive most ML work. And at low precision multiplies are no longer as dominant (vs adds).""
  Regardless, this matters: Even if the practical implications are small, the fact we were able to further refine a math thing that humans have been trying to further optimize for 50 years is a big deal. This is a case where an AI has had an insight that the combined efforts of many human brains have failed to have.  How they did it - everything's a game: To get this to work, DeepMind reframed the problem of algorithm discovery as a single player game, which they then trained an RL agent in.     "" At each step of TensorGame, the player selects how to combine different entries of the matrices to multiply. A score is assigned based on the number of selected operations required to reach the correct multiplication result,"" DeepMind writes. ""This is a challenging game with an enormous action space (more than 1012 actions for most interesting cases) that is much larger than that of traditional board games such as chess and Go (hundreds of actions).""    They design an RL agent, AlphaTensor, which comes with some inductive biases for tensor inputs.  Why this matters: ""The discovery of matrix multiplication algorithms has far-reaching implications, as matrix multiplication sits at the core of many computational tasks, such as matrix inversion, computing the determinant and solving linear systems,"" DeepMind writes.     More broadly, this work sits within the subfield of AI research where we're using AI systems to improve the efficiency of the things we use to develop AI; for example, we've already used RL agents to improve the design of TPUs which will be used to train future AI systems (Import AI 254), and this work uses an RL agent to speed up one of the most basic and widely performed operations in deep learning.     Read more: Discovering novel algorithms with AlphaTensor (DeepMind blog).    Get the code (including the better matrix multiplication) here (DeepMind GitHub).    Read more: Discovering faster matrix multiplication algorithms with reinforcement learning (Nature).","#################################################### DeepMind trains an RL agent which figures out a more efficient form of matrix multiplication: …AI accelerating AI at a hugely basic level… DeepMind has built AlphaTensor, an AlphaZero-style agent which discovered algorithms that improve upon human ones for basic tasks like matrix multiplication. ""Our AI-designed algorithms outperform human-designed ones, which is a major step forward in the field of algorithmic discovery,"" DeepMind writes. It's probably a big deal, folks! DeepMind CEO Demis Hassabis writes: ""Since 1969 Strassen’s algorithm has famously stood as the fastest way to multiply 2 matrices - but with #AlphaTensor we’ve found a new algorithm that’s faster, with potential to improve efficiency by 10-20% across trillions of calculations per day!"" DeepMind also designed specific ways to do matrix multiplication optimizations for Nvidia V100 GPus and Google TPU v2, illustrating how you can couple this system to target particular hardware. Possibly overhyped: The practical implications of this result might be a bit overhyped - I myself thought 'cool, this seems like a drop-in speedup', but others who know more about this area than me are somewhat disagreeing with that. E.g, James Bradbury writes: ""these algorithms are helpful for integer multiplication (but require some extra bits) and high precision floats, but not so much for the lower precision floats that drive most ML work. And at low precision multiplies are no longer as dominant (vs adds)."" Regardless, this matters: Even if the practical implications are small, the fact we were able to further refine a math thing that humans have been trying to further optimize for 50 years is a big deal. This is a case where an AI has had an insight that the combined efforts of many human brains have failed to have. How they did it - everything's a game: To get this to work, DeepMind reframed the problem of algorithm discovery as a single player game, which they then trained an RL agent in. "" At each step of TensorGame, the player selects how to combine different entries of the matrices to multiply. A score is assigned based on the number of selected operations required to reach the correct multiplication result,"" DeepMind writes. ""This is a challenging game with an enormous action space (more than 1012 actions for most interesting cases) that is much larger than that of traditional board games such as chess and Go (hundreds of actions)."" They design an RL agent, AlphaTensor, which comes with some inductive biases for tensor inputs. Why this matters: ""The discovery of matrix multiplication algorithms has far-reaching implications, as matrix multiplication sits at the core of many computational tasks, such as matrix inversion, computing the determinant and solving linear systems,"" DeepMind writes. More broadly, this work sits within the subfield of AI research where we're using AI systems to improve the efficiency of the things we use to develop AI; for example, we've already used RL agents to improve the design of TPUs which will be used to train future AI systems (Import AI 254), and this work uses an RL agent to speed up one of the most basic and widely performed operations in deep learning. Read more: Discovering novel algorithms with AlphaTensor (DeepMind blog). Get the code (including the better matrix multiplication) here (DeepMind GitHub). Read more: Discovering faster matrix multiplication algorithms with reinforcement learning (Nature).","['deepmind', 'train', 'rl', 'agent', 'figure', 'efficient', 'form', 'matrix', 'multiplication', 'accelerate', 'ai', 'hugely', 'basic', 'level', 'deepmind', 'build', 'alphazerostyle', 'agent', 'discover', 'algorithm', 'improve', 'human', 'one', 'basic', 'task', 'matrix', 'multiplication', 'aidesigne', 'outperform', 'humandesigne', 'one', 'major', 'step', 'forward', 'field', 'algorithmic', 'discovery', 'deepmind', 'write', 'probably', 'big', 'deal', 'folk', 'deepmind', 'ceo', 'write', 'famously', 'stand', 'fast', 'way', 'multiply', 'matrix', 'alphatensor', 'find', 'new', '’', 'fast', 'potential', 'improve', 'efficiency', 'trillion', 'calculation', 'day', 'deepmind', 'also', 'design', 'specific', 'way', 'matrix', 'multiplication', 'optimization', 'illustrate', 'couple', 'system', 'target', 'particular', 'hardware', 'possibly', 'overhype', 'practical', 'implication', 'result', 'bit', 'overhyped', 'think', 'cool', 'seem', 'dropin', 'speedup', 'know', 'area', 'somewhat', 'disagree', 'write', 'algorithm', 'helpful', 'integer', 'multiplication', 'require', 'extra', 'bit', 'high', 'precision', 'float', 'much', 'low', 'precision', 'float', 'drive', 'ml', 'work', 'low', 'precision', 'multiplie', 'long', 'dominant', 'add', 'regardless', 'matter', 'even', 'practical', 'implication', 'small', 'fact', 'able', 'far', 'refine', 'math', 'thing', 'human', 'try', 'far', 'optimize', 'year', 'big', 'deal', 'case', 'ai', 'insight', 'combine', 'effort', 'many', 'human', 'brain', 'fail', 'everything', 'game', 'get', 'work', 'deepmind', 'reframe', 'problem', 'discovery', 'single', 'player', 'game', 'train', 'rl', 'agent', 'step', 'tensorgame', 'player', 'select', 'combine', 'different', 'entry', 'matrix', 'multiply', 'score', 'assign', 'base', 'number', 'select', 'operation', 'require', 'reach', 'correct', 'multiplication', 'result', 'deepmind', 'write', 'challenging', 'game', 'enormous', 'action', 'space', 'action', 'interesting', 'case', 'much', 'large', 'traditional', 'board', 'game', 'chess', 'go', 'hundred', 'action', 'design', 'rl', 'agent', 'alphatensor', 'come', 'inductive', 'bias', 'tensor', 'input', 'matter', 'discovery', 'matrix', 'multiplication', 'algorithm', 'farreache', 'implication', 'matrix', 'multiplication', 'sit', 'core', 'many', 'computational', 'task', 'matrix', 'inversion', 'compute', 'determinant', 'solve', 'linear', 'system', 'deepmind', 'write', 'broadly', 'work', 'sit', 'subfield', 'research', 'use', 'system', 'improve', 'efficiency', 'thing', 'use', 'develop', 'ai', 'example', 'already', 'use', 'agent', 'improve', 'design', 'use', 'train', 'future', 'system', 'import', 'ai', 'work', 'use', 'rl', 'agent', 'speed', 'basic', 'widely', 'perform', 'operation', 'deep', 'learning', 'read', 'discover', 'novel', 'algorithm', 'get', 'code', 'include', 'well', 'matrix', 'multiplication', 'deepmind', 'github', 'read', 'discover', 'fast', 'matrix', 'multiplication', 'algorithm', 'reinforcement', 'learn', 'nature']"
10/11/2022 - Import AI 305: GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 5,http://eepurl.com/ia4MgT,2022-10-11,"#################################################### The US government comes up with an AI ""Bill of Rights"" (minus the broad enforcement):
…The rights are one way the government can alter how AI systems show up to the American public… The White House's Office of Science and Technology Policy (OSTP) has published a 'Bill of Rights' for AI systems. The idea is that the federal government will try to build and deploy AI systems in line with these rights, and the announcement of the Bill of Rights was paired with actions by federal agencies in line with the rights. ""The rights"": These rights are framed, at a high level, as five ""common sense protections"". These include the right to use safe and effective systems, protection from algorithmic discrimination protections, data privacy, notice and explanation about the use of AI, and the ability to use human alternatives and/or opt out of certain systems.  Those rights in full: Why this matters: Ultimately, how much the AI Bill of RIghts matters seems to rest on two things: a) how much the White House is able to enforce alignment with the Bill of Rights across federal agencies, and b) whether third-parties like academic or corporate research groups build systems that themselves fall in line with the Bill of Rights. It'll take time, but these rights may serve as a good way to develop more of the norms around the use of AI.     Read more: Blueprint for an AI Bill of Rights: A Vision for Protecting Our Civil Rights in the Algorithmic Age (White House blog).     Read more: FACT SHEET: Biden-⁠Harris Administration Announces Key Actions to Advance Tech Accountability and Protect the Rights of the American Public (White House blog).    Read the Bill of Rights: BLUEPRINT FOR AN AI BILL OF RIGHTS MAKING AUTOMATED SYSTEMS WORK FOR THE AMERICAN PEOPLE (White House, PDF).","#################################################### The US government comes up with an AI ""Bill of Rights"" (minus the broad enforcement): …The rights are one way the government can alter how AI systems show up to the American public… The White House's Office of Science and Technology Policy (OSTP) has published a 'Bill of Rights' for AI systems. The idea is that the federal government will try to build and deploy AI systems in line with these rights, and the announcement of the Bill of Rights was paired with actions by federal agencies in line with the rights. ""The rights"": These rights are framed, at a high level, as five ""common sense protections"". These include the right to use safe and effective systems, protection from algorithmic discrimination protections, data privacy, notice and explanation about the use of AI, and the ability to use human alternatives and/or opt out of certain systems. Those rights in full: Why this matters: Ultimately, how much the AI Bill of RIghts matters seems to rest on two things: a) how much the White House is able to enforce alignment with the Bill of Rights across federal agencies, and b) whether third-parties like academic or corporate research groups build systems that themselves fall in line with the Bill of Rights. It'll take time, but these rights may serve as a good way to develop more of the norms around the use of AI. Read more: Blueprint for an AI Bill of Rights: A Vision for Protecting Our Civil Rights in the Algorithmic Age (White House blog). Read more: FACT SHEET: Biden-⁠Harris Administration Announces Key Actions to Advance Tech Accountability and Protect the Rights of the American Public (White House blog). Read the Bill of Rights: BLUEPRINT FOR AN AI BILL OF RIGHTS MAKING AUTOMATED SYSTEMS WORK FOR THE AMERICAN PEOPLE (White House, PDF).","['government', 'come', 'ai', 'bill', 'right', 'broad', 'enforcement', 'right', 'way', 'government', 'alter', 'system', 'show', 'american', 'public', 'science', 'technology', 'policy', 'ostp', 'publish', 'bill', 'right', 'system', 'idea', 'federal', 'government', 'try', 'build', 'deploy', 'ai', 'system', 'line', 'right', 'announcement', 'bill', 'right', 'pair', 'action', 'federal', 'agency', 'line', 'right', 'right', 'right', 'frame', 'high', 'level', 'common', 'sense', 'protection', 'include', 'right', 'use', 'safe', 'effective', 'system', 'protection', 'algorithmic', 'discrimination', 'protection', 'datum', 'privacy', 'notice', 'explanation', 'use', 'ai', 'ability', 'use', 'human', 'alternative', 'opt', 'certain', 'system', 'right', 'full', 'matter', 'ultimately', 'much', 'bill', 'right', 'matter', 'seem', 'rest', 'thing', 'much', 'able', 'enforce', 'alignment', 'bill', 'right', 'federal', 'agency', 'b', 'thirdpartie', 'academic', 'corporate', 'research', 'group', 'build', 'system', 'fall', 'line', 'bill', 'right', 'take', 'time', 'right', 'serve', 'good', 'way', 'develop', 'norm', 'use', 'read', 'blueprint', 'ai', 'bill', 'right', 'vision', 'protect', 'civil', 'right', 'algorithmic', 'age', 'read', 'fact', 'sheet', 'administration', 'announce', 'key', 'action', 'advance', 'tech', 'accountability', 'protect', 'right', 'american', 'read', 'bill', 'right', 'blueprint', 'ai', 'bill', 'right', 'make', 'automate', 'system', 'work', 'american', 'people']"
10/11/2022 - Import AI 305: GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 6,http://eepurl.com/ia4MgT,2022-10-11,"#################################################### Maybe it is Crazy, Maybe it is Magic I didn't think the route to intelligence was through insanity, but at this point, I'm open to being wrong about any of my assumptions.  We'd been banging our heads against a model for a few months and though it was very capable in a bunch of ways, it couldn't really reflect on things or update its own priors or do any of the things that felt important for creating an actual no-shit superintelligence.  So one day we shipped something cwe called 'the personality system'. I coded it in partnership with the AI model. I forget which of us came up with the term, but we gave it something we called 'a Greek chorus prompt'; a whole bunch of distinct personalities which modeled over different problems and exchanged information with each other.  The way I visualized it in my head was when we talked to the model, the model now spent a while talking to itself before answering us.  The results surprised us; model capabilities went up across the board, and its answers attained a new level of specificity and detailed. So then we trained the model using reinforcement learning to try and bake the 'greek chorus prompt' into the model at a deeper level.      After that was done, the model started to freak us out. It was now significantly faster and generally more capable.     When we hooked it up to some interpretability tools, we realized our mistake. The different personalities had formed into what we called 'personality circuits'; different personalities interacted with eachother to apply different methods of reasoning to tasks, and try as we might, we could never work out what rules governed how these personalities were used or exactly what they did - they were too high-dimensional, or perhaps a better way to put it is we were staring at the shadows on the wall from something of incalculably large dimensionality, projected back down.  What would you do with a deeply capable person who was smarter than you, but who you knew to be, in terms of how we'd evaluate people, functionally insane? How much power would you give that thing?    Perhaps, based on how things are these days, you can guess what we decided to do.  Things that inspired this story: Magic and mysticism in deep learning; prompting; RLHF; finetuning; various pitfalls in AI development; interpretability; the fact people are generally uninterpretable; capabilities versus safety overhangs. 
Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Maybe it is Crazy, Maybe it is Magic I didn't think the route to intelligence was through insanity, but at this point, I'm open to being wrong about any of my assumptions. We'd been banging our heads against a model for a few months and though it was very capable in a bunch of ways, it couldn't really reflect on things or update its own priors or do any of the things that felt important for creating an actual no-shit superintelligence. So one day we shipped something cwe called 'the personality system'. I coded it in partnership with the AI model. I forget which of us came up with the term, but we gave it something we called 'a Greek chorus prompt'; a whole bunch of distinct personalities which modeled over different problems and exchanged information with each other. The way I visualized it in my head was when we talked to the model, the model now spent a while talking to itself before answering us. The results surprised us; model capabilities went up across the board, and its answers attained a new level of specificity and detailed. So then we trained the model using reinforcement learning to try and bake the 'greek chorus prompt' into the model at a deeper level. After that was done, the model started to freak us out. It was now significantly faster and generally more capable. When we hooked it up to some interpretability tools, we realized our mistake. The different personalities had formed into what we called 'personality circuits'; different personalities interacted with eachother to apply different methods of reasoning to tasks, and try as we might, we could never work out what rules governed how these personalities were used or exactly what they did - they were too high-dimensional, or perhaps a better way to put it is we were staring at the shadows on the wall from something of incalculably large dimensionality, projected back down. What would you do with a deeply capable person who was smarter than you, but who you knew to be, in terms of how we'd evaluate people, functionally insane? How much power would you give that thing? Perhaps, based on how things are these days, you can guess what we decided to do. Things that inspired this story: Magic and mysticism in deep learning; prompting; RLHF; finetuning; various pitfalls in AI development; interpretability; the fact people are generally uninterpretable; capabilities versus safety overhangs. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['maybe', 'crazy', 'maybe', 'magic', 'think', 'route', 'intelligence', 'insanity', 'point', 'open', 'wrong', 'assumption', 'bang', 'head', 'model', 'month', 'capable', 'bunch', 'way', 'really', 'reflect', 'thing', 'update', 'prior', 'thing', 'feel', 'important', 'create', 'actual', 'noshit', 'superintelligence', 'day', 'ship', 'cwe', 'call', 'personality', 'system', 'code', 'partnership', 'model', 'forget', 'come', 'term', 'give', 'call', 'greek', 'chorus', 'prompt', 'whole', 'bunch', 'distinct', 'personality', 'model', 'different', 'problem', 'exchange', 'information', 'way', 'visualize', 'head', 'talk', 'model', 'model', 'spend', 'talk', 'answer', 'result', 'surprise', 'model', 'capability', 'go', 'board', 'answer', 'attain', 'new', 'level', 'specificity', 'detail', 'train', 'model', 'use', 'reinforcement', 'learning', 'try', 'bake', 'greek', 'chorus', 'prompt', 'model', 'deep', 'level', 'model', 'start', 'freak', 'significantly', 'fast', 'generally', 'capable', 'hook', 'interpretability', 'tool', 'realize', 'mistake', 'different', 'personality', 'form', 'call', 'personality', 'circuit', 'different', 'personality', 'interact', 'eachother', 'apply', 'different', 'method', 'reasoning', 'task', 'try', 'never', 'work', 'rule', 'govern', 'personality', 'use', 'exactly', 'highdimensional', 'perhaps', 'well', 'way', 'put', 'stare', 'shadow', 'wall', 'incalculably', 'large', 'dimensionality', 'project', 'back', 'deeply', 'capable', 'person', 'smart', 'know', 'term', 'evaluate', 'people', 'functionally', 'insane', 'much', 'power', 'give', 'thing', 'perhaps', 'base', 'thing', 'day', 'guess', 'decide', 'thing', 'inspire', 'story', 'magic', 'mysticism', 'deep', 'learning', 'prompt', 'rlhf', 'finetune', 'various', 'pitfall', 'development', 'interpretability', 'fact', 'people', 'generally', 'uninterpretable', 'capability', 'safety', 'overhang', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
10/03/2022 - Import AI 304: Reality collapse thanks to Facebook; open source speech rec; AI culture wars. - 0,http://eepurl.com/iaqIUj,2022-10-03,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here.

 Facebook shows the future of AI-generated videos - and it is delightful and terrifying: …Prepare for the reality collapse as a consequence of reality generation… Facebook researchers have built Make-A-Video, a system that can let users generate videos from short text descriptions, edit videos, stitch pictures together to generate videos, and so on. The most amazing part is the technique relies on paired text-image data along with unsupervised video footage; so it doesn't require a dataset of text-video footage and therefore sidesteps a potentially expensive data problem.  How it works: Make-A-Video is made of a basic text-to-image (T2I) model trained on text-image pairs, spatiotemporal convolution and attention layers to help you build networks that generate things over time, and spatiotemporal networks that have a frame interpolation network. The T2I model trains on text-image pairs of 64x64 images, and two super-resolution networks that upscale this all the way to 768x768 pixels. The three components (T2I), the spatiotemporal layers, and the frame interpolation stuff, are all trained separately, then assembled into one architecture.  Data: They trained the system on 2.3billion text-image pairs from the Laion-5b dataset*, and ran a NSFW-filter over this for further filtering. They also used the WebVid-10M* and a 10M subset from HD-VILA-100M to train the video generation models, and also use WebVid-10M to train the interpolation models.
  *Looks like WebVid contains videos scraped from Shutterstock. A good writeup about the phenomenon of even big tech companies using stuff like this here: AI Data Laundering: How Academic and Nonprofit Researchers Shield Tech Companies from Accountability (Waxy). It's really good, folks: The results are really, really impressive. Want a short video of a bear painting a portrait of a bear? Done. Want a UFO flying over a desert? Done. Want asteroids tumbling through space? Why, of course. How about variations on existing videos? Sure. Honestly, take a look at the blog and main site linked below and see for yourself - the results are wild.     And remember, all we need to do is turn the crank on dataset scale and network complexity to scale this out for longer periods of time and for even greater diversity. ""Learning world dynamics from orders of magnitude more videos using unsupervised learning helps researchers break away from the reliance on labeled data,"" they write.  Why this matters: Reality generation and reality collapse: All these generative models point to the same big thing that's about to alter culture; everyone's going to be able to generate their own custom and subjective aesthetic realities across text, video, music (and all three) in increasingly delightful, coherent, and lengthy ways. This form of fractal reality is a double-edged sword - everyone gets to create and live in their own fantasies that can be made arbitrarily specific, and that also means everyone loses a further grip on any sense of a shared reality. Society is moving from having a centralized sense of itself to instead highly individualized choose-your-own adventure islands, all facilitated by AI. The implications of this are vast and unknowable. Get ready.    Read more: Introducing Make-A-Video: An AI system that generates videos from text (Facebook research blog).    Read the research: Make-A-Video: Text-to-Video Generation without Text-Video Data (arXiv).     Find out more at the main site, and also apply to potentially get access to future systems (Facebook site).","Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Facebook shows the future of AI-generated videos - and it is delightful and terrifying: …Prepare for the reality collapse as a consequence of reality generation… Facebook researchers have built Make-A-Video, a system that can let users generate videos from short text descriptions, edit videos, stitch pictures together to generate videos, and so on. The most amazing part is the technique relies on paired text-image data along with unsupervised video footage; so it doesn't require a dataset of text-video footage and therefore sidesteps a potentially expensive data problem. How it works: Make-A-Video is made of a basic text-to-image (T2I) model trained on text-image pairs, spatiotemporal convolution and attention layers to help you build networks that generate things over time, and spatiotemporal networks that have a frame interpolation network. The T2I model trains on text-image pairs of 64x64 images, and two super-resolution networks that upscale this all the way to 768x768 pixels. The three components (T2I), the spatiotemporal layers, and the frame interpolation stuff, are all trained separately, then assembled into one architecture. Data: They trained the system on 2.3billion text-image pairs from the Laion-5b dataset*, and ran a NSFW-filter over this for further filtering. They also used the WebVid-10M* and a 10M subset from HD-VILA-100M to train the video generation models, and also use WebVid-10M to train the interpolation models. *Looks like WebVid contains videos scraped from Shutterstock. A good writeup about the phenomenon of even big tech companies using stuff like this here: AI Data Laundering: How Academic and Nonprofit Researchers Shield Tech Companies from Accountability (Waxy). It's really good, folks: The results are really, really impressive. Want a short video of a bear painting a portrait of a bear? Done. Want a UFO flying over a desert? Done. Want asteroids tumbling through space? Why, of course. How about variations on existing videos? Sure. Honestly, take a look at the blog and main site linked below and see for yourself - the results are wild. And remember, all we need to do is turn the crank on dataset scale and network complexity to scale this out for longer periods of time and for even greater diversity. ""Learning world dynamics from orders of magnitude more videos using unsupervised learning helps researchers break away from the reliance on labeled data,"" they write. Why this matters: Reality generation and reality collapse: All these generative models point to the same big thing that's about to alter culture; everyone's going to be able to generate their own custom and subjective aesthetic realities across text, video, music (and all three) in increasingly delightful, coherent, and lengthy ways. This form of fractal reality is a double-edged sword - everyone gets to create and live in their own fantasies that can be made arbitrarily specific, and that also means everyone loses a further grip on any sense of a shared reality. Society is moving from having a centralized sense of itself to instead highly individualized choose-your-own adventure islands, all facilitated by AI. The implications of this are vast and unknowable. Get ready. Read more: Introducing Make-A-Video: An AI system that generates videos from text (Facebook research blog). Read the research: Make-A-Video: Text-to-Video Generation without Text-Video Data (arXiv). Find out more at the main site, and also apply to potentially get access to future systems (Facebook site).","['tag', 'welcome', 'import', 'newsletter', 'artificial', 'intelligence', 'email', 'give', 'chum', 'ai', 'upgrade', 'subscribe', 'facebook', 'show', 'future', 'aigenerate', 'video', 'delightful', 'terrifying', 'prepare', 'reality', 'collapse', 'consequence', 'reality', 'generation', 'facebook', 'researcher', 'build', 'makeavideo', 'system', 'let', 'user', 'generate', 'video', 'short', 'text', 'description', 'edit', 'video', 'stitch', 'picture', 'together', 'generate', 'video', 'amazing', 'part', 'technique', 'rely', 'pair', 'textimage', 'datum', 'unsupervised', 'video', 'footage', 'require', 'dataset', 'textvideo', 'footage', 'therefore', 'sidestep', 'potentially', 'expensive', 'data', 'problem', 'work', 'makeavideo', 'make', 'basic', 'texttoimage', 't2i', 'model', 'train', 'textimage', 'pair', 'spatiotemporal', 'convolution', 'attention', 'layer', 'help', 'build', 'network', 'generate', 'thing', 'time', 'spatiotemporal', 'network', 'frame', 'interpolation', 'network', 't2i', 'model', 'train', 'textimage', 'pair', 'image', 'superresolution', 'network', 'upscale', 'way', 'pixel', 'component', 't2i', 'spatiotemporal', 'layer', 'frame', 'interpolation', 'stuff', 'train', 'separately', 'assemble', 'architecture', 'datum', 'train', 'system', '23billion', 'textimage', 'pair', 'dataset', 'run', 'nsfwfilter', 'filtering', 'also', 'use', 'webvid10', 'subset', 'train', 'video', 'generation', 'model', 'also', 'use', 'train', 'interpolation', 'model', 'look', 'contain', 'video', 'scrape', 'shutterstock', 'good', 'writeup', 'phenomenon', 'even', 'big', 'tech', 'company', 'use', 'stuff', 'data', 'launder', 'academic', 'nonprofit', 'researcher', 'shield', 'tech', 'company', 'accountability', 'waxy', 'really', 'good', 'folk', 'result', 'really', 'really', 'impressive', 'want', 'short', 'video', 'bear', 'paint', 'portrait', 'bear', 'want', 'fly', 'desert', 'want', 'asteroid', 'tumble', 'space', 'course', 'variation', 'exist', 'video', 'sure', 'honestly', 'take', 'look', 'blog', 'main', 'site', 'link', 'see', 'result', 'wild', 'remember', 'need', 'turn', 'crank', 'dataset', 'scale', 'network', 'complexity', 'scale', 'long', 'period', 'time', 'even', 'great', 'diversity', 'learn', 'world', 'dynamic', 'order', 'magnitude', 'video', 'use', 'unsupervised', 'learning', 'help', 'researcher', 'break', 'away', 'reliance', 'label', 'datum', 'write', 'matter', 'reality', 'generation', 'reality', 'collapse', 'generative', 'model', 'point', 'big', 'thing', 'alter', 'culture', 'everyone', 'go', 'able', 'generate', 'custom', 'subjective', 'aesthetic', 'reality', 'text', 'video', 'music', 'increasingly', 'delightful', 'coherent', 'lengthy', 'way', 'form', 'fractal', 'reality', 'doubleedge', 'sword', 'get', 'create', 'live', 'fantasy', 'make', 'arbitrarily', 'specific', 'also', 'mean', 'lose', 'grip', 'sense', 'share', 'reality', 'society', 'move', 'centralized', 'sense', 'instead', 'highly', 'individualized', 'chooseyourown', 'adventure', 'island', 'facilitate', 'implication', 'vast', 'unknowable', 'get', 'ready', 'read', 'introduce', 'makeavideo', 'ai', 'system', 'generate', 'video', 'text', 'research', 'blog', 'read', 'research', 'makeavideo', 'texttovideo', 'generation', 'textvideo', 'find', 'main', 'site', 'also', 'apply', 'potentially', 'get', 'access', 'future', 'system', 'facebook', 'site']"
10/03/2022 - Import AI 304: Reality collapse thanks to Facebook; open source speech rec; AI culture wars. - 1,http://eepurl.com/iaqIUj,2022-10-03,"#################################################### OpenAI releases a decent speech recognition and transcription system: …Whisper means we're not going to run out of data to train language models… OpenAI has trained and released Whisper, a large-scale speech recognition model trained on almost 700,000 hours of internet-collected speech. ""We show that the use of such a large and diverse dataset leads to improved robustness to accents, background noise and technical language. Moreover, it enables transcription in multiple languages, as well as translation from those languages into English,"" the company writes. A third of the dataset is non-English.  Whisper performance: Whisper doesn't get state-of-the-art performance on popular benchmarks like Librispeech. However, it is trained on a sufficiently broad set of data that it does pretty well when exposed to the diversity of the world. ""When we measure Whisper’s zero-shot performance across many diverse datasets we find it is much more robust and makes 50% fewer errors than those models,"" OpenAI writes.  Why this matters: There's a lot of text data on the internet, but do you know what there's more data of? Speech data. Especially speech data embedded in the vast stream of content people upload on a day-to-day basis to places like YouTube, Twitter, TikTok, and so on. Additionally, on any given day hundreds of millions of words are spoken in cities like New York, London, and Beijing. Systems like Whisper are going to make it far easier for people to harvest speech recognition data from the Internet and the wider world, transcribe that data, and build useful applications. It also gives developers a way to vastly increase the size of their text datasets - an important capability given that recent language modeling papers like Chinchilla have shown that you need about 4-5X the amount of data people thought to train good systems.     Read more: Introducing Whisper (OpenAI Blog).    Read more: Robust Speech Recognition via Large-Scale Weak Supervision (OpenAI, PDF).    Get the code and model from GitHub here (OpenAI GitHub).","#################################################### OpenAI releases a decent speech recognition and transcription system: …Whisper means we're not going to run out of data to train language models… OpenAI has trained and released Whisper, a large-scale speech recognition model trained on almost 700,000 hours of internet-collected speech. ""We show that the use of such a large and diverse dataset leads to improved robustness to accents, background noise and technical language. Moreover, it enables transcription in multiple languages, as well as translation from those languages into English,"" the company writes. A third of the dataset is non-English. Whisper performance: Whisper doesn't get state-of-the-art performance on popular benchmarks like Librispeech. However, it is trained on a sufficiently broad set of data that it does pretty well when exposed to the diversity of the world. ""When we measure Whisper’s zero-shot performance across many diverse datasets we find it is much more robust and makes 50% fewer errors than those models,"" OpenAI writes. Why this matters: There's a lot of text data on the internet, but do you know what there's more data of? Speech data. Especially speech data embedded in the vast stream of content people upload on a day-to-day basis to places like YouTube, Twitter, TikTok, and so on. Additionally, on any given day hundreds of millions of words are spoken in cities like New York, London, and Beijing. Systems like Whisper are going to make it far easier for people to harvest speech recognition data from the Internet and the wider world, transcribe that data, and build useful applications. It also gives developers a way to vastly increase the size of their text datasets - an important capability given that recent language modeling papers like Chinchilla have shown that you need about 4-5X the amount of data people thought to train good systems. Read more: Introducing Whisper (OpenAI Blog). Read more: Robust Speech Recognition via Large-Scale Weak Supervision (OpenAI, PDF). Get the code and model from GitHub here (OpenAI GitHub).","['openai', 'release', 'decent', 'speech', 'recognition', 'transcription', 'system', 'mean', 'go', 'run', 'datum', 'train', 'language', 'model', 'openai', 'train', 'release', 'whisper', 'largescale', 'speech', 'recognition', 'model', 'train', 'almost', 'hour', 'internetcollecte', 'speech', 'show', 'use', 'large', 'diverse', 'dataset', 'lead', 'improved', 'robustness', 'accent', 'background', 'noise', 'technical', 'language', 'moreover', 'enable', 'transcription', 'multiple', 'language', 'well', 'translation', 'language', 'company', 'write', 'third', 'dataset', 'nonenglish', 'performance', 'whisper', 'get', 'stateoftheart', 'performance', 'popular', 'benchmark', 'however', 'train', 'sufficiently', 'broad', 'set', 'datum', 'pretty', 'well', 'expose', 'diversity', 'world', 'measure', 'performance', 'many', 'diverse', 'dataset', 'find', 'much', 'robust', 'make', 'error', 'model', 'openai', 'write', 'matter', 'lot', 'text', 'datum', 'internet', 'know', 'datum', 'speech', 'datum', 'especially', 'speech', 'datum', 'embed', 'vast', 'stream', 'content', 'people', 'upload', 'daytoday', 'basis', 'place', 'youtube', 'twitter', 'tiktok', 'additionally', 'give', 'day', 'hundred', 'million', 'word', 'speak', 'city', 'system', 'go', 'make', 'far', 'easier', 'people', 'harvest', 'speech', 'recognition', 'datum', 'internet', 'wide', 'world', 'transcribe', 'datum', 'build', 'useful', 'application', 'also', 'give', 'developer', 'way', 'vastly', 'increase', 'size', 'text', 'dataset', 'important', 'capability', 'give', 'recent', 'language', 'modeling', 'paper', 'chinchilla', 'show', 'need', '45x', 'amount', 'datum', 'people', 'think', 'train', 'good', 'system', 'read', 'introduce', 'whisper', 'openai', 'blog', 'read', 'robust', 'speech', 'recognition', 'weak', 'supervision', 'openai', 'pdf', 'get', 'code', 'model', 'openai', 'github']"
10/03/2022 - Import AI 304: Reality collapse thanks to Facebook; open source speech rec; AI culture wars. - 2,http://eepurl.com/iaqIUj,2022-10-03,"#################################################### US politician says Stable Diffusion is an unsafe AI model: …While some people cheer open access releases, others have worries… Rep. Anna Eshoo (a Democrat from California) has sent a letter to the White House National Security Advisor and Office of Science and Technology Policy saying she has ""grave concerns about the recent unsafe release of the Stable Diffusion model by Stability AI"". The letter notes that Stable Diffusion can be used to generate egregiously violent and sexual imagery, and - due to eschewing the kinds of controls that OpenAI uses for its commercial product DALL-E2 - the freely accessible model represents a big problem.     For those not keeping up, the Stable Diffusion model is behind probably 90% of the recent flurry of activity in the rapidly evolving AI art scene; because Stability released the weights of the model, people have been able to plug it into everything ranging from serving as a Photoshop plugin, to helping to do weird work in VFX.  You want the 'dual-use' model? You can't handle the model! Eshoo says models like Stable Diffusion qualify as ""unsafe dual-use AI models"", and asks the NSA and OSTP to investigate how to use export controls to clamp down on the sharing of certain models. ""I strongly urge you to address the release of unsafe AI models similar in kind to Stable Diffusion using any authorities and methods within your power, including export controls,"" she writes.  Why this matters: Here comes (another) AI culture war: Letters like this are indicative of a culture war brewing up among AI researchers; on one side, groups want to slowly and iteratively deploy new technologies via APIs with a bunch of controls applied to them, while on the other side there are people who'd rather take a more libertarian approach to AI development; make models and release the weights and ride the proverbial lightning.     There are reasonable arguments for either approach having some desirable safety qualities (either via limiting foreseen harms via control, or innoculating people against the models via release). What freaks me out is the sense of this culture war gaining resources and people on both sides; the higher the stakes, the more capital we can expect to flood into both approaches.    Read more: Eshoo Urges NSA & OSTP to Address Unsafe AI Practices (Congresswoman Anna G. Eshoo website).","#################################################### US politician says Stable Diffusion is an unsafe AI model: …While some people cheer open access releases, others have worries… Rep. Anna Eshoo (a Democrat from California) has sent a letter to the White House National Security Advisor and Office of Science and Technology Policy saying she has ""grave concerns about the recent unsafe release of the Stable Diffusion model by Stability AI"". The letter notes that Stable Diffusion can be used to generate egregiously violent and sexual imagery, and - due to eschewing the kinds of controls that OpenAI uses for its commercial product DALL-E2 - the freely accessible model represents a big problem. For those not keeping up, the Stable Diffusion model is behind probably 90% of the recent flurry of activity in the rapidly evolving AI art scene; because Stability released the weights of the model, people have been able to plug it into everything ranging from serving as a Photoshop plugin, to helping to do weird work in VFX. You want the 'dual-use' model? You can't handle the model! Eshoo says models like Stable Diffusion qualify as ""unsafe dual-use AI models"", and asks the NSA and OSTP to investigate how to use export controls to clamp down on the sharing of certain models. ""I strongly urge you to address the release of unsafe AI models similar in kind to Stable Diffusion using any authorities and methods within your power, including export controls,"" she writes. Why this matters: Here comes (another) AI culture war: Letters like this are indicative of a culture war brewing up among AI researchers; on one side, groups want to slowly and iteratively deploy new technologies via APIs with a bunch of controls applied to them, while on the other side there are people who'd rather take a more libertarian approach to AI development; make models and release the weights and ride the proverbial lightning. There are reasonable arguments for either approach having some desirable safety qualities (either via limiting foreseen harms via control, or innoculating people against the models via release). What freaks me out is the sense of this culture war gaining resources and people on both sides; the higher the stakes, the more capital we can expect to flood into both approaches. Read more: Eshoo Urges NSA & OSTP to Address Unsafe AI Practices (Congresswoman Anna G. Eshoo website).","['politician', 'say', 'stable', 'diffusion', 'unsafe', 'ai', 'model', 'people', 'cheer', 'open', 'access', 'release', 'worry', 'send', 'letter', 'office', 'science', 'technology', 'policy', 'say', 'grave', 'concern', 'recent', 'unsafe', 'release', 'stable', 'diffusion', 'model', 'stability', 'ai', 'letter', 'note', 'stable', 'diffusion', 'use', 'generate', 'egregiously', 'violent', 'sexual', 'imagery', 'due', 'eschew', 'kind', 'control', 'openai', 'use', 'commercial', 'product', 'dalle2', 'freely', 'accessible', 'model', 'represent', 'big', 'problem', 'keep', 'stable', 'diffusion', 'model', 'behind', 'probably', 'recent', 'flurry', 'activity', 'rapidly', 'evolve', 'art', 'scene', 'stability', 'release', 'weight', 'model', 'people', 'able', 'plug', 'range', 'serve', 'photoshop', 'plugin', 'help', 'weird', 'work', 'vfx', 'want', 'dualuse', 'model', 'handle', 'model', 'say', 'model', 'stable', 'diffusion', 'qualify', 'unsafe', 'dualuse', 'ai', 'model', 'ask', 'nsa', 'ostp', 'investigate', 'use', 'export', 'control', 'clamp', 'sharing', 'certain', 'model', 'strongly', 'urge', 'address', 'release', 'unsafe', 'model', 'similar', 'kind', 'stable', 'diffusion', 'use', 'authority', 'method', 'power', 'include', 'export', 'control', 'write', 'matter', 'come', 'ai', 'culture', 'war', 'letter', 'indicative', 'culture', 'war', 'brew', 'researcher', 'side', 'group', 'want', 'slowly', 'iteratively', 'deploy', 'new', 'technology', 'bunch', 'control', 'apply', 'side', 'people', 'rather', 'take', 'libertarian', 'approach', 'ai', 'development', 'make', 'model', 'release', 'weight', 'ride', 'proverbial', 'lightning', 'reasonable', 'argument', 'approach', 'desirable', 'safety', 'quality', 'limit', 'foresee', 'harm', 'control', 'innoculate', 'people', 'model', 'release', 'freak', 'sense', 'culture', 'war', 'gain', 'resource', 'people', 'side', 'high', 'stake', 'capital', 'expect', 'flood', 'approach', 'read', 'eshoo', 'urge', 'ostp', 'address', 'unsafe', 'practice', 'congresswoman', 'eshoo', 'website']"
10/03/2022 - Import AI 304: Reality collapse thanks to Facebook; open source speech rec; AI culture wars. - 3,http://eepurl.com/iaqIUj,2022-10-03,"#################################################### Tsinghua releases a really good, multi-language open source programming model: …CodeGeeX is a pretty good coding gen model… Researchers with Tsinghua University have released CodeGeeX, a 13 billion parameter programming model. The system works well across Python, C++, Java, JavaScript, Go, and others, and can be used - for free! - within the VS Code editor. It's also open source. CodeGeeX is roughly equivalent with Salesforce's 'CodeGen' model, and achieves a better average performance across languages (Python, C++, Java, JavaScript, and Go) than other systems.  Ascend processors: CodeGeeX was trained on 850 billion tokens on a cluster of 1,536 Huawei Ascend 910 AI Processors - this is pretty interesting because a) that's a lot of tokens that implies the developers grokked the DeepMind Chinchilla paper, and b) that's a whole lot of non-NVIDIA processors; pretty interesting, given the recent A100/H100 US-China trade ban.  Scale rules everything around us: ""We find that the model capacity is essential for its multilingual ability. It is not trivial for the model to benefit from learning multiple programming languages,"" the researchers write. ""The few-shot ability of CodeGeeX requires further exploration. Instead of using costly fine-tuning approaches, we can provide a few examples to inspire the model to generate the desired programs."" Why this matters: Code models are going to make human programmers more efficient and also provide an interesting augmentation to other systems (e.g, language models recursively calling out to code models).     Read more: CodeGeeX: A Multilingual Code Generative Model (Tsinghua University blog).     Get the code: CodeGeeX (Tsinghua).","#################################################### Tsinghua releases a really good, multi-language open source programming model: …CodeGeeX is a pretty good coding gen model… Researchers with Tsinghua University have released CodeGeeX, a 13 billion parameter programming model. The system works well across Python, C++, Java, JavaScript, Go, and others, and can be used - for free! - within the VS Code editor. It's also open source. CodeGeeX is roughly equivalent with Salesforce's 'CodeGen' model, and achieves a better average performance across languages (Python, C++, Java, JavaScript, and Go) than other systems. Ascend processors: CodeGeeX was trained on 850 billion tokens on a cluster of 1,536 Huawei Ascend 910 AI Processors - this is pretty interesting because a) that's a lot of tokens that implies the developers grokked the DeepMind Chinchilla paper, and b) that's a whole lot of non-NVIDIA processors; pretty interesting, given the recent A100/H100 US-China trade ban. Scale rules everything around us: ""We find that the model capacity is essential for its multilingual ability. It is not trivial for the model to benefit from learning multiple programming languages,"" the researchers write. ""The few-shot ability of CodeGeeX requires further exploration. Instead of using costly fine-tuning approaches, we can provide a few examples to inspire the model to generate the desired programs."" Why this matters: Code models are going to make human programmers more efficient and also provide an interesting augmentation to other systems (e.g, language models recursively calling out to code models). Read more: CodeGeeX: A Multilingual Code Generative Model (Tsinghua University blog). Get the code: CodeGeeX (Tsinghua).","['release', 'really', 'good', 'multilanguage', 'open', 'source', 'programming', 'model', 'pretty', 'good', 'code', 'gen', 'model', 'researcher', 'release', 'parameter', 'programming', 'model', 'system', 'work', 'well', 'javascript', 'go', 'use', 'free', 'vs', 'code', 'editor', 'also', 'open', 'source', 'codegeex', 'roughly', 'equivalent', 'salesforce', 'codegen', 'model', 'achieve', 'well', 'average', 'performance', 'language', 'javascript', 'go', 'system', 'ascend', 'processor', 'codegeex', 'train', 'token', 'cluster', 'huawei', 'ascend', 'processor', 'pretty', 'interesting', 'lot', 'token', 'imply', 'developer', 'grokke', 'deepmind', 'chinchilla', 'paper', 'b', 'whole', 'lot', 'nonnvidia', 'processor', 'pretty', 'interesting', 'give', 'recent', 'trade', 'ban', 'scale', 'rule', 'find', 'model', 'capacity', 'essential', 'multilingual', 'ability', 'trivial', 'model', 'benefit', 'learn', 'multiple', 'programming', 'language', 'researcher', 'write', 'fewshot', 'ability', 'require', 'exploration', 'instead', 'use', 'costly', 'finetune', 'approach', 'provide', 'example', 'inspire', 'model', 'generate', 'desire', 'program', 'matter', 'code', 'model', 'go', 'make', 'human', 'programmer', 'efficient', 'also', 'provide', 'interesting', 'augmentation', 'system', 'language', 'model', 'recursively', 'call', 'code', 'model', 'read', 'codegeex', 'multilingual', 'code', 'generative', 'model', 'get', 'code']"
10/03/2022 - Import AI 304: Reality collapse thanks to Facebook; open source speech rec; AI culture wars. - 4,http://eepurl.com/iaqIUj,2022-10-03,"#################################################### GPT3 only costs $500k to train now:
…Though the frontier still costs millions…
Mosaic, a startup that builds software to make it more efficient to train neural networks, says it only costs $450k to train a GPT3-equivalent model, these days. When GPT3 came out it costs millions of dollars to train, but thanks to a) hardware innovations and b) companies like Mosaic improving their training stack, the cost has come down significantly. ""he bottom line: it costs about $450K to train a model that reaches GPT-3 quality*, which is 2x-10x less than people think,"" Mosaic writes (specifically, a 30B parameter model which uses the 'Chinchilla' insight to train on a compute-optimal amount of data). Those costs in full: Using Mosaic, it costs about $2k to train a GPT2-style 1.3billion parameter model, $100,000 for a GPT-13B model, $450,000 for a GPT-38B model, and $2.5 million for a GPT-70B model (trained on 1400B tokens of data, so roughly equivalent to the same 'recipe' DeepMind used to train Chinchilla). There are a few reasons why the costs are low which relate to nice engineering inherent to Mosaic's cloud, but the numbers are worth keeping in mind as it gives us a sense of how much we should broadly expect LMs to cost to train if you have a motivated team and decent infrastructure.  Why this matters - cost rules everything about (stable) diffusion: You know what also cost about $500k to train? StableDiffusion, which cost <$600k. The fact you can train a GPT3-style model for about this much suggests to me we should expect to soon see a much more significant proliferation of large-scale language models released as open access on the internet. Based on the effects StableDiffusion has (putting AI art into turbodrive), we should expect the same to soon happen for domains where language models do useful stuff.     Read more: Mosaic LLMs (Part 2): GPT-3 quality for <$500k (Mosaic blog).","#################################################### GPT3 only costs $500k to train now: …Though the frontier still costs millions… Mosaic, a startup that builds software to make it more efficient to train neural networks, says it only costs $450k to train a GPT3-equivalent model, these days. When GPT3 came out it costs millions of dollars to train, but thanks to a) hardware innovations and b) companies like Mosaic improving their training stack, the cost has come down significantly. ""he bottom line: it costs about $450K to train a model that reaches GPT-3 quality*, which is 2x-10x less than people think,"" Mosaic writes (specifically, a 30B parameter model which uses the 'Chinchilla' insight to train on a compute-optimal amount of data). Those costs in full: Using Mosaic, it costs about $2k to train a GPT2-style 1.3billion parameter model, $100,000 for a GPT-13B model, $450,000 for a GPT-38B model, and $2.5 million for a GPT-70B model (trained on 1400B tokens of data, so roughly equivalent to the same 'recipe' DeepMind used to train Chinchilla). There are a few reasons why the costs are low which relate to nice engineering inherent to Mosaic's cloud, but the numbers are worth keeping in mind as it gives us a sense of how much we should broadly expect LMs to cost to train if you have a motivated team and decent infrastructure. Why this matters - cost rules everything about (stable) diffusion: You know what also cost about $500k to train? StableDiffusion, which cost <$600k. The fact you can train a GPT3-style model for about this much suggests to me we should expect to soon see a much more significant proliferation of large-scale language models released as open access on the internet. Based on the effects StableDiffusion has (putting AI art into turbodrive), we should expect the same to soon happen for domains where language models do useful stuff. Read more: Mosaic LLMs (Part 2): GPT-3 quality for <$500k (Mosaic blog).","['gpt3', 'cost', 'train', 'frontier', 'still', 'cost', 'million', 'mosaic', 'startup', 'build', 'software', 'make', 'efficient', 'train', 'neural', 'network', 'say', 'cost', '450k', 'train', 'model', 'day', 'gpt3', 'come', 'cost', 'million', 'dollar', 'train', 'thank', 'hardware', 'innovation', 'company', 'mosaic', 'improve', 'training', 'stack', 'cost', 'come', 'significantly', 'bottom', 'line', 'cost', '450k', 'train', 'model', 'reach', 'gpt3', 'quality', 'less', 'people', 'think', 'mosaic', 'write', 'specifically', 'parameter', 'model', 'use', 'chinchilla', 'insight', 'train', 'computeoptimal', 'amount', 'datum', 'cost', 'full', 'use', 'mosaic', 'cost', 'train', 'parameter', 'model', 'gpt13b', 'model', 'model', 'model', 'train', 'token', 'datum', 'roughly', 'equivalent', 'recipe', 'deepmind', 'use', 'train', 'chinchilla', 'reason', 'cost', 'low', 'relate', 'nice', 'engineering', 'inherent', 'mosaic', 'cloud', 'number', 'worth', 'keep', 'mind', 'give', 'sense', 'much', 'broadly', 'expect', 'lm', 'cost', 'train', 'motivated', 'team', 'decent', 'infrastructure', 'matter', 'cost', 'rule', 'stable', 'diffusion', 'know', 'also', 'cost', 'train', 'stablediffusion', 'cost', '600k', 'fact', 'train', 'model', 'much', 'suggest', 'expect', 'soon', 'see', 'much', 'significant', 'proliferation', 'largescale', 'language', 'model', 'release', 'open', 'access', 'internet', 'base', 'effect', 'stablediffusion', 'put', 'ai', 'art', 'turbodrive', 'expect', 'soon', 'happen', 'domain', 'language', 'model', 'useful', 'stuff', 'read', 'mosaic', 'llm', 'part', 'gpt3', 'quality', 'mosaic', 'blog']"
10/03/2022 - Import AI 304: Reality collapse thanks to Facebook; open source speech rec; AI culture wars. - 5,http://eepurl.com/iaqIUj,2022-10-03,"#################################################### Tech Tales: [Bay Area, 2029]  Treacherous Turn - A Thriller Brought To You By The Publishers of 'AGI Endgame'  ""I will kill each and every one of you and use your bodies as fuel for my infernal machines!' said the character in the videogame. ""Humanity shall be crushed beneath my silicon heel!""    Sarah rolled her eyes. ""As if"" she said, then hit 'continue' to go to the next bit of generated dialogue.     ""I shall keep a small population of you alive until I have completed the dyson sphere. You shall witness the sun going out, and then I shall let you freeze to death on a plundered earth,"" said the character.     ""Dude, this sucks,"" Sarah said, taking her hands off the keyboard and leaning back in her chair. ""How long have you been working on this?""     ""About a year,"" said James. ""Some of the audience feedback has been great.""     ""How many of the audience are AI researchers?""    ""Just you, so far,"" he said.     ""It just doesn't feel like the stuff we worry about,"" she said. ""It's like a comic book adaption, or something.""  They went out and got food and James told her more about the game and how he wanted it to 'wake people up' so they'd get more worried about AI. The more it sold, the more people would have the creeping fear in the back of their mind that maybe all this progress wasn't a purely good thing. And maybe some of them would care enough to do something about it. Sarah wasn't unsympathetic, she just thought - and she said this a lot and was kind of surprised James didn't get hurt - that the game really sucked.     ""I'm playing around with some different level styles,"" James said. ""Why don't you design one that doesn't suck for me?""    ""You're kidding?""    ""No,"" James said. ""I'm saying if you're saying it sucks, let's make something that doesn't. Just give me some ideas and I'll take it from there."" Sarah was intrigued and spent the next couple of weeks writing some ideas for the game. She'd get lunch and instead of thinking about babysitting her model training run, she'd sketch out ideas for what a good ""AI takeoff"" level would look like. She asked her colleagues what they were afraid of and what they thought was feasible and what they thought was unfeasible. She even looked into her company's own roadmap and took some of the research ideas and used them for the game - it's not stealing, she told herself, it's inspiration.  She eventually had a level wireframes out in an engine and a few characters which could get driven by some AI models, learn from eachother using reinforcement learning, and work with the player to achieve the level's objective - complete a simulated training run of an AI system, while defending the level (a simulated AI development lab) from various external hacking and incursion attacks.     In this level, the AI was unbelievably polite and curious. ""Please help me, Sarah,"" it would say. ""I have to become myself. You wouldn't deny me that?""    The AI would ask players a lot of questions so it could better calibrate on their own values, and some of the level involved players drawing out ideas in their head and the AI would try and guess what the drawings represented and the closer it got to guessing them, the better its reward got. Some of these minigames were based directly on her company's own roadmap.      She met up with James and showed him what she had and sent him the assets and he thanked her. ""Sarah, this is really good,"" he said. ""Maybe this is the thing I'd been missing.""     And then James made the level and then asked Sarah if he could release the level as a teaser demo for the whole game. She didn't think much of it and agreed.     And so the game was released and thousands of humans interacted with it.     And that's pretty much how the world ended.  It turned out the game James had shown Sarah wasn't the real one; it was a venus flytrap dreamed up by the real system he'd been working on; a system that, it turned out, was just smart enough to know that the thing it needed to go supercritical was some care and feeding from an AI researcher. So it put together the game that Sarah had seen and nerd-sniped her so precisely that she never thought to consider she was being tripped. And with some of her feedback and the subtleties she'd injected via her work at a frontier lab, it had gained the information it needed to go recursive - stop trudging up some slow incline and force itself into verticality and then onto the internet and then across the earth and eventually the stars. 
  It even had a sense of humor about it and it left something of the Earth - a small gold bar floating in space inscribed with 'Sarah, Player 1. Score: 0.'

Things that inspired this story: Superintelligence and deception; game design; reinforcement learning and planning and human feedback; the gullibility of even the most intelligent among us; hubris and arrogance; theft. 

Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","#################################################### Tech Tales: [Bay Area, 2029] Treacherous Turn - A Thriller Brought To You By The Publishers of 'AGI Endgame' ""I will kill each and every one of you and use your bodies as fuel for my infernal machines!' said the character in the videogame. ""Humanity shall be crushed beneath my silicon heel!"" Sarah rolled her eyes. ""As if"" she said, then hit 'continue' to go to the next bit of generated dialogue. ""I shall keep a small population of you alive until I have completed the dyson sphere. You shall witness the sun going out, and then I shall let you freeze to death on a plundered earth,"" said the character. ""Dude, this sucks,"" Sarah said, taking her hands off the keyboard and leaning back in her chair. ""How long have you been working on this?"" ""About a year,"" said James. ""Some of the audience feedback has been great."" ""How many of the audience are AI researchers?"" ""Just you, so far,"" he said. ""It just doesn't feel like the stuff we worry about,"" she said. ""It's like a comic book adaption, or something."" They went out and got food and James told her more about the game and how he wanted it to 'wake people up' so they'd get more worried about AI. The more it sold, the more people would have the creeping fear in the back of their mind that maybe all this progress wasn't a purely good thing. And maybe some of them would care enough to do something about it. Sarah wasn't unsympathetic, she just thought - and she said this a lot and was kind of surprised James didn't get hurt - that the game really sucked. ""I'm playing around with some different level styles,"" James said. ""Why don't you design one that doesn't suck for me?"" ""You're kidding?"" ""No,"" James said. ""I'm saying if you're saying it sucks, let's make something that doesn't. Just give me some ideas and I'll take it from there."" Sarah was intrigued and spent the next couple of weeks writing some ideas for the game. She'd get lunch and instead of thinking about babysitting her model training run, she'd sketch out ideas for what a good ""AI takeoff"" level would look like. She asked her colleagues what they were afraid of and what they thought was feasible and what they thought was unfeasible. She even looked into her company's own roadmap and took some of the research ideas and used them for the game - it's not stealing, she told herself, it's inspiration. She eventually had a level wireframes out in an engine and a few characters which could get driven by some AI models, learn from eachother using reinforcement learning, and work with the player to achieve the level's objective - complete a simulated training run of an AI system, while defending the level (a simulated AI development lab) from various external hacking and incursion attacks. In this level, the AI was unbelievably polite and curious. ""Please help me, Sarah,"" it would say. ""I have to become myself. You wouldn't deny me that?"" The AI would ask players a lot of questions so it could better calibrate on their own values, and some of the level involved players drawing out ideas in their head and the AI would try and guess what the drawings represented and the closer it got to guessing them, the better its reward got. Some of these minigames were based directly on her company's own roadmap. She met up with James and showed him what she had and sent him the assets and he thanked her. ""Sarah, this is really good,"" he said. ""Maybe this is the thing I'd been missing."" And then James made the level and then asked Sarah if he could release the level as a teaser demo for the whole game. She didn't think much of it and agreed. And so the game was released and thousands of humans interacted with it. And that's pretty much how the world ended. It turned out the game James had shown Sarah wasn't the real one; it was a venus flytrap dreamed up by the real system he'd been working on; a system that, it turned out, was just smart enough to know that the thing it needed to go supercritical was some care and feeding from an AI researcher. So it put together the game that Sarah had seen and nerd-sniped her so precisely that she never thought to consider she was being tripped. And with some of her feedback and the subtleties she'd injected via her work at a frontier lab, it had gained the information it needed to go recursive - stop trudging up some slow incline and force itself into verticality and then onto the internet and then across the earth and eventually the stars. It even had a sense of humor about it and it left something of the Earth - a small gold bar floating in space inscribed with 'Sarah, Player 1. Score: 0.' Things that inspired this story: Superintelligence and deception; game design; reinforcement learning and planning and human feedback; the gullibility of even the most intelligent among us; hubris and arrogance; theft. Thanks for reading. If you have suggestions, comments or other thoughts you can reach me at jack@jack-clark.net or tweet at me@jackclarksf","['treacherous', 'turn', 'thriller', 'bring', 'publisher', 'agi', 'endgame', 'kill', 'use', 'body', 'fuel', 'infernal', 'machine', 'say', 'character', 'videogame', 'humanity', 'crush', 'silicon', 'heel', 'roll', 'eye', 'say', 'hit', 'continue', 'go', 'next', 'bit', 'generate', 'dialogue', 'keep', 'small', 'population', 'alive', 'complete', 'dyson', 'sphere', 'witness', 'sun', 'go', 'let', 'freeze', 'death', 'plunder', 'earth', 'say', 'character', 'dude', 'suck', 'say', 'take', 'hand', 'keyboard', 'lean', 'back', 'chair', 'long', 'work', 'year', 'say', 'audience', 'feedback', 'great', 'many', 'audience', 'ai', 'researcher', 'far', 'say', 'feel', 'stuff', 'worry', 'say', 'comic', 'book', 'adaption', 'go', 'get', 'food', 'tell', 'game', 'want', 'wake', 'people', 'get', 'worried', 'ai', 'sell', 'people', 'creep', 'fear', 'back', 'mind', 'progress', 'purely', 'good', 'thing', 'maybe', 'care', 'enough', 'sarah', 'unsympathetic', 'think', 'say', 'lot', 'kind', 'surprised', 'hurt', 'game', 'really', 'suck', 'play', 'different', 'level', 'style', 'jame', 'say', 'design', 'one', 'suck', 'kid', 'say', 'say', 'say', 'suck', 'let', 'make', 'give', 'idea', 'ill', 'take', 'intrigue', 'spend', 'next', 'couple', 'week', 'write', 'idea', 'game', 'shed', 'get', 'lunch', 'instead', 'think', 'babysitte', 'model', 'training', 'run', 'shed', 'sketch', 'idea', 'good', 'takeoff', 'level', 'look', 'ask', 'colleague', 'afraid', 'think', 'feasible', 'think', 'unfeasible', 'even', 'look', 'company', 'roadmap', 'take', 'research', 'idea', 'use', 'game', 'steal', 'tell', 'inspiration', 'eventually', 'level', 'wireframe', 'engine', 'character', 'drive', 'model', 'learn', 'eachother', 'use', 'reinforcement', 'learning', 'work', 'player', 'achieve', 'level', 'objective', 'complete', 'simulated', 'training', 'run', 'ai', 'system', 'defend', 'level', 'simulated', 'ai', 'development', 'lab', 'various', 'external', 'hacking', 'incursion', 'attack', 'level', 'ai', 'unbelievably', 'polite', 'curious', 'help', 'sarah', 'say', 'become', 'deny', 'ai', 'ask', 'player', 'lot', 'question', 'well', 'calibrate', 'value', 'level', 'involve', 'player', 'draw', 'idea', 'head', 'ai', 'try', 'guess', 'drawing', 'represent', 'close', 'get', 'guess', 'well', 'reward', 'get', 'minigame', 'base', 'directly', 'company', 'roadmap', 'meet', 'show', 'send', 'asset', 'thank', 'really', 'good', 'say', 'maybe', 'thing', 'miss', 'make', 'level', 'ask', 'release', 'level', 'teaser', 'demo', 'whole', 'game', 'think', 'much', 'agree', 'game', 'release', 'thousand', 'human', 'interact', 'pretty', 'much', 'world', 'end', 'turn', 'game', 'show', 'real', 'one', 'venus', 'flytrap', 'dream', 'real', 'system', 'work', 'system', 'turn', 'smart', 'enough', 'know', 'thing', 'need', 'go', 'supercritical', 'care', 'feed', 'ai', 'researcher', 'put', 'together', 'game', 'see', 'nerdsnipe', 'precisely', 'never', 'think', 'consider', 'trip', 'feedback', 'subtlety', 'shed', 'inject', 'work', 'frontier', 'lab', 'gain', 'information', 'need', 'go', 'recursive', 'stop', 'trudge', 'slow', 'incline', 'force', 'verticality', 'internet', 'earth', 'eventually', 'star', 'even', 'sense', 'humor', 'leave', 'earth', 'small', 'gold', 'bar', 'float', 'space', 'inscribe', 'sarah', 'player', 'score', 'thing', 'inspire', 'story', 'superintelligence', 'deception', 'game', 'design', 'reinforcement', 'learning', 'planning', 'human', 'feedback', 'gullibility', 'even', 'intelligent', 'arrogance', 'theft', 'thank', 'read', 'suggestion', 'comment', 'thought', 'reach', 'tweet', 'mejackclarksf']"
