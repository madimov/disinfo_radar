title,url,date,summary,text,cleaning,tokens
Sony’s racing AI destroyed its human competitors by being nice (and fast),https://www.technologyreview.com/2022/07/19/1056176/sonys-racing-ai-destroyed-its-human-competitors-by-being-nice-and-fast/,2022-07-19,"<p>What Gran Turismo Sophy learned on the racetrack could help shape the future of machines that can work alongside humans, or join us on the roads.</p>
","“Wait, what? How?” Emily Jones wasn’t used to being left behind. A top sim-racing driver with multiple wins to her name, Jones jerked the steering wheel in the e-sports rig, eyes fixed on the screen in front of her: “I’m pushing way too hard to keep up— How does it do that?” Her staccato commentary intercut with squealing tires, Jones flung her virtual car around the virtual track at 120 miles per hour—then 140, 150—chasing the fastest Gran Turismo driver in the world. Built by Sony AI, a research lab launched by the company in 2020, Gran Turismo Sophy is a computer program trained to control racing cars inside the world of Gran Turismo, a video game known for its super-realistic simulations of real vehicles and tracks. In a series of events held behind closed doors last year, Sony put its program up against the best humans on the professional sim-racing circuit.  What they discovered during those racetrack battles—and the ones that followed—could help shape the future of machines that work alongside humans, or join us on the roads.  Back in July 2021, Jones, who is based in Melbourne, Australia, and races for the e-sports team Trans Tasman Racing, didn’t know what to expect. “I wasn’t told much about it,” she says now, a year later. “‘Don’t do any practice,’ they said. ‘Don’t look at its lap times.’ I was like, it’s obviously going to be good if they’re keeping it secret from me.” In the end, GT Sophy beat Jones’s best lap by 1.5 seconds. At a level where records are smashed in millisecond increments, 1.5 seconds is an age.   But Sony soon learned that speed alone wasn’t enough to make GT Sophy a winner. The program outpaced all human drivers on an empty track, setting superhuman lap times on three different virtual courses. Yet when Sony tested GT Sophy in a race against multiple human drivers, where intelligence as well as speed is needed, GT Sophy lost. The program was at times too aggressive, racking up penalties for reckless driving, and at other times too timid, giving way when it didn’t need to. Sony regrouped, retrained its AI, and set up a rematch in October. This time GT Sophy won with ease. What made the difference? It’s true that Sony came back with a larger neural network, giving its program more capabilities to draw from on the fly. But ultimately, the difference came down to giving GT Sophy something that Peter Wurman, head of Sony AI America, calls “etiquette”: the ability to balance its aggression and timidity, picking the most appropriate behavior for the situation at hand. This is also what makes GT Sophy relevant beyond Gran Turismo. Etiquette between drivers on a track is a specific example of the kind of dynamic, context-aware behavior that robots will be expected to have when they interact with people, says Wurman. An awareness of when to take risks and when to play it safe would be useful for AI that is better at interacting with people, whether it be on the manufacturing floor, in home robots, or in driverless cars.  “I don’t think we’ve learned general principles yet about how to deal with human norms that you have to respect,” says Wurman. “But it’s a start and hopefully gives us some insight into this problem in general.” GT Sophy is just the latest in a line of AI systems that have beaten the world’s best human players at various games, from chess and Go to video games like Starcraft and DOTA. But Gran Turismo offered Sony a new kind of challenge. Unlike other games, especially those that are turn-based, Gran Turismo calls on its best players to control a vehicle at the limits of what’s physically possible, in real time, and in close proximity with other players all trying to do the same. Cars hurtle around corners at more than 100 miles per hour with only inches between them. At those speeds, the smallest errors can lead to a crash. Gran Turismo captures real-world physics in extreme detail, simulating the aerodynamics of a car and the friction of its tires on the track. The game is sometimes used to train and recruit drivers for real-world racing. “It does an excellent job with the realism,” says Davide Scaramuzza, who leads the robotics and perception group at the University of Zurich in Switzerland. Scaramuzza was not involved with GT Sophy, but his team has used Gran Turismo to train a previous AI driver—though not one that was ever tested against humans.  Virtual cycling races boomed during the pandemic. Now the best riders are getting ready for the race of their lives. GT Sophy doesn’t get the same view of the game that human players do. Instead of reading pixels off a screen, the program takes in updates about the position of its car on the track and the positions of the cars around it. It also gets sent information about the virtual physical forces affecting its vehicle. In response, GT Sophy tells the car to turn or brake. This back-and-forth between GT Sophy and the game happens 10 times a second, which Wurman and his colleagues claim matches the reaction time of human players. Sony used reinforcement learning to train GT Sophy from scratch via trial and error. At first the AI struggled to keep a car on the road. But after training on 10 PlayStation 4s, each running 20 instances of the program, GT Sophy matched Gran Turismo’s built-in AI, which amateur players use for practice, in around eight hours. In 24 hours it was laying down lap times near the very top of an online leaderboard of 17,700 human players. It took nine days before GT Sophy stopped shaving fractions of a second off its lap times. By then it was faster than any human. Sony’s AI learned how to drive at the limits of what the game allowed, pulling off moves that human players can only gawk at. In particular, Jones was struck by the way GT Sophy took corners, braking early before accelerating out on a much tighter line than she was. “It used the curve in a weird way, doing stuff that I just didn’t even think of,” she says. For example, GT Sophy often drops a wheel onto the grass at the edge of the track and then skids into turns. “You don’t want to do that because you’ll make a mistake. It’s like a controlled crash,” she says. “I could maybe do that one in a hundred times.” GT Sophy was quick to master the game’s physics. The bigger problem was the referees. At a professional level, Gran Turismo races are watched by human judges, who can award penalty points for dangerous driving. Racking up penalties was a key reason for GT Sophy’s loss in the first round of races last July, even though it was faster than any of the human drivers. And learning to avoid them made all the difference in round two.   Wurman has been working on GT Sophy for several years. There’s a painting of two cars jostling for position hanging on the wall behind his desk. “It’s a GT Sophy car passing Yamanaka,” says Wurman, referring to Tomoaki Yamanaka, one of the four Japanese professional sim-racing drivers who competed against GT Sophy last year. Wurman can’t recall which race the painting is taken from. If it’s the October event, Yamanaka may well be having a great time, pushing himself against a tough but fair opponent. If it’s the July event, he’s probably cussing at the computer.   Yamanaka’s teammate Takuma Miyazono told me about that July race via a translator. “There were a few times where we were pushed off the track because of how aggressively it would go into the corners,” he said. “That threw us off. The human drivers had to hold back on the turns to avoid being run off the road.” Training the AI to play fair without losing its competitive edge was hard, says Wurman. The human referees make subjective judgments that depend on context, making it difficult to turn them into simple dos and don’ts that the AI can learn from.   The Sony researchers tried giving the AI lots of different cues, adjusting them as they went, hoping to find a mix that worked. They tried penalizing it if it went off the track or bumped into wall. They penalized it for crashes it caused, and for crashes where a referee’s call might go either way. They experimented with different-size penalties for each and checked how GT Sophy’s driving changed in response. Sony also upped the competition GT Sophy faced in its training. Before, it had trained mostly against previous versions of itself. Leading into the October rematch, Sony tested its AI every week or two against top drivers, tweaking it constantly. “That gave us the kind of feedback we needed to find the right balance between aggression and timidity,” Wurman says. It worked. When Miyazono went up against GT Sophy three months later, the aggression was gone—but the AI was not simply backing down. “When you go into a corner with two cars side by side, it leaves just enough space for your car to go through,” he told me. “It really does feel like you’re racing with another person.” “You get a different sort of passion and fun from driving against something that reacts that way,” he added. “That was something that really left a big impression on my mind.” Scaramuzza is impressed with Sony’s work. “We measure the progress of robotics against what humans can do,” he says. But Elia Kaufman, who works with Scaramuzza at the University of Zurich, points out that it is still human researchers who choose which of GT Sophy’s learned behaviors to bake in during training. “They’re the ones who judge what is good racing etiquette or not,” he says. “It would be really interesting if that could be done in an automated way.” Such a machine would not only have good manners but could recognize what good manners were, and be able to adapt its behavior to new settings. Scaramuzza’s team is now applying its Gran Turismo research to real-world drone racing, training an AI to fly using raw video input instead of data from a simulation. Last month they invited two world-champion drone racers to take on the computer. No prizes for guessing who won. “It was very interesting to look at their faces after they saw our AI racing,” says Scaramuzza. “They were mind-blown.”  Scaramuzza thinks that making the jump to the real world is essential for true progress in robotics. “There will always be a mismatch between simulation and the real world,” he says. “This is something that gets forgotten when people talk about AI making incredible progress. In terms of strategy, yes. In terms of real-world deployment, we are definitely not there yet.” For now, Sony is sticking to games. It plans to put GT Sophy in a future version of Gran Turismo. “We’d like this to become part of the product,” says Peter Stone, executive director of Sony AI America. “Sony’s an entertainment company, and we want this to make the game more entertaining.” Jones thinks the sim-racing community could learn a lot from GT Sophy once more people get a chance to see it drive. “There will be tracks where we’re like, hang on a second, we’ve been doing this for years but there’s actually a faster way of doing it.” Miyazono has already tried to copy some of the lines the AI takes around corners, now that it has shown him they can be done. “If the benchmark changes, everybody rises up as well,” says Jones. ","“Wait, what? How?” Emily Jones wasn’t used to being left behind. A top sim-racing driver with multiple wins to her name, Jones jerked the steering wheel in the e-sports rig, eyes fixed on the screen in front of her: “I’m pushing way too hard to keep up— How does it do that?” Her staccato commentary intercut with squealing tires, Jones flung her virtual car around the virtual track at 120 miles per hour—then 140, 150—chasing the fastest Gran Turismo driver in the world. Built by Sony AI, a research lab launched by the company in 2020, Gran Turismo Sophy is a computer program trained to control racing cars inside the world of Gran Turismo, a video game known for its super-realistic simulations of real vehicles and tracks. In a series of events held behind closed doors last year, Sony put its program up against the best humans on the professional sim-racing circuit. What they discovered during those racetrack battles—and the ones that followed—could help shape the future of machines that work alongside humans, or join us on the roads. Back in July 2021, Jones, who is based in Melbourne, Australia, and races for the e-sports team Trans Tasman Racing, didn’t know what to expect. “I wasn’t told much about it,” she says now, a year later. “‘Don’t do any practice,’ they said. ‘Don’t look at its lap times.’ I was like, it’s obviously going to be good if they’re keeping it secret from me.” In the end, GT Sophy beat Jones’s best lap by 1.5 seconds. At a level where records are smashed in millisecond increments, 1.5 seconds is an age. But Sony soon learned that speed alone wasn’t enough to make GT Sophy a winner. The program outpaced all human drivers on an empty track, setting superhuman lap times on three different virtual courses. Yet when Sony tested GT Sophy in a race against multiple human drivers, where intelligence as well as speed is needed, GT Sophy lost. The program was at times too aggressive, racking up penalties for reckless driving, and at other times too timid, giving way when it didn’t need to. Sony regrouped, retrained its AI, and set up a rematch in October. This time GT Sophy won with ease. What made the difference? It’s true that Sony came back with a larger neural network, giving its program more capabilities to draw from on the fly. But ultimately, the difference came down to giving GT Sophy something that Peter Wurman, head of Sony AI America, calls “etiquette”: the ability to balance its aggression and timidity, picking the most appropriate behavior for the situation at hand. This is also what makes GT Sophy relevant beyond Gran Turismo. Etiquette between drivers on a track is a specific example of the kind of dynamic, context-aware behavior that robots will be expected to have when they interact with people, says Wurman. An awareness of when to take risks and when to play it safe would be useful for AI that is better at interacting with people, whether it be on the manufacturing floor, in home robots, or in driverless cars. “I don’t think we’ve learned general principles yet about how to deal with human norms that you have to respect,” says Wurman. “But it’s a start and hopefully gives us some insight into this problem in general.” GT Sophy is just the latest in a line of AI systems that have beaten the world’s best human players at various games, from chess and Go to video games like Starcraft and DOTA. But Gran Turismo offered Sony a new kind of challenge. Unlike other games, especially those that are turn-based, Gran Turismo calls on its best players to control a vehicle at the limits of what’s physically possible, in real time, and in close proximity with other players all trying to do the same. Cars hurtle around corners at more than 100 miles per hour with only inches between them. At those speeds, the smallest errors can lead to a crash. Gran Turismo captures real-world physics in extreme detail, simulating the aerodynamics of a car and the friction of its tires on the track. The game is sometimes used to train and recruit drivers for real-world racing. “It does an excellent job with the realism,” says Davide Scaramuzza, who leads the robotics and perception group at the University of Zurich in Switzerland. Scaramuzza was not involved with GT Sophy, but his team has used Gran Turismo to train a previous AI driver—though not one that was ever tested against humans. Virtual cycling races boomed during the pandemic. Now the best riders are getting ready for the race of their lives. GT Sophy doesn’t get the same view of the game that human players do. Instead of reading pixels off a screen, the program takes in updates about the position of its car on the track and the positions of the cars around it. It also gets sent information about the virtual physical forces affecting its vehicle. In response, GT Sophy tells the car to turn or brake. This back-and-forth between GT Sophy and the game happens 10 times a second, which Wurman and his colleagues claim matches the reaction time of human players. Sony used reinforcement learning to train GT Sophy from scratch via trial and error. At first the AI struggled to keep a car on the road. But after training on 10 PlayStation 4s, each running 20 instances of the program, GT Sophy matched Gran Turismo’s built-in AI, which amateur players use for practice, in around eight hours. In 24 hours it was laying down lap times near the very top of an online leaderboard of 17,700 human players. It took nine days before GT Sophy stopped shaving fractions of a second off its lap times. By then it was faster than any human. Sony’s AI learned how to drive at the limits of what the game allowed, pulling off moves that human players can only gawk at. In particular, Jones was struck by the way GT Sophy took corners, braking early before accelerating out on a much tighter line than she was. “It used the curve in a weird way, doing stuff that I just didn’t even think of,” she says. For example, GT Sophy often drops a wheel onto the grass at the edge of the track and then skids into turns. “You don’t want to do that because you’ll make a mistake. It’s like a controlled crash,” she says. “I could maybe do that one in a hundred times.” GT Sophy was quick to master the game’s physics. The bigger problem was the referees. At a professional level, Gran Turismo races are watched by human judges, who can award penalty points for dangerous driving. Racking up penalties was a key reason for GT Sophy’s loss in the first round of races last July, even though it was faster than any of the human drivers. And learning to avoid them made all the difference in round two. Wurman has been working on GT Sophy for several years. There’s a painting of two cars jostling for position hanging on the wall behind his desk. “It’s a GT Sophy car passing Yamanaka,” says Wurman, referring to Tomoaki Yamanaka, one of the four Japanese professional sim-racing drivers who competed against GT Sophy last year. Wurman can’t recall which race the painting is taken from. If it’s the October event, Yamanaka may well be having a great time, pushing himself against a tough but fair opponent. If it’s the July event, he’s probably cussing at the computer. Yamanaka’s teammate Takuma Miyazono told me about that July race via a translator. “There were a few times where we were pushed off the track because of how aggressively it would go into the corners,” he said. “That threw us off. The human drivers had to hold back on the turns to avoid being run off the road.” Training the AI to play fair without losing its competitive edge was hard, says Wurman. The human referees make subjective judgments that depend on context, making it difficult to turn them into simple dos and don’ts that the AI can learn from. The Sony researchers tried giving the AI lots of different cues, adjusting them as they went, hoping to find a mix that worked. They tried penalizing it if it went off the track or bumped into wall. They penalized it for crashes it caused, and for crashes where a referee’s call might go either way. They experimented with different-size penalties for each and checked how GT Sophy’s driving changed in response. Sony also upped the competition GT Sophy faced in its training. Before, it had trained mostly against previous versions of itself. Leading into the October rematch, Sony tested its AI every week or two against top drivers, tweaking it constantly. “That gave us the kind of feedback we needed to find the right balance between aggression and timidity,” Wurman says. It worked. When Miyazono went up against GT Sophy three months later, the aggression was gone—but the AI was not simply backing down. “When you go into a corner with two cars side by side, it leaves just enough space for your car to go through,” he told me. “It really does feel like you’re racing with another person.” “You get a different sort of passion and fun from driving against something that reacts that way,” he added. “That was something that really left a big impression on my mind.” Scaramuzza is impressed with Sony’s work. “We measure the progress of robotics against what humans can do,” he says. But Elia Kaufman, who works with Scaramuzza at the University of Zurich, points out that it is still human researchers who choose which of GT Sophy’s learned behaviors to bake in during training. “They’re the ones who judge what is good racing etiquette or not,” he says. “It would be really interesting if that could be done in an automated way.” Such a machine would not only have good manners but could recognize what good manners were, and be able to adapt its behavior to new settings. Scaramuzza’s team is now applying its Gran Turismo research to real-world drone racing, training an AI to fly using raw video input instead of data from a simulation. Last month they invited two world-champion drone racers to take on the computer. No prizes for guessing who won. “It was very interesting to look at their faces after they saw our AI racing,” says Scaramuzza. “They were mind-blown.” Scaramuzza thinks that making the jump to the real world is essential for true progress in robotics. “There will always be a mismatch between simulation and the real world,” he says. “This is something that gets forgotten when people talk about AI making incredible progress. In terms of strategy, yes. In terms of real-world deployment, we are definitely not there yet.” For now, Sony is sticking to games. It plans to put GT Sophy in a future version of Gran Turismo. “We’d like this to become part of the product,” says Peter Stone, executive director of Sony AI America. “Sony’s an entertainment company, and we want this to make the game more entertaining.” Jones thinks the sim-racing community could learn a lot from GT Sophy once more people get a chance to see it drive. “There will be tracks where we’re like, hang on a second, we’ve been doing this for years but there’s actually a faster way of doing it.” Miyazono has already tried to copy some of the lines the AI takes around corners, now that it has shown him they can be done. “If the benchmark changes, everybody rises up as well,” says Jones.","['wait', 'use', 'leave', 'top', 'simracing', 'driver', 'multiple', 'win', 'name', 'jerk', 'steering', 'wheel', 'esport', 'rig', 'eye', 'fix', 'screen', 'front', 'push', 'way', 'hard', 'keep', 'staccato', 'commentary', 'intercut', 'squeal', 'tire', 'fling', 'virtual', 'car', 'virtual', 'track', 'mile', 'hour', 'chase', 'fast', 'gran', 'turismo', 'driver', 'world', 'build', 'ai', 'research', 'lab', 'launch', 'company', 'gran', 'turismo', 'sophy', 'computer', 'program', 'train', 'control', 'race', 'car', 'world', 'gran', 'turismo', 'video', 'game', 'know', 'superrealistic', 'simulation', 'real', 'vehicle', 'track', 'series', 'event', 'hold', 'closed', 'door', 'last', 'year', 'put', 'program', 'good', 'human', 'professional', 'simracing', 'circuit', 'discover', 'racetrack', 'battle', 'one', 'follow', 'help', 'shape', 'future', 'machine', 'work', 'human', 'join', 'road', 'back', 'base', 'melbourne', 'australia', 'race', 'esport', 'team', 'racing', 'know', 'expect', 'tell', 'much', 'say', 'year', 'later', 'practice', 'say', 'look', 'lap', 'time', 'obviously', 'go', 'good', 'keep', 'secret', 'end', 'good', 'lap', 'second', 'level', 'record', 'smash', 'increment', 'second', 'age', 'soon', 'learn', 'speed', 'alone', 'enough', 'make', 'sophy', 'winner', 'program', 'outpace', 'human', 'driver', 'empty', 'track', 'set', 'superhuman', 'lap', 'time', 'different', 'virtual', 'course', 'yet', 'test', 'sophy', 'race', 'multiple', 'human', 'driver', 'intelligence', 'well', 'speed', 'need', 'sophy', 'lose', 'program', 'time', 'aggressive', 'racking', 'penalty', 'reckless', 'driving', 'time', 'timid', 'giving', 'way', 'need', 'regroup', 'retrain', 'ai', 'set', 'rematch', 'time', 'sophy', 'win', 'ease', 'make', 'difference', '’', 'true', 'come', 'back', 'large', 'neural', 'network', 'give', 'program', 'capability', 'draw', 'fly', 'ultimately', 'difference', 'come', 'give', 'gt', 'sophy', 'call', 'etiquette', 'ability', 'balance', 'aggression', 'timidity', 'pick', 'appropriate', 'behavior', 'situation', 'hand', 'also', 'make', 'gt', 'sophy', 'relevant', 'gran', 'turismo', 'etiquette', 'driver', 'track', 'specific', 'example', 'kind', 'dynamic', 'contextaware', 'behavior', 'robot', 'expect', 'interact', 'people', 'say', 'awareness', 'take', 'risk', 'play', 'safe', 'useful', 'well', 'interact', 'people', 'manufacturing', 'floor', 'home', 'robot', 'driverless', 'car', 'think', 'learn', 'general', 'principle', 'yet', 'deal', 'human', 'norm', 'respect', 'say', '’', 'start', 'hopefully', 'give', 'insight', 'problem', 'general', 'sophy', 'late', 'line', 'system', 'beat', 'world', 'good', 'human', 'player', 'various', 'game', 'chess', 'go', 'video', 'game', 'starcraft', 'dota', 'gran', 'turismo', 'offer', 'new', 'kind', 'challenge', 'game', 'especially', 'turnbase', 'gran', 'turismo', 'call', 'good', 'player', 'control', 'vehicle', 'limit', '’s', 'physically', 'possible', 'real', 'time', 'close', 'proximity', 'try', 'car', 'hurtle', 'corner', 'mile', 'hour', 'inch', 'speed', 'small', 'error', 'lead', 'crash', 'gran', 'turismo', 'capture', 'physics', 'extreme', 'detail', 'simulate', 'aerodynamic', 'car', 'friction', 'tire', 'track', 'game', 'sometimes', 'use', 'train', 'recruit', 'driver', 'realworld', 'race', 'excellent', 'job', 'realism', 'say', 'davide', 'scaramuzza', 'lead', 'robotic', 'perception', 'group', 'scaramuzza', 'involve', 'sophy', 'team', 'use', 'gran', 'turismo', 'train', 'previous', 'ai', 'driver', 'ever', 'test', 'human', 'virtual', 'cycling', 'race', 'boom', 'pandemic', 'good', 'rider', 'get', 'ready', 'race', 'life', 'gt', 'sophy', 'get', 'view', 'game', 'human', 'player', 'instead', 'read', 'pixel', 'screen', 'program', 'take', 'update', 'position', 'car', 'track', 'position', 'car', 'also', 'send', 'information', 'virtual', 'physical', 'force', 'affect', 'vehicle', 'response', 'sophy', 'tell', 'car', 'turn', 'brake', 'backandforth', 'sophy', 'game', 'happen', 'time', 'second', 'wurman', 'colleague', 'claim', 'match', 'reaction', 'time', 'human', 'player', 'use', 'reinforcement', 'learn', 'train', 'gt', 'sophy', 'scratch', 'trial', 'error', 'first', 'ai', 'struggle', 'keep', 'car', 'road', 'training', 'playstation', 'run', 'instance', 'program', 'gt', 'sophy', 'match', 'gran', '’s', 'builtin', 'amateur', 'player', 'use', 'practice', 'around', 'hour', 'hour', 'lay', 'lap', 'time', 'top', 'online', 'leaderboard', 'human', 'player', 'take', 'day', 'sophy', 'stop', 'shaving', 'fraction', 'second', 'lap', 'time', 'fast', 'human', 'learn', 'drive', 'limit', 'game', 'allow', 'pull', 'move', 'human', 'player', 'gawk', 'particular', 'strike', 'way', 'sophy', 'take', 'corner', 'brake', 'early', 'accelerate', 'much', 'tight', 'line', 'use', 'curve', 'weird', 'way', 'stuff', 'even', 'think', 'say', 'example', 'gt', 'sophy', 'often', 'drop', 'wheel', 'grass', 'edge', 'track', 'skid', 'turn', 'want', 'make', 'mistake', '’', 'control', 'crash', 'say', 'maybe', 'time', 'sophy', 'quick', 'master', 'game', 'physics', 'big', 'problem', 'referee', 'professional', 'level', 'gran', 'turismo', 'race', 'watch', 'human', 'judge', 'award', 'penalty', 'point', 'dangerous', 'driving', 'rack', 'penalty', 'key', 'reason', 'loss', 'first', 'round', 'race', 'last', 'even', 'fast', 'human', 'driver', 'learn', 'avoid', 'make', 'difference', 'round', 'wurman', 'work', 'sophy', 'several', 'year', '’', 'painting', 'car', 'jostle', 'position', 'hang', 'wall', 'desk', '’', 'gt', 'sophy', 'car', 'pass', 'say', 'wurman', 'refer', 'japanese', 'professional', 'simracing', 'driver', 'compete', 'sophy', 'last', 'year', 'wurman', 'recall', 'race', 'painting', 'take', '’', 'event', 'well', 'great', 'time', 'push', 'tough', 'fair', 'opponent', '’', 'event', 'probably', 'cuss', 'computer', 'tell', 'race', 'translator', 'time', 'push', 'track', 'aggressively', 'go', 'corner', 'say', 'throw', 'human', 'driver', 'hold', 'turn', 'avoid', 'run', 'road', 'training', 'ai', 'play', 'fair', 'lose', 'competitive', 'edge', 'hard', 'say', 'human', 'referee', 'make', 'subjective', 'judgment', 'depend', 'context', 'make', 'difficult', 'turn', 'simple', 'don’ts', 'ai', 'learn', 'researcher', 'try', 'give', 'ai', 'lot', 'different', 'cue', 'adjust', 'hope', 'find', 'mix', 'work', 'try', 'penalize', 'go', 'track', 'bump', 'wall', 'penalize', 'crash', 'cause', 'crash', 'referee', '’s', 'call', 'go', 'way', 'experiment', 'differentsize', 'penalty', 'check', 'sophy', '’s', 'driving', 'change', 'response', 'also', 'competition', 'gt', 'sophy', 'face', 'training', 'train', 'mostly', 'previous', 'version', 'lead', 'test', 'ai', 'week', 'top', 'driver', 'tweak', 'constantly', 'give', 'kind', 'feedback', 'need', 'find', 'right', 'balance', 'aggression', 'timidity', 'wurman', 'say', 'work', 'go', 'sophy', 'month', 'later', 'aggression', 'go', 'ai', 'simply', 'back', 'go', 'corner', 'car', 'side', 'side', 'leave', 'enough', 'space', 'car', 'go', 'tell', 'really', 'feel', 'race', 'person', 'get', 'different', 'sort', 'passion', 'fun', 'drive', 'react', 'way', 'add', 'really', 'leave', 'big', 'impression', 'mind', 'scaramuzza', 'impressed', 'work', 'measure', 'progress', 'robotic', 'human', 'say', 'kaufman', 'work', 'scaramuzza', 'point', 'still', 'human', 'researcher', 'choose', 'learn', 'behavior', 'bake', 'training', '’re', 'one', 'judge', 'good', 'racing', 'etiquette', 'say', 'really', 'interesting', 'automate', 'way', 'machine', 'good', 'manner', 'recognize', 'good', 'manner', 'able', 'adapt', 'behavior', 'new', 'setting', 'scaramuzza', 'team', 'apply', 'gran', 'turismo', 'research', 'drone', 'racing', 'training', 'ai', 'fly', 'use', 'raw', 'video', 'input', 'instead', 'datum', 'simulation', 'last', 'month', 'invite', 'worldchampion', 'drone', 'racer', 'take', 'computer', 'prize', 'guess', 'win', 'interesting', 'look', 'face', 'see', 'racing', 'say', 'scaramuzza', 'mindblown', 'scaramuzza', 'think', 'make', 'jump', 'real', 'world', 'essential', 'true', 'progress', 'robotic', 'always', 'mismatch', 'simulation', 'real', 'world', 'say', 'forget', 'people', 'talk', 'make', 'incredible', 'progress', 'term', 'strategy', 'term', 'realworld', 'deployment', 'definitely', 'yet', 'stick', 'game', 'plan', 'put', 'gt', 'sophy', 'future', 'version', 'gran', 'turismo', '’d', 'become', 'part', 'product', 'say', 'executive', 'director', 'entertainment', 'company', 'want', 'make', 'game', 'entertaining', 'think', 'simrace', 'community', 'learn', 'lot', 'sophy', 'people', 'get', 'chance', 'see', 'drive', 'track', '’re', 'hang', 'second', 'year', '’', 'actually', 'fast', 'way', 'miyazono', 'already', 'try', 'copy', 'line', 'ai', 'take', 'corner', 'show', 'benchmark', 'change', 'rise', 'well', 'say']"
This robot dog just taught itself to walk,https://www.technologyreview.com/2022/07/18/1056059/robot-dog-ai-reinforcement/,2022-07-18,"<p>AI could help robots learn new skills and adapt to the real world quickly.</p>
","The robot dog is waving its legs in the air like an exasperated beetle. After 10 minutes of struggling, it manages to roll over to its front. Half an hour in, the robot is taking its first clumsy steps, like a newborn calf. But after one hour, the robot is strutting around the lab with confidence.  What makes this four-legged robot special is that it learned to do all this by itself, without being shown what to do in a computer simulation.  Danijar Hafner and colleagues at the University of California, Berkeley, used an AI technique called reinforcement learning, which trains algorithms by rewarding them for desired actions, to train the robot to walk from scratch in the real world. The team used the same algorithm to successfully train three other robots, such as one that was able to pick up balls and move them from one tray to another. Traditionally, robots are trained in a computer simulator before they attempt to do anything in the real world. For example, a pair of robot legs called Cassie taught itself to walk using reinforcement learning, but only after it had done so in a simulation.  “The problem is your simulator will never be as accurate as the real world. There’ll always be aspects of the world you’re missing,” says Hafner, who worked with colleagues Alejandro Escontrela and Philipp Wu on the project and is now an intern at DeepMind. Adapting lessons from the simulator to the real world also requires extra engineering, he says.  Within a few years, any task that previously required hands to perform could be partially or fully automated away. The team’s algorithm, called Dreamer, uses past experiences to build up a model of the surrounding world. Dreamer also allows the robot to conduct trial-and-error calculations in a computer program as opposed to the real world, by predicting potential future outcomes of its potential actions. This allows it to learn faster than it could purely by doing. Once the robot had learned to walk, it kept learning to adapt to unexpected situations, such as resisting being toppled by a stick.  “Teaching robots through trial and error is a difficult problem, made even harder by the long training times such teaching requires,” says Lerrel Pinto, an assistant professor of computer science at New York University, who specializes in robotics and machine learning. Dreamer shows that deep reinforcement learning and world models are able to teach robots new skills in a really short amount of time, he says.  Jonathan Hurst, a professor of robotics at Oregon State University, says the findings, which have not yet been peer-reviewed, make it clear that “reinforcement learning will be a cornerstone tool in the future of robot control.” Removing the simulator from robot training has many perks. The algorithm could be useful for teaching robots how to learn skills in the real world and adapt to situations like hardware failures, Hafner says–for example, a robot could learn to walk with a malfunctioning motor in one leg.  The approach could also have huge potential for more complicated things like autonomous driving, which require complex and expensive simulators, says Stefano Albrecht, an assistant professor of artificial intelligence at the University of Edinburgh. A new generation of reinforcement-learning algorithms could “super quickly pick up in the real world how the environment works,” Albrecht says.  But there are some big unsolved problems, Pinto says.  With reinforcement learning, engineers need to specify in their code which behaviors are good and are thus rewarded, and which behaviors are undesirable. In this case, turning over and walking is good, while not walking is bad. “A roboticist will need to do this for each and every task [or] problem they want the robot to solve,” says Pinto. That is incredibly time consuming, and it is difficult to program behaviors for unexpected situations.  And while simulators can be inaccurate, so can world models, Albrecht says. “World models start from nothing, so initially the predictions from the models will be completely all over the place,” he says. It takes time until they get enough data to make them accurate.  In the future, Hafner says, it would be nice to teach the robot to understand spoken commands. Hafner says the team also wants to connect cameras to the robot dog to give it vision. This would allow it to navigate in complex indoor situations, such as walking to a room, finding objects, and—yes!—playing fetch.  ","The robot dog is waving its legs in the air like an exasperated beetle. After 10 minutes of struggling, it manages to roll over to its front. Half an hour in, the robot is taking its first clumsy steps, like a newborn calf. But after one hour, the robot is strutting around the lab with confidence. What makes this four-legged robot special is that it learned to do all this by itself, without being shown what to do in a computer simulation. Danijar Hafner and colleagues at the University of California, Berkeley, used an AI technique called reinforcement learning, which trains algorithms by rewarding them for desired actions, to train the robot to walk from scratch in the real world. The team used the same algorithm to successfully train three other robots, such as one that was able to pick up balls and move them from one tray to another. Traditionally, robots are trained in a computer simulator before they attempt to do anything in the real world. For example, a pair of robot legs called Cassie taught itself to walk using reinforcement learning, but only after it had done so in a simulation. “The problem is your simulator will never be as accurate as the real world. There’ll always be aspects of the world you’re missing,” says Hafner, who worked with colleagues Alejandro Escontrela and Philipp Wu on the project and is now an intern at DeepMind. Adapting lessons from the simulator to the real world also requires extra engineering, he says. Within a few years, any task that previously required hands to perform could be partially or fully automated away. The team’s algorithm, called Dreamer, uses past experiences to build up a model of the surrounding world. Dreamer also allows the robot to conduct trial-and-error calculations in a computer program as opposed to the real world, by predicting potential future outcomes of its potential actions. This allows it to learn faster than it could purely by doing. Once the robot had learned to walk, it kept learning to adapt to unexpected situations, such as resisting being toppled by a stick. “Teaching robots through trial and error is a difficult problem, made even harder by the long training times such teaching requires,” says Lerrel Pinto, an assistant professor of computer science at New York University, who specializes in robotics and machine learning. Dreamer shows that deep reinforcement learning and world models are able to teach robots new skills in a really short amount of time, he says. Jonathan Hurst, a professor of robotics at Oregon State University, says the findings, which have not yet been peer-reviewed, make it clear that “reinforcement learning will be a cornerstone tool in the future of robot control.” Removing the simulator from robot training has many perks. The algorithm could be useful for teaching robots how to learn skills in the real world and adapt to situations like hardware failures, Hafner says–for example, a robot could learn to walk with a malfunctioning motor in one leg. The approach could also have huge potential for more complicated things like autonomous driving, which require complex and expensive simulators, says Stefano Albrecht, an assistant professor of artificial intelligence at the University of Edinburgh. A new generation of reinforcement-learning algorithms could “super quickly pick up in the real world how the environment works,” Albrecht says. But there are some big unsolved problems, Pinto says. With reinforcement learning, engineers need to specify in their code which behaviors are good and are thus rewarded, and which behaviors are undesirable. In this case, turning over and walking is good, while not walking is bad. “A roboticist will need to do this for each and every task [or] problem they want the robot to solve,” says Pinto. That is incredibly time consuming, and it is difficult to program behaviors for unexpected situations. And while simulators can be inaccurate, so can world models, Albrecht says. “World models start from nothing, so initially the predictions from the models will be completely all over the place,” he says. It takes time until they get enough data to make them accurate. In the future, Hafner says, it would be nice to teach the robot to understand spoken commands. Hafner says the team also wants to connect cameras to the robot dog to give it vision. This would allow it to navigate in complex indoor situations, such as walking to a room, finding objects, and—yes!—playing fetch.","['robot', 'dog', 'wave', 'leg', 'air', 'exasperated', 'beetle', 'minute', 'struggle', 'manage', 'roll', 'front', 'hour', 'robot', 'take', 'first', 'clumsy', 'step', 'newborn', 'calf', 'hour', 'robot', 'strut', 'lab', 'confidence', 'make', 'fourlegge', 'robot', 'special', 'learn', 'show', 'computer', 'simulation', 'danijar', 'hafner', 'colleague', 'use', 'ai', 'technique', 'call', 'reinforcement', 'learning', 'train', 'algorithm', 'reward', 'desire', 'action', 'train', 'robot', 'walk', 'scratch', 'real', 'world', 'team', 'use', 'successfully', 'train', 'robot', 'able', 'pick', 'ball', 'move', 'tray', 'traditionally', 'robot', 'train', 'computer', 'simulator', 'attempt', 'real', 'world', 'example', 'pair', 'robot', 'leg', 'call', 'cassie', 'teach', 'walk', 'use', 'reinforcement', 'learning', 'simulation', 'problem', 'simulator', 'never', 'accurate', 'real', 'world', 'always', 'aspect', 'world', 'miss', 'say', 'hafner', 'work', 'colleague', 'alejandro', 'project', 'intern', 'deepmind', 'adapt', 'lesson', 'simulator', 'real', 'world', 'also', 'require', 'extra', 'engineering', 'say', 'year', 'task', 'previously', 'require', 'hand', 'perform', 'partially', 'fully', 'automate', 'away', 'team', 'call', 'dreamer', 'use', 'past', 'experience', 'build', 'model', 'surround', 'world', 'dreamer', 'also', 'allow', 'robot', 'conduct', 'calculation', 'computer', 'program', 'oppose', 'real', 'world', 'predict', 'potential', 'future', 'outcome', 'potential', 'action', 'allow', 'learn', 'fast', 'purely', 'robot', 'learn', 'walk', 'keep', 'learn', 'adapt', 'unexpected', 'situation', 'resist', 'topple', 'stick', 'teach', 'robot', 'trial', 'error', 'difficult', 'problem', 'make', 'even', 'hard', 'long', 'training', 'time', 'teaching', 'require', 'say', 'assistant', 'professor', 'computer', 'science', 'specialize', 'robotic', 'machine', 'learn', 'dreamer', 'show', 'deep', 'reinforcement', 'learning', 'world', 'model', 'able', 'teach', 'robot', 'new', 'skill', 'really', 'short', 'amount', 'time', 'say', 'hurst', 'professor', 'robotic', 'say', 'finding', 'yet', 'peerreviewe', 'make', 'clear', 'reinforcement', 'learning', 'cornerstone', 'tool', 'future', 'robot', 'control', 'remove', 'simulator', 'robot', 'training', 'many', 'perk', 'useful', 'teach', 'robot', 'learn', 'skill', 'real', 'world', 'adapt', 'situation', 'hardware', 'failure', 'hafner', 'say', 'example', 'robot', 'learn', 'walk', 'malfunction', 'motor', 'leg', 'approach', 'also', 'huge', 'potential', 'complicated', 'thing', 'autonomous', 'driving', 'require', 'complex', 'expensive', 'simulator', 'say', 'assistant', 'professor', 'artificial', 'intelligence', 'new', 'generation', 'reinforcementlearne', 'algorithm', 'super', 'quickly', 'pick', 'real', 'world', 'environment', 'work', 'albrecht', 'say', 'big', 'unsolved', 'problem', 'say', 'reinforcement', 'learn', 'engineer', 'need', 'specify', 'code', 'behavior', 'good', 'thus', 'reward', 'behavior', 'undesirable', 'case', 'turn', 'walk', 'good', 'walk', 'bad', 'roboticist', 'need', 'task', 'problem', 'want', 'robot', 'solve', 'say', 'incredibly', 'time', 'consume', 'difficult', 'program', 'behavior', 'unexpected', 'situation', 'simulator', 'inaccurate', 'world', 'model', 'say', 'world', 'model', 'start', 'initially', 'prediction', 'model', 'place', 'say', 'take', 'time', 'get', 'enough', 'datum', 'make', 'accurate', 'future', 'hafner', 'say', 'nice', 'teach', 'robot', 'understand', 'spoken', 'command', 'say', 'team', 'also', 'want', 'connect', 'camera', 'robot', 'dog', 'give', 'vision', 'allow', 'navigate', 'complex', 'indoor', 'situation', 'walk', 'room', 'find', 'object', 'play', 'fetch']"
Inside a radical new project to democratize AI,https://www.technologyreview.com/2022/07/12/1055817/inside-a-radical-new-project-to-democratize-ai/,2022-07-12,"<p>A group of over 1,000 AI researchers has created a multilingual large language model bigger than GPT-3—and they’re giving it out for free.</p>
","PARIS — This is as close as you can get to a rock concert in AI research. Inside the supercomputing center of the French National Center for Scientific Research, on the outskirts of Paris, rows and rows of what look like black fridges hum at a deafening 100 decibels.  They form part of a supercomputer that has spent 117 days gestating a new large language model (LLM) called BLOOM that its creators hope represents a radical departure from the way AI is usually developed.  Unlike other, more famous large language models such as OpenAI’s GPT-3 and Google’s LaMDA, BLOOM (which stands for BigScience Large Open-science Open-access Multilingual Language Model) is designed to be as transparent as possible, with researchers sharing details about the data it was trained on, the challenges in its development, and the way they evaluated its performance. OpenAI and Google have not shared their code or made their models available to the public, and external researchers have very little understanding of how these models are trained.  BLOOM was created over the last year by over 1,000 volunteer researchers in a project called BigScience, which was coordinated by AI startup Hugging Face using funding from the French government. It officially launched on July 12. The researchers hope developing an open-access LLM that performs as well as other leading models will lead to long-lasting changes in the culture of AI development and help democratize access to cutting-edge AI technology for researchers around the world.  The model’s ease of access is its biggest selling point. Now that it’s live, anyone can download it and tinker with it free of charge on Hugging Face’s website. Users can pick from a selection of languages and then type in requests for BLOOM to do tasks like writing recipes or poems, translating or summarizing texts, or writing programming code. AI developers can use the model as a foundation to build their own applications.  At 176 billion parameters (variables that determine how input data is transformed into the desired output), it is bigger than OpenAI’s 175-billion-parameter GPT-3, and BigScience claims that it offers similar levels of accuracy and toxicity as other models of the same size. For languages such as Spanish and Arabic, BLOOM is the first large language model of this size.  But even the model’s creators warn it won’t fix the deeply entrenched problems around large language models, including the lack of adequate policies on data governance and privacy and the algorithms’ tendency to spew toxic content, such as racist or sexist language. Large language models are deep-learning algorithms that are trained on massive amounts of data. They are one of the hottest areas of AI research. Powerful models such as GPT-3 and LaMDA, which produce text that reads as if a human wrote it, have huge potential to change the way we process information online. They can be used as chatbots or to search for information, moderate online content, summarize books, or generate entirely new passages of text based on prompts. But they are also riddled with problems. It takes only a little prodding before these models start producing harmful content. The models are also extremely exclusive. They need to be trained on massive amounts of data using lots of expensive computing power, which is something only large (and mostly American) technology companies such as Google can afford.  Most big tech companies developing cutting-edge LLMs restrict their use by outsiders and have not released information about the inner workings of their models. This makes it hard to hold them accountable. The secrecy and exclusivity are what the researchers working on BLOOM hope to change. Meta has already taken steps away from the status quo: in May 2022 the company released its own large language model, Open Pretrained Transformer (OPT-175B), along with its code and a logbook detailing how the model was trained.  Hundreds of scientists around the world are working together to understand one of the most powerful emerging technologies before it’s too late. But Meta’s model is available only upon request, and it has a license that limits its use to research purposes. Hugging Face goes a step further. The meetings detailing its work over the past year are recorded and uploaded online, and anyone can download the model free of charge and use it for research or to build commercial applications.   A big focus for BigScience was to embed ethical considerations into the model from its inception, instead of treating them as an afterthought. LLMs are trained on tons of data collected by scraping the internet. This can be problematic, because these data sets include lots of personal information and often reflect dangerous biases. The group developed data governance structures specifically for LLMs that should make it clearer what data is being used and who it belongs to, and it sourced different data sets from around the world that weren’t readily available online.   The group is also launching a new Responsible AI License, which is something like a terms-of-service agreement. It is designed to act as a deterrent from using BLOOM in high-risk sectors such as law enforcement or health care, or to harm, deceive, exploit, or impersonate people. The license is an experiment in self-regulating LLMs before laws catch up, says Danish Contractor, an AI researcher who volunteered on the project and co-created the license. But ultimately, there’s nothing stopping anyone from abusing BLOOM. The project had its own ethical guidelines in place from the very beginning, which worked as guiding principles for the model’s development, says Giada Pistilli, Hugging Face’s ethicist, who drafted BLOOM’s ethical charter. For example, it made a point of recruiting volunteers from diverse backgrounds and locations, ensuring that outsiders can easily reproduce the project’s findings, and releasing its results in the open.  This philosophy translates into one major difference between BLOOM and other LLMs available today: the vast number of human languages the model can understand. It can handle 46 of them, including French, Vietnamese, Mandarin, Indonesian, Catalan, 13 Indic languages (such as Hindi), and 20 African languages. Just over 30% of its training data was in English. The model also understands 13 programming languages. This is highly unusual in the world of large language models, where English dominates. That’s another consequence of the fact that LLMs are built by scraping data off the internet: English is the most commonly used language online. The reason BLOOM was able to improve on this situation is that the team rallied volunteers from around the world to build suitable data sets in other languages even if those languages weren’t as well represented online. For example, Hugging Face organized workshops with African AI researchers to try to find data sets such as records from local authorities or universities that could be used to train the model on African languages, says Chris Emezue, a Hugging Face intern and a researcher at Masakhane, an organization working on natural-language processing for African languages. Including so many different languages could be a huge help to AI researchers in poorer countries, who often struggle to get access to natural-language processing because it uses a lot of expensive computing power. BLOOM allows them to skip the expensive part of developing and training the models in order to focus on building applications and fine-tuning the models for tasks in their native languages.  “If you want to include African languages in the future of [natural-language processing] … it’s a very good and important step to include them while training language models,” says Emezue. BigScience has done a “phenomenal” job of building a community around BLOOM, and its approach of involving ethics and governance from the beginning is a thoughtful one, says Percy Liang, director of Stanford's Center for Research on Foundation Models.  However, Liang doesn’t think it will lead to significant changes to LLM development. “OpenAI and Google and Microsoft are still blazing ahead,” he says. Ultimately, BLOOM is still a large language model, and it still comes with all the associated flaws and risks. Companies such as OpenAI have not released their models or code to the public because, they argue, the sexist and racist language that has gone into them makes them too dangerous to use that way.  BLOOM is also likely to incorporate inaccuracies and biased language, but since everything about the model is out in the open, people will be able to interrogate the model’s strengths and weaknesses, says Margaret Mitchell, an AI researcher and ethicist at Hugging Face. BigScience’s biggest contribution to AI might end up being not BLOOM itself, but the numerous spinoff research projects its volunteers are getting involved in. For example, such projects could bolster the model’s privacy credentials and come up with ways to use the technology in different fields, such as biomedical research.   “One new large language model is not going to change the course of history,” says Teven Le Scao, a researcher at Hugging Face who co-led BLOOM's training. “But having one good open language model that people can actually do research on has a strong long-term impact.” When it comes to the potential harms of LLMs, “ Pandora's box is already wide open,” says Le Scao. “The best you can do is to create the best conditions possible for researchers to study them.” ","PARIS — This is as close as you can get to a rock concert in AI research. Inside the supercomputing center of the French National Center for Scientific Research, on the outskirts of Paris, rows and rows of what look like black fridges hum at a deafening 100 decibels. They form part of a supercomputer that has spent 117 days gestating a new large language model (LLM) called BLOOM that its creators hope represents a radical departure from the way AI is usually developed. Unlike other, more famous large language models such as OpenAI’s GPT-3 and Google’s LaMDA, BLOOM (which stands for BigScience Large Open-science Open-access Multilingual Language Model) is designed to be as transparent as possible, with researchers sharing details about the data it was trained on, the challenges in its development, and the way they evaluated its performance. OpenAI and Google have not shared their code or made their models available to the public, and external researchers have very little understanding of how these models are trained. BLOOM was created over the last year by over 1,000 volunteer researchers in a project called BigScience, which was coordinated by AI startup Hugging Face using funding from the French government. It officially launched on July 12. The researchers hope developing an open-access LLM that performs as well as other leading models will lead to long-lasting changes in the culture of AI development and help democratize access to cutting-edge AI technology for researchers around the world. The model’s ease of access is its biggest selling point. Now that it’s live, anyone can download it and tinker with it free of charge on Hugging Face’s website. Users can pick from a selection of languages and then type in requests for BLOOM to do tasks like writing recipes or poems, translating or summarizing texts, or writing programming code. AI developers can use the model as a foundation to build their own applications. At 176 billion parameters (variables that determine how input data is transformed into the desired output), it is bigger than OpenAI’s 175-billion-parameter GPT-3, and BigScience claims that it offers similar levels of accuracy and toxicity as other models of the same size. For languages such as Spanish and Arabic, BLOOM is the first large language model of this size. But even the model’s creators warn it won’t fix the deeply entrenched problems around large language models, including the lack of adequate policies on data governance and privacy and the algorithms’ tendency to spew toxic content, such as racist or sexist language. Large language models are deep-learning algorithms that are trained on massive amounts of data. They are one of the hottest areas of AI research. Powerful models such as GPT-3 and LaMDA, which produce text that reads as if a human wrote it, have huge potential to change the way we process information online. They can be used as chatbots or to search for information, moderate online content, summarize books, or generate entirely new passages of text based on prompts. But they are also riddled with problems. It takes only a little prodding before these models start producing harmful content. The models are also extremely exclusive. They need to be trained on massive amounts of data using lots of expensive computing power, which is something only large (and mostly American) technology companies such as Google can afford. Most big tech companies developing cutting-edge LLMs restrict their use by outsiders and have not released information about the inner workings of their models. This makes it hard to hold them accountable. The secrecy and exclusivity are what the researchers working on BLOOM hope to change. Meta has already taken steps away from the status quo: in May 2022 the company released its own large language model, Open Pretrained Transformer (OPT-175B), along with its code and a logbook detailing how the model was trained. Hundreds of scientists around the world are working together to understand one of the most powerful emerging technologies before it’s too late. But Meta’s model is available only upon request, and it has a license that limits its use to research purposes. Hugging Face goes a step further. The meetings detailing its work over the past year are recorded and uploaded online, and anyone can download the model free of charge and use it for research or to build commercial applications. A big focus for BigScience was to embed ethical considerations into the model from its inception, instead of treating them as an afterthought. LLMs are trained on tons of data collected by scraping the internet. This can be problematic, because these data sets include lots of personal information and often reflect dangerous biases. The group developed data governance structures specifically for LLMs that should make it clearer what data is being used and who it belongs to, and it sourced different data sets from around the world that weren’t readily available online. The group is also launching a new Responsible AI License, which is something like a terms-of-service agreement. It is designed to act as a deterrent from using BLOOM in high-risk sectors such as law enforcement or health care, or to harm, deceive, exploit, or impersonate people. The license is an experiment in self-regulating LLMs before laws catch up, says Danish Contractor, an AI researcher who volunteered on the project and co-created the license. But ultimately, there’s nothing stopping anyone from abusing BLOOM. The project had its own ethical guidelines in place from the very beginning, which worked as guiding principles for the model’s development, says Giada Pistilli, Hugging Face’s ethicist, who drafted BLOOM’s ethical charter. For example, it made a point of recruiting volunteers from diverse backgrounds and locations, ensuring that outsiders can easily reproduce the project’s findings, and releasing its results in the open. This philosophy translates into one major difference between BLOOM and other LLMs available today: the vast number of human languages the model can understand. It can handle 46 of them, including French, Vietnamese, Mandarin, Indonesian, Catalan, 13 Indic languages (such as Hindi), and 20 African languages. Just over 30% of its training data was in English. The model also understands 13 programming languages. This is highly unusual in the world of large language models, where English dominates. That’s another consequence of the fact that LLMs are built by scraping data off the internet: English is the most commonly used language online. The reason BLOOM was able to improve on this situation is that the team rallied volunteers from around the world to build suitable data sets in other languages even if those languages weren’t as well represented online. For example, Hugging Face organized workshops with African AI researchers to try to find data sets such as records from local authorities or universities that could be used to train the model on African languages, says Chris Emezue, a Hugging Face intern and a researcher at Masakhane, an organization working on natural-language processing for African languages. Including so many different languages could be a huge help to AI researchers in poorer countries, who often struggle to get access to natural-language processing because it uses a lot of expensive computing power. BLOOM allows them to skip the expensive part of developing and training the models in order to focus on building applications and fine-tuning the models for tasks in their native languages. “If you want to include African languages in the future of [natural-language processing] … it’s a very good and important step to include them while training language models,” says Emezue. BigScience has done a “phenomenal” job of building a community around BLOOM, and its approach of involving ethics and governance from the beginning is a thoughtful one, says Percy Liang, director of Stanford's Center for Research on Foundation Models. However, Liang doesn’t think it will lead to significant changes to LLM development. “OpenAI and Google and Microsoft are still blazing ahead,” he says. Ultimately, BLOOM is still a large language model, and it still comes with all the associated flaws and risks. Companies such as OpenAI have not released their models or code to the public because, they argue, the sexist and racist language that has gone into them makes them too dangerous to use that way. BLOOM is also likely to incorporate inaccuracies and biased language, but since everything about the model is out in the open, people will be able to interrogate the model’s strengths and weaknesses, says Margaret Mitchell, an AI researcher and ethicist at Hugging Face. BigScience’s biggest contribution to AI might end up being not BLOOM itself, but the numerous spinoff research projects its volunteers are getting involved in. For example, such projects could bolster the model’s privacy credentials and come up with ways to use the technology in different fields, such as biomedical research. “One new large language model is not going to change the course of history,” says Teven Le Scao, a researcher at Hugging Face who co-led BLOOM's training. “But having one good open language model that people can actually do research on has a strong long-term impact.” When it comes to the potential harms of LLMs, “ Pandora's box is already wide open,” says Le Scao. “The best you can do is to create the best conditions possible for researchers to study them.”","['close', 'get', 'rock', 'concert', 'research', 'supercomputing', 'center', 'french', 'center', 'scientific', 'research', 'outskirt', 'row', 'row', 'look', 'black', 'fridge', 'hum', 'deafen', 'decibel', 'form', 'part', 'supercomputer', 'spend', 'day', 'gestate', 'new', 'large', 'language', 'model', 'llm', 'call', 'bloom', 'creator', 'hope', 'represent', 'radical', 'departure', 'way', 'usually', 'develop', 'famous', 'large', 'language', 'model', 'openai', 'lamda', 'bloom', 'stand', 'bigscience', 'large', 'openscience', 'openaccess', 'multilingual', 'language', 'model', 'design', 'transparent', 'possible', 'researcher', 'share', 'detail', 'datum', 'train', 'challenge', 'development', 'way', 'evaluate', 'performance', 'openai', 'share', 'code', 'make', 'model', 'available', 'public', 'external', 'researcher', 'little', 'understanding', 'model', 'train', 'bloom', 'create', 'last', 'year', 'volunteer', 'researcher', 'project', 'call', 'bigscience', 'coordinate', 'startup', 'hug', 'face', 'use', 'funding', 'french', 'government', 'officially', 'launch', 'researcher', 'hope', 'develop', 'openaccess', 'llm', 'perform', 'well', 'lead', 'model', 'lead', 'longlaste', 'change', 'culture', 'development', 'help', 'democratize', 'access', 'cuttingedge', 'technology', 'researcher', 'world', 'model', 'ease', 'access', 'big', 'selling', 'point', '’', 'live', 'download', 'tinker', 'free', 'charge', 'hug', 'face', '’s', 'website', 'user', 'pick', 'selection', 'language', 'type', 'request', 'bloom', 'task', 'write', 'recipe', 'poem', 'translate', 'summarize', 'text', 'writing', 'programming', 'code', 'ai', 'developer', 'use', 'model', 'foundation', 'build', 'application', 'parameter', 'variable', 'determine', 'input', 'datum', 'transform', 'desire', 'output', 'big', 'openai', 'gpt3', 'bigscience', 'claim', 'offer', 'similar', 'level', 'accuracy', 'toxicity', 'model', 'size', 'language', 'spanish', 'arabic', 'bloom', 'first', 'large', 'language', 'model', 'size', 'even', 'model', 'creator', 'warn', 'fix', 'deeply', 'entrenched', 'problem', 'large', 'language', 'model', 'include', 'lack', 'adequate', 'policy', 'datum', 'governance', 'privacy', 'algorithm', 'tendency', 'spew', 'toxic', 'content', 'racist', 'sexist', 'language', 'large', 'language', 'model', 'deeplearning', 'algorithm', 'train', 'massive', 'amount', 'datum', 'hot', 'area', 'research', 'powerful', 'model', 'gpt3', 'lamda', 'produce', 'text', 'read', 'human', 'write', 'huge', 'potential', 'change', 'way', 'process', 'information', 'online', 'use', 'chatbot', 'search', 'information', 'moderate', 'online', 'content', 'summarize', 'book', 'generate', 'entirely', 'new', 'passage', 'text', 'base', 'prompt', 'also', 'riddle', 'problem', 'take', 'little', 'prodding', 'model', 'start', 'produce', 'harmful', 'content', 'model', 'also', 'extremely', 'exclusive', 'need', 'train', 'massive', 'amount', 'datum', 'use', 'lot', 'expensive', 'computing', 'power', 'large', 'mostly', 'american', 'technology', 'company', 'afford', 'big', 'tech', 'company', 'develop', 'cuttingedge', 'llm', 'restrict', 'use', 'outsider', 'release', 'information', 'inner', 'working', 'model', 'make', 'hard', 'hold', 'accountable', 'secrecy', 'exclusivity', 'researcher', 'work', 'hope', 'change', 'meta', 'already', 'take', 'step', 'away', 'status', 'quo', 'company', 'release', 'large', 'language', 'model', 'open', 'pretraine', 'transformer', 'opt175b', 'code', 'logbook', 'detail', 'model', 'train', 'hundred', 'scientist', 'world', 'work', 'together', 'understand', 'powerful', 'emerge', 'technology', '’', 'late', 'model', 'available', 'request', 'license', 'limit', 'use', 'research', 'purpose', 'hug', 'face', 'go', 'step', 'far', 'meeting', 'detail', 'work', 'past', 'year', 'record', 'upload', 'online', 'download', 'model', 'free', 'charge', 'use', 'research', 'build', 'commercial', 'application', 'big', 'focus', 'bigscience', 'embed', 'ethical', 'consideration', 'model', 'inception', 'instead', 'treat', 'afterthought', 'llm', 'train', 'ton', 'datum', 'collect', 'scrape', 'internet', 'problematic', 'data', 'set', 'include', 'lot', 'personal', 'information', 'often', 'reflect', 'dangerous', 'bias', 'group', 'develop', 'data', 'governance', 'structure', 'specifically', 'llm', 'make', 'clear', 'data', 'use', 'belong', 'source', 'different', 'datum', 'set', 'world', 'readily', 'available', 'online', 'group', 'also', 'launch', 'new', 'responsible', 'ai', 'license', 'termsofservice', 'agreement', 'design', 'act', 'deterrent', 'use', 'bloom', 'highrisk', 'sector', 'law', 'enforcement', 'health', 'care', 'harm', 'deceive', 'exploit', 'impersonate', 'people', 'license', 'experiment', 'selfregulate', 'llm', 'law', 'catch', 'say', 'contractor', 'researcher', 'volunteer', 'project', 'cocreate', 'license', 'ultimately', '’', 'stop', 'abuse', 'bloom', 'project', 'ethical', 'guideline', 'place', 'beginning', 'work', 'guide', 'principle', 'model', 'development', 'say', 'hug', 'face', 'ethicist', 'draft', 'ethical', 'charter', 'example', 'make', 'point', 'recruit', 'volunteer', 'diverse', 'background', 'location', 'ensure', 'outsider', 'easily', 'reproduce', 'project', 'finding', 'release', 'result', 'open', 'philosophy', 'translate', 'major', 'difference', 'bloom', 'llm', 'available', 'today', 'vast', 'number', 'human', 'language', 'model', 'understand', 'handle', 'include', 'french', 'vietnamese', 'indic', 'language', 'african', 'language', 'training', 'datum', 'model', 'also', 'understand', 'programming', 'language', 'highly', 'unusual', 'world', 'large', 'language', 'model', 'dominate', '’', 'consequence', 'fact', 'llm', 'build', 'scrape', 'datum', 'internet', 'commonly', 'use', 'language', 'online', 'reason', 'bloom', 'able', 'improve', 'situation', 'team', 'rally', 'volunteer', 'world', 'build', 'suitable', 'data', 'set', 'language', 'even', 'language', 'well', 'represent', 'online', 'example', 'hugging', 'face', 'organize', 'workshop', 'ai', 'researcher', 'try', 'find', 'data', 'set', 'record', 'local', 'authority', 'university', 'use', 'train', 'model', 'african', 'language', 'say', 'hugging', 'face', 'intern', 'researcher', 'masakhane', 'organization', 'work', 'naturallanguage', 'processing', 'african', 'language', 'include', 'many', 'different', 'language', 'huge', 'help', 'ai', 'researcher', 'poor', 'country', 'often', 'struggle', 'get', 'access', 'naturallanguage', 'processing', 'use', 'lot', 'expensive', 'computing', 'power', 'bloom', 'allow', 'skip', 'expensive', 'part', 'develop', 'train', 'model', 'order', 'focus', 'building', 'application', 'finetune', 'model', 'task', 'native', 'language', 'want', 'include', 'african', 'language', 'future', 'naturallanguage', 'processing', '’', 'good', 'important', 'step', 'include', 'train', 'language', 'model', 'say', 'emezue', 'bigscience', 'phenomenal', 'job', 'build', 'community', 'bloom', 'approach', 'involve', 'ethic', 'governance', 'beginning', 'thoughtful', 'one', 'say', 'percy', 'research', 'foundation', 'model', 'think', 'lead', 'significant', 'change', 'llm', 'development', 'openai', 'still', 'blaze', 'ahead', 'say', 'ultimately', 'bloom', 'still', 'large', 'language', 'model', 'still', 'come', 'associated', 'flaw', 'risk', 'company', 'openai', 'release', 'model', 'code', 'public', 'argue', 'sexist', 'racist', 'language', 'go', 'make', 'dangerous', 'use', 'way', 'bloom', 'also', 'likely', 'incorporate', 'inaccuracy', 'biased', 'language', 'model', 'open', 'people', 'able', 'interrogate', 'model', 'strength', 'weakness', 'say', 'ai', 'researcher', 'ethicist', 'hug', 'face', 'bigscience', 'big', 'contribution', 'ai', 'end', 'bloom', 'numerous', 'spinoff', 'research', 'project', 'volunteer', 'involve', 'example', 'project', 'bolster', 'model', 'privacy', 'credential', 'come', 'way', 'use', 'technology', 'different', 'field', 'biomedical', 'research', 'new', 'large', 'language', 'model', 'go', 'change', 'course', 'history', 'say', 'scao', 'researcher', 'hugging', 'face', 'cole', 'bloom', 'training', 'good', 'open', 'language', 'model', 'people', 'actually', 'research', 'strong', 'longterm', 'impact', 'come', 'potential', 'harm', 'llm', 'box', 'already', 'wide', 'open', 'say', 'scao', 'good', 'create', 'good', 'condition', 'possible', 'researcher', 'study']"
Doctors using AI catch breast cancer more often than either does alone,https://www.technologyreview.com/2022/07/11/1055677/ai-diagnose-breast-cancer-mammograms/,2022-07-11,"<p>A new study shows that artificial intelligence can also handle more than half of scans automatically, dramatically reducing radiologists’ workloads.</p>
","Radiologists assisted by an AI screen for breast cancer more successfully than they do when they work alone, according to new research. That same AI also produces more accurate results in the hands of a radiologist than it does when operating solo. The large-scale study, published this month in The Lancet Digital Health, is the first to directly compare an AI’s performance in breast cancer screening according to whether it’s used alone or to assist a human expert. The hope is that such AI systems could save lives by detecting cancers doctors miss, free up radiologists to see more patients, and ease the burden in places where there is a dire lack of specialists. The software being tested comes from Vara, a startup based in Germany that also led the study. The company’s AI is already used in over a fourth of Germany’s breast cancer screening centers and was introduced earlier this year to a hospital in Mexico and another in Greece. The Vara team, with help from radiologists at the Essen University Hospital in Germany and the Memorial Sloan Kettering Cancer Center in New York, tested two approaches. In the first, the AI works alone to analyze mammograms. In the other, the AI automatically distinguishes between scans it thinks look normal and those that raise a concern. It refers the latter to a radiologist, who would review them before seeing the AI’s assessment. Then the AI would issue a warning if it detected cancer when the doctor did not. ""In the proposed AI-driven process nearly three-quarters of the screening studies didn’t need to be reviewed by a radiologist, while improving accuracy overall.” To train the neural network, Vara fed the AI data from over 367,000 mammograms—including radiologists’ notes, original assessments, and information on whether the patient ultimately had cancer—to learn how to place these scans into one of three buckets: “confident normal,” “not confident” (in which no prediction is given), and “confident cancer.” The conclusions from both approaches were then compared with the decisions real radiologists originally made on 82,851 mammograms sourced from screening centers that didn’t contribute scans used to train the AI. The second approach—doctor and AI working together—was 2.6% better at detecting breast cancer than a doctor working alone, and raised fewer false alarms. It accomplished this while automatically setting aside scans it classified as confidently normal, which amounted to 63% of all mammograms. This intense streamlining could slash radiologists’ workloads. After breast cancer screenings, patients with a normal scan are sent on their way, while an abnormal or unclear scan triggers follow-up testing. But radiologists examining mammograms miss 1 in 8 cancers. Fatigue, overwork, and even the time of day all affect how well radiologists can identify tumors as they view thousands of scans. Signs that are visually subtle are also generally less likely to set off alarms, and dense breast tissue—found mostly in younger patients—makes signs of cancer harder to see. Radiologists using the AI in the real world are required by German law to look at every mammogram, at least glancing at those the AI calls fine. The AI still lends them a hand by pre-filling reports on scans labeled normal, though the radiologist can always reject the AI’s call.  Thilo Töllner, a radiologist who heads a German breast cancer screening center, has used the program for two years. He’s sometimes disagreed when the AI classified scans as confident normal and manually filled out reports to reflect a different conclusion, but he says “normals are almost always normal.” Mostly, “you just have to press enter.”  Mammograms the AI has labeled as ambiguous or “confident cancer” are referred to a radiologist—but only after the doctor has offered an initial, independent assessment. Radiologists classify mammograms on a 0 to 6 scale known as BI-RADS, where lower is better. A score of 3 indicates that something is probably benign, but worth checking up on. If Vara has assigned a BI-RADS score of 3 or higher to a mammogram the radiologist labels normal, a warning appears. AI generally excels at image classification. So why did Vara’s AI on its own underperform a lone doctor? Part of the problem is that a mammogram alone can’t determine whether someone has cancer—that requires removing and testing the abnormal-looking tissue. Instead, the AI examines mammograms for hints. Christian Leibig, lead author on the study and director of machine learning at Vara, says that mammograms of healthy and cancerous breasts can look very similar, and both types of scans can present a wide range of visual results. This complicates AI training. So does the low prevalence of cancer in breast screenings (according to Leibig, “in Germany, it’s roughly six in 1,000”). Because AIs trained to catch cancer are mostly trained on healthy breast scans, they can be prone to false positives. The study tested the AI only on past mammogram decisions and assumed that radiologists would agree with the AI each time it issued a decision of “confident normal” or “confident cancer.” When the AI was unsure, the study defaulted to the original radiologist’s reading. That means it couldn’t test how using AI affects radiologists’ decisions—and whether any such changes may create new risks. Töllner admits he spends less time scrutinizing scans Vara labels normal than those it deems suspicious. “You get quicker with the normals because you get confident with the system,” he says. Curtis Langlotz, director of Stanford's Center for Artificial Intelligence in Medicine and Imaging, is impressed, but he says the next step would be to confirm how well the AI performs over a long period of time in actual clinics with real patients. So far, attempts to fully replace radiologists with AI have failed. A 2021 review found that in 34 of 36 studies, the AI did worse than a single radiologist at screening for breast cancer from mammograms. All 36 were less accurate than the consensus of two radiologists, which some countries require. “We often say that AI will not replace radiologists,” Langlotz says. “This study doesn’t change that, but in the proposed AI-driven process nearly three-quarters of the screening studies didn’t need to be reviewed by a radiologist, while improving accuracy overall.” That, he says, is “groundbreaking.” Langlotz adds that this approach could ease the shortage of radiologists, especially in countries such as Malawi, where there is one radiologist per 8.8 million people, or India, a country of 1.4 billion served by one radiologist for every 100,000 people. Even the US, which proportionally has 10 times as many radiologists as India, is projected to be short 17,000 radiologists by 2033. Töllner is optimistic that more radiologists using AI will mean earlier breast cancer detection, which could improve survival rates. He also hopes Vara will help quash the high number of false positives—patients recalled for further testing who are actually fine. Correction: An earlier version of this story incorrectly stated that a doctor and AI working together were 3.6% better at detecting breast cancer than a doctor working alone. The correct figure is 2.6%.  ","Radiologists assisted by an AI screen for breast cancer more successfully than they do when they work alone, according to new research. That same AI also produces more accurate results in the hands of a radiologist than it does when operating solo. The large-scale study, published this month in The Lancet Digital Health, is the first to directly compare an AI’s performance in breast cancer screening according to whether it’s used alone or to assist a human expert. The hope is that such AI systems could save lives by detecting cancers doctors miss, free up radiologists to see more patients, and ease the burden in places where there is a dire lack of specialists. The software being tested comes from Vara, a startup based in Germany that also led the study. The company’s AI is already used in over a fourth of Germany’s breast cancer screening centers and was introduced earlier this year to a hospital in Mexico and another in Greece. The Vara team, with help from radiologists at the Essen University Hospital in Germany and the Memorial Sloan Kettering Cancer Center in New York, tested two approaches. In the first, the AI works alone to analyze mammograms. In the other, the AI automatically distinguishes between scans it thinks look normal and those that raise a concern. It refers the latter to a radiologist, who would review them before seeing the AI’s assessment. Then the AI would issue a warning if it detected cancer when the doctor did not. ""In the proposed AI-driven process nearly three-quarters of the screening studies didn’t need to be reviewed by a radiologist, while improving accuracy overall.” To train the neural network, Vara fed the AI data from over 367,000 mammograms—including radiologists’ notes, original assessments, and information on whether the patient ultimately had cancer—to learn how to place these scans into one of three buckets: “confident normal,” “not confident” (in which no prediction is given), and “confident cancer.” The conclusions from both approaches were then compared with the decisions real radiologists originally made on 82,851 mammograms sourced from screening centers that didn’t contribute scans used to train the AI. The second approach—doctor and AI working together—was 2.6% better at detecting breast cancer than a doctor working alone, and raised fewer false alarms. It accomplished this while automatically setting aside scans it classified as confidently normal, which amounted to 63% of all mammograms. This intense streamlining could slash radiologists’ workloads. After breast cancer screenings, patients with a normal scan are sent on their way, while an abnormal or unclear scan triggers follow-up testing. But radiologists examining mammograms miss 1 in 8 cancers. Fatigue, overwork, and even the time of day all affect how well radiologists can identify tumors as they view thousands of scans. Signs that are visually subtle are also generally less likely to set off alarms, and dense breast tissue—found mostly in younger patients—makes signs of cancer harder to see. Radiologists using the AI in the real world are required by German law to look at every mammogram, at least glancing at those the AI calls fine. The AI still lends them a hand by pre-filling reports on scans labeled normal, though the radiologist can always reject the AI’s call. Thilo Töllner, a radiologist who heads a German breast cancer screening center, has used the program for two years. He’s sometimes disagreed when the AI classified scans as confident normal and manually filled out reports to reflect a different conclusion, but he says “normals are almost always normal.” Mostly, “you just have to press enter.” Mammograms the AI has labeled as ambiguous or “confident cancer” are referred to a radiologist—but only after the doctor has offered an initial, independent assessment. Radiologists classify mammograms on a 0 to 6 scale known as BI-RADS, where lower is better. A score of 3 indicates that something is probably benign, but worth checking up on. If Vara has assigned a BI-RADS score of 3 or higher to a mammogram the radiologist labels normal, a warning appears. AI generally excels at image classification. So why did Vara’s AI on its own underperform a lone doctor? Part of the problem is that a mammogram alone can’t determine whether someone has cancer—that requires removing and testing the abnormal-looking tissue. Instead, the AI examines mammograms for hints. Christian Leibig, lead author on the study and director of machine learning at Vara, says that mammograms of healthy and cancerous breasts can look very similar, and both types of scans can present a wide range of visual results. This complicates AI training. So does the low prevalence of cancer in breast screenings (according to Leibig, “in Germany, it’s roughly six in 1,000”). Because AIs trained to catch cancer are mostly trained on healthy breast scans, they can be prone to false positives. The study tested the AI only on past mammogram decisions and assumed that radiologists would agree with the AI each time it issued a decision of “confident normal” or “confident cancer.” When the AI was unsure, the study defaulted to the original radiologist’s reading. That means it couldn’t test how using AI affects radiologists’ decisions—and whether any such changes may create new risks. Töllner admits he spends less time scrutinizing scans Vara labels normal than those it deems suspicious. “You get quicker with the normals because you get confident with the system,” he says. Curtis Langlotz, director of Stanford's Center for Artificial Intelligence in Medicine and Imaging, is impressed, but he says the next step would be to confirm how well the AI performs over a long period of time in actual clinics with real patients. So far, attempts to fully replace radiologists with AI have failed. A 2021 review found that in 34 of 36 studies, the AI did worse than a single radiologist at screening for breast cancer from mammograms. All 36 were less accurate than the consensus of two radiologists, which some countries require. “We often say that AI will not replace radiologists,” Langlotz says. “This study doesn’t change that, but in the proposed AI-driven process nearly three-quarters of the screening studies didn’t need to be reviewed by a radiologist, while improving accuracy overall.” That, he says, is “groundbreaking.” Langlotz adds that this approach could ease the shortage of radiologists, especially in countries such as Malawi, where there is one radiologist per 8.8 million people, or India, a country of 1.4 billion served by one radiologist for every 100,000 people. Even the US, which proportionally has 10 times as many radiologists as India, is projected to be short 17,000 radiologists by 2033. Töllner is optimistic that more radiologists using AI will mean earlier breast cancer detection, which could improve survival rates. He also hopes Vara will help quash the high number of false positives—patients recalled for further testing who are actually fine. Correction: An earlier version of this story incorrectly stated that a doctor and AI working together were 3.6% better at detecting breast cancer than a doctor working alone. The correct figure is 2.6%.","['radiologist', 'assist', 'ai', 'screen', 'breast', 'cancer', 'successfully', 'work', 'alone', 'accord', 'new', 'research', 'ai', 'also', 'produce', 'accurate', 'result', 'hand', 'radiologist', 'operate', 'largescale', 'study', 'publish', 'month', 'digital', 'health', 'first', 'directly', 'compare', 'ai', '’s', 'performance', 'breast', 'cancer', 'screen', 'accord', 'use', 'alone', 'assist', 'human', 'expert', 'hope', 'system', 'save', 'life', 'detect', 'cancer', 'doctor', 'miss', 'free', 'radiologist', 'see', 'patient', 'ease', 'burden', 'place', 'dire', 'lack', 'specialist', 'software', 'test', 'come', 'startup', 'base', 'also', 'lead', 'study', 'company', 'ai', 'already', 'use', 'fourth', 'breast', 'cancer', 'screen', 'center', 'introduce', 'early', 'year', 'hospital', 'mexico', 'vara', 'team', 'help', 'radiologist', 'university', 'hospital', 'memorial', 'sloan', 'cancer', 'test', 'approach', 'first', 'ai', 'work', 'alone', 'analyze', 'mammogram', 'ai', 'automatically', 'distinguishe', 'scan', 'think', 'look', 'normal', 'raise', 'concern', 'refer', 'latter', 'radiologist', 'review', 'see', 'assessment', 'ai', 'issue', 'warning', 'detect', 'cancer', 'doctor', 'propose', 'aidriven', 'process', 'nearly', 'threequarter', 'screening', 'study', 'need', 'review', 'radiologist', 'improve', 'accuracy', 'overall', 'train', 'network', 'feed', 'datum', 'mammogram', 'include', 'radiologist', 'note', 'original', 'assessment', 'information', 'patient', 'ultimately', 'cancer', 'learn', 'place', 'scan', 'bucket', 'confident', 'normal', 'confident', 'prediction', 'give', 'confident', 'cancer', 'conclusion', 'approach', 'compare', 'decision', 'real', 'radiologist', 'originally', 'make', 'mammogram', 'source', 'screen', 'center', 'contribute', 'scan', 'use', 'train', 'second', 'approach', 'doctor', 'work', 'together', 'well', 'detect', 'breast', 'cancer', 'doctor', 'work', 'alone', 'raise', 'false', 'alarm', 'accomplish', 'automatically', 'set', 'scan', 'classify', 'confidently', 'normal', 'amount', 'mammogram', 'intense', 'streamlining', 'slash', 'radiologist', 'workload', 'breast', 'cancer', 'screening', 'patient', 'normal', 'scan', 'send', 'way', 'abnormal', 'unclear', 'scan', 'trigger', 'followup', 'testing', 'radiologist', 'examine', 'mammogram', 'miss', 'cancer', 'fatigue', 'overwork', 'even', 'time', 'affect', 'well', 'radiologist', 'identify', 'tumor', 'view', 'thousand', 'scan', 'sign', 'visually', 'subtle', 'also', 'generally', 'less', 'likely', 'set', 'alarm', 'dense', 'breast', 'tissue', 'find', 'mostly', 'young', 'patient', 'make', 'sign', 'cancer', 'hard', 'see', 'radiologist', 'use', 'ai', 'real', 'world', 'require', 'german', 'law', 'look', 'mammogram', 'least', 'glance', 'ai', 'call', 'fine', 'ai', 'still', 'lend', 'hand', 'prefille', 'report', 'scan', 'label', 'normal', 'radiologist', 'always', 'reject', 'call', 'thilo', 'töllner', 'radiologist', 'head', 'german', 'breast', 'cancer', 'screening', 'center', 'use', 'program', 'year', 'sometimes', 'disagree', 'ai', 'classified', 'scan', 'confident', 'normal', 'manually', 'fill', 'report', 'reflect', 'different', 'conclusion', 'say', 'normal', 'almost', 'always', 'normal', 'mostly', 'press', 'enter', 'mammogram', 'ai', 'label', 'ambiguous', 'confident', 'cancer', 'refer', 'radiologist', 'doctor', 'offer', 'initial', 'independent', 'assessment', 'radiologist', 'classify', 'mammogram', 'scale', 'know', 'birad', 'low', 'well', 'score', 'indicate', 'probably', 'benign', 'worth', 'check', 'assign', 'birad', 'score', 'high', 'mammogram', 'radiologist', 'label', 'normal', 'warning', 'appear', 'ai', 'generally', 'excel', 'image', 'classification', 'ai', 'underperform', 'lone', 'doctor', 'part', 'problem', 'mammogram', 'alone', 'determine', 'cancer', 'require', 'remove', 'test', 'abnormallooke', 'tissue', 'instead', 'examine', 'mammogram', 'hint', 'lead', 'author', 'study', 'director', 'machine', 'learning', 'say', 'mammogram', 'healthy', 'cancerous', 'breast', 'look', 'similar', 'type', 'scan', 'present', 'wide', 'range', 'visual', 'result', 'complicate', 'train', 'low', 'prevalence', 'cancer', 'breast', 'screening', 'accord', 'leibig', '’', 'roughly', 'train', 'catch', 'cancer', 'mostly', 'train', 'healthy', 'breast', 'scan', 'prone', 'false', 'positive', 'study', 'test', 'ai', 'past', 'mammogram', 'decision', 'assume', 'radiologist', 'agree', 'ai', 'time', 'issue', 'decision', 'confident', 'normal', 'confident', 'cancer', 'ai', 'unsure', 'study', 'default', 'original', 'radiologist', 'reading', 'mean', 'test', 'use', 'affect', 'radiologist', 'decision', 'change', 'create', 'new', 'risk', 'töllner', 'admit', 'spend', 'less', 'time', 'scrutinize', 'scan', 'label', 'normal', 'deem', 'suspicious', 'get', 'quick', 'normal', 'get', 'confident', 'system', 'say', 'director', 'artificial', 'intelligence', 'medicine', 'imaging', 'impressed', 'say', 'next', 'step', 'confirm', 'well', 'perform', 'long', 'period', 'time', 'actual', 'clinic', 'real', 'patient', 'far', 'attempt', 'fully', 'replace', 'radiologist', 'ai', 'fail', 'review', 'find', 'study', 'ai', 'bad', 'single', 'radiologist', 'screen', 'breast', 'cancer', 'mammogram', 'less', 'accurate', 'consensus', 'radiologist', 'country', 'require', 'often', 'say', 'ai', 'replace', 'radiologist', 'langlotz', 'say', 'study', 'change', 'propose', 'aidriven', 'process', 'nearly', 'threequarter', 'screening', 'study', 'need', 'review', 'radiologist', 'improve', 'accuracy', 'overall', 'say', 'groundbreake', 'langlotz', 'add', 'approach', 'ease', 'shortage', 'radiologist', 'especially', 'country', 'radiologist', 'people', 'country', 'serve', 'radiologist', 'people', 'even', 'proportionally', 'time', 'many', 'radiologist', 'project', 'short', 'radiologist', 'töllner', 'optimistic', 'radiologist', 'use', 'mean', 'early', 'breast', 'cancer', 'detection', 'improve', 'survival', 'rate', 'also', 'hope', 'help', 'quash', 'high', 'number', 'false', 'positive', 'patient', 'recall', 'testing', 'actually', 'fine', 'correction', 'early', 'version', 'story', 'incorrectly', 'state', 'doctor', 'work', 'together', 'well', 'detect', 'breast', 'cancer', 'doctor', 'work', 'alone', 'correct', 'figure']"
Why business is booming for military AI startups ,https://www.technologyreview.com/2022/07/07/1055526/why-business-is-booming-for-military-ai-startups/,2022-07-07,"<p>The invasion of Ukraine has prompted militaries to update their arsenals—and Silicon Valley stands to capitalize.</p>
","Exactly two weeks after Russia invaded Ukraine in February, Alexander Karp, the CEO of data analytics company Palantir, made his pitch to European leaders. With war on their doorstep, Europeans ought to modernize their arsenals with Silicon Valley’s help, he argued in an open letter.  For Europe to “remain strong enough to defeat the threat of foreign occupation,” Karp wrote, countries need to embrace “the relationship between technology and the state, between disruptive companies that seek to dislodge the grip of entrenched contractors and the federal government ministries with funding.” Militaries are responding to the call. NATO announced on June 30 that it is creating a $1 billion innovation fund that will invest in early-stage startups and venture capital funds developing “priority” technologies such as artificial intelligence, big-data processing, and automation. Since the war started, the UK has launched a new AI strategy specifically for defense, and the Germans have earmarked just under half a billion for research and artificial intelligence within a $100 billion cash injection to the military.  “War is a catalyst for change,” says Kenneth Payne, who leads defense studies research at King’s College London and is the author of the book I, Warbot: The Dawn of Artificially Intelligent Conflict.  The war in Ukraine has added urgency to the drive to push more AI tools onto the battlefield. Those with the most to gain are startups such as Palantir, which are hoping to cash in as militaries race to update their arsenals with the latest technologies. But long-standing ethical concerns over the use of AI in warfare have become more urgent as the technology becomes more and more advanced, while the prospect of restrictions and regulations governing its use looks as remote as ever. The relationship between tech and the military wasn’t always so amicable. In 2018, following employee protests and outrage, Google pulled out of the Pentagon’s Project Maven, an attempt to build image recognition systems to improve drone strikes. The episode caused heated debate about human rights and the morality of developing AI for autonomous weapons.  It also led high-profile AI researchers such as Yoshua Bengio, a winner of the Turing Prize, and Demis Hassabis, Shane Legg, and Mustafa Suleyman, the founders of leading AI lab DeepMind, to pledge not to work on lethal AI.  But four years later, Silicon Valley is closer to the world’s militaries than ever. And it’s not just big companies, either—startups are finally getting a look in, says Yll Bajraktari, who was previously executive director of the US National Security Commission on AI (NSCAI) and now works for the Special Competitive Studies Project, a group that lobbies for more adoption of AI across the US.  Companies that sell military AI make expansive claims for what their technology can do. They say it can help with everything from the mundane to the lethal, from screening résumés to processing data from satellites or recognizing patterns in data to help soldiers make quicker decisions on the battlefield. Image recognition software can help with identifying targets. Autonomous drones can be used for surveillance or attacks on land, air, or water, or to help soldiers deliver supplies more safely than is possible by land.  These technologies are still in their infancy on the battlefield, and militaries are going through a period of experimentation, says Payne, sometimes without much success. There are countless examples of AI companies’ tendency to make grand promises about technologies that turn out not to work as advertised, and combat zones are perhaps among the most technically challenging areas in which to deploy AI because there is little relevant training data. This could cause autonomous systems to fail in a “complex and unpredictable manner,” argued Arthur Holland Michel, an expert on drones and other surveillance technologies, in a paper for the United Nations Institute for Disarmament Research Nevertheless, many militaries are pressing forward. In a vaguely worded press release in 2021, the British army proudly announced it had used AI in a military operation for the first time, to provide information on the surrounding environment and terrain. The US is working with startups to develop autonomous military vehicles. In the future, swarms of hundreds or even thousands of autonomous drones that the US and British militaries are developing could prove to be powerful and lethal weapons.  Many experts are worried. Meredith Whittaker, a senior advisor on AI at the Federal Trade Commission and a faculty director at the AI Now Institute, says this push is really more about enriching tech companies than improving military operations.  In a piece for Prospect magazine co-written with Lucy Suchman, a sociology professor at Lancaster University, she argued that AI boosters are stoking Cold War rhetoric and trying to create a narrative that positions Big Tech as “critical national infrastructure,” too big and important to break up or regulate. They warn that AI adoption by the military is being presented as an inevitability rather than what it really is: an active choice that involves ethical complexities and trade-offs.  The controversy over Project Maven shows the department has a serious trust problem. This is an attempt to fix that. With the controversy around Maven receding into the past, the voices calling for more AI in defense have become louder and louder in the last couple of years.  One of the loudest has been Google’s former CEO Eric Schmidt, who chaired the NSCAI and has called for the US to take a more aggressive approach to adopting military AI. In a report last year, outlining steps the United States should take to be up to speed in AI by 2025, the NSCAI called on the US military to invest $8 billion a year into these technologies or risk falling behind China.   The Chinese military likely spends at least $1.6 billion a year on AI, according to a report by the Georgetown Center for Security and Emerging Technologies, and in the US there is already a significant push underway to reach parity, says Lauren Kahn, a research fellow at the Council on Foreign Relations. The US Department of Defense requested $874 million for artificial intelligence for 2022, although that figure does not reflect the total of the department’s AI investments, it said in a March 2022 report. It’s not just the US military that’s convinced of the need. European countries, which tend to be more cautious about adopting new technologies, are also spending more money on AI, says Heiko Borchert, co-director of the Defense AI Observatory at the Helmut Schmidt University in Hamburg, Germany.  The French and the British have identified AI as a key defense technology, and the European Commission, the EU’s executive arm, has earmarked $1 billion to develop new defense technologies.  Building demand for AI is one thing. Getting militaries to adopt it is entirely another.  A lot of countries are pushing the AI narrative, but they’re struggling to move from concept to deployment, says Arnaud Guérin, the CEO of Preligens, a French startup that sells AI surveillance. That’s partly because the defense industry in most countries is still usually dominated by a clutch of large contractors, which tend to have more expertise in military hardware than AI software, he says.  It’s also because clunky military vetting processes move slowly compared with the breakneck speed we’re used to seeing in AI development: military contracts can span decades, but in the fast-paced startup cycle, companies have just a year or so to get off the ground. Startups and venture capitalists have expressed frustration that the process is moving so slowly. The risk, argues Katherine Boyle, a general partner at venture capital firm Andreessen Horowitz, is that talented engineers will leave in frustration for jobs at Facebook and Google, and startups will go bankrupt waiting for defense contracts.  “Some of those hoops are totally critical, particularly in this sector where security concerns are very real,” says Mark Warner, who founded FacultyAI, a data analytics company that works with the British military. “But others are not … and in some ways have enshrined the position of incumbents.” AI companies with military ambitions have to “stay in business for a long time,” says Ngor Luong, a research analyst who has studied AI investment trends at the Georgetown Center for Security and Emerging Technologies.  Militaries are in a bind, says Kahn: go too fast, and risk deploying dangerous and broken systems, or go too slow and miss out on technological advancement. The US wants to go faster, and the DoD has enlisted the help of Craig Martell, the former AI chief at ride-hailing company Lyft.  In June 2022, Martell took the helm of the Pentagon’s new Chief Digital Artificial Intelligence Office, which aims to coordinate the US military’s AI efforts. Martell’s mission, he told Bloomberg, is to change the culture of the department and boost the military’s use of AI despite “bureaucratic inertia.”  He may be pushing at an open door, as AI companies are already starting to snap up lucrative military contracts. In February, Anduril, a five-year-old startup that develops autonomous defense systems such as sophisticated underwater drones, won a $1 billion defense contract with the US. In January, ScaleAI, a startup that provides data labeling services for AI, won a $250 million contract with the US Department of Defense.  Despite the steady march of AI into the field of battle, the ethical concerns that prompted the protests around Project Maven haven’t gone away.  There have been some efforts to assuage those concerns. Aware it has a trust issue, the US Department of Defense has rolled out “responsible artificial intelligence” guidelines for AI developers, and it has its own ethical guidelines for the use of AI. NATO has an AI strategy that sets out voluntary ethical guidelines for its member nations.  All these guidelines call on militaries to use AI in a way that is lawful, responsible, reliable, and traceable and seeks to mitigate biases embedded in the algorithms.  One of their key concepts is that humans must always retain control of AI systems. But as the technology develops, that won’t really be possible, says Payne.   “The whole point of an autonomous [system] is to allow it to make a decision faster and more accurately than a human could do and at a scale that a human can’t do,” he says. “You’re effectively hamstringing yourself if you say ‘No, we’re going to lawyer each and every decision.’”   Still, critics say stronger rules are needed. There is a global campaign called Stop Killer Robots that seeks to ban lethal autonomous weapons, such as drone swarms. Activists, high-profile officials such as UN chief António Guterres, and governments such as New Zealand’s argue that autonomous weapons are deeply unethical, because they give machines control over life-and-death decisions and could disproportionately harm marginalized communities through algorithmic biases.  Swarms of thousands of autonomous drones, for example, could essentially become weapons of mass destruction. Restricting these technologies will be an uphill battle because the idea of a global ban has faced opposition from big military spenders, such as the US, France, and the UK. Ultimately, the new era of military AI raises a slew of difficult ethical questions that we don’t have answers to yet.  One of those questions is how automated we want armed forces to be in the first place, says Payne. On one hand, AI systems might reduce casualties by making war more targeted, but on the other, you’re “effectively creating a robot mercenary force to fight on your behalf,” he says. “It distances your society from the consequences of violence.”  ","Exactly two weeks after Russia invaded Ukraine in February, Alexander Karp, the CEO of data analytics company Palantir, made his pitch to European leaders. With war on their doorstep, Europeans ought to modernize their arsenals with Silicon Valley’s help, he argued in an open letter. For Europe to “remain strong enough to defeat the threat of foreign occupation,” Karp wrote, countries need to embrace “the relationship between technology and the state, between disruptive companies that seek to dislodge the grip of entrenched contractors and the federal government ministries with funding.” Militaries are responding to the call. NATO announced on June 30 that it is creating a $1 billion innovation fund that will invest in early-stage startups and venture capital funds developing “priority” technologies such as artificial intelligence, big-data processing, and automation. Since the war started, the UK has launched a new AI strategy specifically for defense, and the Germans have earmarked just under half a billion for research and artificial intelligence within a $100 billion cash injection to the military. “War is a catalyst for change,” says Kenneth Payne, who leads defense studies research at King’s College London and is the author of the book I, Warbot: The Dawn of Artificially Intelligent Conflict. The war in Ukraine has added urgency to the drive to push more AI tools onto the battlefield. Those with the most to gain are startups such as Palantir, which are hoping to cash in as militaries race to update their arsenals with the latest technologies. But long-standing ethical concerns over the use of AI in warfare have become more urgent as the technology becomes more and more advanced, while the prospect of restrictions and regulations governing its use looks as remote as ever. The relationship between tech and the military wasn’t always so amicable. In 2018, following employee protests and outrage, Google pulled out of the Pentagon’s Project Maven, an attempt to build image recognition systems to improve drone strikes. The episode caused heated debate about human rights and the morality of developing AI for autonomous weapons. It also led high-profile AI researchers such as Yoshua Bengio, a winner of the Turing Prize, and Demis Hassabis, Shane Legg, and Mustafa Suleyman, the founders of leading AI lab DeepMind, to pledge not to work on lethal AI. But four years later, Silicon Valley is closer to the world’s militaries than ever. And it’s not just big companies, either—startups are finally getting a look in, says Yll Bajraktari, who was previously executive director of the US National Security Commission on AI (NSCAI) and now works for the Special Competitive Studies Project, a group that lobbies for more adoption of AI across the US. Companies that sell military AI make expansive claims for what their technology can do. They say it can help with everything from the mundane to the lethal, from screening résumés to processing data from satellites or recognizing patterns in data to help soldiers make quicker decisions on the battlefield. Image recognition software can help with identifying targets. Autonomous drones can be used for surveillance or attacks on land, air, or water, or to help soldiers deliver supplies more safely than is possible by land. These technologies are still in their infancy on the battlefield, and militaries are going through a period of experimentation, says Payne, sometimes without much success. There are countless examples of AI companies’ tendency to make grand promises about technologies that turn out not to work as advertised, and combat zones are perhaps among the most technically challenging areas in which to deploy AI because there is little relevant training data. This could cause autonomous systems to fail in a “complex and unpredictable manner,” argued Arthur Holland Michel, an expert on drones and other surveillance technologies, in a paper for the United Nations Institute for Disarmament Research Nevertheless, many militaries are pressing forward. In a vaguely worded press release in 2021, the British army proudly announced it had used AI in a military operation for the first time, to provide information on the surrounding environment and terrain. The US is working with startups to develop autonomous military vehicles. In the future, swarms of hundreds or even thousands of autonomous drones that the US and British militaries are developing could prove to be powerful and lethal weapons. Many experts are worried. Meredith Whittaker, a senior advisor on AI at the Federal Trade Commission and a faculty director at the AI Now Institute, says this push is really more about enriching tech companies than improving military operations. In a piece for Prospect magazine co-written with Lucy Suchman, a sociology professor at Lancaster University, she argued that AI boosters are stoking Cold War rhetoric and trying to create a narrative that positions Big Tech as “critical national infrastructure,” too big and important to break up or regulate. They warn that AI adoption by the military is being presented as an inevitability rather than what it really is: an active choice that involves ethical complexities and trade-offs. The controversy over Project Maven shows the department has a serious trust problem. This is an attempt to fix that. With the controversy around Maven receding into the past, the voices calling for more AI in defense have become louder and louder in the last couple of years. One of the loudest has been Google’s former CEO Eric Schmidt, who chaired the NSCAI and has called for the US to take a more aggressive approach to adopting military AI. In a report last year, outlining steps the United States should take to be up to speed in AI by 2025, the NSCAI called on the US military to invest $8 billion a year into these technologies or risk falling behind China. The Chinese military likely spends at least $1.6 billion a year on AI, according to a report by the Georgetown Center for Security and Emerging Technologies, and in the US there is already a significant push underway to reach parity, says Lauren Kahn, a research fellow at the Council on Foreign Relations. The US Department of Defense requested $874 million for artificial intelligence for 2022, although that figure does not reflect the total of the department’s AI investments, it said in a March 2022 report. It’s not just the US military that’s convinced of the need. European countries, which tend to be more cautious about adopting new technologies, are also spending more money on AI, says Heiko Borchert, co-director of the Defense AI Observatory at the Helmut Schmidt University in Hamburg, Germany. The French and the British have identified AI as a key defense technology, and the European Commission, the EU’s executive arm, has earmarked $1 billion to develop new defense technologies. Building demand for AI is one thing. Getting militaries to adopt it is entirely another. A lot of countries are pushing the AI narrative, but they’re struggling to move from concept to deployment, says Arnaud Guérin, the CEO of Preligens, a French startup that sells AI surveillance. That’s partly because the defense industry in most countries is still usually dominated by a clutch of large contractors, which tend to have more expertise in military hardware than AI software, he says. It’s also because clunky military vetting processes move slowly compared with the breakneck speed we’re used to seeing in AI development: military contracts can span decades, but in the fast-paced startup cycle, companies have just a year or so to get off the ground. Startups and venture capitalists have expressed frustration that the process is moving so slowly. The risk, argues Katherine Boyle, a general partner at venture capital firm Andreessen Horowitz, is that talented engineers will leave in frustration for jobs at Facebook and Google, and startups will go bankrupt waiting for defense contracts. “Some of those hoops are totally critical, particularly in this sector where security concerns are very real,” says Mark Warner, who founded FacultyAI, a data analytics company that works with the British military. “But others are not … and in some ways have enshrined the position of incumbents.” AI companies with military ambitions have to “stay in business for a long time,” says Ngor Luong, a research analyst who has studied AI investment trends at the Georgetown Center for Security and Emerging Technologies. Militaries are in a bind, says Kahn: go too fast, and risk deploying dangerous and broken systems, or go too slow and miss out on technological advancement. The US wants to go faster, and the DoD has enlisted the help of Craig Martell, the former AI chief at ride-hailing company Lyft. In June 2022, Martell took the helm of the Pentagon’s new Chief Digital Artificial Intelligence Office, which aims to coordinate the US military’s AI efforts. Martell’s mission, he told Bloomberg, is to change the culture of the department and boost the military’s use of AI despite “bureaucratic inertia.” He may be pushing at an open door, as AI companies are already starting to snap up lucrative military contracts. In February, Anduril, a five-year-old startup that develops autonomous defense systems such as sophisticated underwater drones, won a $1 billion defense contract with the US. In January, ScaleAI, a startup that provides data labeling services for AI, won a $250 million contract with the US Department of Defense. Despite the steady march of AI into the field of battle, the ethical concerns that prompted the protests around Project Maven haven’t gone away. There have been some efforts to assuage those concerns. Aware it has a trust issue, the US Department of Defense has rolled out “responsible artificial intelligence” guidelines for AI developers, and it has its own ethical guidelines for the use of AI. NATO has an AI strategy that sets out voluntary ethical guidelines for its member nations. All these guidelines call on militaries to use AI in a way that is lawful, responsible, reliable, and traceable and seeks to mitigate biases embedded in the algorithms. One of their key concepts is that humans must always retain control of AI systems. But as the technology develops, that won’t really be possible, says Payne. “The whole point of an autonomous [system] is to allow it to make a decision faster and more accurately than a human could do and at a scale that a human can’t do,” he says. “You’re effectively hamstringing yourself if you say ‘No, we’re going to lawyer each and every decision.’” Still, critics say stronger rules are needed. There is a global campaign called Stop Killer Robots that seeks to ban lethal autonomous weapons, such as drone swarms. Activists, high-profile officials such as UN chief António Guterres, and governments such as New Zealand’s argue that autonomous weapons are deeply unethical, because they give machines control over life-and-death decisions and could disproportionately harm marginalized communities through algorithmic biases. Swarms of thousands of autonomous drones, for example, could essentially become weapons of mass destruction. Restricting these technologies will be an uphill battle because the idea of a global ban has faced opposition from big military spenders, such as the US, France, and the UK. Ultimately, the new era of military AI raises a slew of difficult ethical questions that we don’t have answers to yet. One of those questions is how automated we want armed forces to be in the first place, says Payne. On one hand, AI systems might reduce casualties by making war more targeted, but on the other, you’re “effectively creating a robot mercenary force to fight on your behalf,” he says. “It distances your society from the consequences of violence.”","['exactly', 'week', 'invade', 'ukraine', 'ceo', 'datum', 'analytic', 'company', 'palantir', 'make', 'pitch', 'european', 'leader', 'war', 'doorstep', 'european', 'modernize', 'arsenal', 'help', 'argue', 'open', 'letter', 'remain', 'strong', 'enough', 'defeat', 'threat', 'foreign', 'occupation', 'karp', 'write', 'country', 'need', 'embrace', 'relationship', 'technology', 'state', 'disruptive', 'company', 'seek', 'dislodge', 'grip', 'entrenched', 'contractor', 'federal', 'government', 'ministry', 'funding', 'military', 'respond', 'call', 'announce', 'create', 'innovation', 'fund', 'invest', 'earlystage', 'startup', 'venture', 'capital', 'fund', 'develop', 'priority', 'technology', 'artificial', 'intelligence', 'bigdata', 'processing', 'automation', 'war', 'start', 'launch', 'new', 'ai', 'strategy', 'specifically', 'defense', 'german', 'earmark', 'research', 'artificial', 'intelligence', 'cash', 'injection', 'military', 'war', 'catalyst', 'change', 'say', 'lead', 'defense', 'study', 'research', 'author', 'book', 'warbot', 'dawn', 'artificially', 'intelligent', 'conflict', 'war', 'add', 'urgency', 'drive', 'push', 'ai', 'tool', 'battlefield', 'gain', 'startup', 'palantir', 'hope', 'cash', 'military', 'race', 'update', 'arsenal', 'late', 'technology', 'longstande', 'ethical', 'concern', 'use', 'warfare', 'become', 'urgent', 'technology', 'become', 'advanced', 'prospect', 'restriction', 'regulation', 'govern', 'use', 'look', 'remote', 'ever', 'relationship', 'tech', 'military', 'always', 'amicable', 'follow', 'employee', 'protest', 'outrage', 'pull', 'project', 'maven', 'attempt', 'build', 'image', 'recognition', 'system', 'improve', 'drone', 'strike', 'episode', 'cause', 'heated', 'debate', 'human', 'right', 'morality', 'develop', 'ai', 'autonomous', 'weapon', 'also', 'lead', 'highprofile', 'ai', 'researcher', 'bengio', 'winner', 'ture', 'prize', 'demis', 'shane', 'legg', 'suleyman', 'founder', 'lead', 'lab', 'deepmind', 'pledge', 'work', 'lethal', 'ai', 'year', 'later', 'close', 'world', 'military', 'ever', '’', 'big', 'company', 'either', 'startup', 'finally', 'get', 'look', 'say', 'previously', 'executive', 'director', 'work', 'special', 'competitive', 'study', 'project', 'group', 'lobby', 'adoption', 'ai', 'company', 'sell', 'military', 'make', 'expansive', 'claim', 'technology', 'say', 'help', 'mundane', 'lethal', 'screen', 'résumés', 'process', 'datum', 'satellite', 'recognize', 'pattern', 'datum', 'help', 'soldier', 'make', 'quick', 'decision', 'battlefield', 'image', 'recognition', 'software', 'help', 'identify', 'target', 'autonomous', 'drone', 'use', 'surveillance', 'attack', 'land', 'air', 'water', 'help', 'soldier', 'deliver', 'supply', 'safely', 'possible', 'land', 'technology', 'still', 'infancy', 'battlefield', 'military', 'go', 'period', 'experimentation', 'say', 'sometimes', 'much', 'success', 'countless', 'example', 'company', 'tendency', 'make', 'grand', 'promise', 'technology', 'turn', 'work', 'advertised', 'combat', 'zone', 'perhaps', 'technically', 'challenging', 'area', 'deploy', 'ai', 'little', 'relevant', 'training', 'datum', 'cause', 'autonomous', 'system', 'fail', 'complex', 'unpredictable', 'manner', 'argue', 'expert', 'drone', 'surveillance', 'technology', 'paper', 'disarmament', 'research', 'nevertheless', 'many', 'military', 'press', 'forward', 'vaguely', 'word', 'press', 'release', 'british', 'proudly', 'announce', 'use', 'ai', 'military', 'operation', 'first', 'time', 'provide', 'information', 'surround', 'environment', 'terrain', 'work', 'startup', 'develop', 'autonomous', 'military', 'vehicle', 'future', 'swarm', 'hundred', 'even', 'thousand', 'autonomous', 'drone', 'british', 'military', 'develop', 'prove', 'powerful', 'lethal', 'weapon', 'many', 'expert', 'worried', 'meredith', 'senior', 'advisor', 'faculty', 'director', 'say', 'push', 'really', 'enrich', 'tech', 'company', 'improve', 'military', 'operation', 'piece', 'prospect', 'magazine', 'cowritten', 'sociology', 'professor', 'lancaster', 'university', 'argue', 'booster', 'stoke', 'cold', 'war', 'rhetoric', 'try', 'create', 'narrative', 'position', 'big', 'tech', 'critical', 'national', 'infrastructure', 'big', 'important', 'break', 'regulate', 'warn', 'ai', 'adoption', 'military', 'present', 'inevitability', 'rather', 'really', 'active', 'choice', 'involve', 'ethical', 'complexity', 'tradeoff', 'controversy', 'project', 'show', 'department', 'serious', 'trust', 'problem', 'attempt', 'fix', 'controversy', 'recede', 'past', 'voice', 'call', 'ai', 'defense', 'become', 'loud', 'louder', 'last', 'couple', 'year', 'loudest', 'former', 'ceo', 'chair', 'call', 'take', 'aggressive', 'approach', 'adopt', 'military', 'ai', 'report', 'last', 'year', 'outline', 'step', 'take', 'speed', 'call', 'military', 'invest', 'year', 'technology', 'risk', 'fall', 'chinese', 'military', 'likely', 'spend', 'least', 'year', 'ai', 'accord', 'report', 'georgetown', 'center', 'security', 'emerge', 'technology', 'already', 'significant', 'push', 'underway', 'reach', 'parity', 'say', 'research', 'fellow', 'council', 'foreign', 'relation', 'request', 'artificial', 'intelligence', 'figure', 'reflect', 'total', 'department', 'investment', 'say', 'report', '’', 'military', 'convince', 'need', 'european', 'country', 'tend', 'cautious', 'adopt', 'new', 'technology', 'also', 'spend', 'money', 'say', 'heiko', 'codirector', 'defense', 'ai', 'observatory', 'french', 'identify', 'key', 'defense', 'technology', 'executive', 'arm', 'earmark', 'develop', 'new', 'defense', 'technology', 'building', 'demand', 'thing', 'get', 'military', 'adopt', 'entirely', 'lot', 'country', 'push', 'narrative', 'struggle', 'move', 'concept', 'deployment', 'say', 'ceo', 'preligen', 'french', 'startup', 'sell', 'surveillance', '’', 'partly', 'defense', 'industry', 'country', 'still', 'usually', 'dominate', 'clutch', 'large', 'contractor', 'tend', 'expertise', 'military', 'hardware', 'software', 'say', '’', 'also', 'clunky', 'military', 'vetting', 'process', 'move', 'slowly', 'compare', 'breakneck', 'speed', 'use', 'see', 'development', 'military', 'contract', 'span', 'decade', 'fastpaced', 'startup', 'cycle', 'company', 'year', 'get', 'ground', 'startup', 'venture', 'capitalist', 'express', 'frustration', 'process', 'move', 'slowly', 'risk', 'argue', 'general', 'partner', 'talented', 'engineer', 'leave', 'frustration', 'job', 'startup', 'go', 'bankrupt', 'wait', 'defense', 'contract', 'hoop', 'totally', 'critical', 'particularly', 'sector', 'security', 'concern', 'real', 'say', 'found', 'datum', 'analytic', 'company', 'work', 'british', 'military', 'way', 'enshrine', 'position', 'incumbent', 'ai', 'company', 'military', 'ambition', 'stay', 'business', 'long', 'time', 'say', 'ngor', 'luong', 'research', 'analyst', 'study', 'investment', 'trend', 'georgetown', 'center', 'security', 'emerge', 'technology', 'military', 'bind', 'say', 'go', 'fast', 'risk', 'deploy', 'dangerous', 'broken', 'system', 'go', 'slow', 'miss', 'technological', 'advancement', 'want', 'go', 'fast', 'dod', 'enlist', 'help', 'former', 'ai', 'chief', 'ridehaile', 'company', 'lyft', 'martell', 'take', 'helm', 'new', 'chief', 'digital', 'artificial', 'intelligence', 'office', 'aim', 'coordinate', 'military', 'ai', 'mission', 'tell', 'change', 'culture', 'department', 'boost', 'military', 'use', 'ai', 'bureaucratic', 'inertia', 'push', 'open', 'door', 'company', 'already', 'start', 'snap', 'lucrative', 'military', 'contract', 'anduril', 'fiveyearold', 'startup', 'develop', 'autonomous', 'defense', 'system', 'sophisticated', 'underwater', 'drone', 'win', 'defense', 'contract', 'startup', 'provide', 'datum', 'labeling', 'service', 'win', 'contract', 'defense', 'steady', 'march', 'ai', 'field', 'battle', 'ethical', 'concern', 'prompt', 'protest', 'project', 'go', 'away', 'effort', 'assuage', 'concern', 'aware', 'trust', 'issue', 'defense', 'roll', 'responsible', 'artificial', 'intelligence', 'guideline', 'developer', 'ethical', 'guideline', 'use', 'ai', 'strategy', 'set', 'voluntary', 'ethical', 'guideline', 'member', 'nation', 'guideline', 'call', 'military', 'use', 'ai', 'way', 'lawful', 'responsible', 'reliable', 'traceable', 'seek', 'mitigate', 'bias', 'embed', 'algorithm', 'key', 'concept', 'human', 'always', 'retain', 'control', 'system', 'technology', 'develop', 'really', 'possible', 'say', 'whole', 'point', 'autonomous', 'system', 'allow', 'make', 'decision', 'fast', 'accurately', 'human', 'scale', 'human', 'say', 'effectively', 'hamstring', 'say', 'go', 'lawyer', 'decision', 'still', 'critic', 'say', 'strong', 'rule', 'need', 'global', 'campaign', 'call', 'stop', 'killer', 'robot', 'seek', 'ban', 'lethal', 'autonomous', 'weapon', 'drone', 'swarm', 'activist', 'highprofile', 'official', 'chief', 'guterre', 'government', 'argue', 'autonomous', 'weapon', 'deeply', 'unethical', 'give', 'machine', 'control', 'lifeanddeath', 'decision', 'disproportionately', 'harm', 'marginalize', 'community', 'algorithmic', 'bias', 'swarm', 'thousand', 'autonomous', 'drone', 'example', 'essentially', 'become', 'weapon', 'mass', 'destruction', 'restrict', 'technology', 'uphill', 'battle', 'idea', 'global', 'ban', 'face', 'opposition', 'big', 'military', 'spender', 'ultimately', 'new', 'era', 'military', 'ai', 'raise', 'slew', 'difficult', 'ethical', 'question', 'answer', 'yet', 'question', 'automate', 'want', 'armed', 'force', 'first', 'place', 'say', 'payne', 'hand', 'system', 'reduce', 'casualty', 'make', 'war', 'target', 'effectively', 'create', 'robot', 'mercenary', 'force', 'fight', 'behalf', 'say', 'distance', 'society', 'consequence', 'violence']"
These simple changes can make AI research much more energy efficient,https://www.technologyreview.com/2022/07/06/1055458/ai-research-emissions-energy-efficient/,2022-07-06,"<p>Tweaking the settings of the cloud service an algorithm runs on can have a big impact, researchers found. But not many people bother to do it.</p>
","Deep learning is behind machine learning’s most high-profile successes, such as advanced image recognition, the board game champion AlphaGo, and language models like GPT-3. But this incredible performance comes at a cost: training deep-learning models requires huge amounts of energy. Now, new research shows how scientists who use cloud platforms to train deep-learning algorithms can dramatically reduce the energy they consume, and therefore the emissions this work generates. Simple changes to cloud settings are the key.  Since the first paper studying this technology’s impact on the environment was published three years ago, a movement has grown among researchers to self-report the energy consumed and emissions generated from their work. Having accurate numbers is an important step toward making changes, but actually gathering those numbers can be a challenge. “You can’t improve what you can’t measure,” says Jesse Dodge, a research scientist at the Allen Institute for AI in Seattle. “The first step for us, if we want to make progress on reducing emissions, is we have to get a good measurement.” To that end, the Allen Institute recently collaborated with Microsoft, the AI company Hugging Face, and three universities to create a tool that measures the electricity usage of any machine-learning program that runs on Azure, Microsoft’s cloud service. With it, Azure users building new models can view the total electricity consumed by graphics processing units (GPUs)—computer chips specialized for running calculations in parallel—during every phase of their project, from selecting a model to training it and putting it to use. It’s the first major cloud provider to give users access to information about the energy impact of their machine-learning programs.  While tools already exist that measure energy use and emissions from machine-learning algorithms running on local servers, those tools don’t work when researchers use cloud services provided by companies like Microsoft, Amazon, and Google. Those services don’t give users direct visibility into the GPU, CPU, and memory resources their activities consume—and the existing tools, like Carbontracker, Experiment Tracker, EnergyVis, and CodeCarbon, need those values in order to provide accurate estimates. The new Azure tool, which debuted in October, currently reports energy use, not emissions. So Dodge and other researchers figured out how to map energy use to emissions, and they presented a companion paper on that work at FAccT, a major computer science conference, in late June. Researchers used a service called Watttime to estimate emissions based on the zip codes of cloud servers running 11 machine-learning models. They found that emissions can be significantly reduced if researchers use servers in specific geographic locations and at certain times of day. Emissions from training small machine-learning models can be reduced up to 80% if the training starts at times when more renewable electricity is available on the grid, while emissions from large models can be reduced over 20% if the training work is paused when renewable electricity is scarce and restarted when it’s more plentiful.  Honorees from this year's 35 Innovators list are employing AI to find new molecules, fold proteins, and analyze massive amounts of medical data. Energy-conscious cloud users can lower their emissions by adjusting those factors through preference settings on the three biggest cloud services (Microsoft Azure, Google Cloud, and Amazon Web Services).  But Lynn Kaack, cofounder of Climate Change AI, an organization that studies the impact of machine learning on climate change, says cloud providers should pause and restart these projects automatically to optimize for lower emissions. “You can schedule, of course, when to run the algorithm, but it’s a lot of manual work,” says Kaack. “You need policy incentives, probably, to really do this at scale.” She says policies like carbon pricing could incentivize cloud providers to build workflows that enable automatic pauses and restarts and allow users to opt in. A lot more work still needs to be done to make machine learning more environmentally friendly, especially while most countries are still dependent on fossil fuels. And Dodge says that Azure’s tool only measures the electricity consumed by GPUs. A more accurate calculation of machine learning’s energy consumption would include CPU and memory usage—not to mention the energy for building and cooling the physical servers. And changing habits can take time. Only 13% of Azure users running machine-learning programs have looked at the energy measurement tool since it debuted in October, Dodge says. And Raghavendra Selvan, who helped create Carbontracker, said even he has trouble persuading researchers to use the tool in their machine-learning research.  “I don’t think I have been able to convince my own group,” Selvan says. But he is optimistic. More researchers are getting into the habit of reporting energy use in their papers, encouraged by major conferences like NeurIPS that suggest it. Selvan says if more people start to factor in these energy and emissions costs when planning future projects, it could start to reduce machine learning’s impact on climate change. ","Deep learning is behind machine learning’s most high-profile successes, such as advanced image recognition, the board game champion AlphaGo, and language models like GPT-3. But this incredible performance comes at a cost: training deep-learning models requires huge amounts of energy. Now, new research shows how scientists who use cloud platforms to train deep-learning algorithms can dramatically reduce the energy they consume, and therefore the emissions this work generates. Simple changes to cloud settings are the key. Since the first paper studying this technology’s impact on the environment was published three years ago, a movement has grown among researchers to self-report the energy consumed and emissions generated from their work. Having accurate numbers is an important step toward making changes, but actually gathering those numbers can be a challenge. “You can’t improve what you can’t measure,” says Jesse Dodge, a research scientist at the Allen Institute for AI in Seattle. “The first step for us, if we want to make progress on reducing emissions, is we have to get a good measurement.” To that end, the Allen Institute recently collaborated with Microsoft, the AI company Hugging Face, and three universities to create a tool that measures the electricity usage of any machine-learning program that runs on Azure, Microsoft’s cloud service. With it, Azure users building new models can view the total electricity consumed by graphics processing units (GPUs)—computer chips specialized for running calculations in parallel—during every phase of their project, from selecting a model to training it and putting it to use. It’s the first major cloud provider to give users access to information about the energy impact of their machine-learning programs. While tools already exist that measure energy use and emissions from machine-learning algorithms running on local servers, those tools don’t work when researchers use cloud services provided by companies like Microsoft, Amazon, and Google. Those services don’t give users direct visibility into the GPU, CPU, and memory resources their activities consume—and the existing tools, like Carbontracker, Experiment Tracker, EnergyVis, and CodeCarbon, need those values in order to provide accurate estimates. The new Azure tool, which debuted in October, currently reports energy use, not emissions. So Dodge and other researchers figured out how to map energy use to emissions, and they presented a companion paper on that work at FAccT, a major computer science conference, in late June. Researchers used a service called Watttime to estimate emissions based on the zip codes of cloud servers running 11 machine-learning models. They found that emissions can be significantly reduced if researchers use servers in specific geographic locations and at certain times of day. Emissions from training small machine-learning models can be reduced up to 80% if the training starts at times when more renewable electricity is available on the grid, while emissions from large models can be reduced over 20% if the training work is paused when renewable electricity is scarce and restarted when it’s more plentiful. Honorees from this year's 35 Innovators list are employing AI to find new molecules, fold proteins, and analyze massive amounts of medical data. Energy-conscious cloud users can lower their emissions by adjusting those factors through preference settings on the three biggest cloud services (Microsoft Azure, Google Cloud, and Amazon Web Services). But Lynn Kaack, cofounder of Climate Change AI, an organization that studies the impact of machine learning on climate change, says cloud providers should pause and restart these projects automatically to optimize for lower emissions. “You can schedule, of course, when to run the algorithm, but it’s a lot of manual work,” says Kaack. “You need policy incentives, probably, to really do this at scale.” She says policies like carbon pricing could incentivize cloud providers to build workflows that enable automatic pauses and restarts and allow users to opt in. A lot more work still needs to be done to make machine learning more environmentally friendly, especially while most countries are still dependent on fossil fuels. And Dodge says that Azure’s tool only measures the electricity consumed by GPUs. A more accurate calculation of machine learning’s energy consumption would include CPU and memory usage—not to mention the energy for building and cooling the physical servers. And changing habits can take time. Only 13% of Azure users running machine-learning programs have looked at the energy measurement tool since it debuted in October, Dodge says. And Raghavendra Selvan, who helped create Carbontracker, said even he has trouble persuading researchers to use the tool in their machine-learning research. “I don’t think I have been able to convince my own group,” Selvan says. But he is optimistic. More researchers are getting into the habit of reporting energy use in their papers, encouraged by major conferences like NeurIPS that suggest it. Selvan says if more people start to factor in these energy and emissions costs when planning future projects, it could start to reduce machine learning’s impact on climate change.","['deep', 'learning', 'machine', 'learning', 'highprofile', 'success', 'advanced', 'image', 'recognition', 'board', 'game', 'champion', 'alphago', 'language', 'model', 'gpt3', 'incredible', 'performance', 'come', 'cost', 'training', 'deeplearning', 'model', 'require', 'huge', 'amount', 'energy', 'new', 'research', 'show', 'scientist', 'use', 'cloud', 'platform', 'train', 'deeplearning', 'algorithm', 'dramatically', 'reduce', 'energy', 'consume', 'therefore', 'emission', 'work', 'generate', 'simple', 'change', 'cloud', 'setting', 'key', 'first', 'paper', 'study', 'technology', 'impact', 'environment', 'publish', 'year', 'ago', 'movement', 'grow', 'researcher', 'selfreport', 'energy', 'consume', 'emission', 'generate', 'work', 'accurate', 'number', 'important', 'step', 'make', 'change', 'actually', 'gather', 'number', 'challenge', 'improve', 'measure', 'say', 'research', 'scientist', 'seattle', 'first', 'step', 'want', 'make', 'progress', 'reduce', 'emission', 'get', 'good', 'measurement', 'end', 'recently', 'collaborate', 'company', 'hug', 'face', 'university', 'create', 'tool', 'measure', 'electricity', 'usage', 'machinelearne', 'program', 'run', 'service', 'azure', 'user', 'build', 'new', 'model', 'view', 'total', 'electricity', 'consume', 'graphic', 'processing', 'unit', 'gpus', 'computer', 'chip', 'specialize', 'run', 'calculation', 'parallel', 'phase', 'project', 'select', 'model', 'train', 'put', 'use', '’', 'first', 'major', 'cloud', 'provider', 'give', 'user', 'access', 'information', 'energy', 'impact', 'machinelearning', 'program', 'tool', 'already', 'exist', 'measure', 'energy', 'use', 'emission', 'machinelearne', 'algorithm', 'run', 'local', 'server', 'tool', 'work', 'researcher', 'use', 'cloud', 'service', 'provide', 'company', 'amazon', 'service', 'give', 'user', 'direct', 'visibility', 'cpu', 'memory', 'resource', 'activity', 'consume', 'exist', 'tool', 'carbontracker', 'experiment', 'tracker', 'codecarbon', 'need', 'value', 'order', 'provide', 'accurate', 'estimate', 'new', 'azure', 'tool', 'debut', 'currently', 'report', 'energy', 'use', 'emission', 'dodge', 'researcher', 'figure', 'map', 'energy', 'use', 'emission', 'present', 'companion', 'paper', 'work', 'facct', 'major', 'computer', 'science', 'conference', 'late', 'researcher', 'use', 'service', 'call', 'watttime', 'estimate', 'emission', 'base', 'zip', 'code', 'cloud', 'server', 'run', 'machinelearning', 'model', 'find', 'emission', 'significantly', 'reduce', 'researcher', 'use', 'server', 'specific', 'geographic', 'location', 'certain', 'time', 'day', 'emission', 'train', 'small', 'machinelearning', 'model', 'reduce', 'training', 'start', 'time', 'renewable', 'electricity', 'available', 'grid', 'emission', 'large', 'model', 'reduce', 'training', 'work', 'pause', 'renewable', 'electricity', 'scarce', 'restart', '’', 'plentiful', 'honoree', 'year', 'innovator', 'list', 'employ', 'ai', 'find', 'new', 'molecule', 'fold', 'protein', 'analyze', 'massive', 'amount', 'medical', 'datum', 'energyconscious', 'cloud', 'user', 'lower', 'emission', 'adjust', 'factor', 'preference', 'setting', 'big', 'cloud', 'service', 'azure', 'cloud', 'amazon', 'web', 'service', 'lynn', 'kaack', 'cofounder', 'climate', 'change', 'ai', 'organization', 'study', 'impact', 'machine', 'learn', 'climate', 'change', 'say', 'cloud', 'provider', 'pause', 'restart', 'project', 'automatically', 'optimize', 'low', 'emission', 'schedule', 'course', 'run', 'algorithm', '’', 'lot', 'manual', 'work', 'say', 'kaack', 'need', 'policy', 'incentive', 'probably', 'really', 'scale', 'say', 'policy', 'carbon', 'pricing', 'incentivize', 'cloud', 'provider', 'build', 'workflow', 'enable', 'automatic', 'pause', 'restart', 'allow', 'user', 'opt', 'lot', 'work', 'still', 'need', 'make', 'machine', 'learn', 'environmentally', 'friendly', 'especially', 'country', 'still', 'dependent', 'fossil', 'fuel', 'say', 'azure', 'tool', 'measure', 'electricity', 'consume', 'accurate', 'calculation', 'machine', 'learning', 'energy', 'consumption', 'include', 'cpu', 'memory', 'usage', 'mention', 'energy', 'build', 'cool', 'physical', 'server', 'change', 'habit', 'take', 'time', 'azure', 'user', 'run', 'machinelearne', 'program', 'look', 'energy', 'measurement', 'tool', 'debut', 'say', 'help', 'create', 'carbontracker', 'say', 'even', 'trouble', 'persuade', 'researcher', 'use', 'tool', 'machinelearne', 'research', 'think', 'able', 'convince', 'group', 'say', 'optimistic', 'researcher', 'get', 'habit', 'report', 'energy', 'use', 'paper', 'encourage', 'major', 'conference', 'neurip', 'suggest', 'say', 'people', 'start', 'factor', 'energy', 'emission', 'cost', 'plan', 'future', 'project', 'start', 'reduce', 'machine', 'learning', 'impact', 'climate', 'change']"
The book ban movement has a chilling new tactic: harassing teachers on social media,https://www.technologyreview.com/2022/07/15/1055959/book-bans-social-media-harassment/,2022-07-15,"<p>Educators who stand up to conservative activists are being harassed and called “groomers” online, turning them into potential targets for real-world violence.</p>
","Nancy Vera was awakened suddenly at midnight on July 12 by the sound of a single gunshot fired into her yard. She looked at a security camera just in time to see a truck speed away. Vera was shocked but not surprised. The president of the Corpus Christi, Texas, branch of the American Federation of Teachers (AFT), she had recently handed out books with LGBTQ characters at a pride event for local students, alongside a drag queen.  Vera thought the event was a fun opportunity to connect with local parents and distribute books to kids. But conservatives, including her local sheriff, called the event an example of the ""grooming and indoctrination of young people in our country."" ""Grooming"" is a slur commonly used by devotees of the conspiracy theory QAnon, which claims that powerful people and institutions are ensnaring children in sex trafficking rings. “This type of rhetoric is going to get people killed,” she says.  Corpus Christi, where Vera lives, has become a flashpoint for a growing push among Christian and conservative groups across the US to get certain books and topics they deem inappropriate for children removed from school libraries and curriculums. Now the fight is turning increasingly ugly, with people targeting individual teachers’ private social media accounts for scrutiny and even harassment.  On July 9, the conservative group County Citizens Defending Freedom (CCDF) held a public seminar in Corpus Christi about monitoring school curriculums and “researching social media of teachers, school board members, staff of school districts and elected officials,” effectively teaching people how to stalk and harass educators online. Moms for Liberty and allied groups are now training members to monitor teachersâ€™ social media. pic.twitter.com/qlKfpc0LkX “I have been tracking this current movement of book banning since last summer, and this is the first I have seen of a deliberate effort to track or monitor teachers and staff,” says Jonathan Friedman, the director of Free Expression and Education at PEN America, a nonprofit that defends free expression. The CCDF event represents a pivotal moment in the recent spate of book bans across America, he says. (CCDF declined to comment for this story.) The specific issues that would-be book banners focus on vary. Some parents are offended by discussions of race in their children’s books like The Bluest Eye by Toni Morrison. Others want kids kept away from books that discuss gender, sexual orientation, or sex, such as Gender Queer by Maia Kobabe and All Boys Aren’t Blue by George Johnson. And some want books that depict non-Christian beliefs or cultures out of libraries. Activists who flee repression increasingly face zero-click software hacks and other digital threats The recent turn to monitoring educators’ social media accounts is no surprise, says Friedman, given the book-ban movement’s origins in online messaging boards and Facebook groups. “This is a movement that was formed online, so it’s not so much that these activists are moving online so much as they are moving the target from schools to teachers and librarians,” Friedman says. “And it’s not going to stop there.” Vera has seen the impact of this firsthand. In the week since the pride book event, she says, she has been bombarded with threatening Facebook messages and phone calls. In an effort to protect herself, she now carries Mace and has installed home security cameras. Other conservative groups are also tracking teachers’ social media accounts. Moms for Liberty and its offshoot group, Moms for Libraries, have been engaging in this type of  monitoring and have also started distributing “liberty-minded books” with conservative publishing house Brave Books, which claim to “empower this generation’s youth with conservative values” while “glorifying the Lord in all we do.” The Leadership Institute is another conservative group that has justified this tactic. “Anyone who wades into the public discourse using social media is presenting their personal or political views for all to see,” Matthew Hurtt, the director of graduate relations at the Leadership Institute, said in an email to me. “If teachers, administrators, and elected officials espouse objectionable views on social media, there is a good chance they are espousing those views in the classroom or in school board meetings.” One clear pattern is emerging: educators who support teaching sex education and discussing LGBTQ issues are labeled “groomers.”  Gloria Gonzales Dholakia, a school board member in Leander, Texas, says she was called a groomer at a school board meeting that was broadcast online, leading to a slew of hateful comments. A man who attended the meeting made several highly personal remarks, including suggesting that Gonzales Dholakia’s husband, who was sitting just a few feet away in the room, must be abusive. “My kids were watching this online at home. I was so angry and was ready to quit,” she says.  These are small peanuts accounts but this is a glistening example of what these groups are doing: tracking down educators and putting their info on social. This is why you need to lock down *everything* right now. Until you're protected, protect yourself. pic.twitter.com/Ys0zflcPOh Gonzales Dholakia did not quit, but the need to grapple with slurs and online harassment is yet another burden for teachers exhausted by the pandemic and other issues, including mass shootings at schools. Thousands of teachers have either retired or quit the profession in the last two years. Those who stay have to try to work out what to do to protect themselves and their colleagues. But resources for dealing with the online harassment of educators are sparse because it is such a relatively new problem, says Viktorya Vilk, the director for digital safety and free expression at PEN America. PEN America has created a step-by-step guide to prepare and help people respond to online harassment. But sometimes it’s too little, too late, Vilk says: “So many of these educators are quitting jobs, and those quitting jobs are disproportionately women and people of color — the exact people we don’t want to quit because that means our libraries and schools are less diverse and don’t reflect the full range of American experience. It’s really alarming.” Educators like Vera refuse to stand back. She recently joined colleagues at a counterprotest to voice their concerns about their safety, and she’s spending the next few weeks before schools reopen beefing up protection measures for her colleagues. The Corpus Christi police department is investigating the shooting, and the AFT has added a security detail for her. She’s working to install security cameras at schools and is advising new teachers on how to deal with online harassment. “I’m not going to stop what I’m doing,” she says. Editors note: This story previously misstated that a gunshot ricocheted off Nancy Vera's home, but the gunshot was fired into her yard. The story has been updated. ","Nancy Vera was awakened suddenly at midnight on July 12 by the sound of a single gunshot fired into her yard. She looked at a security camera just in time to see a truck speed away. Vera was shocked but not surprised. The president of the Corpus Christi, Texas, branch of the American Federation of Teachers (AFT), she had recently handed out books with LGBTQ characters at a pride event for local students, alongside a drag queen. Vera thought the event was a fun opportunity to connect with local parents and distribute books to kids. But conservatives, including her local sheriff, called the event an example of the ""grooming and indoctrination of young people in our country."" ""Grooming"" is a slur commonly used by devotees of the conspiracy theory QAnon, which claims that powerful people and institutions are ensnaring children in sex trafficking rings. “This type of rhetoric is going to get people killed,” she says. Corpus Christi, where Vera lives, has become a flashpoint for a growing push among Christian and conservative groups across the US to get certain books and topics they deem inappropriate for children removed from school libraries and curriculums. Now the fight is turning increasingly ugly, with people targeting individual teachers’ private social media accounts for scrutiny and even harassment. On July 9, the conservative group County Citizens Defending Freedom (CCDF) held a public seminar in Corpus Christi about monitoring school curriculums and “researching social media of teachers, school board members, staff of school districts and elected officials,” effectively teaching people how to stalk and harass educators online. Moms for Liberty and allied groups are now training members to monitor teachersâ€™ social media. pic.twitter.com/qlKfpc0LkX “I have been tracking this current movement of book banning since last summer, and this is the first I have seen of a deliberate effort to track or monitor teachers and staff,” says Jonathan Friedman, the director of Free Expression and Education at PEN America, a nonprofit that defends free expression. The CCDF event represents a pivotal moment in the recent spate of book bans across America, he says. (CCDF declined to comment for this story.) The specific issues that would-be book banners focus on vary. Some parents are offended by discussions of race in their children’s books like The Bluest Eye by Toni Morrison. Others want kids kept away from books that discuss gender, sexual orientation, or sex, such as Gender Queer by Maia Kobabe and All Boys Aren’t Blue by George Johnson. And some want books that depict non-Christian beliefs or cultures out of libraries. Activists who flee repression increasingly face zero-click software hacks and other digital threats The recent turn to monitoring educators’ social media accounts is no surprise, says Friedman, given the book-ban movement’s origins in online messaging boards and Facebook groups. “This is a movement that was formed online, so it’s not so much that these activists are moving online so much as they are moving the target from schools to teachers and librarians,” Friedman says. “And it’s not going to stop there.” Vera has seen the impact of this firsthand. In the week since the pride book event, she says, she has been bombarded with threatening Facebook messages and phone calls. In an effort to protect herself, she now carries Mace and has installed home security cameras. Other conservative groups are also tracking teachers’ social media accounts. Moms for Liberty and its offshoot group, Moms for Libraries, have been engaging in this type of monitoring and have also started distributing “liberty-minded books” with conservative publishing house Brave Books, which claim to “empower this generation’s youth with conservative values” while “glorifying the Lord in all we do.” The Leadership Institute is another conservative group that has justified this tactic. “Anyone who wades into the public discourse using social media is presenting their personal or political views for all to see,” Matthew Hurtt, the director of graduate relations at the Leadership Institute, said in an email to me. “If teachers, administrators, and elected officials espouse objectionable views on social media, there is a good chance they are espousing those views in the classroom or in school board meetings.” One clear pattern is emerging: educators who support teaching sex education and discussing LGBTQ issues are labeled “groomers.” Gloria Gonzales Dholakia, a school board member in Leander, Texas, says she was called a groomer at a school board meeting that was broadcast online, leading to a slew of hateful comments. A man who attended the meeting made several highly personal remarks, including suggesting that Gonzales Dholakia’s husband, who was sitting just a few feet away in the room, must be abusive. “My kids were watching this online at home. I was so angry and was ready to quit,” she says. These are small peanuts accounts but this is a glistening example of what these groups are doing: tracking down educators and putting their info on social. This is why you need to lock down *everything* right now. Until you're protected, protect yourself. pic.twitter.com/Ys0zflcPOh Gonzales Dholakia did not quit, but the need to grapple with slurs and online harassment is yet another burden for teachers exhausted by the pandemic and other issues, including mass shootings at schools. Thousands of teachers have either retired or quit the profession in the last two years. Those who stay have to try to work out what to do to protect themselves and their colleagues. But resources for dealing with the online harassment of educators are sparse because it is such a relatively new problem, says Viktorya Vilk, the director for digital safety and free expression at PEN America. PEN America has created a step-by-step guide to prepare and help people respond to online harassment. But sometimes it’s too little, too late, Vilk says: “So many of these educators are quitting jobs, and those quitting jobs are disproportionately women and people of color — the exact people we don’t want to quit because that means our libraries and schools are less diverse and don’t reflect the full range of American experience. It’s really alarming.” Educators like Vera refuse to stand back. She recently joined colleagues at a counterprotest to voice their concerns about their safety, and she’s spending the next few weeks before schools reopen beefing up protection measures for her colleagues. The Corpus Christi police department is investigating the shooting, and the AFT has added a security detail for her. She’s working to install security cameras at schools and is advising new teachers on how to deal with online harassment. “I’m not going to stop what I’m doing,” she says. Editors note: This story previously misstated that a gunshot ricocheted off Nancy Vera's home, but the gunshot was fired into her yard. The story has been updated.","['awaken', 'suddenly', 'midnight', 'sound', 'single', 'gunshot', 'fire', 'yard', 'look', 'security', 'camera', 'time', 'see', 'truck', 'speed', 'vera', 'shock', 'surprise', 'president', 'branch', 'teacher', 'aft', 'recently', 'hand', 'book', 'lgbtq', 'character', 'pride', 'event', 'local', 'student', 'drag', 'queen', 'vera', 'think', 'event', 'fun', 'opportunity', 'connect', 'local', 'parent', 'distribute', 'book', 'kid', 'conservative', 'include', 'local', 'sheriff', 'call', 'event', 'example', 'grooming', 'indoctrination', 'young', 'people', 'country', 'grooming', 'slur', 'commonly', 'use', 'devotee', 'conspiracy', 'theory', 'qanon', 'claim', 'powerful', 'people', 'institution', 'ensnare', 'child', 'sex', 'trafficking', 'ring', 'type', 'rhetoric', 'go', 'get', 'people', 'kill', 'say', 'vera', 'live', 'become', 'flashpoint', 'grow', 'push', 'christian', 'conservative', 'group', 'get', 'certain', 'book', 'topic', 'deem', 'inappropriate', 'child', 'remove', 'school', 'library', 'curriculum', 'fight', 'turn', 'increasingly', 'ugly', 'people', 'target', 'individual', 'teacher', 'private', 'social', 'medium', 'account', 'scrutiny', 'even', 'harassment', 'conservative', 'group', 'county', 'citizen', 'defend', 'freedom', 'hold', 'public', 'seminar', 'monitor', 'school', 'curriculum', 'research', 'social', 'medium', 'teacher', 'board', 'member', 'staff', 'school', 'district', 'elect', 'official', 'effectively', 'teach', 'people', 'stalk', 'harass', 'educator', 'online', 'mom', 'liberty', 'allied', 'group', 'train', 'member', 'monitor', 'teachersâ€', '™', 'social', 'medium', 'pictwittercomqlkfpc0lkx', 'track', 'current', 'movement', 'book', 'ban', 'last', 'summer', 'first', 'see', 'deliberate', 'effort', 'track', 'monitor', 'teacher', 'staff', 'say', 'director', 'free', 'expression', 'education', 'nonprofit', 'defend', 'free', 'expression', 'event', 'represent', 'pivotal', 'moment', 'recent', 'spate', 'book', 'ban', 'say', 'decline', 'comment', 'story', 'specific', 'issue', 'wouldbe', 'book', 'banner', 'focus', 'vary', 'parent', 'offend', 'discussion', 'race', 'child', 'book', 'blue', 'eye', 'toni', 'morrison', 'want', 'kid', 'keep', 'away', 'book', 'discuss', 'gender', 'sexual', 'orientation', 'sex', 'gender', 'queer', 'boy', 'blue', 'want', 'book', 'depict', 'nonchristian', 'belief', 'culture', 'library', 'activist', 'flee', 'repression', 'increasingly', 'face', 'software', 'hack', 'digital', 'threat', 'recent', 'turn', 'monitor', 'educator', 'social', 'medium', 'account', 'surprise', 'say', 'friedman', 'give', 'bookban', 'movement', 'origin', 'online', 'messaging', 'board', 'facebook', 'group', 'movement', 'form', 'online', '’', 'much', 'activist', 'move', 'online', 'much', 'move', 'target', 'school', 'teacher', 'say', 'go', 'stop', 'vera', 'see', 'impact', 'firsthand', 'week', 'pride', 'book', 'event', 'say', 'bombard', 'threaten', 'facebook', 'message', 'phone', 'call', 'effort', 'protect', 'carry', 'mace', 'instal', 'home', 'security', 'camera', 'conservative', 'group', 'also', 'track', 'teacher', 'social', 'medium', 'account', 'mom', 'liberty', 'offshoot', 'group', 'mom', 'library', 'engage', 'type', 'monitoring', 'also', 'start', 'distribute', 'libertyminde', 'book', 'conservative', 'publishing', 'house', 'brave', 'book', 'claim', 'empower', 'generation', 'youth', 'conservative', 'value', 'glorify', 'leadership', 'institute', 'conservative', 'group', 'justify', 'tactic', 'wade', 'public', 'discourse', 'use', 'social', 'medium', 'present', 'personal', 'political', 'view', 'see', 'matthew', 'hurtt', 'director', 'graduate', 'relation', 'say', 'email', 'teacher', 'administrator', 'elect', 'official', 'espouse', 'objectionable', 'view', 'social', 'medium', 'good', 'chance', 'espouse', 'view', 'classroom', 'school', 'board', 'meeting', 'clear', 'pattern', 'emerge', 'educator', 'support', 'teach', 'sex', 'education', 'discuss', 'lgbtq', 'issue', 'label', 'groomer', 'gloria', 'gonzale', 'dholakia', 'school', 'board', 'member', 'say', 'call', 'groomer', 'school', 'board', 'meeting', 'broadcast', 'online', 'lead', 'slew', 'hateful', 'comment', 'man', 'attend', 'meeting', 'make', 'several', 'highly', 'personal', 'remark', 'include', 'suggest', 'husband', 'sit', 'foot', 'away', 'room', 'abusive', 'kid', 'watch', 'online', 'home', 'angry', 'ready', 'quit', 'say', 'small', 'peanut', 'account', 'glisten', 'example', 'group', 'track', 'educator', 'put', 'info', 'social', 'need', 'lock', 'right', 'protect', 'protect', 'pictwittercomys0zflcpoh', 'gonzale', 'dholakia', 'quit', 'need', 'grapple', 'slur', 'online', 'harassment', 'yet', 'burden', 'teacher', 'exhaust', 'pandemic', 'issue', 'include', 'mass', 'shooting', 'school', 'thousand', 'teacher', 'retire', 'quit', 'profession', 'last', 'year', 'stay', 'try', 'work', 'protect', 'colleague', 'resource', 'deal', 'online', 'harassment', 'educator', 'sparse', 'relatively', 'new', 'problem', 'say', 'vilk', 'director', 'digital', 'safety', 'free', 'expression', 'create', 'stepbystep', 'guide', 'prepare', 'help', 'people', 'respond', 'online', 'harassment', 'sometimes', '’', 'little', 'late', 'vilk', 'say', 'many', 'educator', 'quit', 'job', 'quit', 'job', 'disproportionately', 'woman', 'people', 'color', 'exact', 'people', 'want', 'quit', 'mean', 'library', 'school', 'less', 'diverse', 'reflect', 'full', 'range', 'american', 'experience', '’', 'really', 'alarm', 'educator', 'vera', 'refuse', 'stand', 'back', 'recently', 'join', 'colleague', 'counterprot', 'voice', 'concern', 'safety', 'spend', 'next', 'week', 'school', 'reopen', 'beef', 'protection', 'measure', 'colleague', 'department', 'investigate', 'shooting', 'aft', 'add', 'security', 'detail', 'work', 'install', 'security', 'camera', 'school', 'advise', 'new', 'teacher', 'deal', 'online', 'harassment', 'go', 'stop', 'say', 'editor', 'note', 'story', 'previously', 'misstate', 'gunshot', 'ricochet', 'nancy', 'vera', 'home', 'gunshot', 'fire', 'yard', 'story', 'update']"
How aspiring influencers are forced to fight the algorithm,https://www.technologyreview.com/2022/07/14/1055906/tiktok-influencers-moderation-bias/,2022-07-14,"<p>Figuring out social media platforms’ hidden rules is hard work—and it falls more heavily on creators from marginalized backgrounds.</p>
","Last summer, a TikTok creator named Ziggi Tyler posted a video calling out a disturbing problem he found in the app’s Creator Marketplace, a tool that matches creators with brands looking to pay for sponsored content. Tyler said he was unable to enter phrases like “Black Lives Matter” and “supporting Black excellence” into his Marketplace profile. However, phrases like “white supremacy” and “supporting white excellence” were allowed.  If you’ve spent much time on TikTok, you’ve probably seen creators discuss similar incidents.  There are two ways to try to understand the impact of content moderation and the algorithms that enforce those rules: by relying on what the platform says, and by asking creators themselves. In Tyler’s case, TikTok apologized and blamed an automatic filter that was set up to flag words associated with hate speech—but that was apparently unable to understand context.  Brooke Erin Duffy, an associate professor at Cornell University, teamed up with graduate student Colten Meisner to interview 30 creators on TikTok, Instagram, Twitch, YouTube, and Twitter around the time Tyler’s video went viral. They wanted to know how creators, particularly those from marginalized groups, navigate the algorithms and moderation practices of the platforms they use.  What they found: Creators invest a lot of labor in understanding the algorithms that shape their experiences and relationships on these platforms. Because many creators use multiple platforms, they must learn the hidden rules for each one. Some creators adapt their entire approach to producing and promoting content in response to the algorithmic and moderation biases they encounter.  Below is our conversation with Duffy about her forthcoming research (edited and condensed for clarity).  Creators have long discussed how algorithms and moderation affect their visibility on the platforms that made them famous. So what most surprised you while doing these interviews?  We had a sense that creators’ experiences are shaped by their understanding of the algorithm, but after doing the interviews, we really started to see how profound [this impact] is in their everyday lives and work … the amount of time, energy, and attention they devote to learning about these algorithms, investing in them. They have this kind of critical awareness that these algorithms are understood to be uneven. Despite that, they’re still investing all of this energy in hopes of understanding them. It just really draws attention to the lopsided nature of the creator economy.  How often are creators thinking about the possibility of being censored or having their content not reach their audience because of algorithmic suppression or moderation practices?  I think it fundamentally structures their content creation process and also their content promotion process. These algorithms change at whim; there’s no insight. There’s no direct communication from the platforms, in many cases. And this completely, fundamentally impacts not just your experience, but your income.  They would invest so much time and labor in these grassroots experiments and would talk about “I would do the same kind of content, but I would vary this thing one day. I would wear this kind of outfit one day, and another kind the next.” Or they’d try different sets of hashtags.  People would say they have both online and offline interactions with their creator community, and they would talk about how to game the algorithm, what’s okay to say, what can potentially be flagged. There are some important forms of collective organization that may not look like what we would traditionally think of as organized workers but are still powerful ways for creators to band together and kind of challenge the top-down systems of power.  One of the things I kept thinking about while reading your findings was the concept of “shadow banning,” the moderation practice of hiding or limiting the reach of content without informing its creator. From a journalist’s perspective, “shadow banning” is hard to report on because it is by definition hidden, but it’s one of the main concerns creators have expressed over the years. How did you consider this concept in your research?  Some people swear they've been shadow-banned, and other people say, “Well, your content is just bad.” It’s a very fraught problem, because anyone can issue these claims.  The ambiguity of shadow banning is in part what makes it so powerful. Because there’s no way to actually prove that any one person on these platforms was or was not shadow-banned, that fuels a lot of speculation. But you know, whether it exists or it doesn’t, the fact that people act as if they are punished through limits on their visibility is worth taking seriously.  Is there anything that can be done to help resolve some of these issues?  Platforms tout all over their websites their benefits to creators and [say] if you are talented enough and have the right content, you can connect with audiences and make all kinds of money. The creators are drawing so much money, through data and eyeballs, to these platforms but don't have much of a say in their content moderation policies and how these are unevenly enacted. Radical transparency is a bit pie-in-the-sky, but I do think creators should have more representation in terms of the decisions that fundamentally impact their businesses.  ","Last summer, a TikTok creator named Ziggi Tyler posted a video calling out a disturbing problem he found in the app’s Creator Marketplace, a tool that matches creators with brands looking to pay for sponsored content. Tyler said he was unable to enter phrases like “Black Lives Matter” and “supporting Black excellence” into his Marketplace profile. However, phrases like “white supremacy” and “supporting white excellence” were allowed. If you’ve spent much time on TikTok, you’ve probably seen creators discuss similar incidents. There are two ways to try to understand the impact of content moderation and the algorithms that enforce those rules: by relying on what the platform says, and by asking creators themselves. In Tyler’s case, TikTok apologized and blamed an automatic filter that was set up to flag words associated with hate speech—but that was apparently unable to understand context. Brooke Erin Duffy, an associate professor at Cornell University, teamed up with graduate student Colten Meisner to interview 30 creators on TikTok, Instagram, Twitch, YouTube, and Twitter around the time Tyler’s video went viral. They wanted to know how creators, particularly those from marginalized groups, navigate the algorithms and moderation practices of the platforms they use. What they found: Creators invest a lot of labor in understanding the algorithms that shape their experiences and relationships on these platforms. Because many creators use multiple platforms, they must learn the hidden rules for each one. Some creators adapt their entire approach to producing and promoting content in response to the algorithmic and moderation biases they encounter. Below is our conversation with Duffy about her forthcoming research (edited and condensed for clarity). Creators have long discussed how algorithms and moderation affect their visibility on the platforms that made them famous. So what most surprised you while doing these interviews? We had a sense that creators’ experiences are shaped by their understanding of the algorithm, but after doing the interviews, we really started to see how profound [this impact] is in their everyday lives and work … the amount of time, energy, and attention they devote to learning about these algorithms, investing in them. They have this kind of critical awareness that these algorithms are understood to be uneven. Despite that, they’re still investing all of this energy in hopes of understanding them. It just really draws attention to the lopsided nature of the creator economy. How often are creators thinking about the possibility of being censored or having their content not reach their audience because of algorithmic suppression or moderation practices? I think it fundamentally structures their content creation process and also their content promotion process. These algorithms change at whim; there’s no insight. There’s no direct communication from the platforms, in many cases. And this completely, fundamentally impacts not just your experience, but your income. They would invest so much time and labor in these grassroots experiments and would talk about “I would do the same kind of content, but I would vary this thing one day. I would wear this kind of outfit one day, and another kind the next.” Or they’d try different sets of hashtags. People would say they have both online and offline interactions with their creator community, and they would talk about how to game the algorithm, what’s okay to say, what can potentially be flagged. There are some important forms of collective organization that may not look like what we would traditionally think of as organized workers but are still powerful ways for creators to band together and kind of challenge the top-down systems of power. One of the things I kept thinking about while reading your findings was the concept of “shadow banning,” the moderation practice of hiding or limiting the reach of content without informing its creator. From a journalist’s perspective, “shadow banning” is hard to report on because it is by definition hidden, but it’s one of the main concerns creators have expressed over the years. How did you consider this concept in your research? Some people swear they've been shadow-banned, and other people say, “Well, your content is just bad.” It’s a very fraught problem, because anyone can issue these claims. The ambiguity of shadow banning is in part what makes it so powerful. Because there’s no way to actually prove that any one person on these platforms was or was not shadow-banned, that fuels a lot of speculation. But you know, whether it exists or it doesn’t, the fact that people act as if they are punished through limits on their visibility is worth taking seriously. Is there anything that can be done to help resolve some of these issues? Platforms tout all over their websites their benefits to creators and [say] if you are talented enough and have the right content, you can connect with audiences and make all kinds of money. The creators are drawing so much money, through data and eyeballs, to these platforms but don't have much of a say in their content moderation policies and how these are unevenly enacted. Radical transparency is a bit pie-in-the-sky, but I do think creators should have more representation in terms of the decisions that fundamentally impact their businesses.","['last', 'summer', 'tiktok', 'creator', 'name', 'post', 'video', 'call', 'disturbing', 'problem', 'find', 'creator', 'marketplace', 'tool', 'match', 'creator', 'brand', 'look', 'pay', 'sponsor', 'content', 'say', 'unable', 'enter', 'phrase', 'black', 'life', 'matter', 'support', 'black', 'excellence', 'marketplace', 'profile', 'however', 'phrase', 'white', 'supremacy', 'support', 'white', 'excellence', 'allow', 'spend', 'much', 'time', 'tiktok', 'probably', 'see', 'creator', 'discuss', 'similar', 'incident', 'way', 'try', 'understand', 'impact', 'content', 'moderation', 'algorithm', 'enforce', 'rule', 'rely', 'platform', 'say', 'ask', 'creator', 'case', 'tiktok', 'apologize', 'blame', 'automatic', 'filter', 'set', 'flag', 'word', 'associate', 'hate', 'speech', 'apparently', 'unable', 'understand', 'context', 'duffy', 'associate', 'professor', 'team', 'graduate', 'student', 'colten', 'meisner', 'interview', 'creator', 'tiktok', 'instagram', 'twitch', 'youtube', 'twitter', 'time', 'video', 'go', 'viral', 'want', 'know', 'creator', 'particularly', 'marginalized', 'group', 'navigate', 'algorithm', 'moderation', 'practice', 'platform', 'use', 'find', 'creator', 'invest', 'lot', 'labor', 'understand', 'algorithm', 'shape', 'experience', 'relationship', 'platform', 'many', 'creator', 'use', 'multiple', 'platform', 'learn', 'hide', 'rule', 'one', 'creator', 'adapt', 'entire', 'approach', 'produce', 'promote', 'content', 'response', 'algorithmic', 'moderation', 'bias', 'encounter', 'conversation', 'duffy', 'forthcoming', 'research', 'edit', 'condense', 'clarity', 'creator', 'long', 'discuss', 'algorithm', 'moderation', 'affect', 'visibility', 'platform', 'make', 'famous', 'surprise', 'interview', 'sense', 'creator', 'experience', 'shape', 'understanding', 'interview', 'really', 'start', 'see', 'profound', 'impact', 'everyday', 'life', 'work', 'amount', 'time', 'energy', 'attention', 'devote', 'learn', 'algorithm', 'invest', 'kind', 'critical', 'awareness', 'algorithm', 'understand', 'uneven', 'still', 'invest', 'energy', 'hope', 'understand', 'really', 'draw', 'attention', 'lopsided', 'nature', 'creator', 'economy', 'often', 'creator', 'think', 'possibility', 'censor', 'content', 'reach', 'audience', 'algorithmic', 'suppression', 'moderation', 'practice', 'think', 'fundamentally', 'structure', 'content', 'creation', 'process', 'also', 'content', 'promotion', 'process', 'algorithm', 'change', '’', 'insight', '’', 'direct', 'communication', 'platform', 'many', 'case', 'completely', 'fundamentally', 'impact', 'experience', 'income', 'invest', 'much', 'time', 'labor', 'grassroots', 'experiment', 'talk', 'kind', 'content', 'vary', 'thing', 'day', 'wear', 'kind', 'outfit', 'day', 'kind', 'next', '’d', 'try', 'different', 'set', 'hashtag', 'people', 'say', 'online', 'offline', 'interaction', 'creator', 'community', 'talk', 'game', '’', 'okay', 'say', 'potentially', 'flag', 'important', 'form', 'collective', 'organization', 'look', 'traditionally', 'think', 'organize', 'worker', 'still', 'powerful', 'way', 'creator', 'band', 'together', 'kind', 'challenge', 'topdown', 'system', 'power', 'thing', 'keep', 'think', 'read', 'finding', 'concept', 'shadow', 'ban', 'moderation', 'practice', 'hiding', 'limit', 'reach', 'content', 'inform', 'creator', 'journalist', 'perspective', 'shadow', 'ban', 'hard', 'report', 'definition', 'hide', '’', 'main', 'concern', 'creator', 'express', 'year', 'consider', 'concept', 'research', 'people', 'swear', 'shadowbanne', 'people', 'say', 'content', 'bad', '’', 'fraught', 'problem', 'issue', 'claim', 'ambiguity', 'shadow', 'ban', 'part', 'make', 'powerful', '’', 'way', 'actually', 'prove', 'person', 'platform', 'shadowbanne', 'fuel', 'lot', 'speculation', 'know', 'exist', 'fact', 'people', 'act', 'punish', 'limit', 'visibility', 'worth', 'take', 'seriously', 'help', 'resolve', 'issue', 'platform', 'tout', 'website', 'benefit', 'creator', 'say', 'talente', 'enough', 'right', 'content', 'connect', 'audience', 'make', 'kind', 'money', 'creator', 'draw', 'much', 'money', 'datum', 'eyeball', 'platform', 'much', 'say', 'content', 'moderation', 'policy', 'unevenly', 'enact', 'radical', 'transparency', 'bit', 'pieinthesky', 'think', 'creator', 'representation', 'term', 'decision', 'fundamentally', 'impact', 'business']"
New York City is drowning in packages,https://www.technologyreview.com/2022/07/12/1055161/new-york-city-packages/,2022-07-12,"<p>Online orders, which ramped up with the start of the pandemic, are still clogging city streets.</p>
","Amazon, Hello Fresh, Stitch Fix. Click a button, and it’s there in three to five days—perhaps even one. Packages, packages, and more packages—goods from all over the world, delivered after just a couple of clicks. But this height of consumer convenience has been complicating urban life for years, giving rise to increased theft and traffic, package waste, and a landscape of struggling local businesses. Some cities, especially in Europe and Japan, are implementing regulations that dramatically curtail package-related stress. But not New York City—not yet. Three years ago, more than 1.8 million packages were delivered to the Big Apple on a typical day, according to data collected by the Rensselaer Polytechnic Institute Center of Excellence for Sustainable Urban Freight Systems. Just a few months into the pandemic, however, that number had increased to nearly 2.3 million. And that’s only counting your typical e-commerce packages, says José Holguín-Veras, the center’s director. Altogether, with groceries and prepared food, total daily deliveries stacked up to more than 3.7 million, the center estimates. That’s nearly enough to deliver one item each to half the people in New York every day. Two years into the pandemic, in March 2022, the number had barely dropped, to just under 3.6 million. People, Holguín-Veras surmises, simply got used to ordering everything to their door.  “It makes logical sense,” Holguín-Veras says. After all, the pandemic upended how we move through the world, especially when it comes to shopping and eating out. But e-commerce comes with significant costs that are not reflected in the purchase price. For instance, a recent study found that New York ranks as the most traffic-congested city in the US. Freight delivery plays a significant role in the problem: a November 2021 report estimates that delivering more than 2 million e-commerce parcels a day requires some 7,800 freight vehicles, each occupying city streets and roads for eight hours. That’s a total of more 60,000 vehicle-hours each day. A singular focus on high-tech will dilute the vibrancy of our cities and limit their potential. Noticing the increase in e-commerce delivery traffic, then-mayor Bill de Blasio allocated $38 million in the November 2021 budget to shipping these packages via the “blue highway”––by ferry instead of by truck. “One of the best ways to fight climate change is to get away from a society and economy dominated by big trucks,” he said in late 2021. “[A]nd that’s just the truth in New York City and America today: the reign of the 18-wheeler. It’s supreme; it’s everywhere, and it’s a danger to our future.”  Other attempts to reduce delivery-truck congestion have popped up. There are cargo bikes, for example, and a potential $3 surcharge on every “nonessential” package delivered. Lockers are also a key player; they help tackle the “last mile” problem—or the last leg of the delivery process—by centralizing drop-off locations to save the door-to-door toil. Amazon-exclusive lockers live in 7-Elevens, Rite-Aids, Whole Foods markets, and Chase Banks. Retailer-agnostic locker services, such as Stowfly, also exist. The company’s lockers can be found at a range of locations including smaller mom-and-pop stores. Stowfly CEO Sid Khattri says the approach solves two problems at once: centralizing e-commerce delivery while helping local businesses “make extra income and get foot traffic at a time when physical retail is dying.”  It’s helpful to step back and put the parcel problem in historical context, says David Vega-Barachowitz, an associate at WXY, an architecture firm in New York City. The city’s package problem is not just about congested streets or inefficient distribution of resources, he says. Rather, it’s another crisis of convenience, akin to when, in the 1950s, suburban shopping centers began competing with city downtowns. “We live in a city whose main pitch is the ability to walk out your door, get a carton of milk, go to a bookstore, go to a movie, etc.,” he says, “and convenience culture is threatening all of that.” Arthur Getman, who was director of analytics at the New York City Department of Transportation and is now at Replica, agrees. “A lot of people coming to New York bring the mentality of the ‘American Dream,’” he says, but here’s the problem: that dream is largely based in suburbia. The city just doesn’t have the space for that—for everyone to have their house, their lawn, their car, and their stuff. With its public transportation, bike lanes, sidewalks, parks, and apartment buildings, New York City is made to share. As everyone from city planners to apartment building managers copes with the rise of e-commerce, Holguín-Veras, after poring over the data for years, can’t help but ask: “Of all the purchases made, what percent of those are truly urgent?” Sarah Simon is a freelance multimedia journalist based in New York City.  This story has been updated to reflect Arthur Getman's present affiliation.  ","Amazon, Hello Fresh, Stitch Fix. Click a button, and it’s there in three to five days—perhaps even one. Packages, packages, and more packages—goods from all over the world, delivered after just a couple of clicks. But this height of consumer convenience has been complicating urban life for years, giving rise to increased theft and traffic, package waste, and a landscape of struggling local businesses. Some cities, especially in Europe and Japan, are implementing regulations that dramatically curtail package-related stress. But not New York City—not yet. Three years ago, more than 1.8 million packages were delivered to the Big Apple on a typical day, according to data collected by the Rensselaer Polytechnic Institute Center of Excellence for Sustainable Urban Freight Systems. Just a few months into the pandemic, however, that number had increased to nearly 2.3 million. And that’s only counting your typical e-commerce packages, says José Holguín-Veras, the center’s director. Altogether, with groceries and prepared food, total daily deliveries stacked up to more than 3.7 million, the center estimates. That’s nearly enough to deliver one item each to half the people in New York every day. Two years into the pandemic, in March 2022, the number had barely dropped, to just under 3.6 million. People, Holguín-Veras surmises, simply got used to ordering everything to their door. “It makes logical sense,” Holguín-Veras says. After all, the pandemic upended how we move through the world, especially when it comes to shopping and eating out. But e-commerce comes with significant costs that are not reflected in the purchase price. For instance, a recent study found that New York ranks as the most traffic-congested city in the US. Freight delivery plays a significant role in the problem: a November 2021 report estimates that delivering more than 2 million e-commerce parcels a day requires some 7,800 freight vehicles, each occupying city streets and roads for eight hours. That’s a total of more 60,000 vehicle-hours each day. A singular focus on high-tech will dilute the vibrancy of our cities and limit their potential. Noticing the increase in e-commerce delivery traffic, then-mayor Bill de Blasio allocated $38 million in the November 2021 budget to shipping these packages via the “blue highway”––by ferry instead of by truck. “One of the best ways to fight climate change is to get away from a society and economy dominated by big trucks,” he said in late 2021. “[A]nd that’s just the truth in New York City and America today: the reign of the 18-wheeler. It’s supreme; it’s everywhere, and it’s a danger to our future.” Other attempts to reduce delivery-truck congestion have popped up. There are cargo bikes, for example, and a potential $3 surcharge on every “nonessential” package delivered. Lockers are also a key player; they help tackle the “last mile” problem—or the last leg of the delivery process—by centralizing drop-off locations to save the door-to-door toil. Amazon-exclusive lockers live in 7-Elevens, Rite-Aids, Whole Foods markets, and Chase Banks. Retailer-agnostic locker services, such as Stowfly, also exist. The company’s lockers can be found at a range of locations including smaller mom-and-pop stores. Stowfly CEO Sid Khattri says the approach solves two problems at once: centralizing e-commerce delivery while helping local businesses “make extra income and get foot traffic at a time when physical retail is dying.” It’s helpful to step back and put the parcel problem in historical context, says David Vega-Barachowitz, an associate at WXY, an architecture firm in New York City. The city’s package problem is not just about congested streets or inefficient distribution of resources, he says. Rather, it’s another crisis of convenience, akin to when, in the 1950s, suburban shopping centers began competing with city downtowns. “We live in a city whose main pitch is the ability to walk out your door, get a carton of milk, go to a bookstore, go to a movie, etc.,” he says, “and convenience culture is threatening all of that.” Arthur Getman, who was director of analytics at the New York City Department of Transportation and is now at Replica, agrees. “A lot of people coming to New York bring the mentality of the ‘American Dream,’” he says, but here’s the problem: that dream is largely based in suburbia. The city just doesn’t have the space for that—for everyone to have their house, their lawn, their car, and their stuff. With its public transportation, bike lanes, sidewalks, parks, and apartment buildings, New York City is made to share. As everyone from city planners to apartment building managers copes with the rise of e-commerce, Holguín-Veras, after poring over the data for years, can’t help but ask: “Of all the purchases made, what percent of those are truly urgent?” Sarah Simon is a freelance multimedia journalist based in New York City. This story has been updated to reflect Arthur Getman's present affiliation.","['amazon', 'fresh', 'stitch', 'fix', 'click', 'button', '’', 'day', 'perhaps', 'even', 'package', 'package', 'package', 'good', 'world', 'deliver', 'couple', 'click', 'height', 'consumer', 'convenience', 'complicate', 'urban', 'life', 'year', 'give', 'rise', 'increase', 'theft', 'traffic', 'package', 'waste', 'landscape', 'struggle', 'local', 'business', 'city', 'especially', 'implement', 'regulation', 'dramatically', 'curtail', 'packagerelate', 'stress', 'yet', 'year', 'ago', 'package', 'deliver', 'big', 'apple', 'typical', 'day', 'accord', 'datum', 'collect', 'center', 'excellence', 'sustainable', 'urban', 'freight', 'system', 'month', 'pandemic', 'however', 'number', 'increase', 'nearly', 'count', 'typical', 'ecommerce', 'package', 'say', 'center', 'director', 'altogether', 'grocery', 'prepare', 'food', 'total', 'daily', 'delivery', 'stack', 'center', 'estimate', '’', 'nearly', 'enough', 'deliver', 'item', 'people', 'day', 'year', 'pandemic', 'number', 'barely', 'drop', 'people', 'holguínveras', 'surmise', 'simply', 'use', 'order', 'door', 'make', 'logical', 'sense', 'holguínveras', 'say', 'pandemic', 'upended', 'move', 'world', 'especially', 'come', 'shop', 'eat', 'ecommerce', 'come', 'significant', 'cost', 'reflect', 'purchase', 'price', 'instance', 'recent', 'study', 'find', 'rank', 'trafficcongeste', 'city', 'freight', 'delivery', 'play', 'significant', 'role', 'problem', 'report', 'estimate', 'deliver', 'ecommerce', 'parcel', 'day', 'require', 'freight', 'vehicle', 'occupy', 'city', 'street', 'road', 'hour', '’', 'total', 'vehiclehour', 'day', 'singular', 'focus', 'dilute', 'vibrancy', 'city', 'limit', 'potential', 'notice', 'increase', 'ecommerce', 'delivery', 'traffic', 'thenmayor', 'bill', 'allocate', 'budget', 'ship', 'package', 'ferry', 'instead', 'truck', 'good', 'way', 'fight', 'climate', 'change', 'get', 'away', 'society', 'economy', 'dominate', 'big', 'truck', 'say', 'late', '’', 'truth', 'today', 'reign', '’', '’', 'everywhere', '’', 'danger', 'future', 'attempt', 'reduce', 'deliverytruck', 'congestion', 'pop', 'cargo', 'bike', 'example', 'potential', 'surcharge', 'nonessential', 'package', 'deliver', 'locker', 'also', 'key', 'player', 'help', 'tackle', 'last', 'mile', 'problem', 'last', 'leg', 'delivery', 'process', 'centralize', 'dropoff', 'location', 'save', 'doortodoor', 'toil', 'amazonexclusive', 'locker', 'live', 'riteaid', 'whole', 'food', 'market', 'chase', 'bank', 'retaileragnostic', 'locker', 'service', 'stowfly', 'also', 'exist', 'company', 'locker', 'find', 'range', 'location', 'include', 'small', 'momandpop', 'store', 'stowfly', 'say', 'approach', 'solve', 'problem', 'centralize', 'ecommerce', 'delivery', 'help', 'local', 'business', 'make', 'extra', 'income', 'get', 'foot', 'traffic', 'time', 'physical', 'retail', 'die', '’', 'helpful', 'step', 'back', 'put', 'parcel', 'problem', 'historical', 'context', 'say', 'associate', 'architecture', 'firm', 'city', 'package', 'problem', 'congested', 'street', 'inefficient', 'distribution', 'resource', 'say', 'rather', '’', 'crisis', 'convenience', 'akin', 'suburban', 'shopping', 'center', 'begin', 'compete', 'city', 'downtown', 'live', 'city', 'main', 'pitch', 'ability', 'walk', 'door', 'get', 'carton', 'milk', 'go', 'bookstore', 'go', 'movie', 'say', 'convenience', 'culture', 'threaten', 'director', 'analytic', 'transportation', 'agree', 'lot', 'people', 'come', 'bring', 'mentality', 'say', '’', 'problem', 'dream', 'largely', 'base', 'suburbia', 'city', 'space', 'house', 'lawn', 'car', 'stuff', 'public', 'transportation', 'bike', 'lane', 'sidewalk', 'park', 'apartment', 'building', 'make', 'share', 'city', 'planner', 'apartment', 'building', 'manager', 'cope', 'rise', 'ecommerce', 'holguínveras', 'pore', 'datum', 'year', 'help', 'ask', 'purchase', 'make', 'percent', 'truly', 'urgent', 'freelance', 'multimedia', 'journalist', 'base', 'story', 'update', 'reflect', 'affiliation']"
Digital repression across borders is on the rise,https://www.technologyreview.com/2022/07/08/1055582/digital-repression-across-borders-is-on-the-rise/,2022-07-08,"<p>Activists who flee repression increasingly face zero-click software hacks and other digital threats</p>
","Khatab Alrawhani, a Yemen-born journalist and activist, thought he could escape the persecution that journalists were experiencing in the Middle East when he left the region. But it followed him. While studying in Washington, DC, in 2015, he published posts denouncing the Houthi coup, in which an armed faction overthrew the Yemeni government. His father was briefly arrested. Soon after, his brother was as well.  When Alrawhani settled in Toronto, though, his online life took an unexpected turn. He started to get WhatsApp messages from women he’d never met, urging him to click a link they shared. The messages didn’t seem like ordinary phishing attempts. They were personalized: they included details about his background, making comments about specific articles he had written or referencing where he used to live in Yemen.  Then pro-Houthi hackers hijacked the Facebook page for his news network, which covers human rights abuses in Yemen, and used it to post positive messages in Arabic about the coup. “What was terrible is how our readers thought these messages were coming from us,” he says. Ultimately, his team had to delete the page entirely and launch a new one. These kinds of online threats have changed how Alrawhani navigates the world and interacts with others. “I don’t write full sentences in my phone when I text friends or colleagues or family,” he says. Instead, he writes in code. “I assume my phone activity is always being monitored by the Houthi regime,” he says. Alrawhani is not alone. Around the world, activists have fled authoritarian states for their safety. But in their new homes, the intimidation continues, albeit in the digital realm. Those threats—generally referred to as digital transnational repression—include phishing attacks, zero-click spyware hacks, social media page takedowns, SIM card hacks, and fake invitations to conferences. Physical threats against activists tend to make the headlines. Earlier this year, for example, five Chinese nationals were arrested for plotting attacks on dissidents living in New York City. But digital harassment, which can be conducted with the click of a mouse button, frequently occurs behind the scenes. And it seems to be on the rise. The London-based research agency Forensic Architecture has counted 326 incidents of digital transnational repression between 2019 and 2021, up from 105 incidents between 2017 and 2019.  One reason these online attacks are growing more frequent is that they can be much less expensive than physical attacks, says Isabel Linzer, a research analyst at the human rights organization Freedom House, which published a report in June on repression tactics used against dissidents who have moved from their home country to the US. “These [digital] attacks happen far more frequently than some people think,” Linzer says, and they “have serious consequences for people going out to live their daily lives and to engage in their work or activism.” The full range of digital transnational repression is difficult to track, as many incidents aren’t reported. But some institutions are working to show how much harm they can do—and how hollow the response from governments and law enforcement can be.  A report this year by the Citizen Lab, a research group at the University of Toronto, includes the findings from interviews with more than a dozen activists who fled their country of origin to live in Canada.  “Digital targeting has a serious impact on the well-being of victims, undermines their ability to engage in transnational advocacy work, violates fundamental rights such as the right to privacy, freedom of expression, and peaceful assembly, and increases the dangers faced by their family members and friends who remain within the country of origin,” the report concluded. The countries the Citizen Lab identified as some of the more common perpetrators of digital transnational repression include Yemen as well as Afghanistan, China, Iran, Rwanda, and Syria. Zero-click software hacks, which allow an attacker to break into a phone or computer even if its user doesn’t open a malicious link or attachment, are especially concerning, says Noura Al-Jizawi, a research officer at the Citizen Lab and coauthor of the report. That’s because “they can evade digital hygiene practices,” she says.  In 2021, hackers used such code to infiltrate and install spyware on the cell phone of Saudi women’s rights activist Loujain al-Hathloul, who was then living in British Columbia. In that case, the perpetrators mistakenly left an image file on her phone that allowed researchers to pin down the source of the code. The digital blueprint led to NSO Group, an Israeli technology firm that has made headlines for selling spyware to authoritarian nation-states. Some forms of digital repression are meant to embarrass and doxx. One unnamed interviewee in the Citizen Lab report, who moved from China to Canada, found out that fabricated nude photos of her were being circulated among attendees of a conference she intended to visit. Her personal information was also posted in online ads soliciting sex services. Victims of this type of harassment experienced distress, anxiety, and fear for their family’s safety, the report notes. “There’s also a bit of a sense of resignation among those that continued activism, like a realization that this type of targeting would continue,” says coauthor Siena Anstis, senior legal advisor at the Citizen Lab. Many activists have become paranoid about the messages they receive. Kaveh Shahrooz, an Iraqi lawyer living in Canada who lobbies on behalf of dissidents, gives each email special scrutiny. Shahrooz says he once received a message from a supposed organizer of a human rights conference in Germany inviting him to speak and asking him to fill in personal information via a provided link. He researched more about the conference and found out he wasn’t invited, professional-sounding though the personalized email had been.  “That is one end of the spectrum,” Shahrooz says, “where you might get fooled into clicking a link. But then the other end is getting threatening messages about my activist work—things like ‘We know what you’re doing and we’ll deal with you later.’” There is little legal recourse. Several victims of spyware attacks in the UK have brought (or are bringing) civil claims against state operators and NSO Group, Anstis says. She adds that such cases can expected to be challenged, because they  generally focus on claims against companies outside the purview of the host country.  In the US, there is growing momentum behind calls to ban the software and tools exploited by authoritarian regimes. In 2021, the US Department of Commerce placed several surveillance companies on its Entity List, which restricts trade and business that runs contrary to the national security or foreign policy interests of the United States. New additions included NSO Group and Candiru, an Israeli-based spyware firm that develops surveillance and cyber-espionage technology for governmental clients. That won’t keep activists from being persecuted, however. Ten years ago, Eliana, a pseudonym for a Canadian-Syrian who asked to remain anonymous, began sharing the stories of the Assad regime’s victims by pitching news stories about them to local media, both in print and online. She also dedicated time to lobbying the Canadian government about resettling the many Syrian refugees who arrived in the country in 2016.  She says she regularly received messages from Google warning her that someone was attempting to access her Gmail account. She suspected the Syrian regime—she couldn’t think of who else it might be. Her biggest concern was the safety of the Syrian activists she was communicating with. “I knew that if such information fell into the hands of the dictatorship, it might lead to very catastrophic repercussions, including enforced abduction, torture, and assassination,” she says. Today, Eliana says she isn’t as extroverted as she used to be. “I used to be extremely open in interacting with people,” she says. “But I’ve realized that I need to be extra cautious, since I can’t predict who or where the hurt would come from.” David Silverberg is a writer and editor based in Toronto. ","Khatab Alrawhani, a Yemen-born journalist and activist, thought he could escape the persecution that journalists were experiencing in the Middle East when he left the region. But it followed him. While studying in Washington, DC, in 2015, he published posts denouncing the Houthi coup, in which an armed faction overthrew the Yemeni government. His father was briefly arrested. Soon after, his brother was as well. When Alrawhani settled in Toronto, though, his online life took an unexpected turn. He started to get WhatsApp messages from women he’d never met, urging him to click a link they shared. The messages didn’t seem like ordinary phishing attempts. They were personalized: they included details about his background, making comments about specific articles he had written or referencing where he used to live in Yemen. Then pro-Houthi hackers hijacked the Facebook page for his news network, which covers human rights abuses in Yemen, and used it to post positive messages in Arabic about the coup. “What was terrible is how our readers thought these messages were coming from us,” he says. Ultimately, his team had to delete the page entirely and launch a new one. These kinds of online threats have changed how Alrawhani navigates the world and interacts with others. “I don’t write full sentences in my phone when I text friends or colleagues or family,” he says. Instead, he writes in code. “I assume my phone activity is always being monitored by the Houthi regime,” he says. Alrawhani is not alone. Around the world, activists have fled authoritarian states for their safety. But in their new homes, the intimidation continues, albeit in the digital realm. Those threats—generally referred to as digital transnational repression—include phishing attacks, zero-click spyware hacks, social media page takedowns, SIM card hacks, and fake invitations to conferences. Physical threats against activists tend to make the headlines. Earlier this year, for example, five Chinese nationals were arrested for plotting attacks on dissidents living in New York City. But digital harassment, which can be conducted with the click of a mouse button, frequently occurs behind the scenes. And it seems to be on the rise. The London-based research agency Forensic Architecture has counted 326 incidents of digital transnational repression between 2019 and 2021, up from 105 incidents between 2017 and 2019. One reason these online attacks are growing more frequent is that they can be much less expensive than physical attacks, says Isabel Linzer, a research analyst at the human rights organization Freedom House, which published a report in June on repression tactics used against dissidents who have moved from their home country to the US. “These [digital] attacks happen far more frequently than some people think,” Linzer says, and they “have serious consequences for people going out to live their daily lives and to engage in their work or activism.” The full range of digital transnational repression is difficult to track, as many incidents aren’t reported. But some institutions are working to show how much harm they can do—and how hollow the response from governments and law enforcement can be. A report this year by the Citizen Lab, a research group at the University of Toronto, includes the findings from interviews with more than a dozen activists who fled their country of origin to live in Canada. “Digital targeting has a serious impact on the well-being of victims, undermines their ability to engage in transnational advocacy work, violates fundamental rights such as the right to privacy, freedom of expression, and peaceful assembly, and increases the dangers faced by their family members and friends who remain within the country of origin,” the report concluded. The countries the Citizen Lab identified as some of the more common perpetrators of digital transnational repression include Yemen as well as Afghanistan, China, Iran, Rwanda, and Syria. Zero-click software hacks, which allow an attacker to break into a phone or computer even if its user doesn’t open a malicious link or attachment, are especially concerning, says Noura Al-Jizawi, a research officer at the Citizen Lab and coauthor of the report. That’s because “they can evade digital hygiene practices,” she says. In 2021, hackers used such code to infiltrate and install spyware on the cell phone of Saudi women’s rights activist Loujain al-Hathloul, who was then living in British Columbia. In that case, the perpetrators mistakenly left an image file on her phone that allowed researchers to pin down the source of the code. The digital blueprint led to NSO Group, an Israeli technology firm that has made headlines for selling spyware to authoritarian nation-states. Some forms of digital repression are meant to embarrass and doxx. One unnamed interviewee in the Citizen Lab report, who moved from China to Canada, found out that fabricated nude photos of her were being circulated among attendees of a conference she intended to visit. Her personal information was also posted in online ads soliciting sex services. Victims of this type of harassment experienced distress, anxiety, and fear for their family’s safety, the report notes. “There’s also a bit of a sense of resignation among those that continued activism, like a realization that this type of targeting would continue,” says coauthor Siena Anstis, senior legal advisor at the Citizen Lab. Many activists have become paranoid about the messages they receive. Kaveh Shahrooz, an Iraqi lawyer living in Canada who lobbies on behalf of dissidents, gives each email special scrutiny. Shahrooz says he once received a message from a supposed organizer of a human rights conference in Germany inviting him to speak and asking him to fill in personal information via a provided link. He researched more about the conference and found out he wasn’t invited, professional-sounding though the personalized email had been. “That is one end of the spectrum,” Shahrooz says, “where you might get fooled into clicking a link. But then the other end is getting threatening messages about my activist work—things like ‘We know what you’re doing and we’ll deal with you later.’” There is little legal recourse. Several victims of spyware attacks in the UK have brought (or are bringing) civil claims against state operators and NSO Group, Anstis says. She adds that such cases can expected to be challenged, because they generally focus on claims against companies outside the purview of the host country. In the US, there is growing momentum behind calls to ban the software and tools exploited by authoritarian regimes. In 2021, the US Department of Commerce placed several surveillance companies on its Entity List, which restricts trade and business that runs contrary to the national security or foreign policy interests of the United States. New additions included NSO Group and Candiru, an Israeli-based spyware firm that develops surveillance and cyber-espionage technology for governmental clients. That won’t keep activists from being persecuted, however. Ten years ago, Eliana, a pseudonym for a Canadian-Syrian who asked to remain anonymous, began sharing the stories of the Assad regime’s victims by pitching news stories about them to local media, both in print and online. She also dedicated time to lobbying the Canadian government about resettling the many Syrian refugees who arrived in the country in 2016. She says she regularly received messages from Google warning her that someone was attempting to access her Gmail account. She suspected the Syrian regime—she couldn’t think of who else it might be. Her biggest concern was the safety of the Syrian activists she was communicating with. “I knew that if such information fell into the hands of the dictatorship, it might lead to very catastrophic repercussions, including enforced abduction, torture, and assassination,” she says. Today, Eliana says she isn’t as extroverted as she used to be. “I used to be extremely open in interacting with people,” she says. “But I’ve realized that I need to be extra cautious, since I can’t predict who or where the hurt would come from.” David Silverberg is a writer and editor based in Toronto.","['yemenborn', 'journalist', 'activist', 'think', 'escape', 'persecution', 'journalist', 'experience', 'leave', 'region', 'follow', 'study', 'publish', 'post', 'denounce', 'houthi', 'coup', 'armed', 'faction', 'overthrow', 'yemeni', 'government', 'father', 'briefly', 'arrest', 'soon', 'brother', 'well', 'settle', 'online', 'life', 'take', 'unexpected', 'turn', 'start', 'get', 'whatsapp', 'message', 'woman', '’d', 'never', 'meet', 'urge', 'click', 'link', 'share', 'message', 'seem', 'ordinary', 'phishing', 'attempt', 'personalize', 'include', 'detail', 'background', 'make', 'comment', 'specific', 'article', 'write', 'reference', 'use', 'live', 'hacker', 'hijack', 'facebook', 'page', 'news', 'network', 'cover', 'human', 'right', 'abuse', 'use', 'post', 'positive', 'message', 'arabic', 'coup', 'terrible', 'reader', 'think', 'message', 'come', 'say', 'ultimately', 'team', 'delete', 'page', 'entirely', 'launch', 'new', 'one', 'kind', 'online', 'threat', 'change', 'navigate', 'world', 'interact', 'write', 'full', 'sentence', 'phone', 'text', 'friend', 'colleague', 'family', 'say', 'instead', 'write', 'code', 'assume', 'phone', 'activity', 'always', 'monitor', 'regime', 'say', 'alone', 'world', 'activist', 'flee', 'authoritarian', 'state', 'safety', 'new', 'home', 'intimidation', 'continue', 'digital', 'realm', 'threat', 'generally', 'refer', 'digital', 'transnational', 'repression', 'include', 'phishe', 'attack', 'zeroclick', 'spyware', 'hack', 'social', 'medium', 'page', 'takedown', 'sim', 'card', 'hack', 'fake', 'invitation', 'conference', 'physical', 'threat', 'activist', 'tend', 'make', 'headline', 'early', 'year', 'example', 'chinese', 'national', 'arrest', 'plot', 'attack', 'dissident', 'live', 'digital', 'harassment', 'conduct', 'click', 'mouse', 'button', 'frequently', 'occur', 'scene', 'seem', 'rise', 'londonbased', 'research', 'agency', 'forensic', 'architecture', 'count', 'incident', 'digital', 'transnational', 'repression', 'incident', 'reason', 'online', 'attack', 'grow', 'frequent', 'much', 'less', 'expensive', 'physical', 'attack', 'say', 'research', 'analyst', 'freedom', 'house', 'publish', 'report', 'repression', 'tactic', 'use', 'dissident', 'move', 'home', 'country', 'digital', 'attack', 'happen', 'far', 'frequently', 'people', 'think', 'linzer', 'say', 'serious', 'consequence', 'people', 'go', 'live', 'daily', 'life', 'engage', 'work', 'activism', 'full', 'range', 'digital', 'transnational', 'repression', 'difficult', 'track', 'many', 'incident', 'report', 'institution', 'work', 'show', 'much', 'harm', 'hollow', 'response', 'government', 'law', 'enforcement', 'report', 'year', 'citizen', 'lab', 'research', 'group', 'include', 'finding', 'interview', 'dozen', 'activist', 'flee', 'country', 'origin', 'live', 'targeting', 'serious', 'impact', 'wellbeing', 'victim', 'undermine', 'ability', 'engage', 'transnational', 'advocacy', 'work', 'violate', 'fundamental', 'right', 'right', 'privacy', 'freedom', 'expression', 'peaceful', 'assembly', 'increase', 'danger', 'face', 'family', 'member', 'friend', 'remain', 'country', 'origin', 'report', 'conclude', 'country', 'citizen', 'lab', 'identify', 'common', 'perpetrator', 'digital', 'transnational', 'repression', 'include', 'well', 'zeroclick', 'software', 'hack', 'allow', 'attacker', 'break', 'phone', 'computer', 'even', 'user', 'open', 'malicious', 'link', 'attachment', 'especially', 'concern', 'say', 'noura', 'aljizawi', 'research', 'officer', 'citizen', 'lab', 'coauthor', 'report', '’', 'evade', 'digital', 'hygiene', 'practice', 'say', 'hacker', 'use', 'code', 'infiltrate', 'install', 'spyware', 'cell', 'phone', 'saudi', 'woman', 'right', 'live', 'case', 'perpetrator', 'mistakenly', 'leave', 'image', 'file', 'phone', 'allow', 'researcher', 'pin', 'source', 'code', 'digital', 'blueprint', 'lead', 'israeli', 'technology', 'firm', 'make', 'headline', 'sell', 'spyware', 'authoritarian', 'nationstate', 'form', 'digital', 'repression', 'mean', 'embarrass', 'unnamed', 'interviewee', 'citizen', 'lab', 'report', 'move', 'find', 'fabricate', 'nude', 'photo', 'circulate', 'attendee', 'conference', 'intend', 'visit', 'personal', 'information', 'also', 'post', 'online', 'ad', 'solicit', 'sex', 'service', 'victim', 'type', 'harassment', 'experienced', 'distress', 'anxiety', 'fear', 'family', 'safety', 'report', 'note', '’', 'also', 'bit', 'sense', 'resignation', 'continue', 'activism', 'realization', 'type', 'target', 'continue', 'say', 'coauthor', 'siena', 'anstis', 'senior', 'legal', 'advisor', 'citizen', 'lab', 'many', 'activist', 'become', 'paranoid', 'message', 'receive', 'shahrooz', 'iraqi', 'lawyer', 'live', 'lobby', 'behalf', 'dissident', 'give', 'email', 'special', 'scrutiny', 'shahrooz', 'say', 'receive', 'message', 'suppose', 'organizer', 'human', 'right', 'conference', 'invite', 'speak', 'ask', 'fill', 'personal', 'information', 'provide', 'link', 'research', 'conference', 'find', 'invite', 'professionalsounding', 'personalized', 'email', 'end', 'spectrum', 'shahrooz', 'say', 'fool', 'click', 'link', 'end', 'get', 'threaten', 'message', 'activist', 'work', 'thing', 'know', 'deal', 'later', 'little', 'legal', 'recourse', 'several', 'victim', 'spyware', 'attack', 'bring', 'bring', 'civil', 'claim', 'state', 'operator', 'anstis', 'say', 'add', 'case', 'expect', 'challenge', 'generally', 'focus', 'claim', 'company', 'purview', 'host', 'country', 'grow', 'momentum', 'call', 'ban', 'software', 'tool', 'exploit', 'authoritarian', 'regime', 'place', 'several', 'surveillance', 'company', 'entity', 'list', 'restrict', 'trade', 'business', 'run', 'contrary', 'national', 'security', 'foreign', 'policy', 'interest', 'new', 'addition', 'include', 'group', 'candiru', 'israelibased', 'spyware', 'firm', 'develop', 'surveillance', 'cyberespionage', 'technology', 'governmental', 'client', 'keep', 'activist', 'persecute', 'however', 'year', 'ago', 'pseudonym', 'canadiansyrian', 'ask', 'remain', 'anonymous', 'begin', 'share', 'story', 'victim', 'pitch', 'news', 'story', 'local', 'medium', 'print', 'online', 'also', 'dedicate', 'time', 'lobby', 'canadian', 'government', 'resettle', 'many', 'syrian', 'refugee', 'arrive', 'country', 'say', 'regularly', 'receive', 'message', 'warn', 'attempt', 'access', 'gmail', 'account', 'suspect', 'syrian', 'regime', 'think', 'else', 'big', 'concern', 'safety', 'syrian', 'activist', 'communicate', 'know', 'information', 'fall', 'hand', 'dictatorship', 'lead', 'catastrophic', 'repercussion', 'include', 'enforce', 'abduction', 'torture', 'assassination', 'say', 'today', 'say', 'extroverte', 'use', 'extremely', 'open', 'interact', 'people', 'say', 'realize', 'need', 'extra', 'cautious', 'predict', 'hurt', 'come', 'writer', 'editor', 'base']"
The US military wants to understand the most important software on Earth,https://www.technologyreview.com/2022/07/14/1055894/us-military-sofware-linux-kernel-open-source/,2022-07-14,"<p>Open-source code runs on every computer on the planet—and keeps America’s critical infrastructure going. DARPA is worried about how well it can be trusted</p>
","It’s not much of an exaggeration to say that the whole world is built on top of the Linux kernel—although most people have never heard of it. It is one of the very first programs that load when most computers power up. It enables the hardware running the machine to interact with the software, governs its use of resources, and acts as the foundation of the operating system.  It is the core building block of nearly all cloud computing, virtually every supercomputer, the entire internet of things, billions of smartphones, and more. But the kernel is also open source, meaning anyone can write, read, and use its code. And that’s got cybersecurity experts inside the US military seriously worried. Its open-source nature means the Linux kernel—along with a host of other pieces of critical open-source software—is exposed to hostile manipulation in ways that we still barely understand. “People are realizing now: wait a minute, literally everything we do is underpinned by Linux,”  says Dave Aitel, a cybersecurity researcher and former NSA computer security scientist. “This is a core technology to our society. Not understanding kernel security means we can’t secure critical infrastructure.” Volunteer-run projects like Log4J keep the internet running. The result is unsustainable burnout, and a national security risk when they go wrong. Now DARPA, the US military’s research arm, wants to understand the collision of code and community that makes these open-source projects work, in order to better understand the risks they face. The goal is to be able to effectively recognize malicious actors and prevent them from disrupting or corrupting crucially important open-source code before it’s too late. DARPA’s  “SocialCyber” program is an 18-month-long, multimillion-dollar project that will combine sociology with recent technological advances in artificial intelligence to map, understand, and protect these massive open-source communities and the code they create. It’s different from most previous research because it combines automated analysis of both the code and the social dimensions of open-source software. “The open-source ecosystem is one of the grandest enterprises in human history,” says Sergey Bratus, the DARPA program manager behind the project. “It’s now grown from enthusiasts to a global endeavor forming the basis of global infrastructure, of the internet itself, of critical industries and mission-critical systems pretty much everywhere,” he says. “The systems that run our industry, power grids, shipping, transportation.” Much of modern civilization now depends on an ever-expanding corpus of open-source code because it saves money, attracts talent, and makes a lot of work easier. But while the open-source movement has spawned a colossal ecosystem that we all depend on, we do not fully understand it, experts like Aitel argue. There are countless software projects, millions of lines of code,  numerous mailing lists and forums, and an ocean of contributors whose identities and motivation are often obscure, making it hard to hold them accountable.  That can be dangerous. For example, hackers have quietly inserted malicious code into open-source projects numerous times in recent years. Back doors can long escape detection, and, in the worst case, entire projects have been handed over to bad actors who take advantage of the trust people place in open-source communities and code. Sometimes there are disruptions or even takeovers of the very social networks that these projects depend on. Tracking it all has been mostly—though not entirely—a manual effort, which means it does not match the astronomical size of the problem. Bratus argues that we need machine learning to digest and comprehend the expanding universe of code—meaning useful tricks like automated vulnerability discovery—as well as tools to understand the community of people who write, fix, implement, and influence that code.  The ultimate goal is to detect and counteract any malicious campaigns to submit flawed code, launch influence operations, sabotage development, or even take control of open-source projects.  To do this, the researchers will use tools such as sentiment analysis to analyze the social interactions within open-source communities such as the Linux kernel mailing list, which should help identify who is being positive or constructive and who is being negative and destructive.  The researchers want insight into what kinds of events and behavior can disrupt or hurt open-source communities, which members are trustworthy, and whether there are particular groups that justify extra vigilance. These answers are necessarily subjective. But right now there are few ways to find them at all. Experts are worried that blind spots about the people who run open-source software make the whole edifice ripe for potential manipulation and attacks. For Bratus, the primary threat is the prospect of “untrustworthy code” running America’s critical infrastructure—a situation that could invite unwelcome surprises.  Here’s how the SocialCyber program works. DARPA has contracted with multiple teams of what it calls “performers,” including small, boutique cybersecurity research shops with deep technical chops. One such performer is New York–based Margin Research, which has put together a team of well-respected researchers for the task. “There is a desperate need to treat open-source communities and projects with a higher level of care and respect,” said Sophia d’Antoine, the firm’s  founder. “A lot of existing infrastructure is very fragile because it depends on open source, which we assume will always be there because it’s always been there. This is walking back from the implicit trust we have in open-source code bases and software.” Margin Research is focused on the Linux kernel in part because it’s so big and critical that succeeding here, at this scale, means you can make it anywhere else. The plan is to analyze both the code and the community in order to visualize and finally understand the whole ecosystem. Margin’s work maps out who is working on what specific parts of open-source projects. For example, Huawei is currently the biggest contributor to the Linux kernel.  Another contributor works for Positive Technologies, a Russian cybersecurity firm that—like Huawei—has been sanctioned by the US government, says Aitel. Margin has also mapped code written by NSA employees, many of whom participate in different open-source projects. “This subject kills me,” says d’Antoine of the quest to better understand the open-source movement, “because, honestly, even the most simple things seem so novel to so many important people. The government is only just realizing that our critical infrastructure is running code that could be literally being written by sanctioned entities. Right now.” This kind of research also aims to find underinvestment—that is critical software run entirely by one or two volunteers. It’s more common than you might think—so common that one common way software projects currently measure risk is the “bus factor”: Does this whole project fall apart if just one person gets hit by a bus?  While the Linux kernel’s importance to the world’s computer systems may be the most pressing issue for SocialCyber, it will tackle other open-source projects too. Certain performers will focus on projects like Python, an open-source programming language used in a huge number of artificial-intelligence and machine-learning projects.  The hope is that greater understanding will make it easier to prevent a future disaster, whether it’s caused by malicious activity or not.  “Pretty much everywhere you look, you find open-source software,” says Bratus.“Even when you look at proprietary software, a recent study showed it’s actually 70% or more open source.” “This is a critical infrastructure problem,” Aitel says. “We don’t have a grip on it. We need to get a grip on it. The potential impact is that malicious hackers will always have access to Linux machines. That includes your phone. It’s that simple.” ","It’s not much of an exaggeration to say that the whole world is built on top of the Linux kernel—although most people have never heard of it. It is one of the very first programs that load when most computers power up. It enables the hardware running the machine to interact with the software, governs its use of resources, and acts as the foundation of the operating system. It is the core building block of nearly all cloud computing, virtually every supercomputer, the entire internet of things, billions of smartphones, and more. But the kernel is also open source, meaning anyone can write, read, and use its code. And that’s got cybersecurity experts inside the US military seriously worried. Its open-source nature means the Linux kernel—along with a host of other pieces of critical open-source software—is exposed to hostile manipulation in ways that we still barely understand. “People are realizing now: wait a minute, literally everything we do is underpinned by Linux,” says Dave Aitel, a cybersecurity researcher and former NSA computer security scientist. “This is a core technology to our society. Not understanding kernel security means we can’t secure critical infrastructure.” Volunteer-run projects like Log4J keep the internet running. The result is unsustainable burnout, and a national security risk when they go wrong. Now DARPA, the US military’s research arm, wants to understand the collision of code and community that makes these open-source projects work, in order to better understand the risks they face. The goal is to be able to effectively recognize malicious actors and prevent them from disrupting or corrupting crucially important open-source code before it’s too late. DARPA’s “SocialCyber” program is an 18-month-long, multimillion-dollar project that will combine sociology with recent technological advances in artificial intelligence to map, understand, and protect these massive open-source communities and the code they create. It’s different from most previous research because it combines automated analysis of both the code and the social dimensions of open-source software. “The open-source ecosystem is one of the grandest enterprises in human history,” says Sergey Bratus, the DARPA program manager behind the project. “It’s now grown from enthusiasts to a global endeavor forming the basis of global infrastructure, of the internet itself, of critical industries and mission-critical systems pretty much everywhere,” he says. “The systems that run our industry, power grids, shipping, transportation.” Much of modern civilization now depends on an ever-expanding corpus of open-source code because it saves money, attracts talent, and makes a lot of work easier. But while the open-source movement has spawned a colossal ecosystem that we all depend on, we do not fully understand it, experts like Aitel argue. There are countless software projects, millions of lines of code, numerous mailing lists and forums, and an ocean of contributors whose identities and motivation are often obscure, making it hard to hold them accountable. That can be dangerous. For example, hackers have quietly inserted malicious code into open-source projects numerous times in recent years. Back doors can long escape detection, and, in the worst case, entire projects have been handed over to bad actors who take advantage of the trust people place in open-source communities and code. Sometimes there are disruptions or even takeovers of the very social networks that these projects depend on. Tracking it all has been mostly—though not entirely—a manual effort, which means it does not match the astronomical size of the problem. Bratus argues that we need machine learning to digest and comprehend the expanding universe of code—meaning useful tricks like automated vulnerability discovery—as well as tools to understand the community of people who write, fix, implement, and influence that code. The ultimate goal is to detect and counteract any malicious campaigns to submit flawed code, launch influence operations, sabotage development, or even take control of open-source projects. To do this, the researchers will use tools such as sentiment analysis to analyze the social interactions within open-source communities such as the Linux kernel mailing list, which should help identify who is being positive or constructive and who is being negative and destructive. The researchers want insight into what kinds of events and behavior can disrupt or hurt open-source communities, which members are trustworthy, and whether there are particular groups that justify extra vigilance. These answers are necessarily subjective. But right now there are few ways to find them at all. Experts are worried that blind spots about the people who run open-source software make the whole edifice ripe for potential manipulation and attacks. For Bratus, the primary threat is the prospect of “untrustworthy code” running America’s critical infrastructure—a situation that could invite unwelcome surprises. Here’s how the SocialCyber program works. DARPA has contracted with multiple teams of what it calls “performers,” including small, boutique cybersecurity research shops with deep technical chops. One such performer is New York–based Margin Research, which has put together a team of well-respected researchers for the task. “There is a desperate need to treat open-source communities and projects with a higher level of care and respect,” said Sophia d’Antoine, the firm’s founder. “A lot of existing infrastructure is very fragile because it depends on open source, which we assume will always be there because it’s always been there. This is walking back from the implicit trust we have in open-source code bases and software.” Margin Research is focused on the Linux kernel in part because it’s so big and critical that succeeding here, at this scale, means you can make it anywhere else. The plan is to analyze both the code and the community in order to visualize and finally understand the whole ecosystem. Margin’s work maps out who is working on what specific parts of open-source projects. For example, Huawei is currently the biggest contributor to the Linux kernel. Another contributor works for Positive Technologies, a Russian cybersecurity firm that—like Huawei—has been sanctioned by the US government, says Aitel. Margin has also mapped code written by NSA employees, many of whom participate in different open-source projects. “This subject kills me,” says d’Antoine of the quest to better understand the open-source movement, “because, honestly, even the most simple things seem so novel to so many important people. The government is only just realizing that our critical infrastructure is running code that could be literally being written by sanctioned entities. Right now.” This kind of research also aims to find underinvestment—that is critical software run entirely by one or two volunteers. It’s more common than you might think—so common that one common way software projects currently measure risk is the “bus factor”: Does this whole project fall apart if just one person gets hit by a bus? While the Linux kernel’s importance to the world’s computer systems may be the most pressing issue for SocialCyber, it will tackle other open-source projects too. Certain performers will focus on projects like Python, an open-source programming language used in a huge number of artificial-intelligence and machine-learning projects. The hope is that greater understanding will make it easier to prevent a future disaster, whether it’s caused by malicious activity or not. “Pretty much everywhere you look, you find open-source software,” says Bratus.“Even when you look at proprietary software, a recent study showed it’s actually 70% or more open source.” “This is a critical infrastructure problem,” Aitel says. “We don’t have a grip on it. We need to get a grip on it. The potential impact is that malicious hackers will always have access to Linux machines. That includes your phone. It’s that simple.”","['’', 'much', 'exaggeration', 'say', 'whole', 'world', 'build', 'top', 'linux', 'kernel', 'people', 'never', 'hear', 'first', 'program', 'load', 'computer', 'power', 'enable', 'hardware', 'run', 'machine', 'interact', 'software', 'govern', 'use', 'resource', 'act', 'foundation', 'operating', 'system', 'core', 'building', 'block', 'cloud', 'compute', 'virtually', 'supercomputer', 'entire', 'internet', 'thing', 'billion', 'smartphone', 'kernel', 'also', 'open', 'source', 'mean', 'write', 'read', 'use', 'code', '’s', 'get', 'cybersecurity', 'expert', 'military', 'seriously', 'worry', 'opensource', 'nature', 'mean', 'linux', 'kernel', 'host', 'piece', 'critical', 'opensource', 'software', 'expose', 'hostile', 'manipulation', 'way', 'still', 'barely', 'understand', 'people', 'realize', 'wait', 'minute', 'literally', 'underpin', 'linux', 'say', 'cybersecurity', 'researcher', 'former', 'security', 'scientist', 'core', 'technology', 'society', 'understand', 'kernel', 'security', 'mean', 'secure', 'critical', 'infrastructure', 'volunteerrun', 'project', 'log4j', 'keep', 'internet', 'run', 'result', 'unsustainable', 'burnout', 'national', 'security', 'risk', 'go', 'wrong', 'darpa', 'research', 'arm', 'want', 'understand', 'collision', 'code', 'community', 'make', 'opensource', 'project', 'work', 'order', 'well', 'understand', 'risk', 'face', 'goal', 'able', 'effectively', 'recognize', 'malicious', 'actor', 'prevent', 'disrupt', 'corrupt', 'crucially', 'important', 'opensource', 'code', '’', 'multimilliondollar', 'project', 'combine', 'sociology', 'recent', 'technological', 'advance', 'artificial', 'intelligence', 'map', 'understand', 'protect', 'massive', 'opensource', 'community', 'code', 'create', '’', 'different', 'previous', 'research', 'combine', 'automated', 'analysis', 'code', 'social', 'dimension', 'opensource', 'software', 'opensource', 'ecosystem', 'grand', 'enterprise', 'human', 'history', 'say', 'darpa', 'program', 'manager', 'project', 'grow', 'enthusiast', 'global', 'endeavor', 'form', 'basis', 'global', 'infrastructure', 'internet', 'critical', 'industry', 'missioncritical', 'system', 'pretty', 'much', 'everywhere', 'say', 'system', 'run', 'industry', 'power', 'grid', 'ship', 'transportation', 'much', 'modern', 'civilization', 'depend', 'everexpande', 'corpus', 'opensource', 'code', 'save', 'money', 'attract', 'talent', 'make', 'lot', 'work', 'easier', 'opensource', 'movement', 'spawn', 'colossal', 'ecosystem', 'depend', 'fully', 'understand', 'expert', 'aitel', 'argue', 'countless', 'software', 'project', 'million', 'line', 'code', 'numerous', 'mailing', 'list', 'forum', 'ocean', 'contributor', 'identity', 'motivation', 'often', 'obscure', 'make', 'hard', 'hold', 'accountable', 'dangerous', 'example', 'hacker', 'quietly', 'insert', 'malicious', 'code', 'opensource', 'project', 'numerous', 'time', 'recent', 'year', 'door', 'long', 'escape', 'detection', 'bad', 'case', 'entire', 'project', 'hand', 'bad', 'actor', 'take', 'advantage', 'trust', 'people', 'place', 'opensource', 'community', 'code', 'sometimes', 'disruption', 'even', 'takeover', 'social', 'network', 'project', 'depend', 'track', 'mostly', 'entirely', 'manual', 'effort', 'mean', 'match', 'astronomical', 'size', 'problem', 'bratus', 'argue', 'need', 'machine', 'learning', 'digest', 'comprehend', 'expand', 'universe', 'code', 'mean', 'useful', 'trick', 'automate', 'vulnerability', 'discovery', 'well', 'tool', 'understand', 'community', 'people', 'write', 'fix', 'implement', 'influence', 'code', 'ultimate', 'goal', 'detect', 'counteract', 'malicious', 'campaign', 'submit', 'flawed', 'code', 'launch', 'influence', 'operation', 'sabotage', 'development', 'even', 'take', 'control', 'opensource', 'project', 'researcher', 'use', 'tool', 'sentiment', 'analysis', 'analyze', 'social', 'interaction', 'opensource', 'community', 'linux', 'kernel', 'mailing', 'list', 'help', 'identify', 'positive', 'constructive', 'negative', 'destructive', 'researcher', 'want', 'insight', 'kind', 'event', 'behavior', 'disrupt', 'hurt', 'opensource', 'community', 'member', 'trustworthy', 'particular', 'group', 'justify', 'extra', 'vigilance', 'answer', 'necessarily', 'subjective', 'right', 'way', 'find', 'expert', 'worried', 'blind', 'spot', 'people', 'run', 'opensource', 'software', 'make', 'whole', 'edifice', 'ripe', 'potential', 'manipulation', 'attack', 'bratus', 'primary', 'threat', 'prospect', 'untrustworthy', 'code', 'run', 'critical', 'infrastructure', 'situation', 'invite', 'unwelcome', 'surprise', '’', 'program', 'work', 'darpa', 'contract', 'multiple', 'team', 'call', 'performer', 'include', 'small', 'boutique', 'cybersecurity', 'research', 'shop', 'deep', 'technical', 'chop', 'performer', 'base', 'margin', 'research', 'put', 'together', 'team', 'wellrespecte', 'researcher', 'task', 'desperate', 'need', 'treat', 'opensource', 'community', 'project', 'high', 'level', 'care', 'respect', 'say', 'firm', 'founder', 'lot', 'exist', 'infrastructure', 'fragile', 'depend', 'open', 'source', 'assume', 'always', 'always', 'walk', 'back', 'implicit', 'trust', 'opensource', 'code', 'basis', 'software', 'margin', 'research', 'focus', 'linux', 'kernel', 'part', '’', 'big', 'critical', 'succeed', 'scale', 'mean', 'make', 'anywhere', 'else', 'plan', 'analyze', 'code', 'community', 'order', 'visualize', 'finally', 'understand', 'whole', 'ecosystem', 'margin', 'work', 'map', 'work', 'specific', 'part', 'opensource', 'project', 'example', 'huawei', 'currently', 'big', 'contributor', 'linux', 'kernel', 'contributor', 'work', 'positive', 'technology', 'russian', 'cybersecurity', 'firm', 'sanction', 'government', 'say', 'aitel', 'margin', 'also', 'map', 'code', 'write', 'employee', 'many', 'participate', 'different', 'opensource', 'project', 'subject', 'kill', 'say', 'd’antoine', 'quest', 'well', 'understand', 'opensource', 'movement', 'honestly', 'even', 'simple', 'thing', 'seem', 'novel', 'many', 'important', 'people', 'government', 'realize', 'critical', 'infrastructure', 'run', 'code', 'literally', 'write', 'sanctioned', 'entity', 'right', 'kind', 'research', 'also', 'aim', 'find', 'underinvestment', 'critical', 'software', 'run', 'entirely', 'volunteer', '’', 'common', 'think', 'common', 'common', 'way', 'software', 'project', 'currently', 'measure', 'risk', 'bus', 'factor', 'whole', 'project', 'fall', 'apart', 'person', 'hit', 'bus', 'linux', 'importance', 'world', 'computer', 'system', 'pressing', 'issue', 'tackle', 'opensource', 'project', 'certain', 'performer', 'focus', 'project', 'opensource', 'programming', 'language', 'use', 'huge', 'number', 'artificialintelligence', 'machinelearning', 'project', 'hope', 'great', 'understanding', 'make', 'easy', 'prevent', 'future', 'disaster', 'cause', 'malicious', 'activity', 'pretty', 'much', 'everywhere', 'look', 'find', 'opensource', 'software', 'say', 'bratuseven', 'look', 'proprietary', 'software', 'recent', 'study', 'show', '’', 'actually', 'open', 'source', 'critical', 'infrastructure', 'problem', 'aitel', 'say', 'grip', 'need', 'get', 'grip', 'potential', 'impact', 'malicious', 'hacker', 'always', 'access', 'linux', 'machine', 'include', 'phone', '’', 'simple']"
Increasing amounts of data require holistic governance,https://www.technologyreview.com/2022/07/11/1055450/increasing-amounts-of-data-require-holistic-governance/,2022-07-11,"As companies struggle to process, store, and leverage ever-increasing amounts of structured and unstructured data, data governance is becoming a critical part of every company’s data management. Governance not only helps a company understand and use its data, but it ensures everyone has access to the data they need, when they need it. “Data doesn’t…","In association withCapital One As companies struggle to process, store, and leverage ever-increasing amounts of structured and unstructured data, data governance is becoming a critical part of every company’s data management. Governance not only helps a company understand and use its data, but it ensures everyone has access to the data they need, when they need it. “Data doesn't have much value if it lies dormant in your system, where no one can gain insight from it,” says Salim Syed, head of engineering for Capital One Slingshot. “A well-governed data platform brings data out of that darkness.” Effective governance also enables a company to implement and manage internal policies and standards related to the security and usage of data. This not only supports a company’s response to external compliance directives, but also standardizes the data for use across the company. Standardized data provides the “single source of truth” required for critical business decisions, as well as the data quality and trustworthiness teams need to do their jobs. On the surface, implementing data governance might seem obvious and straightforward, but the act of governing data across a company’s teams and products introduces levels of complexity that many companies either half-heartedly attempt to address or avoid altogether. Instilling the processes, policies, and protections of governance requires new mindsets around people, processes, and technology. “It's not the run-time activities that persuade someone not to do governance,” says Syed. “It's all the work that's needed to set up governance.” For many, the approach to data governance is to establish policies that are overseen by individual sectors of the business, which makes implementation all the more difficult. “Think about all the different teams that are doing that in a large organization,” explains Syed. “They all have to do that dependency check, and each team is also doing separate development work to meet those requirements, which is a lot of duplicated effort.” A siloed data governance initiative that requires each team to monitor its own data dependencies takes time and effort away from other work as well. “It becomes cumbersome to innovate because at every step of innovation, you have to check if there are dependencies on your governance policies,” says Syed. Siloed approaches also introduce the possibility of error and make it more difficult to ensure all governance policies are followed consistently, in all cases. These hurdles can result in a lack of buy-in from employees and stakeholders, deflating any realized data governance benefits. In many companies, data is viewed as an IT asset, and thus an IT responsibility. Although that might have been true in the past, the volume and speed of data today, and the innovative ways companies are using their data, means data is the responsibility—and the driving force—for all business units. To build an effective data governance program to serve every area of the business, it’s best to centralize the framework to reduce errors and to reduce duplicate efforts. “For federated teams to be successful in applying data management rules and governance, you can't just set a policy and let every team go build technology to enforce it,” says Syed. A centralized approach is less complicated to monitor, facilitates data consistency and accuracy, and is easier to make transparent, all of which helps with stakeholder buy-in. “If you have a centrally managed data platform, a centrally managed data ingestion pipeline, and a centrally managed data policy, then you only make changes to [the data] in one place,” Syed explains. This ensures data remains compliant, secure, and consistent wherever it is used. A best practice in establishing a centralized data governance initiative, Syed argues, is to build a central data catalog. All incoming data is ingested to a central location where it is first classified—meaning, data is identified and labeled with metadata, and restriction levels are determined. From there, access and permissions can be assigned, which facilitates sharing across the organization. “With a centralized catalog, wherever your data resides, it's the map,” explains Syed. “Once it's cataloged and classified, then you can share. You can basically break the silo.” A data catalog effectively creates a compliant, secure data marketplace that allows teams to access any data they have permissions to use. This is beneficial on several fronts. It assists in data discovery at scale, which in turn can decrease development time and spur innovation. Cataloged data also comes with context, making it easier and faster to understand and use when making business decisions. This level of data stability fosters data quality, data integrity, and data lineage—all of which instills trust in the data, an essential component of data value. If the data can’t be trusted, it can’t be used to inform business decisions. No data governance initiative will be successful without buy-in and adoption. To cultivate the culture shift needed to implement data governance, it’s essential that the tools and processes work in concert. “It’s so important that the data catalog is always in sync with the data platform,” says Syed. “A lot of companies don't pay attention to that problem. You forget to register something and then you have orphan data everywhere. If something is added or changed and it’s not reflected in your catalog, then it just completely loses its value.” If users are frustrated with a lack of functionality, adoption will be an uphill battle. It's also important that the tools and processes remain flexible. Tools need to accommodate the unique load patterns of each line of business, notes Syed. Otherwise, teams will find workarounds to achieve their goals (sometimes called shadow IT), which can jeopardize the integrity of a data governance framework. Perhaps most important for adoption, the tools and processes need to be clear-cut. “It has to be simple and very easy to use.” says Syed. “And you really have to empower the business to own the data and treat data as a product—with its own service-level agreements, with its own quality, and own resiliency. That is also a completely different mindset that must be changed.” This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In association withCapital One As companies struggle to process, store, and leverage ever-increasing amounts of structured and unstructured data, data governance is becoming a critical part of every company’s data management. Governance not only helps a company understand and use its data, but it ensures everyone has access to the data they need, when they need it. “Data doesn't have much value if it lies dormant in your system, where no one can gain insight from it,” says Salim Syed, head of engineering for Capital One Slingshot. “A well-governed data platform brings data out of that darkness.” Effective governance also enables a company to implement and manage internal policies and standards related to the security and usage of data. This not only supports a company’s response to external compliance directives, but also standardizes the data for use across the company. Standardized data provides the “single source of truth” required for critical business decisions, as well as the data quality and trustworthiness teams need to do their jobs. On the surface, implementing data governance might seem obvious and straightforward, but the act of governing data across a company’s teams and products introduces levels of complexity that many companies either half-heartedly attempt to address or avoid altogether. Instilling the processes, policies, and protections of governance requires new mindsets around people, processes, and technology. “It's not the run-time activities that persuade someone not to do governance,” says Syed. “It's all the work that's needed to set up governance.” For many, the approach to data governance is to establish policies that are overseen by individual sectors of the business, which makes implementation all the more difficult. “Think about all the different teams that are doing that in a large organization,” explains Syed. “They all have to do that dependency check, and each team is also doing separate development work to meet those requirements, which is a lot of duplicated effort.” A siloed data governance initiative that requires each team to monitor its own data dependencies takes time and effort away from other work as well. “It becomes cumbersome to innovate because at every step of innovation, you have to check if there are dependencies on your governance policies,” says Syed. Siloed approaches also introduce the possibility of error and make it more difficult to ensure all governance policies are followed consistently, in all cases. These hurdles can result in a lack of buy-in from employees and stakeholders, deflating any realized data governance benefits. In many companies, data is viewed as an IT asset, and thus an IT responsibility. Although that might have been true in the past, the volume and speed of data today, and the innovative ways companies are using their data, means data is the responsibility—and the driving force—for all business units. To build an effective data governance program to serve every area of the business, it’s best to centralize the framework to reduce errors and to reduce duplicate efforts. “For federated teams to be successful in applying data management rules and governance, you can't just set a policy and let every team go build technology to enforce it,” says Syed. A centralized approach is less complicated to monitor, facilitates data consistency and accuracy, and is easier to make transparent, all of which helps with stakeholder buy-in. “If you have a centrally managed data platform, a centrally managed data ingestion pipeline, and a centrally managed data policy, then you only make changes to [the data] in one place,” Syed explains. This ensures data remains compliant, secure, and consistent wherever it is used. A best practice in establishing a centralized data governance initiative, Syed argues, is to build a central data catalog. All incoming data is ingested to a central location where it is first classified—meaning, data is identified and labeled with metadata, and restriction levels are determined. From there, access and permissions can be assigned, which facilitates sharing across the organization. “With a centralized catalog, wherever your data resides, it's the map,” explains Syed. “Once it's cataloged and classified, then you can share. You can basically break the silo.” A data catalog effectively creates a compliant, secure data marketplace that allows teams to access any data they have permissions to use. This is beneficial on several fronts. It assists in data discovery at scale, which in turn can decrease development time and spur innovation. Cataloged data also comes with context, making it easier and faster to understand and use when making business decisions. This level of data stability fosters data quality, data integrity, and data lineage—all of which instills trust in the data, an essential component of data value. If the data can’t be trusted, it can’t be used to inform business decisions. No data governance initiative will be successful without buy-in and adoption. To cultivate the culture shift needed to implement data governance, it’s essential that the tools and processes work in concert. “It’s so important that the data catalog is always in sync with the data platform,” says Syed. “A lot of companies don't pay attention to that problem. You forget to register something and then you have orphan data everywhere. If something is added or changed and it’s not reflected in your catalog, then it just completely loses its value.” If users are frustrated with a lack of functionality, adoption will be an uphill battle. It's also important that the tools and processes remain flexible. Tools need to accommodate the unique load patterns of each line of business, notes Syed. Otherwise, teams will find workarounds to achieve their goals (sometimes called shadow IT), which can jeopardize the integrity of a data governance framework. Perhaps most important for adoption, the tools and processes need to be clear-cut. “It has to be simple and very easy to use.” says Syed. “And you really have to empower the business to own the data and treat data as a product—with its own service-level agreements, with its own quality, and own resiliency. That is also a completely different mindset that must be changed.” This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.","['association', 'withcapital', 'company', 'struggle', 'process', 'store', 'leverage', 'everincreasing', 'amount', 'structured', 'unstructured', 'datum', 'datum', 'governance', 'become', 'critical', 'part', 'company', 'datum', 'management', 'governance', 'help', 'company', 'understand', 'use', 'datum', 'ensure', 'access', 'datum', 'need', 'need', 'datum', 'much', 'value', 'lie', 'dormant', 'system', 'one', 'gain', 'insight', 'say', 'head', 'engineering', 'capital', 'slingshot', 'wellgoverne', 'datum', 'platform', 'bring', 'datum', 'darkness', 'effective', 'governance', 'also', 'enable', 'company', 'implement', 'manage', 'internal', 'policy', 'standard', 'relate', 'security', 'usage', 'datum', 'support', 'company', 'response', 'external', 'compliance', 'directive', 'also', 'standardize', 'datum', 'use', 'company', 'standardized', 'datum', 'provide', 'single', 'source', 'truth', 'require', 'critical', 'business', 'decision', 'well', 'datum', 'quality', 'trustworthiness', 'team', 'need', 'job', 'surface', 'implement', 'data', 'governance', 'seem', 'obvious', 'straightforward', 'act', 'govern', 'datum', 'company', 'team', 'product', 'introduce', 'level', 'complexity', 'many', 'company', 'halfheartedly', 'attempt', 'address', 'avoid', 'altogether', 'instill', 'process', 'policy', 'protection', 'governance', 'require', 'new', 'mindset', 'people', 'process', 'technology', 'runtime', 'activity', 'persuade', 'governance', 'say', 'syed', 'work', 'need', 'set', 'governance', 'many', 'approach', 'datum', 'governance', 'establish', 'policy', 'oversee', 'individual', 'sector', 'business', 'make', 'implementation', 'difficult', 'think', 'different', 'team', 'large', 'organization', 'explain', 'syed', 'dependency', 'check', 'team', 'also', 'separate', 'development', 'work', 'meet', 'requirement', 'lot', 'duplicated', 'effort', 'siloe', 'datum', 'governance', 'initiative', 'require', 'team', 'monitor', 'datum', 'dependency', 'take', 'time', 'effort', 'away', 'work', 'well', 'become', 'cumbersome', 'innovate', 'step', 'innovation', 'check', 'dependency', 'governance', 'policy', 'say', 'syed', 'siloed', 'approach', 'also', 'introduce', 'possibility', 'error', 'make', 'difficult', 'ensure', 'governance', 'policy', 'follow', 'consistently', 'case', 'hurdle', 'result', 'lack', 'buyin', 'employee', 'stakeholder', 'deflate', 'realize', 'data', 'governance', 'benefit', 'many', 'company', 'datum', 'view', 'asset', 'thus', 'responsibility', 'true', 'past', 'volume', 'speed', 'datum', 'today', 'innovative', 'way', 'company', 'use', 'datum', 'mean', 'datum', 'responsibility', 'drive', 'force', 'business', 'unit', 'build', 'effective', 'data', 'governance', 'program', 'serve', 'area', 'business', '’', 'good', 'centralize', 'framework', 'reduce', 'error', 'reduce', 'duplicate', 'effort', 'federated', 'team', 'successful', 'apply', 'datum', 'management', 'rule', 'governance', 'set', 'policy', 'let', 'team', 'go', 'build', 'technology', 'enforce', 'say', 'syed', 'centralized', 'approach', 'less', 'complicated', 'monitor', 'facilitate', 'datum', 'consistency', 'accuracy', 'easy', 'make', 'transparent', 'help', 'centrally', 'manage', 'datum', 'platform', 'centrally', 'manage', 'datum', 'ingestion', 'pipeline', 'centrally', 'manage', 'datum', 'policy', 'make', 'change', 'datum', 'place', 'sye', 'explain', 'ensure', 'datum', 'remain', 'compliant', 'secure', 'consistent', 'use', 'good', 'practice', 'establish', 'centralized', 'data', 'governance', 'initiative', 'argue', 'build', 'central', 'data', 'catalog', 'incoming', 'datum', 'ingest', 'central', 'location', 'first', 'classified', 'mean', 'datum', 'identify', 'label', 'metadata', 'restriction', 'level', 'determine', 'access', 'permission', 'assign', 'facilitate', 'share', 'organization', 'centralized', 'catalog', 'datum', 'reside', 'map', 'explain', 'syed', 'catalog', 'classify', 'share', 'basically', 'break', 'silo', 'data', 'catalog', 'effectively', 'create', 'compliant', 'secure', 'datum', 'marketplace', 'allow', 'team', 'access', 'datum', 'permission', 'use', 'beneficial', 'several', 'front', 'assist', 'datum', 'discovery', 'scale', 'turn', 'decrease', 'development', 'time', 'spur', 'innovation', 'catalog', 'datum', 'also', 'come', 'context', 'make', 'easy', 'fast', 'understand', 'use', 'make', 'business', 'decision', 'level', 'datum', 'stability', 'foster', 'datum', 'quality', 'datum', 'integrity', 'datum', 'lineage', 'instill', 'trust', 'datum', 'essential', 'component', 'datum', 'value', 'datum', 'trust', 'use', 'inform', 'business', 'decision', 'data', 'governance', 'initiative', 'successful', 'buyin', 'adoption', 'cultivate', 'culture', 'shift', 'need', 'implement', 'data', 'governance', '’', 'essential', 'tool', 'process', 'work', 'concert', '’', 'important', 'data', 'catalog', 'always', 'sync', 'datum', 'platform', 'say', 'sye', 'lot', 'company', 'pay', 'attention', 'problem', 'forget', 'register', 'orphan', 'data', 'everywhere', 'add', 'change', 'reflect', 'catalog', 'completely', 'lose', 'value', 'user', 'frustrate', 'lack', 'functionality', 'adoption', 'uphill', 'battle', 'also', 'important', 'tool', 'process', 'remain', 'flexible', 'tool', 'need', 'accommodate', 'unique', 'load', 'pattern', 'line', 'business', 'note', 'sye', 'otherwise', 'team', 'find', 'workaround', 'achieve', 'goal', 'sometimes', 'call', 'shadow', 'jeopardize', 'integrity', 'datum', 'governance', 'framework', 'perhaps', 'important', 'adoption', 'tool', 'process', 'need', 'clearcut', 'simple', 'easy', 'use', 'say', 'syed', 'really', 'empower', 'business', 'datum', 'treat', 'datum', 'product', 'servicelevel', 'agreement', 'quality', 'resiliency', 'also', 'completely', 'different', 'mindset', 'change', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']"
Achieving flexibility with no- and low-code applications,https://www.technologyreview.com/2022/07/06/1055376/achieving-flexibility-with-no-and-low-code-applications/,2022-07-06,"There was a time when digital transformation plans were mapped out over five years or more and implementation meant a slow, staged adoption. But then covid-19 hit, and the world changed overnight. The pandemic created more urgency than any business could have anticipated in their digitization journeys as companies were forced to quickly adapt to…","Provided byNeptune Software  There was a time when digital transformation plans were mapped out over five years or more and implementation meant a slow, staged adoption. But then covid-19 hit, and the world changed overnight. The pandemic created more urgency than any business could have anticipated in their digitization journeys as companies were forced to quickly adapt to remote and hybrid working styles. Since then, rapid prototyping, flexible and evolving solutions, and the need for development alacrity have become vital for enterprises. Likewise, enterprise application development has changed. There are still teams of programmers and IT professionals building solutions, but line-of-business (LOB) teams often need to provide their own solutions to meet tight deadlines. In the new landscape, LOB teams must improvise and innovate on the spot, adding technology where possible to save time or link teams and data. It's not just about individuals and teams working from home. It's also about expansion into remote areas, where warehouse and field workers may not always have reliable connectivity. These workers need offline solutions that can connect with corporate systems and update back-end databases when connections are available. And it all must work together to ensure everyone stays on track, sticking to deadline and budget. When developers are assigned a project with a seemingly impossible timeframe, eyerolls along with comments like it’s a “mere matter of programming” are common. Unfortunately, business teams now need a constant flow of ""mere-matter-of-programming"" miracles to simply keep up. These include custom solutions that work on mobile devices, and those that can integrate into corporate data centers, enterprise resource planning systems, Internet-of-Things-based networks, and deeply entrenched operations systems like SAP. The challenge is that programming these systems takes years to learn, and with programming teams already working at capacity, meeting the immediate and evolving needs of frontline workers seems impossible. So, this is the conundrum: LOB teams absolutely need new and innovative applications right now. Programming teams just don't have the bandwidth. Yet, these are challenges that must be solved, because the new normal accepts no excuses. However, what if the LOB teams could build their own solutions? Forget that coding skills are required for a moment—think about how seamless this could be. In a traditional setup, LOB leaders need to explain processes and guide coders in developing solutions. But if frontline team members could create their own software, there would be no need to pass the message down the line and risks of miscommunication would be eliminated. This is where the ""citizen developer""—employees with little to no coding experience that build applications with technology approved by IT units—comes into play, as LOB teams can build their own software. To be sure, some problems will require a qualified computer scientist, but citizen-developer toolkits enable laymen to manage simpler solutions on their own. Many users who build their own solutions rely on ""no-code"" tools, which are usually sets of pre-built components and templates that can be combined for specific needs. They often start with a form or app builder, which enables the user or developer to capture information in a structured way that's unique to their business processes. But no-code solutions are limited to their component parts and often lack fine-tuned integration capabilities. They'll interface with major cloud platforms, but not with production workflow environments like the full range of SAP offerings. As it turns out, there's a middle ground between ""build it from the ground up"" and ""build it by choosing a few menu items."" That middle ground is called “low-code.” The idea is that much of the solution can be crafted by individual users without code, but there are coding ""hooks"" that allow more experienced developers to bridge these creations with back-end systems and even other custom applications from other LOBs.  By combining no-code with application programming interfaces (APIs), companies get the best of both worlds: empowered users who can build custom solutions and IT management with oversight of critical back-end data systems. Such low-code approaches can also reduce months, and even years, from the development cycle, enabling custom solutions to be fielded at the pace of our new normal, which is to say, ""fast."" One leading provider of low-code solutions is Neptune Software, a company founded by three experienced SAP consultants over a decade ago. The founders' SAP experience is embedded deeply in Neptune's DNA. Neptune DXP is the only low-code platform that sits within the SAP system and is certified by SAP to be compatible with SAP NetWeaver 7.X, SAP S/4HANA, and SAP BTP. The company's fully certified platform also includes SAP Solution Manager Ready functionality, and it works with SAP S/4HANA Cloud, private edition, and RISE with SAP. But all that compatibility wouldn't mean anything if Neptune Software didn't empower users to create, build, and develop. Fortunately, that's exactly what it does: it allows users to craft application interfaces, digital employee experiences, and user experiences that are appropriate to the workflow challenge at hand. Neptune Software provides online, offline, and mobile capabilities that can scale with SAP installation, bringing low-code capabilities to almost any project. For more information, visit Neptune Software. This content was produced by Neptune Software. It was not written by MIT Technology Review's editorial staff.  ","Provided byNeptune Software There was a time when digital transformation plans were mapped out over five years or more and implementation meant a slow, staged adoption. But then covid-19 hit, and the world changed overnight. The pandemic created more urgency than any business could have anticipated in their digitization journeys as companies were forced to quickly adapt to remote and hybrid working styles. Since then, rapid prototyping, flexible and evolving solutions, and the need for development alacrity have become vital for enterprises. Likewise, enterprise application development has changed. There are still teams of programmers and IT professionals building solutions, but line-of-business (LOB) teams often need to provide their own solutions to meet tight deadlines. In the new landscape, LOB teams must improvise and innovate on the spot, adding technology where possible to save time or link teams and data. It's not just about individuals and teams working from home. It's also about expansion into remote areas, where warehouse and field workers may not always have reliable connectivity. These workers need offline solutions that can connect with corporate systems and update back-end databases when connections are available. And it all must work together to ensure everyone stays on track, sticking to deadline and budget. When developers are assigned a project with a seemingly impossible timeframe, eyerolls along with comments like it’s a “mere matter of programming” are common. Unfortunately, business teams now need a constant flow of ""mere-matter-of-programming"" miracles to simply keep up. These include custom solutions that work on mobile devices, and those that can integrate into corporate data centers, enterprise resource planning systems, Internet-of-Things-based networks, and deeply entrenched operations systems like SAP. The challenge is that programming these systems takes years to learn, and with programming teams already working at capacity, meeting the immediate and evolving needs of frontline workers seems impossible. So, this is the conundrum: LOB teams absolutely need new and innovative applications right now. Programming teams just don't have the bandwidth. Yet, these are challenges that must be solved, because the new normal accepts no excuses. However, what if the LOB teams could build their own solutions? Forget that coding skills are required for a moment—think about how seamless this could be. In a traditional setup, LOB leaders need to explain processes and guide coders in developing solutions. But if frontline team members could create their own software, there would be no need to pass the message down the line and risks of miscommunication would be eliminated. This is where the ""citizen developer""—employees with little to no coding experience that build applications with technology approved by IT units—comes into play, as LOB teams can build their own software. To be sure, some problems will require a qualified computer scientist, but citizen-developer toolkits enable laymen to manage simpler solutions on their own. Many users who build their own solutions rely on ""no-code"" tools, which are usually sets of pre-built components and templates that can be combined for specific needs. They often start with a form or app builder, which enables the user or developer to capture information in a structured way that's unique to their business processes. But no-code solutions are limited to their component parts and often lack fine-tuned integration capabilities. They'll interface with major cloud platforms, but not with production workflow environments like the full range of SAP offerings. As it turns out, there's a middle ground between ""build it from the ground up"" and ""build it by choosing a few menu items."" That middle ground is called “low-code.” The idea is that much of the solution can be crafted by individual users without code, but there are coding ""hooks"" that allow more experienced developers to bridge these creations with back-end systems and even other custom applications from other LOBs. By combining no-code with application programming interfaces (APIs), companies get the best of both worlds: empowered users who can build custom solutions and IT management with oversight of critical back-end data systems. Such low-code approaches can also reduce months, and even years, from the development cycle, enabling custom solutions to be fielded at the pace of our new normal, which is to say, ""fast."" One leading provider of low-code solutions is Neptune Software, a company founded by three experienced SAP consultants over a decade ago. The founders' SAP experience is embedded deeply in Neptune's DNA. Neptune DXP is the only low-code platform that sits within the SAP system and is certified by SAP to be compatible with SAP NetWeaver 7.X, SAP S/4HANA, and SAP BTP. The company's fully certified platform also includes SAP Solution Manager Ready functionality, and it works with SAP S/4HANA Cloud, private edition, and RISE with SAP. But all that compatibility wouldn't mean anything if Neptune Software didn't empower users to create, build, and develop. Fortunately, that's exactly what it does: it allows users to craft application interfaces, digital employee experiences, and user experiences that are appropriate to the workflow challenge at hand. Neptune Software provides online, offline, and mobile capabilities that can scale with SAP installation, bringing low-code capabilities to almost any project. For more information, visit Neptune Software. This content was produced by Neptune Software. It was not written by MIT Technology Review's editorial staff.","['provide', 'byneptune', 'software', 'time', 'digital', 'transformation', 'plan', 'map', 'year', 'implementation', 'mean', 'slow', 'stage', 'adoption', 'covid19', 'hit', 'world', 'change', 'overnight', 'pandemic', 'create', 'urgency', 'business', 'anticipate', 'digitization', 'journey', 'company', 'force', 'quickly', 'adapt', 'remote', 'hybrid', 'work', 'style', 'rapid', 'prototype', 'flexible', 'evolve', 'solution', 'need', 'development', 'alacrity', 'become', 'vital', 'enterprise', 'likewise', 'enterprise', 'application', 'development', 'change', 'still', 'team', 'programmer', 'professional', 'build', 'solution', 'lineofbusiness', 'lob', 'team', 'often', 'need', 'provide', 'solution', 'meet', 'tight', 'deadline', 'new', 'landscape', 'lob', 'team', 'improvise', 'innovate', 'spot', 'add', 'technology', 'possible', 'save', 'time', 'link', 'team', 'datum', 'individual', 'team', 'work', 'home', 'also', 'expansion', 'remote', 'area', 'warehouse', 'field', 'worker', 'always', 'reliable', 'connectivity', 'worker', 'need', 'offline', 'solution', 'connect', 'corporate', 'system', 'update', 'backend', 'database', 'connection', 'available', 'work', 'together', 'ensure', 'stay', 'track', 'stick', 'deadline', 'budget', 'developer', 'assign', 'project', 'seemingly', 'impossible', 'timeframe', 'eyeroll', 'comment', '’', 'mere', 'matter', 'programming', 'common', 'unfortunately', 'business', 'team', 'need', 'constant', 'flow', 'merematterofprogramme', 'miracle', 'simply', 'keep', 'include', 'custom', 'solution', 'work', 'mobile', 'device', 'integrate', 'corporate', 'datum', 'center', 'enterprise', 'resource', 'planning', 'system', 'internetofthingsbase', 'network', 'deeply', 'entrenched', 'operation', 'system', 'sap', 'challenge', 'program', 'system', 'take', 'year', 'learn', 'programming', 'team', 'already', 'work', 'capacity', 'meet', 'immediate', 'evolve', 'need', 'frontline', 'worker', 'seem', 'impossible', 'conundrum', 'lob', 'team', 'absolutely', 'need', 'new', 'innovative', 'application', 'right', 'program', 'team', 'bandwidth', 'yet', 'challenge', 'solve', 'new', 'normal', 'accept', 'excuse', 'however', 'lob', 'team', 'build', 'solution', 'forget', 'code', 'skill', 'require', 'moment', 'think', 'seamless', 'traditional', 'setup', 'lob', 'leader', 'need', 'explain', 'process', 'guide', 'coder', 'develop', 'solution', 'frontline', 'team', 'member', 'create', 'software', 'need', 'pass', 'message', 'line', 'risk', 'miscommunication', 'eliminate', 'citizen', 'developer', 'employee', 'little', 'code', 'experience', 'build', 'application', 'technology', 'approve', 'unit', 'come', 'play', 'lob', 'team', 'build', 'software', 'sure', 'problem', 'require', 'qualified', 'computer', 'scientist', 'citizendeveloper', 'toolkit', 'enable', 'layman', 'manage', 'simple', 'solution', 'many', 'user', 'build', 'solution', 'rely', 'nocode', 'tool', 'usually', 'set', 'prebuilt', 'component', 'template', 'combine', 'specific', 'need', 'often', 'start', 'form', 'app', 'builder', 'enable', 'user', 'developer', 'capture', 'information', 'structured', 'way', 'unique', 'business', 'process', 'nocode', 'solution', 'limit', 'component', 'part', 'often', 'lack', 'finetune', 'integration', 'capability', 'interface', 'major', 'cloud', 'platform', 'production', 'workflow', 'environment', 'full', 'range', 'sap', 'offering', 'turn', 'middle', 'ground', 'build', 'ground', 'build', 'choose', 'menu', 'item', 'middle', 'ground', 'call', 'idea', 'much', 'solution', 'craft', 'individual', 'user', 'code', 'code', 'hook', 'allow', 'experienced', 'developer', 'bridge', 'creation', 'backend', 'system', 'even', 'custom', 'application', 'lob', 'combine', 'nocode', 'application', 'programming', 'interface', 'apis', 'company', 'get', 'good', 'world', 'empower', 'user', 'build', 'custom', 'solution', 'management', 'oversight', 'critical', 'backend', 'datum', 'system', 'lowcode', 'approach', 'also', 'reduce', 'month', 'even', 'year', 'development', 'cycle', 'enable', 'custom', 'solution', 'field', 'pace', 'new', 'normal', 'say', 'fast', 'lead', 'provider', 'solution', 'neptune', 'software', 'company', 'found', 'experienced', 'sap', 'consultant', 'decade', 'ago', 'founder', 'experience', 'embed', 'deeply', 'neptune', 'dxp', 'platform', 'sit', 'system', 'certify', 'compatible', 'company', 'fully', 'certify', 'platform', 'also', 'include', 'solution', 'manager', 'ready', 'functionality', 'work', 'edition', 'rise', 'compatibility', 'mean', 'neptune', 'software', 'empower', 'user', 'create', 'build', 'develop', 'fortunately', 'exactly', 'allow', 'user', 'craft', 'application', 'interface', 'digital', 'employee', 'experience', 'user', 'experience', 'appropriate', 'workflow', 'challenge', 'hand', 'neptune', 'software', 'provide', 'online', 'offline', 'mobile', 'capability', 'scale', 'sap', 'installation', 'bring', 'lowcode', 'capability', 'almost', 'project', 'information', 'visit', 'neptune', 'software', 'content', 'produce', 'neptune', 'software', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']"
