title,url,date,summary,text,cleaning,tokens
CMU Takes a Big Step Toward Real-Time Realistic Video Generation Based on Language Descriptions,https://syncedreview.com/2022/10/26/cmu-takes-a-big-step-toward-real-time-realistic-video-generation-based-on-language-descriptions/,2022-10-26,"
In the new paper Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization, researchers from Carnegie Mellon University leverage CLIP-guided, pixel-level optimization to generate 720p resolution videos from natural language descriptions at a rate of one-to-two frames per second — taking a big step towards a real-time text-to-video system.

 ","There are now dozens of AI-powered text-to-image models on the Internet, with DALL.E Mini alone generating more than 50,000 images daily from users’ natural language prompts. The next challenging step for such generative AI models is text-to-video — which brings the potential for creating animated scenes based on users’ storytelling inputs. While current text-to-video approaches guided by Open AI’s CLIP network can translate text into highly-realistic imagery, they are slow — requiring from 17 seconds to five minutes to generate a single frame of video. In the new paper Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization, researchers from Carnegie Mellon University leverage CLIP-guided, pixel-level optimization to generate 720p resolution videos from natural language descriptions at a rate of one-to-two frames per second — taking a big step towards a real-time text-to-video system. Existing CLIP-Guided text-to-video approaches generate their highly realistic imagery by optimizing through large pretrained image generator diffusion models, a process that is both time-consuming and computationally heavy. The CMU team employs a novel two-step approach to approach real-time text-to-video generation: 1) Generating noisy semantic content at a fast speed; and 2) Refining the generated image textures in a post-processing step. The proposed approach generates each frame sequentially while iterating through the input language to guide the content. CLIP-Guided techniques are used to compare the frame and the language description and evolve the frame toward consistency with the content. A trained CycleGAN model smooths and denoises the generated images. In their empirical study, the team demonstrated their approach’s ability to generate realistic videos at up to 720p resolution at speeds 20-300 times faster than existing methods. The code and sample videos are available on the team’s website. In future work, the researchers plan to add priors to enable smoother motion in the videos and improve user control over their style and appearance. The paper Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","There are now dozens of AI-powered text-to-image models on the Internet, with DALL.E Mini alone generating more than 50,000 images daily from users’ natural language prompts. The next challenging step for such generative AI models is text-to-video — which brings the potential for creating animated scenes based on users’ storytelling inputs. While current text-to-video approaches guided by Open AI’s CLIP network can translate text into highly-realistic imagery, they are slow — requiring from 17 seconds to five minutes to generate a single frame of video. In the new paper Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization, researchers from Carnegie Mellon University leverage CLIP-guided, pixel-level optimization to generate 720p resolution videos from natural language descriptions at a rate of one-to-two frames per second — taking a big step towards a real-time text-to-video system. Existing CLIP-Guided text-to-video approaches generate their highly realistic imagery by optimizing through large pretrained image generator diffusion models, a process that is both time-consuming and computationally heavy. The CMU team employs a novel two-step approach to approach real-time text-to-video generation: 1) Generating noisy semantic content at a fast speed; and 2) Refining the generated image textures in a post-processing step. The proposed approach generates each frame sequentially while iterating through the input language to guide the content. CLIP-Guided techniques are used to compare the frame and the language description and evolve the frame toward consistency with the content. A trained CycleGAN model smooths and denoises the generated images. In their empirical study, the team demonstrated their approach’s ability to generate realistic videos at up to 720p resolution at speeds 20-300 times faster than existing methods. The code and sample videos are available on the team’s website. In future work, the researchers plan to add priors to enable smoother motion in the videos and improve user control over their style and appearance. The paper Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['dozen', 'aipowere', 'texttoimage', 'model', 'internet', 'dalle', 'mini', 'alone', 'generate', 'image', 'daily', 'user', 'natural', 'language', 'prompt', 'next', 'challenge', 'step', 'generative', 'ai', 'model', 'texttovideo', 'bring', 'potential', 'create', 'animate', 'scene', 'base', 'user', 'storytelling', 'input', 'current', 'texttovideo', 'approach', 'guide', 'open', 'ai', 'network', 'translate', 'text', 'highlyrealistic', 'imagery', 'slow', 'require', 'second', 'minute', 'generate', 'single', 'frame', 'video', 'new', 'paper', 'realtime', 'text2video', 'clipguide', 'pixellevel', 'optimization', 'researcher', 'leverage', 'clipguide', 'pixellevel', 'optimization', 'generate', 'resolution', 'video', 'natural', 'language', 'description', 'rate', 'onetotwo', 'frame', 'second', 'take', 'big', 'step', 'realtime', 'texttovideo', 'system', 'exist', 'clipguide', 'texttovideo', 'approach', 'generate', 'highly', 'realistic', 'imagery', 'optimize', 'large', 'pretraine', 'image', 'generator', 'diffusion', 'model', 'process', 'timeconsuming', 'computationally', 'heavy', 'cmu', 'team', 'employ', 'novel', 'twostep', 'approach', 'approach', 'realtime', 'texttovideo', 'generation', 'generate', 'noisy', 'semantic', 'content', 'fast', 'speed', 'refine', 'generate', 'image', 'texture', 'postprocesse', 'step', 'propose', 'approach', 'generate', 'frame', 'sequentially', 'iterate', 'input', 'language', 'guide', 'content', 'clipguide', 'technique', 'use', 'compare', 'frame', 'language', 'description', 'evolve', 'frame', 'consistency', 'content', 'train', 'cyclegan', 'model', 'smooth', 'denoise', 'generate', 'image', 'empirical', 'study', 'team', 'demonstrate', 'approach', 'ability', 'generate', 'realistic', 'video', '720p', 'resolution', 'speed', 'time', 'fast', 'exist', 'method', 'code', 'sample', 'video', 'available', 'team', '’s', 'website', 'future', 'work', 'researcher', 'plan', 'add', 'prior', 'enable', 'smooth', 'motion', 'video', 'improve', 'user', 'control', 'style', 'appearance', 'paper', 'realtime', 'text2video', 'clipguide', 'pixellevel', 'optimization', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
DeepMind Study Shows That Language Models Can Learn From Explanations in Context Even Without Tuning,https://syncedreview.com/2022/10/25/deepmind-study-shows-that-language-models-can-learn-from-explanations-in-context-even-without-tuning/,2022-10-25,"
In the new paper Can Language Models Learn From Explanations in Context?, DeepMind researchers investigate how different types of explanations, instructions, and controls affect language models’ zero- and few-shot performance and how such explanations can support in-context learning for large language models on challenging tasks.
","If you have ever considered the answer to a question and asked “…but why?” you are not alone. Humans have an innate ability to improve their learning and broaden their understanding via explanations that relate examples to principles. The machine learning community in recent years has witnessed the rapid growth of few-shot prompting language models (LMs) that exhibit impressive transfer learning capability, enabling them to successfully perform new tasks by adapting to a few in-context examples. Might these LMs benefit, as humans do, from explanations of these few-shot examples? In the new paper Can Language Models Learn From Explanations in Context?, DeepMind researchers investigate how different types of explanations, instructions, and controls affect language models’ zero- and few-shot performance and how such explanations can support in-context learning for large language models on challenging tasks. The team highlights their main contributions as follows: The team considered a set of decoder-only transformer models ranging from 1 billion to 280 billion parameters and crafted a variety of control explanations that match different aspects of the semantics and word- or sentence-level content, including scrambled explanations, true non-explanations, and other item explanations. They tested model performance under each prompt condition on all task dataset items (except those in the prompt) and calculated the model’s likelihood of returning each answer option. They then chose the highest-likelihood answer from the set and evaluated model accuracy based on the answer scores defined by the task. In their empirical experiments, the team examined the benefits of different prompt components for the largest (280B parameter) LM and the relative distribution of benefits from different explanation types. They also provided raw summaries of the average effects of untuned explanations across model scales. Their findings can be summarized as follows: The researchers believe their work can contribute to improved prompt engineering and scientific understanding of the in-context learning abilities of large LMs. The paper Can Language Models Learn From Explanations in Context? is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","If you have ever considered the answer to a question and asked “…but why?” you are not alone. Humans have an innate ability to improve their learning and broaden their understanding via explanations that relate examples to principles. The machine learning community in recent years has witnessed the rapid growth of few-shot prompting language models (LMs) that exhibit impressive transfer learning capability, enabling them to successfully perform new tasks by adapting to a few in-context examples. Might these LMs benefit, as humans do, from explanations of these few-shot examples? In the new paper Can Language Models Learn From Explanations in Context?, DeepMind researchers investigate how different types of explanations, instructions, and controls affect language models’ zero- and few-shot performance and how such explanations can support in-context learning for large language models on challenging tasks. The team highlights their main contributions as follows: The team considered a set of decoder-only transformer models ranging from 1 billion to 280 billion parameters and crafted a variety of control explanations that match different aspects of the semantics and word- or sentence-level content, including scrambled explanations, true non-explanations, and other item explanations. They tested model performance under each prompt condition on all task dataset items (except those in the prompt) and calculated the model’s likelihood of returning each answer option. They then chose the highest-likelihood answer from the set and evaluated model accuracy based on the answer scores defined by the task. In their empirical experiments, the team examined the benefits of different prompt components for the largest (280B parameter) LM and the relative distribution of benefits from different explanation types. They also provided raw summaries of the average effects of untuned explanations across model scales. Their findings can be summarized as follows: The researchers believe their work can contribute to improved prompt engineering and scientific understanding of the in-context learning abilities of large LMs. The paper Can Language Models Learn From Explanations in Context? is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['ever', 'consider', 'answer', 'question', 'ask', 'alone', 'human', 'innate', 'ability', 'improve', 'learning', 'broaden', 'understanding', 'explanation', 'relate', 'example', 'principle', 'machine', 'learn', 'community', 'recent', 'year', 'witness', 'rapid', 'growth', 'fewshot', 'prompt', 'language', 'model', 'lm', 'exhibit', 'impressive', 'transfer', 'learning', 'capability', 'enable', 'successfully', 'perform', 'new', 'task', 'adapt', 'incontext', 'example', 'lm', 'benefit', 'human', 'explanation', 'fewshot', 'example', 'new', 'paper', 'language', 'model', 'learn', 'explanation', 'context', 'deepmind', 'researcher', 'investigate', 'different', 'type', 'explanation', 'instruction', 'control', 'affect', 'language', 'model', 'fewshot', 'performance', 'explanation', 'support', 'incontext', 'learn', 'large', 'language', 'model', 'challenging', 'task', 'team', 'highlight', 'main', 'contribution', 'follow', 'team', 'consider', 'set', 'decoderonly', 'transformer', 'model', 'range', 'parameter', 'craft', 'variety', 'control', 'explanation', 'match', 'different', 'aspect', 'semantic', 'word', 'sentencelevel', 'content', 'include', 'scramble', 'explanation', 'true', 'nonexplanation', 'item', 'explanation', 'test', 'model', 'performance', 'prompt', 'condition', 'task', 'dataset', 'item', 'prompt', 'calculate', 'model', 'likelihood', 'return', 'answer', 'option', 'choose', 'highestlikelihood', 'answer', 'set', 'evaluate', 'model', 'accuracy', 'base', 'answer', 'score', 'define', 'task', 'empirical', 'experiment', 'team', 'examine', 'benefit', 'different', 'prompt', 'component', 'large', '280b', 'parameter', 'relative', 'distribution', 'benefit', 'different', 'explanation', 'type', 'also', 'provide', 'raw', 'summary', 'average', 'effect', 'untuned', 'explanation', 'model', 'scale', 'finding', 'summarize', 'follow', 'researcher', 'believe', 'work', 'contribute', 'improved', 'prompt', 'engineering', 'scientific', 'understanding', 'incontext', 'learn', 'ability', 'large', 'lm', 'paper', 'language', 'model', 'learn', 'explanation', 'context', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Google & Stanford Team Applies Chain-of-Thought Prompting to Surpass Human Performance on Challenging BIG-Bench Tasks,https://syncedreview.com/2022/10/24/google-stanford-team-applies-chain-of-thought-prompting-to-surpass-human-performance-on-challenging-big-bench-tasks/,2022-10-24,"
In the new paper Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them, a Google Research and Stanford University team applies chain-of-thought (CoT) prompting — a series of intermediate reasoning steps — to 23 BIG-Bench tasks on which language models have failed to outperform the average human rater. The proposed approach enables models to surpass human performance on 17 of the 23 tasks.
","Today’s large language models (LLMs) have demonstrated game-changing performance across a wide range of tasks and domains, but they have their limits. These weaknesses can be identified by the Beyond the Imitation Game benchmark (BIG-Bench, Srivastava et al., 2022), which evaluates LLM capabilities on a diverse suite of especially challenging tasks. A 540B parameter PaLM language model surpasses average human-rater performance on 65 percent of the BIG-Bench tasks, but what about the remainder — are they simply unsolvable by LLMs? A Google Research and Stanford University team addresses this question in the new paper Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. The team applies chain-of-thought (CoT) prompting — a series of intermediate reasoning steps inspired by the paper Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (Wei et al., 2022b) — to 23 BIG-Bench tasks on which LLMs have failed to match the average human rater. Their strongest resulting model outperforms the human baseline on 17 of the 23 tasks. The team selected their 23 evaluation tasks — a subset they dub BIG-Bench Hard (BBH) — from BIG-Bench tasks where state-of-the-art LLMs perform worse than the average human rater, tasks fundamentally not solvable by scaling existing LLMs, and tasks that require prompting techniques beyond the standard few-shot prompting setup. In their experiments, the team applied the standard BIG-Bench answer-only prompting setup and the proposed CoT prompting approach on three language model families — Codex, InstructGPT and PaLM — to explore whether and to what extent CoT prompting can improve performance on the 23 BBH tasks. The results show that conventional answer-only prompting underestimates LLM performance and capabilities on challenging tasks that require multiple reasoning steps; as CoT prompting achieves double-digit improvements for all three models, surpassing the average human-rater score on 10 of the 23 tasks on PaLM, on 15/23 tasks on InstructGPT, and on 17/23 tasks on Codex. The paper also details the effects of CoT prompting on four BBH task categories: algorithmic and multi-step arithmetic reasoning, natural language understanding, use of world knowledge, and multilingual knowledge and reasoning.The data, prompts, and Codex model outputs are available on the project’s GitHub. The paper Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Today’s large language models (LLMs) have demonstrated game-changing performance across a wide range of tasks and domains, but they have their limits. These weaknesses can be identified by the Beyond the Imitation Game benchmark (BIG-Bench, Srivastava et al., 2022), which evaluates LLM capabilities on a diverse suite of especially challenging tasks. A 540B parameter PaLM language model surpasses average human-rater performance on 65 percent of the BIG-Bench tasks, but what about the remainder — are they simply unsolvable by LLMs? A Google Research and Stanford University team addresses this question in the new paper Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. The team applies chain-of-thought (CoT) prompting — a series of intermediate reasoning steps inspired by the paper Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (Wei et al., 2022b) — to 23 BIG-Bench tasks on which LLMs have failed to match the average human rater. Their strongest resulting model outperforms the human baseline on 17 of the 23 tasks. The team selected their 23 evaluation tasks — a subset they dub BIG-Bench Hard (BBH) — from BIG-Bench tasks where state-of-the-art LLMs perform worse than the average human rater, tasks fundamentally not solvable by scaling existing LLMs, and tasks that require prompting techniques beyond the standard few-shot prompting setup. In their experiments, the team applied the standard BIG-Bench answer-only prompting setup and the proposed CoT prompting approach on three language model families — Codex, InstructGPT and PaLM — to explore whether and to what extent CoT prompting can improve performance on the 23 BBH tasks. The results show that conventional answer-only prompting underestimates LLM performance and capabilities on challenging tasks that require multiple reasoning steps; as CoT prompting achieves double-digit improvements for all three models, surpassing the average human-rater score on 10 of the 23 tasks on PaLM, on 15/23 tasks on InstructGPT, and on 17/23 tasks on Codex. The paper also details the effects of CoT prompting on four BBH task categories: algorithmic and multi-step arithmetic reasoning, natural language understanding, use of world knowledge, and multilingual knowledge and reasoning.The data, prompts, and Codex model outputs are available on the project’s GitHub. The paper Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['today', 'large', 'language', 'model', 'llm', 'demonstrate', 'gamechange', 'performance', 'wide', 'range', 'task', 'domain', 'limit', 'weakness', 'identify', 'imitation', 'game', 'benchmark', 'bigbench', 'evaluate', 'llm', 'capability', 'diverse', 'suite', 'especially', 'challenging', 'task', 'parameter', 'model', 'surpasse', 'average', 'humanrater', 'performance', 'percent', 'bigbench', 'task', 'remainder', 'simply', 'unsolvable', 'llm', 'university', 'team', 'address', 'question', 'new', 'paper', 'challenge', 'bigbench', 'task', 'chainofthought', 'solve', 'team', 'apply', 'chainofthought', 'cot', 'prompt', 'series', 'intermediate', 'reasoning', 'step', 'inspire', 'paper', 'chainofthought', 'prompt', 'elicit', 'reasoning', 'large', 'language', 'model', 'bigbench', 'task', 'llm', 'fail', 'match', 'average', 'human', 'rater', 'strong', 'result', 'model', 'outperform', 'human', 'baseline', 'task', 'team', 'select', 'evaluation', 'task', 'subset', 'dub', 'bigbench', 'hard', 'bbh', 'bigbench', 'task', 'stateoftheart', 'llm', 'perform', 'bad', 'average', 'human', 'rater', 'task', 'fundamentally', 'solvable', 'scale', 'exist', 'llm', 'task', 'require', 'prompt', 'technique', 'standard', 'fewshot', 'prompting', 'setup', 'experiment', 'team', 'apply', 'standard', 'bigbench', 'answeronly', 'prompt', 'setup', 'propose', 'cot', 'prompt', 'approach', 'language', 'model', 'family', 'codex', 'instructgpt', 'palm', 'explore', 'extent', 'cot', 'prompt', 'improve', 'performance', 'bbh', 'task', 'result', 'show', 'conventional', 'answeronly', 'prompt', 'underestimate', 'llm', 'performance', 'capability', 'challenging', 'task', 'require', 'multiple', 'reasoning', 'step', 'cot', 'prompt', 'achieve', 'doubledigit', 'improvement', 'model', 'surpass', 'average', 'humanrater', 'score', 'task', 'palm', 'task', 'instructgpt', 'task', 'codex', 'paper', 'also', 'detail', 'effect', 'cot', 'prompt', 'bbh', 'task', 'category', 'algorithmic', 'multistep', 'arithmetic', 'reasoning', 'natural', 'language', 'understanding', 'use', 'world', 'knowledge', 'multilingual', 'knowledge', 'reasoningthe', 'data', 'prompt', 'codex', 'model', 'output', 'available', 'project', 'paper', 'challenge', 'bigbench', 'task', 'chainofthought', 'solve', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
"Wider, Not Deeper: Cambridge, Oxford & ICL Challenge Conventional Transformer Design Approaches",https://syncedreview.com/2022/10/20/wider-not-deeper-cambridge-oxford-icl-challenge-conventional-transformer-design-approaches/,2022-10-20,"
In the new paper Wide Attention Is The Way Forward For Transformers, a research team from the University of Cambridge, Imperial College London, and the University of Oxford challenges the commonly held belief that deeper is better for transformer architectures, demonstrating that wider layers result in superior performance on natural language processing tasks.
","Transformers have become a preferred architecture in the machine learning community, and building deeper models is the common approach for improving their performance. But is deeper necessarily better? In the new paper Wide Attention Is The Way Forward For Transformers, a research team from the University of Cambridge, Imperial College London, and the University of Oxford challenges the commonly held belief that deeper is better for transformer architectures, demonstrating that wider layers result in superior performance on natural language processing (NLP) tasks. The team summarizes their main contributions as follows: The paper first evaluates the impact of model aspect ratio — the ratio of layers to heads — on model accuracy, runtime performance, model size, and interpretability. Unlike conventional approaches, which focus on finding more efficient attention styles or using network architecture search (NAS) to obtain optimal combination operators, the team considers a more coarse-grained design space by changing the model aspect ratio. This enables them to evaluate novel architectures, such as a single-layer model with many parallel heads. The researchers performed experiments on four text classification tasks: sentiment analysis on the IMDb dataset at both the token and byte level, Listops 10-way classification, and byte-level document matching. They also investigated how widening the attention layer would affect ten different types of transformer attention mechanisms. The researchers summarize the empirical results as follows: Overall, this work shows that the proposed wide transformer networks can achieve performance comparable to or better than deep transformers. The researchers conclude that wider and shallower models are thus a “viable and desirable alternative” for transformers when there is no pretraining of weights or embeddings.The paper Wide Attention Is The Way Forward For Transformers is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Transformers have become a preferred architecture in the machine learning community, and building deeper models is the common approach for improving their performance. But is deeper necessarily better? In the new paper Wide Attention Is The Way Forward For Transformers, a research team from the University of Cambridge, Imperial College London, and the University of Oxford challenges the commonly held belief that deeper is better for transformer architectures, demonstrating that wider layers result in superior performance on natural language processing (NLP) tasks. The team summarizes their main contributions as follows: The paper first evaluates the impact of model aspect ratio — the ratio of layers to heads — on model accuracy, runtime performance, model size, and interpretability. Unlike conventional approaches, which focus on finding more efficient attention styles or using network architecture search (NAS) to obtain optimal combination operators, the team considers a more coarse-grained design space by changing the model aspect ratio. This enables them to evaluate novel architectures, such as a single-layer model with many parallel heads. The researchers performed experiments on four text classification tasks: sentiment analysis on the IMDb dataset at both the token and byte level, Listops 10-way classification, and byte-level document matching. They also investigated how widening the attention layer would affect ten different types of transformer attention mechanisms. The researchers summarize the empirical results as follows: Overall, this work shows that the proposed wide transformer networks can achieve performance comparable to or better than deep transformers. The researchers conclude that wider and shallower models are thus a “viable and desirable alternative” for transformers when there is no pretraining of weights or embeddings.The paper Wide Attention Is The Way Forward For Transformers is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['transformer', 'become', 'preferred', 'architecture', 'machine', 'learn', 'community', 'build', 'deep', 'model', 'common', 'approach', 'improve', 'performance', 'deeply', 'necessarily', 'well', 'new', 'paper', 'wide', 'attention', 'way', 'forward', 'transformer', 'research', 'team', 'imperial', 'college', 'challenge', 'commonly', 'hold', 'belief', 'deep', 'well', 'transformer', 'architecture', 'demonstrate', 'wide', 'layer', 'result', 'superior', 'performance', 'natural', 'language', 'processing', 'task', 'team', 'summarize', 'main', 'contribution', 'follow', 'paper', 'first', 'evaluate', 'impact', 'model', 'aspect', 'ratio', 'ratio', 'layer', 'head', 'model', 'accuracy', 'runtime', 'performance', 'model', 'size', 'interpretability', 'conventional', 'approach', 'focus', 'find', 'efficient', 'attention', 'style', 'use', 'network', 'architecture', 'search', 'na', 'obtain', 'optimal', 'combination', 'operator', 'team', 'consider', 'coarsegrained', 'design', 'space', 'change', 'model', 'aspect', 'ratio', 'enable', 'evaluate', 'novel', 'architecture', 'singlelayer', 'model', 'many', 'parallel', 'head', 'researcher', 'perform', 'experiment', 'text', 'classification', 'task', 'sentiment', 'analysis', 'imdb', 'dataset', 'token', 'byte', 'level', 'listop', '10way', 'classification', 'bytelevel', 'document', 'matching', 'also', 'investigate', 'widen', 'attention', 'layer', 'affect', 'different', 'type', 'transformer', 'attention', 'mechanism', 'researcher', 'summarize', 'empirical', 'result', 'follow', 'overall', 'work', 'show', 'propose', 'wide', 'transformer', 'network', 'achieve', 'performance', 'comparable', 'well', 'deep', 'transformer', 'researcher', 'conclude', 'wide', 'shallow', 'model', 'thus', 'viable', 'desirable', 'alternative', 'transformer', 'pretraining', 'weight', 'paper', 'wide', 'attention', 'way', 'forward', 'transformer', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
"Embedding Training With 1% GPU Memory and 100 Times Less Budget, an Open Source Solution for Super-Large Recommendation Model Training on a Single GPU",https://syncedreview.com/2022/10/19/embedding-training-with-1-gpu-memory-and-100-times-less-budget-an-open-source-solution-for-super-large-recommendation-model-training-on-a-single-gpu/,2022-10-19,"
Colossal-AI has successfully used a heterogeneous training strategy to increase the number of NLP model training parameters capacity by hundreds of times at the same hardware. And experiment results show that it only needs to keep 1~5% of the embedding parameters in the GPU, and is still able to maintain excellent end-to-end training speed.
","Deep recommendation models (DLRMs) have become critical for deep learning applications in IT companies. DLRMs can be used to improve user experience for video recommendations, shopping searches, and companies’ advertisements. However, DLRMs have several limitations, including difficulties managing too much user data, frequent model updates, and high training costs. DLRMs first search an embedding bag (EmbeddingBags), and then go through a dense DNN. Embedded tables usually hold more than 99% of the memory in the DLRM, and only 1% of the computation requirements. With the help of GPU’s on-chip high-speed memory (High Bandwidth Memory) and increased computing power, GPU has become the mainstream hardware for DLRM training. However, with the increasing research depth of recommendation systems, the embedding tables are also growing in size, and the limited GPU memory is not able to keep up. The question remains: How can we use GPU to efficiently train super-large DLRM models despite the limitation of GPU memory? Colossal-AI has successfully used a heterogeneous training strategy to increase the number of NLP model training parameters capacity by hundreds of times at the same hardware. Recently, it has used a software cache method which dynamically stores the embedding table in the CPU and GPU memory to extend the parameters to the recommendation system. In relation to the software cache design, Colossal-AI also incorporates pipeline prefetching which reduces software cache retrieval and data movement overhead by observing future training data. At the same time, it trains the entire DLRM model on the GPU in a synchronized update manner, which can be scaled to multiple GPUs with the widely used hybrid parallel training method. Experiments show that Colossal-AI only needs to keep 1~5% of the embedding parameters in the GPU, and is still able to maintain excellent end-to-end training speed. Compared with other PyTorch solutions, the memory requirements are reduced by an immense magnitude, with a single GPU being able to train a terabyte-level recommendation model. As a result, the cost advantage is significant. For example, only 5GB of GPU memory can be used to train a DLRM that occupies a 91GB Embedding Bag. The training hardware cost is reduced from two NVIDIA A100s totaling about 30,000 USD, to an entry-level graphics card like RTX 3050 which only costs about 300 USD. Open Source Repo：https://github.com/hpcaitech/ColossalAI The purpose of the embedding table is to map categorical variables into floating-point variables. The following figure shows the training process of the embedding table in DLRMs. First, it identifies the corresponding records for each feature in the embedding table, outputs a feature vector through reduction operations (i.e. max, mean, and sum operations) and then inputs them to the subsequent dense neural network. The embedding table DLRM training process is comprised mainly of irregular memory access operations, so it is severely limited by the hardware memory access bandwidth. In real applications, the embedding table of a DLRM may reach hundreds of GB, or even TB levels, far exceeding the single GPU capacity of only a few tens of GB. There are many ways to increase the size of DLRM’s embedded table. Taking the memory hierarchy diagram of the GPU cluster shown in the figure below as an example, we can analyze the advantages and disadvantages of several common solutions. GPU model parallelism: During this method, the embedding table is sharded and distributed in the memory of multiple GPUs, and the intermediate results are synchronized via the interconnection network between GPUs. The disadvantage of this method is that the workload of the embedded table may not be equal, and it is difficult to scale. Furthermore, the initial investment of adding GPUs is high, and the computing power of GPU is not fully utilized. DLRM mainly utilizes the HBM bandwidth of GPUs, while computing units are not utilized well. Hybrid Training: This method starts by splitting the embedding table into two parts, one trained on the GPU and the other trained on the CPU. By using long-tail input data distribution, we can minimize the CPU computing and maximize GPU computing. However, as the batch size increases, it becomes difficult to ensure that all the data in the mini-batch hits the CPU or GPU. Additionally, since the DDR bandwidth and HBM differ by magnitude, even if just 10% of the input data is trained on the CPU, the entire system will slow down by at least two times. The CPU and GPU also need to transmit intermediate results, which requires lots of overhead communication, further slowing down the training speed. Consequently, researchers have designed methods like asynchronous updates to combat these issues, but asynchronous methods can cause uncertainty in training accuracy, and are not ideal for algorithm engineers. Software Cache: All training is performed on the GPU with this method, and the embedding tables are kept in the heterogeneous memory space composed of the CPU and GPU. Each time the software cache is used, the used part is exchanged into the GPU. In this way, storage resources are expanded inexpensively while meeting the increased demand for embedded tables. Compared to using the CPU to calculate, the entire training process is completed on the GPU, making full use of the HBM bandwidth advantage. On the contrary, cache query and data movement of this method will bring additional performance loss. Currently, there are some excellent software cache solutions for embedding tables, but they are often implemented using customized EmbeddingBags Kernel, such as fbgemm, or with the help of third-party deep learning frameworks. With native PyTorch, Colossal-AI can implement a unique set of software Cache EmbeddingBags, further optimize the DLRM training process, and propose a prefetch pipeline to further reduce Cache overhead. Colossal-AI implements a class of CachedEmbedding which works as a subclass of the nn.Module in PyTorch and can replace the native PyTorch EmbeddingBag. It consists of software which manages the CPU and GPU memory. It maintains EmbeddingBag parameters as CPU Weight. A small part of the EmbeddingBag called the CUDA Cached Weight is stored as GPU memory, which will be used for future training. During DLRM training, records of the embedding table are first identified by the current mini-batch. If some records are not in the GPU yet, they are transmitted from the CPU Weight to the CUDA Cached Weight. If there is not enough space in the GPU, the LFU algorithm will be used to discard the least frequently used embedding records. In order to query the cache efficiently, some auxiliary data structures are needed: the cached_idx_map is a 1-D array mapping the indices of records in the CPU Weight to the indices of CUDA Cached Weight, as well as the GPU access frequency. The ratio of the CUDA Cached Weight size to CPU Weight size is named cache_ratio and defaults to 1.0%.The cache operates before each forward iteration to adjust the data in the CUDA Weight in three steps. Step 1:Query CPU records: Query record indices of CPU Weight that need to be cached. This requires intersecting the cached_idx_map and the input of the current mini-batch. Step 2: Query GPU records: Identify CUDA Cached weight that should be evicted according to frequency. This requires performing a top-k operation on the different sets of cache_idx_map and input of the current mini-batch. Step 3: Data transmission: Free enough space on the CUDA Cached Weight for CPU Weight, which may lead to queried GPU records being transferred from GPU to CPU. Then move the to-be queried CPU records from the CPU Weight into CUDA Cached Weight. The processes in Step 1 and Step 2 of the Cache are memory demanding. In order to take advantage of the bandwidth on the GPU’s HBM, they are run on the GPU and implemented using drop-in API provided by PyTorch. The overhead of Cache operations is particularly prominent compared to the training operations of the embedding table on the GPU. For example, for a training task that takes 199 seconds, the overhead of the cache operation is 99 seconds, which accounts for nearly 50% of the overall computing time. The main overhead of the Cache is mainly caused by Step 1 and Step 2 in the cache operation, and the base in the figure below shows the total time decomposition of the cache operation. The red and orange stages (Step 1, 2) account for 70% of the total cache overhead. The problem above arose because the traditional Cache strategy is somewhat “short-sighted”, so the Cache is adjusted according to the input of the current mini-batch, and most of the time is wasted on query operations. In order to reduce the overhead time of the Cache, Colossal-AI has designed a “far-sighted” Cache mechanism. Instead of only performing Cache operations on the first mini-batch, Colossal-AI fetches several mini-batches that will be used later, and performs Cache query operations together. As shown in the figure below, Colossal-AI uses prefetching to merge multiple mini-batches of data and conduct one cache operation after merging. It also uses a pipeline method to overlap the overhead of data loading and model training. As shown in the following figure, the number of mini-batches prefetched in the example is 2. Before starting training, it loads mini-batch 0 and 1’s data from disk to GPU memory, conducts Cache operation, and then performs forward and back propagation and a parameter update of these two mini-batches. This can simultaneously be read with the initial data of mini-batch 2 & 3, and this part of the overhead can overlap with the calculation. Compared with the execution mode of baseline cache, Figure [Time decomposition of Cache operation] compares the time decomposition of cache operation using 8 mini-batches prefetching with a baseline cache without prefetching. The total training time dropped from 201 seconds to 120 seconds, and the proportion of cache queries shown in the figure also dropped significantly. To sum up, Cache pipeline prefetching brings two benefits. The most obvious benefit of prefetching is reducing cache operation’s Step 1 and Step 2 overhead, so that this two-step operation accounts for less than 5% of the total training process. As shown in Fig. [Time Decomposition of Cache Operations], by pre-fetching 8 mini-batches of data, the overhead of cache queries is significantly reduced compared to the baseline. By concentrating more data and improving the granularity of data transmission, the CPU-GPU transmission bandwidth can be fully utilized. For the example above, the CUDA→CPU bandwidth is increased from 860MB/s to 1477MB/s, and the CPU→CUDA bandwidth is increased from 1257MB/s to 2415MB/s, almost double the performance gain. Our CachedEmbeddingBag is consistent with the basic usage of the PyTorch EmbeddingBag. When building a recommendation model, only a few lines of code can significantly increase the capacity of the embedding table and complete TB super-large recommendation model training at a low cost. The testbed is a GPU cluster with 8x NVIDIA A100 GPU (80GB) and AMD EPYC 7543 32-Core Processor (512GB) CPU. Colossal-AI also uses Meta’s DLRM model implementation, and evaluates on a Cretio 1TB dataset, as well as a synthetic dataset. The PyTorch training with the entire embedding table on the GPU is used as the baseline in the experiments. ### Cretio 1TB embedding table has 177,944,275 records, and its memory allocation takes up 91.10GB, with embedding dim equal to 128. To accommodate EmbeddingBags on one single GPU is impossible, even with top-end NVIDIA A100 with 80GB GPU memory. Hopefully, Colossal-AI will make it possible to accomplish the training task on one GPU, with memory consumption dropping to 5.01 GB (lowering approx. 18 times), and show the possibility of training super large (terabyte-level) recommendation system models on just one GPU. In terms of training speed, the following figure shows the latency of training 100M samples with different batch sizes. Prefetch1 (shown in dark green) is the latency without pre-fetching, and Prefetch8 (shown in blue) is the latency with prefetching (prefetch mini-batch=8). This shows that prefetch flow optimization plays an important role in overall performance improvement. Each bar colored with darker colors in the figure is part of the Cache overhead, being controlled within 15% of the total training time after pre-fetching. In our experiment, DLRM is trained with 100M samples on 8 GPUs, using table-wise sharding as EmbeddingBags in a parallel manner (global batch size = 8192, prefetch size = 4). The following figure shows the training latency for cases with a different number of GPUs. ColossalAI-cr-0.05 in the figures indicates the cache ratio is 0.05, while Colossal-cr-0.5 is 0.5. The PyTorch and Colossal-AI training times are mostly similar, but PyTorch encounters OOM issues when training on 1 GPU. It can be observed that adding GPUs (increasing to 4 or 8) does not bring significant performance benefits as synchronizing results requires huge communication overhead and table-wise sharding, leading to an unbalanced slice load. In other words, using multiple GPUs to scale embedding table training does not have significant advantages. The graph below shows the maximum memory usage, varying across different numbers of GPUs. When using one GPU, only a software Cache method from Colossal-AI works, with the memory assumption of multiple cards in parallel showing a significant reduction. The synthetic dlrm_datasets from Meta Research mimic the training access behavior of embedding tables. As a result, it is usually used as a reference for testing hardware and software designs that relate to recommendation systems. Subsequently, 500 million rows of these embedding table items are selected as sub-datasets, and two EmbeddingBags of 256GB and 128GB are constructed for testing. With GPU memory limitations, PyTorch has poor performance when training on one NVIDIA A100 GPU. In contrast, Colossal-AI’s software cache significantly eases GPU memory requirements, is capable of training embedding tables as large as 256GB, and also shows the potential to scale to terabytes. The acceleration is also demonstrated by running prefetching, where total training time decreases by 60% (#prefetches = 32) and GPU memory demand does not increase. Colossal-AI is a user-friendly deep learning system that allows companies to maximize AI deployment efficiency while drastically reducing costs. Since becoming open source to the public, Colossal-AI has reached №1 in trending projects on GitHub and Papers With Code multiple times, amidst other projects that have as many as 10K stars. Colossal-AI values open source community construction, providing English and Chinese tutorials, while supporting the latest cutting-edge applications such as PaLM and AlphaFold. Ultimately, Colossal-AI is constantly increasing the availability of AI solutions across a variety of fields, including medicine, autonomous vehicles, cloud computing, retail, chip production, etc. PortalProject address: https://github.com/hpcaitech/ColossalAI Reference Embedding Training With 1% GPU Memory and 100 Times Less Budget, an Open Source Solution for Super-Large Recommendation Model Training on a Single GPU | by Yang You | Oct, 2022 | Medium https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/ We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Deep recommendation models (DLRMs) have become critical for deep learning applications in IT companies. DLRMs can be used to improve user experience for video recommendations, shopping searches, and companies’ advertisements. However, DLRMs have several limitations, including difficulties managing too much user data, frequent model updates, and high training costs. DLRMs first search an embedding bag (EmbeddingBags), and then go through a dense DNN. Embedded tables usually hold more than 99% of the memory in the DLRM, and only 1% of the computation requirements. With the help of GPU’s on-chip high-speed memory (High Bandwidth Memory) and increased computing power, GPU has become the mainstream hardware for DLRM training. However, with the increasing research depth of recommendation systems, the embedding tables are also growing in size, and the limited GPU memory is not able to keep up. The question remains: How can we use GPU to efficiently train super-large DLRM models despite the limitation of GPU memory? Colossal-AI has successfully used a heterogeneous training strategy to increase the number of NLP model training parameters capacity by hundreds of times at the same hardware. Recently, it has used a software cache method which dynamically stores the embedding table in the CPU and GPU memory to extend the parameters to the recommendation system. In relation to the software cache design, Colossal-AI also incorporates pipeline prefetching which reduces software cache retrieval and data movement overhead by observing future training data. At the same time, it trains the entire DLRM model on the GPU in a synchronized update manner, which can be scaled to multiple GPUs with the widely used hybrid parallel training method. Experiments show that Colossal-AI only needs to keep 1~5% of the embedding parameters in the GPU, and is still able to maintain excellent end-to-end training speed. Compared with other PyTorch solutions, the memory requirements are reduced by an immense magnitude, with a single GPU being able to train a terabyte-level recommendation model. As a result, the cost advantage is significant. For example, only 5GB of GPU memory can be used to train a DLRM that occupies a 91GB Embedding Bag. The training hardware cost is reduced from two NVIDIA A100s totaling about 30,000 USD, to an entry-level graphics card like RTX 3050 which only costs about 300 USD. Open Source Repo：https://github.com/hpcaitech/ColossalAI The purpose of the embedding table is to map categorical variables into floating-point variables. The following figure shows the training process of the embedding table in DLRMs. First, it identifies the corresponding records for each feature in the embedding table, outputs a feature vector through reduction operations (i.e. max, mean, and sum operations) and then inputs them to the subsequent dense neural network. The embedding table DLRM training process is comprised mainly of irregular memory access operations, so it is severely limited by the hardware memory access bandwidth. In real applications, the embedding table of a DLRM may reach hundreds of GB, or even TB levels, far exceeding the single GPU capacity of only a few tens of GB. There are many ways to increase the size of DLRM’s embedded table. Taking the memory hierarchy diagram of the GPU cluster shown in the figure below as an example, we can analyze the advantages and disadvantages of several common solutions. GPU model parallelism: During this method, the embedding table is sharded and distributed in the memory of multiple GPUs, and the intermediate results are synchronized via the interconnection network between GPUs. The disadvantage of this method is that the workload of the embedded table may not be equal, and it is difficult to scale. Furthermore, the initial investment of adding GPUs is high, and the computing power of GPU is not fully utilized. DLRM mainly utilizes the HBM bandwidth of GPUs, while computing units are not utilized well. Hybrid Training: This method starts by splitting the embedding table into two parts, one trained on the GPU and the other trained on the CPU. By using long-tail input data distribution, we can minimize the CPU computing and maximize GPU computing. However, as the batch size increases, it becomes difficult to ensure that all the data in the mini-batch hits the CPU or GPU. Additionally, since the DDR bandwidth and HBM differ by magnitude, even if just 10% of the input data is trained on the CPU, the entire system will slow down by at least two times. The CPU and GPU also need to transmit intermediate results, which requires lots of overhead communication, further slowing down the training speed. Consequently, researchers have designed methods like asynchronous updates to combat these issues, but asynchronous methods can cause uncertainty in training accuracy, and are not ideal for algorithm engineers. Software Cache: All training is performed on the GPU with this method, and the embedding tables are kept in the heterogeneous memory space composed of the CPU and GPU. Each time the software cache is used, the used part is exchanged into the GPU. In this way, storage resources are expanded inexpensively while meeting the increased demand for embedded tables. Compared to using the CPU to calculate, the entire training process is completed on the GPU, making full use of the HBM bandwidth advantage. On the contrary, cache query and data movement of this method will bring additional performance loss. Currently, there are some excellent software cache solutions for embedding tables, but they are often implemented using customized EmbeddingBags Kernel, such as fbgemm, or with the help of third-party deep learning frameworks. With native PyTorch, Colossal-AI can implement a unique set of software Cache EmbeddingBags, further optimize the DLRM training process, and propose a prefetch pipeline to further reduce Cache overhead. Colossal-AI implements a class of CachedEmbedding which works as a subclass of the nn.Module in PyTorch and can replace the native PyTorch EmbeddingBag. It consists of software which manages the CPU and GPU memory. It maintains EmbeddingBag parameters as CPU Weight. A small part of the EmbeddingBag called the CUDA Cached Weight is stored as GPU memory, which will be used for future training. During DLRM training, records of the embedding table are first identified by the current mini-batch. If some records are not in the GPU yet, they are transmitted from the CPU Weight to the CUDA Cached Weight. If there is not enough space in the GPU, the LFU algorithm will be used to discard the least frequently used embedding records. In order to query the cache efficiently, some auxiliary data structures are needed: the cached_idx_map is a 1-D array mapping the indices of records in the CPU Weight to the indices of CUDA Cached Weight, as well as the GPU access frequency. The ratio of the CUDA Cached Weight size to CPU Weight size is named cache_ratio and defaults to 1.0%.The cache operates before each forward iteration to adjust the data in the CUDA Weight in three steps. Step 1:Query CPU records: Query record indices of CPU Weight that need to be cached. This requires intersecting the cached_idx_map and the input of the current mini-batch. Step 2: Query GPU records: Identify CUDA Cached weight that should be evicted according to frequency. This requires performing a top-k operation on the different sets of cache_idx_map and input of the current mini-batch. Step 3: Data transmission: Free enough space on the CUDA Cached Weight for CPU Weight, which may lead to queried GPU records being transferred from GPU to CPU. Then move the to-be queried CPU records from the CPU Weight into CUDA Cached Weight. The processes in Step 1 and Step 2 of the Cache are memory demanding. In order to take advantage of the bandwidth on the GPU’s HBM, they are run on the GPU and implemented using drop-in API provided by PyTorch. The overhead of Cache operations is particularly prominent compared to the training operations of the embedding table on the GPU. For example, for a training task that takes 199 seconds, the overhead of the cache operation is 99 seconds, which accounts for nearly 50% of the overall computing time. The main overhead of the Cache is mainly caused by Step 1 and Step 2 in the cache operation, and the base in the figure below shows the total time decomposition of the cache operation. The red and orange stages (Step 1, 2) account for 70% of the total cache overhead. The problem above arose because the traditional Cache strategy is somewhat “short-sighted”, so the Cache is adjusted according to the input of the current mini-batch, and most of the time is wasted on query operations. In order to reduce the overhead time of the Cache, Colossal-AI has designed a “far-sighted” Cache mechanism. Instead of only performing Cache operations on the first mini-batch, Colossal-AI fetches several mini-batches that will be used later, and performs Cache query operations together. As shown in the figure below, Colossal-AI uses prefetching to merge multiple mini-batches of data and conduct one cache operation after merging. It also uses a pipeline method to overlap the overhead of data loading and model training. As shown in the following figure, the number of mini-batches prefetched in the example is 2. Before starting training, it loads mini-batch 0 and 1’s data from disk to GPU memory, conducts Cache operation, and then performs forward and back propagation and a parameter update of these two mini-batches. This can simultaneously be read with the initial data of mini-batch 2 & 3, and this part of the overhead can overlap with the calculation. Compared with the execution mode of baseline cache, Figure [Time decomposition of Cache operation] compares the time decomposition of cache operation using 8 mini-batches prefetching with a baseline cache without prefetching. The total training time dropped from 201 seconds to 120 seconds, and the proportion of cache queries shown in the figure also dropped significantly. To sum up, Cache pipeline prefetching brings two benefits. The most obvious benefit of prefetching is reducing cache operation’s Step 1 and Step 2 overhead, so that this two-step operation accounts for less than 5% of the total training process. As shown in Fig. [Time Decomposition of Cache Operations], by pre-fetching 8 mini-batches of data, the overhead of cache queries is significantly reduced compared to the baseline. By concentrating more data and improving the granularity of data transmission, the CPU-GPU transmission bandwidth can be fully utilized. For the example above, the CUDA→CPU bandwidth is increased from 860MB/s to 1477MB/s, and the CPU→CUDA bandwidth is increased from 1257MB/s to 2415MB/s, almost double the performance gain. Our CachedEmbeddingBag is consistent with the basic usage of the PyTorch EmbeddingBag. When building a recommendation model, only a few lines of code can significantly increase the capacity of the embedding table and complete TB super-large recommendation model training at a low cost. The testbed is a GPU cluster with 8x NVIDIA A100 GPU (80GB) and AMD EPYC 7543 32-Core Processor (512GB) CPU. Colossal-AI also uses Meta’s DLRM model implementation, and evaluates on a Cretio 1TB dataset, as well as a synthetic dataset. The PyTorch training with the entire embedding table on the GPU is used as the baseline in the experiments. ### Cretio 1TB embedding table has 177,944,275 records, and its memory allocation takes up 91.10GB, with embedding dim equal to 128. To accommodate EmbeddingBags on one single GPU is impossible, even with top-end NVIDIA A100 with 80GB GPU memory. Hopefully, Colossal-AI will make it possible to accomplish the training task on one GPU, with memory consumption dropping to 5.01 GB (lowering approx. 18 times), and show the possibility of training super large (terabyte-level) recommendation system models on just one GPU. In terms of training speed, the following figure shows the latency of training 100M samples with different batch sizes. Prefetch1 (shown in dark green) is the latency without pre-fetching, and Prefetch8 (shown in blue) is the latency with prefetching (prefetch mini-batch=8). This shows that prefetch flow optimization plays an important role in overall performance improvement. Each bar colored with darker colors in the figure is part of the Cache overhead, being controlled within 15% of the total training time after pre-fetching. In our experiment, DLRM is trained with 100M samples on 8 GPUs, using table-wise sharding as EmbeddingBags in a parallel manner (global batch size = 8192, prefetch size = 4). The following figure shows the training latency for cases with a different number of GPUs. ColossalAI-cr-0.05 in the figures indicates the cache ratio is 0.05, while Colossal-cr-0.5 is 0.5. The PyTorch and Colossal-AI training times are mostly similar, but PyTorch encounters OOM issues when training on 1 GPU. It can be observed that adding GPUs (increasing to 4 or 8) does not bring significant performance benefits as synchronizing results requires huge communication overhead and table-wise sharding, leading to an unbalanced slice load. In other words, using multiple GPUs to scale embedding table training does not have significant advantages. The graph below shows the maximum memory usage, varying across different numbers of GPUs. When using one GPU, only a software Cache method from Colossal-AI works, with the memory assumption of multiple cards in parallel showing a significant reduction. The synthetic dlrm_datasets from Meta Research mimic the training access behavior of embedding tables. As a result, it is usually used as a reference for testing hardware and software designs that relate to recommendation systems. Subsequently, 500 million rows of these embedding table items are selected as sub-datasets, and two EmbeddingBags of 256GB and 128GB are constructed for testing. With GPU memory limitations, PyTorch has poor performance when training on one NVIDIA A100 GPU. In contrast, Colossal-AI’s software cache significantly eases GPU memory requirements, is capable of training embedding tables as large as 256GB, and also shows the potential to scale to terabytes. The acceleration is also demonstrated by running prefetching, where total training time decreases by 60% (#prefetches = 32) and GPU memory demand does not increase. Colossal-AI is a user-friendly deep learning system that allows companies to maximize AI deployment efficiency while drastically reducing costs. Since becoming open source to the public, Colossal-AI has reached №1 in trending projects on GitHub and Papers With Code multiple times, amidst other projects that have as many as 10K stars. Colossal-AI values open source community construction, providing English and Chinese tutorials, while supporting the latest cutting-edge applications such as PaLM and AlphaFold. Ultimately, Colossal-AI is constantly increasing the availability of AI solutions across a variety of fields, including medicine, autonomous vehicles, cloud computing, retail, chip production, etc. PortalProject address: https://github.com/hpcaitech/ColossalAI Reference Embedding Training With 1% GPU Memory and 100 Times Less Budget, an Open Source Solution for Super-Large Recommendation Model Training on a Single GPU | by Yang You | Oct, 2022 | Medium https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/ We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['deep', 'recommendation', 'model', 'dlrm', 'become', 'critical', 'deep', 'learning', 'application', 'company', 'dlrm', 'use', 'improve', 'user', 'experience', 'video', 'recommendation', 'shopping', 'search', 'company', 'advertisement', 'dlrm', 'several', 'limitation', 'include', 'difficulty', 'manage', 'much', 'user', 'datum', 'frequent', 'model', 'update', 'high', 'training', 'cost', 'dlrm', 'first', 'search', 'embed', 'bag', 'embeddingbag', 'go', 'dense', 'dnn', 'embed', 'table', 'usually', 'hold', 'memory', 'dlrm', 'computation', 'requirement', 'help', 'onchip', 'highspeed', 'memory', 'high', 'bandwidth', 'memory', 'increase', 'computing', 'power', 'become', 'mainstream', 'hardware', 'dlrm', 'training', 'however', 'increase', 'research', 'depth', 'recommendation', 'system', 'embed', 'table', 'also', 'grow', 'size', 'limited', 'memory', 'able', 'keep', 'question', 'remain', 'use', 'efficiently', 'train', 'superlarge', 'dlrm', 'model', 'limitation', 'memory', 'colossalai', 'successfully', 'use', 'heterogeneous', 'training', 'strategy', 'increase', 'number', 'nlp', 'model', 'training', 'parameter', 'capacity', 'hundred', 'time', 'hardware', 'recently', 'use', 'software', 'cache', 'method', 'dynamically', 'store', 'embed', 'table', 'cpu', 'memory', 'extend', 'parameter', 'recommendation', 'system', 'relation', 'software', 'design', 'also', 'incorporate', 'pipeline', 'prefetching', 'reduce', 'software', 'cache', 'datum', 'movement', 'overhead', 'observe', 'future', 'training', 'datum', 'time', 'train', 'entire', 'dlrm', 'model', 'gpu', 'synchronize', 'update', 'manner', 'scale', 'multiple', 'gpus', 'widely', 'use', 'hybrid', 'parallel', 'training', 'method', 'experiment', 'show', 'colossalai', 'need', 'keep', 'embed', 'parameter', 'gpu', 'still', 'able', 'maintain', 'excellent', 'endtoend', 'training', 'speed', 'compare', 'pytorch', 'solution', 'memory', 'requirement', 'reduce', 'immense', 'magnitude', 'single', 'gpu', 'able', 'train', 'terabytelevel', 'recommendation', 'model', 'result', 'cost', 'advantage', 'significant', 'example', 'gb', 'memory', 'use', 'train', 'dlrm', 'occupy', 'gb', 'embed', 'bag', 'training', 'hardware', 'cost', 'reduce', 'nvidia', 'total', 'usd', 'entrylevel', 'graphic', 'card', 'rtx', 'cost', 'usd', 'open', 'source', 'purpose', 'embed', 'table', 'map', 'categorical', 'variable', 'floatingpoint', 'variable', 'follow', 'figure', 'show', 'training', 'process', 'embed', 'table', 'dlrm', 'first', 'identify', 'correspond', 'record', 'feature', 'embed', 'table', 'output', 'feature', 'vector', 'reduction', 'operation', 'mean', 'sum', 'operation', 'input', 'subsequent', 'dense', 'neural', 'network', 'embed', 'table', 'dlrm', 'training', 'process', 'comprise', 'mainly', 'irregular', 'memory', 'access', 'operation', 'severely', 'limit', 'hardware', 'memory', 'access', 'bandwidth', 'real', 'application', 'embed', 'table', 'dlrm', 'reach', 'hundred', 'even', 'level', 'far', 'exceed', 'single', 'capacity', 'ten', 'many', 'way', 'increase', 'size', 'embed', 'table', 'take', 'memory', 'hierarchy', 'diagram', 'cluster', 'show', 'figure', 'example', 'analyze', 'advantage', 'disadvantage', 'several', 'common', 'solution', 'model', 'parallelism', 'method', 'embed', 'table', 'sharde', 'distribute', 'memory', 'multiple', 'gpus', 'intermediate', 'result', 'synchronize', 'interconnection', 'network', 'disadvantage', 'method', 'workload', 'embed', 'table', 'equal', 'difficult', 'scale', 'furthermore', 'initial', 'investment', 'add', 'high', 'compute', 'power', 'fully', 'utilize', 'dlrm', 'mainly', 'utilize', 'bandwidth', 'gpus', 'compute', 'unit', 'utilize', 'well', 'hybrid', 'training', 'method', 'start', 'split', 'embed', 'table', 'part', 'train', 'gpu', 'train', 'cpu', 'use', 'longtail', 'input', 'data', 'distribution', 'minimize', 'cpu', 'computing', 'maximize', 'computing', 'however', 'batch', 'size', 'increase', 'become', 'difficult', 'ensure', 'datum', 'minibatch', 'hit', 'cpu', 'additionally', 'ddr', 'bandwidth', 'differ', 'magnitude', 'even', 'input', 'datum', 'train', 'cpu', 'entire', 'system', 'slow', 'least', 'time', 'cpu', 'also', 'need', 'transmit', 'intermediate', 'result', 'require', 'lot', 'overhead', 'communication', 'far', 'slow', 'training', 'speed', 'consequently', 'researcher', 'design', 'method', 'asynchronous', 'update', 'combat', 'issue', 'asynchronous', 'method', 'cause', 'uncertainty', 'training', 'accuracy', 'ideal', 'engineer', 'software', 'cache', 'training', 'perform', 'gpu', 'method', 'embed', 'table', 'keep', 'heterogeneous', 'memory', 'space', 'compose', 'cpu', 'time', 'software', 'cache', 'use', 'use', 'part', 'exchange', 'gpu', 'way', 'storage', 'resource', 'expand', 'inexpensively', 'meet', 'increase', 'demand', 'embed', 'table', 'compare', 'use', 'cpu', 'calculate', 'entire', 'training', 'process', 'complete', 'gpu', 'make', 'full', 'use', 'bandwidth', 'advantage', 'contrary', 'cache', 'query', 'datum', 'movement', 'method', 'bring', 'additional', 'performance', 'loss', 'currently', 'excellent', 'software', 'cache', 'solution', 'embed', 'table', 'often', 'implement', 'use', 'customize', 'embeddingbag', 'kernel', 'fbgemm', 'help', 'thirdparty', 'deep', 'learning', 'framework', 'native', 'pytorch', 'colossalai', 'implement', 'unique', 'set', 'software', 'cache', 'embeddingbag', 'far', 'optimize', 'dlrm', 'training', 'process', 'propose', 'prefetch', 'pipeline', 'far', 'reduce', 'cache', 'overhead', 'colossalai', 'implement', 'class', 'cachedembedding', 'work', 'subclass', 'nnmodule', 'pytorch', 'replace', 'native', 'pytorch', 'embeddingbag', 'consist', 'software', 'manage', 'cpu', 'memory', 'maintain', 'embeddingbag', 'parameter', 'cpu', 'weight', 'small', 'part', 'embeddingbag', 'call', 'cuda', 'cached', 'weight', 'store', 'memory', 'use', 'future', 'training', 'dlrm', 'training', 'record', 'embed', 'table', 'first', 'identify', 'current', 'minibatch', 'record', 'gpu', 'transmit', 'cpu', 'weight', 'cuda', 'cache', 'weight', 'enough', 'space', 'gpu', 'lfu', 'use', 'discard', 'least', 'frequently', 'use', 'embed', 'record', 'order', 'query', 'cache', 'efficiently', 'auxiliary', 'datum', 'structure', 'need', 'cachedidxmap', 'array', 'map', 'index', 'record', 'cpu', 'weight', 'index', 'cuda', 'cache', 'weight', 'well', 'access', 'frequency', 'ratio', 'cuda', 'cache', 'weight', 'size', 'cpu', 'weight', 'size', 'name', 'cacheratio', 'default', '10the', 'cache', 'operate', 'forward', 'iteration', 'adjust', 'datum', 'cuda', 'weight', 'step', 'step', 'cpu', 'record', 'query', 'record', 'index', 'cpu', 'weight', 'need', 'cache', 'require', 'intersect', 'cachedidxmap', 'input', 'current', 'minibatch', 'step', 'query', 'record', 'identify', 'cuda', 'cached', 'weight', 'evict', 'accord', 'frequency', 'require', 'perform', 'topk', 'operation', 'different', 'set', 'cacheidxmap', 'input', 'current', 'minibatch', 'step', 'datum', 'transmission', 'free', 'enough', 'space', 'cuda', 'cache', 'weight', 'cpu', 'weight', 'lead', 'query', 'record', 'transfer', 'cpu', 'move', 'tobe', 'query', 'cpu', 'record', 'cpu', 'weight', 'cuda', 'cache', 'weight', 'process', 'step', 'step', 'cache', 'memory', 'demand', 'order', 'take', 'advantage', 'bandwidth', 'run', 'gpu', 'implement', 'use', 'dropin', 'api', 'provide', 'pytorch', 'overhead', 'cache', 'operation', 'particularly', 'prominent', 'compare', 'training', 'operation', 'embed', 'table', 'gpu', 'example', 'training', 'task', 'take', 'second', 'overhead', 'cache', 'operation', 'second', 'account', 'nearly', 'overall', 'computing', 'time', 'main', 'overhead', 'cache', 'mainly', 'cause', 'step', 'step', 'cache', 'operation', 'base', 'figure', 'show', 'total', 'time', 'decomposition', 'cache', 'operation', 'red', 'orange', 'stage', 'step', 'account', 'total', 'cache', 'overhead', 'problem', 'arise', 'traditional', 'cache', 'strategy', 'somewhat', 'shortsighted', 'cache', 'adjust', 'accord', 'input', 'current', 'minibatch', 'time', 'waste', 'query', 'operation', 'order', 'reduce', 'overhead', 'time', 'cache', 'design', 'farsighte', 'cache', 'mechanism', 'instead', 'perform', 'cache', 'operation', 'first', 'minibatch', 'colossalai', 'fetch', 'several', 'minibatche', 'use', 'later', 'perform', 'cache', 'query', 'operation', 'together', 'show', 'figure', 'colossalai', 'use', 'prefetche', 'merge', 'multiple', 'minibatche', 'datum', 'conduct', 'cache', 'operation', 'merge', 'also', 'use', 'pipeline', 'method', 'overlap', 'overhead', 'datum', 'loading', 'model', 'training', 'show', 'follow', 'figure', 'number', 'minibatche', 'prefetche', 'example', 'start', 'train', 'load', 'minibatch', '’s', 'datum', 'disk', 'memory', 'conduct', 'cache', 'operation', 'perform', 'forward', 'back', 'propagation', 'parameter', 'update', 'minibatche', 'simultaneously', 'read', 'initial', 'datum', 'minibatch', 'part', 'overhead', 'overlap', 'calculation', 'compare', 'execution', 'mode', 'baseline', 'cache', 'figure', 'time', 'decomposition', 'cache', 'operation', 'compare', 'time', 'decomposition', 'operation', 'use', 'minibatche', 'prefetche', 'baseline', 'cache', 'prefetche', 'total', 'training', 'time', 'drop', 'second', 'second', 'proportion', 'cache', 'query', 'show', 'figure', 'also', 'drop', 'significantly', 'sum', 'cache', 'pipeline', 'prefetching', 'bring', 'benefit', 'obvious', 'benefit', 'prefetching', 'reduce', 'step', 'step', 'overhead', 'twostep', 'operation', 'account', 'less', 'total', 'training', 'process', 'show', 'fig', 'time', 'decomposition', 'cache', 'operation', 'prefetche', 'minibatche', 'datum', 'overhead', 'cache', 'query', 'significantly', 'reduce', 'compare', 'baseline', 'concentrate', 'datum', 'improve', 'granularity', 'datum', 'transmission', 'transmission', 'bandwidth', 'fully', 'utilize', 'example', 'bandwidth', 'increase', '860mb', 'cpu→cuda', 'bandwidth', 'increase', '1257mb', '2415mb', 'almost', 'performance', 'gain', 'cachedembeddingbag', 'consistent', 'basic', 'usage', 'pytorch', 'embeddingbag', 'build', 'recommendation', 'model', 'line', 'code', 'significantly', 'increase', 'capacity', 'embed', 'table', 'complete', 'superlarge', 'recommendation', 'model', 'training', 'low', 'cost', 'testbed', 'cluster', 'epyc', 'processor', 'cpu', 'also', 'use', 'model', 'implementation', 'evaluate', 'cretio', 'tb', 'dataset', 'well', 'synthetic', 'dataset', 'pytorch', 'training', 'entire', 'embed', 'table', 'gpu', 'use', 'baseline', 'experiment', 'cretio', 'tb', 'embed', 'table', 'record', 'memory', 'allocation', 'take', 'gb', 'embed', 'dim', 'equal', 'accommodate', 'embeddingbag', 'single', 'gpu', 'impossible', 'even', 'topend', 'memory', 'hopefully', 'make', 'possible', 'accomplish', 'training', 'task', 'gpu', 'memory', 'consumption', 'drop', 'lower', 'approx', 'time', 'show', 'possibility', 'train', 'super', 'large', 'terabytelevel', 'recommendation', 'system', 'model', 'gpu', 'term', 'training', 'speed', 'follow', 'figure', 'show', 'latency', 'training', 'sample', 'different', 'batch', 'size', 'prefetch1', 'show', 'dark', 'green', 'latency', 'prefetching', 'prefetch8', 'show', 'blue', 'latency', 'prefetche', 'prefetch', 'minibatch8', 'show', 'prefetch', 'flow', 'optimization', 'play', 'important', 'role', 'overall', 'performance', 'improvement', 'bar', 'color', 'dark', 'color', 'figure', 'part', 'cache', 'overhead', 'control', 'total', 'training', 'time', 'prefetche', 'experiment', 'dlrm', 'train', 'sample', 'gpus', 'use', 'tablewise', 'sharding', 'embeddingbag', 'parallel', 'manner', 'global', 'batch', 'size', 'prefetch', 'size', 'follow', 'figure', 'show', 'training', 'latency', 'case', 'different', 'number', 'figure', 'indicate', 'cache', 'ratio', 'pytorch', 'colossalai', 'training', 'time', 'mostly', 'similar', 'pytorch', 'encounter', 'oom', 'issue', 'train', 'gpu', 'observe', 'add', 'gpus', 'increase', 'bring', 'significant', 'performance', 'benefit', 'synchronizing', 'result', 'require', 'huge', 'communication', 'overhead', 'tablewise', 'sharding', 'lead', 'unbalanced', 'slice', 'load', 'word', 'use', 'multiple', 'gpus', 'scale', 'embed', 'table', 'training', 'significant', 'advantage', 'graph', 'show', 'maximum', 'memory', 'usage', 'vary', 'different', 'number', 'gpus', 'use', 'gpu', 'software', 'cache', 'method', 'work', 'memory', 'assumption', 'multiple', 'card', 'parallel', 'show', 'significant', 'reduction', 'synthetic', 'dlrmdataset', 'meta', 'research', 'mimic', 'training', 'access', 'behavior', 'embed', 'table', 'result', 'usually', 'use', 'reference', 'test', 'hardware', 'software', 'design', 'relate', 'recommendation', 'system', 'subsequently', 'row', 'embed', 'table', 'item', 'select', 'subdataset', 'embeddingbag', 'gb', 'construct', 'testing', 'memory', 'limitation', 'pytorch', 'poor', 'performance', 'training', 'contrast', 'software', 'cache', 'significantly', 'ease', 'memory', 'requirement', 'capable', 'train', 'embed', 'table', 'large', 'also', 'show', 'potential', 'scale', 'terabyte', 'acceleration', 'also', 'demonstrate', 'run', 'prefetche', 'total', 'training', 'time', 'decrease', 'prefetche', 'memory', 'demand', 'increase', 'colossalai', 'userfriendly', 'deep', 'learning', 'system', 'allow', 'company', 'maximize', 'deployment', 'efficiency', 'drastically', 'reduce', 'cost', 'become', 'open', 'source', 'public', 'colossalai', 'reach', 'trend', 'project', 'github', 'paper', 'code', 'multiple', 'time', 'project', 'many', 'star', 'colossalai', 'value', 'open', 'source', 'community', 'construction', 'provide', 'chinese', 'tutorial', 'support', 'late', 'cuttingedge', 'application', 'palm', 'alphafold', 'ultimately', 'constantly', 'increase', 'availability', 'solution', 'variety', 'field', 'include', 'medicine', 'autonomous', 'vehicle', 'cloud', 'compute', 'retail', 'chip', 'production', 'portalproject', 'address', 'reference', 'embed', 'training', 'memory', 'time', 'less', 'budget', 'open', 'source', 'solution', 'superlarge', 'recommendation', 'model', 'training', 'single', 'medium', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Meet Magneto: Microsoft’s Foundation Transformer for General-Purpose Modelling Across Tasks and Modalities,https://syncedreview.com/2022/10/18/meet-magneto-microsofts-foundation-transformer-for-general-purpose-modelling-across-tasks-and-modalities/,2022-10-18,"
In the new paper Foundation Transformers, a Microsoft team proposes a method for true general-purpose modelling. Their Foundation Transformer is a single unified transformer that provides guaranteed training stability and can handle diverse tasks and modalities without performance degradation.
","The machine learning community has seen a trend in recent years, with researchers working to converge their model architectures across language, vision, speech, and multimodal classes. While transformer architectures have become the de facto standard for building such highly desirable general-purpose foundation models, the optimal transformer variants still differ for different input modalities. In the new paper Foundation Transformers, a Microsoft team proposes a method for true general-purpose modelling. Their Foundation Transformer is a single unified transformer that provides guaranteed training stability and is capable of handling diverse tasks and modalities without performance degradation. The team first identifies the properties a foundation model should possess for true general-purpose modelling: 1) The desired modelling should be able to serve as a go-to architecture for various tasks and modalities, so that we can use the same backbone without trial and error, and 2) The architecture should provide guaranteed training stability. The proposed Magneto is a Foundation Transformer implementation designed to achieve the abovementioned goals. Magento uses Sub-LayerNorm (Sub-LN), which adds another LayerNorm inside each sublayer. The team also introduces a novel initialization method theoretically proven to guarantee training stability, enabling the model to be scaled relatively easily. In their empirical studies, the team compared Magneto with popular transformer variants such as BERT, GPT, and BEiT-3 on a wide range of tasks and modalities, including natural language processing, speech recognition, vision tasks, etc. Magneto significantly surpassed its baseline counterparts in the experiments. Moreover, it was shown to be more stable in terms of optimization, indicating its potential for effectively scaling up all manner of transformer models.The paper Foundation Transformers is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The machine learning community has seen a trend in recent years, with researchers working to converge their model architectures across language, vision, speech, and multimodal classes. While transformer architectures have become the de facto standard for building such highly desirable general-purpose foundation models, the optimal transformer variants still differ for different input modalities. In the new paper Foundation Transformers, a Microsoft team proposes a method for true general-purpose modelling. Their Foundation Transformer is a single unified transformer that provides guaranteed training stability and is capable of handling diverse tasks and modalities without performance degradation. The team first identifies the properties a foundation model should possess for true general-purpose modelling: 1) The desired modelling should be able to serve as a go-to architecture for various tasks and modalities, so that we can use the same backbone without trial and error, and 2) The architecture should provide guaranteed training stability. The proposed Magneto is a Foundation Transformer implementation designed to achieve the abovementioned goals. Magento uses Sub-LayerNorm (Sub-LN), which adds another LayerNorm inside each sublayer. The team also introduces a novel initialization method theoretically proven to guarantee training stability, enabling the model to be scaled relatively easily. In their empirical studies, the team compared Magneto with popular transformer variants such as BERT, GPT, and BEiT-3 on a wide range of tasks and modalities, including natural language processing, speech recognition, vision tasks, etc. Magneto significantly surpassed its baseline counterparts in the experiments. Moreover, it was shown to be more stable in terms of optimization, indicating its potential for effectively scaling up all manner of transformer models.The paper Foundation Transformers is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['machine', 'learning', 'community', 'see', 'trend', 'recent', 'year', 'researcher', 'work', 'converge', 'model', 'architecture', 'language', 'vision', 'speech', 'multimodal', 'class', 'transformer', 'architecture', 'become', 'standard', 'build', 'highly', 'desirable', 'generalpurpose', 'foundation', 'model', 'optimal', 'transformer', 'variant', 'still', 'differ', 'different', 'input', 'modality', 'new', 'paper', 'foundation', 'transformer', 'team', 'propose', 'method', 'true', 'generalpurpose', 'model', 'foundation', 'transformer', 'single', 'unified', 'transformer', 'provide', 'guarantee', 'training', 'stability', 'capable', 'handle', 'diverse', 'task', 'modality', 'performance', 'degradation', 'team', 'first', 'identify', 'property', 'foundation', 'model', 'possess', 'true', 'generalpurpose', 'modelling', 'desire', 'modelling', 'able', 'serve', 'goto', 'architecture', 'various', 'task', 'modality', 'use', 'backbone', 'trial', 'error', 'architecture', 'provide', 'guarantee', 'training', 'stability', 'propose', 'magneto', 'foundation', 'transformer', 'implementation', 'design', 'achieve', 'abovementione', 'goal', 'magento', 'use', 'sublayernorm', 'subln', 'add', 'layernorm', 'sublayer', 'team', 'also', 'introduce', 'novel', 'initialization', 'method', 'theoretically', 'prove', 'guarantee', 'training', 'stability', 'enable', 'model', 'scale', 'relatively', 'easily', 'empirical', 'study', 'team', 'compare', 'magneto', 'popular', 'transformer', 'variant', 'beit3', 'wide', 'range', 'task', 'modality', 'include', 'natural', 'language', 'processing', 'speech', 'recognition', 'vision', 'task', 'magneto', 'significantly', 'surpass', 'baseline', 'counterpart', 'experiment', 'moreover', 'show', 'stable', 'term', 'optimization', 'indicate', 'potential', 'effectively', 'scale', 'manner', 'transformer', 'paper', 'foundation', 'transformer', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Stanford U & Google Brain’s Classifier-Free Guidance Model Diffusion Technique Reduces Sampling Steps by 256x,https://syncedreview.com/2022/10/17/stanford-u-google-brains-classifier-free-guidance-model-diffusion-technique-reduces-sampling-steps-by-256x/,2022-10-17,"
In the new paper On Distillation of Guided Diffusion Models, researchers from Google Brain and Stanford University propose a novel approach for distilling classifier-free guided diffusion models with high sampling efficiency. The resulting models achieve performance comparable to the original model but with sampling steps reduced by up to 256 times. 
","Denoising diffusion probabilistic models (DDPMs) with classifier-free guidance such as DALL·E 2, GLIDE, and Imagen have achieved state-of-the-art results in high-resolution image generation. The downside to such models is that their inference process requires evaluating both a class-conditional model and an unconditional model hundreds of times, rendering them prohibitively compute-expensive for many real-world applications. In the new paper On Distillation of Guided Diffusion Models, researchers from Google Brain and Stanford University propose a novel approach for distilling classifier-free guided diffusion models with high sampling efficiency. The resulting models achieve performance comparable to the original model but with sampling steps reduced by up to 256 times. The researchers’ distillation approach comprises two steps: Given a trained guided teacher model, a single student model first matches the combined output of the teacher’s two diffusion models, and this learned student model is then progressively distilled to a fewer-step model. The resulting single distilled model can handle a wide range of different guidance strengths and enable efficient tradeoffs between sample quality and diversity. The proposed sampling method employs a deterministic sampler and a novel stochastic sampling process. One deterministic sampling step is first applied with two times the original step length, and one stochastic step is then performed backward (i.e., perturb with noise) using the original step length. This approach was inspired by Karras et al.’s paperElucidating the Design Space of Diffusion-Based Generative Models, published earlier this year. In their empirical study, the team applied their method to classifier-free guidance DDPMs and performed image generation experiments on the ImageNet 64×64 and CIFAR-10 datasets. The results show that the proposed approach can achieve “visually decent” samples using as few as one step and obtain FID/IS (Frechet Inception Distance/Inception) scores comparable to that of the original baseline models while being up to 256 times faster to sample from. Overall, this work demonstrates the effectiveness of the proposed approach in addressing the high computational costs that have limited the deployment of denoising diffusion probabilistic models.The paper On Distillation of Guided Diffusion Models is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Denoising diffusion probabilistic models (DDPMs) with classifier-free guidance such as DALL·E 2, GLIDE, and Imagen have achieved state-of-the-art results in high-resolution image generation. The downside to such models is that their inference process requires evaluating both a class-conditional model and an unconditional model hundreds of times, rendering them prohibitively compute-expensive for many real-world applications. In the new paper On Distillation of Guided Diffusion Models, researchers from Google Brain and Stanford University propose a novel approach for distilling classifier-free guided diffusion models with high sampling efficiency. The resulting models achieve performance comparable to the original model but with sampling steps reduced by up to 256 times. The researchers’ distillation approach comprises two steps: Given a trained guided teacher model, a single student model first matches the combined output of the teacher’s two diffusion models, and this learned student model is then progressively distilled to a fewer-step model. The resulting single distilled model can handle a wide range of different guidance strengths and enable efficient tradeoffs between sample quality and diversity. The proposed sampling method employs a deterministic sampler and a novel stochastic sampling process. One deterministic sampling step is first applied with two times the original step length, and one stochastic step is then performed backward (i.e., perturb with noise) using the original step length. This approach was inspired by Karras et al.’s paperElucidating the Design Space of Diffusion-Based Generative Models, published earlier this year. In their empirical study, the team applied their method to classifier-free guidance DDPMs and performed image generation experiments on the ImageNet 64×64 and CIFAR-10 datasets. The results show that the proposed approach can achieve “visually decent” samples using as few as one step and obtain FID/IS (Frechet Inception Distance/Inception) scores comparable to that of the original baseline models while being up to 256 times faster to sample from. Overall, this work demonstrates the effectiveness of the proposed approach in addressing the high computational costs that have limited the deployment of denoising diffusion probabilistic models.The paper On Distillation of Guided Diffusion Models is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['denoise', 'diffusion', 'probabilistic', 'model', 'ddpm', 'classifierfree', 'guidance', 'glide', 'imagen', 'achieve', 'stateoftheart', 'result', 'highresolution', 'image', 'generation', 'downside', 'model', 'inference', 'process', 'require', 'evaluate', 'classconditional', 'model', 'unconditional', 'model', 'hundred', 'time', 'render', 'prohibitively', 'computeexpensive', 'many', 'realworld', 'application', 'new', 'paper', 'distillation', 'guide', 'diffusion', 'model', 'researcher', 'propose', 'novel', 'approach', 'distil', 'classifierfree', 'guide', 'diffusion', 'model', 'high', 'sampling', 'efficiency', 'result', 'model', 'achieve', 'performance', 'comparable', 'original', 'model', 'sample', 'step', 'reduce', 'time', 'researcher', 'distillation', 'approach', 'comprise', 'step', 'give', 'train', 'guide', 'teacher', 'model', 'single', 'student', 'model', 'first', 'match', 'combine', 'output', 'teacher', 'diffusion', 'model', 'learn', 'student', 'model', 'progressively', 'distil', 'fewerstep', 'model', 'result', 'single', 'distil', 'model', 'handle', 'wide', 'range', 'different', 'guidance', 'strength', 'enable', 'efficient', 'tradeoff', 'sample', 'quality', 'diversity', 'propose', 'sampling', 'method', 'employ', 'deterministic', 'sampler', 'novel', 'stochastic', 'sampling', 'process', 'deterministic', 'sampling', 'step', 'first', 'apply', 'time', 'original', 'step', 'length', 'stochastic', 'step', 'perform', 'backward', 'perturb', 'noise', 'use', 'original', 'step', 'length', 'approach', 'inspire', 'karra', 'paperelucidate', 'design', 'space', 'diffusionbased', 'generative', 'model', 'publish', 'early', 'year', 'empirical', 'study', 'team', 'apply', 'method', 'classifierfree', 'guidance', 'ddpm', 'perform', 'image', 'generation', 'experiment', 'imagenet', 'cifar10', 'dataset', 'result', 'show', 'propose', 'approach', 'achieve', 'visually', 'decent', 'sample', 'use', 'step', 'obtain', 'frechet', 'inception', 'distanceinception', 'score', 'comparable', 'original', 'baseline', 'model', 'time', 'fast', 'sample', 'overall', 'work', 'demonstrate', 'effectiveness', 'propose', 'approach', 'address', 'high', 'computational', 'cost', 'limit', 'deployment', 'denoise', 'diffusion', 'probabilistic', 'modelsthe', 'paper', 'distillation', 'guide', 'diffusion', 'model', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Beyond Bayes-Optimality: DeepMind & Stanford’s Meta-Learning Approach Builds Risk & Ambiguity Sensitive Agents,https://syncedreview.com/2022/10/13/beyond-bayes-optimality-deepmind-stanfords-meta-learning-approach-builds-risk-ambiguity-sensitive-agents/,2022-10-13,"
In the new paper Beyond Bayes-Optimality: Meta-Learning What You Know You Don’t Know, researchers from DeepMind and Stanford University use modified meta-training algorithms to build agents with risk- and ambiguity-sensitivity.
","Reasoning about uncertainty is one of the crucial components informing human intelligence and our very survival — it can encourage us to take risks that have a higher expected return but also discourage us from impulsively doing things that may lead to catastrophic consequences. Contemporary Bayes-optimal AI agents however are generally risk- and ambiguity-neutral, lacking this natural human capacity for advanced reasoning with regard to uncertainty. In the new paper Beyond Bayes-Optimality: Meta-Learning What You Know You Don’t Know, a research team from DeepMind and Stanford University employs modified meta-training algorithms to build agents with risk- and ambiguity-sensitivity, and empirically demonstrates the validity of such agents. The team first clarifies the distinction between Bayesian-optimal, risk-sensitive, and ambiguity-sensitive agents. Simple put, both risk and ambiguity belong to uncertainty: risk applies to scenarios where the output of an event is uncertain but can be roughly calculated using probability functions; while in ambiguity, the probability is unknown or cannot be reliably determined. Bayesian-optimal agents assign certainty-equivalents that are equal to the expected payoff, then choose the optimal action. Such agents are both risk-neutral (insensitive to the distribution over returns except for the target value) and ambiguity-neutral (acting as if the uncertainty were known). In their bid to create risk-sensitive agents, the team modifies the meta-training protocol by tweaking the distribution of observations to make these observations sensitive to the valuations of the agent. They leverage an ensemble of agents and a meta-policy to build a mechanism able to detect and use novelty. As such, the resulting agent can evolve to become uncertainty-seeking and risk-averting based on its experiences. In their empirical study, the team applied their proposed meta-training algorithms to agents in various decision-making experiments, with the results confirming that the agents can learn both risk- and ambiguity-sensitivity. Overall, this work demonstrates the possibility of building risk- and ambiguity-sensitive agents via the modification of meta-training algorithms. The team hopes their contributions can serve as the starting point in the development of data-dependent methods for the study and application of uncertainty-sensitivity in humans and machines. The paper Beyond Bayes-Optimality: Meta-Learning What You Know You Don’t Know is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Reasoning about uncertainty is one of the crucial components informing human intelligence and our very survival — it can encourage us to take risks that have a higher expected return but also discourage us from impulsively doing things that may lead to catastrophic consequences. Contemporary Bayes-optimal AI agents however are generally risk- and ambiguity-neutral, lacking this natural human capacity for advanced reasoning with regard to uncertainty. In the new paper Beyond Bayes-Optimality: Meta-Learning What You Know You Don’t Know, a research team from DeepMind and Stanford University employs modified meta-training algorithms to build agents with risk- and ambiguity-sensitivity, and empirically demonstrates the validity of such agents. The team first clarifies the distinction between Bayesian-optimal, risk-sensitive, and ambiguity-sensitive agents. Simple put, both risk and ambiguity belong to uncertainty: risk applies to scenarios where the output of an event is uncertain but can be roughly calculated using probability functions; while in ambiguity, the probability is unknown or cannot be reliably determined. Bayesian-optimal agents assign certainty-equivalents that are equal to the expected payoff, then choose the optimal action. Such agents are both risk-neutral (insensitive to the distribution over returns except for the target value) and ambiguity-neutral (acting as if the uncertainty were known). In their bid to create risk-sensitive agents, the team modifies the meta-training protocol by tweaking the distribution of observations to make these observations sensitive to the valuations of the agent. They leverage an ensemble of agents and a meta-policy to build a mechanism able to detect and use novelty. As such, the resulting agent can evolve to become uncertainty-seeking and risk-averting based on its experiences. In their empirical study, the team applied their proposed meta-training algorithms to agents in various decision-making experiments, with the results confirming that the agents can learn both risk- and ambiguity-sensitivity. Overall, this work demonstrates the possibility of building risk- and ambiguity-sensitive agents via the modification of meta-training algorithms. The team hopes their contributions can serve as the starting point in the development of data-dependent methods for the study and application of uncertainty-sensitivity in humans and machines. The paper Beyond Bayes-Optimality: Meta-Learning What You Know You Don’t Know is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['reasoning', 'uncertainty', 'crucial', 'component', 'inform', 'human', 'intelligence', 'survival', 'encourage', 'take', 'risk', 'high', 'expect', 'return', 'also', 'discourage', 'impulsively', 'thing', 'lead', 'catastrophic', 'consequence', 'contemporary', 'bayesoptimal', 'agent', 'however', 'generally', 'risk', 'ambiguityneutral', 'lack', 'natural', 'human', 'capacity', 'advanced', 'reasoning', 'regard', 'uncertainty', 'new', 'paper', 'bayesoptimality', 'metalearne', 'know', 'know', 'research', 'team', 'employ', 'modify', 'metatraine', 'algorithm', 'build', 'agent', 'risk', 'ambiguitysensitivity', 'empirically', 'demonstrate', 'validity', 'agent', 'team', 'first', 'clarify', 'distinction', 'bayesianoptimal', 'risksensitive', 'ambiguitysensitive', 'agent', 'simple', 'put', 'risk', 'ambiguity', 'belong', 'uncertainty', 'risk', 'apply', 'scenario', 'output', 'event', 'uncertain', 'roughly', 'calculate', 'use', 'probability', 'function', 'ambiguity', 'probability', 'unknown', 'reliably', 'determine', 'bayesianoptimal', 'agent', 'assign', 'certaintyequivalent', 'equal', 'expect', 'payoff', 'choose', 'optimal', 'action', 'agent', 'riskneutral', 'insensitive', 'distribution', 'return', 'target', 'value', 'ambiguityneutral', 'acting', 'uncertainty', 'know', 'bid', 'create', 'risksensitive', 'agent', 'team', 'modify', 'metatraining', 'protocol', 'tweak', 'distribution', 'observation', 'make', 'observation', 'sensitive', 'valuation', 'agent', 'leverage', 'ensemble', 'agent', 'metapolicy', 'build', 'mechanism', 'able', 'detect', 'use', 'novelty', 'result', 'agent', 'evolve', 'become', 'uncertaintyseeke', 'riskaverting', 'base', 'experience', 'empirical', 'study', 'team', 'apply', 'propose', 'metatraining', 'algorithm', 'agent', 'various', 'decisionmake', 'experiment', 'result', 'confirm', 'agent', 'learn', 'risk', 'ambiguitysensitivity', 'overall', 'work', 'demonstrate', 'possibility', 'build', 'risk', 'ambiguitysensitive', 'agent', 'modification', 'metatraine', 'algorithm', 'team', 'hope', 'contribution', 'serve', 'starting', 'point', 'development', 'datadependent', 'method', 'study', 'application', 'uncertaintysensitivity', 'human', 'machine', 'paper', 'bayesoptimality', 'metalearne', 'know', 'know', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
"‘Ask Me Anything’: Stanford U, Numbers Station & UW Madison’s Novel Prompting Strategy Enables LLMs With 30x Fewer Parameters to Outperform Few-Shot GPT3-175B",https://syncedreview.com/2022/10/12/ask-me-anything-stanford-u-numbers-station-uw-madisons-novel-prompting-strategy-enables-llms-with-30x-fewer-parameters-to-outperform-few-shot-gpt3-175b/,2022-10-12,"
In the new paper Ask Me Anything: A Simple Strategy for Prompting Language Models, a research team from Stanford University, Numbers Station, and the University of Wisconsin-Madison presents Ask Me Anything Prompting (AMA), a simple large language model prompting strategy that enables a 30x smaller language model to outperform few-shot GPT3-175B.
","Large language models (LLMs) have taken a step toward task-agnostic machine learning by leveraging user prompts — instructions written in natural language — to help them target specific tasks without additional training or fine-tuning. Prompts can significantly boost model performance, but designing the perfect prompt, aka “prompt engineering,” remains a time-consuming, hands-on process that often comes down to trial and error. In the new paper Ask Me Anything: A Simple Strategy for Prompting Language Models, a research team from Stanford University, Numbers Station, and the University of Wisconsin-Madison presents Ask Me Anything Prompting (AMA), a simple LLM prompting strategy that aggregates multiple “effective yet imperfect” prompts to enable a 30x smaller language model to outperform few-shot GPT3-175B. The team summarizes their main contributions as follows: The researchers first explore different prompt formats, concluding that open-ended question-answering (QA) prompts (e.g. “Who went to the park?”) outperform prompts that restrict the model to particular tokens (e.g. “John went to the park. Output True or False”). They recursively use the LLM to transform task inputs to the effective open-ended question-answering format noted above, collecting multiple candidate prompts with different accuracies and complex dependencies. Finally, they apply a weak supervision (WS) technique to aggregate the outputs and produce final predictions that demonstrably improve the prompting reliability and performance of off-the-shelf LLMs without further training. In their empirical study, the team evaluated AMA’s impact on the out-of-the-box few-shot performance of four open-source LLMs (EleutherAI, OPT, BLOOM, and T0) on seven tasks. In the experiments, AMA achieved an average improvement of 10.2 percent over the few-shot baselines; and also enabled a 30x smaller LLM to outperform few-shot GPT3-175B on 15 of 20 popular benchmarks. Overall, this work validates the effectiveness of the proposed AMA prompting strategy. The team believes AMA could also benefit LLM applications that involve private data or require operating over large amounts of data. The AMA code is available on the project GitHub. The paper Ask Me Anything: A Simple Strategy for Prompting Language Models is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Large language models (LLMs) have taken a step toward task-agnostic machine learning by leveraging user prompts — instructions written in natural language — to help them target specific tasks without additional training or fine-tuning. Prompts can significantly boost model performance, but designing the perfect prompt, aka “prompt engineering,” remains a time-consuming, hands-on process that often comes down to trial and error. In the new paper Ask Me Anything: A Simple Strategy for Prompting Language Models, a research team from Stanford University, Numbers Station, and the University of Wisconsin-Madison presents Ask Me Anything Prompting (AMA), a simple LLM prompting strategy that aggregates multiple “effective yet imperfect” prompts to enable a 30x smaller language model to outperform few-shot GPT3-175B. The team summarizes their main contributions as follows: The researchers first explore different prompt formats, concluding that open-ended question-answering (QA) prompts (e.g. “Who went to the park?”) outperform prompts that restrict the model to particular tokens (e.g. “John went to the park. Output True or False”). They recursively use the LLM to transform task inputs to the effective open-ended question-answering format noted above, collecting multiple candidate prompts with different accuracies and complex dependencies. Finally, they apply a weak supervision (WS) technique to aggregate the outputs and produce final predictions that demonstrably improve the prompting reliability and performance of off-the-shelf LLMs without further training. In their empirical study, the team evaluated AMA’s impact on the out-of-the-box few-shot performance of four open-source LLMs (EleutherAI, OPT, BLOOM, and T0) on seven tasks. In the experiments, AMA achieved an average improvement of 10.2 percent over the few-shot baselines; and also enabled a 30x smaller LLM to outperform few-shot GPT3-175B on 15 of 20 popular benchmarks. Overall, this work validates the effectiveness of the proposed AMA prompting strategy. The team believes AMA could also benefit LLM applications that involve private data or require operating over large amounts of data. The AMA code is available on the project GitHub. The paper Ask Me Anything: A Simple Strategy for Prompting Language Models is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['large', 'language', 'model', 'llm', 'take', 'step', 'taskagnostic', 'machine', 'learning', 'leverage', 'user', 'prompt', 'instruction', 'write', 'natural', 'language', 'help', 'target', 'specific', 'task', 'additional', 'training', 'finetune', 'prompt', 'significantly', 'boost', 'model', 'performance', 'design', 'perfect', 'prompt', 'aka', 'prompt', 'engineering', 'remain', 'timeconsuming', 'handson', 'process', 'often', 'come', 'trial', 'error', 'new', 'paper', 'ask', 'simple', 'strategy', 'prompt', 'language', 'model', 'research', 'team', 'number', 'station', 'wisconsinmadison', 'present', 'ask', 'prompt', 'simple', 'llm', 'prompt', 'strategy', 'aggregate', 'multiple', 'effective', 'imperfect', 'prompt', 'enable', '30x', 'small', 'language', 'model', 'outperform', 'fewshot', 'gpt3175b', 'team', 'summarize', 'main', 'contribution', 'follow', 'researcher', 'first', 'explore', 'different', 'prompt', 'format', 'conclude', 'openende', 'questionanswere', 'qa', 'prompt', 'go', 'park', 'outperform', 'prompt', 'restrict', 'model', 'particular', 'go', 'park', 'output', 'true', 'false', 'recursively', 'use', 'llm', 'transform', 'task', 'input', 'effective', 'openended', 'questionanswere', 'format', 'note', 'collect', 'multiple', 'candidate', 'prompt', 'different', 'accuracy', 'complex', 'dependency', 'finally', 'apply', 'weak', 'supervision', 'ws', 'technique', 'aggregate', 'output', 'produce', 'final', 'prediction', 'demonstrably', 'improve', 'prompt', 'reliability', 'performance', 'llm', 'training', 'empirical', 'study', 'team', 'evaluate', 'impact', 'outofthebox', 'fewshot', 'performance', 'opensource', 'llm', 'eleutherai', 'task', 'experiment', 'ama', 'achieve', 'average', 'improvement', 'percent', 'fewshot', 'baseline', 'also', 'enable', '30x', 'small', 'llm', 'outperform', 'fewshot', 'gpt3175b', 'popular', 'benchmark', 'overall', 'work', 'validate', 'effectiveness', 'propose', 'ama', 'prompt', 'strategy', 'team', 'believe', 'also', 'benefit', 'llm', 'application', 'involve', 'private', 'datum', 'require', 'operate', 'large', 'amount', 'datum', 'ama', 'code', 'available', 'project', 'github', 'paper', 'ask', 'simple', 'strategy', 'prompt', 'language', 'model', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Maximizing FLOPS Utilization: DeepMind & NYU Propose Efficiency Evaluations for Visual Pretraining Methods,https://syncedreview.com/2022/10/11/maximizing-flops-utilization-deepmind-nyu-propose-efficiency-evaluations-for-visual-pretraining-methods/,2022-10-11,"
In the new paper Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods, DeepMind and NYU Center for Neural Systems researchers introduce computational efficiency evaluation approaches designed to aid in the selection of optimal methods, datasets and models for pretraining visual tasks on a fixed FLOP budget.
","While self-supervised learning (SSL) has achieved impressive results in recent years thanks to complex data augmentation techniques and lengthy training schedules, these approaches also lead to extremely high computation costs. Given a fixed FLOPS budget, is it possible to identify the best datasets, models, and self-supervised training strategies for obtaining high accuracy on visual tasks? In the new paper Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods, a research team from DeepMind and the NYU Center for Neural Systems introduces evaluation approaches designed to measure the computational efficiency of various visual pretraining strategies across multiple datasets and model sizes and aid in the selection of optimal methods, datasets and models for pretraining visual tasks on a fixed FLOP budget. Previous studies on SSL have mainly focused on improving performance with little regard for the associated computational costs. This work takes the first steps toward identifying computationally optimal pretraining methods, datasets and models. The team analyzes four common self-supervised methods (BYOL, SimCLR, DINO, and MAE) and two supervised methods (CLIP and standard softmax classification). The methods’ per gradient-step FLOP costs are computed and used for comparisons across three axes: pretraining method, model size, and dataset. Downstream task performance is measured by finetuning the pretrained encoders on semantic segmentation tasks on the ADE20K dataset. Based on the evaluations, the team concludes that: 1) Self-supervised methods are generally less FLOP efficient and supervised representations dominate the efficiency Pareto-front; 2) For most methods, the small and large model curves intersect, indicating the point at which it is better to switch to larger model sizes for a given FLOP budget; 3) Dataset quality and curation level significantly affect model accuracy. The team sees their work as a first step towards more rigorously measuring the computational efficiency of contemporary supervised and self-supervised pretraining approaches in terms of pretraining method, dataset and model size. They hope their results will spark future research into visual SSL methods that learn more effectively and scalably on uncurated data. The paper Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","While self-supervised learning (SSL) has achieved impressive results in recent years thanks to complex data augmentation techniques and lengthy training schedules, these approaches also lead to extremely high computation costs. Given a fixed FLOPS budget, is it possible to identify the best datasets, models, and self-supervised training strategies for obtaining high accuracy on visual tasks? In the new paper Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods, a research team from DeepMind and the NYU Center for Neural Systems introduces evaluation approaches designed to measure the computational efficiency of various visual pretraining strategies across multiple datasets and model sizes and aid in the selection of optimal methods, datasets and models for pretraining visual tasks on a fixed FLOP budget. Previous studies on SSL have mainly focused on improving performance with little regard for the associated computational costs. This work takes the first steps toward identifying computationally optimal pretraining methods, datasets and models. The team analyzes four common self-supervised methods (BYOL, SimCLR, DINO, and MAE) and two supervised methods (CLIP and standard softmax classification). The methods’ per gradient-step FLOP costs are computed and used for comparisons across three axes: pretraining method, model size, and dataset. Downstream task performance is measured by finetuning the pretrained encoders on semantic segmentation tasks on the ADE20K dataset. Based on the evaluations, the team concludes that: 1) Self-supervised methods are generally less FLOP efficient and supervised representations dominate the efficiency Pareto-front; 2) For most methods, the small and large model curves intersect, indicating the point at which it is better to switch to larger model sizes for a given FLOP budget; 3) Dataset quality and curation level significantly affect model accuracy. The team sees their work as a first step towards more rigorously measuring the computational efficiency of contemporary supervised and self-supervised pretraining approaches in terms of pretraining method, dataset and model size. They hope their results will spark future research into visual SSL methods that learn more effectively and scalably on uncurated data. The paper Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['selfsupervise', 'learning', 'achieve', 'impressive', 'result', 'recent', 'year', 'thank', 'complex', 'datum', 'augmentation', 'technique', 'lengthy', 'training', 'schedule', 'approach', 'also', 'lead', 'extremely', 'high', 'computation', 'cost', 'give', 'fix', 'flop', 'budget', 'possible', 'identify', 'good', 'dataset', 'model', 'selfsupervise', 'training', 'strategy', 'obtain', 'high', 'accuracy', 'visual', 'task', 'new', 'paper', 'spend', 'flop', 'efficiency', 'evaluation', 'visual', 'pretraine', 'method', 'research', 'team', 'deepmind', 'neural', 'system', 'introduce', 'evaluation', 'approach', 'design', 'measure', 'computational', 'efficiency', 'various', 'visual', 'pretraine', 'strategy', 'multiple', 'dataset', 'model', 'size', 'aid', 'selection', 'optimal', 'method', 'dataset', 'model', 'pretraine', 'visual', 'task', 'fix', 'flop', 'budget', 'previous', 'study', 'mainly', 'focus', 'improve', 'performance', 'little', 'regard', 'associate', 'computational', 'cost', 'work', 'take', 'first', 'step', 'identify', 'computationally', 'optimal', 'pretraine', 'method', 'dataset', 'model', 'team', 'analyze', 'common', 'selfsupervise', 'method', 'byol', 'simclr', 'dino', 'supervised', 'method', 'clip', 'standard', 'softmax', 'classification', 'method', 'gradientstep', 'flop', 'cost', 'compute', 'use', 'comparison', 'axis', 'pretraine', 'method', 'model', 'size', 'dataset', 'downstream', 'task', 'performance', 'measure', 'finetune', 'pretraine', 'encoder', 'semantic', 'segmentation', 'task', 'dataset', 'base', 'evaluation', 'team', 'conclude', 'selfsupervise', 'method', 'generally', 'less', 'flop', 'efficient', 'supervised', 'representation', 'dominate', 'efficiency', 'paretofront', 'method', 'small', 'large', 'model', 'curve', 'intersect', 'indicate', 'point', 'well', 'switch', 'large', 'model', 'size', 'give', 'flop', 'budget', 'dataset', 'quality', 'curation', 'level', 'significantly', 'affect', 'model', 'accuracy', 'team', 'see', 'work', 'first', 'step', 'rigorously', 'measure', 'computational', 'efficiency', 'contemporary', 'supervised', 'selfsupervise', 'pretraine', 'approach', 'term', 'pretraine', 'method', 'dataset', 'model', 'size', 'hope', 'result', 'spark', 'future', 'research', 'visual', 'ssl', 'method', 'learn', 'effectively', 'scalably', 'uncurated', 'datum', 'paper', 'spend', 'flop', 'efficiency', 'evaluation', 'visual', 'pretraine', 'method', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
MIT’s DIFFDOCK Boosts the Molecular Docking Top-1 Success Rate from 23% to 38%,https://syncedreview.com/2022/10/06/mits-diffdock-boosts-the-molecular-docking-top-1-success-rate-from-23-to-38/,2022-10-06,"
MIT Researchers propose DIFFDOCK, a diffusion generative model that significantly improves the molecular docking top-1 prediction success rate, from state-of-the-art traditional docking approaches’ 23 percent to 38 percent.
","Market research firm Emersion Insights reports that global funding for AI-powered drug development topped US$4 billion in 2021, a 36 percent year-over-year increase, and is expected to continue its rapid growth. A critical component of computer-aided drug discovery is molecular docking, a task that predicts the binding structure of small molecule ligands to a protein. Although new approaches using deep learning models have increased the speed of such research compared to traditional search-based methods, they have not significantly boosted prediction accuracy. In the new paper DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking, researchers from the Massachusetts Institute of Technology’s Computer Science & Artificial Intelligence Laboratory propose DIFFDOCK, a diffusion generative model (DGM) that significantly improves the molecular docking top-1 prediction success rate, from state-of-the-art traditional docking approaches’ 23 percent to 38 percent. The team summarizes their main contributions as follows: The team defines molecular docking as a generative modelling problem, i.e., given a ligand and target protein structure, the goal is to learn a distribution over ligand poses. They introduce a diffusion process over the degrees of freedom involved in docking, covering: 1) the position of the ligand, 2) its orientation in the pocket, and 3) its torsion angles. DIFFDOCK samples poses during the diffusion process to iteratively transform an uninformed, noisy prior distribution over ligand poses into the output model distribution. The researchers also train a model to estimate the confidence level of the poses sampled from the DGM, to enable it to pick the most likely sample. In their empirical study, the team evaluated DIFFDOCK on molecular complexes from PDBBind benchmark and compared it with state-of-the-art search-based methods such as SMINA and GLIDE and recent deep learning methods EquiBind and TANKBind. DIFFDOCK excelled in the evaluations, surpassing all the baselines and achieving an impressive 38.2 percent top-1 success rate when sampling 40 poses while reaching much faster inference times. This work presents an exciting breakthrough in molecular docking. The team believes DIFFDOCK can help existing real-world pipelines and open new research avenues for downstream task integration and protein-protein and protein-nucleic acid docking. The paper DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Market research firm Emersion Insights reports that global funding for AI-powered drug development topped US$4 billion in 2021, a 36 percent year-over-year increase, and is expected to continue its rapid growth. A critical component of computer-aided drug discovery is molecular docking, a task that predicts the binding structure of small molecule ligands to a protein. Although new approaches using deep learning models have increased the speed of such research compared to traditional search-based methods, they have not significantly boosted prediction accuracy. In the new paper DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking, researchers from the Massachusetts Institute of Technology’s Computer Science & Artificial Intelligence Laboratory propose DIFFDOCK, a diffusion generative model (DGM) that significantly improves the molecular docking top-1 prediction success rate, from state-of-the-art traditional docking approaches’ 23 percent to 38 percent. The team summarizes their main contributions as follows: The team defines molecular docking as a generative modelling problem, i.e., given a ligand and target protein structure, the goal is to learn a distribution over ligand poses. They introduce a diffusion process over the degrees of freedom involved in docking, covering: 1) the position of the ligand, 2) its orientation in the pocket, and 3) its torsion angles. DIFFDOCK samples poses during the diffusion process to iteratively transform an uninformed, noisy prior distribution over ligand poses into the output model distribution. The researchers also train a model to estimate the confidence level of the poses sampled from the DGM, to enable it to pick the most likely sample. In their empirical study, the team evaluated DIFFDOCK on molecular complexes from PDBBind benchmark and compared it with state-of-the-art search-based methods such as SMINA and GLIDE and recent deep learning methods EquiBind and TANKBind. DIFFDOCK excelled in the evaluations, surpassing all the baselines and achieving an impressive 38.2 percent top-1 success rate when sampling 40 poses while reaching much faster inference times. This work presents an exciting breakthrough in molecular docking. The team believes DIFFDOCK can help existing real-world pipelines and open new research avenues for downstream task integration and protein-protein and protein-nucleic acid docking. The paper DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['market', 'research', 'firm', 'emersion', 'insight', 'report', 'global', 'funding', 'aipowere', 'drug', 'development', 'top', 'percent', 'yearoveryear', 'increase', 'expect', 'continue', 'rapid', 'growth', 'critical', 'component', 'computeraide', 'drug', 'discovery', 'molecular', 'dock', 'task', 'predict', 'binding', 'structure', 'small', 'molecule', 'ligand', 'protein', 'new', 'approach', 'use', 'deep', 'learning', 'model', 'increase', 'speed', 'research', 'compare', 'traditional', 'searchbased', 'method', 'significantly', 'boost', 'prediction', 'accuracy', 'new', 'paper', 'diffdock', 'diffusion', 'step', 'twist', 'turn', 'molecular', 'dock', 'researcher', 'computer', 'science', 'artificial', 'intelligence', 'laboratory', 'propose', 'diffdock', 'diffusion', 'generative', 'model', 'dgm', 'significantly', 'improve', 'molecular', 'docking', 'top1', 'prediction', 'success', 'rate', 'stateoftheart', 'traditional', 'docking', 'approach', 'percent', 'percent', 'team', 'summarize', 'main', 'contribution', 'follow', 'team', 'define', 'molecular', 'docking', 'generative', 'modelling', 'problem', 'ie', 'give', 'ligand', 'target', 'protein', 'structure', 'goal', 'learn', 'distribution', 'ligand', 'pose', 'introduce', 'diffusion', 'process', 'degree', 'freedom', 'involve', 'docking', 'cover', 'position', 'ligand', 'orientation', 'pocket', 'torsion', 'angle', 'diffdock', 'sample', 'pose', 'diffusion', 'process', 'iteratively', 'transform', 'uninformed', 'noisy', 'prior', 'distribution', 'ligand', 'pose', 'output', 'model', 'distribution', 'researcher', 'also', 'train', 'model', 'estimate', 'confidence', 'level', 'pose', 'sample', 'dgm', 'enable', 'pick', 'likely', 'sample', 'empirical', 'study', 'team', 'evaluate', 'diffdock', 'molecular', 'complex', 'pdbbind', 'benchmark', 'compare', 'stateoftheart', 'searchbased', 'method', 'smina', 'glide', 'recent', 'deep', 'learning', 'method', 'equibind', 'tankbind', 'diffdock', 'excel', 'evaluation', 'surpass', 'baseline', 'achieve', 'impressive', 'percent', 'top1', 'success', 'rate', 'sample', 'pose', 'reach', 'much', 'fast', 'inference', 'time', 'work', 'present', 'exciting', 'breakthrough', 'molecular', 'dock', 'team', 'believe', 'diffdock', 'help', 'exist', 'realworld', 'pipeline', 'open', 'new', 'research', 'avenue', 'downstream', 'task', 'integration', 'proteinprotein', 'proteinnucleic', 'acid', 'dock', 'paper', 'diffdock', 'diffusion', 'step', 'twist', 'turn', 'molecular', 'docking', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Google & TUAT’s WaveFit Neural Vocoder Achieves Inference Speeds 240x Faster Than WaveRNN,https://syncedreview.com/2022/10/05/google-tuats-wavefit-neural-vocoder-achieves-inference-speeds-240x-faster-than-wavernn/,2022-10-05,"
A neural vocoder is a neural network designed to generate speech waveforms given acoustic features — often used as aContinue Reading
","A neural vocoder is a neural network designed to generate speech waveforms given acoustic features — often used as a backbone module for speech recognition tasks such as text-to-speech (TTS), speech-to-speech translation (S2ST), etc. Current neural vocoders however can struggle to maintain high sound quality without incurring high computational costs. In the new paper WaveFit: An Iterative and Non-autoregressive Neural Vocoder based on Fixed-Point Iteration, a team from Google Research and the Tokyo University of Agriculture and Technology presents WaveFit, a fast and high-quality neural vocoder that achieves natural human speech with inference speeds that are 240 times faster than WaveRNN. The first breakthrough in neural vocoder development was the introduction of autoregressive (AR) models such as WaveNet (van den Oord et al., 2016), which revolutionized the quality of speech generation but proved inefficient as they required a huge number of sequential operations for signal generation. Non-AR models were subsequently proposed to speed up inference speeds, with denoising diffusion probabilistic models (DDPMs) and generative adversarial networks (GANs) among the most popular. Generating human-comparable speech waveforms in a few iterations however remains challenging, and typically involves an undesirable trade-off between sound quality and computational cost. The proposed WaveFit non-AR neural vocoder is inspired by the theory of fixed-point iteration and introduces a novel method for combining DDPMs and GANs to boost the performance of conventional non-AR models. WaveFit iteratively applies a DNN as a denoising mapping that eliminates noise components from an input signal. A GAN-based and a short-time Fourier transform (STFT)- based loss are combined to produce a loss function that is insensitive to imperceptible phase differences and to encourage the intermediate output signals to approach the target speech along with the iterations. In their empirical study, the team evaluated WaveFit on subjective listening experiments and compared it with baselines that included WaveRNN, DDPM-based models and GAN-based models. The results show that WaveFit with five iterations can generate synthetic speech with audio quality comparable to that of WaveRNN and natural human speech while achieving inference speeds more than 240 times faster than WaveRNN. Audio demos are available at google.github.io/df-conformer/wavefit/. The paper WaveFit: An Iterative and Non-autoregressive Neural Vocoder based on Fixed-Point Iteration is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","A neural vocoder is a neural network designed to generate speech waveforms given acoustic features — often used as a backbone module for speech recognition tasks such as text-to-speech (TTS), speech-to-speech translation (S2ST), etc. Current neural vocoders however can struggle to maintain high sound quality without incurring high computational costs. In the new paper WaveFit: An Iterative and Non-autoregressive Neural Vocoder based on Fixed-Point Iteration, a team from Google Research and the Tokyo University of Agriculture and Technology presents WaveFit, a fast and high-quality neural vocoder that achieves natural human speech with inference speeds that are 240 times faster than WaveRNN. The first breakthrough in neural vocoder development was the introduction of autoregressive (AR) models such as WaveNet (van den Oord et al., 2016), which revolutionized the quality of speech generation but proved inefficient as they required a huge number of sequential operations for signal generation. Non-AR models were subsequently proposed to speed up inference speeds, with denoising diffusion probabilistic models (DDPMs) and generative adversarial networks (GANs) among the most popular. Generating human-comparable speech waveforms in a few iterations however remains challenging, and typically involves an undesirable trade-off between sound quality and computational cost. The proposed WaveFit non-AR neural vocoder is inspired by the theory of fixed-point iteration and introduces a novel method for combining DDPMs and GANs to boost the performance of conventional non-AR models. WaveFit iteratively applies a DNN as a denoising mapping that eliminates noise components from an input signal. A GAN-based and a short-time Fourier transform (STFT)- based loss are combined to produce a loss function that is insensitive to imperceptible phase differences and to encourage the intermediate output signals to approach the target speech along with the iterations. In their empirical study, the team evaluated WaveFit on subjective listening experiments and compared it with baselines that included WaveRNN, DDPM-based models and GAN-based models. The results show that WaveFit with five iterations can generate synthetic speech with audio quality comparable to that of WaveRNN and natural human speech while achieving inference speeds more than 240 times faster than WaveRNN. Audio demos are available at google.github.io/df-conformer/wavefit/. The paper WaveFit: An Iterative and Non-autoregressive Neural Vocoder based on Fixed-Point Iteration is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['neural', 'vocoder', 'neural', 'network', 'design', 'generate', 'speech', 'waveform', 'give', 'acoustic', 'feature', 'often', 'use', 'backbone', 'module', 'speech', 'recognition', 'task', 's2st', 'current', 'neural', 'vocoder', 'however', 'struggle', 'maintain', 'high', 'sound', 'quality', 'incur', 'high', 'computational', 'cost', 'new', 'paper', 'wavefit', 'iterative', 'nonautoregressive', 'neural', 'vocoder', 'base', 'fixedpoint', 'iteration', 'team', 'technology', 'present', 'wavefit', 'fast', 'highquality', 'neural', 'vocoder', 'achieve', 'natural', 'human', 'speech', 'inference', 'speed', 'time', 'fast', 'wavernn', 'first', 'breakthrough', 'neural', 'vocoder', 'development', 'introduction', 'autoregressive', 'ar', 'model', 'revolutionize', 'quality', 'speech', 'generation', 'prove', 'inefficient', 'require', 'huge', 'number', 'sequential', 'operation', 'signal', 'generation', 'nonar', 'model', 'subsequently', 'propose', 'speed', 'inference', 'speed', 'denoise', 'diffusion', 'probabilistic', 'model', 'ddpm', 'generative', 'adversarial', 'network', 'gan', 'popular', 'generate', 'humancomparable', 'speech', 'waveform', 'iteration', 'however', 'remain', 'challenge', 'typically', 'involve', 'undesirable', 'tradeoff', 'sound', 'quality', 'computational', 'cost', 'propose', 'wavefit', 'nonar', 'neural', 'vocoder', 'inspire', 'theory', 'fixedpoint', 'iteration', 'introduce', 'novel', 'method', 'combine', 'ddpm', 'gan', 'boost', 'performance', 'conventional', 'nonar', 'model', 'wavefit', 'iteratively', 'apply', 'dnn', 'denoise', 'mapping', 'eliminate', 'noise', 'component', 'input', 'signal', 'ganbased', 'shorttime', 'fouri', 'transform', 'stft', 'base', 'loss', 'combine', 'produce', 'loss', 'function', 'insensitive', 'imperceptible', 'phase', 'difference', 'encourage', 'intermediate', 'output', 'signal', 'approach', 'target', 'speech', 'iteration', 'empirical', 'study', 'team', 'evaluate', 'wavefit', 'subjective', 'listening', 'experiment', 'compare', 'baseline', 'include', 'ddpmbase', 'model', 'ganbased', 'model', 'result', 'show', 'wavefit', 'iteration', 'generate', 'synthetic', 'speech', 'audio', 'quality', 'comparable', 'wavernn', 'natural', 'human', 'speech', 'achieve', 'inference', 'speed', 'time', 'fast', 'wavernn', 'audio', 'demo', 'available', 'googlegithubiodfconformerwavefit', 'paper', 'wavefit', 'iterative', 'nonautoregressive', 'neural', 'vocoder', 'base', 'fixedpoint', 'iteration', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
UNC Chapel Hill’s Textless Vision-Language Transformer: Comparable Performance to Text-Based Approaches but 28x Faster,https://syncedreview.com/2022/10/04/unc-chapel-hills-textless-vision-language-transformer-comparable-performance-to-text-based-approaches-but-28x-faster/,2022-10-04,"
 In the new paper TVLT: Textless Vision-Language Transformer, researchers from UNC Chapel Hill present the Textless Vision-Language Transformer (TVLT) for vision-and-language representation learning. TVLT uses only raw visual and audio inputs and performs comparably to its text-based counterparts but requires only 1/3 the parameters and achieves 28x faster inference speeds.
","Transformer architectures have achieved impressive performance in vision-language (VL) representation learning when trained on text-annotated images or videos. It remains challenging, however, for transformers to learn VL representations without relying on text, i.e. using only low-level visual and acoustic inputs. In the new paper TVLT: Textless Vision-Language Transformer, researchers from UNC Chapel Hill present the Textless Vision-Language Transformer (TVLT) for vision-and-language representation learning. TVLT uses only raw visual and audio inputs and performs comparably to its text-based counterparts but requires only 1/3 the parameters and achieves 28x faster inference speeds. The TVLT’s main architecture is a transformer comprising a 12-layer encoder and an 8-layer decoder. It takes its inputs as a list of embeddings obtained directly from perception-level video and audio and does not include any text-specific modules for automatic speech recognition (ASR) or tokenization. The input embeddings are a combination of 1) modality embedding, 2) temporal/spatial embeddings for video, 3) temporal/frequency embeddings for audio, and 4) vision/audio patch embeddings. The TVLT is pretrained with two objectives: vision-audio matching (VAM) and masked autoencoding (MAE). VAM is employed to learn the global cross-modal representations, and a linear layer with sigmoid activation is then applied to the encoder to obtain a matching probability. Finally, the binary cross-entropy loss is computed. MAE is used to improve unimodal representations by masking random patches of visual frames and the audio spectrogram and reconstructing missing inputs. The novel approach slices the audio and video parts of the encoder output and feeds them to the decoder independently instead of jointly, which saves compute costs and boosts finetuning performance. In their empirical study, the team compared TVLT with text-based counterparts on audio-to-video retrieval, video-based multimodal sentiment analysis, and visual question-answering benchmarks. In the experiments, TVLT achieved performance competitive with state-of-the-art audio-based vision-and-language models on visual question answering, image retrieval, video retrieval and multimodal sentiment analysis. Moreover, it required only 1/3 of the parameters, and its inference speed was 28x faster than the text-based methods. Overall, this paper showcases the powerful performance of TVLT and advances the possibility of learning compact and efficient visual-linguistic representations from low-level visual and audio signals without the need for traditional but computationally expensive text modelling. The code and checkpoints are available on the project’s GitHub. The paper TVLT: Textless Vision-Language Transformer is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Transformer architectures have achieved impressive performance in vision-language (VL) representation learning when trained on text-annotated images or videos. It remains challenging, however, for transformers to learn VL representations without relying on text, i.e. using only low-level visual and acoustic inputs. In the new paper TVLT: Textless Vision-Language Transformer, researchers from UNC Chapel Hill present the Textless Vision-Language Transformer (TVLT) for vision-and-language representation learning. TVLT uses only raw visual and audio inputs and performs comparably to its text-based counterparts but requires only 1/3 the parameters and achieves 28x faster inference speeds. The TVLT’s main architecture is a transformer comprising a 12-layer encoder and an 8-layer decoder. It takes its inputs as a list of embeddings obtained directly from perception-level video and audio and does not include any text-specific modules for automatic speech recognition (ASR) or tokenization. The input embeddings are a combination of 1) modality embedding, 2) temporal/spatial embeddings for video, 3) temporal/frequency embeddings for audio, and 4) vision/audio patch embeddings. The TVLT is pretrained with two objectives: vision-audio matching (VAM) and masked autoencoding (MAE). VAM is employed to learn the global cross-modal representations, and a linear layer with sigmoid activation is then applied to the encoder to obtain a matching probability. Finally, the binary cross-entropy loss is computed. MAE is used to improve unimodal representations by masking random patches of visual frames and the audio spectrogram and reconstructing missing inputs. The novel approach slices the audio and video parts of the encoder output and feeds them to the decoder independently instead of jointly, which saves compute costs and boosts finetuning performance. In their empirical study, the team compared TVLT with text-based counterparts on audio-to-video retrieval, video-based multimodal sentiment analysis, and visual question-answering benchmarks. In the experiments, TVLT achieved performance competitive with state-of-the-art audio-based vision-and-language models on visual question answering, image retrieval, video retrieval and multimodal sentiment analysis. Moreover, it required only 1/3 of the parameters, and its inference speed was 28x faster than the text-based methods. Overall, this paper showcases the powerful performance of TVLT and advances the possibility of learning compact and efficient visual-linguistic representations from low-level visual and audio signals without the need for traditional but computationally expensive text modelling. The code and checkpoints are available on the project’s GitHub. The paper TVLT: Textless Vision-Language Transformer is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['transformer', 'architecture', 'achieve', 'impressive', 'performance', 'visionlanguage', 'vl', 'representation', 'learn', 'train', 'textannotated', 'image', 'video', 'remain', 'challenge', 'however', 'transformer', 'learn', 'vl', 'representation', 'rely', 'text', 'use', 'lowlevel', 'visual', 'acoustic', 'input', 'new', 'paper', 'textless', 'visionlanguage', 'transformer', 'researcher', 'present', 'textless', 'visionlanguage', 'transformer', 'visionandlanguage', 'representation', 'learn', 'use', 'raw', 'visual', 'audio', 'input', 'perform', 'comparably', 'textbase', 'counterpart', 'require', 'parameter', 'achieve', '28x', 'fast', 'inference', 'speed', 'main', 'architecture', 'transformer', 'comprise', 'encoder', 'decoder', 'take', 'input', 'list', 'embedding', 'obtain', 'directly', 'perceptionlevel', 'video', 'audio', 'include', 'textspecific', 'module', 'automatic', 'speech', 'recognition', 'asr', 'tokenization', 'input', 'embedding', 'combination', 'modality', 'embed', 'temporalspatial', 'embedding', 'video', 'temporalfrequency', 'embedding', 'audio', 'visionaudio', 'patch', 'embedding', 'tvlt', 'pretraine', 'objective', 'visionaudio', 'matching', 'vam', 'mask', 'autoencode', 'vam', 'employ', 'learn', 'global', 'crossmodal', 'representation', 'linear', 'layer', 'sigmoid', 'activation', 'apply', 'encoder', 'obtain', 'matching', 'probability', 'finally', 'binary', 'crossentropy', 'loss', 'compute', 'use', 'improve', 'unimodal', 'representation', 'mask', 'random', 'patch', 'visual', 'frame', 'audio', 'spectrogram', 'reconstruct', 'miss', 'input', 'novel', 'approach', 'slice', 'audio', 'video', 'part', 'encod', 'output', 'feed', 'decoder', 'independently', 'instead', 'jointly', 'save', 'compute', 'cost', 'boost', 'finetune', 'performance', 'empirical', 'study', 'team', 'compare', 'textbase', 'counterpart', 'videobase', 'multimodal', 'sentiment', 'analysis', 'visual', 'questionanswere', 'benchmark', 'experiment', 'achieve', 'performance', 'competitive', 'stateoftheart', 'audiobase', 'visionandlanguage', 'model', 'visual', 'question', 'answer', 'image', 'retrieval', 'video', 'retrieval', 'multimodal', 'sentiment', 'analysis', 'moreover', 'require', 'parameter', 'inference', 'speed', '28x', 'fast', 'textbase', 'method', 'overall', 'paper', 'showcase', 'powerful', 'performance', 'advance', 'possibility', 'learn', 'compact', 'efficient', 'visuallinguistic', 'representation', 'lowlevel', 'visual', 'audio', 'signal', 'need', 'traditional', 'computationally', 'expensive', 'text', 'model', 'code', 'checkpoint', 'available', 'project', 'paper', 'textless', 'visionlanguage', 'transformer', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Google & DeepMind Propose Geometric Complexity for DNN Analysis and Evaluation,https://syncedreview.com/2022/10/03/google-deepmind-propose-geometric-complexity-for-dnn-analysis-and-evaluation/,2022-10-03,"
In the new paper Why Neural Networks Find Simple Solutions: The Many Regularizers of Geometric Complexity, a research team from Google and DeepMind proposes Geometric Complexity (GC), a measure of deep neural network model complexity that serves as a useful tool for understanding the underlying mechanisms of complexity control.
","Bigger is not always better. While large language models and complex deep neural networks (DNNs) have resulted in huge performance gains across a variety of AI-related tasks, lighter and simpler models are often preferable in industrial applications. It is thus crucial for continued efficient DNN development and deployment that the machine learning research community improves its understanding of fundamental model complexity control methods such as regularization. In the new paper Why Neural Networks Find Simple Solutions: The Many Regularizers of Geometric Complexity, a research team from Google and DeepMind proposes Geometric Complexity (GC), a measure of DNN model complexity that serves as a useful tool for understanding the underlying mechanisms of complexity control. The team summarizes their main contributions as follows: Previous studies have proposed numerous complexity measures — naive parameter count, data-driven approaches, VC dimension and Rademacher complexity — but most of these fail to clarify the properties of regularizers and the connections between implicit and explicit regularizers. The proposed GC aims at solving these issues. The researchers provide a clear definition of GC and how it relates to important aspects of deep learning, including linear models, ReLU networks, Lipschitz smoothness, arc length, and harmonic maps. The paper explores the impacts of initialization, explicit regularization and implicit regularization on geometric complexity; examining different parameter initialization choices, L2 regularization, Lipschitz regularization via spectral norm regularization, noise regularization, flatness regularization, explicit GC regularization and Jacobian regularization. The researchers conclude that common training heuristics such as parameter norm regularization, spectral norm regularization, flatness regularization, implicit gradient regularization, noise regularization and the choice of parameter initialization can all play a part in reducing geometric complexity. Finally, the team demonstrates that GC can also capture double-descent behaviour in the test loss when a model’s parameter count increases. Overall, this paper validates GC as an effective tool for understanding DNN models and sheds light on how DNNs achieve low test errors with highly expressive models. The team hopes their work will encourage further research in this area and lead to a better understanding of current best practices and the discovery of new methods for efficient model training. The paper Why Neural Networks Find Simple Solutions: The Many Regularizers of Geometric Complexity has been accepted by the 36th Conference on Neural Information Processing Systems (NeurIPS 2022), which runs from November 28 to December 9 in New Orleans, USA; and is available on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Bigger is not always better. While large language models and complex deep neural networks (DNNs) have resulted in huge performance gains across a variety of AI-related tasks, lighter and simpler models are often preferable in industrial applications. It is thus crucial for continued efficient DNN development and deployment that the machine learning research community improves its understanding of fundamental model complexity control methods such as regularization. In the new paper Why Neural Networks Find Simple Solutions: The Many Regularizers of Geometric Complexity, a research team from Google and DeepMind proposes Geometric Complexity (GC), a measure of DNN model complexity that serves as a useful tool for understanding the underlying mechanisms of complexity control. The team summarizes their main contributions as follows: Previous studies have proposed numerous complexity measures — naive parameter count, data-driven approaches, VC dimension and Rademacher complexity — but most of these fail to clarify the properties of regularizers and the connections between implicit and explicit regularizers. The proposed GC aims at solving these issues. The researchers provide a clear definition of GC and how it relates to important aspects of deep learning, including linear models, ReLU networks, Lipschitz smoothness, arc length, and harmonic maps. The paper explores the impacts of initialization, explicit regularization and implicit regularization on geometric complexity; examining different parameter initialization choices, L2 regularization, Lipschitz regularization via spectral norm regularization, noise regularization, flatness regularization, explicit GC regularization and Jacobian regularization. The researchers conclude that common training heuristics such as parameter norm regularization, spectral norm regularization, flatness regularization, implicit gradient regularization, noise regularization and the choice of parameter initialization can all play a part in reducing geometric complexity. Finally, the team demonstrates that GC can also capture double-descent behaviour in the test loss when a model’s parameter count increases. Overall, this paper validates GC as an effective tool for understanding DNN models and sheds light on how DNNs achieve low test errors with highly expressive models. The team hopes their work will encourage further research in this area and lead to a better understanding of current best practices and the discovery of new methods for efficient model training. The paper Why Neural Networks Find Simple Solutions: The Many Regularizers of Geometric Complexity has been accepted by the 36th Conference on Neural Information Processing Systems (NeurIPS 2022), which runs from November 28 to December 9 in New Orleans, USA; and is available on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['big', 'always', 'well', 'large', 'language', 'model', 'complex', 'deep', 'neural', 'network', 'dnn', 'result', 'huge', 'performance', 'gain', 'variety', 'airelate', 'task', 'light', 'simple', 'model', 'often', 'preferable', 'industrial', 'application', 'thus', 'crucial', 'continue', 'efficient', 'dnn', 'development', 'deployment', 'machine', 'learn', 'research', 'community', 'improve', 'understanding', 'fundamental', 'model', 'complexity', 'control', 'method', 'regularization', 'new', 'paper', 'neural', 'network', 'find', 'simple', 'solution', 'many', 'regularizer', 'geometric', 'complexity', 'research', 'team', 'deepmind', 'propose', 'geometric', 'complexity', 'measure', 'dnn', 'model', 'complexity', 'serve', 'useful', 'tool', 'understand', 'underlie', 'mechanism', 'complexity', 'control', 'team', 'summarize', 'main', 'contribution', 'follow', 'previous', 'study', 'propose', 'numerous', 'complexity', 'measure', 'naive', 'parameter', 'count', 'datadriven', 'approach', 'dimension', 'rademacher', 'complexity', 'fail', 'clarify', 'property', 'regularizer', 'connection', 'implicit', 'explicit', 'regularizer', 'propose', 'aim', 'solve', 'issue', 'researcher', 'provide', 'clear', 'definition', 'gc', 'relate', 'important', 'aspect', 'deep', 'learning', 'include', 'linear', 'model', 'relu', 'network', 'lipschitz', 'smoothness', 'arc', 'length', 'harmonic', 'map', 'paper', 'explore', 'impact', 'initialization', 'explicit', 'regularization', 'implicit', 'regularization', 'geometric', 'complexity', 'examine', 'different', 'parameter', 'initialization', 'choice', 'l2', 'regularization', 'lipschitz', 'regularization', 'spectral', 'norm', 'regularization', 'researcher', 'conclude', 'common', 'training', 'heuristic', 'parameter', 'spectral', 'regularization', 'noise', 'regularization', 'choice', 'parameter', 'initialization', 'play', 'part', 'reduce', 'geometric', 'complexity', 'finally', 'team', 'demonstrate', 'also', 'capture', 'doubledescent', 'behaviour', 'test', 'loss', 'model', 'parameter', 'increase', 'overall', 'paper', 'validate', 'gc', 'effective', 'tool', 'understand', 'dnn', 'model', 'shed', 'light', 'dnn', 'achieve', 'low', 'test', 'error', 'highly', 'expressive', 'model', 'team', 'hope', 'work', 'encourage', 'research', 'area', 'lead', 'well', 'understanding', 'current', 'good', 'practice', 'discovery', 'new', 'method', 'efficient', 'model', 'train', 'paper', 'neural', 'network', 'find', 'simple', 'solution', 'many', 'regularizer', 'geometric', 'complexity', 'accept', '36th', 'conference', 'neural', 'information', 'processing', 'system', 'neurip', 'run', 'available', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
