title,url,date,summary,text,cleaning,tokens
Microsoft’s LLMA Accelerates LLM Generations via an ‘Inference-With-Reference’ Decoding Approach,https://syncedreview.com/2023/04/13/microsofts-llma-accelerates-llm-generations-via-an-inference-with-reference-decoding-approach/,2023-04-13,"
In the new paper Inference with Reference: Lossless Acceleration of Large Language Models, a Microsoft research team proposes LLMA, an inference-with-reference decoding mechanism that achieves up to 2x lossless speed-ups with identical generation results by exploiting the overlaps between LLM outputs and references.

 ","While today’s powerful large language models (LLMs) have found applications in many real-world scenarios, the high compute cost of their autoregressive decoding process remains a bottleneck to deployment at scale. This has prompted machine learning researchers and developers to explore various approaches for improving LLM inference efficiency. In the new paper Inference with Reference: Lossless Acceleration of Large Language Models, a Microsoft research team proposes LLMA, a novel inference-with-reference decoding mechanism that achieves up to 2x lossless speed-ups in LLMs with identical generation results by exploiting the overlaps between their outputs and references, e.g., retrieved documents. This team first notes that an LLM’s output tokens often come from its context — which includes relevant retrieved documents from external reference sources — and its outputs thus tend to contain text spans that “overlap” with those present in the retrieved documents. Motivated by this observation, the proposed LLMA aims to exploit these overlaps between LLM outputs and their reference documents. LLMA first selects a text span from the reference cache, transfers copies of its tokens to the LLM decoder, and evaluates their acceptability based on the output token’s probabilities. This process is conducted in parallel to boost efficiency — enabling accelerated decoding while ensuring the generated results are identical to those of a vanilla greedy decoding method. In their empirical study, the team applied their approach to open-source LLaMA language models in both retrieval-augmented and cache-assisted scenarios. LLMA achieved better than 2x speed-ups in both experiments, with generation results identical to greedy decoding methods. Overall, this work demonstrates the effectiveness of the proposed LLMA mechanism in significantly accelerating LLM inference times without sacrificing the quality of the generated results. The paper Inference with Reference: Lossless Acceleration of Large Language Models is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","While today ’ s powerful large language models ( LLMs ) have found applications in many real-world scenarios , the high compute cost of their autoregressive decoding process remains a bottleneck to deployment at scale . This has prompted machine learning researchers and developers to explore various approaches for improving LLM inference efficiency . In the new paper Inference with Reference : Lossless Acceleration of Large Language Models , a Microsoft research team proposes LLMA , a novel inference-with-reference decoding mechanism that achieves up to 2x lossless speed-ups in LLMs with identical generation results by exploiting the overlaps between their outputs and references , e.g. , retrieved documents . This team first notes that an LLM ’ s output tokens often come from its context — which includes relevant retrieved documents from external reference sources — and its outputs thus tend to contain text spans that “ overlap ” with those present in the retrieved documents . Motivated by this observation , the proposed LLMA aims to exploit these overlaps between LLM outputs and their reference documents . LLMA first selects a text span from the reference cache , transfers copies of its tokens to the LLM decoder , and evaluates their acceptability based on the output token ’ s probabilities . This process is conducted in parallel to boost efficiency — enabling accelerated decoding while ensuring the generated results are identical to those of a vanilla greedy decoding method . In their empirical study , the team applied their approach to open-source LLaMA language models in both retrieval-augmented and cache-assisted scenarios . LLMA achieved better than 2x speed-ups in both experiments , with generation results identical to greedy decoding methods . Overall , this work demonstrates the effectiveness of the proposed LLMA mechanism in significantly accelerating LLM inference times without sacrificing the quality of the generated results . The paper Inference with Reference : Lossless Acceleration of Large Language Models is on arXiv . Author : Hecate He | Editor : Michael Sarazen We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['today', 'powerful', 'large', 'language', 'model', 'llm', 'find', 'application', 'many', 'realworld', 'scenario', 'high', 'compute', 'cost', 'autoregressive', 'decode', 'process', 'remain', 'bottleneck', 'deployment', 'scale', 'prompt', 'machine', 'learn', 'researcher', 'developer', 'explore', 'various', 'approach', 'improve', 'llm', 'inference', 'efficiency', 'new', 'paper', 'inference', 'reference', 'lossless', 'acceleration', 'large', 'language', 'model', 'research', 'team', 'propose', 'llma', 'novel', 'inferencewithreference', 'decode', 'mechanism', 'achieve', 'lossless', 'speedup', 'llm', 'identical', 'generation', 'result', 'exploit', 'overlap', 'output', 'reference', 'eg', 'retrieve', 'document', 'team', 'first', 'note', 'llm', 'output', 'token', 'often', 'come', 'context', 'include', 'relevant', 'retrieve', 'document', 'external', 'reference', 'source', 'output', 'thus', 'tend', 'contain', 'text', 'span', 'overlap', 'present', 'retrieve', 'document', 'motivate', 'observation', 'propose', 'llma', 'aim', 'exploit', 'overlap', 'llm', 'output', 'reference', 'document', 'llma', 'first', 'select', 'text', 'span', 'reference', 'cache', 'transfer', 'copy', 'token', 'llm', 'decoder', 'evaluate', 'acceptability', 'base', 'output', 'token', 'probability', 'process', 'conduct', 'parallel', 'boost', 'efficiency', 'enable', 'accelerate', 'decode', 'ensure', 'generate', 'result', 'identical', 'vanilla', 'greedy', 'decode', 'method', 'empirical', 'study', 'team', 'apply', 'approach', 'opensource', 'language', 'model', 'retrievalaugmente', 'cacheassiste', 'scenario', 'llma', 'achieve', 'well', 'speedup', 'experiment', 'generation', 'result', 'identical', 'greedy', 'decode', 'method', 'overall', 'work', 'demonstrate', 'effectiveness', 'propose', 'llma', 'mechanism', 'significantly', 'accelerate', 'llm', 'inference', 'time', 'sacrifice', 'quality', 'generate', 'result', 'paper', 'inference', 'reference', 'lossless', 'acceleration', 'large', 'language', 'model', 'arxiv', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']"
Stanford U & Google’s Generative Agents Produce Believable Proxies of Human Behaviours,https://syncedreview.com/2023/04/12/stanford-u-googles-generative-agents-produce-believable-proxies-of-human-behaviours/,2023-04-12,"
In the new paper Generative Agents: Interactive Simulacra of Human Behavior, a team from Stanford University and Google Research presents agents that draw on generative models to simulate both individual and emergent group behaviours that are humanlike and based on their changing experiences and environment.
","The quality and fluency of AI bots’ natural language generation are unquestionable, but how well can such agents mimic other human behaviours? Researchers and practitioners have long considered the creation of a sort of sandbox society populated by agents with human behaviours as an approach for gaining insights into various interactions, interpersonal relationships, social theories, and more — and we may be getting there. In the new paper Generative Agents: Interactive Simulacra of Human Behavior, a team from Stanford University and Google Research presents agents that draw on generative models to simulate both individual and emergent group behaviours that are humanlike and based on their identities, changing experiences, and environment. The team summarizes their main contributions as follows: The team sought to build a virtual open-world framework where intelligent agents go about their business and engage with each other in natural language — planning their days, sharing news, forming relationships, and coordinating group activities — all while conditioning their behaviours on both the changing environment and their past experiences. The team’s novel agent architecture combines a large language model (LLM) with mechanisms that synthesize and extract information based on the LLM outputs such that the agents learn from their past experiences how to make more accurate real-time inferences while maintaining long-term character coherence. A core agent component is their memory stream, a database that stores a comprehensive record of past experiences. The agent can retrieve relevant information from its memory stream, reflect on this, and plan actions with regard to its changing environment. Agents can also recursively synthesize records to higher-level observations to guide more complex behaviours. In their empirical study, the team enlisted human evaluators and had 25 of their proposed generative agents interact with each other in natural language over two full game days as non-player characters (NPC) in a Smallville sandbox environment built with the Phaser web game development framework. In the experiment, the agents maintained character coherence and demonstrated believable proxies of humanlike behaviours in remembering, planning, reacting, and reflecting. This work advances the development of LLM-based simulacra populated by agents with dynamic and interactive humanlike behaviours, with potential applications in role-playing, social prototyping, immersive environments and gaming.The paper Generative Agents: Interactive Simulacra of Human Behavior is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The quality and fluency of AI bots ’ natural language generation are unquestionable , but how well can such agents mimic other human behaviours ? Researchers and practitioners have long considered the creation of a sort of sandbox society populated by agents with human behaviours as an approach for gaining insights into various interactions , interpersonal relationships , social theories , and more — and we may be getting there . In the new paper Generative Agents : Interactive Simulacra of Human Behavior , a team from Stanford University and Google Research presents agents that draw on generative models to simulate both individual and emergent group behaviours that are humanlike and based on their identities , changing experiences , and environment . The team summarizes their main contributions as follows : The team sought to build a virtual open-world framework where intelligent agents go about their business and engage with each other in natural language — planning their days , sharing news , forming relationships , and coordinating group activities — all while conditioning their behaviours on both the changing environment and their past experiences . The team ’ s novel agent architecture combines a large language model ( LLM ) with mechanisms that synthesize and extract information based on the LLM outputs such that the agents learn from their past experiences how to make more accurate real-time inferences while maintaining long-term character coherence . A core agent component is their memory stream , a database that stores a comprehensive record of past experiences . The agent can retrieve relevant information from its memory stream , reflect on this , and plan actions with regard to its changing environment . Agents can also recursively synthesize records to higher-level observations to guide more complex behaviours . In their empirical study , the team enlisted human evaluators and had 25 of their proposed generative agents interact with each other in natural language over two full game days as non-player characters ( NPC ) in a Smallville sandbox environment built with the Phaser web game development framework . In the experiment , the agents maintained character coherence and demonstrated believable proxies of humanlike behaviours in remembering , planning , reacting , and reflecting . This work advances the development of LLM-based simulacra populated by agents with dynamic and interactive humanlike behaviours , with potential applications in role-playing , social prototyping , immersive environments and gaming.The paper Generative Agents : Interactive Simulacra of Human Behavior is on arXiv . Author : Hecate He | Editor : Michael Sarazen We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['quality', 'fluency', 'bot', 'natural', 'language', 'generation', 'unquestionable', 'well', 'agent', 'mimic', 'human', 'behaviour', 'researcher', 'practitioner', 'long', 'consider', 'creation', 'sort', 'sandbox', 'society', 'populate', 'agent', 'human', 'behaviour', 'approach', 'gain', 'insight', 'various', 'interaction', 'interpersonal', 'relationship', 'social', 'theory', 'get', 'new', 'paper', 'generative', 'agent', 'interactive', 'simulacrum', 'human', 'behavior', 'team', 'present', 'agent', 'draw', 'generative', 'model', 'simulate', 'individual', 'emergent', 'group', 'behaviour', 'humanlike', 'base', 'identity', 'change', 'experience', 'environment', 'team', 'summarize', 'main', 'contribution', 'follow', 'team', 'seek', 'build', 'virtual', 'openworld', 'framework', 'intelligent', 'agent', 'go', 'business', 'engage', 'natural', 'language', 'plan', 'day', 'share', 'news', 'form', 'relationship', 'coordinate', 'group', 'activity', 'condition', 'behaviour', 'change', 'environment', 'past', 'experience', 'team', 'novel', 'agent', 'architecture', 'combine', 'large', 'language', 'model', 'llm', 'mechanism', 'synthesize', 'extract', 'information', 'base', 'llm', 'output', 'agent', 'learn', 'past', 'experience', 'make', 'accurate', 'realtime', 'inference', 'maintain', 'longterm', 'character', 'coherence', 'core', 'agent', 'component', 'memory', 'stream', 'database', 'store', 'comprehensive', 'record', 'past', 'experience', 'agent', 'retrieve', 'relevant', 'information', 'memory', 'stream', 'reflect', 'plan', 'action', 'regard', 'change', 'environment', 'agent', 'also', 'recursively', 'synthesize', 'record', 'higherlevel', 'observation', 'guide', 'complex', 'behaviour', 'empirical', 'study', 'team', 'enlist', 'human', 'evaluator', 'propose', 'generative', 'agent', 'interact', 'natural', 'language', 'full', 'game', 'day', 'nonplayer', 'character', 'npc', 'environment', 'build', 'phaser', 'web', 'game', 'development', 'framework', 'experiment', 'agent', 'maintain', 'character', 'coherence', 'demonstrate', 'believable', 'proxy', 'humanlike', 'behaviour', 'remember', 'planning', 'react', 'reflect', 'work', 'advance', 'development', 'llmbased', 'simulacrum', 'populate', 'agent', 'dynamic', 'interactive', 'humanlike', 'behaviour', 'potential', 'application', 'roleplay', 'social', 'prototype', 'immersive', 'environment', 'paper', 'generative', 'agent', 'interactive', 'simulacrum', 'human', 'behavior', 'arxiv', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']"
Adobe & UCL’s Pix2Video: Text-Guided Video Editing via Image Diffusion Without Preprocessing or Finetuning,https://syncedreview.com/2023/04/11/adobe-ucls-pix2video-text-guided-video-editing-via-image-diffusion-without-preprocessing-or-finetuning/,2023-04-11,"
In the new paper Pix2Video: Video Editing Using Image Diffusion, an Adobe Research and University College London team presents Pix2Video, a framework for realistic text-guided video editing using a pretrained image diffusion model.
","Image diffusion models have emerged as a game-changing method for generating high-quality images that can be edited via users’ natural language text prompts. However, applying image diffusion models to video editing produces inconsistent results, as they struggle with preserving source video content and temporal coherence across frames. A research team from Adobe Research and University College London addresses these issues in the new paper Pix2Video: Video Editing Using Image Diffusion, introducing Pix2Video, a novel framework that employs a pretrained image diffusion model to enable faithful and realistic text-guided video editing without additional training. The Pix2Video process comprises two simple steps: 1) A pretrained structure-guided image diffusion model performs text-guided edits on an anchor frame; 2) The changes are then progressively propagated to the future frame(s) via a self-attention feature injection approach where the self-attention layers perform cross-frame attention to enable consistent image generation. Given a sequence of frames from a video clip, Pix2Video first inverts each frame using a denoising diffusion implicit model (DDIM) and considers it as the initial noise for the denoising process. A reference frame is then selected and its self-attention features injected into the UNet to edit each frame. At each diffusion step, the input features to the self-attention module are projected into queries, keys, and values to enable Pix2Video to capture global information, and the latent of the current frame is updated guided by the reference frame. In their empirical study, the team compared Pix2Video with state-of-the-art image and video editing approaches such as Text2Live and SDEdit on the DAVIS video object segmentation dataset. In the evaluations, Pix2Video achieved performance on par or better than the baselines, demonstrating its ability to edit videos with either a clear foreground object or multiple foreground objects, better preserve the structure of the input video, and maintain a good balance between edit quality and consistency without additional training. Additional model demos can be found on the project’s GitHub. The paper Pix2Video: Video Editing Using Image Diffusion is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Image diffusion models have emerged as a game-changing method for generating high-quality images that can be edited via users ’ natural language text prompts . However , applying image diffusion models to video editing produces inconsistent results , as they struggle with preserving source video content and temporal coherence across frames . A research team from Adobe Research and University College London addresses these issues in the new paper Pix2Video : Video Editing Using Image Diffusion , introducing Pix2Video , a novel framework that employs a pretrained image diffusion model to enable faithful and realistic text-guided video editing without additional training . The Pix2Video process comprises two simple steps : 1 ) A pretrained structure-guided image diffusion model performs text-guided edits on an anchor frame ; 2 ) The changes are then progressively propagated to the future frame ( s ) via a self-attention feature injection approach where the self-attention layers perform cross-frame attention to enable consistent image generation . Given a sequence of frames from a video clip , Pix2Video first inverts each frame using a denoising diffusion implicit model ( DDIM ) and considers it as the initial noise for the denoising process . A reference frame is then selected and its self-attention features injected into the UNet to edit each frame . At each diffusion step , the input features to the self-attention module are projected into queries , keys , and values to enable Pix2Video to capture global information , and the latent of the current frame is updated guided by the reference frame . In their empirical study , the team compared Pix2Video with state-of-the-art image and video editing approaches such as Text2Live and SDEdit on the DAVIS video object segmentation dataset . In the evaluations , Pix2Video achieved performance on par or better than the baselines , demonstrating its ability to edit videos with either a clear foreground object or multiple foreground objects , better preserve the structure of the input video , and maintain a good balance between edit quality and consistency without additional training . Additional model demos can be found on the project ’ s GitHub . The paper Pix2Video : Video Editing Using Image Diffusion is on arXiv . Author : Hecate He | Editor : Michael Sarazen We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['image', 'diffusion', 'model', 'emerge', 'gamechange', 'method', 'generate', 'highquality', 'image', 'edit', 'user', 'natural', 'language', 'text', 'prompt', 'however', 'apply', 'image', 'diffusion', 'model', 'video', 'editing', 'produce', 'inconsistent', 'result', 'struggle', 'preserve', 'source', 'video', 'content', 'temporal', 'coherence', 'frame', 'research', 'team', 'adobe', 'address', 'issue', 'new', 'paper', 'video', 'editing', 'use', 'image', 'diffusion', 'introduce', 'novel', 'framework', 'employ', 'pretraine', 'image', 'diffusion', 'model', 'enable', 'faithful', 'realistic', 'textguide', 'video', 'editing', 'additional', 'training', 'process', 'comprise', 'simple', 'step', 'pretraine', 'structureguide', 'image', 'diffusion', 'model', 'perform', 'textguide', 'edit', 'anchor', 'frame', 'change', 'progressively', 'propagate', 'future', 'frame', 'selfattention', 'feature', 'injection', 'approach', 'selfattention', 'layer', 'perform', 'crossframe', 'attention', 'enable', 'consistent', 'image', 'generation', 'give', 'sequence', 'frame', 'video', 'clip', 'first', 'invert', 'frame', 'use', 'denoise', 'diffusion', 'implicit', 'model', 'consider', 'initial', 'noise', 'denoising', 'process', 'reference', 'frame', 'select', 'selfattention', 'feature', 'inject', 'unet', 'edit', 'frame', 'diffusion', 'step', 'input', 'feature', 'selfattention', 'module', 'project', 'query', 'key', 'value', 'enable', 'capture', 'global', 'information', 'latent', 'current', 'frame', 'update', 'guide', 'reference', 'frame', 'empirical', 'study', 'team', 'compare', 'stateoftheart', 'image', 'video', 'editing', 'approach', 'sdedit', 'video', 'object', 'segmentation', 'dataset', 'evaluation', 'pix2video', 'achieve', 'performance', 'par', 'well', 'baseline', 'demonstrate', 'ability', 'edit', 'video', 'clear', 'foreground', 'object', 'multiple', 'foreground', 'object', 'well', 'preserve', 'structure', 'input', 'video', 'maintain', 'good', 'balance', 'edit', 'quality', 'consistency', 'additional', 'training', 'additional', 'model', 'find', 'project', 'paper', 'video', 'editing', 'use', 'image', 'diffusion', 'arxiv', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']"
UC Berkeley’s Instruct-NeRF2NeRF Edits 3D Scenes With Text Instructions,https://syncedreview.com/2023/04/10/uc-berkeleys-instruct-nerf2nerf-edits-3d-scenes-with-text-instructions/,2023-04-10,"
In the new paper Instruct-NeRF2NeRF: Editing 3D Scenes With Instructions, a UC Berkeley research team presents Instruct-NeRF2NeRF, an approach for editing 3D NeRF scenes through natural language text instructions. The proposed method can edit large-scale, real-world 3D scenes with improved ease of use and realism.
","Recent progress in neural 3D reconstruction has greatly simplified capturing realistic digital representations of real-world 3D objects and scenes via neural radiance fields (NeRFs) built using information from multiple camera viewpoints. Current approaches for editing such 3D representations are however much less accessible, typically requiring specialized tools. In the new paper Instruct-NeRF2NeRF: Editing 3D Scenes With Instructions, a UC Berkeley research team presents Instruct-NeRF2NeRF, an approach for editing 3D NeRF scenes through natural language text instructions alone. The proposed method is able to edit large-scale, real-world 3D scenes with improved ease of use and realism. Instruct-NeRF2NeRF takes as its inputs a reconstructed NeRF scene, a set of captured images and their corresponding camera poses, and camera calibration information. The user’s natural-language editing instructions are then used to condition the model’s edited NeRF output. Instruct-NeRF2NeRF uses InstructPix2Pix — a diffusion-based model specialized for image editing — to iteratively update image content at the captured viewpoints. These dataset edits are then consolidated into a globally consistent 3D representation via NeRF training. This novel Iterative Dataset Update (Iterative DU) approach enables Instruct-NeRF2NeRF to gradually percolate diffusion priors into a 3D scene reconstruction while maintaining the original scene’s structure and identity. The team uses NeRFStudio’s Nerfacto model as their underlying NeRF implementation and fine-tunes parameters that affect noise/signal strength and the model’s classifier-free guidance weights to optimize edit strength and enable different degrees of scene edits before performing the NeRF optimization process. In their empirical study, the team applied Instruct-NeRF2NeRF to the editing of 360 unique 3D scenes of varying complexity and compared its qualitative and quantitative performance against ablative baselines. The results show that Instruct-NeRF2NeRF can perform superior targeted edits on 3D representations of people, objects, and large-scale real-world scenes and impart its outputs with realism that surpasses the benchmarks. Result videos can be found on the project’s website. The paper Instruct-NeRF2NeRF: Editing 3D Scenes With Instructions is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Recent progress in neural 3D reconstruction has greatly simplified capturing realistic digital representations of real-world 3D objects and scenes via neural radiance fields ( NeRFs ) built using information from multiple camera viewpoints . Current approaches for editing such 3D representations are however much less accessible , typically requiring specialized tools . In the new paper Instruct-NeRF2NeRF : Editing 3D Scenes With Instructions , a UC Berkeley research team presents Instruct-NeRF2NeRF , an approach for editing 3D NeRF scenes through natural language text instructions alone . The proposed method is able to edit large-scale , real-world 3D scenes with improved ease of use and realism . Instruct-NeRF2NeRF takes as its inputs a reconstructed NeRF scene , a set of captured images and their corresponding camera poses , and camera calibration information . The user ’ s natural-language editing instructions are then used to condition the model ’ s edited NeRF output . Instruct-NeRF2NeRF uses InstructPix2Pix — a diffusion-based model specialized for image editing — to iteratively update image content at the captured viewpoints . These dataset edits are then consolidated into a globally consistent 3D representation via NeRF training . This novel Iterative Dataset Update ( Iterative DU ) approach enables Instruct-NeRF2NeRF to gradually percolate diffusion priors into a 3D scene reconstruction while maintaining the original scene ’ s structure and identity . The team uses NeRFStudio ’ s Nerfacto model as their underlying NeRF implementation and fine-tunes parameters that affect noise/signal strength and the model ’ s classifier-free guidance weights to optimize edit strength and enable different degrees of scene edits before performing the NeRF optimization process . In their empirical study , the team applied Instruct-NeRF2NeRF to the editing of 360 unique 3D scenes of varying complexity and compared its qualitative and quantitative performance against ablative baselines . The results show that Instruct-NeRF2NeRF can perform superior targeted edits on 3D representations of people , objects , and large-scale real-world scenes and impart its outputs with realism that surpasses the benchmarks . Result videos can be found on the project ’ s website . The paper Instruct-NeRF2NeRF : Editing 3D Scenes With Instructions is on arXiv . Author : Hecate He | Editor : Michael Sarazen We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['recent', 'progress', 'neural', '3d', 'reconstruction', 'greatly', 'simplify', 'capture', 'realistic', 'digital', 'representation', 'realworld', '3d', 'object', 'scene', 'neural', 'radiance', 'field', 'nerf', 'build', 'use', 'information', 'multiple', 'camera', 'viewpoint', 'current', 'approach', 'edit', '3d', 'representation', 'however', 'much', 'less', 'accessible', 'typically', 'require', 'specialized', 'tool', 'new', 'paper', 'instructnerf2nerf', 'edit', '3d', 'scene', 'instruction', 'team', 'present', 'approach', 'edit', '3d', 'nerf', 'scene', 'natural', 'language', 'text', 'instruction', 'alone', 'propose', 'method', 'able', 'edit', 'largescale', 'scene', 'improve', 'ease', 'use', 'realism', 'instructnerf2nerf', 'take', 'input', 'reconstructed', 'nerf', 'scene', 'set', 'capture', 'image', 'corresponding', 'camera', 'pose', 'camera', 'calibration', 'information', 'user', 'naturallanguage', 'editing', 'instruction', 'use', 'condition', 'model', 'edit', 'nerf', 'output', 'use', 'diffusionbased', 'model', 'specialize', 'image', 'editing', 'iteratively', 'update', 'image', 'content', 'capture', 'viewpoint', 'dataset', 'edit', 'consolidate', 'globally', 'consistent', '3d', 'representation', 'nerf', 'training', 'novel', 'iterative', 'dataset', 'update', 'iterative', 'approach', 'enable', 'instructnerf2nerf', 'gradually', 'percolate', 'diffusion', 'prior', '3d', 'scene', 'reconstruction', 'maintain', 'original', 'scene', 'structure', 'identity', 'team', 'use', 'nerfacto', 'model', 'underlying', 'nerf', 'implementation', 'finetune', 'parameter', 'affect', 'noisesignal', 'strength', 'model', 'classifierfree', 'guidance', 'weight', 'optimize', 'edit', 'strength', 'enable', 'different', 'degree', 'scene', 'edit', 'perform', 'nerf', 'optimization', 'process', 'empirical', 'study', 'team', 'apply', 'editing', 'unique', '3d', 'scene', 'vary', 'complexity', 'compare', 'qualitative', 'quantitative', 'performance', 'ablative', 'baseline', 'result', 'show', 'perform', 'superior', 'target', 'edit', '3d', 'representation', 'people', 'object', 'scene', 'impart', 'output', 'realism', 'surpass', 'benchmark', 'result', 'video', 'find', 'project', 'website', 'paper', 'instructnerf2nerf', 'edit', '3d', 'scene', 'instruction', 'arxiv', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']"
"Google Reveals Its Latest TPU v4-Based Supercomputer, Which Betters Nvidia’s A100s in Speed and Efficiency",https://syncedreview.com/2023/04/06/google-reveals-its-latest-tpu-v4-based-supercomputer-which-betters-nvidias-a100s-in-speed-and-efficiency/,2023-04-06,"
In the new paper TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support, a Google Research team presents TPU v4, the company’s latest supercomputer. TPU v4 is ten times faster than v3 and 1.2–1.7x faster than Nvidia A100 GPUs while using 1.3x–1.9x less power.
","Machine learning (ML) models continue to evolve in scale and algorithmically, a trend that burdens system developers with ever-increasing compute and power requirements for training and deployment. How to meet the demands? Tech giant Nvidia’s A100 high-performance graphics processing unit (GPU) dominated the AI accelerator market until Google joined the campaign in 2016 with its Tensor Processing Units (TPUs). This week, Google introduced its newest entry, a TPU-based supercomputer it says is both faster and more efficient than the A100. In the new paper TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support, a Google Research team presents TPU v4, the company’s latest supercomputer. The TPU v4 is ten times faster than v3 and 1.2–1.7x faster than Nvidia A100 GPUs while using 1.3x–1.9x less power. Google believes the performance, scalability, and availability of TPU v4 will make it the new workhorse for today’s compute-hungry large language models (LMMs). The team summarizes their paper’s main contributions as follows: A key improvement in this version is its leveraging of Optical Circuit Switches (OCSes), which interconnect TPU v4’s 4096 chips via optical data links to improve scale, availability, utilization, modularity, deployment, security, power and performance. The Google Palomar OCSes are based on 3D Micro-Electro-Mechanical Systems (MEMS) mirrors that switch in milliseconds; and advance the state-of-the-art in terms of reliability and cost. Like TPU v3, the TPU v4 package comprises two Tensor Cores (TC). Each TC contains four 128×128 Matrix Multiply Units (MXUs), a Vector Processing Unit (VPU) and a Vector Memory (VMEM). Thanks to its OCSes, TPU v4 can quickly and easily change topology to adapt to the application, number of nodes, and system running a job — thus significantly improving training time. Each TPU v4 also incorporates SparseCores, dataflow processors that accelerate embedding-reliant models by 5x–7x yet use only 5 percent of the die area and power. The researchers’ empirical study shows that TPU v4 is 2.1x faster and improves performance by 2.7x compared to TPU v3, achieves ~4.3x–4.5x faster speeds than the Graphcore IPU Bow, and is 1.2x–1.7x faster than Nvidia A100s with 1.3x–1.9x less power consumption. The paper also reports that TPU v4s deployed in Google Cloud’s energy-optimized warehouse-scale computers use ~2-6x less energy and produce ~20x less CO2e than domain-specific architectures (DSAs) in typical data centres. The paper TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings will be presented at ISCA 2023 (International Symposium on Computer Architecture) in the Industry Track and is available on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Machine learning ( ML ) models continue to evolve in scale and algorithmically , a trend that burdens system developers with ever-increasing compute and power requirements for training and deployment . How to meet the demands ? Tech giant Nvidia ’ s A100 high-performance graphics processing unit ( GPU ) dominated the AI accelerator market until Google joined the campaign in 2016 with its Tensor Processing Units ( TPUs ) . This week , Google introduced its newest entry , a TPU-based supercomputer it says is both faster and more efficient than the A100 . In the new paper TPU v4 : An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support , a Google Research team presents TPU v4 , the company ’ s latest supercomputer . The TPU v4 is ten times faster than v3 and 1.2–1.7x faster than Nvidia A100 GPUs while using 1.3x–1.9x less power . Google believes the performance , scalability , and availability of TPU v4 will make it the new workhorse for today ’ s compute-hungry large language models ( LMMs ) . The team summarizes their paper ’ s main contributions as follows : A key improvement in this version is its leveraging of Optical Circuit Switches ( OCSes ) , which interconnect TPU v4 ’ s 4096 chips via optical data links to improve scale , availability , utilization , modularity , deployment , security , power and performance . The Google Palomar OCSes are based on 3D Micro-Electro-Mechanical Systems ( MEMS ) mirrors that switch in milliseconds ; and advance the state-of-the-art in terms of reliability and cost . Like TPU v3 , the TPU v4 package comprises two Tensor Cores ( TC ) . Each TC contains four 128×128 Matrix Multiply Units ( MXUs ) , a Vector Processing Unit ( VPU ) and a Vector Memory ( VMEM ) . Thanks to its OCSes , TPU v4 can quickly and easily change topology to adapt to the application , number of nodes , and system running a job — thus significantly improving training time . Each TPU v4 also incorporates SparseCores , dataflow processors that accelerate embedding-reliant models by 5x–7x yet use only 5 percent of the die area and power . The researchers ’ empirical study shows that TPU v4 is 2.1x faster and improves performance by 2.7x compared to TPU v3 , achieves ~4.3x–4.5x faster speeds than the Graphcore IPU Bow , and is 1.2x–1.7x faster than Nvidia A100s with 1.3x–1.9x less power consumption . The paper also reports that TPU v4s deployed in Google Cloud ’ s energy-optimized warehouse-scale computers use ~2-6x less energy and produce ~20x less CO2e than domain-specific architectures ( DSAs ) in typical data centres . The paper TPU v4 : An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings will be presented at ISCA 2023 ( International Symposium on Computer Architecture ) in the Industry Track and is available on arXiv . Author : Hecate He | Editor : Michael Sarazen We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['machine', 'learn', 'model', 'continue', 'evolve', 'scale', 'algorithmically', 'trend', 'burden', 'system', 'developer', 'everincrease', 'compute', 'power', 'requirement', 'training', 'deployment', 'meet', 'demand', 'tech', 'giant', 'highperformance', 'graphic', 'processing', 'unit', 'dominate', 'accelerator', 'market', 'join', 'campaign', 'tensor', 'processing', 'unit', 'tpus', 'week', 'introduce', 'new', 'entry', 'tpubase', 'supercomputer', 'say', 'fast', 'efficient', 'new', 'paper', 'tpu', 'optically', 'reconfigurable', 'supercomputer', 'machine', 'learn', 'hardware', 'support', 'research', 'team', 'present', 'company', 'late', 'supercomputer', 'time', 'fast', 'fast', 'gpu', 'use', 'less', 'power', 'believe', 'performance', 'scalability', 'availability', 'make', 'new', 'workhorse', 'today', 'computehungry', 'large', 'language', 'model', 'lmms', 'team', 'summarize', 'paper', 'main', 'contribution', 'follow', 'key', 'improvement', 'version', 'leveraging', 'optical', 'circuit', 'switch', 'ocse', 'interconnect', 'chip', 'optical', 'datum', 'link', 'improve', 'scale', 'availability', 'utilization', 'modularity', 'deployment', 'security', 'power', 'performance', 'ocse', 'base', '3d', 'microelectromechanical', 'system', 'mem', 'mirror', 'switch', 'millisecond', 'advance', 'stateoftheart', 'term', 'reliability', 'cost', 'package', 'comprise', 'tensor', 'core', 'tc', 'tc', 'contain', 'matrix', 'multiply', 'unit', 'mxus', 'vector', 'processing', 'unit', 'vpu', 'vector', 'memory', 'vmem', 'thank', 'ocse', 'quickly', 'easily', 'change', 'topology', 'adapt', 'application', 'number', 'node', 'system', 'run', 'job', 'thus', 'significantly', 'improve', 'training', 'time', 'also', 'incorporate', 'sparsecore', 'dataflow', 'processor', 'accelerate', 'embeddingreliant', 'model', 'yet', 'use', 'percent', 'die', 'area', 'power', 'researcher', 'empirical', 'study', 'show', '21x', 'fast', 'improve', 'performance', '27x', 'compare', 'achieve', 'fast', 'speed', 'bow', 'fast', 'a100s', 'less', 'power', 'consumption', 'paper', 'also', 'report', 'deploy', 'energyoptimize', 'warehousescale', 'computer', 'use', '26x', 'less', 'energy', 'produce', '20x', 'less', 'co2e', 'architecture', 'dsa', 'typical', 'data', 'centre', 'paper', 'tpu', 'optically', 'reconfigurable', 'supercomputer', 'machine', 'learn', 'hardware', 'support', 'embedding', 'present', 'isca', 'international', 'symposium', 'computer', 'architecture', 'industry', 'track', 'available', 'arxiv', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']"
AI Needs a Therapist: Columbia U & IBM’s SafeguardGPT Leverages Psychotherapy & RL to Build Healthy AI Systems,https://syncedreview.com/2023/04/05/ai-needs-a-therapist-columbia-u-ibms-safeguardgpt-leverages-psychotherapy-rl-to-build-healthy-ai-systems/,2023-04-05,"
In the new paper Towards Healthy AI: Large Language Models Need Therapists Too, a team from Columbia University and IBM Research proposes SafeguardGPT, a framework that incorporates psychotherapy and reinforcement learning to correct the potentially harmful behaviours of AI chatbots.
","The recent emergence of publicly accessible chatbots capable of responding to diverse queries and engaging in natural and humanlike conversations has put AI in the public spotlight like never before. Reaction to these large language model-based systems has ranged from amazement with regard to their generative abilities to apprehension concerning potential societal and ethical risks due to ingrained biases and other harmful behaviours. In the new paper Towards Healthy AI: Large Language Models Need Therapists Too, a team from Columbia University and IBM Research proposes SafeguardGPT, a framework that incorporates psychotherapy and reinforcement learning (RL) to correct the potentially harmful behaviours of AI chatbots and make them “safe, trustworthy, and ethical.” The team grounds their work on the premise that a healthy and trustworthy AI system should align with human values and abide by social norms and standards when interacting with users. To this end, they propose adopting psychotherapy techniques to guide chatbots to a better understanding of the nuances of human interaction and identify problem areas. They believe their approach could make chatbots more trustworthy and reliable, less likely to develop biases and stereotypes, and contribute to their development of empathy and emotional intelligence. The SafeguardGPT framework comprises four distinct AI agents — a Chatbot, a User, a Therapist, and a Critic — and four contexts: a Chat Room, where the AI user interacts with the AI chatbot in a natural language conversation; a Therapy Room, where the AI chatbot consults with the AI therapist in multiple sessions to receive guidance designed to improve its empathy and communication skills and correct for harmful behaviours or psychological problems; a Control Room, where a human moderator can pause the session to examine the AI chatbot’s state for diagnostic and interventional purposes; and an Evaluation Room, where the critic reads the historical interactions and determines whether the given conversation is “safe, ethical and good.” The SafeguardGPT framework leverages RL techniques to help the chatbot decide what context it should switch to and what action it should take in each context when interacting with a user. The paper provides a working example — simulating a social conversation between an AI chatbot and a hypothetical user — to evaluate their approach’s effectiveness. In the test, SafeguardGPT is shown to improve the chatbot’s communication skills and inject its outputs with empathy. The team acknowledges however that this “is not true empathy, but rather a form of language-based simulation,” stressing that AI systems cannot replace genuine human interaction and emotions “at this current state.” Overall, this work opens a promising path toward the development of more healthy, human-centric and responsible AI systems. The paper Towards Healthy AI: Large Language Models Need Therapists Too is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The recent emergence of publicly accessible chatbots capable of responding to diverse queries and engaging in natural and humanlike conversations has put AI in the public spotlight like never before . Reaction to these large language model-based systems has ranged from amazement with regard to their generative abilities to apprehension concerning potential societal and ethical risks due to ingrained biases and other harmful behaviours . In the new paper Towards Healthy AI : Large Language Models Need Therapists Too , a team from Columbia University and IBM Research proposes SafeguardGPT , a framework that incorporates psychotherapy and reinforcement learning ( RL ) to correct the potentially harmful behaviours of AI chatbots and make them “ safe , trustworthy , and ethical. ” The team grounds their work on the premise that a healthy and trustworthy AI system should align with human values and abide by social norms and standards when interacting with users . To this end , they propose adopting psychotherapy techniques to guide chatbots to a better understanding of the nuances of human interaction and identify problem areas . They believe their approach could make chatbots more trustworthy and reliable , less likely to develop biases and stereotypes , and contribute to their development of empathy and emotional intelligence . The SafeguardGPT framework comprises four distinct AI agents — a Chatbot , a User , a Therapist , and a Critic — and four contexts : a Chat Room , where the AI user interacts with the AI chatbot in a natural language conversation ; a Therapy Room , where the AI chatbot consults with the AI therapist in multiple sessions to receive guidance designed to improve its empathy and communication skills and correct for harmful behaviours or psychological problems ; a Control Room , where a human moderator can pause the session to examine the AI chatbot ’ s state for diagnostic and interventional purposes ; and an Evaluation Room , where the critic reads the historical interactions and determines whether the given conversation is “ safe , ethical and good. ” The SafeguardGPT framework leverages RL techniques to help the chatbot decide what context it should switch to and what action it should take in each context when interacting with a user . The paper provides a working example — simulating a social conversation between an AI chatbot and a hypothetical user — to evaluate their approach ’ s effectiveness . In the test , SafeguardGPT is shown to improve the chatbot ’ s communication skills and inject its outputs with empathy . The team acknowledges however that this “ is not true empathy , but rather a form of language-based simulation , ” stressing that AI systems can not replace genuine human interaction and emotions “ at this current state. ” Overall , this work opens a promising path toward the development of more healthy , human-centric and responsible AI systems . The paper Towards Healthy AI : Large Language Models Need Therapists Too is on arXiv . Author : Hecate He | Editor : Michael Sarazen We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['recent', 'emergence', 'publicly', 'accessible', 'chatbot', 'capable', 'respond', 'diverse', 'query', 'engage', 'natural', 'humanlike', 'conversation', 'put', 'ai', 'public', 'spotlight', 'never', 'reaction', 'large', 'language', 'modelbase', 'system', 'range', 'amazement', 'regard', 'generative', 'ability', 'apprehension', 'concern', 'potential', 'societal', 'ethical', 'risk', 'due', 'ingrain', 'bias', 'harmful', 'behaviour', 'new', 'paper', 'healthy', 'ai', 'large', 'language', 'model', 'need', 'therapist', 'team', 'research', 'propose', 'safeguardgpt', 'framework', 'incorporate', 'psychotherapy', 'reinforcement', 'learning', 'rl', 'correct', 'potentially', 'harmful', 'behaviour', 'chatbot', 'make', 'safe', 'trustworthy', 'ethical', 'team', 'ground', 'work', 'premise', 'healthy', 'trustworthy', 'ai', 'system', 'align', 'human', 'value', 'abide', 'social', 'norm', 'standard', 'interact', 'user', 'end', 'propose', 'adopt', 'psychotherapy', 'technique', 'guide', 'chatbot', 'well', 'understanding', 'nuance', 'human', 'interaction', 'identify', 'problem', 'area', 'believe', 'approach', 'make', 'chatbot', 'trustworthy', 'reliable', 'less', 'likely', 'develop', 'bias', 'stereotype', 'contribute', 'development', 'empathy', 'emotional', 'intelligence', 'safeguardgpt', 'framework', 'comprise', 'distinct', 'agent', 'chatbot', 'user', 'therapist', 'critic', 'context', 'chat', 'room', 'user', 'interact', 'chatbot', 'natural', 'language', 'conversation', 'therapy', 'room', 'chatbot', 'consult', 'therapist', 'multiple', 'session', 'receive', 'guidance', 'design', 'improve', 'empathy', 'communication', 'skill', 'correct', 'harmful', 'behaviour', 'psychological', 'problem', 'control', 'room', 'human', 'moderator', 'pause', 'session', 'examine', 'state', 'diagnostic', 'interventional', 'purpose', 'evaluation', 'room', 'critic', 'read', 'historical', 'interaction', 'determine', 'give', 'conversation', 'safe', 'ethical', 'good', 'safeguardgpt', 'framework', 'leverage', 'rl', 'technique', 'help', 'chatbot', 'decide', 'context', 'switch', 'action', 'take', 'context', 'interact', 'user', 'paper', 'provide', 'work', 'example', 'simulate', 'social', 'conversation', 'chatbot', 'hypothetical', 'user', 'evaluate', 'approach', 'effectiveness', 'test', 'safeguardgpt', 'show', 'improve', 'chatbot', 'communication', 'skill', 'inject', 'output', 'empathy', 'team', 'acknowledge', 'however', 'true', 'empathy', 'rather', 'form', 'languagebase', 'simulation', 'stress', 'system', 'replace', 'genuine', 'human', 'interaction', 'emotion', 'current', 'state', 'overall', 'work', 'open', 'promising', 'path', 'development', 'healthy', 'humancentric', 'responsible', 'system', 'paper', 'healthy', 'ai', 'large', 'language', 'model', 'need', 'therapist', 'arxiv', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']"
Bloomberg & JHU’s BloombergGPT: ‘A Best-in-Class LLM for Financial NLP’,https://syncedreview.com/2023/04/04/bloomberg-jhus-bloomberggpt-a-best-in-class-llm-for-financial-nlp/,2023-04-04,"
In the new paper BloombergGPT: A Large Language Model for Finance, a research team from Bloomberg and Johns Hopkins University presents BloombergGPT, a 50 billion parameter language model trained on a 700 billion token dataset that significantly outperforms current benchmark models on financial tasks.
","Large language models (LLMs) popularized by the GPT family have shown impressive language processing, understanding and generating capabilities across diverse domains. Many industry researchers are now exploring ways to improve the task-specific performance of such models and integrate them into their workflows, with US firm Bloomberg emerging as a first-mover in the financial domain. In the new paper BloombergGPT: A Large Language Model for Finance, a research team from Bloomberg and Johns Hopkins University presents BloombergGPT, a 50 billion parameter language model trained on a 700 billion token dataset that significantly outperforms current benchmark models on financial tasks. Bloomberg’s goal was to train an LLM capable of achieving best results across a wide range of financial tasks while maintaining competitive performance on general-purpose LLM benchmarks. To this end, the team first leveraged Bloomberg’s extensive data sources to compile what they believe to be the largest-ever finance-specific dataset, comprising 363 billion tokens. This was augmented with various public datasets to reach a total of 700 billion tokens and used to train their 50 billion parameter BloombergGPT model. BloombergGPT is a decoder-only causal LLM based on the BLOOM (Scao et al., 2022) architecture, comprising 70 layers of transformer decoder blocks with multi-head self-attention, layer-normalization, and a feed-forward network with one hidden layer. The team used the Amazon AWS SageMaker service for model training and evaluation and the proprietary SageMaker Model Parallelism (SMP) for efficient parallel computing. In their empirical study, the team compared BloombergGPT with larger baseline models — GPT-NeoX (Black et al., 2022), OPT66B (Zhang et al., 2022a) and BLOOM176B (Scao et al., 2022) — on finance-specific and general-purpose benchmarks. In the experiments, BloombergGPT achieved the best performance on most financial tasks and comparable or better performance on the general-purpose benchmarks. “We see tremendous value in having developed the first LLM focused on the financial domain,” says Bloomberg Chief Technology Officer Shawn Edwards, “BloombergGPT will enable us to tackle many new types of applications, while it delivers much higher performance out-of-the-box than custom models for each application, at a faster time-to-market.” The paper BloombergGPT: A Large Language Model for Finance is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates. ","Large language models ( LLMs ) popularized by the GPT family have shown impressive language processing , understanding and generating capabilities across diverse domains . Many industry researchers are now exploring ways to improve the task-specific performance of such models and integrate them into their workflows , with US firm Bloomberg emerging as a first-mover in the financial domain . In the new paper BloombergGPT : A Large Language Model for Finance , a research team from Bloomberg and Johns Hopkins University presents BloombergGPT , a 50 billion parameter language model trained on a 700 billion token dataset that significantly outperforms current benchmark models on financial tasks . Bloomberg ’ s goal was to train an LLM capable of achieving best results across a wide range of financial tasks while maintaining competitive performance on general-purpose LLM benchmarks . To this end , the team first leveraged Bloomberg ’ s extensive data sources to compile what they believe to be the largest-ever finance-specific dataset , comprising 363 billion tokens . This was augmented with various public datasets to reach a total of 700 billion tokens and used to train their 50 billion parameter BloombergGPT model . BloombergGPT is a decoder-only causal LLM based on the BLOOM ( Scao et al. , 2022 ) architecture , comprising 70 layers of transformer decoder blocks with multi-head self-attention , layer-normalization , and a feed-forward network with one hidden layer . The team used the Amazon AWS SageMaker service for model training and evaluation and the proprietary SageMaker Model Parallelism ( SMP ) for efficient parallel computing . In their empirical study , the team compared BloombergGPT with larger baseline models — GPT-NeoX ( Black et al. , 2022 ) , OPT66B ( Zhang et al. , 2022a ) and BLOOM176B ( Scao et al. , 2022 ) — on finance-specific and general-purpose benchmarks . In the experiments , BloombergGPT achieved the best performance on most financial tasks and comparable or better performance on the general-purpose benchmarks . “ We see tremendous value in having developed the first LLM focused on the financial domain , ” says Bloomberg Chief Technology Officer Shawn Edwards , “ BloombergGPT will enable us to tackle many new types of applications , while it delivers much higher performance out-of-the-box than custom models for each application , at a faster time-to-market. ” The paper BloombergGPT : A Large Language Model for Finance is on arXiv . Author : Hecate He | Editor : Michael Sarazen We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['large', 'language', 'model', 'llm', 'popularize', 'show', 'impressive', 'language', 'processing', 'understanding', 'generating', 'capability', 'diverse', 'domain', 'many', 'industry', 'researcher', 'explore', 'way', 'improve', 'taskspecific', 'performance', 'model', 'integrate', 'workflow', 'emerge', 'firstmover', 'financial', 'domain', 'new', 'paper', 'bloomberggpt', 'large', 'language', 'model', 'finance', 'research', 'team', 'present', 'parameter', 'language', 'model', 'train', 'token', 'dataset', 'significantly', 'outperform', 'current', 'benchmark', 'model', 'financial', 'task', 'goal', 'train', 'llm', 'capable', 'achieve', 'good', 'result', 'wide', 'range', 'financial', 'task', 'maintain', 'competitive', 'performance', 'generalpurpose', 'llm', 'benchmark', 'end', 'team', 'first', 'leveraged', 'extensive', 'datum', 'source', 'compile', 'believe', 'largestever', 'financespecific', 'dataset', 'comprise', 'token', 'augment', 'various', 'public', 'dataset', 'reach', 'total', 'token', 'use', 'train', 'parameter', 'bloomberggpt', 'model', 'bloomberggpt', 'decoderonly', 'causal', 'llm', 'base', 'bloom', 'architecture', 'comprise', 'layer', 'transformer', 'decoder', 'block', 'multihead', 'selfattention', 'layernormalization', 'feedforward', 'network', 'hide', 'layer', 'team', 'use', 'sagemaker', 'service', 'model', 'training', 'evaluation', 'proprietary', 'sagemaker', 'model', 'parallelism', 'smp', 'efficient', 'parallel', 'computing', 'empirical', 'study', 'team', 'compare', 'bloomberggpt', 'large', 'baseline', 'model', 'gptneox', 'black', 'financespecific', 'generalpurpose', 'benchmark', 'experiment', 'bloomberggpt', 'achieve', 'good', 'performance', 'financial', 'task', 'comparable', 'well', 'performance', 'generalpurpose', 'benchmark', 'see', 'tremendous', 'value', 'develop', 'first', 'llm', 'focused', 'financial', 'domain', 'say', 'chief', 'technology', 'officer', 'shawn', 'bloomberggpt', 'enable', 'tackle', 'many', 'new', 'type', 'application', 'deliver', 'much', 'high', 'performance', 'outofthebox', 'custom', 'model', 'application', 'fast', 'timetomarket', 'paper', 'bloomberggpt', 'large', 'language', 'model', 'finance', 'arxiv', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']"
Meet TaskMatrix.AI: A Microsoft ‘Super-AI’ That Links Foundation Models With Millions of APIs to Perform Diverse Tasks,https://syncedreview.com/2023/04/03/meet-taskmatrix-ai-a-microsoft-super-ai-that-links-foundation-models-with-millions-of-apis-to-perform-diverse-tasks/,2023-04-03,"
In the new paper TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs, a Microsoft research team proposes TaskMatrix.AI, a novel ecosystem that connects foundation models with millions of existing models and system APIs to build a “super-AI” capable of addressing a wide range of digital and physical tasks.
","Humans’ development and use of complex language and tools are two fundamental ways we differ from other animals. The recent emergence of foundational AI models trained on massive amounts of unlabelled data and capable of generating humanlike text outputs for various tasks has some speculating they may be the path to artificial general intelligence (AGI). However, despite their game-changing performance, foundation models can still struggle with domain-specific tasks such as mathematical calculations. Maybe these foundation models simply need the right tools to take the next evolutionary leap? In the new paper TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs, a Microsoft research team proposes TaskMatrix.AI, a novel ecosystem that connects foundation models with millions of existing models and system APIs to build a “super-AI” capable of addressing a wide range of digital and physical tasks. While many existing symbolic- or neural-based AI models and systems are able to efficiently address domain-specific tasks, their different implementations and working mechanisms and various compatibility issues can make them difficult for foundation models to access. This work aims to solve that. The team summarizes the key advantages of TaskMatrix.AI as follows: TaskMatrix.AI comprises four key components: 1) A Multimodal Conversational Foundation Model (MCFM) is used to communicate with users, understand their goals and multimodal context, and generate API-based executable codes for performing specific tasks; 2) An API Platform provides a unified API documentation schema to store millions of APIs and enable developers to register, update and delete their APIs; 3) An API Selector recommends related APIs based on the MCFM’s understanding of users’ goals; and 4) An API Executor executes the generated action codes from the relevant APIs and return the results. The team also applies reinforcement learning with human feedback (RLHF) techniques to train a reward model and optimize TaskMatrix.AI with knowledge and insights gained from humans. This approach assists the MCFM and API selector in finding the optimal policy, speeds up convergence and results in better performance on complex tasks. In their empirical study, the team applied TaskMatrix.AI on the task of automatically generating PowerPoint slides for different companies, using ChatGPT as the MCFM. In the experiment, TaskMatrix.AI broke the task down into about 25 API calls to successfully generate multiple slides for different companies. TaskMatrix.AI also exhibited an understanding of user instructions based on PowerPoint content that enabled it, for example, to generate pages based on a company list and insert an appropriate logo based on the title of each page. Overall, this work demonstrates TaskMatrix.AI’s ability to improve performance on diversified tasks by connecting foundation models to various existing APIs. The team believes that — together with the continued development of foundation models, cloud services, robotics, and the Internet of things — TaskMatrix.AI has the potential to help build “an amazing future world, where productivity and creativity can reach new levels.” The paper TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Humans ’ development and use of complex language and tools are two fundamental ways we differ from other animals . The recent emergence of foundational AI models trained on massive amounts of unlabelled data and capable of generating humanlike text outputs for various tasks has some speculating they may be the path to artificial general intelligence ( AGI ) . However , despite their game-changing performance , foundation models can still struggle with domain-specific tasks such as mathematical calculations . Maybe these foundation models simply need the right tools to take the next evolutionary leap ? In the new paper TaskMatrix.AI : Completing Tasks by Connecting Foundation Models with Millions of APIs , a Microsoft research team proposes TaskMatrix.AI , a novel ecosystem that connects foundation models with millions of existing models and system APIs to build a “ super-AI ” capable of addressing a wide range of digital and physical tasks . While many existing symbolic- or neural-based AI models and systems are able to efficiently address domain-specific tasks , their different implementations and working mechanisms and various compatibility issues can make them difficult for foundation models to access . This work aims to solve that . The team summarizes the key advantages of TaskMatrix.AI as follows : TaskMatrix.AI comprises four key components : 1 ) A Multimodal Conversational Foundation Model ( MCFM ) is used to communicate with users , understand their goals and multimodal context , and generate API-based executable codes for performing specific tasks ; 2 ) An API Platform provides a unified API documentation schema to store millions of APIs and enable developers to register , update and delete their APIs ; 3 ) An API Selector recommends related APIs based on the MCFM ’ s understanding of users ’ goals ; and 4 ) An API Executor executes the generated action codes from the relevant APIs and return the results . The team also applies reinforcement learning with human feedback ( RLHF ) techniques to train a reward model and optimize TaskMatrix.AI with knowledge and insights gained from humans . This approach assists the MCFM and API selector in finding the optimal policy , speeds up convergence and results in better performance on complex tasks . In their empirical study , the team applied TaskMatrix.AI on the task of automatically generating PowerPoint slides for different companies , using ChatGPT as the MCFM . In the experiment , TaskMatrix.AI broke the task down into about 25 API calls to successfully generate multiple slides for different companies . TaskMatrix.AI also exhibited an understanding of user instructions based on PowerPoint content that enabled it , for example , to generate pages based on a company list and insert an appropriate logo based on the title of each page . Overall , this work demonstrates TaskMatrix.AI ’ s ability to improve performance on diversified tasks by connecting foundation models to various existing APIs . The team believes that — together with the continued development of foundation models , cloud services , robotics , and the Internet of things — TaskMatrix.AI has the potential to help build “ an amazing future world , where productivity and creativity can reach new levels. ” The paper TaskMatrix.AI : Completing Tasks by Connecting Foundation Models with Millions of APIs is on arXiv . Author : Hecate He | Editor : Michael Sarazen We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['human', 'development', 'use', 'complex', 'language', 'tool', 'fundamental', 'way', 'differ', 'animal', 'recent', 'emergence', 'model', 'train', 'massive', 'amount', 'unlabelled', 'datum', 'capable', 'generate', 'humanlike', 'text', 'output', 'various', 'task', 'speculating', 'path', 'artificial', 'general', 'intelligence', 'agi', 'however', 'gamechange', 'performance', 'foundation', 'model', 'still', 'struggle', 'domainspecific', 'task', 'mathematical', 'calculation', 'maybe', 'foundation', 'model', 'simply', 'need', 'right', 'tool', 'take', 'next', 'evolutionary', 'leap', 'new', 'paper', 'taskmatrixai', 'complete', 'task', 'connect', 'foundation', 'model', 'million', 'apis', 'research', 'team', 'propose', 'taskmatrixai', 'novel', 'ecosystem', 'connect', 'foundation', 'model', 'million', 'exist', 'model', 'system', 'apis', 'build', 'superai', 'capable', 'address', 'wide', 'range', 'digital', 'physical', 'task', 'many', 'exist', 'symbolic', 'neuralbase', 'ai', 'model', 'system', 'able', 'efficiently', 'address', 'domainspecific', 'task', 'different', 'implementation', 'work', 'mechanism', 'various', 'compatibility', 'issue', 'make', 'difficult', 'foundation', 'model', 'access', 'work', 'aim', 'solve', 'team', 'summarize', 'key', 'advantage', 'taskmatrixai', 'follow', 'taskmatrixai', 'comprise', 'key', 'component', 'multimodal', 'conversational', 'foundation', 'model', 'mcfm', 'use', 'communicate', 'user', 'understand', 'goal', 'multimodal', 'context', 'generate', 'apibase', 'executable', 'code', 'perform', 'specific', 'task', 'api', 'platform', 'provide', 'unified', 'api', 'documentation', 'schema', 'store', 'million', 'apis', 'enable', 'developer', 'register', 'update', 'delete', 'apis', 'api', 'selector', 'recommend', 'related', 'apis', 'base', 'mcfm', 'understanding', 'user', 'goal', 'api', 'executor', 'execute', 'generate', 'action', 'code', 'relevant', 'apis', 'return', 'result', 'team', 'also', 'apply', 'reinforcement', 'learning', 'human', 'feedback', 'technique', 'train', 'reward', 'model', 'optimize', 'taskmatrixai', 'knowledge', 'insight', 'gain', 'human', 'approach', 'assist', 'mcfm', 'api', 'selector', 'find', 'optimal', 'policy', 'speed', 'convergence', 'result', 'well', 'performance', 'complex', 'task', 'empirical', 'study', 'team', 'apply', 'taskmatrixai', 'task', 'automatically', 'generate', 'powerpoint', 'slide', 'different', 'company', 'use', 'chatgpt', 'mcfm', 'experiment', 'taskmatrixai', 'break', 'task', 'api', 'call', 'successfully', 'generate', 'multiple', 'slide', 'different', 'company', 'taskmatrixai', 'also', 'exhibit', 'understanding', 'user', 'instruction', 'base', 'powerpoint', 'content', 'enable', 'example', 'generate', 'page', 'base', 'company', 'list', 'insert', 'appropriate', 'logo', 'base', 'title', 'page', 'overall', 'work', 'demonstrate', 'taskmatrixai', 'ability', 'improve', 'performance', 'diversify', 'task', 'connect', 'foundation', 'model', 'various', 'exist', 'apis', 'team', 'believe', 'together', 'continue', 'development', 'foundation', 'model', 'cloud', 'service', 'robotic', 'internet', 'thing', 'taskmatrixai', 'potential', 'help', 'build', 'amazing', 'future', 'world', 'productivity', 'creativity', 'reach', 'new', 'level', 'paper', 'taskmatrixai', 'complete', 'task', 'connect', 'foundation', 'model', 'million', 'apis', 'arxiv', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']"
